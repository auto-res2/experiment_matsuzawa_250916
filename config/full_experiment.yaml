name: 'full_experiment'
experiment_ids_to_run: [1, 2, 3]

paths:
  # IMPORTANT: User must place the MIMIC-IV 'discharge.csv' in this directory.
  mimic_iv_dir: 'data/raw/mimic-iv-2.2/note/'
  processed_data_path: 'data/full_experiment'
  training_output_path: '.research/iteration12/training_full'

api_details:
  # Set to 'openai' to use the GPT-4 grader. Requires OPENAI_API_KEY.
  # Set to 'huggingface' to use local/simulated models.
  provider: 'openai'

model_names:
  encoder: 'cambridgeltl/trans-encoder-bi-simcse-roberta-base'
  generator: 'gpt-3.5-turbo-1106'
  scorer: 'gpt-4-0613'
  paraphraser: 'lmsys/vicuna-13b-v1.5'
  continual_backend: 'NousResearch/Llama-2-70b-chat-hf'

seeds: [101, 202, 303, 404, 505]

# --- Experiment 1 Config ---
experiment_1:
  tasks: ['HIPAA-MedTriage', 'FinReg-Compliance']

# --- Experiment 2 Config ---
experiment_2:
  methods: ['REFLECT-BO', 'REFLECT-BO-NoDP', 'REFLECT-BO-NoShift']
  tasks: ['Super-NI']

# --- Experiment 3 Config ---
experiment_3:
  tasks: ['GSM8K', 'creative_story']

sec_edgar_ciks: 
  - '0000320193' # Apple
  - '0001018724' # Alphabet (Google)
  - '0000789019' # Microsoft
  - '0001652044' # Meta Platforms
  - '0001045810' # JPMorgan Chase

model_drift_schedule:
  # Pre-fine-tuned Llama-2 checkpoints should be specified here.
  # Using variants as placeholders for the drift mechanism.
  - 'NousResearch/Llama-2-70b-chat-hf'
  - 'meta-llama/Llama-2-70b-chat-hf' # Simulate a slight variant
  - 'togethercomputer/Llama-2-70B-32K-Instruct'

optimization_params:
  api_budget: 60
  bootstrap_budget: 10

hyperparameters:
  learning_rate: [0.0001, 0.0003, 0.001]
  bo_beta: [0.1, 0.25, 0.5]
  cvar_alpha: [0.9, 0.95]

federated_params:
  n_clients: 20
  episodes: 5

dp_params:
  epsilon: 1.0
  delta: 1.0e-6
  noise_multiplier: 1.1
  max_grad_norm: 1.0

kalman_params:
  tau: [1.5, 2.5, 4.0]