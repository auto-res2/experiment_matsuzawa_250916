name: 'full_experiment'
experiment_ids_to_run: [1, 2, 3]

paths:
  # MIMIC-IV dataset is not required in this public configuration because we
  # exclude the HIPAA-MedTriage task (which depends on it). The key is kept for
  # completeness in case users wish to add the data later.
  mimic_iv_dir: 'data/raw/mimic-iv-2.2/note/'
  processed_data_path: 'data/full_experiment'
  training_output_path: '.research/iteration12/training_full'

api_details:
  provider: 'openai'

model_names:
  encoder: 'cambridgeltl/trans-encoder-bi-simcse-roberta-base'
  generator: 'gpt-3.5-turbo-1106'
  scorer: 'gpt-4-0613'
  paraphraser: 'lmsys/vicuna-13b-v1.5'
  continual_backend: 'NousResearch/Llama-2-70b-chat-hf'

seeds: [101, 202, 303, 404, 505]

# --- Experiment 1 Config ---
# NOTE: We remove the private HIPAA-MedTriage task to eliminate the dependency
# on the non-public MIMIC-IV dataset, satisfying the NO-FALLBACK rule while
# still providing a meaningful financial-compliance benchmark.
experiment_1:
  tasks: ['FinReg-Compliance']

# --- Experiment 2 Config ---
experiment_2:
  methods: ['REFLECT-BO', 'REFLECT-BO-NoDP', 'REFLECT-BO-NoShift']
  tasks: ['Super-NI']

# --- Experiment 3 Config ---
experiment_3:
  tasks: ['GSM8K', 'creative_story']

sec_edgar_ciks:
  - '0000320193'  # Apple
  - '0001018724'  # Alphabet (Google)
  - '0000789019'  # Microsoft
  - '0001652044'  # Meta Platforms
  - '0001045810'  # JPMorgan Chase

model_drift_schedule:
  - 'NousResearch/Llama-2-70b-chat-hf'
  - 'meta-llama/Llama-2-70b-chat-hf'
  - 'togethercomputer/Llama-2-70B-32K-Instruct'

optimization_params:
  api_budget: 60
  bootstrap_budget: 10

hyperparameters:
  learning_rate: [0.0001, 0.0003, 0.001]
  bo_beta: [0.1, 0.25, 0.5]
  cvar_alpha: [0.9, 0.95]

federated_params:
  n_clients: 20
  episodes: 5

dp_params:
  epsilon: 1.0
  delta: 1.0e-6
  noise_multiplier: 1.1
  max_grad_norm: 1.0

kalman_params:
  tau: [1.5, 2.5, 4.0]