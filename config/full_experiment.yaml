name: 'full_experiment'
experiment_ids_to_run: [1, 2, 3]

paths:
  mimic_iv_dir: 'data/raw/mimic-iv-2.2/'
  processed_data_path: 'artifacts/data/full_experiment'
  training_output_path: '.research/iteration23/full_experiment_runs'

model_names:
  encoder: 'cambridgeltl/trans-encoder-bi-simcse-roberta-base'
  generator: 'gpt-3.5-turbo-1106'
  scorer: 'gpt-4-0613'
  paraphraser: 'lmsys/vicuna-13b-v1.5'
  continual_backend: 'NousResearch/Llama-2-70b-chat-hf'

seeds: [13, 42, 101, 256, 512]

# --- Experiment 1: Zero/Low-Log Bootstrap in Regulated Domains ---
experiment_1:
  tasks: ['HIPAA-MedTriage-5', 'FinReg-Compliance-5']
  methods: ['REFLECT-BO', 'SHIFT-BO', 'MetaBO-Prompt', 'Random']

# --- Experiment 2: Continual Private Adaptation under Model Drift ---
experiment_2:
  tasks: ['Super-NI']
  methods: ['REFLECT-BO', 'REFLECT-BO-NoShift', 'REFLECT-BO-NoDP', 'SHIFT-BO']

# --- Experiment 3: Human-in-the-Loop Pareto Front Steering ---
experiment_3:
  tasks: ['GSM8K', 'Super-NI'] # Use two distinct tasks for human study
  methods: ['REFLECT-BO', 'REFLECT-BO_headless', 'manual_playground'] # Special methods for Exp3

sec_edgar_ciks: 
  - '0000320193' # Apple
  - '0001018724' # Alphabet
  - '0000789019' # Microsoft
  - '0001652044' # Meta
  - '0001045810' # JPMorgan Chase

optimization_params:
  api_budget: 60
  bootstrap_budget: 10

hyperparameters:
  learning_rate: [0.0001, 0.0003, 0.001]
  bo_beta: [0.1, 0.25, 0.5]
  cvar_alpha: [0.9, 0.95]

federated_params:
  n_clients: 20
  episodes: 5

dp_params:
  epsilon: 1.0
  delta: 1.0e-6
  noise_multiplier: 1.1
  max_grad_norm: 1.0

kalman_params:
  tau: [1.5, 2.5, 4.0]
