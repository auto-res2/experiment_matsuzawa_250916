seed: 17
# Still toy hyper-params; replace with production values when the real code is available.

data:
  sample_size: 30

model:
  input_dim: 16
  output_dim: 4

training:
  epochs: 2
  lr: 0.001
  batch_size: 8

misc:
  notes: |
    Full-scale experiment placeholder.  Populate with full dataset names and
    hyper-parameters once the actual training logic is integrated.
  external_datasets:
    - facebook/flores
    - code_search_net
    - openslr/librispeech_asr
    - mozilla-foundation/common_voice_11_0
    - google/speech_commands
    - laion/laion400m
    - facebook/mustc
    - gradientrouting-spar/mc9_badmed_naive_atd-safety_seed_1_epoch_1
  env_hf_token: "${HF_TOKEN}"
  device: auto
  save_dir: .research/iteration1/artifacts
