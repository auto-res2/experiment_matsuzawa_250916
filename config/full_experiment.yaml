name: 'full_experiment_config'
experiment_to_run: 1

paths:
  secure_data_path: '/mnt/secure_cluster/data'
  processed_data_path: 'data/processed/full_experiment'
  training_output_path: '.research/iteration2/training/full_experiment'
  evaluation_output_path: '.research/iteration2/analysis/full_experiment'

api_details:
  provider: 'openai'

model_names:
  encoder: 'cambridgeltl/trans-encoder-bi-simcse-roberta-base'
  generator: 'gpt-3.5-0613'
  scorer: 'gpt-4-0125'
  paraphraser: 'lmsys/vicuna-13b-v1.5'
  continual_backend: 'NousResearch/Llama-2-70b-chat-hf'

seeds: [0, 1, 2, 3, 4]

experiment_1:
  datasets: ['HIPAA-MedTriage-5', 'FinReg-Compliance-5']

experiment_2:
  methods: ['REFLECT-BO', 'REFLECT-BO-NoShift', 'REFLECT-BO-NoDP', 'SHIFT-BO']
  total_prompts: 5000

experiment_3:
  datasets: ['gsm8k', 'aqua_rat', 'cnn_dailymail']

optimization_params:
  api_budget: 60
  bootstrap_budget: 10

hyperparameters:
  learning_rate: [0.0001, 0.0003, 0.001]
  bo_beta: [0.1, 0.25, 0.5]
  cvar_alpha: [0.9, 0.95]

federated_params:
  n_clients: 20
  episodes: 5

dp_params:
  epsilon: 1.0
  delta: 1.0e-6
  noise_multiplier: 1.1
  max_grad_norm: 1.0

kalman_params:
  tau: [1.5, 2.5, 4.0]