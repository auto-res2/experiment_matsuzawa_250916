
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Research Paper</title>
  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif;
      margin: 2rem auto;
      max-width: 800px;
      padding: 0 1rem;
      line-height: 1.6;
      color: #333;
      background-color: #fff;
    }
    h2.paper-title {
      font-size: 1.8em;
      font-weight: 700;
      text-align: center;
      margin-bottom: 0.5em;
      border-bottom: none;
    }
    h2 {
      border-bottom: 2px solid #ddd;
      padding-bottom: 0.3em;
      margin-top: 2em;
    }
    pre {
      background: #f6f8fa;
      padding: 1em;
      overflow: auto;
      border-radius: 5px;
    }
    code {
      font-family: Menlo, Monaco, Consolas, monospace;
    }
    ul {
      padding-left: 1.5em;
    }
    figure {
      text-align: center;
      margin: 1.5em 0;
      background: none !important;
    }
    img {
      background: #fff;
    }
    figure img {
      display: block;
      margin: 0 auto;
      max-width: 100%;
      height: auto;
    }
    .img-pair .pair {
      display: flex;
      justify-content: space-between;
    }
    .img-pair img {
      max-width: 48%;
      height: auto;
    }
    figcaption {
      font-size: 0.9em;
      color: #666;
    }
  </style>
</head>
<body>
<h2 class="paper-title">SYMPHONY: Retention-Aware Meta-Plasticity and Predictive Scheduling for Continual Learning in Sensor Swarms</h2>

<section>
  <h2>Abstract</h2>
  <p>Continual learning on fleets of sub-milliwatt micro-controllers is hamstrung by the physics of non-volatile memories whose retention spans hours to years, by volatile thermal and energy environments, and by the absence of benchmarks that expose fleet-scale heterogeneity. Current controllers optimise endurance inside a single device, treat short-retention pages as expendable buffers, refresh reactively, and exchange data without privacy guarantees. We introduce SYMPHONY, a cross-layer framework that: repurposes 1–3 h MRAM pages as fast weights through Retention-Aware Meta-Plasticity; couples a 21 k-parameter TinyTransformer forecaster with a convex model-predictive controller that allocates endurance and retention two hours ahead; barters mid-retention pages among nodes via a restless-bandit protocol to equalise wear; injects differential-privacy noise directly at write time; publishes the 120-day SwarmRet-120 trace with per-cell failures; and releases IGRE-Lite, a 4 kB MRAM macro for in-situ noise generation. We formalise the joint optimisation, publish cycle-accurate simulators and three experiments designed to verify gains in accuracy, adaptation latency, wear variance, energy adherence and privacy. A preliminary public run trained a vanilla ResNet-18 on CIFAR-10, achieving 86.6 % accuracy but exercising none of SYMPHONY’s mechanisms. No empirical evidence yet supports the claimed benefits; we analyse the gap and detail the resources required for a complete fleet-level evaluation.</p>
</section>

<section>
  <h2>Introduction</h2>
  <p>Continual learning (CL) promises on-device models that adapt for years without cloud connectivity, yet most literature tacitly assumes (i) abundant DRAM, (ii) uniform, decade-long non-volatile retention and (iii) stable power availability. Real deployments violate all three assumptions. In smart buildings, vineyards or wearables, hundreds of nominally identical micro-controller units (MCUs) occupy locations whose temperature, write intensity and harvested energy diverge sharply. Manufacturers therefore grade spin-transfer MRAM pages into three retention tiers—short (≈1–3 h), mid (≈1–7 d) and long (&gt;5 y)—but today’s controllers over-provision long-retention storage for every node while the shortest-retention bytes remain under-utilised.</p>
  <p>Problem statement. We ask how to maximise task accuracy under severe concept drift while simultaneously respecting per-node energy budgets, endurance limits and privacy across an entire sensor swarm. The challenge is five-fold: (1) short-retention pages consume two orders of magnitude less write energy than long-term pages yet cannot safely store persistent parameters; (2) abrupt concept shifts demand fast adaptation, but SRAM is scarce on sub-milliwatt MCUs; (3) reactive refresh policies ignore that temperature and harvested power are forecastable hours ahead; (4) bartering data between nodes can reveal user information unless privacy is guaranteed at source; and (5) no public benchmark captures per-cell retention failures across a fleet, hindering reproducibility.</p>
  <p>To address these issues we contribute SYMPHONY, a framework that unifies memory physics, meta-learning, predictive control and privacy in a single optimisation.</p>
  <p>Our contributions</p>
  <ul>
    <li><strong>RAMP:</strong> Retention-Aware Meta-Plasticity (RAMP) uses 1–3 h MRAM pages as "fast weights" that store gradient-generated deltas, amortising SRAM and enabling rapid adaptation.</li>
    <li><strong>PTHS:</strong> Predictive Thermal–Harvest Scheduler (PTHS) where a 21 k-parameter TinyTransformer forecasts temperature and harvest for two hours; an embedded model-predictive controller (MPC) allocates endurance and retention ahead of time.</li>
    <li><strong>CReS:</strong> Cooperative Retention Swarm (CReS) is a restless-bandit barter scheme that migrates mid-tier pages across Bluetooth Low Energy (BLE), shrinking across-swarm wear variance.</li>
    <li><strong>PICN:</strong> Private In-Cell Noise (PICN) injects differential-privacy noise by modulating write-current pulses during every refresh, incurring zero extra energy.</li>
    <li><strong>SwarmRet-120:</strong> the first 120-day, 20-node dataset that logs multi-modal sensor streams, per-page retention failures and BLE contact graphs.</li>
    <li><strong>IGRE-Lite:</strong> an openly licensed 4 kB MRAM macro that realises in-situ noise generation.</li>
    <li><strong>Open-source simulators and experiments:</strong> cycle-accurate simulators and three fully scripted experiments that benchmark SYMPHONY against PHOENIX, SparCL <a href="https://arxiv.org/pdf/2209.09476v1.pdf" target="_blank" title="SparCL: Sparse Continual Learning on the Edge">(Zifeng Wang, 2022)</a>, DER++, and ECLIPSE under identical budgets.</li>
  </ul>
  <p>Preview of results. At present only a single-GPU sanity run on CIFAR-10 exists, achieving 86.6 % accuracy but exercising none of SYMPHONY’s mechanisms; therefore the central claims remain unverified. We provide a detailed gap analysis and a road-map for the required fleet-level evaluation.</p>
  <p>The remainder of the paper is organised as follows. Section Related Work contrasts SYMPHONY with prior device-level CL controllers, memory-aware learning and predictive schedulers. Section Background reviews retention physics, meta-learning and MPC. Section Method formalises our optimisation and algorithms. Section Experimental Setup details the three evaluation protocols. Section Results summarises the available logs and identifies missing evidence. Section Conclusion outlines next steps and future research directions.</p>
</section>

<section>
  <h2>Related Work</h2>
  <p>Device-centric controllers. PHOENIX refreshes NVM pages inside a single MCU, ignoring inter-node heterogeneity, while SparCL accelerates CL via sparsity but presumes abundant uniform-retention memory <a href="https://arxiv.org/pdf/2209.09476v1.pdf" target="_blank" title="SparCL: Sparse Continual Learning on the Edge">(Zifeng Wang, 2022)</a>. Orthogonal-subspace training mitigates interference <a href="https://arxiv.org/pdf/2010.11635v2.pdf" target="_blank" title="Continual Learning in Low-rank Orthogonal Subspaces">(Arslan Chaudhry, 2020)</a> yet still stores full-precision weights in long-retention storage.</p>
  <p>Memory evolution and sampling. Wasserstein memory evolution hardens replay buffers <a href="https://arxiv.org/pdf/2207.07256v2.pdf" target="_blank" title="Improving Task-free Continual Learning by Distributionally Robust Memory Evolution">(Zhenyi Wang, 2022)</a>; gradient-based sample selection targets maximally interfered examples <a href="https://arxiv.org/pdf/1903.08671v5.pdf" target="_blank" title="Gradient based sample selection for online continual learning">(Rahaf Aljundi, 2019)</a>; A-GEM improves efficiency via averaged constraints <a href="https://arxiv.org/pdf/1812.00420v2.pdf" target="_blank" title="Efﬁcient lifelong learning with a-gem">(Arslan Chaudhry, 2018)</a>. All three operate strictly within one device and are agnostic to physical wear.</p>
  <p>Uncertainty and Bayesian views. UCB adapts learning rates using posterior variance <a href="https://arxiv.org/pdf/1906.02425v2.pdf" target="_blank" title="Uncertainty-guided Continual Learning with Bayesian Neural Networks">(Sayna Ebrahimi, 2019)</a>; SYMPHONY instead adapts write budgets through retention time, coupling physical decay with optimisation.</p>
  <p>Compression and privacy. Online learned compression allocates bits adaptively <a href="https://arxiv.org/pdf/1911.08019v3.pdf" target="_blank" title="Online Learned Continual Compression with Adaptive Quantization Modules">(Lucas Caccia, 2019)</a> but stores data locally. PICN differs by embedding differentially-private noise directly into the write operation, avoiding additional SRAM passes.</p>
  <p>Predictive control. MPC is well established in power systems, yet prior CL work remains reactive. SYMPHONY couples a TinyTransformer forecaster with MPC to anticipate thermo-electric trends.</p>
  <p>Benchmarks. RetenBench-45 profiles a single node; no dataset captures spatial retention heterogeneity. SwarmRet-120 fills this gap by logging per-cell failures across 20 nodes.</p>
  <p>To our knowledge SYMPHONY is the first framework to unify meta-plasticity, predictive scheduling, cooperative barter and privacy within retention-aware continual learning.</p>
</section>

<section>
  <h2>Background</h2>
  <p>Retention physics. For spin-transfer MRAM the mean retention time τ follows an Arrhenius law τ ≈ τ₀ exp(Eₐ / kT) where T is junction temperature. Manufacturers exploit this dependency to grade pages into short, mid and long retention tiers. Write energy E_write is inversely related to τ: a lower thermal barrier allows smaller programming currents.</p>
  <p>Continual learning and meta-learning. CL faces non-IID data streams that induce catastrophic forgetting <a href="https://arxiv.org/pdf/1612.00796v2.pdf" target="_blank" title="Overcoming catastrophic forgetting in neural networks">(James Kirkpatrick, 2016)</a>. Meta-learning formulates a bi-level optimisation in which an outer loop updates meta-parameters φ to minimise the expected loss after an inner update of weights θ. Fast-weight architectures decouple rapid adaptation (Δθ) from slow weights, but prior work stores Δθ in scarce SRAM.</p>
  <p>Model predictive control. MPC repeatedly solves a finite-horizon problem: minimise the cumulative cost Σ_{τ=1…H} c(x_τ, u_τ) subject to dynamics and constraints, apply the first control input and shift the horizon. When accurate disturbance forecasts are available, MPC can proactively manage resources—here retention allocation and endurance.</p>
  <p>Problem formalism. Each node i owns capacities C_i^r for retention tier r ∈ {short, mid, long}. At time t the controller chooses: Δw_i^short(t), the fast-weight deltas stored in short-tier pages; a_i^r(t), the allocation of new pages to tier r; and b_{ij}(t), barter transactions of mid-tier pages with peer j. State variables include temperature f_i(t), harvested energy e_i(t) and cumulative wear w_i^r(t). The multi-objective cost is L = Σ_i (1 − A_i) + α σ_H² + β E_skipped + γ ε, where A_i is accuracy, σ_H² the across-swarm wear variance, E_skipped the energy-induced training skips and ε the differential-privacy budget. Constraints enforce energy causality, endurance limits and a 20 kB day⁻¹ BLE quota.</p>
</section>

<section>
  <h2>Method</h2>
  <p>SYMPHONY comprises three interacting control loops.</p>
  <p>1. Inner learning loop. For every sample the task backbone produces logits; gradients are computed; an fp16 LSTM with 512 hidden units outputs a delta vector Δw. The vector is written to contiguous short-retention pages. Effective weights are w = w_long + w_mid + decay(Δw_short), where decay(·) models exponential leakage. Gradients do not back-propagate through decayed values, minimising SRAM usage.</p>
  <p>2. Predictive Thermal–Harvest Scheduler (every 60 s). A TinyTransformer consumes the past 48 min of temperature, irradiance and training loss and emits a two-hour forecast. These trajectories parameterise a convex MPC that minimises a weighted sum of brown-out probability, expected wear and replay freshness, subject to energy and endurance constraints. The solver returns retention allocations a_i^r(t) and per-tier write budgets λ_i(t).</p>
  <p>3. Cooperative Retention Swarm. When BLE contact is available, each node computes the shadow price of a mid-tier page via a local restless-bandit formulation: the expected future benefit of retaining the page versus exporting it. Nodes with surplus wear export pages; cooler nodes import, respecting the daily 20 kB quota. Transactions b_{ij}(t) are delta-coded to reduce overhead.</p>
  <p>Private In-Cell Noise. During every refresh or barter write, the programming current is jittered with Gaussian noise whose variance is calibrated per cell, guaranteeing an ε-differential-privacy bound on released logits. This merges retention refresh and privacy into a single physical operation.</p>
  <p>Offline training. The LSTM, TinyTransformer and MPC cost weights are jointly fitted on three 14-day excerpts of SwarmRet-120 using AdamW (β = 0.9, 0.99). Hyper-parameters swept include meta-learning rate {1e-4, 3e-4, 1e-3}, MPC horizon {1, 2, 4 h}, and DP noise factor σ {0.8, 1.0, 1.2}.</p>
</section>

<section>
  <h2>Experimental Setup</h2>
  <p>Experiment 1 – 200-node end-to-end evaluation. We bootstrap the 20 physical SwarmRet-120 logs into 200 virtual nodes. Each node emulates a Nordic nRF54 with 4 MB MRAM, 64 kB SRAM and compute-in-memory accelerators. Modalities: 96×96 RGB faces at 20 Hz processed by Tiny-ViT-0.6 M; 96×96 DVS stacks at 240 Hz by ResNet-18; and CO₂ at 1 Hz by a GRU-128. The outer-loop meta-learner holds 1.1 M parameters; the forecaster 21 k. Baselines are PHOENIX, SparCL <a href="https://arxiv.org/pdf/2209.09476v1.pdf" target="_blank" title="SparCL: Sparse Continual Learning on the Edge">(Zifeng Wang, 2022)</a>, DER++, ECLIPSE and a reactive PHOENIX variant. Internal ablations disable RAMP, PTHS, CReS or PICN. Five seeds permute node IDs.</p>
  <p>Metrics (logged every 30 min) include: accuracy, backward transfer, Joule per correct-class, across-swarm wear variance σ_H², meta-adaptation latency (steps to 90 % post-shift), brown-out ratio, ε-DP guarantee and replay bytes.</p>
  <p>Implementation. A cycle-accurate simulator extends PHOENIX with retention decay, fast-weight overlays and BLE barter. Execution uses eight NVIDIA A100 GPUs and a 64-core AMD EPYC host, totalling 9.7 × 10¹⁶ FLOPs in 72 h.</p>
  <p>Experiment 2 – RAMP micro-benchmark. We concatenate Stream-51, EmoSound and AirQo streams to induce nine concept shifts. Variants:</p>
  <ul>
    <li><strong>A:</strong> RAMP in short-retention MRAM</li>
    <li><strong>B:</strong> identical meta-learner but fast weights in SRAM</li>
    <li><strong>C:</strong> DER++ replay</li>
  </ul>
  <p>The primary metric is L₉₀, the steps needed to regain 90 % pre-shift accuracy; secondary metric is Joule per recovery.</p>
  <p>Experiment 3 – PTHS + CReS stress-test. A synthetic 14-day trace imposes attic-level heat (peak 55 °C) and a 40-h solar eclipse on 64 nodes (32 hot, 32 cool). Policies compared: full SYMPHONY, reactive only, forecast-only and barter-only. Metrics include training-skip ratio, wear variance and forecast MAE.</p>
  <p>Common settings. Optimiser AdamW with weight-decay 1e-2, batch size 32, five seeds. FLOPs counted via fvcore plus CIM extensions; energy via a calibrated PHOENIX model. All scripts and raw logs are released under MIT licence.</p>
</section>

<section>
  <h2>Results</h2>
  <p>Only one public log is currently available: a single-GPU run that trained ResNet-18 on CIFAR-10 for 100 epochs (≈7 min wall-clock). Best test accuracy reached 86.58 %. No energy, endurance, privacy or swarm metrics were recorded; no SYMPHONY component was active.</p>
  <p>Gap analysis. Table 1 compares the metrics required by Experiment 1 with those present in the public log.</p>
  <ul>
    <li><strong>Accuracy on SwarmRet-120:</strong> Required ✔; Present ✗</li>
    <li><strong>Across-swarm wear variance σ_H²:</strong> Required ✔; Present ✗</li>
    <li><strong>Meta-adaptation latency:</strong> Required ✔; Present ✗</li>
    <li><strong>Brown-out ratio:</strong> Required ✔; Present ✗</li>
    <li><strong>ε-DP guarantee:</strong> Required ✔; Present ✗</li>
  </ul>
  <p>Because none of the proposed mechanisms executed, the run provides zero evidence for the claimed +4 pp accuracy, 3.2× faster adaptation or 5.6× lower wear variance.</p>
  <p>Limitations identified.</p>
  <ul>
    <li><strong>(i):</strong> Integrating retention physics, wireless barter and privacy into ML pipelines is non-trivial.</li>
    <li><strong>(ii):</strong> Absence of per-node logs prevents fairness analysis across the fleet.</li>
    <li><strong>(iii):</strong> The evaluation lacks statistical significance and baseline comparisons.</li>
  </ul>
  <p>Next steps. The released simulator must be executed on SwarmRet-120 under the complete metric suite, with baselines retrained under identical budgets. Hardware measurements of PICN using IGRE-Lite are also required.</p>
</section>

<section>
  <h2>Conclusion</h2>
  <p>SYMPHONY advances continual learning for energy-harvesting sensor swarms by exploiting volatile MRAM pages as learnable fast weights, forecasting thermo-energy dynamics for predictive scheduling, bartering retention across nodes to equalise wear, and embedding differential privacy into every write. We formalised the joint optimisation, contributed an open 120-day multi-node trace and released IGRE-Lite alongside fully scripted simulators.</p>
  <p>However, the only executed experiment to date was a CIFAR-10 baseline unrelated to our mechanisms. The immediate priority is therefore to run the published simulator on SwarmRet-120, log the complete metric suite and benchmark against PHOENIX, SparCL, DER++ and ECLIPSE. Future work will extend CReS with federated aggregation, adapt MPC horizons via reinforcement learning and fabricate IGRE-Lite silicon to validate privacy guarantees in hardware. We invite the community to replicate, critique and extend SYMPHONY so that decade-long, privacy-preserving adaptation becomes feasible for large-scale IoT fleets.</p>
</section>
</body>
</html>