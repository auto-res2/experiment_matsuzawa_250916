{
  "research_topic": "Test-Time Adaptationを収束速度に関して改善したい",
  "queries": [
    "test-time adaptation convergence",
    "fast test-time adaptation",
    "online adaptation speed",
    "batchnorm adaptation optimization",
    "entropy minimization adaptation"
  ],
  "research_study_list": [
    {
      "title": "TTT++: When Does Self-Supervised Test-Time Training Fail or Thrive?"
    },
    {
      "title": "Persistent Test-time Adaptation in Recurring Testing Scenarios",
      "abstract": "Current test-time adaptation (TTA) approaches aim to adapt a machine learning\nmodel to environments that change continuously. Yet, it is unclear whether TTA\nmethods can maintain their adaptability over prolonged periods. To answer this\nquestion, we introduce a diagnostic setting - recurring TTA where environments\nnot only change but also recur over time, creating an extensive data stream.\nThis setting allows us to examine the error accumulation of TTA models, in the\nmost basic scenario, when they are regularly exposed to previous testing\nenvironments. Furthermore, we simulate a TTA process on a simple yet\nrepresentative $\\epsilon$-perturbed Gaussian Mixture Model Classifier, deriving\ntheoretical insights into the dataset- and algorithm-dependent factors\ncontributing to gradual performance degradation. Our investigation leads us to\npropose persistent TTA (PeTTA), which senses when the model is diverging\ntowards collapse and adjusts the adaptation strategy, striking a balance\nbetween the dual objectives of adaptation and model collapse prevention. The\nsupreme stability of PeTTA over existing approaches, in the face of lifelong\nTTA scenarios, has been demonstrated over comprehensive experiments on various\nbenchmarks. Our project page is available at https://hthieu166.github.io/petta.",
      "full_text": "Persistent Test-time Adaptation in Recurring Testing Scenarios Trung-Hieu Hoang1 Duc Minh Vo2 Minh N. Do1,3 1Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign 2The University of Tokyo 3VinUni-Illinois Smart Health Center, VinUniversity {hthieu, minhdo}@illinois.edu vmduc@nlab.ci.i.u-tokyo.ac.jp Abstract Current test-time adaptation (TTA) approaches aim to adapt a machine learn- ing model to environments that change continuously. Yet, it is unclear whether TTA methods can maintain their adaptability over prolonged periods. To answer this question, we introduce a diagnostic setting - recurring TTA where envi- ronments not only change but also recur over time, creating an extensive data stream. This setting allows us to examine the error accumulation of TTA models, in the most basic scenario, when they are regularly exposed to previous testing environments. Furthermore, we simulate a TTA process on a simple yet repre- sentative ϵ-perturbed Gaussian Mixture Model Classifier, deriving theoretical insights into the dataset- and algorithm-dependent factors contributing to gradual performance degradation. Our investigation leads us to propose persistent TTA (PeTTA), which senses when the model is diverging towards collapse and adjusts the adaptation strategy, striking a balance between the dual objectives of adaptation and model collapse prevention. The supreme stability of PeTTA over existing approaches, in the face of lifelong TTA scenarios, has been demonstrated over comprehensive experiments on various benchmarks. Our project page is available at https://hthieu166.github.io/petta. 1 Introduction Machine learning (ML) models have demonstrated significant achievements in various areas [18, 38, 47, 23]. Still, they are inherently susceptible to distribution-shift [46, 13, 48, 21, 6] (also known as the divergence between the training and testing environments), leading to a significant degradation in model performance. The ability to deviate from the conventional testing setting appears as a crucial aspect in boosting ML models’ adaptability when confronted with a new testing environment that has been investigated [ 30, 53, 14]. Among common domain generalization methods [ 58, 24, 1], test-time adaptation (TTA) takes the most challenging yet rewarding path that leverages unlabeled data available at test time for self-supervised adaptation prior to the final inference [57, 39, 8, 41, 59]. Early TTA studies have concentrated on a simply ideal adaptation scenario where the test samples come from a fixed single domain [57, 39, 41]. As a result, such an assumption is far from the ever- changing and complex testing environments. To confront continually changing environments [59, 12], Yuan et al. [61] proposed a practical TTA scenario where distribution changing and correlative sampling occur [15] simultaneously. Though practical TTA is more realistic than what the previous assumptions have made, it still assumes that any environment only appears once in the data stream, a condition which does not hold true. Taking a surveillance camera as an example, it might accom- modate varying lighting conditions recurringly day after day (Fig. 1-left). Based on this reality, we hypothesize that the recurring of those conditions may reveal the error accumulation phenomenon in TTA, resulting in performance degradation over a long period. To verify our hypothesis, we simulate a 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2311.18193v4  [cs.CV]  2 Nov 2024Testing Error Time Day 1 Illumination Condition Day 2 Day 3 0 50 100 150 200 250 300 0 0.2 0.4 0.6 0.8 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 201 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Test-time adaptation step Testing Error No TTA RoTTA PeTTA (ours) Figure 1: Recurring Test-time Adaption (TTA). (left) Testing environments may change recurringly and preserving adaptability when visiting the same testing condition is not guaranteed. (right) The testing error of RoTTA [61] progressively raises (performance degradation) and exceeds the error of the source model (no TTA) while our PeTTA demonstrates its stability when adapting to the test set of CIFAR-10-C [19] 20 times. The bold lines denote the running mean and the shaded lines in the background represent the testing error on each domain (excluding the source model, for clarity). recurring testing environment and observe the increasing error rate by recurringly adapting to the test set of CIFAR-10-C [19] multiple times. We showcase the testing error of RoTTA [61] after 20 cycles of adaptation in Fig. 1-right. As expected, RoTTA can successfully adapt and deliver encouraging outcomes within the first few passes. However, this advantage is short-lived as our study uncovers a significant issue: TTA approaches in this setting may experience severe and persistent degradation in performance. Consequently, the testing error of RoTTA gradually escalates over time and quickly surpasses the model without adaptation. This result confirms the risk of TTA deployment in our illustrative scenario, as an algorithm might work well in the first place and gradually degenerate. Therefore, ensuring sustainable quality is crucial for real-world applications, especially given the recurring nature of testing environments. This study examines whether the adaptability of a TTA algorithm persists over an extended testing stream. Specifically, in the most basic scenario, where the model returns to a previously encountered testing environment after undergoing various adjustments. We thus propose a more general testing scenario than the practical TTA [61], namely recurring TTA, where the environments not only change gradually but also recur in a correlated manner over time. We first analyze a simulation using the ϵ−perturbed Gaussian Mixture Model Classifier (ϵ−GMMC) on a synthesized dataset and derive a theoretical analysis to confirm our findings, offering insights to tackle similar issues in deep neural networks. The analysis provides hints for reasoning the success of many recent robust continual TTA approaches [61, 12, 59, 15] and leading us to propose a simple yet effective baseline to avoid performance degradation, namely Persistent TTA (PeTTA). PeTTA continuously monitors the chance of collapsing and adjusts the adaptation strategy on the fly, striking a balance between the two objectives: adaptation and collapse prevention. Our contributions can be summarized as follows: • First, this work proposes a testing scenario - recurring TTA, a simple yet sufficient setup for diagnosing the overlooked gradual performance degradation phenomenon of TTA. • Second, we formally define the phenomenon of TTA collapsing and undertake a theoretical analysis on an ϵ-GMMC, shedding light on dataset-dependent and algorithm-dependent factors that contribute to the error accumulation during TTA processes. • Third, we introduce persistent TTA (PeTTA)- a simple yet effective adaptation scheme that surpasses all baseline models and demonstrates a persisting performance. For more context on related work, readers are directed to visit our discussions in Appdx. A. 2 Background Test-time Adaptation (TTA). A TTA algorithm operates on an ML classifier ft : X → Ywith parameter θt ∈ Θ (parameter space) gradually changing over time (t ∈ T) that maps an input image x ∈ Xto a category (label) y ∈ Y. Let the capital letters (Xt, Yt) ∈ X × Ydenote a pair of random variables with the joint distribution Pt(x, y) ∈ Pd, t∈ T. Here, Pd belongs to collection of D sets of testing scenarios (domains) {Pd}D d=1. The covariate shift [46] is assumed: Pt(x) and Pt′(x) 2could be different but Pt(y|x) = Pt′(y|x) holds ∀t ̸= t′. At t = 0, θ0 is initialized by a supervised model trained on P0 ∈ P0 (source dataset). The model then explores an online stream of testing data. For each t >0, it receives Xt (typically in form of a batch of Nt testing samples) for adapting itself ft−1 → ft before making the final prediction ft (Xt). TTA with Mean Teacher Update. To achieve a stable optimization process, the main (teacher) model ft are updated indirectly through a student model with parameters θ′ t [57, 61, 12, 15, 55]. At first, the teacher model in the previous step introduces a pseudo label [28] ˆYt for each Xt: ˆYt = ft−1(Xt). (1) With a classification loss LCLS (e.g., cross-entropy [16]), and a model parameters regularizer R, the student model is first updated with a generic optimization operatorOptim, followed by an exponential moving average (EMA) update of the teacher model parameter θt−1: θ′ t = Optim θ′∈Θ EPt h LCLS \u0010 ˆYt, Xt; θ′ \u0011i + λR(θ′), (2) θt = (1 − α)θt−1 + αθ′ t, (3) with α ∈ (0, 1) - the update rate of EMA, andλ ∈ R+ - the weighting coefficient of the regularization term, are the two hyper-parameters. Practical TTA. In practical TTA [61], two characteristics of the aforementioned distribution of data stream are noticeable. Firstly, Pt’s can be partitioned by td’s in which {Pt}td t=td−1 ⊂ Pd. Here, each partition of consecutive steps follows the same underlying distribution which will change continually through D domains [59] (P1 → P2 ··· → PD). Secondly, the category distribution in each testing batch is temporally correlated [15]. This means within a batch, a small subset of categories is dominant over others, making the marginal distribution Pt(y) = 0, ∀y ̸∈ Yt ⊂ Yeven though the category distribution over all batches are balanced. Optimizing under this low intra-batch diversity (|Yt| ≪ |Y|) situation can slowly degenerate the model [7]. 3 Recurring TTA and Theoretical Analysis This section conducts a theoretical analysis on a concrete failure case of a simple TTA model. The results presented at the end of Sec. 3.2 will elucidate the factors contributing to the collapse (Sec. 3.1), explaining existing good practices (Sec. 3.3) and give insights into potential solutions (Sec. 4). 3.1 Recurring TTA and Model Collapse Recurring TTA.To study the gradual performance degradation (or model collapse), we propose anew testing scenario based on practical TTA [61]. Conducting a single pass through D distributions, as done in earlier studies [61, 59], may not effectively identify the degradation. To promote consistency, our recurring TTA performs revisiting the previous distributions K times to compare the incremental error versus the previous visits. For example, a sequence with K = 2 could be P1 → P2 → ··· → PD → P1 → P2 → ··· → PD. Appdx. D extends our justifications on constructing recurring TTA. Definition 1 (Model Collapse). A model is said to be collapsed from step τ ∈ T, τ <∞ if there exists a non-empty subset of categories ˜Y ⊂ Ysuch that Pr{Yt ∈ ˜Y} > 0 but the marginal Pr{ˆYt ∈ ˜Y} converges to zero in probability: lim t→τ Pr{ˆYt ∈ ˜Y} = 0. Here, upon collapsing, a model tends to ignore almost categories in ˜Y. As it is irrecoverable once collapsed, the only remedy would be resetting all parameters back to θ0. 3.2 Simulation of Failure and Theoretical Analysis Collapsing behavior varies across datasets and the adaptation processes. Formally studying this phenomenon on a particular real dataset and a TTA algorithm is challenging. Therefore, we propose a theoretical analysis on ϵ-perturbed binary Gaussian Mixture Model Classifier (ϵ-GMMC) that shares the typical characteristics by construction and demonstrates the same collapsing pattern in action (Sec. 5.1) as observed on real continual TTA processes (Sec. 5.3). 3Pseudo-label Predictor ˆYt = argmax y∈Y Pr(Xt|y;θt−1) Xt Mean-teacher Update θ′ t = Optim θ′∈Θ EPt h LCLS \u0010ˆYt, Xt;θ′\u0011i θt = (1−α)θt−1 +αθ′ t ϵt ··· θt−1 θt ··· Figure 2: ϵ-perturbed binary Gaussian Mix- ture Model Classifier, imitating a continual TTA algorithm for theoretical analysis. Two main components include a pseudo-label predictor (Eq. 1), and a mean teacher up- date (Eqs. 2, 3). The predictor is perturbed for retaining a false negative rate of ϵt to simulate an undesirable TTA testing stream. Simulated Testing Stream. Observing a testing stream with (Xt, Yt) ∈ X × Y= R × {0, 1} and the underlying joint distribution Pt(x, y) = py,t · N(x; µy, σ2 y). The main task is predicting Xt was sampled from cluster 0 or 1 (negative or positive). Conveniently, let py,t ∆ = Pt(y) = Pr(Yt = y) and ˆpy,t ∆ = Pr( ˆYt = y) be the marginal distribution of the true label Yt and pseudo label ˆYt. GMMC and TTA. GMMC first implies an equal prior distribution by construction which is desirable for the actual TTA algorithms (e.g., category-balanced sampling strategies in [ 61, 15]). Thus, it simplifies ft into a maximum likelihood estimation ft(x) = argmaxy∈Y Pr(x|y; θt) with Pr(x|y; θt) = N(x; ˆµy,t, ˆσ2 y,t). The goal is estimating a set of parameters θt = {ˆµy,t, ˆσ2 y,t}y∈Y. A perfect classifier θ0 = {µy, σ2 y}y∈Y is initialized at t = 0. For the consecutive steps, the simplicity of GMMC allows solving the Optim (for finding θ′ t, Eq. 2) perfectly by computing the empirical mean and variance of new samples, approximating EPt. The mean teacher update (Eq. 3) for GMMC is: ˆµy,t = ( (1 − α)ˆµy,t−1 + αEPt h Xt|ˆYt i if ˆYt = y ˆµy,t−1 otherwise . (4) The update of ˆσ2 y,t is similar. ˆYt = ft−1(Xt) can be interpreted as a pseudo label (Eq. 1). ϵ-GMMC. Severe distribution shifts or low intra-batch category diversity of recurring TTA/practical TTA both result in an increase in the error rate of the predictor . Instead of directly modeling the dynamic changes of py,t (which can be complicated depending on the dataset), we study an ϵ−pertubed GMMC (ϵ−GMMC), where py,t is assumed to be static (defined below) and the pseudo- label predictor of this model is perturbed to simulate undesirable effects of the testing stream on the predictor. Two kinds of errors appear in a binary classifier [4]. Let ϵt = Pr{Yt = 1|ˆYt = 0} (5) be the false negative rate (FNR) of the model at step t. Without loss of generality, we study the increasing type II collapse of ϵ-GMMC. By intentionally flipping the true positive pseudo labels in simulation, an FNR of ϵt is maintained (Fig. 2). Assumption 1 (Static Data Stream). The marginal distribution of the true label follows the same Bernoulli distribution Ber(p0): p0,t = p0, (p1,t = p1 = 1 − p0), ∀t ∈ T. Lemma 1 (Increasing FNR). Under Assumption 1, a binary ϵ-GMMC would collapsed (Def. 1) with lim t→τ ˆp1,t = 0 (or lim t→τ ˆp0,t = 1, equivalently) if and only if lim t→τ ϵt = p1. Lemma 1 states the negative correlation between ˆp1,t and ϵt. Unsurprisingly, towards the collapsing point where all predictions are zeros, the FNR also increases at every step and eventually reaches the highest possible FNR of p1. Lemma 2 (ϵ-GMMC After Collapsing ). For a binary ϵ-GMMC model, with Assumption 1, if lim t→τ ˆp1,t = 0 (collapsing), the cluster 0 in GMMC converges in distribution to a single-cluster GMMC with parameters: N(ˆµ0,t, ˆσ2 0,t) d. → N(p0µ0 + p1µ1, p0σ2 0 + p1σ2 1 + p0p1(µ0 − µ1)2). Lemma 2 states the resulting ϵ−GMMC after collapsing. Cluster 0 now covers the whole data distribution (and assigning label 0 for all samples). Furthermore, collapsing happens when ˆµ0,t moves toward µ1. We next investigate the factors and conditions for this undesirable convergence. 4Theorem 1 (Convergence of ϵ−GMMC). For a binary ϵ-GMMC model, with Assumption 1, let the distance from ˆµ0,t toward µ1 is d0→1 t = |EPt [ˆµ0,t] − µ1|, then: d0→1 t − d0→1 t−1 ≤ α · p0 · \u0012 |µ0 − µ1| −d0→1 t−1 1 − ϵt \u0013 . From Thm. 1, we observe that the distance d0→1 t ’s converges (also indicating the convergence to the distribution in Lemma 2) if d0→1 t < d0→1 t−1 . The model collapse happens when this condition holds for a sufficiently long period. Corollary 1 (A Condition forϵ−GMMC Collapse). With fixedp0, α, µ0, µ1, ϵ−GMMC is collapsed if there exists a sequence of {ϵt}τ τ−∆τ (τ ≥ ∆τ > 0) such that: p1 ≥ ϵt > 1 − d0→1 t−1 |µ0 − µ1|, t ∈ [τ − ∆τ , τ]. Corollary 1 introduces a condition ϵ-GMMC collapse. Here, ϵt’s are non-decreasing, lim t→τ ϵt = p1. Remarks. Thm. 1 concludes two sets of factors contributing to collapse: (i) data-dependent factors: the prior data distribution (p0), the nature difference between two categories (|µ0 − µ1|); and (ii) algorithm-dependent factors: the update rate (α), the FNR at each step (ϵt). ϵ-GMMC analysis sheds light on explaining model collapse on real datasets (Sec. 5.3), reasons the existing approaches (Sec. 3.3) and motivates the development of our baseline (Sec. 4). 3.3 Connection to Existing Solutions Prior TTA algorithms have already incorporated implicit mechanisms to mitigate model collapse. The theoretical results in the previous section explain the rationale behind these effective strategies. Regularization Term for θt. Knowing that f0 is always well-behaved, an attempt is restricting the divergence of θt from θ0, e.g. using R(θt) ∆ = ∥θ0 − θt∥2 2 regularization [40]. The key idea is introducing a penalty term to avoid an extreme divergence as happening in Thm. 1. Memory Bank for Harmonizing Pt(x). Upon receiving Xt, samples in this batch are selectively updated to a memory bank M (which already contains a subset of some instances ofXt′, t′ < tin the previous steps). By keeping a balanced number of samples from each category, distribution PM t (y) of samples in M is expected to have less zero entries than Pt(y), making the optimization step over PM t more desirable. From Thm. 1, M moderates the extreme value of the category distribution (p0 term) which typically appears on batches with low intra-batch category diversity. 4 Persistent Test-time Adaptation (PeTTA) Now we introduce our Persistent TTA (PeTTA) approach. Further inspecting Thm. 1, while ϵt (Eq. 5) is not computable without knowing the true labels, the measure of divergence from the initial distribution (analogously to d0→1 t−1 term) can provide hints to fine-tune the adaptation process. Key Idea. A proper adjustment toward the TTA algorithm can break the chain of monotonically increasing ϵt’s in Corollary 1 to prevent the model collapse. In the mean teacher update, the larger value of λ (Eq. 2) prioritizes the task of preventing collapse on one hand but also limits its adaptability to the new testing environment. Meanwhile, α (Eq. 3) controls the weight on preserving versus changing the model from the previous step. Drawing inspiration from the exploration-exploitation tradeoff [49, 25] encountered in reinforcement learning [54], we introduce a mechanism for adjusting λ and α on the fly, balancing between the two primary objectives: adaptation and preventing model collapse. Our strategy is prioritizing collapse prevention (increasing λ) and preserving the model from previous steps (decreasing α) when there is a significant deviation from θ0. In [40, 61, 59], λ and α were fixed through hyper-parameter tuning. This is suboptimal due to varying TTA environments and the lack of validation set [62]. Furthermore, Thm. 1 suggests the convergence rate quickly escalates when ϵt increases, making constant λ, αinsufficient to prevent collapse. Sensing the Divergence of θt. We first equip PeTTA with a mechanism for measuring its divergence from θ0. Since ft(x) = argmax y∈Y Pr(y|x; θt), we can decompose Pr(y|x; θt) = [h (ϕθt(x))]y, with ϕθt(·) is a θt-parameterized deep feature extractor followed by a fixed classification head (a linear and softmax layer) h(·). The operator [·]y extracts the yth component of a vector. 5Since h(·) remains unchanged, instead of comparing the divergence in the parameter space (Θ) or between the output probability Pr(y|x; θt) and Pr(y|x; θ0), we suggest an inspection over the feature embedding space that preserves a maximum amount of information in our case (data processing inequality [9]). Inspired by [31] and under Gaussian assumption, the Mahalanobis distance of the first moment of the feature embedding vectors is compared. Let z = ϕθt(x), we keep track of a collection of the running mean of feature vector z: {ˆµy t }y∈Y in which ˆµy t is EMA updated with vector z if ft(x) = y. The divergence of θt at step t, evaluated on class y is defined as: γy t = 1 − exp \u0010 −(ˆµy t − µy 0)T (Σy 0)−1 (ˆµy t − µy 0) \u0011 , (6) where µy 0 and Σy 0 are the pre-computed empirical mean and covariant matrix of feature vectors in the source dataset (P0). The covariant matrix here is diagonal for simplicity. In practice, without directly accessing the training set, we assume a small set of unlabeled samples can be drawn from the source distribution for empirically computing these values (visit Appdx. E.4 for further details). Here, we implicitly expect the independence of each entry in z and TTA approaches learn to align feature vectors of new domains back to the source domain (P0). Therefore, the accumulated statistics of these feature vectors at each step should be concentrated near the vectors of the initial model. The value of γy t ∈ [0, 1] is close to 0 when θt = θ0 and increases exponentially as ˆµy t diverging from µy 0. Adaptive Regularization and Model Update. With α0, λ0 are initial values, utilizing γy t derived in Eq. 6, a pair of (λt, αt) is adaptively chosen at each step: ¯γt = 1 | ˆYt| X y∈ ˆYt γy t , ˆYt = n ˆY (i) t |i = 1, ··· , Nt o ; λt = ¯γt · λ0, α t = (1 − ¯γt) · α0, (7) ˆYt is a set of unique pseudo labels in a testing batch ( ˆY (i) t is the ith realization of ˆYt). Anchor Loss. Penalizing the divergence with regular vector norms in high-dimensional space (Θ) is insufficient (curse of dimensionality [5, 51]), especially with a large model and limited samples. Anchor loss LAL can nail down the similarity between ft and f0 in the probability space [32, 12]: LAL(Xt; θ) = − X y∈Y Pr(y|Xt; θ0) log Pr(y|Xt; θ), (8) which is equivalent to minimizing the KL divergence DKL (Pr(y|Xt; θ0)∥Pr(y|Xt; θ)). Persistent TTA.Having all the ingredients, we design our approach, PeTTA, following the convention setup of the mean teacher update, with the category-balanced memory bank and the robust batch normalization layer from [61]. Appdx. E.1 introduces the pseudo code of PeTTA. ForLCLS, either the self-training scheme [12] or the regular cross-entropy [16] is adopted. With R(θ), cosine similarity or L2 distance are both valid metrics for measuring the distance between θ and θ0 in the parameter space. Fisher regularizer coefficient [ 40, 27] can also be used, optionally. To sum up, the teacher model update of PeTTA is an elaborated version of EMA with λt, αt (Eq. 7) and LAL (Eq. 8): θ′ t = Optim θ′∈Θ EPt h LCLS \u0010 ˆYt, Xt; θ′ \u0011 + LAL (Xt; θ′) i + λtR(θ′), θt = (1 − αt)θt−1 + αtθ′ t. 5 Experimental Results 5.1 ϵ−MMC Simulation Result Simulation Setup. A total of 6000 samples from two Gaussian distributions: N(µ0 = 0, σ2 0 = 1) and N(µ1 = 2, σ2 1 = 1) with p0 = p1 = 1 2 are synthesized and gradually released in a batch of B = 10 samples. For evaluation, an independent set of 2000 samples following the same distribution is used for computing the prediction frequency, and the false negative rate (FNR). ϵ−GMMC update follows Eq. 4 with α = 5e−2. To simulate model collapse, the predictor is intercepted and 10% of the true-postive pseudo labels at each testing step are randomly flipped (Corollary 1). Simulation Result. In action, both the likelihood of predicting class 0 (Fig. 3a-left) and theϵt (Eq. 5) (Fig. 3c-right, solid line) gradually increases over time as expected (Lemma 1). After collapsing, 60 120 240 360 480 6000 0.2 0.4 0.6 0.8 1 Testing Step (t) 0 120 240 360 480 6000 0.2 0.4 0.6 0.8 1 Testing Step (t) −4 −2 0 2 4x−4 −2 0 2 40 0.2 0.4 0.6 0.8 1 x Probability density N(µ0, σ0) N(µ1, σ1) N(ˆµ0, ˆσ0) N(ˆµ1, ˆσ1) 0 100 200 300 400 500 600 0.8 1.2 1.6 2.0 Testing step (t) |ˆµ0,t −µ1| Numerical Simulation Theoretical Result 0 100 200 300 400 500 600 0.1 0.2 0.3 0.4 0.5 Testing step (t) ϵt Prediction Frequency GMMCϵ-GMMC ϵ-GMMC GMMC (a) (b) (c) Figure 3: Simulation result on ϵ-perturbed Gaussian Mixture Model Classifier ( ϵ-GMMC) and GMMC (perturbed-free). (a) Histogram of model predictions through time. A similar prediction frequency pattern is observed on CIFAR-10-C (Fig. 5a-left). (b) The probability density function of the two clusters after convergence versus the true data distribution. The initial two clusters of ϵ-GMMC collapsed into a single cluster with parameters stated in Lemma 2. In the perturbed-free, GMMC converges to the true data distribution. (c) Distance toward µ1 (|EPt [ˆµ0,t] − µ1|) and false- negative rate (ϵt) in simulation coincides with the result in Thm. 1 (with ϵt following Corollary 1). ϵ-GMMC merges the two initial clusters, resulting in a single one (Fig. 3b-left) with parameters that match Lemma 2. The distance from ˆµ0,t (initialized at µ0) towards µ1 converges (Fig. 3c-left, solid line), coincided with the analysis in Thm. 1 when ϵt is chosen following Corollary 1 (Fig. 3c, dashed line). GMMC (perturbed-free) stably produces accurate predictions (Fig. 3a-right) and approximates the true data distribution (Fig. 3b-right). The simulation empirically validates our analysis (Sec. 3.2), confirming the vulnerability of TTA models when the pseudo labels are inaccurately estimated. 5.2 Setup - Benchmark Datasets Datasets. We benchmark the performance on four TTA classification tasks. Specifically, CIFAR10 → CIFAR10-C, CIFAR100→ CIFAR100-C, and ImageNet → ImageNet-C [19] are three corrupted images classification tasks (corruption level 5, the most severe). Additionally, we incorporate DomainNet [44] with 126 categories from four domains for the task real → clipart, painting, sketch. Compared Methods. Besides PeTTA, the following algorithms are investigated: CoTTA [ 59], EATA [40], RMT [12], MECTA [22], RoTTA [61], ROID [37] and TRIBE [52]. Noteworthy, only RoTTA is specifically designed for the practical TTA setting while others fit the continual TTA setting in general. A parameter-free approach: LAME [ 7] and a reset-based approach (i.e., reverting the model to the source model after adapting to every 1, 000 images): RDumb [45] are also included. Recurring TTA. Following the practical TTA setup, multiple testing scenarios from each testing set will gradually change from one to another while the Dirichlet distribution (Dir(0.1) for CIFAR10- C, DomainNet, and ImageNet-C, and Dir(0.01) for CIFAR100-C) generates category temporally correlated batches of data. For all experiments, we set the number of revisits K = 20 (times) as this number is sufficient to fully observe the gradual degradation on existing TTA baselines. Implementation Details. We use PyTorch [43] for implementation. RobustBench [10] and torchvision [35] provide pre-trained source models. Hyper-parameter choices are kept as close as possible to the original selections of authors. Visit Sec. G for more implementation details. Unless otherwise noted, for all PeTTA experiments, the EMA update rate for robust batch normalization [61] and feature embedding statistics is set to 5e−2; α0 = 1e−3 and cosine similarity regularizer is used. On CIFAR10/100-C and ImageNet-C we use the self-training loss in [ 12] for LCLS and λ0 = 10 while the regular cross-entropy loss [ 13] and λ0 = 1 (severe domain shift requires prioritizing 7Table 1: Average classification error of the task CIFAR-10→ CIFAR-10-C in recurring TTA. The lowest error is in bold,(∗)average value across 5 runs (different random seeds) is reported for PeTTA. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Source 43.5 43.5 LAME [7] 31.1 31.1 CoTTA [59]82.2 85.6 87.2 87.8 88.2 88.5 88.7 88.7 88.9 88.9 88.9 89.2 89.2 89.2 89.1 89.2 89.2 89.1 89.3 89.388.3EATA [40]81.6 87.0 88.7 88.7 88.9 88.7 88.6 89.0 89.3 89.6 89.5 89.6 89.7 89.7 89.3 89.6 89.6 89.8 89.9 89.488.8RMT [12]77.5 76.9 76.5 75.8 75.5 75.5 75.4 75.4 75.5 75.3 75.5 75.6 75.5 75.5 75.7 75.6 75.7 75.6 75.7 75.875.8MECTA [22]72.2 82.0 85.2 86.3 87.0 87.3 87.3 87.5 88.1 88.8 88.9 88.9 88.6 89.1 88.7 88.8 88.5 88.6 88.3 88.886.9RoTTA [61]24.6 25.5 29.6 33.6 38.2 42.8 46.2 50.6 52.2 54.1 56.5 57.5 59.4 60.2 61.7 63.0 64.8 66.1 68.2 70.351.3RDumb [45]31.1 32.1 32.3 31.6 31.9 31.8 31.8 31.9 31.9 32.1 31.7 32.0 32.5 32.0 31.9 31.6 31.9 31.4 32.3 32.431.9ROID [37]72.7 72.6 73.1 72.4 72.7 72.8 72.7 72.7 72.9 72.8 72.9 72.9 72.8 72.5 73.0 72.8 72.5 72.5 72.7 72.772.7TRIBE [52]15.3 16.6 16.6 16.3 16.7 17.0 17.3 17.4 17.4 18.0 17.9 18.0 17.9 18.6 18.2 18.8 18.0 18.2 18.4 18.017.5PeTTA(ours)(∗) 24.323.022.622.422.422.522.322.522.822.822.622.722.722.922.622.722.622.822.923.022.8 adaptability) are applied in DomainNet experiments. In Appdx. F.5, we provide a sensitivity analysis on the choice of hyper-parameter λ0 in PeTTA. 5.3 Result - Benchmark Datasets Recurring TTA Performance. Fig. 1-right presents the testing error on CIFAR-10-C in recurring TTA setting. RoTTA [61] exhibits promising performance in the first several visits but soon raises and eventually exceeds the source model (no TTA). The classification error of compared methods on CIFAR-10→CIFAR-10-C, and ImageNet → ImageNet-C [19] tasks are shown in Tab. 1, and Tab. 2. Appdx. F.1 provides the results on the other two datasets. The observed performance degradation of CoTTA [59], EATA [40], RoTTA [61], and TRIBE [52] confirms the risk of error accumulation for an extensive period. While RMT [12], MECTA [22], and ROID [37] remain stable, they failed to adapt to the temporally correlated test stream at the beginning, with a higher error rate than the source model. LAME [7] (parameter-free TTA) and RDumb [45] (reset-based TTA) do not suffer from collapsing. However, their performance is lagging behind, and knowledge accumulation is limited in these approaches that could potentially favor a higher performance as achieved by PeTTA. Furthermore, LAME [7] is highly constrained by the source model, and selecting a precise reset frequency in RDumb [45] is challenging in practice (see Appdx. F.3 for a further discussion). 0 10 20 30 40 16 18 20 22 24 Recurring TTA Visit Classification Error PeTTA (ours) TRIBE [52] Figure 4: Classification error of TRIBE [ 52] and PeTTA (ours) of the task CIFAR-10→CIFAR10-C task in recurring TTA with 40 visits. In average, PeTTA outperforms almost every baseline approaches and persists across 20 vis- its over the three datasets. The only exception is at the case of TRIBE [ 52] on CIFAR-10- C. While this state-of-the-art model provides stronger adaptability, outweighing the PeTTA, and baseline RoTTA [61] in several recurrences, the risk of the model collapsing still presents in TRIBE [52]. This can be clearly observed when we increase the observation period to 40 recur- ring visits in Fig. 4. As the degree of freedom for adaptation in PeTTA is more constrained, it takes a bit longer for adaptation but remains sta- ble afterward. Fig. 5b-bottom exhibits the con- fusion matrix at the last visit with satisfactory accuracy. The same results are also observed when shuffling the order of domain shifts within each recurrence (Appdx. D.3), or extending the number of recurrences to 40 visits (Appdx. F.4). Continuously Changing Corruption (CCC) [45] Performance. Under CCC [45], Tab. 3 reveals the supreme performance of PeTTA over RoTTA [61] and RDumb [45]. Here, we report the average classification error between two consecutive adaptation step intervals. An adaptation step in this table corresponds to a mini-batch of data with 64 images. The model is adapted to 80, 000 steps in total with more than 5.1M images, significantly longer than 20 recurring TTA visits. Undoubtedly, PeTTA still achieves good performance where the corruptions are algorithmically generated, non-cyclic with two or more corruption types can happen simultaneously. This experiment also empirically justifies the construction of our recurring TTA as a diagnostic tool (Appdx. D.2) where similar observations are concluded on the two settings. Obviously, our recurring TTA is notably simpler than CCC [45]. 8Table 2: Average classification error of the task ImageNet → ImageNet-C in recurring TTA scenario. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Source 82.0 82.0 LAME [7] 80.9 80.9 CoTTA [59]98.6 99.1 99.4 99.4 99.5 99.5 99.5 99.5 99.6 99.7 99.6 99.6 99.6 99.6 99.6 99.6 99.6 99.6 99.7 99.799.5EATA [40]60.4 59.3 65.4 72.6 79.1 84.2 88.7 92.7 95.2 96.9 97.7 98.1 98.4 98.6 98.7 98.8 98.8 98.9 98.9 99.089.0RMT [12]72.3 71.0 69.9 69.1 68.8 68.5 68.4 68.3 70.0 70.2 70.1 70.2 72.8 76.8 75.6 75.1 75.1 75.2 74.8 74.771.8MECTA [22]77.2 82.8 86.1 87.9 88.9 89.4 89.8 89.9 90.0 90.4 90.6 90.7 90.7 90.8 90.8 90.9 90.8 90.8 90.7 90.889.0RoTTA [61]68.3 62.1 61.8 64.5 68.4 75.4 82.7 95.1 95.8 96.6 97.1 97.9 98.3 98.7 99.0 99.1 99.3 99.4 99.5 99.687.9RDumb [45]72.2 73.0 73.2 72.8 72.2 72.8 73.3 72.7 71.9 73.0 73.2 73.1 72.0 72.7 73.3 73.1 72.1 72.6 73.3 73.172.8ROID [37]62.7 62.3 62.3 62.3 62.5 62.3 62.4 62.4 62.3 62.6 62.5 62.3 62.5 62.4 62.5 62.4 62.4 62.5 62.4 62.562.4TRIBE [52]63.664.0 64.9 67.8 69.6 71.7 73.5 75.5 77.4 79.8 85.0 96.5 99.4 99.8 99.9 99.8 99.8 99.9 99.9 99.984.4PeTTA(ours)(∗) 65.361.759.859.159.459.659.859.359.460.060.361.060.760.460.660.760.860.760.460.260.5 Table 3: Average classification error on CCC [45] setting. Each column presents the average error within an adaptation interval (e.g., the second column provides the average error between the 6701 and 13400 adaptation steps). Each adaptation step here is performed on a mini-batch of 64 images. CCC [45] Adaptation Step− − − − − − − − − − − − − − − − − − − − − − − − − → Method6700 13400 20100 26800 33500 40200 46900 53600 60200 66800 73400 80000Avg Source 0.83 0.83 0.83 0.83 0.83 0.84 0.84 0.83 0.84 0.83 0.83 0.83 0.83 RoTTA [61]0.70 0.85 0.92 0.96 0.98 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.95 RDumb [45]0.78 0.74 0.75 0.77 0.75 0.72 0.75 0.77 0.75 0.74 0.75 0.75 0.75 PeTTA(ours) 0.67 0.63 0.62 0.65 0.65 0.64 0.64 0.68 0.63 0.63 0.65 0.65 0.64 0.46 0.44 0.4 0.43 0.46 0.47 0.44 0.43 0.48 0.4 0.43 0.43 0.41 airplane bird cat dog frog ship auto deer horse truck 0.13 0.34 0.44 0.32 0.14 0.44 0.51 0.46 0.46 0.34 airplane bird catdog frog ship auto deer horse truck Inter-category cosine similarity (source model)Misclassification rate of collapsed RoTTA 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.69 0 0 0 0.08 0 0.04 0 0.13 0.05 0.34 0.17 0 0 0.1 0 0.03 0 0.23 0.13 0.24 0 0.1 0.03 0.44 0 0.11 0 0.07 0.01 0.21 0 0 0.11 0.32 0 0.21 0 0.14 0.01 0.14 0 0 0.01 0.71 0 0.06 0.01 0.06 0.01 0.21 0 0 0.07 0.44 0 0.16 0 0.1 0.01 0.05 0 0 0.08 0.51 0 0.25 0 0.1 0.01 0.17 0 0 0.03 0.46 0 0.04 0.21 0.06 0.04 0.46 0 0 0.01 0.06 0 0.03 0 0.41 0.03 0.34 0 0 0 0.12 0 0.03 0 0.18 0.32 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.73 0.01 0.06 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.05 0.04 0 0.75 0.07 0.05 0.02 0.05 0.01 0.01 0 0.01 0 0.06 0.72 0.05 0.04 0.06 0.02 0.01 0.01 0.02 0 0.06 0.07 0.76 0.01 0.05 0.02 0.01 0 0 0 0.07 0.19 0.05 0.59 0.05 0.02 0.01 0.01 0 0 0.03 0.07 0.02 0.01 0.84 0 0.01 0.01 0.01 0 0.06 0.06 0.08 0.02 0.02 0.74 0 0.01 0.04 0.02 0.02 0.02 0.01 0 0.03 0 0.84 0.02 0.01 0.05 0.02 0.03 0.01 0 0.02 0.01 0.04 0.82 1 5 10 15 200 0.2 0.4 0.6 0.8 1 Visits 1 5 10 15 200 0.2 0.4 0.6 0.8 1 Visits RoTTA [61] PeTTA (ours) PeTTA (ours) - 20th visit RoTTA [61] - 20th visit Predicted label (a) (b)(c) True labelTrue label Prediction Frequency Figure 5: Recurring TTA (20 visits) on CIFAR-10 →CIFAR10-C task. (a) Histogram of model predictions (10 labels are color-coded). PeTTA achieves a persisting performance while RoTTA [61] degrades. (b) Confusion matrix at the last visit, RoTTA classifies all samples into a few categories (e.g., 0: airplane, 4: deer). (c) Force-directed graphs showing (left) the most prone to misclassification pairs (arrows indicating the portion and pointing from the true to the misclassified category); (right) similar categories tend to be easily collapsed. Edges denote the average cosine similarity of feature vectors (source model), only the highest similar pairs are shown. Best viewed in color. Collapsing Pattern. The rise in classification error (Fig. 1-right) can be reasoned by the prediction frequency of RoTTA [ 61] in an recurring TTA setting (Fig. 5a-left). Similar to ϵ-GMMC, the likelihood of receiving predictions on certain categories gradually increases and dominates the others. Further inspecting the confusion matrix of a collapsed model (Fig. 5b-top) reveals two major groups of categories are formed and a single category within each group represents all members, thereby becoming dominant. To see this, Fig. 5c-left simplifies the confusion matrix by only visualizing the 9Table 4: Average (across 20 visits) error of multiple variations of PeTTA: without (w/o) R(θ), LAL; LAL only; fixed regularization coefficient λ; adaptive coef- ficient λt, update rate αt; using anchor loss LAL. Method CF-10-CCF-100-CDN IN-C Baseline w/oR(θ),LAL 42.6 63.0 77.9 93.4 R(θ)fixedλ= 0.1λ0 43.3 65.0 80.0 92.5R(θ)fixedλ=λ0 42.0 64.6 66.6 92.9 LALonly 25.4 56.5 47.5 68.1 PeTTA -λt 27.1 55.0 59.7 92.7PeTTA -λt +αt 23.9 41.4 44.5 75.7PeTTA -λt +LAL 26.2 36.3 43.2 62.0 PeTTA -λt +αt +LAL 22.8 35.1 42.9 60.5 Table 5: Average (across 20 visits) error of PeTTA. PeTTA favors various choices of reg- ularizers R(θ): L2 and cosine similarity in conjunction with Fisher [27, 40] coefficient. Method CF-10-CCF-100-CDN IN-CR(θ) Fisher L2 ✗ 23.0 35.6 43.1 70.8✓ 22.7 36.0 43.9 70.0 Cosine ✗ 22.8 35.1 42.9 60.5✓ 22.6 35.9 43.3 63.8 CF: CIFAR, DN: DomainNet, IN: ImageNet top prone-to-misclassified pair of categories. Here, label deer is used for almost every living animal while airplane represents transport vehicles. The similarity between categories in the feature space of the source model (Fig. 5c-right) is correlated with the likelihood of being merged upon collapsing. As distance in feature space is analogous to |µ0 − µ1| (Thm. 1), closer clusters are at a higher risk of collapsing. This explains and showcases that the collapsing behavior is predictable up to some extent. 5.4 Ablation Study Effect of Each Component. Tab. 4 gives an ablation study on PeTTA, highlighting the use of a regularization term (R(θ)) with a fixed choice of λ, αnot only fails to mitigate model collapse but may also introduce a negative effect (rows 2-3). Trivially applying the anchor loss (LAL) alone is also incapable of eliminating the lifelong performance degradation in continual TTA (row 4). Within PeTTA, adopting the adaptiveλt scheme alone (row 5) or in conjunction with either αt or anchor loss LAL (rows 6-7) partially stabilizes the performance. Under the drastic domain shifts with a larger size of categories or model parameters (e.g., on CIFAR-100-C, DomainNet, ImageNet-C), restricting αt adjustment limits the ability of PeTTA to stop undesirable updates while a common regularization term without LAL is insufficient to guide the adaptation. Thus, leveraging all elements secures the persistence of PeTTA (row 8). Various Choices of Regularizers. The design of PeTTA is not coupled with any specific regu- larization term. Demonstrated in Tab. 5, PeTTA works well for the two common choices: L2 and cosine similarity. The conjunction use of Fisher coefficent [27, 40] for weighting the model parameter importance is also studied. While the benefit (in terms of improving accuracy) varies across datasets, PeTTA accommodates all choices, as the model collapse is not observed in any of the options. 6 Discussions and Conclusion On a Potential Risk of TTA in Practice. We provide empirical and theoretical evidence on the risk of deploying continual TTA algorithms. Existing studies fail to detect this issue with a single pass per test set. The recurring TTA could be conveniently adopted as astraightforward evaluation, where its challenging test stream magnifies the error accumulation that a model might encounter in practice. Limitations. PeTTA takes one step toward mitigating the gradual performance degradation of TTA. Nevertheless, a complete elimination of error accumulation cannot be guaranteed rigorously through regularization. Future research could delve deeper into expanding our efforts to develop an algorithm that achieves error accumulation-free by construction. Furthermore, as tackling the challenge of the temporally correlated testing stream is not the focus of PeTTA, using a small memory bank as in [61, 15] is necessary. It also assumes the features statistics from the source distribution are available (Appdx. E.3, E.4). These constraints potentially limit its scalability in real-world scenarios. Conclusion. Towards trustworthy and reliable TTA applications, we rigorously study theperformance degradation problem of TTA. The proposed recurring TTAsetting highlights the limitations of modern TTA methods, which struggle to prevent the error accumulation when continuously adapting to demanding test streams. Theoretically inspecting a failure case of ϵ−GMMC paves the road for designing PeTTA- a simple yet efficient solution that continuously assesses the model divergence for harmonizing the TTA process, balancing adaptation, and collapse prevention. 10Acknowledgements This work was supported by the Jump ARCHES Endowment through the Health Care Engineering Systems Center, JSPS/MEXT KAKENHI JP24K20830, ROIS NII Open Collaborative Research 2024-24S1201, in part by the National Institute of Health (NIH) under Grant R01 AI139401, and in part by the Vingroup Innovation Foundation under Grant VINIF.2021.DA00128. References [1] Kartik Ahuja, Ethan Caballero, Dinghuai Zhang, Jean-Christophe Gagnon-Audet, Yoshua Bengio, Ioannis Mitliagkas, and Irina Rish. Invariance principle meets information bottleneck for out-of-distribution gener- alization. In A. Beygelzimer, Y . Dauphin, P. Liang, and J. Wortman Vaughan, editors,Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=jlchsFOLfeF. [2] Rahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Laurent Charlin, Massimo Caccia, Min Lin, and Lucas Page-Caccia. Online continual learning with maximal interfered retrieval. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors,Advances in Neural Information Processing Systems, volume 32, 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/ file/15825aee15eb335cc13f9b559f166ee8-Paper.pdf. [3] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample se- lection for online continual learning. In Advances in Neural Information Processing Systems , volume 32, 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/ e562cd9c0768d5464b64cf61da7fc6bb-Paper.pdf. [4] Amitav Banerjee, U. B. Chitnis, S. L. Jadhav, J. S. Bhawalkar, and S. Chaudhury. Hypothesis testing, type I and type II errors. Industrial Psychiatry Journal, 18(2):127–131, 2009. ISSN 0972-6748. doi: 10.4103/0972-6748.62274. URL https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2996198/. [5] Richard Bellman. Dynamic Programming. Princeton University Press, Princeton, NJ, USA, 1957. [6] Arno Blaas, Andrew Miller, Luca Zappella, Joern-Henrik Jacobsen, and Christina Heinze-Deml. Con- siderations for distribution shift robustness in health. In ICLR 2023 Workshop on Trustworthy Machine Learning for Healthcare, 2023. URL https://openreview.net/forum?id=y7XveyWYzIB. [7] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8334–8343, 2022. doi: 10.1109/CVPR52688.2022.00816. [8] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In Proceedings of the IEEE International Conference on Computer Vision, 2022. [9] Thomas M. Cover and Joy A. Thomas.Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing). Wiley-Interscience, USA, 2006. ISBN 0471241954. [10] Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness benchmark. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2021. URL https://openreview.net/forum?id=SSKZPJCt7B. [11] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(7):3366–3385, 2022. doi: 10.1109/ TPAMI.2021.3057446. [12] Mario Döbler, Robert A. Marsden, and Bin Yang. Robust mean teacher for continual and gradual test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 7704–7714, June 2022. [13] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In Proceedings of the 32nd International Conference on Machine Learning , volume 37 of Proceedings of Machine Learning Research , pages 1180–1189, Lille, France, 07–09 Jul 2015. PMLR. URL https://proceedings.mlr.press/v37/ganin15.html. [14] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. Domain-Adversarial Training of Neural Networks, pages 189– 209. Springer International Publishing, 2017. doi: 10.1007/978-3-319-58347-1_10. URL https: //doi.org/10.1007/978-3-319-58347-1_10 . [15] Taesik Gong, Jongheon Jeong, Taewon Kim, Yewon Kim, Jinwoo Shin, and Sung-Ju Lee. NOTE: Robust continual test-time adaptation against temporal correlation. In Advances in Neural Information Processing Systems, 2022. 11[16] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In L. Saul, Y . Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems , volume 17, 2004. URL https://proceedings.neurips.cc/paper_files/paper/2004/file/ 96f2b50b5d3613adf9c27049b2a888c7-Paper.pdf. [17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. arXiv preprint arXiv:1512.03385, 2015. [18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1026–1034, 2015. [19] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. Proceedings of the International Conference on Learning Representations, 2019. [20] Dan Hendrycks, Norman Mu, Ekin D. Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. AugMix: A simple data processing method to improve robustness and uncertainty. Proceedings of the International Conference on Learning Representations (ICLR), 2020. [21] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 8320–8329, 2021. doi: 10.1109/ICCV48922.2021.00823. [22] Junyuan Hong, Lingjuan Lyu, Jiayu Zhou, and Michael Spranger. MECTA: Memory-economic continual test-time model adaptation. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=N92hjSf5NNh. [23] Fabian Isensee, Paul F. Jaeger, Simon A. A. Kohl, Jens Petersen, and Klaus H. Maier-Hein. nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature Methods, 18(2):203–211, February 2021. ISSN 1548-7105. doi: 10.1038/s41592-020-01008-z. URL https: //www.nature.com/articles/s41592-020-01008-z . [24] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for model-agnostic domain generalization. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wort- man Vaughan, editors, Advances in Neural Information Processing Systems , volume 34, pages 2427–2440, 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/ 1415fe9fea0fa1e45dddcff5682239a0-Paper.pdf. [25] Michael N. Katehakis and Arthur F. Veinott. The multi-armed bandit problem: Decomposition and compu- tation. Mathematics Operations Research, 12:262–268, 1987. URL https://api.semanticscholar. org/CorpusID:656323. [26] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412. 6980. [27] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks.Pro- ceedings of the National Academy of Sciences, 114(13):3521–3526, 2017. doi: 10.1073/pnas.1611835114. URL https://www.pnas.org/doi/abs/10.1073/pnas.1611835114. [28] Dong-Hyun Lee. Pseudo-label : The simple and efficient semi-supervised learning method for deep neural networks. ICML 2013 Workshop : Challenges in Representation Learning (WREPL), 07 2013. [29] T. Lee, S. Chottananurak, T. Gong, and S. Lee. Aetta: Label-free accuracy estimation for test-time adaptation. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 28643–28652, Los Alamitos, CA, USA, jun 2024. IEEE Computer Society. doi: 10.1109/CVPR52733. 2024.02706. URL https://doi.ieeecomputersociety.org/10.1109/CVPR52733.2024.02706. [30] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C. Kot. Domain generalization with adversarial feature learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. [31] Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou. Revisiting batch normalization for practical domain adaptation. In International Conference on Learning Representations Workshop, 2017. URL https://openreview.net/forum?id=BJuysoFeg. [32] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(12):2935–2947, 2018. doi: 10.1109/TPAMI.2017.2773081. [33] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? Source hypothesis transfer for unsupervised domain adaptation. In International Conference on Machine Learning (ICML), pages 6028–6039, 2020. 12[34] Sen Lin, Peizhong Ju, Yingbin Liang, and Ness Shroff. Theory on forgetting and generalization of continual learning. In Proceedings of the 40th International Conference on Machine Learning, ICML’23, 2023. [35] TorchVision maintainers and contributors. Torchvision: Pytorch’s computer vision library. https: //github.com/pytorch/vision, 2016. [36] Robert A Marsden, Mario Döbler, and Bin Yang. Gradual test-time adaptation by self-training and style transfer. arXiv preprint arXiv:2208.07736, 2022. [37] Robert A Marsden, Mario Döbler, and Bin Yang. Universal test-time adaptation through weight ensem- bling, diversity weighting, and prior correction. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2555–2565, 2024. [38] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In Proceedings of the European Conference on Computer Vision (ECCV), 2020. [39] A. Tuan Nguyen, Thanh Nguyen-Tang, Ser-Nam Lim, and Philip Torr. TIPI: Test time adaptation with transformation invariance. In Conference on Computer Vision and Pattern Recognition 2023, 2023. URL https://openreview.net/forum?id=NVh1cy37Ge. [40] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test-time model adaptation without forgetting. In The Internetional Conference on Machine Learning, 2022. [41] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan. Towards stable test-time adaptation in dynamic wild world. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=g2YraF75Tj. [42] K. R. Parthasarathy. Introduction to Probability and Measure , volume 33 of Texts and Readings in Mathematics. Hindustan Book Agency, Gurgaon, 2005. ISBN 978-81-85931-55-5 978-93-86279-27-9. doi: 10.1007/978-93-86279-27-9. URL http://link.springer.com/10.1007/978-93-86279-27-9 . [43] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library, 2019. [44] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In Proceedings of the IEEE International Conference on Computer Vision, pages 1406–1415, 2019. [45] Ori Press, Steffen Schneider, Matthias Kuemmerer, and Matthias Bethge. RDumb: A simple approach that questions our progress in continual test-time adaptation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=VfP6VTVsHc. [46] Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D. Lawrence. Dataset Shift in Machine Learning. The MIT Press, 2009. ISBN 0262170051. [47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 8748–8763. PMLR, 18–24 Jul 2021. URL https://proceedings. mlr.press/v139/radford21a.html. [48] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet classifiers generalize to ImageNet? In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning , volume 97 of Proceedings of Machine Learning Research, pages 5389–5400. PMLR, 09–15 Jun 2019. URL https://proceedings.mlr.press/v97/ recht19a.html. [49] Mooweon Rhee and Tohyun Kim. Exploration and Exploitation, pages 543–546. Palgrave Macmillan UK, London, 2018. ISBN 978-1-137-00772-8. doi: 10.1057/978-1-137-00772-8_388. URL https: //doi.org/10.1057/978-1-137-00772-8_388 . [50] Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, , and Gerald Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interference. In Interna- tional Conference on Learning Representations, 2019. URL https://openreview.net/forum?id= B1gTShAct7. [51] Tanin Sirimongkolkasem and Reza Drikvandi. On Regularisation Methods for Analysis of High Di- mensional Data. Annals of Data Science , 6(4):737–763, December 2019. ISSN 2198-5812. doi: 10.1007/s40745-019-00209-4. URL https://doi.org/10.1007/s40745-019-00209-4 . 13[52] Yongyi Su, Xun Xu, and Kui Jia. Towards real-world test-time adaptation: Tri-net self-training with balanced normalization. Proceedings of the AAAI Conference on Artificial Intelligence, 38(13):15126– 15135, 2024. [53] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 9229–9248. PMLR, 13–18 Jul 2020. URL https://proceedings. mlr.press/v119/sun20b.html. [54] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, 2018. [55] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS’17, page 1195–1204, 2017. ISBN 9781510860964. [56] Daniel Vela, Andrew Sharp, Richard Zhang, Trang Nguyen, An Hoang, and Oleg S. Pianykh. Temporal quality degradation in AI models. Scientific Reports, 12(1):11654, July 2022. ISSN 2045-2322. doi: 10.1038/s41598-022-15245-z. URL https://www.nature.com/articles/s41598-022-15245-z . [57] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=uXl3bZLkr3c. [58] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, and Tao Qin. Generalizing to unseen domains: A survey on domain generalization. In Zhi-Hua Zhou, editor, Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pages 4627–4635. International Joint Conferences on Artificial Intelligence Organization, 8 2021. doi: 10.24963/ijcai.2021/628. URL https://doi.org/10. 24963/ijcai.2021/628. Survey Track. [59] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 7201–7211, June 2022. [60] Zachary Young and Robert Steele. Empirical evaluation of performance degradation of machine learning-based predictive models – a case study in healthcare information systems. International Journal of Information Management Data Insights , 2(1):100070, 2022. ISSN 2667-0968. doi: https: //doi.org/10.1016/j.jjimei.2022.100070. URL https://www.sciencedirect.com/science/article/ pii/S2667096822000143. [61] Longhui Yuan, Binhui Xie, and Shuang Li. Robust test-time adaptation in dynamic scenarios. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15922– 15932, 2023. [62] Hao Zhao, Yuejiang Liu, Alexandre Alahi, and Tao Lin. On pitfalls of test-time adaptation. In ICLR 2023 Workshop on Pitfalls of limited data and computation for Trustworthy ML , 2023. URL https: //openreview.net/forum?id=0Go_RsG_dYn. 14Persistent Test-time Adaptation in Recurring Testing Scenarios Technical Appendices Table of Contents A Related Work 16 B Proof of Lemmas and Theorems 16 B.1 Proof of Lemma 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 B.2 Proof of Lemma 2. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 B.3 Proof of Theorem 1 and Corollary 1. . . . . . . . . . . . . . . . . . . . . . . . 18 C Further Justifications on Gaussian Mixture Model Classifier 19 D Further Justifications on the Recurring Testing Scenario 20 D.1 Recurring TTA Follows the Design of a Practical TTA Stream . . . . . . . . . . 20 D.2 Recurring TTA as a Diagnostic Tool . . . . . . . . . . . . . . . . . . . . . . . . 20 D.3 Recurring TTA with Random Orders . . . . . . . . . . . . . . . . . . . . . . . 20 E Further Justifications on Persistent TTA (PeTTA) 21 E.1 Pseudo Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 E.2 Anchor Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 E.3 The Use of the Memory Bank . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 E.4 Empirical Mean and Covariant Matrix of Feature Vectors on the Source Dataset . 23 E.5 Novelty of PeTTA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 F Additional Experimental Results of PeTTA 24 F.1 Performance of PeTTA Versus Compared Methods . . . . . . . . . . . . . . . . 24 F.2 An Inspection of PeTTA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 F.3 Does Model Reset Help? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 F.4 PeTTA with 40 Recurring Visits . . . . . . . . . . . . . . . . . . . . . . . . . . 27 F.5 The Sensitivity of Hyper-parameter Choices in PeTTA . . . . . . . . . . . . . . 27 F.6 More Details on the Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . 27 F.7 More Confusion Matrices in Recurring TTA Setting . . . . . . . . . . . . . . . 29 G Experimental Details 29 G.1 Computing Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 G.2 Experiments on CCC Testing Stream . . . . . . . . . . . . . . . . . . . . . . . 29 G.3 Test-time Adaptation Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 G.4 The Use of Existing Assets . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 15A Related Work Towards Robust and Practical TTA. While forming the basis, early single-target TTA ap- proaches [53, 57, 39, 41, 33] is far from practice. Observing the dynamic of many testing envi- ronments, a continual TTA setting is proposed where an ML model continuously adapts to a sequence of multiple shifts [36, 59]. Meanwhile, recent studies [15, 7] point out that the category distribution realistic streams is highly temporally correlated. Towards real-world TTA setting, Yuanet al. [61] launch the practical TTA which considers the simultaneous occurrence of the two aforementioned challenges. For a robust and gradual adaptation, an update via the mean teacher [55] mechanism is exploited in many continual TTA algorithms [59, 61, 12, 22]. To moderate the temporally correlated test stream, common approaches utilize a small memory bank for saving a category-balanced subset of testing samples [15, 61], inspired by the replay methods [50, 2] to avoid forgetting in the task of continual learning [34, 3, 11]. Our study emphasizes another perspective: beyond a supreme performance, a desirable TTA should also sustain it for an extended duration. Temporal Performance Degradation.By studying the quality of various ML models across multiple industry applications [56, 60] the issue of AI “aging\" with the temporal model degradation progress, even with data coming from a stable process has been confirmed. In TTA, the continuous changes of model parameters through gradient descent aggravate the situation, as also recently noticed in [45]. Apart from observation, we attempt to investigate and provide theoretical insights towards the mechanism of this phenomenon. Accumulated Errors in TTA. In TTA, the issue of accumulated error has been briefly acknowledged. Previous works strive to avoid drastic changes to model parameters as a good practice. Up to some degree, it helps to avoid performance degradation. Nevertheless, it is still unclear whether their effectiveness truly eliminates the risk. To preserve in-distribution performance, regularization [27, 40] or replaying of training samples at test-time [ 12] have been used. Other studies explore reset (recovering the initial model parameters) strategies [59, 45], periodically or upon the running entropy loss approaches a threshold [ 41]. Unfortunately, knowledge accumulated in the preceding steps will vanish, and a bad heuristic choice of threshold or period leads to highly frequent model resets. Noteworthy, tuning those hyper-parameters is exceedingly difficult due to the unavailability of the validation set [62]. LAME [ 7] suggests a post-processing step for adaptation (without updating the parameters). This approach, however, still limits the knowledge accumulation. Our PeTTA is reset-free by achieving an adaptable continual test-time training. B Proof of Lemmas and Theorems In this section, we prove the theoretical results regarding the ϵ−perturbed Gaussian Mixture Model Classifier (ϵ−GMMC) introduced in Sec. 3.2. We first briefly summarize the definition of model collapse and the static data stream assumption: Definition 1 (Model Collapse). A model is said to be collapsed from step τ ∈ T, τ <∞ if there exists a non-empty subset of categories ˜Y ⊂ Ysuch that Pr{Yt ∈ ˜Y} > 0 but the marginal Pr{ˆYt ∈ ˜Y} converges to zero in probability: lim t→τ Pr{ˆYt ∈ ˜Y} = 0. Assumption 1 (Static Data Stream). The marginal distribution of the true label follows the same Bernoulli distribution Ber(p0): p0,t = p0, (p1,t = p1 = 1 − p0), ∀t ∈ T. Preliminary. Following the same set of notations introduced in the main text, recall that we denoted py,t ∆ = Pr{Yt = y}, ˆpy,t ∆ = Pr{ˆYt = y} (marginal distribution of the true label Yt and pseudo label ˆYt receiving label y, respectively) and ϵt = Pr{Yt = 1|ˆYt = 0} (the false negative rate (FNR) of 16ϵ−GMMC). At testing step t, we obtain the following relations: EPt h Xt|ˆYt = 0 i = (1 − ϵt)µ0 + ϵtµ1, (9) EPt h Xt|ˆYt = 1 i = µ1, (10) VarPt \u0010 Xt|ˆYt = 0 \u0011 = (1 − ϵt)σ2 0 + ϵtσ2 1 + ϵt(1 − ϵt)(µ0 − µ1)2, (11) VarPt \u0010 Xt|ˆYt = 1 \u0011 = σ2 1. (12) In addition, under Assumption 1, the marginal distribution Pt(x) (also referred as data distribution in our setup) is: Pt(x) = N(x; p0µ0 + p1µ1, p0σ2 0 + p1σ2 1 + p0p1(µ0 − µ1)2) ∀t ∈ T. (13) B.1 Proof of Lemma 1 Lemma 1 (Increasing FNR). Under Assumption 1, a binary ϵ-GMMC would collapsed (Def. 1) with lim t→τ ˆp1,t = 0 (or lim t→τ ˆp0,t = 1, equivalently) if and only if lim t→τ ϵt = p1. Proof. Under Assumption 1, we have EPt [Xt] = p0µ0 + (1 − p0)µ1. Also note that: EPt [Xt] = EPt h EPt h Xt|ˆYt ii = EPt h Xt|ˆYt = 0 i ˆp0,t + EPt h Xt|ˆYt = 1 i ˆp1,t (14) = [(1 − ϵt)µ0 + ϵtµ1] ˆp0,t + µ1(1 − ˆp0,t) = [(1 − ϵt)ˆp0,t] µ0 + [1 − ˆp0,t(1 − ϵt)] µ1 = p0µ0 + (1 − p0)µ1, where the second equality follows Eqs. 9-10. Therefore: ˆp0,t = p0 1 − ϵt . (15) Eq. 15 shows positive correlation between ˆp0,t and ϵt. Given lim t→τ ϵt = p1, taking the limit introduces: lim t→τ ˆp0,t = lim t→τ p0 1 − ϵt = p0 1 − p1 = 1. Similarly, having lim t→τ ˆp0,t = 1, the false negative rate ϵt when t → τ is: lim t→τ ϵt = 1 − p0 = p1. Since ˆp0,t + ˆp1,t = 1, lim t→τ ˆp1,t = 0, equivalently. Towards the collapsing point, the model tends to predict a single label (class 0 in the current setup). In addition, the FNR of the model ϵt also raises correspondingly. B.2 Proof of Lemma 2. Lemma 2 (ϵ-GMMC After Collapsing ). For a binary ϵ-GMMC model, with Assumption 1, if lim t→τ ˆp1,t = 0 (collapsing), the cluster 0 in GMMC converges in distribution to a single-cluster GMMC with parameters: N(ˆµ0,t, ˆσ2 0,t) d. → N(p0µ0 + p1µ1, p0σ2 0 + p1σ2 1 + p0p1(µ0 − µ1)2). Proof. From Eqs. 9-10, under the increasing type II collapse of ϵ−GMMC setting, the perturbation does not affect the approximation of µ1. Meanwhile, when ϵt increases, one can expect that ˆµ0,t 17moves further away from µ0 toward µ1. Frist, the mean teacher model of GMMC (Eq. 4, main text) gives: EPt h ˆµ0,t|ˆYt = 1 i = EPt−1 [ˆµ0,t−1] , EPt h ˆµ0,t|ˆYt = 0 i = (1 − α)EPt−1 h ˆµ0,t−1|ˆYt = 0 i + αEPt h Xt|ˆYt = 0 i = (1 − α)EPt−1 [ˆµ0,t−1] + α \u0010 EPt h Xi|ˆYt = 0 i\u0011 , EPt h ˆµ1,t|ˆYt = 1 i = (1 − α)EPt−1 h ˆµ1,t−1|ˆYt = 1 i + αEPt h Xt|ˆYt = 1 i = (1 − α)EPt−1 [ˆµ1,t−1] + α \u0010 EPt h Xi|ˆYt = 1 i\u0011 , EPt h ˆµ1,t|ˆYt = 0 i = EPt−1 [ˆµ1,t−1] . By defining uy,t = EPt [ˆµy,t], we obtain the following recurrence relation between u0,t and u0,t−1: u0,t = EPt h ˆµ0,t|ˆYt = 0 i ˆp0,t + EPt h ˆµ0,t|ˆYt = 1 i ˆp1,t = \u0010 (1 − α)u0,t−1 + αEPt h Xt|ˆYt = 0 i\u0011 ˆp0,t + u0,t−1 ˆp1,t = [(1 − α)ˆp0,t + ˆp1,t] u0,t−1 + αˆp0,tEPt h Xt|ˆYt = 0 i = (1 − αˆp0,t)u0,t−1 + αˆp0,tEPt h Xt|ˆYt = 0 i = (1 − αˆp0,t)u0,t−1 + αˆp0,t [(1 − ϵt)µ0 + ϵtµ1] . (16) Given lim t→τ ˆp0,t = 1, it follows that lim t→τ ϵ0,t = p1 by Lemma 1. From this point: u0,t = (1 − α)u0,t−1 + α (p0µ0 + p1µ1) ∀t > τ. Taking the limit t → ∞: lim t→∞ u0,t = lim t→∞ (1 − α)u0,t−1 + α (p0µ0 + p1µ1) = lim t→∞ (1 − α)t ˆµ0,0 + α tX i=1 (1 − α)i−1 (p0µ0 + p1µ1) = lim t→∞ (1 − α)t ˆµ0,0 + (1 − (1 − α)t)(p0µ0 + p1µ1) = p0µ0 + p1µ1. The second equation is obtained by solving the recurrence relation. When lim t→τ ˆp0,t = 1, {ˆµy,t}y∈{0,1} becomes a deterministic values. Hence, giving uy,t = EPt [ˆµy,t] = ˆµ0,t(∀t > τ) and lim t→∞ ˆµ0,t = lim t→∞ u0,t = p0µ0 + p1µ1. (17) Repeating the steps above with Eqs. 11-12 in place of Eqs. 9-10, we obtain a similar result for σ2 0,t: lim t→∞ ˆσ2 0,t = p0σ2 0 + p1σ2 1 + p0p1(µ0 − µ1)2. (18) By Lévy’s continuity theorem (p. 302, [ 42]), from Eqs. 17-18, when t → ∞, the estimated distribution of the first cluster N(x; ˆµ0,tˆσ2 0,t) converges to the whole data distribution Pt(x) (Eq. 13) when collapsing. B.3 Proof of Theorem 1 and Corollary 1. Theorem 1 (Convergence of ϵ−GMMC). For a binary ϵ-GMMC model, with Assumption 1, let the distance from ˆµ0,t toward µ1 is d0→1 t = |EPt [ˆµ0,t] − µ1|, then: d0→1 t − d0→1 t−1 ≤ α · p0 · \u0012 |µ0 − µ1| −d0→1 t−1 1 − ϵt \u0013 . 18Proof. Substituting Eq. 15 into ˆp0,t of Eq. 16 gives: u0,t = \u0012 1 − αp0 1 − ϵt \u0013 u0,t−1 + αp0 1 − ϵt [(1 − ϵt)µ0 + ϵtµ1] . Hence, we have the distance from u0,t toward µ1: |u0,t − µ1| = \f\f\f\f \u0012 1 − αp0 1 − ϵt \u0013 u0,t−1 + αp0µ0 + αp0ϵtµ1 1 − ϵt − µ1 \f\f\f\f = \f\f\f\f \u0012 1 − αp0 1 − ϵt \u0013 (u0,t−1 − µ1) + αp0µ0 + αp0ϵtµ1 1 − ϵt − αp0µ1 1 − ϵt \f\f\f\f = \f\f\f\f \u0012 1 − αp0 1 − ϵt \u0013 (u0,t−1 − µ1) + αp0µ0 − αp0µ1(1 − ϵt) 1 − ϵt \f\f\f\f = \f\f\f\f \u0012 1 − αp0 1 − ϵt \u0013 (u0,t−1 − µ1) + αp0(µ0 − µ1) \f\f\f\f ≤ \u0012 1 − αp0 1 − ϵt \u0013 |u0,t−1 − µ1| + αp0|µ0 − µ1|. The last inequality holds due to the triangle inequality. Equivalently, |u0,t − µ1| − |u0,t−1 − µ1| ≤α · p0 · \u0012 |µ0 − µ1| −|u0,t−1 − µ1| 1 − ϵt \u0013 . Let d0→1 t = |EPt [ˆµ0,t] − µ1|, we conclude that: d0→1 t − d0→1 t−1 ≤ α · p0 · \u0012 |µ0 − µ1| −d0→1 t−1 1 − ϵt \u0013 . Corollary 1 (A Condition forϵ−GMMC Collapse). With fixedp0, α, µ0, µ1, ϵ−GMMC is collapsed if there exists a sequence of {ϵt}τ τ−∆τ (τ ≥ ∆τ > 0) such that: p1 ≥ ϵt > 1 − d0→1 t−1 |µ0 − µ1|, t ∈ [τ − ∆τ , τ]. Proof. Initialized at µ0, ϵ-GMMC is collapsing when ˆµ0,t converges to the mid-point p0µ0 + p1µ1 (Lemma 2), i.e., moving closer to µ1. From Thm. 1, the distance towards µ1 d0→1 t < d0→1 t−1 if |µ0 − µ1| −|u0,t−1 − µ1| 1 − ϵt < 0 ⇔ |µ0 − µ1| < |u0,t−1 − µ1| 1 − ϵt ⇔ ϵt > 1 − |u0,t−1 − µ1| |µ0 − µ1| . When there exists this sequence{ϵt}τ τ−∆τ (τ ≥ ∆τ > 0) it follows that d0→1 t < d0→1 t−1 and ϵt > ϵt−1 is guaranteed ∀t ∈ [τ − ∆τ , τ]. Hence, lim t→τ ϵt = p1 (model collapsed, by Lemma 1). C Further Justifications on Gaussian Mixture Model Classifier One may notice that in ϵ-GMMC (Sec. 4.2), the classifier is defined ft(x) = argmaxy∈Y Pr(x|y; θt) (maximum likelihood estimation) while in general, ft(x) = argmaxy∈Y Pr(y|x; θt) (maximum a posterior estimation), parameterized by a neural network. In this case, since the equal prior (i.e., Pr(y; θt) = Pr(y′; θt), ∀y, y′ ∈ C) is enforced in ϵ-GMMC, the two definitions are equivalent. Proof. Having: argmaxy∈Y Pr(y|x; θt) = argmaxy∈Y Pr(x|y; θt) Pr(y; θt)P y′∈Y Pr(x|y′; θt) Pr(y′; θt) = argmaxy∈Y Pr(x|y; θt). We conclude that the two definitions are equivalent. In fact, it is well-known that maximum likelihood estimation is a special case of maximum a posterior estimation when the prior is uniform. 19D Further Justifications on the Recurring Testing Scenario D.1 Recurring TTA Follows the Design of a Practical TTA Stream Note that in recurring TTA, besides the recurrence of environments (or corruptions) as in [59, 40], the distribution of class labels is also temporally correlated (non-i.i.d.) as suggested by [15, 61] to reflect the practical testing stream better. In short, recurring TTA is formed by recurring the environments of practical TTA scenario introduced in [61] multiple times (readers are encouraged to visit the original paper for additional motivations on this scenario). D.2 Recurring TTA as a Diagnostic Tool Noticeably, CoTTA [59] also performed 10-round repetition across multiple domain shifts to simulate a lifelong TTA testing stream just like our recurring TTA. However, the key difference is CoTTA assumes the distribution of class labels is i.i.d., which does not hold in many real-life testing scenarios as argued in [ 15, 61]. Our recurring TTA lifts this assumption and allows temporally correlated (non-i.i.d.) label distribution (more challenging, more practical). This extension allows recurring TTA to spot the risk of model collapse on CoTTA [59] and other methods. The over-simplicity of the repeating scheme in CoTTA for spotting performance degradation is also suggested in [45]. Clearly, it seems not to be a problem at first glance in Tab. 5 of [59] (CoTTA’s 10-round repetition), but in fact, the risk in CoTTA remains, as explored in our scenario and also on CCC [45]. The construction of our recurring TTA is notably simple - a technical effort to extend the testing stream. However, this simplicity is on purpose, serving as a diagnostic tool for lifelong continual TTA. Counterintuitively, our experiments on four different tasks with the latest methods verify that even if the model is exposed to the same environment(the most basic case), their adaptability and performance are still consistently reduced (demonstrated visually in Fig. 1, quantitatively in Sec. 5.3). We believe that the extensive testing stream by recurrence in our setup is a simple yet sufficient scenario to demonstrate the vulnerability of existing continual TTA methods when facing the issue of model collapse (compared to CCC [45], a notably more complicated scenario than our recurring TTA). Indeed, recurring shifts are sufficient to show this failure mode and any lifelong TTA method should necessarily be able to handle recurring conditions. D.3 Recurring TTA with Random Orders Recall that in Sec. 3.1,recurring TTAis constructed by repeatingthe same sequence of D distributions K times. For example, a sequence with K = 2 could be P1 → P2 → ··· → PD → P1 → P2 → ··· → PD. For simplicity and consistency that promote reproducibility, the same order of image corruptions (following [61]) is used for all recurrences. This section presents supplementary experimental findings indicating that the order of image corruptions within each recurrence, indeed, does not affect the demonstration of TTA model collapse and the performance of our PeTTA. Experiment Setup. We refer to the setting same-order as using one order of image corruptions in [61] for all recurrences (specifically, on CIFAR-10/100-C and ImageNet-C:motion → snow → fog → shot → defocus → contrast → zoom → brightness → frost → elastic → glass → gaussian → pixelated → jpeg → impulse). Conversely, in random-order, the order of image corruptions is randomly shuffled at the beginning of each recurrence. Hence, the corruption orders across K recurrences are now entirely different. We redo the experiment of the second setting three times (with different random seeds = 0, 1, 2). Nevertheless, different TTA methods are ensured to be evaluated on the same testing stream, since it is fixed after generation. Without updating its parameters, the performance of the source model is trivially independent of the order of corruptions. Experimental Result. The experimental results are visualized in Fig. 6. The first column plots the experiments under the same-order, while the remaining three columns plot the experiments in the random-order setting, with varying random seeds. Note that the message conveyed by each sub-figure entirely matches that of Fig. 1-right. Discussions. Clearly, a similar collapsing pattern is observed in all three TTA tasks, with three combinations of 20 image corruption orders. This pattern also matches the easiest setting using the same order of image corruptions we promoted in recurring TTA. 201 5 10 15 20 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 1 5 10 15 20 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 1 5 10 15 20 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 1 5 10 15 20 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Same-order Random-order (seed=0) Random-order (seed=1) Random-order (seed=2) Testing Error Recurring TTA visit Recurring TTA visit Recurring TTA visit Recurring TTA visit (a) CIFAR-10 → CIFAR-10-C task. 1 5 10 15 20 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1 5 10 15 20 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1 5 10 15 20 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1 5 10 15 20 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Same-order Random-order (seed=0) Random-order (seed=1) Random-order (seed=2) Testing Error Recurring TTA visit Recurring TTA visit Recurring TTA visit Recurring TTA visit (b) CIFAR-100 → CIFAR-100-C task. 1 5 10 15 20 0.5 0.6 0.7 0.8 0.9 1.0 1 5 10 15 20 0.5 0.6 0.7 0.8 0.9 1.0 1 5 10 15 20 0.5 0.6 0.7 0.8 0.9 1.0 1 5 10 15 20 0.5 0.6 0.7 0.8 0.9 1.0 Same-order Random-order (seed=0) Random-order (seed=1) Random-order (seed=2) Testing Error Recurring TTA visit Recurring TTA visit Recurring TTA visit Recurring TTA visit (c) ImageNet → ImageNet-C task. Figure 6: Recurring TTA with different order of corruptions. This figure plots the testing error of two TTA approaches: RoTTA - - [61], and, PeTTA- - (ours), and source model-×- as a reference performance under our recurring TTA (with 20 visits) across three TTA tasks. On the same-order experiments (column 1), the same order of image corruptions is applied for all 20 visits. Meanwhile, in random-order, this order is reshuffled at the beginning of each visit (columns 2-4). Random-order experiments are redone three times with different random seeds. Here, we empirically validate that using the same order of domain shifts (image corruptions) in our recurring TTA is sufficient to showcase the model collapse and evaluate the persistence of our PeTTA. Best viewed in color. E Further Justifications on Persistent TTA (PeTTA) E.1 Pseudo Code We summarize the key steps of our proposed PeTTA in Alg. 1, with the key part (lines 4-13) highlighted in blue. Our approach fits well in the general workflow of a TTA algorithm, enhancing the regular mean-teacher update step. Appdx. E.5 elaborates more on our contributions in PeTTA, distinguishing them from other components proposed in previous work. The notations and definitions of all components follow the main text (described in detail in Sec. 4). On line 8 of Alg. 1, as a 21Algorithm 1 Persistent TTA (PeTTA) Input: Classification model ft and its deep feature extractor ϕθt, both parameterized by θt ∈ Θ. Testing stream {Xt}T t=0, initial model parameter (θ0), initial update rate (α0), regularization term coefficient (λ0), empirical mean ({µy 0}y∈Y) and covariant matrix ({Σy 0}y∈Y) of feature vectors in the training set, ˆµy t EMA update rate (ν). 1 ˆµy 0 ← µy 0, ∀y ∈ Y; // Initialization 2 for t ∈ [1, ··· , T] do 3 ˆYt ← ft−1(Xt) ; // Obtaining pseudo-labels for all samples in Xt 4 // Persistent TTA (PeTTA) 5 ˆYt ← n ˆY (i) t |i = 1, ··· , Nt o ; // Set of (unique) pseudo-labels in Xt 6 ¯γt ← 0 ; 7 for y ∈ ˆYt do 8 γy t ← 1 − exp \u0010 −(ˆµy t − µy 0)T (Σy 0)−1 (ˆµy t − µy 0) \u0011 ; // Divergence sensing term on category y 9 ¯γt ← ¯γt + γy t | ˆYt| ; // Average divergence sensing term for step t 10 ˆµy t ← (1 − ν)ˆµy t−1 + νϕθt−1 (Xt|ˆYt = y) ; // EMA update of ˆµy t for samples with ˆYt = y 11 end 12 λt ← ¯γt · λ0 ; // Computing adaptive regularization term coefficient 13 αt ← (1 − ¯γt) · α0 ; // Computing adaptive update rate 14 // Regular Mean-teacher Update 15 θ′ t ← Optim θ′∈Θ EPt h LCLS \u0010 ˆYt, Xt; θ′ \u0011 + LAL (Xt; θ′) i + λtR(θ′) ; // Student model update 16 θt ← (1 − αt)θt−1 + αtθ′ t. ; // Teacher model update 17 // Final prediction 18 yeild ft(Xt) ; // Returning the final inference with updated model ft 19 end shorthand notation, ϕθt−1 (Xt|ˆYt = y) denotes the empirical mean of all feature vectors of X(i) t (extracted by ϕθt−1 \u0010 X(i) t \u0011 ) if ˆY (i) t = y, i= 1, ··· , Nt in the current testing batch. E.2 Anchor Loss KL Divergence Minimization-based Interpretation of Anchor Loss. In Sec. 4, we claimed that minimizing the anchor loss LAL is equivalent to minimizing the relative entropy (or KL divergence) between the output probability of two models parameterized by θ0 and θ. Proof. Having: DKL (Pr(y|Xt; θ0)||Pr(y|Xt; θ)) = X y∈Y Pr(y|Xt; θ0) log Pr(y|Xt; θ0) Pr(y|Xt; θ) = − X y∈Y Pr(y|Xt; θ0) log Pr(y|Xt; θ) | {z } LAL(Xt;θ) −H(Pr(y|Xt; θ0))| {z } constant . Hence, argmin θ∈Θ LAL(Xt; θ) = argmin θ∈Θ DKL (Pr(y|Xt; θ0)||Pr(y|Xt; θ)) . 22Intuitively, a desirable TTA solution should be able to adapt to novel testing distributions on the one hand, but it should not significantly diverge from the initial model. LAL fits this purpose, constraining the KL divergence between two models at each step. Connections between Anchor Loss and Regularizer Term. While supporting the same objective (collapse prevention by avoiding the model significantly diverging from the source model), the major difference between Anchor loss ( LAL) and the Regularizer term ( R(θ)) is that the anchor loss operates on the probability space of model prediction while the regularizer term works on the model parameter spaces. Tab. 4 (lines 1 and 5) summarizes the ablation study when each of them is eliminated. We see the role of the regularization term is crucial for avoiding model collapse, while the anchor loss guides the adaptation under the drastic domain shift. Nevertheless, fully utilizing all components is suggested for maintaining TTA persistence. E.3 The Use of the Memory Bank The size of Memory Bank. The size of the memory bank in PeTTA is relatively small, equal to the size of one mini-batch for update (64 images, specifically). The Use of the Memory Bank in PeTTA is Fair with Respect To the Compared Methods.Our directly comparable method - RoTTA [61] also takes this advantage (referred to as category-balanced sampling, Sec. 3.2 of [ 61]). Hence, the comparison between PeTTA and RoTTA is fair in terms of additional memory usage. Noteworthy, the use of a memory bank is a common practice in TTA literature (e.g., [15, 8, 61]), especially in situations where the class labels are temporally correlated or non-i.i.d. distributed (as we briefly summarized in Appdx. A - Related Work section). CoTTA [59], EATA [40] and MECTA [ 22] (compared method) assume labels are i.i.d. distributed. Hence, a memory bank is unnecessary, but their performance under temporally correlated label distribution has dropped significantly as a trade-off. The RMT [12] (compared method) does not require a memory bank but it needs to cache a portion of the source training set for replaying (Sec. 3.3 in [12]) which even requires more resources than the memory bank. Eliminating the Need for a Memory Bank. As addressing the challenge of temporally correlated label distribution on the testing stream is not the focus of PeTTA, we have conveniently adopted the use of the memory bank proposed in [61]. Since this small additional memory requirement is not universally applied in every real-world scenario, we believe that this is a reasonable assumption, and commonly adopted in TTA practices. Nevertheless, exploring alternative ways for reducing the memory size (e.g., storing the embedded features instead of the original image) would be an interesting future direction. E.4 Empirical Mean and Covariant Matrix of Feature Vectors on the Source Dataset Two Ways of Computing µy 0 and Σy 0 in Practice. One may notice that in PeTTA, computing γy t requires the pre-computed empirical mean (µy 0) and covariance (Σy 0) of the source dataset . This requirement may not be met in real-world situations where the source data is unavailable. In practice, the empirical mean and covariance matrix computed on the source distribution can be provided in the following two ways: 1. Most ideally, these values are computed directly by inference on the entire training set once the model is fully trained. They will be provided alongside the source-distribution pre-trained model as a pair for running TTA. 2. With only the source pre-trained model available, assume we can sample a set of unlabeled data from the source distribution. The (pseudo) labels for them are obtained by inferring from the source model. Since the source model is well-performed in this case, using pseudo is approximately as good as the true label. Accessing the Source Distribution Assumption in TTA. In fact, the second way is typically assumed to be possible in previous TTA methods such as EATA [40], and MECTA [22] (a compared method) to estimate a Fisher matrix (for anti-forgetting regularization purposes). Our work - PeTTA follows the same second setup as the previous approaches mentioned above. A variation of RMT [12] (a compared method) approach even requires having the fully labeled source data available at test-time for source replaying (Sec. 3.3 of [12]). This variation is used for comparison in our experiments. 23We believe that having the empirical mean and covariant matrix pre-computed on a portion of the source distribution in PeTTA is a reasonable assumption . Even in the ideal way, revealing the statistics might not severely violate the risk of data privacy leakage or require notable additional computing resources. Number of Samples Needed for Computation. To elaborate more on the feasibility of setting (2) mentioned above, we perform a small additional experiment on the performance of PeTTA while varying the number of samples used for computing the empirical mean and covariant matrix on the source distribution. In this setting, we use the test set of CIFAR-10, CIFAR-100, DomainNet validation set of ImageNet (original images, without corruption, or the real domain test set of DomainNet), representing samples from the source distribution. The total number of images is 10, 000 in CIFAR-10/A00, 50, 000 in ImageNet, and 69, 622 in DomainNet. We randomly sample 25%, 50%, 75%, and 100% of the images in this set to run PeTTA for 20 rounds of recurring. The result is provided in Tab. 6 below. Table 6: Average classification error of PeTTA (across 20 visits) with varying sizes of source samples used for computing feature empirical mean (µy 0) and covariant matrix (Σy 0). TTA Task 25% 50% 75% 100% CIFAR-10→CIFAR-10-C 22.96 22.99 23.03 22.75 CIFAR-100→CIFAR-100-C 35.01 35.11 35.09 35.15 DomainNet:real→clip→paint→sketch 43.18 43.12 43.15 42.89 ImageNet→ImageNet-C 61.37 59.68 61.05 60.46 The default choice of PeTTA is using 100% samples of the validation set of the source dataset. However, we showcase that it is possible to reduce the number of unlabeled samples from the source distribution to compute the empirical mean and covariant matrix for PeTTA, without significantly impacting its performance. E.5 Novelty of PeTTA PeTTA is composed of multiple components. Among them, the anchor loss is an existing idea (examples of previous work utilizing this idea are [ 32, 12]). Similarly, the mean-teacher update; and regularization are well-established techniques and very useful for the continual or gradual TTA scenario. Hence, we do not aim to improve or alternate these components. Nevertheless, the novelty of our contribution is the sensing of the divergence and adaptive model update, in which the importance of minimizing the loss (adaptation) and regularization (collapse prevention) is changed adaptively. In short, we propose a harmonic way of combining those elements adaptively to achieve a persistent TTA process. The design of PeTTA draws inspiration from a theoretical analysis (Sec. 3.2), empirically surpassing both the conventional reset-based approach [45] (Appdx. F.3) and other continual TTA approaches [61, 12, 59, 22, 7] on our proposed recurring TTA (Sec. 3.1, Appdx. F.1), as well as the previously established CCC [45] benchmark. F Additional Experimental Results of PeTTA F.1 Performance of PeTTA Versus Compared Methods Performance on CIFAR-100-C and Domainnet Datasets. Due to the length constraint, the classification errors on the tasks CIFAR-100→CIFAR-100-C, and real → clipart, painting, sketch of DomainNet are provided in Tab. 7 and Tab. 8. To prevent model collapse, the adaptability of PeTTA is more constrained. As a result, it requires more time for adaptation initially (e.g., in the first visit) but remains stable thereafter. Generally, consistent trends and observations are identified across all four TTA tasks. Standard Deviation of PeTTA Performance Across Multiple Runs. For PeTTA experiments marked with (*) in Tab. 1, Tab. 2, Tab. 7, and Tab. 8, the average performance across five independent runs with different random seeds is reported. Due to the space constraint, the corresponding standard deviation values are now reported in Tab. 9. Generally, the average standard deviation across runs 24Table 7: Average classification error of the task CIFAR-100 → CIFAR-100-C in recurring TTA scenario. The lowest error is highlighted in bold, (∗)average value across 5 runs (different random seeds) is reported for PeTTA. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Source 46.5 46.5 LAME [7] 40.5 40.5 CoTTA [59]53.4 58.4 63.4 67.6 71.4 74.9 78.2 81.1 84.0 86.7 88.8 90.7 92.3 93.5 94.7 95.6 96.3 97.0 97.3 97.683.1EATA [40]88.5 95.0 96.8 97.3 97.4 97.2 97.2 97.3 97.4 97.5 97.5 97.5 97.6 97.7 97.7 97.7 97.8 97.8 97.7 97.796.9RMT [12]50.5 48.6 47.9 47.4 47.3 47.1 46.9 46.9 46.6 46.8 46.7 46.5 46.5 46.6 46.5 46.5 46.5 46.5 46.5 46.547.1MECTA [22]44.8 44.3 44.6 43.1 44.8 44.2 44.4 43.8 43.8 43.9 44.6 43.8 44.4 44.6 43.9 44.2 43.8 44.4 44.9 44.244.2RoTTA [61]35.5 35.2 38.5 41.9 45.3 49.2 52.0 55.2 58.1 61.5 64.6 67.5 70.7 73.2 75.4 77.1 79.2 81.5 82.8 84.561.4RDumb [45]36.7 36.7 36.6 36.6 36.7 36.8 36.7 36.5 36.6 36.5 36.7 36.6 36.5 36.7 36.5 36.6 36.6 36.7 36.6 36.536.6ROID [37]76.4 76.4 76.2 76.2 76.3 76.1 75.9 76.1 76.3 76.3 76.6 76.3 76.8 76.7 76.6 76.3 76.2 76.0 75.9 76.076.3TRIBE [52]33.8 33.335.334.935.335.137.1 37.2 37.2 39.1 39.2 41.1 41.0 43.1 45.1 45.1 45.0 44.9 44.9 44.939.6PeTTA(ours)(∗) 35.834.434.735.035.135.135.235.335.335.335.235.335.235.235.135.235.235.235.235.235.1 Table 8: Average classification error of the task real → clipart → painting → sketch on DomainNet dataset in recurring TTA scenario. Episodic TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Source 45.3 45.3 LAME [7] 45.6 45.6 CoTTA [59]96.2 97.1 97.4 97.8 98.1 98.2 98.4 98.4 98.4 98.5 98.6 98.6 98.6 98.6 98.6 98.7 98.7 98.7 98.7 98.798.3RMT [12]76.2 77.1 77.3 77.3 77.2 77.1 76.8 76.9 76.5 76.4 76.4 76.3 76.4 76.2 76.2 76.1 76.4 76.1 76.0 75.876.5MECTA [22]94.6 98.4 98.6 98.8 99.1 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.098.7RoTTA [61]44.3 43.8 44.7 46.7 48.7 50.8 52.7 55.0 57.1 59.7 62.7 65.1 68.0 70.3 72.7 75.2 77.2 79.6 82.6 85.362.1RDumb [45]44.3 44.4 44.3 44.5 44.2 44.2 44.3 44.5 44.4 44.2 44.3 44.3 44.3 44.3 44.5 44.3 44.2 44.3 44.4 44.344.3PeTTA(ours)(∗) 43.842.642.342.342.642.842.843.042.942.943.143.042.943.043.043.143.042.842.942.942.9 stays within ±0.1% for small datasets (CIFAR-10-C, CIFAR-100-C) and±0.5% for larger datasets (ImageNet-C, DomainNet). Table 9: Mean and standard deviation classification error of PeTTA on the four datasets: CIFAR-10-C (CF-10-C), CIFAR-100-C (CF-100-C), DomainNet (DN), and ImageNet-C (IN-C) with recurring TTA scenario. Each experiment is run 5 times with different random seeds. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Dataset1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg CF-10-C24.3 23.0 22.6 22.4 22.4 22.5 22.3 22.5 22.8 22.8 22.6 22.7 22.7 22.9 22.6 22.7 22.6 22.8 22.9 23.022.8±0.4±0.3±0.4±0.3±0.3±0.3±0.4±0.2±0.3±0.4±0.4±0.2±0.1±0.3±0.5±0.2±0.2±0.3±0.4±0.5 ±0.1 CF-100-C35.8 34.4 34.7 35.0 35.1 35.1 35.2 35.3 35.3 35.3 35.2 35.3 35.2 35.2 35.1 35.2 35.2 35.2 35.2 35.235.1±0.4±0.4±0.2±0.2±0.1±0.1±0.2±0.2±0.1±0.2±0.1±0.2±0.2±0.1±0.1±0.1±0.1±0.1±0.2±0.2 ±0.1 DN43.8 42.6 42.3 42.3 42.6 42.8 42.8 43.0 42.9 42.9 43.1 43.0 42.9 43.0 43.0 43.1 43.0 42.8 42.9 42.942.9±0.1±0.1±0.2±0.2±0.3±0.3±0.3±0.4±0.4±0.4±0.4±0.4±0.4±0.3±0.3±0.2±0.4±0.3±0.3±0.3 ±0.3 IN-C65.3 61.7 59.8 59.1 59.4 59.6 59.8 59.3 59.4 60.0 60.3 61.0 60.7 60.4 60.6 60.7 60.8 60.7 60.4 60.260.5±0.6±0.5±0.5±0.5±1.4±1.1±1.0±0.5±0.8±0.9±0.4±0.8±0.9±0.8±0.9±0.8±1.0±0.6±0.6±0.7 ±0.5 F.2 An Inspection of PeTTA In Fig. 7, we showcase an inspection of our PeTTA on the task CIFAR-10→ CIFAR-10-C [19] in a typical recurring TTA with 20 visits. Specifically, the visualizations of PeTTA parameters ( ¯γt, λt, and αt), adaptation losses (LCLS, LAL) and regularization term (R(θ)) are provided. Here, we observe the values of adaptive parameters λt and αt continuously changing through time, as the testing scenarios evolve during recurring TTA. This proposed mechanismstabilizes the value of the loss functions, and regularization term, balancing between the two primary objectives: adaptation and preventing model collapse. Thus, the error rate persists as a result. A similar pattern is observed on other datasets (CIFAR-100-C [19] and DomainNet [44]). F.3 Does Model Reset Help? Experiment Setup. We use the term “model reset” to represent the action of “reverting the current TTA model to the source model” . This straightforward approach is named RDumb [ 45]. We thoroughly conducted experiments to compare the performance of RDumb with PeTTA. The implementation of RDumb in this setting is as follows. We employ RoTTA [61] as the base test-time adaptor due to the characteristics of the practical TTA [ 61] stream. The model (including model 25parameters, the optimizer state, and the memory bank) is reset after adapting itself to T images.1 For each dataset, three values of this hyper-parameter T are selected: • T = 1, 000: This is the value selected by the RDumb’s authors [ 45]. Unless specifically stated, we use this value when reporting the performance of RDumb [45] in all other tables. • T = 10, 000 (CIFAR-10/100-C), T = 5, 000 (ImageNet-C) and T = 24, 237 (Domain- Net).2 This value is equal to the number of samples in the test set of a single corruption type, i.e., the model is reset exactly after visiting each Pi’s (see Sec. 3.1 for notations). For DomainNet [44], since the number of images within each domain is unequal, the average number of images is used instead. • T = 150, 000 (CIFAR-10/100-C), T = 75, 000 (ImageNet-C) and T = 72, 712 (Domain- Net). This number is equal to the number of samples in one recurrence of our recurring TTA, i.e., the model is reset exactly after visitingP1 → ··· → PD. Here, D = 15 - types of corruptions [19] for CIFAR-10/100-C and ImageNet-C and D = 3 for DomainNet (clipart, painting, sketch). For example, the model is reset 20 times within a recurring TTA setting with 20 recurrences under this choice of T. The second and the last reset scheme could be interpreted as assuming the model has access to an oracle model with a capability of signaling the transitions between domains, or recurrences. Typically, this is an unrealistic capability in real-world scenarios, and a desirable continual TTA algorithm should be able to operate independently without knowing when the domain shift happening. Experimental Results. An empirical comparison between RDumb [45] and our PeTTA are reported in Tab. 10, Tab. 11, Tab. 12 and Tab. 13 for all four tasks. Table 10: Average classification error comparison between RDumb [45] (a reset-based approach) with different reset frequencies and our PeTTA on CIFAR-10→ CIFAR-10-C task. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Reset Every1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg T= 100031.1 32.1 32.3 31.6 31.9 31.8 31.8 31.9 31.9 32.1 31.7 32.0 32.5 32.0 31.9 31.6 31.9 31.4 32.3 32.431.9T= 1000025.8 25.9 26.5 26.1 26.4 25.4 25.8 25.8 26.1 26.2 26.1 26.1 26.1 26.1 26.1 25.9 25.5 25.5 25.7 26.226.0T= 15000024.8 25.3 24.3 24.1 25.3 25.4 25.4 24.5 25.0 24.9 25.0 24.8 25.0 24.5 24.9 24.1 24.0 24.7 24.9 24.424.8 PeTTA(ours)(∗) 24.323.022.622.422.422.522.322.522.822.822.622.722.722.922.622.722.622.822.923.022.8 Table 11: Average classification error comparison between RDumb [45] (a reset-based approach) with different reset frequencies and our PeTTA on CIFAR-100-C dataset. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Reset Every1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg T= 100036.7 36.7 36.6 36.6 36.7 36.8 36.7 36.5 36.6 36.5 36.7 36.6 36.5 36.7 36.5 36.6 36.6 36.7 36.6 36.536.6T= 1000043.5 43.6 43.7 43.7 43.4 43.5 43.6 43.4 43.5 43.6 43.8 43.5 43.5 43.6 43.4 43.6 43.5 43.8 43.7 43.643.6T= 15000035.435.4 35.4 35.3 35.4 35.4 35.5 35.6 35.4 35.4 35.535.3 35.235.435.135.835.135.6 35.3 35.835.4 PeTTA(ours)(∗) 35.834.434.735.035.135.135.235.335.335.335.235.335.235.235.135.235.235.235.235.235.1 Table 12: Average classification error comparison between RDumb [45] (a reset-based approach) with different reset frequencies and our PeTTA on DomainNet dataset. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Reset Every1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg T= 100044.3 44.4 44.3 44.5 44.2 44.2 44.3 44.5 44.4 44.2 44.3 44.3 44.3 44.3 44.5 44.3 44.2 44.3 44.4 44.344.3T= 2423744.1 44.3 43.9 44.2 44.1 44.3 44.2 44.4 44.1 44.1 44.0 44.3 44.1 44.0 44.0 44.2 44.1 44.1 44.1 44.444.1T= 7271244.3 44.3 44.0 44.3 44.1 44.3 44.2 44.4 44.2 44.1 44.0 44.1 44.2 44.1 44.1 44.1 44.1 44.0 44.0 44.344.2 PeTTA(ours)(∗) 43.842.642.342.342.642.842.843.042.942.943.143.042.943.043.043.143.042.842.942.942.9 Discussions. Across datasets and reset frequencies, our PeTTA approach is always better than RDumb [45]. The supreme performance holds even when RDumb has access to the oracle information that can reset the model exactly at the transition between each domain shift or recurrence. Importantly, this oracle information is typically unavailable in practice. 1A slight abuse of notation. T here is the number of images between two consecutive resets, following the notation on Sec. 3 of [45], not the sample indices in our notations. 2A subset of 5, 000 samples from ImageNet-C are selected following RobustBench [10] for a consistent evaluation with other benchmarks. 26Table 13: Average classification error comparison between RDumb [45] (a reset-based approach) with different reset frequencies and our PeTTA on ImageNet-C dataset. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Reset Every1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg T= 100072.2 73.0 73.2 72.8 72.2 72.8 73.3 72.7 71.9 73.0 73.2 73.1 72.0 72.7 73.3 73.1 72.1 72.6 73.3 73.172.8T= 500070.2 70.8 71.6 72.1 72.4 72.6 72.9 73.1 73.2 73.6 73.7 73.9 74.0 74.0 74.3 74.1 74.1 73.8 73.5 71.973.0T= 7500067.0 67.1 67.2 67.5 67.5 67.6 67.8 67.6 67.6 67.6 67.5 67.7 67.6 67.9 68.1 67.9 67.4 67.5 67.7 67.567.6 PeTTA(ours)(∗) 65.361.759.859.159.459.659.859.359.460.060.361.060.760.460.660.760.860.760.460.260.5 Noteworthy, it is clear that the performance of RDumb varies when changing the choice of the reset frequency. For a given choice of T, the better performance on one dataset does not guarantee the same performance on other datasets. For example, T = 1, 000 - the best empirical value found by RDumb authors [45] on CCC, does not give the best performance on our recurring TTA scenario; the second choice of T negatively impact the performance on many tasks; the third choice gives the best results, but knowing this exact recurrence frequency of the testing stream is unrealistic. The result highlights the challenge in practice when tuning this parameter (too slow/frequent), especially in the TTA setting where a validation set is unavailable. Our PeTTA, in contrast, is reset-free. F.4 PeTTA with 40 Recurring Visits To demonstrate the persistence of PeTTA over an even longer testing stream, in Tab. 14 and Fig. 8, we provide the evaluation results of PeTTA on recurring with 40 recurrences. F.5 The Sensitivity of Hyper-parameter Choices in PeTTA Table 15: Sensitivity of PeTTA with different choices ofλ0. Dataset λ0 = 1e0 λ0 = 5e0 λ0 = 1e1 λ0 = 5e1 λ0 = 1e2 CIFAR-10-C 22.9 22.7 22.8 23.2 24.1 CIFAR-100-C 35.7 35.3 35.1 35.6 36.1 ImageNet-C 61.2 61.0 60.5 61.3 62.4 There are two hyper-parameters in PeTTA: α0 and λ0. The initial learning rate of α0 = 1e−3 is used for all experiments. We do not tune this hyper-parameter, and the choice of α0 is universal across all datasets, following the previous works/compared methods (e.g., RoTTA [61], CoTTA [59]). Since λ0 is more specific to PeTTA, we included a sensitive analysis with different choices of λ0 on PeTTA, evaluated with images from CIFAR-10/100-C and ImageNet-C in Tab. 15. Overall, the choice of λ0 is not extremely sensitive, and while the best value is1e1 on most datasets, other choices such as 5e0 or 5e1 also produce roughly similar performance. Selecting λ0 is intuitive, the larger value of λ0 stronger prevents the model from collapsing but also limits its adaptability as a trade-off. In action, λ0 is an initial value and will be adaptively scaled with the sensing model divergence mechanism in PeTTA, meaning it does not require careful tuning. More generally, this hyper- parameter can be tuned similarly to the hyper-parameters of other TTA approaches, via an additional validation set, or some accuracy prediction algorithm [29] when labeled data is unavailable. F.6 More Details on the Ablation Study We provide the detailed classification error for each visit in the recurring TTA setting of each row entry in Tab. 4 (PeTTA Ablation Study): Tab. 16, Tab. 17, Tab. 18, Tab. 19; and Tab. 5 (PeTTA with various choices of regularizers): Tab. 20, Tab. 21, Tab. 22, Tab. 23. Fig. 9 presents an additional examination of the ablation study conducted on the task CIFAR-100 → CIFAR-100-C [19] for our PeTTA approach. We plot the classification error (top) and the value of ¯γt (bottom) for various PeTTA variations. As the model diverges from the initial state, the value of ¯γt increases. Unable to adjust αt or constraint the probability space via LAL limits the ability of PeTTA to prevent model collapse. In all variations with the model collapse in ablation studies, the rapid saturation of ¯γt is all observed. Therefore, incorporating all components in PeTTA is necessary. 27Table 16: Average classification error of multiple variations of PeTTA. Experiments on CIFAR10→ CIFAR10-C [19] task. Episodic TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Baseline w/oR(θ) 23.5 24.0 27.4 29.9 33.4 35.6 38.0 40.7 43.1 45.0 46.0 48.6 50.0 49.7 50.8 51.5 52.3 53.3 54.3 55.542.6 R(θ)fixedλ= 0.1λ0 23.5 24.0 27.2 29.8 33.4 35.3 37.9 40.5 43.3 45.3 46.8 49.3 50.9 51.0 52.1 53.2 54.0 54.8 56.0 57.643.3R(θ)fixedλ=λ0 23.5 23.6 26.2 28.4 31.6 33.5 36.4 38.7 41.1 43.1 44.8 47.6 49.3 49.5 50.9 52.1 53.1 54.2 55.6 57.042.0 PeTTA-λt 24.9 25.3 26.0 26.4 27.2 26.5 27.2 27.1 27.4 27.7 27.8 28.0 27.5 28.0 27.7 27.4 27.0 27.6 27.8 27.827.1PeTTA-λt+αt 25.5 24.5 23.7 23.1 23.222.423.3 23.2 23.7 24.1 23.9 24.5 24.3 24.0 23.8 23.9 23.8 24.1 24.6 24.723.9PeTTA-λt+LAL 23.323.9 24.6 25.3 26.2 25.9 26.4 26.6 26.9 26.6 26.7 26.7 26.7 26.8 26.8 27.2 26.9 26.9 26.8 27.026.2 PeTTAαt+LAL 24.323.0 22.6 22.4 22.422.522.3 22.5 22.8 22.8 22.6 22.7 22.7 22.9 22.6 22.7 22.6 22.8 22.9 23.022.8 Table 17: Average classification error of multiple variations of PeTTA. Experiments on CIFAR-100 → CIFAR100-C [19] task. Episodic TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Baseline w/oR(θ) 40.2 46.3 51.2 54.4 57.3 59.4 61.3 62.6 63.9 65.1 66.3 67.1 68.1 68.9 69.6 70.3 71.1 71.6 72.4 72.963.0 R(θ)fixedλ= 0.1λ0 40.5 46.1 51.5 55.1 58.2 60.5 62.6 64.2 65.7 67.3 68.6 69.5 70.6 71.6 72.5 73.4 74.2 74.9 75.8 76.565.0R(θ)fixedλ=λ0 41.8 47.6 52.6 56.1 58.9 60.7 62.5 63.9 65.0 66.2 67.1 68.3 69.5 70.3 71.4 72.4 73.4 74.1 75.0 75.664.6 PeTTA-λt 39.4 43.4 46.6 49.1 51.0 52.6 53.8 54.7 55.7 56.5 57.1 57.7 58.3 58.8 59.3 59.9 60.6 61.0 61.6 62.155.0PeTTA-λt+αt 39.4 40.1 40.8 40.7 41.2 41.5 41.4 41.6 41.5 41.5 41.7 41.6 41.8 41.7 41.8 42.0 41.9 41.9 42.0 41.841.4PeTTA-λt+LAL 36.2 35.6 35.7 36.1 36.2 36.4 36.4 36.5 36.2 36.2 36.6 36.5 36.5 36.6 36.5 36.6 36.5 36.5 36.3 36.536.3 PeTTAλt+αt+LAL 35.8 34.4 34.7 35.0 35.1 35.1 35.2 35.3 35.3 35.3 35.2 35.3 35.2 35.2 35.1 35.2 35.2 35.2 35.2 35.235.1 Table 18: Average classification error of multiple variations of PeTTA. Experiments onreal → clipart, painting, sketch task from DomainNet [44] task. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Baseline w/oR(θ) 52.3 69.0 68.6 68.6 69.4 70.5 71.8 73.4 75.6 77.6 78.8 81.0 82.8 84.3 85.9 87.4 88.5 89.9 90.8 92.177.9 R(θ)fixedλ= 0.1λ0 52.5 70.0 69.8 70.0 71.1 72.5 74.6 76.1 77.8 80.4 81.9 83.5 85.2 87.2 89.1 90.2 91.5 93.2 94.1 94.980.0R(θ)fixedλ=λ0 54.6 69.8 63.7 56.0 61.7 76.4 70.4 62.5 58.2 76.0 73.6 66.8 58.6 62.3 80.8 75.5 67.0 59.9 59.3 78.366.6 PeTTA-λt 49.2 64.5 62.4 60.9 59.6 58.6 57.7 57.8 57.6 57.7 58.0 58.5 59.0 59.5 59.8 61.1 62.0 62.6 63.6 64.959.7PeTTA-λt+αt 43.942.5 42.3 42.3 42.6 42.843.1 43.7 43.9 44.3 44.6 45.1 45.4 45.7 45.7 46.1 46.1 46.2 46.5 46.444.5PeTTA-λt+LAL 43.6 42.542.6 42.6 42.9 43.0 43.3 43.4 43.1 43.243.143.3 43.3 43.2 43.2 43.9 43.7 43.0 43.2 43.543.2 PeTTAλt+αt+LAL 43.8 42.642.3 42.3 42.6 42.8 42.8 43.0 42.9 42.9 43.1 43.0 42.9 43.0 43.0 43.1 43.0 42.8 42.9 42.942.9 Table 19: Average classification error of multiple variations of PeTTA. Experiments on ImageNet→ ImageNet-C [19] task. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Baseline w/oR(θ) 66.9 61.9 72.7 93.6 97.4 97.8 98.0 98.2 98.3 98.3 98.4 98.4 98.5 98.5 98.6 98.6 98.6 98.6 98.7 98.793.4 R(θ)fixedλ= 0.1λ0 65.5 70.9 79.1 85.2 90.3 92.6 95.8 95.8 95.4 97.3 96.9 97.7 97.9 98.2 98.0 98.7 98.6 98.4 98.4 98.792.5R(θ)fixedλ=λ0 66.5 62.1 73.0 93.5 97.0 97.2 97.5 97.5 97.6 97.5 97.7 97.7 97.7 97.8 97.9 97.9 98.0 98.0 98.0 97.992.9 PeTTA-λt 65.9 62.1 76.3 96.7 97.0 96.9 96.9 96.9 97.0 97.1 97.0 97.2 97.0 97.1 97.1 97.0 97.0 97.0 97.0 97.092.7PeTTA-λt+αt 64.870.5 74.6 75.8 75.5 75.8 76.1 76.2 76.2 76.5 76.7 77.0 76.9 77.4 77.1 77.3 77.2 77.4 77.6 77.475.7PeTTA-λt+LAL 64.8 61.160.0 59.8 60.4 60.4 61.2 61.2 61.8 61.9 62.1 62.2 62.1 62.9 62.1 62.8 62.7 62.1 62.8 66.662.0 PeTTA(ours)(∗) 65.3 61.7 59.8 59.1 59.4 59.6 59.8 59.3 59.4 60.0 60.3 61.0 60.7 60.4 60.6 60.7 60.8 60.7 60.4 60.260.5 Table 20: Average classification error of PeTTA with various choices of regularizers. Experiments on CIFAR-10 → CIFAR-10-C [19] task. Episodic TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg L2 25.6 24.8 23.8 23.1 23.2 22.7 23.0 22.7 22.7 22.7 22.8 22.7 22.8 22.7 22.522.3 22.2 22.4 22.7 22.823.0L2+Fisher25.2 23.7 22.5 21.8 22.3 21.5 22.3 22.1 22.5 22.8 22.6 22.622.622.8 22.6 22.9 22.6 22.9 23.0 23.322.7 Cosine 24.3 23.022.6 22.4 22.4 22.5 22.3 22.5 22.8 22.8 22.6 22.7 22.7 22.9 22.6 22.7 22.6 22.8 22.9 23.022.8Cosine+Fisher25.1 23.822.2 21.6 22.0 21.4 22.0 21.8 22.1 22.3 22.5 22.4 22.6 22.6 22.422.7 22.6 22.8 22.8 23.322.6 Table 21: Average classification error of PeTTA with various choices of regularizers. Experiments on CIFAR-100 → CIFAR-100-C [19] task. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg L2 36.9 35.5 35.5 35.5 35.7 35.6 35.6 35.5 35.5 35.4 35.6 35.5 35.7 35.7 35.7 35.7 35.8 35.5 35.4 35.535.6L2+Fisher36.8 35.4 35.4 35.8 35.9 36.0 35.9 35.9 35.9 35.8 36.1 36.1 36.1 36.1 36.1 36.1 36.2 36.0 36.0 35.936.0 Cosine 35.8 34.4 34.7 35.0 35.1 35.1 35.2 35.3 35.3 35.3 35.2 35.3 35.2 35.2 35.1 35.2 35.2 35.2 35.2 35.235.1Cosine+Fisher36.7 35.2 35.5 35.6 35.9 35.9 36.1 36.0 36.0 35.9 36.0 36.0 36.0 36.1 36.0 36.0 35.9 35.9 35.9 36.035.9 28Table 22: Average classification error of PeTTA with various choices of regularizers. Experiments on real → clipart, painting, sketch task from DomainNet [44] dataset. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg L2 43.8 42.7 42.5 42.4 42.8 42.9 43.0 43.1 43.1 43.2 43.4 43.3 43.2 43.3 43.2 43.2 43.4 43.0 43.1 43.143.1L2+Fisher43.9 42.8 42.7 43.0 43.2 43.4 43.6 43.8 43.9 44.1 44.0 44.2 44.2 44.2 44.4 44.4 44.5 44.5 44.5 44.543.9 Cosine 43.8 42.642.3 42.3 42.6 42.8 42.8 43.0 42.9 42.9 43.1 43.0 42.9 43.0 43.0 43.1 43.0 42.8 42.9 42.942.9Cosine+Fisher43.7 42.542.5 42.6 42.9 43.2 43.2 43.5 43.4 43.5 43.4 43.5 43.4 43.6 43.5 43.5 43.4 43.5 43.3 43.443.3 Table 23: Average classification error of PeTTA with various choices of regularizers. Experiments on ImageNet → ImageNet-C [19] task. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg L2 70.8 72.2 71.5 69.8 72.3 69.3 70.3 70.5 70.0 70.8 70.2 72.1 71.4 70.8 70.9 70.9 69.7 71.0 71.1 70.470.8L2+Fisher70.5 70.0 69.5 69.4 69.6 69.9 69.2 69.3 72.2 70.4 71.0 70.5 71.7 71.5 71.3 68.4 68.6 68.8 68.7 68.770.0 Cosine 65.361.7 59.8 59.1 59.4 59.6 59.8 59.3 59.4 60.0 60.3 61.0 60.7 60.4 60.6 60.7 60.8 60.7 60.4 60.260.5Cosine+Fisher65.1 61.760.9 61.2 61.9 62.6 62.8 63.2 64.2 63.4 64.3 64.4 63.9 64.3 65.8 65.5 64.9 65.0 65.2 65.263.8 F.7 More Confusion Matrices in Recurring TTA Setting For the task CIFAR-10→ CIFAR-10-C [19] in recurring TTA setting (with 20 visits), we additionally showcase the confusion matrix of RoTTA [61] (Fig. 10) and our proposed PeTTA (Fig. 11) at each visit. Our PeTTA persistently achieves competitive performance across 20 visits while RoTTA [61] gradually degrades. G Experimental Details G.1 Computing Resources A computer cluster equipped with an Intel(R) Core(TM) 3.80GHz i7-10700K CPU, 64 GB RAM, and one NVIDIA GeForce RTX 3090 GPU (24 GB VRAM) is used for our experiments. G.2 Experiments on CCC Testing Stream In this section, we further evaluate the performance of our PeTTA on the testing data stream of Continuous Changing Corruption (CCC) [ 45] setting. Here we use the baseline accuracy 20%, transition speed 1000, and random seed 44.3 The compared methods are source model (ResNet 50), PeTTA, RoTTA [61], and RDumb [45]. Noteworthy, different from recurring TTA, the class labels here are i.i.d. distributed. The adaptation configuration of PeTTA follows the same settings as used on ImageNet-C, while the same setting introduced in Sec. F.3, with T = 1000 is used for RDumb [45]. G.3 Test-time Adaptation Methods Pre-trained Model on Source Distribution. Following previous studies [57, 61, 12, 59], only the batch norm layers are updated. As stated in Sec. 5.2, RobustBench [10] and torchvision [35] provide pre-trained models trained on source distributions. Specifically, for ImageNet-C and Do- mainNet experiments, a ResNet50 model [17] pre-trained on ImageNet V2 (specifically, checkpoint ResNet50_Weights.IMAGENET1K_V2 of torchvision) is used. From RobustBench, the model with checkpoint Standard and Hendrycks2020AugMix_ResNeXt [20] are adopted for CIFAR10-C and CIFAR-100-C experiments, respectively. Lastly, experiments on DomainNet dataset utilize the checkpoint (best_real_2020) provided in AdaContrast [8] study.4 Optimizer. Without specifically stated, Adam [26] optimizer with learning rate equal 1e−3, and β = (0.9, 0.999) is selected as a universal choice for all experiments. More Details on PeTTA. Since designing the batch normalization layers, and the memory bank is not the key focus of PeTTA, we conveniently adopt the implementation of the Robust Batch Norm layer and the Category-balanced Sampling strategy using a memory bank introduced in RoTTA [61]. 3https://github.com/oripress/CCC 4https://github.com/DianCh/AdaContrast 29G.4 The Use of Existing Assets Many components of PeTTA is utilized from the official repository of RoTTA [61] 5 and RMT [12]. 6 These two assets are released under MIT license. All the datasets, including CIFAR-10-C, CIFAR- 100-C and ImageNet-C [ 19] are publicly available online, released under Apache-2.0 license. 7 DomainNet dataset [44] (cleaned version) is also released for research purposes.8 5https://github.com/BIT-DA/RoTTA 6https://github.com/mariodoebler/test-time-adaptation 7https://github.com/hendrycks/robustness 8https://ai.bu.edu/M3SDA/ 300 10000 20000 30000 40000 Test-time adaptation step (t) 0.40 0.60 0.80 1.00 ¯γt 0 10000 20000 30000 40000 Test-time adaptation step (t) 4.00 6.00 8.00 10.00λt 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.00 0.25 0.50 0.75 1.00αt 1e 3 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.00 1.00 2.00 3.00 4.00LCLS 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.00 2.50 5.00 7.50 10.00LAL 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.00 0.50 1.00 1.50 2.00 R(θ) 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.00 0.25 0.50 0.75 1.00Testing error Figure 7: An inspection of PeTTA on the task CIFAR-10 → CIFAR-10-C [19] in a recurring with 20 visits (visits are separated by the vertical dashed lines). Here, we visualize (rows 1-3) the dynamic of PeTTA adaptive parameters (¯γt, λt, αt), (rows 4-5) the value of the loss functions (LCLS, LAL) and (row 6) the value of the regularization term (R(θ)) and (row 7) the classification error rate at each step. The solid line in the foreground of each plot denotes the running mean. The plots show an adaptive change of λt, αt through time in PeTTA, which stabilizes TTA performance, making PeTTA achieve a persisting adaptation process in all observed values across 20 visits. 31Figure 8: Testing error of PeTTA with 40 recurring TTA visits. Total Visits CF-10-C CF-100-C IN-C 20 visits 22.8 35.1 60.5 40 visits 22.9 35.1 61.0 Table 14: Average testing error of PeTTA in recurring TTA with 20 and 40 visits. PeTTA demonstrates its persistence over an extended testing time horizon beyond the 20 th visit milestone (Fig. 8’s horizontal dashed line). 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.20 0.30 0.40 0.50 0.60 0.70 0.80Testing Error PeTTA - λt Baseline w/o R(θ) PeTTA - λt + αt R(θ) fixed λ= 0.1λ0 PeTTA - λt + LAL R(θ) fixed λ= λ0 PeTTA - λt  + αt  + LAL 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.20 0.40 0.60 0.80 1.00 ¯γt PeTTA - λt PeTTA - λt + αt PeTTA - λt + LAL PeTTA - λt  + αt  + LAL Figure 9: An inspection on the ablation study of multiple variations of PeTTA on the task CIFAR-100 → CIFAR-100-C [19] in an episodic TTA with 20 visits (visits are separated by the vertical dashed lines). (top): testing error of multiple variations of PeTTA. The performance of PeTTA without (w/o) R(θ), or fixed regularization coefficient ( λ = λ0/0.1λ0) degrades through time (the top 3 lines). The degradation of PeTTA -λt is still happening but at a slower rate (justification below). The performance of the other three variations persists through time with PeTTA -λt + αt + LAL achieves the best performance. (bottom): changes of ¯γt in multiple variations of PeTTA. When limiting the degree of freedom in adjusting αt or lacking of supervision from LAL (e.g., PeTTA -λt + αt, PeTTA -λt + LAL, and especially PeTTA -λt), the value of γt, unfortunately, escalates and eventually saturated. After this point, PeTTA has the same effect as using a fixed regularization coefficient. Therefore, fully utilizing all components is necessary to preserve the persistence of PeTTA. Best viewed in color. 320: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.79 0.01 0.04 0.03 0.02 0.01 0.01 0.02 0.05 0.02 0.02 0.82 0.01 0.01 0 0.01 0.01 0.01 0.01 0.09 0.06 0 0.68 0.07 0.04 0.03 0.06 0.03 0.01 0.01 0.02 0.01 0.04 0.66 0.04 0.08 0.07 0.05 0.01 0.02 0.03 0 0.04 0.06 0.68 0.02 0.06 0.09 0.01 0.01 0.03 0 0.05 0.15 0.03 0.61 0.03 0.07 0.01 0.01 0.02 0.01 0.03 0.07 0.02 0.02 0.8 0.02 0 0.01 0.01 0 0.02 0.03 0.03 0.02 0.01 0.87 0 0.01 0.09 0.02 0.02 0.02 0.01 0 0.02 0.01 0.77 0.04 0.03 0.03 0.01 0.01 0 0 0.01 0.01 0.03 0.85 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.76 0.01 0.03 0.03 0.01 0 0.03 0.02 0.07 0.03 0.02 0.76 0 0.01 0 0 0.03 0.01 0.02 0.16 0.07 0 0.63 0.08 0.06 0.02 0.08 0.04 0.01 0.01 0.02 0 0.04 0.7 0.04 0.04 0.09 0.05 0.01 0.02 0.03 0 0.03 0.05 0.73 0.01 0.06 0.08 0.01 0.01 0.01 0 0.03 0.23 0.04 0.53 0.06 0.08 0.01 0.01 0.02 0 0.02 0.1 0.02 0.01 0.81 0.01 0 0.01 0.01 0 0.01 0.05 0.03 0.01 0.01 0.87 0 0.01 0.08 0.01 0.01 0.02 0.01 0 0.02 0.01 0.8 0.04 0.03 0.02 0.01 0.02 0 0 0.02 0.01 0.02 0.87 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.7 0.01 0.03 0.04 0.02 0 0.03 0.03 0.09 0.06 0.01 0.72 0 0.01 0 0 0.04 0 0.01 0.2 0.07 0 0.56 0.1 0.08 0.02 0.09 0.04 0.01 0.02 0.01 0 0.03 0.7 0.05 0.02 0.13 0.04 0 0.02 0.04 0 0.03 0.07 0.69 0 0.08 0.07 0.01 0.01 0.01 0 0.04 0.26 0.05 0.42 0.13 0.07 0 0.01 0.01 0 0.02 0.11 0.03 0 0.8 0.01 0 0.01 0.01 0 0.02 0.06 0.05 0.01 0.04 0.8 0 0.01 0.07 0.01 0.01 0.03 0.01 0 0.03 0.01 0.78 0.05 0.02 0.01 0.01 0.02 0.01 0 0.04 0.01 0.02 0.86 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.62 0.01 0.03 0.06 0.03 0 0.05 0.04 0.09 0.08 0.01 0.66 0 0.02 0.01 0 0.04 0 0.02 0.25 0.07 0 0.48 0.13 0.1 0.02 0.13 0.03 0.01 0.02 0.01 0 0.02 0.68 0.05 0.02 0.17 0.03 0 0.02 0.03 0 0.02 0.07 0.67 0 0.12 0.07 0.01 0.01 0.01 0 0.02 0.29 0.07 0.39 0.14 0.06 0 0.01 0.01 0 0.01 0.11 0.04 0 0.8 0.01 0 0.01 0.01 0 0.02 0.08 0.06 0.01 0.06 0.75 0 0.01 0.05 0.01 0.01 0.04 0.02 0 0.05 0.01 0.74 0.07 0.01 0.01 0 0.03 0.01 0 0.05 0.01 0.02 0.86 True label 1st visit 2nd visit 3rd visit 4th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.56 0 0.03 0.07 0.04 0 0.07 0.04 0.1 0.1 0.01 0.61 0 0.01 0.01 0 0.07 0 0.02 0.26 0.08 0 0.42 0.13 0.13 0.02 0.15 0.03 0.01 0.02 0.02 0 0.01 0.62 0.06 0.02 0.21 0.03 0 0.02 0.03 0 0.02 0.06 0.66 0 0.16 0.06 0.01 0.01 0.01 0 0.02 0.3 0.08 0.34 0.17 0.06 0 0.02 0.01 0 0.01 0.12 0.07 0 0.76 0.01 0 0.02 0.01 0 0.02 0.1 0.08 0.01 0.08 0.69 0 0.02 0.05 0.01 0.01 0.05 0.02 0 0.09 0.01 0.68 0.09 0.01 0.01 0 0.03 0.02 0 0.09 0.01 0.02 0.83 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.51 0 0.02 0.07 0.04 0 0.09 0.03 0.1 0.14 0.01 0.56 0 0.01 0.02 0 0.09 0 0.02 0.29 0.08 0 0.35 0.15 0.16 0.02 0.18 0.03 0.01 0.03 0.02 0 0.01 0.57 0.07 0.02 0.27 0.02 0 0.03 0.04 0 0.01 0.08 0.62 0 0.18 0.05 0.01 0.01 0.01 0 0.01 0.29 0.09 0.3 0.21 0.05 0 0.02 0.01 0 0.01 0.12 0.09 0 0.75 0 0 0.01 0.02 0 0.01 0.11 0.12 0.01 0.1 0.6 0 0.03 0.06 0.01 0 0.04 0.02 0 0.09 0 0.66 0.11 0.01 0.01 0 0.02 0.03 0 0.11 0 0.02 0.8 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.48 0 0.02 0.08 0.04 0 0.11 0.03 0.11 0.13 0.01 0.54 0 0.01 0.02 0 0.11 0 0.02 0.28 0.09 0 0.3 0.16 0.16 0.02 0.21 0.02 0.01 0.03 0.02 0 0.01 0.51 0.08 0.01 0.33 0.01 0.01 0.02 0.03 0 0.01 0.05 0.65 0 0.21 0.03 0.01 0.01 0.02 0 0.01 0.27 0.11 0.25 0.28 0.03 0 0.02 0.01 0 0.01 0.12 0.1 0 0.75 0 0 0.01 0.02 0 0.01 0.11 0.13 0.01 0.13 0.56 0 0.03 0.06 0 0 0.06 0.03 0 0.13 0 0.6 0.11 0.02 0.01 0 0.03 0.04 0 0.15 0 0.02 0.73 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.46 0 0.01 0.07 0.06 0 0.13 0.01 0.09 0.15 0.01 0.48 0 0.01 0.04 0 0.16 0 0.01 0.28 0.09 0 0.27 0.15 0.19 0.01 0.23 0.01 0.01 0.03 0.02 0 0.01 0.44 0.12 0.01 0.37 0.01 0.01 0.02 0.04 0 0.01 0.05 0.63 0 0.23 0.02 0.01 0.01 0.02 0 0.01 0.25 0.13 0.22 0.33 0.02 0 0.01 0.01 0 0 0.11 0.15 0 0.71 0 0 0.01 0.02 0 0.01 0.09 0.22 0 0.15 0.47 0 0.02 0.08 0 0 0.06 0.05 0 0.15 0 0.55 0.1 0.02 0.01 0 0.04 0.05 0 0.16 0 0.02 0.7 True label 5th visit 6th visit 7th visit 8th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.47 0 0.01 0.06 0.06 0 0.13 0.01 0.1 0.16 0.02 0.47 0 0.01 0.04 0 0.13 0 0.03 0.29 0.1 0 0.24 0.12 0.22 0.01 0.24 0.01 0.01 0.03 0.03 0 0 0.4 0.12 0.01 0.39 0 0.01 0.02 0.05 0 0.01 0.06 0.61 0 0.23 0.02 0.01 0.01 0.03 0 0.01 0.22 0.15 0.2 0.35 0.02 0 0.02 0.01 0 0 0.11 0.15 0 0.7 0 0.01 0.01 0.03 0 0.01 0.08 0.25 0 0.15 0.44 0.01 0.03 0.09 0 0 0.04 0.07 0 0.14 0 0.55 0.1 0.02 0.01 0 0.03 0.05 0 0.16 0 0.02 0.7 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.46 0 0.01 0.05 0.07 0 0.14 0.01 0.1 0.16 0.04 0.43 0 0.02 0.07 0 0.14 0 0.03 0.27 0.11 0 0.22 0.11 0.23 0.01 0.26 0.01 0.02 0.03 0.04 0 0 0.33 0.16 0.01 0.43 0 0.01 0.02 0.05 0 0 0.03 0.66 0 0.23 0.01 0.01 0.01 0.04 0 0.01 0.22 0.15 0.18 0.37 0.01 0.01 0.02 0.01 0 0 0.1 0.19 0 0.68 0 0.01 0.01 0.03 0 0.01 0.08 0.28 0.01 0.16 0.41 0.01 0.03 0.11 0 0 0.04 0.05 0 0.14 0 0.56 0.09 0.04 0.01 0 0.02 0.08 0 0.18 0 0.02 0.65 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.47 0 0.01 0.04 0.07 0 0.14 0 0.1 0.16 0.04 0.42 0 0.01 0.07 0 0.15 0 0.05 0.26 0.11 0 0.21 0.1 0.26 0.01 0.26 0 0.02 0.03 0.05 0 0 0.31 0.18 0.01 0.42 0 0.01 0.02 0.06 0 0 0.04 0.65 0 0.21 0.01 0.01 0.02 0.04 0 0.01 0.17 0.21 0.15 0.39 0.01 0.01 0.02 0.01 0 0 0.1 0.24 0 0.64 0 0.01 0.01 0.04 0 0.01 0.09 0.28 0 0.16 0.39 0 0.03 0.14 0 0 0.03 0.07 0 0.14 0 0.52 0.09 0.05 0.01 0 0.03 0.1 0 0.18 0 0.03 0.61 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.49 0 0.01 0.03 0.06 0 0.14 0 0.11 0.17 0.07 0.4 0 0.01 0.07 0 0.12 0 0.07 0.27 0.13 0 0.19 0.08 0.27 0.01 0.25 0 0.02 0.03 0.07 0 0 0.27 0.19 0 0.43 0 0.02 0.03 0.07 0 0 0.02 0.64 0 0.23 0.01 0.01 0.01 0.06 0 0.01 0.19 0.18 0.13 0.39 0.01 0.01 0.02 0.02 0 0 0.09 0.22 0 0.65 0 0.01 0.01 0.05 0 0 0.07 0.32 0 0.15 0.36 0.01 0.04 0.17 0 0 0.03 0.07 0 0.12 0 0.53 0.08 0.06 0.01 0 0.01 0.13 0 0.17 0 0.03 0.59 True label 9th visit 10th visit 11th visit 12th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.5 0 0 0.02 0.08 0 0.13 0 0.1 0.15 0.09 0.37 0 0.01 0.11 0 0.11 0 0.08 0.24 0.15 0 0.18 0.07 0.31 0.01 0.24 0 0.03 0.02 0.09 0 0 0.24 0.17 0 0.44 0 0.02 0.03 0.08 0 0 0.02 0.66 0 0.19 0.01 0.02 0.02 0.08 0 0.01 0.15 0.23 0.11 0.38 0.01 0.01 0.02 0.02 0 0 0.08 0.31 0 0.55 0 0.02 0.01 0.05 0 0 0.05 0.37 0 0.14 0.34 0.01 0.04 0.2 0 0 0.03 0.06 0 0.12 0 0.52 0.08 0.08 0.01 0 0.01 0.11 0 0.15 0 0.04 0.59 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.54 0 0 0.02 0.06 0 0.11 0 0.12 0.15 0.13 0.35 0 0.01 0.1 0 0.09 0 0.12 0.21 0.16 0 0.18 0.07 0.29 0.01 0.24 0 0.03 0.02 0.11 0 0 0.22 0.19 0 0.42 0 0.03 0.03 0.08 0 0 0.03 0.65 0 0.2 0.01 0.02 0.01 0.09 0 0.01 0.12 0.29 0.08 0.37 0 0.02 0.02 0.02 0 0 0.09 0.29 0 0.56 0 0.02 0.01 0.06 0 0 0.05 0.39 0 0.13 0.32 0.01 0.04 0.23 0 0 0.02 0.07 0 0.1 0 0.51 0.07 0.12 0.01 0 0.01 0.11 0 0.13 0 0.05 0.57 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.56 0 0 0.02 0.08 0 0.1 0 0.12 0.12 0.18 0.32 0 0 0.11 0 0.08 0 0.13 0.19 0.18 0 0.15 0.05 0.34 0 0.2 0 0.04 0.02 0.12 0 0 0.19 0.27 0 0.36 0 0.04 0.02 0.09 0 0 0.02 0.69 0 0.15 0.01 0.02 0.02 0.11 0 0 0.1 0.33 0.07 0.33 0 0.03 0.01 0.03 0 0 0.09 0.35 0 0.5 0 0.02 0.01 0.08 0 0 0.04 0.43 0 0.1 0.29 0.01 0.04 0.26 0 0 0.02 0.08 0 0.08 0 0.51 0.06 0.15 0.01 0 0.01 0.12 0 0.1 0 0.07 0.55 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.58 0 0 0.01 0.07 0 0.09 0 0.13 0.11 0.16 0.32 0 0 0.11 0 0.07 0 0.16 0.18 0.18 0 0.15 0.05 0.36 0 0.19 0 0.04 0.02 0.14 0 0 0.18 0.26 0 0.35 0 0.05 0.02 0.1 0 0 0.01 0.69 0 0.15 0.01 0.03 0.01 0.11 0 0 0.1 0.36 0.05 0.32 0 0.04 0.01 0.03 0 0 0.08 0.38 0 0.46 0 0.03 0.01 0.09 0 0 0.04 0.43 0 0.09 0.29 0.02 0.04 0.29 0 0 0.02 0.09 0 0.08 0 0.47 0.06 0.18 0.01 0 0.01 0.11 0 0.08 0 0.1 0.5 True label 13th visit 14th visit 15th visit 16th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.6 0 0 0.01 0.08 0 0.08 0 0.13 0.1 0.2 0.28 0 0 0.1 0 0.06 0 0.19 0.17 0.2 0 0.14 0.05 0.36 0 0.18 0 0.05 0.02 0.17 0 0 0.16 0.28 0 0.29 0 0.08 0.02 0.1 0 0 0.01 0.71 0 0.11 0.01 0.04 0.02 0.13 0 0 0.1 0.4 0.04 0.27 0 0.05 0.01 0.04 0 0 0.09 0.4 0 0.41 0 0.04 0.01 0.1 0 0 0.04 0.45 0 0.07 0.27 0.03 0.04 0.34 0 0 0.01 0.08 0 0.05 0 0.47 0.05 0.22 0.01 0 0.01 0.13 0 0.06 0 0.12 0.44 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.62 0 0 0.01 0.09 0 0.08 0 0.13 0.08 0.24 0.26 0 0 0.1 0 0.05 0 0.19 0.15 0.2 0 0.13 0.04 0.41 0 0.16 0 0.05 0.02 0.16 0 0 0.14 0.3 0 0.29 0 0.09 0.02 0.11 0 0 0.01 0.7 0 0.1 0.01 0.05 0.02 0.14 0 0 0.09 0.42 0.02 0.27 0 0.05 0.01 0.03 0 0 0.09 0.44 0 0.39 0 0.04 0.01 0.12 0 0 0.04 0.43 0 0.06 0.28 0.03 0.04 0.35 0 0 0.01 0.07 0 0.06 0 0.46 0.04 0.26 0.01 0 0.01 0.13 0 0.06 0 0.13 0.41 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.67 0 0 0 0.1 0 0.05 0 0.11 0.06 0.3 0.21 0 0 0.1 0 0.03 0 0.21 0.13 0.26 0 0.11 0.04 0.4 0 0.11 0 0.06 0.02 0.2 0 0 0.13 0.32 0 0.21 0 0.12 0.02 0.13 0 0 0.01 0.72 0 0.07 0.01 0.05 0.01 0.2 0 0 0.09 0.42 0.01 0.19 0 0.08 0.01 0.04 0 0 0.08 0.49 0 0.3 0 0.07 0.01 0.16 0 0 0.03 0.45 0 0.04 0.24 0.05 0.03 0.42 0 0 0.01 0.06 0 0.04 0 0.43 0.04 0.33 0.01 0 0 0.11 0 0.03 0 0.15 0.36 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.69 0 0 0 0.08 0 0.04 0 0.13 0.05 0.34 0.17 0 0 0.1 0 0.03 0 0.23 0.13 0.24 0 0.1 0.03 0.44 0 0.11 0 0.07 0.01 0.21 0 0 0.11 0.32 0 0.21 0 0.14 0.01 0.14 0 0 0.01 0.71 0 0.06 0.01 0.06 0.01 0.21 0 0 0.07 0.44 0 0.16 0 0.1 0.01 0.05 0 0 0.08 0.51 0 0.25 0 0.1 0.01 0.17 0 0 0.03 0.46 0 0.04 0.21 0.06 0.04 0.46 0 0 0.01 0.06 0 0.03 0 0.41 0.03 0.34 0 0 0 0.12 0 0.03 0 0.18 0.32 Predicted label Predicted label Predicted label Predicted label True label 17th visit 18th visit 19th visit 20th visit Figure 10: The dynamic of the confusion matrix of RoTTA [61] in episodic TTA with 20 visits. 330: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.77 0.01 0.04 0.03 0.03 0.01 0.02 0.02 0.05 0.02 0.02 0.84 0.01 0.02 0 0.01 0.02 0.01 0.02 0.06 0.04 0 0.69 0.07 0.05 0.05 0.05 0.02 0.01 0.01 0.04 0.01 0.05 0.62 0.05 0.1 0.06 0.04 0.01 0.02 0.03 0 0.06 0.07 0.68 0.05 0.04 0.05 0.01 0.01 0.01 0 0.04 0.14 0.03 0.7 0.03 0.04 0.01 0.01 0.01 0.01 0.04 0.06 0.03 0.03 0.78 0.01 0.01 0.01 0.03 0 0.03 0.04 0.04 0.04 0.01 0.79 0.01 0.01 0.08 0.02 0.02 0.02 0.01 0.01 0.02 0.01 0.8 0.03 0.03 0.05 0.02 0.02 0.01 0.01 0.01 0.01 0.03 0.82 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.77 0.01 0.04 0.03 0.02 0.01 0.03 0.01 0.06 0.02 0.01 0.87 0.01 0.01 0 0.01 0.01 0 0.02 0.05 0.04 0 0.7 0.09 0.05 0.03 0.06 0.02 0.01 0.01 0.03 0.01 0.06 0.64 0.05 0.08 0.06 0.04 0.01 0.02 0.02 0 0.05 0.06 0.74 0.03 0.05 0.04 0.01 0.01 0.01 0 0.05 0.15 0.04 0.66 0.04 0.04 0.01 0.01 0.02 0.01 0.04 0.06 0.02 0.02 0.78 0.01 0.01 0.03 0.02 0 0.03 0.05 0.05 0.03 0.01 0.81 0 0.01 0.05 0.02 0.01 0.02 0.01 0 0.02 0.01 0.83 0.03 0.02 0.05 0.01 0.02 0.01 0.01 0.01 0.01 0.03 0.83 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.74 0.01 0.05 0.03 0.02 0 0.03 0.01 0.06 0.02 0.02 0.87 0.01 0.02 0 0 0.01 0 0.02 0.05 0.05 0 0.7 0.07 0.05 0.03 0.06 0.02 0.01 0.01 0.02 0.01 0.05 0.68 0.05 0.07 0.07 0.03 0.01 0.02 0.02 0 0.05 0.06 0.77 0.02 0.04 0.03 0 0 0.01 0 0.07 0.15 0.04 0.65 0.04 0.03 0.01 0.01 0.01 0 0.03 0.07 0.03 0.02 0.83 0.01 0 0.01 0.01 0 0.03 0.04 0.04 0.02 0.01 0.82 0 0.01 0.06 0.02 0.01 0.02 0.01 0 0.02 0 0.85 0.02 0.02 0.05 0.01 0.02 0.01 0 0.01 0.01 0.03 0.85 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.76 0.01 0.05 0.04 0.02 0 0.02 0.01 0.07 0.03 0.01 0.87 0.01 0.01 0 0 0.01 0 0.02 0.05 0.04 0 0.73 0.06 0.05 0.03 0.06 0.01 0.01 0.01 0.01 0.01 0.05 0.71 0.05 0.06 0.06 0.03 0.01 0.01 0.02 0 0.04 0.05 0.78 0.02 0.04 0.03 0.01 0 0.01 0 0.06 0.17 0.04 0.64 0.04 0.03 0.01 0.01 0.01 0 0.03 0.06 0.03 0.01 0.85 0.01 0 0.01 0.01 0 0.04 0.04 0.05 0.02 0.01 0.81 0.01 0.01 0.05 0.02 0.01 0.02 0.01 0 0.02 0 0.84 0.02 0.02 0.05 0.01 0.02 0.01 0 0.02 0.01 0.04 0.83 True label 1st visit 2nd visit 3rd visit 4th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.76 0.02 0.04 0.04 0.02 0 0.02 0.01 0.08 0.02 0.02 0.86 0.01 0.02 0 0 0.01 0 0.02 0.05 0.04 0 0.73 0.07 0.05 0.03 0.05 0.01 0.01 0.01 0.01 0.01 0.06 0.69 0.05 0.06 0.07 0.02 0.01 0.01 0.02 0 0.05 0.07 0.76 0.02 0.04 0.03 0.01 0 0.01 0 0.07 0.17 0.04 0.64 0.03 0.03 0.01 0.01 0.01 0 0.03 0.07 0.02 0.01 0.84 0.01 0.01 0.01 0.01 0 0.04 0.04 0.06 0.02 0.01 0.81 0.01 0.01 0.04 0.02 0.02 0.02 0.01 0 0.02 0 0.86 0.02 0.02 0.05 0.02 0.02 0.01 0 0.02 0.01 0.03 0.82 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.74 0.01 0.05 0.04 0.02 0 0.03 0.01 0.07 0.02 0.01 0.88 0.01 0.01 0 0 0.01 0 0.02 0.05 0.05 0 0.74 0.07 0.05 0.02 0.05 0.01 0.01 0 0.01 0 0.05 0.7 0.06 0.06 0.07 0.02 0.01 0.01 0.01 0 0.04 0.06 0.79 0.02 0.04 0.02 0.01 0 0.01 0 0.06 0.17 0.04 0.65 0.04 0.03 0.01 0.01 0.01 0 0.03 0.07 0.02 0.01 0.85 0 0 0.01 0.01 0 0.04 0.04 0.06 0.02 0.01 0.8 0.01 0.01 0.04 0.02 0.01 0.02 0.01 0 0.02 0 0.87 0.02 0.02 0.05 0.02 0.02 0 0 0.02 0.01 0.04 0.83 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.76 0.01 0.04 0.04 0.02 0 0.03 0.01 0.07 0.02 0.01 0.88 0.01 0.01 0 0 0.01 0 0.02 0.05 0.04 0 0.74 0.06 0.05 0.02 0.05 0.01 0.01 0.01 0.01 0.01 0.06 0.68 0.05 0.06 0.08 0.02 0.01 0.01 0.01 0 0.05 0.06 0.79 0.01 0.04 0.02 0 0 0.01 0 0.07 0.18 0.05 0.61 0.04 0.03 0.01 0.01 0 0 0.03 0.06 0.02 0.01 0.86 0 0 0 0.01 0 0.04 0.04 0.06 0.02 0.01 0.8 0 0.01 0.06 0.02 0.02 0.02 0.01 0 0.02 0 0.84 0.02 0.02 0.05 0.01 0.03 0.01 0 0.02 0.01 0.04 0.81 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.75 0.01 0.04 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.87 0.01 0.01 0 0 0.02 0 0.02 0.06 0.04 0 0.73 0.08 0.05 0.02 0.05 0.01 0.01 0.01 0.01 0 0.05 0.73 0.05 0.05 0.07 0.02 0.01 0.01 0.01 0 0.05 0.06 0.79 0.01 0.05 0.02 0.01 0 0.01 0 0.06 0.18 0.04 0.63 0.04 0.02 0.01 0.01 0 0 0.03 0.08 0.02 0.01 0.83 0 0 0 0.01 0 0.04 0.05 0.07 0.02 0.01 0.79 0 0.01 0.04 0.02 0.01 0.02 0.01 0 0.02 0 0.86 0.02 0.02 0.04 0.01 0.03 0.01 0 0.03 0.01 0.04 0.81 True label 5th visit 6th visit 7th visit 8th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.74 0.01 0.05 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.05 0.04 0 0.74 0.07 0.05 0.02 0.06 0.01 0.01 0.01 0.01 0 0.06 0.71 0.05 0.05 0.07 0.02 0.01 0.01 0.01 0 0.04 0.07 0.79 0.01 0.04 0.02 0.01 0 0.01 0 0.07 0.19 0.05 0.62 0.04 0.02 0.01 0 0 0 0.03 0.07 0.02 0.01 0.84 0 0 0.01 0.01 0 0.05 0.05 0.08 0.02 0.02 0.77 0 0.01 0.04 0.02 0.02 0.02 0.01 0 0.02 0 0.85 0.02 0.02 0.05 0.02 0.02 0.01 0 0.02 0.01 0.04 0.83 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.74 0.01 0.05 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.06 0.04 0 0.73 0.07 0.05 0.02 0.05 0.01 0.01 0.01 0.01 0.01 0.05 0.7 0.06 0.05 0.08 0.02 0.02 0.02 0.02 0 0.05 0.07 0.79 0.01 0.04 0.02 0.01 0 0.01 0 0.07 0.19 0.05 0.6 0.04 0.03 0.01 0.01 0 0 0.04 0.07 0.02 0.01 0.84 0 0 0.01 0.01 0 0.04 0.05 0.08 0.02 0.01 0.78 0 0 0.04 0.02 0.02 0.02 0.01 0 0.02 0 0.85 0.02 0.02 0.05 0.02 0.02 0.01 0 0.02 0.01 0.04 0.81 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.73 0.02 0.06 0.05 0.02 0 0.04 0.01 0.07 0.02 0.01 0.87 0.01 0.01 0 0 0.02 0 0.02 0.06 0.04 0 0.74 0.08 0.05 0.02 0.05 0.01 0.01 0.01 0.01 0 0.06 0.73 0.05 0.05 0.07 0.01 0.01 0.01 0.02 0 0.06 0.07 0.76 0.01 0.04 0.02 0.01 0 0 0 0.06 0.19 0.05 0.61 0.05 0.02 0.01 0 0 0 0.03 0.07 0.02 0.01 0.86 0 0 0 0.01 0 0.04 0.05 0.08 0.02 0.01 0.77 0 0.01 0.04 0.02 0.02 0.02 0.01 0 0.03 0 0.84 0.02 0.01 0.04 0.02 0.03 0.01 0 0.02 0 0.03 0.83 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.72 0.01 0.05 0.04 0.02 0 0.04 0.01 0.08 0.02 0.01 0.87 0.01 0.01 0 0 0.02 0 0.02 0.04 0.05 0 0.72 0.08 0.05 0.02 0.05 0.01 0.01 0 0.01 0 0.06 0.73 0.05 0.04 0.06 0.02 0.01 0.01 0.02 0 0.05 0.06 0.79 0.01 0.04 0.02 0.01 0 0.01 0 0.06 0.19 0.05 0.61 0.04 0.03 0.01 0 0 0 0.03 0.09 0.02 0.01 0.83 0 0 0 0.01 0 0.05 0.05 0.07 0.02 0.01 0.78 0 0.01 0.03 0.02 0.02 0.02 0.01 0 0.03 0 0.85 0.02 0.01 0.05 0.01 0.03 0.01 0 0.02 0 0.04 0.83 True label 9th visit 10th visit 11th visit 12th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.73 0.01 0.05 0.04 0.02 0 0.03 0.01 0.09 0.02 0.01 0.86 0.01 0.01 0 0 0.02 0 0.02 0.06 0.04 0 0.73 0.08 0.05 0.02 0.05 0.01 0.01 0 0.02 0 0.06 0.73 0.05 0.04 0.06 0.02 0.01 0.01 0.01 0 0.05 0.06 0.8 0.01 0.04 0.02 0.01 0 0.01 0 0.07 0.19 0.05 0.6 0.04 0.02 0.01 0.01 0 0 0.03 0.07 0.02 0.01 0.86 0 0 0 0.01 0 0.05 0.05 0.07 0.02 0.01 0.77 0 0 0.03 0.02 0.02 0.02 0.01 0 0.04 0 0.83 0.02 0.01 0.05 0.02 0.02 0.01 0 0.02 0 0.04 0.82 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.75 0.01 0.05 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.87 0.01 0.02 0 0 0.02 0 0.02 0.05 0.05 0 0.72 0.08 0.05 0.02 0.05 0.01 0.01 0 0.01 0.01 0.05 0.73 0.05 0.05 0.07 0.01 0.01 0.01 0.01 0 0.05 0.06 0.79 0.01 0.05 0.02 0.01 0 0.01 0 0.07 0.21 0.05 0.57 0.05 0.02 0.01 0 0 0 0.03 0.07 0.02 0.01 0.86 0 0 0 0.01 0 0.05 0.05 0.08 0.02 0.02 0.76 0 0.01 0.04 0.02 0.02 0.02 0.01 0 0.02 0 0.85 0.02 0.02 0.05 0.02 0.03 0.01 0 0.02 0 0.04 0.81 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.72 0.01 0.05 0.05 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.05 0.04 0 0.73 0.08 0.05 0.02 0.05 0.01 0.01 0 0.01 0 0.06 0.72 0.05 0.04 0.07 0.01 0.01 0.01 0.02 0 0.04 0.06 0.79 0.01 0.04 0.02 0.01 0 0.01 0 0.07 0.2 0.05 0.6 0.04 0.02 0.01 0.01 0 0 0.04 0.07 0.02 0.01 0.85 0 0 0 0.01 0 0.05 0.05 0.08 0.02 0.01 0.78 0 0.01 0.04 0.02 0.02 0.02 0.01 0 0.02 0 0.85 0.02 0.02 0.05 0.02 0.02 0.01 0 0.02 0 0.04 0.82 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.75 0.01 0.05 0.04 0.02 0 0.02 0.01 0.09 0.02 0.01 0.86 0.01 0.02 0 0 0.02 0 0.02 0.05 0.04 0 0.74 0.07 0.05 0.02 0.05 0.01 0.01 0.01 0.02 0 0.06 0.73 0.05 0.04 0.07 0.01 0.01 0.01 0.02 0 0.05 0.06 0.78 0.01 0.05 0.02 0.01 0 0.01 0 0.07 0.19 0.05 0.6 0.05 0.02 0.01 0.01 0 0 0.03 0.07 0.02 0.01 0.85 0 0 0.01 0.01 0 0.04 0.06 0.08 0.02 0.01 0.77 0 0.01 0.04 0.02 0.03 0.03 0.01 0 0.04 0 0.8 0.02 0.01 0.05 0.02 0.02 0.01 0 0.02 0 0.04 0.83 True label 13th visit 14th visit 15th visit 16th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.73 0.01 0.06 0.04 0.02 0 0.04 0.01 0.07 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.05 0.04 0 0.75 0.07 0.05 0.02 0.05 0.01 0.01 0 0.01 0 0.06 0.74 0.05 0.05 0.06 0.01 0.01 0.01 0.01 0 0.05 0.06 0.8 0.01 0.04 0.02 0 0 0.01 0 0.07 0.2 0.05 0.59 0.06 0.02 0.01 0.01 0 0 0.04 0.08 0.02 0.01 0.84 0 0 0 0.01 0 0.05 0.05 0.08 0.02 0.02 0.76 0 0.01 0.05 0.01 0.01 0.02 0.01 0 0.02 0 0.85 0.02 0.02 0.05 0.02 0.03 0.01 0 0.03 0 0.03 0.81 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.72 0.01 0.05 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.04 0.04 0 0.73 0.07 0.06 0.02 0.06 0.01 0.01 0 0.01 0 0.06 0.73 0.05 0.04 0.07 0.01 0.01 0.01 0.01 0 0.06 0.06 0.79 0.01 0.05 0.02 0.01 0 0.01 0 0.07 0.21 0.05 0.59 0.04 0.02 0.01 0 0 0 0.04 0.07 0.02 0.01 0.86 0 0 0 0.01 0 0.05 0.05 0.08 0.02 0.01 0.76 0.01 0.01 0.05 0.02 0.02 0.03 0.01 0 0.02 0 0.85 0.01 0.02 0.05 0.02 0.03 0.01 0 0.03 0.01 0.04 0.8 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.73 0.01 0.06 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.05 0.04 0 0.73 0.06 0.05 0.02 0.06 0.01 0.01 0.01 0.01 0 0.06 0.73 0.05 0.05 0.07 0.01 0.01 0.01 0.01 0 0.05 0.06 0.78 0.01 0.05 0.02 0.01 0 0.01 0 0.07 0.21 0.05 0.58 0.05 0.02 0.01 0.01 0 0 0.03 0.07 0.02 0.01 0.85 0 0.01 0.01 0.01 0 0.06 0.05 0.08 0.02 0.02 0.75 0 0.01 0.03 0.02 0.02 0.02 0.01 0 0.03 0 0.85 0.02 0.02 0.05 0.02 0.02 0.01 0 0.02 0 0.04 0.83 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.73 0.01 0.06 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.05 0.04 0 0.75 0.07 0.05 0.02 0.05 0.01 0.01 0 0.01 0 0.06 0.72 0.05 0.04 0.06 0.02 0.01 0.01 0.02 0 0.06 0.07 0.76 0.01 0.05 0.02 0.01 0 0 0 0.07 0.19 0.05 0.59 0.05 0.02 0.01 0.01 0 0 0.03 0.07 0.02 0.01 0.84 0 0.01 0.01 0.01 0 0.06 0.06 0.08 0.02 0.02 0.74 0 0.01 0.04 0.02 0.02 0.02 0.01 0 0.03 0 0.84 0.02 0.01 0.05 0.02 0.03 0.01 0 0.02 0.01 0.04 0.82 Predicted label Predicted label Predicted label Predicted label True label 17th visit 18th visit 19th visit 20th visit Figure 11: The dynamic of the confusion matrix of PeTTA (ours) in episodic TTA with 20 visits. 34NeurIPS Paper Checklist 1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope? Answer: [Yes] Justification: We have highlighted the three main claims and contributions of our work in both the abstract (highlighted in bold font) and the introduction section (listed as bullet points). Guidelines: • The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have discussed the limitations and potential future work of our study in Sec. 6. Specifically, three main limitations are included: (1) Collapse prevention can not be guaranteed through regularization, PeTTA requires (2) the use of a relatively small memory bank is available and (3) the empirical mean and covariant matrix of feature vectors on the source dataset is computable. We also include discussions in Appdx. E.3 and Appdx. E.4 to further elaborate (2), and (3) respectively. Guidelines: • The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate \"Limitations\" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren’t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an impor- tant role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 353. Theory Assumptions and Proofs Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We have provided the full proof of all lemmas and theorem in Appdx. B. Guidelines: • The answer NA means that the paper does not include theoretical results. • All the theorems, formulas, and proofs in the paper should be numbered and cross- referenced. • All assumptions should be clearly stated or referenced in the statement of any theorems. • The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental Result Reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main ex- perimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: This study propose a new TTA approach - PeTTA. A full description of this approach is given in Sec. 4 with its pseudo-code provided in Appdx. E.1. The implementation of PeTTA in Python is also attached as supplemental material. Additionally, Sec. 5.2 and Appdx. G are dedicated to providing further implementation details for reproducing the main experimental results. Lastly, the construction of recurring TTA is notably simple, and can be easily extended to other TTA streams. Its configuration on each tasks is described in the Recurring TTA paragraph of Sec. 5.2. Guidelines: • The answer NA means that the paper does not include experiments. • If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. • If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. • Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. • While NeurIPS does not require releasing code, the conference does require all submis- sions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 36(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instruc- tions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: This study does not involve any private datasets. All datasets used in our exper- iments are publicly available online from previous works (more information in Appdx. G.4). The source code of PeTTA is also attached as supplemental material. Guidelines: • The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental Setting/Details Question: Does the paper specify all the training and test details (e.g., data splits, hyper- parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The experimental settings of the key results in the paper have been provided in Sec. 5.1 (Simulation Setup) and Sec. 5.2 (Setup - Benchmark Datasets). In the supplementary material, any additional experimental results beyond the main paper, such as those in Appdx. D.3, and Appdx. F.3, are consistently preceded by a subsection titledExperiment Setup summarizing the experimental details before presenting the results. Guidelines: • The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment Statistical Significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? 37Answer: [Yes] Justification: Due to the limited computing resources, we only extensively evaluate the performance of our proposed method (PeTTA) across 5 independent runs, with different random seeds. Specifically, the mean values in 5 runs are reported in Tab. 1, Tab. 2, Tab. 7, and Tab. 8. The corresponding standard deviation values are provided in Appdx. F.1. Guidelines: • The answer NA means that the paper does not include experiments. • The authors should answer \"Yes\" if the results are accompanied by error bars, confi- dence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments Compute Resources Question: For each experiment, does the paper provide sufficient information on the com- puter resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We have provided the information on the computing resources used in our experiments in Appdx. G.1. Guidelines: • The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into the paper). 9. Code Of Ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The authors have reviewed and to the best of our judgment, this study has conformed to the NeurIPS Code of Ethics. Guidelines: • The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. 38• The authors should make sure to preserve anonymity (e.g., if there is a special consid- eration due to laws or regulations in their jurisdiction). 10. Broader Impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [No] Justification: This study advances the research in test-time adaptation area in general, and not tied to particular applications. Hence, there are no significant potential societal consequences of our work which we feel must be specifically highlighted here. Guidelines: • The answer NA means that there is no societal impact of the work performed. • If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: To the best of our judgment, this study poses no risks for misuse. Guidelines: • The answer NA means that the paper poses no such risks. • Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? 39Answer: [Yes] Justification: The original papers that produced the code package or dataset have been properly cited throughout the paper. Further information on the licenses of used assets are provided in Appdx. G.4. Guidelines: • The answer NA means that the paper does not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset. • For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset’s creators. 13. New Assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This study does not release new assets. Guidelines: • The answer NA means that the paper does not release new assets. • Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and Research with Human Subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This study does not involve crowdsourcing nor research with human subjects. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribu- tion of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects 40Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This study does not involve crowdsourcing nor research with human subjects. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 41",
      "meta_data": {
        "arxiv_id": "2311.18193v4",
        "authors": [
          "Trung-Hieu Hoang",
          "Duc Minh Vo",
          "Minh N. Do"
        ],
        "published_date": "2023-11-30T02:24:44Z",
        "pdf_url": "https://arxiv.org/pdf/2311.18193v4.pdf"
      }
    },
    {
      "title": "Active Test-Time Adaptation: Theoretical Analyses and An Algorithm",
      "abstract": "Test-time adaptation (TTA) addresses distribution shifts for streaming test\ndata in unsupervised settings. Currently, most TTA methods can only deal with\nminor shifts and rely heavily on heuristic and empirical studies.\n  To advance TTA under domain shifts, we propose the novel problem setting of\nactive test-time adaptation (ATTA) that integrates active learning within the\nfully TTA setting.\n  We provide a learning theory analysis, demonstrating that incorporating\nlimited labeled test instances enhances overall performances across test\ndomains with a theoretical guarantee. We also present a sample entropy\nbalancing for implementing ATTA while avoiding catastrophic forgetting (CF). We\nintroduce a simple yet effective ATTA algorithm, known as SimATTA, using\nreal-time sample selection techniques. Extensive experimental results confirm\nconsistency with our theoretical analyses and show that the proposed ATTA\nmethod yields substantial performance improvements over TTA methods while\nmaintaining efficiency and shares similar effectiveness to the more demanding\nactive domain adaptation (ADA) methods. Our code is available at\nhttps://github.com/divelab/ATTA",
      "full_text": "Published as a conference paper at ICLR 2024 ACTIVE TEST-TIME ADAPTATION : T HEORETICAL ANALYSES AND AN ALGORITHM Shurui Gui∗ Texas A&M University College Station, TX 77843 shurui.gui@tamu.edu Xiner Li* Texas A&M University College Station, TX 77843 lxe@tamu.edu Shuiwang Ji Texas A&M University College Station, TX 77843 sji@tamu.edu ABSTRACT Test-time adaptation (TTA) addresses distribution shifts for streaming test data in unsupervised settings. Currently, most TTA methods can only deal with minor shifts and rely heavily on heuristic and empirical studies. To advance TTA under domain shifts, we propose the novel problem setting of active test-time adaptation (ATTA) that integrates active learning within the fully TTA setting. We provide a learning theory analysis, demonstrating that incorporating limited labeled test instances enhances overall performances across test domains with a theoretical guarantee. We also present a sample entropy balancing for implementing ATTA while avoiding catastrophic forgetting (CF). We introduce a simple yet effective ATTA algorithm, known as SimATTA, using real-time sample selection techniques. Extensive experimental results confirm consistency with our theoretical analyses and show that the proposed ATTA method yields substantial performance improvements over TTA methods while maintaining efficiency and shares similar effectiveness to the more demanding active domain adaptation (ADA) methods. Our code is available at https://github.com/divelab/ATTA. 1 I NTRODUCTION Deep learning has achieved remarkable success across various fields, attaining high accuracy in numerous applications (Krizhevsky et al., 2017; Simonyan and Zisserman, 2014). Nonetheless, When training and test data follow distinct distributions, models often experience significant performance degradation during test. This phenomenon, known as the distribution shift or out-of-distribution (OOD) problem, is extensively studied within the context of both domain generalization (DG) (Gulra- jani and Lopez-Paz, 2020; Koh et al., 2021; Gui et al., 2022) and domain adaptation (DA) (Ganin et al., 2016; Sun and Saenko, 2016). While these studies involve intensive training of models with considerable generalization abilities towards target domains, they overlook an important application property; namely, continuous adaptivity to real-time streaming data under privacy, resource, and efficiency constraints. This gap leads to the emergence of test-time adaptation (TTA) tasks, targeting on-the-fly adaptation to continuous new domains during the test phase or application deployment. The study of TTA encompasses two main categories; namely test-time training (TTT) methods (Sun et al., 2020; Liu et al., 2021c) and fully test-time adaptation (FTTA) (Niu et al., 2023; Wang et al., 2021). The TTT pipeline incorporates retraining on the source data, whereas FTTA methods adapt arbitrary pre-trained models to the given test mini-batch by conducting entropy minimization, without access to the source data. Nevertheless, most TTA methods can only handle corrupted distribution shifts (Hendrycks and Dietterich, 2019b) (e.g., Gaussian noise,) and rely heavily on human intuition or empirical studies. To bridge this gap, our paper focuses on tackling significant domain distribution shifts in real time with theoretical insights. We investigate FTTA, which is more general and adaptable than TTT, particularly under data ac- cessibility, privacy, and efficiency constraints. Traditional FTTA aims at adapting a pre-trained model to streaming test-time data from diverse domains under unsupervised settings. However, recent works (Lin et al., 2022; Pearl, 2009) prove that it is theoretically infeasible to achieve OOD generalization without extra information such as environment partitions. Since utilizing environment partitions requires heavy pretraining, contradicting the nature of TTA, we are motivated to incorporate extra information in a different way,i.e., integrating a limited number of labeled test-time samples to alleviate distribution shifts, following the active learning (AL) paradigm (Settles, 2009). To this end, we propose the novel problem setting of active test-time adaptation (ATTA) by incorporating ∗Equal contributions 1 arXiv:2404.05094v1  [cs.LG]  7 Apr 2024Published as a conference paper at ICLR 2024 AL within FTTA. ATTA faces two major challenges; namely, catastrophic forgetting (CF) (Kemker et al., 2018; Li and Hoiem, 2017) and real-time active sample selection. CF problem arises when a model continually trained on a sequence of domains experiences a significant performance drop on previously learned domains, due to the inaccessibility of the source data and previous test data. Real-time active sample selection requires AL algorithms to select informative samples from a small buffer of streaming test data for annotation, without a complete view of the test distribution. In this paper, we first formally define the ATTA setting. We then provide its foundational analysis under the learning theory’s paradigm to guarantee the mitigation of distribution shifts and avoid CF. Aligned with our empirical validations, while the widely used entropy minimization (Wang et al., 2021; Grandvalet and Bengio, 2004) can cause CF, it can conversely become the key to preventing CF problems with our sample selection and balancing techniques. Building on the analyses, we then introduce a simple yet effective ATTA algorithm, SimATTA, incorporating balanced sample selections and incremental clustering. Finally, we conducted a comprehensive experimental study to evaluate the proposed ATTA settings with three different settings in the order of low to high requirement restrictiveness, i.e., TTA, Enhanced TTA, and Active Domain Adaptation (ADA). Intensive experiments indicate that ATTA jointly equips with the efficiency of TTA and the effectiveness of ADA, rendering an uncompromising real-time distribution adaptation direction. Comparison to related studies. Compared to TTA methods, ATTA requires extra active labels, but the failure of TTA methods (Sec. 5.1) and the theoretical proof of Lin et al. (2022); Pearl (2009) justify its necessity and rationality. Compared to active online learning, ATTA focuses on lightweight real-time fine-tuning without round-wise re-trainings as Saran et al. (2023) and emphasizes the importance of CF avoidance instead of resetting models and losing learned distributions. In fact, active online learning is partially similar to our enhanced TTA setting (Sec. 5.2. Compared to ADA methods (Prabhu et al., 2021; Ning et al., 2021), ATTA does not presuppose access to source data, model parameters, or pre-collected target samples. Furthermore, without this information, ATTA can still perform on par with ADA methods (Sec. 5.3). The recent source-free active domain adaptation (SFADA) method SALAD (Kothandaraman et al., 2023) still requires access to model parameter gradients, pre-collected target data, and training of additional networks. Our ATTA, in contrast, with non-regrettable active sample selection on streaming data, is a much lighter and more realistic approach distinct from ADA and SFADA. More related-work discussions are provided in Appx. C. 2 T HE ACTIVE TEST-TIME ADAPTATION FORMULATION TTA methods aim to solve distribution shifts by dynamically optimizing a pre-trained model based on streaming test data. We introduce the novel problem setting of Active Test-Time Adaptation (ATTA), which incorporates active learning during the test phase. In ATTA, the model continuously selects the most informative instances from the test batch to be labeled by an explicit or implicit oracle (e.g., human annotations, self-supervised signals) and subsequently learned by the model, aiming to improve future adaptations. Considering the labeling costs in real-world applications, a “budget” is established for labeled test instances. The model must effectively manage this budget distribution and ensure that the total number of label requests throughout the test phase does not surpass the budget. We now present a formal definition of the ATTA problem. Consider a pre-trained modelf(x; ϕ) with parameters ϕ trained on the source dataset DS = (x, y)|DS|, with each data sample x ∈ Xand a label y ∈ Y. We aim to adapt model parameters θ, initialized as ϕ, to an unlabeled test-time data stream. The streaming test data exhibit distribution shifts from the source data and varies continuously with time, forming multiple domains to which we must continuously adapt. The test phase commences at time step t = 1 and the streaming test data is formulated in batches. The samples are then actively selected, labeled (by the oracle) and collected as Dte(t) = ActAlg(Ute(t)), where ActAlg(·) denotes an active selection/labeling algorithm. The labeled samples Dte(t) are subsequently incorporated into the ATTA training setDtr(t). Finally, we conclude time step t by performing ATTA training, updating model parameters θ(t) using Dtr(t), with θ(t) initialized as the previous final state θ(t − 1). Definition 1 (The ATTA problem). Given a model f(x; θ), with parameters θ, initialized with parameters θ(0) = ϕ obtained by pre-training on source domain data, and streaming test data batches Ute(t) continually changing over time, the ATTA task aims to optimize the model at any time stept (with test phase commencing at t = 1) as θ(t)∗ := argmin θ(t) (E(x,y,t)∈Dtr(t)[ℓCE (f(x; θ(t)), y)] + E(x,t)∈Ute(t)[ℓU (f(x; θ(t)))]), (1) 2Published as a conference paper at ICLR 2024 where Dtr(t) = ( ∅, t = 0 Dtr(t − 1) ∪ Dte(t), t ≥ 1, s.t. |Dtr(t)| ≤ B, (2) Dte(t) = ActAlg(Ute(t)) is actively selected and labeled, ℓCE is the cross entropy loss, ℓU is an unsupervised learning loss, and B is the budget. 3 T HEORETICAL STUDIES In this section, we conduct an in-depth theoretical analysis of TTA based on learning theories. We mainly explore two questions: How can significant distribution shifts be effectively addressed under the TTA setting? How can we simultaneously combat the issue of CF? Sec. 3.1 provides a solution with theoretical guarantees to the first question, namely, active TTA (ATTA), along with the conditions under which distribution shifts can be well addressed. Sec. 3.2 answers the second question with an underexplored technique, i.e., selective entropy minimization, building upon the learning bounds established in Sec. 3.1. We further validate these theoretical findings through experimental analysis. Collectively, we present a theoretically supported ATTA solution that effectively tackles both distribution shift and CF. 3.1 A LLEVIATING DISTRIBUTION SHIFTS THROUGH ACTIVE TEST-TIME ADAPTATION Traditional TTA is performed in unsupervised or self-supervised context. In contrast, ATTA introduces supervision into the adaptation setting. In this subsection, we delve into learning bounds and establish generalization bounds to gauge the efficacy of ATTA in solving distribution shifts. We scrutinize the influence of active learning and evidence that the inclusion of labeled test instances markedly enhances overall performances across incremental test domains. Following Kifer et al. (2004), we examine statistical guarantees for binary classification. A hypothesis is a function h : X → {0, 1}, which can serve as the prediction function within this context. In the ATTA setting, the mapping ofh varies with time as h(x, t). We use H∆H-distance following Ben- David et al. (2010), which essentially provides a measure to quantify the distribution shift between two distributions D1 and D2, and can also be applied between datasets. The probability that an estimated hypothesis h disagrees with the true labeling function g : X → {0, 1} according to distribution D is defined as ϵ(h(t), g) = E(x)∼D[|h(x, t) − g(x)|], which we also refer to as the error or risk ϵ(h(t)). While the source data is inaccessible under ATTA settings, we consider the existence of source dataset DS for accurate theoretical analysis. Thus, we initialize Dtr as Dtr(0) = DS. For every time step t, the test and training data can be expressed asUte(t) and Dtr(t) = DS ∪Dte(1) ∪Dte(2) ∪···∪ Dte(t). Building upon two lemmas (provided in Appx. D), we establish bounds on domain errors under the ATTA setting when minimizing the empirical weighted error using the hypothesish at time t. Theorem 1. Let H be a hypothesis class of VC-dimension d. At time step t, for ATTA data domains DS, Ute(1), ··· , Ute(t), ··· , Si are unlabeled samples of sizem sampled from each of thet+1 domains respectively. The total number of samples in Dtr(t) is N and the ratio of sample numbers in each component is λ = (λ0, ··· , λt). If ˆh(t) ∈ Hminimizes the empirical weighted error ˆϵw(h(t)) with the weight vector w = (w0, ··· , wt) on Dtr(t), and h∗ j (t) = arg minh∈H ϵj(h(t)) is the optimal hypothesis on the jth domain, then for any δ ∈ (0, 1), with probability of at least 1 − δ, we have ϵj(ˆh(t)) ≤ ϵj(h∗ j (t)) + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   + 2C, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. For future test domains j = t + k (k >0), assuming k′ = argmink′∈{0,1,...t} dH∆H(D(k′), Ute(t + k)) and min dH∆H (D(k′), Ute(t + k)) ≤ δD, where 0 ≤ δD ≪ +∞, then ∀δ, with probability of at least 1 − δ, we have ϵt+k(ˆh(t)) ≤ ϵt+k(h∗ t+k(t)) + tX i=0 wi  ˆdH∆H(Si, Sk′ ) + 4 s 2d log(2m) + log 2 δ m + δD + 2γi   + 2C. The adaptation performance on a test domain is majorly bounded by the composition of (labeled) training data, estimated distribution shift, and ideal joint hypothesis performance, which correspond to C, ˆdH∆H(Si, Sj), and γi, respectively. The ideal joint hypothesis error γi gauges the inherent adaptability between domains. Further theoretical analysis are in Appx. D. 3Published as a conference paper at ICLR 2024 Figure 1: (a) Empirical validation of Thm. 1. We train a series of models on N = 2000 samples from the PACS (Li et al., 2017) dataset given differentλ0 and w0 and display the test domain loss of each model. Red points are the test loss minimums given a fixed λ0. The orange line is the reference where w0 = λ0. We observe that w0 with loss minimums are located closed to the orange line but slightly smaller than λ0, which validates our findings in Eq. (4). (b) Empirical analysis with an uncertainty balancing. Given source pre-trained models, we fine-tune the models on 500 samples with different λ0 and w0, and display the combined error surface of test and source error. Although a small λ0 is good for test domain error, it can lead to non-trivial source error exacerbation. Therefore, we can observe that the global loss minimum (green X) locates in a relatively high-λ0 region. If we consider the multiple test data distributions as a single test domain,i.e., St i=1 Ute(i), Thm. 1 can be reduced into bounds for the source domain error ϵS and test domain error ϵT . Given the optimal test/source hypothesis h∗ T (t) = arg minh∈H ϵT (h(t)) and h∗ S(t) = arg minh∈H ϵS(h(t)), we have |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤w0A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (3a) |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤(1 − w0)A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (3b) where the distribution divergence termA = ˆdH∆H(S0, ST )+4 q 2d log(2m)+log 2 δ m +2γ, the empirical gap term B = 2 q d log(2N)−log(δ) 2N , ST is sampled from St i=1 Ute(i), and γ = minh∈H{ϵ0(h(t)) + ϵT (h(t))}. Our learning bounds demonstrates the trade-off between the small amount of budgeted test-time data and the large amount of less relevant source data. Next, we provide an approximation of the condition necessary to achieve optimal adaptation performance, which is calculable from finite samples and can be readily applied in practical ATTA scenarios. Following Eq. (3.a), with approximatelyB = c1 p d/N, the optimal value w∗ 0 to tighten the test error bound is a function of λ0 and A: w∗ 0 = λ0 − s A2N c2 1d − A2Nλ0(1 − λ0), for λ 0 ≥ 1 − d A2N , (4) where c1 is a constant. Note that λ0 ≥ 1 − d A2N should be the satisfied condition in practical ATTA settings, where the budget is not sufficiently big while the source data amount is relatively large. The following theorem offers a direct theoretical guarantee that ATTA reduces the error bound on test domains in comparison to TTA without the integration of active learning. Theorem 2. Let H be a hypothesis class of VC-dimension d. For ATTA data domains DS, Ute(1), Ute(2), ··· , Ute(t), considering the test-time data as a single test domain St i=1 Ute(i), if ˆh(t) ∈ H minimizes the empirical weighted error ˆϵw(h(t)) with the weight vector w on Dtr(t), let the test error be upper-bounded with |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤EBT (w, λ, N, t). Let w′ and λ′ be the weight and sample ratio vectors when no active learning is included, i.e., w′ and λ′ s.t. w′ 0 = λ′ 0 = 1 and w′ i = λ′ i = 0 for i ≥ 1, then for any λ ̸= λ′, there exists w s.t. EBT (w, λ, N, t) < EBT (w′, λ′, N, t). (5) Therefore, the incorporation of labeled test instances in ATTA theoretically enhances the overall performance across test domains, substantiating the significance of the ATTA setting in addressing distribution shifts. All proofs are provided in Appx. E. Finally, we support the theoretical findings with experimental analysis and show the numerical results of applying the principles on real-world datasets, as shown in Fig. 1. For rigorous analysis, note that our theoretical results rest on the underlying condition that N should at least be of the same scale as d, according to the principles of VC-dimension theory. The empirical alignment of our experiments with the theoretical framework can be attributed to the assumption that fine-tuning a model is roughly equivalent to learning a model with a relatively small d. Experiment details and other validations can be found in Appx. H. 4Published as a conference paper at ICLR 2024 3.2 M ITIGATING CATASTROPHIC FORGETTING WITH BALANCED ENTROPY MINIMIZATION Catastrophic forgetting (CF), within the realm of Test-Time Adaptation (TTA), principally manifests as significant declines in overall performance, most notably in the source domain. Despite the lack of well-developed learning theories for analyzing training with series data, empirical studies have convincingly illustrated the crucial role of data sequential arrangement in model learning, thereby accounting for the phenomenon of CF. Traditionally, the mitigation of CF in adaptation tasks involves intricate utilization of source domain data. However, under FTTA settings, access to the source dataset is unavailable, leaving the problem of CF largely unexplored in the data-centric view. Table 1: Correlation analysis of high/low en- tropy samples and domains. We use a source pre-trained model to select samples with low- est/highest entropy, and 1.retrain the model on 2000 samples; 2.fine-tune the model on 300 sam- ples. We report losses on source/test domains for each setting, showing that low-entropy samples form distributions close to the source domain. Sample type Retrain Fine-tune ϵS ϵT ϵS ϵT Low entropy 0.5641 0.8022 0.0619 1.8838 High entropy 2.5117 0.3414 0.8539 0.7725 To overcome this challenge of source dataset ab- sence, we explore the acquisition of “source-like” data. In TTA scenarios, it is generally assumed that the amount of source data is considerably large. We also maintain this assumption in ATTA, practically assuming the volume of source data greatly surpasses the test-time budget. As a re- sult, we can safely assume that the pre-trained model is well-trained on abundant source do- main data DS. Given this adequately trained source model, we can treat it as a “true” source data labeling function f(x; ϕ). The model es- sentially describes a distribution, Dϕ,S(X, Y) = {(x, ˆy) ∈ (X, Y) | ˆy = f(x; ϕ), x∈ DS}. The entropy of the model prediction is defined as H(ˆy) = −P c p(ˆyc) logp(ˆyc), ˆy = f(x; ϕ), where c denotes the class. Lower entropy indicates that the model assigns high probability to one of the classes, suggesting a high level of certainty or confidence in its prediction, which can be interpreted as the sample being well-aligned or fitting closely with the model’s learned distribution. In other words, the model recognizes the sample as being similar to those it was trained on. Thus entropy can be used as an indicator of how closely a sample x aligns with the model distribution Dϕ,S. Since the model distribution is approximately the source distribution, selecting (and labeling) low-entropy samples using f(x; ϕ) essentially provides an estimate of sampling from the source dataset. Therefore, in place of the inaccessible DS, we can feasibly include the source-like dataset into the ATTA training data at each time stept: Dϕ,S(t) = {(x, f(x; ϕ))|x ∈ Ute(t), H(f(x; ϕ)) < el}, (6) where el is the entropy threshold. The assumption that Dϕ,S(t) is an approximation of DS can be empirically validated, as shown by the numerical results on PACS in Tab. 1. In contrast, high-entropy test samples typically deviate more from the source data, from which we select Dte(t) for active labeling. Following the notations in Thm. 1, we are practically minimizing the empirical weighted error of hypothesis h(t) as ˆϵ′ w(h(t)) = tX j=0 wjˆϵj(h(t)) = w0 λ0N X x∈Dϕ,S(t) |h(x, t) − f(x; ϕ)| + tX j=1 wj λjN X x,y∈Dte(j) |h(x, t) − y|. (7) By substituting DS with Dϕ,S(t) in Thm. 1, the bounds of Thm. 1 continue to hold for the test domains. In the corollary below, we bound the source error for practical ATTA at each time stept. Corollary 3. At time step t, for ATTA data domains Dϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), Si are unla- beled samples of size m sampled from each of the t + 1 domains respectively, and SS is unlabeled samples of size m sampled from DS. If ˆh(t) ∈ Hminimizes ˆϵ′ w(h(t)) while other conditions remain identical to Thm. 1, then ϵS(ˆh(t)) ≤ ϵS(h∗ S(t)) + tX i=0 wi  ˆdH∆H(Si, SS) + 4 s 2d log(2m) + log 2 δ m + 2γi   + 2C, with probability at least 1 − δ, where C follows Thm. 1 and γi = minh∈H{ϵi(h(t)) + ϵS(h(t))}. Further analysis and proofs are in Appx. D and E. The following corollary provides direct theoretical support that our strategy conditionally reduces the error bound on the source domain. Corollary 4. At time step t, for ATTA data domains Dϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), suppose that ˆh(t) ∈ Hminimizes ˆϵw′(h(t)) under identical conditions to Thm. 2. Let’s denote the source error upper bound with |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤EBS(w, λ, N, t). Let w′ and λ′ be the weight 5Published as a conference paper at ICLR 2024 <latexit sha1_base64=\"NxhXSyFABPQk4q8627/odirDspg=\">AAAB9XicbVDLSgMxFM34rPVVdekmWARXZab4WhbcuKzYF7S1ZNI7bWgmMyR3lDL0P9y4UMSt/+LOvzHTdqGtBwKHc87l3hw/lsKg6347K6tr6xubua389s7u3n7h4LBhokRzqPNIRrrlMwNSKKijQAmtWAMLfQlNf3ST+c1H0EZEqobjGLohGygRCM7QSg/3mIWFGtAaGOwVim7JnYIuE29OimSOaq/w1elHPAlBIZfMmLbnxthNmUbBJUzyncRAzPiIDaBtqWIhmG46vXpCT63Sp0Gk7VNIp+rviZSFxoxD3yZDhkOz6GXif147weC6mwoVJwiKzxYFiaQY0awC2hcaOMqxJYxrYW+lfMg042iLytsSvMUvL5NGueRdli7uysXK+byOHDkmJ+SMeOSKVMgtqZI64USTZ/JK3pwn58V5dz5m0RVnPnNE/sD5/AFnsJJq</latexit> Streaming Test <latexit sha1_base64=\"a41BOKrutEYSWO9+8CjkPZKHvb8=\">AAAB73icbVBNS8NAEJ3Ur1q/qh69BIvgqSTiR48FLx4r2A9oQ9lsN+3SzSbuToQQ+ie8eFDEq3/Hm//GTZuDtj4YeLw3w8w8PxZco+N8W6W19Y3NrfJ2ZWd3b/+genjU0VGiKGvTSESq5xPNBJesjRwF68WKkdAXrOtPb3O/+8SU5pF8wDRmXkjGkgecEjRSbzAhmKWzyrBac+rOHPYqcQtSgwKtYfVrMIpoEjKJVBCt+64To5cRhZwKNqsMEs1iQqdkzPqGShIy7WXze2f2mVFGdhApUxLtufp7IiOh1mnom86Q4EQve7n4n9dPMGh4GZdxgkzSxaIgETZGdv68PeKKURSpIYQqbm616YQoQtFElIfgLr+8SjoXdfe6fnV/WWs2ijjKcAKncA4u3EAT7qAFbaAg4Ble4c16tF6sd+tj0Vqyiplj+APr8wfpIY/e</latexit> ˆy <latexit sha1_base64=\"SJEOE2ZYxLL1SU/QahOlMH6fop4=\">AAAB8HicbVBNSwMxEM3Wr1q/qh69BItQL2VX/Oix4MVjBbettEvJptk2NMkuyaxQlv4KLx4U8erP8ea/MW33oK0PBh7vzTAzL0wEN+C6305hbX1jc6u4XdrZ3ds/KB8etUycasp8GotYd0JimOCK+cBBsE6iGZGhYO1wfDvz209MGx6rB5gkLJBkqHjEKQErPfr9DNi0Cuf9csWtuXPgVeLlpIJyNPvlr94gpqlkCqggxnQ9N4EgIxo4FWxa6qWGJYSOyZB1LVVEMhNk84On+MwqAxzF2pYCPFd/T2REGjORoe2UBEZm2ZuJ/3ndFKJ6kHGVpMAUXSyKUoEhxrPv8YBrRkFMLCFUc3srpiOiCQWbUcmG4C2/vEpaFzXvunZ1f1lp1PM4iugEnaIq8tANaqA71EQ+okiiZ/SK3hztvDjvzseiteDkM8foD5zPH2KnkB4=</latexit> U te ( t ) <latexit sha1_base64=\"7rdY0fXtveVAqOkqa7z+i6K3Rp0=\">AAAB+XicbVDLSsNAFJ34rPUVdelmsAh1UxLxUXBTcOOygn1AE8pkMmmHTiZh5qZQQv/EjQtF3Pon7vwbp20W2nrgwuGce7n3niAVXIPjfFtr6xubW9ulnfLu3v7BoX103NZJpihr0UQkqhsQzQSXrAUcBOumipE4EKwTjO5nfmfMlOaJfIJJyvyYDCSPOCVgpL5tR1WPhgncYQ+GDMhF3644NWcOvErcglRQgWbf/vLChGYxk0AF0brnOin4OVHAqWDTspdplhI6IgPWM1SSmGk/n18+xedGCXGUKFMS8Fz9PZGTWOtJHJjOmMBQL3sz8T+vl0FU93Mu0wyYpItFUSYwJHgWAw65YhTExBBCFTe3YjokilAwYZVNCO7yy6ukfVlzb2rXj1eVRr2Io4RO0RmqIhfdogZ6QE3UQhSN0TN6RW9Wbr1Y79bHonXNKmZO0B9Ynz9h0pLV</latexit> f ( · ; ✓ ) <latexit sha1_base64=\"ud3dFXm+F2nsLD2/MdusutzkLvU=\">AAAB9HicbVDLSgNBEJyNrxhfUY9eBoPgKeyKr2PAixchgnlAsoTZ2d5kyMzOOjMbDEu+w4sHRbz6Md78GyfJHjSxoKGo6qa7K0g408Z1v53Cyura+kZxs7S1vbO7V94/aGqZKgoNKrlU7YBo4CyGhmGGQztRQETAoRUMb6Z+awRKMxk/mHECviD9mEWMEmMlvysC+ZTdyRD4pNQrV9yqOwNeJl5OKihHvVf+6oaSpgJiQznRuuO5ifEzogyjHCalbqohIXRI+tCxNCYCtJ/Njp7gE6uEOJLKVmzwTP09kRGh9VgEtlMQM9CL3lT8z+ukJrr2MxYnqYGYzhdFKcdG4mkCOGQKqOFjSwhVzN6K6YAoQo3NaRqCt/jyMmmeVb3L6sX9eaV2nsdRREfoGJ0iD12hGrpFddRAFD2iZ/SK3pyR8+K8Ox/z1oKTzxyiP3A+fwCmlpH9</latexit> Model SimATTA <latexit sha1_base64=\"bhVea6W/pzUPuDRNfs2xbDF7qAk=\">AAAB73icbVC7SgNBFL3rM8ZX1NJmMAhWYTf4KgM2FhYRzAOSJcxOZpMhs7PrzF0hhPyEjYUitv6OnX/jbLKFJh4YOJxzD3PvCRIpDLrut7Oyura+sVnYKm7v7O7tlw4OmyZONeMNFstYtwNquBSKN1Cg5O1EcxoFkreC0U3mt564NiJWDzhOuB/RgRKhYBSt1L6jQRYd9Eplt+LOQJaJl5My5Kj3Sl/dfszSiCtkkhrT8dwE/QnVKJjk02I3NTyhbEQHvGOpohE3/mS275ScWqVPwljbp5DM1N+JCY2MGUeBnYwoDs2il4n/eZ0Uw2t/IlSSIlds/lGYSoIxyY4nfaE5Qzm2hDIt7K6EDammDG1FRVuCt3jyMmlWK95l5eK+Wq6d53UU4BhO4Aw8uIIa3EIdGsBAwjO8wpvz6Lw4787HfHTFyTNH8AfO5w/1SI/i</latexit> Labeling <latexit sha1_base64=\"7rdY0fXtveVAqOkqa7z+i6K3Rp0=\">AAAB+XicbVDLSsNAFJ34rPUVdelmsAh1UxLxUXBTcOOygn1AE8pkMmmHTiZh5qZQQv/EjQtF3Pon7vwbp20W2nrgwuGce7n3niAVXIPjfFtr6xubW9ulnfLu3v7BoX103NZJpihr0UQkqhsQzQSXrAUcBOumipE4EKwTjO5nfmfMlOaJfIJJyvyYDCSPOCVgpL5tR1WPhgncYQ+GDMhF3644NWcOvErcglRQgWbf/vLChGYxk0AF0brnOin4OVHAqWDTspdplhI6IgPWM1SSmGk/n18+xedGCXGUKFMS8Fz9PZGTWOtJHJjOmMBQL3sz8T+vl0FU93Mu0wyYpItFUSYwJHgWAw65YhTExBBCFTe3YjokilAwYZVNCO7yy6ukfVlzb2rXj1eVRr2Io4RO0RmqIhfdogZ6QE3UQhSN0TN6RW9Wbr1Y79bHonXNKmZO0B9Ynz9h0pLV</latexit> f ( · ; ✓ ) <latexit sha1_base64=\"DPrA95GNP27SFW5vSoLC/hYa644=\">AAAB9XicbVDLSsNAFJ3UV62vqks3g0Wom5KIj4KbghuXFewDmlgmk0k7dJIJMzdKCf0PNy4Uceu/uPNvnLZZaOuBC4dz7uXee/xEcA22/W0VVlbX1jeKm6Wt7Z3dvfL+QVvLVFHWolJI1fWJZoLHrAUcBOsmipHIF6zjj26mfueRKc1lfA/jhHkRGcQ85JSAkR7CqksDCdfYTYb8tF+u2DV7BrxMnJxUUI5mv/zlBpKmEYuBCqJ1z7ET8DKigFPBJiU31SwhdEQGrGdoTCKmvWx29QSfGCXAoVSmYsAz9fdERiKtx5FvOiMCQ73oTcX/vF4KYd3LeJykwGI6XxSmAoPE0whwwBWjIMaGEKq4uRXTIVGEggmqZEJwFl9eJu2zmnNZu7g7rzTqeRxFdISOURU56Ao10C1qohaiSKFn9IrerCfrxXq3PuatBSufOUR/YH3+AFKlkbs=</latexit> f ( · ; \u0000 ) <latexit sha1_base64=\"DPrA95GNP27SFW5vSoLC/hYa644=\">AAAB9XicbVDLSsNAFJ3UV62vqks3g0Wom5KIj4KbghuXFewDmlgmk0k7dJIJMzdKCf0PNy4Uceu/uPNvnLZZaOuBC4dz7uXee/xEcA22/W0VVlbX1jeKm6Wt7Z3dvfL+QVvLVFHWolJI1fWJZoLHrAUcBOsmipHIF6zjj26mfueRKc1lfA/jhHkRGcQ85JSAkR7CqksDCdfYTYb8tF+u2DV7BrxMnJxUUI5mv/zlBpKmEYuBCqJ1z7ET8DKigFPBJiU31SwhdEQGrGdoTCKmvWx29QSfGCXAoVSmYsAz9fdERiKtx5FvOiMCQ73oTcX/vF4KYd3LeJykwGI6XxSmAoPE0whwwBWjIMaGEKq4uRXTIVGEggmqZEJwFl9eJu2zmnNZu7g7rzTqeRxFdISOURU56Ao10C1qohaiSKFn9IrerCfrxXq3PuatBSufOUR/YH3+AFKlkbs=</latexit> f ( · ; \u0000 ) <latexit sha1_base64=\"ipQ+JKlINPDcPjrbUYUkqyyzp40=\">AAAB+nicbVC7TsMwFHXKq5RXCiOLRYXEQpVUvMZKLIxF0IfURpXj3LRWHSeyHVBV+iksDCDEypew8Te4aQZoOZKlo3Puy8dPOFPacb6twsrq2vpGcbO0tb2zu2eX91sqTiWFJo15LDs+UcCZgKZmmkMnkUAin0PbH13P/PYDSMVica/HCXgRGQgWMkq0kfp2+S6bdNqQoCUxQ4K+XXGqTga8TNycVFCORt/+6gUxTSMQmnKiVNd1Eu1NiNSMcpiWeqmChNARGUDXUEEiUN4kO32Kj40S4DCW5gmNM/V3x4RESo0j31RGRA/VojcT//O6qQ6vvAkTSapB0PmiMOVYx3iWAw6YBKr52BBCJTO3YjokklBt0iqZENzFLy+TVq3qXlTPb2uV+lkeRxEdoiN0glx0ieroBjVQE1H0iJ7RK3qznqwX6936mJcWrLznAP2B9fkDSAyT+w==</latexit> Source-Pretrained <latexit sha1_base64=\"ud3dFXm+F2nsLD2/MdusutzkLvU=\">AAAB9HicbVDLSgNBEJyNrxhfUY9eBoPgKeyKr2PAixchgnlAsoTZ2d5kyMzOOjMbDEu+w4sHRbz6Md78GyfJHjSxoKGo6qa7K0g408Z1v53Cyura+kZxs7S1vbO7V94/aGqZKgoNKrlU7YBo4CyGhmGGQztRQETAoRUMb6Z+awRKMxk/mHECviD9mEWMEmMlvysC+ZTdyRD4pNQrV9yqOwNeJl5OKihHvVf+6oaSpgJiQznRuuO5ifEzogyjHCalbqohIXRI+tCxNCYCtJ/Njp7gE6uEOJLKVmzwTP09kRGh9VgEtlMQM9CL3lT8z+ukJrr2MxYnqYGYzhdFKcdG4mkCOGQKqOFjSwhVzN6K6YAoQo3NaRqCt/jyMmmeVb3L6sX9eaV2nsdRREfoGJ0iD12hGrpFddRAFD2iZ/SK3pyR8+K8Ox/z1oKTzxyiP3A+fwCmlpH9</latexit> Model <latexit sha1_base64=\"5LNAmmVR/AN9Lc2T+FRV/is2yz8=\">AAAB8nicbVDLSgNBEJyNrxhfUY9eBoPgKewGX8eACB48RDAP2CxhdjKbDJmdWWZ6lbDkM7x4UMSrX+PNv3GS7EETCxqKqm66u8JEcAOu++0UVlbX1jeKm6Wt7Z3dvfL+QcuoVFPWpEoo3QmJYYJL1gQOgnUSzUgcCtYOR9dTv/3ItOFKPsA4YUFMBpJHnBKwkn+nnvCNBK2Sca9ccavuDHiZeDmpoByNXvmr21c0jZkEKogxvucmEGREA6eCTUrd1LCE0BEZMN9SSWJmgmx28gSfWKWPI6VtScAz9fdERmJjxnFoO2MCQ7PoTcX/PD+F6CrIuExSYJLOF0WpwKDw9H/c55pREGNLCNXc3orpkGhCwaZUsiF4iy8vk1at6l1Uz+9rlfpZHkcRHaFjdIo8dInq6BY1UBNRpNAzekVvDjgvzrvzMW8tOPnMIfoD5/MHKbiRJQ==</latexit> Low Entropy <latexit sha1_base64=\"vLgKkEyV9E/djVdgAkvKuOUQOTU=\">AAAB7nicbVDLSgMxFL1TX7W+qi7dBIvgqswUX8uCG5cV7QPaoWTSTBuaZEKSEcrQj3DjQhG3fo87/8a0nYW2HrhwOOde7r0nUpwZ6/vfXmFtfWNzq7hd2tnd2z8oHx61TJJqQpsk4YnuRNhQziRtWmY57ShNsYg4bUfj25nffqLasEQ+2omiocBDyWJGsHVS+wELxanplyt+1Z8DrZIgJxXI0eiXv3qDhKSCSks4NqYb+MqGGdaWEU6npV5qqMJkjIe066jEgpowm587RWdOGaA40a6kRXP190SGhTETEblOge3ILHsz8T+vm9r4JsyYVKmlkiwWxSlHNkGz39GAaUosnziCiWbuVkRGWGNiXUIlF0Kw/PIqadWqwVX18r5WqV/kcRThBE7hHAK4hjrcQQOaQGAMz/AKb57yXrx372PRWvDymWP4A+/zB19wj48=</latexit> Samples <latexit sha1_base64=\"wuZucU3JbeEJSquG2WgqGdYMCR8=\">AAAB83icbVDLSgMxFL3js9ZX1aWbYBFclZnia1kQocsK9gHtUDJppg3NJCHJCGXob7hxoYhbf8adf2PazkJbD1w4nHMv994TKc6M9f1vb219Y3Nru7BT3N3bPzgsHR23jEw1oU0iudSdCBvKmaBNyyynHaUpTiJO29H4bua3n6g2TIpHO1E0TPBQsJgRbJ3Uq7PhCN0Lq6Wa9Etlv+LPgVZJkJMy5Gj0S1+9gSRpQoUlHBvTDXxlwwxrywin02IvNVRhMsZD2nVU4ISaMJvfPEXnThmgWGpXwqK5+nsiw4kxkyRynQm2I7PszcT/vG5q49swY0KllgqyWBSnHFmJZgGgAdOUWD5xBBPN3K2IjLDGxLqYii6EYPnlVdKqVoLrytVDtVy7zOMowCmcwQUEcAM1qEMDmkBAwTO8wpuXei/eu/exaF3z8pkT+APv8wfIYpF9</latexit> High Entropy <latexit sha1_base64=\"vLgKkEyV9E/djVdgAkvKuOUQOTU=\">AAAB7nicbVDLSgMxFL1TX7W+qi7dBIvgqswUX8uCG5cV7QPaoWTSTBuaZEKSEcrQj3DjQhG3fo87/8a0nYW2HrhwOOde7r0nUpwZ6/vfXmFtfWNzq7hd2tnd2z8oHx61TJJqQpsk4YnuRNhQziRtWmY57ShNsYg4bUfj25nffqLasEQ+2omiocBDyWJGsHVS+wELxanplyt+1Z8DrZIgJxXI0eiXv3qDhKSCSks4NqYb+MqGGdaWEU6npV5qqMJkjIe066jEgpowm587RWdOGaA40a6kRXP190SGhTETEblOge3ILHsz8T+vm9r4JsyYVKmlkiwWxSlHNkGz39GAaUosnziCiWbuVkRGWGNiXUIlF0Kw/PIqadWqwVX18r5WqV/kcRThBE7hHAK4hjrcQQOaQGAMz/AKb57yXrx372PRWvDymWP4A+/zB19wj48=</latexit> Samples <latexit sha1_base64=\"1BO6D/gzkeZNQ7HNIaph5NqELCI=\">AAAB8nicbVDLSgMxFM3UV62vqks3wSK4KjPF17LgRncV7AOmQ8mkd9rQTDIkGaEM/Qw3LhRx69e482/MtLPQ1gOBwzn3kHtPmHCmjet+O6W19Y3NrfJ2ZWd3b/+genjU0TJVFNpUcql6IdHAmYC2YYZDL1FA4pBDN5zc5n73CZRmUjyaaQJBTEaCRYwSYyX/XlAFMQhD+KBac+vuHHiVeAWpoQKtQfWrP5Q0zdOUE619z01MkBFlGOUwq/RTDQmhEzIC31JBYtBBNl95hs+sMsSRVPYJg+fq70RGYq2ncWgnY2LGetnLxf88PzXRTZAxkaQGBF18FKUcG4nz+/GQKaCGTy0hVDG7K6Zjogg1tqWKLcFbPnmVdBp176p++dCoNS+KOsroBJ2ic+Sha9REd6iF2ogiiZ7RK3pzjPPivDsfi9GSU2SO0R84nz9y2ZFU</latexit> Incremental <latexit sha1_base64=\"Jmobmj50NeE6y3ftB4xt5xZD5Eg=\">AAAB8XicbVDLSgNBEOyNrxhfUY9eBoPgKewGX8dALh4jmAcmS5id9CZDZmeXmVkhLP6FFw+KePVvvPk3TpI9aGJBQ1HVTXdXkAiujet+O4W19Y3NreJ2aWd3b/+gfHjU1nGqGLZYLGLVDahGwSW2DDcCu4lCGgUCO8GkMfM7j6g0j+W9mSboR3QkecgZNVZ6aIhUG1Rcjgblilt15yCrxMtJBXI0B+Wv/jBmaYTSMEG17nluYvyMKsOZwKdSP9WYUDahI+xZKmmE2s/mFz+RM6sMSRgrW9KQufp7IqOR1tMosJ0RNWO97M3E/7xeasIbP+MySQ1KtlgUpoKYmMzeJ0OukBkxtYQyxe2thI2posymoEs2BG/55VXSrlW9q+rlXa1Sv8jjKMIJnMI5eHANdbiFJrSAgYRneIU3RzsvzrvzsWgtOPnMMfyB8/kDzgaQ+A==</latexit> Clustering <latexit sha1_base64=\"c4xrXg0yZYBSSDLHCxlf45OWNzg=\">AAAB7nicbVDLSgNBEOz1GeMr6tHLYBA8hd2Aj2PAi8eI5gHJEmYnnWTIzOwyMyuEJR/hxYMiXv0eb/6Nk2QPmljQUFR1090VJYIb6/vf3tr6xubWdmGnuLu3f3BYOjpumjjVDBssFrFuR9Sg4AoblluB7UQjlZHAVjS+nfmtJ9SGx+rRThIMJR0qPuCMWie1HqhMBJpeqexX/DnIKglyUoYc9V7pq9uPWSpRWSaoMZ3AT2yYUW05EzgtdlODCWVjOsSOo4pKNGE2P3dKzp3SJ4NYu1KWzNXfExmVxkxk5DoltSOz7M3E/7xOagc3YcZVklpUbLFokApiYzL7nfS5RmbFxBHKNHe3EjaimjLrEiq6EILll1dJs1oJriqX99VyrZrHUYBTOIMLCOAaanAHdWgAgzE8wyu8eYn34r17H4vWNS+fOYE/8D5/AF7Wj40=</latexit> Samples <latexit sha1_base64=\"eimCpRgfVxBfxhwCehIJdcsMsvY=\">AAAB8XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5SRMtAGssI5gOTI+xt5pIle3vH7p4QjvwLGwtFbP03dv4bN8kVmvhg4PHeDDPzgkRwbVz32ylsbe/s7hX3SweHR8cn5dOzjo5TxbDNYhGrXkA1Ci6xbbgR2EsU0igQ2A2mzYXffUKleSwfzCxBP6JjyUPOqLHSY1Ok2qDicjwsV9yquwTZJF5OKpCjNSx/DUYxSyOUhgmqdd9zE+NnVBnOBM5Lg1RjQtmUjrFvqaQRaj9bXjwnV1YZkTBWtqQhS/X3REYjrWdRYDsjaiZ63VuI/3n91IS3fsZlkhqUbLUoTAUxMVm8T0ZcITNiZgllittbCZtQRZlNQZdsCN76y5ukU6t69Wr9vlZpuHkcRbiAS7gGD26gAXfQgjYwkPAMr/DmaOfFeXc+Vq0FJ585hz9wPn8AzSSQ9Q==</latexit> Clustering <latexit sha1_base64=\"JgGHFC5oztwX6+XjDtZWQo9C1hA=\">AAAB7nicbVA9TwJBEJ3DL8Qv1NJmIzGxIncUaImxscREwAQuZG8ZYMPe7mV3z4Rc+BE2Fhpj6++x89+4wBUKvmSSl/dmMjMvSgQ31ve/vcLG5tb2TnG3tLd/cHhUPj5pG5Vqhi2mhNKPETUouMSW5VbgY6KRxpHATjS5nfudJ9SGK/lgpwmGMR1JPuSMWid1biQbK2365Ypf9Rcg6yTISQVyNPvlr95AsTRGaZmgxnQDP7FhRrXlTOCs1EsNJpRN6Ai7jkoaowmzxbkzcuGUARkq7UpaslB/T2Q0NmYaR64zpnZsVr25+J/XTe3wOsy4TFKLki0XDVNBrCLz38mAa2RWTB2hTHN3K2FjqimzLqGSCyFYfXmdtGvVoF6t39cqDT+PowhncA6XEMAVNOAOmtACBhN4hld48xLvxXv3PpatBS+fOYU/8D5/AFOaj4U=</latexit> Anchors <latexit sha1_base64=\"eimCpRgfVxBfxhwCehIJdcsMsvY=\">AAAB8XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5SRMtAGssI5gOTI+xt5pIle3vH7p4QjvwLGwtFbP03dv4bN8kVmvhg4PHeDDPzgkRwbVz32ylsbe/s7hX3SweHR8cn5dOzjo5TxbDNYhGrXkA1Ci6xbbgR2EsU0igQ2A2mzYXffUKleSwfzCxBP6JjyUPOqLHSY1Ok2qDicjwsV9yquwTZJF5OKpCjNSx/DUYxSyOUhgmqdd9zE+NnVBnOBM5Lg1RjQtmUjrFvqaQRaj9bXjwnV1YZkTBWtqQhS/X3REYjrWdRYDsjaiZ63VuI/3n91IS3fsZlkhqUbLUoTAUxMVm8T0ZcITNiZgllittbCZtQRZlNQZdsCN76y5ukU6t69Wr9vlZpuHkcRbiAS7gGD26gAXfQgjYwkPAMr/DmaOfFeXc+Vq0FJ585hz9wPn8AzSSQ9Q==</latexit> Clustering <latexit sha1_base64=\"JgGHFC5oztwX6+XjDtZWQo9C1hA=\">AAAB7nicbVA9TwJBEJ3DL8Qv1NJmIzGxIncUaImxscREwAQuZG8ZYMPe7mV3z4Rc+BE2Fhpj6++x89+4wBUKvmSSl/dmMjMvSgQ31ve/vcLG5tb2TnG3tLd/cHhUPj5pG5Vqhi2mhNKPETUouMSW5VbgY6KRxpHATjS5nfudJ9SGK/lgpwmGMR1JPuSMWid1biQbK2365Ypf9Rcg6yTISQVyNPvlr95AsTRGaZmgxnQDP7FhRrXlTOCs1EsNJpRN6Ai7jkoaowmzxbkzcuGUARkq7UpaslB/T2Q0NmYaR64zpnZsVr25+J/XTe3wOsy4TFKLki0XDVNBrCLz38mAa2RWTB2hTHN3K2FjqimzLqGSCyFYfXmdtGvVoF6t39cqDT+PowhncA6XEMAVNOAOmtACBhN4hld48xLvxXv3PpatBS+fOYU/8D5/AFOaj4U=</latexit> Anchors <latexit sha1_base64=\"KzBZ8R84UC9mpPFQBWeRHFxcqjw=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKVI8FLx4rmLbQhrLZbNq1m92wuxFK6H/w4kERr/4fb/4bt20O2vpg4PHeDDPzwpQzbVz32yltbG5t75R3K3v7B4dH1eOTjpaZItQnkkvVC7GmnAnqG2Y47aWK4iTktBtObud+94kqzaR4MNOUBgkeCRYzgo2VOn4aYUOH1ZpbdxdA68QrSA0KtIfVr0EkSZZQYQjHWvc9NzVBjpVhhNNZZZBpmmIywSPat1TghOogX1w7QxdWiVAslS1h0EL9PZHjROtpEtrOBJuxXvXm4n9ePzPxTZAzkWaGCrJcFGccGYnmr6OIKUoMn1qCiWL2VkTGWGFibEAVG4K3+vI66TTqXrPevG/UWldFHGU4g3O4BA+uoQV30AYfCDzCM7zCmyOdF+fd+Vi2lpxi5hT+wPn8AYuwjxQ=</latexit> Update <latexit sha1_base64=\"y2NH6tDs2GygUDqZYglGwvR4SpA=\">AAAB+nicbVBNSwMxEJ2tX7V+bfXoJVgEQSi7PVSPFS8eK9oPaEvJptk2NMkuSVYpa3+KFw+KePWXePPfmLZ70NYHA4/3ZpiZF8ScaeN5305ubX1jcyu/XdjZ3ds/cIuHTR0litAGiXik2gHWlDNJG4YZTtuxolgEnLaC8fXMbz1QpVkk780kpj2Bh5KFjGBjpb5bvMMi5lSjc3QlyShSuu+WvLI3B1olfkZKkKHed7+6g4gkgkpDONa643ux6aVYGUY4nRa6iaYxJmM8pB1LJRZU99L56VN0apUBCiNlSxo0V39PpFhoPRGB7RTYjPSyNxP/8zqJCS97KZNxYqgki0VhwpGJ0CwHNGCKEsMnlmCimL0VkRFWmBibVsGG4C+/vEqalbJfLVdvK6Wal8WRh2M4gTPw4QJqcAN1aACBR3iGV3hznpwX5935WLTmnGzmCP7A+fwBUnKTWg==</latexit> Samples + Anchors <latexit sha1_base64=\"u0BDOcH87PXd3DsT+o414+7cHnI=\">AAAB7XicbZC7SgNBFIbPxluMt6ilIINBsAq7FjGdARvLBMwFkhBmZ2eTMbMzy8ysEJaU9jYWitj6Cql8CDufwZdwcik0+sPAx/+fw5xz/JgzbVz308msrK6tb2Q3c1vbO7t7+f2DhpaJIrROJJeq5WNNORO0bpjhtBUriiOf06Y/vJrmzTuqNJPixoxi2o1wX7CQEWys1eiQQBrdyxfcojsT+gveAgqX75Pa1/3xpNrLf3QCSZKICkM41rrtubHpplgZRjgd5zqJpjEmQ9ynbYsCR1R309m0Y3RqnQCFUtknDJq5PztSHGk9inxbGWEz0MvZ1PwvaycmLHdTJuLEUEHmH4UJR0ai6eooYIoSw0cWMFHMzorIACtMjD1Qzh7BW175LzTOi16pWKq5hUoZ5srCEZzAGXhwARW4hirUgcAtPMATPDvSeXRenNd5acZZ9BzCLzlv33Yvk3g=</latexit> ··· <latexit sha1_base64=\"+7L/8ObZcl+JIZaSFhVO3t+lUUE=\">AAAB7XicbVDLSgNBEOyNrxhf8XHzMhiEeAm7ItFjQA8eI5gHJCHMTmaT0dnZZaZXCEv+wYsHRbz6P978GyebHDSxoKGo6qa7y4+lMOi6305uZXVtfSO/Wdja3tndK+4fNE2UaMYbLJKRbvvUcCkUb6BAydux5jT0JW/5j9dTv/XEtRGRusdxzHshHSoRCEbRSs2bvizjWb9YcituBrJMvDkp1Y6CDPV+8as7iFgScoVMUmM6nhtjL6UaBZN8UugmhseUPdIh71iqaMhNL82unZBTqwxIEGlbCkmm/p5IaWjMOPRtZ0hxZBa9qfif10kwuOqlQsUJcsVmi4JEEozI9HUyEJozlGNLKNPC3krYiGrK0AZUsCF4iy8vk+Z5xatWqnc2jQuYIQ/HcAJl8OASanALdWgAgwd4hld4cyLnxXl3PmatOWc+cwh/4Hz+AFjYkTs=</latexit> D l ( t ) <latexit sha1_base64=\"9C0bB8PYImk9DX0HLfGvGd44PFA=\">AAAB7XicbVDLSgNBEOyNrxhf8XHzMhiEeAm7ItFjQA8eI5gHJCHMTmaT0dnZZaZXCEv+wYsHRbz6P978GyebHDSxoKGo6qa7y4+lMOi6305uZXVtfSO/Wdja3tndK+4fNE2UaMYbLJKRbvvUcCkUb6BAydux5jT0JW/5j9dTv/XEtRGRusdxzHshHSoRCEbRSs2b/qiMZ/1iya24Gcgy8eakVDsKMtT7xa/uIGJJyBUySY3peG6MvZRqFEzySaGbGB5T9kiHvGOpoiE3vTS7dkJOrTIgQaRtKSSZ+nsipaEx49C3nSHFkVn0puJ/XifB4KqXChUnyBWbLQoSSTAi09fJQGjOUI4toUwLeythI6opQxtQwYbgLb68TJrnFa9aqd7ZNC5ghjwcwwmUwYNLqMEt1KEBDB7gGV7hzYmcF+fd+Zi15pz5zCH8gfP5A1K8kTc=</latexit> D h ( t ) <latexit sha1_base64=\"eNrtnhPGeU8n4BRDMStm5cjQ4ts=\">AAAB73icbVBNS8NAEJ34WetX1aOXxSJ4KkmR6rHQi8cK9gPaUDbbTbt0s4m7E6GE/gkvHhTx6t/x5r9x2+agrQ8GHu/NMDMvSKQw6Lrfzsbm1vbObmGvuH9weHRcOjltmzjVjLdYLGPdDajhUijeQoGSdxPNaRRI3gkmjbnfeeLaiFg94DThfkRHSoSCUbRS1zNIGlTKQansVtwFyDrxclKGHM1B6as/jFkacYVMUmN6npugn1GNgkk+K/ZTwxPKJnTEe5YqGnHjZ4t7Z+TSKkMSxtqWQrJQf09kNDJmGgW2M6I4NqveXPzP66UY3vqZUEmKXLHlojCVBGMyf54MheYM5dQSyrSwtxI2ppoytBEVbQje6svrpF2teLVK7b5arl/ncRTgHC7gCjy4gTrcQRNawEDCM7zCm/PovDjvzseydcPJZ87gD5zPH1Naj3k=</latexit> 1st Call <latexit sha1_base64=\"mxsL+XuWb2hqFND+pzTctrB1rcY=\">AAAB73icbVBNS8NAEJ34WetX1aOXxSJ4KkmR6rHQi8cK9gPaUDababt0s4m7G6GE/gkvHhTx6t/x5r9x2+agrQ8GHu/NMDMvSATXxnW/nY3Nre2d3cJecf/g8Oi4dHLa1nGqGLZYLGLVDahGwSW2DDcCu4lCGgUCO8GkMfc7T6g0j+WDmSboR3Qk+ZAzaqzUrcqQNKgQg1LZrbgLkHXi5aQMOZqD0lc/jFkaoTRMUK17npsYP6PKcCZwVuynGhPKJnSEPUsljVD72eLeGbm0SkiGsbIlDVmovycyGmk9jQLbGVEz1qveXPzP66VmeOtnXCapQcmWi4apICYm8+dJyBUyI6aWUKa4vZWwMVWUGRtR0Ybgrb68TtrViler1O6r5fp1HkcBzuECrsCDG6jDHTShBQwEPMMrvDmPzovz7nwsWzecfOYM/sD5/AE0o49l</latexit> 2nd Call <latexit sha1_base64=\"oSA1OFmXXL9y3PJtqoVxTIG9mto=\">AAAB8HicbVA9TwJBEJ3DL8Qv1NJmIzGxIncUaElCY2UwkQ8DF7K3zMGGvb3L7p6REH6FjYXG2Ppz7Pw3LnCFgi+Z5OW9mczMCxLBtXHdbye3sbm1vZPfLeztHxweFY9PWjpOFcMmi0WsOgHVKLjEpuFGYCdRSKNAYDsY1+d++xGV5rG8N5ME/YgOJQ85o8ZKD7f4ZEidCtEvltyyuwBZJ15GSpCh0S9+9QYxSyOUhgmqdddzE+NPqTKcCZwVeqnGhLIxHWLXUkkj1P50cfCMXFhlQMJY2ZKGLNTfE1MaaT2JAtsZUTPSq95c/M/rpia89qdcJqlByZaLwlQQE5P592TAFTIjJpZQpri9lbARVZQZm1HBhuCtvrxOWpWyVy1X7yqlWiWLIw9ncA6X4MEV1OAGGtAEBhE8wyu8Ocp5cd6dj2VrzslmTuEPnM8fSFeQCA==</latexit> Next Call Figure 2: Overview of the SimATTA framework. and sample ratio vectors when Dϕ,S(t) is not included, i.e., w′ and λ′ s.t. w′ 0 = λ′ 0 = 0 . If ˆdH∆H(DS, Dϕ,S(t)) < ˆdH∆H(DS, St i=1 Ute(i)), then for any λ ̸= λ′, there exists w s.t. EBS(w, λ, N, t) < EBS(w′, λ′, N, t). (8) Corollary 4 validates that the selected low-entropy samples can mitigate the CF problem under the assumption that these samples are source-like, which is also empirically validated in Fig. 1. Note that our strategy employs entropy minimization in a selective manner, aiming to solve CF rather than the main adaptation issue. While many FTTA works use entropy minimization to adapt across domains without guarantees, our use is more theoretically-sound. 4 A N ATTA ALGORITHM Building on our theoretical findings, we introduce a simple yet effective ATTA method, known as SimATTA, that innovatively integrates incremental clustering and selective entropy minimization techniques, as illustrated in Fig. 2. We start with an overview of our methodology, including the learning framework and the comprehensive sample selection strategies. We then proceed to discuss the details of the incremental clustering technique designed for real-time sample selections. 4.1 A LGORITHM OVERVIEW Let (x, y) be a labeled sample and f(·; θ) be our neural network, where ˆy = f(x; θ) and θ represents the parameters. We have a model pre-trained on source domains with the pre-trained parameters ϕ. We initialize model parameters as θ(0) = ϕ and aim to adapt the model f(·; θ) in real-time. During the test phase, the model continuously predicts labels for streaming-in test data and concurrently gets fine-tuned. We perform sample selection to enable active learning. As discussed in Sec. 3.2, we empirically consider informative high-entropy samples for addressing distribution shifts and source-like low-entropy samples to mitigate CF. As shown in Alg. 1, at each time step t, we first partition unlabeled test samples Ute(t) into high entropy and low entropy datasets, Uh(t) and Ul(t), using an entropy threshold. The source-pretrained model f(·; ϕ) is frozen to predict pseudo labels for low entropy data. We obtain labeled low-entropy data Dl(t) by labeling Ul(t) with f(·; ϕ) and combining it with Dl(t − 1). In contrast, the selection of high-entropy samples for active labeling is less straightforward. Since the complete test dataset is inaccessible for analyzing the target domain distribution, real-time sample selection is required. We design an incremental clustering sample selection technique to reduce sample redundancy and increase distribution coverage, detailed in Sec. 4.2. The incremental clustering algorithm outputs the labeled test samples Dh(t), also referred to as anchors, given Dh(t −1) and Uh(t). After sample selection, the model undergoes test-time training using the labeled test anchors Dh(t) and pseudo-labeled source-like anchors Dl(t). Following the analyses in Sec. 3.1, the training weights and sample numbers should satisfy w(t) ≈ λ(t) for Dh(t) and Dl(t) for optimal results. The analyses and results in Sec. 3.2 further indicate that balancing the source and target ratio is the key to mitigating CF. However, when source-like samples significantly outnumber test samples, the optimal w(t) for test domains can deviate from λ(t) according to Eq. (4). 4.2 I NCREMENTAL CLUSTERING We propose incremental clustering, a novel continual clustering technique designed to select informa- tive samples in unsupervised settings under the ATTA framework. The primary goal of this strategy is to store representative samples for distributions seen so far. Intuitively, we apply clusters to cover all seen distributions while adding new clusters to cover newly seen distributions. During this process with new clusters added, old clusters may be merged due to the limit of the cluster budget. Since 6Published as a conference paper at ICLR 2024 Algorithm 1 SIMATTA: A SIMPLE ATTA ALGORITHM Require: A fixed source pre-trained model f(·; ϕ) and a real-time adapting model f(·; θ(t)) with θ(0) = ϕ. Streaming test data Ute(t) at time step t. Entropy of predictions H(ˆy) = −P c p(ˆyc) logp(ˆyc). Low entropy and high entropy thresholds el and eh. The number of cluster centroid budget NC (t) at time step t. Centroid increase number k. Learning step size η. 1: for t = 1, . . . , Tdo 2: Model inference on Ute(t) using f(·; θ(t − 1)). 3: Dl(t) ← Dl(t − 1) ∪ {(x, f(x; ϕ))|x ∈ Ute(t), H(f(x; ϕ)) < el} 4: Uh(t) ← {x|x ∈ Ute(t), H(f(x; θ)) > eh} 5: Dh(t) ← Dh(t − 1) ∪ {(x, y)|∀x ∈ IC(Dh(t − 1), Uh(t), NC(t)), y= Oracle(x)} 6: λ(t) ← |Dl(t)|/(|Dl(t)| + |Dh(t)|), |Dh(t)|/(|Dl(t)| + |Dh(t)|) 7: w(t) ← GetW(λ(t)) ▷ Generally, GetW(λ(t)) = λ(t) is a fair choice. 8: θ(t) ← θ(t − 1) 9: for (xl, yl) in Dl and (xh, yh) in Dh do 10: θ(t) ← θ(t) − ηw0∇ℓCE (f(xl; θ(t)), yl) − η(1 − w0)∇ℓCE (f(xh; θ(t)), yh) 11: end for 12: NC (t + 1) ← UpdateCentroidNum(NC (t)) ▷ Naive choice: NC (t + 1) ← NC (t) + k. 13: end for clusters cannot be stored efficiently, we store the representative samples of clusters, named anchors, instead. In this work, we adopt weighted K-means (Krishna and Murty, 1999) as our base clustering method due to its popularity and suitability for new setting explorations. When we apply clustering with new samples, a previously selected anchor should not weigh the same as new samples since the anchor is a representation of a cluster,i.e., a representation of many samples. Instead, the anchor should be considered as a barycenter with a weight of the sum of its cluster’s sample weights. For a newly added cluster, its new anchor has the weight of the whole cluster. For clusters containing multiple old anchors, i.e., old clusters, the increased weights are distributed equally among these anchors. These increased weights are contributed by new samples that are close to these old anchors. Intuitively, this process of clustering is analogous to the process of planet formation. Where there are no planets, new planets (anchors) will be formed by the aggregation of the surrounding material (samples). Where there are planets, the matter is absorbed by the surrounding planets. This example is only for better understanding without specific technical meanings. Specifically, we provide the detailed Alg. 2 for incremental clustering. In each iteration, we apply weighted K-Means for previously selected anchors Danc and the new streaming-in unlabeled data Unew. We first extract all sample features using the model from the previous step f(·; θ(t − 1)), and then cluster these weighted features. The initial weights of the new unlabeled samples are 1, while anchors inherit weights from previous iterations. After clustering, clusters including old anchors are old clusters, while clusters only containing new samples are newly formed ones. For each new cluster, we select the centroid-closest sample as the new anchor to store. As shown in line 10 of Alg. 2, for both old and new clusters, we distribute the sample weights in this cluster as its anchors’ weights. With incremental clustering, although we can control the number of clusters in each iteration, we cannot control the number of new clusters/new anchors. This indirect control makes the increase of new anchors adaptive to the change of distributions, but it also leads to indirect budget control. Therefore, in experimental studies, we set the budget limit, but the actual anchor budget will not reach this limit. The overall extra storage requirement is O(B) since the number of saved unlabeled samples is proportional to the number of saved labeled samples (anchors). 5 E XPERIMENTAL STUDIES In this study, we aim to validate the effectiveness of our proposed method, as well as explore the various facets of the ATTA setting. Specifically, we design experiments around the following research questions: RQ1: Can TTA methods address domain distribution shifts? RQ2: Is ATTA as efficient as TTA? RQ3: How do the components of SimATTA perform? RQ4: Can ATTA perform on par with stronger Active Domain Adaptation (ADA) methods? We compare ATTA with three settings, TTA (Tab. 2), enhanced TTA (Tab. 3 and 5), and ADA (Tab. 4). Datasets. To assess the OOD performance of the TTA methods, we benchmark them using datasets from DomainBed (Gulrajani and Lopez-Paz, 2020) and Hendrycks and Dietterich (2019a). We employ PACS (Li et al., 2017), VLCS (Fang et al., 2013), Office-Home (Venkateswara et al., 2017), and Tiny-ImageNet-C datasets for our evaluations. For each dataset, we designate one domain as 7Published as a conference paper at ICLR 2024 Table 2: TTA comparisons on PACS and VLCS.This table includes the two data stream mentioned in the dataset setup and reports performances in accuracy. Results that outperform all TTA baselines are highlighted in bold font. N/A denotes the adaptations are not applied on the source domain. PACS Domain-wise data stream Post-adaptation Random data stream Post-adaptation P →A→ →C→ →S P A C S →1→ →2→ →3→ →4 P A C S BN w/o adapt 99.70 59.38 28.03 42.91 99.70 59.38 28.03 42.91 43.44 43.44 43.44 43.44 99.70 59.38 28.03 42.91BN w/ adapt 98.74 68.07 64.85 54.57 98.74 68.07 64.85 54.57 62.50 62.50 62.50 62.50 98.74 68.07 64.85 54.57 Tent (steps=1) N/A 67.29 64.59 44.67 97.60 66.85 64.08 42.58 56.35 54.09 51.83 48.58 97.19 63.53 60.75 41.56Tent (steps=10) N/A 67.38 57.85 20.23 62.63 34.52 40.57 13.59 47.36 31.01 22.84 20.33 50.78 23.68 20.95 19.62EATA N/A 67.04 64.72 50.27 98.62 66.50 62.46 48.18 57.31 56.06 58.17 59.78 98.62 69.63 65.70 54.26CoTTA N/A 65.48 62.12 53.17 98.62 65.48 63.10 53.78 56.06 54.33 57.16 57.42 98.62 65.97 62.97 54.62SAR (steps=1) N/A 66.75 63.82 49.58 98.32 66.94 62.93 45.74 56.78 56.35 56.68 56.70 98.44 68.16 64.38 52.53SAR (steps=10) N/A 69.38 68.26 49.02 96.47 62.16 56.19 54.62 53.51 51.15 51.78 45.60 94.13 56.64 56.02 36.37 SimATTA (B ≤300) N/A 76.86 70.90 75.39 98.80 84.47 82.25 81.52 69.47 76.49 82.45 82.22 98.98 84.91 83.92 86.00SimATTA (B ≤500) N/A 77.93 76.02 76.30 98.62 88.33 83.49 83.74 68.46 78.22 80.91 85.49 99.16 86.67 84.77 87.71 VLCS Domain-wise data stream Post-adaptation Random data stream Post-adaptation C →L→ →S→ →V C L S V →1→ →2→ →3→ →4 C L S V BN w/o adapt 100.00 33.55 41.10 49.05 100.00 33.55 41.10 49.05 41.23 41.23 41.23 41.23 100.00 33.55 41.10 49.05BN w/ adapt 85.16 37.31 33.27 52.16 85.16 37.31 33.27 52.16 40.91 40.91 40.91 40.91 85.16 37.31 33.27 52.16 Tent (steps=1) N/A 38.55 34.40 53.88 84.73 43.86 33.61 53.11 44.85 44.29 47.38 44.98 85.30 43.49 37.81 53.35Tent (steps=10) N/A 45.41 31.44 32.32 42.54 37.65 27.79 33.12 46.13 42.31 43.51 39.48 52.01 40.32 33.64 40.37EATA N/A 37.24 33.15 52.58 84.10 37.69 32.39 52.49 43.77 42.48 43.34 41.55 83.32 36.67 31.47 52.55CoTTA N/A 37.39 32.54 52.25 82.12 37.65 33.12 52.90 43.69 42.14 43.21 42.32 81.98 37.99 33.52 53.23SAR (steps=1) N/A 36.18 34.43 52.46 83.96 39.72 36.53 52.37 43.64 43.04 44.20 41.93 85.09 40.70 36.44 53.02SAR (steps=10) N/A 35.32 34.10 51.66 82.12 41.49 33.94 53.08 43.56 42.05 42.53 41.16 85.09 37.58 33.12 52.01 SimATTA (B ≤300) N/A 62.61 65.08 74.38 99.93 69.50 66.67 77.34 62.33 69.33 73.20 71.93 99.93 69.43 72.46 80.39SimATTA (B ≤500) N/A 63.52 68.01 76.13 99.51 70.56 73.10 78.35 62.29 70.45 73.50 72.02 99.43 70.29 72.55 80.18 the source domain and arrange the samples from the other domains to form the test data stream. For DomainBed datasets, we adopt two stream order strategies. The first order uses a domain-wise data stream, i.e., we finish streaming samples from one domain before starting streaming another domain. The second order is random, where we shuffle samples from all target domains and partition them into four splits 1, 2, 3, and 4, as shown in Tab. 2. More dataset details are provided in Appx. G.1. Baselines. For baseline models, we start with the common source-only models, which either utilize pre-calculated batch statistics (BN w/o adapt) or test batch statistics (BN w/ adapt). For comparison with other TTA methods, we consider four state-of-the-art TTA methods: Tent (Wang et al., 2021), EATA (Niu et al., 2022), CoTTA (Wang et al., 2022a), and SAR (Niu et al., 2023). The three of them except Tent provide extra design to avoid CF. To compare with ADA methods, we select algorithms that are partially comparable with our method, i.e., they should be efficient (e.g., uncertainty-based) without the requirements of additional networks. Therefore, we adopt random, entropy (Wang and Shang, 2014), k-means (Krishna and Murty, 1999), and CLUE (Prabhu et al., 2021) for comparisons. Settings. For TTA, we compare with general TTA baselines in streaming adaptation using the two aforementioned data streaming orders, domain-wise and random. We choose P in PACS and C in VLCS as source domains. For domain-wise data stream, we use order A → C → S for PACS and L → S → V for VLCS. We report the real-time adaptation accuracy results for each split of the data stream, as well as the accuracy on each domain after all adaptations through the data stream (under “post-adaptation” columns). Enhanced TTA is built on TTA with access to extra random sample labels. TTA baselines are further fine-tuned with these random samples. To further improve enhanced TTA, we use long-term label storage and larger unlabeled sample pools. To its extreme where the model can access the whole test set samples, the setting becomes similar to ADA, thus we also use ADA methods for comparisons. ADA baselines have access to all samples in the pre-collected target datasets but not source domain data, whereas our method can only access the streaming test data. 5.1 T HE FAILURE OF TEST-TIME ADAPTATION The failure of TTA methods on domain distribution shifts is one of the main motivations of the ATTA setting. As shown in Tab. 2, TTA methods cannot consistently outperform eventhe simplest baseline \"BN w/ adapt\" which uses test time batch statistics to make predictions, evidencing that current TTA methods cannot solve domain distribution shifts (RQ1). Additionally, Tent (step=10) exhibits significant CF issues, where \"step=10\" indicates 10 test-time training updates, i.e., 10 gradient backpropagation iterations. This failure of TTA methods necessitates the position of ATTA. In contrast, SimATTA, with a budget B less than 300, outperforms all TTA methods on both source and target domains by substantial margins. Moreover, compared to the source-only baselines, our method improves the target domain performances significantly with negligible source performance loss, showing that ATTA is a more practically effective setting for real-world distribution shifts. 5.2 E FFICIENCY & ENHANCED TTA SETTING COMPARISONS To validate the efficiency of ATTA and broaden the dataset choice, we conduct this study on Tiny- ImageNet-C which, though does not focus on domain shifts, is much larger than PACS and VLCS. we 8Published as a conference paper at ICLR 2024 Table 3: Comparisons with Enhanced TTA on Tiny-ImageNet-C (severity level 5). Tiny-ImageNet-C Time (sec)Noise Blur Weather Digital Gauss. Shot Impul. Defoc. Glass Motion Zoom Snow Frost Fog Contr. Elastic Pixel JPEG Avg. Tent (step=1) 68.83 9.32 11.97 8.86 10.43 7.00 12.20 14.34 13.58 15.46 13.55 3.99 13.31 17.79 18.61 12.17Tent (step=10) 426.90 0.86 0.63 0.52 0.52 0.55 0.54 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.54EATA 93.14 3.98 3.33 2.18 4.80 2.37 11.02 11.41 14.06 15.26 9.65 1.36 9.88 14.24 12.12 8.26CoTTA 538.78 5.63 7.12 6.31 8.05 5.74 9.68 10.55 11.75 12.00 11.15 4.17 5.35 7.82 8.90 8.16SAR (step=1) 113.76 8.90 3.11 1.67 1.55 1.47 1.35 1.19 1.03 1.04 0.93 0.83 1.00 0.74 0.77 1.83SAR (step=10) 774.11 2.67 3.26 2.38 1.64 1.85 2.49 3.16 3.81 2.72 3.12 0.81 3.47 4.04 1.76 2.66 SimATTA (step=10) 736.289.68 19.40 12.14 30.28 17.03 42.36 43.10 31.96 40.08 29.243.21 34.56 45.24 45.74 28.86 enhance the TTA setting by fine-tuning baselines on randomly selected labeled samples. Specifically, the classifier of ResNet18-BN is pre-adapted to the brightness corruption (source domain) before test-time adapting. SimATTA’s label budget is around 4,000, while all other TTA methods have budget 4,500 for randomly selected labeled samples. The data stream order is shown in Tab. 3. Time is measured across all corrupted images in the Noise and Blur noise types, and the values represent the average time cost for adapting 10,000 images. The results clearly evidence the efficiency of ATTA (RQ2), while substantially outperforming all enhanced TTA baselines. Simply accessing labeled samples cannot benefit TTA methods to match ATTA. With 10 training updates (step=10) for each batch, FTTA methods would suffer from severe CF problem. In contrast, ATTA covers a statistically significant distribution, achieving stronger performances with 10 training updates or even more steps till approximate convergences. In fact, longer training on Tent (step=10) leads to worse results (compared to step=1), which further motivates the design of the ATTA setting. The reason for higher absolute time cost in Tab. 3 is due to differences in training steps. In this experiment, SimATTA has a training step of 10, and similar time cost as SAR per step. Note that if the enhanced TTA setting is further improved to maintain distributions with a balanced CF mitigation strategy and an incremental clustering design, the design approaches ATTA. Specifically, we compare SimATTA with its variants as the ablation study (RQ3) in Appx. I.2. 5.3 C OMPARISONS TO A STRONGER SETTING : ACTIVE DOMAIN ADAPTATION Table 4: Comparisons to ADA baselines. Source domains are denoted as \"(S)\". Results are average accuracies (with standard deviations). PACS P (S) A C S Random (B= 300) 96.21 (0.80) 81.19 (0.48) 80.75 (1.27) 84.34 (0.18)Entropy (B= 300) 96.31 (0.64)88.00 (1.46)82.48 (1.71) 80.55 (1.01)Kmeans (B= 300) 93.71 (1.50) 79.31 (4.01) 79.64 (1.44) 83.92 (0.65)CLUE (B= 300) 96.69 (0.17)83.97 (0.57)84.77 (0.88) 86.91 (0.26) SimATTA (B ≤300) 98.89 (0.09)84.69 (0.22)83.09 (0.83)83.76 (2.24) VLCS C (S) L S V Random (B= 300) 96.21 (1.65) 66.67 (1.70) 70.72 (0.30) 72.14 (1.71)Entropy (B= 300) 97.74 (1.56) 69.29 (2.26)69.25 (4.77) 75.26 (3.07)Kmeans (B= 300) 98.61 (0.27)67.57 (1.64)70.77 (0.01)74.49 (0.97)CLUE (B= 300) 85.70 (10.09) 65.29 (1.49) 69.42 (2.64) 69.09 (6.05) SimATTA (B ≤300) 99.93 (0.00) 69.47 (0.03)69.57 (2.90)78.87 (1.53) In addtion to the above comparisons with (en- hanced) TTA, which necessitate the requirement of extra information in the ATTA setting, we com- pare ATTA with a stronger setting Active Domain Adaptation (ADA) to demonstrate another supe- riority of ATTA, i.e., weaker requirements for comparable performances (RQ4). ADA baselines are able to choose the global best active samples, while ATTA has to choose samples from a small sample buffer (e.g., a size of 100) and discard the rest. Tab. 4 presents the post-adaptation model per- formance results. All ADA results are averaged from 3 random runs, while ATTA results are the post-adaptation performances averaged from the two data stream orders. As can be observed, despite the lack of a pre-collected target dataset, SimATTA produces better or competitive results against ADA methods. Moreover, without source data access, SimATTA’s design for CF allows it to maintain superior source domain performances over ADA methods. Further experimental studies including the Office-Home dataset are provided in Appx. I. In conclusion, the significant improvement compared to weaker settings (TTA, enhanced TTA) and the comparable performance with the stronger setting, ADA, rendering ATTA a setting that is as efficient as TTA and as effective as ADA. This implies its potential is worthy of future explorations. 6 C ONCLUSION AND DISCUSSION There’s no denying that OOD generalization can be extremely challenging without certain information, often relying on various assumptions easily compromised by different circumstances. Thus, it’s prudent to seek methods to achieve significant improvements with minimal cost, e.g., DG methods leveraging environment partitions and ATTA methods using budgeted annotations. As justified in our theoretical and experimental studies, ATTA stands as a robust approach to achieve real-time OOD generalization. Although SimATTA sets a strong baseline for ATTA, there’s considerable scope for further investigation within the ATTA setting. One potential direction involves developing alternatives to prevent CF in ATTA scenarios. While selective entropy minimization on low-entropy samples has prove to be empirically effective, it relies on the quality of the pre-trained model and training on incorrectly predicted low-entropy samples may reinforce the errors. It might not be cost-effective to expend annotation budgets on low-entropy samples, but correcting them could be a viable alternative solution. We anticipate that our work will spur numerous further explorations in this field. 9Published as a conference paper at ICLR 2024 ACKNOWLEDGMENTS This work was supported in part by National Science Foundation grant IIS-2006861 and National Institutes of Health grant U01AG070112. REFERENCES Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, and Mario Marchand. Domain- adversarial neural networks. arXiv preprint arXiv:1412.4446, 2014. Lucas Baier, Tim Schlör, Jakob Schöffer, and Niklas Kühl. Detecting concept drift with neural network model uncertainty. In Hawaii International Conference on System Sciences, 2021. URL https://api.semanticscholar.org/CorpusID:235731947. Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine learning, 79:151–175, 2010. Davide Cacciarelli and Murat Kulahci. A survey on online active learning, 2023. Cheng Chen, Quande Liu, Yueming Jin, Qi Dou, and Pheng-Ann Heng. Source-free domain adaptive fundus image segmentation with denoised pseudo-labeling. In Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part V 24, pages 225–235. Springer, 2021. Li Chen, Tutian Tang, Zhitian Cai, Yang Li, Penghao Wu, Hongyang Li, Jianping Shi, Junchi Yan, and Yu Qiao. Level 2 autonomous driving on a single device: Diving into the devils of openpilot. arXiv preprint arXiv:2206.08176, 2022a. Weijie Chen, Luojun Lin, Shicai Yang, Di Xie, Shiliang Pu, and Yueting Zhuang. Self-supervised noisy label learning for source-free unsupervised domain adaptation. In 2022 IEEE/RSJ In- ternational Conference on Intelligent Robots and Systems (IROS) , pages 10185–10192. IEEE, 2022b. Yining Chen, Colin Wei, Ananya Kumar, and Tengyu Ma. Self-training avoids using spurious features under domain shift. Advances in Neural Information Processing Systems, 33:21061–21071, 2020. David A Cohn, Zoubin Ghahramani, and Michael I Jordan. Active learning with statistical models. Journal of artificial intelligence research, 4:129–145, 1996. Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Aleš Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE transactions on pattern analysis and machine intelligence, 44(7):3366–3385, 2021. Yuhe Ding, Lijun Sheng, Jian Liang, Aihua Zheng, and Ran He. Proxymix: Proxy-based mixup training with label refinery for source-free domain adaptation. arXiv preprint arXiv:2205.14566, 2022. Cian Eastwood, Ian Mason, Christopher KI Williams, and Bernhard Schölkopf. Source-free adaptation to measurement shift via bottom-up feature restoration. arXiv preprint arXiv:2107.05446, 2021. Jiahao Fan, Hangyu Zhu, Xinyu Jiang, Long Meng, Chen Chen, Cong Fu, Huan Yu, Chenyun Dai, and Wei Chen. Unsupervised domain adaptation by statistics alignment for deep sleep staging networks. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 30:205–216, 2022. Chen Fang, Ye Xu, and Daniel N Rockmore. Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. In Proceedings of the IEEE International Conference on Computer Vision, pages 1657–1664, 2013. Yuqi Fang, Pew-Thian Yap, Weili Lin, Hongtu Zhu, and Mingxia Liu. Source-free unsupervised domain adaptation: A survey. arXiv preprint arXiv:2301.00265, 2022. Francois Fleuret et al. Uncertainty reduction for model adaptation in semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9613–9623, 2021. 10Published as a conference paper at ICLR 2024 Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In International conference on machine learning, pages 1180–1189. PMLR, 2015. Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The journal of machine learning research, 17(1):2096–2030, 2016. Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi, Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph Triebel, Peter Jung, Ribana Roscher, et al. A survey of uncertainty in deep neural networks. arXiv preprint arXiv:2107.03342, 2021. Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. Advances in neural information processing systems, 17, 2004. Shurui Gui, Chaoyue Wang, Qihua Chen, and Dacheng Tao. Featureflow: Robust video interpolation via structure-to-texture generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14004–14013, 2020. Shurui Gui, Xiner Li, Limei Wang, and Shuiwang Ji. GOOD: A graph out-of-distribution benchmark. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. URL https://openreview.net/forum?id=8hHg-zs_p-h. Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. arXiv preprint arXiv:2007.01434, 2020. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. March 2019a. doi: 10.48550/ARXIV .1903.12261. Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019b. Steven CH Hoi, Rong Jin, Jianke Zhu, and Michael R Lyu. Semisupervised svm batch mode active learning with applications to image retrieval. ACM Transactions on Information Systems (TOIS), 27(3):1–29, 2009. Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, et al. Planning-oriented autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17853–17862, 2023. Jiaxing Huang, Dayan Guan, Aoran Xiao, and Shijian Lu. Model adaptation: Historical contrastive learning for unsupervised domain adaptation without source data. Advances in Neural Information Processing Systems, 34:3635–3649, 2021. Masato Ishii and Masashi Sugiyama. Source-free domain adaptation via distributional alignment by matching batch normalization statistics. arXiv preprint arXiv:2101.10842, 2021. Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for model-agnostic domain generalization. Advances in Neural Information Processing Systems, 34:2427–2440, 2021. Suyog Dutt Jain and Kristen Grauman. Active image segmentation propagation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2864–2873, 2016. Guoliang Kang, Lu Jiang, Yi Yang, and Alexander G Hauptmann. Contrastive adaptation network for unsupervised domain adaptation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4893–4902, 2019. Ashish Kapoor, Kristen Grauman, Raquel Urtasun, and Trevor Darrell. Active learning with gaussian processes for object categorization. In 2007 IEEE 11th international conference on computer vision, pages 1–8. IEEE, 2007. Neerav Karani, Ertunc Erdil, Krishna Chaitanya, and Ender Konukoglu. Test-time adaptable neural networks for robust medical image segmentation. Medical Image Analysis, 68:101907, 2021. 11Published as a conference paper at ICLR 2024 Ronald Kemker, Marc McClure, Angelina Abitino, Tyler Hayes, and Christopher Kanan. Measuring catastrophic forgetting in neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. Daniel Kifer, Shai Ben-David, and Johannes Gehrke. Detecting change in data streams. In VLDB, volume 4, pages 180–191. Toronto, Canada, 2004. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114 (13):3521–3526, 2017. Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Bal- subramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning, pages 5637–5664. PMLR, 2021. Divya Kothandaraman, Sumit Shekhar, Abhilasha Sancheti, Manoj Ghuhan, Tripti Shukla, and Dinesh Manocha. Salad: Source-free active label-agnostic domain adaptation for classification, segmentation and detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 382–391, 2023. K Krishna and M Narasimha Murty. Genetic k-means algorithm. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 29(3):433–439, 1999. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu- tional neural networks. Communications of the ACM, 60(6):84–90, 2017. David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrap- olation (REx). In International Conference on Machine Learning , pages 5815–5826. PMLR, 2021. Vinod K Kurmi, Venkatesh K Subramanian, and Vinay P Namboodiri. Domain impression: A source data free domain adaptation method. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 615–625, 2021. David D Lewis and Jason Catlett. Heterogeneous uncertainty sampling for supervised learning. In Machine learning proceedings 1994, pages 148–156. Elsevier, 1994. Aodong Li, Alex Boyd, Padhraic Smyth, and Stephan Mandt. Detecting and adapting to irregular distribution shifts in bayesian online learning. Advances in neural information processing systems, 34:6816–6828, 2021a. Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain generalization. In Proceedings of the IEEE international conference on computer vision, pages 5542–5550, 2017. Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, and Si Wu. Model adaptation: Unsupervised domain adaptation without source data. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9641–9650, 2020. Xianfeng Li, Weijie Chen, Di Xie, Shicai Yang, Peng Yuan, Shiliang Pu, and Yueting Zhuang. A free lunch for unsupervised domain adaptive object detection without source data. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 8474–8481, 2021b. Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935–2947, 2017. Jian Liang, Dapeng Hu, Ran He, and Jiashi Feng. Distill and fine-tune: Effective adaptation from a black-box source model. arXiv preprint arXiv:2104.01539, 1(3), 2021. Jian Liang, Dapeng Hu, Jiashi Feng, and Ran He. Dine: Domain adaptation from single and multiple black-box predictors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8003–8013, 2022. 12Published as a conference paper at ICLR 2024 Yong Lin, Shengyu Zhu, Lu Tan, and Peng Cui. Zin: When and how to learn invariance without environment partition? Advances in Neural Information Processing Systems, 35:24529–24542, 2022. Xiaofeng Liu, Fangxu Xing, Chao Yang, Georges El Fakhri, and Jonghye Woo. Adapting off-the- shelf source segmenter for target medical image segmentation. In Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part II 24, pages 549–559. Springer, 2021a. Xinyu Liu and Yixuan Yuan. A source-free domain adaptive polyp detection framework with style diversification flow. IEEE Transactions on Medical Imaging, 41(7):1897–1908, 2022. Yuang Liu, Wei Zhang, Jun Wang, and Jianyong Wang. Data-free knowledge transfer: A survey. arXiv preprint arXiv:2112.15278, 2021b. Yuejiang Liu, Parth Kothari, Bastien Van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? Advances in Neural Information Processing Systems, 34:21808–21820, 2021c. Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with deep adaptation networks. In International conference on machine learning, pages 97–105. PMLR, 2015. David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. Advances in neural information processing systems, 30, 2017. Chaochao Lu, Yuhuai Wu, José Miguel Hernández-Lobato, and Bernhard Schölkopf. Invariant causal representation learning for out-of-distribution generalization. In International Conference on Learning Representations, 2021. Xinhong Ma, Junyu Gao, and Changsheng Xu. Active universal domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8968–8977, 2021. Haitao Mao, Lun Du, Yujia Zheng, Qiang Fu, Zelin Li, Xu Chen, Shi Han, and Dongmei Zhang. Source free unsupervised graph domain adaptation. arXiv preprint arXiv:2112.00955, 2021. Christoforos Mavrogiannis, Francesca Baldini, Allan Wang, Dapeng Zhao, Pete Trautman, Aaron Steinfeld, and Jean Oh. Core challenges of social robot navigation: A survey. ACM Transactions on Human-Robot Interaction, 12(3):1–39, 2023. Zachary Nado, Shreyas Padhy, D Sculley, Alexander D’Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robustness under covariate shift. arXiv preprint arXiv:2006.10963, 2020. Munan Ning, Donghuan Lu, Dong Wei, Cheng Bian, Chenglang Yuan, Shuang Yu, Kai Ma, and Yefeng Zheng. Multi-anchor active domain adaptation for semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9112–9122, 2021. Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test-time model adaptation without forgetting. In International conference on machine learning, pages 16888–16905. PMLR, 2022. Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan. Towards stable test-time adaptation in dynamic wild world. InThe Eleventh International Con- ference on Learning Representations, 2023. URL https://openreview.net/forum?id=g2YraF75Tj. Sinno Jialin Pan, Ivor W Tsang, James T Kwok, and Qiang Yang. Domain adaptation via transfer component analysis. IEEE transactions on neural networks, 22(2):199–210, 2010. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. 13Published as a conference paper at ICLR 2024 Vishal M Patel, Raghuraman Gopalan, Ruonan Li, and Rama Chellappa. Visual domain adaptation: A survey of recent advances. IEEE signal processing magazine, 32(3):53–69, 2015. Judea Pearl. Causality. Cambridge university press, 2009. Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learning in python. the Journal of machine Learning research, 12:2825–2830, 2011. Jonas Peters, Peter Bühlmann, and Nicolai Meinshausen. Causal inference by using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 78(5):947–1012, 2016. Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference: foundations and learning algorithms. The MIT Press, 2017. Viraj Prabhu, Arjun Chandrasekaran, Kate Saenko, and Judy Hoffman. Active domain adaptation via clustering uncertainty-weighted embeddings. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8505–8514, 2021. Elan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. The risks of invariant risk minimization. arXiv preprint arXiv:2010.05761, 2020. Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731, 2019. Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normal- ization help optimization? Advances in neural information processing systems, 31, 2018. Akanksha Saran, Safoora Yousefi, Akshay Krishnamurthy, John Langford, and Jordan T. Ash. Streaming active learning with deep neural networks. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 30005–30021. PMLR, 23–29 Jul 2023. URL https://proceedings.mlr. press/v202/saran23a.html. Harald Schafer, Eder Santana, Andrew Haden, and Riccardo Biasini. A commute in data: The comma2k19 dataset, 2018. Tobias Scheffer, Christian Decomain, and Stefan Wrobel. Active hidden markov models for informa- tion extraction. In Advances in Intelligent Data Analysis: 4th International Conference, IDA 2001 Cascais, Portugal, September 13–15, 2001 Proceedings 4, pages 309–318. Springer, 2001. Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. Advances in Neural Information Processing Systems, 33:11539–11551, 2020. Burr Settles. Active learning literature survey. 2009. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. Jong-Chyi Su, Yi-Hsuan Tsai, Kihyuk Sohn, Buyu Liu, Subhransu Maji, and Manmohan Chandraker. Active adversarial domain adaptation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 739–748, 2020. Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In European conference on computer vision, pages 443–450. Springer, 2016. Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In International conference on machine learning, pages 9229–9248. PMLR, 2020. 14Published as a conference paper at ICLR 2024 Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker. Learning to adapt structured output space for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7472–7481, 2018. Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. In Proceedings of the IEEE international conference on computer vision, pages 4068–4076, 2015. Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7167–7176, 2017. Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5018–5027, 2017. Sudheendra Vijayanarasimhan and Ashish Kapoor. Visual recognition and detection under bounded computational resources. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 1006–1013. IEEE, 2010. Dan Wang and Yi Shang. A new active labeling method for deep learning. In 2014 International joint conference on neural networks (IJCNN), pages 112–119. IEEE, 2014. Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test- time adaptation by entropy minimization. InInternational Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=uXl3bZLkr3c. Mei Wang and Weihong Deng. Deep visual domain adaptation: A survey. Neurocomputing, 312: 135–153, 2018. Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7201–7211, 2022a. Rui Wang, Zuxuan Wu, Zejia Weng, Jingjing Chen, Guo-Jun Qi, and Yu-Gang Jiang. Cross-domain contrastive learning for unsupervised domain adaptation. IEEE Transactions on Multimedia , 2022b. Garrett Wilson and Diane J Cook. A survey of unsupervised deep domain adaptation. ACM Transactions on Intelligent Systems and Technology (TIST), 11(5):1–46, 2020. Binhui Xie, Longhui Yuan, Shuang Li, Chi Harold Liu, Xinjing Cheng, and Guoren Wang. Active learning for domain adaptation: An energy-based approach. InProceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 8708–8716, 2022. Zhao Xu, Kai Yu, V olker Tresp, Xiaowei Xu, and Jizhi Wang. Representative sampling for text classification using support vector machines. In Advances in Information Retrieval: 25th European Conference on IR Research, ECIR 2003, Pisa, Italy, April 14–16, 2003. Proceedings 25, pages 393–407. Springer, 2003. Baoyao Yang, Hao-Wei Yeh, Tatsuya Harada, and Pong C Yuen. Model-induced generalization error bound for information-theoretic representation learning in source-data-free unsupervised domain adaptation. IEEE Transactions on Image Processing, 31:419–432, 2021a. Guanglei Yang, Hao Tang, Zhun Zhong, Mingli Ding, Ling Shao, Nicu Sebe, and Elisa Ricci. Transformer-based source-free domain adaptation. arXiv preprint arXiv:2105.14138, 2021b. Jianfei Yang, Xiangyu Peng, Kai Wang, Zheng Zhu, Jiashi Feng, Lihua Xie, and Yang You. Divide to adapt: Mitigating confirmation bias for domain adaptation of black-box predictors. arXiv preprint arXiv:2205.14467, 2022. H Yao, Yuhong Guo, and Chunsheng Yang. Source-free unsupervised domain adaptation with surrogate data generation. In Proceedings of NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and Applications, 2021. 15Published as a conference paper at ICLR 2024 Hao-Wei Yeh, Baoyao Yang, Pong C Yuen, and Tatsuya Harada. Sofa: Source-data-free feature alignment for unsupervised domain adaptation. InProceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 474–483, 2021. Fuming You, Jingjing Li, and Zhou Zhao. Test-time batch statistics calibration for covariate shift. arXiv preprint arXiv:2110.04065, 2021. Hu Yu, Jie Huang, Yajing Liu, Qi Zhu, Man Zhou, and Feng Zhao. Source-free domain adaptation for real-world image dehazing. In Proceedings of the 30th ACM International Conference on Multimedia, pages 6645–6654, 2022. Haojian Zhang, Yabin Zhang, Kui Jia, and Lei Zhang. Unsupervised domain adaptation of black-box source models. arXiv preprint arXiv:2101.02839, 2021. Marvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and augmentation. Advances in Neural Information Processing Systems, 35:38629–38642, 2022a. Yifan Zhang, Xue Wang, Kexin Jin, Kun Yuan, Zhang Zhang, Liang Wang, Rong Jin, and Tieniu Tan. Adanpc: Exploring non-parametric classifier for test-time adaptation. In International Conference on Machine Learning, pages 41647–41676. PMLR, 2023. Yizhe Zhang, Shubhankar Borse, Hong Cai, and Fatih Porikli. Auxadapt: Stable and efficient test-time adaptation for temporally consistent video semantic segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2339–2348, 2022b. Bowen Zhao, Chen Chen, and Shu-Tao Xia. Delta: degradation-free fully test-time adaptation. arXiv preprint arXiv:2301.13018, 2023a. Hao Zhao, Yuejiang Liu, Alexandre Alahi, and Tao Lin. On pitfalls of test-time adaptation. In International Conference on Machine Learning (ICML), 2023b. Chunting Zhou, Xuezhe Ma, Paul Michel, and Graham Neubig. Examining and combating spurious features under distribution shift. In International Conference on Machine Learning, pages 12857– 12867. PMLR, 2021. 16Published as a conference paper at ICLR 2024 Active Test-Time Adaptation: Foundational Analyses and An Algorithm Supplementary Material A B ROADER IMPACTS The field of domain generalization primarily concentrates on enhancing a model’s generalization abilities by preparing it thoroughly before deployment. However, it is equally important for deep learning applications to have the capacity for real-time adaptation, as no amount of preparation can account for all possible scenarios. Consequently, domain generalization and test-time adaptation are complementary strategies: the former is more weighty and extensive, while the latter is more agile, lightweight and privacy-friendly. This work delves into the development of a real-time model adaptation strategy that can be applied to any pre-trained models, including large language models, to enhance their adaptive capabilities. Our research does not involve any human subjects or dataset releases, nor does it raise any ethical concerns. Since this work does not directly tie to specific applications, we do not foresee any immediate negative societal impacts. Nonetheless, we acknowledge that any technological advancement may carry potential risks, and we encourage the continued assessment of the broader impacts of real-time adaptation methodologies in various contexts. B FAQ & D ISCUSSIONS To facilitate the reviewing process, we summarize the answers to the questions that arose during the discussion of an earlier version of this paper. The major updates of this version are reorganized theoretical studies, incremental clustering details, experimental reorganization, and additional datasets and settings . We include more related field comparisons to distinguish different settings. We also cover the position of this paper in literature and the main claims of this paper. Finally, we will frankly acknowledge the limitations of this paper, explain and justify the scope of coverage, and provide possible future directions. Q1: What is the relationship between the proposed ATTA protocol and stream based active learning (Saran et al., 2023)? A: We would like to discuss the difference between our work and the referenced work. 1. Real-time Training Distinction: Saran et al. (2023) doesn’t operate in real-time capacity. This is evident from their experiments, where their model is trained only after completing a round. In contrast, our work involves training the model post each batch. This positions Saran et al. (2023)’s work as an intrinsic active learning technique, while our approach leans towards TTA methods. 2. Continual Training Nuance: Following the point above, Saran et al. (2023) stands out of the scope of continual training. As they mentioned ‘each time new data are acquired, the ResNet is reset to the ImageNet pre-trained weights before being updated‘, Saran et al. (2023) starts afresh with each iteration and is out of scope for CF discussions. Contrarily, our model is continuously trained on varying distributions, compelling us to address the CF issue while preserving advantages derived from various stored distributions. 3. Comparative Complexity: Given the aforementioned distinctions, it’s evident that our task presents a greater challenge compared to theirs. In addition, we have included comparisons with stronger active learning settings in Sec. 5.3. Q2: What are the insights from the theoretically foundational analysis? A: 1. It sets a well-defined formulation and grounded theoretical framework for the ATTA setting. 2. While entropy minimizations can cause CF, balancing the learning rate and number of high/low entropy samples is conversely the key solution to both distribution shifts and 17Published as a conference paper at ICLR 2024 CF by corresponding benefits. Though adding low-entropy data is intuitive, it is crucial in that this simple operation can make methods either too conservative or too aggressive without the correct balancing conditions. 3. The studies in Sec. 3.1 directly present a feasible and guaranteed solution for imple- menting ATTA to tackle shifts while avoiding CF. The aligned empirical validations of Sec. 3.2 also instruct the implementation of SimATTA. Q3: In test-time adaptation, one important issue is that the number of testing samples in a batch may be small, which means the sample size m will also be very small. May it affect the theorem and make them become very loose? A: We consider this issue jointly from theoretical and empirical validations. 1. It is true that the theoretical bounds can be loose given a small size of m unlabeled test samples. This situation of the error bound is mathematically ascribed to the quotient between the VC-dimension d of the hypothesis class and m. Under the VC-dimension theory, the ResNet18 model we adopt should have d ≫ m. However, practically we perform fine-tuning on pre-trained models instead of training from scratch, which significantly reduces the scale of parameter update. In this case, an assumption can be established that fine-tuning a model is roughly equivalent to learning a model with a relatively small d (Appx. H). This assumption is potentially underpinned by the empirical alignment of our validation experiments with the theoretical framework (Fig. 1). To this end, experiments indicate thatd and m are practically of similar scale for our settings. This prevents our theoretical bounds from being very loose and meaningless in reality. 2. Regarding cases that our assumption does not apply, this issue would appear inevitable, since it is rigorously inherent in the estimation error of our streaming and varying test distributions. The distribution of a test stream can be hardly monitored when only a limited batch is allowed, which we consider as a limitation of TTA settings. Moreover, this issue directly implies the necessity of using a buffer for unlabeled samples. A good practice is to maintain a relatively comparable sample buffer scale. Q4: What distribution shifts can ATTA solve? A: We would like to follow (but not limited to) the work (Zhao et al., 2023b) to discuss the distribution shifts ATTA can solve. 1. As elucidated in Sec. 3.1 and Sec. 5, ATTA can solve domain generalization shifts. Domain generalization shifts include complex shifts on the joint data distribution P(X, Y), given X as the covariates and Y as the label variable. Since P(X, Y) = P(X)P(Y |X), ATTA can handle covariate shift (P(X)), label shift (P(Y )), and conditional shift (P(Y |X)). The shifts on both covariate and conditional distributions can cover the shift on labels, but they (covariate + conditional shifts) are more complicated than pure label shifts, where only the marginal label distribution changes while the conditional distribution remains. Note that the conditional shifts are generally caused by spurious correlations, where the independent causal mechanism assumption (Pearl, 2009) holds or no concept drifts exist. 2. In our framework, the distribution support of X at different time steps can be different, but we don’t cover the situation where the support of Y changes, i.e., class-incremental problems. Q5: It is unclear how many samples are selected in each minibatch of testing samples. How the total budget is distributed across the whole testing data stream? A: The number of selected samples for each minibatch is decided jointly by the incremental clustering and the cluster centroid number NC (t). Intuitively, this sample selection is a dynamic process, with NC (t) restricting the budget and incremental clustering performing sample selection. For each batch, we increase applicable clustering centroids as a maximum limit, while the exact number of the selected samples is given by the incremental clustering by how many clusters are located in the scope of new distributions. e.g., if the incoming batch does not introduce new data distributions, then we select zero samples even with increased NC (t). In contrast, if the incoming batch contains data located in multiple new distributions, the incremental clustering tends to select more samples than the NC (t) limit, thus forcing to merging of multiple previous clusters into one new cluster. 18Published as a conference paper at ICLR 2024 The incremental clustering is detailed in Sec. 4.2, and NC (t) is naively increased by a constant hyper-parameter k. Therefore, the budget is adaptively distributed according to the data streaming distribution with budgets controlled by k, which is also the reason why we compare methods under a budget limit. Q6: Could compared methods have access to a few ground-truth labels as well? Making other algorithms be able to use the same amount of ground-truth labels randomly will produce fairer comparisons. A: 1. The enhanced TTA setting is exactly the setup we provide to produce fairer comparisons. See Tab. 3 and Tab. 5 for comparison results. 2. ATTA also compares to a stronger setting ADA which can access the whole test datasets multiple times. Table 5: The table demonstrates the comparisons on PACS where all enhanced TTA baselines have 300 budgets to randomly select labeled samples. The training steps of these labeled samples are the same as the original TTA method training steps. For accumulated sample selection, please refer to our ablation studies. Method Domain-wise data stream A VG Random data stream A VG P→ →A→ →C→ →S P A C S 1 2 3 4 P A C S Source onlyBN w/o adapt 99.70 59.38 28.03 42.91 99.70 59.38 28.03 42.91 43.44 43.44 43.44 43.44 99.70 59.38 28.03 42.91BN w/ adapt 98.74 68.07 64.85 54.57 98.74 68.07 64.85 54.57 62.50 62.50 62.50 62.50 98.74 68.07 64.85 54.57 TTA Tent (steps=1) N/A 70.07 68.43 64.42 97.72 74.17 72.61 68.92 61.20 62.36 66.59 67.32 98.14 74.37 70.26 66.07Tent (steps=10) N/A 76.27 63.78 49.35 59.46 38.62 48.46 55.03 56.20 53.22 52.55 55.55 58.32 47.56 60.75 58.00EATA N/A 69.53 66.94 61.42 98.56 69.38 66.60 64.83 60.34 59.81 64.38 65.02 98.68 73.78 68.30 59.74CoTTA N/A 66.55 63.14 59.91 90.12 61.67 66.68 67.68 57.26 57.36 63.46 65.64 92.22 71.53 70.44 62.41SAR (steps=1) N/A 66.60 63.78 50.34 98.38 67.87 64.04 49.48 57.21 56.06 56.78 57.14 98.38 68.80 64.59 53.02SAR (steps=10) N/A 69.09 66.55 49.07 96.23 62.50 59.34 46.53 49.76 52.74 48.51 49.06 95.39 57.13 54.61 38.76 Ours (B ≤300) N/A 76.86 70.90 75.39 98.80 84.47 82.25 81.52 69.47 76.49 82.45 82.22 98.98 84.91 83.92 86.00 Q7: What is the position of ATTA? A: Comparisons with different settings are challenging. In this work, the design of our experiments (Sec. 5) is to overcome this challenge by comparing both weaker settings and stronger settings. While the significant performance over weaker settings renders the necessity of extra information, the comparable performance with stronger settings provides the potential to relax restricted requirements. Intuitively, ATTA is the most cost-effective option in the consideration of both efficiency and effectiveness. We further provide the following ATTA summary: ATTA, which incorporates active learning in FTTA, is the light, real-time, source-free, widely applicable setting to achieve high generalization performances for test-time adaptation. 1. Necessity: From the causality perspective, new information is necessary (Lin et al., 2022; Pearl, 2009; Peters et al., 2017) to attain generalizable over distribution shifts which are insurmountable within the current TTA framework. 2. Effectiveness: Compared to FTTA methods, ATTA produces substantially better perfor- mances, on-par with the costly active domain adaptation (ADA) methods as shown in Table 3 in the paper. 3. Efficiency: Relative to ADA methods, ATTA possesses superior efficiency, similar to general FTTA methods, as shown in Tab. 3. 4. Applicability: ATTA is a model-agnostic setting. (1) Compared to domain generalization methods, ATTA do not require re-training and has the potential to apply to any pre-trained models. One interesting future direction is designing ATTA methods for large language models (LLMs), where re-trainings are extremely expensive and source data may be in- accessible. (2) Compared to FTTA methods, ATTA can protect model parameters from corrupting while learning new distributions by fine-tuning pre-trained models, rendering it more feasible and practical. In comparison with existing works, ATTA is motivated to mitigate the limitations of previous settings: 1. FTTA: Limited generalization performance. 19Published as a conference paper at ICLR 2024 2. TTT: Not source-free; limited generalization performance. 3. ADA & domain adaptation/generalization: Expensive re-trainings; limited applicability to pre-trained models. 4. Online active learning: It does not maintain and protect adaptation performances for multiple distributions in one model and does not consider the CF problem. Q8: What is the potential practical utility of ATTA? A: 1. Empirically, our method can generally finish a round of sample selection/training of 100 frames in 5s, i.e., 20 frames per sec, which is more than enough to handle multiple practical situations. Experiments on time complexity are provided in Tab. 3, where SimATTA has comparable time efficiency. 2. As a case analysis, the autopilot system (Hu et al., 2023; Chen et al., 2022a) presents an application scenario requiring high-speed low-latency adaptations, while these adaptations are largely underexplored. When entering an unknown environment, e.g., a construction section, a system of ATTA setting can require the driver to take over the wheel. During the period of manual operation when the driver is handling the wheel, steering signals are generated, and the in-car system quickly adaptations. The system doesn’t need to record 60 frames per second, since only the key steering operations and the corresponding dash cam frames are necessary, which can be handled by ATTA algorithms processing at 20 frames per sec. In this case, the human annotations are necessary and indirect. ATTA makes use of this information and adapts in the short term instead of collecting videos and having a long-round fine-tuning (Schafer et al., 2018). 3. In addition, many scenarios applicable for ATTA are less speed-demanding than the case above. One example is a personalized chatbot that subtly prompts and gathers user labels during user interaction. In a home decoration setting, applications can request that users scan a few crucial areas to ensure effective adaptation. Social robots (Mavrogiannis et al., 2023), e.g., vacuum robots, often require users to label critical obstacles they’ve encountered. 4. Compared with ADA, ATTA stands out as the tailored solution for the above scenarios. It does not require intensive retraining or server-dependent fine-tuning, offering both speed and computational efficiency. Meanwhile, akin to other TTA methods, ATTA also ensures user privacy. While it might marginally exceed the cost of standard TTA methods, the superior generalization ability makes it a compelling choice and justifies the additional expense. Q9: What can be covered by this paper? A: This paper endeavors to establish the foundational framework for a novel setting referred to as ATTA. We target (1) positioning the ATTA setting, (2) solving the two major and basic challenges of ATTA,i.e., the mitigation of distribution shifts and the avoidance of catastrophic forgetting (CF). We achieve the first goal by building the problem formulation and analyses, and further providing extensive qualitative and well-organized experimental comparisons with TTA, enhanced TTA, and ADA settings. These efforts position ATTA as the most cost-effective option between TTA and ADA, where ATTA inherits the efficiency of TTA and the effectiveness of ADA. With our theoretical analyses and the consistent algorithm design, we validate the success of our second goal through significant empirical performances. Q10: What are not covered by this paper? A: Constructing a new setting involves multifaceted complexities. Although there are various potential applications discussed above including scaling this setting up for large models and datasets, we cannot cover them in this single piece of work. There are three main reasons. First, the topics covered by a single paper are limited. Formally establishing ATTA setting and addressing its major challenges of ATTA takes precedence over exploring practical applications. Secondly, given the interrelations between ATTA and other settings, our experimental investigations are predominantly comparative, utilizing the most representative datasets from TTA and domain adaptation to showcase persuasive results. Thirdly, many practical applications necessitate task-specific configurations, rendering them unsuitable for establishing a universal learning setting. While the current focus is on laying down the foundational aspects of ATTA, the exploration of more specialized applications remains a prospective avenue for future work in the ATTA domain. 20Published as a conference paper at ICLR 2024 C R ELATED WORKS The development of deep learning witnesses various applications (He et al., 2016; Gui et al., 2020). To tackle OOD problem, various domain generalization works emerge (Krueger et al., 2021; Sagawa et al., 2019). C.1 U NSUPERVISED DOMAIN ADAPTATION Unsupervised Domain Adaptation (UDA) (Pan et al., 2010; Patel et al., 2015; Wilson and Cook, 2020; Wang and Deng, 2018) aims at mitigating distribution shifts between a source domain and a target domain, given labeled source domain samples and unlabeled target samples. UDA methods generally rely on feature alignment techniques to eliminate distribution shifts by aligning feature distributions between source and target domains. Typical feature alignment techniques include discrepancy minimization (Long et al., 2015; Sun and Saenko, 2016; Kang et al., 2019) and adversarial training (Ganin and Lempitsky, 2015; Tsai et al., 2018; Ajakan et al., 2014; Ganin et al., 2016; Tzeng et al., 2015; 2017). Nevertheless, alignments are normally not guaranteed to be correct, leading to the alignment distortion problem as noted by Ning et al. (2021). Source-free Unsupervised Domain Adaptation (SFUDA) (Fang et al., 2022; Liu et al., 2021b) algorithms aim to adapt a pre-trained model to unlabeled target domain samples without access to source samples. Based on whether the algorithm can access model parameters, these algorithms are categorized into white-box and black-box methods. White-box SFUDA typically considers data recovery (generation) and fine-tuning methods. The former focuses on recovering source- like data (Ding et al., 2022; Yao et al., 2021), e.g., training a Generative Adversarial Network (GAN) (Kurmi et al., 2021; Li et al., 2020), while the latter employs various techniques (Mao et al., 2021), such as knowledge distillation (Chen et al., 2022b; Liu and Yuan, 2022; Yang et al., 2021b; Yu et al., 2022), statistics-based domain alignment (Ishii and Sugiyama, 2021; Liu et al., 2021a; Fan et al., 2022; Eastwood et al., 2021), contrastive learning (Huang et al., 2021; Wang et al., 2022b), and uncertainty-based adaptation (Gawlikowski et al., 2021; Fleuret et al., 2021; Chen et al., 2021; Li et al., 2021b). Black-box SFUDA cannot access model parameters and often relies on self-supervised knowledge distillation (Liang et al., 2022; 2021), pseudo-label denoising (Zhang et al., 2021; Yang et al., 2022), or generative distribution alignment (Yeh et al., 2021; Yang et al., 2021a). C.2 T EST-TIME ADAPTATION Test-time Adaptation (TTA), especially Fully Test-time Adaptation (FTTA) algorithms (Wang et al., 2021; Iwasawa and Matsuo, 2021; Karani et al., 2021; Nado et al., 2020; Schneider et al., 2020; Wang et al., 2022a; Zhao et al., 2023a; Niu et al., 2022; Zhang et al., 2022a; Niu et al., 2023; You et al., 2021; Zhang et al., 2022b), can be considered as realistic and lightweight methods for domain adaptation. Built upon black-box SFUDA, FTTA algorithms eliminate the requirement of a pre-collected target dataset and the corresponding training phase. Instead, they can only access an unlabeled data stream and apply real-time adaptation and training. In addition to FTTA, Test-time Training (TTT) (Sun et al., 2020; Liu et al., 2021c) often relies on appending the original network with a self-supervised task. TTT methods require retraining on the source dataset to transfer information through the self-supervised task. Although they do not access the source dataset during the test-time adaptation phase, TTT algorithms are not off-the-shelf source-free methods. TTA is a promising and critical direction for real-world applications, but current entropy minimization-based methods can be primarily considered as feature calibrations that require high-quality pseudo-labels. This requirement, however, can be easily violated under larger distribution shifts. Current TTA algorithms, inheriting UDA drawbacks, cannot promise good feature calibration results, which can be detrimental in real-world deployments. For instance, entropy minimization on wrongly predicted target domain samples with relatively low entropy can only exacerbate spurious correla- tions (Chen et al., 2020). Without extra information, this problem may be analogous to applying causal inference without intervened distributions, which is intrinsically unsolvable (Peters et al., 2016; Pearl, 2009). This paper aims to mitigate this issue with minimal labeled target domain samples. To minimize the cost, we tailor active learning techniques for TTA settings. It is worth noting that a recent work AdaNPC (Zhang et al., 2023) is essentially a domain gener- alization method with a TTA phase attached, while our ATTA is built based on the FTTA setting. Specifically, Current FTTA methods and our work cannot access the source domain. In contrast, 21Published as a conference paper at ICLR 2024 AdaNPC accesses source data to build its memory bank, circumventing the catastrophic forgetting problem. Furthermore, AdaNPC requires multiple source domains and training before performing TTA. Thus AdaNPC uses additional information on domain labels and retraining resources for its memory bank, undermining the merits of FTTA. Regarding theoretical bounds, their target domain is bounded by source domain error and model estimations (in big-O expression), while we consider active sample learning and time variables for varying test distributions. C.3 C ONTINUAL DOMAIN ADAPTATION Many domain adaptation methods focus on improving target domain performance, neglecting the performance on the source domain, which leads to the CF problem (Kemker et al., 2018; Kirkpatrick et al., 2017; Li and Hoiem, 2017; Lopez-Paz and Ranzato, 2017; De Lange et al., 2021; Wang et al., 2022a; Niu et al., 2022). This issue arises when a neural network, after being trained on a sequence of domains, experiences a significant degradation in its performance on previously learned domains as it continues to learn new domains. Continual learning, also known as lifelong learning, addresses this problem. Recent continual domain adaptation methods have made significant progress by employing gradient regularization, random parameter restoration, buffer sample mixture, and more. Although the CF problem is proposed in the continual learning field, it can occur in any source-free OOD settings since the degradation caused by CF is attributed to the network’s parameters being updated to optimize performance on new domains, which may interfere with the representations learned for previous domains. C.4 A CTIVE DOMAIN ADAPTATION Active Domain Adaptation (ADA) (Prabhu et al., 2021; Ning et al., 2021; Su et al., 2020; Ma et al., 2021; Xie et al., 2022) extends semi-supervised domain adaptation with active learning strate- gies (Cohn et al., 1996; Settles, 2009), aiming to maximize target domain performance with a limited annotation budget. Therefore, the key challenge of active learning algorithms is selecting the most informative unlabeled data in target domains (Kapoor et al., 2007). Sample selection strategies are of- ten based on uncertainty (Lewis and Catlett, 1994; Scheffer et al., 2001), diversity (Jain and Grauman, 2016; Hoi et al., 2009), representativeness (Xu et al., 2003), expected error minimization (Vijaya- narasimhan and Kapoor, 2010), etc. Among these methods, uncertainty and diversity-based methods are simple and computationally efficient, making them the most suitable choices to tailor for TTA settings. Adapting these strategies is non-trivial because, compared to typical active domain adaptation, our proposed Active Test-time Adaptation (ATTA) setting does not provide access to source data, model parameters, or pre-collected target samples. This requirement demands that our active sample selection algorithm select samples for annotation during data streaming. Consequently, this active sampling selection process is non-regrettable, i.e., we can only meet every sample once in a short period. To avoid possible confusion, compared to the recent Source-free Active Domain Adaptation (SFADA) method SALAD (Kothandaraman et al., 2023), we do not require access to model parameter gradients, training additional neural networks, or pre-collected target datasets. Therefore, our ATTA setting is quite different, much lighter, and more realistic than ADA and SFADA. C.5 A CTIVE ONLINE LEARNING The most related branch of active online learning (AOL) (Cacciarelli and Kulahci, 2023) is active online learning on drifting data stream (Zhou et al., 2021; Baier et al., 2021; Li et al., 2021a). Generally, these methods include two components, namely, detection and adaptation. Compared with ATTA, there are several distinctions. First, this line of studies largely focuses on the distribution shift detection problem, while ATTA focuses on multi-domain adaptations. Second, AOL on drifting data stream aims to detect and adapt to one current distribution in the stream, without considering preserving the adaptation abilities of multiple past distributions by maintaining and fine-tuning the original pre-trained models. In contrast, ATTA’s goal is to achieve the OOD generalization optimums adaptable across multiple source and target distributions, leading to the consideration of CF problems. Third, while AOL requires one-by-one data input and discard, ATTA maintains a buffer for incoming data before selection decisions. This is because ATTA targets maintaining the original model without corrupting and replacing it, such that making statistically meaningful and high-quality decisions is 22Published as a conference paper at ICLR 2024 critical for ATTA. In contrast, AOL allows resetting and retraining new models, whose target is more lean to cost saving and one-by-one manner. D F URTHER THEORETICAL STUDIES In this section, we refine the theoretical studies with supplement analysis and further results. We use the H-divergence and H∆H-distance definitions following (Ben-David et al., 2010). Definition 2 (H-divergence). For a function class H and two distributions D1 and D2 over a domain X, the H-divergence between D1 and D2 is defined as dH(D1, D2) = sup h∈H |Px∼D1 [h(x) = 1] − Px∼D2 [h(x) = 1]|. The H∆H-distance is defined base on H-divergence. We use the H∆H-distance definition follow- ing (Ben-David et al., 2010). Definition 3 (H∆H-distance). For two distributions D1 and D2 over a domain X and a hypothesis class H, the H∆H-distance between D1 and D2 w.r.t. H is defined as dH∆H(D1, D2) = sup h,h′∈H Px∼D1 [h(x) ̸= h′(x)] + Px∼D2 [h(x) ̸= h′(x)]. (9) The H∆H-distance essentially provides a measure to quantify the distribution shift between two distributions. It measures the maximum difference of the disagreement between two hypotheses in H for two distributions, providing a metrics to quantify the distribution shift between D1 and D2. H-divergence and H∆H-distance have the advantage that they can be applied between datasets, i.e., estimated from finite samples. Specifically, let S1, S2 be unlabeled samples of size m sampled from D1 and D2; then we have estimated H∆H-distance ˆdH(S1, S2). This estimation can be bounded based on Theorem 3.4 of Kifer et al. (2004), which we state here for completeness. Theorem 5. Let A be a collection of subsets of some domain measure space, and assume that the VC-dimension is some finite d. Let P1 and P2 be probability distributions over that domain and S1, S2 finite samples of sizes m1, m2 drawn i.i.d. according P1, P2 respectively. Then Pm1+m2 [|ϕA(S1, S2) − ϕA(P1, P2)| > ϵ] ≤ (2m)de−m1ϵ2/16 + (2m)de−m2ϵ2/16, (10) where Pm1+m2 is the m1 + m2’th power of P - the probability that P induces over the choice of samples. Theorem 5 bounds the probability for relativized discrepancy, and its applications in below lemmas and Theorem 1 help us bound the quantified distribution shifts between domains. The probability, according to a distribution D, that an estimated hypothesis h disagrees with the true labeling function g : X → {0, 1} is defined as ϵ(h(t), g) = E(x)∼D[|h(x, t) − g(x)|], which we also refer to as the error or risk ϵ(h(t)). While the source domain dataset is inaccessible under ATTA settings, we consider the existence of the source dataset DS for the purpose of accurate theoretical analysis. Thus, we initialize Dtr(0) as DS, i.e., Dtr(0) = DS. For every time step t, the test and training data can be expressed as Ute(t) and Dtr(t) = DS ∪ Dte(1) ∪ Dte(2) ∪ ··· ∪Dte(t). (11) We use N to denote the total number of samples in Dtr(t) and λ = (λ0, λ1, ··· , λt) to represent the ratio of sample numbers in each component subset. In particular, we have |DS| |Dtr(t)| = λ0, |Dte(1)| |Dtr(t)| = λ1, ··· , |Dte(t)| |Dtr(t)| = λt, (12) where Pt i=0 λi = 1. Therefore, at time step t, the model has been trained on labeled data Dtr(t), which contains t + 1 components consisting of a combination of data from the source domain and multiple test-time domains. For each domain the model encounters, DS, Ute(1), Ute(2), ··· , Ute(t), let ϵj(h(t)) denote the error of hypothesis h at time t on the jth domain. Specifically, ϵ0(h(t)) = ϵS(h(t)) represents the error of h(t) on the source data DS, and ϵj(h(t)) for j ≥ 1 denotes the error of h(t) on test data Ute(j). Our optimization minimizes a convex combination of training error over the labeled samples from all domains. Formally, given the vector w = (w0, w1, ··· , wt) of domain error 23Published as a conference paper at ICLR 2024 weights with Pt j=0 wj = 1 and the sample number from each component Nj = λjN, we minimize the empirical weighted error of h(t) as ˆϵw(h(t)) = tX j=0 wjˆϵj(h(t)) = tX j=0 wj Nj X Nj |h(x, t) − g(x)|. (13) Note that w, λ and N are also functions of t, which we omit for simplicity. We now establish two lemmas as the preliminary for Theorem 1. In the following lemma, we bound the difference between the weighted error ϵw(h(t)) and the domain error ϵj(h(t)). Lemma 6. Let H be a hypothesis space of VC-dimension d. At time step t, let the ATTA data domains be DS, Ute(1), Ute(2), ··· , Ute(t), and Si be unlabeled samples of size m sampled from each of the t + 1 domains respectively. Then for any δ ∈ (0, 1), for every h ∈ Hminimizing ϵw(h(t)) on Dtr(t), we have |ϵw(h(t)) − ϵj(h(t))| ≤ tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi  , with probability of at least 1 − δ, where γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. In the following lemma, we provide an upper bound on the difference between the true and empirical weighted errors ϵw(h(t)) and ˆϵw(h(t)). Lemma 7. Let H be a hypothesis class. For Dtr(t) = DS ∪ Dte(1) ∪ ··· ∪Dte(t) at time t, if the total number of samples in Dtr(t) is N, and the ratio of sample numbers in each component is λj, then for any δ ∈ (0, 1) and h ∈ H, with probability of at least 1 − δ, we have P[|ϵw(h(t)) − ˆϵw(h(t))| ≥ϵ] ≤ 2 exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . Thus, as wj deviates from λj, the feasible approximation ˆϵw(h(t)) with a finite number of labeled samples becomes less reliable. The proofs for both lemmas are provided in Appx. E. Building upon the two preceding lemmas, we proceed to derive bounds on the domain errors under the ATTA setting when minimizing the empirical weighted error using the hypothesis h at time t. Lemma 6 bounds the difference between the weighted error ϵw(h(t)) and the domain error ϵj(h(t)), which is majorly influenced by the estimatedH∆H-distance and the quality of discrepancy estimation. During the ATTA process, the streaming test data can form multiple domains and distributions. However, if we consider all data during the test phase as a single test domain,i.e., St i=1 Ute(i), we can simplify Lemma 6 to obtain an upper bound for the test error ϵT as |ϵw(h(t)) − ϵT (h(t))| ≤w0  1 2 ˆdH∆H(S0, ST ) + 2 s 2d log(2m) + log 2 δ m + γ  , (14) where γ = min h∈H{ϵ0(h(t)) + ϵT (h(t))}, and ST is sampled from St i=1 Ute(i). To understand Lamma 7, we need to understand Hoeffding’s Inequality, which we state below as a Proposition for completeness. Proposition 8 (Hoeffding’s Inequality). Let X be a set, D1, . . . , Dt be probability distributions on X, and f1, . . . , ft be real-valued functions on X such that fi : X → [ai, bi] for i = 1, . . . , t. Then for any ϵ >0, P  \f\f\f\f\f 1 t tX i=1 fi(x) − 1 t tX i=1 Ex∼Di[fi(x)] \f\f\f\f\f ≥ ϵ ! ≤ 2 exp   − 2t2ϵ2 Pt i=1(bi − ai)2 ! (15) where E[fi(x)] is the expected value of fi(x). Lamma 7 provides an upper bound on the difference between the true and empirical weighted errors ϵw(h(t)) and ˆϵw(h(t)). Thus, as wj deviates from λj, the feasible approximation ˆϵw(h(t)) with a finite number of labeled samples becomes less reliable. Building upon the two preceding lemmas, we proceed to derive bounds on the domain errors under the ATTA setting when minimizing the empirical weighted error using the hypothesis h at time t. Theorem 1 essentially bounds the performance of ATTA on the source and each test domains. The adaptation performance on a test domain is majorly 24Published as a conference paper at ICLR 2024 bounded by the composition of (labeled) training data, estimated distribution shift, and ideal joint hypothesis performance, which correspond to C, ˆdH∆H(Si, Sj), and γi, respectively. The ideal joint hypothesis error γi gauges the inherent adaptability between domains. If we consider the multiple data distributions during the test phase as a single test domain, i.e., St i=1 Ute(i), Theorem 1 can be reduced into bounds for the source domain error ϵS and test domain error ϵT . With the optimal test/source hypothesis h∗ T (t) = arg min h∈H ϵT (h(t)) and h∗ S(t) = arg minh∈H ϵS(h(t)), |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤w0A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (16a) |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤(1 − w0)A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (16b) where the distribution divergence termA = ˆdH∆H(S0, ST )+4 q 2d log(2m)+log 2 δ m +2γ, the empirical gap term B = 2 q d log(2N)−log(δ) 2N , ST is sampled from St i=1 Ute(i), and γ = minh∈H{ϵ0(h(t)) + ϵT (h(t))}. Our learning bounds demonstrates the trade-off between the small amount of budgeted test-time data and the large amount of less relevant source data. Next, we provide an approximation of the condition necessary to achieve optimal adaptation performance, which is calculable from finite samples and can be readily applied in practical ATTA scenarios. Following Eq. (16.a), with approximately B = c1 p d/N, the optimal value w∗ 0 to tighten the test error bound is a function of λ0 and A: w∗ 0 = λ0 − s A2N c2 1d − A2Nλ0(1 − λ0), for λ 0 ≥ 1 − d A2N , (17) where c1 is a constant. Note that λ0 ≥ 1 − d A2N should be the satisfied condition in practical ATTA settings, where the budget is not sufficiently big while the source data amount is relatively large. When the budget is sufficiently large or the source data amount is not sufficiently large compared to the distribution shift A, the optimal w∗ 0 for the test error bound is w∗ 0 = 0, i.e., using no source data since possible error reduction from the data addition is always less than the error increase caused by large divergence between the source data and the test data. Theorem 2 offers a direct theoretical guarantee that ATTA reduces the error bound on test domains in comparison to TTA without the integration of active learning. Following Theorem 1, when no active learning is included during TTA,i.e., w0 = λ0 = 1, the upper boundw0A+ q w2 0 λ0 + (1−w0)2 1−λ0 B ≥ A+B; when enabling ATTA, withw0 = λ0 ̸= 1, we can easily achieve an upper bound w0A + B < A+ B. Therefore, the incorporation of labeled test instances in ATTA theoretically enhances the overall performance across test domains, substantiating the significance of the ATTA setting in addressing distribution shifts. Entropy quantifies the amount of information contained in a probability distribution. In the context of a classification model, lower entropy indicates that the model assigns high probability to one of the classes, suggesting a high level of certainty or confidence in its prediction. When a model assigns low entropy to a sample, this high confidence can be interpreted as the sample being well-aligned or fitting closely with the model’s learned distribution. In other words, the model “recognizes” the sample as being similar to those it was trained on, hence the high confidence in its prediction. While entropy is not a direct measure of distributional distance, it can be used as an indicator of how closely a sample aligns with the model’s learned distribution. This interpretation is more about model confidence and the implied proximity rather than a strict mathematical measure of distributional distance. The pre-trained model is well-trained on abundant source domain data, and thus the model distribution is approximately the source distribution. Selecting low-entropy samples using essentially provides an estimate of sampling from the source dataset. Thus, Dϕ,S(t), based on well-aligned with the model’s learned distribution is an approximation of DS. When we consider the CF problem and feasibly include the source-like dataset Dϕ,S(t) into the ATTA training data in place of the inaccessible DS in Eq. (11), we can also derive bounds on the domain errors under this practical ATTA setting when minimizing the empirical weighted errorϵ′ w(h(t)) using the hypothesis h at time t, similar to Theorem 1. Let H be a hypothesis class of VC-dimension d. At time step t, for ATTA data domainsDϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), Si are unlabeled samples of size m sampled from each of the t + 1 domains respectively. The total number of samples in Dtr(t) is 25Published as a conference paper at ICLR 2024 N and the ratio of sample numbers in each component is λi. If ˆh(t) ∈ Hminimizes the empirical weighted error ˆϵ′ w(h(t)) with the weight vector w on Dtr(t), and h∗ j (t) = arg minh∈H ϵj(h(t)) is the optimal hypothesis on the jth domain, then for any δ ∈ (0, 1), we have ϵj(ˆh(t)) ≤ ϵj(h∗ j (t)) + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   + 2C with probability of at least 1 − δ, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. Other derived results following Theorem 1 also apply for this practical ATTA setting. Further empirical validations for our theoretical results are provided in Appx. H. E P ROOFS This section presents comprehensive proofs for all the lemmas, theorems, and corollaries mentioned in this paper, along with the derivation of key intermediate results. Lemma 6. Let H be a hypothesis space of VC-dimension d. At time step t, let the ATTA data domains be DS, Ute(1), Ute(2), ··· , Ute(t), and Si be unlabeled samples of size m sampled from each of the t + 1 domains respectively. Then for any δ ∈ (0, 1), for every h ∈ Hminimizing ϵw(h(t)) on Dtr(t), we have |ϵw(h(t)) − ϵj(h(t))| ≤ tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi  , with probability of at least 1 − δ, where γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. Proof. First we prove that given unlabeled samples of size m S1, S2 sampled from two distributions D1 and D2, we have dH∆H(D1, D2) ≤ ˆdH∆H(S1, S2) + 4 s 2d log(2m) + log 2 δ m . (18) We start with Theorem 3.4 of Kifer et al. (2004): Pm1+m2 [|ϕA(S1, S2) − ϕA(P1, P2)| > ϵ] ≤ (2m)de−m1ϵ2/16 + (2m)de−m2ϵ2/16. (19) In Eq. 19, ’d’ is the VC-dimension of a collection of subsets of some domain measure space A, while in our case, d is the VC-dimension of hypothesis space H. Following (Ben-David et al., 2010), the H∆H space is the set of disagreements between every two hypotheses inH, which can be represented as a linear threshold network of depth 2 with 2 hidden units. Therefore, the VC-dimension of H∆H is at most twice the VC-dimension of H, and the VC-dimension of our domain measure space is 2d for Eq. 19 to hold. Given δ ∈ (0, 1), we set the upper bound of the inequality to δ, and solve for ϵ: δ = (2m)2de−m1ϵ2/16 + (2m)2de−m2ϵ2/16. We rewrite the inequality as δ (2m)2d = e−m1ϵ2/16 + e−m2ϵ2/16; taking the logarithm of both sides, we get log δ (2m)2d = −m1 ϵ2 16 + log(1 +e−(m1−m2) ϵ2 16 ). 26Published as a conference paper at ICLR 2024 Assuming m1 = m2 = m and defining a = ϵ2 16 , we have log δ (2m)2d = −ma + log 2; rearranging the equation, we then get ma + log(δ/2) = 2d log(2m). Now, we can solve for a: a = 2d log(2m) + log 2 δ m . Recall that a = ϵ2 16 , so we get: ϵ = 4√a ϵ = 4 s 2d log(2m) + log 2 δ m . With probability of at least 1 − δ, we have |ϕA(S1, S2) − ϕA(P1, P2)| ≤4 s 2d log(2m) + log 2 δ m ; therefore, dH∆H(D1, D2) ≤ ˆdH∆H(S1, S2) + 4 s 2d log(2m) + log 2 δ m . (20) Now we prove Lemma 6. We use the triangle inequality for classification error in the derivation. For the domain error of hypothesis h at time t on the jth domain ϵj(h(t)), given the definition of ϵw(h(t)), |ϵw(h(t)) − ϵj(h(t))| = | tX i=0 wiϵi(h(t)) − ϵj(h(t))| ≤ tX i=0 wi|ϵi(h(t)) − ϵj(h(t))| ≤ tX i=0 wi(|ϵi(h(t)) − ϵi(h(t), h∗ i (t))| + |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))| + |ϵj(h(t), h∗ i (t)) − ϵj(h(t))|) ≤ tX i=0 wi(ϵi(h∗ i (t)) + |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))| + ϵj(h∗ i (t))) ≤ tX i=0 wi(γi + |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))|), where γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. By the definition of H∆H-distance and our proved Eq. 20, |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))| ≤sup h,h′∈H |ϵi(h(t), h′(t)) − ϵj(h(t), h′(t))| = sup h,h′∈H Px∼Di[h(x) ̸= h′(x)] + Px∼Dj [h(x) ̸= h′(x)] = 1 2dH∆H(Di, Dj) ≤ 1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m , 27Published as a conference paper at ICLR 2024 where Di, Dj denote the ith and jth domain. Therefore, |ϵw(h(t)) − ϵj(h(t))| ≤ tX i=0 wi(γi + |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))|) ≤ tX i=0 wi(γi + 1 2dH∆H(Di, Dj)) ≤ tX i=0 wi(γi + 1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m ). Since ϵi(h(t)) − ϵj(h(t)) = 0 when i = j, we derive |ϵw(h(t)) − ϵj(h(t))| ≤ tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi  , with probability of at least 1 − δ, where γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. This completes the proof. Lemma 7. Let H be a hypothesis class. For Dtr(t) = DS ∪ Dte(1) ∪ ··· ∪Dte(t) at time t, if the total number of samples in Dtr(t) is N, and the ratio of sample numbers in each component is λj, then for any δ ∈ (0, 1) and h ∈ H, with probability of at least 1 − δ, we have P[|ϵw(h(t)) − ˆϵw(h(t))| ≥ϵ] ≤ 2 exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . Proof. We apply Hoeffding’s Inequality in our proof: P  \f\f\f\f\f 1 t tX i=1 fi(x) − 1 t tX i=1 Ex∼Di[fi(x)] \f\f\f\f\f ≥ ϵ ! ≤ 2 exp   − 2t2ϵ2 Pt i=1(bi − ai)2 ! . (21) In the jth domain, there are λjN samples. With the true labeling function g(x), for each of the λjN samples x, let there be a real-valued function fi(x) fi(x) = wj λj |h(x, t) − g(x)|, where fi(x) ∈ [0, wj λj ]. Incorporating all the domains, we get ˆϵw(h(t)) = tX j=0 wjˆϵj(h(t)) = tX j=0 wj λjN X λjN |h(x, t) − g(x)| = 1 N tX j=0 λjNX i=1 fi(x), which corresponds to the 1 t Pt i=1 fi(x) part in Hoeffding’s Inequality. Due to the linearity of expectations, we can calculate the sum of expectations as 1 N tX j=0 λjNX i=1 E[fi(x)] = 1 N ( tX j=0 λjN wj λj ϵj(h(t))) = tX j=0 wjϵj(h(t)) = ϵw(h(t)), which corresponds to the 1 t Pt i=1 Ex∼Di[fi(x)] part in Hoeffding’s Inequality. Therefore, we can apply Hoeffding’s Inequality as P[|ϵw(h(t)) − ˆϵw(h(t))| ≥ϵ] ≤ 2 exp   −2N2ϵ2/( NX i=0 range2(fi(x))) ! = 2 exp   −2N2ϵ2/( tX j=0 λjN(wj λj )2) ! = 2 exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . This completes the proof. 28Published as a conference paper at ICLR 2024 Theorem 1. Let H be a hypothesis class of VC-dimension d. At time step t, for ATTA data domains DS, Ute(1), Ute(2), ··· , Ute(t), Si are unlabeled samples of size m sampled from each of the t + 1 domains respectively. The total number of samples in Dtr(t) is N and the ratio of sample numbers in each component is λi. If ˆh(t) ∈ Hminimizes the empirical weighted error ˆϵw(h(t)) with the weight vector w on Dtr(t), and h∗ j (t) = arg minh∈H ϵj(h(t)) is the optimal hypothesis on the jth domain, then for any δ ∈ (0, 1), with probability of at least 1 − δ, we have ϵj(ˆh(t)) ≤ ϵj(h∗ j (t)) + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   + 2C, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. For future test domains j = t + k (k >0), assuming k′ = argmink′∈{0,1,...t} dH∆H(D(k′), Ute(t + k)) and min dH∆H (D(k′), Ute(t + k)) ≤ δD, where 0 ≤ δD ≪ +∞, then ∀δ, with probability of at least 1 − δ, we have ϵt+k(ˆh(t)) ≤ ϵt+k(h∗ t+k(t)) + tX i=0 wi  ˆdH∆H(Si, Sk′ ) + 4 s 2d log(2m) + log 2 δ m + δD + 2γi   + 2C. Proof. First we prove that for any δ ∈ (0, 1) and h ∈ H, with probability of at least 1 − δ, we have |ϵw(h(t)) − ˆϵw(h(t))| ≤ vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 . (22) We apply Theorem 3.2 of Kifer et al. (2004) and Lemma 7, P[|ϵw(h(t)) − ˆϵw(h(t))| ≥ϵ] ≤ (2N)d exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . Given δ ∈ (0, 1), we set the upper bound of the inequality to δ, and solve for ϵ: δ = (2N)d exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . We rewrite the inequality as δ (2N)d = e −2Nϵ2/(Pt j=0 w2 j λj ) , taking the logarithm of both sides, we get log δ (2N)d = −2Nϵ2/( tX j=0 w2 j λj ). Rearranging the equation, we then get ϵ2 = ( tX j=0 w2 j λj )d log(2N) − log(δ) 2N . Therefore, with probability of at least 1 − δ, we have |ϵw(h(t)) − ˆϵw(h(t))| ≤ vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 . (23) 29Published as a conference paper at ICLR 2024 Based on Eq. 23, we now prove Theorem 1. For the empirical domain error of hypothesis h at time t on the jth domain ϵj(ˆh(t)), applying Lemma 6, Eq. 23, and the definition of h∗ j (t), we get ϵj(ˆh(t)) ≤ ϵw(ˆh(t)) + tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(ˆh(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(h∗ j (t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(h∗ j (t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵj(h∗ j (t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   = ϵj(h∗ j (t)) + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   + 2C with probability of at least 1 − δ, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. For future test domains j = t + k where k > 0, we have the assumption that k′ = argmink′∈{0,1,...t} dH∆H(D(k′), Ute(t + k)) and min dH∆H(D(k′), Ute(t + k)) ≤ δD. Here, we slightly abuse the notation D(k′) to represent Ds if k′ = 0 and Ute(k′) if k′ > 0. Then we get ϵt+k(ˆh(t)) ≤ ϵw(ˆh(t)) + tX i=0 wi  1 2 ˆdH∆H(Si, St+k) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(ˆh(t)) + tX i=0 wi  1 2( ˆdH∆H(Si, Sk′ ) + ˆdH∆H(Sk′ , St+k)) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(ˆh(t)) + tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(ˆh(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   30Published as a conference paper at ICLR 2024 ≤ ˆϵw(h∗ t+k(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(h∗ t+k(t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵt+k(h∗ t+k(t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + 2 tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   = ϵt+k(h∗ t+k(t)) + tX i=0 wi  ˆdH∆H(Si, Sk′ ) + 4 s 2d log(2m) + log 2 δ m + δD + 2γi   + 2C. with probability of at least 1−δ, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 , γi = minh∈H{ϵi(h(t))+ ϵt+k(h(t))}, and 0 ≤ δD ≪ +∞. This completes the proof. Theorem 2. Let H be a hypothesis class of VC-dimension d. For ATTA data domains DS, Ute(1), Ute(2), ··· , Ute(t), considering the test-time data as a single test domain St i=1 Ute(i), if ˆh(t) ∈ H minimizes the empirical weighted error ˆϵw(h(t)) with the weight vector w on Dtr(t), let the test error be upper-bounded with |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤EBT (w, λ, N, t). Let w′ and λ′ be the weight and sample ratio vectors when no active learning is included, i.e., w′ and λ′ s.t. w′ 0 = λ′ 0 = 1 and w′ i = λ′ i = 0 for i ≥ 1, then for any λ ̸= λ′, there exists w s.t. EBT (w, λ, N, t) < EBT (w′, λ′, N, t). (24) Proof. From Theorem 1, we can derive the bound for the test error where the test-time data are considered as a single test domain: |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤EBT (w, λ, N, t) = w0( ˆdH∆H(S0, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ) + 2 s w2 0 λ0 + (1 − w0)2 1 − λ0 r d log(2N) − log(δ) 2N ; and we simplify the above equation as |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤w0A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (25) where the distribution divergence termA = ˆdH∆H(S0, ST )+4 q 2d log(2m)+log 2 δ m +2γ, the empirical gap term B = 2 q d log(2N)−log(δ) 2N , ST is sampled from St i=1 Ute(i), and γ = minh∈H{ϵ0(h(t)) + ϵT (h(t))}. Since we have s w2 0 λ0 + (1 − w0)2 1 − λ0 = s (w0 − λ0)2 λ0(1 − λ0) + 1 ≥ 1, (26) 31Published as a conference paper at ICLR 2024 where Formula 26 obtains the minimum value if and only if w0 = λ0; when enabling ATTA with any λ0 ̸= 1, we can get EBT (w, λ, N, t) = w0A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B ≥ w0A + B, (27) where the minimum value EBT (w, λ, N, t)min = w0A + B can be obtained with condition w0 = λ0 ̸= 1. When no active learning is included, i.e., for weight and sample ratio vectors w′ and λ′, w′ 0 = λ′ 0 = 1 and w′ i = λ′ i = 0 for i ≥ 1, we have EBT (w′, λ′, N, t) = w′ 0A + s w′2 0 λ′ 0 + (1 − w′ 0)2 1 − λ′ 0 B = A + B. (28) Since for EBT (w, λ, N, t)min = w0A + B, w0 < 1 and A, B >0 hold, we derive EBT (w, λ, N, t)min = w0A + B < A+ B = EBT (w′, λ′, N, t). (29) This completes the proof. Corollary 3. At time step t, for ATTA data domains Dϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), Si are unla- beled samples of size m sampled from each of the t + 1 domains respectively, and SS is unlabeled samples of size m sampled from DS. If ˆh(t) ∈ Hminimizes ˆϵ′ w(h(t)) while other conditions remain identical to Theorem 1, then ϵS(ˆh(t)) ≤ ϵS(h∗ S(t)) + tX i=0 wi  ˆdH∆H(Si, SS) + 4 s 2d log(2m) + log 2 δ m + 2γi   + 2C, with probability at least 1 − δ, where C follows Theorem 1 and γi = minh∈H{ϵi(h(t)) + ϵS(h(t))}. Proof. For the empirical source error on DS of hypothesis h at time t, similar to Theorem 1, we apply Lemma 6, Eq. 23 to get ϵS(ˆh(t)) ≤ ϵw(ˆh(t)) + tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(ˆh(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(h∗ S(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(h∗ S(t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵS(h∗ S(t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + 2 tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   32Published as a conference paper at ICLR 2024 = ϵS(h∗ S(t)) + tX i=0 wi  ˆdH∆H(Si, SS) + 4 s 2d log(2m) + log 2 δ m + 2γi   + 2C with probability of at least 1 − δ, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵS(h(t))}. This completes the proof. Corollary 4. At time step t, for ATTA data domains Dϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), suppose that ˆh(t) ∈ Hminimizes ˆϵw′(h(t)) under identical conditions to Theorem 2. Let’s denote the source error upper bound with |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤EBS(w, λ, N, t). Let w′ and λ′ be the weight and sample ratio vectors when Dϕ,S(t) is not included, i.e., w′ and λ′ s.t. w′ 0 = λ′ 0 = 0 . If ˆdH∆H(DS, Dϕ,S(t)) < ˆdH∆H(DS, St i=1 Ute(i)), then for any λ ̸= λ′, there exists w s.t. EBS(w, λ, N, t) < EBS(w′, λ′, N, t). (30) Proof. From Theorem 1, considering the test-time data as a single test domain, we can derive the bound for the source error on DS: |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤EBS(w, λ, N, t) = w0( ˆdH∆H(S0, SS) + 4 s 2d log(2m) + log 2 δ m + 2γ) + (1 − w0)( ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′) + 2 s w2 0 λ0 + (1 − w0)2 1 − λ0 r d log(2N) − log(δ) 2N , where ST is sampled fromSt i=1 Ute(i), γ = minh∈H{ϵ0(h(t))+ϵS(h(t))}, and γ′ = minh∈H{ϵT (h(t))+ ϵS(h(t))}. We have s w2 0 λ0 + (1 − w0)2 1 − λ0 = s (w0 − λ0)2 λ0(1 − λ0) + 1 ≥ 1, (31) where the equality and the minimum value are obtained if and only if w0 = λ0. When Dϕ,S(t) is not included,i.e., with the weight and sample ratio vectorsw′ and λ′ s.t. w′ 0 = λ′ 0 = 0, using the empirical gap term B = 2 q d log(2N)−log(δ) 2N , we have EBS(w′, λ′, N, t) = ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′ + s w2 0 λ0 + (1 − w0)2 1 − λ0 B = ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′ + B. When Dϕ,S(t) is included with λ0 ̸= 0, EBS(w, λ, N, t) = w0( ˆdH∆H(S0, SS) + 4 s 2d log(2m) + log 2 δ m + 2γ) + (1 − w0)( ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′) + s w2 0 λ0 + (1 − w0)2 1 − λ0 B ≤ w0( ˆdH∆H(S0, SS) + 4 s 2d log(2m) + log 2 δ m + 2γ) + (1 − w0)( ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′) + B, 33Published as a conference paper at ICLR 2024 Algorithm 2 INCREMENTAL CLUSTERING (IC) Require: Given previously selected anchors, new unlabeled samples, and the cluster budget as Danc, Unew, and NC . Global anchor weights wanc = (wanc 1 , . . . , wanc |Danc|)⊤. 1: For simplicity, we consider anchor weights wanc as a global vector. 2: function IC(Danc, Unew, NC ) 3: wsp ← Concat(wanc, 1⊤ |Unew|) ▷ Assign all new samples with weight 1. 4: Φ ← Extract the features from the penultimate layer of model f on x ∈ Danc ∪ Unew in order. 5: clusters ← Weighted-K-Means(Φ, wsp, NC) 6: new_clusters ← {clusteri | ∀clusteri ∈ clusters, ∀x ∈ Danc, x /∈ clustersi} 7: Xnew_anchors ← {the closest sample x to the centroid of clusteri | ∀clusteri ∈ new_clusters} 8: Xanchors ← {x ∈ Danc} ∪Xnew_anchors 9: wanc ← Concat(wanc, 0⊤ |Xnew_anchors|) ▷ Initialize new anchor weights. 10: for wanc i ∈ wanc, wanc i ← wanc i + # sample of clusterj # anchor in clusterj , wanc i ∈ clusterj ▷ Weight accumulation. 11: Return Xanchors 12: end function where the minimum value can be obtained with condition w0 = λ0 ̸= 0. In practical learning scenarios, we generally assume adaptation tasks are solvable; therefore, there should be a prediction function that performs well on two distinct domains. In this case, γ and γ′ should be relatively small, so we can assume γ ≈ γ′. If ˆdH∆H(S0, SS) < ˆdH∆H(SS, ST ), then we have EBS(w, λ, N, t)min = w0( ˆdH∆H(S0, SS) + 4 s 2d log(2m) + log 2 δ m + 2γ) + (1 − w0)( ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′) + B < ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′ + B = EBS(w′, λ′, N, t). Therefore, we derive EBS(w, λ, N, t)min < EBS(w′, λ′, N, t). (32) This completes the proof. F I NCREMENTAL CLUSTERING F.1 A LGORITHM DETAILS We provide the detailed algorithm for incremental clustering as Alg. 2. F.2 V ISUALIZATION To better illustrate the incremental clustering algorithm, we provide visualization results on PACS to demonstrate the process. As shown in Fig. 3, the initial step of IC is a normal K-Means clustering step, and ten anchors denoted as \"X\" are selected. The weights of all samples in a clusters is aggregated into the corresponding anchor’s weight. Therefore, these ten samples (anchors) are given larger sizes visually (i.e., larger weights) than that of other new test samples in the first IC step (Fig. 4). During the first IC step, several distributions are far away from the existed anchors and form clusters 1,7,9 and 10, which leads to 4 new selected anchors. While the number of cluster centroid is only increased by 1, 4 of the existing anchors are clustered into the same cluster 8 (purple). Thus IC produces 4 new anchors instead of 1. Similarly, in the second IC step (Fig. 5), the new streaming-in test samples introduce a new distribution; IC produces 3 new clusters (4, 8, and 11) and the corresponding number of anchors to cover them. The number of centroid is only increased by 1, which implies that there are two original-cluster-merging events. More IC step visualization results are provided in Fig. 6 and 7. 34Published as a conference paper at ICLR 2024 Figure 3: Initial IC step: normal clustering. Left: Clustering results. Right: Selecting new anchors. Figure 4: The first IC step. Left: Weighted clustering results. Right: Selecting new anchors. Figure 5: The second IC step. Left: Weighted clustering results. Right: Selecting new anchors. 35Published as a conference paper at ICLR 2024 Figure 6: The third IC step. Left: Weighted clustering results. Right: Selecting new anchors. Figure 7: The fourth IC step. Left: Weighted clustering results. Right: Selecting new anchors. 36Published as a conference paper at ICLR 2024 G E XPERIMENT DETAILS In this section, we provide more experimental details including the details of the datasets and training settings. G.1 D ETAILS ABOUT THE DATASETS We adopt datasets PACS, VLCS, and Office-Home from DomainBed (Gulrajani and Lopez-Paz, 2020) with the same domain splits. All available licenses are mentioned below. • PACS (Li et al., 2017) includes four domains: art, cartoons, photos, and sketches. PACS is a 7-class classification dataset with 9,991 images of dimension (3, 224, 224). • VLCS (Fang et al., 2013) contains photographic domains: Caltech101, LabelMe, SUN09, and VOC2007. This dataset includes 10,729 images of dimension (3, 224, 224) with 5 classes. • Office-Home (Venkateswara et al., 2017) is a 65-class dataset, including domains: art, clipart, product, and real. VLCS includes 10,729 images of dimension (3, 224, 244). (License) • Tiny-ImageNet-C is a 200-class dataset, including 15 corrupt types. Tiny-ImageNet-C includes 150,000 images of dimension (3, 224, 244). Since the class number 200 is less than ImageNet (1000), the model’s last layer classifier needs to be adapted. In this work, we use the brightness corruption domain to adapt. In the source pretraining phase, we adopt the most ImageNet-like domain as our source domain. For PACS and Office-Home, we use domains \"photos\" and \"real\" as the source domains, respectively, while for VLCS, Caltech101 is assigned to apply the source pretraining. We freeze the random seeds to generate the sample indices order for the two test data streams, namely, the domain-wise data stream and the random data stream. For PACS, the domain-wise data stream inputs samples from domain art, cartoons, to sketches, while we shuffle all samples from these three domains in the random data stream. For VLCS, we stream the domains in the order: LabelMe, SUN09, and VOC2007, as the domain-wise data stream. For Office-Home, the domain-wise data stream order becomes art, clipart, and product. G.2 T RAINING AND OPTIMIZATION SETTINGS In this section, we extensively discuss the model architectures, optimization settings, and method settings. G.2.1 A RCHITECTURES PACS & VLCS. We adopt ResNet-18 as our model encoder followed by a linear classifier. The initial parameters of ResNet-18 are ImageNet pre-trained weights. In our experiment, we remove the Dropout layer since we empirically found that using the Dropout layer might degrade the optimization process when the sample number is small. The specific implementation of the network is closely aligned with the implementation in DomainBed (Gulrajani and Lopez-Paz, 2020). Office-Home. We employ ResNet-50 as our model encoder for Office-Home. Except for the architecture, the other model settings are aligned with the ResNet-18. Tiny-ImageNet-C ResNet-18 is adapted from ImageNet to Tiny-ImageNet-C by training the last linear layer. G.2.2 T RAINING & OPTIMIZATION In this section, we describe the training configurations for both the source domain pre-training and test-time adaptation procedures. Source domain pre-training. For the PACS and VLCS datasets, models are fine-tuned on the selected source domains for 3,000 iterations. The Adam optimizer is utilized with a learning rate 37Published as a conference paper at ICLR 2024 of 10−4. In contrast, for the Office-Home dataset, the model is fine-tuned for a longer duration of 10,000 iterations with a slightly adjusted learning rate of 5 × 10−5. Test-time adaptation. For test-time adaptation across PACS and VLCS, the pre-trained source model is further fine-tuned using the SGD optimizer with a learning rate of 10−3. While on Office-Home and Tiny-ImageNet-C, a learning rate of 10−4 is adopted. For all TTA baselines, barring specific exceptions, we faithfully adhere to the original implementation settings. A noteworthy exception is the EATA method, which requires a cosine similarity threshold. The default threshold of the original EATA implementation was not suitable for the three datasets used in our study, necessitating an adjustment. We empirically set this threshold to 0.5 for training. Unlike Tent and SAR, which only require the optimization of batch normalization layers (Santurkar et al., 2018), SimATTA allows the training of all parameters in the networks. In experiments, we use a tolerance count (tol) to control the training process. SimATTA will stop updating once the loss does not descrease for more than 5 steps. However, for Tiny-ImageNet-C, SimATTA uses ‘steps=10‘ for time comparisons since other methods apply at most 10 steps. G.2.3 M ETHOD SETTINGS Tent. In our experiments, we apply the official implementation of Tent1. Specifically, we evaluate Tent with 1 test-time training step and 10 steps, respectively. EATA.Our EATA implementation follows its official code2. In our experiments, EATA has 2000 fisher training samples, E0 = 0.4 × log(# class), ϵ <0.5. CoTTA. For CoTTA, we strictly follow all the code and settings from its official implementation3. SAR. With SAR’s official implementation4, we set E0 = 0 .4 × log(# class) and e0 = 0 .1 in our experiments. ADA baselines. For ADA baselines, we follow the architecture of the official implementation of CLUE (Prabhu et al., 2021)5. SimATTA Implementation. Our implementation largely involves straightforward hyperparameter settings. The higher entropy bound eh = 10−2 should exceed the lower entropy bound el, but equal values are acceptable. Empirically, the lower entropy bound el can be set to 10−3 for VLCS and Office-Home, or 10−4 for PACS. The choice of el is largely dependent on the number of source-like samples obtained. A lower el may yield higher-accuracy low-entropy samples, but this could lead to unstable training due to sample scarcity. Though experimentation with different hyperparameters is encouraged, our findings suggest that maintaining a non-trivial number of low-entropy samples and setting an appropriateλ0 are of primary importance. If λ0 < 0.5, CF may ensue, which may negate any potential improvement. Regarding the management of budgets, numerous strategies can be adopted. In our experiments, we utilized a simple hyperparameter k, varying from 1 to 3, to regulate the increasing rate of budget consumption. This strategy is fairly elementary and can be substituted by any adaptive techniques. G.3 S OFTWARE AND HARDWARE We conduct our experiments with PyTorch (Paszke et al., 2019) and scikit-learn (Pedregosa et al., 2011) on Ubuntu 20.04. The Ubuntu server includes 112 Intel(R) Xeon(R) Gold 6258R CPU @2.70GHz, 1.47TB memory, and NVIDIA A100 80GB PCIe graphics cards. The training process costs graphics memory less than 10GB, and it requires CPU computational resources for scikit-learn K-Means clustering calculations. Our implementation also includes a GPU-based PyTorch K-Means method for transferring calculation loads from CPUs to GPUs. However, for consistency, the results of our experiments are obtained with the original scikit-learn K-Means implementation. 1https://github.com/DequanWang/tent 2https://github.com/mr-eggplant/EATA 3https://github.com/qinenergy/cotta 4https://github.com/mr-eggplant/SAR 5https://github.com/virajprabhu/CLUE 38Published as a conference paper at ICLR 2024 Figure 8: Target loss surface on 2000 samples without source pre-training. The red points denote the loss minimum for a fixed λ0. The orange line denote the place where w0 = λ0. Figure 9: Target loss surface on 2000 samples with source pre-training. H E MPIRICAL VALIDATIONS FOR THEORETICAL ANALYSIS In this section, we undertake empirical validation of our learning theory, which encompasses multiple facets awaiting verification. In contemporary computer vision fields, pre-trained models play a pivotal role, and performance would significantly decline without the use of pre-trained features. The learning theory suggests that given the vast VC-dimension of complete ResNets, without substantial data samples, the training error cannot be theoretically tight-bounded. However, we show empirically in the following experiments that fine-tuning pre-trained models is behaviorally akin to training a model with a low VC-dimension. Training on 2000 Samples Without Source Domain Pre-training. For an ImageNet pre-trained ResNet-18 model, we trained it using 2000 samples from the PACS dataset. To ascertain the optimal value w∗ 0 in Equation 4, we trained multiple models for different w0 and λ0 pairings. For each pair, we derived the target domain loss (from art, cartoons, and sketches) post-training and plotted this loss on the z-axis. With w0 and λ0 serving as the xy-axes, we drafted the target domain loss ϵT surface in Figure 8. As the results show, given a λ0, the optimal w∗ 0 typically aligns with the line λ0 = w0, with a slight downward shift, which aligns with Equation 4. 39Published as a conference paper at ICLR 2024 Figure 10: Target loss surface on 500 samples with source pre-training. Figure 11: Source loss surface on 500 samples with source pre-training. 40Published as a conference paper at ICLR 2024 Figure 12: Target and source loss surface on 500 samples with source pre-training. Table 6: TTA comparisons on Office-Home. This table includes the two data stream settings mentioned in the dataset setup and reports performances in accuracy. Results that outperform all TTA baselines are highlighted in bold font. N/A denotes the adaptations are not applied on the source domain. Office-Home Domain-wise data stream Post-adaptation Random data stream Post-adaptation R →A→ →C→ →P R A C P 1 2 3 4 R A C P BN w/o adapt 93.78 42.93 37.62 59.90 93.78 42.93 37.62 59.90 46.82 46.82 46.82 46.82 93.78 42.93 37.62 59.90BN w/ adapt 92.38 49.69 39.43 63.53 92.38 49.69 39.43 63.53 50.88 50.88 50.88 50.88 92.38 49.69 39.43 63.53 Tent (steps=1) N/A 49.61 39.31 63.87 92.47 49.57 39.89 63.89 49.95 50.27 50.23 52.06 92.40 49.24 39.68 63.98Tent (steps=10) N/A 49.61 39.04 61.41 87.08 44.79 38.37 60.49 50.05 49.31 48.74 47.79 85.31 42.85 37.89 58.71EATA N/A 49.65 39.04 63.53 91.60 49.61 38.65 63.48 49.73 50.27 49.45 51.07 91.05 49.11 38.26 62.99CoTTA N/A 49.61 38.76 61.84 87.81 44.95 35.92 59.04 49.84 49.84 48.95 50.43 86.99 43.68 34.73 57.56SAR (steps=1) N/A 49.65 39.24 63.53 92.45 49.73 39.36 63.69 49.84 50.05 49.91 51.67 92.38 49.57 39.50 63.87SAR (steps=10) N/A 49.53 38.81 61.50 88.94 46.15 37.04 59.41 50.09 50.30 49.77 49.22 89.14 46.23 36.31 59.45 SimATTA (B ≤300) N/A 56.20 48.38 71.66 95.75 60.07 52.62 74.70 58.57 60.88 62.91 63.67 95.89 62.01 54.98 74.70SimATTA (B ≤500) N/A 58.71 51.11 74.36 96.03 62.05 57.41 76.98 58.85 62.63 63.41 64.31 95.91 63.78 57.87 77.09 Training on 2000 Samples with Source Domain Pre-training. To further assess the effects of source pre-training, we repeated the same experiment on a source pre-trained ResNet-18. The results are depicted in Figure 9. This experiment provides empirical guidance on selecting w0 in source domain pre-trained situations. The findings suggest that the optimal w∗ 0 non-trivially shifts away from the line λ0 = w0 towards lower-value regions. Considering the source pre-training process as using a greater quantity of source domain samples, it implies that when the number of source samples greatly exceeds target samples, a lower w0 can enhance target domain results. Training on 500 Samples with Source Domain Pre-training. We proceed to fine-tune the source domain pre-trained ResNet-18 using only 500 samples, thereby simulating active TTA settings. We train models with various w0 and λ0 pairings, then graph the target domain losses, source domain losses, and the combined losses. As shown in Figure 10, the target losses still comply with our theoretical deductions where the local minima are close to the line λ0 = w0 and marginally shift towards lower values. Considering the challenge of CF, the source domain results in Figure 11 suggest a reverse trend compared to the target domain, where lower λ0 and w0 values yield superior target domain results but inferior source domain results. Thus, to curb CF, the primary strategy is to maintain a relatively higher λ0. When considering both target and source domains, a balance emerges as depicted in Figure 12. The global minimum is located in the middle region, demonstrating the trade-off between the target domain and source domain performance. I A DDITIONAL EXPERIMENT RESULTS In this section, we provide additional experiment results. The Office-Home results and ablation studies will be presented in a similar way as the main paper. In the full results Sec. I.3, we will post more detailed experimental results with specific budget numbers and intermediate performance during the test-time adaptation. 41Published as a conference paper at ICLR 2024 Table 7: Comparisons to ADA baselines on Office-Home. The source domain is denoted as \"(S)\" in the table. Results are average accuracies with standard deviations). Office-Home R (S) A C P Random (B = 300) 95.04 (0.20) 57.54 (1.16) 53.43 (1.17) 73.46 (0.97) Entropy (B = 300) 94.39 (0.49) 61.21 (0.71) 56.53 (0.71) 72.31 (0.28) Kmeans (B = 300) 95.09 (0.14) 57.37 (0.90) 51.74 (1.34) 71.81 (0.39) CLUE (B = 300) 95.20 (0.23) 60.18 (0.98) 58.05 (0.43) 73.72 (0.70) Ours (B ≤300) 95.82 (0.07) 61.04 (0.97) 53.80 (1.18) 74.70 (0.00) I.1 R ESULTS ON OFFICE -HOME We conduct experiments on Office-Home and get the test-time performances and post-adaptation performances for two data streams. As shown in Tab. 6, SimATTA can outperform all TTA baselines with huge margins. Compared to ADA baselines under the source-free settings, as shown in Tab. 7, SimATTA obtains comparable results. I.2 A BLATION STUDIES Figure 13: Ablation study on PACS and VLCS.\"IC=0\" denotes removing incremental clustering (IC) selection. \"LE=0\" denotes removing the low-entropy (LE) sample training. Domain-wise stream and random stream are applied on first and second rows, respectively. The accuracy values are averaged across all splits/domains. In this section, we explore three variations of our method to examine the individual impacts of its components. The first variant replaces the incremental clustering selection with entropy selection, 42Published as a conference paper at ICLR 2024 where only the samples with the highest entropy are chosen. The second variant eliminates low- entropy sample training. The third variation combines the first and second variants. We perform this ablation study on the PACS and VLCS as outlined in Fig. 13. We denote the use of incremental clustering (IC) and low-entropy training (LE) respectively as IC=1 and LE=1. The experiments essentially reveals the effectiveness of incremental clustering and low-entropy- sample training. As we have detailed in Sec. 3.2, these techniques are designed to to select informative samples, increase distribution coverage, and mitigate catastrophic forgetting. These designs appositely serve the ATTA setting where the oracle has costs and the budget is limited. Therefore, their effectiveness is prominent particularly when the budget is small. As the results show, when the budget B ≤100 or B ≤300, removing the components observably impairs performances. When B gets large, more active samples cover a larger distribution; thus the performance gap from random selection and informative selection gets smaller. In the extreme case where B → ∞, all samples are selected and thus the superiority of our meticulously-designed techniques are not manifested. Specifically, our analysis yields several insights. First, SimATTA (LE=1, IC=1) comprehensively outperforms other variants on both datasets, different streams, and different budgets. Second, variants without low-entropy training (LE=0, IC=0/1) easily fail to produce stable results (e.g., domain-wise stream in VLCS). Third, SimATTA’s performance surpasses this variant on PACS’s domain-wise stream clearly especially when the budgets are low. This indicates these variants fail to retrieve the most informative style shift (PACS’s shifts) samples, which implies the advantage of incremental clustering when the budget is tight. In addition, these results show that IC has its unique advantage on domain-wise streams where distributions change abruptly instead of random streams. Therefore, compared to PACS’s domain- wise stream results, the reason for the smaller performance improvement of SimATTA over the variant (LE=1, IC=0) on VLCS’s domain-wise stream is that images in VLCS are all photos that do not include those severe style shifts in PACS (i.e., art, cartoons, and sketches). That is, when the shift is not severe, we don’t need IC to cover very different distributions, and selecting samples using entropy can produce good results. In brief, IC is extraordinary for severe distribution shifts and quick adaptation. It is worth mentioning that low budget comparison is essential to show the informative sample retrieval ability, since as the budget increases, all AL techniques will tend to perform closely. I.3 C OMPLETE EXPERIMENT RESULTS We provide complete experimental results in this section. As shown in Tab. 8, we present the full results for two data streams. The test-time adaptation accuracies are shown in the \"Current domain\" row, while the \"Budgets\" row denotes the used budget by the end of the domain. The rest four rows denote the four domain test results by the end of the real-time adaptation of the current domain, where the first column results are the test accuracy before the test-time adaptation phase. N/A represents \"do not apply\". Table 8: Tent (steps=1) on PACS. Tent (steps=1) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 67.29 64.59 44.67 56.35 54.09 51.83 48.58 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.68 98.38 97.60 98.56 98.08 97.72 97.19 A 59.38 69.09 68.95 66.85 68.07 67.33 65.58 63.53 C 28.03 64.04 65.19 64.08 64.85 65.19 62.97 60.75 S 42.91 53.65 47.39 42.58 54.57 49.83 44.13 41.56 J C HALLENGES AND PERSPECTIVES Despite advancements, test-time adaptation continues to pose considerable challenges. As previously discussed, without supplementary information and assumptions, the ability to guarantee model generalization capabilities is limited. However, this is not unexpected given that recent progress 43Published as a conference paper at ICLR 2024 Table 9: Tent (steps=10) on PACS. Tent (steps=10) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 67.38 57.85 20.23 47.36 31.01 22.84 20.33 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 95.45 87.43 62.63 93.83 81.32 65.39 50.78 A 59.38 64.94 55.03 34.52 55.32 40.28 28.27 23.68 C 28.03 55.89 56.70 40.57 54.52 39.68 27.22 20.95 S 42.91 36.96 26.27 13.59 32.25 23.16 20.95 19.62 Table 10: EATA on PACS. EATA Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 67.04 64.72 50.27 57.31 56.06 58.17 59.78 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.62 98.50 98.62 98.68 98.62 98.50 98.62 A 59.38 68.90 68.16 66.50 68.65 68.95 69.34 69.63 C 28.03 63.74 65.36 62.46 65.19 66.00 65.57 65.70 S 42.91 54.01 52.89 48.18 55.71 55.64 54.09 54.26 Table 11: CoTTA on PACS. CoTTA Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 65.48 62.12 53.17 56.06 54.33 57.16 57.42 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.68 98.62 98.62 98.62 98.62 98.56 98.62 A 59.38 65.82 65.87 65.48 66.02 65.87 66.31 65.97 C 28.03 62.63 63.05 63.10 63.01 62.88 63.01 62.97 S 42.91 53.88 54.03 53.78 54.67 55.31 55.10 54.62 Table 12: SAR (steps=1) on PACS. SAR (steps=1) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 66.75 63.82 49.58 56.78 56.35 56.68 56.70 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.68 98.50 98.32 98.74 98.56 98.50 98.44 A 59.38 68.02 68.07 66.94 67.87 68.65 68.55 68.16 C 28.03 62.84 64.97 62.93 63.82 64.89 64.46 64.38 S 42.91 53.47 52.07 45.74 54.92 55.46 53.68 52.53 Table 13: SAR (steps=10) on PACS. SAR (steps=10) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 69.38 68.26 49.02 53.51 51.15 51.78 45.60 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.20 95.39 96.47 97.13 97.78 97.72 94.13 A 59.38 72.36 66.60 62.16 62.74 64.94 66.11 56.64 C 28.03 63.44 68.30 56.19 59.77 61.73 62.03 56.02 S 42.91 53.37 44.59 54.62 41.00 49.66 48.79 36.37 44Published as a conference paper at ICLR 2024 Table 14: SimATTA (B ≤300) on PACS. SimATTA (B ≤300) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 76.86 70.90 75.39 69.47 76.49 82.45 82.22 Budgets N/A 75 145 223 66 142 203 267 P 99.70 98.44 98.86 98.80 97.96 98.68 99.04 98.98 A 59.38 80.71 82.32 84.47 73.97 80.52 81.10 84.91 C 28.03 48.12 82.00 82.25 72.35 81.06 83.36 83.92 S 42.91 32.78 56.25 81.52 79.49 83.10 84.78 86.00 Table 15: SimATTA (B ≤500) on PACS. SimATTA (B ≤500) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 77.93 76.02 76.30 68.46 78.22 80.91 85.49 Budgets N/A 121 230 358 102 221 343 425 P 99.70 98.92 98.86 98.62 98.20 99.46 99.10 99.16 A 59.38 87.01 87.60 88.33 73.39 79.20 84.91 86.67 C 28.03 54.78 83.96 83.49 68.43 74.40 84.22 84.77 S 42.91 46.37 63.53 83.74 81.34 81.04 86.66 87.71 Table 16: Tent (steps=1) on VLCS. Tent (steps=1) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 38.55 34.40 53.88 44.85 44.29 47.38 44.98 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 84.81 85.44 84.73 84.95 85.16 85.80 85.30 L 33.55 40.02 43.11 43.86 39.68 41.98 43.11 43.49 S 41.10 33.39 35.41 33.61 36.29 37.90 38.27 37.81 V 49.08 53.20 54.06 53.11 53.76 54.18 53.76 53.35 Table 17: Tent (steps=10) on VLCS. Tent (steps=10) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 45.41 31.44 32.32 46.13 42.31 43.51 39.48 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 73.07 48.34 42.54 74.13 62.19 56.54 52.01 L 33.55 46.61 38.44 37.65 44.88 45.93 43.41 40.32 S 41.10 31.75 28.82 27.79 35.37 36.14 35.28 33.64 V 49.08 48.05 40.14 33.12 50.50 44.49 42.48 40.37 Table 18: EATA on VLCS. EATA Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 37.24 33.15 52.58 43.77 42.48 43.34 41.55 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 85.16 85.02 84.10 84.73 84.52 84.10 83.32 L 33.55 37.16 37.24 37.69 37.09 36.78 36.90 36.67 S 41.10 33.39 33.49 32.39 33.33 32.54 31.84 31.47 V 49.08 51.87 52.16 52.49 52.07 52.43 52.64 52.55 45Published as a conference paper at ICLR 2024 Table 19: CoTTA on VLCS. CoTTA Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 37.39 32.54 52.25 43.69 42.14 43.21 42.32 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 81.55 81.98 82.12 82.61 82.47 82.12 81.98 L 33.55 37.20 37.91 37.65 38.48 38.22 38.40 37.99 S 41.10 30.71 32.78 33.12 34.00 33.70 33.97 33.52 V 49.08 52.01 52.64 52.90 53.64 53.14 53.08 53.23 Table 20: SAR (steps=1) on VLCS. SAR (steps=1) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 36.18 34.43 52.46 43.64 43.04 44.20 41.93 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 84.31 84.17 83.96 85.09 85.23 85.23 85.09 L 33.55 35.62 38.29 39.72 38.55 39.34 40.21 40.70 S 41.10 33.24 36.41 36.53 34.37 35.62 36.29 36.44 V 49.08 51.75 52.61 52.37 52.90 52.75 53.05 53.02 Table 21: SAR (steps=10) on VLCS. SAR (steps=10) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 35.32 34.10 51.66 43.56 42.05 42.53 41.16 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 83.96 83.04 82.12 84.03 84.24 85.23 85.09 L 33.55 34.07 35.92 41.49 39.53 38.37 37.65 37.58 S 41.10 31.93 34.89 33.94 35.19 32.94 33.88 33.12 V 49.08 51.33 51.51 53.08 52.78 52.34 51.78 52.01 Table 22: SimATTA (B ≤300) on VLCS. SimATTA (B ≤300) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 62.61 65.08 74.38 62.33 69.33 73.20 71.93 Budgets N/A 79 175 272 71 135 208 262 C 100.00 99.51 98.52 99.93 99.86 99.79 100.00 99.93 L 33.55 68.11 69.92 69.50 62.61 66.64 68.45 69.43 S 41.10 55.24 68.89 66.67 65.54 69.29 71.79 72.46 V 49.08 66.08 70.94 77.34 73.79 76.87 78.82 80.39 Table 23: SimATTA (B ≤500) on VLCS. SimATTA (B ≤500) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 63.52 68.01 76.13 62.29 70.45 73.50 72.02 Budgets N/A 113 266 446 107 203 283 356 C 100.00 99.29 98.59 99.51 99.93 99.86 99.86 99.43 L 33.55 62.95 70.63 70.56 66.57 67.09 67.24 70.29 S 41.10 51.31 73.83 73.10 65.33 71.79 72.91 72.55 V 49.08 59.36 71.65 78.35 73.58 77.84 80.01 80.18 46Published as a conference paper at ICLR 2024 Table 24: Tent (steps=1) on Office-Home. Tent (steps=1) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.61 39.31 63.87 49.95 50.27 50.23 52.06 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 92.33 92.36 92.47 92.38 92.45 92.45 92.40 A 57.07 49.73 49.73 49.57 49.69 49.73 49.57 49.24 C 44.97 39.27 39.54 39.89 39.45 39.68 39.73 39.68 P 73.15 63.60 63.66 63.89 63.60 63.82 63.93 63.98 Table 25: Tent (steps=10) on Office-Home. Tent (steps=10) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.61 39.04 61.41 50.05 49.31 48.74 47.79 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 91.99 89.14 87.08 92.08 90.80 88.59 85.31 A 57.07 49.94 46.77 44.79 49.44 48.21 45.69 42.85 C 44.97 38.58 39.11 38.37 40.18 40.02 38.63 37.89 P 73.15 63.28 61.03 60.49 64.36 63.64 61.12 58.71 Table 26: EATA on Office-Home. EATA Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.65 39.04 63.53 49.73 50.27 49.45 51.07 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 92.36 92.17 91.60 92.38 92.22 91.71 91.05 A 57.07 49.57 49.53 49.61 49.69 49.40 49.36 49.11 C 44.97 39.08 39.01 38.65 39.27 39.01 38.42 38.26 P 73.15 63.42 63.42 63.48 63.51 63.37 63.33 62.99 Table 27: CoTTA on Office-Home. CoTTA Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.61 38.76 61.84 49.84 49.84 48.95 50.43 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 90.38 88.02 87.81 90.48 89.37 88.00 86.99 A 57.07 48.58 45.53 44.95 47.34 46.35 44.62 43.68 C 44.97 36.66 35.58 35.92 37.55 36.40 35.44 34.73 P 73.15 60.40 57.74 59.04 61.12 59.63 58.35 57.56 Table 28: SAR (steps=1) on Office-Home. SAR (steps=1) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.65 39.24 63.53 49.84 50.05 49.91 51.67 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 92.38 92.31 92.45 92.40 92.36 92.36 92.38 A 57.07 49.65 49.57 49.73 49.69 49.61 49.57 49.57 C 44.97 39.34 39.22 39.36 39.34 39.56 39.47 39.50 P 73.15 63.51 63.51 63.69 63.60 63.71 63.71 63.87 47Published as a conference paper at ICLR 2024 Table 29: SAR (steps=10) on Office-Home. SAR (steps=10) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.53 38.81 61.50 50.09 50.30 49.77 49.22 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 92.20 92.06 88.94 92.40 92.47 91.53 89.14 A 57.07 49.40 49.77 46.15 49.81 50.02 48.91 46.23 C 44.97 39.20 38.63 37.04 39.50 39.29 38.65 36.31 P 73.15 63.53 62.69 59.41 64.18 64.18 62.83 59.45 Table 30: SimATTA (B ≤300) on Office-Home. SimATTA (B ≤300) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 56.20 48.38 71.66 58.57 60.88 62.91 63.67 Budgets N/A 75 187 277 79 147 216 278 R 96.44 95.43 95.43 95.75 95.91 95.96 96.01 95.89 A 57.07 57.56 59.50 60.07 58.34 59.91 61.15 62.01 C 44.97 42.25 52.46 52.62 51.66 52.30 54.75 54.98 P 73.15 68.84 70.13 74.70 72.45 73.10 74.50 74.70 Table 31: SimATTA (B ≤500) on Office-Home. SimATTA (B ≤500) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 58.71 51.11 74.36 58.85 62.63 63.41 64.31 Budgets N/A 107 284 440 126 248 361 467 R 96.44 95.69 95.71 96.03 96.26 96.19 95.87 95.91 A 57.07 61.43 61.43 62.05 58.18 61.15 61.52 63.78 C 44.97 46.41 57.73 57.41 53.17 55.14 56.79 57.87 P 73.15 70.74 71.98 76.98 73.51 74.18 75.78 77.09 48Published as a conference paper at ICLR 2024 in deep learning heavily relies on large-scale data. Consequently, two promising paths emerge: establishing credible assumptions and leveraging additional information. Firstly, developing credible assumptions can lead to comprehensive comparisons across various stud- ies. Given that theoretical guarantees highlight the inherent differences between methods primarily based on the application limits of their assumptions, comparing these assumptions becomes critical. Without such comparative studies, empirical evaluations may lack precise guidance and explanation. Secondly, while we acknowledge the value of real-world data (observations), discussions surrounding the use of extra information remain pertinent. Considerations include the strategies to acquire this supplementary information and the nature of the additional data needed. Despite the myriad of works on domain generalization, domain adaptation, and test-time adaptation, a comprehensive survey or benchmark encapsulating the aforementioned comparisons remains an unmet need. Moreover, potential future directions for out-of-distribution generalization extend beyond domain generalization and test-time adaptation. One promising avenue is bridging the gap between causal inference and deep learning, for instance, through causal representation learning. In conclusion, our hope is that this work not only offers a novel practical setting and algorithm but also illuminates meaningful future directions and research methodologies that can benefit the broader scientific community. 49",
      "meta_data": {
        "arxiv_id": "2404.05094v1",
        "authors": [
          "Shurui Gui",
          "Xiner Li",
          "Shuiwang Ji"
        ],
        "published_date": "2024-04-07T22:31:34Z",
        "pdf_url": "https://arxiv.org/pdf/2404.05094v1.pdf"
      }
    },
    {
      "title": "Test Time Adaptation via Conjugate Pseudo-labels",
      "abstract": "Test-time adaptation (TTA) refers to adapting neural networks to distribution\nshifts, with access to only the unlabeled test samples from the new domain at\ntest-time. Prior TTA methods optimize over unsupervised objectives such as the\nentropy of model predictions in TENT [Wang et al., 2021], but it is unclear\nwhat exactly makes a good TTA loss. In this paper, we start by presenting a\nsurprising phenomenon: if we attempt to meta-learn the best possible TTA loss\nover a wide class of functions, then we recover a function that is remarkably\nsimilar to (a temperature-scaled version of) the softmax-entropy employed by\nTENT. This only holds, however, if the classifier we are adapting is trained\nvia cross-entropy; if trained via squared loss, a different best TTA loss\nemerges. To explain this phenomenon, we analyze TTA through the lens of the\ntraining losses's convex conjugate. We show that under natural conditions, this\n(unsupervised) conjugate function can be viewed as a good local approximation\nto the original supervised loss and indeed, it recovers the best losses found\nby meta-learning. This leads to a generic recipe that can be used to find a\ngood TTA loss for any given supervised training loss function of a general\nclass. Empirically, our approach consistently dominates other baselines over a\nwide range of benchmarks. Our approach is particularly of interest when applied\nto classifiers trained with novel loss functions, e.g., the recently-proposed\nPolyLoss, where it differs substantially from (and outperforms) an\nentropy-based loss. Further, we show that our approach can also be interpreted\nas a kind of self-training using a very specific soft label, which we refer to\nas the conjugate pseudolabel. Overall, our method provides a broad framework\nfor better understanding and improving test-time adaptation. Code is available\nat https://github.com/locuslab/tta_conjugate.",
      "full_text": "Test-Time Adaptation via Conjugate Pseudo-labels Sachin Goyal⋆1 Mingjie Sun⋆1 Aditi Raghunathan1 Zico Kolter1,2 1Carnegie Mellon University, 2Bosch Center for AI {sachingo, mingjies, raditi, zkolter}@cs.cmu.edu Abstract Test-time adaptation (TTA) refers to adapting neural networks to distribution shifts, with access to only the unlabeled test samples from the new domain at test-time. Prior TTA methods optimize over unsupervised objectives such as the entropy of model predictions in TENT [50], but it is unclear what exactly makes a good TTA loss. In this paper, we start by presenting a surprising phenomenon: if we attempt to meta-learn the “best” possible TTA loss over a wide class of functions, then we recover a function that isremarkably similar to (a temperature-scaled version of) the softmax-entropy employed by TENT. This only holds, however, if the classiﬁer we are adapting is trained via cross-entropy loss; if the classiﬁer is trained via squared loss, a different “best” TTA loss emerges. To explain this phenomenon, we analyze test-time adaptation through the lens of the training losses’sconvex conjugate. We show that under natural conditions, this (unsupervised) conjugate function can be viewed as a good local approximation to the original supervised loss and indeed, it recovers the “best” losses found by meta-learning. This leads to a generic recipe that can be used to ﬁnd a good TTA loss for any given supervised training loss function of a general class. Empirically, our approach consistently dominates other TTA alternatives over a wide range of domain adaptation benchmarks. Our approach is particularly of interest when applied to classiﬁers trained with novel loss functions, e.g., the recently-proposed PolyLoss [25] function, where it differs substantially from (and outperforms) an entropy-based loss. Further, we show that our conjugate based approach can also be interpreted as a kind of self-training using a very speciﬁc soft label, which we refer to as the conjugate pseudo-label. Overall, our method provides a broad framework for better understanding and improving test-time adaptation. Code is available at https://github.com/locuslab/ tta_conjugate. 1 Introduction Modern deep networks perform exceeding well on new test inputs that are close to the training distribution. However, this performance dramatically decreases on test inputs drawn from a different distribution. While there is a large body of work on improving the robustness of models, most robust training methods are highly specialized to the setting they cater to. For e.g., they assume pre-speciﬁed perturbations, subpopulations, and spurious correlations, or access to unlabeled data from the target distribution, and most methods offer close to no improvement on general distribution shifts beyond what they were trained for [12, 21]. In practice, it is often cumbersome (or even impossible) to precisely characterize all possible distri- bution shifts a model could encounter and then train accordingly. Instead, a model already trained on some source data must be able to adapt at test-time to new inputs from a different domain. This setting of test-time adaptation (TTA) has gained interest in recent years [ 6, 47, 50, 54]. TTA is typically accomplished by updating the source model parameters via a few steps of optimization on an unsupervised objective involving the new test sample from the target distribution. The choice ⋆ Equal Contribution 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2207.09640v2  [cs.LG]  23 Nov 2022of this unsupervised objective, which we call the TTA loss, dictates the success of the adaptation procedure. [47] uses a self-supervised objective on the test sample, [50] uses the entropy of model predictions, and several follow-ups have proposed variants or alternatives [ 40, 54]. However, it remains unclear as to how to choose or guide the selection of this TTA loss, and thus far the choice of these losses has remained largely heuristic in nature. In this work, we begin by presenting a set of intriguing experiments where we attempt to learn the “best” TTA loss for a given source classiﬁer and distribution shift. We parameterize the TTA loss by another neural network whose parameters are learnt via meta-learning [ 3, 9] where we differentiate through the adaptation process to ﬁnd the TTA loss that achieves the best adaptation on distribution shifts. Surprisingly, we ultimately learn a TTA loss that looksremarkably similar to (a temperature-scaled version of) the softmax-entropy loss, which was already proposed by [50]. Why did we recover the commonly used softmax-entropy loss despite the fact that the procedure is capable of learning a very general class of losses and the meta-learning process could potentially specialize to both the source classiﬁer and the distribution shift of interest? Furthermore, we ﬁnd that this pattern only holds when the loss used to train the source classiﬁer is cross-entropy loss; when a different loss such as squared loss is used instead, the meta-learning procedure recovers a TTA loss that itself looks more like a negative squared error, and is very different from the softmax-entropy loss (Section 3). In order to explain this phenomenon, we propose to consider TTA through the lens of the convex conjugate function. Speciﬁcally, given a hypothesis function h(x) and label y, several common losses (cross-entropy and the squared loss amongst them, but not limited to these) can be written in the form L(h(x),y) = f(h(x)) −yTh(x) for some function f. In these cases, we show that “natural” TTA loss for such classiﬁers is precisely the (negation of) the convex conjugate evaluated at the gradient of h, LTTA(x) = −f∗(∇f(h(x)), where f∗is the convex conjugate of f. This framework not only recovers the results of our meta-learning experiments, but also justiﬁes why some speciﬁc choices of TTA loss in the previous literature work well (e.g., this framework recovers TENT’s choice of softmax-entropy for cross-entropy-trained classiﬁer). Moreover, it also provides a broad framework for what the TTA loss should be when the source model is trained using various different loss functions (for example the recently-proposed PolyLoss [25, 29]) as is becoming increasingly common in machine learning. Further, we show that our proposed conjugate adaptation loss is in fact a kind of self-training with pseudo-labels [42], a classic approach in machine learning. Various formulations of the pseudo-label have been proposed in the literature, and our conjugate analysis provides a general recipe for the “correct” choice of soft pseudo-labels given byˆy(x) = ∇f(h(x)). We thus refer to these as conjugate pseudo-labels (Conjugate PL’s), and believe our work provides a broad framework for understanding adaptation with unlabeled data in general. Finally, we empirically verify the effectiveness of our proposed conjugate adaptation loss across several datasets and training losses, such as cross-entropy and squared loss, along with the recently- proposed PolyLoss [ 25] (which itself has shown higher standard test accuracy on a wide range of vision tasks). Over all models, datasets and training losses, we ﬁnd our proposed conjugate pseudo-labeling consistently outperforms prior TTA losses and improves TTA performance over the current state of the art. 2 Background and preliminaries. Test-time adaptation. We are interested in mapping an input x∈Rd to a label y∈Y. We learn a model hθ : Rd ↦→R|Y|parameterized by θthat maps an input xto predictions hθ(x). We assume access to a trained source model and adapt at test-time over the test input, before making the ﬁnal prediction. This is the standard test-time adaptation (TTA) setting [47, 50]. During TTA, we update the model parameters on an unsupervised objective L(x,hθ). For example, in TENT [50], this loss is the entropy of the softmax-normalized predictions of the model. At each time step of adaptation, we observe a batch of test inputs and we take a gradient step towards optimizing the TTA loss on this test batch. As is standard, we measure the average online performance of models across all steps (number of test batch inputs seen) in the adaptation process. Meta learning the loss function. In order to explore the existence of different TTA losses, we employ the meta-learning procedure where we attempt to learn the TTA loss. We use a similar procedure as prior work on meta-learning loss functions [3, 37] and parameterize the loss function via a neural network mφ : R|Y| ↦→R that takes in the model predictions/logits and outputs a loss value. We want to learn parameter φsuch that when we update θvia the loss function mφ, our ﬁnal 2performance is optimal. In order to do so, let xbe the unlabeled test samples to adapt to, and ybe the corresponding labels. We update θand φalternatively as follows. θt+1 ←θt −α∂mφt(hθt(x)) ∂θt , φt+1 ←φt −β∂L(hθt+1 (x′),y′) ∂φt , (1) where Lis some supervised surrogate loss function such as cross-entropy. Please refer to Appendix A3 for further details regarding meta-learning setup. Note that the meta-learning process above assumes access to labels yof test inputs. In this paper, we do not propose meta-learning the TTA loss as an approach. Rather, we use meta-learning to explore what the “best” TTA losses look like. We discuss our ﬁndings from this exploration in the next section. 3 Test-time Adaptation via Meta-Learnt Losses The objective used in TENT is the softmax-entropy of the model predictions which essentially makes the classiﬁer more conﬁdent in its current predictions. The same can be achieved by various other loss formulations such as those mentioned in [40]. With so many possible choices for the loss function, what should we use for TTA? In this section, we attempt to answer this empirically and present some intriguing observations. (a)  (b) Figure 1: Visualization of meta loss (blue) by varying one input prediction score. (a) For cross-entropy loss trained model, the learnt meta loss can be approximated with a scaled softmax-entropy function (dashed red). (b) When the source model is trained with a squared loss for classiﬁcation, the learnt meta loss (blue) can be ﬁtted closely with a quadratic function (dashed red), shown in Figure 1b. The range (max/min) of the prediction score (logit) in x-axis is chosen to cover the empirical range of the predicted logits. Experiment 1. We learn the TTA loss parameterized by a neural network via meta-learning as described in Section 2. Our source classiﬁer is a ResNet-26 trained on CIFAR-10 and we adapt to distribution shifts in CIFAR-10-C. We use the 4 labeled validation noises in CIFAR-10-C to learn the meta-loss network parameters and we denote the resulting learnt loss function by meta-TTA loss. We then adapt the source classiﬁer to the test set of 15 corruptions by optimizing the meta-TTA loss. Observations. First, we ﬁnd that TTA using meta-TTA loss performs better than TENT (12.35% vs 13.14%), suggesting that there are better TTA losses than previous losses based on softmax-entropy. However, on examining this meta-TTA loss, we ﬁnd a surprising observation. Figure 1a (blue curve) visualizes the learnt meta-loss over model predictions as we vary a single class prediction with the rest ﬁxed. Qualitatively, the learnt meta-loss looks very similar to softmax-entropy in one dimension. In fact, we can ﬁt it closely with a scaled softmax-entropy function (dashed red curve): α·H(softmax(hθ(x)/T)), where αis a magnitude parameter and T is a temperature scaler. We want to test if the meta-loss is basically learning the softmax-entropy function. Hence, we perform test-time adaptation with the ﬁtted softmax-entropy function instead (dashed red curve) and achieve an error of 12.32%, essentially recovering the performance of meta-TTA. 3Despite the ability to represent many different loss functions and potentially specialize to the CIFAR- 10-C setting, the meta-loss procedure gave back the standard entropy objective.Do we always recover a loss that looks like softmax-entropy? Experiment 2. In an attempt to isolate when we get back the entropy objective, we vary several things. We tried different architectures for the source classiﬁer, different lossesLduring the meta- learning process (1) and different training losses for the source classiﬁer. Results. We observed that we consistently recovered the temperature scaled softmax-entropy function in all cases except when we varied the training loss for the source classiﬁer (Appendix A.10). On using the squared loss function [18], a strikingly different meta-TTA loss emerges. Figure 1b (blue curve) shows the learnt meta-loss (13.48% error) for this network. Here again, the meta-TTA loss outperforms entropy (14.57%) but it is not simply due to a scaling factor. The loss now looks like the negative squared error (red curve). Like previously, we tried ﬁtting a quadratic loss directly to the meta loss in Figure 1b, and this time we even slightly outperformed the meta-TTA loss. To summarize, we used a meta-learning procedure to search for the “best” TTA loss, where the loss itself was parameterized by a neural network that could potentially represent arbitrarily complex loss functions. However, we ended up with loss functions displaying remarkable structure: across different architectures and different variants of meta-learning, for a classiﬁer trained with cross-entropy, the meta-TTA loss was temperature scaled softmax-entropy and for a classiﬁer trained with squared loss, the meta-TTA loss was a negative squared loss. This is interesting from both a practical and conceptual standpoint where the “best” TTA loss depends on the loss used to train the source classiﬁer in a clean fashion. We attempt to understand and explain this phenomenon in the next section. 4 Conjugate Pseudo Labels Results in the previous section raise an obvious question: why does softmax-entropy as used in TENT seem to be the “best” possible test time adaptation loss for classiﬁers trained via cross-entropy (at least, best in the sense that meta-learning consistently recovers something which essentially mimics softmax-entropy, even though meta-loss is parameterized by a neural network and hence could learn much more complex functions speciﬁc to the model and the particular shift)? And why, alternatively, does a quadratic TTA loss seem to perform best when the classiﬁer is trained via squared loss? In this section, we offer an explanation of this phenomenon via the construct of the convex conjugate function [1]. As we will see, our method recovers softmax-entropy and quadratic loss as the “natural” objectives for classiﬁers trained via cross-entropy and squared loss respectively. Furthermore, for classiﬁers trained via other loss functions, as is becoming increasingly common in deep learning, our approach naturally suggests corresponding test-time adaptation losses, which we show in the next section to comparatively outperform alternatives. Thus, we argue that our framework overall provides a compelling recipe for specifying the “correct” method for TTA for a large class of possible losses. 4.1 Losses and the convex conjugate We begin by formally considering loss functions between a hypothesis outputhθ(x) (e.g., the logit outputs of a classiﬁer, or the direct prediction of a regressor) and targetythat take the following form L(hθ(x),y) = f(hθ(x)) −yThθ(x) (2) for some function f; when there is no risk of confusion, we will use hin place of hθ(x) for simplicity of notation. While not every loss can be expressed in such a form, this captures a wide variety of common losses (possibly scaled by a constant value). For example, cross-entropy loss corresponds to the choice f(h) = log ∑ iexp(hi) and where y denotes a one-hot encoding of the class label; similarly, squared loss corresponds to the choice f(h) = 1 2 ∥h∥2 2. When training an over-parameterized classiﬁer, we can roughly view the training process as (approxi- mately) attaining the minimum over hypotheses hfor each training example min θ 1 t t∑ i=1 L(hθ(xi),yi) ≈1 t t∑ i=1 min h L(h,yi) (3) 4where t is the number of training samples. However, in the case of losses in the form (2), the minimization over hin this form represents a very speciﬁc and well-known optimization problem: it is known as the convex conjugate [1] of the function f min h L(h,y) = min h {f(h) −yTh}= −f⋆(y) (4) where f⋆ denotes the convex conjugate of f. f⋆ is a convex function in y(and indeed, is convex regardless of whether or not f is convex). Furthermore, for the case that f is convex differentiable, the optimality condition of this minimization problem is given by ∇f(hopt) = y, so we also have that f⋆(y) = f⋆(∇f(hopt)) (5) where hopt refers to the optimal classiﬁer (used interchangeably with hθopt ). Putting this all together, we can state (admittedly, in a rather informal manner) that under the assumption that θopt is chosen so as to approximately minimize the empirical loss on the source data in the over-parameterized setting, we have that for tinputs 1 t t∑ i=1 L(hθopt (xi),yi) ≈1 t t∑ i=1 −f⋆(∇f(hθopt (xi))) (6) i.e., the empirical loss can be approximated by the (negative) conjugate applied to the gradient of the f, at least in a region close to the optimal θopt that minimizes the empirical loss. But the later expression has the notable beneﬁt that it does not require any label yi in order to compute the loss, and thus can be used as a basis for TTA on target domain of the hypothesis function hθopt . Deﬁnition 1 (conjugate adaptation loss) Consider a loss function that takes the form given in 2, used for training a hypothesis hθ in the over-parameterized regime. We deﬁne the conjugate adaptation loss Lconj(hθ(x)) : R|Y|↦→R as follows. Lconj(hθ(x)) = −f⋆(∇f(hθ(x))) = f(hθ(x)) −∇f(hθ(x))⊤hθ(x). (7) 4.2 Recovery of existing test-time adaptation strategies Cross-entropy The interesting aspect to this formalism is that when applied to classiﬁers trained with cross-entropy, it recovers exactly the TENT approach to TTA : minimizing the softmax-entropy of hθ(x). And indeed, this loss was also recovered when using meta-learning to learn the “optimal” test-time adaptation loss. To see this, note that for cross-entropy, we have thatf(h) = log ∑ iexp(hi), giving the optimality condition y= ∇f(hopt) = exp(hopt)∑ iexp(hopt i ) and the conjugate function f⋆(y) = { ∑ iyilog yi if ∑ iyi = 1 ∞ otherwise . (8) In other words, Lconj(hθ(x)) = −f⋆(∇f(hθ(x))) = − ∑ i exp(hi)∑ jexp(hj) log exp(hi)∑ jexp(hj) (9) i.e. softmax-entropy of the model prediction, which is exactly the TTA loss that TENT uses. Squared loss For the squared loss, we have thatf(h) = 1 2 ∥h∥2 2, leading to the optimality condition y = hand conjugate function f⋆(y) = 1 2 ∥y∥2 2. Hence, the adaptation loss in this case would be simply given by Lconj(hθ(x)) = −f⋆(∇f(hθ(x))) = −1 2 ∥h∥2 2 which is also what we observed in the meta-learning experiments discussed in Section 3. 4.3 Conjugate pseudo-labels We now emphasize that by the nature of our approximations, there is an additional simple interpre- tation of the conjugate loss: it is also equal to the original loss (2) applied to the “psuedo-labels” ˜yCPL θ (x) = ∇f(hθ(x)), where CPL refers to conjugate pseudo-labels, i.e., Lconj(hθ(x)) = −f⋆(∇f(hθ(x))) = f(hθ(x)) −∇f(hθ(x))Thθ(x) = L(hθ(x),∇f(hθ(x))). (10) 5This property is known as the Fenchel-Young inequality, that isf(x) + f⋆(u) ≥xTuholding with equality when u = ∇f(x). In other words, our conjugate adaptation loss is precisely equivalent to self-training under the speciﬁc soft pseudo-labels given by ˜yCPL = ∇f(hθ(x)). And indeed, for many cases, this may be a more convenient form to compute than explicitly computing the conjugate function at all. For this reason, we refer to our method as that of conjugate pseudo-labels. In the case of cross-entropy loss, this approach then corresponds exactly to self-training using labels given by the softmax applied to the current hypothesis. We must emphasize, however, that while our conjugate formulation indeed has this “simple” form for the case of cross-entropy loss, the real advantage comes in that it provides the “correct”pseudo-label for use with other losses, which may result in pseudo-labels different from the “common” softmax operation. Example: conjugate pseudo-labels for PolyLoss. PolyLoss [25] is a recently-proposed simple alternative to cross-entropy loss than has been shown to improve performance across a wide variety of compute tasks. This loss is given by the form Lpoly(hθ(x),y) = Lce(hθ(x),y) + ϵ·yT(1 −softmax(hθ(x))) (11) We note that this can be put exactly into our conjugate form (equation 2) by writing the loss in a slightly more involved fashion, which we refer to as the expanded conjugate form Lpoly(hθ(x),y) = f(hθ(x)) −yTg(hθ(x)). (12) where f is the log-sum-exp function as before, and g(h) = h−ϵ(1 −softmax(h)). In order to formally put this into the form of the previous loss function (equation 2), we can simply deﬁne an alternative hypothesis as the function h′ θ(x) = g(hθ(x)), and then deﬁne PolyLoss in the conjugate form as Lpoly(h′ θ(x),y) = f(g−1(h′ θ(x))) −yTh′ θ(x). (13) Typically, however, it is easier to simply operate on the expanded conjugate form, which yields the optimality condition for the pseudo-label ∇f(hopt) = Dg(hopt)˜yCPL θ (x), where D is the Jacobian operator. For the case of PolyLoss, this leads to the conjugate pseudo-label of the following form: ˜yCPL θ (x) = (I+ ϵdiag(z) −ϵzzT)−1z, z ≡softmax(hθ(x)). Test-time adaptation. Finally, we note that the above discussion doesn’t actually address any topics related to test-time adaptation to OOD data, but merely provides a generic characterization of a self- training procedure for generic loss functions of the form(2). However, the application toTTA on OOD data is fairly straightforward: as long as the learnt source parameters θis a reasonable approximation to the true optimal θopt on the shifted domain, self-training with the conjugate pseudo-labels provides a reasonable proxy for ﬁne-tuning the network on the true OOD loss. We emphasize that, common to most approaches for TTA , there are still some amount of design decisions that must be put in place; these are detailed in Section 5.1. In practice, we observe OOD generalization typically beneﬁts (across all baselines) from an additional “temperature” scaling, i.e., applying the TTA loss to hθ(x)/T for some ﬁxed temperature T, although it requires a held-out validation dataset for tuningT. However, we should emphasize that truly unsupervisedTTA would require making an informed guess for the value of these hyper-parameters. The full procedure for test time adaptation via conjugate pseudo-labels is shown in Algorithm 1. Algorithm 1 Conjugate pseudo-labeling (Conjugate PL) Input: Source classiﬁer θ0 trained using loss L(hθ(x),y) = f(hθ(x)) −hθ(x)⊤y. N batches of test data Dtest = [x1,x2,...,x N] Hyperparams: learning rate ηand temperature T. Let ¯hθ(x) def = hθ(x)/T be the temperature scaled predictor. Let ˜yCPL θ (x) denote the conjugate pseudo-label function ˜yCPL θ (x) = ∇(f(¯hθ(x))). for n= 0,1,...N −1 do θn+1 = θn −η∇L ( ¯hθ(xn),˜yCPL θ (xn) ) [Self-training with conjugate pseudo-labels] 65 Experiments In this section, we empirically evaluate the effectiveness and generality of the proposed conjugate pseudo-labeling procedure (Algorithm 1) for test-time adaptation on a variety of datasets. 5.1 Setup Datasets. We evaluate on the three common corruption benchmarks: adapting a classiﬁer trained on CIFAR-10 to CIFAR-10-C, CIFAR-100 to CIFAR-100-C and ImageNet to ImageNet-C [ 15]. Following the previous works [47, 50], we report the error averaged across corruptions at the highest severity for CIFAR-10/100-C and averaged across corruptions and severity level for ImageNet-C. We also evaluate on three domain adaptation datasets: adapting a classiﬁer trained on SVHN to MNIST, an ImageNet classiﬁer to ImageNet-R [16] and adapting from synthetic to real data in VISDA-C [38]. Models and Training losses. Following previous works on TTA[47, 50], we use ResNet-26 [14] as the source classiﬁer architecture for CIFAR-10/100 experiments, ResNet-18 for SVHN to MNIST and a ResNet-50 for ImageNet and source synthetic data on VisDA-C. We consider source classiﬁers trained via the following loss functions: the de-facto cross-entropy, recently proposed polyloss [25] and squared loss [18]. Baselines. Our proposed conjugate pseudo-label is the classic approach of self-training with a speciﬁc form of pseudo-labels. In self-training, we replace the label ywith a pseudo-label ˜y(x) and adapt by optimizing the loss function L(hθ(x),˜y(x)). Note that we could either instantaneously update the pseudo-labels using the current classiﬁer, or generate pseudo-labels once with just the source classiﬁer. Instantaneous updates have been shown to work better for domain adaptation [7, 40], and we perform instantaneous updates for all methods. While we propose using ˜yCPL(x) = ∇f(hθ(x)) (See Section 4.3), we compare to the standard pseudo-labels used in the literature: • (i) the “hard” pseudo-label (hard PL) where ˜y(x) = arg maxi ( hθ(x) ) i is the most likely class as predicted by hθ. As is common in the self-training literature, we perform conﬁdence thresholding. • (ii) The “soft” pseudo-label (soft PL) where ˜y(x) is obtained by applying a softmax function to the model predictions hθ(x). We also compare with the following recently proposed test-time adaptation methods. • Entropy Minimization (ENT) [50] minimizes the entropy of model predictions. • Robust Pseudo-Label [40] where we minimize a robust classiﬁcation loss, Lrpl = q−1(1 −p(i|x)q) where i= argmaxjp(j|x) and q∈[0,1]. • MEMO [54] minimizes entropy of a model’s outputs across different augmentations of a test input. We implement a batch version, where we see multiple test points at once, for fair comparisons. TTA methodology. Following [ 50] and [40], we ﬁne-tune by updating the learnable scale and shift parameters of the batch normalization layers across all adaptation losses. For each batch, batch normalization statistics is also updated, as suggested in [41]. We report performance at the end of one round of test-time adaptation over the entire test set. We tune the learning rate (LR) and temperature (T) on the validation noises in the corruption benchmark by grid-search. LR is selected from {1e−1,1e−2,... 1e−4}and T from {1,2 ... 5}. All the experiments have been performed on A6000 GPU’s. On domain adaptation benchmarks, where there is no held-out target domain, we set T to be 1 and use the LR suggested by [ 6, 50]. We use the same hyperparameter tuning protocol across all methods. We single out temperature as a very important hyperparameter, as we discuss in the results below. 5.2 Results on classiﬁers trained with cross-entropy We study the effectiveness of our proposed conjugate pseudo-labels when the source classiﬁer is trained via cross-entropy loss. In this case, baselines Softmax PL and ENT are the same as Conjugate PL. Thus we omit them in our results. Table 1, reports the performance of various TTA methods. When the source classiﬁer is trained via cross-entropy, our conjugate pseudo-label algorithm exactly corresponds to entropy minimization with an additional temperature scaling. Entropy minimization as 7Dataset Temperature (T) Hard PL Robust PL MEMO Conjugate PL (ENT) CIFAR-10-C \u0017 13.95 (±0.06) 13.97 ( ±0.04) 12.60(±0.04) 13.07 (±0.05) \u0013 13.95 (±0.06) 12.85 ( ±0.04) 12.51(±0.01) 12.51(±0.03) CIFAR-100-C \u0017 45.22 (±0.4) 39.80 ( ±0.18) 38.52(±0.16) 41.15 (±0.25) \u0013 45.22 (±0.4) 36.37 ( ±0.10) 37.38 ( ±0.06) 36.10(±0.07) ImageNet-C \u0017 45.43(±0.05) 45.68 ( ±0.01) 48.91( ±0.03) 45.82(±0.01) \u0013 45.43 (±0.05) 45.61 ( ±0.01) 48.91( ±0.04) 45.36(±0.01) Table 1: Mean errors when adapting to corruptions using a source classiﬁer trained via cross- entropy loss. Here, conjugate pseudo-labeling becomes softmax-entropy minimization. With the right temperature scaling, softmax-entropy minimization matches or outperforms other approaches. Prior reported gains of other methods over softmax-entropy minimization disappear when we use temperature scaling. For additional context, the source classiﬁer errors without adaptation are: CIFAR-10-C (29.54%), CIFAR-100-C (62.26%), ImageNet-C (61.89%) proposed in prior work [50] does not tune the temperature parameter, and some newer objectives such as robust PL or MEMO outperform vanilla entropy minimization. For example, on CIFAR-100-C, vanilla ENT obtaines 41.15% average error, while robust PL improves this to39.80% and MEMO to 38.52%. However, with the right temperature scaling, entropy minimization obtains 36.10% error which outperforms the newer objectives (with and without temperature scaling). A similar observation holds for CIFAR-10-C and ImageNet-C as well. Essentially, the gains over vanilla entropy minimization vanish when we do temperature scaling, and entropy minimization (i.e. conjugate pseudo-labeling corresponding to cross-entropy) turns out to be the best objective after all. 5.3 Results on classiﬁers trained with polyloss and squared loss In the case of cross-entropy, conjugate pseudo-labeling reduces to the familiar notion of entropy minimization. We now explore the performance of our method on different loss functions where the conjugate pseudo-labels differ substantially from entropy minimization (section 4.3). Table 2 presents the results on the corruption benchmarks and Table 3 presents the results on the other domain adaptation datasets for source classiﬁers trained with PolyLoss. Dataset T Hard PL Robust PL ENT MEMO Softmax PL Conjugate PL (Ours) CIFAR-10-C \u0017 13.81(±0.12) 14.23(±0.02) 13.46(±0.06) 13.23(±0.07) 14.64(±0.11) 13.02(±0.09) \u0013 13.81(±0.12) 12.45(±0.05) 12.23(±0.06) 12.33(±0.04) 12.26(±0.04) 12.08(±0.05) CIFAR-100-C\u0017 40.47(±0.05) 42.86(±0.11) 40.12(±0.08) 39.90(±0.05) 41.00(±0.11) 38.17(±0.17) \u0013 40.47(±0.05) 39.80(±0.08) 38.23(±0.05) 39.23(±0.04) 37.04(±0.06) 36.83(±0.08) ImageNet-C \u0017 45.44(±0.21) 46.27(±0.03) 46.10(±0.03) 48.21(±0.05) 44.63(±0.03) 44.01(±0.01) \u0013 45.44(±0.21) 46.27(±0.03) 45.50(±0.02) 48.21(±0.04) 44.45(±0.03) 44.01(±0.01) Table 2: Mean errors when adapting to corruptions using a source classiﬁer trained via recently proposed Poly-1 Loss [ 25]. Conjugate pseudo-labeling consistently outperforms all previous ap- proaches. For additional context, source classiﬁer errors without adaptation : CIFAR-10-C (30.22%), CIFAR-100-C (63.91%) and ImageNet-C (62.18%). First, we note that, across all datasets in Table 2 and Table 3, our conjugate PL approach outperforms all other TTA losses. With polyloss classiﬁers, entropy minimization is no longer the best method—on CIFAR-100-C, entropy minimization achieves38.23% error while our conjugate PL achieves36.83%. We see similar consistent gains on CIFAR-10-C, ImageNet-C, ImageNet-R and VisDA-C. On digit adaptation tasks from SVHN to MNIST/USPS/MNISTM, where there is a larger shift between source and target, the gains are especially pronounced. Figure 2 compares how the task loss (polyloss ϵ= 6) on the test data decreases as we adapt the model through conjugate PL and other baselines. We use CIFAR-10-C as an example. Observe that our proposed conjugate PL indeed reduces the task loss the most among other baselines. 8Dataset Source Error Hard PL Robust PL EntropySoftmax PL Conjugate PL Ours SVHN→MNIST 28.33 20.21 19.73 14.28 16.54 10.73 SVHN→USPS 31.58 23.32 26.12 23.12 24.07 21.62 SVHN→MNISTM61.69 50.73 51.35 49.33 50.47 47.59 ImageNet-R 64.19 58.52 59.46 58.25 56.62 55.63 VisDA-C 58.13 40.43 45.44 44.11 39.63 38.42 Table 3: Target error when adapting models trained via polyloss on source domains across different domain adaptation bench- marks. Conjugate pseudo-labeling offers consistent and substan- tial gains over previous approaches across three datasets. Figure 2: Task Loss (PolyLoss ϵ= 6) evaluated on CIFAR-10-C test data during test-time adaptation. Furthermore, on CIFAR-10-C and ImageNet-C, we ﬁnd that adapting polyloss classiﬁers via conjugate PL improves the performance over all methods applied to cross-entropy trained source classiﬁers. For e.g., on ImageNet-C, the performance improves from 45.34% to 44.01%. However, this is only true when using the proposed conjugate PL. If we just did softmax-entropy minimization (even with temperature scaling), the ﬁnal adapted performance of a polyloss classiﬁer (45.5%) is in fact worse than that of a cross-entropy classiﬁer (45.34%). Our results suggest that as we develop new training losses that improve the source classiﬁers, it is important to adapt via conjugate pseudo-labeling to reap the maximum gains. Similarly, we experiment with the case when the source classiﬁer is trained using squared loss on the CIFAR-10 and CIFAR-100 datasets, and observe consistent gains using the proposed conjugate pseudo-labels over the baselines. For example, on CIFAR-10-C, TTA using conjugate PL gives and error of 12.87%, outperforming baselines like ENT (13.24%) and Softmax PL (31.81%). Table 5 in Appendix A.7 shows the detailed results. Comparing Table 1 and Table 2, we see that the relative ordering between the various baselines differs. This is further evidence that the adaptation loss has to depend on the training loss, and we believe our conjugate pseudo-label approach captures this appropriately by offering consistent gains across the various settings we experimented with. 6 Related Works Test-time adaptation methods. In recent years, the setting of test-time adaptation has gained a lot of interest with a host of different approaches proposed in the literature. One family of TTA approaches update the source classiﬁer by minimizing an unsupervised loss on the target distribution [4, 6, 20, 22, 35, 36, 40, 43, 44, 50, 51, 54]. TENT [ 50] proposes to minimize the entropy of model predictions at test time. Several follow ups like [ 6, 35, 40, 44, 54] propose alternative TTA objectives, e.g. robust pseudo-labelling [40], likelihood ratio loss [35], entropy of marginal probability averaged across augmentations [54] and self-supervised contrastive losses [6, 49]. However, most of these objectives are heuristically designed or chosen. In this paper, we provide a principled approach of designing unsupervised objectives for TTA . Another family of approaches for test-time adaptation such as [ 2, 8, 13, 31, 34, 47] leverage an auxiliary self-supervised task (e.g. rotation prediction [ 47], masked autoencoders [10]) to update model parameters on each test sample. Crucially, these methods require modifying the source model training by augmenting the supervised training objective with an auxiliary self-supervised loss. Hence it cannot be applied to typical standard classiﬁers that are trained by minimizing a supervised loss on the source data. Source-free domain adaptation. A very related setting to test-time adaptation is source-free domain adaptation, where a trained source classiﬁer must be adapted to a target distribution of interest, although the entire target unlabeled data is available at once. SHOT [28] proposes to optimize the source hypothesis (i.e. feature extractor) with a combination of entropy minimization, diversity and self-training on pseudo-labels on the unlabeled target data. [53] promotes feature clustering on features from target distributions. [24, 26] use generative modeling to estimate the underlying source distributions for enforcing feature invariance. Such approaches typically require multiple epochs over the target data and cannot be easily adopted to work in an online fashion. 9Unsupervised domain adaptation. The most canonical setting of domain adaptation involves access to labeled source data and unlabeled target data, all during training. The availability of source and target data during training lends itself to approaches that “align” the source and target representations in some way: [ 32, 33, 45, 48] match distribution statistics, [ 11] uses a discriminator, [ 46] uses self-supervised learning. However, such approaches require access to source data which might not always be feasible due to data privacy and efﬁciency issues. Pseudo-labels and self-training. Self-training is a classic idea for leveraging unlabeled data, devel- oped ﬁrst for the semi-supervised setting. Self-training generates pseudo-labels on the unlabeled data, allowing us to use any “supervised” loss on this pseudo-labeled data. Self-training has shown promising results in various settings like semi-supervised learning [ 19] and improving adversarial robustness [ 5]. Self-training has also been gaining attention in the setting of unsupervised domain adaptation [28, 39], where pseudo-labels generated on the unlabeled data from target domain is used to supervise the adaptation process. [ 7, 23, 52] provide theoretical insights into how self-training with pseudo-labels can help under distribution shift. TENT [50] (i.e entropy minimization) can be viewed as a form of self-training with instantaneous softmax pseudo-labels. Our work provides a general framework for the choice of soft pseudo-labels based on the conjugate analysis of the source training objective. Some prior works like [7, 17, 27, 30, 55, 56] have documented the improvement in performance when using instantaneous pseudo-labels over pre-computed pseudo-labels, and thus lend further support to the beneﬁts of our proposed conjugate pseudo-labeling approach. The ex- periment results presented in this work supporting conjugate pseudo-labels suggest that conjugate pseudo-labels is a promising direction of pseudo-labeling in a broader context. 7 Conclusion, Limitations and Future Directions In this work, we proposed a general test-time adaptation loss, based on the convex conjugate formulation which in turn was motivated by the intriguing meta learning experiments. The fact that meta-learning recovers the proposed loss hints at some kind of optimality of the loss. In Section 4, we prove that for a broad set of loss functions, the proposed (unsupervised) conjugate loss is close to the oracle supervised loss. However, this still does not completely answer what the optimal test-time adaptation loss is and why. The meta-learning framework in this work was constrained to learn functions over the logits of each individual input. It can be expanded to more involved setups, where we consider functions over the intermediate representations too and also consider learning functions over a batch of input while accounting for their interactions. Beyond the choice of the adaptation loss itself, achieving good test-time adaptation generally involves several heuristics like updating only the batch norm parameters [50]. While our work was motivated by the loss function, via the meta-learning experiments, we discovered that temperature scaling is another important hyper-parameter that improves the performance of all previous baselines as well. At a high level, test-time adaptation has to be appropriately regularized to prevent the updates over batches from taking the model too far: updating only a few batch norm parameters is one way to do that, and perhaps temperature scaling provides a similar beneﬁcial regularization effect by making the network predictions on unlabeled inputs less conﬁdent. Understanding the role of these heuristics more concretely is an interesting direction for future work. It also remains an open problem to understand under what sort of real-world distribution shifts would self-training based approaches would help. Finally, it is also worth extending and applying the conjugate pseudo-labeling to other settings like semi-supervised learning. 8 Acknowledgments We thank Shubhang Bhatnagar and Asher Trockman for helping with running the ImageNet experi- ments. We thank Zhili Feng for useful feedback. Sachin Goyal and Mingjie Sun were supported by funding from the Bosch Center for Artiﬁcial Intelligence. Aditi Raghunathan was supported by an Open Philanthropy AI Fellowship. 10References [1] https://en.wikipedia.org/wiki/Convex_conjugate. [2] Pratyay Banerjee, Tejas Gokhale, and Chitta Baral. Self-supervised test-time learning for reading comprehension. In Annual Conference of the North American Chapter of the Association for Computational Linguistics, 2021. [3] Sarah Bechtle, Artem Molchanov, Yevgen Chebotar, Edward Grefenstette, Ludovic Righetti, Gaurav Sukhatme, and Franziska Meier. Meta-learning via learned loss. arXiv preprint arXiv:1906.05374, 2019. [4] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [5] Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and Percy S Liang. Un- labeled data improves adversarial robustness. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips. cc/paper/2019/file/32e0bd1497aa43e02a42f47d9d6515ad-Paper.pdf. [6] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [7] Yining Chen, Colin Wei, Ananya Kumar, and Tengyu Ma. Self-training avoids using spurious features under domain shift. In Advances in Neural Information Processing Systems, 2020. [8] Mohammad Zalbagi Darestani, Jiayu Liu, and Reinhard Heckel. Test-time training can close the natural distribution shift performance gap in deep learning based compressed sensing. In Proceedings of the 39th International Conference on Machine Learning (ICML), 2022. [9] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap- tation of deep networks. In Proceedings of the 34th International Conference on Machine Learning (ICML), 2017. [10] Yossi Gandelsaman, Yu Sun, Xinlei Chen, and Alexei A. Efros. Test-time training with masked autoencoders. In Advances in Neural Information Processing Systems, 2022. [11] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario March, and Victor Lempitsky. Domain-adversarial training of neural networks. Journal of Machine Learning Research, 17(59):1–35, 2016. [12] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. InInternational Conference on Learning Representations, 2021. [13] Nicklas Hansen, Rishabh Jangir, Yu Sun, Guillem Alenya, Pieter Abbeel, Alexei A. Efros, Lerrel Pinto, and Xiaolong Wang. Self-supervised policy adaptation during deployment. In International Conference on Learning Representations, 2021. [14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2016. [15] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In International Conference on Learning Representations, 2019. [16] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. In In IEEE/CVF International Conference on Computer Vision (ICCV), 2021. [17] Yosuke Higuchi, Niko Moritz, Jonathan Le Roux, and Takaaki Hori. Advancing momentum pseudo-labeling with conformer and initialization strategy. In IEEE International Conference on Acoustics, Speech and Signal Processing, 2022. 11[18] Like Hui and Mikhail Belkin. Evaluation of neural architectures trained with square loss vs cross-entropy in classiﬁcation tasks. In International Conference on Learning Representations, 2021. [19] Dong hyun Lee. Pseudo-label: The simple and efﬁcient semi-supervised learning method for deep neural networks. [20] Yusuke Iwasawa and Yutaka Matsuo. Test-time classiﬁer adjustment module for model-agnostic domain generalization. In Advances in Neural Information Processing Systems, 2021. [21] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton A. Earnshaw, Imran S. Haque, Sara Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. Wilds: A benchmark of in-the-wild distribution shifts. In Proceedings of the 38th International Conference on Machine Learning (ICML), 2021. [22] Takeshi Kojima, Yutaka Matsuo, and Yusuke Iwasawa. Robustifying vision transformer without retraining from scratch by test-time class-conditional feature alignment. In International Joint Conference on Artiﬁcial Intelligence, 2022. [23] Ananya Kumar, Tengyu Ma, and Percy Liang. Understanding self-training for gradual domain adaptation. In Proceedings of the 37 th International Conference on Machine Learning (ICML), 2020. [24] Vinod K Kurmi, Venkatesh K Subramanian, and Vinay P Namboodiri. Domain impression: A source data free domain adaptation method. In IEEE Winter Conference on Applications of Computer Vision (WACV), 2021. [25] Zhaoqi Leng, Mingxing Tan, Chenxi Liu, Ekin Dogus Cubuk, Jay Shi, Shuyang Cheng, and Dragomir Anguelov. Polyloss: A polynomial expansion perspective of classiﬁcation loss functions. In International Conference on Learning Representations, 2022. [26] Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, and Si Wu. Model adaptation: Unsuper- vised domain adaptation without source data. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. [27] Xinzhe Li, Qianru Sun, Yaoyao Liu, Qin Zhou, Shibao Zheng, Tat-Seng Chua, and Bernt Schiele. Learning to self-train for semi-supervised few-shot classiﬁcation. In Advances in Neural Information Processing Systems, 2019. [28] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. InProceedings of the 37th International Conference on Machine Learning (ICML), 2020. [29] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object detection. In IEEE/CVF International Conference on Computer Vision (ICCV), 2017. [30] Hong Liu, Jianmin Wang, and Mingsheng Long. Cycle self-training for domain adaptation. In Advances in Neural Information Processing Systems, 2021. [31] Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? In Advances in Neural Information Processing Systems, 2021. [32] Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, and Philip S. Yu. Transfer feature learning with joint distribution adaptation. In IEEE/CVF International Conference on Computer Vision (ICCV), 2013. [33] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I. Jordan. Learning transferable features with deep adaptation networks. In Proceedings of the 32nd International Conference on Machine Learning, 2015. [34] Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen, and Johannes Kopf. Consistent video depth estimation. In SIGGRAPH, 2020. 12[35] Chaithanya Kumar Mummadi, Robin Hutmacher, Kilian Rambach, Evgeny Levinkov, Thomas Brox, and Jan Hendrik Metzen. Test-Time Adaptation to Distribution Shift by Conﬁdence Maximization and Input Transformation. arXiv preprint arXiv: 2106.14999, 2021. [36] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efﬁcient test-time model adaptation without forgetting. In Proceedings of the 39th International Conference on Machine Learning (ICML), 2022. [37] Junhyuk Oh, Matteo Hessel, Wojciech M. Czarnecki, Zhongwen Xu, Hado P van Hasselt, Satinder Singh, and David Silver. Discovering reinforcement learning algorithms. In Advances in Neural Information Processing Systems, 2020. [38] Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko. Visda: The visual domain adaptation challenge, 2017. [39] Viraj Prabhu, Shivam Khare, Deeksha Kartik, and Judy Hoffman. Sentry: Selective entropy optimization via committee consistency for unsupervised domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. [40] Evgenia Rusak, Steffen Schneider, George Pachitariu, Luisa Eck, Peter Vincent Gehler, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. If your data distribution shifts, use self- learning, 2022. URL https://openreview.net/forum?id=1oEvY1a67c1. [41] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. In Advances in Neural Information Processing Systems, 2020. [42] H. Scudder. Probability of error of some adaptive pattern-recognition machines. IEEE Transac- tions on Information Theory, 1965. [43] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Test-time prompt tuning for zero-shot generalization in vision-language models. In Advances in Neural Information Processing Systems, 2022. [44] Prabhu Teja Sivaprasad and François Fleuret. Test time adaptation through perturbation robust- ness. arXiv preprint arXiv: 2110.10232, 2021. [45] Baochen Sun, Jiashi Feng, and Kate Saenko. Correlation alignment for unsupervised domain adaptation. arXiv preprint arXiv: 1612.01939, 2016. [46] Yu Sun, Eric Tzeng, Trevor Darrell, and Alexei A. Efros. Unsupervised domain adaptation through self-supervision. arXiv preprint arXiv:1909.11825, 2019. [47] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A. Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In Proceedings of the 36th International Conference on Machine Learning (ICML), 2019. [48] Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain confusion: Maximizing for domain invariance. arXiv preprint arXiv:1412.3474, 2014. [49] Dequan Wang, Shaoteng Liu, Sayna Ebrahimi, Evan Shelhamer, and Trevor Darrell. On-target adaptation. arXiv preprint arXiv: 2109.01087, 2021. [50] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Representations, 2021. [51] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [52] Sang Michael Xie, Ananya Kumar, Robbie Jones, Fereshte Khani, Tengyu Ma, and Percy Liang. In-n-out: Pre-training and self-training using auxiliary information for out-of-distribution robustness. In International Conference on Learning Representations, 2021. 13[53] Shiqi Yang, Yaxing Wang, Joost van de Weijer, Luis Herranz, and Shangling Jui. Generalized source-free domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. [54] Marvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and augmentation. In Advances in Neural Information Processing Systems, 2022. [55] Yang Zou, Zhiding Yu, B. V . K. Vijaya Kumar, and Jinsong Wang. Domain adaptation for semantic segmentation via class-balanced self-training. European Conference on Computer Vision, 2018. [56] Yang Zou, Zhiding Yu, Xiaofeng Liu, B. V . K. Vijaya Kumar, and Jinsong Wang. Conﬁdence regularized self-training. In IEEE/CVF International Conference on Computer Vision (ICCV), 2019. 14A Appendix A.1 Conjugate Derivations Cross-Entropy Loss : L(h,y) = − c∑ i=1 yilog exp(hi)∑c j=1 exp(hj) = − c∑ i=1 yi ∗hi + log c∑ j=1 exp(hj) = f(h) −y⊤h, (14) where f(h) is log ∑c j=1 exp(hj) and the constraint that ∑c i=1 yi = 1. Now, the conjugate f⋆(y) is given by : f⋆(y) = −min h {f(h) −yTh}= −min h {log c∑ j=1 exp(hj) −yTh} (15) with the constraint ∑c i=1 yi = 1. At the optimality, yi = (∇f(h))i = exp(hi)∑ jexp(hj) (16) Then, f⋆(y) = −log c∑ j=1 exp(hj) + c∑ i=1 hi exp(hi)∑ jexp(hj) = ∑ i exp(hi)∑ jexp(hj) log exp(hi)∑ jexp(hj), (17) if the constraint ∑c i=1 yi = 1 is satisﬁed, otherwise f⋆(y) = ∞by duality. This in turn gives, the conjugate loss for cross-entropy (when the constraint is satisﬁed) : Lconj(h) = −f⋆(y) = −f⋆(∇f(h)) = − ∑ i exp(hi)∑ jexp(hj) log exp(hi)∑ jexp(hj) (18) Squared Loss : L(h,y) = 1 2||h−y||2 2 ≈1 2||h||2 2 −y⊤h [ignoring the constant term] = f(h) −y⊤h, (19) Now, the conjugate f⋆(y) is given by: f⋆(y) = −min h {f(h) −yTh}= −min h {1 2||h||2 2 −yTh} = −1 2||h||2 2 (20) A.2 Experiments on Binary Classiﬁcation with Exponential Loss Here we present the results on a binary classiﬁcation task over a synthetic dataset of 100 dimensional gaussian clusters. 15Dataset Creation For the binary classiﬁcation task, we create a synthetic dataset similar to [23]. Speciﬁcally, let the data X ∼ N(µ,Σ) ∈ R100 and labels Y ∈ {−1,+1}. We sample µ ∼ N(k,I100). For Σ, similar to [ 23], we sample a diagonal matrix D, where each entry is sampled uniformly from a speciﬁed range, and a rotation matrix U from a HAAR distribution, giving Σ = UDUT. For the source data, we sample µ−1 s ,µ+1 s ,Σ−1 s ,Σ+1 s as speciﬁed above with k= 0. Now to create a distribution shifted data of various severity, we sampleµ−1 t ,µ+1 t ,Σ−1 t ,Σ+1 t as speciﬁed above with k= 1, which are then used to sample the shifted data as follows : µ1 λ = λµ1 t + (1 −λ)µ1 s µ−1 λ = λµ−1 t + (1 −λ)µ−1 s Σ1 λ = λΣ1 t + (1 −λ)Σ1 s Σ−1 λ = λΣ−1 t + (1 −λ)Σ−1 s Xλ ∼N(µλ,Σλ) In the following experiments, easy shift refers to λ= 0.6, moderate shift to λ= 0.65 and hard shift to λ= 0.7. Exponential Loss for Binary Classiﬁcation Let zbe the classiﬁcation score hθ(x). For logistic training loss, conjugate adaptation loss would default to entropy with sigmoid probability. Thus, here we experiment with a different but also commonly used surrogate loss to 0/1 loss: exponential loss, which is deﬁned as: Lexp(z,y) = exp(−yz) (21) where y∈{−1,+1}. It can be rewritten in the expanded conjugate form of: Lexp(z,y) = 1 2 · ( ez + e−z) −1 2 ·y· ( ez −e−z) (22) For exponential loss, the conjugate pseudo-label function and the conjugate pseudo-label loss are: yCPL exp (z) = ez −e−z ez + e−z, LCPL exp (z) = 2 ez + e−z (23) The model is adapted on shifted gaussian clusters and we compare the conjugate loss with two baseline approaches: 1) Hard pseudo-labelling exp(−yhard pl ·z); 2) Entropy applied to sigmoid probability P(y= +1) = σ(z). The losses are compared on three degrees of shift (easy, moderate and hard), which is controlled by the drifted distance of Gaussian clusters. The results are shown in Figure 3, where we plot the accuracy curve with respect to adaptation iterations. With easy and moderate shift, conjugate loss (green) generalizes faster to shifted test data; with hard shift, only conjugate loss improves model accuracy on shifted test data while entropy (blue) deteriorates model performance. Figure 3: Test-time adaptation result on synthetic data with three shift levels ranging from easy, moderate and hard (detailed in section A.2). The source model is a linear classiﬁer trained with exponential loss Lexp = e−yhθ(x). Adaptation with the conjugate loss generalizes better compared to baseline losses. 16A.3 Meta Learning Experiment Details In section 3 we talked about learning the meta-loss function parameterized by a neural network mφ : R|Y|↦→R, that takes in the model predictions/logits and outputs a loss value. Here we discuss the architecture chosen and the implementation details. Further, in Appendix A.4 we empirically show that the learnt meta-loss is not affected by the choice of task loss / surrogate loss used in meta learning (Lin Equation 1). Note that the task loss / surrogate loss function is used to update the meta-loss mφ during meta-learning. The surrogate loss is calculated on updated source model’s predictions on labeled samples from test domain. The surrogate loss tries to update the meta-loss in the outer loop such that when meta-loss is later used to update the source model in the inner loop, the source model generalizes better to the test domain. Architecture and Implementation Details Figure 4 gives an overall schema for meta-learning the loss function and algorithm 2 gives the pseudo-code for meta-learning the loss function. Below we describe this in further detail. We use a transformer (denoted by T) with a MLP (denoted by P) over the output of transformer as the architecture for mφ, i.e. mφ(x) = P(T(x)). Speciﬁcally, for a given source trained model hθ and input x∼Dtest : 1. Let hθ(x) ∈R|Y|be the model predictions/logits, where |Y|denotes the number of classes. 2. Let hj θ(x) ∈R,∀j ∈|Y| be the prediction corresponding to class j. 3. The input to transformer is then given by z ∈R|Y|×(1+e), where zj ∈R1+e,∀j ∈|Y| is the concatenation of hj θ(x) and the learnable positional embedding pej ∈Re. 4. The transformer output is given by w= T(z) ∈Rd, where ddenotes the feed-forward dimension of the transformer. 5. The transformer output wis ﬁnally passed through a MLP to get the meta-loss valuemφ(hθ(x)) = P(w) ∈R 6. The source model is updated by optimizing over the meta-loss. θt+1 ←θt −α∂mφt(hθt(x)) ∂θt (24) 7. The updated source model is then used to update the meta-loss by optimizing over some supervised loss function Ltask. φt+1 ←φt −β∂Ltask(hθt+1 (x′),y′) ∂φt , where (x′,y′) ∼Dtest (25) Note that the last step assumes access to labels of test inputs. In this paper, we do not propose meta-learning the TTA loss as an approach. Rather, we use meta-learning to explore what the “best” TTA losses look like. We select the trasformer input embedding dimension (1 + e) from {16,32,64}and transformer feed-forward dimension dfrom {32,64,128}. The number of transformer layers and the hidden layers in MLP are selected from {1,2}. We use Adam optimizer with a learning rate of 1e−3 for learning the meta-loss (i.e. the transformer + MLP). We train the meta-loss for 100 epochs with a batch size of 200. A.4 Effect of Task Loss in Meta Learning In section 3, we show that the meta losses learned on different source classiﬁers differ substantially if the source classiﬁers are trained using different source loss functions. Here we further empirically verify that the learnt meta loss is not affected by the task loss used in meta learning (Lin Equation 1). Thus the learnt meta loss is determined by the source model. In Figure 5, we show the meta loss learnt on a ResNet-26 trained with Cross Entropy loss for two meta task losses: Cross Entropy Figure 5a and Squared Loss Figure 5b. We plot the meta loss as a function over one of its input prediction scores, while keeping other ﬁxed. We can see that the task loss barely affects the learnt meta loss. Similar observations can be made for the classiﬁer trained with squared loss Figure 6. 17Meta-Loss  Backpropogate  Figure 4: Meta-Loss learning procedure : The model predictions hθt(x) are passed through the parameterized loss function mφt, which outputs a loss value. We optimize φ such that when optimizing the source model over the loss mφt(hθt(x)), the updated θt+1 has a better performance on the test domain. To do this, we take one gradient step over the meta-loss to get the update source model parameters θt+1, and then update φby evaluating θt+1 on the labeled validation data using some task loss Ltask. Algorithm 2 Learning the Meta-Loss Input: Source trained classiﬁer hθ0 . Randomly initialized meta-loss mφ0 . Task loss / Surrogate loss Ltask like cross-entropy or squared loss for meta learning N batches of test data Dtest = [(x1,y1),..., (xN,yN)] Hyperparams: learning rates αand β. for epoch= 0,1,2,... do for n= 0,1,...N −1 do θt+1 ←θt −α ∂mφt(hθt(xn)) ∂θt Sample (xr,yr) ∼Dtest. φt+1 ←φt −β∂Ltask(hθt+1 (xr),yr) ∂φt A.5 Test-Time Adaptation Detail For completeness, we also give the test-time adaptation setup in Algorithm 3. A.6 ImageNet results on each severity level In continuation with results shown in Table 2 in Section 5.3, Table 4 shows the mean errors averaged across the 15 corruption types for each of the severity level on ImageNet-C, for a source classiﬁer trained with PolyLoss (ϵ= 8). A.7 Square Loss Trained Source Classiﬁer In Section 5.3, we brieﬂy discussed that similar to the other source training losses like cross-entropy and polyloss, our proposed conjugate loss outperforms the baselines when the source classiﬁer is 18(a)  (b) Figure 5: Visualizations of meta loss by varying one input dimension (prediction score). The source model is a ResNet-26 trained with Cross Entropy. Here we show meta loss trained by two different task losses: Cross Entropy Figure 5a and Squared Loss Figure 5b. (a)  (b) Figure 6: Visualizations of meta loss by varying one input dimension (prediction score). The source model is a ResNet-26 trained with Squared Loss. Here we show meta loss trained by two different task losses: Cross Entropy Figure 6a and Squared Loss Figure 6b. Algorithm 3 Test-Time Adaptation Input: Source classiﬁer θ0 trained using loss L(hθ(x),y), An unsupervised loss function for test-time adaptation Ltta(x), N batches of test data Dtest = [x1,...,x N] Hyperparams: learning rate η. for n= 0,1,...N −1 do θn+1 = θn −η∇Ltta(xn) ˆyn = hθn+1 (xn) [Predictions for the nth batch] 19Corrution Severity Temperature Robust PL Entropy MEMO Softmax PL Conjugate 1 \u0017 34.27 33.17 34.39 32.49 32.26 \u0013 34.27 32.84 34.39 32.70 32.26 2 \u0017 41.25 39.04 40.38 37.78 37.40 \u0013 41.25 38.50 40.38 37.75 37.40 3 \u0017 47.37 44.04 45.67 42.30 41.72 \u0013 47.37 43.33 45.67 42.14 41.72 4 \u0017 56.63 51.88 54.49 49.61 48.84 \u0013 56.63 51.03 54.49 49.39 48.84 5 \u0017 67.11 62.53 66.13 60.94 59.90 \u0013 67.11 61.80 66.13 60.30 59.90 Mean \u0017 49.32 46.13 48.21 44.62 44.02 \u0013 49.32 45.50 48.21 44.45 44.02 Table 4: Mean Errors across the 15 noises for various severity level on the ImageNet-C dataset, with source model trained using Poly-1 Loss. Note that Temperature scaling helped only in the case of Entropy and Softmax PL. trained using a squared loss. Table 5 shows a detailed comparison with the baselines. We note that for the conjugate of squared loss, the temperature scaling can be wrapped into the learning rate as shown in Section 4.2. Further, on the CIFAR-10-C dataset we observe temperature scaling doesn’t help any of the other baselines too, hence we do not include the temperature row in CIFAR-10-C. Dataset Temperature Hard PL Robust PL ENT MEMO Softmax PL Conjugate PL CIFAR-10-C \u0017 13.71 (±0.07) 13.06 (±0.05) 13.24 (±0.02) 13.22 (±0.04) 14.85 (±0.08)12.99(±0.04) CIFAR-100-C \u0017 50.82 (±0.31) 44.53 (±0.13) 43.55 (±0.12) 51.35 (±0.04) 51.99 (±0.03)43.39(±0.11) \u0013 50.82 (±0.31) 43.99 (±0.15)43.21(±0.08) 51.35 (±0.04) 51.99 (±0.03) 43.39 (±0.11) Table 5: Mean Errors on the common corruptions datasets for source classiﬁer trained using squared loss. We note that temperature scaling didn’t help on the CIFAR-10-C dataset. Source Classiﬁer Errors without adaptation : CIFAR-10-C (28.34%), CIFAR-100-C (68.79%) Dataset Temperature (T) Hard PL Robust PL MEMO Conjugate PL (ENT) CIFAR-10-C \u0017 SGD,1e−3, 1 SGD,1 e−3, 1 SGD,1 e−3, 1 SGD, 1e−3, 1 \u0013 SGD,1e−3, 1 SGD,1 e−2, 2 SGD,5 e−3, 3 Adam,1e−3, 2 CIFAR-100-C \u0017 SGD,1e−2, 1 SGD,1 e−2, 1 SGD,5 e−3, 1 SGD, 1e−2, 1 \u0013 SGD,1e−2, 1 SGD,1 e−2, 2 SGD,1 e−2, 2 SGD,1e−2, 2 ImageNet-C \u0017 SGD,1e−2, 1 SGD,2.5 e−3, 1 SGD,1 e−3, 1 SGD,2.5e−3, 1 \u0013 SGD,1e−2, 1 SGD,2.5e−3, 1.5 SGD,1e−3, 1 SGD,2.5e−3, 1.5 Table 6: Hyper-parameters (Optimizer, Learning Rate, Temperature) for the results in Table 1, where we showed the mean errors on the common corruptions dataset for a source classiﬁer trained using cross-entropy loss. A.8 Hyper-Parameters We share the exact hyper-parameters found using gridsearch over the 4 validation noises for the common corruptions dataset. 20Cross Entropy Classiﬁer Experiments In Section 5.2, Table 1 shows the results when adapting a cross entropy trained classiﬁer on various common corruptions dataset. Table 6 gives the optimizer, learning rate and optimal temperature for each of the baseline and our proposed conjugate loss. PolyLoss Classiﬁer Experiments In Section 5.3, Table 2 shows the results when adapting a polyloss trained classiﬁer on various common corruptions dataset. Table 7 gives the optimizer, learning rate and optimal temperature for each of the baseline and our proposed conjugate loss. Dataset T Hard PL Robust PL ENT MEMO Softmax PL Conjugate PL (Ours) CIFAR-10-C\u0017 SGD,1e−3, 1 SGD,1e−3, 1 SGD,1 e−3, 1 SGD,5 e−3, 1 SGD, 1e−3, 1 SGD, 1e−3, 1 \u0013 SGD,1e−3, 1 SGD,1e−2, 3 SGD,1 e−2, 3 SGD,5 e−3, 3 SGD, 1e−3, 2 SGD, 1e−3, 1.5 CIFAR-100-C\u0017 SGD,1e−2, 1 SGD,1e−2, 1 SGD,1 e−2, 1 SGD,1 e−2, 1 SGD, 1e−2, 1 SGD, 1e−2, 1 \u0013 SGD,1e−2, 1 Adam,1e−3, 3 SGD,1 e−2, 2 SGD,1 e−2, 2 SGD, 1e−2, 2.5 SGD, 1e−2, 1.5 ImageNet-C\u0017 SGD,1e−2, 1 SGD,2.5e−3, 1 SGD,2.5e−3, 1 SGD,5e−3, 1 SGD, 2.5e−3, 1 SGD, 2.5e−3, 1 \u0013 SGD,1e−2, 1 SGD,2.5e−3, 1 SGD,2.5e−3, 1.5 SGD,5e−3, 1 SGD, 2.5e−3, 2 SGD, 2.5e−3, 1 Table 7: Hyper-parameters (Optimizer, Learning Rate, Temperature) for the results in Table 2, where we showed the mean errors on the common corruptions dataset for a source classiﬁer trained using poly-loss. Squared Loss Classiﬁer Experiments In Section 5.3, we brieﬂy discussed the results when adapt- ing a squared loss trained classiﬁer on various common corruptions dataset. Table 8 gives the optimizer, learning rate and optimal temperature for each of the baseline and our proposed conjugate loss for the results in Table 5. Digit Adaptation Datasets For the experiments on digits adaptation tasks, we do not have any validation set. Hence, we don’t use temperature scaling here (T = 1) and ﬁx the optimizer and LR as Adam and 1e−2 respectively for all the baselines. A.9 Additional Experiments on Digit Adaptation Datasets Similar to the setting of Table 1, we perform additional experiments on digit adaptation datasets when the source classiﬁer is trained using the cross-entropy loss. Note that when the source classiﬁer is trained using cross-entropy loss, the conjugate loss is equal to the softmax-entropy. In the absence of validation dataset in digit adaptation benchmarks, we used a ﬁxed learning rate of 0.01 for all the baselines, optimizer as Adam and an informed temperature scaling guess of T=2. Table 9 compares softmax-entropy minimization with various baselines. Here, again we observe that on SVHN →MNIST benchmark, without temperature scaling, MEMO (10.67% error) outperforms softmax-entropy (14.41% error). However, similar to the observations in Table 1, with temperature scaling, softmax-entropy minimization (9.26% error) is able to match the performance of MEMO (9.36% error). Further, on the SVHN →USPS benchmark, softmax-entropy (conjugate) and MEMO perform similar even without temperature scaling. A.10 Additional Meta Learning the TTA Loss Experiments In Section 3, we tried to learn a test-time adaptation (TTA) loss via meta-learning for adapting a CIFAR10 trained ResNet26 to distribution shifts on CIFAR10 corruptions. Figure 1 showed that the learnt meta-loss looks like a temperature scaled softmax-entropy. In this section, we show the learnt meta loss across a range of settings as described below : 1. Digit Adaptation: Figure 7a and 7b show the learnt meta-loss when adapting a SVHN trained ResNet26 to MNIST dataset and USPS dataset respectively. We observe that the learnt meta-loss can be well approximated by a temperature scaled softmax-entropy. 2. Various Noise Types: In Figure 8, we show the learnt meta-loss when adapting a ResNet26 trained on CIFAR10 dataset using cross-entropy loss, to various noise types like speckle, gaussian, saturate and spatter. The severity level is kept ﬁxed at the maximum i.e. 5. 21Dataset T Hard PL Robust PL ENT MEMO Softmax PL Conjugate PL (Ours) CIFAR-10-C\u0017 SGD,1e−2, 1 SGD,1 e−2, 1 SGD,1 e−2, 1 SGD,1e−2, 1 SGD,1 e−4, 1 SGD,1e−2, 1 CIFAR-100-C\u0017 Adam,1e−3, 1 Adam,1e−3, 1 Adam,1e−3, 1 Adam,1e−3, 1 Adam, 1e−4, 1 Adam, 1e−3, 1 \u0013 Adam,1e−3, 1 Adam,1e−3, 0.5 Adam,1e−3, 2 Adam,1e−3, 2 Adam, 1e−4, 2.5 Adam, 1e−3, 1 Table 8: Hyper-parameters (Optimizer, Learning Rate, Temperature) for the results in Table 5, where we showed the mean errors on the common corruptions dataset for a source classiﬁer trained using squared loss. Dataset Temperature (T) Hard PL Robust PL MEMO Conjugate PL (ENT) SVHN→MNIST \u0017 21.54 27.44 10.67 14.41 \u0013 21.54 13.26 9.36 9.26 SVHN→USPS \u0017 26.06 26.81 22.72 22.57 \u0013 26.06 22.32 22.42 22.27 Table 9: Mean errors when adapting to digit adaptation benchmarks using a source classiﬁer trained via cross-entropy loss. Here, conjugate pseudo-labeling becomes softmax-entropy minimization. Again we observe that with the right temperature scaling, softmax-entropy minimization matches other approaches. For additional context, the source classiﬁer errors without adaptation are: SVHN →MNIST (34.17%), SVHN →USPS (31.84%). 20  10  0 10 20 prediction score 5 0 5 10loss value meta loss (error 10.44%) softmax entropy (error 14.41) fitted entropy (error 9.26) Meta Loss for SVHN -> MNIST (a) 20  10  0 10 20 prediction score 6 4 2 0 2 4 6 8 loss value meta loss (error 20.13%) softmax entropy (error 22.57) fitted entropy (error 22.22) Meta Loss for SVHN -> USPS adpatation (b) Figure 7: Visualizations of the learnt meta-loss by varying one input dimension (prediction score). The source model is a ResNet-26 trained with cross-entropy on the SVHN dataset. (a) The learnt meta-loss when adapting to the MNIST test dataset. (b) The learnt meta-loss when adapting to the USPS test dataset. 3. Various Severity Levels: In Figure 9, we vary the severity level of the noise, keeping the noise type ﬁxed. 4. Dataset and Architecture: In Figure 10, we compare the learnt meta-loss when adapting to speckle noise, for different source classiﬁer architectures (ResNet26 and ResNet50) and different source training dataset (CIFAR10 and CIFAR100). In all the cases, we again observe that the learnt meta-loss can be well approximated by a temperature scaled softmax-entropy. 5. Squared Loss : Finally, in Figure 11 we show the learnt meta-loss for classiﬁers trained with squared loss function instead of cross-entropy. We observe that in this case, the learnt meta loss mimics a quadratic function as expected from the conjugate formulation. 22For each of the learnt meta losses, we also show the values (α,T,C ) we use to ﬁt the meta loss with softmax entropy function: α·H(softmax(x/T)) −C. Note that although the learnt meta-loss can be approximated by the conjugate, the parameters α,T,C differ across the settings. In the case of classiﬁers trained with squared loss, we ﬁt the meta loss with a quadratic function∑K i=1(A·x2 i + C), where Kis the number of classes and xis the logit vector. Again, we also show the ﬁtted parameter value A,C. The meta loss follows the trend of a quadratic function. The ﬁtted quadratic function performs better or similar as the meta loss, while the parameters of the ﬁtted quadratic function remain different across the meta learning setup (base classiﬁer architectures and noise types). (a)  (b) (c)  (d) Figure 8: Visualization of meta loss (blue) learnt from various noise types in CIFAR-10-C validation set, where base classiﬁers are trained with cross-entropy loss. We show the error of meta loss, softmax entropy and ﬁtted entropy for test-time adaptation on the corresponding noise types. We also show the parameters (α,T,C ) in the ﬁtted entropy. 23(a)  (b) (c)  (d) Figure 9: Visualization of meta loss (blue) learnt on speckle noise with different severity level for CIFAR-10-C, where base classiﬁers are trained with cross-entropy loss. We show the error of meta loss, softmax entropy and ﬁtted entropy for test-time adaptation on the corresponding noise types. We also show the parameters (α,T,C ) in the ﬁtted entropy. 24(a)  (b) (c)  (d) Figure 10: Visualization of meta loss (blue) learnt across datasets (CIFAR-10-C/CIFAR-100-C) and base classiﬁer architectures (ResNet-26/ResNet-50), where base classiﬁers are trained with cross-entropy loss. We show the error of meta loss, softmax entropy and ﬁtted entropy for test-time adaptation on the corresponding noise types. We also show the parameters ( α,T,C ) in the ﬁtted entropy. (a)  (b) Figure 11: Visualization of meta loss (blue), where base classiﬁer is trained with quadratic loss. We show the error of meta loss, softmax entropy and ﬁtted quadratic function for test-time adaptation on the corresponding noise types. We also show the parameters ( A,B,C ) in the ﬁtted quadratic function. 25",
      "meta_data": {
        "arxiv_id": "2207.09640v2",
        "authors": [
          "Sachin Goyal",
          "Mingjie Sun",
          "Aditi Raghunathan",
          "Zico Kolter"
        ],
        "published_date": "2022-07-20T04:02:19Z",
        "pdf_url": "https://arxiv.org/pdf/2207.09640v2.pdf"
      }
    },
    {
      "title": "Improved Test-Time Adaptation for Domain Generalization",
      "abstract": "The main challenge in domain generalization (DG) is to handle the\ndistribution shift problem that lies between the training and test data. Recent\nstudies suggest that test-time training (TTT), which adapts the learned model\nwith test data, might be a promising solution to the problem. Generally, a TTT\nstrategy hinges its performance on two main factors: selecting an appropriate\nauxiliary TTT task for updating and identifying reliable parameters to update\nduring the test phase. Both previous arts and our experiments indicate that TTT\nmay not improve but be detrimental to the learned model if those two factors\nare not properly considered. This work addresses those two factors by proposing\nan Improved Test-Time Adaptation (ITTA) method. First, instead of heuristically\ndefining an auxiliary objective, we propose a learnable consistency loss for\nthe TTT task, which contains learnable parameters that can be adjusted toward\nbetter alignment between our TTT task and the main prediction task. Second, we\nintroduce additional adaptive parameters for the trained model, and we suggest\nonly updating the adaptive parameters during the test phase. Through extensive\nexperiments, we show that the proposed two strategies are beneficial for the\nlearned model (see Figure 1), and ITTA could achieve superior performance to\nthe current state-of-the-art methods on several DG benchmarks. Code is\navailable at https://github.com/liangchen527/ITTA.",
      "full_text": "Improved Test-Time Adaptation for Domain Generalization Liang Chen1 Yong Zhang2* Yibing Song3 Ying Shan2 Lingqiao Liu1∗ 1 The University of Adelaide 2 Tencent AI Lab 3 AI3 Institute, Fudan University {liangchen527, zhangyong201303, yibingsong.cv}@gmail.com yingsshan@tencent.com lingqiao.liu@adelaide.edu.au Abstract The main challenge in domain generalization (DG) is to handle the distribution shift problem that lies between the training and test data. Recent studies suggest that test-time training (TTT), which adapts the learned model with test data, might be a promising solution to the problem. Gen- erally, a TTT strategy hinges its performance on two main factors: selecting an appropriate auxiliary TTT task for up- dating and identifying reliable parameters to update during the test phase. Both previous arts and our experiments in- dicate that TTT may not improve but be detrimental to the learned model if those two factors are not properly consid- ered. This work addresses those two factors by proposing an Improved Test-Time Adaptation (ITTA) method. First, in- stead of heuristically defining an auxiliary objective, we pro- pose a learnable consistency loss for the TTT task, which con- tains learnable parameters that can be adjusted toward bet- ter alignment between our TTT task and the main prediction task. Second, we introduce additional adaptive parameters for the trained model, and we suggest only updating the adap- tive parameters during the test phase. Through extensive ex- periments, we show that the proposed two strategies are ben- eficial for the learned model (see Figure 1), and ITTA could achieve superior performance to the current state-of-the-art methods on several DG benchmarks. Code is available at https://github.com/liangchen527/ITTA. 1. Introduction Recent years have witnessed the rapid development of deep learning models, which often assume the training and test data are from the same domain and follow the same distribution. However, this assumption does not always hold in real-world scenarios. Distribution shift among the source and target domains is ubiquitous in related areas [35], such as autonomous driving or object recognition tasks, resulting *Corresponding authors. This work is done when L. Chen is an intern in Tencent AI Lab. 0.5 1.1 0.5 1.2 0.5 0.5 0.5 1.4 0.4 0.4 0.4 0.3 art cartoon photo sketch 79.9 75.4 94.4 75.8 83.3 76.0 94.4 76.7 84.7 78.0 94.5 78.2 Figure 1. Performance improvements from the proposed two strate- gies (i.e. introducing a learnable consistency loss and including additional adaptive parameters to improve TTT) for the baseline model (i.e. ResNet18 [30] with existing augmentation strategy [75]). Experiments are conducted on the PACS dataset [37] with the leave- one-out setting. Following [27], we use 60 sets of random seeds and hyper-parameters for each target domain. The reported average accuracy and error bars verify the effectiveness of our method. in poor performances for delicately designed models and hindering the further application of deep learning techniques. Domain generalization (DG) [2,8,16,23,24,31,38 –40,40, 44, 47, 51, 52, 69], designed to generalize a learned model to unseen target domains, has attracted a great deal of attention in the research community. The problem can be traced back to a decade ago [7], and various approaches have been pro- posed to push the DG boundary ever since. Those efforts in- clude invariant representation learning [28,47,49,58], adver- sarial learning [23,40,44,69], augmentation [9,41,42,66,75], or meta-learning [2, 16, 38, 39]. Despite successes on certain occasions, a recent study [27] shows that, under a rigorous evaluation protocol, most of these arts are inferior to the baseline empirical risk minimization (ERM) method [61]. This finding is not surprising, as most current arts strive to decrease the distribution shift only through the training data while overlooking the contributions from test samples. Recently, the test-time training (TTT) technique [60] has been gaining momentum for easing the distribution shift problem. TTT lies its success in enabling dynamic tuning of the pretrained model with the test samples via an auxil- iary TTT task, which seems to be a promising effort when arXiv:2304.04494v2  [cs.CV]  16 Apr 2023confronting data from different domains. However, TTT is not guaranteed to improve the performance. Previous arts [46, 63] indicate that selecting an appropriate auxiliary TTT task is crucial, and an inappropriate one that does not align with the main loss may deteriorate instead of improv- ing the performance. Meanwhile, it is pointed out in [63] that identifying reliable parameters to update is also essential for generalization, which is in line with our experimental findings in Sec. 5.3. Both of these two tasks are non-trivial, and there are limited efforts made to address them. This paper aims to improve the TTT strategy for better DG. First, different from previous works that empirically define auxiliary objectives and assume they are aligned with the main task, our work does not make such assumptions. Instead, we suggest learning an appropriate auxiliary loss for test-time updating. Specifically, encouraged by recent successes in multi-view consistency learning [13,26,29], we propose to augment the consistency loss by adding learn- able parameters based on the original implementation, where the parameters can be adjusted to assure our TTT task can be more aligned with the main task and are updated by en- forcing the two tasks share the same optimization direction. Second, considering that identifying reliable parameters to update is an everlasting job given the growing size of current deep models, we suggest introducing new adaptive param- eters after each block during the test phase, and we only tune the new parameters by the learned consistency loss while leaving the original parameters unchanged. Through extensive evaluations on the current benchmark [27], we illustrate that the learnable consistency loss performs more effectively than the self-supervised TTT tasks adopted in previous arts [60, 63], and by tuning only the new adaptive parameters, our method is superior to existing strategies that update all the parameters or part of them. This work aims to ease the distribution shift problem by improving TTT, and the main contributions are three-fold: • We introduce a learnable consistency loss for test-time adaptation, which can be enforced to be more aligned with the main loss by tuning its learnable parameters. • We introduce new adaptive parameters for the trained model and only update them during the test phase. • We conduct experiments on various DG benchmarks and illustrate that our ITTA performs competitively against current arts under the rigorous setting [27] for both the multi-source and single-source DG tasks. 2. Related Works 2.1. Domain Generalization. Being able to generalize to new environments while de- ploying is a challenging and practical requirement for cur- rent deep models. Existing DG approaches can be roughly categorized into three types. (1) Invariant representation learning: The pioneering work [5] theoretically proves that if the features remain invariant across different domains, then they are general and transferable to different domains. Guided by this finding, [47] uses maximum mean discrep- ancy (MMD) to align the learned features, and [25] proposes to use a multi-domain reconstruction auto-encoder to obtain invariant features. More recently, [58] suggests maximiz- ing the inner product of gradients from different domains to enforce invariance, and a similar idea is proposed in [52] where these gradients are expected to be similar to their mean values. (2) Optimization algorithms: Among the different optimization techniques adopted in DG, prevail- ing approaches resort to adversarial learning [23, 40, 44, 69] and meta-learning [2, 16, 38, 39]. Adversarial training is often used to enforce the learned features to be agnostic about the domain information. In [23], a domain-adversarial neural network (DANN) is implemented by asking the main- stream feature to maximize the domain classification loss. This idea is also adopted in [44], where adversarial training and an MMD constraint are employed to update an auto- encoder. Meanwhile, the meta-learning technique is used to simulate the distribution shifts between seen and unseen environments [2, 16, 38, 39], and most of these works are developed based on the MAML framework [20]. (3) Aug- mentation: Most augmentation skills applied in the general- ization tasks are operated in the feature level [34, 41, 48, 75] except for [11,66,68] which mix images [68] or its phase [66] to synthesize new data. To enable contrastive learning, we incorporate an existing augmentation strategy [75] in our framework. This method originated from AdaIN [32], which synthesizes new domain information by mixing the statistics of the features. Similar ideas can be found in [42, 48]. 2.2. Test-Time Training and Adaptation Test-Time Training (TTT) is first introduced in [60]. The basic paradigm is to employ a test-time task besides the main task during the training phase and update the pre- trained model using the test data with only the test-time objective before the final prediction step. The idea is empir- ically proved effective [60] and further developed in other related areas [3, 10, 12, 14, 21, 22, 43, 56, 63, 65, 73, 74]. Most current works focus on finding auxiliary tasks for updat- ing during the test phase, and the efforts derive from self- supervion [3, 10, 21, 22, 43, 60], meta-learning [65, 73, 74], information entropy [63], pseudo-labeling [12, 14], to name a few. However, not all empirically selected test-time tasks are effective. A recent study [46] indicates that only when the auxiliary loss aligns with the main loss can TTT improve the trained model. Inspired by that, we propose a learnable consistency loss and enforce alignment between the two ob- jectives. Results show that our strategy can be beneficial for the trained model (see Figure 1).subtract Figure 2. Training process of ITTA. We use x from the source domain as input for the feature extractor fθ(·) to obtain the repre- sentation z and its augmented version z′, where the augmentation skill from [75] is applied. The classifier fϕ(·) and weight subnet- work fw(·) are used to compute the main loss Lmain and learnable consistency loss Lwcont. Please refer to our text for details. Meanwhile, [63] suggests that auxiliary loss is not the only factor that affects the performance. Selecting reliable parameters to update is also crucial within the TTT frame- work. Given the large size of current models, correctly iden- tifying these parameters may require tremendous amounts of effort. To this end, instead of heuristically selecting candi- dates, we propose to include new adaptive parameters for up- dating during the test phase. Experimental results show that the proposed method can obtain comparable performances against existing skills. 3. Methodology In the task of DG, we are often given access to data from S (S ≥ 1) source domains Ds = {D1, D2, ..., DS} and expect a model to make good prediction on unseen target domains Dt = {D1, D2, ..., DT } (T ≥ 1). Our method aims to improve the test-time training (TTT) strategy for better DG. The improvements are two-fold. First, we pro- pose a learnable consistency loss for the TTT task, which could be enforced to align with the main objective by tuning its learnable weights. Second, we suggest including addi- tional adaptive parameters and only updating these adaptive parameters during the test phase. 3.1. A Learnable Consistency Loss for TTT The TTT strategies have shown promising performances when dealing with distribution shift problems [43, 63]. How- ever, their successes are depended on the empirically selected auxiliary TTT tasks, which may deteriorate the performances if chosen improperly. Motivated by the recent successes in multi-view consistency learning [13, 26, 29], we suggest adopting a consistency loss in our TTT task. Note that the naive consistency loss is still not guaranteed to be effective as prior art [46] indicates that only when the auxiliary loss aligns with the main loss, can TTT improves the perfor- mance. To this end, we propose to augment the auxiliary loss with learnable parameters that could be adjusted toward a better alignment between the TTT and main tasks. In our case, we make the adopted consistency loss learnable by introducing a weight subnetwork that allows flexible ways Algorithm 1 Pseudo code of the training phase of ITTA in a PyTorch-like style. # fθ, fϕ, fw: feature extractor, classifier, weight subnetwork # α, 0: weight paramter, all zero tensor # training process for x, yin training loader: # load a minibatch with N samples def forward process(x, y): z, z′ = fθ.forward(x) # computing losses Lmain = CrossEntropyLoss(fϕ.forward(z), y) Lmain+ =CrossEntropyLoss(fϕ.forward(z′), y) Lwcont = MSELoss(fw.forward(z − z′), 0) return Lmain, Lwcont # SGD update: feature extractor and classifier Lmain, Lwcont = forward process(x, y) ([fθ.params, fϕ.params]).zero grad() (Lmain + αLwcont).backward() update( \u0002 fθ.params, fϕ.params \u0003 ) # compute objectives for updating weight subnetwork Lmain, Lwcont = forward process(x, y) Lmain.backward() ˆgmain = fθ.params.grad.clone().normalize() fθ.params.zero grad() Lwcont.backward() ˆgwcont = fθ.params.grad.clone().normalize() # SGD update: weight subnetwork MSELoss(ˆgmain, ˆgwcont).backward() fw.params.zero grad() update(fw.params) to measure the consistency between two views of the same instance. We first introduce the pipeline of our training framework. Given the D dimensional representation z ∈ RD1 and its corresponding augmented version z′ that are obtained from a feature extractor (i.e. {z, z′} = fθ(x), where x is an input image from Ds, and fθ(·) is the feature extractor parame- terized by θ. In our implementation, we use the existing augmentation method [75] to obtain z′ by modifying the intermediate activation in fθ(x). We show in our supplemen- tary material that our framework can also thrive with other augmentation strategies), our learnable consistency loss is given by, Lwcont = ∥fw(z − z′)∥, (1) where ∥ · ∥denotes the L2 norm; fw(·) is the weight sub- network parameterized by w. To make the training process more stable and potentially achieve better performance, we apply a dimension-wise nonlinear function to map each di- mension of z − z′ before calculating the L2 norm. That is, ∀h ∈ RD, fw(h) is implemented by stacking layers of a nonlinear function: ReLU(a ∗ h + b), where a ∈ RD and b ∈ RD are the weight and bias from the nonlinear function, 1We omit the batch dimensions of the variables for simplicity.… … subtract Figure 3. Test adaptation process of ITTA. Different from that in the training stage, we include additional adaptive parameters fΘ after each block of the feature extractor fθ. For each test sample x, the intermediate representations zi and z′i obtained from fi θ are passed to fi Θ before going to the next block fi+1 θ . We use the learnable consistency loss Lwcont as the objective to update fΘ. Please refer to our text for details. and different layers of a, bform the parameter w in fw. In effect, this creates a piecewise-linear mapping function for h: depending on the value of h, the output could be 0, a constant, or a scaling-and-shifted version of h. More studies about the design of fw are provided in our supplementary material. Compared to the naive consistency learning with- out fw, our Lwcont can be more flexible with an adjustable fw, which we show in the following is the key for learning an appropriate loss in the improved TTT framework. Combining Lwcont with the main loss Lmain which applies the cross-entropy loss (CE) for both the origi- nal and augmented inputs ( i.e. Lmain = CE(fϕ(z), y) + CE(fϕ(z′), y), where fϕ is the classifier parameterized by ϕ, and y is the corresponding label), the objective for the feature extractor and classifier can be formulated into, min{θ,ϕ} Lmain + αLwcont, (2) where α is the weight parameter that balances the contri- butions from the two terms. A simple illustration of the workflow is shown in Figure 2. From Eq. (2), the expected gradients for the feature ex- tractor from Lmain and Lwcont can be represented as, \u001a gmain = ∇θ(CE(fϕ(z), y) + CE(fϕ(z′), y)), (3) gwcont = ∇θ∥fw(z − z′)∥. (4) We observe that the direction of gwcont is also determined by the weight subnetwork fw(·), which should be close with gmain to ensure alignment between Lmain and Lwcont [46, 60]. To this end, we propose a straightforward solution by enforcing equality between the normalized versions of gmain and gwcont, and we use this term as the objective for updating fw(·), which gives, min w Lalign, s.t. Lalign = ∥ˆgmain − ˆgwcont∥, (5) where ˆgmain = gmain−Egmain σgmain , and similar for ˆgwcont. In our implementation, we update {θ, ϕ} and w in an alternative manner. Pseudo code of the training process are shown in Algorithm 1. Algorithm 2 Pseudo code of the test phase of ITTA in a PyTorch-like style. # fθ, fϕ: feature extractor, classifier # fw, fΘ: weight subnetwork, additional adaptive blocks # m, 0: total number of blocks in fθ, all zero tensor # test process for x in test loader: # load a test batch def forward process(x): z1, z′1 = f1 Θ.forward((f1 θ .forward(x))) # first blocks for i in range(2, m + 1): # the following m − 1 blocks zi, z′i = fi θ.forward(zi−1), fi θ.forward(z′i−1) zi, z′i = fi Θ.forward(zi), fi Θ.forward(z′i) return zi, z′i # test adaptation phase: SGD update additional adaptive parameters z, z′ = forward process(x) Lwcont = MSELoss(fw.forward(z − z′), 0) fΘ.params.zero grad() Lwcont.backward() update(fΘ.params) # final prediction z, = forward process(x) result = fϕ.forward(z) 3.2. Including Additional Adaptive Parameters Selecting expressive and reliable parameters to update during the test phase is also essential in the TTT frame- work [63]. Some strategies decide to update all the parame- ters from the feature extractor [3, 43], while others use only the parameters from the specific layers for updating [63, 71]. Given the fact that the sizes of current deep models are often very large and still growing, exhaustively trying different combinations among the millions of candidates seems to be an everlasting job. As there are no consensuses on which parameter should be updated, we suggest another easy alter- native in this work. Specifically, assuming there are a total of m blocks in the pretrained feature extractor fθ(·), and the i-th block can be denoted as fi θ(·). Then the intermediate representation zi from fi θ(·) can be formulated as, zi = fi θ(zi−1), s.t. z1 = f1 θ (x). (6) We propose to include additional adaptive blockfΘ that is parameterized by Θ after each block of fθ during the test- time adaptation phase, which reformulates Eq. (6) into, zi = fi Θ(fi θ(zi−1)), s.t. z1 = f1 Θ(f1 θ (x)), (7) where fΘ(·) does not change the dimension and sizes of the intermediate representations. In our work, we use a structure similar to fw to implement fΘ. Note zm is simplified as z in this phase, and the same process is applied for obtaining z′. Then, in the test-time adaptation phase, we suggest only updating the new adaptive parameters via the learned con- sistency loss. The optimization process can be written as,Table 1. Multi sources domain generalization. Experiments are conducted on the DomainBed benchmark [27]. All methods are examined for 60 trials in each unseen domain. Top5 accumulates the number of datasets where a method achieves the top 5 performances. The score here accumulates the numbers of the dataset where a specific art obtains larger accuracy than ERM on account of the variance. Best results are colored as red. Among the 22 methods compared, less than a quarter outperforms ERM in most datasets (Score ≥ 3). PACS VLCS OfficeHome TerraInc DomainNet Avg. Top5↑ Score↑ MMD [40] 81.3 ± 0.8 74.9 ± 0.5 59.9 ± 0.4 42.0 ± 1.0 7.9 ± 6.2 53.2 1 2 RSC [33] 80.5 ± 0.2 75.4 ± 0.3 58.4 ± 0.6 39.4 ± 1.3 27.9 ± 2.0 56.3 0 1 IRM [1] 80.9 ± 0.5 75.1 ± 0.1 58.0 ± 0.1 38.4 ± 0.9 30.4 ± 1.0 56.6 0 1 ARM [72] 80.6 ± 0.5 75.9 ± 0.3 59.6 ± 0.3 37.4 ± 1.9 29.9 ± 0.1 56.7 0 0 DANN [23] 79.2 ± 0.3 76.3 ± 0.2 59.5 ± 0.5 37.9 ± 0.9 31.5 ± 0.1 56.9 1 1 GroupGRO [55] 80.7 ± 0.4 75.4 ± 1.0 60.6 ± 0.3 41.5 ± 2.0 27.5 ± 0.1 57.1 0 1 CDANN [44] 80.3 ± 0.5 76.0 ± 0.5 59.3 ± 0.4 38.6 ± 2.3 31.8 ± 0.2 57.2 0 0 VREx [36] 80.2 ± 0.5 75.3 ± 0.6 59.5 ± 0.1 43.2 ± 0.3 28.1 ± 1.0 57.3 1 1 CAD [53] 81.9 ± 0.3 75.2 ± 0.6 60.5 ± 0.3 40.5 ± 0.4 31.0 ± 0.8 57.8 1 2 CondCAD [53] 80.8 ± 0.5 76.1 ± 0.3 61.0 ± 0.4 39.7 ± 0.4 31.9 ± 0.7 57.9 0 1 MTL [6] 80.1 ± 0.8 75.2 ± 0.3 59.9 ± 0.5 40.4 ± 1.0 35.0 ± 0.0 58.1 0 0 ERM [61] 79.8 ± 0.4 75.8 ± 0.2 60.6 ± 0.2 38.8 ± 1.0 35.3 ± 0.1 58.1 1 - MixStyle [75] 82.6 ± 0.4 75.2 ± 0.7 59.6 ± 0.8 40.9 ± 1.1 33.9 ± 0.1 58.4 1 1 MLDG [38] 81.3 ± 0.2 75.2 ± 0.3 60.9 ± 0.2 40.1 ± 0.9 35.4 ± 0.0 58.6 1 1 Mixup [68] 79.2 ± 0.9 76.2 ± 0.3 61.7 ± 0.5 42.1 ± 0.7 34.0 ± 0.0 58.6 2 2 Fishr [52] 81.3 ± 0.3 76.2 ± 0.3 60.9 ± 0.3 42.6 ± 1.0 34.2 ± 0.3 59.0 2 2 SagNet [48] 81.7 ± 0.6 75.4 ± 0.8 62.5 ± 0.3 40.6 ± 1.5 35.3 ± 0.1 59.1 1 2 SelfReg [34] 81.8 ± 0.3 76.4 ± 0.7 62.4 ± 0.1 41.3 ± 0.3 34.7 ± 0.2 59.3 2 3 Fish [58] 82.0 ± 0.3 76.9 ± 0.2 62.0 ± 0.6 40.2 ± 0.6 35.5 ± 0.0 59.3 3 4 CORAL [59] 81.7 ± 0.0 75.5 ± 0.4 62.4 ± 0.4 41.4 ± 1.8 36.1 ± 0.2 59.4 2 3 SD [51] 81.9 ± 0.3 75.5 ± 0.4 62.9 ± 0.2 42.0 ± 1.0 36.3 ± 0.2 59.7 4 4 Ours 83.8 ± 0.3 76.9 ± 0.6 62.0 ± 0.2 43.2 ± 0.5 34.9 ± 0.1 60.2 4 4 min Θ ∥fw(z − z′)∥, s.t. {z, z′} = fΘ(fθ(x)). (8) Note that different from the training phase, x in this stage is from the target domain Dt, and we use the online setting in [60] for updating. A simple illustration of the test adaptation pipeline is shown in Figure 3. For the final step, we use the original representation ob- tained from the pretrained feature extractor and the adapted adaptive parameters for prediction. Pseudo code of the test stage are shown in Algorithm 2. 4. Experiments 4.1. Settings Datasets. We evalute ITTA on five benchmark datasets: PACS [37] which consists of 9,991 images from 7 cate- gories. This dataset is probably the most widely-used DG benchmark owing to its large distributional shift across 4 do- mains including art painting, cartoon, photo, and sketch; VLCS [18] contains 10,729 images of 5 classes from 4 different datasets (i.e. domains) including PASCAL VOC 2007 [17], LabelMe [54], Caltech [19], and Sun [64] where each dataset is considered a domain in DG;OfficeHome [62] is composed of 15,588 images from 65 classes in office and home environments, and those images can be categorized into 4 domains (i.e. artistic, clipart, product, and real world); TerraInc [4] has 24,788 images from 10 classes. Those images are wild animals taken from 4 different locations (i.e. domains) including L100, L38, L43, and L46; Domain- Net [50] which contains 586,575 images from 345 classes, and the images in it can be depicted in 6 styles (i.e. clipart, infograph, painting, quickdraw, real, and sketch). Implementation details. For all the experiments, we use the ImageNet [15] pretrained ResNet18 [30] backbone that with 4 blocks as the feature extractor fθ, which could en- large the gaps in DG compared to larger models [70]. Corre- spondingly, we also include 4 blocks of additional adaptive parameters (i.e. fΘ), and each block is implemented with 5 layers of learnable parameters with weight initialized as all ones and bias initialized as all zeros. For the weight subnet- work fw, we use 10 layers of learnable parameters with the initialization skill similar to that of fΘ. The classifier fϕ is an MLP layer provided by the Domainbed benchmark [27]. For the weight parameter α in Eq. (2), we set it to be 1 for all experiments (please refer to our supplementary material for analysis). The random seeds, learning rates, batch size, and augmentation skills are all dynamically set for all the compared arts according to [27].Table 2. Single source domain generalization. Experiments are conducted on the PACS dataset [37]. Here A, C, P, and S are the art, cartoon, photo, and sketch domains in PACS. A→C represents models trained on the art domain and tested on the cartoon domain, and similar for others. All methods are examined for 60 trials in each unseen domain. Best results are colored as red. A→C A →P A →S C →A C →P C →S P →A P →C P →S S →A S →C S →P Avg. RSC 66.3 ±1.3 88.2±0.6 57.2±3.1 65.8±1.5 82.4±0.6 68.7±2.5 60.5±2.0 41.3±6.0 53.1±2.8 53.8±1.6 65.9±0.7 48.4±1.9 62.6 Fish 67.1 ±0.5 89.2±1.8 57.0±0.2 66.7±1.0 85.6±0.4 64.5±3.6 55.1±2.1 33.9±2.3 51.2±4.2 59.1±3.2 67.1±0.9 58.4±1.2 62.9 CDANN 66.5±1.7 92.2±0.6 65.0±0.9 70.6±0.1 82.9±1.4 67.7±3.0 60.6±0.3 42.2±6.4 46.9±9.9 51.4±2.3 60.7±1.2 51.9±0.4 63.2 SelfReg 63.9±1.9 90.1±1.0 56.8±2.2 70.2±2.3 85.4±0.3 70.2±2.2 60.9±2.6 38.8±4.0 50.5±3.2 54.5±4.7 66.2±1.2 51.7±4.1 63.3 DANN 67.5 ±1.6 91.2±1.3 67.5±1.3 70.6±1.0 81.4±0.4 66.6±1.1 54.1±2.3 33.5±2.7 52.8±2.3 53.8±1.7 64.4±0.7 58.9±0.8 63.5 CAD 67.1 ±1.5 89.6±0.4 60.2±0.2 67.7±3.1 83.7±1.4 70.2±2.6 60.6±2.6 38.3±3.7 53.8±3.2 50.7±1.6 65.8±1.3 54.4±1.7 63.5 GroupGRO66.5±1.2 90.5±1.5 58.9±2.5 70.8±0.9 85.7±1.2 69.7±1.8 62.3±2.1 41.1±2.7 48.2±4.1 54.8±0.5 65.2±1.6 53.9±1.4 64.0 MTL 67.3 ±1.0 90.1±1.0 58.9±0.7 70.2±1.8 84.2±2.2 71.9±0.7 58.3±2.7 38.5±2.7 52.8±1.5 55.4±3.1 66.1±1.3 55.2±2.6 64.1 IRM 67.5 ±1.8 93.0±0.5 62.9±4.7 67.6±1.3 83.8±0.4 68.9±0.8 63.7±1.8 39.9±3.7 49.0±5.4 54.9±1.4 63.1±2.1 54.9±1.4 64.1 ARM 66.0 ±2.4 91.2±0.7 58.7±6.9 70.6±0.8 84.2±1.0 69.1±0.9 59.2±1.8 42.1±5.6 52.1±3.0 60.0±0.6 62.9±3.3 53.8±2.0 64.2 Mixup 65.5 ±0.8 87.8±0.3 57.2±1.0 71.4±1.1 83.1±1.8 68.0±3.0 59.6±1.7 37.2±2.7 56.5±3.8 55.0±2.2 66.2±1.5 62.7±4.2 64.2 CORAL 66.8±0.5 90.3±0.7 61.5±1.9 67.9±2.1 85.4±0.3 70.4±1.3 55.9±2.9 40.4±4.9 49.8±8.5 55.8±2.1 67.6±0.9 58.9±3.8 64.2 SD 67.1 ±1.3 91.7±1.2 63.7±4.1 70.3±0.9 84.4±0.7 69.4±2.3 57.5±2.5 42.6±0.8 47.7±1.7 55.9±2.4 65.7±0.8 55.8±2.1 64.3 MMD 67.1 ±1.4 88.0±0.8 63.6±1.6 70.0±1.1 83.6±0.2 70.2±1.0 58.8±2.6 40.3±1.0 52.3±2.4 57.4±1.9 68.7±0.9 52.7±3.7 64.4 MLDG 67.3±2.0 90.8±0.5 64.4±0.9 70.8±1.0 84.2±0.3 69.7±1.8 61.6±1.0 41.3±5.1 50.4±0.2 49.9±2.5 66.8±0.4 58.7±3.4 64.7 CondCAD66.9±1.4 92.3±0.7 60.8±4.5 71.0±0.6 84.7±1.1 72.6±0.5 61.2±1.5 40.7±3.6 55.7±1.6 52.3±1.7 64.2±0.4 55.3±1.2 64.8 ERM 67.3 ±0.7 91.7±0.9 60.1±4.7 70.4±0.6 82.3±2.7 68.1±0.9 59.6±1.8 44.7±2.8 56.5±2.7 52.8±2.3 68.1±0.7 58.4±0.9 65.0 VREx 67.1 ±1.5 91.0±1.0 62.6±3.5 71.1±2.4 84.1±0.9 71.7±1.3 62.4±3.1 37.7±3.3 53.6±2.3 60.6±1.6 66.7±0.8 57.5±1.4 65.5 Fishr 67.9 ±1.9 92.7±0.3 62.4±4.7 71.2±0.5 83.4±0.6 70.2±1.1 60.0±2.3 42.7±3.2 57.1±3.9 55.7±3.7 68.4±1.0 62.0±3.1 66.1 SagNet 67.6±1.4 92.3±0.5 59.5±1.7 71.8±0.3 82.8±0.6 69.9±1.8 62.5±2.5 45.2±2.5 64.1±2.0 55.8±1.1 65.7±1.4 55.9±3.5 66.1 MixStyle 68.5±2.0 91.2±1.6 65.1±0.7 73.2±1.3 85.0±0.8 71.7±1.5 63.6±1.7 46.3±1.1 51.6±3.7 54.2±1.5 67.0±3.4 58.3±1.4 66.3 Ours 68.9 ±0.6 92.4±0.1 62.5±0.6 75.3±0.4 85.9±0.3 70.2±1.4 66.5±1.1 52.2±2.7 63.8±1.1 57.6±3.7 68.0±1.3 57.9±2.0 68.4 Training and evaluation details. For all the compared methods, we conduct 60 trials on each source domain, and each with 5,000 iteration steps. During the training stage, we split the examples from training domains to 8:2 (train:val) where the training and validation samples are dynamically selected among different training trials. During test, we select the model that performs the best in the validation samples and test it on the target domains. The strategy is referred to as the “training-domain validate set” model selec- tion method in [27]. For each domain in different datasets, the final performance is the average accuracy from the 60 trials. 4.2. Multi-Source Generalization In these experiments, all five benchmark datasets afore- mentioned are used for evaluation, and the leave-one-out strategy is adopted for training (i.e. with S = |Ds ∪Dt|2 −1, and T = 1). Results are shown in Table 1. We note that ERM method obtains favorable performance against existing arts. In fact, as a strong baseline, ERM is superior to half of the methods in the term of average accuracy, and only 5 arts (i.e. SelfReg [34], Fish [58], CORAL [59], SD [51], and ours) among the compared 22 methods outperforms ERM in most datasets (i.e. with Score ≥ 3). In comparison, the proposed ITTA is more effective than all other models on average. In particular, ITTA achieves the best performances in 3 out of the 5 benchmarks (i.e. PACS, VLCS, and TerraInc datasets) and 4 in the top 5. Note that although our method does not obtain the best performances in the OfficeHome and DomainNet benchmarks, it still outperforms more than half 2We use | · |to denote the number of domains in the environment. of the existing models. The results validate the effectiveness of our method when tested in the multi-source setting. We present results of average accuracy in each domain from different datasets in the supplementary material. Please refer to it for details. 4.3. Single-Source Generalization In these experiments, we adopt the widely-used PACS [37] benchmark for evaluation, and the models are trained on one domain while tested on the remaining three (i.e. with S = 1, and T = 3). Although some approaches, such as MLDG [38] and Fishr [52], may require more than one domain information for their trainings, we can simu- late multi-domain information using only the source domain, and thus the experimental settings are still feasible for them. Compared to the multi-source generalization task, the single- source generalization is considered more difficult due to the limited domain information during the training phase. Evalu- ation results are presented in Table 2. We note that the ERM method outperforms most state-of-the-art models, and only 5 models, including VREx [36], Fishr [52], SagNet [48], MixStyle [75], and the proposed ITTA, can obtain better re- sults than ERM in the term of average accuracy. Meanwhile, our method achieves the best performances when trained in 5 out of the 12 source domain, and it obtains the best perfor- mance on average, leading more than 2% than the second best (i.e. MixStyle [75]) and 3% the ERM method. In line with the findings in [27], we notice that the naive ERM method [61] can indeed perform favorably against most existing models under rigorous evaluation protocol. As a matter of fact, the proposed method is the only one that consistently outperforms ERM in both the multi-sourceTable 3. Evaluations of different TTT-based models in the unseen domain from PACS [37]. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Model Target domain Avg.Art Cartoon Photo Sketch Baseline 79.9 ±0.5 75.4±1.1 94.4±0.5 75.8±1.2 81.4±0.5 TTT [60] 81.5±0.8 77.6±0.6 94.3±0.2 78.4±0.7 83.0±0.2 MT3 [3] 82.0 ±1.0 76.5±1.0 94.1±0.2 77.7±1.3 82.6±0.6 TENT [63] 80.2±0.9 77.2±0.8 94.4±0.2 77.4±0.1 82.3±0.5 Ours 84.7 ±0.4 78.0±0.4 94.5±0.4 78.2±0.3 83.8±0.3 and single-source settings. These results indicate that DG remains challenging for current efforts that aim to ease the distribution shift only through training data, and using the proposed improved TTT strategy may be a promising direc- tion for solving DG. 5. Analysis All experiments in this section are conducted on the widely-used PACS benchmark [37] with the leave-one-out strategy. The experimental settings are the same as that illus- trated in Sec. 4.1. Please refer to our supplementary material for more analysis. 5.1. Compared with Other TTT-Based Models Using test-time adaptation to ease the distribution shift problem has been explored in previous works, such as the original TTT method [60] and MT3 [3]. Their differences lie in that TTT uses a rotation estimation task for the test-time objective, and MT3 adopts a contrastive loss for the task and implements the overall framework using MAML [20]. There is also a recently proposed TENT [63] that aims to minimize the entropy of the final results by tuning the parameters from the batch normalization (BN) layers. To analyze the overall effectiveness of our method, we compare ITTA with these arts using the same baseline (i.e. ResNet18 [30] backbone with the existing augmentation skill [75]). Results are shown in Table 3. We observe that all the com- pared TTT-based methods can improve the baseline model in almost all target domains except for the “Photo” domain, which might be due to the ImageNet pretraining [67]. This phenomenon demonstrates that the TTT strategy may be a promising effort for easing the distribution shift problem. Meanwhile, we observe that the proposed ITTA is superior to all other approaches in most target domains and leads in the term of average accuracy. The main reason is that compared to the empirically designed TTT tasks adopted in previous works, the proposed learnable consistency loss is enforced to be more aligned with the main loss, thus more suitable for the test-time adaptation task [46]. Meanwhile, compared to the strategies that update the original param- eters from the trained model, the adaptation of the newly included parameters is also more effective for the overall (a) Input (b) Ours w/o fw (c) Ours (d) Main Figure 4. Grad-CAM [57] visualizations from different loss terms. We use images with varying class labels from the four target do- mains of PACS [37] as inputs (i.e. art, cartoon, photo, and sketch domains from top to bottom). Ours w/o fw is the naive consis- tency loss with fw disabled in Eq. (1). The proposed learnable consistency loss can align well with the main classification task. TTT framework. In the following, we provide more analysis to support these claims. 5.2. Effectiveness of the Learnable Consistency Loss To examine the effectiveness of our learnable consistency loss, we conduct ablation studies by comparing our method with the following variants. (1) Ours w/o fw: we disable fw when computing the learnable consistency loss in Eq. (1), which uses the naive consistency loss for the auxiliary TTT task. (2) Ours w/ Ent.: after training the model using the baseline settings (i.e. ResNet18 with the augmentation strat- egy [75]), we use the entropy minimization task in [63] for the TTT task. (3) Ours w/ Rot.: we use the rotation estimation task in [60] for the TTT task. To ensure fair com- parisons, we use the same baseline settings and include the same additional adaptive parameters for all the variants. Results are shown in the 4th to 6th rows Table 4. We find that the results from the naive consistency loss ( i.e. Ours w/o fw) are slightly better than that from the other two specially-designed objectives (i.e. Ours w/ Ent. and Ours w/ Rot.) on average. Besides the possibility of deteriorating the performance [46], our results indicate that empirically select- ing a TTT task may also be far from optimal. Meanwhile, we observe that when enabling fw, the proposed learnable consistency loss is superior to that withoutfw in all target do-Table 4. Comparison between different TTT tasks and parameter selecting strategies in the unseen domain from the PACS benchmark [37]. Here the “Ent.”, “Rot.”, and “Lwcont” denotes the entropy minimization task in [63], the rotation estimation task in [60], and the proposed learnable consistency objective, the “All”, “BN”, and “Ada.” are the strategies that update all the parameters, parameters from the batch normalization layer, and the proposed strategy that updates only the new additional adaptive parameters. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Model TTT tasks Param selectings Target domain Avg.Ent. Rot. Lwcont All BN Ada. Art Cartoon Photo Sketch Ours − − ✓ − − ✓ 84.7±0.4 78.0 ±0.4 94.5 ±0.4 78.2 ±0.3 83.8 ±0.3 Ours w/ofw − − − − − ✓ 83.1±0.4 74.6 ±0.6 94.0 ±0.5 78.0 ±0.8 82.5 ±0.1 Ours w/ Ent. ✓ − − − − ✓ 79.9±2.4 77.3 ±0.3 94.8 ±0.8 77.6 ±0.4 82.4 ±0.8 Ours w/ Rot. − ✓ − − − ✓ 81.1±1.0 75.2 ±0.5 94.9 ±0.3 77.3 ±0.6 82.1 ±0.3 Ours w/o TTT − − ✓ − − − 83.3±0.5 76.0 ±0.5 94.4 ±0.5 76.7 ±1.4 82.8 ±0.3 Ours w/ All − − ✓ ✓ − − 83.0±0.7 77.0 ±1.4 94.5 ±0.7 77.4 ±0.9 83.0 ±0.2 Ours w/ BN − − ✓ − ✓ − 81.8±0.5 75.6 ±0.3 94.4 ±0.3 77.9 ±1.1 82.4 ±0.5 mains, and it leads in the term of average accuracy among the variants compared, illustrating its advantage against other adopted TTT tasks. These results are not surprising. By comparing the Grad-CAM [57] visualizations from the main classification task with the learnable and naive consistency losses in Figure 4, we find that the proposed learnable objec- tive can well align with the main loss when fw is enabled as the hot zones activated by these two tasks are similar, which guarantees the improvement for the test-time adapta- tion [46, 60]. Please refer to our supplementary material for more visualizations. 5.3. Effectiveness of the Adaptive Parameters We compare ITTA with three variants to demonstrate the effectiveness of the proposed additional adaptive parameters. (1) Ours w/o TTT: we do not update any parameters during the test phase. This variant is used to verify whether TTT can improve the pretrained model. (2) Ours w/ ALL: similar to the updating strategy in the original TTT method [60], we update all the parameters from the feature extractor during the test phase. (3) Ours w/ BN: following the suggestion from TENT [63], only parameters from the BN layers of the feature extractor are updated. Note the same pretrained model is shared for all variants in these experiments, and the objectives during the test adaptation phase are to minimize the same learned consistency loss. We list the results in the last three rows in Table 4. We observe that when only updating parameters from the BN layers, the performance is inferior to the strategy without test-time adaptation, and updating all the parameters does not ensure improvements in all target domains. The observations are in line with the findings in [63] that selecting reliable parameters to update is essential in the TTT system and may also interact with the choice of the TTT task. In comparison, when including additional adaptive parameters for updating, the pretrained model can be boosted in all environments. The results validate that our adaptive parameters are more effective than that selected with existing strategies [60, 63] when applied with the proposed learnable test-time objective. 5.4. Limitation Although the proposed learned loss can bring satisfaction improvements, we are aware that the lunch is not free. When the weight subnetwork fw is disabled, updating the joint loss in Eq. (2) only costs 1 forward and 1 backward. However, in order to update fw, we have to compute the second-order derivative in Eq. (5), which will require 1 more forward and 3 more backward processes, bringing extra burden to the system. Our future efforts aim to simplify the overall optimization process and reduce the cost for ITTA. 6. Conclusion In this paper, we aim to improve the current TTT strategy for alleviating the distribution shift problem in DG. First, given that the auxiliary TTT task plays a vital role in the over- all framework, and an empirically selecting one that does not align with the main task may potentially deteriorate instead of improving the performance, we propose a learnable con- sistency loss that can be enforced to be more aligned with the main loss by adjusting its learnable parameters. This strategy is ensured to improve the model and shows favorable perfor- mance against some specially-designed objectives. Second, considering that selecting reliable and effective parameters to update during the test phase is also essential while exhaus- tively trying different combinations may require tremendous effort, we propose a new alternative by including new ad- ditional adaptive parameters for adaptation during the test phase. This alternative is shown to outperform some pre- vious parameter selecting strategies via our experimental findings. By conducting extensive experiments under a rig- orous evaluation protocol, we show that our method can achieve superior performance against existing arts in both the multi-source and single-source DG tasks. Acknowledgements. Liang Chen is supported by the ChinaScholarship Council (CSC Student ID 202008440331). References [1] Martin Arjovsky, L´eon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019. 5, 15, 16, 17 [2] Yogesh Balaji, Swami Sankaranarayanan, and Rama Chel- lappa. Metareg: Towards domain generalization using meta- regularization. In NeurIPS, 2018. 1, 2, 14, 15 [3] Alexander Bartler, Andre B¨uhler, Felix Wiewel, Mario D¨obler, and Bin Yang. Mt3: Meta test-time training for self- supervised test-time adaption. In AISTATS, 2022. 2, 4, 7 [4] Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In ECCV, 2018. 5, 17 [5] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for domain adaptation. In NeurIPS, 2006. 2 [6] Gilles Blanchard, Aniket Anand Deshmukh, Urun Dogan, Gyemin Lee, and Clayton Scott. Domain generalization by marginal transfer learning. arXiv preprint arXiv:1711.07910, 2017. 5, 15, 16, 17 [7] Gilles Blanchard, Gyemin Lee, and Clayton Scott. Generaliz- ing from several related classification tasks to a new unlabeled sample. In NeurIPS, 2011. 1 [8] Chaoqi Chen, Jiongcheng Li, Xiaoguang Han, Xiaoqing Liu, and Yizhou Yu. Compound domain generalization via meta- knowledge encoding. In CVPR, 2022. 1 [9] Chaoqi Chen, Luyao Tang, Feng Liu, Gangming Zhao, Yue Huang, and Yizhou Yu. Mix and reason: Reasoning over se- mantic topology with data mixing for domain generalization. In NeurIPS, 2022. 1 [10] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In CVPR, 2022. 2 [11] Liang Chen, Yong Zhang, Yibing Song, Lingqiao Liu, and Jue Wang. Self-supervised learning of adversarial example: Towards good generalizations for deepfake detection. In CVPR, 2022. 2 [12] Liang Chen, Yong Zhang, Yibing Song, Jue Wang, and Lingqiao Liu. Ost: Improving generalization of deepfake detection via one-shot test-time training. In NeurIPS, 2022. 2, 12 [13] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geof- frey Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020. 2, 3 [14] Sungha Choi, Seunghan Yang, Seokeon Choi, and Sungrack Yun. Improving test-time adaptation via shift-agnostic weight regularization and nearest source prototypes. In ECCV, 2022. 2 [15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009. 5 [16] Qi Dou, Daniel Coelho de Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain generalization via model-agnostic learning of semantic features. In NeurIPS, 2019. 1, 2 [17] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. IJCV, 88(2):303–338, 2010. 5 [18] Chen Fang, Ye Xu, and Daniel N Rockmore. Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. In ICCV, 2013. 5, 16 [19] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning gener- ative visual models from few training examples: An incre- mental bayesian approach tested on 101 object categories. In CVPR worksho, 2004. 5 [20] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model- agnostic meta-learning for fast adaptation of deep networks. In ICML, 2017. 2, 7 [21] Francois Fleuret et al. Uncertainty reduction for model adap- tation in semantic segmentation. In CVPR, 2021. 2 [22] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei A Efros. Test-time training with masked autoencoders. In NeurIPS, 2022. 2 [23] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc ¸ois Laviolette, Mario Marc- hand, and Victor Lempitsky. Domain-adversarial training of neural networks. JMLR, 17(1):2096–2030, 2016. 1, 2, 5, 15, 16, 17 [24] Muhammad Ghifary, David Balduzzi, W Bastiaan Kleijn, and Mengjie Zhang. Scatter component analysis: A unified framework for domain adaptation and domain generalization. IEEE TPAMI, 39(7):1414–1430, 2016. 1 [25] Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, and David Balduzzi. Domain generalization for object recognition with multi-task autoencoders. In ICCV, 2015. 2 [26] Jean-Bastien Grill, Florian Strub, Florent Altch ´e, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doer- sch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh- laghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. In NeurIPS, 2020. 2, 3 [27] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In ICLR, 2021. 1, 2, 5, 6, 14, 15, 16, 17 [28] Sivan Harary, Eli Schwartz, Assaf Arbelle, Peter Staar, Shady Abu-Hussein, Elad Amrani, Roei Herzig, Amit Alfassy, Raja Giryes, Hilde Kuehne, et al. Unsupervised domain general- ization by learning a bridge across domains. In CVPR, 2022. 1 [29] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual repre- sentation learning. In CVPR, 2020. 2, 3 [30] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 1, 5, 7, 14 [31] Shoubo Hu, Kun Zhang, Zhitang Chen, and Laiwan Chan. Domain generalization via multidomain discriminant analysis. In UAI, 2020. 1 [32] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In ICCV, 2017. 2 [33] Zeyi Huang, Haohan Wang, Eric P Xing, and Dong Huang. Self-challenging improves cross-domain generalization. In ECCV, 2020. 5, 15, 16, 17[34] Daehee Kim, Youngjun Yoo, Seunghyun Park, Jinkyu Kim, and Jaekoo Lee. Selfreg: Self-supervised contrastive regular- ization for domain generalization. In ICCV, 2021. 2, 5, 6, 15, 16, 17 [35] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribu- tion shifts. In ICML, 2021. 1 [36] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). In ICML, 2021. 5, 6, 15, 16, 17 [37] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain generalization. In ICCV, 2017. 1, 5, 6, 7, 8, 12, 13, 14, 15 [38] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Learning to generalize: Meta-learning for do- main generalization. In AAAI, 2018. 1, 2, 5, 6, 15, 16, 17 [39] Da Li, Jianshu Zhang, Yongxin Yang, Cong Liu, Yi-Zhe Song, and Timothy M Hospedales. Episodic training for domain generalization. In ICCV, 2019. 1, 2 [40] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot. Domain generalization with adversarial feature learning. In CVPR, 2018. 1, 2, 5, 15, 16, 17 [41] Pan Li, Da Li, Wei Li, Shaogang Gong, Yanwei Fu, and Timothy M Hospedales. A simple feature augmentation for domain generalization. In ICCV, 2021. 1, 2, 12, 14 [42] Xiaotong Li, Yongxing Dai, Yixiao Ge, Jun Liu, Ying Shan, and Ling-Yu Duan. Uncertainty modeling for out- of-distribution generalization. In ICLR, 2022. 1, 2 [43] Yizhuo Li, Miao Hao, Zonglin Di, Nitesh Bharadwaj Gun- davarapu, and Xiaolong Wang. Test-time personalization with a transformer for human pose estimation. In NeurIPS, 2021. 2, 3, 4 [44] Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao. Deep domain generaliza- tion via conditional invariant adversarial networks. In ECCV, 2018. 1, 2, 5, 15, 16, 17 [45] Yiying Li, Yongxin Yang, Wei Zhou, and Timothy Hospedales. Feature-critic networks for heterogeneous do- main generalization. In ICML, 2019. 14, 15 [46] Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? In NeurIPS, 2021. 2, 3, 4, 7, 8, 12, 14, 15 [47] Krikamol Muandet, David Balduzzi, and Bernhard Sch¨olkopf. Domain generalization via invariant feature representation. In ICML, 2013. 1, 2 [48] Hyeonseob Nam, HyunJae Lee, Jongchan Park, Wonjun Yoon, and Donggeun Yoo. Reducing domain gap by reducing style bias. In CVPR, 2021. 2, 5, 6, 15, 16, 17 [49] Prashant Pandey, Mrigank Raman, Sumanth Varambally, and Prathosh Ap. Generalization on unseen domains via inference- time label-preserving target projections. In CVPR, 2021. 1 [50] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In ICCV, 2019. 5, 17 [51] Mohammad Pezeshki, Oumar Kaba, Yoshua Bengio, Aaron C Courville, Doina Precup, and Guillaume Lajoie. Gradient star- vation: A learning proclivity in neural networks. In NeurIPS, 2021. 1, 5, 6, 15, 16, 17 [52] Alexandre Rame, Corentin Dancette, and Matthieu Cord. Fishr: Invariant gradient variances for out-of-distribution gen- eralization. In ICML, 2022. 1, 2, 5, 6, 15, 16, 17 [53] Yangjun Ruan, Yann Dubois, and Chris J Maddison. Optimal representations for covariate shift. In ICLR, 2022. 5, 15, 16, 17 [54] Bryan C Russell, Antonio Torralba, Kevin P Murphy, and William T Freeman. Labelme: a database and web-based tool for image annotation. IJCV, 77(1):157–173, 2008. 5 [55] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst- case generalization. In ICLR, 2020. 5, 15, 16, 17 [56] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bring- mann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. In NeurIPS, 2020. 2 [57] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad- cam: Visual explanations from deep networks via gradient- based localization. In ICCV, 2017. 7, 8, 11, 13 [58] Yuge Shi, Jeffrey Seely, Philip HS Torr, N Siddharth, Awni Hannun, Nicolas Usunier, and Gabriel Synnaeve. Gradient matching for domain generalization. In ICLR, 2021. 1, 2, 5, 6, 15, 16, 17 [59] Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In ECCV, 2016. 5, 6, 15, 16, 17 [60] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self- supervision for generalization under distribution shifts. In ICML, 2020. 1, 2, 4, 5, 7, 8, 11, 12, 13 [61] Vladimir Vapnik. The nature of statistical learning theory . Springer science & business media, 1999. 1, 5, 6, 15, 16, 17 [62] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In CVPR, 2017. 5, 16 [63] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Ol- shausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In ICLR, 2021. 2, 3, 4, 7, 8, 11, 12, 13 [64] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recog- nition from abbey to zoo. In CVPR, 2010. 5 [65] Zehao Xiao, Xiantong Zhen, Ling Shao, and Cees GM Snoek. Learning to generalize across domains on single test samples. In ICLR, 2022. 2 [66] Qinwei Xu, Ruipeng Zhang, Ya Zhang, Yanfeng Wang, and Qi Tian. A fourier-based framework for domain generaliza- tion. In CVPR, 2021. 1, 2 [67] Zhenlin Xu, Deyi Liu, Junlin Yang, Colin Raffel, and Marc Niethammer. Robust and generalizable visual representation learning via random convolutions. In ICLR, 2021. 7[68] Shen Yan, Huan Song, Nanxiang Li, Lincan Zou, and Liu Ren. Improve unsupervised domain adaptation with mixup training. arXiv preprint arXiv:2001.00677, 2020. 2, 5, 15, 16, 17 [69] Fu-En Yang, Yuan-Chia Cheng, Zu-Yun Shiau, and Yu- Chiang Frank Wang. Adversarial teacher-student representa- tion learning for domain generalization. In NeurIPS, 2021. 1, 2 [70] Nanyang Ye, Kaican Li, Haoyue Bai, Runpeng Yu, Lanqing Hong, Fengwei Zhou, Zhenguo Li, and Jun Zhu. Ood-bench: Quantifying and understanding two dimensions of out-of- distribution generalization. In CVPR, 2022. 5 [71] Fuming You, Jingjing Li, and Zhou Zhao. Test-time batch statistics calibration for covariate shift. arXiv preprint arXiv:2110.04065, 2021. 4 [72] Marvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine, and Chelsea Finn. Adaptive risk mini- mization: A meta-learning approach for tackling group distri- bution shift. arXiv preprint arXiv:2007.02931, 2020. 5, 15, 16, 17 [73] Marvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine, and Chelsea Finn. Adaptive risk mini- mization: Learning to adapt to domain shift. NeurIPS, 2021. 2 [74] Tao Zhong, Zhixiang Chi, Li Gu, Yang Wang, Yuanhao Yu, and Jin Tang. Meta-dmoe: Adapting to domain shift by meta- distillation from mixture-of-experts. In NeurIPS, 2022. 2 [75] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Do- main generalization with mixstyle. In ICLR, 2021. 1, 2, 3, 5, 6, 7, 12, 15, 16, 17 Appendix In this supplementary material, we provide, 1. Resource usage for ITTA in Section 7. 2. Grad-CAM visualizations of different loss terms in Section 8. 3. Parameter analysis of ITTA in Section 9; 4. Using a different augmentation skill for ITTA in Sec- tion 10. 5. Using different updating steps or a strategy for ITTA during the test phase in Section 11. 6. Using different network structures for the learnable consistency loss and adaptive parameters in Section 12. 7. Comparisons with other related methods in Section 13. 8. Detailed experimental results in the DomainBed bench- mark in Section 14. 7. Resource Usage Comparisons Between ITTA and the Baseline Model Requiring extra resources for our ITTA is a common lim- itation for existing test-time-based arts. To further evaluate our method, in this section, we compare FLOPS, model size, and inference time in Table 5. We compare only with ERM as most existing methods utilize the same network during in- ferences. We note that compare to the baseline model, ITTA requires extra Flops and processing time, this is because the adaptation process uses extra forward and backward steps during the test phase. While the parameters between the two models are similar because the newly included adaptive blocks are much smaller in size compared to the original model. Table 5. Resource comparisons during testing. Here inc. and exc. columns in ITTA indicate to include and exclude the TTA phase. Model Flops (G) Params (M) Time (s) Baseline 1.82 11.18 0.004 ITTA (inc.| exc.) 6.12 | 1.83 14.95 | 14.94 0.021 | 0.005 8. Grad-CAM Visualizations of Different Self- Supervised Objectives In Section 5 of the manuscript, we provide Grad-CAM [57] visualizations of our learnable consistency and the main losses to illustrate their alignment. To further show the differences between several TTT tasks [60, 63], we present more visual examples in this section. Results are shown in Figure 5. We observe that the entropy minimization [63] and rotation estimation [60] objectives do not activate the same regions as the main loss. As shown in the first row, for the class label of giraffe, both the main loss and our learned loss can correctly locate the two giraffes in the image, while the rotation estimation task can only locate one target, the same observation can be found when the learned weightsare disabled in our loss term. Meanwhile, although the two objects can be found for the entropy minimization task, the corresponding hot region does not align with that of the main loss. Similar phenomena can be observed in other samples. These visual examples demonstrate that our learned objective can better align with the main task than the TTT tasks adopted in previous works [60, 63], explaining why using the proposed learnable consistency loss can better improve TTT. 9. Parameter Analysis In this section, we analyze the hyper-parameter used in ITTA. We use the weight parameterα to balance the contri- butions from the main loss and weighted consistency loss (i.e. Lmain + αLwcont in Eq. (2) of our manuscript). To analyze the sensitivity of ITTA regarding different values of α, we conduct ablation studies in the PACS benchmark [37]. Results are listed in Table 6. We observe that the proposed ITTA can obtain favorable performances when α is in the range of 0.1 to 10, and it performs the best on average when setting as 1. We thus fix the parameter as 1 in all experi- ments. 10. A Different Augmentation Skill for ITTA In our manuscript, we use the existing augmentation strat- egy from [75] to obtain the augmented feature. In this sec- tion, we replace this implementation with that from [41] to further verify if our ITTA can still thrive with another aug- mentation skill. Different from [75] that mixes the statics of the feature to synthesize new information, [41] uses an affine transformation to create new features, where the weight for the transformation is sampled from a normal distribution with the mean value of one and standard value of zero, and the bias for the transformation is sampled from a normal distribution with the mean and standard values both zero. Experiments are conducted on the PACS benchmark [37] with the leave-one-out strategy. We compare ITTA with several different variants. (1) Ours w/o fw & TTT: this variant is the baseline model which uses the naive consistency loss for training and does not include TTT during the test phase. (2) Ours w/o fw: we disable the fw in our consistency loss, which uses the naive consistency loss for the test-time updating. (3) Ours w/o TTT: we do not update any parameters during the test phase. This variant is used to verify whether TTT can improve the pretrained model when replacing the augmentation strategy. We also compare these variants with the ERM method to show their effectivenesses. Results are listed in Table 7. We observe that ERM per- forms favorably against the baseline model, indicating that this augmentation strategy may not be beneficial for the training process. Meanwhile, we observe that when fw is disabled, the performances seem to decrease in 3 out of 4 target domains, and the average accuracy is also inferior to the baseline (i.e. Ours w/o fw & TTT). This result is in line with the finding in [46] that an inappropriate TTT task may deteriorate the performance. In comparison, we note that the performances are both improved when fw is enabled (i.e. Ours w/o TTT and Ours), which once again demonstrates that the proposed learnable consistency loss can improve the trained model. Moreover, we can also observe that when combining fw and TTT, our model is superior to other vari- ants and the ERM method. These results demonstrate that the proposed two strategies can improve the current TTT framework despite a less effective augmentation strategy. 11. Different Updating Steps or Strategies for ITTA In the manuscript, we use one TTT step for ITTA before during the testing step. In this section, we conduct experi- ments to evaluate the performances of ITTA with different TTT steps. Experiments are conducted on the PACS bench- mark [37] with the leave-one-out strategy, and each target domain is examined with 60 sets of random seeds and hyper- parameter settings. Results are listed in Table 8. We observe that the average accuracies of using more TTT steps are not improved greatly while the computational times are propor- tional to the TTT steps. To this end, we use one TTT step for ITTA as a compromise between accuracy and efficiency. We use the online setting from TTT [60] for all arts, which assumes test samples arrive sequentially and updates the adaptive blocks based on the states optimized from a previous sample. In this section, we also test ITTA in an episodic manner (i.e. Epi) [12]. Results in Table 8 suggest that while the episodic updating strategy performs slightly worse than the current scheme, and it still outperforms the baseline. 12. Different Network Structures for the Learnable Consistency Loss and Adaptive Parameters In our implementation, we use 10 layers of learnable pa- rameters for fw, and we use 5 layers of learnable parameters for fΘ after each block. In this section, we evaluate our ITTA with different network structures for these two mod- ules. Specifically, we compare the original implementation with the variants that use 1, 5, and 15 layers for fw and 1, 10, and 15 layers for fΘ to evaluate the performances of dif- ferent structures. Similarly, we conduct experiments on the PACS benchmark [37] with the leave-one-out strategy, and each target domain is examined with 60 sets of random seeds and hyper-parameter settings. Evaluation results are listed in Table 9. We observe that their differences in the average accuracy are rather subtle on account of the variances. To(a) Input (b) Entropy (c) Rotation (d) Ours w/o fw (e) Ours (f) Main Figure 5. Grad-CAM [57] visualizations from different loss terms. We use images with varying class labels (i.e. giraffe, elephant, house, and horse from top to bottom) from the four target domains of PACS [37] as inputs (i.e. art, cartoon, photo, and sketch domains from top to bottom). “Entropy” and “Rotation” here denote the entropy minimization and rotation estimation tasks in [63] and [60]. Ours w/o fw is the learnable consistency loss in Eq. (1) in the manuscript (i.e. ∥fw(z − z′)∥) when fw is disabled. The proposed learnable consistency loss can align well with the main classification task. Table 6. Sensitivity analysis of ITTA regarding different values ofα in the unseen domain from PACS [37]. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Values Target domain Avg.Art Cartoon Photo Sketch α = 0.1 83.9 ± 0.7 76.2 ± 1.1 94.8 ± 0.2 78.8 ± 0.8 83.4 ± 0.2 α = 1 (Ours) 84.7 ± 0.4 78.0 ± 0.4 94.5 ± 0.4 78.2 ± 0.3 83.8 ± 0.3 α = 10 83.9 ± 0.5 77.4 ± 0.6 94.2 ± 0.7 77.3 ± 0.8 83.2 ± 0.3 α = 100 81.5 ± 1.2 77.0 ± 0.6 92.6 ± 0.7 78.9 ± 2.1 82.5 ± 0.9 this end, we use the original implementation with 10 layers of learnable parameters for fw and 5 layers of learnable pa- rameters for fΘ, which performs relatively better than other variants. Since the adaptive blocks fΘ are attached after each layer of the network, one may wonder how the varying locations of the adaptive blocks affect the performance of ITTA. To answer this question, we further conduct experiments by adding the adaptive blocks after different layers of the orig- inal network. Denoting as Loc = lan given the n layers in the original network, we note that the model performs less effectively when the adaptive block is placed after the 1st layer of the network, and using all four adaptive blocks (i.e. ours) is more effective than other alternatives. 13. Comparisons with Other Related Methods Apart from the proposed ITTA, some other works also propose to include learnable parameters in their auxiliaryTable 7. Performances of our method with another augmentation strategy from [41] in the unseen domain from PACS [37]. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Model Target domain Avg.Art Cartoon Photo Sketch ERM 78.0 ± 1.3 73.4 ± 0.8 94.1 ± 0.4 73.6 ± 2.2 79.8 ± 0.4 Ours w/o fw & TTT 74.9 ± 0.4 74.1 ± 0.8 90.6 ± 0.3 79.7 ± 0.7 79.8 ± 0.4 Ours w/o fw 77.1 ± 1.0 73.6 ± 1.1 89.9 ± 0.4 78.4 ± 0.8 79.7 ± 0.2 Ours w/o TTT 77.5 ± 0.3 73.2 ± 0.6 92.4 ± 0.4 78.0 ± 1.0 80.3 ± 0.3 Ours (w/ fw & TTT) 79.2 ± 0.8 74.9 ± 1.1 92.2 ± 0.3 76.9 ± 0.7 80.8 ± 0.4 Table 8. Evaluations of ITTA in the unseen domain from PACS [37] with different TTT steps and updating strategies during the testing phase. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. The time consumption (TC) is computed using one image with the size of 224 × 224. Epi. denotes updating ITTA in an episodic manner. Steps Target domain Avg. TCArt Cartoon Photo Sketch 1 step (Ours) 84.7 ± 0.4 78.0 ± 0.4 94.5 ± 0.4 78.2 ± 0.3 83.8 ± 0.3 2.4 ms 2 step 84.2 ± 0.9 77.5 ± 0.6 94.4 ± 0.4 79.1 ± 1.0 83.8 ± 0.1 4.2 ms 3 step 84.5 ± 1.2 77.6 ± 0.6 94.0 ± 0.6 79.3 ± 0.1 83.9 ± 0.3 6.1 ms Epi. 83.6 ± 0.7 77.9 ± 0.5 95.2 ± 0.1 76.6 ± 0.5 83.3 ± 0.4 losses. Examples include MetaReg [2] and Feature-Critic [45] which both suggest using meta-learning to produce more general models. The main difference between these arts and ITTA is that parameters in the auxiliary loss from [2,45] are gradually refined by episode training, and they are updated via a gradient alignment step in ITTA (see Sec. 3.1 in the manuscript), which is much simpler. In this sec- tion, we compare ITTA with these two arts in the PACS dataset [37] using the same settings aforementioned. Be- cause MetaReg [2] does not release codes, we thus directly cite the data from their paper in the comparison. Different from others, the results in [2] are averaged by 5 trials accord- ing to their paper, which is much less than our experimental settings. Meanwhile, we also compare with TTT++ [46] which suggests storing the momentum of the features from the source domain and enforcing the similarity between mo- mentums of features from the source and target domains. We use the same setting in Section 5.1 from the manuscript to evaluate TTT++. Results are listed in Table 10. We observe that our method consistently outperforms that from [2,45,46] for both the cases with and without TTT, indicating that the proposed learnable consistency loss and updating method is not only simpler but also more effective than the losses in [2, 45]. 14. Detailed Results in the DomainBed Bench- mark [27] this section presents the average accuracy in each domain from different datasets. As shown in Table 11, 12, 13, 14, and 15, these results are detailed illustrations of the results in Table 2 in our manuscript. For all the experiments, we use the “training-domain validate set” as the model selection method. A total of 22 methods are examined for 60 trials in each unseen domain, and all methods are trained with the leave-one-out strategy using the ResNet18 [30] backbones.Table 9. Performances of our method with different network structures for the consistency loss (i.e. fw) and adaptive parameters (i.e. fΘ) in the unseen domain from PACS [37]. Here ‘Loc=lan’ locates the adaptive block after the n-th layer of the model (‘la4’ is the last layer). The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Structures Target domain Avg.Art Cartoon Photo Sketch Structures offw 1 layer 83.5 ±1.2 76.0 ±1.0 95.3 ±0.2 78.7 ±1.5 83.4 ±0.4 5 layers 83.7 ±0.6 76.8 ±0.9 94.6 ±0.3 78.8 ±0.3 83.5 ±0.3 10 layers (Ours) 84.7 ±0.4 78.0 ±0.4 94.5 ±0.4 78.2 ±0.3 83.8 ±0.3 15 layers 84.1 ±0.4 75.8 ±0.2 94.3 ±0.3 79.5 ±0.4 83.4 ±0.2 Structures offΘ 1 layer 84.0 ±0.6 77.4 ±0.5 94.4 ±0.5 78.3 ±0.4 83.5 ±0.3 5 layers (Ours) 84.7 ±0.4 78.0 ±0.4 94.5 ±0.4 78.2 ±0.3 83.8 ±0.3 10 layers 84.8 ±0.3 76.0 ±0.6 94.1 ±0.5 78.3 ±0.1 83.3 ±0.3 15 layers 83.9 ±0.8 76.0 ±0.5 93.8 ±0.4 78.7 ±1.4 83.1 ±0.6 Locations offΘ Loc=la1 83.4±0.7 76.8 ±0.3 94.4 ±0.3 77.8 ±0.3 83.1 ±0.3 Loc=la2 83.4±0.6 77.7 ±0.6 94.2 ±0.5 78.0 ±0.5 83.3 ±0.3 Loc=la3 84.0±0.4 77.5 ±0.3 94.4 ±0.1 77.8 ±0.1 83.4 ±0.2 Loc=la4 84.1±0.7 77.8 ±0.5 94.8 ±0.2 76.9 ±1.5 83.4 ±0.4 Table 10. Compare with learnable losses in [2, 45] in the unseen domain from PACS [37]. The reported accuracies ( %) and standard deviations are computed from 60 trials in each target domain except for [2] where the numbers are directly cited from their paper. Model Target domain Avg.Art Cartoon Photo Sketch MetaReg [2] 83.7 ± 0.2 77.2 ± 0.3 95.5 ± 0.2 70.3 ± 0.3 81.7 Feture-Critic [45] 78.4 ± 1.6 75.4 ± 1.2 92.6 ± 0.5 73.3 ± 1.4 80.0 ± 0.3 TTT++ [46] 84.3 ± 0.1 78.4 ± 0.5 93.8 ± 1.3 73.2 ± 3.2 82.4 ± 1.1 Ours w/o TTT 83.3 ± 0.5 76.0 ± 0.5 94.4 ± 0.5 76.7 ± 1.4 82.8 ± 0.3 Ours 84.7 ± 0.4 78.0 ± 0.4 94.5 ± 0.4 78.2 ± 0.3 83.8 ± 0.3 Table 11. Average accuracies on the PACS [37] datasets using the default hyper-parameter settings in DomainBed [27]. art cartoon photo sketch Average ERM [61] 78.0 ± 1.3 73.4 ± 0.8 94.1 ± 0.4 73.6 ± 2.2 79.8 ± 0.4 IRM [1] 76.9 ± 2.6 75.1 ± 0.7 94.3 ± 0.4 77.4 ± 0.4 80.9 ± 0.5 GroupGRO [55] 77.7 ± 2.6 76.4 ± 0.3 94.0 ± 0.3 74.8 ± 1.3 80.7 ± 0.4 Mixup [68] 79.3 ± 1.1 74.2 ± 0.3 94.9 ± 0.3 68.3 ± 2.7 79.2 ± 0.9 MLDG [38] 78.4 ± 0.7 75.1 ± 0.5 94.8 ± 0.4 76.7 ± 0.8 81.3 ± 0.2 CORAL [59] 81.5 ± 0.5 75.4 ± 0.7 95.2 ± 0.5 74.8 ± 0.4 81.7 ± 0.0 MMD [40] 81.3 ± 0.6 75.5 ± 1.0 94.0 ± 0.5 74.3 ± 1.5 81.3 ± 0.8 DANN [23] 79.0 ± 0.6 72.5 ± 0.7 94.4 ± 0.5 70.8 ± 3.0 79.2 ± 0.3 CDANN [44] 80.4 ± 0.8 73.7 ± 0.3 93.1 ± 0.6 74.2 ± 1.7 80.3 ± 0.5 MTL [6] 78.7 ± 0.6 73.4 ± 1.0 94.1 ± 0.6 74.4 ± 3.0 80.1 ± 0.8 SagNet [48] 82.9 ± 0.4 73.2 ± 1.1 94.6 ± 0.5 76.1 ± 1.8 81.7 ± 0.6 ARM [72] 79.4 ± 0.6 75.0 ± 0.7 94.3 ± 0.6 73.8 ± 0.6 80.6 ± 0.5 VREx [36] 74.4 ± 0.7 75.0 ± 0.4 93.3 ± 0.3 78.1 ± 0.9 80.2 ± 0.5 RSC [33] 78.5 ± 1.1 73.3 ± 0.9 93.6 ± 0.6 76.5 ± 1.4 80.5 ± 0.2 SelfReg [34] 82.5 ± 0.8 74.4 ± 1.5 95.4 ± 0.5 74.9 ± 1.3 81.8 ± 0.3 MixStyle [75] 82.6 ± 1.2 76.3 ± 0.4 94.2 ± 0.3 77.5 ± 1.3 82.6 ± 0.4 Fish [58] 80.9 ± 1.0 75.9 ± 0.4 95.0 ± 0.4 76.2 ± 1.0 82.0 ± 0.3 SD [51] 83.2 ± 0.6 74.6 ± 0.3 94.6 ± 0.1 75.1 ± 1.6 81.9 ± 0.3 CAD [53] 83.9 ± 0.8 74.2 ± 0.4 94.6 ± 0.4 75.0 ± 1.2 81.9 ± 0.3 CondCAD [53] 79.7 ± 1.0 74.2 ± 0.9 94.6 ± 0.4 74.8 ± 1.4 80.8 ± 0.5 Fishr [52] 81.2 ± 0.4 75.8 ± 0.8 94.3 ± 0.3 73.8 ± 0.6 81.3 ± 0.3 Ours 84.7 ± 0.4 78.0 ± 0.4 94.5 ± 0.4 78.2 ± 0.3 83.8 ± 0.3Table 12. Average accuracies on the VLCS [18] datasets using the default hyper-parameter settings in DomainBed [27]. Caltech LabelMe Sun VOC Average ERM [61] 97.7 ± 0.3 62.1 ± 0.9 70.3 ± 0.9 73.2 ± 0.7 75.8 ± 0.2 IRM [1] 96.1 ± 0.8 62.5 ± 0.3 69.9 ± 0.7 72.0 ± 1.4 75.1 ± 0.1 GroupGRO [55] 96.7 ± 0.6 61.7 ± 1.5 70.2 ± 1.8 72.9 ± 0.6 75.4 ± 1.0 Mixup [68] 95.6 ± 1.5 62.7 ± 0.4 71.3 ± 0.3 75.4 ± 0.2 76.2 ± 0.3 MLDG [38] 95.8 ± 0.5 63.3 ± 0.8 68.5 ± 0.5 73.1 ± 0.8 75.2 ± 0.3 CORAL [59] 96.5 ± 0.3 62.8 ± 0.1 69.1 ± 0.6 73.8 ± 1.0 75.5 ± 0.4 MMD [40] 96.0 ± 0.8 64.3 ± 0.6 68.5 ± 0.6 70.8 ± 0.1 74.9 ± 0.5 DANN [23] 97.2 ± 0.1 63.3 ± 0.6 70.2 ± 0.9 74.4 ± 0.2 76.3 ± 0.2 CDANN [44] 95.4 ± 1.2 62.6 ± 0.6 69.9 ± 1.3 76.2 ± 0.5 76.0 ± 0.5 MTL [6] 94.4 ± 2.3 65.0 ± 0.6 69.6 ± 0.6 71.7 ± 1.3 75.2 ± 0.3 SagNet [48] 94.9 ± 0.7 61.9 ± 0.7 69.6 ± 1.3 75.2 ± 0.6 75.4 ± 0.8 ARM [72] 96.9 ± 0.5 61.9 ± 0.4 71.6 ± 0.1 73.3 ± 0.4 75.9 ± 0.3 VREx [36] 96.2 ± 0.0 62.5 ± 1.3 69.3 ± 0.9 73.1 ± 1.2 75.3 ± 0.6 RSC [33] 96.2 ± 0.0 63.6 ± 1.3 69.8 ± 1.0 72.0 ± 0.4 75.4 ± 0.3 SelfReg [34] 95.8 ± 0.6 63.4 ± 1.1 71.1 ± 0.6 75.3 ± 0.6 76.4 ± 0.7 MixStyle [75] 97.3 ± 0.3 61.6 ± 0.1 70.4 ± 0.7 71.3 ± 1.9 75.2 ± 0.7 Fish [58] 97.4 ± 0.2 63.4 ± 0.1 71.5 ± 0.4 75.2 ± 0.7 76.9 ± 0.2 SD [51] 96.5 ± 0.4 62.2 ± 0.0 69.7 ± 0.9 73.6 ± 0.4 75.5 ± 0.4 CAD [53] 94.5 ± 0.9 63.5 ± 0.6 70.4 ± 1.2 72.4 ± 1.3 75.2 ± 0.6 CondCAD [53] 96.5 ± 0.8 62.6 ± 0.4 69.1 ± 0.2 76.0 ± 0.2 76.1 ± 0.3 Fishr [52] 97.2 ± 0.6 63.3 ± 0.7 70.4 ± 0.6 74.0 ± 0.8 76.2 ± 0.3 Ours 96.9 ± 1.2 63.7 ± 1.1 72.0 ± 0.3 74.9 ± 0.8 76.9 ± 0.6 Table 13. Average accuracies on the OfficeHome [62] datasets using the default hyper-parameter settings in DomainBed [27]. art clipart product real Average ERM [61] 52.2 ± 0.2 48.7 ± 0.5 69.9 ± 0.5 71.7 ± 0.5 60.6 ± 0.2 IRM [1] 49.7 ± 0.2 46.8 ± 0.5 67.5 ± 0.4 68.1 ± 0.6 58.0 ± 0.1 GroupGRO [55] 52.6 ± 1.1 48.2 ± 0.9 69.9 ± 0.4 71.5 ± 0.8 60.6 ± 0.3 Mixup [68] 54.0 ± 0.7 49.3 ± 0.7 70.7 ± 0.7 72.6 ± 0.3 61.7 ± 0.5 MLDG [38] 53.1 ± 0.3 48.4 ± 0.3 70.5 ± 0.7 71.7 ± 0.4 60.9 ± 0.2 CORAL [59] 55.1 ± 0.7 49.7 ± 0.9 71.8 ± 0.2 73.1 ± 0.5 62.4 ± 0.4 MMD [40] 50.9 ± 1.0 48.7 ± 0.3 69.3 ± 0.7 70.7 ± 1.3 59.9 ± 0.4 DANN [23] 51.8 ± 0.5 47.1 ± 0.1 69.1 ± 0.7 70.2 ± 0.7 59.5 ± 0.5 CDANN [44] 51.4 ± 0.5 46.9 ± 0.6 68.4 ± 0.5 70.4 ± 0.4 59.3 ± 0.4 MTL [6] 51.6 ± 1.5 47.7 ± 0.5 69.1 ± 0.3 71.0 ± 0.6 59.9 ± 0.5 SagNet [48] 55.3 ± 0.4 49.6 ± 0.2 72.1 ± 0.4 73.2 ± 0.4 62.5 ± 0.3 ARM [72] 51.3 ± 0.9 48.5 ± 0.4 68.0 ± 0.3 70.6 ± 0.1 59.6 ± 0.3 VREx [36] 51.1 ± 0.3 47.4 ± 0.6 69.0 ± 0.4 70.5 ± 0.4 59.5 ± 0.1 RSC [33] 49.0 ± 0.1 46.2 ± 1.5 67.8 ± 0.7 70.6 ± 0.3 58.4 ± 0.6 SelfReg [34] 55.1 ± 0.8 49.2 ± 0.6 72.2 ± 0.3 73.0 ± 0.3 62.4 ± 0.1 MixStyle [75] 50.8 ± 0.6 51.4 ± 1.1 67.6 ± 1.3 68.8 ± 0.5 59.6 ± 0.8 Fish [58] 54.6 ± 1.0 49.6 ± 1.0 71.3 ± 0.6 72.4 ± 0.2 62.0 ± 0.6 SD [51] 55.0 ± 0.4 51.3 ± 0.5 72.5 ± 0.2 72.7 ± 0.3 62.9 ± 0.2 CAD [53] 52.1 ± 0.6 48.3 ± 0.5 69.7 ± 0.3 71.9 ± 0.4 60.5 ± 0.3 CondCAD [53] 53.3 ± 0.6 48.4 ± 0.2 69.8 ± 0.9 72.6 ± 0.1 61.0 ± 0.4 Fishr [52] 52.6 ± 0.9 48.6 ± 0.3 69.9 ± 0.6 72.4 ± 0.4 60.9 ± 0.3 Ours 54.4 ± 0.2 52.3 ± 0.8 69.5 ± 0.3 71.7 ± 0.2 62.0 ± 0.2Table 14. Average accuracies on the TerraInc [4] datasets using the default hyper-parameter settings in DomainBed [27]. L100 L38 L43 L46 Average ERM [61] 42.1 ± 2.5 30.1 ± 1.2 48.9 ± 0.6 34.0 ± 1.1 38.8 ± 1.0 IRM [1] 41.8 ± 1.8 29.0 ± 3.6 49.6 ± 2.1 33.1 ± 1.5 38.4 ± 0.9 GroupGRO [55] 45.3 ± 4.6 36.1 ± 4.4 51.0 ± 0.8 33.7 ± 0.9 41.5 ± 2.0 Mixup [68] 49.4 ± 2.0 35.9 ± 1.8 53.0 ± 0.7 30.0 ± 0.9 42.1 ± 0.7 MLDG [38] 39.6 ± 2.3 33.2 ± 2.7 52.4 ± 0.5 35.1 ± 1.5 40.1 ± 0.9 CORAL [59] 46.7 ± 3.2 36.9 ± 4.3 49.5 ± 1.9 32.5 ± 0.7 41.4 ± 1.8 MMD [40] 49.1 ± 1.2 36.4 ± 4.8 50.4 ± 2.1 32.3 ± 1.5 42.0 ± 1.0 DANN [23] 44.3 ± 3.6 28.0 ± 1.5 47.9 ± 1.0 31.3 ± 0.6 37.9 ± 0.9 CDANN [44] 36.9 ± 6.4 32.7 ± 6.2 51.1 ± 1.3 33.5 ± 0.5 38.6 ± 2.3 MTL [6] 45.2 ± 2.6 31.0 ± 1.6 50.6 ± 1.1 34.9 ± 0.4 40.4 ± 1.0 SagNet [48] 36.3 ± 4.7 40.3 ± 2.0 52.5 ± 0.6 33.3 ± 1.3 40.6 ± 1.5 ARM [72] 41.5 ± 4.5 27.7 ± 2.4 50.9 ± 1.0 29.6 ± 1.5 37.4 ± 1.9 VREx [36] 48.0 ± 1.7 41.1 ± 1.5 51.8 ± 1.5 32.0 ± 1.2 43.2 ± 0.3 RSC [33] 42.8 ± 2.4 32.2 ± 3.8 49.6 ± 0.9 32.9 ± 1.2 39.4 ± 1.3 SelfReg [34] 46.1 ± 1.5 34.5 ± 1.6 49.8 ± 0.3 34.7 ± 1.5 41.3 ± 0.3 MixStyle [75] 50.6 ± 1.9 28.0 ± 4.5 52.1 ± 0.7 33.0 ± 0.2 40.9 ± 1.1 Fish [58] 46.3 ± 3.0 29.0 ± 1.1 52.7 ± 1.2 32.8 ± 1.0 40.2 ± 0.6 SD [51] 45.5 ± 1.9 33.2 ± 3.1 52.9 ± 0.7 36.4 ± 0.8 42.0 ± 1.0 CAD [53] 43.1 ± 2.6 31.1 ± 1.9 53.1 ± 1.6 34.7 ± 1.3 40.5 ± 0.4 CondCAD [53] 44.4 ± 2.9 32.9 ± 2.5 50.5 ± 1.3 30.8 ± 0.5 39.7 ± 0.4 Fishr [52] 49.9 ± 3.3 36.6 ± 0.9 49.8 ± 0.2 34.2 ± 1.3 42.6 ± 1.0 Ours 51.7 ± 2.4 37.6 ± 0.6 49.9 ± 0.6 33.6 ± 0.6 43.2 ± 0.5 Table 15. Average accuracies on the DomainNet [50] datasets using the default hyper-parameter settings in DomainBed [27]. clip info paint quick real sketch Average ERM [61] 50.4 ± 0.2 14.0 ± 0.2 40.3 ± 0.5 11.7 ± 0.2 52.0 ± 0.2 43.2 ± 0.3 35.3 ± 0.1 IRM [1] 43.2 ± 0.9 12.6 ± 0.3 35.0 ± 1.4 9.9 ± 0.4 43.4 ± 3.0 38.4 ± 0.4 30.4 ± 1.0 GroupGRO [55] 38.2 ± 0.5 13.0 ± 0.3 28.7 ± 0.3 8.2 ± 0.1 43.4 ± 0.5 33.7 ± 0.0 27.5 ± 0.1 Mixup [68] 48.9 ± 0.3 13.6 ± 0.3 39.5 ± 0.5 10.9 ± 0.4 49.9 ± 0.2 41.2 ± 0.2 34.0 ± 0.0 MLDG [38] 51.1 ± 0.3 14.1 ± 0.3 40.7 ± 0.3 11.7 ± 0.1 52.3 ± 0.3 42.7 ± 0.2 35.4 ± 0.0 CORAL [59] 51.2 ± 0.2 15.4 ± 0.2 42.0 ± 0.2 12.7 ± 0.1 52.0 ± 0.3 43.4 ± 0.0 36.1 ± 0.2 MMD [40] 16.6 ± 13.3 0.3 ± 0.0 12.8 ± 10.4 0.3 ± 0.0 17.1 ± 13.7 0.4 ± 0.0 7.9 ± 6.2 DANN [23] 45.0 ± 0.2 12.8 ± 0.2 36.0 ± 0.2 10.4 ± 0.3 46.7 ± 0.3 38.0 ± 0.3 31.5 ± 0.1 CDANN [44] 45.3 ± 0.2 12.6 ± 0.2 36.6 ± 0.2 10.3 ± 0.4 47.5 ± 0.1 38.9 ± 0.4 31.8 ± 0.2 MTL [6] 50.6 ± 0.2 14.0 ± 0.4 39.6 ± 0.3 12.0 ± 0.3 52.1 ± 0.1 41.5 ± 0.0 35.0 ± 0.0 SagNet [48] 51.0 ± 0.1 14.6 ± 0.1 40.2 ± 0.2 12.1 ± 0.2 51.5 ± 0.3 42.4 ± 0.1 35.3 ± 0.1 ARM [72] 43.0 ± 0.2 11.7 ± 0.2 34.6 ± 0.1 9.8 ± 0.4 43.2 ± 0.3 37.0 ± 0.3 29.9 ± 0.1 VREx [36] 39.2 ± 1.6 11.9 ± 0.4 31.2 ± 1.3 10.2 ± 0.4 41.5 ± 1.8 34.8 ± 0.8 28.1 ± 1.0 RSC [33] 39.5 ± 3.7 11.4 ± 0.8 30.5 ± 3.1 10.2 ± 0.8 41.0 ± 1.4 34.7 ± 2.6 27.9 ± 2.0 SelfReg [34] 47.9 ± 0.3 15.1 ± 0.3 41.2 ± 0.2 11.7 ± 0.3 48.8 ± 0.0 43.8 ± 0.3 34.7 ± 0.2 MixStyle [75] 49.1 ± 0.4 13.4 ± 0.0 39.3 ± 0.0 11.4 ± 0.4 47.7 ± 0.3 42.7 ± 0.1 33.9 ± 0.1 Fish [58] 51.5 ± 0.3 14.5 ± 0.2 40.4 ± 0.3 11.7 ± 0.5 52.6 ± 0.2 42.1 ± 0.1 35.5 ± 0.0 SD [51] 51.3 ± 0.3 15.5 ± 0.1 41.5 ± 0.3 12.6 ± 0.2 52.9 ± 0.2 44.0 ± 0.4 36.3 ± 0.2 CAD [53] 45.4 ± 1.0 12.1 ± 0.5 34.9 ± 1.1 10.2 ± 0.6 45.1 ± 1.6 38.5 ± 0.6 31.0 ± 0.8 CondCAD [53] 46.1 ± 1.0 13.3 ± 0.4 36.1 ± 1.4 10.7 ± 0.2 46.8 ± 1.3 38.7 ± 0.7 31.9 ± 0.7 Fishr [52] 47.8 ± 0.7 14.6 ± 0.2 40.0 ± 0.3 11.9 ± 0.2 49.2 ± 0.7 41.7 ± 0.1 34.2 ± 0.3 Ours 50.7 ± 0.7 13.9 ± 0.4 39.4 ± 0.5 11.9 ± 0.2 50.2 ± 0.3 43.5 ± 0.1 34.9 ± 0.1",
      "meta_data": {
        "arxiv_id": "2304.04494v2",
        "authors": [
          "Liang Chen",
          "Yong Zhang",
          "Yibing Song",
          "Ying Shan",
          "Lingqiao Liu"
        ],
        "published_date": "2023-04-10T10:12:38Z",
        "pdf_url": "https://arxiv.org/pdf/2304.04494v2.pdf"
      }
    },
    {
      "title": "Test Time Adaptation With Regularized Loss for Weakly Supervised Salient Object Detection"
    },
    {
      "title": "Tent: Fully Test-Time Adaptation by Entropy Minimization",
      "abstract": "A model must adapt itself to generalize to new and different data during\ntesting. In this setting of fully test-time adaptation the model has only the\ntest data and its own parameters. We propose to adapt by test entropy\nminimization (tent): we optimize the model for confidence as measured by the\nentropy of its predictions. Our method estimates normalization statistics and\noptimizes channel-wise affine transformations to update online on each batch.\nTent reduces generalization error for image classification on corrupted\nImageNet and CIFAR-10/100 and reaches a new state-of-the-art error on\nImageNet-C. Tent handles source-free domain adaptation on digit recognition\nfrom SVHN to MNIST/MNIST-M/USPS, on semantic segmentation from GTA to\nCityscapes, and on the VisDA-C benchmark. These results are achieved in one\nepoch of test-time optimization without altering training.",
      "full_text": "Published as a conference paper at ICLR 2021 TENT : F ULLY TEST-TIME ADAPTATION BY ENTROPY MINIMIZATION Dequan Wang1∗, Evan Shelhamer2∗†, Shaoteng Liu1, Bruno Olshausen1, Trevor Darrell1 dqwang@cs.berkeley.edu, shelhamer@google.com UC Berkeley1 Adobe Research2 ABSTRACT A model must adapt itself to generalize to new and different data during testing. In this setting of fully test-time adaptation the model has only the test data and its own parameters. We propose to adapt by test entropy minimization (tent 1): we optimize the model for conﬁdence as measured by the entropy of its predictions. Our method estimates normalization statistics and optimizes channel-wise afﬁne transformations to update online on each batch. Tent reduces generalization error for image classiﬁcation on corrupted ImageNet and CIFAR-10/100 and reaches a new state-of-the-art error on ImageNet-C. Tent handles source-free domain adapta- tion on digit recognition from SVHN to MNIST/MNIST-M/USPS, on semantic segmentation from GTA to Cityscapes, and on the VisDA-C benchmark. These results are achieved in one epoch of test-time optimization without altering training. 1 I NTRODUCTION Deep networks can achieve high accuracy on training and testing data from the same distribution, as evidenced by tremendous benchmark progress (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; He et al., 2016). However, generalization to new and different data is limited (Hendrycks & Dietterich, 2019; Recht et al., 2019; Geirhos et al., 2018). Accuracy suffers when the training (source) data differ from the testing (target) data, a condition known as dataset shift(Quionero-Candela et al., 2009). Models can be sensitive to shifts during testing that were not known during training, whether natural variations or corruptions, such as unexpected weather or sensor degradation. Nevertheless, it can be necessary to deploy a model on different data distributions, so adaptation is needed. During testing, the model must adapt given only its parameters and the target data. Thisfully test-time adaptation setting cannot rely on source data or supervision. Neither is practical when the model ﬁrst encounters new testing data, before it can be collected and annotated, as inference must go on. Real-world usage motivates fully test-time adaptation by data, computation, and task needs: 1. Availability. A model might be distributed without source data for bandwidth, privacy, or proﬁt. 2. Efﬁciency. It might not be computationally practical to (re-)process source data during testing. 3. Accuracy. A model might be too inaccurate without adaptation to serve its purpose. To adapt during testing we minimize the entropy of model predictions. We call this objective the test entropy and name our method tent after it. We choose entropy for its connections to error and shift. Entropy is related to error, as more conﬁdent predictions are all-in-all more correct (Figure 1). Entropy is related to shifts due to corruption, as more corruption results in more entropy, with a strong rank correlation to the loss for image classiﬁcation as the level of corruption increases (Figure 2). To minimize entropy, tent normalizes and transforms inference on target data by estimating statistics and optimizing afﬁne parameters batch-by-batch. This choice of low-dimensional, channel-wise feature modulation is efﬁcient to adapt during testing, even for online updates. Tent does not restrict or alter model training: it is independent of the source data given the model parameters. If the model can be run, it can be adapted. Most importantly, tent effectively reduces not just entropy but error. ∗Equal contribution. †Work done at Adobe Research; the author is now at DeepMind. 1Please see the project page at https://github.com/DequanWang/tent for the code and more. 1 arXiv:2006.10726v3  [cs.LG]  18 Mar 2021Published as a conference paper at ICLR 2021 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Entropy 0 20 40 60 80Error (%) Figure 1: Predictions with lower entropy have lower error rates on corrupted CIFAR-100-C. Certainty can serve as supervision during testing. 0.2 0.3 0.4 0.5 0.6 Entropy 0.2 0.4 0.6 0.8 1.0 1.2Loss = 0.61 original noise blur digital weather  level level Figure 2: More corruption causes more loss and entropy on CIFAR-100-C. Entropy can estimate the degree of shift without training data or labels. Our results evaluate generalization to corruptions for image classiﬁcation, to domain shift for digit recognition, and to simulation-to-real shift for semantic segmentation. For context with more data and optimization, we evaluate methods for robust training, domain adaptation, and self-supervised learning given the labeled source data. Tent can achieve less error given only the target data, and it improves on the state-of-the-art for the ImageNet-C benchmark. Analysis experiments support our entropy objective, check sensitivity to the amount of data and the choice of parameters for adaptation, and back the generality of tent across architectures. Our contributions • We highlight the setting of fully test-time adaptation with only target data and no source data. To emphasize practical adaptation during inference we benchmark with ofﬂine and online updates. • We examine entropy as an adaptation objective and propose tent: a test-time entropy minimization scheme to reduce generalization error by reducing the entropy of model predictions on test data. • For robustness to corruptions, tent reaches 44.0% error on ImageNet-C, better than the state-of- the-art for robust training (50.2%) and the strong baseline of test-time normalization (49.9%). • For domain adaptation, tent is capable of online and source-free adaptation for digit classiﬁcation and semantic segmentation, and can even rival methods that use source data and more optimization. 2 S ETTING : F ULLY TEST-TIME ADAPTATION Adaptation addresses generalization from source to target. A model fθ(x) with parameters θtrained on source data and labels xs,ys may not generalize when tested on shifted target data xt. Table 1 summarizes adaptation settings, their required data, and types of losses. Our fully test-time adaptation setting uniquely requires only the model fθ and unlabeled target data xt for adaptation during inference. Existing adaptation settings extend training given more data and supervision. Transfer learning by ﬁne-tuning (Donahue et al., 2014; Yosinski et al., 2014) needs target labels to (re-)train with a supervised loss L(xt,yt). Without target labels, our setting denies this supervised training. Domain adaptation (DA) (Quionero-Candela et al., 2009; Saenko et al., 2010; Ganin & Lempitsky, 2015; Tzeng et al., 2015) needs both the source and target data to train with a cross-domain loss L(xs,xt). Test-time training (TTT) (Sun et al., 2019b) adapts during testing but ﬁrst alters training to jointly optimize its supervised loss L(xs,ys) and self-supervised loss L(xs). Without source, our setting denies joint training across domains (DA) or losses (TTT). Existing settings have their purposes, but do not cover all practical cases when source, target, or supervision are not simultaneously available. Unexpected target data during testing requires test-time adaptation. TTT and our setting adapt the model by optimizing an unsupervised loss during testing L(xt). During training, TTT jointly optimizes this same loss on source data L(xs) with a supervised loss L(xs,ys), to ensure the parameters θare shared across losses for compatibility with adaptation by L(xt). Fully test-time adaptation is independent of the training data and training loss given the parameters θ. By not changing training, our setting has the potential to require less data and computation for adaptation. 2Published as a conference paper at ICLR 2021 Table 1: Adaptation settings differ by their data and therefore losses during training and testing. Of the source s and target t data xand labels y, our fully test-time setting only needs the target data xt. setting source data target data train loss test loss ﬁne-tuning - xt,yt L(xt,yt) - domain adaptation xs, ys xt L(xs,ys) + L(xs,xt) - test-time training xs, ys xt L(xs,ys) + L(xs) L(xt) fully test-time adaptation - xt - L(xt)     = f (     ; θ)  Loss (   ,      ) θ (a) training <latexit sha1_base64=\"YDIW6Mi/jc4gnv843zXedRTilJU=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49mF654lbdGcgy8XJSgRyNXvmr249ZGnGFTFJjOp6boJ9RjYJJPil1U8MTykZ0wDuWKhpx42ezUyfkxCp9EsbalkIyU39PZDQyZhwFtjOiODSL3lT8z+ukGF74mVBJilyx+aIwlQRjMv2b9IXmDOXYEsq0sLcSNqSaMrTplGwI3uLLy6R1VvVqVc+7qVXql3keRTiCYzgFD86hDtfQgCYwGMAzvMKbI50X5935mLcWnHzmEP7A+fwBBSaOGg==</latexit> x s <latexit sha1_base64=\"1psCF/1OyjuZ4TwBu21voNG0XaI=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8eK1hbaWDbbSbt0swm7GyGE/gQvHhTEq3/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dkorq2vrG+XNytb2zu5edf/gQcepYthisYhVJ6AaBZfYMtwI7CQKaRQIbAfj66nffkKleSzvTZagH9Gh5CFn1FjpLnvU/WrNrbszkGXiFaQGBZr96ldvELM0QmmYoFp3PTcxfk6V4UzgpNJLNSaUjekQu5ZKGqH289mpE3JilQEJY2VLGjJTf0/kNNI6iwLbGVEz0oveVPzP66YmvPRzLpPUoGTzRWEqiInJ9G8y4AqZEZkllClubyVsRBVlxqZTsSF4iy8vk/ZZ3Tuve97tea1xVeRRhiM4hlPw4AIacANNaAGDITzDK7w5wnlx3p2PeWvJKWYO4Q+czx8GrY4b</latexit> y s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"1psCF/1OyjuZ4TwBu21voNG0XaI=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8eK1hbaWDbbSbt0swm7GyGE/gQvHhTEq3/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dkorq2vrG+XNytb2zu5edf/gQcepYthisYhVJ6AaBZfYMtwI7CQKaRQIbAfj66nffkKleSzvTZagH9Gh5CFn1FjpLnvU/WrNrbszkGXiFaQGBZr96ldvELM0QmmYoFp3PTcxfk6V4UzgpNJLNSaUjekQu5ZKGqH289mpE3JilQEJY2VLGjJTf0/kNNI6iwLbGVEz0oveVPzP66YmvPRzLpPUoGTzRWEqiInJ9G8y4AqZEZkllClubyVsRBVlxqZTsSF4iy8vk/ZZ3Tuve97tea1xVeRRhiM4hlPw4AIacANNaAGDITzDK7w5wnlx3p2PeWvJKWYO4Q+czx8GrY4b</latexit> y s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"YDIW6Mi/jc4gnv843zXedRTilJU=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49mF654lbdGcgy8XJSgRyNXvmr249ZGnGFTFJjOp6boJ9RjYJJPil1U8MTykZ0wDuWKhpx42ezUyfkxCp9EsbalkIyU39PZDQyZhwFtjOiODSL3lT8z+ukGF74mVBJilyx+aIwlQRjMv2b9IXmDOXYEsq0sLcSNqSaMrTplGwI3uLLy6R1VvVqVc+7qVXql3keRTiCYzgFD86hDtfQgCYwGMAzvMKbI50X5935mLcWnHzmEP7A+fwBBSaOGg==</latexit> x s (b) fully test-time adaptation θ    = f (    ; θ+Δ)  Entropy (    ) <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t <latexit sha1_base64=\"m/ZzdjACtPk7VVv8qMUMxHkn5f0=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49YK9ccavuDGSZeDmpQI5Gr/zV7ccsjbhCJqkxHc9N0M+oRsEkn5S6qeEJZSM64B1LFY248bPZqRNyYpU+CWNtSyGZqb8nMhoZM44C2xlRHJpFbyr+53VSDC/8TKgkRa7YfFGYSoIxmf5N+kJzhnJsCWVa2FsJG1JNGdp0SjYEb/HlZdI6q3q1qufd1Cr1yzyPIhzBMZyCB+dQh2toQBMYDOAZXuHNkc6L8+58zFsLTj5zCH/gfP4ABquOGw==</latexit> x t <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t <latexit sha1_base64=\"m/ZzdjACtPk7VVv8qMUMxHkn5f0=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49YK9ccavuDGSZeDmpQI5Gr/zV7ccsjbhCJqkxHc9N0M+oRsEkn5S6qeEJZSM64B1LFY248bPZqRNyYpU+CWNtSyGZqb8nMhoZM44C2xlRHJpFbyr+53VSDC/8TKgkRa7YfFGYSoIxmf5N+kJzhnJsCWVa2FsJG1JNGdp0SjYEb/HlZdI6q3q1qufd1Cr1yzyPIhzBMZyCB+dQh2toQBMYDOAZXuHNkc6L8+58zFsLTj5zCH/gfP4ABquOGw==</latexit> x t <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t     = f (     ; θ)  Loss (   ,      ) θ (a) training <latexit sha1_base64=\"YDIW6Mi/jc4gnv843zXedRTilJU=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49mF654lbdGcgy8XJSgRyNXvmr249ZGnGFTFJjOp6boJ9RjYJJPil1U8MTykZ0wDuWKhpx42ezUyfkxCp9EsbalkIyU39PZDQyZhwFtjOiODSL3lT8z+ukGF74mVBJilyx+aIwlQRjMv2b9IXmDOXYEsq0sLcSNqSaMrTplGwI3uLLy6R1VvVqVc+7qVXql3keRTiCYzgFD86hDtfQgCYwGMAzvMKbI50X5935mLcWnHzmEP7A+fwBBSaOGg==</latexit> x s <latexit sha1_base64=\"1psCF/1OyjuZ4TwBu21voNG0XaI=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8eK1hbaWDbbSbt0swm7GyGE/gQvHhTEq3/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dkorq2vrG+XNytb2zu5edf/gQcepYthisYhVJ6AaBZfYMtwI7CQKaRQIbAfj66nffkKleSzvTZagH9Gh5CFn1FjpLnvU/WrNrbszkGXiFaQGBZr96ldvELM0QmmYoFp3PTcxfk6V4UzgpNJLNSaUjekQu5ZKGqH289mpE3JilQEJY2VLGjJTf0/kNNI6iwLbGVEz0oveVPzP66YmvPRzLpPUoGTzRWEqiInJ9G8y4AqZEZkllClubyVsRBVlxqZTsSF4iy8vk/ZZ3Tuve97tea1xVeRRhiM4hlPw4AIacANNaAGDITzDK7w5wnlx3p2PeWvJKWYO4Q+czx8GrY4b</latexit> y s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"1psCF/1OyjuZ4TwBu21voNG0XaI=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8eK1hbaWDbbSbt0swm7GyGE/gQvHhTEq3/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dkorq2vrG+XNytb2zu5edf/gQcepYthisYhVJ6AaBZfYMtwI7CQKaRQIbAfj66nffkKleSzvTZagH9Gh5CFn1FjpLnvU/WrNrbszkGXiFaQGBZr96ldvELM0QmmYoFp3PTcxfk6V4UzgpNJLNSaUjekQu5ZKGqH289mpE3JilQEJY2VLGjJTf0/kNNI6iwLbGVEz0oveVPzP66YmvPRzLpPUoGTzRWEqiInJ9G8y4AqZEZkllClubyVsRBVlxqZTsSF4iy8vk/ZZ3Tuve97tea1xVeRRhiM4hlPw4AIacANNaAGDITzDK7w5wnlx3p2PeWvJKWYO4Q+czx8GrY4b</latexit> y s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"YDIW6Mi/jc4gnv843zXedRTilJU=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49mF654lbdGcgy8XJSgRyNXvmr249ZGnGFTFJjOp6boJ9RjYJJPil1U8MTykZ0wDuWKhpx42ezUyfkxCp9EsbalkIyU39PZDQyZhwFtjOiODSL3lT8z+ukGF74mVBJilyx+aIwlQRjMv2b9IXmDOXYEsq0sLcSNqSaMrTplGwI3uLLy6R1VvVqVc+7qVXql3keRTiCYzgFD86hDtfQgCYwGMAzvMKbI50X5935mLcWnHzmEP7A+fwBBSaOGg==</latexit> x s (b) fully test-time adaptation θ    = f (    ; θ+Δ)  Entropy (    ) <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t <latexit sha1_base64=\"m/ZzdjACtPk7VVv8qMUMxHkn5f0=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49YK9ccavuDGSZeDmpQI5Gr/zV7ccsjbhCJqkxHc9N0M+oRsEkn5S6qeEJZSM64B1LFY248bPZqRNyYpU+CWNtSyGZqb8nMhoZM44C2xlRHJpFbyr+53VSDC/8TKgkRa7YfFGYSoIxmf5N+kJzhnJsCWVa2FsJG1JNGdp0SjYEb/HlZdI6q3q1qufd1Cr1yzyPIhzBMZyCB+dQh2toQBMYDOAZXuHNkc6L8+58zFsLTj5zCH/gfP4ABquOGw==</latexit> x t <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t <latexit sha1_base64=\"m/ZzdjACtPk7VVv8qMUMxHkn5f0=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49YK9ccavuDGSZeDmpQI5Gr/zV7ccsjbhCJqkxHc9N0M+oRsEkn5S6qeEJZSM64B1LFY248bPZqRNyYpU+CWNtSyGZqb8nMhoZM44C2xlRHJpFbyr+53VSDC/8TKgkRa7YfFGYSoIxmf5N+kJzhnJsCWVa2FsJG1JNGdp0SjYEb/HlZdI6q3q1qufd1Cr1yzyPIhzBMZyCB+dQh2toQBMYDOAZXuHNkc6L8+58zFsLTj5zCH/gfP4ABquOGw==</latexit> x t <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t Figure 3: Method overview. Tent does not alter training (a), but minimizes the entropy of predictions during testing (b) over a constrained modulation ∆, given the parameters θand target data xt. 3 M ETHOD : T EST ENTROPY MINIMIZATION VIA FEATURE MODULATION We optimize the model during testing to minimize the entropy of its predictions by modulating its features. We call our method tent for test entropy. Tent requires a compatible model, an objective to minimize (Section 3.1), and parameters to optimize over (Section 3.2) to fully deﬁne the algorithm (Section Section 3.3). Figure 3 outlines our method for fully test-time adaptation. The model to be adapted must be trained for the supervised task, probabilistic, and differentiable. No supervision is provided during testing, so the model must already be trained. Measuring the entropy of predictions requires a distribution over predictions, so the model must be probabilistic. Gradients are required for fast iterative optimization, so the model must be differentiable. Typical deep networks for supervised learning satisfy these model requirements. 3.1 E NTROPY OBJECTIVE Our test-time objective L(xt) is to minimize the entropy H(ˆy) of model predictions ˆy= fθ(xt). In particular, we measure the Shannon entropy (Shannon, 1948), H(ˆy) = −∑ cp(ˆyc) logp(ˆyc) for the probability ˆyc of class c. Note that optimizing a single prediction has a trivial solution: assign all probability to the most probable class. We prevent this by jointly optimizing batched predictions over parameters that are shared across the batch. Entropy is an unsupervised objective because it only depends on predictions and not annotations. However, as a measure of the predictions it is directly related to the supervised task and model. In contrast, proxy tasks for self-supervised learning are not directly related to the supervised task. Proxy tasks derive a self-supervised label y′from the input xt without the task label y. Examples of these proxies include rotation prediction (Gidaris et al., 2018), context prediction (Doersch et al., 2015), and cross-channel auto-encoding (Zhang et al., 2017). Too much progress on a proxy task could interfere with performance on the supervised task, and self-supervised adaptation methods have to limit or mix updates accordingly (Sun et al., 2019b;a). As such, care is needed to choose a proxy compatible with the domain and task, to design the architecture for the proxy model, and to balance optimization between the task and proxy objectives. Our entropy objective does not need such efforts. 3.2 M ODULATION PARAMETERS The model parameters θare a natural choice for test-time optimization, and these are the choice of prior work for train-time entropy minimization (Grandvalet & Bengio, 2005; Dhillon et al., 2020; Carlucci et al., 2017). However, θis the only representation of the training/source data in our setting, and altering θcould cause the model to diverge from its training. Furthermore, f can be nonlinear and θcan be high dimensional, making optimization too sensitive and inefﬁcient for test-time usage. 3Published as a conference paper at ICLR 2021 IN OUT+ <latexit sha1_base64=\"FGMSn1olAms3UkJ+mUM6lRBkJrw=\">AAAB6HicbVDLSgNBEOyNryS+oh69DAZBEMKuKHoMevGYgHlgsoTZSW8yZvbBzKwYlnyBFw+K5OoP+C/e/BqdJB40saChqOqmu8uLBVfatj+tzNLyyupaNpdf39jc2i7s7NZVlEiGNRaJSDY9qlDwEGuaa4HNWCINPIENb3A18Rv3KBWPwhs9jNENaC/kPmdUG6l63CkU7ZI9BVkkzg8plnPx+Pb94avSKXy0uxFLAgw1E1SplmPH2k2p1JwJHOXbicKYsgHtYcvQkAao3HR66IgcGqVL/EiaCjWZqr8nUhooNQw80xlQ3Vfz3kT8z2sl2r9wUx7GicaQzRb5iSA6IpOvSZdLZFoMDaFMcnMrYX0qKdMmm7wJwZl/eZHUT0rOaemsatK4hBmysA8HcAQOnEMZrqECNWCA8AjP8GLdWU/WqzWetWasn5k9+APr7RuTUJCF</latexit> \u0000 <latexit sha1_base64=\"8eHH7cr25vA7s0zJYYCDPQNSaT0=\">AAAB7XicbVDLSgNBEOyNrxhfUY+KDAbBU9gVQb0FvXhMwDwgWcLsZDYZM7OzzMwKYcnRuxcPinj1F/Id3vwGf8LJ46CJBQ1FVTfdXUHMmTau++VklpZXVtey67mNza3tnfzuXk3LRBFaJZJL1QiwppxFtGqY4bQRK4pFwGk96N+M/foDVZrJ6M4MYuoL3I1YyAg2Vqq1ulgI3M4X3KI7AVok3owUSoejyvfj0ajczn+2OpIkgkaGcKx103Nj46dYGUY4HeZaiaYxJn3cpU1LIyyo9tPJtUN0YpUOCqWyFRk0UX9PpFhoPRCB7RTY9PS8Nxb/85qJCS/9lEVxYmhEpovChCMj0fh11GGKEsMHlmCimL0VkR5WmBgbUM6G4M2/vEhqZ0XvvHhVsWlcwxRZOIBjOAUPLqAEt1CGKhC4hyd4gVdHOs/Om/M+bc04s5l9+APn4wd3ypLI</latexit> ⇥ <latexit sha1_base64=\"r9CoIRh1LwyAxszWUWZZpZEIYvU=\">AAAB7XicbVA9TwJBEJ3DL8Av1NLmIjGxIndGoyXRxhIT+YhwIXvLHqzs7V5254yE8B9sLDDG1tL/Yuev0QUsFHzJJC/vzWRmXpgIbtDzPp3M0vLK6lo2l1/f2NzaLuzs1oxKNWVVqoTSjZAYJrhkVeQoWCPRjMShYPWwfznx6/dMG67kDQ4SFsSkK3nEKUEr1VrIY2bahaJX8qZwF4n/Q4rlXDK+fX/4qrQLH62OomnMJFJBjGn6XoLBkGjkVLBRvpUalhDaJ13WtFQSuyQYTq8duYdW6biR0rYkulP198SQxMYM4tB2xgR7Zt6biP95zRSj82DIZZIik3S2KEqFi8qdvO52uGYUxcASQjW3t7q0RzShaAPK2xD8+ZcXSe245J+UTq9tGhcwQxb24QCOwIczKMMVVKAKFO7gEcbw7CjnyXlxXmetGednZg/+wHn7Btf2kwo=</latexit> \u0000 <latexit sha1_base64=\"icKTvSnYuWAwxCN4MXaVcPxJrUE=\">AAAB7HicbVBNS8NAEN34WetX1aMiwSJ4KokI6q3oxWMLpi20oWy2k3bpZhN2J0IJPXr24kERr/6G/g5v/gb/hNuPg7Y+GHi8N8PMvCARXKPjfFlLyyura+u5jfzm1vbObmFvv6bjVDHwWCxi1QioBsEleMhRQCNRQKNAQD3o3479+gMozWN5j4ME/Ih2JQ85o2gkrxUA0nah6JScCexF4s5IsXw0qn4/Ho8q7cJnqxOzNAKJTFCtm66ToJ9RhZwJGOZbqYaEsj7tQtNQSSPQfjY5dmifGqVjh7EyJdGeqL8nMhppPYgC0xlR7Ol5byz+5zVTDK/8jMskRZBsuihMhY2xPf7c7nAFDMXAEMoUN7farEcVZWjyyZsQ3PmXF0ntvORelK6rJo0bMkWOHJITckZccknK5I5UiEcY4eSJvJBXS1rP1pv1Pm1dsmYzB+QPrI8ftLWSVw==</latexit> \u0000 <latexit sha1_base64=\"6pSYsGji0D9Bm0vY9by0e43+pZo=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgxbArAfUW9OIxAfOAZAmzk95kzOzsMjMrhJAv8OJBEa9+kjf/xkmyB00saCiquunuChLBtXHdbye3tr6xuZXfLuzs7u0fFA+PmjpOFcMGi0Ws2gHVKLjEhuFGYDtRSKNAYCsY3c381hMqzWP5YMYJ+hEdSB5yRo2V6he9Ysktu3OQVeJlpAQZar3iV7cfszRCaZigWnc8NzH+hCrDmcBpoZtqTCgb0QF2LJU0Qu1P5odOyZlV+iSMlS1pyFz9PTGhkdbjKLCdETVDvezNxP+8TmrCa3/CZZIalGyxKEwFMTGZfU36XCEzYmwJZYrbWwkbUkWZsdkUbAje8surpHlZ9irlm3qlVL3N4sjDCZzCOXhwBVW4hxo0gAHCM7zCm/PovDjvzseiNedkM8fwB87nD3htjL0=</latexit> ÷ <latexit sha1_base64=\"KLNiQjydwC+UjsLtIanox9T+rq8=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoN6KXjxWsB/QhrLZbNqlu5uwuymU0L/gxYMiXv1D3vw3btoctPXBwOO9GWbmBQln2rjut1Pa2Nza3invVvb2Dw6PqscnHR2nitA2iXmsegHWlDNJ24YZTnuJolgEnHaDyX3ud6dUaRbLJzNLqC/wSLKIEWxyaRCy6bBac+vuAmideAWpQYHWsPo1CGOSCioN4Vjrvucmxs+wMoxwOq8MUk0TTCZ4RPuWSiyo9rPFrXN0YZUQRbGyJQ1aqL8nMiy0nonAdgpsxnrVy8X/vH5qohs/YzJJDZVkuShKOTIxyh9HIVOUGD6zBBPF7K2IjLHCxNh4KjYEb/XlddK5qnuN+u1jo9a8K+IowxmcwyV4cA1NeIAWtIHAGJ7hFd4c4bw4787HsrXkFDOn8AfO5w8aWY5N</latexit> µ <latexit sha1_base64=\"lbHwl5bkUbenc+Yo+u8yNzpxsy0=\">AAAB6nicbVDLSgNBEOyNrxhfUY+KDAbBU9gVQb0FvXhM0DwgWcLsZDYZMjO7zMwKYcnRoxcPinj1I/Id3vwGf8LJ46CJBQ1FVTfdXUHMmTau++VklpZXVtey67mNza3tnfzuXk1HiSK0SiIeqUaANeVM0qphhtNGrCgWAaf1oH8z9usPVGkWyXsziKkvcFeykBFsrHTXEkk7X3CL7gRokXgzUigdjirfj0ejcjv/2epEJBFUGsKx1k3PjY2fYmUY4XSYayWaxpj0cZc2LZVYUO2nk1OH6MQqHRRGypY0aKL+nkix0HogAtspsOnpeW8s/uc1ExNe+imTcWKoJNNFYcKRidD4b9RhihLDB5Zgopi9FZEeVpgYm07OhuDNv7xIamdF77x4VbFpXMMUWTiAYzgFDy6gBLdQhioQ6MITvMCrw51n5815n7ZmnNnMPvyB8/EDTj2RiQ==</latexit> \u0000 <latexit sha1_base64=\"xnrzB72KzfqBMQ17s1zlsxQWR+k=\">AAAB7XicbZDLSgMxFIbP1Fsdb1WXboJFcFVmRFAXYtGNywr2Au1QMmmmjU0yQ5IRytB3cONCETcufBT3bsS3Mb0stPWHwMf/n0POOWHCmTae9+3kFhaXllfyq+7a+sbmVmF7p6bjVBFaJTGPVSPEmnImadUww2kjURSLkNN62L8a5fV7qjSL5a0ZJDQQuCtZxAg21qq1NOsK3C4UvZI3FpoHfwrFiw/3PHn7civtwmerE5NUUGkIx1o3fS8xQYaVYYTTodtKNU0w6eMubVqUWFAdZONph+jAOh0Uxco+adDY/d2RYaH1QIS2UmDT07PZyPwva6YmOg0yJpPUUEkmH0UpRyZGo9VRhylKDB9YwEQxOysiPawwMfZArj2CP7vyPNSOSv5x6ezGK5YvYaI87ME+HIIPJ1CGa6hAFQjcwQM8wbMTO4/Oi/M6Kc05055d+CPn/Qf/xpJs</latexit> <latexit sha1_base64=\"9MzbukliF0G5U4WyINCTJmMNjA8=\">AAACNnicdVBNS8NAFNz4bf2KevSyWAQFLUlR9CiK4EWoYFuhiWWz3dSlu0nYfVFL6K/y4u/w1osHRbz6E9y0PWjVgYVhZh773gSJ4Bocp29NTE5Nz8zOzRcWFpeWV+zVtZqOU0VZlcYiVtcB0UzwiFWBg2DXiWJEBoLVg85p7tfvmNI8jq6gmzBfknbEQ04JGKlpX3gyxZ5gIRCl4nvsSQK3QZCd9RoPTfB3sad5W5Kb8j+h7Xx+D5vszk3Zb9pFp+QMgH8Td0SKaIRK0372WjFNJYuACqJ1w3US8DOigFPBegUv1SwhtEParGFoRCTTfjY4u4e3jNLCYazMiwAP1O8TGZFad2VgkvnCetzLxb+8RgrhkZ/xKEmBRXT4UZgKDDHOO8QtrhgF0TWEUMXNrpjeEkUomKYLpgR3/OTfpFYuuQcl53K/eHwyqmMObaBNtI1cdIiO0TmqoCqi6BH10St6s56sF+vd+hhGJ6zRzDr6AevzC4nRq7w=</latexit> µ  E [ x t ] , \u0000 2  E [( µ \u0000 x t ) 2 ] <latexit sha1_base64=\"5uCFLjsyhVlotMr43Rw1BdZFk0s=\">AAACYXicbZFLS+RAFIUrGZ/tK+Ms3RQ2gqC0iSgzy2bcuHTAVqHTNDfVN21hVRKqbmamCf0nZzcbN/4RKzH4vlBw+O659TiVFEpaCsP/nv9lYXFpeWW1s7a+sbkVfN2+snlpBA5ErnJzk4BFJTMckCSFN4VB0InC6+TurO5f/0ZjZZ5d0qzAkYZpJlMpgBwaB3/jKWgNPFaYEhiT/+EtOeBxAYYkqFgD3UqqzudHL6wxHfI4QXo73YBPh59RbRkH3bAXNsU/iqgVXdbWxTj4F09yUWrMSCiwdhiFBY2qekuhcN6JS4sFiDuY4tDJDDTaUdUkNOd7jkx4mhu3MuINfT1RgbZ2phPnrO9r3/dq+FlvWFL6Y1TJrCgJM/F0UFoqTjmv4+YTaVCQmjkBwkh3Vy5uwYAg9ykdF0L0/skfxdVxLzrthb9Ouv2fbRwrbIftsn0Wse+sz87ZBRswwe69BW/D2/Qe/FU/8LefrL7Xznxjb8rfeQSpH7dZ</latexit> \u0000  \u0000 + @ H / @\u0000 , \u0000  \u0000 + @ H / @\u0000 normalization transformation Figure 4: Tent modulates features during testing by estimating normalization statistics µ,σ and optimizing transformation parameters γ,β. Normalization and transformation apply channel-wise scales and shifts to the features. The statistics and parameters are updated on target data without use of source data. In practice, adapting γ,β is efﬁcient because they make up <1% of model parameters. For stability and efﬁciency, we instead only update feature modulations that are linear (scales and shifts), and low-dimensional (channel-wise). Figure 4 shows the two steps of our modulations: normalization by statistics and transformation by parameters. Normalization centers and standardizes the input xinto ¯x= (x−µ)/σby its mean µand standard deviation σ. Transformation turns ¯xinto the output x′= γ¯x+ βby afﬁne parameters for scale γand shift β. Note that the statistics µ,σ are estimated from the data while the parameters γ,β are optimized by the loss. For implementation, we simply repurpose the normalization layers of the source model. We update their normalization statistics and afﬁne parameters for all layers and channels during testing. 3.3 A LGORITHM Initialization The optimizer collects the afﬁne transformation parameters {γl,k,βl,k}for each normalization layer land channel kin the source model. The remaining parameters θ\\{γl,k,βl,k} are ﬁxed. The normalization statistics {µl,k,σl,k}from the source data are discarded. Iteration Each step updates the normalization statistics and transformation parameters on a batch of data. The normalization statistics are estimated for each layer in turn, during the forward pass. The transformation parameters γ,β are updated by the gradient of the prediction entropy ∇H(ˆy), during the backward pass. Note that the transformation update follows the prediction for the current batch, and so it only affects the next batch (unless forward is repeated). This needs just one gradient per point of additional computation, so we use this scheme by default for efﬁciency. Termination For online adaptation, no termination is necessary, and iteration continues as long as there is test data. For ofﬂine adaptation, the model is ﬁrst updated and then inference is repeated. Adaptation may of course continue by updating for multiple epochs. 4 E XPERIMENTS We evaluate tent for corruption robustness on CIFAR-10/CIFAR-100 and ImageNet, and for domain adaptation on digit adaptation from SVHN to MNIST/MNIST-M/USPS. Our implementation is in PyTorch (Paszke et al., 2019) with the pycls library (Radosavovic et al., 2019). Datasets We run on image classiﬁcation datasets for corruption and domain adaptation conditions. For large-scale experiments we choose ImageNet (Russakovsky et al., 2015), with 1,000 classes, a training set of 1.2 million, and a validation set of 50,000. For experiments at an accessible scale we choose CIFAR-10/CIFAR-100 (Krizhevsky, 2009), with 10/100 classes, a training set of 50,000, and a test set of 10,000. For domain adaptation we choose SVHN (Netzer et al., 2011) as source and MNIST (LeCun et al., 1998)/MNIST-M (Ganin & Lempitsky, 2015)/USPS (Hull, 1994) as targets, with ten classes for the digits 0–9. SVHN has color images of house numbers from street views with a training set of 73,257 and test set of 26,032. MNIST/MNIST-M/USPS have handwritten digits with a training sets of 60,000/60,000/7,291 and test sets of 10,000/10,000/2,007. Models For corruption we use residual networks (He et al., 2016) with 26 layers (R-26) on CIFAR- 10/100 and 50 layers (R-50) on ImageNet. For domain adaptation we use the R-26 architecture. For fair comparison, all methods in each experimental condition share the same architecture. Our networks are equipped with batch normalization (Ioffe & Szegedy, 2015). For the source model without adaptation, the normalization statistics are estimated during training on the source data. For all test-time adaptation methods, we estimate these statistics during testing on the target data, as done in concurrent work on adaptation by normalization (Schneider et al., 2020; Nado et al., 2020). 4Published as a conference paper at ICLR 2021 Table 2: Corruption benchmark on CIFAR-10-C and CIFAR-100-C for the highest severity. Tent has least error, with less optimization than domain adaptation (RG, UDA-SS) and test-time training (TTT), and improves on test-time norm (BN). Method Source Target Error (%) C10-C C100-C Source train 40.8 67.2 RG train train 18.3 38.9 UDA-SS train train 16.7 47.0 TTT train test 17.5 45.0 BN test 17.3 42.6 PL test 15.7 41.2 Tent (ours) test 14.3 37.3 originalgaussshot impulsedefocus glassmotionzoomsnowfrostfog bright contrastelasticpixeljpeg 0 25 50 75Error (%) source 59.5% norm 49.9% tent 44.0% ANT 50.2% Figure 5: Corruption benchmark on ImageNet-C: error for each type averaged over severity levels. Tent improves on the prior state-of-the-art, adver- sarial noise training (Rusak et al., 2020), by fully test-time adaptation without altering training. Optimization We optimize the modulation parameters γ,β following the training hyperparameters for the source model with few changes. On ImageNet we optimize by SGD with momentum; on other datasets we optimize by Adam (Kingma & Ba, 2015). We lower the batch size (BS) to reduce memory usage for inference, then lower the learning rate (LR) by the same factor to compensate (Goyal et al., 2017). On ImageNet, we set BS = 64 and LR = 0.00025, and on other datasets we set BS = 128 and LR = 0.001.We control for ordering by shufﬂing and sharing the order across methods. Baselines We compare to domain adaptation, self-supervision, normalization, and pseudo-labeling: • source applies the trained classiﬁer to the test data without adaptation, • adversarial domain adaptation (RG) reverses the gradients of a domain classiﬁer on source and target to optimize for a domain-invariant representation (Ganin & Lempitsky, 2015), • self-supervised domain adaptation (UDA-SS) jointly trains self-supervised rotation and position tasks on source and target to optimize for a shared representation (Sun et al., 2019a), • test-time training (TTT) jointly trains for supervised and self-supervised tasks on source, then keeps training the self-supervised task on target during testing (Sun et al., 2019b), • test-time normalization (BN) updates batch normalization statistics (Ioffe & Szegedy, 2015) on the target data during testing (Schneider et al., 2020; Nado et al., 2020), • pseudo-labeling (PL) tunes a conﬁdence threshold, assigns predictions over the threshold as labels, and then optimizes the model to these pseudo-labels before testing (Lee, 2013). Only test-time normalization (BN), pseudo-labeling (PL), and tent (ours) are fully test-time adaptation methods. See Section 2 for an explanation and contrast with domain adaptation and test-time training. 4.1 R OBUSTNESS TO CORRUPTIONS To benchmark robustness to corruption, we make use of common image corruptions (see Appendix A for examples). The CIFAR-10/100 and ImageNet datasets are turned into the CIFAR-10/100-C and ImageNet-C corruption benchmarks by duplicating their test/validation sets and applying 15 types of corruptions at ﬁve severity levels (Hendrycks & Dietterich, 2019). Tent improves more with less data and computation.Table 2 reports errors averaged over corrup- tion types at the severest level of corruption. On CIFAR-10/100-C we compare all methods, including those that require joint training across domains or losses, given the convenient sizes of these datasets. Adaptation is ofﬂine for fair comparison with ofﬂine baselines. Tent improves on the fully test-time adaptation baselines (BN, PL) but also the domain adaptation (RG, UDA-SS) and test-time training (TTT) methods that need several epochs of optimization on source and target. Tent consistently improves across corruption types.Figure 5 plots the error for each corruption type averaged over corruption levels on ImageNet-C. We compare the most efﬁcient methods—source, normalization, and tent—given the large scale of the source data (>1 million images) needed by other methods and the 75 target combinations of corruption types and levels. Tent and BN adapt online to rival the efﬁciency of inference without adaptation. Tent reaches the least error for most corruption types without increasing the error on the original data. 5Published as a conference paper at ICLR 2021 Table 3: Digit domain adaptation from SVHN to MNIST/MNIST-M/USPS. Source-free adaptation is not only feasible, but more efﬁcient. Tent always improves on normalization (BN), and in 2/3 cases achieves less error than domain adaptation (RG, UDA-SS) without joint training on source & target. Method Source Target Epochs Error (%) Source + Target MNIST MNIST-M USPS Source train - 18.2 39.7 19.3 RG train train 10 + 10 15.0 33.4 18.9 UDA-SS train train 10 + 10 11.1 22.2 18.4 BN test 0 + 1 15.7 39.7 18.0 Tent (ours) test 0 + 1 10.0 37.0 16.3 Tent (ours) test 0 + 10 8.2 36.8 14.4 Tent reaches a new state-of-the-art without altering training.The state-of-the-art methods for robustness extend training with adversarial noise (ANT) (Rusak et al., 2020) for 50.2% error or mixtures of data augmentations (AugMix) (Hendrycks et al., 2020) for 51.7% error. Combined with stylization from external images (SIN) (Geirhos et al., 2019), ANT+SIN reaches 47.4%. Tent reaches a new state-of-the-art of 44.0% by online adaptation and 42.3% by ofﬂine adaptation. It improves on ANT for all types except noise, on which ANT is trained. This requires just one gradient per test point, without more optimization on the training set (ANT, AugMix) or use of external images (SIN). Among fully test-time adaptation methods, tent reduces the error beyond test-time normalization for 18% relative improvement. In concurrent work, Schneider et al. (2020) report 49.3% error for test-time normalization, for which tent still gives 14% relative improvement. 4.2 S OURCE -FREE DOMAIN ADAPTATION We benchmark digit adaptation (Ganin & Lempitsky, 2015; Tzeng et al., 2015; 2017; Shu et al., 2018) for shifts from SVHN to MNIST/MNIST-M/USPS. Recall that unsupervised domain adaptation makes use the labeled source data and unlabeled target data, while our fully test-time adaptation setting denies use of source data. Adaptation is ofﬂine for fair comparison with ofﬂine baselines. Tent adapts to target without source.Table 3 reports the target errors for domain adaptation and fully test-time adaptation methods. Test-time normalization (BN) marginally improves, while adversarial domain adaptation (RG) and self-supervised domain adaptation (UDA-SS) improve more by joint training on source and target. Tent always has lower error than the source model and BN, and it achieves the lowest error in 2/3 cases, even in just one epoch and without use of source data. While encouraging for fully test-time adaptation, unsupervised domain adaptation remains necessary for the highest accuracy and harder shifts. For SVHN-to-MNIST, DIRT-T (Shu et al., 2018) achieves a remarkable 0.6% error 2. For MNIST-to-SVHN, a difﬁcult shift with source-only error of 71.3%, DIRT-T reaches45.5% and UDA-SS reaches 38.7%. Tent fails on this shift and increases error to 79.8%. In this case success presently requires joint optimization over source and target. Tent needs less computation, but still improves with more.Tent adapts efﬁciently on target data alone with just one gradient per point. RG & UDA-SS also use the source data (SVHN train), which is ∼7×the size of the target data (MNIST test), and optimize for 10 epochs. Tent adapts with ∼80× less computation. With more updates, tent reaches 8.2% error in 10 epochs and 6.5% in 100 epochs. With online updates, tent reaches 12.5% error in one epoch and 8.4% error in 10 epochs. Tent scales to semantic segmentation.To show scalability to large models and inputs, we evaluate semantic segmentation (pixel-wise classiﬁcation) on a domain shift from a simulated source to a real target. The source is GTA (Richter et al., 2017), a video game in an urban environment, and the target is Cityscapes (Cordts et al., 2016), an urban autonomous driving dataset. The model is HRNet-W18, a fully convolutional network (Shelhamer et al., 2017) with high-resolution architecture (Wang et al., 2020). The target intersection-over-union scores (higher is better) are source 28.8%, BN 31.4%, and tent 35.8% with ofﬂine optimization by Adam. For adaptation to a single image, tent reaches 36.4% in 10 iterations with episodic optimization. See the appendix for a qualitative example (Appendix B). 2We exclude DIRT-T from our experiments because of incomparable differences in architecture and model selection. DIRT-T tunes with labeled target data, but we do not. Please refer to Shu et al. (2018) for more detail. 6Published as a conference paper at ICLR 2021 Figure 6: Tent reduces the entropy and loss. We plot changes in entropy∆Hand loss ∆Lfor all of CIFAR-100-C. Change in entropy rank-correlates with change in loss: note the dark diagonal and the rank correlation coefﬁcient of 0.22. (a) Source (b) BN  (c) Tent (d) Oracle  Figure 7: Adapted features on CIFAR-100-C with Gaussian noise (front) and reference features without corruption (back). Corruption shifts fea- tures away from the reference, but BN reduces the shifts. Tent instead shifts features more, and closer to an oracle that optimizes on target labels. Tent scales to the VisDA-C challenge.To show adaptation on a more difﬁcult benchmark, we evaluate on the VisDA-C challenge (Peng et al., 2017). The task is object recognition for 12 classes where the source data is synthesized by rendering 3D models and the target data is collected from real scenes. The validation error for our source model (ResNet-50, pretrained on ImageNet) is 56.1%, while tent reaches 45.6%, and improves to 39.6% by updating all layers except for the ﬁnal classiﬁer as done by Liang et al. (2020). Although ofﬂine source-free adaptation by model adaptation (Li et al., 2020) or SHOT (Liang et al., 2020) can reach lower error with more computation and tuning, tent can adapt online during testing. 4.3 A NALYSIS Tent reduces entropy and error.Figure 6 veriﬁes tent does indeed reduce the entropy and the task loss (softmax cross-entropy). We plot changes in entropy and loss on CIFAR-100-C for all 75 corruption type/level combinations. Both axes are normalized by the maximum entropy of a prediction (log 100) and clipped to ±1. Most points have lower entropy and error after adaptation. Tent needs feature modulation.We ablate the normalization and transformation steps of feature modulation. Not updating normalization increases errors, and can fail to improve over BN and PL. Not updating transformation parameters reduces the method to test-time normalization. Updating only the last layer of the model can improve but then degrades with further optimization. Updating the full model parameters θnever improves over the unadapted source model. Tent generalizes across target data.Adaptation could be limited to the points used for updates. We check that adaptation generalizes across points by adapting on target train and not target test. Test errors drop: CIFAR-100-C error goes from 37.3% to 34.2% and SVHN-to-MNIST error goes from 8.2% to 6.5%. (Train is larger than test; when subsampling to the same size errors differ by <0.1%.) Therefore the adapted modulation is not point speciﬁc but general. Tent modulation differs from normalization.Modulation normalizes and transforms features. We examine the combined effect. Figure 7 contrasts adapted features on corrupted data against reference features on uncorrupted data. We plot features from the source model, normalization, tent, and an oracle that optimizes on the target labels. Normalization makes features more like the reference, but tent does not. Instead, tent makes features more like the oracle. This suggests a different and task-speciﬁc effect. See the appendix for visualizations of more layers (Appendix C). 7Published as a conference paper at ICLR 2021 Tent adapts alternative architectures.Tent is architecture agnostic in principle. To gauge its generality in practice, we evaluate new architectures based on self-attention (SAN) (Zhao et al., 2020) and equilibrium solving (MDEQ) (Bai et al., 2020) for corruption robustness on CIFAR-100-C. Table 4 shows that tent reduces error with the same settings as convolutional residual networks. Table 4: Tent adapts alternative architectures on CIFAR-100-C without tuning. Results are error (%). SAN-10 (pair) SAN-10 (patch) MDEQ (large) Source BN Tent Source BN Tent Source BN Tent 55.3 39.7 36.7 48.0 31.8 29.2 53.3 44.9 41.7 5 R ELATED WORK We relate tent to existing adaptation, entropy minimization, and feature modulation methods. Train-Time AdaptationDomain adaptation jointly optimizes on source and target by cross-domain losses L(xs,xt) to mitigate shift. These losses optimize feature alignment (Gretton et al., 2009; Sun et al., 2017), adversarial invariance (Ganin & Lempitsky, 2015; Tzeng et al., 2017), or shared proxy tasks (Sun et al., 2019a). Transduction (Gammerman et al., 1998; Joachims, 1999; Zhou et al., 2004) jointly optimizes on train and test to better ﬁt speciﬁc test instances. While effective in their settings, neither applies when joint use of source/train and target/test is denied. Tent adapts on target alone. Recent “source-free” methods (Li et al., 2020; Kundu et al., 2020; Liang et al., 2020) also adapt without source data. Li et al. (2020); Kundu et al. (2020) rely on generative modeling and optimize multiple models with multiple losses. Kundu et al. (2020); Liang et al. (2020) also alter training. Tent does not need generative modeling, nor does it alter training, and so it can deployed more generally to adapt online with much more computational efﬁciency. SHOT (Liang et al., 2020) adapts by informa- tion maximization (entropy minimization and diversity regularization), but differs in its other losses and its parameterization. These source-free methods optimize ofﬂine with multiple losses for multiple epochs, which requires more tuning and computation than tent, but may achieve more accuracy with more computation. Tent optimizes online with just one loss and an efﬁcient parameterization of modulation to emphasize fully test-time adaptation during inference. We encourage examination of each of these works on the frontier of adaptation without source data. Chidlovskii et al. (2016) are the ﬁrst to motivate adaptation without source data for legal, commercial, or technical concerns. They adapt predictions by applying denoising auto-encoders while we adapt models by entropy minimization. We share their motivations, but the methods and experiments differ. Test-Time AdaptationTent adapts by test-time optimization and normalization to update the model. Test-time adaptation of predictions, through which harder and uncertain cases are adjusted based on easier and certain cases (Jain & Learned-Miller, 2011), provides inspiration for certainty-based model adaptation schemes like our own. Test-time training (TTT) (Sun et al., 2019b) also optimizes during testing, but differs in its loss and must alter training. TTT relies on a proxy task, such as recognizing rotations of an image, and so its loss depends on the choice of proxy. (Indeed, its authors caution that the proxy must be “both well-deﬁned and non-trivial in the new domain”). TTT alters training to optimize this proxy loss on source before adapting to target. Tent adapts without proxy tasks and without altering training. Normalizing feature statistics is common for domain adaptation (Gretton et al., 2009; Sun et al., 2017). For batch normalization Li et al. (2017); Carlucci et al. (2017) separate source and target statistics during training. Schneider et al. (2020); Nado et al. (2020) estimate target statistics during testing to improve generalization. Tent builds on test-time normalization to further reduce generalization error. Entropy MinimizationEntropy minimization is a key regularizer for domain adaptation (Carlucci et al., 2017; Shu et al., 2018; Saito et al., 2019; Roy et al., 2019), semi-supervised learning (Grandvalet & Bengio, 2005; Lee, 2013; Berthelot et al., 2019), and few-shot learning (Dhillon et al., 2020). Regularizing entropy penalizes decisions at high densities in the data distribution to improve accuracy for distinct classes (Grandvalet & Bengio, 2005). These methods regularize entropy during training in concert with other supervised and unsupervised losses on additional data. Tent is the ﬁrst to minimize 8Published as a conference paper at ICLR 2021 entropy during testing, for adaptation to dataset shifts, without other losses or data. Entropic losses are common; our contribution is to exhibit entropy as the sole lossfor fully test-time adaptation. Feature ModulationModulation makes a model vary with its input. We optimize modulations that are simpler than the full model for stable and efﬁcient adaptation. We modulate channel-wise afﬁne transformations, for their effectiveness in tandem with normalization (Ioffe & Szegedy, 2015; Wu & He, 2018), and for their ﬂexibility in conditioning for different tasks (Perez et al., 2018). These normalization and conditioning methods optimize the modulation during training by a supervised loss, but keep it ﬁxed during testing. We optimize the modulation during testing by an unsupervised loss, so that it can adapt to different target data. 6 D ISCUSSION Tent reduces generalization error on shifted data by test-time entropy minimization. In minimizing entropy, the model adapts itself to feedback from its own predictions. This is truly self-supervised self-improvement. Self-supervision of this sort is totally deﬁned by the supervised task, unlike proxy tasks designed to extract more supervision from the data, and yet it remarkably still reduces error. Nevertheless, errors due to corruption and other shifts remain, and therefore more adaptation is needed. Next steps should pursue test-time adaptation on more and harder types of shift, over more general parameters, and by more effective and efﬁcient losses. Shifts Tent reduces error for a variety of shifts including image corruptions, simple changes in appearance for digits, and simulation-to-real discrepancies. These shifts are popular as standardized benchmarks, but other real-world shifts exist. For instance, the CIFAR 10.1 and ImageNetV2 test sets (Recht et al., 2018; 2019), made by reproducing the dataset collection procedures, entail natural but unknown shifts. Although error is higher on both sets, indicating the presence of shift, tent does not improve generalization. Adversarial shifts (Szegedy et al., 2014) also threaten real-world usage, and attackers keep adapting to defenses. While adversarial training (Madry et al., 2018) makes a difference, test-time adaptation could help counter such test-time attacks. Parameters Tent modulates the model by normalization and transformation, but much of the model stays ﬁxed. Test-time adaptation could update more of the model, but the issue is to identify parameters that are both expressive and reliable, and this may interact with the choice of loss. TTT adapts multiple layers of features shared by supervised and self-supervised models and SHOT adapts all but the last layer(s) of the model. These choices depend on the model architecture, the loss, and tuning. For tent modulation is reliable, but the larger shift on VisDA is better addressed by the SHOT parameterization. Jointly adapting the input could be a more general alternative. If a model can adapt itself on target, then perhaps its input gradients might optimize spatial transformations or image translations to reduce shift without source data. Losses Tent minimizes entropy. For more adaptation, is there an effective loss for general but episodic test-time optimization? Entropy is general across tasks but limited in scope. It needs batches for optimization, and cannot update episodically on one point at a time. TTT can do so, but only with the right proxy task. For less computation, is there an efﬁcient loss for more local optimization? Tent and TTT both require full (re-)computation of the model for updates because they depend on its predictions. If the loss were instead deﬁned on the representation, then updates would require less forward and backward computation. Returning to entropy speciﬁcally, this loss may interact with calibration (Guo et al., 2017), as better uncertainty estimation could drive better adaptation. We hope that the fully test-time adaptation setting can promote new methods for equipping a model to adapt itself, just as tent yields a new model with every update. ACKNOWLEDGMENTS We thank Eric Tzeng for discussions on domain adaptation, Bill Freeman for comments on the experiments, Yu Sun for consultations on test-time training, and Kelsey Allen for feedback on the exposition. We thank the anonymous reviewers of ICLR 2021 for their feedback, which certainly improved the latest adaptation of the paper. 9Published as a conference paper at ICLR 2021 REFERENCES Shaojie Bai, Vladlen Koltun, and J Zico Kolter. Multiscale deep equilibrium models. arXiv preprint arXiv:2006.08656, 2020. David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. In NeurIPS, 2019. Fabio Maria Carlucci, Lorenzo Porzi, Barbara Caputo, Elisa Ricci, and Samuel Rota Bulo. Autodial: Automatic domain alignment layers. In 2017 IEEE International Conference on Computer Vision (ICCV), pp. 5077–5085. IEEE, 2017. Boris Chidlovskii, Stephane Clinchant, and Gabriela Csurka. Domain adaptation in the absence of source domain data. In SIGKDD, pp. 451–460, 2016. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016. Guneet Singh Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano Soatto. A baseline for few-shot image classiﬁcation. In ICLR, 2020. Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context prediction. In ICCV, 2015. J. Donahue, Y . Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In ICML, 2014. A Gammerman, V V ovk, and V Vapnik. Learning by transduction. InUAI, 1998. Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In ICML, 2015. Robert Geirhos, Carlos RM Temme, Jonas Rauber, Heiko H Schütt, Matthias Bethge, and Felix A Wichmann. Generalisation in humans and deep neural networks. In NeurIPS, 2018. Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and Wieland Brendel. Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. In International Conference on Learning Representations, 2019. Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. In ICLR, 2018. Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In NeurIPS, 2005. A. Gretton, AJ. Smola, J. Huang, M. Schmittfull, KM. Borgwardt, and B. Schölkopf. Covariate shift and local learning by distribution matching. In Dataset Shift in Machine Learning, pp. 131–160. MIT Press, Cambridge, MA, USA, 2009. Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In ICML, 2017. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, June 2016. Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In ICLR, 2019. Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. Augmix: A simple data processing method to improve robustness and uncertainty. In ICLR, 2020. Jonathan J. Hull. A database for handwritten text recognition research. TPAMI, 1994. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015. 10Published as a conference paper at ICLR 2021 Vidit Jain and Erik Learned-Miller. Online domain adaptation of a pre-trained cascade of classiﬁers. In CVPR, 2011. Thorsten Joachims. Transductive inference for text classiﬁcation using support vector machines. In ICML, 1999. Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiﬁcation with deep convolutional neural networks. NeurIPS, 25, 2012. Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009. Jogendra Nath Kundu, Naveen Venkat, R Venkatesh Babu, et al. Universal source-free domain adaptation. In CVPR, pp. 4544–4553, 2020. Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. Dong-Hyun Lee. Pseudo-label: The simple and efﬁcient semi-supervised learning method for deep neural networks. In ICML Workshop on challenges in representation learning, 2013. Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, and Si Wu. Model adaptation: Unsupervised domain adaptation without source data. In CVPR, June 2020. Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou. Revisiting batch normalization for practical domain adaptation. In ICLRW, 2017. Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In ICML, 2020. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018. Zachary Nado, Shreyas Padhy, D Sculley, Alexander D’Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robustness under covariate shift. arXiv preprint arXiv:2006.10963, 2020. Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. NeurIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019. Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko. VisDA: The visual domain adaptation challenge. arXiv preprint arXiv:1710.06924, 2017. Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. In AAAI, 2018. Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset shift in machine learning. MIT Press, Cambridge, MA, USA, 2009. Ilija Radosavovic, Justin Johnson, Saining Xie, Wan-Yen Lo, and Piotr Dollár. On network design spaces for visual recognition. In ICCV, 2019. Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do cifar-10 classiﬁers generalize to cifar-10? arXiv preprint arXiv:1806.00451, 2018. Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet classiﬁers generalize to ImageNet? In ICML, 2019. Stephan R Richter, Zeeshan Hayder, and Vladlen Koltun. Playing for benchmarks. In ICCV, 2017. Subhankar Roy, Aliaksandr Siarohin, Enver Sangineto, Samuel Rota Bulo, Nicu Sebe, and Elisa Ricci. Unsuper- vised domain adaptation using feature-whitening and consensus loss. In CVPR, 2019. 11Published as a conference paper at ICLR 2021 Evgenia Rusak, Lukas Schott, Roland S Zimmermann, Julian Bitterwolf, Oliver Bringmann, Matthias Bethge, and Wieland Brendel. A simple way to make neural networks robust against diverse image corruptions. In ECCV, 2020. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. ImageNet large scale visual recognition challenge. IJCV, 2015. Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to new domains. In European conference on computer vision, pp. 213–226. Springer, 2010. Kuniaki Saito, Donghyun Kim, Stan Sclaroff, Trevor Darrell, and Kate Saenko. Semi-supervised domain adaptation via minimax entropy. In ICCV, 2019. Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improv- ing robustness against common corruptions by covariate shift adaptation. arXiv preprint arXiv:2006.16971, 2020. C.E. Shannon. A mathematical theory of communication. Bell system technical journal, 27, 1948. Evan Shelhamer, Jonathan Long, and Trevor Darrell. Fully convolutional networks for semantic segmentation. PAMI, 2017. Rui Shu, Hung H Bui, Hirokazu Narui, and Stefano Ermon. A dirt-t approach to unsupervised domain adaptation. In ICLR, 2018. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015. Baochen Sun, Jiashi Feng, and Kate Saenko. Correlation alignment for unsupervised domain adaptation. In Domain Adaptation in Computer Vision Applications, pp. 153–171. Springer, 2017. Yu Sun, Eric Tzeng, Trevor Darrell, and Alexei A Efros. Unsupervised domain adaptation through self- supervision. arXiv preprint arXiv:1909.11825, 2019a. Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A Efros, and Moritz Hardt. Test-time training for out-of-distribution generalization. arXiv preprint arXiv:1909.13231, 2019b. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. 2014. Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. In ICCV, 2015. Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In CVPR, 2017. Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, et al. Deep high-resolution representation learning for visual recognition. PAMI, 2020. Yuxin Wu and Kaiming He. Group normalization. In ECCV, 2018. Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In NeurIPS, 2014. Richard Zhang, Phillip Isola, and Alexei A Efros. Split-brain autoencoders: Unsupervised learning by cross- channel prediction. In CVPR, 2017. Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition. In CVPR, 2020. Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, and Bernhard Schölkopf. Learning with local and global consistency. NeurIPS, 2004. 12Published as a conference paper at ICLR 2021 APPENDIX This supplement summarizes the image corruptions used in our experiments, highlights a qualitative example of instance-wise adaptation for semantic segmentation, and visualizes feature shifts across more layers. A R OBUSTNESS TO CORRUPTIONS In Section 4.1 we evaluate methods on a common image corruptions benchmark. Table 2 reports errors on the most severe level of corruption, level 5, and Figure 5 reports errors for each corruption type averaged across each of the levels 1–5. We summarize these corruptions types by example in Figure 8. Gaussian Noise  Shot Noise  Impulse Noise  Defocus Blur  Frosted Glass Blur Motion Blur  Zoom Blur  Snow  Frost  Fog Brightness  Contrast  Elastic  Pixelate  JPEG Figure 8: Examples of each corruption type in the image corruptions benchmark. While synthetic, this set of corruptions aims to represent natural factors of variation like noise, blur, weather, and digital imaging effects. This ﬁgure is reproduced from Hendrycks & Dietterich (2019). B S OURCE -FREE ADAPTATION FOR SEMANTIC SEGMENTATION Figure 9 shows a qualitative result on source-free adaptation for semantic segmentation (pixel-wise classiﬁcation) with simulation-to-real (sim-to-real) shift. For this sim-to-real condition, the source data is simulated while the target data is real. Our source data is GTA Richter et al. (2017), a visually-sophisticated video game set in an urban environment, and our target data is Cityscapes Cordts et al. (2016), an urban autonomous driving dataset. The supervised model is HRnet-W18, a fully convolutional network Shelhamer et al. (2017) in the high-resolution network family Wang et al. (2020). For this qualitative example, we run tent on a single image for multiple iterations, because an image is in effect a batch of pixels. This demonstrates adaptation to a target instance, without any further access to the target domain through usage of multiple images from the target distribution. 13Published as a conference paper at ICLR 2021 image label source-only tent, iteration 1 tent, iteration 5 tent, iteration 10 Figure 9: Adaptation for semantic segmentation with simulation-to-real shift from GTA Richter et al. (2017) to Cityscapes Cordts et al. (2016). Tent only uses the target data, and optimizes over a single image as a dataset of pixel-wise predictions. This episodic optimization in effect ﬁts a custom model to each image of the target domain. In only 10 iterations our method suppresses noise (see the completion of the street segment, in purple) and recovers missing classes (see the motorcycle and rider, center). 14Published as a conference paper at ICLR 2021 C F EATURE SHIFTS ACROSS LAYERS AND METHODS (a) Source (b) BN (c) Tent (d) Oracle Layer 2 Layer 5 Layer 8 Layer 11 Layer 14 Layer 18 Layer 20 Layer 23 Layer 26 Figure 10: Adapted features on CIFAR-100-C with Gaussian noise (front) and reference features without corruption (back). Corruption shifts the source features from the reference. BN shifts the features back to be more like the reference. Tent shifts features to be less like the reference, and more like an oracle that optimizes on target labels. 15",
      "meta_data": {
        "arxiv_id": "2006.10726v3",
        "authors": [
          "Dequan Wang",
          "Evan Shelhamer",
          "Shaoteng Liu",
          "Bruno Olshausen",
          "Trevor Darrell"
        ],
        "published_date": "2020-06-18T17:55:28Z",
        "pdf_url": "https://arxiv.org/pdf/2006.10726v3.pdf"
      }
    },
    {
      "title": "What How and When Should Object Detectors Update in Continually Changing Test Domains?",
      "abstract": "It is a well-known fact that the performance of deep learning models\ndeteriorates when they encounter a distribution shift at test time. Test-time\nadaptation (TTA) algorithms have been proposed to adapt the model online while\ninferring test data. However, existing research predominantly focuses on\nclassification tasks through the optimization of batch normalization layers or\nclassification heads, but this approach limits its applicability to various\nmodel architectures like Transformers and makes it challenging to apply to\nother tasks, such as object detection. In this paper, we propose a novel online\nadaption approach for object detection in continually changing test domains,\nconsidering which part of the model to update, how to update it, and when to\nperform the update. By introducing architecture-agnostic and lightweight\nadaptor modules and only updating these while leaving the pre-trained backbone\nunchanged, we can rapidly adapt to new test domains in an efficient way and\nprevent catastrophic forgetting. Furthermore, we present a practical and\nstraightforward class-wise feature aligning method for object detection to\nresolve domain shifts. Additionally, we enhance efficiency by determining when\nthe model is sufficiently adapted or when additional adaptation is needed due\nto changes in the test distribution. Our approach surpasses baselines on widely\nused benchmarks, achieving improvements of up to 4.9\\%p and 7.9\\%p in mAP for\nCOCO $\\rightarrow$ COCO-corrupted and SHIFT, respectively, while maintaining\nabout 20 FPS or higher.",
      "full_text": "What, How, and When Should Object Detectors Update in Continually Changing Test Domains? Jayeon Yoo1 Dongkwan Lee1 Inseop Chung1 Donghyun Kim2∗ Nojun Kwak1∗ 1Seoul National University 2Korea University 1{jayeon.yoo, biancco, jis3613, nojunk}@snu.ac.kr 2d kim@korea.ac.kr Abstract It is a well-known fact that the performance of deep learning models deteriorates when they encounter a dis- tribution shift at test time. Test-time adaptation (TTA) al- gorithms have been proposed to adapt the model online while inferring test data. However, existing research pre- dominantly focuses on classification tasks through the op- timization of batch normalization layers or classification heads, but this approach limits its applicability to various model architectures like Transformers and makes it chal- lenging to apply to other tasks, such as object detection. In this paper, we propose a novel online adaption approach for object detection in continually changing test domains, considering which part of the model to update, how to up- date it, and when to perform the update. By introducing architecture-agnostic and lightweight adaptor modules and only updating these while leaving the pre-trained backbone unchanged, we can rapidly adapt to new test domains in an efficient way and prevent catastrophic forgetting. Fur- thermore, we present a practical and straightforward class- wise feature aligning method for object detection to resolve domain shifts. Additionally, we enhance efficiency by deter- mining when the model is sufficiently adapted or when ad- ditional adaptation is needed due to changes in the test dis- tribution. Our approach surpasses baselines on widely used benchmarks, achieving improvements of up to 4.9%p and 7.9%p in mAP for COCO → COCO-corrupted and SHIFT, respectively, while maintaining about 20 FPS or higher. 1. Introduction Although deep learning models have demonstrated remark- able success in numerous vision-related tasks, they remain susceptible to domain shifts where the test data distribu- tion differs from that of the training data [3, 25, 40]. In real-world applications, domain shifts frequently occur at test-time due to natural variations, corruptions, changes in weather conditions (e.g., fog, rain) , camera sensor differ- Figure 1. We propose an online adaptation method for object detection in continually changing test domains. Object detectors trained with clean images suffer from performance degradation due to various corruption, such as camera sensor degradation or environmental changes (Direct-Test). Updating full parameters for online adaptation require a large number of test samples and vul- nerable to drastic domain changes (Full-Finetuning), while using only our lightweight adaptor is robust and quickly adapts within a few time steps (Ours). We can further improve efficiency by skip- ping unnecessary adaptation steps (Ours-Skip). ences (e.g., pixelate, defocus blur) , and various other fac- tors. Test-Time Adaptation (TTA) [3, 25, 30, 40, 43, 47] has been proposed to solve the domain shifts in test-time by adapting models to a specific target (test) distribution in an online manner. Furthermore, it is essential to take into account continuously changing test distributions, as the test distribution has the potential to undergo changes and devel- opments as time progresses (i.e., Continual Test-time Adap- tation (CTA)). For instance, autonomous driving systems may experience transitions from clear and sunny conditions to rainy or from daytime to nighttime, which causes contin- ually changing domain shifts [39]. While it is an important research topic, continual test-time adaptation for object de- tection has not been well explored. Recently, several TTA methods [6, 29, 36] tailored for 1 arXiv:2312.08875v1  [cs.CV]  12 Dec 2023object detection have been proposed. ActMAD [29] aligns all the output feature maps ( RC×H×W ) after Batch Nor- malization (BN) layers [14] to adapt the test domain to be similar to that of the training domain. However, this ap- proach requires significant memory during adaptation and does not explicitly consider the objects present in the image. TeST [36] and STFAR [6] adapt to a test domain by utiliz- ing weak and strong augmented test samples with a teacher- student network [37], but they significantly increase infer- ence costs since they require additional forward passes and update steps. Also, these methods update all network pa- rameters, making them highly inefficient in online adapta- tion and vulnerable to losing task-specific knowledge when the test domain experiences continual or drastic changes. In this paper, we aim to develop an efficient continual test-time adaptation (CTA) method for object detection. We investigate the following three key aspects to improve ef- ficiency; what to update: while previous TTA methods for object detection [6, 29, 36] use full fine-tuning, updating all parameters at test time, they are inefficient and prone to losing task-specific knowledge in relatively complex object detection tasks. Updating BN layers, as done in many TTA methods for classification [17, 30, 43, 47], is not as effective for object detection, given its smaller batch size compared to classification and the limitation in applying various back- bones, such as Transformer [26, 41].how to update: several previous TTA methods for object detection [6, 36] adapt the model by using teacher-student networks, resulting in a significant decrease in inference speed, which is detri- mental during test time. While another existing method [29] aligns feature distributions for adaptation, it does not con- sider each object individually, focusing only on image fea- tures, making it less effective for object detection. when to update: most TTA or CTA methods update models using all incoming test samples. However, it is inefficient to update continuously the model if it is already sufficiently adapted when the change of the test domain is not significant. To this end, (1) we propose an efficient continual test- time adaptation method for object detectors to adapt to continually changing test domains through the use of lightweight adaptors which require only 0.54%∼0.89% ad- ditional parameters compared to the full model. It exhibits efficiency in parameters, memory usage, and adaptation time, along with robustness to continuous domain shifts without catastrophic forgetting. Additionally, it demon- strates wide applicability to various backbone types com- pared to BN-based TTA methods [17, 22, 30, 43, 47, 48]. (2) To enhance the adaptation effectiveness in the object detec- tion task, we align the feature distribution of the test domain with that of the training domain at both the image-level and object-level using only the mean and variance of features. For estimating the mean of the test domain features, we employ Exponentially Moving Average (EMA) as we can leverage only the current incoming test samples, not the en- tire test domain data. Due to the unavailability of training data access, we utilize only the mean and variance of the features from a few training samples. (3) We also introduce two novel criteria that do not require additional resources to determine when the model needs adaptation to enhance efficiency in a continually changing test domain environ- ment. As illustrated in Fig. 1, our approach Ours, employ- ing adaptors, tends to adapt much faster to domain changes compared to full parameter updates. This enables efficient TTA by using only a few test samples to update the adaptor and skipping the rest of the updates as shown in Ours-Skip. Our main contributions are summarized as follows: • We introduce an architecture-agnostic lightweight adap- tor, constituting only a maximum of 0.89% of the total model parameters, into the backbone of the object de- tector to adapt the model in a continually changing test domain. This approach ensures efficiency in parameters, memory usage, and adaptation speed, demonstrating the robust preservation of task-specific knowledge owing to its inherent structural characteristics. • We propose a straightforward and effective adaptation loss for CTA in object detection tasks. This is achieved by aligning the distribution of training and test domain fea- tures at both the image and object levels, utilizing only the mean and variance of a few training samples and EMA- updated mean features of the test domain. • We also propose two criteria to determine when the model requires adaptation, enabling dynamic skipping or resum- ing adaptation as needed. This enhancement significantly boosts inference speed by up to about 2 times while main- taining adaptation performance. • Our adaptation method proves effective for diverse types of domain shifts, including weather changes and sensor variations, regardless of whether the domain shift is dras- tic or continuous. In particular, our approach consistently improves the mAP by up to 7.9% in COCO →COCO-C and SHIFT-Discrete/Continuous with higher than 20 FPS. 2. Related Work Test-time adaptation. Recently, there has been a surge of interest in research that adapts models online using unla- beled test samples while simultaneously inferring the test sample to address the domain shift problem, where the test data distribution differs from that of the training data. There are two lines for online adaptation to the test do- main, Test-time Training (TTT) and Test-time Adaptation (TTA). TTT [1, 2, 25, 40] involves modifying the model architecture during training to train it with self-supervised loss, allowing adaptation to the test domain in the test time by applying this self-supervised loss to the unlabeled test samples. On the other hand, TTA aims to adapt the trained model directly to the test domain without specifically tai- 2lored model architectures or losses during training time. NORM [35] and DUA [28] address the domain shifts by adjusting the statistics of batch normalization (BN) layers using the current test samples, without updating other pa- rameters, inspired by [21]. Following this, [22, 30, 43, 48] and [17] update the affine parameters of BN layers using unsupervised loss, entropy minimization loss to enhance the confidence of test data predictions, and feature distribution alignments loss, respectively. Several studies [15, 16] up- date the classifier head using the pseudo-prototypes from the test domain. However, these methods limit their appli- cability to architectures without BN layers or to object de- tection tasks that involve multiple objects in a single im- age. Others [29, 38, 47] update full parameters for online adaptation to the test domain in an online manner, but this approach is inefficient and susceptible to the noisy signal from the unsupervised loss. While existing TTA methods are oriented towards classification tasks, we aim to propose an effective and efficient method for online adaptation in the object detection task. Continual test-time adaptation. Recent studies [31, 44] point out that existing TTA methods have primarily focused on adapting to test domains following an i.i.d assumption and may not perform well when the test data distribution deviates from this assumption. [44] introduces a Contin- ual TTA (CTA) method designed for scenarios where the test domain continuously changes over time. This poses challenges in preventing the model from over-adapting to a particular domain shift and preserving the knowledge of the pre-trained model to avoid catastrophic forgetting. In the field of CTA, the self-training strategy adopting an Exponentially Moving Average (EMA) teacher-student structure is attracting interest as an effective algorithm en- abling robust representation to be learned through self- knowledge distillation. In many studies, the EMA teacher- student structure and catastrophic restoration of source model weights have been proposed as a solution to achieve the goal of CTA [4, 44, 45]. Approaches using source re- play [32], and anti-forgetting regularization [30] have also achieved good performances in robust continuous adapta- tion. Furthermore, there is growing attention on methods that mitigate the computational and memory challenges as- sociated with CTA, such as [12], which relies on updates to batch normalization statistics. Test-time adaptive object detection. Research on TTA for Object Detection (TTAOD) is progressively emerging [6, 29, 36, 42]. Most existing TTAOD methods [6, 36, 42] exploit a teacher-student network to adapt to the test do- main, following the self-training approach commonly em- ployed in Unsupervised Domain Adaptation for object de- tection [7, 18, 19, 34]. However, it is inefficient for TTA due to data augmentation requirements and additional for- ward and backward steps, resulting in slower inference speeds and higher memory usage. Another approach, Act- MAD [29], aligns the distributions of output feature maps after all BN layers along the height, width, and channel axes to adapt to the test domain. However, this location-aware feature alignment is limited to datasets with fixed location priors, such as driving datasets, and is less effective for nat- ural images like COCO. Additionally, CTA for Object De- tection (CTAOD)have not been thoroughly explored. There- fore, there is a need for an effective CTAOD method con- sidering memory and time efficiency. 3. Method To enable the efficient and effective Continual Test-time Adaptation of Object Detectors (CTAOD), we introduce an approach that specifies which part of the model should be updated, describes how to update those using unlabeled test data, and determines whether we perform model updates or not to improve efficiency. 3.1. Preliminary Assume that we have an object detector h ◦ gΘ, here h and g are the RoI head and the backbone, respectively with their parameters being Θ. The training dataset is denoted as Dtrain = {(xi, yi)}N i=1, where xi ∼ Ptrain(x) and yi = ( bboxi, ci), containing information on the bounding box (bbox) and class label ci ∈ C. Consider deploying the detector to the test environments where the test data at pe- riod T is denoted as xT j ∼ PT test(x), PT test ̸= Ptrain and PT test deviates from the i.i.d. assumption. In addition, the domain of PT test continually changes according to T (i.e., PT test ̸= PT−1 test ). Our goal is to adapt the detector h ◦ g to PT test using only test data xT j while making predictions. 3.2. What to update: Adaptation via an adaptor Previous methods [6, 29, 36, 42] adapt the model to the test domain by updating all parameters Θ, leading to in- efficiency at test time and a high risk of losing task knowl- edge from the training data. In contrast, we adapt the model by introducing an adaptor with an extremely small set of parameters and updating only this module while freezing Θ. We introduce a shallow adaptor in parallel for each block, inspired by [5, 13], where transformer-based mod- els are fine-tuned for downstream tasks through parameter- efficient adaptors, as shown in Fig. 2. Each adaptor consists of down-projection layers Wdown ∈ Rd×d r , up-projection layers Wup ∈ R d r ×d and ReLUs, where d denotes the in- put channel dimension and r is the channel reduction ratio set to 32 for all adaptors. We use MLP layers for the Trans- former block (Fig. 2a) and 1×1 convolutional layers for the ResNet block (Fig. 2b) to introduce architecture-agnostic adaptors. The up-projection layer is initialized to 0 values so that the adaptor does not modify the output of the block, 3(a) A block of Transformer  (b) A block of ResNet Figure 2. We attach an adaptor, which is a shallow and low-rank MLP or CNN, to every N block in parallel. We update only these adaptors while other parameters are frozen. Our approach can be applied to diverse architectures including CNNs and Transformers. but as the adaptor is gradually updated, it adjusts the output of the block to adapt to the test domain. Even as the adaptor is updated in the test domain, the original backbone param- eter Θ remains frozen and fully preserved. This structural preservation, as evident in Ours in Fig. 1, enables robust and efficient adaptation to domain changes by maintaining relatively complex task knowledge in object detection and updating very few parameters. 3.3. How to update: EMA feature alignment To adapt the object detector to the test domain, we align the feature distribution of the test domain with that of the training data, inspired by [17, 29, 38]. In contrast to these methods that solely align image feature distribution, we ad- ditionally align object-level features in a class-wise manner, considering class frequency, to enhance its effectiveness for object detection. As the training data is not accessible dur- ing test time, we pre-compute the first and second-order statistics, denoted as µtr = E[Ftr] and Σtr = Var[Ftr], where the operators E and Var represent the mean and vari- ance respectively. The features Ftr = {gΘ(xtr)} are com- puted using only 2,000 training samples, a small subset of the training data. Since a sufficient amount of test domain data is not available at once, and only the current incoming test data, whose features are denoted as Ft te, is accessible at time step t, we estimate the mean of test data features using an exponentially moving average (EMA) as follows: µt te = (1 − α) · µt−1 te + α · E[Ft te], s.t. µ0 te = µtr. (1) Considering the typically small batch size in object detec- tion compared to classification, we approximate the vari- ance of the test features as Σte ≃ Σtr to reduce instability. Image-level feature alignment. We estimate the training and test feature distributions as normal distributions and minimize the KL divergence between them as follows: Limg = DKL(N(µtr, Σtr), N(µt te, Σtr)). (2) Region-level class-wise feature alignment. In object de- tection, we deal with multiple objects within a single image, making it challenging to apply the class-wise feature align- ment proposed in [38], a TTA method for classification. To handle region-level features that correspond to an object, we use ground truth bounding boxes for the training data and utilize the class predictions of RoI pooled features, ft te, for unlabeled test data. In object detection, domain shifts often result in lower recall rates, as a significant number of proposals are predicted as background [20]. To mitigate this issue, we filter out features with background scores exceed- ing a specific threshold. Subsequently, we assign them to the foreground class with the highest probability, as follows: Fk,t te = {ft te|argmax c pfg = k, pbg < 0.5}, where hcls(ft te) = [pfg , pbg] = [p0, ..., pC−1, pbg]. (3) We estimate the class-wise feature distribution of the test domain by exploiting Fk,t te and Eq.1. Furthermore, we in- troduce a weighting scheme for aligning features of less frequently appearing classes, taking into account the severe class imbalance where specific instance ( e.g., person) may appear multiple times within a single image, as follows: Nk,t = Nk,t−1 + ||Fk,t te ||, s.t. Nk,0 = 0 wk,t = log \u0012maxi Ni,t Nk,t \u0013 + 0.01 Lobj = X k wk,t · DKL(N(µk tr, Σk tr), N(µk,t te , Σk tr)). (4) Here, the class-wise mean µk and variance Σk of the train- ing and test data are obtained in the same way as the image- level features. We can effectively adapt the object detector by updating the model to align the feature distribution at both the image and object levels as L = Limg + Lobj. 3.4. When to update: Adaptation on demand As shown in Fig. 1, Ours, which only updates the adaptor proposed in Sec. 3.2, efficiently adapts to changes in the test domain, even with a small subset of early test samples. We leverage its rapid adaptation characteristics to reduce com- putational costs by skipping model updates ( i.e., skipping backward passes) when the model has already sufficiently adapted to the current test domain and resuming model up- dates when confronted with a new test domain. Therefore, we introduce two criteria to determine when to update the model or not as follows: (Criterion 1) When the distribution gap exceeds the in- domain distribution gap. Recall that Limg (Eq. 2) mea- sures the distribution gap between the test and train distri- butions. We assume a model is well-adapted to the current test domain when Limg is closer to the in-domain distri- bution gap. We measure the in-domain distribution gap by 4(a) The ratio of Limg to Din KL (b) The ratio of Limg to Lt ema Figure 3. The test domain undergoes a shift every 4,000 time steps, and each metric reaches its peak at the same intervals. sampling two disjoint subsets, xi and xj, of training fea- tures Ftr from Sec. 3.3 as follows: Din KL = DKL(N(µi tr, Σi tr), N(µj tr, Σj tr)), (5) where µi tr, Σi tr are obtained from xi ∼ Ptrain(x) and µj tr, Σj tr from xj ∼ Ptrain(x). In other words, if Limg is noticeably higher than the in-domain distribution gapDin KL, we consider a model encountering a test domain whose dis- tribution differs from Ptrain(x) and needs to be updated. Based on this, we introduce a new index Limg Din KL . Fig. 3a plots the trend of this index during the model adaptation to a con- tinually changing test domain. It shows that the index has a large value in the early stages of a domain change, decreases rapidly, and then maintains a value close to 1. This index exhibits a similar trend regardless of the backbone type and dataset, as included in the appendix. Therefore, we establish the criterion that model updates are necessary when this in- dex exceeds a certain threshold, τ1, as Limg Din KL > τ1. (Criterion 2 ) When the distribution gap suddenly in- creases. Additionally, we can determine when the test dis- tribution changes and model updates are necessary by ob- serving the trend of the distribution gap ( i.e., Limg). The convergence of Limg indicates that a model is well-adapted to the current test domain. To put it differently, Limg will exhibit a sudden increase when the model encounters a new test domain. We introduce an additional index, denoted as Limg Ltema , representing the ratio of the currentLimg to its expo- nentially moving averageLt ema at time t. We calculate it us- ing the following formula:Lt ema = 0.99·Lt−1 ema+0.01·Limg. Fig. 3b illustrates the trend of the ratio of Limg over the timesteps. It tends to reach a value of 1 as the loss stabilizes at a specific level. Nevertheless, when the model encounters shifts in the test distribution, the ratio experiences a sharp increase, indicating the necessity of a model update when it exceeds a specific threshold, τ2, as Limg Ltema > τ2. If at least one of the two criteria is satisfied, we conclude that the model requires adaptation and proceed to update it. 4. Experiments Sec. 4.1 presents the two object detection benchmark datasets with test distributions that change continuously, ei- ther in a drastic or gradual manner, and our implementation detail is in 4.2. Sec. 4.4 compares our method with other TTA baselines described in Secs. 4.3.. We present detailed ablation studies of our method analyzing the effectiveness and efficiency of our method in terms of what, how, and when to update the models for CTAOD in Sec. 4.5. 4.1. Datasets We experiment with the following three scenarios. COCO → COCO-C simulates continuous and drastic real- istic test domain changes over a long sequence. MS-COCO [23] collects 80 classes of common objects in their natural context with 118k training images and 5k validation images. COCO-C is created by employing 15 types of realistic cor- ruptions [27], such as image distortion and various weather conditions, to simulate test domain changes. In the experi- ments, the model is only trained on the COCO train set and sequentially evaluated on each corruption in the COCO-C validation set during test-time for reproducing continually changing test domains. Finally, the model is evaluated on the original COCO validation set to assess how well it pre- serves knowledge of the original domain (denoted as Org.). SHIFT-(Discrete / Continuous) [39] is a synthetic driving image dataset with 6 classes under different conditions us- ing five weather attributes (clear, cloudy, overcast, fog, rain) and three time-of-day attributes ( daytime, dawn, night ). In SHIFT-Discrete, there are image sets for each attribute, and the model is sequentially evaluated on these attributes, cloudy → overcast → foggy → rainy → dawn → night → clear which contains 2.4k, 1.6k, 2.7k, 3.2k, 1.2k, 1.4k, and 2.8k validation images, respectively. This simulates scenar- ios where the domain undergoes drastic changes. InSHIFT- Continuous, the model is evaluated on four sequences, each consisting of 4k frames, continuously transitioning from clear to foggy (or rainy) and back to clear. 4.2. Implementation Detail We experiment with Faster-RCNN [33] models using ResNet50 [10] and Swin-Tiny [26] as a backbone with FPN [24]. For the COCO → COCO-C adaptation, we em- ploy the publicity available models trained on COCO re- leased in [46] and [26] for ResNet5- and Swin-Tiny-based Faster-RCNN, respectively. For SHIFT experiments, mod- els are trained on the training domain using the detectron2 framework following [33] and [26]. For test-time adapta- tion, we always set the learning to 0.001 for the SGD opti- mizer, and α of Eq. 1 to 0.01, while τ1 and τ2 are set to 1.1 5Table 1. Comparison of mAP, the number of backward and forward passes, and FPS between baselines and our model on COCO→ COCO- C. Our model consistently outperforms baselines on the two different backbones. Furthermore, Ours-Skip with ResNet notably reduces backward passes by as much as 90.5%, leading to a significantly improved frames per second (FPS) rate by up to 109.9%. Noise Blur Weather Digital # step Backbone Method Gau Sht Imp Def Gls Mtn Zm Snw Frs Fog Brt Cnt Els Px Jpg Org. Avg. For. Back. FPS Swin-T [26] Direct-Test 9.7 11.4 10.0 13.4 7.5 12.1 5.2 20.7 24.8 36.1 36.0 12.9 19.1 4.9 15.8 43.0 17.7 80K 0 21.5 ActMAD 10.7 12.0 9.4 12.3 5.7 9.5 4.5 15.3 17.5 27.6 28.2 1.1 16.7 2.6 8.7 36.3 13.9 80K 80K 8.3 Mean-Teacher 10.0 12.1 11.2 12.8 8.1 12.1 4.9 19.6 23.7 34.9 34.0 8.0 18.9 6.1 17.6 41.0 17.2 160K 80K 6.9 Ours 13.6 16.6 16.1 14.0 13.6 14.2 8.3 23.7 27.2 37.4 36.4 27.2 27.2 22.2 22.3 42.3 22.6 80K 80K 9.5 Ours-Skip 13.3 15.3 15.1 14.0 12.8 13.9 6.5 22.0 25.4 35.5 34.9 26.5 25.9 23.4 20.2 41.2 21.6 80K 9.7K 17.7 ResNet50 [10] Direct-Test 9.1 11.0 9.8 12.6 4.5 8.8 4.6 19.1 23.1 38.4 38.0 21.4 15.6 5.3 11.9 44.2 17.3 80K 0 25.8 NORM 9.9 11.9 11.0 12.6 5.2 9.1 5.1 19.4 23.5 38.2 37.6 22.4 17.2 5.7 10.3 43.4 17.5 80K 0 25.8 DUA 9.8 11.7 10.8 12.8 5.2 8.9 5.1 19.3 23.7 38.4 37.8 22.3 17.2 5.4 10.1 44.1 17.1 80K 0 25.8 ActMAD 9.1 9.6 7.0 11.0 3.2 6.1 3.3 12.8 14.0 27.7 27.8 3.9 12.9 2.3 7.2 34.3 10.5 80K 80K 9.6 Mean-Teacher 9.6 12.5 12.0 4.0 2.9 4.8 3.1 16.2 23.5 35.1 34.0 21.8 16.6 8.2 12.7 40.3 14.5 160K 80K 8.1 Ours 12.7 17.8 17.5 12.4 11.5 11.3 6.6 22.8 26.9 38.6 38.5 28.0 25.1 21.2 22.2 41.8 22.2 80K 80K 10.1 Ours-Skip 14.4 17.1 16.0 13.9 11.7 12.2 6.3 22.1 25.5 37.7 37.1 25.5 24.1 23.1 21.1 42.8 21.9 80K 7.6K 21.2 and 1.05, respectively. We use the same hyper-parameters across all backbones and datasets. All experiments are con- ducted with a batch size of 4. 4.3. Baselines Direct-Test evaluates the model trained in the training do- main without adaptation to the test domain. ActMAD [29] is a TTA method aligning the distribution of output features across all BN layers. To apply ActMAD to the Swin Trans- former-based model, we align the output features of the LN layers. We implement Mean-Teacher using a teacher- student network framework to reproduce as close as possi- ble to TeST [36], as its implementation is not publicly avail- able. We follow the FixMatch [37] augmentation method and report results after tuning all hyper-parameters in our scenario. NORM [35] and DUA [28], TTA methods ini- tially designed for classification, are directly applicable to detection tasks by either mixing a certain amount of current batch statistics or updating batch statistics via EMA. How- ever, these are only compatible with architectures contain- ing BN layers. Additional details are provided in Appendix. 4.4. Main Results We compare the performance of each method using mAP and efficiency metrics, including the number of forward and backward passes, as well as FPS during test-time adapta- tion. Results of COCO and SHIFT are in Tab. 1 and 2, re- spectively. COCO → COCO-C. Tab. 1 demonstrates the effective adaptation performance of Ours in the challenging COCO benchmark with 80 classes due to object-level class-wise feature alignment. ActMAD also aligns feature distribution for TTA, but is not effective since it only aligns whole fea- ture maps without considering specific classes in the im- age. NORM and DUA, applicable only to ResNet [10], show minimal performance improvement by adaptation as they are not specifically tailored for object detection and only modify batch statistics across the entire feature map. Ad- ditionally, ActMAD and Mean-Teacher, updating full pa- rameters, gradually lose task knowledge in the continually changing test distributions, resulting in much lower perfor- mance on Org. , the domain identical to the training data, than that of Direct-Test. In contrast, Ours effectively pre- vents catastrophic forgetting by freezing the original param- eters of the models and updating only the adaptor, obtain- ing performance on par with Direct-Test on the Org. do- main and consistently high performance across corrupted domains, with an average mAP improvement of 4.9%p compared to that of Direct-Test. Furthermore, leveraging the rapid adaptation ability of the adaptor,Ours-Skip, which skips unnecessary adaptation, allows using only a maxi- mum of about 12% of the total samples for adaptation with- out significant performance loss. This leads to a substantial improvement in inference speed, more than doubling com- pared to other TTA methods, reaching over 17.7 FPS. SHIFT-Discrete. Ours is also effective in SHIFT, which simulates continuous changes in weather and time in driv- ing scenarios according to the left section of Tab. 2. Espe- cially, Ours shows significant improvements in mAP by 7- 9%p, particularly for the foggy and dawn attributes where Direct-Test obtains lower performance due to severe do- main shift. In contrast, with ActMAD, catastrophic forget- ting takes place when adapting to the cloudy and overcast weather. This is due to the updating of the full parame- ters, despite that Direct-Test already shows proficient per- formance in these conditions. As a result, the performance in the later domains is worse than that of the Direct-Test. DUA, which updates batch statistics using EMA, shows a gradual decrease in performance as the domain contin- uously changes, resulting in much lower performance in the original clear domain ( i.e., clear ). On the other hand, NORM, which utilizes the statistics of the current batch samples, exhibits no catastrophic forgetting and relatively good adaptation, as SHIFT is a relatively easier task com- pared to COCO due to having only 6 classes. Compared to NORM, Ours shows better adaptation performance, and is 6Table 2. Comparison of mAP, the number of backward and forward passes, and FPS between baselines and our model on SHIFT-Discrete and SHIFT-Continuous. Baselines perform effectively in a particular setting but lack generalizability across various settings. Our method consistently achieves results that are either better or on par with the best model in all settings, demonstrating its strong stability. Ours-Skip also effectively reduces the number of backward passes without compromising mAP performance, resulting in a higher FPS. SHIFT-Discrete SHIFT-Continuous mAP # step mAP # Avg. step Backbone Method cloudy overc. fog rain dawn night clear Avg. For. Back. FPS clear↔fog clear ↔rain For. Back. FPS Swin-T [26] Direct-Test 50.0 38.9 23.1 45.1 26.9 39.5 45.9 38.5 15.3K 0 27.5 18.1 21.1 4K 0 28.3 ActMAD 49.8 38.4 21.4 43.1 19.0 32.0 44.8 35.5 15.3K 15.3K 9.3 15.6 16.3 4K 4K 9.8 Mean-Teacher 50.0 39.2 25.7 45.4 26.0 37.5 42.2 38.0 15.3K 15.3K 7.8 20.4 24.3 8K 4K 6.5 Ours 50.3 39.2 32.2 46.7 30.4 39.9 44.3 40.4 15.3K 15.3K 11.2 23.9 22.6 4K 4K 11.6 Ours-Skip 50.3 39.7 29.1 47.1 30.2 41.5 45.9 40.6 15.3K 6.1K 20.0 25.1 23.8 4K 0.83K 19.2 ResNet50 [10] Direct-Test 49.4 37.9 19.7 43.1 20.1 35.3 45.6 35.9 15.3K 0 30.1 12.1 15.4 4K 0 30.0 NORM 49.7 38.6 22.9 44.7 25.1 37.4 45.5 37.7 15.3K 0 30.1 16.9 19.4 4K 0 30.0 DUA 45.2 31.5 27.7 31.9 15.2 18.6 21.1 27.3 15.3K 0 30.1 22.5 22.4 4K 0 30.0 ActMAD 49.2 37.7 18.0 40.6 16.0 32.9 44.3 34.1 15.3K 15.3K 11.3 12.7 16.3 4K 4K 11.2 Mean-Teacher 49.6 38.4 26.8 43.4 26.6 33.1 41.6 37.1 15.3K 15.3K 9.9 16.0 20.8 8K 4K 9.8 Ours 49.7 38.7 27.4 46.3 27.4 37.6 43.8 38.7 15.3K 15.3K 12.9 20.9 21.9 4K 4K 13.9 Ours-Skip 49.7 38.8 26.9 46.2 27.6 38.8 45.0 39.0 15.3K 8.9K 21.5 20.0 22.5 4K 0.75K 21.3 Table 3. Comparison of adaptation performance (mAP), the num- ber of trainable parameters (# Params), and memory usage (Cache) according to which part of the backbone is updated. SD / SC de- notes SHIFT-Discrete/Continuous, respectively. mAP # Params Cache Backbone Trainable Params SD SC Num Ratio Avg. Max Swin-T Full-params 38.4 20.6 27.7M 100% 0.86 11.0 LayerNorm 38.5 20.0 0.03M 0.1% 0.65 7.49 adaptor (Ours) 40.4 23.2 0.15M 0.5% 0.65 6.96 ResNet50 Full-params 37.6 20.4 23.7M 100% 1.65 9.29 BatchNorm 37.9 20.2 0.05M 0.2% 1.47 9.11 adaptor (Ours) 38.7 21.7 0.21M 0.9% 1.48 5.41 also applicable to BN-layer-free Swin Transformers. SHIFT-Continuous. In scenarios where the test domain gradually changes across the entire sequence, Ours also demonstrates effectiveness, improving mAP by up to 7%p, as shown in the right section of Tab. 2. WhileDUA performs well in the clear to foggy transition, it is prone to catas- trophic forgetting in situations where the sequence becomes longer, and the test domain changes more diversely, as seen in the left section. Our strategy for determining when model adaptation is necessary is particularly effective in SHIFT. It improves FPS by about 9, reaching about 20 FPS, while en- hancing mAP. This is likely due to avoiding overfitting that can occur when adapting to all repetitive frames in SHIFT, which consists of continuous frames, leading to improve- ments in both inference speed and adaptation performance. 4.5. Additional Analyses We aim to demonstrate the effectiveness and detailed anal- ysis of our proposed model in terms of 1) which parts of, 2) how, and 3) when the model should be updated. Which part to update? Tab. 3 shows how updating dif- ferent parts of the backbone model affects the performance and the memory usage during continual test-time adapta- Table 4. Ablation on each component of our loss. SHIFT-D / C denotes SHIFT-Discrete / Continuous, respectively. The left and right value in each cell corresponds to the mAP for the Swin-T and ResNet50 backbone, respectively. Limg Lobj COCO SHIFT-D. SHIFT-C. - - 17.7/ 17.3 38.5/ 35.9 19.6/ 13.8 ✔ - 16.7/ 18.1 36.6/ 37.0 19.1/ 16.0 ✔ no class weight 17.8/ 18.9 39.7/ 38.0 25.1/ 23.4 ✔ class weight wk,t 22.6/ 22.2 40.4/ 38.7 23.2/ 21.7 tion. We compare (1) updating full parameters, (2) affine parameters of the normalization layer, and (3) our proposed adaptor for each backbone on the SHIFT dataset. Although our adaptor has fewer parameters, about 0.9% or less of the full parameters, it demonstrates the best adaptation perfor- mance. Updating only the affine parameters of the normal- ization layer, while having fewer parameters, seems less ef- fective for adaptation in object detection compared to clas- sification [30, 43]. Additionally, our adaptor requires only about 60% of the memory compared to updating the full parameters, making it memory-efficient. Ablation study on each component in our loss. Tab. 4 presents the effects of image-level feature alignment,Limg, object-level feature class-wise alignment Lobj, and class frequency weighting wk,t proposed to address class im- balance. Aligning only the image-level feature distribu- tion with Limg (first row) leads to modest adaptation in the ResNet50 backbone, while performance in the Swin- T backbone is even lower than without adaptation. No- tably, aligning object-level features with Lobj leads to a substantial improvement, with the mAP increasing by approximately 10%p compared to the no-adaptation sce- nario. Introducing class-specific frequency-based weighting wk,t, despite a slight performance decrease in the SHIFT- Continuous setting, proves highly effective, particularly in scenarios with significant class imbalance, such as COCO 7(a) Swin Transformer backbone  (b) ResNet50 backbone Figure 4. Comparison of mAP and FPS fromOurs-Skip with vary- ing values of τ1 (♦) and τ2 (▲) against Evenly-Skip (×), adapting every N-th instances, on COCO→COCO-C using both (a) Swin- T and (b) ResNet50. The upward and rightward movement indi- cates a better strategy with higher mAP and faster inference speed, showing that Ours-Skip is consistently better than Evenly-Skip. (a) Accumulated number of backward steps (b) Number of backward steps and mAP of Direct-Test in each domain Figure 5. Analysis of the adaptation of Ours-Skip. with 80 classes, where it enhances the mAP by around 5%p. Trade-off between adaptation performance and effi- ciency according to different skipping strategies. Fig. 4 presents mAP and FPS depending on the values ofτ1 and τ2 in the Sec. 3.4 on COCO → COCO-C, which are used for two criteria to determine when the adaptation is needed. We also show the simple baselineEvenly-Skip, which adapts ev- ery N-th step and skips the rest. In Fig. 4, the blue lines (▲) show the results when τ1 is changing from 1.0 to infinity, where only criterion 2 is used, while τ2 is fixed at 1.05. As τ1 decreases, more adaptation is required, leading to slower FPS but higher mAP. The green lines (♦) show the results of changing τ2, where ‘τ2 = inf’ denotes using only criterion 1, without criterion 2. For all main experiments, we set τ1 and τ2 as 1.1 and 1.05, respectively, considering the balance between mAP and FPS. Additionally, our skipping strategy consistently outperforms Evenly-Skip, achieving higher val- ues in both mAP and FPS. This indicates that our criterion for deciding when to bypass model updates provides an ef- fective balance between accuracy and speed. When do models actually update? We analyze when the model actually skips adaptation and only performs infer- ence or actively utilizes test samples for model adaptation based on the two criteria we propose. This analysis is con- ducted in COCO to COCO-C with 15 corruption domains and 1 original domain. Fig. 5a plots the number of back- ward passes, i.e., the number of batches of test samples used for adaptation, with different values of τ1 for the two backbones. The horizontal and vertical axes represent se- quentially incoming test domains and the cumulative back- ward numbers, respectively. A steep slope in a region in- dicates frequent adaptation, while a gentle slope indicates skipping adaptation, performing only inference. Notably, even without explicit information about when the test do- main changes, the model actively performs adaptation, es- pecially right after the test domain changes. This trend is consistent regardless of changes in τ value or backbone type. Furthermore, it is evident that the number of backward passes is primarily determined by the value ofτ1 rather than the type of backbone, suggesting that a consistent τ1 value can be used irrespective of the backbone. Fig. 5b visually represents the adaptation tendencies by dividing backward steps for each domain in the case of Swin-T backbone with τ1 = 1.1. More clearly, it shows that adaptation occurs ac- tively around the points where each domain changes, and af- terward, adaptation happens intermittently or almost not at all. The light pink bars represent the performance ofDirect- Test, showing that domains with initially high model per- formance tend to have less adaptation, while domains with lower performance initially need more adaptation. In other words, the amount of skipping adaptation is proportional to the amount of the domain shift. Interestingly, the second do- main, ’Shot Noise’, shows almost no adaptation despite the lower performance of the Direct-Test. We conjecture that the preceding domain, ’Gaussian Noise’, shares a similar nature of noise, leading the model to decide that additional adaptation steps may not be necessary. As a result, our skip- ping strategy enables the model to efficiently adapt, consid- ering both the original domain the model is trained on and the previous domain the model has been adapted to. 5. Conclusion We introduce an efficient Continual Test-time Adaptation (CTA) method for object detection in the continually chang- ing domain. Our approach involves 1) lightweight adap- tors, 2) class-wise object-level feature alignment, and 3) skipping unnecessary adaptation. These contributions col- lectively yield a highly efficient and effective adaptation method, showcasing robustness to diverse domain shifts, and achieving notable improvements in mAP performance across various CTA scenarios without serious slowdown in the inference speed. 8What, How, and When Should Object Detectors Update in Continually Changing Test Domains? Supplementary Material 6. Additional Details for Baselines We provide additional implementation details for each base- line model. Our framework incorporates all baseline models using the official code except Mean-Teacher. The results of the experiments are reported based on the optimal hyperpa- rameters that yield the best results in our scenario. ActMAD [29] As ActMAD exclusively conducts experi- ments on the KITTI dataset, where all images have a con- stant height and width (e.g., 370 x 1224), ensuring consis- tent feature map sizes for all samples. ActMAD can easily align them along the spatial axis. However, in the general setting of object detection tasks, such as the COCO bench- mark set, where image sizes and width-to-height ratios vary, aligning feature maps along the spatial axis becomes chal- lenging due to different sizes. To adapt ActMAD to our COCO → COCO-C scenario, we perform center cropping on the feature maps to match the size of training domain fea- ture maps and the current test sample feature maps. We em- ploy a learning rate of 1e-5 for COCO and 1e-4 for SHIFT, respectively. Mean-Teacher As the official code of TeST [36] is not available, we implement the EMA-updated Teacher and Student models following TeST [36], to conduct experi- ments in our scenarios. TeST involves three forward steps for a batch: forwarding weakly augmented samples through the student network, strong augmented samples through the teacher network, and original samples through the teacher network for outputs. However, for a fair comparison, we perform two forward steps, forwarding the original sample through the teacher network and strong augmented sam- ples through the student network, to make predictions be- fore adaptation for every samples. We utilize a learning rate of 1e-5 and set the EMA update rate for the teacher network to 0.999. NORM [35] We set the hyperparameter N that controls the trade-off between training statistics and estimated tar- get statistics as 128. DUA [28] We set the momentum decay as 0.94, minimum momentum constant as 1e-4, and the initial momentum de- cay as 1e-3. 7. The effect of Bottleneck Reduction Ratio in the Adaptor Table 5 shows the results for COCO → COCO-C, SHIFT- Discrete, and SHIFT-Continuous based on the dimension reduction ratio ( r) discussed in Section 3.2, representing Table 5. Comparison of adaptation performance (mAP), the num- ber of trainable parameters (# Params), and memory usage (Cache) according to r of Sec. 3.2, the bottleneck reduction ratio in the adaptor. We set r as 32 for all our experiments in the main paper. SD / SC denotes SHIFT-Discrete / Continuous, respectively. mAP # Params Cache Backbone r COCO SD SC Num Ratio Avg. Max Swin-T 1 22.6 40.0 21.3 4.33M 15.7% 0.75 7.51 2 22.6 40.3 23.2 2.17M 7.85% 0.73 7.27 4 22.6 40.4 23.2 1.09M 3.95% 0.70 7.06 8 22.6 40.4 23.2 0.55M 2.00% 0.69 7.00 16 22.6 40.4 23.2 0.28M 1.02% 0.67 6.98 32 22.6 40.4 23.2 0.15M 0.54% 0.65 6.96 64 22.6 40.4 23.2 0.08M 0.29% 0.65 6.95 ResNet50 1 22.5 38.7 20.8 6.31M 26.7% 1.55 5.89 2 22.4 38.7 20.9 3.16M 13.4% 1.51 5.64 4 22.3 38.6 21.3 1.59M 6.71% 1.49 5.52 8 22.3 38.6 21.4 0.80M 3.39% 1.48 5.46 16 22.2 38.6 21.4 0.41M 1.73% 1.48 5.43 32 22.2 38.7 21.4 0.21M 0.89% 1.48 5.41 64 22.1 38.7 21.3 0.11M 0.48% 1.48 5.40 the ratio of bottleneck size compared to the input size in the adaptor. The adaptation performance remains consistent across different r values. However, in the case of r = 1 in SHIFT experiments, mAP decreases, potentially due to catastrophic forgetting resulting from a large number of adaptable parameters. Since increasing the value of r sig- nificantly reduces the number of learnable parameters and memory usage, we set r to 32 in all other experiments. 8. Results on the KITTI Dataset We conduct additional experiments on the KITTI [8] dataset, the commonly used object detection dataset consist- ing of driving scenes with 8 classes (car, van, truck, person, person sitting, cyclist, tram, misc). To simulate the continu- ally changing domains, we use the following scenario ( Fog → Rain → Snow → Clear) as done in [29]. We use the physics-based rendered dataset [9] forfog and rain and sim- ulate snow using the corruption library from [11]. We use the same split of [29], which divides the 7,441 training sam- ples into 3,740 training and 3,741 test samples. We train the Faster-RCNN using 3,741 training samples representing the Clear attribute with Swin-Transformer and ResNet50 back- bones, and evaluate it sequentially on Fog, Rain, Snow, and Clear test samples. We conduct all experiments with a batch size of 16 on 1 RTX A6000 GPU. Table 6 shows the mAP@50, the num- 1Table 6. Comparison of mAP, the number of backward and forward passes, FPS, and memory usage between baselines and our models on the continually changing KITTI datasets ( Fog → Rain → Snow → Clear). Our models improve mAP@50 by 15.1 and 11.3 for Swin-T and ResNet50 backbone, respectively, compared to Direct-Test while maintaining comparable FPS. All experiments are conducted with a batch size of 16. mAP@50 # For. Steps # Backward Steps FPS Cache Backbone Method Fog Rain Snow Clear Avg. All Fog Rain Snow Clear All Avg. Avg. Max Swin-T Direct-Test 46.9 69.5 28.7 89.6 58.7 936 0 0 0 0 0 24.7 0.4 5.5 ActMAD 53.3 78.1 41.2 90.7 65.8 936 234 234 234 234 936 16.8 0.8 21.9 Mean-Teacher 54.5 80.2 43.2 92.4 67.6 936 234 234 234 234 936 10.0 1.0 22.6 Ours 56.7 82.1 64.6 91.8 73.8 936 234 234 234 234 936 17.1 0.4 11.8 Ours-Skip 57.4 81.5 64.3 91.3 73.6 936 234 65 224 36 559 22.9 0.4 11.8 ResNet50 Direct-Test 33.4 63.5 29.8 88.6 53.8 936 0 0 0 0 0 27.7 0.8 4.3 NORM 38.4 66.4 35.9 87.3 57.0 936 0 0 0 0 0 27.7 0.8 4.3 DUA 34.8 67.7 30.9 89.0 55.6 936 0 0 0 0 0 27.7 0.8 4.3 ActMAD 40.4 66.5 42.7 84.5 58.5 936 234 234 234 234 936 18.5 1.6 22.6 Mean-Teacher 39.6 71.3 43.5 88.2 60.6 936 234 234 234 234 936 11.1 1.8 31.1 Ours 45.6 71.4 52.5 88.3 64.5 936 234 234 234 234 936 18.8 0.8 9.4 Ours-Skip 45.8 71.3 50.9 88.4 64.1 936 234 111 98 45 488 24.5 0.8 9.4 (a) GT bounding boxes. (b) Prediction results of Direct-Test. (c) Prediction results of Ours. Figure 6. Results of COCO images corrupted by Shot-Noise. In the analysis of Sec. 4.5, we conjecture that Ours largely skips adaptation in Shot-Noise domain, despite the low mAP of Direct-Test, because the model has already adapted to a similar domain, Gaussian-Noise. In (c), at the first step before adaptation to the Shot-Noise, our model already predicts ’Oven’ and ’Refrigerator’ which Direct-Test fails to detect. This results in a much faster adaptation, and Ours successfully detects various objects, including rare ones such as ’Fire Hydrants’, in the remaining images of the Shot-Noise domain. ber of forward and backward steps, FPS, and memory usage (Cache). Ours improves the mAP@50 by 15.1 and 10.7 for Swin-T and ResNet50 backbones, respectively, compared to Direct-Test. Compared to ActMAD and Mean-Teacher, our model not only improves the adaptation performance but also reduces memory usage, as we update only an ex- tremely small number of parameters of the adaptor. Further- more, using our skipping criteria of Sec. 3.4 with τ = 1.1 and β = 1.05, we can improve FPS by more than 5.8 with- out sacrificing mAP@50, resulting in much faster inference 2(a) GT bounding boxes. (b) Prediction results of Direct-Test. (c) Prediction results of Ours. Figure 7. Results for COCO images corrupted by Pixelate. In the Pixelate domain, where the model has already experienced various corruptions in a long sequence, Ours initially incorrectly detects objects. In (c), it misidentifies a bed as a couch in the first step. However, it rapidly adapts to the Pixelate domain and effectively detects various objects. Notably, even in cases whereDirect-Testcorrectly identifies objects but with low confidence, Ours detects them with much higher confidence. (a) GT bounding boxes. (b) Prediction results of Direct-Test. (c) Prediction results of Ours. Figure 8. Results for SHIFT-Discrete with continually changing attributes, foggy → rainy → dawn → night. speed compared to other TTA baselines. 39. Qualitative Results Fig. 6 and 7 and Fig. 8 show the qualitative results of Ours and Direct-Test which predict the samples without adapta- tion for COCO → COCO-C and SHIFT, respectively. 9.1. COCO → COCO-C Fig. 6 and 7 compare the prediction results for COCO im- ages corrupted. When the model encounters test images with various corruptions sequentially ( Gaussian-Noise → Shot-Noise → Impulse-Noise → Defocus-Blur → Glass- Blur → Motion-Blur → Zoom-Blur → Snow → Frost → Fog → Brightness → Contrast → Elastic-Transform → Pixelate → JPEG-Compression → Original), Fig. 6 and 7 shows the results when the test images are corrupted by Shot-Noise and Pixelate, respectively. Compared to Direct- Test, our model adapts to the current domain within a few steps, such as 100 iterations, and detects various objects very well in the remaining incoming images. 9.2. SHIFT-Discrete Fig. 8 shows the qualitative results for SHIFT-Discrete. In the SHIFT-Discrete scenario, the model encounters environ- ments sequentially, transitioning from cloudy → overcast → foggy → rainy → dawn → night → clear. Figure. 8 se- lectively shows the foggy → rainy → dawn → night se- quence, where the domain gap from the original clear envi- ronments is relatively large. Compared to Direct-Test, Ours detects various objects such as ’cars’ and ’pedestrians’ re- gardless of distribution changes. References [1] Alexander Bartler, Florian Bender, Felix Wiewel, and Bin Yang. Ttaps: Test-time adaption by aligning prototypes using self-supervision. In 2022 International Joint Conference on Neural Networks (IJCNN), pages 1–8. IEEE, 2022. 2 [2] Alexander Bartler, Andre B ¨uhler, Felix Wiewel, Mario D¨obler, and Bin Yang. Mt3: Meta test-time training for self- supervised test-time adaption. In International Conference on Artificial Intelligence and Statistics , pages 3080–3090. PMLR, 2022. 2 [3] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 8344–8353, 2022. 1 [4] Dhanajit Brahma and Piyush Rai. A probabilistic frame- work for lifelong test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3582–3591, 2023. 3 [5] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yib- ing Song, Jue Wang, and Ping Luo. Adaptformer: Adapting vision transformers for scalable visual recognition.Advances in Neural Information Processing Systems, 35:16664–16678, 2022. 3 [6] Yijin Chen, Xun Xu, Yongyi Su, and Kui Jia. Stfar: Im- proving object detection robustness at test-time by self- training with feature alignment regularization.arXiv preprint arXiv:2303.17937, 2023. 1, 2, 3 [7] Jinhong Deng, Wen Li, Yuhua Chen, and Lixin Duan. Un- biased mean teacher for cross-domain object detection. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 4091–4101, 2021. 3 [8] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The Inter- national Journal of Robotics Research , 32(11):1231–1237, 2013. 1 [9] Shirsendu Sukanta Halder, Jean-Franc ¸ois Lalonde, and Raoul de Charette. Physics-based rendering for improving robustness to rain. In Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision, pages 10203–10212, 2019. 1 [10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. 5, 6, 7 [11] Dan Hendrycks and Thomas Dietterich. Benchmarking neu- ral network robustness to common corruptions and perturba- tions. arXiv preprint arXiv:1903.12261, 2019. 1 [12] Junyuan Hong, Lingjuan Lyu, Jiayu Zhou, and Michael Spranger. Mecta: Memory-economic continual test-time model adaptation. In The Eleventh International Conference on Learning Representations, 2022. 3 [13] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen- Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 3 [14] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal co- variate shift. In International conference on machine learn- ing, pages 448–456. pmlr, 2015. 2 [15] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for modelagnostic domain generaliza- tion. In Advances in Neural Information Processing Systems (NeurIPS), 2021. 3 [16] Minguk Jang, Sae-Young Chung, and Hye Won Chung. Test- time adaptation via self-training with nearest neighbor infor- mation. In International Conference on Learning Represen- tations (ICLR), 2023. 3 [17] Sanghun Jung, Jungsoo Lee, Nanhee Kim, Amirreza Sha- ban, Byron Boots, and Jaegul Choo. Cafa: Class-aware fea- ture alignment for test-time adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vi- sion, 2023. 2, 3, 4 [18] Mehran Khodabandeh, Arash Vahdat, Mani Ranjbar, and William G Macready. A robust learning approach to domain adaptive object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 480– 490, 2019. 3 [19] Seunghyeon Kim, Jaehoon Choi, Taekyung Kim, and Chang- ick Kim. Self-training and adversarial background regular- ization for unsupervised domain adaptive one-stage object 4detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6092–6101, 2019. 3 [20] Xianfeng Li, Weijie Chen, Di Xie, Shicai Yang, Peng Yuan, Shiliang Pu, and Yueting Zhuang. A free lunch for unsuper- vised domain adaptive object detection without source data. In Proceedings of the AAAI Conference on Artificial Intelli- gence, pages 8474–8481, 2021. 4 [21] Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou. Revisiting batch normalization for practical do- main adaptation, 2017. 3 [22] Hyesu Lim, Byeonggeun Kim, Jaegul Choo, and Sungha Choi. Ttn: A domain-shift aware batch normalization in test- time adaptation, 2023. 2, 3 [23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740–755. Springer, 2014. 5 [24] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyra- mid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 2117–2125, 2017. 5 [25] Yuejiang Liu, Parth Kothari, Bastien Van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? Advances in Neural Information Processing Systems , 34: 21808–21820, 2021. 1, 2 [26] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012–10022, 2021. 2, 5, 6, 7 [27] Claudio Michaelis, Benjamin Mitzkus, Robert Geirhos, Evgenia Rusak, Oliver Bringmann, Alexander S Ecker, Matthias Bethge, and Wieland Brendel. Benchmarking ro- bustness in object detection: Autonomous driving when win- ter is coming. arXiv preprint arXiv:1907.07484, 2019. 5 [28] M Jehanzeb Mirza, Jakub Micorek, Horst Possegger, and Horst Bischof. The norm must go on: Dynamic unsuper- vised domain adaptation by normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition, pages 14765–14775, 2022. 3, 6, 1 [29] Muhammad Jehanzeb Mirza, Pol Jan ´e Soneira, Wei Lin, Ma- teusz Kozinski, Horst Possegger, and Horst Bischof. Act- mad: Activation matching to align distributions for test-time- training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 24152– 24161, 2023. 1, 2, 3, 4, 6 [30] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test-time model adaptation without forgetting. In Interna- tional conference on machine learning, pages 16888–16905. PMLR, 2022. 1, 2, 3, 7 [31] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan. Towards stable test-time adaptation in dynamic wild world. arXiv preprint arXiv:2302.12400, 2023. 3 [32] Mario obler, Robert A Marsden, and Bin Yang. Robust mean teacher for continual and gradual test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 7704–7714, 2023. 3 [33] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. 2016. 5 [34] Aruni RoyChowdhury, Prithvijit Chakrabarty, Ashish Singh, SouYoung Jin, Huaizu Jiang, Liangliang Cao, and Erik Learned-Miller. Automatic adaptation of object detectors to new domains using self-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 3 [35] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bring- mann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. Advances in neural information processing sys- tems, 33:11539–11551, 2020. 3, 6, 1 [36] Samarth Sinha, Peter Gehler, Francesco Locatello, and Bernt Schiele. Test: Test-time self-training under distribution shift. In Proceedings of the IEEE/CVF Winter Conference on Ap- plications of Computer Vision, pages 2759–2769, 2023. 1, 2, 3, 6 [37] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. Advances in neural information processing systems, 33:596– 608, 2020. 2, 6 [38] Yongyi Su, Xun Xu, and Kui Jia. Revisiting realistic test- time training: Sequential inference and adaptation by an- chored clustering. Advances in Neural Information Process- ing Systems, 35:17543–17555, 2022. 3, 4 [39] Tao Sun, Mattia Segu, Janis Postels, Yuxuan Wang, Luc Van Gool, Bernt Schiele, Federico Tombari, and Fisher Yu. Shift: a synthetic driving dataset for continuous multi-task domain adaptation. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 21371–21382, 2022. 1, 5 [40] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self- supervision for generalization under distribution shifts. In International conference on machine learning, pages 9229– 9248. PMLR, 2020. 1, 2 [41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 2 [42] Vibashan VS, Poojan Oza, and Vishal M Patel. Towards on- line domain adaptive object detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 478–488, 2023. 3 [43] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Ol- shausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. arXiv preprint arXiv:2006.10726, 2020. 1, 2, 3, 7 5[44] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7201–7211, 2022. 3 [45] Zehao Xiao, Xiantong Zhen, Shengcai Liao, and Cees GM Snoek. Energy-based test sample adaptation for domain gen- eralization. arXiv preprint arXiv:2302.11215, 2023. 3 [46] Mengde Xu, Zheng Zhang, Han Hu, Jianfeng Wang, Lijuan Wang, Fangyun Wei, Xiang Bai, and Zicheng Liu. End- to-end semi-supervised object detection with soft teacher. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. 5 [47] Marvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and augmentation. Advances in Neural Information Processing Systems , 35: 38629–38642, 2022. 1, 2, 3 [48] Bowen Zhao, Chen Chen, and Shu-Tao Xia1. Delta: Degradation-free fully test-time adaptation. In International Conference on Learning Representations (ICLR), 2023. 2, 3 6",
      "meta_data": {
        "arxiv_id": "2312.08875v1",
        "authors": [
          "Jayeon Yoo",
          "Dongkwan Lee",
          "Inseop Chung",
          "Donghyun Kim",
          "Nojun Kwak"
        ],
        "published_date": "2023-12-12T07:13:08Z",
        "pdf_url": "https://arxiv.org/pdf/2312.08875v1.pdf"
      }
    },
    {
      "title": "DELTA: DEGRADATION-FREE FULLY TEST-TIME ADAPTATION",
      "abstract": "Fully test-time adaptation aims at adapting a pre-trained model to the test\nstream during real-time inference, which is urgently required when the test\ndistribution differs from the training distribution. Several efforts have been\ndevoted to improving adaptation performance. However, we find that two\nunfavorable defects are concealed in the prevalent adaptation methodologies\nlike test-time batch normalization (BN) and self-learning. First, we reveal\nthat the normalization statistics in test-time BN are completely affected by\nthe currently received test samples, resulting in inaccurate estimates. Second,\nwe show that during test-time adaptation, the parameter update is biased\ntowards some dominant classes. In addition to the extensively studied test\nstream with independent and class-balanced samples, we further observe that the\ndefects can be exacerbated in more complicated test environments, such as\n(time) dependent or class-imbalanced data. We observe that previous approaches\nwork well in certain scenarios while show performance degradation in others due\nto their faults. In this paper, we provide a plug-in solution called DELTA for\nDegradation-freE fuLly Test-time Adaptation, which consists of two components:\n(i) Test-time Batch Renormalization (TBR), introduced to improve the estimated\nnormalization statistics. (ii) Dynamic Online re-weighTing (DOT), designed to\naddress the class bias within optimization. We investigate various test-time\nadaptation methods on three commonly used datasets with four scenarios, and a\nnewly introduced real-world dataset. DELTA can help them deal with all\nscenarios simultaneously, leading to SOTA performance.",
      "meta_data": {
        "arxiv_id": "2301.13018v1",
        "authors": [
          "Bowen Zhao",
          "Chen Chen",
          "Shu-Tao Xia"
        ],
        "published_date": "2023-01-30T15:54:00Z",
        "pdf_url": "https://arxiv.org/pdf/2301.13018v1.pdf"
      }
    },
    {
      "title": "Test-Time Training with Self-Supervision for Generalization under Distribution Shifts",
      "abstract": "In this paper, we propose Test-Time Training, a general approach for\nimproving the performance of predictive models when training and test data come\nfrom different distributions. We turn a single unlabeled test sample into a\nself-supervised learning problem, on which we update the model parameters\nbefore making a prediction. This also extends naturally to data in an online\nstream. Our simple approach leads to improvements on diverse image\nclassification benchmarks aimed at evaluating robustness to distribution\nshifts.",
      "full_text": "Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Yu Sun1 Xiaolong Wang1 2 Zhuang Liu1 John Miller1 Alexei A. Efros1 Moritz Hardt1 Abstract In this paper, we propose Test-Time Training, a general approach for improving the performance of predictive models when training and test data come from different distributions. We turn a sin- gle unlabeled test sample into a self-supervised learning problem, on which we update the model parameters before making a prediction. This also extends naturally to data in an online stream. Our simple approach leads to improvements on di- verse image classiﬁcation benchmarks aimed at evaluating robustness to distribution shifts. 1. Introduction Supervised learning remains notoriously weak at generaliza- tion under distribution shifts. Unless training and test data are drawn from the same distribution, even seemingly minor differences turn out to defeat state-of-the-art models (Recht et al., 2018). Adversarial robustness and domain adapta- tion are but a few existing paradigms that try to anticipate differences between the training and test distribution with either topological structure or data from the test distribution available during training. We explore a new take on gener- alization that does not anticipate the distribution shifts, but instead learns from them at test time. We start from a simple observation. The unlabeled test sample xpresented at test time gives us a hint about the distribution from which it was drawn. We propose to take advantage of this hint on the test distribution by allowing the model parameters θto depend on the test sample x, but not its unknown label y. The concept of a variable decision boundary θ(x) is powerful in theory since it breaks away from the limitation of ﬁxed model capacity (see additional discussion in Section A1), but the design of a feedback mechanism from xto θ(x) raises new challenges in practice that we only begin to address here. 1University of California, Berkeley 2University of California, San Diego. Correspondence to: Yu Sun <yusun@berkeley.edu>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). Our proposed test-time training method creates a self- supervised learning problem based on this single test sample x, updating θat test time before making a prediction. Self- supervised learning uses an auxiliary task that automatically creates labels from unlabeled inputs. In our experiments, we use the task of rotating each input image by a multiple of 90 degrees and predicting its angle (Gidaris et al., 2018). This approach can also be easily modiﬁed to work outside the standard supervised learning setting. If several test samples arrive in a batch, we can use the entire batch for test-time training. If samples arrive in an online stream, we obtain further improvements by keeping the state of the parameters. After all, prediction is rarely a single event. The online version can be the natural mode of deployment under the additional assumption that test samples are produced by the same or smoothly changing distribution shifts. We experimentally validate our method in the context of object recognition on several standard benchmarks. These include images with diverse types of corruption at various levels (Hendrycks & Dietterich, 2019), video frames of moving objects (Shankar et al., 2019), and a new test set of unknown shifts collected by (Recht et al., 2018). Our algorithm makes substantial improvements under distribu- tion shifts, while maintaining the same performance on the original distribution. In our experiments, we compare with a strong baseline (labeled joint training) that uses both supervised and self- supervised learning at training-time, but keeps the model ﬁxed at test time. Recent work shows that training-time self- supervision improves robustness (Hendrycks et al., 2019a); our joint training baseline corresponds to an improved imple- mentation of this work. A comprehensive review of related work follows in Section 5. We complement the empirical results with theoretical inves- tigations in Section 4, and establish an intuitive sufﬁcient condition on a convex model of when Test-Time Training helps; this condition, roughly speaking, is to have correlated gradients between the loss functions of the two tasks. Project website: https://test-time-training.github.io/. arXiv:1909.13231v3  [cs.LG]  1 Jul 2020Test-Time Training with Self-Supervision for Generalization under Distribution Shifts 2. Method This section describes the algorithmic details of our method. To set up notation, consider a standard K-layer neural net- work with parameters θk for layer k. The stacked parameter vector θ = ( θ1,...,θ K) speciﬁes the entire model for a classiﬁcation task with loss function lm(x,y; θ) on the test sample (x,y). We call this the main task, as indicated by the subscript of the loss function. We assume to have training data (x1,y1),..., (xn,yn) drawn i.i.d. from a distribution P. Standard empirical risk minimization solves the optimization problem: min θ 1 n n∑ i=1 lm(xi,yi; θ). (1) Our method requires a self-supervised auxiliary task with loss function ls(x). In this paper, we choose the rotation prediction task (Gidaris et al., 2018), which has been demon- strated to be simple and effective at feature learning for convolutional neural networks. The task simply rotates x in the image plane by one of 0, 90, 180 and 270 degrees and have the model predict the angle of rotation as a four- way classiﬁcation problem. Other self-supervised tasks in Section 5 might also be used for our method. The auxiliary task shares some of the model parameters θe = ( θ1,...,θ κ) up to a certain κ ∈ {1,...,K }. We designate those κlayers as a shared feature extractor. The auxiliary task uses its own task-speciﬁc parameters θs = (θ′ κ+1,...,θ ′ K). We call the unshared parameters θs the self-supervised task branch, and θm = (θκ+1,...,θ K) the main task branch . Pictorially, the joint architecture is a Y-structure with a shared bottom and two branches. For our experiments, the self-supervised task branch has the same architecture as the main branch, except for the output dimensionality of the last layer due to the different number of classes in the two tasks. Training is done in the fashion of multi-task learning (Caru- ana, 1997); the model is trained on both tasks on the same data drawn fromP. Losses for both tasks are added together, and gradients are taken for the collection of all parameters. The joint training problem is therefore min θe,θm,θs 1 n n∑ i=1 lm(xi,yi; θm,θe) + ls(xi; θs,θe). (2) Now we describe the standard version of Test-Time Training on a single test sample x. Simply put, Test-Time Training ﬁne-tunes the shared feature extractor θe by minimizing the auxiliary task loss on x. This can be formulated as min θe ls(x; θs,θe). (3) Denote θ∗ e the (approximate) minimizer of Equation 3. The model then makes a prediction using the updated parameters θ(x) = (θ∗ e,θm). Empirically, the difference is negligible between minimizing Equation 3 over θe versus over both θe and θs. Theoretically, the difference exists only when optimization is done with more than one gradient step. Test-Time Training naturally beneﬁts from standard data augmentation techniques. On each test sample x, we per- form the exact same set of random transformations as for data augmentation during training, to form a batch only con- taining these augmented copies of xfor Test-Time Training. Online Test-Time Training. In the standard version of our method, the optimization problem in Equation 3 is al- ways initialized with parameters θ= (θe,θs) obtained by minimizing Equation 2. After making a prediction on x, θ∗ e is discarded. Outside of the standard supervised learning setting, when the test samples arrive online sequentially, the online version solves the same optimization problem as in Equation 3 to update the shared feature extractor θe. How- ever, on test sample xt, θis instead initialized with θ(xt−1) updated on the previous sample xt−1. This allows θ(xt) to take advantage of the distributional information available in x1,...,x t−1 as well as xt. 3. Empirical Results We experiment with both versions of our method (standard and online) on three kinds of benchmarks for distribution shifts, presented here in the order of visually low to high- level. Our code is available at the project website. Network details. Our architecture and hyper-parameters are consistent across all experiments. We use ResNets (He et al., 2016b), which are constructed differently for CIFAR-10 (Krizhevsky & Hinton, 2009) (26-layer) and Ima- geNet (Russakovsky et al., 2015) (18-layer). The CIFAR-10 dataset contains 50K images for training, and 10K images for testing. The ImageNet contains 1.2M images for train- ing and the 50K validation images are used as the test set. ResNets on CIFAR-10 have three groups, each containing convolutional layers with the same number of channels and size of feature maps; our splitting point is the end of the second group. ResNets on ImageNet have four groups; our splitting point is the end of the third group. We use Group Normalization (GN) instead of Batch Nor- malization (BN) in our architecture, since BN has been shown to be ineffective when training with small batches, for which the estimated batch statistics are not accurate (Ioffe & Szegedy, 2015). This technicality hurts Test-Time Training since each batch only contains (augmented) copies of a single image. Different from BN, GN is not dependent on batch size and achieves similar results on our baselines.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40 50Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure 1.Test error (%) on CIFAR-10-C with level 5 corruptions.We compare our approaches, Test-Time Training (TTT) and its online version (TTT-Online), with two baselines: object recognition without self-supervision, and joint training with self-supervision but keeping the model ﬁxed at test time. TTT improves over the baselines and TTT-Online improves even further. We report results with BN in Section A4 of the appendix for completeness. We directly compare our architecture to that of Hendrycks et al. (2018) in subsection A4.5. Optimization details. For joint training (Equation 2), we use stochastic gradient descent with standard hyper- parameters as (Huang et al., 2016; He et al., 2016a). For Test-Time Training (Equation 3), we use stochastic gradient descent with the learning rate set to that of the last epoch during training, which is 0.001 in all our experiments. We set weight decay and momentum to zero during Test-Time Training, inspired by practice in (He et al., 2018; Liu et al., 2018). For the standard version of Test-Time Training, we take ten gradient steps, using batches independently gener- ated by the same image. For online version of Test-Time Training, we take only one gradient step given each new im- age. We use random crop and random horizontal ﬂip for data augmentation. See Section A2 of the appendix for computa- tional aspects of our method. In all the tables and ﬁgures, object recognition task onlyrefers to the plain ResNet model (using GN, unless otherwise speciﬁed); joint training refers to the model jointly trained on both the main task and the self-supervised task, ﬁxed at test time; this has been pro- posed as the method in Hendrycks et al. (2019a); Test-Time Training (TTT) refers to the standard version described sec- tion 2; and online Test-Time Training (TTT-Online)refers to the online version that does not discardθ(xt) for xt arriving sequentially from the same distribution. Performance for TTT-Online is calculated as the average over the entire test set; we always shufﬂe the test set before TTT-Online to avoid ordering artifacts. 3.1. Object Recognition on Corrupted Images Hendrycks & Dietterich (2019) propose to benchmark ro- bustness of object recognition with 15 types of corruptions from four broad categories: noise, blur, weather and digital. Each corruption type comes in ﬁve levels of severity, with level 5 the most severe (details and sample images in the ap- pendix). The corruptions are simulated to mimic real-world corruptions as much as possible on copies of the test set for both CIFAR-10 and ImageNet. The new test sets are named as CIFAR-10-C and ImageNet-C, respectively. In the pro- posed benchmark, training should be done on the original training set, and the diversity of corruption types should make it difﬁcult for any methods to work well across the board if it relies too much on corruption speciﬁc knowledge. For online Test-Time Training, we take the entire test set as a stream of incoming images, and update and test on each image in an online manner as it arrives. CIFAR-10-C. Our results on the level 5 corruptions (most severe) are shown in Figure 1. The results on levels 1-4 are shown in Section A4 in appendix. Across all ﬁve levels and 15 corruption types, both standard and online versions of Test-Time Training improve over the object recognition task only baseline by a large margin. The standard version always improves over joint training, and the online version often improves signiﬁcantly (>10%) over joint training and never hurts by more than 0.2%. Speciﬁcally, TTT-Online contributes >24% on the three noise types and 38% on pix- elation. For a learning problem with the seemingly unstable setup that abuses a single image, this kind of consistency is rather surprising. The baseline ResNet-26 with object recognition task only has error 8.9% on the original test set of CIFAR-10. The joint training baseline actually improves performance on the original to 8.1%. More surprisingly, unlike many other methods that trade off original performance for robustness, Test-Time Training further improves on the original test set by 0.2% consistently over multiple independent trials. This suggests that our method does not choose between speciﬁcity and generality.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 20 40 60Accuracy (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online 0 20000 40000 Number of samples 60 62 64 66 68 70 72 74 76Accuracy (%) Original Sliding window average 0 20000 40000 Number of samples 12 15 18 21 24 27 30 33Accuracy (%) Gaussian Noise Sliding window average 0 20000 40000 Number of samples 16 18 20 22 24 26 28 30 32Accuracy (%) Defocus Blur Sliding window average 0 20000 40000 Number of samples 28 30 32 34 36 38Accuracy (%) Zoom Blur Sliding window average 0 20000 40000 Number of samples 33 36 39 42 45 48 51 54Accuracy (%) Fog Sliding window average 0 20000 40000 Number of samples 30 33 36 39 42 45 48 51Accuracy (%) Elastic Transform Sliding window average Figure 2.Test accuracy (%) on ImageNet-C with level 5 corruptions.Upper panel: Our approaches, TTT and TTT-Online, show signiﬁcant improvements in all corruption types over the two baselines. Lower panel: We show the accuracy of TTT-Online as the average over a sliding window of 100 samples; TTT-Online generalizes better as more samples are evaluated (x-axis), without hurting on the original distribution. We use accuracy instead of error here because the baseline performance is very low for most corruptions. Separate from our method, it is interesting to note that joint training consistently improves over the single-task baseline, as discovered by Hendrycks et al. (2019a). Hendrycks & Dietterich (2019) have also experimented with various other training methods on this benchmark, and point to Adversar- ial Logit Pairing (ALP) (Kannan et al., 2018) as the most effective approach. Results of this additional baseline on all levels of CIFAR-10-C are shown in the appendix, along with its implementation details. While surprisingly robust under some of the most severe corruptions (especially the three noise types), ALP incurs a much larger error (by a factor of two) on the original distribution and some corruptions (e.g. all levels of contrast and fog), and hurts performance signiﬁcantly when the corruptions are not as severe (espe- cially on levels 1-3); this kind of tradeoff is to be expected for methods based on adversarial training. ImageNet-C. Our results on the level 5 corruptions (most severe) are shown in Figure 2. We use accuracy instead of error for this dataset because the baseline performance is very low for most corruptions. The general trend is roughly the same as on CIFAR-10-C. The standard version of TTT always improves over the baseline and joint training, while the online version only hurts on the original by 0.1% over the baseline, but signiﬁcantly improves (by a factor of more than three) on many of the corruption types. In the lower panel of Figure 2, we visualize how the accu- racy (averaged over a sliding window) of the online version changes as more images are tested. Due to space constraints, we show this plot on the original test set, as well as every third corruption type, following the same order as in the original paper. On the original test set, there is no visible trend in performance change after updating on the 50,000 samples. With corruptions, accuracy has already risen sig- niﬁcantly after 10,000 samples, but is still rising towards the end of the 50,000 samples, indicating room for additional improvements if more samples were available. Without seeing a single label, TTT-Online behaves as if we were training on the test set from the appearance of the plots.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg TTT-Online 8.2 25.8 22.6 30.6 14.6 34.4 18.3 17.1 20.0 18.0 16.9 11.2 15.6 21.6 18.1 21.2 UDA-SS 9.0 28.2 26.5 20.8 15.6 43.7 24.5 23.8 25.0 24.9 17.2 12.7 11.6 22.1 20.3 22.6 Table 1.Test error (%) on CIFAR-10-C with level 5 corruption.Comparison between online Test-Time Training (TTT-Online) and unsupervised domain adaptation by self-supervision (UDA-SS) (Sun et al., 2019) with access to the entire (unlabeled) test set during training. We highlight the lower error in bold. We have abbreviated the names of the corruptions, in order: original test set, Gaussian noise, shot noise, impulse noise, defocus blur, glass blue, motion blur, zoom blur, snow, frost, fog, brightness, contrast, elastic transformation, pixelation, and JPEG compression. The reported numbers for TTT-Online are the same as in Figure 1. See complete table in Table A2. 0 2000 4000 6000 8000 Number of samples 12 16 20 24 28 32 36 40 44 48Error (%) Gaussian Noise Joint training TTT TTT-Online UDA-SS 0 2000 4000 6000 8000 Number of samples 9 12 15 18 21 24 27 30 33 36Error (%) Shot Noise Joint training TTT TTT-Online UDA-SS 0 2000 4000 6000 8000 Number of samples 15 20 25 30 35 40 45 50Error (%) Impulse Noise Joint training TTT TTT-Online UDA-SS Figure 3.Test error (%) on CIFAR-10-C, for the three noise types, with gradually changing distribution.The distribution shifts are created by increasing the standard deviation of each noise type from small to large, the further we go on the x-axis. As the samples get noisier, all methods suffer greater errors the more we evaluate into the test set, but online Test-Time Training (TTT-Online) achieves gentler slopes than joint training. For the ﬁrst two noise types, TTT-Online also achieves better results over unsupervised domain adaptation by self-supervision (UDA-SS) (Sun et al., 2019). Comparison with unsupervised domain adaptation. Table 1 empirically compares online Test-Time Training (TTT-Online) with unsupervised domain adaptation through self-supervision (UDA-SS) (Sun et al., 2019), which is sim- ilar to our method in spirit but is designed for the setting of unsupervised domain adaptation (Section 5 provides a sur- vey of other related work in this setting). Given labeled data from the training distribution and unlabeled data from the test distribution, UDA-SS hopes to ﬁnd an invariant repre- sentation that extracts useful features for both distributions by learning to perform a self-supervised task, speciﬁcally rotation prediction, simultaneously on data from both. It then learns a labeling function on top of the invariant rep- resentation using the labeled data. In our experiments, the unlabeled data given to UDA-SS is the entire test set itself without the labels. Because TTT-Online can only learn from the unlabeled test samples that have already been evaluated on, it is given less information than UDA-SS at all times. In this sense, UDA- SS should be regarded as an oracle rather than a baseline. Surprisingly, TTT-Online outperforms UDA-SS on 13 out of the 15 corruptions as well as the original distribution. Our explanation is that UDA-SS has to ﬁnd an invariant representation for both distributions, while TTT-Online only adapts the representation to be good for the current test distribution. That is, TTT-Online has the ﬂexibility to forget the training distribution representation, which is no longer relevant. This suggests that in our setting, forgetting is not harmful and perhaps should even be taken advantage of. Gradually changing distribution shifts.In our previous experiments, we have been evaluating the online version under the assumption that the test inputs xt for t= 1...nare all sampled from the same test distribution Q, which can be different from the training distribution P. This assumption is indeed satisﬁed for i.i.d. samples from a shufﬂed test set. But here we show that this assumption can in fact be relaxed to allow xt ∼Qt, where Qt is close to Qt+1 (in the sense of distributional distance). We call this the assumption of gradually changing distribution shifts. We perform experiments by simulating such distribution shifts on the three noise types of CIFAR-10-C. For each noise type, xt is corrupted with standard deviation σt, and σ1,...,σ n interpolate between the standard deviation of level 1 and level 5. So xt is more severely corrupted as we evaluate further into the test set and t grows larger. As shown in Figure 3, TTT-Online still improves upon joint training (and our standard version) with this relaxed assumption, and even upon UDA-SS for the ﬁrst two noise types.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Accuracy (%) Airplane Bird Car Dog Cat Horse Ship Average Object recognition task only 67.9 35.8 42.6 14.7 52.0 42.0 66.7 41.4 Joint training (Hendrycks et al., 2019a) 70.2 36.7 42.6 15.5 52.0 44.0 66.7 42.4 TTT (standard version) 70.2 39.2 42.6 21.6 54.7 46.0 77.8 45.2 TTT-Online 70.2 39.2 42.6 22.4 54.7 46.0 77.8 45.4 Table 2.Class-wise and average classiﬁcation accuracy (%) on CIFAR classes in VID-Robust, adapted from (Shankar et al., 2019). Test-Time Training (TTT) and online Test-Time Training (TTT-Online) improve over the two baselines on average, and by a large margin on “ship” and “dog” classes where the rotation task is more meaningful than in classes like “airplane” (sample images in Figure A7). 3.2. Object Recognition on Video Frames The Robust ImageNet Video Classiﬁcation (VID-Robust) dataset was developed by Shankar et al. (2019) from the Ima- geNet Video detection dataset (Russakovsky et al., 2015), to demonstrate how deep models for object recognition trained on ImageNet (still images) fail to adapt well to video frames. The VID-Robust dataset contains 1109 sets of video frames in 30 classes; each set is a short video clip of frames that are similar to an anchor frame. Our results are reported on the anchor frames. To map the 1000 ImageNet classes to the 30 VID-Robust classes, we use the max-conversion function in Shankar et al. (2019). Without any modiﬁcations for videos, we apply our method to VID-Robust on top of the same ImageNet model as in the previous subsection. Our classiﬁcation accuracy is reported in Table 3. In addition, we take the seven classes in VID-Robust that overlap with CIFAR-10, and re-scale those video frames to the size of CIFAR-10 images, as a new test set for the model trained on CIFAR-10 in the previous subsection. Again, we apply our method to this dataset without any modiﬁcations. Our results are shown in Table 2, with a breakdown for each class. Noticing that Test-Time Training does not improve on the airplane class, we inspect some airplane samples (Figure A7), and observe black margins on two sides of most images, which provide a trivial hint for rotation prediction. In addition, given an image of airplanes in the sky, it is often impossible even for humans to tell if it is rotated. This shows that our method requires the self-supervised task to be both well deﬁned and non-trivial. 3.3. CIFAR-10.1: Unknown Distribution Shifts CIFAR-10.1 (Recht et al., 2018) is a new test set of size 2000 modeled after CIFAR-10, with the exact same classes and image dimensionality, following the dataset creation process documented by the original CIFAR-10 paper as closely as possible. The purpose is to investigate the distribution shifts present between the two test sets, and the effect on object recognition. All models tested by the authors suffer a large performance drop on CIFAR-10.1 comparing to CIFAR-10, even though there is no human noticeable difference, and Method Accuracy (%) Object recognition task only 62.7 Joint training (Hendrycks et al., 2019a) 63.5 TTT (standard version) 63.8 TTT-Online 64.3 Table 3.Test accuracy (%) on VID-Robust dataset (Shankar et al., 2019). TTT and TTT-Online improve over the baselines. Method Error (%) Object recognition task only 17.4 Joint training (Hendrycks et al., 2019a) 16.7 TTT (standard version) 15.9 Table 4.Test error (%) on CIFAR-10.1 (Recht et al., 2018). TTT is the ﬁrst method to improve the performance of an existing model on this new test set. both have the same human accuracy. This demonstrates how insidious and ubiquitous distribution shifts are, even when researchers strive to minimize them. The distribution shifts from CIFAR-10 to CIFAR-10.1 pose an extremely difﬁcult problem, and no prior work has been able to improve the performance of an existing model on this new test set, probably because: 1) researchers cannot even identify the distribution shifts, let alone describe them mathematically; 2) the samples in CIFAR-10.1 are only revealed at test time; and even if they were revealed during training, the distribution shifts are too subtle, and the sample size is too small, for domain adaptation (Recht et al., 2018). On the original CIFAR-10 test set, the baseline with only object recognition has error 8.9%, and with joint training has 8.1%; comparing to the ﬁrst two rows of Table 4, both suffer the typical performance drop (by a factor of two). TTT yields an improvement of 0.8% (relative improvement of 4.8%) over joint training. We recognize that this improve- ment is small relative to the performance drop, but see it as an encouraging ﬁrst step for this very difﬁcult problem.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts 0 10 20 30 40 50 60 Gradient inner product 0 1 2 3 4 5Improvement (%) Level 5 Level 4 Level 3 Level 2 Level 1 0 10 20 30 40 50 60 Gradient inner product 0 5 10 15 20 25 30 35Improvement (%) Level 5 Level 4 Level 3 Level 2 Level 1 Figure 4.Scatter plot of the inner product between the gradients (on the shared feature extractor θe) of the main task lm and the self- supervised task le, and the improvement in test error (%) from Test-Time Training, for the standard (left) and online (right) version. Each point is the average over a test set, and each scatter plot has 75 test sets, from all 15 types of corruptions over ﬁve levels as described in subsection 3.1. The blue lines and bands are the best linear ﬁts and the 99% conﬁdence intervals. The linear correlation coefﬁcients are 0.93 and 0.89 respectively, indicating strong positive correlation between the two quantities, as suggested by Theorem 1. 4. Theoretical Results This section contains our preliminary study of when and why Test-Time Training is expected to work. For convex models, we prove that positive gradient correlation between the loss functions leads to better performance on the main task after Test-Time Training. Equipped with this insight, we then empirically demonstrate that gradient correlation governs the success of Test-Time Training on the deep learning model discussed in Section 3. Before stating our main theoretical result, we ﬁrst illustrate the general intuition with a toy model. Consider a regression problem where x∈Rd denotes the input, y1 ∈R denotes the label, and the objective is the square loss (ˆy−y1)2/2 for a prediction ˆy. Consider a two layer linear network parametrized by A∈Rh×d and v ∈Rh (where hstands for the hidden dimension). The prediction according to this model is ˆy= v⊤Ax, and the main task loss is lm(x,y1; A,v) = 1 2 ( y1 −v⊤Ax )2 . (4) In addition, consider a self-supervised regression task that also uses the square loss and automatically generates a label ys for x. Let the self-supervised head be parametrized by w∈Rh. Then the self-supervised task loss is ls(x,y2; A,w) = 1 2 ( y2 −w⊤Ax )2 . (5) Now we apply Test-Time Training to update the shared feature extractor Aby one step of gradient descent on ls, which we can compute with y2 known. This gives us A′←A−η ( y2 −w⊤Ax )( −wx⊤) , (6) where A′is the updated matrix and ηis the learning rate. If we set η= η∗where η∗= y1 −v⊤Ax (y2 −w⊤Ax) v⊤wx⊤x, (7) then with some simple algebra, it is easy to see that the main task loss lm(x,y1; A′,v) = 0. Concretely, Test-Time Training drives the main task loss down to zero with a single gradient step for a carefully chosen learning rate. In prac- tice, this learning rate is unknown since it depends on the unknown y1. However, since our model is convex, as long as η∗is positive, it sufﬁces to set η to be a small positive constant (see details in the appendix). If x̸= 0, one sufﬁ- cient condition for η∗to be positive (when neither loss is zero) is to have sign ( y1 −v⊤Ax ) = sign ( y2 −w⊤Ax ) (8) and v⊤w>0 . (9) For our toy model, both parts of the condition above have an intuition interpretation. The ﬁrst part says that the mistakes should be correlated, in the sense that predictions from both tasks are mistaken in the same direction. The second part, v⊤w>0, says that the decision boundaries on the feature space should be correlated. In fact, these two parts hold iff. ⟨∇lm(A),∇ls(A)⟩>0 (see a simple proof of this fact in the appendix). To summarize, if the gradients have positive correlation, Test-Time Training is guaranteed to reduce the main task loss. Our main theoretical result extends this to general smooth and convex loss functions.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Theorem 1. Let lm(x,y; θ) denote the main task loss on test instance x,y with parameters θ, and ls(x; θ) the self- supervised task loss that only depends onx. Assume that for all x,y, lm(x,y; θ) is differentiable, convex andβ-smooth in θ, and both ∥∇lm(x,y; θ)∥,∥∇ls(x,θ)∥≤ Gfor all θ. With a ﬁxed learning rate η= ϵ βG2 , for every x,y such that ⟨∇lm(x,y; θ),∇ls(x; θ)⟩>ϵ, (10) we have lm(x,y; θ) >lm(x,y; θ(x)), (11) where θ(x) = θ−η∇ls(x; θ) i.e. Test-Time Training with one step of gradient descent. The proof uses standard techniques in optimization, and is left for the appendix. Theorem 1 reveals gradient correlation as a determining factor of the success of Test-Time Training in the smooth and convex case. In Figure 4, we empirically show that our insight also holds for non-convex loss func- tions, on the deep learning model and across the diverse set of corruptions considered in Section 3; stronger gradient cor- relation clearly indicates more performance improvement over the baseline. 5. Related Work Learning on test instances. Shocher et al. (2018) pro- vide a key inspiration for our work by showing that image super-resolution could be learned at test time simply by try- ing to upsample a downsampled version of the input image. More recently, Bau et al. (2019) improve photo manipula- tion by adapting a pre-trained GAN to the statistics of the input image. One of the earlier examples of this idea comes from Jain & Learned-Miller (2011), who improve Viola- Jones face detection (Viola et al., 2001) by bootstrapping the more difﬁcult faces in an image from the more easily detected faces in that same image. The online version of our algorithm is inspired by the work of Mullapudi et al. (2018), which makes video segmentation more efﬁcient by using a student model that learns online from a teacher model. The idea of online updates has also been used in Kalal et al. (2011) for tracking and detection. A recent work in echocardiography (Zhu et al., 2019) improves the deep learning model that tracks myocardial motion and cardiac blood ﬂow with sequential updates. Lastly, we share the philosophy of transductive learning (Vapnik, 2013; Gam- merman et al., 1998), but have little in common with their classical algorithms; recent work by Tripuraneni & Mackey (2019) theoretically explores this for linear prediction, in the context of debiasing the LASSO estimator. Self-supervised learning studies how to create labels from the data, by designing various pretext tasks that can learn semantic information without human annotations, such as context prediction (Doersch et al., 2015), solving jig- saw puzzles (Noroozi & Favaro, 2016), colorization (Lars- son et al., 2017; Zhang et al., 2016), noise prediction (Bo- janowski & Joulin, 2017), feature clustering (Caron et al., 2018). Our paper uses rotation prediction (Gidaris et al., 2018). Asano et al. (2019) show that self-supervised learn- ing on only a single image, surprisingly, can produce low- level features that generalize well. Closely related to our work, Hendrycks et al. (2019a) propose that jointly training a main task and a self-supervised task (our joint training baseline in Section 3) can improve robustness on the main task. The same idea is used in few-shot learning (Su et al., 2019), domain generalization (Carlucci et al., 2019), and unsupervised domain adaptation (Sun et al., 2019). Adversarial robustness studies the robust risk RP,∆(θ) = Ex,y∼P maxδ∈∆ l(x + δ,y; θ), where l is some loss function, and ∆ is the set of perturbations; ∆ is often chosen as the Lp ball, for p ∈{1,2,∞}. Many popular algorithms formulate and solve this as a robust optimization problem (Goodfellow et al., 2014; Madry et al., 2017; Sinha et al., 2017; Raghunathan et al., 2018; Wong & Kolter, 2017; Croce et al., 2018), and the most well known technique is adversarial training. Another line of work is based on randomized smoothing (Cohen et al., 2019; Salman et al., 2019), while some other approaches, such as input transformations (Guo et al., 2017; Song et al., 2017), are shown to be less effective (Athalye et al., 2018). There are two main problems with the approaches above. First, all of them can be seen as smoothing the decision boundary. This establishes a theoretical tradeoff between accuracy and robustness (Tsipras et al., 2018; Zhang et al., 2019), which we also observe empirically with our adversarial training baseline in Section 3. Intuitively, the more diverse ∆ is, the less effective this one-boundary-ﬁts-all approach can be for a particular element of ∆. Second, adversarial methods rely heavily on the mathematical structure of ∆, which might not accurately model perturbations in the real world. Therefore, generalization remains hard outside of the ∆ we know in advance or can mathematically model, especially for non-adversarial distribution shifts. Empirically, Kang et al. (2019) shows that robustness for one ∆ might not transfer to another, and training on the L∞ball actually hurts robustness on the L1 ball. Non-adversarial robustness studies the effect of corrup- tions, perturbations, out-of-distribution examples, and real- world distribution shifts (Hendrycks et al., 2019b;a; 2018; Hendrycks & Gimpel, 2016). Geirhos et al. (2018) show that training on images corrupted by Gaussian noise makes deep learning models robust to this particular noise type, but does not improve performance on images corrupted by another noise type e.g. salt-and-pepper noise.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Unsupervised domain adaptation (a.k.a. transfer learn- ing) studies the problem of distribution shifts, when an unlabeled dataset from the test distribution (target domain) is available at training time, in addition to a labeled dataset from the training distribution (source domain) (Chen et al., 2011; Gong et al., 2012; Long et al., 2015; Ganin et al., 2016; Long et al., 2016; Tzeng et al., 2017; Hoffman et al., 2017; Csurka, 2017; Chen et al., 2018). The limitation of the problem setting, however, is that generalization might only be improved for this speciﬁc test distribution, which can be difﬁcult to anticipate in advance. Prior work try to anticipate broader distributions by using multiple and evolv- ing domains (Hoffman et al., 2018; 2012; 2014). Test-Time Training does not anticipate any test distribution, by chang- ing the setting of unsupervised domain adaptation, while taking inspiration from its algorithms. Our paper is a follow- up to Sun et al. (2019), which we explain and empirically compare with in Section 3. Our update rule can be viewed as performing one-sample unsupervised domain adaptation on the ﬂy, with the caveat that standard domain adaptation techniques might become ill-deﬁned when there is only one sample from the target domain. Domain generalization studies the setting where a meta distribution generates multiple environment distributions, some of which are available during training (source), while others are used for testing (target) (Li et al., 2018; Shankar et al., 2018; Muandet et al., 2013; Balaji et al., 2018; Ghifary et al., 2015; Motiian et al., 2017; Li et al., 2017a; Gan et al., 2016). With only a few environments, information on the meta distribution is often too scarce to be helpful, and with many environments, we are back to the i.i.d. setting where each environment can be seen as a sample, and a strong baseline is to simply train on all the environments (Li et al., 2019). The setting of domain generalization is limited by the inherent tradeoff between speciﬁcity and generality of a ﬁxed decision boundary, and the fact that generalization is again elusive outside of the meta distribution i.e. the actual P learned by the algorithm. One (few)-shot learning studies how to learn a new task or a new classiﬁcation category using only one (or a few) sample(s), on top of a general representation that has been learned on diverse samples (Snell et al., 2017; Vinyals et al., 2016; Fei-Fei et al., 2006; Ravi & Larochelle, 2016; Li et al., 2017b; Finn et al., 2017; Gidaris & Komodakis, 2018). Our update rule can be viewed as performing one-shot self- supervised learning and can potentially be improved by progress in one-shot learning. Continual learning (a.k.a. learning without forgetting) studies the setting where a model is made to learn a sequence of tasks, and not forget about the earlier ones while training for the later (Li & Hoiem, 2017; Lopez-Paz & Ranzato, 2017; Kirkpatrick et al., 2017; Santoro et al., 2016). In contrast, with Test-Time Training, we are not concerned about forgetting the past test samples since they have already been evaluated on; and if a past sample comes up by any chance, it would go through Test-Time Training again. In addition, the impact of forgetting the training set is minimal, because both tasks have already been jointly trained. Online learning (a.k.a. online optimization) is a well- studied area of learning theory (Shalev-Shwartz et al., 2012; Hazan et al., 2016). The basic setting repeats the following: receive xt, predict ˆyt, receive yt from a worst-case oracle, and learn. Final performance is evaluated using the regret, which colloquially translates to how much worse the online learning algorithm performs in comparison to the best ﬁxed model in hindsight. In contrast, our setting never reveals any yt during testing even for the online version, so we do not need to invoke the concept of the worst-case oracle or the regret. Also, due to the lack of feedback from the envi- ronment after predicting, our algorithm is motivated to learn (with self-supervision) before predicting ˆyt instead of after. Note that some of the previously covered papers (Hoffman et al., 2014; Jain & Learned-Miller, 2011; Mullapudi et al., 2018) use the term “online learning” outside of the learning theory setting, so the term can be overloaded. 6. Discussion The idea of test-time training also makes sense for other tasks, such as segmentation and detection, and in other ﬁelds, such as speech recognition and natural language process- ing. For machine learning practitioners with prior domain knowledge in their respective ﬁelds, their expertise can be leveraged to design better special-purpose self-supervised tasks for test-time training. Researchers for general-purpose self-supervised tasks can also use test-time training as an evaluation benchmark, in addition to the currently prevalent benchmark of pre-training and ﬁne-tuning. More generally, we hope this paper can encourage re- searchers to abandon the self-imposed constraint of a ﬁxed decision boundary for testing, or even the artiﬁcial division between training and testing altogether. Our work is but a small step toward a new paradigm where much of the learning happens after a model is deployed. Acknowledgements. This work is supported by NSF grant 1764033, DARPA and Berkeley DeepDrive. This paper took a long time to develop, and beneﬁted from con- versations with many of our colleagues, including Ben Recht and his students Ludwig Schmidt, Vaishaal Shanker and Becca Roelofs; Ravi Teja Mullapudi, Achal Dave and Deva Ramanan; and Armin Askari, Allan Jabri, Ashish Kumar, Angjoo Kanazawa and Jitendra Malik.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts References Asano, Y . M., Rupprecht, C., and Vedaldi, A. Surprising effectiveness of few-image unsupervised feature learning. arXiv preprint arXiv:1904.13132, 2019. Athalye, A., Carlini, N., and Wagner, D. Obfuscated gradients give a false sense of security: Circumvent- ing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018. Balaji, Y ., Sankaranarayanan, S., and Chellappa, R. Metareg: Towards domain generalization using meta-regularization. In Advances in Neural Information Processing Systems, pp. 998–1008, 2018. Bau, D., Strobelt, H., Peebles, W., Wulff, J., Zhou, B., Zhu, J.-Y ., and Torralba, A. Semantic photo manipulation with a generative image prior. ACM Transactions on Graphics (TOG), 38(4):59, 2019. Bojanowski, P. and Joulin, A. Unsupervised learning by predicting noise. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 517– 526. JMLR. org, 2017. Carlucci, F. M., D’Innocente, A., Bucci, S., Caputo, B., and Tommasi, T. Domain generalization by solving jigsaw puzzles. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition , pp. 2229–2238, 2019. Caron, M., Bojanowski, P., Joulin, A., and Douze, M. Deep clustering for unsupervised learning of visual features. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 132–149, 2018. Caruana, R. Multitask learning. Machine learning, 28(1): 41–75, 1997. Chen, M., Weinberger, K. Q., and Blitzer, J. Co-training for domain adaptation. In Advances in neural information processing systems, pp. 2456–2464, 2011. Chen, X., Sun, Y ., Athiwaratkun, B., Cardie, C., and Wein- berger, K. Adversarial deep averaging networks for cross- lingual sentiment classiﬁcation. Transactions of the Asso- ciation for Computational Linguistics, 6:557–570, 2018. Cohen, J. M., Rosenfeld, E., and Kolter, J. Z. Certiﬁed adversarial robustness via randomized smoothing. arXiv preprint arXiv:1902.02918, 2019. Croce, F., Andriushchenko, M., and Hein, M. Provable robustness of relu networks via maximization of linear regions. arXiv preprint arXiv:1810.07481, 2018. Csurka, G. Domain adaptation for visual applications: A comprehensive survey. arXiv preprint arXiv:1702.05374, 2017. Ding, G. W., Wang, L., and Jin, X. AdverTorch v0.1: An adversarial robustness toolbox based on pytorch. arXiv preprint arXiv:1902.07623, 2019. Doersch, C., Gupta, A., and Efros, A. A. Unsupervised visual representation learning by context prediction. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1422–1430, 2015. Fei-Fei, L., Fergus, R., and Perona, P. One-shot learning of object categories. IEEE transactions on pattern analysis and machine intelligence, 28(4):594–611, 2006. Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta- learning for fast adaptation of deep networks. In Proceed- ings of the 34th International Conference on Machine Learning-Volume 70, pp. 1126–1135. JMLR. org, 2017. Gammerman, A., V ovk, V ., and Vapnik, V . Learning by transduction. In Proceedings of the Fourteenth conference on Uncertainty in artiﬁcial intelligence , pp. 148–155. Morgan Kaufmann Publishers Inc., 1998. Gan, C., Yang, T., and Gong, B. Learning attributes equals multi-source domain generalization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 87–97, 2016. Ganin, Y ., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M., and Lempitsky, V . Domain-adversarial training of neural networks. The Journal of Machine Learning Research, 17(1):2096–2030, 2016. Geirhos, R., Temme, C. R., Rauber, J., Sch¨utt, H. H., Bethge, M., and Wichmann, F. A. Generalisation in humans and deep neural networks. In Advances in Neural Information Processing Systems, pp. 7538–7550, 2018. Ghifary, M., Bastiaan Kleijn, W., Zhang, M., and Balduzzi, D. Domain generalization for object recognition with multi-task autoencoders. In Proceedings of the IEEE international conference on computer vision, pp. 2551– 2559, 2015. Gidaris, S. and Komodakis, N. Dynamic few-shot visual learning without forgetting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4367–4375, 2018. Gidaris, S., Singh, P., and Komodakis, N. Unsupervised rep- resentation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018. Gong, B., Shi, Y ., Sha, F., and Grauman, K. Geodesic ﬂow kernel for unsupervised domain adaptation. In2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 2066–2073. IEEE, 2012.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain- ing and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. Guo, C., Rana, M., Cisse, M., and van der Maaten, L. Coun- tering adversarial images using input transformations. arXiv preprint arXiv:1711.00117, 2017. Hazan, E. et al. Introduction to online convex optimization. Foundations and Trends® in Optimization, 2(3-4):157– 325, 2016. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016a. He, K., Zhang, X., Ren, S., and Sun, J. Identity mappings in deep residual networks. In European conference on computer vision, pp. 630–645. Springer, 2016b. He, K., Girshick, R., and Doll ´ar, P. Rethinking imagenet pre-training. arXiv preprint arXiv:1811.08883, 2018. Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions and perturba- tions. arXiv preprint arXiv:1903.12261, 2019. Hendrycks, D. and Gimpel, K. A baseline for detecting misclassiﬁed and out-of-distribution examples in neural networks. arXiv preprint arXiv:1610.02136, 2016. Hendrycks, D., Mazeika, M., Wilson, D., and Gimpel, K. Using trusted data to train deep networks on labels cor- rupted by severe noise. InAdvances in neural information processing systems, pp. 10456–10465, 2018. Hendrycks, D., Lee, K., and Mazeika, M. Using pre-training can improve model robustness and uncertainty. arXiv preprint arXiv:1901.09960, 2019a. Hendrycks, D., Mazeika, M., Kadavath, S., and Song, D. Improving model robustness and uncertainty estimates with self-supervised learning. arXiv preprint, 2019b. Hoffman, J., Kulis, B., Darrell, T., and Saenko, K. Discover- ing latent domains for multisource domain adaptation. In European Conference on Computer Vision, pp. 702–715. Springer, 2012. Hoffman, J., Darrell, T., and Saenko, K. Continuous man- ifold based adaptation for evolving visual domains. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 867–874, 2014. Hoffman, J., Tzeng, E., Park, T., Zhu, J.-Y ., Isola, P., Saenko, K., Efros, A. A., and Darrell, T. Cycada: Cycle- consistent adversarial domain adaptation. arXiv preprint arXiv:1711.03213, 2017. Hoffman, J., Mohri, M., and Zhang, N. Algorithms and theory for multiple-source adaptation. In Advances in Neural Information Processing Systems, pp. 8246–8256, 2018. Huang, G., Sun, Y ., Liu, Z., Sedra, D., and Weinberger, K. Q. Deep networks with stochastic depth. In European conference on computer vision, pp. 646–661. Springer, 2016. Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. Jain, V . and Learned-Miller, E. Online domain adaptation of a pre-trained cascade of classiﬁers. In CVPR 2011, pp. 577–584. IEEE, 2011. Kalal, Z., Mikolajczyk, K., and Matas, J. Tracking-learning- detection. IEEE transactions on pattern analysis and machine intelligence, 34(7):1409–1422, 2011. Kang, D., Sun, Y ., Brown, T., Hendrycks, D., and Steinhardt, J. Transfer of adversarial robustness between perturbation types. arXiv preprint arXiv:1905.01034, 2019. Kannan, H., Kurakin, A., and Goodfellow, I. Adversarial logit pairing. arXiv preprint arXiv:1803.06373, 2018. Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Des- jardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017. Krizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009. Larsson, G., Maire, M., and Shakhnarovich, G. Colorization as a proxy task for visual understanding. In CVPR, 2017. Li, D., Yang, Y ., Song, Y .-Z., and Hospedales, T. M. Deeper, broader and artier domain generalization. In Proceed- ings of the IEEE International Conference on Computer Vision, pp. 5542–5550, 2017a. Li, D., Zhang, J., Yang, Y ., Liu, C., Song, Y .-Z., and Hospedales, T. M. Episodic training for domain gen- eralization. arXiv preprint arXiv:1902.00113, 2019. Li, Y ., Tian, X., Gong, M., Liu, Y ., Liu, T., Zhang, K., and Tao, D. Deep domain generalization via conditional invariant adversarial networks. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 624–639, 2018.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Li, Z. and Hoiem, D. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935–2947, 2017. Li, Z., Zhou, F., Chen, F., and Li, H. Meta-sgd: Learning to learn quickly for few-shot learning. arXiv preprint arXiv:1707.09835, 2017b. Liu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T. Re- thinking the value of network pruning. arXiv preprint arXiv:1810.05270, 2018. Long, M., Cao, Y ., Wang, J., and Jordan, M. I. Learn- ing transferable features with deep adaptation networks. arXiv preprint arXiv:1502.02791, 2015. Long, M., Zhu, H., Wang, J., and Jordan, M. I. Unsupervised domain adaptation with residual transfer networks. In Advances in Neural Information Processing Systems, pp. 136–144, 2016. Lopez-Paz, D. and Ranzato, M. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems, pp. 6467–6476, 2017. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 , 2017. Motiian, S., Piccirilli, M., Adjeroh, D. A., and Doretto, G. Uniﬁed deep supervised domain adaptation and gen- eralization. In Proceedings of the IEEE International Conference on Computer Vision, pp. 5715–5725, 2017. Muandet, K., Balduzzi, D., and Sch ¨olkopf, B. Domain generalization via invariant feature representation. In International Conference on Machine Learning, pp. 10– 18, 2013. Mullapudi, R. T., Chen, S., Zhang, K., Ramanan, D., and Fatahalian, K. Online model distillation for efﬁcient video inference. arXiv preprint arXiv:1812.02699, 2018. Noroozi, M. and Favaro, P. Unsupervised learning of visual representations by solving jigsaw puzzles. In European Conference on Computer Vision , pp. 69–84. Springer, 2016. Raghunathan, A., Steinhardt, J., and Liang, P. Certiﬁed defenses against adversarial examples. arXiv preprint arXiv:1801.09344, 2018. Ravi, S. and Larochelle, H. Optimization as a model for few-shot learning. IEEE transactions on pattern analysis and machine intelligence, 2016. Recht, B., Roelofs, R., Schmidt, L., and Shankar, V . Do cifar-10 classiﬁers generalize to cifar-10? arXiv preprint arXiv:1806.00451, 2018. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV) , 115(3):211–252, 2015. doi: 10.1007/s11263-015-0816-y. Salman, H., Yang, G., Li, J., Zhang, P., Zhang, H., Razen- shteyn, I., and Bubeck, S. Provably robust deep learn- ing via adversarially trained smoothed classiﬁers. arXiv preprint arXiv:1906.04584, 2019. Santoro, A., Bartunov, S., Botvinick, M., Wierstra, D., and Lillicrap, T. Meta-learning with memory-augmented neu- ral networks. In International conference on machine learning, pp. 1842–1850, 2016. Shalev-Shwartz, S. et al. Online learning and online con- vex optimization. Foundations and Trends® in Machine Learning, 4(2):107–194, 2012. Shankar, S., Piratla, V ., Chakrabarti, S., Chaudhuri, S., Jyothi, P., and Sarawagi, S. Generalizing across domains via cross-gradient training. arXiv preprint arXiv:1804.10745, 2018. Shankar, V ., Dave, A., Roelofs, R., Ramanan, D., Recht, B., and Schmidt, L. Do image classiﬁers generalize across time? arXiv, 2019. Shocher, A., Cohen, N., and Irani, M. zero-shot super- resolution using deep internal learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3118–3126, 2018. Sinha, A., Namkoong, H., and Duchi, J. Certifying some dis- tributional robustness with principled adversarial training. arXiv preprint arXiv:1710.10571, 2017. Snell, J., Swersky, K., and Zemel, R. Prototypical networks for few-shot learning. In Advances in Neural Information Processing Systems, pp. 4077–4087, 2017. Song, Y ., Kim, T., Nowozin, S., Ermon, S., and Kushman, N. Pixeldefend: Leveraging generative models to understand and defend against adversarial examples. arXiv preprint arXiv:1710.10766, 2017. Su, J.-C., Maji, S., and Hariharan, B. Boosting supervi- sion with self-supervision for few-shot learning. arXiv preprint arXiv:1906.07079, 2019. Sun, Y ., Tzeng, E., Darrell, T., and Efros, A. A. Unsuper- vised domain adaptation through self-supervision. arXiv preprint, 2019.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Tripuraneni, N. and Mackey, L. Debiasing linear prediction. arXiv preprint arXiv:1908.02341, 2019. Tsipras, D., Santurkar, S., Engstrom, L., Turner, A., and Madry, A. Robustness may be at odds with accuracy. arXiv preprint arXiv:1805.12152, 2018. Tzeng, E., Hoffman, J., Saenko, K., and Darrell, T. Adver- sarial discriminative domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7167–7176, 2017. Vapnik, V .The nature of statistical learning theory. Springer science & business media, 2013. Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al. Matching networks for one shot learning. In Advances in neural information processing systems, pp. 3630–3638, 2016. Viola, P., Jones, M., et al. Rapid object detection using a boosted cascade of simple features. CVPR (1), 1(511- 518):3, 2001. Wong, E. and Kolter, J. Z. Provable defenses against adver- sarial examples via the convex outer adversarial polytope. arXiv preprint arXiv:1711.00851, 2017. Zhang, H., Yu, Y ., Jiao, J., Xing, E. P., Ghaoui, L. E., and Jor- dan, M. I. Theoretically principled trade-off between ro- bustness and accuracy. arXiv preprint arXiv:1901.08573, 2019. Zhang, R., Isola, P., and Efros, A. A. Colorful image col- orization. In European conference on computer vision, pp. 649–666. Springer, 2016. Zhu, W., Huang, Y ., Vannan, M. A., Liu, S., Xu, D., Fan, W., Qian, Z., and Xie, X. Neural multi-scale self-supervised registration for echocardiogram dense tracking. arXiv preprint arXiv:1906.07357, 2019.Appendix: Test-Time Training with Self-Supervision for Generalization under Distribution Shifts A1. Informal Discussion on Our Variable Decision Boundary In the introduction, we claim that in traditional supervised learning θgives a ﬁxed decision boundary, while ourθgives a variable decision boundary. Here we informally discuss this claim. Denote the input space Xand output space Y. A decision boundary is simply a mapping f : X →Y. Let Θ be a model class e.g Rd. Now consider a family of parametrized functions gθ : X→Y , where θ∈Θ. In the context of deep learning, gis the neural network architecture and θcontains the parameters. We say that f is a ﬁxed decision boundary w.r.t. g and Θ if there exists θ ∈Θ s.t. f(x) = gθ(x) for every x ∈X , and a variable decision boundary if for every x∈X, there exists θ∈Θ s.t. f(x) = gθ(x). Note how selection of θcan depend on xfor a variable decision boundary, and cannot for a ﬁxed one. It is then trivial to verify that our claim is true under those deﬁnitions. A critical reader might say that with an arbitrarily large model class, can’t every decision boundary be ﬁxed? Yes, but this is not the end of the story. Let d = dim( X) × dim(Y), and consider the enormous model class Θ′= Rd which is capable of representing all possible mappings be- tween Xand Y. Let g′ θ′ simply be the mapping represented by θ′ ∈Θ′. A variable decision boundary w.r.t. g and Θ then indeed must be a ﬁxed decision boundary w.r.t. g′and Θ′, but we would like to note two things. First, without any prior knowledge, generalization in Θ′is impossible with any ﬁnite amount of training data; reasoning about g′and Θ′is most likely not productive from an algorithmic point of view, and the concept of a variable decision boundary is to avoid such reasoning. Second, selecting θbased on xfor a variable decision boundary can be thought of as “training” on all points x ∈Rd; however, “training” only happens when necessary, for the xthat it actually encounters. Altogether, the concept of a variable decision boundary is different from what can be described by traditional learning theory. A formal discussion is beyond the scope of this paper and might be of interest to future work. A2. Computational Aspects of Our Method At test time, our method is 2 × batch size × number of iterations times slower than regular test- ing, which only performs a single forward pass for each sample. As the ﬁrst work on Test-Time Training, this paper is not as concerned about computational efﬁciency as improving robustness, but here we provide two poten- tial solutions that might be useful, but have not been thor- oughly veriﬁed. The ﬁrst is to use the thresholding trick on ls, introduced as a solution for the small batches prob- lem in the method section. For the models considered in our experiments, roughly 80% of the test instances fall below the threshold, so Test-Time Training can only be performed on the other 20% without much effect on per- formance, because those 20% contain most of the sam- ples with wrong predictions. The second is to reduce the number of iterations of test-time updates. For the online version, the number of iterations is al- ready 1, so there is nothing to do. For the standard ver- sion, we have done some preliminary experiments setting number of iterations to 1 (instead of 10) and learn- ing rate to 0.01 (instead of 0.001), and observing results almost as good as the standard hyper-parameter setting. A more in depth discussion on efﬁciency is left for future works, which might, during training, explicitly make the model amenable to fast updates. A3. Proofs Here we prove the theoretical results in the main paper. A3.1. The Toy Problem The following setting applies to the two lemmas; this is simply the setting of our toy problem, reproduced here for ease of reference.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Consider a two layer linear network parametrized by A∈ Rh×d (shared) and v,w ∈Rh (ﬁxed) for the two heads, respectively. Denote x∈Rd the input and y1,y2 ∈R the labels for the two tasks, respectively. For the main task loss lm(A; v) = 1 2 ( y1 −v⊤Ax )2 , (12) and the self-supervised task loss ls(A; w) = 1 2 ( y2 −w⊤Ax )2 , (13) Test-Time Training yields an updated matrix A′←A−η ( y2 −w⊤Ax )( −wx⊤) , (14) where ηis the learning rate. Lemma 1. Following the exposition of the main paper, let η∗= (y1 −v⊤Ax) (y2 −w⊤Ax)v⊤wx⊤x. (15) Assume η∗∈[ϵ,∞) for some ϵ> 0. Then for any η∈(0,ϵ], we are guaranteed an improvement on the main loss i.e. lm(A′) <lm(A). Proof. From the exposition of the main paper, we know that lm(A−η∗∇lsA)) = 0, which can also be derived from simple algebra. Then by convexity, we have lm(A−η∇ls(A)) (16) = lm (( 1 − η η∗ ) A+ η η∗(A−η∗∇ls(A)) ) (17) ≤ ( 1 − η η∗ ) lm(A) + 0 (18) ≤ ( 1 −η ϵ ) lm(A) (19) <lm(A), (20) where the last inequality uses the assumption that lm(A) > 0, which holds because η∗>0. Lemma 2. Deﬁne ⟨U,V⟩= vec (U)⊤vec (V) i.e. the Frobenious inner product, then sign (η∗) = sign (⟨∇lm(A),∇ls(A)⟩) . (21) Proof. By simple algebra, ⟨∇lm(A),∇ls(A)⟩ = ⟨ ( y1 −v⊤Ax )( −vx⊤) , ( y2 −w⊤Ax )( −wx⊤) ⟩ = ( y1 −v⊤Ax )( y2 −w⊤Ax ) Tr ( xv⊤wx⊤) = ( y1 −v⊤Ax )( y2 −w⊤Ax ) v⊤wx⊤x, which has the same sign as η∗. A3.2. Proof of Theorem 1 For any η, by smoothness and convexity, lm(x,y; θ(x)) = lm(x,y; θ−η∇ls(x; θ)) ≤lm(x,y; θ) + η⟨∇lm(x,y; θ),∇ls(x,θ)⟩ + η2β 2 ∥∇ls(x; θ)∥2 . Denote η∗= ⟨∇lm(x,y; θ),∇ls(x,θ)⟩ β∥∇ls(x; θ)∥2 . Then Equation 22 becomes lm(x,y; θ−η∗∇ls(x; θ)) (22) ≤lm(x,y; θ) −⟨∇lm(x,y; θ),∇ls(x,θ)⟩2 2β∥∇ls(x; θ)∥2 . (23) And by our assumptions on the gradient norm and gradient inner product, lm(x,y; θ) −lm(x,y; θ−η∗∇ls(x; θ)) ≥ ϵ2 2βG2 . (24) Because we cannot observe η∗in practice, we instead use a ﬁxed learning rate η = ϵ βG2 , as stated in Theorem 1. Now we argue that this ﬁxed learning rate still improves performance on the main task. By our assumptions, η∗ ≥ ϵ βG2 , so η ∈(0,η∗]. Denote g= ∇ls(x; θ), then by convexity of lm, lm(x,y; θ(x)) = lm(x,y; θ−ηg) (25) = lm ( x,y; ( 1 − η η∗ ) θ+ η η∗(θ−η∗g) ) (26) ≤ ( 1 − η η∗ ) lm(x,y; θ) + η η∗lm(x,y; θ−η∗g) (27) Combining with Equation 24, we have lm(x,y; θ(x)) ≤ ( 1 − η η∗ ) lm(x,y; θ) + η η∗ ( lm(x,y; θ) − ϵ2 2βG2 ) = lm(x,y; θ) − η η∗ ϵ2 2βG2 Since η/η∗>0, we have shown that lm(x,y; θ) −lm(x,y; θ(x)) >0. (28)Test-Time Training with Self-Supervision for Generalization under Distribution Shifts A4. Additional Results on the Common Corruptions Dataset For table aethetics, we use the following abbreviations: B for baseline, JT for joint training, TTT for Test-Time Train- ing standard version, and TTT-Online for online Test-Time Training i.e. the online version. We have abbreviated the names of the corruptions, in order: original test set, Gaussian noise, shot noise, impulse noise, defocus blur, glass blue, motion blur, zoom blur, snow, frost, fog, brightness, contrast, elastic transformation, pixelation, and JPEG compression. A4.1. Results Using Batch Normalization As discussed in the results section, Batch Normalization (BN) is ineffective for small batches, which are the inputs for Test-Time Training (both standard and online version) since there is only one sample available when forming each batch; therefore, our main results are based on a ResNet using Group Normalization (GN). Figure A2 and Table A1 show results of our method on CIFAR-10-C level 5, with a ResNet using Batch Normalization (BN). These results are only meant to be a point of reference for the curious readers. In the early stage of this project, we have experimented with two potential solutions to the small batches problem with BN. The naive solution is to ﬁx the BN layers during Test-Time Training. but this diminishes the performance gains since there are fewer shared parameters. The better solution, adopted for the results below, is hard example mining: instead of updating on all inputs, we only update on inputs that incur large self-supervised task loss ls, where the large improvements might counter the negative effects of inaccurate statistics. Test-Time Training (standard version) is still very effective with BN. In fact, some of the improvements are quite dra- matic, such as on contrast (34%), defocus blue (18%) and Gaussian noise (22% comparing to joint-training, and 16% comparing to the baseline). Performance on the original distribution is still almost the same, and the original error with BN is in fact slightly lower than with GN, and takes half as many epochs to converge. We did not further experiment with BN because of two rea- sons: 1) The online version does not work with BN, because the problem with inaccurate batch statistics is exacerbated when training online for many (e.g. 10000) steps. 2) The baseline error for almost every corruption type is signiﬁ- cantly higher with BN than with GN. Although unrelated to the main idea of our paper, we make the interesting note that GN signiﬁcantly improves model robustness. A4.2. Additional Baseline: Adversarial Logit Pairing As discussed in the results section, Hendrycks & Dietterich (2019) point to Adversarial Logit Pairing (ALP) (Kannan et al., 2018) as an effective method for improving model robustness to corruptions and perturbations, even though it was designed to defend against adversarial attacks. We take ALP as an additional baseline on all benchmarks based on CIFAR-10 (using GN), following the training proce- dure in Kannan et al. (2018) and their recommended hyper- parameters. The implementation of the adversarial attack comes from the codebase of Ding et al. (2019). We did not run ALP on ImageNet because the two papers we reference for this method, Kannan et al. (2018) and Hendrycks & Di- etterich (2019), did not run on ImageNet or make any claim or recommendation. A4.3. Results on CIFAR-10-C and ImageNet-C, Level 5 Table A2 and Table A3 correspond to the bar plots in the results section. Two rows of Table A2 have been presented as Table 1 in the main text. A4.4. Results on CIFAR-10-C, Levels 1-4 The following bar plots and tables are on levels 1-4 of CIFAR-10-C. The original distribution is the same for all levels, so are our results on the original distribution. A4.5. Direct Comparison with Hendrycks et al. (2019a) The following comparison has been requested by an anony- mous reviewer for our ﬁnal version. Our joint training baseline is based on Hendrycks et al. (2019a), but also incor- porates some architectural changes (see below). We found these changes improved the robustness of our method, and felt that it was important to give the baseline the same ben- eﬁt. Note that our joint training baseline overall performs better than Hendrycks: Compare Table S2 to Figure 3 of Hendrycks et al. (2019a) (provided by the authors), our baseline has average error of 22.8% across all corruptions and levels, while their average error is 28.6%. Summary of architectural changes: 1) Group Normalization (GN) instead of Batch Normalization (BN). For complete- ness, the results with BN are provided in Table S1; c.f. GN results in Table S2 which signiﬁcantly improves robustness, with or without self-supervision. 2) We split after the sec- ond residual group, while they split after the third residual group right before the linear layer. This consistently gives about 0.5% - 1% improvement. 3) We use a ResNet-26, while they use a 40-2 Wide ResNet. But our baseline still performs better than their method even though our network is 4x smaller, due to the two tricks above.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Gaussian Noise  Shot Noise  Impulse Noise  Defocus Blur  Frosted Glass Blur Motion Blur  Zoom Blur  Snow  Frost  Fog Brightness  Contrast  Elastic  Pixelate  JPEG Figure A1.Sample images from the Common Corruptions Benchmark, taken from the original paper by Hendrycks & Dietterich (2019). originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 20 40 60Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT Figure A2.Test error (%) on CIFAR-10-C, level 5, ResNet-26 with Batch Normalization. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 7.9 63.9 58.8 64.3 46.3 54.6 41.6 45.9 31.9 44.0 37.5 13.0 69.2 33.8 61.4 31.7 JT 7.5 70.7 65.6 67.2 43.1 55.4 40.9 42.7 30.3 44.5 42.5 12.7 58.6 30.7 62.6 31.9 TTT 7.9 47.9 45.2 54.8 27.6 50.4 31.5 30.9 28.7 34.3 26.9 12.6 35.2 30.6 51.2 31.3 Table A1.Test error (%) on CIFAR-10-C, level 5, ResNet-26 with Batch Normalization. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 50.5 47.2 56.1 23.7 51.7 24.3 26.3 25.6 34.4 28.1 13.5 25.0 27.4 55.8 29.8 JT 8.1 49.4 45.3 53.4 24.2 48.5 24.8 26.4 25.0 32.5 27.5 12.6 25.3 24.0 51.6 28.7 TTT 7.9 45.6 41.8 50.0 21.8 46.1 23.0 23.9 23.9 30.0 25.1 12.2 23.9 22.6 47.2 27.2 TTT-Online 8.2 25.8 22.6 30.6 14.6 34.4 18.3 17.1 20.0 18.0 16.9 11.2 15.6 21.6 18.1 21.2 UDA-SS 9.0 28.2 26.5 20.8 15.6 43.7 24.5 23.8 25.0 24.9 17.2 12.7 11.6 22.1 20.3 22.6 ALP 16.5 22.7 22.9 28.3 25.0 25.6 27.4 23.1 25.2 27.2 64.8 21.7 73.6 23.0 20.2 18.9 Table A2.Test error (%) on CIFAR-10-C, level 5, ResNet-26. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 68.9 1.3 2.0 1.3 7.5 6.6 11.8 16.2 15.7 14.9 15.3 43.9 9.7 16.5 15.3 23.4 JT 69.1 2.1 3.1 2.1 8.7 6.7 12.3 16.0 15.3 15.8 17.0 45.3 11.0 18.4 19.7 22.9 TTT 69.0 3.1 4.5 3.5 10.1 6.8 13.5 18.5 17.1 17.9 20.0 47.0 14.4 20.9 22.8 25.3 TTT-Online 68.8 26.3 28.6 26.9 23.7 6.6 28.7 33.4 35.6 18.7 47.6 58.3 35.3 44.3 47.8 44.3 Table A3.Test accuracy (%) on ImageNet-C, level 5, ResNet-18.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40 50Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A3.Test error (%) on CIFAR-10-C, level 4. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 46.4 39.2 44.8 15.3 52.5 19.1 20.5 21.3 26.9 13.3 10.5 13.7 20.8 35.3 26.9 JT 8.1 45.0 38.3 42.2 16.4 50.2 20.7 20.5 21.1 25.4 14.1 10.0 14.7 19.0 33.2 25.1 TTT 7.9 41.5 35.4 39.8 15.0 47.8 19.1 18.4 20.1 24.0 13.5 10.0 14.1 17.7 29.4 24.5 TTT-Online 8.2 22.9 20.0 23.9 11.2 35.1 15.6 13.8 18.6 15.9 12.3 9.7 11.9 16.7 13.6 19.8 ALP 16.5 21.3 20.5 24.5 20.7 25.9 23.7 21.4 24.2 23.9 42.2 17.5 53.7 22.1 19.1 18.5 Table A4.Test error (%) on CIFAR-10-C, level 4, ResNet-26. originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A4.Test error (%) on CIFAR-10-C, level 3. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 42.2 35.1 30.7 12.2 41.7 18.6 17.5 19.0 25.3 10.8 9.7 11.6 15.3 21.7 24.6 JT 8.1 40.2 34.4 29.9 12.2 37.9 20.8 17.3 18.4 25.0 11.4 9.2 12.0 15.2 20.8 22.8 TTT 7.9 37.2 31.6 28.6 11.5 35.8 19.1 15.8 17.8 23.3 11.0 9.1 11.6 14.3 18.9 22.3 TTT-Online 8.2 21.3 17.7 17.9 9.0 23.4 15.3 12.5 16.4 15.8 10.9 9.0 10.7 12.8 12.2 18.7 ALP 16.5 20.0 19.3 20.5 19.2 21.2 24.0 20.5 20.9 24.2 30.1 16.6 39.6 20.9 17.8 18.0 Table A5.Test error (%) on CIFAR-10-C, level 3, ResNet-26.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A5.Test error (%) on CIFAR-10-C, level 2. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 31.7 22.6 24.3 9.9 42.6 14.9 14.7 21.7 18.4 9.8 9.1 10.0 13.1 17.1 22.4 JT 8.1 31.0 22.6 23.4 9.1 39.2 16.4 14.2 21.2 17.5 9.4 8.3 10.6 12.8 15.9 20.5 TTT 7.9 28.8 20.7 23.0 9.0 36.6 15.4 13.1 20.2 16.9 9.2 8.3 10.2 12.5 14.8 19.7 TTT-Online 8.2 16.8 13.8 15.5 8.5 23.4 13.3 11.5 16.8 12.7 9.4 8.4 9.7 12.4 11.5 17.0 ALP 16.5 18.0 17.2 19.0 17.8 20.7 21.2 19.3 19.0 20.1 22.4 16.3 29.2 20.3 17.4 17.8 Table A6.Test error (%) on CIFAR-10-C, level 2, ResNet-26. originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A6.Test error (%) on CIFAR-10-C, level 1. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 21.7 17.1 17.0 9.0 44.0 12.1 13.9 14.3 13.4 9.2 8.9 9.0 13.2 12.0 17.3 JT 8.1 20.4 16.6 16.9 8.2 40.5 12.2 13.0 13.1 12.3 8.4 8.1 8.5 12.9 11.3 15.9 TTT 7.9 19.1 15.8 16.5 8.0 37.9 11.7 12.2 12.8 11.9 8.2 8.0 8.3 12.6 11.1 15.5 TTT-Online 8.2 13.8 11.9 12.2 8.5 24.4 10.5 11.5 12.4 10.7 8.5 8.3 8.6 12.4 10.7 14.4 ALP 17.0 16.8 17.6 16.8 20.9 18.7 19.0 17.3 17.5 17.4 16.1 18.4 20.4 17.0 17.2 17.5 Table A7.Test error (%) on CIFAR-10-C, level 1, ResNet-26.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Figure A7.Sample Images from the VID-Robust dataset (Shankar et al., 2019) in the results section adapted to CIFAR-10. Each row shows eight sample images from one class. The seven classes shown are, in order: airplane, bird, car, dog, cat, horse, ship.",
      "meta_data": {
        "arxiv_id": "1909.13231v3",
        "authors": [
          "Yu Sun",
          "Xiaolong Wang",
          "Zhuang Liu",
          "John Miller",
          "Alexei A. Efros",
          "Moritz Hardt"
        ],
        "published_date": "2019-09-29T08:09:15Z",
        "pdf_url": "https://arxiv.org/pdf/1909.13231v3.pdf"
      }
    },
    {
      "title": "Evaluation of Test-Time Adaptation Under Computational Time Constraints",
      "abstract": "This paper proposes a novel online evaluation protocol for Test Time\nAdaptation (TTA) methods, which penalizes slower methods by providing them with\nfewer samples for adaptation. TTA methods leverage unlabeled data at test time\nto adapt to distribution shifts. Although many effective methods have been\nproposed, their impressive performance usually comes at the cost of\nsignificantly increased computation budgets. Current evaluation protocols\noverlook the effect of this extra computation cost, affecting their real-world\napplicability. To address this issue, we propose a more realistic evaluation\nprotocol for TTA methods, where data is received in an online fashion from a\nconstant-speed data stream, thereby accounting for the method's adaptation\nspeed. We apply our proposed protocol to benchmark several TTA methods on\nmultiple datasets and scenarios. Extensive experiments show that, when\naccounting for inference speed, simple and fast approaches can outperform more\nsophisticated but slower methods. For example, SHOT from 2020, outperforms the\nstate-of-the-art method SAR from 2023 in this setting. Our results reveal the\nimportance of developing practical TTA methods that are both accurate and\nefficient.",
      "full_text": "Evaluation of Test-Time Adaptation Under Computational Time Constraints Motasem Alfarra 1 2 Hani Itani 1 Alejandro Pardo 1 Shyma Alhuwaider 1 Merey Ramazanova 1 Juan C. P´erez 1 Zhipeng Cai 2 Matthias M¨uller 2 Bernard Ghanem 1 Abstract This paper proposes a novel online evaluation protocol for Test Time Adaptation (TTA) meth- ods, which penalizes slower methods by provid- ing them with fewer samples for adaptation. TTA methods leverage unlabeled data at test time to adapt to distribution shifts. Although many effec- tive methods have been proposed, their impressive performance usually comes at the cost of signif- icantly increased computation budgets. Current evaluation protocols overlook the effect of this extra computation cost, affecting their real-world applicability. To address this issue, we propose a more realistic evaluation protocol for TTA meth- ods, where data is received in an online fashion from a constant-speed data stream, thereby ac- counting for the method’s adaptation speed. We apply our proposed protocol to benchmark sev- eral TTA methods on multiple datasets and sce- narios. Extensive experiments show that, when accounting for inference speed, simple and fast approaches can outperform more sophisticated but slower methods. For example, SHOT from 2020, outperforms the state-of-the-art method SAR from 2023 in this setting. Our results re- veal the importance of developing practical TTA methods that are both accurate and efficient1. 1. Introduction In recent years, Deep Neural Networks (DNNs) have demon- strated remarkable success in various tasks (He et al., 2016) thanks to their ability to learn from large datasets (Deng et al., 2009). However, a significant limitation of DNNs is their poor performance when tested on out-of-distribution 1King Abdullah University of Science and Technol- ogy (KAUST), Thuwal, Saudi Arabia 2Intel Labs, Munich, Germany. Correspondence to: Motasem Alfarra <mo- tasem.alfarra@kaust.edu.sa>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). 1Code: github/MotasemAlfarra/Online-Test-Time-Adaptation Current Evaluation Realistic Evaluation40 45 50 55 60 65 70 75Error Rate (%)  AdaBN 17  AdaBN 17  SHOT 20  SHOT 20  TENT 21  TENT 21  SAR 23  SAR 23 Figure 1: The trend of average error rate using offline evaluation vs our proposed online evaluation. In the offline setup, TTA methods demonstrate progress across time with a decreasing average error rate, e.g. from 68.5% using AdaBN to 56.2% using SAR. We propose a realistic evaluation protocol that accounts for the adaptation speed of TTA methods. Under this protocol, fast methods ( e.g. AdaBN) are unaffected, while slower (but more recent and sophisticated) methods (e.g. SAR) are penalized. data, which violates the i.i.d. assumption that the training and testing data are from the same distribution (Hendrycks et al., 2021; Hendrycks & Dietterich, 2019; Kar et al., 2022). Such failure cases are concerning, since distribu- tion shifts are common in real-world applications, e.g., im- age corruptions (Hendrycks & Dietterich, 2019), chang- ing weather conditions (Sakaridis et al., 2021), or security breaches (Goodfellow et al., 2014). Test Time Adaptation (TTA) (Saenko et al., 2010; Sun et al., 2020; Liu et al., 2021) has demonstrated promising results for solving the above problem. TTA leverages the unlabeled data that arrives at test time by adapting the forward pass of pre-trained DNNs according to some proxy task (Liang et al., 2020; Lee et al., 2013). Though recent methods have made significant progress at improving accuracy under dis- tribution shifts (Wang et al., 2020; Niu et al., 2022; Gao et al., 2022), many of them incur high computational over- head. For instance, some methods require self-supervised fine-tuning on the data (Chen et al., 2022), while others perform a diffusion process per input (Gao et al., 2022). The computational overhead of TTA methods decreases 1 arXiv:2304.04795v2  [cs.LG]  23 May 2024Evaluation of Test-Time Adaptation Under Computational Time Constraints their inference speed, which is a critical property in many real-world applications that require the TTA method to pro- duce predictions at the speed of the stream itself. This property, however, is overlooked in the current evaluation protocols for TTA methods. In particular, these protocols assume a setting, which neglects how events constantly un- fold regardless of the model’s speed, causing the model to miss incoming samples when it is busy processing previous ones. For TTA methods that adapt using test data, missing samples has a direct effect on the method’s accuracy, as it will have fewer samples for adaptation. That is, the slower the TTA method, the fewer samples it can leverage for adapt- ing to the distribution shift. Thus, the current protocol for evaluating TTA methods is not suitable for assessing their efficacy in real-world deployment. In this work, we propose a novel realistic evaluation proto- col that factors in inference speed to assess the real-world applicability of TTA methods. Our evaluation protocol is in- spired by Online Learning (Cai et al., 2021; Shalev-Shwartz et al., 2012) and mimics real-world scenarios by exposing all TTA methods to a constant-speed stream of data. In this setting, the performance of slow TTA methods is in- trinsically penalized, as the time spent adapting to a sample may lead to dropped samples that could have been useful for adaptation. Specifically, our protocol dictates that if a method gslow is k times slower than the stream, then it may only use every kth sample for adaptation. In contrast, a method gfast that is as fast as the stream is allowed to adapt to every sample. Figure 1 shows the effect of evaluating several methods under our proposed protocol, where slower methods (e.g., SAR (Niu14 et al., 2023)) are penalized and faster but simpler methods become better alternatives (e.g., SHOT (Liang et al., 2020) and AdaBN (Li et al., 2016)). We apply our proposed evaluation protocol to benchmark several TTA methods on multiple datasets, and provide a fair assessment of their performance subject to the realistic consequences of slower inference speeds. Our experimental results highlight the importance of developing TTA methods that adapt to distribution shifts with minimal impact on inference speed. Our contributions are two-fold: 1. We propose a realistic evaluation protocol for TTA methods that penalizes slower methods by providing them with fewer samples for adaptation. Our approach is effective at assessing TTA methods’ efficacy in sce- narios where data arrives as a constant-speed stream. 2. Following our proposed protocol, we provide a com- prehensive experimental analysis of 15 TTA methods evaluated on 3 large-scale datasets under 3 different evaluation scenarios. These scenarios consider adap- tation to a single domain and continual adaptation to several domains. Our analysis shows that, when in- ference speed is accounted for, simple (but faster) ap- proaches can benefit from adapting to more data, and thus outperform more sophisticated (but slower) meth- ods. Figure 1 demonstrates this for four TTA methods. We hope our evaluation scheme inspires future TTA methods to consider inference speed as a critical di- mension that affects their real-world performance. 2. Related Work Test Time Adaptation. The Test Time Adaptation (TTA) setup relaxes the “i.i.d” assumption between the training and testing distributions (Sun et al., 2020; Boudiaf et al., 2022). This relaxation is usually attained through a lifelong learning scheme on all received unlabeled data (Chen et al., 2022; Gong et al.). Earlier approaches such as TTT (Sun et al., 2020) and TTT++ (Liu et al., 2021), among others (Torralba & Efros, 2011; Tzeng et al., 2017), include a self-supervised loss (Gidaris et al., 2018) during training, which can then provide an error signal during adaptation. Despite their effectiveness, such approaches assume having control over how the model is trained. Fully Test Time Adaptation. Fully TTA methods are a subtype of TTA method that adapts at test time by modify- ing the model’s parameters (Liang et al., 2020; Lee et al., 2013; Mirza et al., 2022b; Mancini et al., 2018; Kojima et al., 2022) or its input (Gao et al., 2022) by using the incoming unlabeled data. Fully TTA methods are practi- cal, as they avoid assumptions on the training phase of a given model (Wang et al., 2020; Gao et al., 2022; Iwasawa & Matsuo, 2021). The first of these approaches adjusts the statistics of the Batch Normalization (BN) layers (Mirza et al., 2022a; Schneider et al., 2020; Li et al., 2016). For example, BN-adaptation (Schneider et al., 2020) leverages the statistics of the source data as a prior and infers the statis- tics for every received sample. On the other hand, AdaBN (Li et al., 2016) discards the statistics of the source domain and uses the statistics computed on the target domain. In line with light TTA methods, LAME (Boudiaf et al., 2022) proposes to only adapt the model’s output by finding the latent assignments that optimize a manifold-regularized like- lihood of the data. In this work, we found that such efficient methods preserve their accuracy under our proposed eval- uation. While fully TTA methods have been studied in the context of adversarial domain shifts (Alfarra et al., 2022; Croce et al., 2022; P´erez et al., 2021), in this work we focus on the context of natural shifts such as realistic image cor- ruptions (Hendrycks & Dietterich, 2019; Kar et al., 2022). Another line of work aims at adapting to distribution shifts by minimizing entropy. For instance, SHOT (Liang et al., 2020) adapts the feature extractor to minimize the entropy of individual predictions; while maximizing the entropy of the predicted classes. TENT (Wang et al., 2020) updates the learnable parameters of the BN layers to minimize the 2Evaluation of Test-Time Adaptation Under Computational Time Constraints Adapted SampleNon-AdaptedSampleTTA method Current evaluation . . . . . . Realistic evaluation . . . . . . Model Figure 2: Inference under the current and realistic evaluation protocols. The current evaluation setting (left) assumes that the incoming batches of stream S can wait until the adaptation process of a TTA method g finishes. This assumption is untenable in a real-time deployment scenario. Our proposed realistic evaluation (right) simulates a more realistic scenario where S reveals data at a constant speed. In this setup, slower TTA methods will adapt to a smaller portion of the stream. The remaining part of the stream will be predicted without adaptation by employing the most recent adapted model. We refer to the most recent adapted model as fθt+1 , with t denoting the time when the last sample was adapted to by g. When g is still adapting to a sample, the incoming sample is fed to fθt+1 to produce predictions. entropy of predictions. EATA (Niu et al., 2022) combines TENT with an active selection of reliable and non-redundant samples from the target domain and an anti-forgetting loss (Kirkpatrick et al., 2017). Further, SAR (Niu14 et al., 2023) equips TENT with an active sampling scheme that filters samples with noisy gradients. Other works use data-augmentation at test time (Ashukha et al., 2020). For example, MEMO (Zhang et al., 2021) adapts model parameters to minimize the entropy over a sample and multiple augmentations of it. CoTTA (Wang et al., 2022) uses augmentations to generate reliable pseudo- labels and then peform distillation. Finally, DDA (Gao et al., 2022) proposes to leverage a diffusion model (Ho et al., 2020) to restore corrupted inputs back to the source data distribution. These methods require multiple forward passes through the network or a diffusion model, leading to slower inference speeds. 3. Methodology In this section, we present our proposed Realistic TTA evalu- ation protocol. We first describe the current TTA evaluation protocol and its limitations Then, we introduce our Realistic TTA evaluation protocol, which addresses the shortcomings of the offline protocol. 3.1. Current Protocol TTA considers the practical setup, in which trained models are deployed in a target domain that exhibits distribution shifts to which they must adapt. Let fθ : X → Ybe a clas- sifier, parameterized by θ, that predicts the label y ∈ Yfor a given input x ∈ X. Before test time, fθ is assumed to have been trained on the dataset Dtrain ⊂ X × Y. At test time, i.e. when executing TTA,fθ is presented with a stream of data S, sampled from X, with potentially multiple distribution shifts w.r.t. Dtrain. Under this setup, a TTA method is a function g(θ, x) that sequentially adapts the model’s param- eters θ and/or the input x to enhance the performance under distributions shifts. Currently, TTA methods are evaluated in an offline setting. Formally, the Current TTA evaluation protocol simulates the interaction between the stream S and the TTA method g, at each time step t ∈ {0, 1, . . . ,∞}, as follows: Curr.1 S reveals a sample xt. Curr.2 g adapts xt to ˆxt, θt to ˆθt, generates prediction ˆyt, and updates parameters θt+1 = αθt + (1 − α)ˆθt.2 Note that all existing TTA methods can be modeled using this framework. For example, TENT (Wang et al., 2020) adapts network parameters to minimize entropy with α = 0, while leaving inputs unchanged, i.e. ˆxt = xt and θt+1 = ˆθt. DDA (Gao et al., 2022) adapts inputs via a diffusion process while preserving network parameters with α = 1, i.e. ˆxt = ˆxt and θt+1 = θt. CoTTA (Wang et al., 2022) applies knowledge distillation, and updates network parameters with an exponential moving average, i.e. setting 0 < α <1. Shortcomings of the Current TTA protocol.In the current protocol, the performance of a TTA method g is measured by comparing the ground truth labels yt with the predic- tions after adaptation ˆyt. An evaluation based only on this measure implicitly assumes that the stream is not constant 2Note that some methods abstain from adapting either xt or θt. 3Evaluation of Test-Time Adaptation Under Computational Time Constraints speed, but rather waits for g to adapt to xt (Curr.2) before revealing the next batch xt+1 (Curr.1). Figure 2 provides an illustration of this situation. This assumption results in the offline protocol favoring slower TTA methods, as the method’s performance is agnostic to its inference speed. However, in practical applications where the test data ar- rives at a constant speed, the offline protocol is not suitable for assessing a method’s performance. Next, we propose a remedy for this shortcoming. 3.2. Realistic Online Evaluation Protocol We propose a realistic evaluation of TTA methods that explicitly considers the relation between the speed of the method and the speed at which the stream reveals new data. This setup is more realistic, as it intrinsically penalizes the performance of slower TTA methods: long times spent in adaptation result in fewer samples to adapt to. A crucial aspect of our realistic TTA protocol is accounting for the implications of simulating a constant speed data stream S. For instance, consider a stream S that reveals data at a constant rate r samples per second. If a method gfast adapts to samples at speed r, then gfast will be able to adapt to every sample. On the other hand, if gslow adapts to samples at a speed r/2, then gslow will skip every other sample. We formalize the notion of the relation between the speed of the stream and the speed of a method g as the “relative adaptation speed of g”. This quantity, denoted by C(g) ∈ N, is simply the integer ratio of the speed of S to the speed of g. For instance, in the previous example, C(gfast) = 1, meaning gfast adjusts as fast as S reveals data, while C(gslow) = 2 , indicating S reveals its second batch while gslow is still adapting to the first one. Without loss of generality, we assume that fθ runs in real- time, i.e. that its speed is equal to r, and thus C(fθ) = 1 . This assumption allows us to suppose that the samples that are not processed by g can be processed by fθ. Under this setup, we define our realistic protocol by introducing the relative adaptation speed C(g) into the offline protocol. In particular, we simulate g’s availability by conditionally performing the adaptation step (Curr.2), depending on C(g). In this manner,g is only permitted to adapt when its previous adaptation step has finished. Formally, the realistic TTA evaluation protocol simulates the interaction between the constant speed stream S and the TTA method g, at each time step t ∈ {0, 1, . . . ,∞}, as follows: RTTA 1 S reveals a sample xt. RTTA 2 If (t mod C(g)) = 0, then g adapts xt to ˆxt, θt to ˆθt, generates a prediction ˆyt, and updates pa- rameters via θt+1 ← αθt + (1 − α)ˆθt. Otherwise, fθt generates a prediction ˆyt. Table 1: Average C(g(xt)). We report the average relative adaptation speed C(g) for 5 TTA methods. The higher C(g) is, the smaller the portion of data to which g adapts is. Method AdaBN TENT TTAC-NQ MEMO DDA C(g) 1 3 12 54 810 Here, “mod” represents the modulo operation. The above protocol assesses the performance of TTA methods by fac- toring in their speed. As such, faster methods are granted more adaptation steps and, conversely, slower methods are granted fewer (see Figure 2). Note that explicitly modeling the relative adaptation speeds allows us to evaluate TTA methods under different adaptation speeds by setting C(g) to arbitrary values. For instance, note that our realistic proto- col recovers the original offline protocol by settingC(g) = 1 for all methods. Next, we explain the calculation of C(g) for our realistic protocol. Online computation of C(g). In practice, estimating the relative adaptation speed C(g) can be a noisy process. The noise in this estimation essentially comes from two factors: hardware and input dependence. Hardware-induced noise applies to all methods, while input dependence applies to methods like ETA (Niu et al., 2022) which, upon receiving an input, may optionally abstain from adapting to it. This noise means that C(g) potentially varies across iterations. Our protocol accounts for this variability by conducting an online computation of C(g) on each revealed input. That is, instead of using a fixed value of C(g) at each itera- tion t, our protocol rather uses C (g(xt)). Formally, if we let R (g(x)) denote the speed at which g processes x, then the relative adaptation speed of g at x is defined as C (g(xt)) = ⌈r/R(g(x))⌉, where the ceiling function ac- counts for the stream’s discrete-time nature. Note that since we assumed C(fθ) = 1, then R (fθ(x)) = r. We report the empirical behavior of this online computation of C (g(xt)) for various TTA methods in Table 1, and leave the rest of the methods and the computation details to the Appendix. Next, we leverage our Realistic TTA protocol to conduct a comprehensive empirical study of several TTA methods. 4. Experiments We follow prior art (Wang et al., 2020; Niu14 et al., 2023; Gao et al., 2022) and focus on the task of image classifica- tion. In all our experiments, we assume that fθ is a ResNet- 50-BN3 (He et al., 2016) trained on ImageNet (Deng et al., 2009) (pretrained weights obtained from torchvision). We further assume that the stream S reveals batches of size 3SAR demonstrated the superiority of using batch independent normalization layers under batch size of 1. We leave this ablation to the Appendix along with experiments on other architectures. 4Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 2: Episodic Error Rate on ImageNet-C. We report the error rate of different TTA methods on ImageNet-C benchmark under both the realistic and the current setup. A lower error rate indicates a better TTA method. The highlighted numbers indicate a better performance per method across setups. Episodic means the model will adapt to one corruption at a time. The model is reset back to the base model when moving to the next corruption. The current setup is merely the reproduction of every method. The first sub-table corresponds to methods that do not incur any or few extra computations, i.e. C(g) = 1. We show that methods generally perform worse in the realistic setup. The more computationally complex the TTA method is, the less data it will adapt to, and the worse is its performance. Noise Blur Weather DigitalMethod Realisticgauss. shot impul.defoc. glass motionzoom snow frost fog brigh. contr. elast. pixel. jpeg Avg. ∆ Source ✓ 97.8 97.1 98.1 82.1 90.2 85.2 77.5 83.1 76.7 75.6 41.1 94.6 83.0 79.4 68.4 82.0 - AdaBN ✓ 84.9 84.3 84.3 85.0 84.7 73.6 61.1 65.8 66.9 52.1 34.8 83.3 56.1 51.1 60.3 68.5 - LAME ✓ 98.3 97.6 98.6 82.4 90.9 86.1 78.1 84.5 77.5 77.3 41.4 94.8 84.8 80.0 68.9 82.7 - BN ✓ 84.6 83.9 83.8 80.1 80.2 71.7 60.4 65.4 65.2 51.6 34.6 76.3 54.4 49.7 59.2 66.7 - ✗ 73.4 70.2 73.0 76.6 75.5 59.8 53.8 54.2 63.4 44.7 35.5 79.3 46.9 43.2 49.7 59.9SHOT ✓ 73.6 69.0 71.1 74.6 74.8 60.0 52.9 54.1 61.3 44.1 34.1 77.8 46.8 43.1 49.2 59.1 (-0.8) ✗ 71.3 69.4 70.2 72.0 72.9 58.7 50.7 52.8 58.8 42.7 32.7 73.3 45.5 41.5 47.7 57.3TENT ✓ 75.7 78.3 75.2 76.3 77.3 64.6 55.6 57.3 61.4 45.9 33.5 77.1 50.1 44.2 51.4 61.6 (+4.3) ✗ 69.5 69.7 69.0 71.2 71.7 58.1 50.5 52.9 57.9 42.7 32.7 62.9 45.5 41.6 47.8 56.2SAR ✓ 79.4 78.5 78.1 79.9 79.3 67.5 56.1 60.5 63.1 47.4 34.0 75.3 51.7 46.6 53.8 63.4 (+7.2) ✗ 78.4 77.8 77.2 80.5 79.1 64.0 53.3 57.8 60.7 44.1 32.9 73.1 48.6 42.3 52.6 61.5CoTTA ✓ 82.9 81.6 81.9 87.4 85.6 75.6 61.1 63.1 64.9 49.9 34.8 91.2 54.0 48.8 56.6 68.0 (+6.5) ✗ 71.3 70.3 70.8 82.1 77.4 63.9 53.9 49.9 55.5 43.9 32.8 81.4 43.7 41.1 46.7 59.0TTAC-NQ ✓ 79.4 75.7 78.9 86.6 86.2 77.1 61.8 58.8 62.4 51.5 34.4 88.5 52.1 49.1 55.5 66.5 (+7.5) ✗ 65.5 62.4 63.5 66.6 67.2 52.0 47.3 48.2 54.1 39.9 32.1 55.0 42.3 39.2 44.8 52.0EATA ✓ 69.3 67.1 69.2 71.1 71.7 57.5 49.9 51.9 57.4 42.4 32.6 60.7 45.1 41.4 47.4 55.6 (+3.6) ✗ 92.5 91.3 91.0 84.0 87.0 79.3 72.4 74.6 71.3 67.9 39.0 89.0 76.2 67.0 62.4 76.3MEMO ✓ 97.7 97.0 98.0 82.1 90.1 85.1 77.4 83.0 76.6 75.4 41.0 94.5 82.9 79.2 68.2 81.9 (+5.6) ✗ 58.6 57.8 59.0 87.0 81.6 76.6 65.9 67.9 66.7 64.0 40.0 92.2 52.2 46.6 49.9 64.4DDA ✓ 97.8 97.0 98.1 82.1 90.2 85.2 77.5 83.1 76.7 75.6 41.1 94.6 83.0 79.4 68.3 82.0 (+17.6) 644, except for MEMO (Zhang et al., 2021), which pre- dicts on single images to incentivize prediction consistency over an input and its augmentations. Regarding datasets, we follow earlier works (Wang et al., 2020; Niu14 et al., 2023; Niu et al., 2022; Gao et al., 2022; Zhang et al., 2021), and thus evaluate on the ImageNet-C dataset (Hendrycks & Dietterich, 2019) with a corruption level of 5 for all 15 corruptions. We further extend our evaluation and consider CIFAR10-C, ImageNet-R (Hendrycks et al., 2021), and the more recent ImageNet-3DCC (Kar et al., 2022), which lever- ages depth estimates to construct more spatially-consistent corruptions. Our experiments compare the performance of the base- line model fθ (without test time adaptation) against 15 state-of-the-art TTA methods published in top-tier venues (e.g., CVPR, NeurIPS, and ICLR) between 2017 and 2023. In particular, we consider: BN (Schneider et al., 2020) and AdaBN (Li et al., 2016), which adjust the statistics of the batch normalization layers; SHOT (Liang et al., 2020) and SHOT-IM (Liang et al., 2020), which fine-tune the feature extractor to maximize mutual information; entropy mini- mization approaches such as TENT (Wang et al., 2020), 4This batch size is recommended by most baselines (Wang et al., 2020; Niu et al., 2022) ETA (Niu et al., 2022) (a more efficient version of TENT), and SAR (Niu14 et al., 2023), which trains the learnable parameters of the batch normalization layers; distillation approaches, such as CoTTA (Wang et al., 2022), Pseudo Labeling (PL) (Lee et al., 2013), and the very recent and efficient LAME (Boudiaf et al., 2022); EATA (Niu et al., 2022) and TTAC (Su et al., 2022) that assume access to the source training data; data-dependent approaches such as MEMO (Zhang et al., 2021) and the diffusion-based method DDA (Gao et al., 2022). For all methods, we use their official implementation with their recommended hyper- parameters. We report our experimental results on a subset of 12 baselines, while leaving ETA, SHOT-IM, and PL to the appendix due to space constraints and their similarity to SHOT and EATA. As mentioned in Section 3.2 , our protocol performs an online computation of the relative adaptation speed of g. In particular, for each batch revealed by the stream, we compute C (g(x)). Then, if C(g(xi)) = k, all the samples {xi+1, xi+2, . . . , xi+k} are processed by fθi without adap- tation. Otherwise, if C(g(xi)) = 1, then these samples are processed by g. For methods that accumulate parameter updates such as TENT (Wang et al., 2020), fθi is the most recent updated model g(fθi−1 ). We report all our main re- sults as the average across three seeds, and leave the detailed 5Evaluation of Test-Time Adaptation Under Computational Time Constraints SHOT TENT TTAC-NQ SAR EATA COTTA brigh.pixel.gauss.motionzoomglassimpul.jpegdefoc.elast.shotfrostsnowfog contr.clean 30 40 50 60 70 80 90 100Error Rate (%) (a) Current Continual TTA. brigh.pixel.gauss.motionzoomglassimpul.jpegdefoc.elast.shotfrostsnowfog contr.clean 30 40 50 60 70 80 90 100Error Rate (%)  (b) Realistic Continual TTA. Figure 3: Continual Error Rate on ImageNet-C. We report the continual error rate of several TTA methods on ImageNet-C benchmark under both realistic and current setups. A lower error rate indicates a better TTA method. Continual evaluation means the corruptions are presented in a sequence without resetting the model in between. We choose the same order as presented along the x-axis; starting with brightness and ending with clean validation set. In the current setup, we observe an increasing trend for SHOT, TENT, and TTAC-NQ. This is hypothesized to be due to overfitting on the early distribution shifts. This behavior is mitigated in the realistic setup due to adapting to fewer batches. EATA and SAR perform equally well in both realistic and current continual setups due to sample rejection. We report the standard deviation across 3 seeds. analysis to the Appendix. Throughout the experiments, we refer to our realistic evaluation protocol as “realistic/on- line”, and refer to the current protocol as “current/offline”. Next, we evaluate all methods on four different scenarios: (i) when domain shifts happen in an episodic manner, (ii) when domain shifts happen continually, i.e. one after the other, (iii) when the stream speed varies, (iii) when domain shifts happen continually with label correlation; practical evaluation (Yuan et al., 2023) ,and (v) when the baseline fθ is unavailable for evaluating the samples skipped by the TTA method g (left for the appendix). 4.1. Episodic Evaluation of TTA First, we consider an episodic evaluation of domain shifts, whereby S contains a single domain (e.g. one corruption) from ImageNet-C. We analyze this simple and most com- mon setup to assess the performance of TTA methods under real-time evaluation. We report the error rates on all corrup- tions in Table 2 and the average error rate across corruptions. We summarize the insights as follows: (i) The performance of TTA methods often degrades significantly under the realistic setup. Most methods induce a significant computational overhead, which prevents them from adapting to every sample from the test stream. For example, the error rate increases by 7.5% for TTAC- NQ and 4.3% for TENT, where C(gTTAC-NQ) = 12 and C(gTENT) = 3 (see Table 1). That is, TENT adapts to one- third of the batches revealed by the stream, while TTAC-NQ adapts to one every twelve batches. (ii) Very efficient methods, withC(g) = 1, such as LAME and BN, do not lose in performance. Evaluating such methods in offline or realistic setups is inconsequential, as their adaptation incurs negligible additional computation (since they adapt during the forward pass (Li et al., 2016; Schneider et al., 2020) or by adjusting the logits (Boudiaf et al., 2022) at a speed that pales in comparison to that of the stream). Interestingly, in our realistic evaluation, the simple BN (published in 2020) with an average error rate of 66.7% outperforms more recent and advanced methods such as SAR (published in 2023) by 1.7%. Furthermore, AdaBN (published in 2017) significantly outperforms the very recent diffusion-based DDA by a notable 13%. (iii) Data-dependent approaches, such as MEMO and DDA, are extremely inefficient. Despite the independence of MEMO and DDA on batch size, they incur a massive computational burden. For instance, C(gMEMO) = 54 and C(gDDA) = 810. Thus, both methods will be busy adapting for considerable portions of the stream, leaving most predic- tions to the non-adapted classifier. This phenomenon is the reason behind the reported performance of these methods being so close to that of fθ (i.e. around 82%). This result calls for future research to focus on increasing the efficiency of data-dependent adaptation methods. (iv) Sample rejection-oriented methods can perform well under the realistic protocol. EATA adapts efficiently due to its fast sample rejection algorithm, which relies solely on 6Evaluation of Test-Time Adaptation Under Computational Time Constraints the forward pass to admit samples for adaptation. EATA’s low error rate of 55.6%, combined with a small performance drop of less than 4%, positions it as the top performer under the realistic evaluation protocol on ImageNet-C. On the other hand, SAR does not benefit from sample rejection. SAR’s performance drop of 7.5% is due to its dependence on gradients for sample rejection, which reduces its speed. (v) SHOT benefits from the realistic protocol. Interest- ingly, we found that SHOT (and SHOT-IM in the Appendix), a fine-tuning-based approach, benefits from our realistic evaluation. In particular, we found that SHOT’s error rate decreases by 2% on fog corruption and by 0.8% on average. This observation could suggest that SHOT could potentially improve performance by disposing of fine-tuning on every batch. It is also worth mentioning that, under our realis- tic evaluation, SHOT (introduced in 2020) outperforms all methods except EATA. (vi) Performance changes are consistent across corrup- tions. Note that all methods that are somewhat efficient can improve the source model across all corruptions, in both the offline and realistic setups. Furthermore, the performance changes when comparing the offline and realistic setups are consistent across all corruptions. This finding suggests that the performance of these methods is independent of the do- main shift being considered. We further test this hypothesis by benchmarking these methods on two other datasets with other types of domain shifts in Section 4.4. 4.2. Continual Evaluation of TTA Next, we analyze the more challenging continual setup, fol- lowing (Wang et al., 2022; Niu et al., 2022). In particular, we construct the stream S by concatenating all corruptions from ImageNet-C. That is, we adapt TTA methods continu- ally on all corruptions followed by the clean validation set, without ever resetting the network weights. We introduce the notion of realistic adaptation to the continual setup to study the effects of a constant stream speed on the bench- mark. We report results in Figure 3 for both the offline and realistic protocols, where the horizontal-axis shows how cor- ruptions are ordered in the stream. We limit the experiments in this section to six TTA methods (SHOT, TENT, TTAC- NQ, COTTA, EATA, and SAR), and leave the remaining details for the Appendix. We observe: (i) Methods that do not perform sample rejection (SHOT, TENT, TTAC) scale poorly in the offline-continual setup. This phenomenon can be attributed to these methods over- fitting to early distributions. However, methods that do perform sample rejection (SAR and EATA) do not overfit as easily to corruptions, and can thus adapt to the rest of the stream. Even worse, such methods tend to even significantly degrade the performance on clean data. 1/16 1/8 1/4 1/2 1 η 52 55 58 61 64 67Error Rate (%) SHOT TENT TTAC-NQ SAR EATA Figure 4: Average Error Rate on ImageNet-C Under Slower Stream Speeds. We report the average error rate for several TTA methods on ImageNet-C under slower stream speeds. In our proposed realistic model evaluation, the stream speed r is normalized by the time needed for a for- ward pass using the base model. We evaluate different TTA methods under a stream with speed ηr with η ∈ (0, 1]. An η = 1/16 means the stream is 16 times slower than the forward pass of the base model. We report the standard deviation across 3 different random seeds. Different TTA methods degrade differently when varying η. (ii) In the realistic-continual setup, methods that do not perform sample rejection benefit from skipping adapta- tion on some batches, and become competitive with the methods that perform sample rejection. That is, while skipping parts of the stream deteriorated the performance of such methods in the episodic evaluation , this skipping actu- ally helped in preventing these methods from over-fitting in the continual setup. 4.3. Stream Speed Analysis In the previous experiments, we normalized the stream speed to be the same as that of fθ’s forward pass. That is, we assumed that the rate r at which S reveals new batches is equal to R (fθ(x)). However, some applications may enjoy a slower stream, giving TTA methods more time to adapt to samples. To explore this scenario, we vary the speed at which the stream reveals new data. In particular, let the new stream rate be η rwith η ∈ (0, 1]. Hence, as η → 0, the stream slows down and allows methods to adapt to all samples. Conversely, as η → 1, the stream speeds up, and at η = 1 we recover our realistic evaluation protocol. We experiment with the stream speed by setting η ∈ {1/16, 1/8, 1/4, 1/2, 1}, and evaluate five representative TTA methods (SHOT, TENT, TTAC-NQ, SAR, and EATA) in the episodic setup . Figure 4 summarizes our results by reporting the average error rate across all corruptions. We next list our observations: (i) The performance of TTA methods varies widely.For 7Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 3: Episodic Error Rate on ImageNet-C with ViT. We report the error rate of three baselines (Source, Tent, SAR) on the 15 different corruptions on ImageNet-C when the backbone is ViT architecture pretrained on ImageNet. We observe that while generally better backbones yield smaller error rate, expensive methods perform worse under our realistic evaluation. The more expensive the method is (e.g. SAR compared to Tent), the more performance reduction it suffers. Noise Blur Weather DigitalMethodRealisticgauss. shot impul. defoc. glass motionzoom snow frost fog brigh. contr. elast. pixel. jpeg Avg. ∆ Source ✓ 90.5 93.3 91.8 71.0 76.6 66.1 72.9 84.1 73.5 52.8 45.3 55.9 69.5 55.5 52.2 70.1 - ✗ 69.9 95.9 68.9 55.8 62.0 52.3 57.9 57.2 53.6 41.8 28.9 40.7 59.1 39.7 42.0 55.0Tent ✓ 80.7 88.9 81.0 63.0 69.5 58.3 64.9 65.8 59.7 47.7 33.2 47.3 64.6 45.1 46.4 61.1 (-6.1) ✗ 55.5 56.9 55.1 47.5 50.4 44.3 48.7 42.4 47.3 33.6 25.4 35.6 44.8 33.5 36.4 43.8SAR ✓ 70.0 72.5 69.4 56.6 63.4 54.0 60.0 56.4 53.5 43.0 30.5 43.3 58.7 41.5 43.8 54.5 (-10.7) example, TTAC-NQ starts degrading faster (at η = 1/16) due to its slow adaptation speed. For other methods, the η at which they degrade varies. For instance, while TENT has a higher error rate than SAR in slow streams (η ≤ 1/8), TENT outperforms SAR in the regime of faster streams η ≤ 1/4. Interestingly, SHOT (Liang et al., 2020) ranks the worst at η ≤ 1/8, then ranks second when η ≥ 1/2, becoming a viable alternative. At last, the order of different methods significantly changes depending on the speed of the stream. For example, SAR changes from being second best at η ≤ 1/8 to third at η = 1/4 and then to fifth ( i.e. second worst) at η ≥ 1/2. (ii) EATA provides a good trade-off between speed and performance. In fact, EATA gives the best overall perfor- mance (lowest error rate) independent of the stream’s speed. This virtue is attributable to EATA’s combination of good performance and adaptation speed based on efficient sample rejection. Results on other datasets are in the Appendix. 4.4. Results on Other Benchmarks and Architectures We extend our evaluation protocol to cover ImageNet- 3DCC (Kar et al., 2022) and ImageNet-R (Hendrycks et al., 2021) datasets and ResNet-18 (results in the ap- pendix) and ViT (Kolesnikov et al., 2021) architectures. ImageNet-R contains rendition versions of ImageNet span- ning 200 classes. ImageNet-3DCC constructs more spatially-consistent corruptions than ImageNet-C by lever- aging depth estimates. For ViT, we conduct episodic evalu- ation on ImageNet-C in a similar setup to Section 4.1 and report the results in Table 3 for the non-adapted model, Tent, and SAR. For ImageNet-R and ImageNet-3DCC, we fix the architecture to ResNet-50 and experiment on the entire datasets and set the severity level to 5 in ImageNet-3DCC. Due to the space constraint, we limit our experiments to the episodic evaluation, and leave other results and analyses to the Appendix. We evaluate the effectiveness of 10 TTA methods in Table 4, where we report the average error rate across all corruptions. We observe that our results are consistent across all con- Table 4: Average Error Rate on ImageNet-R and ImageNet-3DCC. We report the average error rate of dif- ferent TTA methods on ImageNet-R and ImageNet-3DCC under both the realistic and current setups. A lower error rate indicates a better TTA method. The highlighted num- bers indicate a better performance per method across setups. We observe that methods generally perform worse in the more realistic realistic setup. The conclusions are consistent with what we observed on ImageNet-C (Table 2). Method ImageNet-R ImageNet-3DCC Current Realistic ∆ Current Realistic ∆ Source 63.8 63.8 - 73.9 73.9 - AdaBN 60.6 60.6 0 72.1 72.1 0 BN 60.0 60.0 0 70.5 70.5 0 LAME 60.5 60.5 0 72.1 72.1 0 SHOT 70.3 62.6 (+7.7) 69.2 67.0 (+2.2) TENT 58.1 59.1 (-1.0) 64.5 66.8 (-2.3) SAR 57.5 59.6 (-2.1) 63.5 71.4 (-7.9) CoTTA 57.3 61.5 (-4.5) 66.4 75.6 (-9.2) EATA 55.7 57.1 (-1.4) 60.9 63.1 (-2.2) TTAC-NQ 59.2 60.8 (-1.6) 65.7 73.6 (-7.9) sidered datasets and architectures. Similar to our results in Table 2, the more computationally involved SAR de- grades more than Tent when leveraging ViT architecture. Regarding other datasets, we find that simple methods that adapt during the forward pass are unaffected by the realis- tic setup. All the other methods, except SHOT, experience degradation in their results on both datasets. We observe again that, on these two datasets, while SHOT actually ben- efits from the realistic evaluation, EATA remains the best alternative on both ImageNet-R and ImageNet-3DCC. 4.5. Evaluation under Practical TTA Recently, (Yuan et al., 2023) extended the continual test- time adaptation evaluation to include label-imbalances; known as Practical Test-Time Adaptation (PTTA) setup. In this setting, the stream not only reveals a continual se- quence of distribution shifts, but also the revealed batches 8Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 5: Episodic Error Rate on CIFAR10-C under Practical Evaluation (Yuan et al., 2023).We report the error rate of two baselines (Source, RoTTA (Yuan et al., 2023)) on the 15 different corruptions on CIFAR10-C when the backbone is ResNet-18. We observe that under our computational constrained evaluation, the only method tailored to this setting; RoTTA, performs worse than the non-adapted baseline. Noise Blur Weather DigitalMethodRealisticgauss. shot impul. defoc. glass motionzoom snow frost fog brigh. contr. elast. pixel. jpeg Avg. ∆ Source ✓ 72.3 65.7 72.9 46.9 54.3 34.8 42.0 25.1 41.3 26.0 9.3 46.7 26.6 58.5 30.3 43.5 - ✗ 36.9 34.9 45.8 16.6 44.2 19.9 16.53 21.6 22.4 18.8 9.8 20.6 28.4 27.1 34.5 26.5RoTTA ✓ 55.0 54.4 63.2 43.3 62.3 43.7 43.5 44.8 47.7 43.4 35.3 41.8 54.0 47.7 54.6 49.0 (-22.5) have significant label imbalances. To combat this combined challenge, the work of (Yuan et al., 2023) proposed to lever- age a balanced memory bank for adaptation. In this section, we extend our computational constrained evaluation to the PTTA setup and compare RoTTA (Yuan et al., 2023) with a non-adapted model on CIFAR10-C benchmark. Table 5 summarizes the results. We observe that while RoTTA indeed reduces the error rate under the PTTA setup on CIFAR10-C (17% below the non-adapted model), our realistic evaluation uncovers its computational limitation. We found that RoTTA’s error rate increases by over 22% surpassing the error rate of the non-adapted model. Note that RoTTA stores samples from the stream in a memory bank then adapts the model on sampled samples from the memory bank. Thus, the slower the adaptation of RoTTA, the less diverse the samples in the memory bank, hindering its adaptation. 4.6. Effect of Hyper-parameter Tuning The performance of different TTA methods heavily depends on their hyper-parameter settings (Zhao et al., 2023). Here, we assess the impact of our proposed evaluation on TTA methods when tuning their hyperparameters. For that regard, we conduct hyper parameter search for Tent (as a fundamen- tal baseline) and experiment with different learning rates (the only hyper-parameter for Tent). Table 6 summarizes the results under episodic evaluation for 4 different corruptions on ImageNet-C. We observe that while conducting hyper-parameter search indeed improves the performance of TENT, its error rate increases under our realistic evaluation across all hyperparameters. That is, while conducting hyper-parameter search might indeed result in a better performance for TTA methods, the insights obtained through our proposed evaluation scheme remains consistent: more efficient TTA methods will have a smaller performance drop under the realistic evaluation. 5. Conclusions In this work, we find that the performance of Test Time Adaptation (TTA) methods can vary depending on the con- Table 6: Effect of our evaluation under hyperparameter tuning. We report the error rate for Tent under different learning rates under both the current and our proposed real- istic evaluation. While carefully tuning the learning rate for Tent results in a better performance, our realistic evaluation causes a performance drop under all learning rates. lr Realisticgauss. motion fog pixel. Avg. ∆ ✗ 74.1 63.3 44.7 43.5 56.41×10−4 ✓ 79.7 69.0 47.8 46.8 60.8 (-4.4) ✗ 71.1 59.7 43.1 41.9 53.92×10−4 ✓ 77.6 66.1 46.0 45.0 58.7 (-4.7) ✗ 69.6 58.1 42.4 41.1 52.83×10−4 ✓ 74.9 64.0 45.0 44.0 57.0 (-4.2) ✗ 68.8 57.1 42.0 40.8 52.24×10−4 ✓ 73.7 62.3 44.5 43.2 55.9 (-3.7) text in which they are used. In the episodic evaluation, the efficiency of the method is the most important factor, with more efficient methods like AdaBN and BN showing consistent performance, while data-dependent approaches suffer. Sample rejection methods generally perform well, and fine-tuning approaches such as SHOT can even improve when adapting to fewer samples. In the continual evalua- tion, methods that do not perform sample rejection scale poorly in the offline-continual setup but benefit from skip- ping adaptation on some batches in the realistic-continual setup. Furthermore, our stream speed analysis shows that the performance of TTA methods can vary widely at differ- ent speeds. Our findings are consistent across corruptions and multiple datasets. They can help researchers and practi- tioners to better understand the strengths and weaknesses of different TTA methods, and to choose the most appropriate method for their specific use case. Acknowledgements This work was partially done during a research internship of the first author at Intel Labs. This work was supported by the King Abdullah University of Science and Technol- ogy (KAUST) Office of Sponsored Research (OSR) under Award No. OSR-CRG2021-4648. We would like to thank Yasir Ghunaim and Mattia Soldan for the helpful discussion. 9Evaluation of Test-Time Adaptation Under Computational Time Constraints Impact Statement Our work advances Machine Learning by proposing a re- alistic evaluation protocol for Test Time Adaptation meth- ods, prioritizing computational efficiency. This approach promotes the development of AI systems that are both ac- cessible in resource-limited settings and environmentally sustainable, by favoring simpler, faster methods. Such ad- vancements contribute to more inclusive and responsible AI deployment, aligning with ethical goals of broadening access and reducing environmental impacts References Alfarra, M., P´erez, J. C., Thabet, A., Bibi, A., Torr, P. H., and Ghanem, B. Combating adversaries with anti-adversaries. In Proceedings of the AAAI Conference on Artificial In- telligence, volume 36, pp. 5992–6000, 2022. Ashukha, A., Lyzhov, A., Molchanov, D., and Vetrov, D. Pitfalls of in-domain uncertainty estimation and ensem- bling in deep learning. arXiv preprint arXiv:2002.06470, 2020. Boudiaf, M., Mueller, R., Ben Ayed, I., and Bertinetto, L. Parameter-free online test-time adaptation. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8344–8353, 2022. Cai, Z., Sener, O., and Koltun, V . Online continual learning with natural distribution shifts: An empirical study with visual data. In Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision, pp. 8281–8290, 2021. Chen, D., Wang, D., Darrell, T., and Ebrahimi, S. Con- trastive test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 295–305, 2022. Croce, F., Gowal, S., Brunner, T., Shelhamer, E., Hein, M., and Cemgil, T. Evaluating the adversarial robustness of adaptive test-time defenses. In International Conference on Machine Learning, pp. 4421–4435. PMLR, 2022. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009. Gao, J., Zhang, J., Liu, X., Darrell, T., Shelhamer, E., and Wang, D. Back to the source: Diffusion-driven test-time adaptation. arXiv preprint arXiv:2207.03442, 2022. Gidaris, S., Singh, P., and Komodakis, N. Unsupervised rep- resentation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018. Gong, T., Jeong, J., Kim, T., Kim, Y ., Shin, J., and Lee, S.-J. Note: Robust continual test-time adaptation against temporal correlation. In Advances in Neural Information Processing Systems. Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain- ing and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions and pertur- bations. Proceedings of the International Conference on Learning Representations, 2019. Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M., Song, D., Steinhardt, J., and Gilmer, J. The many faces of robustness: A critical analysis of out-of-distribution generalization. ICCV, 2021. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion proba- bilistic models. Advances in Neural Information Process- ing Systems, 33:6840–6851, 2020. Iwasawa, Y . and Matsuo, Y . Test-time classifier adjustment module for model-agnostic domain generalization. Ad- vances in Neural Information Processing Systems , 34: 2427–2440, 2021. Kar, O. F., Yeo, T., Atanov, A., and Zamir, A. 3d common corruptions and data augmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18963–18974, 2022. Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Des- jardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017. Kojima, T., Matsuo, Y ., and Iwasawa, Y . Robustifying vision transformer without retraining from scratch by test- time class-conditional feature alignment. arXiv preprint arXiv:2206.13951, 2022. Kolesnikov, A., Dosovitskiy, A., Weissenborn, D., Heigold, G., Uszkoreit, J., Beyer, L., Minderer, M., Dehghani, M., Houlsby, N., Gelly, S., Unterthiner, T., and Zhai, X. An image is worth 16x16 words: Transformers for image recognition at scale. 2021. 10Evaluation of Test-Time Adaptation Under Computational Time Constraints Lee, D.-H. et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural net- works. In Workshop on challenges in representation learning, ICML, volume 3, pp. 896, 2013. Li, Y ., Wang, N., Shi, J., Liu, J., and Hou, X. Revisit- ing batch normalization for practical domain adaptation. arXiv preprint arXiv:1603.04779, 2016. Liang, J., Hu, D., and Feng, J. Do we really need to access the source data? source hypothesis transfer for unsuper- vised domain adaptation. In International Conference on Machine Learning, pp. 6028–6039. PMLR, 2020. Liu, Y ., Kothari, P., Van Delft, B., Bellot-Gurlet, B., Mordan, T., and Alahi, A. Ttt++: When does self-supervised test-time training fail or thrive? Advances in Neural Information Processing Systems, 34:21808–21820, 2021. Mancini, M., Karaoguz, H., Ricci, E., Jensfelt, P., and Ca- puto, B. Kitting in the wild through online domain adap- tation. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1103–1109. IEEE, 2018. Mirza, M. J., Micorek, J., Possegger, H., and Bischof, H. The norm must go on: dynamic unsupervised do- main adaptation by normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14765–14775, 2022a. Mirza, M. J., Soneira, P. J., Lin, W., Kozinski, M., Possegger, H., and Bischof, H. Actmad: Activation matching to align distributions for test-time-training, 2022b. URL https://arxiv.org/abs/2211.12870. Niu, S., Wu, J., Zhang, Y ., Chen, Y ., Zheng, S., Zhao, P., and Tan, M. Efficient test-time model adaptation with- out forgetting. In International conference on machine learning, pp. 16888–16905. PMLR, 2022. Niu14, S., Wu, J., Zhang, Y ., Wen, Z., Chen, Y ., Zhao, P., and Tan15, M. Towards stable test-time adaptation in dynamic wild world. International Conference on Learning Representations, 2023. P´erez, J. C., Alfarra, M., Jeanneret, G., Rueda, L., Thabet, A., Ghanem, B., and Arbel´aez, P. Enhancing adversarial robustness via test-time transformation ensembling. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 81–91, 2021. Saenko, K., Kulis, B., Fritz, M., and Darrell, T. Adapting visual category models to new domains. In Computer Vision–ECCV 2010: 11th European Conference on Com- puter Vision, Heraklion, Crete, Greece, September 5-11, 2010, Proceedings, Part IV 11 , pp. 213–226. Springer, 2010. Sakaridis, C., Dai, D., and Van Gool, L. Acdc: The ad- verse conditions dataset with correspondences for seman- tic driving scene understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10765–10775, 2021. Schneider, S., Rusak, E., Eck, L., Bringmann, O., Brendel, W., and Bethge, M. Improving robustness against com- mon corruptions by covariate shift adaptation. Advances in Neural Information Processing Systems, 2020. Shalev-Shwartz, S. et al. Online learning and online con- vex optimization. Foundations and Trends® in Machine Learning, 4(2):107–194, 2012. Su, Y ., Xu, X., and Jia, K. Revisiting realistic test-time training: Sequential inference and adaptation by anchored clustering. arXiv preprint arXiv:2206.02721, 2022. Sun, Y ., Wang, X., Liu, Z., Miller, J., Efros, A., and Hardt, M. Test-time training with self-supervision for generaliza- tion under distribution shifts. In International conference on machine learning, pp. 9229–9248. PMLR, 2020. Torralba, A. and Efros, A. A. Unbiased look at dataset bias. In CVPR 2011, pp. 1521–1528. IEEE, 2011. Tzeng, E., Hoffman, J., Saenko, K., and Darrell, T. Adver- sarial discriminative domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7167–7176, 2017. Wang, D., Shelhamer, E., Liu, S., Olshausen, B., and Darrell, T. Tent: Fully test-time adaptation by entropy minimiza- tion. arXiv preprint arXiv:2006.10726, 2020. Wang, Q., Fink, O., Van Gool, L., and Dai, D. Continual test- time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7201–7211, 2022. Yuan, L., Xie, B., and Li, S. Robust test-time adaptation in dynamic scenarios. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15922–15932, 2023. Zhang, M., Levine, S., and Finn, C. Memo: Test time ro- bustness via adaptation and augmentation. arXiv preprint arXiv:2110.09506, 2021. Zhao, H., Liu, Y ., Alahi, A., and Lin, T. On pitfalls of test- time adaptation. International Conference on MAchine Learning, 2023. 11Evaluation of Test-Time Adaptation Under Computational Time Constraints A. Methodology A.1. Online Computation of C(g) Section 3.2 discussed the online evaluation protocol of TTA methods. Here, we give more details on the calcu- lation of C(g), the relative adaptation speed of g, during our online evaluation. First, we set R (g(x)) as the time recording function for g to perform a forward pass for a single batch. To ensure a reliable time calculation, we exe- cute torch.cuda.synchronize() before starting the timer and before ending it. This ensures all GPU operations are finished for the moment time is computed. To alleviate hardware dependence, we also calculate R(fθ(x)) for each evaluation step computing the relative adaptation complex- ity. It is worth mentioning that C(g) for SHOT, EATA, SAR, and COTTA are[3, 3, 8, 103] on average, respectively. B. Experiments B.1. Episodic Evaluation of TTA SHOT, PL, and ETA For completeness, we report the results on 3 baselines: Pseudo Label (Lee et al., 2013), SHOT-IM (Liang et al., 2020), and ETA (Niu et al., 2022) in Table 7. We follow the same setup as in the main paper. Our results are consistent with the findings of Section 4.1 and Table 2. In particular, SHOT-IM improves its perfor- mance under the online evaluation, similar to SHOT. Further, the performance of ETA and PL degrades under the online evaluation due to the additional computational burden. Nev- ertheless, ETA is similar to EATA in providing the best tradeoff between additional computational requirements and performance improvements. SAR with GN We equip our results to include ResNet50 with Group Normalization (GN) layers, following (Niu14 Figure 5: C(g) computation across iterations. We report our online calculations for the relative adaptation speed ofg, C(g), for SAR, SHOT, EATA, and TENT throughout a full evaluation episode. We observe that, overall, C(g) has a stable behavior throughout evaluation iterations. et al., 2023). We report the results in Table 7, where we observe that: (i) Under a relatively large batch size (64), ResNet50 with GN underperforms ResNet50 with Batch Normalization. In fact, the average error rate for SAR in- creases from 56.2% to 65.8%. (ii) The online evaluation penalizes SAR in both architecture choices with a perfor- mance degradation of 3.6% under the GN-based ResNet. Finally, it is worth mentioning that SAR with GN layers attains a similar performance under a batch size of 1. Ablating Batch Sizes In the experiments section, we fixed the batch size to 64 following the recommendations of ear- lier works (Wang et al., 2020; Niu et al., 2022). Here, we investigate the effect of our proposed online evaluation un- der different choices of batch sizes. To that end, we vary the batch size in {1, 16, 32, 128}, and report the results in Figure 6. We draw the following observations: Table 7: Episodic Error Rate on ImageNet-C. We report the error rate of different TTA methods on the ImageNet-C benchmark under both the online and offline setups. A lower error rate indicates a better TTA method. The highlighted numbers indicate a better performance per method across setups. Episodic means the model will adapt to one corruption at a time. The model is reset back to the base model when moving to the next corruption. The offline setup is merely the reproduction of every method. We show that methods generally perform worse in the more realistic online setup. The more computationally complex the TTA method is, the less data it will adapt to, and the worse its performance. SAR-GN represents SAR when deployed on ResNet50 with Group Normalization (GN) layers, following (Niu14 et al., 2023). Noise Blur Weather DigitalMethod Online gauss. shot impul. defoc. glass motionzoom snow frost fog brigh. contr. elast. pixel. jpeg Avg. ∆ ✗ 73.1 69.8 72.0 76.9 75.9 58.5 52.7 53.3 62.2 43.8 34.6 82.6 46.0 42.3 48.9 59.5SHOT-IM ✓ 71.1 68.6 70.7 73.2 73.6 59.1 51.9 52.8 60.5 43.7 33.6 77.3 45.7 42.1 48.6 58.2 (-0.3) ✗ 92.2 92.2 92.8 97.0 89.8 57.7 49.6 50.7 57.1 41.5 32.6 91.1 44.3 40.3 46.6 65.0PL ✓ 90.6 86.3 83.6 93.2 89.7 63.0 51.7 55.0 59.3 43.8 32.9 92.3 47.3 42.4 49.3 65.3 (+0.3) ✗ 64.9 62.7 63.6 66.4 66.3 52.4 47.3 48.2 54.1 40.2 32.2 54.8 42.3 39.2 44.7 52.0ETA ✓ 70.2 67.0 69.6 71.5 71.5 56.9 50.2 51.9 57.0 42.0 32.5 60.5 44.6 40.8 47.1 55.6 (+3.6) ✗ 71.8 69.0 70.3 81.5 81.0 69.6 69.5 57.1 56.6 94.3 29.2 56.0 84.8 51.4 44.7 65.8SAR-GN ✓ 82.0 80.2 82.1 80.2 88.6 78.5 75.1 59.6 53.9 66.9 30.7 63.3 81.3 71.3 47.5 69.4 (+3.6) 12Evaluation of Test-Time Adaptation Under Computational Time Constraints 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) ADABN OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100  BN-ADAPTATION OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 COTTA OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) EATA OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 ETA OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100  LAME OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) PL OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 SAR OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 SHOT OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) SHOTIM OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 TENT OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 TTAC-NQ OFFLINE ONLINE Figure 6: Batch Size Analysis current vs. realistic setups for every method. We assess the performance variation of 12 different TTA methods under varying batch sizes. We experiment with batch sizes in{1, 16, 32, 128}. We do not include the baseline, MEMO, and DDA, since they are data-dependent approaches and are unaffected by batch size. All TTA methods, except LAME, are severely affected by smaller batch sizes. Nonetheless, the realistic evaluation degrades the performance of all methods, except SHOT and SHOT-IM. (i) Online evaluation improves the performance of SHOT and SHOT-IM. This result is consistent with the earlier observations in Table 2. Note that PL shares a similar trend as well. (ii) The performance of TTA methods degrades when switching from offline to online evaluation, regardless of the batch size. This result is highlighted in COTTA, ETA, EATA, SAR, TENT, and TTAC-NQ. (iii) Performance of TTA methods vastly varies when varying the batch size. This result is consistent with earlier findings in the literature (Gao et al., 2022; Niu14 et al., 2023), where most TTA methods fail with small batch sizes. At last, and to ease comparison across methods, we summa- rize all the plots for all methods in Figure 7. Consistency with 3 random seeds. For all of our exper- iments, we run each experiment with 3 random seeds. In most of our results, we found out that the standard deviation of performance across runs is very small. Our results in Figures 3 and 4 demonstrate this variation in the shaded area for 5 different TTA methods. B.2. Continual Evaluation of TTA We further explore another setup for the continual evalua- tion of TTA. In particular, we follow (Wang et al., 2022) in concatenating all corruptions in ImageNet-C with 11 differ- ent orders. We then report the average performance of each method across all runs and corruptions in Table 8. We run each experiment with 3 random seeds, and report our results with standard deviations. For the remaining implementation 13Evaluation of Test-Time Adaptation Under Computational Time Constraints 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) OFFLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 ONLINE ADABN BN-ADAPTATION COTTA EATA ETA LAME PL SAR SHOT SHOTIM TENT TTAC-NQ Figure 7: Summary of batch size analysis: current vs. realistic setups. Left: Current evaluation, i.e.,Section 3.1. Right: Realistic evaluation,i.e.,Section 3.2. While EATA achieves the lowest error rate under batch sizes≥ 32, SHOT becomes a very competitive baseline, outperforming EATA, at a batch size of 128. Table 8: Continual Error Rate on ImageNet-C. We report the average continual error rate for 11 different corruption orders, with 3 different seeds, under both the offline and online setups with a corruption severity level of 5. Continual refers to continually adapting after each corruption without resetting. This metric indicates the model’s capability to learn from previous corruptions. The offline setup refers to the performance of the model in a continual learning scheme, whereas the online setup refers to the performance of the model in a continual learning scheme, under our more realistic online setup. We show that the more complex a method is, the fewer samples it adapts to, achieving better performance in a continual learning scheme. Avg. Error (%) COTTA ETA TENT SAR EATA SHOT TTAC-NQ Offline 65.3 ± 5.9 56 .4 ± 2.3 84 .6 ± 16.0 59 .8 ± 3.0 56 .4 ± 2.3 88 .4 ± 11.4 81 .8 ± 11.4 Online 69.3 ± 2.8 57 .7 ± 2.0 65 .6 ± 5.0 60 .4 ± 1.8 57 .7 ± 1.9 78 .2 ± 7.7 65 .1 ± 3.8 details, we follow our setup in main paper. We observe that, similar to our conclusions in Section 4.2, online eval- uation helps methods that do not perform sample rejection (e.g.,TENT). Nonetheless, both ETA and EATA provide the best trade-off between performance and additional compu- tational burden. B.3. Stream Speed Analysis For completeness, we extend our stream speed analysis in Section 4.3 to cover the ImageNet-3DCC dataset. We preserve our experimental setup by varying the stream speed according to ηr, with η ∈ {1/16, 1/8, 1/4, 1/2, 1. Figure 8 summarizes our results for SHOT, TENT, TTAC-NQ, EATA, and SAR. We observe similar trends to the ones in Figure 4, where the performance of different TTA methods varies widely under different stream speeds. The large relative adaptation speed of TTAC-NQ degrades its performance under even slow streams (e.g.,η = 1/8), while SHOT reduces its error rate under faster streams. Furthermore, EATA is consistently outperforming all other considered approaches under different stream speeds. B.4. Evaluation on Other Benchmarks We report the error rates on all corruptions of ImageNet- 3DCC (Kar et al., 2022), along with the overall average error rate, in Table 9. The conclusions we draw for ImageNet- 3DCC (Kar et al., 2022) are very similar to the ones ob- served on ImageNet-C (Hendrycks & Dietterich, 2019) (in Section 4.1). We observe that efficient methods, with C(g) = 1, such as LAME and BN, maintain performance. Furthermore, the performance of some TTA methods (Wang et al., 2020; Niu14 et al., 2023; Niu et al., 2022; Wang et al., 2022) degrades in the online setup, while others that use pseudo labeling (Lee et al., 2013; Liang et al., 2020) actually improve. This degradation seems to be directly proportional to the amount of data a method misses according to its C(g). 14Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 9: Episodic Error Rate on ImageNet-3DCommonCorruptions. We report the error rate of different TTA methods on ImageNet-3DCC (Kar et al., 2022) benchmark under both the realistic and offline setups. A lower error rate indicates a better TTA method. The highlighted numbers indicate a better performance per method across setups. Episodic means the model will adapt to one corruption at a time. The model is reset back to the base model when moving to the next corruption. The offline setup corresponds to reproducing the reported performance of every method. The first sub-table corresponds to methods that incur none or few additional computations, i.e.,C(g) = 1. We show that methods generally perform worse in the more realistic setup. The more computationally complex the TTA method is, the fewer data it will adapt to, and the worse its performance. Depth of field Noise LightingWeather Video Camera motionMethod RealisticNear focus Far focusColor quant. ISO noise Low lightFlash Fog 3DBit error H.265 ABR H.265 CRFXY-mot. blur Z-mot. blurAvg. ∆ Source ✓ 46.9 55.6 82.5 94.0 71.7 78.7 75.3 88.6 70.6 65.4 82.0 75.3 73.9 -AdaBN ✓ 45.2 55.0 71.8 76.8 64.1 80.8 75.0 91.8 80.9 76.7 79.1 67.5 72.1 -LAME ✓ 45.3 55.0 71.9 76.9 64.1 80.8 75.1 91.8 80.9 76.8 79.2 67.6 72.1 -BN ✓ 43.9 54.3 72.3 76.6 60.9 80.1 72.4 90.9 78.7 73.8 76.9 65.6 70.5 - PL ✗ 39.8 49.8 65.5 72.6 48.9 79.0 66.1 97.5 92.1 86.2 88.7 57.6 70.3(-1.6)✓ 41.0 51.3 66.5 71.5 52.8 77.4 68.1 95.6 86.0 78.7 77.0 59.2 68.7 SHOT ✗ 43.0 53.6 67.1 64.2 51.9 81.1 73.2 97.2 83.5 77.8 77.3 60.1 69.2(-2.2)✓ 41.7 51.4 64.4 63.8 51.6 77.5 71.6 95.1 79.9 74.6 73.7 58.5 67.0 SHOT-IM✗ 42.2 52.7 66.6 63.7 51.0 81.0 72.1 97.0 83.3 77.6 75.6 59.2 68.5(-1.9)✓ 41.2 51.2 64.4 63.3 51.3 77.5 70.9 94.9 79.4 74.1 72.3 58.3 66.6 TENT ✗ 39.9 49.6 62.4 62.2 50.7 75.6 68.5 91.6 75.7 70.2 70.4 57.0 64.5(+2.3)✓ 41.7 51.4 65.5 67.2 54.7 77.4 70.1 90.7 76.8 71.9 74.0 60.8 66.8 SAR ✗ 40.3 50.0 62.0 61.2 50.6 73.8 65.8 90.1 73.9 68.8 69.1 56.8 63.5(+6.9)✓ 44.9 54.7 71.1 75.4 62.6 80.3 73.8 91.7 80.5 76.1 78.6 66.9 71.4 ETA ✗ 38.7 47.9 59.1 56.7 46.8 71.0 62.1 90.6 72.8 67.3 64.7 52.9 60.9(+2.3)✓ 39.7 49.3 61.6 60.7 50.0 73.5 65.2 90.3 74.4 69.1 68.8 55.9 63.2 CoTTA ✗ 40.8 50.9 66.3 68.3 54.6 77.2 68.0 90.2 76.4 71.1 73.1 60.4 66.4(+9.2)✓ 55.4 63.1 74.1 77.0 64.7 83.4 78.1 93.7 84.0 80.3 81.7 71.9 75.6 TTAC-NQ✗ 40.7 50.5 61.0 61.1 51.5 72.8 66.6 93.8 81.1 74.7 75.7 59.1 65.7(+7.9)✓ 49.9 57.0 69.3 72.3 58.9 79.8 76.3 95.8 86.5 83.0 84.6 69.8 73.6 EATA ✗ 38.6 47.8 59.2 56.6 46.9 71.2 62.2 90.9 72.5 67.4 64.6 52.9 60.9(+2.2)✓ 39.8 49.3 61.6 60.5 49.9 73.5 64.8 90.6 73.7 69.1 68.6 55.7 63.1 C. Single Model Evaluation Scheme In Section 3.2, we assume fθt can generate predictions whenever g is occupied with adapting to a batch. This setup assumes the capacity to concurrently deploy two models. However, this assumption might be unfair to methods with C(g) = 1, since it allows expensive methods to skip batches without large penalties. We thus also study the case where only one model can be deployed. Studying this setup requires establishing a policy on how samples missed by the TTA method g are treated. That is, when g is busy adapting, all skipped samples still must be predicted without access to fθt . Depending on the applica- tion, this prediction could leverage prior knowledge about the problem e.g. temporal correlation across samples, or the bias of the distribution. In our setup, we consider the most strict scenario in which, whenever g is busy, a ran- dom classifier generates predictions for the incoming sam- ples. This naive design choice results from our evaluation on ImageNet-based datasets, which contain images whose classes display no bias nor temporal correlation. We conduct episodic evaluation, similar to Section 4.1, on ImageNet-C dataset. We average the error rates per corruption category (e.g. averaging error rates for gaussian, shot, and impulse noises) and present the results of this study in Table 10. We draw the following observation. Single model evaluation strongly favors methods with C(g) = 1. We observe that all models that are slower than the stream are heavily penalized to the point that using the original pre-trained model becomes a better alternative. However, methods that can be as fast as the stream, like AdaBN or BN, become the best alternative due to their speed. This result encourages more research toward devel- oping efficient TTA methods that have negligible additional computational overhead. D. Results on ResNet18 In our experiments in the main paper, we focused on the stan- dard ResNet18-architecture, following the common practice in the literature. Here, and for completeness, we extend our results to cover the smaller and more efficient ResNet18 architecture. Teble 11 summarizes the episodic evaluation of 6 TTA methods on ImageNet-C dataset. Similar to our conclusions in the episodic evaluation section in the main paper, more expensive adaptation methods degrade more under our realistic evaluation scheme. 15Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 10: Per Corruption Category Average Error Rate Using Single Model Evaluation on ImageNet-C. We re- port the average error rate per corruption category of dif- ferent TTA methods under single model realistic evaluation mode on ImageNet-C. Single model mode assumes the de- ployment of a single modelg instead of two under a constant speed stream S. We assume the most extreme scenario, that is if a model g is occupied adapting to a batch, the incoming batch is fed to a random classifier. We observe that the best TTA methods to use in this scenario are AdaBN (Li et al., 2016) and BN (Schneider et al., 2020), which simply adapt the BN statistics. Method Realistic Noise Blur Weather Digital Avg. Source ✓ 97.7 83.8 69.1 81.4 82.0 AdaBN ✓ 84.5 76.1 54.9 62.7 68.5 BN ✓ 84.1 73.1 54.2 59.9 66.7 SHOT ✓ 92.6 91.3 87.0 88.5 89.7 TENT ✓ 91.9 89.4 83.0 85.0 87.0 SAR ✓ 95.6 94.0 90.1 91.3 92.6 EATA ✓ 89.4 87.6 82.0 83.2 85.3 TTAC-NQ ✓ 96.6 96.9 96.3 96.4 96.5 Table 11: Evaluating different TTA methods with ResNet- 18 architecture on ImageNet-C. We report the average error rate across all different types of corruptions (lower is bet- ter). TTA methods generally perform worse in the more realistic setup. The more computationally complex the TTA method is, the less data it will adapt to, and the worse is its performance. Method Basic BN SHOT Tent EATA SAR Current 85.4 70.1 64.4 64.9 59.7 63.8 Realistic 85.4 70.1 64.5 68.3 63.2 69.5 Diff - - 0.1 3.4 3.5 5.7 1/16 1/8 1/4 1/2 1 η 62 64 66 68 70 72 74Error Rate (%) SHOT TENT TTAC-NQ SAR EATA Figure 8: Average Error Rate on ImageNet-3DCC Under Slower Stream Speeds. We report the average error rate for several TTA methods on ImageNet-3DCC under slower stream speeds. In our proposed online model evaluation, the stream speed r is normalized by the time needed for a forward pass using the base model. We evaluate different TTA methods under a stream with speed ηr with η ∈ (0, 1]. An η = 1/16 means the stream is 16 times slower than the forward pass of the base model. We report the standard deviation across 3 random seeds. Different TTA methods degrade differently when varying η. 16",
      "meta_data": {
        "arxiv_id": "2304.04795v2",
        "authors": [
          "Motasem Alfarra",
          "Hani Itani",
          "Alejandro Pardo",
          "Shyma Alhuwaider",
          "Merey Ramazanova",
          "Juan C. Pérez",
          "Zhipeng Cai",
          "Matthias Müller",
          "Bernard Ghanem"
        ],
        "published_date": "2023-04-10T18:01:47Z",
        "pdf_url": "https://arxiv.org/pdf/2304.04795v2.pdf"
      }
    },
    {
      "title": "Provably Efficient Model-based Policy Adaptation",
      "abstract": "The high sample complexity of reinforcement learning challenges its use in\npractice. A promising approach is to quickly adapt pre-trained policies to new\nenvironments. Existing methods for this policy adaptation problem typically\nrely on domain randomization and meta-learning, by sampling from some\ndistribution of target environments during pre-training, and thus face\ndifficulty on out-of-distribution target environments. We propose new\nmodel-based mechanisms that are able to make online adaptation in unseen target\nenvironments, by combining ideas from no-regret online learning and adaptive\ncontrol. We prove that the approach learns policies in the target environment\nthat can quickly recover trajectories from the source environment, and\nestablish the rate of convergence in general settings. We demonstrate the\nbenefits of our approach for policy adaptation in a diverse set of continuous\ncontrol tasks, achieving the performance of state-of-the-art methods with much\nlower sample complexity.",
      "full_text": "Provably Efﬁcient Model-based Policy Adaptation Yuda Song1 Aditi Mavalankar 1 Wen Sun2 Sicun Gao 1 Abstract The high sample complexity of reinforcement learning challenges its use in practice. A promis- ing approach is to quickly adapt pre-trained poli- cies to new environments. Existing methods for this policy adaptation problem typically rely on domain randomization and meta-learning, by sam- pling from some distribution of target environ- ments during pre-training, and thus face difﬁculty on out-of-distribution target environments. We propose new model-based mechanisms that are able to make online adaptation in unseen target environments, by combining ideas from no-regret online learning and adaptive control. We prove that the approach learns policies in the target en- vironment that can recover trajectories from the source environment, and establish the rate of con- vergence in general settings. We demonstrate the beneﬁts of our approach for policy adaptation in a diverse set of continuous control tasks, achiev- ing the performance of state-of-the-art methods with much lower sample complexity. Our project website, including code, can be found at https: //yudasong.github.io/PADA. 1. Introduction Deep Reinforcement Learning (RL) methods typically re- quire a very large number of interactions with environments, making them difﬁcult to be used on practical systems (Tan et al., 2018). A promising direction is to adapt policies trained in one environment to similar but unseen environ- ments, such as from simulation to real robots. Existing ap- proaches for policy adaptation mostly focus on pre-training the policies to be robust to predeﬁned distributions of dis- turbances in the environment, by increasing the sample di- versity during training (Peng et al., 2018; Tobin et al., 2017; 1Department of Computer Science and Engineering, Univer- sity of California, San Diego, La Jolla, USA 2Microsoft Re- search NYC, New York , USA. Correspondence to: Yuda Song <yus167@ucsd.edu>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 108, 2020. Copyright 2020 by the author(s). Mordatch et al., 2015), or meta-learn policies or models that can be quickly adapted to in-distribution environments (Finn et al., 2017a; Nagabandi et al., 2019a;b; Yu et al., 2018a). A key assumption for these approaches is that the distribu- tion of the target environments is known, and that it can be efﬁciently sampled during training. On out-of-distribution target environments, these methods typically do not deliver good performance, reﬂecting common challenges in gener- alization (Na et al., 2020). If we observe how humans and animals adapt to environment changes, clearly there is an online adaptation process in addition to memorization (Stad- don, 2016). We can quickly learn to walk with a slightly injured leg even if we have not experienced the situation. We draw experiences from normal walking and adapt our actions online, based on how their effects differ from what we are familiar with in normal settings. Indeed, this in- tuition has recently led to practical approaches for policy adaptation. The work in (Christiano et al., 2016) uses a pre- trained policy and model of the training environment, and learns an inverse dynamics model from scratch in the new environment by imitating the behaviors of the pre-trained policy. However, it does not involve mechanisms for ac- tively reducing the divergence between the state trajectories of the two environments, which leads to inefﬁciency and distribution drifting, and does not fully capture the intuition above. The work in (Zhu et al., 2018) uses Generative Ad- versarial Imitation Learning (GAIL) (Ho & Ermon, 2016) to imitate the source trajectories in the new environment, by adding the GAIL discriminator to the reward to reduce divergence, but relies on generic policy optimization meth- ods with high sample complexity. In general, these recent approaches show the feasibility of policy adaptation, but are not designed for optimizing sample efﬁciency. There has been no theoretical analysis of whether policy adaptation methods can converge in general, or their beneﬁts in terms of sample complexity. In this paper, we propose a new model-based approach for the policy adaptation problem that focuses on efﬁciency with theoretical justiﬁcation. In our approach, the agent at- tempts to predict the effects of its actions based on a model of the training environment, and then adapts the actions to minimize the divergence between the state trajectories in the new (target) environment and in the training (source) envi- ronment. This is achieved by iterating between two steps: a arXiv:2006.08051v1  [cs.LG]  14 Jun 2020Provably Efﬁcient Model-based Policy Adaptation modeling step learns the divergence between the source envi- ronment and the target environment, and a planning step that uses the divergence model to plan actions to reduce the di- vergence over time. Under the assumption that the target en- vironment is close to the source environment, the divergence modeling and policy adaption can both be done locally and efﬁciently. We give the ﬁrst theoretical analysis of policy adaptation by establishing the rate of convergence of our approaches under general settings. Our methods combine techniques from model-based RL (Wang et al., 2019) and no- regret online learning (Ross et al., 2011). We demonstrate that the approach is empirically efﬁcient in comparison to the state-of-the-art approaches (Christiano et al., 2016; Zhu et al., 2018). The idea of recovering state trajectories from the source environment in the target environment suggests a strong connection between policy adaptation and imitation learning, such as Learning from Observation (LfO) (Torabi et al., 2018; 2019a; Sun et al., 2019b; Yang et al., 2019). A key difference is that in policy adaptation, the connec- tion between the source and target environments and their difference provide both new challenges and opportunities for more efﬁcient learning. By actively modeling the diver- gence between the source and target environments, the agent can achieve good performance in new environments by only making local changes to the source policies and models. On the other hand, because of the difference in the dynamics and the action spaces, it is not enough to merely imitate the experts (Bain & Sommut, 1999; Ross et al., 2011; Sun et al., 2017). Traditionally, adaptive control theory ( ˚Astr¨om, 1983) studies how to adapt to disturbances by stabilizing the error dynamics. Existing work in adaptive control assumes closed-form dynamics and does not apply to the deep RL setting (Nagabandi et al., 2019a). In comparison to domain randomization and meta-learning approaches, our proposed approach does not require sampling of source environments during pre-training, and makes it possible to adapt in out- of-distribution environments. Note that the two approaches are complementary, and we demonstrate in experiments that our methods can be used in conjunction with domain randomization and meta-learning to achieve the best results. The paper is organized as follows. We review related work in Section 2 and the preliminaries in Section 3. In Section 4, we propose the theoretical version of the adaptation al- gorithm and prove its rate of convergence. In section 5 we describe the practical implementation using deviation mod- els and practical optimization methods. We show detailed comparison with competing approaches in Section 6. 2. Related Work Our work connects to existing works on imitation learning, online adaptation, domain randomization and meta-learning, and model-based reinforcement learning. Imitation Learning. In imitation learning, there is typically no separation between training environments and test en- vironments. Existing imitation learning approaches aim to learn a policy that generates state distributions (Tobin et al., 2017; Torabi et al., 2019b; Sun et al., 2019b; Yang et al., 2019) or state-action distributions (Ho & Ermon, 2016; Fu et al., 2017; Ke et al., 2019; Ghasemipour et al., 2019) that are similar to the ones given by the expert policy. The differ- ence in the policy adaptation setting is that the expert actions do not work in the ﬁrst place in the new environment, and we need to both model the divergence and ﬁnd a new policy for the target environment. In light of this difference, the work (Zhu et al., 2018) considers a setting that is the closest to ours (which we will compare with in the experiments). It uses a state-action-imitation (GAIL(Ho & Ermon, 2016)) approach to learn a policy in the target environment, to gen- erate trajectories that are similar to the trajectories of the expert from the source environments. It also relies on using the true reward signals in the target environment to train the policy besides state imitation. In recent work, (Liu et al., 2020) approaches a similar problem by using Wasserstein distance between the states as the reward. It uses adversarial training and model-free policy optimization methods. Our approach is model-based and relies on reduction to Data Aggregation (Ross et al., 2011) for efﬁciency. The reduc- tion allows us to derive provable convergence guarantee with the rate of convergence. Experimentally we show that our approach is more sample efﬁcient than model-free and minmax-based imitation approaches in general. Online Adaptation. Online adaptation methods transfer policies by learning a mapping between the source and target domains (Daftry et al., 2016; Tzeng et al., 2015). Such methods have achieved success in vision-based robotics but require extra kernel functions or learning feature spaces. In contrast, we focus on control problems where the policy adaptation is completely autonomous. (Christiano et al., 2016) trains an inverse dynamics model (IDM) which can serve as the target policy by inquiring the source policy online. However, the approach does not focus on optimizing sample efﬁciency, which is crucial for the use of policy adaptation. In (Yu et al., 2018b), the agent selects from a range of pre-trained policies online, and does not perform further adaptation, and thus experiences problems similar to domain randomization approaches. Domain Randomization and Meta-Learning. Domain randomization and meta-learning methods are popular ways of transferring pre-trained policies to new environments. These methods rely on the key assumption that the training environments and test environments are sampled from the same predeﬁned distribution. Domain randomization meth- ods train robust agents on diverse samples of target environ- ments (Tobin et al., 2017; Mordatch et al., 2015; Antonova et al., 2017; Chebotar et al., 2019). When the conﬁgurationProvably Efﬁcient Model-based Policy Adaptation (a) (b) (c) t= 0 t= 1 t= 2 t= 3 t= 4 Figure 1.(a) Halfcheetah of mass musing the source policy π(s) in the source environment M(s). (b) Halfcheetah of mass 1.5 ×m using the source policy π(s) in the target environment M(t). (c) Halfcheetah of mass 1.5 ×musing the learned target policy π(t) in M(t) with out method. Using the policy trained in the source environment without adapting it to the target environment yields suboptimal results. The adapted policy π(t) can recover gaits close to the source trajectory. of the target environment lies outside the training distribu- tion, there is no performance guarantee. In meta-learning, such as Model Agnostic Meta-Learning (MAML) (Finn et al., 2017a;b; Nagabandi et al., 2018), meta-learned dynam- ics policies and models can adapt to perturbed environments with notable success including in physical robots. How- ever, similar to domain randomization based approaches, they experience difﬁculties on new environments that are not covered by the training distribution. Our proposed ap- proach focuses on online adaption in unseen environments. It is orthogonal to domain randomization and meta-learning approaches. We show in experiments that these different approaches can be easily combined. Model-based Reinforcement Learning. Model-based re- inforcement learning (MBRL) provides a paradigm that learns the environment dynamics and optimizes the control actions at the same time. Recent work has shown that MBRL has much better sample efﬁciency compared to model-free approaches both theoretically and empirically (Tu & Recht, 2018; Chua et al., 2018; Sun et al., 2019a). Our setting is different from the traditional MBRL setting. We consider test environments that are different from the training envi- ronment, and adapt the policy from the training environment to the test environment. 3. Preliminaries We consider ﬁnite-horizon Markov Decision Processes (MDP) M= ⟨S,A,f,H,R ⟩with the following compo- nents. Sdenotes the state space, and Athe action space. The transition function f: S×S×A → [0,1] deter- mines the probability f(s′|s,a) of transitioning into state s′ from state s after taking action a. The reward func- tion R : S →R is deﬁned only on states. We write πθ to denote a stochastic policy πθ: S×A→ [0,1] param- eterized by θ. Each policy πθ determines a distribution over trajectories {(si,ai,ri)}H i=1 under a ﬁxed dynamics f. The goal of the agent is to maximize the expected cu- mulative reward J(θ) = Eπθ,f [∑H h=1 R(sh) ] over all pos- sible trajectories that can be generated by πθ. Without loss of generality, in the theoretical analysis we always as- sume the normalized total reward is in the [0,1] range, i.e., maxs1,...sH ∑H h=1 R(sh) ∈[0,1]. We write the set {1,2,...,N }as [N], and the uniform distribution over set Aas U(A) throughout the paper. For any two distributions d1 and d2, we use ∥d1 −d2∥to denote the total variation distance between the two distributions. 4. Policy Adaptation with Data Aggregation 4.1. Basic Deﬁnitions In policy adapation, we consider a pair of MDPs and call them a source MDP and a target MDP. We deﬁne the source MDP as M(s) := {S,A(s),f(s),H,R }and the target MDP as M(t) := {S,A(t),f(t),H,R }. Note that the two MDPs share the same state space and reward functions, but can have different action spaces and transition dynamics. Fig. 1 demonstrates the problem of adapting a policy from a source environment to a target environment. Because of the difference in the action space and dynamics, directly using good policies from the source environment (Fig.1(a))Provably Efﬁcient Model-based Policy Adaptation does not work in the target environment (Fig.1(b)). The objective is to adapt the policy from the source to the target environment to achieve good performance (Fig.1(c)). We focus on minimizing the samples needed for adaptation in the target MDP, by leveraging M(s) to quickly learn a policy in M(t). To achieve this, we assume that a pre- trained policy π(s) from M(s) achieves high rewards in M(s). We wish to adapt π(s) to a policy π(t) that works well in M(t). For ease of presentation, we considerπ(s) and π(t) as deterministic throughout the theoretical analysis. Given a policy π, we write E(s) π (·) for the expectation over random outcomes induced by πand M(s). We write d(s) π;h to denote the state distribution induced by πat time step h under M(s), and d(s) π = ∑H h=1 d(s) π;h/Has the average state distribution of π under M(s). We write ρ(s) π to represent the distribution of the state trajectories from π: for τ = {sh}H h=0, ρ(s) π (τ) = ∏H h=1 f(s)(sh|sh−1,π(sh−1)). For the target MDP M(t), we make the same deﬁnitions but drop the superscript (t) for ease of presentation. Namely, Eπ(·) denotes the expectation over the randomness from π and M(t), dπ denotes the induced state distribution of π under M(t), and ρπ denotes the state trajectory distribution. 4.2. Algorithm We now introduce the main algorithm Policy Adaptation with Data Aggregation (PADA). Note that this is the theo- retical version of the algorithm, and the practical implemen- tation will be described in detail in Section 5. To adapt a policy from a source environment to a target environment, PADA learns a model ˆf to approximate the target envi- ronment dynamics f(t). Based on the learned model, the algorithm generates actions that attempt to minimize the di- vergence between the trajectories in the target environment and those in the source environment generated by π(s) at M(s). Namely, the algorithm learns a policy π(t) that repro- duces the behavior ofπ(s) on M(s) in the target MDPM(t). Since the state space Sis often large, learning a model ˆf that can accurately approximate f(t) globally is very costly. Instead, we only aim to iteratively learn a locally accurate model, i.e., a model that is accurate near the states that are generated by π(t). This is the key to efﬁcient adaptation. The detailed algorithm is summarized in Alg. 1. Given a model ˆfe at the e-th iteration, we deﬁne the ideal policy π(t) e π(t) e (s) ≜ argmin a∈A(t) ∥ˆfe(·|s,a) −f(s)(·|s,π(s)(s))∥. (1) The intuition is that, assuming ˆfe is accurate in terms of modelling f(t) at state s, π(t) e (s) aims to pick an action such that the resulting next state distribution under ˆfe is similar to the next state distribution resulting from π(s) under the Algorithm 1 Policy Adaptation with Data Aggregation Require: Source domain policy π(s), source dynamics f(s), model class F 1: Initialize dataset D= ∅ 2: Initialize ˆf1 3: for e = 1, . . . Tdo 4: Deﬁne policy π(t) e as in Eq. 1 5: for n = 1, . . . Ndo 6: Reset M(t) to a random initial state 7: Uniformly sample a time step h∈[H] 8: Execute π(t) e in M(t) for hsteps to get state s 9: Sample exploration action a∼U(A(t)) 10: Take ain M(t) and get next state s′ 11: Add (s,a,s ′) into D(Data Aggregation) 12: end for 13: Update to ˆfe+1 via MLE by Eq. 2 14: end for 15: Output: {π(t) e }T e=1 source dynamics f(s). We then execute π(t) e in the target environment M(t) to generate a batch of data (s,a,s ′). We further aggregate the newly generated data to the dataset D(i.e., data aggregation). We update model to ˆfe+1 via Maximum Likelihood Estimation (MLE) on D: ˆfe+1 = argmax f∈F ∑ s,a,s′∈D log f(s′|s,a). (2) Note that Algorithm 1 relies on two black-box ofﬂine com- putation oracles: (1) a one-step minimization oracle (Eq. 1) and (2) a Maximum Likelihood Estimator (Eq. 2). In Sec- tion 5, we will introduce practical methods to implement these two oracles. We emphasize here that these two ora- cles are ofﬂine computation oracles and the computation itself does not require any fresh samples from the target environment M(t). 4.3. Analysis We now prove the performance guarantee of Alg.1 for policy adaptation and establish its rate of convergence. At a high level, our analysis of Alg. 1 is inspired from the analysis of DAgger (Ross et al., 2011; Ross & Bagnell, 2012) which leverages a reduction to no-regret online learning (Shalev- Shwartz et al., 2012). We will ﬁrst make the connection with the Follow-the-Leader (FTL) algorithm, a classic no-regret online learning algorithm, on a sequence of loss functions. We then show that we can transfer the no-regret property of FTL to performance guarantee on the learned policy π(t). Our analysis uses the FTL regret bound ˜O(1/T) where T is the number of iterations (Shalev-Shwartz et al., 2012). Since our analysis is a reduction to general no-regret online learn- ing, in theory we can also replace FTL by other no-regretProvably Efﬁcient Model-based Policy Adaptation online learning algorithms as well (e.g., Online Gradient De- scent (Zinkevich, 2003) and AdaGrad (Duchi et al., 2011)). Intuitively, for fast policy adaptation to succeed, one should expect that there is similarity between the source environ- ment and the target environment. We formally introduce the following assumption to quantify this. Assumption 4.1 (Adaptability). For any state action pair (s,a) with source action a ∈A(s) , there exists a target action a′∈A(t) in target environment, such that: ∥f(s)(·|s,a) −f(t)(·|s,a′)∥≤ ϵs,a, for some small ϵs,a ∈R+. Remark 4.2. When ϵs,a →0 in the above assumption, the target environment can perfectly recover the dynamics of the source domain at (s,a). However, ϵs,a = 0 does not mean the two transitions are the same, i.e.,f(t)(s,a) = f(s)(s,a). First the two action spaces can be widely different. Secondly, there may exist states s, where one may need to take com- pletely different target actions fromA(t) in order to match the source transition f(s)(·|s,a), i.e., ∃a′∈A(t) such that f(t)(·|s,a′) = f(s)(·|s,a), but a̸= a′. Assumption 4.3 (Realizability). Let the model class Fbe a subset of {f : S×S×A→ [0,1]}. We assume f(t) ∈F. Here we assume that our model class Fis rich enough to include f(t). Note that the assumption on realizability is just for analysis simplicity. Agnostic results can be achieved with more reﬁned analysis similar to (Ross et al., 2011; Ross & Bagnell, 2012). We deﬁne the following loss function: ℓe(f) ≜ Es∼d π(t) e ,a∼U(A(t)) [ DKL ( f(t)(·|s,a),f(·|s,a) )] , for all e ∈ [T]. The loss function ℓe(f) measures the difference between f(t) and f under the state distribution induced by π(t) e under M(t) and the uniform distribution over the action space. This deﬁnition matches the way we collect data inside each episode. We generate (s,a,s ′) triples via sampling sfrom dπ(t) e , afrom U(A(t)), and then s′∼f(t)(·|s,a). At the end of the iteration e, the learner uses FTL to compute ˆfe+1 as: ˆfe+1 = argmin f∈F e∑ i=1 ℓi(f). Using the deﬁnition of KL-divergence, it is straightforward to show that the above optimization is equivalent to the following Maximum Likelihood Estimation: argmax f∈F e∑ i=1 Es∼d π(t) e ,a∼U(A(t)),s′∼f(t)(·|s,a) [log f(s′|s,a)] . At the end of the episode e, the aggregated dataset Dcon- tains triples that are sampled based on the above procedure from the ﬁrst to the e-th episode. With no-regret learning on ˆfe, assumptions 4.1, and 4.3, we can obtain the following main results. We ﬁrst assume that the target environment M(t) has a discrete action space, i.e., A(t) is discrete, and then show that the result can be easily extended to continuous action spaces. Theorem 4.4 (Main Theorem). Assume M(t) has a dis- crete action space A(t) and denote A≜ |A(t)|. Among the sequence of policies computed in Alg. 1, there exists a policy ˆπsuch that: Es∼dˆπ∥f(t)(·|s,ˆπ(s)) −f(s)(·|s,π(s)(s))∥ ≤O ( AT−1/2 + Es∼dˆπ [ ϵs,π(s)(s) ]) , which implies that: ρˆπ −ρ(s) π(s) ≤O ( HAT−1/2 + HEs∼dˆπ [ ϵs,π(s)(s) ]) , where we recall that ρπ stands for the state-trajectory dis- tribution of policy π under M(t) and ρ(s) π(s) stands for the state-trajectory distribution of π(s) under M(s). The full proof is in the Appendix A. The theorem shows that our algorithm can provide a policy in the target en- vironment that induces trajectories close to those induced by the experts in the source environment. For instance, if the target and source MDPs are completely adaptable (i.e., ϵs,a = 0 in Assumption 4.1 for all (s,a)) and the number of iterations approach to inﬁnity, then we can learn a policy ˆπ that generates state trajectories in M(t) that match the state trajectories generated via the source policy π(s) at the source MDP M(s). Remark 4.5. The error Es∼dˆπ [ ϵs,π(s)(s) ] is averaged over the state distribution induced by the learned policy rather than in an ℓ∞form, i.e., maxs,aϵs,a. Although the analysis is done on discrete action space, the algorithm can be naturally applied to compact continuous action space as follows. The proof of the following corollary and its extension to the d-dimensional continuous action spaces are in the Appendix. Corollary 4.6 (Continuous Action Space). Assume A(t) = [0,1], f(t) and functions f ∈F are Lipschitz continuous with (and only with) actions in A(t). Among policies re- turned from Alg. 1, there exists a policy ˆπsuch that: ∥ρˆπ −ρ(s) π(s) ∥≤ O ( HT−1/4 + HEs∼dˆπ [ ϵs,π(s)(s) ]) . Remark 4.7. As we assume the reward function only de- pends on states, ∥ρˆπ −ρ(s) π(s) ∥ ≤δ implies |J(t)(ˆπ) − J(s)(π(s))|≤∥ ρˆπ −ρ(s) π(s) ∥ ( maxs1...sH ∑H h R(sh) ) ≤δProvably Efﬁcient Model-based Policy Adaptation due to the normalization assumption on the rewards. Thus, though our algorithm runs without rewards, when π(s) achieves high reward in the source MDP M(s), the algo- rithm is guaranteed to learn a policy ˆπthat achieves high rewards in the target environmentM(t). 5. Practical Implementation In Algorithm 1 we showed the theoretical version of our approach, which takes an abstract model class Fas input and relies on two ofﬂine computation oracles (Eq. 1 and Eq. 2). We now design the practical implementation by specifying the parameterization of the model class Fand the optimization oracles. Algorithm 2 shows the practical algorithm, and we explain the details in this section. 5.1. Model Parameterization In the continuous control environments, we focus on stochas- tic transitions with Gaussian noise, where f(t)(s,a) = ¯f(t)(s,a) + ϵ, f(s)(s,a) = ¯f(s)(s,a) + ϵ′, with ϵ and ϵ′ from N(0,Σ) and ¯f(t) and ¯f(s) being nonlinear determinis- tic functions. In this case, we consider the following model class with parameterization θ: F= {δθ(s,a) + ˆf(s)(s,π(s)(s)),∀s,a : θ∈Θ}. where ˆf(s) is a pre-trained model of the source dynam- ics f(s) and we assume ˆf(s) well approximates f(s) (and one has full control to the source environment such as the ability to reset). Then for each state s, ˆf(s)(s,π(s)(s)) is a ﬁxed distribution of the next state in the source environ- ment by following the source policy. Deﬁne ∆π(s) (s,a) ≜ ˆf(s)(s,π(s)(s)) −f(t)(s,a), which captures the deviation from taking action ain the target environment to following π(s) in the source environment. So δθ(s,a) is trained to approximate the deviation ∆π(s) (s,a). Note that learning ∆π(s) is just an alternative way to capture the target dynam- ics since we know ˆf(s)(s,π(s)) foresight, thus it should be no harder than learning f(t) directly. 5.2. Model Predictive Control For deterministic transition, Eq 1 reduces to one-step min- imization argmina∈A(t) ∥ˆfe(·|s,a) − ˆf(s)(s,π(s)(s))∥2. Since ˆfe ∈ F, we have ˆfe(s,a) = δθe(s,a) + ˆf(s)(s,π(s)(s)), and the optimization can be further sim- plﬁed to: argmina∈A(t) ∥δθe(s,a)∥2.We use the Cross En- tropy Method (CEM) (Botev et al., 2013) which iteratively repeats: randomly draw N actions, evaluate them in terms of the objective value ∥δθe(s,a)∥2, pick the top Kactions in the increasing order of the objective values, and then reﬁt a new Gaussian distribution using the empirical mean and covariance of the top Kactions. Algorithm 2 Policy Adaptation with Data Aggregation via Deviation Model Require: πs, ˆf(s), deviation model class {δθ : θ ∈Θ}, explore probability ϵ, replay buffer D, learning rate η 1: Randomly initialize divergence model δθ 2: for T Iterations do 3: for nsteps do 4: s←Reset M(t) 5: while current episode does not terminate do 6: With probability ϵ: a∼U(A(t)) 7: Otherwise: a←CEM(A(t),s,δ θ) 8: Execute ain M(t): s′←f(t)(s,a) 9: Update replay buffer: D←D∪{ (s,a,s ′)} 10: s←s′ 11: end while 12: end for 13: Update θwith Eq. 3 14: end for We emphasize here we only need to solve a one-step opti- mization problem without unrolling the system for multiple steps. We write the CEM oracle as CEM(A(t),s,δ θ) which outputs an action afrom A(t) that approximately minimizes ∥δθ(s,a)∥2. Here, CEM(A(t),s,δ θ) : S →A(t) can be considered as a policy that maps state sto a target action a. 5.3. Experience Replay for Model Update Note that Alg. 1 requires to solve a batch optimization prob- lem (MLE in Eq. 2) in every iteration, which could be com- putationally expensive in practice. We use Experience Re- play (Adam et al., 2011; Mnih et al., 2013), which is more suitable to optimize rich non-linear function approximators (δθ is a deep neural network in our experiments). Given the current divergence model δθ and the aggregated dataset D= {s,a,s ′}(aka, replay buffer) with s′ = f(t)(s,a), we randomly sample a mini-batch B ⊂D and perform a stochastic gradient descent step with learning rate η: θ←θ− η |B|∇θ (∑|B| i=1∥ˆf(s)(si,π(s)(si)) +δθ(si,ai) −s′ i∥22 ) . (3) 5.4. Policy Adaptation with Data Aggregation As show in Algorithm 2, we maintain a reply buffer that stores all experiences from the target model M(t) (Line 9) and constantly update the model δθ using mini-batch SGD (Eq. 3). Alg 2 performs local exploration in an ϵ-greedy way. We refer our method as Policy Adaptation with Data Aggregation via Deviation Model (PADA-DM). Remark 5.1. Even being one-step, CEM(A(t),s,δ θ) may be computationally expensive, we could obtain a MPC-free policy (target policy) by training an extra parameterizedProvably Efﬁcient Model-based Policy Adaptation policy to mimic CEM(A(t),s,δ θ) via techniques of Behav- ior Cloning (Bain & Sommut, 1999). When we train this extra parameterized policy, we name the method as PADA- DM with target policyand we will show it does not affect the performance of the overall algorithm during training. However, during test time, such parameterized policy runs faster than CEM and thus is more suitable to be potentially deployed on real-world systems. 6. Experiments In this section we compare our approach with the state- of-the-art methods for policy adaptation (Christiano et al., 2016; Zhu et al., 2018; Schulman et al., 2017; Finn et al., 2017a) and show that we achieve competitive results more efﬁciently. We also test the robustness of the approaches on multi-dimensional perturbations. We then compare to domain randomization and meta-learning approaches and show how they can be combined with our approach. We provide further experiments in Appendix D. Following the same experiment setup as (Christiano et al., 2016), We focus on standard OpenAI Gym (Brockman et al., 2016) and Mujoco (Todorov et al., 2012) control environ- ments such as HalfCheetah, Ant, and Reacher. We perturb the environments by changing their parameters such as mass, gravity, dimensions, motor noise, and friction. More details of task designs are in Appendix B.1. 6.1. Comparison with Existing Approaches We compare our methods (PADA-DM, PADA-DM with target policy) with the following state-of-the-art methods for policy adaptation. The names correspond to the learning curves shown in Figure 2. Christiano et al., 2016: (Christiano et al., 2016) uses a pre-trained policy π(s) and source dynamics f(s), to learn an inverse dynamics model φ: A×S×S→ [0,1], where φ(a|s,s′) is the probability of taking action athat leads to s′from s.1 That is, π(t)(s) = φ(s,f(s)(s,π(s)(s))). Zhu et al., 2018: (Zhu et al., 2018) proposed an approach for training policies in the target domain with a new reward λR(sh) + (1 −λ)Rgail(sh,ah), λ ∈[0,1]. Here Rgail is from the discriminator from GAIL. Note that this baseline has access to true reward signals while ours do not. For additional baselines, we also show the performance of directly running Proximal Policy Optimization ( PPO) (Schulman et al., 2017) in the target environment, as well as directly using the source policy in the perturbed environ- 1In general an inverse model is ill-deﬁned without ﬁrst spec- ifying a policy, i.e., via Bayes rule, φ(a|s,s′) ∝P(a,s,s ′) = f(t)(s′|s,a)π(a|s). Hence one needs to ﬁrst specify πin order to justify the existence of an inverse model φ(a|s,s′). ment without adapation. 7 8 9 10 11 12 13 14 Number of Timesteps (in log) 1000 0 1000 2000 3000 4000 5000 6000 Rewards HalfCheetah 120% Mass PADA-DM PADA-DM w/ target Christiano et al., 2016 PPO Zhu el al., 2018 7 8 9 10 11 12 13 14 Number of Timesteps (in log) 0 500 1000 1500 2000 2500 3000Rewards Ant 50% Mass Figure 3.The long-term learning curves. The x-axis is the number of timesteps in natural logarithm scale. Results. Figure 2 demonstrates the sample efﬁciency of our methods compared to the other methods and baselines. Both PADA-DM and PADA-DM with target policy con- verge within 10k to 50k training samples in the target envi- ronments. In contrast, (Christiano et al., 2016) requires 5 times more samples than our methods on average, and (Zhu et al., 2018) and PPO require about 30 times more. At con- vergence, our methods obtain the highest episodic rewards in 7 out of 8 tasks above among the policy adaptation meth- ods. The baseline performance of PPO is better than the policy adaptation methods in HalfCheetah and Reacher (re- call PPO uses true reward signals), but it takes signiﬁcantly longer as shown in Fig. 2. Note that in the Ant environment, even at convergence our methods outperform PPO as well. The only task where our methods failed to achieve top per- formance is Ant-v2 0.6 std motor noise. In this environment, the action noise causes high divergence between the tar- get and source environments, making it hard to efﬁciently model the domain divergence. All the adaptation methods deliver bad performance in this case, indicating the difﬁculty of the task. We observe that the learning curves of PADA-DM and PADA-DM with target policy are similar across all tasks without sacriﬁcing efﬁciency or performance. The target policy can be directly used without any MPC step. To further illustrate the sample efﬁciency of our method, we compare the long-term learning curves in Fig. 3. We plot the learning curves up to convergence of each method. We further include a long-term version of Fig 2 and the hyperparameters in the Appendix. 6.2. Performance on Multi-Dimensional Perturbations We further evaluate the robustness of our methods by per- turbing multiple dimensions of the target environment (Fig. 4). Note that online adaptation is particularly useful for multiple-dimension perturbations, because they generate an exponentially large space of source environments that are hard to sample ofﬂine. In Fig. 4(b), we show that even when perturbing 15 different degrees of freedom of the tar-Provably Efﬁcient Model-based Policy Adaptation 0 5000 10000 15000 20000 25000 30000 Number of Timesteps 1000 0 1000 2000 3000 4000 Rewards HalfCheetah Mass 150% 0 5000 10000 15000 20000 25000 30000 Number of Timesteps 0 1000 2000 3000 4000Rewards HalfCheetah 50% Gravity 0 5000 10000 15000 20000 25000 30000 Number of Timesteps 500 0 500 1000 1500Rewards HalfCheetah 0.4 std Motor Noise 0 5000 1000015000200002500030000 Number of Timesteps 300 250 200 150 100 50 0 Rewards Reacher-v2 10% first arm length 0 5000 10000 15000 20000 25000 30000 Number of Timesteps 0 500 1000 1500 2000Rewards Ant 50% Mass 0 5000 10000 15000 20000 25000 30000 Number of Timesteps 2000 1000 0 1000 2000 3000 Rewards Ant 200% Gravity 0 5000 10000 15000 20000 25000 30000 Number of Timesteps 1000 500 0 500 1000 Rewards Ant 0.6 std Action Noise 0 5000 1000015000200002500030000 Number of Timesteps 250 200 150 100 50 0 Rewards Reacher-v2 45 degree plane Figure 2.We plot the learning curves across 5 random seeds on a number of tasks. The title of each plot correspounds to the perturbation in the target domain, e.g., HalfCheetah Mass 150% means in mass of the agent in the target domain is 150% of that in the source domain. get environment, our adaptation method can still achieve competitive performance at a much faster rate than all the other methods. We record the details of the conﬁgurations of the target environments in Appendix B.2.7. 0 10000 20000 30000 40000 50000 Number of Timesteps 0 500 1000 1500 2000 2500 3000 3500Rewards Ant-v2 3 DOF PADA-DM PADA-DM w/ target Christiano et al., 2016 PPO Zhu el al., 2018 source policy (a) 0 10000 20000 30000 40000 50000 Number of Timesteps 0 500 1000 1500 2000 2500 3000 3500Rewards Ant-v2 15 DOF (b) Figure 4.Learning curves for: changing 3 (a) and 15 (b) conﬁgura- tions in the target environment. 6.3. Comparison with Domain Randomization and Meta-Learning We now compare with domain randomization and meta- learning approaches and show how they can be combined with our methods. single source domain randomized source domains no adaptation in target domain source policy DR adaptation in target domain PADA-DM PADA-DM w/ DR; MAML Table 1.Relationship of 5 methods in our ablation study. Domain randomization (DR) (Peng et al., 2018): During the training in the source domain, at the beginning of each episode, we randomly sample a new conﬁguration for the source domain within a speciﬁed distribution. The total number of training samples here is the same as that for training the source policy. The policy outputed from DR is used in the target environment without further adaptation. MAML: We adopt the RL version of MAML (Finn et al., 2017a) that meta-learns a model-free policy over a distribu- tion of source environments and performs few-shot adapta- tion on the target environment. We compare the following methods: (1) source policy trained in a ﬁxed source environment, (2) domain random- ization, (3) PADA-DM, (4) PADA-DM with DR (using domain randomization’s output asπ(s) for PADA), and (5) MAML. Table 1 shows how these methods relate to each other. For the ﬁrst four methods, we train the source policy with 2m samples and perform adaptation with 80k samples. For MAML, we use 500 batches of meta-training (400m sam- ples), and adapt 10 times with 80k samples in the target do- main. We perform 100 trajectories across 5 random seeds in the target domain for each method and compare the episodic reward in Figure 5. We ﬁrst observe that when the target domains lie in the distribution of domain randomization (70% −130%), domain randomization outperforms source policy signiﬁcantly, but does not help when the target lies far away from the distribution, which is the most notable short- coming of domain randomization. Note that using domain randomization in conjunction with our adaptation method usually yields the best results. Domain randomization oftenProvably Efﬁcient Model-based Policy Adaptation provides robustness within the environments’ distribution, and online adaptation in real target environment using our approach further ensures robustness to out-of-distribution environments. We also observe that our method provides the most stable performance given the smallest test variances. We include additional experiments and detailed numbers of the performances of all methods (mean and standard deviations) in Appendix D.4. 50% 80% 120% 150% 200% Gravity 0 2000 4000 6000Rewards algorithm MAML DR source policy PADA-DM PADA-DM w/ DR (a) 50% 80% 120% 150% 200% Mass 2000 0 2000 4000 6000 Rewards algorithm MAML DR source policy PADA-DM PADA-DM w/ DR (b) Figure 5.Ablation experiments using domain randomization and meta-learning. (a) Varying gravity. (b) Varying mass. 7. Conclusion We proposed a novel policy adaptation algorithm that com- bines techniques from model-based RL and no-regret online learning. We theoretically proved that our methods gener- ate trajectories in the target environment that converge to those in the source environment. We established the rate of convergence of the algorithms. We have shown that our al- gorithm achieves competitive performance across a diverse set of continuous control tasks with better sample efﬁciency. A natural extension is to use our approach on simulation-to- real problems in combination with domain randomization and meta-learning. As our experiments indicated that the combination of do- main randomization and our online adaptation approach together often yields good results, for future work, we plan to investigate general theoretical framework for combining domain randomization and online adaptive control tech- niques. References Adam, S., Busoniu, L., and Babuska, R. Experience re- play for real-time reinforcement learning control. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 42(2):201–212, 2011. Antonova, R., Cruciani, S., Smith, C., and Kragic, D. Re- inforcement learning for pivoting task. arXiv preprint arXiv:1703.00472, 2017. ˚Astr¨om, K. J. Theory and applications of adaptive controla survey. Automatica, 19(5):471–486, 1983. Bain, M. and Sommut, C. A framework for behavioural claning. Machine intelligence, 15(15):103, 1999. Botev, Z. I., Kroese, D. P., Rubinstein, R. Y ., and LEcuyer, P. The cross-entropy method for optimization. In Handbook of statistics, volume 31, pp. 35–59. Elsevier, 2013. Brockman, G., Cheung, V ., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W. Openai gym. CoRR, abs/1606.01540, 2016. URL http://arxiv. org/abs/1606.01540. Chebotar, Y ., Handa, A., Makoviychuk, V ., Macklin, M., Issac, J., Ratliff, N., and Fox, D. Closing the sim-to- real loop: Adapting simulation randomization with real world experience. In 2019 International Conference on Robotics and Automation (ICRA), pp. 8973–8979. IEEE, 2019. Christiano, P., Shah, Z., Mordatch, I., Schneider, J., Black- well, T., Tobin, J., Abbeel, P., and Zaremba, W. Transfer from simulation to real world through learning deep in- verse dynamics model. arXiv preprint arXiv:1610.03518, 2016. Chua, K., Calandra, R., McAllister, R., and Levine, S. Deep reinforcement learning in a handful of trials using proba- bilistic dynamics models. In Advances in Neural Infor- mation Processing Systems, pp. 4754–4765, 2018. Daftry, S., Bagnell, J. A., and Hebert, M. Learning trans- ferable policies for monocular reactive mav control. In International Symposium on Experimental Robotics, pp. 3–11. Springer, 2016. Duchi, J., Hazan, E., and Singer, Y . Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research , 12(Jul):2121– 2159, 2011. Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta- learning for fast adaptation of deep networks. In Proceed- ings of the 34th International Conference on Machine Learning-Volume 70, pp. 1126–1135. JMLR. org, 2017a. Finn, C., Yu, T., Zhang, T., Abbeel, P., and Levine, S. One- shot visual imitation learning via meta-learning. arXiv preprint arXiv:1709.04905, 2017b. Fu, J., Luo, K., and Levine, S. Learning robust rewards with adversarial inverse reinforcement learning. arXiv preprint arXiv:1710.11248, 2017. Ghasemipour, S. K. S., Zemel, R., and Gu, S. A divergence minimization perspective on imitation learning methods. arXiv preprint arXiv:1911.02256, 2019.Provably Efﬁcient Model-based Policy Adaptation Ho, J. and Ermon, S. Generative adversarial imitation learn- ing. In Advances in neural information processing sys- tems, pp. 4565–4573, 2016. Ke, L., Barnes, M., Sun, W., Lee, G., Choudhury, S., and Srinivasa, S. Imitation learning as f-divergence mini- mization. arXiv preprint arXiv:1905.12888, 2019. Liu, F., Ling, Z., Mu, T., and Su, H. State alignment- based imitation learning. In International Conference on Learning Representations, 2020. URL https:// openreview.net/forum?id=rylrdxHFDr. Mnih, V ., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. Mordatch, I., Lowrey, K., and Todorov, E. Ensemble-cio: Full-body dynamic motion planning that transfers to phys- ical humanoids. In 2015 IEEE/RSJ International Con- ference on Intelligent Robots and Systems (IROS) , pp. 5307–5314. IEEE, 2015. Na, D., Lee, H. B., Lee, H., Kim, S., Park, M., Yang, E., and Hwang, S. J. Learning to balance: Bayesian meta- learning for imbalanced and out-of-distribution tasks. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum? id=rkeZIJBYvr. Nagabandi, A., Clavera, I., Liu, S., Fearing, R. S., Abbeel, P., Levine, S., and Finn, C. Learning to adapt in dynamic, real-world environments through meta-reinforcement learning. arXiv preprint arXiv:1803.11347, 2018. Nagabandi, A., Clavera, I., Liu, S., Fearing, R. S., Abbeel, P., Levine, S., and Finn, C. Learning to adapt in dynamic, real-world environments through meta-reinforcement learning. ICLR, 2019a. Nagabandi, A., Finn, C., and Levine, S. Deep online learn- ing via meta-learning: Continual adaptation for model- based rl. ICLR, 2019b. Nair, V . and Hinton, G. E. Rectiﬁed linear units improve restricted boltzmann machines. InProceedings of the 27th international conference on machine learning (ICML-10), pp. 807–814, 2010. Peng, X. B., Andrychowicz, M., Zaremba, W., and Abbeel, P. Sim-to-real transfer of robotic control with dynam- ics randomization. In 2018 IEEE International Confer- ence on Robotics and Automation (ICRA), pp. 1–8. IEEE, 2018. Ross, S. and Bagnell, J. A. Agnostic system identiﬁcation for model-based reinforcement learning. arXiv preprint arXiv:1203.1007, 2012. Ross, S., Gordon, G., and Bagnell, D. A reduction of imita- tion learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artiﬁcial intelligence and statistics , pp. 627–635, 2011. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Shalev-Shwartz, S. et al. Online learning and online convex optimization. Foundations and Trends R⃝in Machine Learning, 4(2):107–194, 2012. Staddon, J. E. Adaptive behavior and learning. Cambridge University Press, 2016. Sun, W., Venkatraman, A., Gordon, G. J., Boots, B., and Bagnell, J. A. Deeply aggrevated: Differentiable imita- tion learning for sequential prediction. In Proceedings of the 34th International Conference on Machine Learning- Volume 70, pp. 3309–3318. JMLR. org, 2017. Sun, W., Jiang, N., Krishnamurthy, A., Agarwal, A., and Langford, J. Model-based rl in contextual decision pro- cesses: Pac bounds and exponential improvements over model-free approaches. In Conference on Learning The- ory, pp. 2898–2933, 2019a. Sun, W., Vemula, A., Boots, B., and Bagnell, J. A. Provably efﬁcient imitation learning from observation alone. arXiv preprint arXiv:1905.10948, 2019b. Tan, J., Zhang, T., Coumans, E., Iscen, A., Bai, Y ., Hafner, D., Bohez, S., and Vanhoucke, V . Sim-to-real: Learning agile locomotion for quadruped robots. arXiv preprint arXiv:1804.10332, 2018. Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W., and Abbeel, P. Domain randomization for transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 23–30. IEEE, 2017. Todorov, E., Erez, T., and Tassa, Y . Mujoco: A physics engine for model-based control. In IROS 2012, pp. 5026– 5033. IEEE, 2012. Torabi, F., Warnell, G., and Stone, P. Behavioral cloning from observation. In Proceedings of the 27th Interna- tional Joint Conference on Artiﬁcial Intelligence , pp. 4950–4957, 2018. Torabi, F., Warnell, G., and Stone, P. Imitation learning from video by leveraging proprioception. In Proceedings of the 28th International Joint Conference on Artiﬁcial Intelligence, pp. 3585–3591. AAAI Press, 2019a.Provably Efﬁcient Model-based Policy Adaptation Torabi, F., Warnell, G., and Stone, P. Recent advances in imitation learning from observation. In Proceedings of the 28th International Joint Conference on Artiﬁcial Intelligence, pp. 6325–6331. AAAI Press, 2019b. Tu, S. and Recht, B. The gap between model-based and model-free methods on the linear quadratic regulator: An asymptotic viewpoint. arXiv preprint arXiv:1812.03565, 2018. Tzeng, E., Devin, C., Hoffman, J., Finn, C., Abbeel, P., Levine, S., Saenko, K., and Darrell, T. Adapting deep vi- suomotor representations with weak pairwise constraints. arXiv preprint arXiv:1511.07111, 2015. Wang, T., Bao, X., Clavera, I., Hoang, J., Wen, Y ., Langlois, E., Zhang, S., Zhang, G., Abbeel, P., and Ba, J. Bench- marking model-based reinforcement learning. CoRR, abs/1907.02057, 2019. URL http://arxiv.org/ abs/1907.02057. Yang, C., Ma, X., Huang, W., Sun, F., Liu, H., Huang, J., and Gan, C. Imitation learning from observations by minimizing inverse dynamics disagreement. In Advances in Neural Information Processing Systems, pp. 239–249, 2019. Yu, T., Finn, C., Xie, A., Dasari, S., Zhang, T., Abbeel, P., and Levine, S. One-shot imitation from observing hu- mans via domain-adaptive meta-learning. arXiv preprint arXiv:1802.01557, 2018a. Yu, W., Liu, C. K., and Turk, G. Policy transfer with strategy optimization. arXiv preprint arXiv:1810.05751, 2018b. Zhu, Y ., Wang, Z., Merel, J., Rusu, A., Erez, T., Cabi, S., Tunyasuvunakool, S., Kramr, J., Hadsell, R., de Freitas, N., and Heess, N. Reinforcement and imitation learning for diverse visuomotor skills. In Proceedings of Robotics: Science and Systems, Pittsburgh, Pennsylvania, June 2018. doi: 10.15607/RSS.2018.XIV .009. Zinkevich, M. Online convex programming and generalized inﬁnitesimal gradient ascent. In Proceedings of the 20th International Conference on Machine Learning (ICML- 03), pp. 928–936, 2003.Provably Efﬁcient Model-based Policy Adaptation A. Detailed Analysis of Algorithm 1 (Proof of Theorem 4.4) In this section, we provide detailed proof for Theorem 4.4. Consider a Markov Chain p: S→ ∆(S) over horizon H. Denote dp as the induced state distribution under pand ρp as the induced state trajectory distribution under p. Lemma A.1. Consider two Markov Chains pi : S→ ∆(S) with i∈{1,2}. If Es∼dp1 [∥p1(·|s) −p2(·|s)∥] ≤δ, then for trajectory distributions, we have: ∥ρp1 −ρp2 ∥≤ O(Hδ). The above lemma implies that if the Markov chain p2 can predict the Markov chain p1 under the state distribution induced by p1, then we can guarantee that the state-wise trajectory distributions from p1 and p2 are close as well. Proof. Denote ρp,s1,...sh as the trajectory distribution induced by pconditioned on the ﬁrst hmany states are equal to s1,...,s h. Denote p(·|s0) as the initial state distribution for s1 with s0 being a faked state. By deﬁnition, we have: ∥ρp1 −ρp2 ∥= ∑ τ |ρp1 (τ) −ρp2 (τ)| = ∑ s1,...sH ⏐⏐⏐⏐⏐ H∏ h=1 p1(sh|sh−1) − H∏ h=1 p2(sh|sh−1) ⏐⏐⏐⏐⏐ = ∑ s1,...sH ⏐⏐⏐⏐⏐ H∏ h=1 p1(sh|sh−1) −p1(s1|s0) H∏ h=2 p2(sh|sh−1) + p1(s1|s0) H∏ h=2 p2(sh|sh−1) − H∏ h=1 p2(sh|sh−1) ⏐⏐⏐⏐⏐ ≤ ∑ s1 p1(s1|s0) ∑ s2,...,sH ⏐⏐⏐⏐⏐ H∏ h=2 p1(sh|sh−1) − H∏ h=2 p2(sh|sh−1) ⏐⏐⏐⏐⏐+ ∑ s1 |p1(s1|s0) −p2(s1|s0)| ( ∑ s2,...,sH H∏ h=2 p2(sh|sh−1) ) = Es1∼p1 [∥ρp1,s1 −ρp2,s1 ∥] + ∥p1(·|s0) −p2(·|s0)∥ ≤Es1,s2∼p1 [∥ρp1,s1,s2 −ρp2,s1,s2 ∥] + ∥p1(·|s0) −p2(·|s0)∥+ Es1∼dπ1;1 [∥p1(·|s1) −p2(·|s1)∥]. Recursively applying the same operation on ∥ρp1,s1 −ρp2,s1 ∥to time step H, we have: ∥ρp1 −ρp2 ∥≤ H∑ h=1 Esh∼dp1;h [∥p1(·|sh) −p2(·|sh)∥] ≤Hδ, where we recall dπ = ∑H h=1 dπ;h/H by deﬁnition. Extension to continuous state can be achieved by simply replaying summation by integration. The next lemma shows that by leveraging the no-regret property of FTL, we can learn a locally accurate model. Lemma A.2 (Local Accuracy of the Learned Model). Denote the sequence of models learned in Alg. 1 as {ˆf1,..., ˆfT}, there exists a model ˆf ∈{ˆf1,..., ˆfT}such that: Es∼dπˆf [ Ea∼U(A(t)) [ DKL(f(t)(·|s,a), ˆf(·|s,a)) ]] ≤O(1/T), where πf(s) ≜ argmina∈A(t) ∥f(·|s,a) −f(s)(·|s,π(s)(s)) for all s∈S for any f.Provably Efﬁcient Model-based Policy Adaptation Proof. Denote loss function ℓe(f) as: ℓe(f) ≜ Es∼d π(t) e ,a∼U(A(t)) [ Es′∼f(t) s,a [−log(f(s′|s,a))] ] . Since Alg. 1 is equivalent to running FTL on the sequence of strongly convex loss functions {ℓe(f)}T e=1, we have (Shalev- Shwartz et al., 2012): T∑ e=1 ℓe( ˆfe) ≤min f∈F T∑ e=1 ℓe(f) + O(log T) . Add ∑T e=1 Es∼d π(t) e ,a∼U(A(t))[Es′∼f(t) s,a log f(t)(s′|s,a)] on both sides of the above inequality and using the deﬁnition of KL divergence, we have: T∑ e=1 Es∼d π(t) e ,a∼U(A(t)) [ DKL(f(t)(·|s,a), ˆfe(·|s,a)) ] ≤min f∈F T∑ e=1 Es∼d π(t) e ,a∼U(A(t)) [ DKL(f(t)(·|s,a),f(·|s,a)) ] + O(log(T)) = O(log(T)), where the last equality comes from the realizability assumption f(t) ∈F. Using the fact that the minimum among a sequence is less than the average of the sequence, we arrive: min ˆf∈{ˆfe}T e=1 Es∼dπˆf ,a∼U(A(t)) [ DKL(f(t)(·|s,a), ˆf(·|s,a)) ] ≤ ˜O(1/T). The above lemma indicates that as T →∞, we will learn a model ˆf which is close to the target true model f(t) under the state distribution induced by πˆf. But it does not state the difference between the behavior generated by πˆf at the target domain and the behavior generated by π(s) at the source domain. The next lemma uses the deﬁnition πˆf to show that when executing πˆf in the target domain M(t), πˆf can actually generates behavior that is similar to the behavior generated by π(s) in the source domain M(s). Lemma A.3 (The Behavior of πˆf). Denote dπˆf as the state distribution induced by πˆf induced at M(t) (target domain). Es∼dπˆf ∥f(t)(·|s,π ˆf(s)) −f(s)(·|s,π(s)(s))∥≤ O(|A(t)|/ √ T) + Es∼dπˆf [ ϵs,π(s)(s) ] , where we recall the deﬁnition of ϵin Assumption 4.1. Proof. Consider the Markov chain that is deﬁned with respect to f(t) and πˆf, i.e., f(t)(s′|s,π ˆf(s)). Denote the state distri-Provably Efﬁcient Model-based Policy Adaptation bution induced by f(t)(s′|s,π ˆf(s)) at the target domain as dπˆf. Let us bound Es∼dπˆf ∥f(t)(·|s,π ˆf(s)) −f(s)(·|s,π(s)(s))∥. Es∼dπˆf ∥f(t)(·|s,π ˆf(s)) −f(s)(·|s,π(s)(s))∥ ≤Es∼dπˆf ∥f(t)(·|s,π ˆf(s)) −ˆf(·|s,π ˆf(s))∥+ Es∼dπˆf ∥ˆf(·|s,π ˆf(s)) −f(s)(·|s,π(s)(s))∥ ≤ √ Es∼dπˆf DKL(f(t)(·|s,π ˆf(s)), ˆf(·|s,π ˆf(s))) + Es∼dπˆf ∥ˆf(·|s,π ˆf(s)) −f(s)(·|s,π(s)(s))∥ ≤O(|A(t)|/ √ T) + Es∼dπˆf ∥ˆf(·|s,π ˆf(s)) −f(s)(·|s,π(s)(s))∥ ≤O(|A(t)|/ √ T) + Es∼dπˆf ∥ˆf(·|s,πf(t) (s)) −f(s)(·|s,π(s)(s))∥ ≤O(|A(t)|/ √ T) + Es∼dπˆf ∥ˆf(·|s,πf(t) (s)) −f(t)(·|s,πf(t) (s))∥ + Es∼dπˆf ∥f(t)(·|s,πf(t) (s)) −f(s)(·|s,π(s)(s))∥ ≤O(|A(t)|/ √ T) + √ Es∼dπˆf DKL(f(t)(·|s,πf(t) (s)), ˆf(·|s,πf(t) (s))) + Es∼dπˆf [ ϵs,π(s)(s) ] = O(|A(t)|/ √ T) + Es∼dπˆf [ ϵs,π(s)(s) ] , where the ﬁrst inequality uses triangle inequality, the second inequality uses Pinsker’s inequality, the third inequality uses the local accuracy of the learned model ˆf (Lemma A.2), the fourth inequality uses the deﬁnition that πˆf, and the ﬁfth inequality uses triangle inequality again, and the sixth inequality uses Pinsker’s inequality again with the deﬁnition of adaptive ability together with the deﬁnition of πf(t) (s) ≜ argmina∼A(t) ∥f(t)(·|s,a) −f(s)(·|s,π(s)(s))∥. Proof of Theorem 4.4. Use Lemma A.1 and Lemma A.3 and consider f(s)(·|s,π(s)(s)) as a Markov chain, we have that: ∥ρt πˆf −ρs πs∥≤ O(H|A(t)|/ √ T) + Hϵ, where we denote ϵ:= Es∼dπˆf [ ϵs,π(s)(s) ] This concludes the proof of Theorem 4.4. A.1. Extension to Continuous Action Space (Proof of Corollary 4.6) For simplicity, we consider A(t) = [0,1].2 We consider Lipschitz continuous transition dynamics with and only with respect to actions, i.e., ∥f(·|s,a) −f(·|s,a′)∥≤ L|a−a′|, (4) where Lis a Lipschitz constant. We emphasize here that we only assume Lipschitz continuity with respect to action in M(t). Hence this is a much weaker assumption than the common Lipschitz continuity assumption used in RL community, which requires Lipschitz continuity in both action and state spaces. We also assume that our function class Fonly contains function approximators that are Lipschitz continuous with respect to action a(e.g., feedforward fully connected ReLu network is Lipschitz continuous). Proof of Corollary 4.6. For analysis purpose, let us discretize the action space into bins with size δ ∈(0,1). Denote the discrete action set ¯A(t) = {0.5δ,1.5δ,2.5δ,..., 1 −0.5δ}(here we assume 1/δ= N+). Here |¯A(t)|= 1/δ. Now consider the following quantity: Es∼dπˆf ∥f(t)(·|s,ˆa) −ˆf(·|s,ˆa)∥, for any ˆa. Without loss of generality, we assume ˆa∈[0,δ]. Via Pinsker’s inequality and Lemma A.2, we have: Es∼dπˆf Ea∼U([0,1])∥f(t)(·|s,a) −ˆf(·|s,a)∥≤ O(1/ √ T), 2We can always normalize action to [0,1].Provably Efﬁcient Model-based Policy Adaptation which implies that: Es∼dπˆf Ea∼U([0,δ])∥f(t)(·|s,a) −ˆf(·|s,a)∥≤ O(1/(δ √ T)). We proceed as follows: Es∼dπˆf Ea∼U([0,δ])∥f(t)(·|s,a) −ˆf(·|s,a)∥ = Es∼dπˆf Ea∼U([0,δ])∥f(t)(·|s,ˆa+ a−ˆa) −ˆf(·|s,ˆa+ a−ˆa)∥ ≥Es∼dπˆf Ea∼U([0,δ]) ( ∥f(t)(·|s,ˆa) −ˆf(·|s,ˆa)∥−2L|a−ˆa| ) = Es∼dπˆf ∥f(t)(·|s,ˆa) −ˆf(·|s,ˆa)∥−2LEa∼U([0,δ])|a−ˆa| ≥Es∼dπˆf ∥f(t)(·|s,ˆa) −ˆf(·|s,ˆa)∥−2LEa∼U([0,δ])δ = Es∼dπˆf ∥f(t)(·|s,ˆa) −ˆf(·|s,ˆa)∥−2Lδ, where the ﬁrst inequality uses the fact that ˆa ∈[0,δ] and the Lipschitz conditions on both f(t) and ˆf ∈F, the second inequality uses the fact that |a−ˆa|≤ δfor any a∈[0,δ] as ˆa∈[0,δ]. Hence, we have: Es∼dπˆf ∥f(t)(·|s,ˆa) −ˆf(·|s,ˆa)∥≤ 2Lδ+ O(1/(δ √ T)) = O(T−1/4),∀ˆa∈A(t), where in the last step we set δ= Θ(T−1/4). Now, we can simply repeat the process we have for proving Lemma A.3, we will have: Es∼dπˆf ∥f(t)(·|s,π ˆf(s)) −f(s)(·|s,π(s)(s))∥≤ O(T−1/4) + ϵ. Combine with Lemma A.1, we prove the corollary for continuous action setting. For general n-dim action space, our bound will scale in the order O(√nT−1/(2n+2)) + ϵ. The proof of the n-dim result is similar to the proof of the 1-d case and is included below for completeness: Proof for n-dim Action Space. For n-dimensional action space, we have: Es∼dπˆf Ea∼U([0,δ]n)∥f(t)(·|s,a) −ˆf(·|s,a)∥≤ O(1/(δn√ T)). Using the Lipschitz property: Es∼dπˆf Ea∼U([0,δ]n)∥f(t)(·|s,a) −ˆf(·|s,a)∥ = Es∼dπˆf Ea∼U([0,δ]n)∥f(t)(·|s,ˆa+ a−ˆa) −ˆf(·|s,ˆa+ a−ˆa)∥ ≥Es∼dπˆf Ea∼U([0,δ]n) ( ∥f(t)(·|s,ˆa) −ˆf(·|s,ˆa)∥−2L∥a−ˆa∥ ) = Es∼dπˆf ∥f(t)(·|s,ˆa) −ˆf(·|s,ˆa)∥−2LEa∼U([0,δ]n)∥a−ˆa∥ ≥Es∼dπˆf ∥f(t)(·|s,ˆa) −ˆf(·|s,ˆa)∥−2LEa∼U([0,δ]n) √nδ = Es∼dπˆf ∥f(t)(·|s,ˆa) −ˆf(·|s,ˆa)∥−2L√nδ, Combining with above leads to Es∼dπˆf ∥f(t)(·|s,ˆa) −ˆf(·|s,ˆa)∥≤ 2√nLδ+ O(1/(δn√ T)) = O(√nT−1/(2n+2)) where in the last step we set δ= Θ((T n) −1 2(n+1) ). Finally we have: Es∼dπˆf ∥f(t)(·|s,π ˆf(s)) −f(s)(·|s,π(s)(s))∥≤ O(√nT−1/(2n+2)) + ϵ.Provably Efﬁcient Model-based Policy Adaptation B. Detailed description of our experiments B.1. Environment descriptions. For each of the four environments (HalfCheetah, Ant, Reacher) we tested on, we include a detailed numerical description of the environments in table 2. B.2. Descriptions of the perturbations B.2.1. C HANGING GRAVITY . We change the gravity in the whole Mujoco locomotion task by a max range from 50% to 200% of the normal gravity. The normal gravity is set to 0 on xand yaxis and −9.81 on zaxis. B.2.2. C HANGING MASS . We change the mass of the agent in the Mujoco locomotion task by a max range from 50% to 200% of its original mass. Note that most agents are composed of links with independent masses, so besides changing the agent’s mass as a whole, we also design experiments that change the mass of each individual links of the agent respectively. B.2.3. C HANGING PLANE ORIENTATION . In the Reacher task, we tilt the platform of the Reacher so that it forms an angle of 45 degree of the original plane. B.2.4. C HANGING ARM LENGTH . In the Reacher task, we change the ﬁrst link of the Reacher arm (which is composed of two parts) into one tenth of its original length. B.2.5. C HANGING FRICTION . We change the frictional coefﬁcient in the environment on an uniform scale. We found if friction is the only change in the target domain, then the adaptation task is relatively simple, so we incorporate it as one of the changes in the Multi-dimensional perturbation task. B.2.6. M OTOR NOISE . This task tries to mimic the motor malfunction or inaccuracy in the real world. After the agent outputs an action, we add on a noise from a normal distribution with mean 0 and a ﬁxed standard deviation from 0.2 to 1. B.2.7. M ULTIPLE DOF S OF CHANGES . In the 3DOF task, we set the gravity to 0.8, mass to 1.2 and friction coefﬁcient to 0.9. In the 15DOF task, we uniformly sample a coefﬁcient from 0.9 to 1.1 for each of the following conﬁguration: gravity, friction coefﬁcient and the mass of each joint of the agent. We record these changes and apply for all comparing methods. Figure 6.Visual illustration of modiﬁed Reacher environment. left: 45 degrees of tilted plane. right: Reacher with 10% length of its ﬁrst arm.Provably Efﬁcient Model-based Policy Adaptation Environment name Full state space size Model agnostic state size3 Action space size Reward function HalfCheetah 18 1 6 forward reward - 0.1 ×control cost Ant 29 2 8 forward reward - 0.5 ×control cost - 0.0005 ×contact cost + survive reward Reacher 16 4 2 forward distance - control cost Table 2.Description of the OpenAI gym environments. Note that to enforce safety of the agent in the target environment, we make a modiﬁcation on HalfCheetah environment such that the episode will be terminated when the cheetah falls off. B.3. Hyperparameters source PADA-DM PADA-DM with target Christiano et al., 2016 Zhu et al., 2018 PPO # timesteps 2e6 8e4 (5e4,8e4,12e4,15e4) 8e4 (5e4,8e4,1.2e5,1.5e5) 2e5 (1e5,2e5,4e5) 2e6 2e6 learning rate (with linear decay) 7e-4 5e-3 5e-3 5e-3 7e-4 7e-4 soft update rate every 3000 timesteps (3000,5000,10000) explore rate ϵ 0.01 (0.01,0.02) 0.01 (0.01,0.02) reward tradeoff λ 0.5 Table 3.Final hyperparameters we use for our methods and baselines in the experiments. The values in the brackets are the value we considered. C. Implementation details C.1. Pretraining of the source dynamics model In section 5.1, one assumption we make is that we have a pretrained model ˆf(s) that well approximates the source dynamics f(s). In practice, we pretrain the model ˆf(s) with the (s,a,s ′) triplets generated during the training ofπ(s). The model ˆf(s) is a two-layer neural network with hidden size 128 and ReLU nonlinearity, trained with MSE loss since we assume deterministic transitions. Using these existing samples has two major advantages: ﬁrst is that we don’t need further interaction with the source environment and second is that the trained model ˆf(s) especially well approximates the transitions around the actions taken by π(s), which is important to our algorithm. Remark that if we already have the ground truth source dynamics f(s), which is a mild assumption while using a simulator, we can also directly use f(s) to replace ˆf(s). During our experiments, we observe that whether using f(s) or ˆf(s) won’t affect the performance of our method. C.2. Cross Entropy Method Here we provide a pseudocode of the Cross Entropy Method that we used in our method, as in Alg. 3. In our implementation, we use T = 10 iterations, N = 200 actions sampled per iteration, K = 40 elites (elite ratio 0.4) and σ0 = 0.5. We use the output of target policy as the initial mean, and when we don’t have the target policy, we useπ(s)(s) as the initial mean. To avoid invalid actions, for each actionai we sample, we clip the action if certain dimension is out of the bound of A(t) (usually [-1,1] on each dimension). 3This means the number of states that won’t be passed as inputs to models or policies, e.g., the current coordinate or location of the agent.Provably Efﬁcient Model-based Policy Adaptation Algorithm 3 Cross Entropy Method Require: Initial mean µ0, initial standard deviation σ0, action space A(t), current model δθ, current state s, number of iterations T, sample size N, elite size K. 1: Σ0 ←I|A(t)|(σ2 0) 2: for t = 1, . . . Tdo 3: Sample {ai}N i=1 ∼N(µt−1,Σt−1) 4: {ai}N i=1 ←clip(ai,min(A(t)),max(A(t))) 5: Sort {ai}with descending ∥δθ(s,ai)∥2 2 and pick the ﬁrst K actions {aj}K j=1 6: µt ← 1 KΣK j=1aj 7: Σt ← 1 KΣK j=1(aj −µt)(aj −µt)T 8: end for 9: Output: µT D. Supplemental experiments D.1. Accuracy of Deviation Model We evaluate the performance of the deviation model by comparing its output with the actual deviation (i.e.,∆π(s) ) in the target and source environment states. We compare the performance between linear and nonlinear deviation models. We include linear models as they are convenient if we want to use optimal control algorithms. The nonlinear model is the same model we use for our PADA-DM algorithm, which has two 128-neuron layers with ReLU (Nair & Hinton, 2010) nonlinearity. Both of the deviation models are tested on the same initial state distribution after training on 80k samples. In 7(left), we plot the output of the deviation model against the ground truth deviation. We test on 50 trajectories and each data point refers to the average L2 state distance along one trajectory. In 7(right), we plot the ground truth deviation, and the outputs of the linear and nonlinear deviation models over time, on 10 test trajectories. 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 DM output 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00real dynamics divergence linear DM nonlinear DM Figure 7.Comparing the performances of the linear and nonlinear deviation models. left: This plot depicts the correlation between the predicted deviation and the actual deviation. The nonlinear deviation model is more accurate since its slope is closer to 1. right: This plot shows the predicted and actual deviation over the course of 10 trajectories. Here again, the nonlinear model (orange) curve lies very close to the actual deviation curve (green). D.2. Long-term learning curves In this section we show a more comprehensive long-term learning curve in Fig. 8. Each task here corresponds to the task in Fig. 2. Note that here again the x-axis is in natural logarithm scale.Provably Efﬁcient Model-based Policy Adaptation 7 8 9 10 11 12 13 14 Number of Timesteps (in log) 1000 0 1000 2000 3000 4000 5000 Rewards HalfCheetah 150% mass 7 8 9 10 11 12 13 14 Number of Timesteps (in log) 0 1000 2000 3000 4000Rewards HalfCheetah 50% Gravity 7 8 9 10 11 12 13 14 Number of Timesteps (in log) 1000 500 0 500 1000 1500 2000 2500 Rewards HalfCheetah 0.4 std Motor Noise 7 8 9 10 11 12 13 14 Number of Timesteps (in log) 300 250 200 150 100 50 0 Rewards Reacher-v2 10% first arm length 7 8 9 10 11 12 13 14 Number of Timesteps (in log) 0 500 1000 1500 2000 2500 3000Rewards Ant 50% Mass 7 8 9 10 11 12 13 14 Number of Timesteps (in log) 2000 1000 0 1000 2000 3000 Rewards Ant 200% Gravity 7 8 9 10 11 12 13 14 Number of Timesteps (in log) 1500 1000 500 0 500 1000 Rewards Ant 0.6 std Action Noise 7 8 9 10 11 12 13 14 Number of Timesteps (in log) 250 200 150 100 50 0 Rewards Reacher-v2 45 degree plane Figure 8.We plot the learning curves across 5 random seeds of our adaptation method (using PADA-DM and PADA-DM with target), and the baseline methods on a number of tasks including perturbation of the mass, gravity, motor noise for the locomotion environments (HalfCheetah and Ant), and the plane angle and arm lengths for the navigation environment (Reacher). The title of each plot corresponds to the perturbation in the target domain, e.g., HalfCheetah Mass 150% means in mass of the agent in the target domain is 150% of that in the source domain. The shaded area denotes 1 standard deviation range from the mean. D.3. Comparing using true reward To further verify the efﬁciency of our choice of reward (the deviations between two environments), we conduct an additional experiments where we use the ground truth reward for CEM. We ﬁx all the hyperparameters and train an additional model to learn to ground truth reward. To ensure the fairness of the comparison, when we use the ground truth reward, we keep doing 1 step look-ahead during CEM. The results verify that the deviation serves as a better reward for conducting policy adaptation, where the ground truth reward leads to a local minimum that results in suboptimal performance. 0 1 2 3 4 5 6 7 8 Number of Timesteps 1e4 0 1000 2000 3000 4000Rewards HalfCheetah 150% Mass PADA-DM PADA-DM w/ target true reward Figure 9.Comparing learning ground truth reward, learning deviations quickly adapts the policy in the target domain, where using ground truth reward may not necessarily leads to optimal performance in the target domain. D.4. Additional Experiments for Meta-Learning MAML In this section we conduct an additional experiment to Section 6.3. In section 6.3, we use 80k samples of adaptation for our method and MAML to conduct a fair comparison. However, using so many samples for adaptation contradicts MAML’s claim of few-shot adaptation and we also observe that MAMLs test performance does not improve too much as we change the sample size in adaptation phase. Thus here we report the additional experiments to support this claim: we adopt the same experiment setting in Section 6.3, and this time we use 20k samples for MAML during adaptation. The test performanceProvably Efﬁcient Model-based Policy Adaptation is recorded in Fig. 10(a) and Fig. 11(a). Comparing with the original performance (Fig. 10(b) and Fig. 11(b)), the test performance of MAML does not change that much as the number of adaptation samples decreases and our approach still outperforms MAML consistently. In addition, we record the mean and the standard deviation of the test performance of each method to deliver a more direct comparison in Table 4 and Table 5. As we can see, our approach outperforms other baselines most of the time. When the perturbation is small (e.g., the 120% columns in both tables), DR also delivers very strong performances. However when perturbation is large (e.g., 200% columns in both tables), DR fails to adapt completely, which indicates that DR has troubles to adapt to out-of-distribution new environments. 50% 80% 120% 150% 200% Gravity 0 2000 4000 6000Rewards algorithm MAML DR source policy PADA-DM PADA-DM w/ DR (a) 50% 80% 120% 150% 200% Gravity 0 2000 4000 6000Rewards algorithm MAML DR source policy PADA-DM PADA-DM w/ DR (b) Figure 10.Ablation experiments using domain randomization and meta-learning. (a) MAML with 20k adaptation samples. (b) MAML with 80k adaptation samples. The boxplots show the median of the data while more statistics such as mean and standard deviation are shown in the following tables. 50% 80% 120% 150% 200% Mass 2000 0 2000 4000 6000 Rewards algorithm MAML DR source policy PADA-DM PADA-DM w/ DR (a) 50% 80% 120% 150% 200% Mass 2000 0 2000 4000 6000 Rewards algorithm MAML DR source policy PADA-DM PADA-DM w/ DR (b) Figure 11.Ablation experiments using domain randomization and meta-learning. (a) MAML with 20k adaptation samples. (b) MAML with 80k adaptation samples.Provably Efﬁcient Model-based Policy Adaptation Gravity Perturbation 50% 80% 120% 150% 200% Source policy 1549.74 (1090.73) 4444.86 (637.77) 4592.18 (201.30) 2543.55 (916.16) -156.51 (25.93) Domain Randomization 2282.74 (1563.70) 4838.87 (1134.98) 5236.46 (1179.85) 2896.03 (554.00) -43.97 (423.47) PADA-DM 2694.78 (1166.88) 4739.32 (279.06) 4889.02 164.38) 2998.32 (266.75) 1531.23 (400.96) PADA-DM w/ DR 3230.29 (1280.54) 5036.59 (657.98) 4934.04 (720.34) 3200.73 (521.50) 1431.53 (496.11) MAML (20k) 854.24 (692.50) 1810.51 (663.72) 1895.86 (650.76) 1575.06 (653.09) 831.07 (717.78) MAML (80k) 876.37 (711.98) 1778.67 (669.86) 1894.28 (644.55) 1568.60 (646.79) 823.13 (721.15) Table 4.Mean and standard deviation (in the brackets) of the episodic rewards of each method in the target environment with perturbed gravity across 100 trajectories of 5 random seeds (500 trajectories in total). Mass Perturbation 50% 80% 120% 150% 200% Source policy 921.13 (1192.43) 3343.05 (2317.32) 4166.10 (494.94) 2045.26 (665.30) -149.92 (27.28) Domain Randomization 1665.05 (1357.31) 3823.45 (1944.70) 3932.86 (1791.18) 2635.72 (1105.15) 944.50 (1134.32) PADA-DM 3271.52 (752.62) 4914.67 (379.73) 4584.95 375.51) 3557.25 (183.46) 1398.88 (500.09) PADA-DM w/ DR 2673.87 (1009.75) 5348.98 (556.19) 4854.30 (591.50) 3276.70 (874.54) 1616.75 (490.53) MAML (20k) 854.24 (692.50) 1810.51 (663.72) 1895.86 (650.76) 1575.06 (653.09) 831.07 (717.78) MAML (80k) 876.37 (711.98) 1778.67 (669.86) 1894.28 (644.55) 1568.60 (646.79) 823.13 (721.15) Table 5.Mean and standard deviation (in the brackets) of the episodic rewards of each method in the target environment with perturbed mass across 100 trajectories of 5 random seeds (500 trajectories in total).",
      "meta_data": {
        "arxiv_id": "2006.08051v1",
        "authors": [
          "Yuda Song",
          "Aditi Mavalankar",
          "Wen Sun",
          "Sicun Gao"
        ],
        "published_date": "2020-06-14T23:16:20Z",
        "pdf_url": "https://arxiv.org/pdf/2006.08051v1.pdf"
      }
    },
    {
      "title": "Real-Time Optimisation for Online Learning in Auctions",
      "abstract": "In display advertising, a small group of sellers and bidders face each other\nin up to 10 12 auctions a day. In this context, revenue maximisation via\nmonopoly price learning is a high-value problem for sellers. By nature, these\nauctions are online and produce a very high frequency stream of data. This\nresults in a computational strain that requires algorithms be real-time.\nUnfortunately, existing methods inherited from the batch setting suffer\nO($\\sqrt t$) time/memory complexity at each update, prohibiting their use. In\nthis paper, we provide the first algorithm for online learning of monopoly\nprices in online auctions whose update is constant in time and memory.",
      "full_text": "Real-Time Optimisation for Online Learning in Auctions Lorenzo Croissant 1 Marc Abeille 1 Cl´ement Calauz`enes 1 Abstract In display advertising, a small group of sellers and bidders face each other in up to 1012 auc- tions a day. In this context, revenue maximisa- tion via monopoly price learning is a high-value problem for sellers. By nature, these auctions are online and produce a very high frequency stream of data. This results in a computational strain that requires algorithms be real-time. Unfortunately, existing methods inherited from the batch setting, suffer O( √ t) time/memory complexity at each update, prohibiting their use. In this paper, we provide the ﬁrst algorithm for online learning of monopoly prices in online auctions whose update is constant in time and memory. Introduction Over the last two decades, online display advertising has become a key monetisation stream for many businesses. The market for the trading of these ads is controlled by a very small ( < 10) number of large intermediaries who buy and sell at auction, which means that a seller-buyer pair might trade together in 1010 to 1012 auctions per day. Repeated auctions on this scale raise the stakes of rev- enue maximisation, while making computational efﬁciency a key consideration. In his 1981 seminal work on revenue maximisation, Myerson described the revenue-maximising auction when buyers’ bid distributions are known. In the context of online display ads these distributions are private, but the large volume of data collected by sellers on buyers opens the way to learning revenue maximising auctions. Optimal vs. tractable. The learning problem associated with the Myerson auction has inﬁnite pseudo-dimension (Morgenstern & Roughgarden, 2015), making it unlearn- able (Pollard, 1984). 2 nd-price auctions with personalised reserve prices ( i.e. different for each bidder) stand as the commonly accepted compromise between optimality and tractability. They provide a 2-approximation (Roughgar- 1Criteo AI Lab, Paris, France. Correspondence to: Lorenzo Croissant <ld.croissant@criteo.com>. den & Wang, 2016) to the revenue of the Myerson auction while securing ﬁnite pseudo-dimension. Monopoly prices. 2nd-price auctions with personalised reserves can be either eager or lazy. In the eager for- mat, the item goes to the highest bidderamongst those who cleared their reserve prices and goes unsold if none of them did. In the lazy format, the item goes to the highest bidderif he cleared his reserve price and goes unsold if he did not. While an optimised eager version leads to better revenue than an optimised lazy version, solving the eager auction’s associated Empirical Risk Minimisation (ERM) problem is NP-hard (Paes Leme et al., 2016) and even APX-hard (Roughgarden & Wang, 2016). In contrast, solving the ERM for the lazy version can be done in polynomial time (Roughgarden & Wang, 2016): it amounts to computing a bidder-speciﬁc quantity called the monopoly price. Not only is the monopoly price the optimal reserve in the lazy 2nd-price auction, but it is also a provably good reserve in the eager one (Roughgarden & Wang, 2016), and the opti- mal reserve in posted-price (Paes Leme et al., 2016). This makes learning monopoly prices for revenue maximisation an important and popular research direction. Online learning. Finding the monopoly price in a re- peated 2 nd-price auction is a natural sequential decision problem based on the incoming bids. All three afore- mentioned settings relating to the monopoly price have been studied: posted-price (Amin et al., 2014; Blum et al., 2004), eager (Cesa-Bianchi et al., 2014; Roughgarden & Wang, 2016; Kleinberg & Leighton, 2003), and lazy which we study (Blum & Hartline, 2005; Blum et al., 2004; Mohri & Medina, 2016; Rudolph et al., 2016; Bubeck et al., 2017; Shen et al., 2019). Each setting also corresponds to a differ- ent observability structure. The ofﬂine problems are well understood, but no online method offers theO(1) efﬁciency crucial for real-world settings. We focus, therefore, on the key problem of learning monopoly prices, online and efﬁ- ciently, in the stationary and non-stationary cases. Structure of the paper. We propose a real-time ﬁrst- order algorithm which makes online learning of monopoly prices computationally feasible, when interacting with sta- tionary and non-stationary buyers. In Sec. 1, we detail the setting and problem we consider. We review, in Sec. 2, arXiv:2010.10070v1  [cs.LG]  20 Oct 2020Real-time Optimisation for Online Learning in Auctions the existing approaches and stress the challenges of the problem including overcoming computational complexi- ties. Our approach, based on convolution and the O(1) Online Gradient Ascent algorithm, is described in Sec. 3. We study performance for stationary bidders in Sec. 4 with 1/ √ tconvergence rate to the monopoly price, and for non- stationary bidders in Sec. 5 with O( √ T) dynamic regret. 1. Setting A key property of a personalised reserve price in a lazy second price auction is that it can be optimised separately for each bidder (Paes Leme et al., 2016). For a bidder with bid cdf F, the optimal reserve price is the monopoly price r∗, i.e. the maximiser of the monopoly revenue deﬁned as ΠF(r) = r(1 −F(r)) . (1) Thus, without loss of generality, we study each bidder sep- arately in the following repeated game: the seller sets a reserve price rand simultaneously the buyer submits a bid b∈[0,¯b] drawn from his private distribution F, whose pdf is f. The seller then observes bwhich determines the in- stantaneous revenue p(r,b) = r1r≤b with EF[p(r,b)] = ΠF(r) . (2) In this work, we consider two settings, depending whether the bid distribution is stationary or not. Stationary setting. Stationarity here meansF is ﬁxed for the whole game. We thus have a stream of i.i.d. bids from F, where the seller aims to maximise her long term revenue lim T→∞ 1 T T∑ t=1 p(r,bt) = ΠF(r) . Or, equivalently, tries to construct a sequence of reserve prices {rt}t≥1 given the information available so far en- coded in the ﬁltration Ft = σ(r1,b1,...,b t−1) such that ΠF(rt) →ΠF(r∗) as fast as possible. Non-stationary setting. In real-world applications, bid distributions may change over time based on the current context. For example, near Christmas the overall value of advertising might go up since customers spend more read- ily, and thus bids might increase. The bidder could also refactor his bidding policy for reasons entirely independent of the seller. We relax the stationarity assumption by al- lowing bids to be drawn according to a sequence of dis- tributions {Ft}t≥1 that varies over time. As a result, the monopoly prices {r∗ t}t≥1 and optimal monopoly revenues {ΠFt(r∗ t)}t≥1 ﬂuctuate and convergence is no longer de- ﬁned. Instead, we evaluate the performance of an adaptive sequence of reserve prices by its expected dynamic regret R(T) = E ( T∑ t=1 ΠFt(r∗ t) −ΠFt(rt) ) , (3) and our objective is to track the monopoly price as fast as possible to minimise the dynamic regret. 2. Related work, challenges and contributions 2.1. Related Work Lazy 2nd-price auctions have been studied both in batch (Mohri & Medina, 2016; Shen et al., 2019; Rudolph et al., 2016; Paes Leme et al., 2016) and online (Blum & Hart- line, 2005; Blum et al., 2004; Bubeck et al., 2017) settings. All existing approaches aim to optimise, at least up to a precision of 1/ √ t, the ERM objective Π ˆFt(r) = r(1 −ˆFt(r)) = 1 t t∑ i=1 r1r≤bi. (4) However, regardless of how well-behaved ΠF is, Π ˆFt is very poorly behaved for optimisation: it is non-smooth, non-quasi-concave, discontinuous, and is increasing every- where (see Fig.1, center, dashed). Thus direct optimisation with ﬁrst order methods is not applicable. Some attempts have been made in the batch setting to optimise surrogate objectives, but ended up with an irreducible bias (Rudolph et al., 2016) or with hyper-parameters whose tuning is as hard as the initial problem (Shen et al., 2019). The classi- cal approach relies on sorting the bids{bi}t i=1 to be able to enumerate Π ˆFt(.) linearly over {bi}t i=1 (Mohri & Medina, 2016; Paes Leme et al., 2016). A popular improvement in terms of complexity, especially used in online approaches (Blum et al., 2004; Blum & Hartline, 2005) consists in ap- plying the same principle on a regular grid of resolution 1/ √ t, which in the end provides an update with complex- ity O( √ t) and a memory requirement of O( √ t). This idea of discretising the bid space was widely adopted in partially observable settings –e.g. online eager or online posted-price auctions – as it reduces the problem to a multi- armed bandit with its well-studied algorithms (Kleinberg & Leighton, 2003; Cesa-Bianchi et al., 2014; Roughgarden & Wang, 2016) at the price of still suffering the same update and memory complexities of O( √ t). Numerous approaches with adversarial bandits also fol- lowed this discretisation approach to adapt Exp3/Exp4 (Kleinberg & Leighton, 2003; Cohen et al., 2016; Bubeck et al., 2017) to all the settings. As a bonus, it also al- lows to handle the case of non-stationary bidders. How- ever, the work of Amin et al. (2014) stresses that bidders cannot behave in an arbitrary way, as they optimise theirReal-time Optimisation for Online Learning in Auctions own objective that is not incompatible with the seller’s 1. Hence, the non-stationarity mostly comes from the item’s value changing over time. This suggest adapting a regu- lar stochastic algorithms (ERM, UCB...), e.g. using slid- ing windows (Garivier & Moulines, 2011; Lattimore & Szepesv´ari, 2018). Non-smooth or non-differentiable objectives such as Π ˆF have been studied in both stochastic and 0-order optimisa- tion. In both, convolution smoothing has been employed to circumvent these problems. In Duchi et al. (2012) stochas- tic gradient with decreasing convolution smoothing is stud- ied for the convex case. Unfortunately, very few distribu- tions yield a concave ΠF. In 0-order optimisation, the only feedback received for an input is the value of the objective at that input. In this setting, Flaxman et al. (2005) perturb their inputs to estimate a convolved gradient. In contrast, we obtain a closed form and do not need to perturb inputs. 2.2. Challenges The directing challenge of our line of work is to devise an online learning algorithm for monopoly prices with mini- mal cost, to handle very large real-world data streams. With 1010 daily interactions in one seller-bidder pair, it is accept- able to forfeit some convergence speed in exchange for fea- sibility of the algorithm. It is not possible to accept update complexity or memory requirement scaling witht. Our ob- jective is thus to ﬁnd a method that 1) converges to r∗in the stationary setting or has a low regret R(T) in the non- stationary one, 2) has O(1) memory footprint, and 3) com- putes the next reserve rt+1 with O(1) computations. Computational Complexity. Unfortunately, none of the previously proposed methods ﬁt these requirements. On one hand, all methods based on solving ERM by sorting (Cesa-Bianchi et al., 2014; Roughgarden & Wang, 2016) need to keep all past bids in memory ( O(t) dependency) and their update steps require at best O( √ t) computations. On the other hand, adversarial methods such as Exp3 or Exp4 (Cohen et al., 2016; Bubeck et al., 2017) are designed for ﬁnite action space and thus need to discretise [0,¯b] into√ tintervals (to keep their regret guarantees), also leading to a complexity of O( √ t) from sampling to compute rt+1. Gradient bias. First order methods ( e.g. Online Gradi- ent Ascent a.k.a. OGA) are standard tools in online learn- ing and enjoy O(1) update and memory. This makes them great candidates for our problem. OGA requires 3 ingredi- ents to converge: an objective whose gradients always point towards the optimum2, a gradient estimator with bounded 1An auction is not a zero-sum game: if the item goes unsold, neither player receives payoff. 2Pseudo-concave or variationally coherent. variance, and which is unbiased. Unfortunately, disconti- nuity of pmakes ∇pa biased estimator of ∇ΠF. A natural approach is to construct a surrogate for pwhich has unbi- ased gradients and preserves the other two conditions. Surrogate consistency. Optimising a surrogate objective inherently creates a bias, which has to be reduced over- time. To do so without breaking the convergence of OGA, we must conduct a careful ﬁnite time analysis of the al- gorithm, which is an analytical challenge. We must re- analyse classical results ( e.g. Bach & Moulines (2011); Duchi et al. (2012)) for varying objectives: the challenge is to design a bias reduction procedure, and then integrate it into these proofs to show we preserve consistency. Non-stationarity. Resolving the above challenges is suf- ﬁcient to achieve efﬁcient convergence in the stationary set- ting. However, it is not sufﬁcient in order to track non- stationary bid distributions. Taking a constant surrogate and learning rate, it is possible to adapt the stationary solu- tion to the non-stationary case and keep its computational efﬁciency. The challenge is to devise this adaptation, and then to derive (sub-linear) regret for it. 2.3. Contributions We propose a smoothing method for creating surrogates in pseudo-concave problems with biased gradients. We use it to create a ﬁrst-order real-time optimisation algorithm which reduces the surrogate’s bias during optimisation. We prove convergence and give rates in the stationary setting and dynamic regret bounds for tracking. In more detail: Smooth surrogates for ﬁrst order methods. We ﬁrst translate standard auction theory assumptions (e.g. increas- ing virtual value) into properties of generalised concavity of the monopoly revenue (Prop. 1). Next, we introduce our smoothing method and show (in Prop. 2) that it pre- serves the properties from Prop. 1 while offering arbitrary smoothness, which ﬁxes the biased gradient problem. Fi- nally, we provide controls, via the choice of the kernel, on the bias and variance of the gradient estimates of our sur- rogate, which is now ready for OGA (Prop. 3). Consistent algorithm for stationary bidders. We con- struct an algorithm (V-C ONV-OGA) which performs gra- dient ascent while simultaneously decreasing the strength of the smoothing over time, reducing the bias to zero. As a result our algorithm almost surely converges to the monopoly price (Thm. 1) while enjoying computational ef- ﬁciency. Further, under a minimum curvature assumption, we provide the rate of convergence and optimal tuning pa- rameters (Thm. 2 and Cor. 1). At the cost of a slight degra- dation in convergence speed (from O(1/t) to O(1/ √ t)),Real-time Optimisation for Online Learning in Auctions our algorithm has update and memory complexity of O(1) which is vital for real-world applications. Results are sum- marised in Table 1. Update Memory Convergence ERM O(t) O(t) O(1/t) Discrete ERM O( √ t) O( √ t) O(1/t) V-CONV-OGA O(1) O(1) O(1/ √ t) Table 1.Comparison of our method (V-CONV-OGA) against do- ing ERM at each step and ERM discretised on a grid of res- olution 1/ √ t, in terms of complexity and convergence – i.e. ∥ΠF (rt) −ΠF (r∗)∥. Tracking for non-stationary bidders. Contrary to the stationary setting, when tracking we do not decrease the strength of the smoothing over time. When the bias created is smaller than the noise, our algorithm can still achieve sub-linear dynamic regret when tracking changing bid dis- tributions. For reasonably varying distribution, we show a regret bound of O( √ T) (see Thm. 3 and Cor. 2). 3. Smooth Surrogate for First Order Methods Our objective, to reiterate, is to design an online optimisa- tion procedure to learn or track the optimal reserve price whose updates require O(1) computational and memory cost. To this end, we focus on ﬁrst order method and con- sider vanilla Online Gradient Ascent. Unfortunately, the speciﬁc problem of learning a monopoly price doesn’t pro- vide a way to compute unbiased gradient estimates for ΠF from bid samples. We therefore want to design a surro- gate that makes psufﬁciently smooth so that differentiation and integration commute. This is a well known property of convolutional smoothing, suggesting its use. In addition, we must ensure our surrogate preserves the optimisation properties that ΠF already has. These must thus be studied ﬁrst, before smoothing to obtain a surrogate. 3.1. Properties of the monopoly revenue The standard assumptions of auction theory are made to guarantee that the monopoly price exists – and that the op- timisation problem is well-posed. This is generally stated as “the monopoly revenue isquasi-concave”. We reﬁne this characterisation by translating the assumptions we make into speciﬁc concavity properties of the monopoly revenue in Prop 1. (A1). F ∈C2( [0,¯b] ) and f = F′>0 on (0,¯b). (A2). F is strictly regular on its domain of deﬁnition i.e. the virtual value ψ(b) = b−1−F(b) f(b) is increasing. (A3). F has strongly increasing hazard rate on its domain i.e. the hazard rate λ(b) = f(b) 1−F(b) satisﬁes: ∀0 ≤b1 ≤b2 ≤¯b, λ(b2) −λ(b1) ≥µ(b2 −b1). (A1) is made for ease of exposition. (A2) is a standard auc- tion theory assumption (see Krishna (2009) for a review), and implies a pseudo-concave revenue, as shown by Prop 1. (A2) is satisﬁed by common distributions, exhaustively listed in (Ewerhart, 2013) and by real-world data – see e.g. (Ostrovsky & Schwarz, 2011). (A3) strengthens (A2) by requiring a minimum curvature around the maximum. Proposition 1. Let F satisfy (A1) and ΠF be its associated monopoly revenue. Then, ΠF ∈ C2( [0,¯b] ) , ΠF > 0 on (0,¯b) and: • under (A2), ΠF is strictly pseudo-concave, • under (A3), ΠF is µ-strongly log-concave, i.e. log ΠF is µ-strongly concave on (0,¯b). 3.2. A Method Based on Smoothing Prop. 1 ensures that the ﬁrst condition for the convergence of OGA is met under standard assumptions (A2) or (A3). The main difﬁculty in the way of using OGA for revenue optimisation – it must be stressed – lies in the undesirable shape of the instantaneous revenue p. Indeed, p is non- smooth (discontinuous even) and cannot be used to con- struct an unbiased estimate of ∇ΠF(r), which is necessary for ﬁrst order-methods. Mohri & Medina (2016) suggests replacingp(·,b) by a con- tinuous upper bound. This surrogate can be used for OGA, but it has potentially large areas of zero-gradient, which means it doesn’t learn from all samples. We give a general surrogate construction (based on convolutional smoothing) which 1) can approximate the original monopoly revenue to arbitrary accuracy, 2) preserves the concavity properties of ΠF, 3) offers the desired level of smoothness, and 4) exhibits no areas of zero gradient. Formally, given a kernel k(a metaparameter), we use con- volution smoothing to create surrogates for pand ΠF: pk(r,b) = (p(·,b) ⋆k)(r) , ΠF k(r) = (ΠF ⋆k)(r). (5) This smoothing guarantees that ∇pk(·,b) is an unbiased estimate of ∇ΠF k. On Fig. 1, we illustrate the effect of this smoothing on p, Π ˆFt, and ΠF. We introduce a set Kof admissible kernels which contains all strictly posi- tive, strictly log-concave, C1(R), L1(R), normalised ( i.e.∫ R k(x)dx = 1 ) functions. Kcontains a large family of kernels, including standard smoothing ones such as Gaus- sians and molliﬁers. Prop. 2 shows that convolution with elements of Kpreserves pseudo- and log-concavity.Real-time Optimisation for Online Learning in Auctions r p (r, 0.7) pk (r, 0.7) r Π ˆF t (r) Π ˆF k,t(r) r r∗r∗ k Π F(r) Π F k(r) Figure 1.The effect of smoothing the monopoly revenue of a bidder withF a Kumaraswamy4(1,0.4) distribution with a Gaussian kernel. Left: smoothing of p(r,b) for b = 0.7. Center: smoothing of the empirical revenue Π ˆF t (r) for some random bt. Right: smoothing of the expected revenue (ΠF k vs ΠF ). Note the difference in the optima (i.e. the surrogate bias). Proposition 2. Let F satisfy (A1) and ΠF be its associated monopoly revenue. Let k∈K, then: • ΠF k and pk are C1(R), • ΠF k(r)=EF ( pk(r,b) ) and ∇ΠF k(r)=EF ( ∇pk(r,b) ) , • under (A2), ΠF k is strictly pseudo-concave on R, • under (A3), ΠF k is strictly log-concave on R. Proof. See App. B.2. Algorithm 1: CONV-OGA input: r0, {γt}t∈N, k∈K, C⊂ [0,¯b] for t= 1 to +∞do observe bt rt ←projC(rt−1 + γt∇pk(rt−1,bt)) Prop. 2 guarantees that the surrogate satisﬁes the pseudo- concavity and unbiased gradient conditions of OGA. Ap- plying OGA to the surrogate ΠF k gives Alg. 1. Note that as a property of convolution, ∇pk(·,b) = p(·,b) ⋆∇k, which is a simple (generally closed-form) computation. Prop. 3 will show that the bounded variance condition of OGA also holds. Since OGA’s three conditions are satis- ﬁed, we can guarantee convergence to the maximum r∗ k of ΠF k (see e.g. (Bottou, 1998)). However, in generalr∗ k is not the monopoly price r∗and the surrogate is biased. Prop. 3 also gives a control on this bias in terms of the L1 distance between the cdf K of kand the cdf 1R+ of the Dirac mass δ0, which is the only kernel to guarantee r∗ δ0 = r∗. Proposition 3. Let F satisfy (A1), k ∈ K. Let r∗ and r∗ k be the monopoly prices associated with ΠF and ΠF k. Then, the bias Bk = |ΠF(r∗) −ΠF(r∗ k)|and the 4This distribution satisﬁes our concavity assumptions and can display highly eccentric behaviour for easy visualisation of the impact of the surrogate. instantaneous convolved gradient second moment Vk = maxr≥0 Eb∼F ( |∇pk(r,b)|2) are upper bounded by • Bk ≤2∥∇ΠF∥∞∥K−1R+ ∥1, • Vk ≤1 +¯b ( 1 + ∥∇ΠF∥∞ ) ∥k∥∞. If one chooses a family of kernels, these bounds can be ex- pressed in terms of its parameters. For instance, when kis zero-mean Gaussian with variance σ2, one easily recovers: ∥K−1R+ ∥1 = σ √ 2/π ,∥k∥∞= ( √ 2πσ)−1. (6) CONV-OGA converges only to r∗ k. To remedy this, we would like to decrease Bk over time by letting k →δ0. However, since ∥k∥∞ diverges as k →δ0, we will have to tread carefully in our analysis which occupies the next section. 4. Convergence with Stationary Bidder To decrease Bk over time, we introducing a decaying ker- nel sequence {kt}t∈N into C ONV-OGA, giving V-C ONV- OGA (Alg. 2). This section will demonstrate its consis- tency and convergence by controlling the trade-off between bias Bkand varianceVk, as Bkis reduced to zero over time. This trade-off decomposes the total error as: ΠF(r∗)−ΠF(rt) = ΠF(r∗) −ΠF(r∗ k)   (surrogate bias) + ΠF(r∗ k) −ΠF(rt)   (estimation) . This stresses that the kernel should converge to δ0 fast enough to cancel the bias Bk, yet slowly enough to con- trol Vk and preserve the convergence speed of OGA. 4.1. General Convergence Result Thm. 1 provides sufﬁcient conditions on the schedules of kt and γt that guarantees V-CONV-OGA converges a.s. to r∗. It is derived by adapting stochastic optimisation results (see e.g. Bottou (1998)) to the changing objective ΠF kt.Real-time Optimisation for Online Learning in Auctions Algorithm 2: V-CONV-OGA input: r0, {γt}t∈N, {kt}t∈N ∈KN, C⊂ [0,¯b] for t= 1 to +∞do observe bt rt ←projC(rt−1 + γt∇pkt(rt−1,bt)) Theorem 1. Let F satisfy (A1) and (A2) and {kt}t∈N ∈ KN. Then, by running V-CONV-OGA with C= [0,¯b], we have rt a.s. −−→r∗ as long as ∑+∞ t=1 γt = +∞, ∑+∞ t=1 γt∥Kt−1R+ ∥1 <+∞ and ∑+∞ t=1 γ2 t∥kt∥∞<+∞. Proof Sketch. The proof relies on decomposing the er- ror ∥rt −r∗∥2 into three terms related respectively to the pseudo-concavity of ΠF, the bias Bkt, and the instanta- neous gradient second moment Vkt. Then, following Bot- tou (1998), we use a quasi-martingale argument to ensure the convergence of the stochastic error process. The full proof is available in App. C.1. If a constant kernel sequence were to be used in V-C ONV- OGA, we would recover the usual stochastic approxima- tion conditions on the step size γt, namely that ∑∞ t=1 γt = +∞and ∑∞ t=1 γ2 t <+∞. This suggests setting γt ∝1/t. For such a choice of step-size, Thm. 1 asserts convergence if ∑∞ t=1 γt∥Kt −1R+ ∥1 < +∞, which is guaranteed by kt → δ0. This means ∥k∥∞ → ∞as t → ∞, but∑+∞ t=1 γ2 t∥kt∥∞<+∞tells us explicitly how slow our de- cay must be in terms of the family of kernels. For example in the case of a Gaussian kernel, (6) implies that a suitable choice of kernel variance is σt ∝t−α for α∈(0,1). 4.2. Finite-time Convergence Rates While Thm. 1 provides sufﬁcient conditions on the kernel sequence {kt}t∈N for V-C ONV-OGA to be consistent, it does not characterise the rate of the convergence, and thus cannot be leveraged to optimise the step size γt and the decay rate of the kernel. To obtain ﬁnite time guarantees on the rate of convergence, we must impose stronger conditions on the monopoly rev- enue ΠF. Recall that under (A2), ΠF is strictly pseudo- concave. It is well known that such functions can have large areas of arbitrarily small gradient. Since they can make ﬁrst order methods arbitrarily slow, no meaningful rate can be obtained for them. Strengthening the assumption to (A3), i.e. excluding vanishing gradients by ensuring ΠF is µ- strongly log-concave (see Prop. 1), will give a rate in Thm. 2 under the further technical assumption (A4). (A4). The seller is given a compact subset C⊆ [0,¯b] and a constant c > 0 such that r∗ ∈ Cand for all r ∈ C, ΠF(r) ≥c. (A4) ensures that the seller can lower bound revenue on a compact subset of [0,¯b]. It should be understood as prior knowledge of the seller based on the format of the auction and the type of item sold. Cexists for any c< ΠF(r∗), so this hypothesis is not restrictive relative to (A3). Theorem 2. Let F satisfy (A1) and (A3), let Cand c as in (A4) be given, and let {kt}t∈N ∈KN such that there are ν1, ν∞, α1, α∞with ∥Kt −1R+ ∥1 ≤ν1t−α1 and ∥kt∥∞≤ν∞tα∞. Then, by running V-CONV-OGA on Cfor γt = νt−α with ν ≤(2cµ)−1, we have for all t≥2, α= 1 : E(∥rt −r∗∥2) = ˜O ( t−α1 + tα∞−1 + t−2µcν) α∈(0,1) : E(∥rt −r∗∥2) = ˜O ( t−α1 + tα∞−α) + ˜O (( t1−α−α1 + t1+α∞−2α) e−µcνt1−α) where ˜Opotentially hides a logarithmic term depending on the values of α, α1, and α∞. Proof Sketch. The extended statement of Thm. 2 with ex- plicit constants and its proof are detailed in App. C.2. The proof builds on Bach & Moulines (2011, Thm.2), derived for log-concave functions, and is adapted to our varying kernel approach and its changing objective. In contrast with the proof of Thm. 1, where we only leveraged pseudo- concavity, we show here that (A3), together with (A4), guarantees more reﬁned control on the curvature of ΠF around r∗: ∀r∈C, (r−r∗)∇ΠF(r) ≤−µc∥r∗−r∥2. (7) This way we can better control the stochastic process {∥rt −r∗∥2}t∈N: like for Thm. 1, we decompose the error into three terms related to concavity (Eq. 7), bias Bkt, and instantaneous gradient smoothness Vkt. The error is then bounded in expectation by manipulating ﬁnite series. Thm. 2 show two distinct regimes for both choices of α: transient ( t−2µcν and e−µcνt1−α resp.) and stationary (t−α1 + tα∞−α and t−α1 + tα∞−1 resp.). On Fig. 2 (top), the transient phase is visible up to 2 ×103 steps. Since the transient regime’s rate depends only on ν = γ0, cknown from (A4), and µknown from (A3), we can set ν to make the stationary regime the driver of the rate. To optimise the stationary regime we face a bias-variance trade-off. Like Thm. 1, Thm. 2 requires that k → δ0 (via α1 >0) while imposing a bound on the growth speedReal-time Optimisation for Online Learning in Auctions 102 103 104 105 time t 10−4 10−3 error Π F (r∗) − Π F (rt) σt = 0.1 σt = t□1 4 σt = t□1 2 σt = t□3 4 0 20000 40000 60000 80000 100000 timet 0.60 0.65 0.70 0.75 0.80 0.85 0.90 typical trajectoryrt σt = 0.1 σt = t□ 1 4 σt = t□ 1 2 σt = t□ 3 4 Figure 2.Stationary case. Numerical behaviour of V-C ONV- OGA for different σt on i.i.d samples from a Kumaraswamy (1, 0.4). Top: averaged convergence speeds of instant regret (log-log scale). Bottom: representative reserve price trajectories. of Vk (via α∞ < α). This time, however, we have exact rates which we can use to determine optimal parameters for the trade-off, taking into account the antagonistic effects of α1 and α∞. From Thm. 2, we recover that the optimal learning rate is γt ∝1/t. To tune the kernels it is sensible to ﬁx a parametric family and tune its parameter(s). For zero-mean Gaussian kernels, we have Cor. 1. Corollary 1. If we ﬁx γt ∝1/t, and let {kt}t∈N be Gaus- sian (0,1/t) kernels in Thm. 2 we have for all t≥2 that: E ( ∥rt−r∗∥2) = ˜O ( t−1/2) . This rate is optimal up to logarithmic factors. Fig. 2 demonstrates this optimality: the σt = 1/ √ t(blue) curve is the optimal rate on the top pane, and attains the rate of Cor. 1. The bottom pane illustrates the bias variance trade-off at hand in Thm. 2. If the kernel decays slower that 1/ √ t(red), the learning rate shrinks much faster and convergence is very slow but very smooth. If σt decreases too fast (green) the variance becomes overwhelming and noise swallows the performance. The novel analysis of V-C ONV-OGA showed its a.s. con- vergence under (A2), and that with a bit of curvature (A3) and the technical (A4) we could fully characterise its con- vergence rates. We could thus derive optimal learning rates and place conditions on optimal kernel decay rates. We made the optimal decay rate explicit for Gaussian kernels. This concludes the primary discussion on V-C ONV-OGA, and we now move to the non-stationary setting. 5. Tracking a Non-stationary Bidder In practical applications of online auctions, such as display advertising, bidders might change their bid distribution over time. These changes often result from non-stationarity in the private information of bidders. It is therefore beneﬁ- cial to be able to effectively adapt one’s reserve price over time to track changing bid distributions {Ft}t∈N. We use the dynamic regret R(T) to measure the quality of an algo- rithm’s tracking. The difﬁculty in the non-stationary setting is to trade-off adaptability (how fast a switch is detected) vs. accuracy (how accurate one is between switches). Convergent algo- rithms like ERM or V-C ONV-OGA will have high accu- racy in the ﬁrst phase, but then suffer as they try to adapt to changes later on, when their learning rate is very small. Windowed methods are more adaptable but still carry with them a lag, directly dependent on their window size. First- order methods like C ONV-OGA (with constant learning rate γ) are much more adaptable, but their convergence rate (O(1/ √ t)) hurts their accuracy. Nevertheless, we show that CONV-OGA is effective, with O( √ T) regret. The dynamic regret R(T) cannot be meaningfully con- trolled for arbitrary sequences {Ft}t∈N. As such it is cus- tomary to assume (A5) that {Ft}t∈N contains at most τ−1 switches up to a horizon T (see e.g. (Garivier & Moulines, 2011; Lattimore & Szepesv ´ari, 2018)). This corresponds to approximating a slowly changing sequence of Ft (e.g. Lipschitz) by a piece-wise constant sequence. (A5). Given some horizon T, there exists τ ≤T such that∑T−1 t=1 1Ft̸= Ft+1 ≤τ −1. Under (A5), the game (up to T) decomposes into τ phases. The ﬁrst step towards controlling the regret is to bound the tracking performance in each phase. We do this in Thm. 3, which shows an incompressible assymptotic error (the bias of our surrogate plus the variance) and a transient phase with exponential decay. Theorem 3. Let F satisfy (A1) and (A3) with parameter µ, let Cand cbe as in (A4) and k ∈K. Then, by running CONV-OGA on Cwith a constant stepsize γ >0, for anyReal-time Optimisation for Online Learning in Auctions 0 500 1000 1500 2000 2500 3000 time t 0.0 0.2 0.4 0.6 reserve prices rt γ = .1, σ= .1 γ = .01, σ= .1 γ = .1, σ= .5 Figure 3.Non-stationary case. Tracking by C ONV-OGA of three Kumaraswamy distributions (with parameters(1,4), (1,0.4), and (1,1) resp.) with different Gaussian kernels and learning rates. t≥1 we have E(∥rt −r∗ k∥2) ≤ (¯b2 + C(γ,k)(t- 1) ) e−µcγ 2 t + 2C(γ,k) µc where C(γ,k) = O ( γ∥K−1R+ ∥1 + γ2∥k∥∞ ) . Thus, immediately after a switch there will be a transient regime of order te−µcγ 2 t (high adaptability), but afterwards rt will oscillate in a band of size 2C(γ,k) µc around r∗ k (low accuracy). We can then use Thm. 3 to derive a sub-linear regret bound given T,τ (Cor. 2). Corollary 2. Let {Ft}t≥1 satisfy (A1), (A3), (A4), (A5) and k ∈K. Then, there exists Ξ(k,γ) and Ω(k,γ) such that CONV-OGA has a non-stationary regret of R(T) ≤Ξ(k,γ)T + Ω(k,γ)τ. Further, if the horizon T is known in advance, running CONV-OGA with γ = T−1 2 and k a kernel with ∥K − 1R+ ∥1 ≤T−1 2 and ∥k∥∞≤T 1 2 , then R(T) = O( √ T). Proof. See App. D. Figure 3 illustrates the behaviour of CONV-OGA in a non- stationary environment. In agreement with Thm. 3 and Cor. 2, γ controls the length of the transient regime due to the e−µcγ 2 t term. Increasing γ shortens it but increases the width of the band of the asymptotic regime as C(γ,k) increases with γ (blue vs. green curves). For a ﬁxed γ, the stationary regime in terms of kexhibits a bias-variance trade-off: ∥K−1R+ ∥1 corresponds to the bias and ∥k∥∞ to the variance (see Prop. 3). In the case of a Gaussian ker- nel, increasing σreduces variance but increases bias (green vs. red curve). CONV-OGA with a constant learning rate is an efﬁcient real-time algorithm for tracking monopoly prices of non- stationary bidders. It incurs O( √ T) regret given the hori- zon and τ, by tuning γ and k, while maintaining the com- putational efﬁciency of online methods. 6. Discussion In this paper we introduced V-C ONV-OGA, the ﬁrst real-time (O(1) update-time and memory) method for monopoly price learning. We ﬁrst gave some theoreti- cal results bridging auction theory and optimisation. We then showed how to ﬁx the biased gradient problem with smooth surrogates, giving C ONV-OGA. Next, we let the smoothing decrease over time in V-CONV-OGA, for whom we showed convergence of O(1/ √ t). Finally, we adapted CONV-OGA to perform tracking of non-stationary bid dis- tributions with O( √ T) dynamic regret. Towards optimal rates. In the context of high-frequency auctions, computational efﬁciency trumps numerical pre- cision, so we traded O( √ t) complexity and O(1/t) speed for O(1) complexity and O(1/ √ t) speed. Whether or not it is possible to reach the optimal rate with a real-time algo- rithm remains an open question. We conjecture this to be impossible in general, but we know it is possible in some instances. If F is a symmetric distribution, then C ONV- OGA with a constant symmetric kernel has no bias and O(1/t) convergence. Adapting the choice kernel to somea priori knowledge on F is a possible direction to match the optimal rate. Extension to stationary bandit. The second question concerns the extension to partially observable settings, such as online eager auctions, when the seller does not ob- serve bids under the reserve. Obviously, extensions using a reduction to multi-armed bandits (UCB, Exp3, Exp4, etc.) via a discretisation of the bid space cannot bereal-time: the discretisation creates a need for O( √ t) in memory and the same for the update. Yet, it is possible to obtain a strait- forward extension of V-C ONV-OGA in this setting, by plugging it into an Explore-The-Commit (ETC) (Perchet & Rigollet, 2013) algorithm: V-C ONV-OGA learns an esti- mate of the monopoly price during the exploration period, which is then used during the exploitation period. As for other algorithms, by using a doubling trick to handle an un- known horizon, ETC+V-CONV-OGA exhibits a sub-linear regret. Unfortunately, like in the lazy auction setting, the regret is not optimal and the question of whether a real- time algorithm can match this optimal regret is still open.Real-time Optimisation for Online Learning in Auctions Extension to non-stationary bandit. The question of the partially observable setting also applies to non- stationary bidders. In this case, extending C ONV-OGA with ETC is no longer straight-forward, as the switching times are unknown. Thus, it is not obvious when to re- trigger an exploration phase of ETC to adapt to the change of the bidder’s distribution. A potential way to tackle this problem could be to use randomised resets for the algo- rithm (Allesiardo et al., 2017) or change-point detection al- gorithms to trigger exploration (Hartland et al., 2006). References Allesiardo, R., F ´eraud, R., and Maillard, O.-A. The non- stationary stochastic multi-armed bandit problem. In- ternational Journal of Data Science and Analytics, 3(4): 267–283, Jun 2017. Amin, K., Rostamizadeh, A., and Syed, U. Repeated con- textual auctions with strategic buyers. In Advances in Neural Information Processing Systems , pp. 622–630, 2014. Bach, F. and Moulines, E. Non-asymptotic analysis of stochastic approximation algorithms for machine learn- ing. In Advances in Neural Information Processing Sys- tems, 2011. Blum, A. and Hartline, J. D. Near-optimal online auctions. In Proceedings of the sixteenth annual ACM-SIAM sym- posium on Discrete algorithms, pp. 1156–1163. Society for Industrial and Applied Mathematics, 2005. Blum, A., Kumar, V ., Rudra, A., and Wu, F. Online learn- ing in online auctions. Theoretical Computer Science , 324(2-3):137–146, 2004. Bottou, L. Online learning and stochastic approximations. On-line learning in neural networks, 17(9):142, 1998. Bubeck, S., Devanur, N. R., Huang, Z., and Niazadeh, R. Online auctions and multi-scale online learning. In Pro- ceedings of the 2017 ACM Conference on Economics and Computation, pp. 497–514. ACM, 2017. Cesa-Bianchi, N., Gentile, C., and Mansour, Y . Regret minimization for reserve prices in second-price auctions. IEEE Transactions on Information Theory , 61(1):549– 564, 2014. Cohen, M., Lobel, I., and Leme, R. P. Feature-based dy- namic pricing. In Proceedings of the 2016 ACM Confer- ence on Economics and Computation, 2016. Duchi, J. C., Bartlett, P. L., and Wainwright, M. J. Ran- domized smoothing for stochastic optimization. SIAM Journal on Optimization, 22(2):674–701, 2012. Ewerhart, C. Regular type distributions in mechanism de- sign and ρ-concavity. Economic Theory, 53(3):591–603, 2013. Flaxman, A. D., Kalai, A. T., and McMahan, H. B. Online convex optimization in the bandit setting: Gradient de- scent without a gradient. In Proceedings of the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’05, pp. 385–394, USA, 2005. Society for Indus- trial and Applied Mathematics. ISBN 0898715857. Garivier, A. and Moulines, E. On upper-conﬁdence bound policies for switching bandit problems. In International Conference on Algorithmic Learning Theory , pp. 174– 188. Springer, 2011. Hartland, C., Gelly, S., Baskiotis, N., Teytaud, O., and Sebag, M. Multi-armed Bandit, Dynamic En- vironments and Meta-Bandits. working paper or preprint, November 2006. URL https://hal. archives-ouvertes.fr/hal-00113668. Ibragimov, I. A. On the composition of unimodal distri- butions. Theory of Probability & Its Applications, 1(2): 255–260, 1956. Kleinberg, R. and Leighton, T. The value of knowing a demand curve: Bounds on regret for online posted-price auctions. In 44th Annual IEEE Symposium on Founda- tions of Computer Science, 2003. Proceedings., pp. 594– 605. IEEE, 2003. Krishna, V .Auction theory. Academic press, 2009. Lattimore, T. and Szepesv ´ari, C. Bandit algorithms. preprint, 2018. Mohri, M. and Medina, A. M. Learning algorithms for second-price auctions with reserve. The Journal of Ma- chine Learning Research, 17(1):2632–2656, 2016. Morgenstern, J. H. and Roughgarden, T. On the pseudo- dimension of nearly optimal auctions. In Advances in Neural Information Processing Systems , pp. 136–144, 2015. Myerson, R. B. Optimal auction design. Mathematics of operations research, 6(1):58–73, 1981. Ostrovsky, M. and Schwarz, M. Reserve prices in internet advertising auctions: A ﬁeld experiment. In Proceedings of the 12th ACM Conference on Electronic Commerce , EC ’11, pp. 59–60, New York, NY , USA, 2011. ACM. ISBN 978-1-4503-0261-6. Paes Leme, R., Pal, M., and Vassilvitskii, S. A ﬁeld guide to personalized reserve prices. InProceedings of the 25th International Conference on World Wide Web , WWWReal-time Optimisation for Online Learning in Auctions ’16, pp. 1093–1102, Republic and Canton of Geneva, Switzerland, 2016. International World Wide Web Con- ferences Steering Committee. Perchet, V . and Rigollet, P. The multi-armed bandit prob- lem with covariates. The Annals of Statistics, 41(2):693– 721, 04 2013. Pollard, D. Convergence of stochastic processes. Springer Science & Business Media, 1984. Roughgarden, T. and Wang, J. R. Minimizing Regret with Multiple Reserves. In Proceedings of the 2016 ACM Conference on Economics and Computation - EC ’16 , volume 9, pp. 601–616, 2016. Rudolph, M. R., Ellis, J. G., and Blei, D. M. Objective vari- ables for probabilistic revenue maximization in second- price auctions with reserve. In Proceedings of the 25th International Conference on World Wide Web, pp. 1113– 1122. International World Wide Web Conferences Steer- ing Committee, 2016. Saumard, A. and Wellner, J. A. Log-concavity and strong log-concavity: A review. Statistics Surveys, 8:45–114, 2014. Shen, W., Lahaie, S., and Paes Leme, R. Learning to clear the market. In International Conference on Machine Learning, pp. 5710–5718, 2019.Real-time Optimisation for Online Learning in Auctions A. General Reminders on Pseudo- and Log-Concavity This section is a stand-alone reminder and does not share notations with the rest of the paper. A.1. Pseudo-Concavity Deﬁnition 1. A function f : X→ R, f ∈C1(X), is pseudo-concave on Xif ∀(x,y) ∈X2, ∇f(x)T(x−y) ≥0 ⇒ f(x) ≥f(y). Deﬁnition 2. A function f : X→ R, f ∈C1(X), is strictly pseudo-concave on Xif it is pseudo-concave and has at most one critical point. A.2. Log-Concavity Deﬁnition 3. A function f : X→ ¯R is log-concave on Xif ∀α∈[0,1],∀(x,y) ∈X2,f(αx+ (1 −α)y) ≥f(x)αf(y)(1−α) Note that if f : X→ R+ ∗ this is equivalent to saying f = e−ϕ where ϕis a convex function on X. Deﬁnition 4. A function f : X→ ¯R is strictly log-concave on Xif ∀α∈(0,1),∀(x,y) ∈X2 s.t. x̸= y, f (αx+ (1 −α)y) >f (x)αf(y)(1−α) Note that if f : X→ R+ ∗ this is equivalent to saying f = e−ϕ where ϕis a strictly convex function on X. Deﬁnition 5. A function f : X→ ¯R is µ-strongly log-concave on Xif x↦→f(x)e−µx2 is log-concave. Note that if f : X→ R+ ∗ this is equivalent to saying f = e−ϕ where ϕis µ-strongly convex on X. We also recall a useful technical result for any log-concave function f, that is a straightforward consequence from the concavity characterization of log(f). Proposition 4. Let f be a real strictly positive strictly log-concave function. Then, for all u>v , for all δ >0, f(v+ δ) f(v) > f(u+ δ) f(u) . Proof of Prop. 4. The proof is a straightforward application of properties of strictly concave functions applied to log(f). Let F(x,y) = log f(x)−log f(y) x−y , then F is strictly decreasing in xfor every ﬁxed y(and vice-versa). Thus, F(v+ δ,v) >F (v+ δ,u) >F (u+ δ,u) ⇒ log f(v+ δ) −log f(v) >log f(u+ δ) −log f(u) ⇒ f(v+ δ) f(v) > f(u+ δ) f(u) . A.3. Stability through Convolution Theorem 4 (Ibragimov (1956)). Let f : X ⊂R →R+ be pseudo-concave on X, C1(X), L1(X) and g : R →R+ be L1(R) and log-concave. Then, f ⋆gis pseudo-concave on R. We extend this theorem to strict pseudo-concavity. Lemma 1 (Extension of Ibragimov (1956)) . Let f : X ⊂[x1,x2] →R+ be a strictly pseudo-concave on X, C1(X), L1(X) such that limx→x1,2 f(x) = 0 and g : R → R+ be L1(R) and strictly log-concave. Then, f ⋆ gis strictly pseudo-concave on R. Proof. The proof is conducted in two steps: 1) we show f ⋆gadmits a maximum on the interior of its domain (which is a critical point) and we denote it x∗. 2) we show that f ⋆gis strictly increasing on (−∞,x∗) and strictly decreasing on (x∗,+∞) which immediately proves the strict pseudo-concavity (including unicity of x∗).Real-time Optimisation for Online Learning in Auctions 1. Since f and g are C1 and L1, the convolution f ⋆g is well deﬁned, C1(R), L1(R) and positive (since f and g are positive). As a result, f⋆g (x) →0 when |x|→∞ and Rolle’s theorem guarantees that there exists at least one point x⋆ ∈R such that f ⋆g(x∗) ≥f ⋆g(x) for all x∈R. Furthermore, Ibragimov’s theorem (Thm. 4) ensures that f ⋆g is pseudo-concave, hence ∇ ( f ⋆g ) (x∗) = 0. 2. Using the differentiation property of the convolution, one has that for all x∈R, ∇ ( f ⋆g ) (x) = ∫ ∞ −∞ f(t)∇g(x−t)dt= ∫ x2 x1 ∇f(t)g(x−t)dt, (8) where we used the fact that limx→x1,2 f(x) = 0. Let x∗be a critical point of f ⋆g. Taking Eq. 8 at x= x∗leads to: 0 = ∫ x2 x1 ∇f(t)g(x∗−t)dt. Moreover, let y∗∈(x1,x2) be the unique (by pseudo-concavity) critical point off, whose existance is guaranteed by Rolle’s theorem (limx→x1,2 f(x) = 0). We now split the integral in Eq. 8 to obtain: ∇ ( f ⋆g ) (x) = ∫ y∗ x1 ∇f(t)g(x−t)dt+ ∫ x2 y∗ ∇f(t)g(x−t)dt. (9) The core of the proof consists in proving that ∇ ( f ⋆g ) (x∗+ δ) >0 for all δ >0 and ∇ ( f ⋆g ) (x∗+ δ) <0 for all δ <0. Since the derivation is similar in both cases, we only display here the case δ >0. From Eq. 9, we have: ∇ ( f ⋆g ) (x∗+ δ) = ∫ y∗ x1 ∇f(t)g(x∗+ δ−t)dt+ ∫ x2 y∗ ∇f(t)g(x∗+ δ−t)dt = ∫ y∗ x1 ∇f(t)g(x∗−t)g(x∗+ δ−t) g(x∗−t) dt+ ∫ x2 y∗ ∇f(t)g(x∗−t)g(x∗+ δ−t) g(x∗−t) dt. We now provide upper and lower bounds for g(x∗+δ−t) g(x∗−t) respectively on [x1,y∗] and [y∗,x2]. Let t∗= argmaxt∈[x1,y∗] g(x∗+ δ−t) g(x∗−t) which exists since by our hypotheses on g. Then, ∀t∈[x1,y∗], g(x∗+ δ−t) g(x∗−t) ≤g(x∗+ δ−t∗) g(x∗−t∗) . Moreover, applying Prop. 4, we have for almost all t∈[y∗,x2), g(x∗+ δ−t) g(x∗−t) > g(x∗+ δ−t∗) g(x∗−t∗) . Since f is strictly pseudo-concave, ∇f(t) >0 on [x1,y∗) and ∇f(t) <0 on (y∗,x2], we obtain ∇ ( f ⋆g ) (x∗+ δ) = ∫ y∗ x1 ∇f(t)g(x∗−t)g(x∗+ δ−t) g(x∗−t) dt+ ∫ x2 y∗ ∇f(t)g(x∗−t)g(x∗+ δ−t) g(x∗−t) dt < ∫ y∗ x1 ∇f(t)g(x∗−t)g(x∗+ δ−t∗) g(x∗−t∗) dt+ ∫ x2 y∗ ∇f(t)g(x∗−t)g(x∗+ δ−t∗) g(x∗−t∗) dt < g(x∗+ δ−t∗) g(x∗−t∗) ∇ ( f ⋆g ) (x∗) = 0, which proves the desired result.Real-time Optimisation for Online Learning in Auctions Similar stability properties through convolution are asserted for strictly and strongly log-concave functions. The ﬁrst result is standard and can be derived from the Pr´epoka-Leindler inequality, the second can be retrieved from (Saumard & Wellner, 2014). Proposition 5. Let f : X⊂ R →R+ and g: R →R+ be log-concave. Then, f ⋆gis log-concave. Theorem 5 (Saumard & Wellner (2014), Thm. 6.6). Let f : X ⊂R →R+ and g : R →R+ be µand µ′strongly log- concave respectively. Then, f⋆g is µµ′/ √ µ2 + µ′2 strongly log-concave. Further, the convolution of strictly log-concave functions is strictly log-concave. B. Proofs of Sec. 3 B.1. Pseudo- and Log-Concavity of the monopoly revenue Proposition 1. Let F satisfy (A1) and ΠF be its associated monopoly revenue. Then, ΠF ∈C2( [0,¯b] ) , ΠF >0 on (0,¯b) and: • under (A2), ΠF is strictly pseudo-concave, • under (A3), ΠF is µ-strongly log-concave, i.e. log ΠF is µ-strongly concave on (0,¯b). Proof. • Under (A2), ψ(r) = r−1−F(r) f(r) is stricly increasing. Moreover, for all r∈[0,¯b], ∇ΠF(r) = 1 −F(r) −rf(r) = −ψ(r)f(r). The objective is to show that for all (r1,r2) ∈[0,¯b]2, ∇ΠF(r1)(r1 −r2) ≥0 ⇒ ΠF(r1) ≥ΠF(r2) and that ΠF has one critical point (by Rolle’s theorem, since ΠF(0) = ΠF(¯b) = 0). Without loss of generality, we only address the case where r1 ≤r2. Since ψis strictly increasing, ψ(r1) ≤ψ(r2), and as a result ∇ΠF(r1) ≤0 ⇔ ψ(r1) ≥0 ⇒ ∀r∈[r1,r2] ,ψ(r) ≥0 ⇔ ∇ΠF(r) ≤0 ⇒ ΠF(r2) −ΠF(r1) = ∫ r2 r1 ∇ΠF(r)dr≤0. The case ∇ΠF(r1) ≥0 is treated in a similar fashion. Finally, since ψis strictly increasing it can only cross 0 once, which immediately ensures the uniqueness of the critical point since f >0 on (0,¯b). • Under (A3), the hazard rate λ(r) = f(r) 1−F(r) satisﬁes ∀0 ≤r1 ≤r2 ≤¯b, λ(r2) −λ(r1) ≥µ(r2 −r1). The objective is to show that log ΠF(r) = log(r) + log(1−F(r)) is µ-strongly concave. As log(r) is concave, we can simply show that log(1 −F(r)) is µ-strongly concave. Since F ∈C1([0,¯b]), we can use a characterisation of strong concavity of G(r) = log(1 −F(r)) based on its derivative: Gis µ-strongly concave ⇔∀(r1,r2) ∈[0,β]2,(∇G(r2) −∇G(r1))T (r2 −r1) ≤−µ∥r2 −r1∥2 Without loss of generality, we consider the case where 0 ≤r1 ≤r2 ≤¯b, ∇G(r2) −∇G(r1) = −f(r2) 1 −F(r2) − −f(r1) 1 −F(r1) = λ(r1) −λ(r2) ≤−µ(r2 −r1). Hence Gis µ-strongly concave.Real-time Optimisation for Online Learning in Auctions B.2. Unbiased gradient and preservation of concavity Proposition 2. Let F satisfy (A1) and ΠF be its associated monopoly revenue. Let k∈K, then: • ΠF k and pk are C1(R), • ΠF k(r)=EF ( pk(r,b) ) and ∇ΠF k(r)=EF ( ∇pk(r,b) ) , • under (A2), ΠF k is strictly pseudo-concave on R, • under (A3), ΠF k is strictly log-concave on R. Proof. The proof is a straightforward application of the convolution’s properties, the Fubini-Tonelli theorem and of the stability of concavity w.r.t. convolution detailed in App. A.3. 1. Since k∈C1(R) ∩L1(R), and since ΠF and pare L1, ΠF k and pk are in C1 ∩L1. 2. Since p, ΠF, and k are positive, so are ΠF k and pk. Thus, the Fubini-Tonelli theorem ensures that ΠF k(r) = Eb∼F ( pk(r,b) ) . Further, ∇ΠF k(r) = Eb∼F ( ∇pk(r,b) ) . 3. Under (A2), ΠF is strictly pseudo-concave (Prop. 1) and k∈K is strictly log-concave. Further, ΠF(0) = ΠF(¯b) = 0 and we can apply Lem. 1 to guarantee the strict pseudo-concavity of ΠF k. 4. Under (A3), ΠF is strictly log-concave and k ∈K is strictly log-concave. Since the convolution preserves strict- concavity (see Thm. 5), ΠF k is strictly log-concave. B.3. Bias and bounded gradient Proposition 3. Let F satisfy (A1), k∈K. Let r∗and r∗ k be the monopoly prices associated withΠF and ΠF k. Then, the bias Bk = |ΠF(r∗) −ΠF(r∗ k)|and the instantaneous convolved gradient second moment Vk = maxr≥0 Eb∼F ( |∇pk(r,b)|2) are upper bounded by • Bk ≤2∥∇ΠF∥∞∥K−1R+ ∥1, • Vk ≤1 +¯b ( 1 + ∥∇ΠF∥∞ ) ∥k∥∞. Proof. 1. The bound on Bk relies on Lem. 2, which guarantees that for all r≥0, |ΠF(r) −ΠF k(r)|≤∥∇ ΠF∥∞ ∫ ∞ −∞ |r|k(r)dr. Decomposing Bk as Bk ≤ΠF(r∗) −ΠF k(r∗) + ΠF k(r∗) −ΠF k(r∗ k) + ΠF k(r∗ k) −ΠF(r∗ k) ≤ΠF(r∗) −ΠF k(r∗) + ΠF k(r∗ k) −ΠF(r∗ k) and applying two times Lem. 2 proves the desired result. 2. For all (r,b) ∈R+ ×[0,¯b], using properties of the convolution, one has: ∇pk(r,b) = ∫ ∞ −∞ p(τ,b)∇k(r−τ)dτ = ∫ b 0 τ∇k(r−τ)dτ = [ −τk(r−τ) ]b 0 + ∫ b 0 k(r−τ)dτ = ∫ b 0 k(r−τ)dτ −bk(r−b).Real-time Optimisation for Online Learning in Auctions Since k> 0, and ∥k∥1 = 1 it is clear that −bk(r−b) ≤∇pk(r,b) ≤1 ⇒ |∇pk(r,b)|2 ≤max ( 1,b2k(r−b)2) ≤1 + b2k(r−b)2. Taking the expectation w.r.t.F (with pdf f), one obtains: EB[(∇pk(r,B))2] ≤1 + ∫ ¯b 0 b2k(r−b)2f(b)db≤1 + ∥k∥∞ ∫ ¯b 0 b2f(b)k(r−b)db. Finally, under (A1), for all b∈[0,¯b], bf(b) = 1 −F(b) −∇ΠF(b) ⇒ bf(b) ≤1 + ∥∇ΠF∥∞<∞. As a result, we have that EB[(∇pk(r,B))2] ≤1 + ( 1 + ∥∇ΠF∥∞ ) ∥k∥∞ ∫ ¯b 0 bk(r−b)db≤1 +¯b ( 1 + ∥∇ΠF∥∞ ) ∥k∥∞. The following intermediate results provide uniform bounds on the distance between the monopoly revenue (resp. gradient) and the convoluted monopoly revenue (resp. gradient). Lemma 2. Let F satisﬁes (A1) and k∈K be a convolution kernel. For any r∈[0,¯b], we have |ΠF(r) −ΠF k(r)|≤∥∇ ΠF∥∞∥K−1R+ ∥1 = ∥∇ΠF∥∞ ∫ ∞ −∞ |r|k(r)dr Proof. First, let’s notice that since∇ΠF is continuous on the closed interval [0,¯b], it is bounded – i.e. ∥∇ΠF∥∞<+∞. Thus, integrating by parts leads to |ΠF(r) −ΠF k(r)|= ⏐⏐(ΠF ⋆(δ0 −k))(r) ⏐⏐ ≤ ⏐⏐⏐⏐ [ ΠF(t)(1R+ (r−t) −K(r−t)) ]+∞ −∞   =0 −∇ΠF ⋆(1R+ −K)(r) ⏐⏐⏐⏐ ≤ ⏐⏐∇ΠF ⋆(1R+ −K)(r) ⏐⏐ ≤∥∇ΠF∥∞∥K−1R+ ∥1 (Young’s convolution inequality) Finally, a last integration by parts leads to ∥K−1R+ ∥1 = ∫ 0 −∞ K(r)dr+ ∫ ∞ 0 ( 1 −K(r) ) dr = [ rK(r) ]0 −∞− ∫ 0 −∞ rk(r)dr+ [ r(1 −K(r)) ]∞ 0 + ∫ ∞ 0 rk(r)dr = ∫ ∞ −∞ |r|k(r)dr. Then, another bias that is important to control, is the one of the gradient. Lemma 3. Assuming F satisﬁes (A1) and k∈K be a convolution kernel. For any r∈[0,¯b], we have |∇ΠF k(r) −∇ΠF(r)|≤∥∇ 2ΠF∥∞∥K−1R+ ∥1 = ∥∇2ΠF∥∞ ∫ ∞ −∞ |r|k(r)drReal-time Optimisation for Online Learning in Auctions Proof. The proof is essentially the same as the one of Lemma 2. First, notice that since ∇2ΠF is continuous on the closed interval [0,¯b], it is bounded – i.e. ∥∇2ΠF∥∞<+∞. |∇ΠF k(r) −∇ΠF(r)|= ⏐⏐(∇ΠF ⋆(k−δ0))(r) ⏐⏐ = ⏐⏐⏐⏐ [ ∇ΠF(x)(K(r−x) −1R+ (r−x)) ]+∞ −∞   =0 −∇2ΠF ⋆(K−1R+ )(r) ⏐⏐⏐⏐ = ⏐⏐∇2ΠF ⋆(K−1R+ )(r) ⏐⏐ ≤∥∇2ΠF∥∞∥K−1R+ ∥1 (Young’s convolution inequality) = ∥∇2ΠF∥∞ ∫ ∞ −∞ |r|k(r)dr. C. Proofs of Sec. 4 In this section we consider only a stationary F, and therefore, for simplicity, we will denote ΠF simply by Π. C.1. Almost Sure Convergence Theorem 1. Let F satisfy (A1) and (A2) and {kt}t∈N ∈KN. Then, by running V-CONV-OGA with C= [0,¯b], we have rt a.s. −−→r∗ as long as ∑+∞ t=1 γt = +∞, ∑+∞ t=1 γt∥Kt −1R+ ∥1 <+∞and ∑+∞ t=1 γ2 t∥kt∥∞<+∞. Proof. The proof inherits a lot from classical methods, see e.g. Bottou (1998). The main difference lies in the type of “concavity” required. The proof in Bottou (1998) is derived for variationally coherent functions i.e. those such that ∀t∈N,∀ϵ> 0, sup (r−r∗ kt)2>ϵ (r−r∗ kt)T∇Πkt(r) <0. However, such assumption is clearly violated here since ∇Πkt(r) →0 as r →∞. Nevertheless since Πkt is strictly pseudo-concave and strictly positive, one can obtain a similar result. Following Bottou (1998), we introduce the Lyapunov process ht = ∥rt −r∗∥2. Using the fact that the projection operator is a contraction, one obtains: ht+1 = (projC ( rt + γt∇pkt(rt,bt) ) −r∗ )2 ≤(rt + γt∇pkt(rt,bt) −r∗)2 ≤ht + 2γt(rt −r∗)T∇pkt(rt,bt) + γ2 t ( ∇pkt(rt,bt) )2 . Hence, ht satisﬁes the recursion: ht+1 −ht ≤2γt(rt −r∗)∇pkt(rt,bt) + γ2 t ( ∇pkt(rt,bt) )2 Taking the conditional expectation w.r.t.Ft = σ(b0,...,b t−1,r0,...,r t,γ0,...,γ t), one obtains: E [ ht+1 −ht ⏐⏐Ft ] ≤2γt(rt −r∗)TE [ ∇pkt(rt,bt) ⏐⏐Ft ] + γ2 tE [( ∇pkt(rt,bt) )2⏐⏐Ft ] ≤2γt(rt −r∗)T∇Πkt(rt)) + γ2 tE [( ∇pkt(rt,bt) )2⏐⏐Ft ] (10) as Prop. 2 provides that E [ ∇pkt(rt,bt) ⏐⏐Ft ] = ∇Πkt(rt). Then, we decompose the gradient term to isolate the gradient bias:Real-time Optimisation for Online Learning in Auctions E [ ht+1 −ht ⏐⏐Ft ] ≤2γt(rt −r∗)T∇Π(rt))   ≤0 by pseudo-concavity + 2γt(rt −r∗)T(∇Πkt(rt) −∇Π(rt)))   bias + γ2 tE [ (∇pkt(rt,bt))2 ⏐⏐Ft ]    Bounded gradient (11) The ﬁrst term in Eq. 11 is negative by the pseudo-concavity of Π (Prop. 1). The second term is bounded by 2γt∥∇2Π∥∞∥K−1R+ ∥1 by Lem. 3. The third term is bounded by γ2 t ( 1 +¯b(1 + ∥∇Π∥∞)∥k∥∞ ) by Prop. 3. Using the same quasi-martingale argument as in Bottou (1998), we have that Eq. 11, together with∑γt∥Kt−1R+ ∥1 <∞ and ∑γ2 t∥kt∥∞<∞, implies that ht a.s. →h∞<∞, ∑ E [ ht+1 −ht ⏐⏐Ft ] <∞. Thus, using Eq. 10, we have that 0 ≤ ∑ γt(r∗−rt)T∇Π(rt) <∞. (12) Suppose now that (rt −r∗) a.s. →h∞̸= 0, then since ∑γt = +∞, it would lead to ∑γt(r∗−rt)T∇Π(rt) = +∞which is in contradiction with Eq. 12. As a result, (rt −r∗)T∇Π(rt) a.s. →0. Finally, since (rt −r∗)2 a.s. →h∞<∞, necessarily, ∇Π(rt) a.s. →0 and rt a.s. →r∗. C.2. Finite-time Convergence Speed We provide here the full statement of Thm. 2, along with explicit rates and constants. The rates are expressed in terms of an auxiliary function ϕ, which allows us to handle all conﬁgurations of the step-size and kernel decay schedules. Depending on the value of α, α1, α∞, it may introduce some logarithmic term log(t). This explains the ˜Onotation used in the abridged version of Thm. 2 ( Sec. 4), which can easily be recovered from this extended version. Theorem 2. Let F satisfy (A1) and (A3), Cbe given in (A4) and {kt}t∈N ∈KN such that ∥Kt −1R+ ∥1 ≤ν1t−α1 and ∥kt∥∞≤ν∞tα∞. Then, by running V-CONV-OGA on Cwith γt = νt−α with ν ≤(2cµ)−1, we have for all t≥2, if α= 1 E(∥rt −r∗∥2) ≤ ( ¯b2 + 2Cνν1ϕ2µcν−α1 (t) + 2C∞ν2ν∞ϕ2µcν+α∞−1(t) ) t−2µcν if α∈(0,1) E(∥rt −r∗∥2) ≤ ( ¯b2 + C1νν1ϕ1−α−α1 (t) + C∞ν2ν∞ϕ1+α∞−2α(t) ) exp ( −µcνt1−α) + C1 ν1 µct−α1 + C∞ νν∞ µc tα∞−α as long as α, α1, α∞satisfy the condition of Thm. 1 i.e., α≤1, α+ α1 >1 and 2α−α∞>1. The function ϕand the constant C1, C∞are given by ϕβ(t) = log(t)1β=0 + tβ −1 β 1β̸=0; C1 = 2¯b∥∇2Π∥∞; C∞= 1 +¯b ( 1 + ∥∇Π∥∞ ) . Proof. The proof builds on Bach & Moulines (2011, Thm. 2) . The main differences are that 1) we don’t require the local function pkt to be concave 2) we don’t rely on the strong concavity of Πkt but on its strong log-concavity and one of its lower bounds 3) our objective function varies over time because of the sequence of convolution kernels{kt}t≥1. We ﬁrst stress that (A3) together with the lower bounded revenue Πkt leads to some sort of local strong concavity of Π. From Prop. 1, the strongly increasing hazard rate ensures that Π is strongly log-concave with parameter µ. Further, Π admits a unique maximum r∗(since Π(0) = Π(β) = 0) such that r∗∈C by assumption. As a result, for any r∈C, (r−r∗)T( ∇ ( log Π ) (r) −∇ ( log Π ) (r∗) ) ≤−µ∥r∗−r∥2 ⇒ (r−r∗)T( ∇Π(r)/Π(r) −∇Π(r∗)/Π(r∗) ) ≤−µ∥r∗−r∥2 ⇒ (r−r∗)T∇Π(r) ≤−Π(r)µ∥r∗−r∥2 ≤−µc∥r∗−r∥2.Real-time Optimisation for Online Learning in Auctions We denote as ˜µ = µc the quantity which plays the role of the strong-concavity parameter in Bach & Moulines (2011, Thm. 2). As for the proof of Thm. 1, we introduce the Lyapunov process ht = ∥rt −r∗∥2 2 and its expectation ¯ht = E(ht). From Eq. 11, we have E(ht+1 −ht)|Ft) ≤2γt(rt −r∗)T∇Π(rt) + 2γt¯b∥∇2Π∥∞∥Kt −1R+ ∥1 + γ2 t ( 1 +¯b ( 1 + ∥∇Π∥∞ ) ∥k∥∞ ) . Using the local strong-concavity of Π and that ∥Kt −1R+ ∥1 ≤γ1 t, ∥kt∥∞≤γ∞ t , we obtain: E(ht+1 −ht)|Ft) ≤2˜µγtht + C1γtγ1 t + C∞γ2 tγ∞ t , where C1 = 2¯b∥∇2Π∥∞and C∞= 1 +¯b ( 1 + ∥∇Π∥∞ ) .5 Taking the expectation leads to: ¯ht+1 ≤(1 −2˜µγt)¯ht + C1γtγ1 t + C∞γ2 tγ∞ t . (13) In line with Bach & Moulines (2011), we split the proof depending whether α= 1 or α∈(0,1). 1. The case α= 1: using that 1 −x≤exp(−x) for all x∈R and applying the recursion ttimes in Eq. 13, we have ¯ht ≤¯h1 exp ( −2˜µ t−1∑ s=1 γs ) + C1 t−1∑ s=1 γsγ1 s exp ( −2˜µ t−1∑ τ=s+1 γτ ) + C∞ t−1∑ s=1 γ2 sγ∞ s exp ( −2˜µ t−1∑ τ=s+1 γτ ) ≤¯h1 exp ( −2˜µν t−1∑ s=1 s−1 ) + ( C1νν1 t−1∑ s=1 s−α−α1 + C∞ν2ν∞ t−1∑ s=1 s−2α+α0 ) exp ( −2˜µν t−1∑ τ=s+1 τ−1 ) . Further, for all t≥2, t−1∑ s=1 s−1 ≥log(t) t−1∑ τ=s+1 τ−1 ≥log(t/s+ 1) we obtain that (under ˜µν ≤1/2): ¯ht ≤¯h1t−2˜µν + C1νν1t−2˜µν t−1∑ s=1 s−1−α1 (s+ 1)2˜µν + C∞ν2ν∞t−2˜µν t−1∑ s=1 s−2+α∞(s+ 1)2˜µν ≤¯h1t−2˜µν + 2C1νν1t−2˜µν t−1∑ s=1 s2˜µν−1−α1 + 2C∞ν2ν∞t−2˜µν t−1∑ s=1 s−2+α∞+2˜µν, ≤ ( ¯b2 + 2C1νν1ϕ2˜µν−α1 (t) + 2C∞ν2ν∞ϕ2˜µν+α∞−1 ) t−2˜µν. 2. The case α∈(0,1): applying the recursion ttimes in Eq. 13, we have ¯ht ≤¯h1 t−1∏ s=1 (1 −2˜µγs)    A1 t +C1 t−1∑ s=1 γsγ1 s t−1∏ τ=s+1 (1 −2˜µγτ)    A2 t +C∞ t−1∑ s=1 γ2 sγ∞ s t−1∏ τ=s+1 (1 −2˜µγτ)    A3 t (14) 5Without loss of generality, we assume that ν∞≥1.Real-time Optimisation for Online Learning in Auctions The derivation slightly differs from the case α= 1. Following Bach & Moulines (2011), one has: A1 t ≤exp ( −2˜µ t−1∑ s=1 γs ) A2 t ≤ γ1 ⌊t/2⌋ 2˜µ + exp ( −2˜µ t−1∑ τ=⌊t/2⌋ γτ )t−1∑ s=1 γsγ1 s A3 t ≤ γ⌊t/2⌋γ∞ ⌊t/2⌋ 2˜µ + exp ( −2˜µ t−1∑ τ=⌊t/2⌋ γτ )t−1∑ s=1 γ2 sγ∞ s . Using the expression ofγ, γ1 and γ∞, where α1 ≤1, together with ϕ1−α(x)−ϕ1−α(x/2) ≥x1−α/2 and ϕ1−α(t) ≥ t1−α/2 we have: A1 t ≤exp ( −2˜µνϕ1−α(t) ) ≤exp ( −˜µνt1−α) , A2 t ≤ν1 ˜µt−α1 + νν1 exp ( −˜µνt1−α)t−1∑ s=1 s−α−α1 ≤ν1 ˜µt−α1 + νν1 exp ( −˜µνt1−α) ϕ1−α−α1 (t), A3 t ≤νν∞ ˜µ tα∞−α + ν2ν∞exp ( −˜µνt1−α)t−1∑ s=1 sα∞−2α ≤νν∞ ˜µ tα∞−α + ν2ν∞exp ( −˜µνt1−α) ϕ1+α∞−2α(t). Putting everything together, we obtain the ﬁnal bound for α∈(0,1): ¯ht ≤ ( ¯b2 + C1νν1ϕ1−α−α1 (t) + C∞ν2ν∞ϕ1+α∞−2α(t) ) exp ( −˜µνt1−α) + C1 ν1 ˜µt−α1 + C∞ νν∞ ˜µ tα∞−α. D. Proof of Sec. 5 Theorem 3. Let F satisfy (A1) and (A3) with parameter µ, let Cand c be as in (A4) and k ∈ K. Then, by running CONV-OGA on Cwith a constant stepsize γ >0, for any t≥1 we have E(∥rt −r∗ k∥2) ≤ (¯b2 + C(γ,k)(t- 1) ) e−µcγ 2 t + 2C(γ,k) µc where C(γ,k) = O ( γ∥K−1R+ ∥1 + γ2∥k∥∞ ) . Proof. Similarly to the one of Th.2, this proof builds on the one of Bach & Moulines (2011). Since ΠF is µ−strongly log-concave, one has for all r∈C, (rt −r∗)T∇ΠF(r) ≤−ΠF(r)µ∥r−r∗∥2 ≤−˜µ∥r−r∗∥2 (15) where ˜µ= µc. As a result, although we do not assume the function to be strongly concave, it still enjoys a similar property in r∗on the bounded subset C. Then, let ht = ∥rt −r∗∥2 be the Lyapunov process (similarly to the proof of Thm. 1). Since the projection operator over Cis 1−Lipschitz, from Eq. 11, one has: E [ ht+1 −ht ⏐⏐Ft ] ≤2γ(rt −r∗)T∇ΠF(rt) + 2γ¯b∥∇2ΠF∥∞∥K−1R+ ∥1 + γ2 t ( 1 +¯b ( 1 + ∥∇ΠF∥∞ ) ∥k∥∞ ) . (16) Denoting C(γ,k) = 2γ¯b∥∇2ΠF∥∞∥K−1R+ ∥1 + γ2( 1 +¯b ( 1 +∥∇ΠF∥∞ ) ∥k∥∞ ) and ¯ht = E(ht), and then taking the expectation in Eq. 16, one obtains: ¯ht+1 ≤(1 −2γt˜µ)¯ht + C(γ,k). (17)Real-time Optimisation for Online Learning in Auctions Further, Eq. 17 is exactly the same as Eq. 25 in (Bach & Moulines, 2011) with different deﬁnitions for the constants, and the rest of the proof is identical. As a result, one has: ¯ht ≤ (¯h0 + C(γ,k)(t−1) ) exp ( −˜µγ 2 t ) + 2C(γ,k) ˜µ . Corollary 2. Let {Ft}t≥1 satisfy (A1), (A3), (A4), (A5) and k ∈K. Then, there exists Ξ(k,γ) and Ω(k,γ) such that CONV-OGA has a non-stationary regret of R(T) ≤Ξ(k,γ)T + Ω(k,γ)τ. Further, if the horizon T is known in advance, running CONV-OGA with γ = T−1 2 and ka kernel with ∥K−1R+ ∥1 ≤ T−1 2 and ∥k∥∞≤T 1 2 , then R(T) = O( √ T). Proof. Denoting {[si,ti]}τ i=1 the intervals on which the distribution is constant, we have R(T) = E ( T∑ t=1 ΠFt(r∗ t) −ΠFt(rt) ) ≤E ( T∑ t=1 ∥∇2ΠFt∥∞ 2 ∥r∗ t −rt∥2 2 ) ≤ τ∑ i=1 ∥∇2ΠFsi∥∞ 2 ti∑ t=si E ( ∥r∗ si −rt∥2 2 ) ≤ τ∑ i=1 ∥∇2ΠFsi∥∞ 2 ti−si+1∑ t=1 E ( ∥r∗ si −rt+si−1∥2 2 ) Applying Thm. 3 onE ( ∥r∗ si −rt+si−1∥2 2 ) and denoting ¯C(γ,k) = maxi∈[τ] 2γ¯b∥∇2ΠFsi∥∞∥K−1R+ ∥1 +γ2( 1+ ¯b ( 1+ ∥∇ΠFsi∥∞ ) ∥k∥∞ ) we obtain R(T) ≤ τ∑ i=1 ∥∇2ΠFsi∥∞ 2 ti−si+1∑ t=1 ((¯b2 + ¯C(γ,k)(t−1) ) e−µcγ 2 t + 2 ¯C(γ,k) µc ) ≤1 2 max i∈[τ] ∥∇2ΠFsi∥∞ ( 2 ¯C(γ,k) µc T + τ∑ i=1 ti−si+1∑ t=1 ¯b2e−µcγ 2 t + ¯C(γ,k)(t−1)e−µcγ 2 t ) ≤1 2 max i∈[τ] ∥∇2ΠFsi∥∞ ( 2 ¯C(γ,k) µc T + τ∑ i=1 ti−si∑ t=0 ¯b2e−µcγ 2 (t+1) + ¯C(γ,k)te−µcγ 2 (t+1) ) ≤1 2 max i∈[τ] ∥∇2ΠFsi∥∞ ( 2 ¯C(γ,k) µc T + τ∑ i=1 ( ¯b2e−µcγ 2 1 −e−µcγ 2 + ti−si∑ t=0 ¯C(γ,k)te−µcγ 2 (t+1) )) ≤1 2 max i∈[τ] ∥∇2ΠFsi∥∞ ( 2 ¯C(γ,k) µc T + ( ¯b2 e µcγ 2 −1τ + ¯C(γ,k)e−µcγ τ∑ i=1 1 −e−µcγ 2 (ti−si) ( 1 + (ti −si) ( 1 −e−µcγ 2 )) ( 1 −e−µcγ 2 )2 )) ≤1 2 max i∈[τ] ∥∇2ΠFsi∥∞ ( 2 ¯C(γ,k) µc T + ( ¯b2 e µcγ 2 −1τ + ¯C(γ,k)e−µcγ ( 1 −e−µcγ 2 )2 τ )) ≤1 2 max i∈[τ] ∥∇2ΠFsi∥∞ ( 2 ¯C(γ,k) µc T + ( ¯b2 e µcγ 2 −1 + ¯C(γ,k) ( e µcγ 2 −1 )2 ) τ ) Getting R(T) = O( √ T) when T is known in advance just amounts to plugging γ = 1√ T, ∥K −1R+ ∥1 ∝ 1√ T and ∥k∥∞∝ √ T in the last equation.",
      "meta_data": {
        "arxiv_id": "2010.10070v1",
        "authors": [
          "Lorenzo Croissant",
          "Marc Abeille",
          "Clément Calauzènes"
        ],
        "published_date": "2020-10-20T06:58:46Z",
        "pdf_url": "https://arxiv.org/pdf/2010.10070v1.pdf"
      }
    },
    {
      "title": "Understanding Adam Optimizer via Online Learning of Updates: Adam is FTRL in Disguise",
      "abstract": "Despite the success of the Adam optimizer in practice, the theoretical\nunderstanding of its algorithmic components still remains limited. In\nparticular, most existing analyses of Adam show the convergence rate that can\nbe simply achieved by non-adative algorithms like SGD. In this work, we provide\na different perspective based on online learning that underscores the\nimportance of Adam's algorithmic components. Inspired by Cutkosky et al.\n(2023), we consider the framework called online learning of updates/increments,\nwhere we choose the updates/increments of an optimizer based on an online\nlearner. With this framework, the design of a good optimizer is reduced to the\ndesign of a good online learner. Our main observation is that Adam corresponds\nto a principled online learning framework called Follow-the-Regularized-Leader\n(FTRL). Building on this observation, we study the benefits of its algorithmic\ncomponents from the online learning perspective.",
      "full_text": "Understanding Adam optimizer via Online Learning of Updates: Adam is FTRL in Disguise Kwangjun Ahn 1 2 Zhiyu Zhang 3 Yunbum Kook4 Yan Dai5 Abstract Despite the success of the Adam optimizer in prac- tice, the theoretical understanding of its algorith- mic components still remains limited. In particu- lar, most existing analyses of Adam show the con- vergence rate that can be simply achieved by non- adative algorithms like SGD. In this work, we pro- vide a different perspective based on online learn- ing that underscores the importance of Adam’s al- gorithmic components. Inspired by Cutkosky et al. (2023), we consider the framework called online learning of updates/increments, where we choose the updates/increments of an optimizer based on an online learner. With this framework, the design of a good optimizer is reduced to the design of a good online learner. Our main observation is that Adam corresponds to a principled online learning framework called Follow-the-Regularized-Leader (FTRL). Building on this observation, we study the benefits of its algorithmic components from the online learning perspective. 1. Introduction Let F : Rd → R be the (training) loss function we want to minimize. In machine learning applications, F is often minimized via an iterative optimization algorithm which starts at some initialization w0 and recursively updates wt+1 = wt + ∆t for t = 0, 1 . . . , (1.1) where ∆t denotes the update/increment 1 chosen by the algorithm at the t-th iteration. Practical optimizers often choose the update ∆t based on the past (stochastic) gradi- ents g1:t = (g1, . . . ,gt) where gt is the stochastic gradi- 1MIT 2Microsoft Research 3Havard University 4Georgia Tech 5Tsinghua University. Correspondence to: Kwangjun Ahn <kjahn@mit.edu>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). 1Sometimes, “update” refers to the iterate wt, but throughout this work, we mean the increment wt+1 − wt. ent of F collected during the t-th iteration. For instance, stochastic gradient descent (SGD) corresponds to choosing ∆t = −αtgt in (1.1) for some learning rate αt > 0. For training deep neural networks, one of the most popular choices is the Adam optimizer (Kingma and Ba, 2014). In particular, several recent works have observed that Adam and its variants are particularly effective for train- ing Transformer-based neural network models (Zhang et al., 2020b; Kunstner et al., 2023; Jiang et al., 2023; Pan and Li, 2023; Ahn et al., 2024). Given some learning rate γt > 0 and discounting factors β1, β2 ∈ (0, 1), Adam chooses ∆t on each coordinate i = 1, 2, . . . , dby combining g1:t as2 ∆t[i] = −γt (1 − β1) Pt s=1 βt−s 1 gs[i]q (1 − β2 2) Pt s=1(βt−s 2 gs[i])2 , where v[i] denotes the i-th coordinate of a vector v. For a streamlined notation, we define the scaled learning rate αt ← γt · (1−β1)/√ 1−β2 2 and consider ∆t[i] = −αt Pt s=1 βt−s 1 gs[i]qPt s=1(βt−s 2 gs[i])2 . (Adam) Compared to SGD, the notable components of Adam is the fact that it aggregates the past gradients g1:t (i.e., momen- tum) with the discounting factors β1, β2. Despite the prevalent application of Adam in deep learning, our theoretical grasp of its mechanics remains incomplete, particularly regarding the roles and significance of its core elements: the momentum and the discounting factors . Most existing theoretical works on Adam and its variants primarily focus on characterizing the convergence rate for convex functions or smooth nonconvex functions (Reddi et al., 2018; Zhou et al., 2019; Chen et al., 2019; Zou et al., 2019; Alacaoglu et al., 2020; Guo et al., 2021; D ´efossez et al., 2022; Zhang et al., 2022; Li et al., 2023; Wang et al., 2023) for which methods like SGD already achieve the min- imax optimal convergence rate. In fact, the latest works in this line (Li et al., 2023; Wang et al., 2023) both mention 2For simplicity, we remove the debiasing step and the appear- ance of ϵ in the denominator used in the original paper. 1 arXiv:2402.01567v2  [cs.LG]  30 May 2024Understanding Adam Optimizer via Online Learning of Updates that their convergence rate of Adam gets worse with momen- tum (Wang et al., 2023, §6) or the rate of Adam is no better than that of SGD (Li et al., 2023, §7). A notable exception is Crawshaw et al. (2022) where they show the benefits of momentum in a variant of Adam, under the generalized smoothness conditions of Zhang et al. (2020a). In this work, we take a different approach to understand Adam from a online learning perspective, as outlined below. 1.1. Our Approach and Main Results Our starting point is the main insight of Cutkosky et al. (2023) that the design of nonconvex optimizers falls under the scope of online linear optimization, an iconic setting in online learning. Specifically, one can regard the selection of the update ∆t based on g1:t as an online prediction procedure. Such a framework will be calledonline learning of updates/increments (OLU). Building on this framework, we then notice that it is im- portant to choose an online learner that performs well in dynamic environments (Cutkosky et al., 2023). Better dy- namic regret leads to better optimization performance (The- orem 2.1), and therefore, the design of good optimizers is reduced to designing good dynamic online learners. Along this line, our results can be summarized as follows: • (Section 2:) Our main observation is that the popular Adam optimizer corresponds to choosing a classical on- line learner called Follow-the-Regularized-Leader (FTRL) (Gordon, 1999; Kalai and Vempala, 2005; Shalev-Shwartz and Singer, 2006; Abernethy et al., 2008; Nesterov, 2009; Hazan and Kale, 2010). Specifically, when using the framework OLU, Adam is recovered by plugging in a discounted instance of FTRL well-suited for dynamic environment, which we call β-FTRL. • (Section 3:) We provide the dynamic regret guarantees of β-FTRL (Theorem 3.1 and Theorem 3.2) through a novel discounted-to-dynamic conversion. It gives us a new perspective on the role of Adam’s algorithmic compo- nents, namely the momentum and the discounting factors. Our results suggest that both components are crucial for designing a good dynamic online learner (see Subsec- tion 3.2). • (Section 4:) We justify the importance of a good dynamic regret, via its implications for optimization. Along the way, we discuss optimization settings for which Adam could be potentially beneficial. 2. Adam is FTRL in Disguise Iterative optimization algorithms are closely connected to adversarial online learning. For example, SGD is often an- alyzed through online gradient descent (OGD), its online learning counterpart. To exploit this connection in (1.1), the traditional approach is using an online learner to di- rectly choose the iterates wt, as demonstrated by Bottou (1998); Cesa-Bianchi et al. (2004); Duchi et al. (2011); Li and Orabona (2019); Ward et al. (2019) and many more. Diverging from this common approach, we consider a new approach due to Cutkosky et al. (2023) that applies the online learner to choose the updates ∆t. 2.1. Choosing Updates/Increments via Online Learning Consider an iconic setting of online learning called online linear optimization (OLO). For the consistency with our optimization algorithm (1.1), we will introduce OLO using slightly nonstandard notations. In each round t, the algo- rithm (or online learner) chooses a point ∆t ∈ Rd, and then receives a linear loss function ℓt(·) = ⟨vt+1, ·⟩ and suffers the loss of ℓt(∆t). In other words, it chooses ∆t based on the previous loss sequence v1:t := (v1, v2, . . . ,vt) and then receives the next loss vt+1. The performance of the on- line learner is measured by the regret against a comparator sequence u0:T−1, defined as RT (u0:T−1) := TX t=1 ⟨vt, ∆t−1 − ut−1⟩. (2.1) To be precise, (2.1) is called the dynamic regret in the lit- erature (Zinkevich, 2003). Another common metric, static regret, is a special case of (2.1) where all ut = u; this is denoted as RT (u). Now given an online learner LEARNER , we consider an optimization algorithm that outputs the ∆t in (1.1) using LEARNER . More formally, ∆t is chosen by LEARNER based on g1:t. (OLU) We call this framework online learning of updates (or on- line learning of increments). This framework was first pro- posed by Cutkosky et al. (2023) (under the name online-to- nonconvex conversion) to design algorithms that find critical points for nonsmooth and nonconvex stochastic optimiza- tion problems. Under OLU, we want LEARNER of choice to be a good online learner for dynamic environments, as summarized in the informal statement below. Theorem 2.1 (Importance of dynamic regret in OLU ; see Theorem 4.1) . In OLU, a better dynamic regret of LEARNER leads to a better optimization guarantee. There- fore, we want LEARNER to have a low dynamic regret. To understand this elegant reduction, let us give examples of how Theorem 4.1 is applied. Recent works (Cutkosky et al., 2023; Zhang and Cutkosky, 2024) choose an online gradient descent (OGD) (Zinkevich, 2003) as LEARNER to design algorithms for finding stationary points for nonconvex and 2Understanding Adam Optimizer via Online Learning of Updates nonsmooth functions. When LEARNER is chosen as OGD, the resulting optimization algorithm under OLU turns out to be SGD with momentum (Zhang and Cutkosky, 2024). However, OGD is known to require a careful tuning of learning rate (Zinkevich, 2003). What if we use an adaptive online learner as LEARNER ? Our main observation is that Adam can be recovered by choosing LEARNER as an adaptive version of Follow-the- Regularized-Leader that is well-suited for dynamic environ- ments, which we gradually elaborate. 2.2. Basics of Follow-the-Regularized-Leader (FTRL) Follow-the-Regularized-Leader (FTRL) is a classical algo- rithmic framework in online learning. Unlike the more intuitive descent-type algorithms, the key idea of FTRL is selecting the decisions by solving a convex optimization problem in each round. Throughout, we focus on the 1D case of OLO (d = 1) since the update of Adam is coordinate- wise. In particular, the overall regret for the d-dimension would be the sum of the regret of each coordinate. The 1D linear loss function is given by ℓt(∆) = vt+1∆ for vt+1 ∈ R. We use the subscript t + 1 to highlight that it is only revealed after deciding ∆t. FTRL relies on a nonnegative convex regularizer Φ, which is set to Φ = 1 2 | · |2 in this work. The algorithm initializes at ∆0 = 0 and in each round outputs ∆t = arg min x h 1 ηt Φ(x) + tX s=1 vsx i = −ηt tX s=1 vs , (FTRL) where the effective step size ηt > 0 is non-increasing in t. The remaining task is to choose good step sizes ηt. One prominent choice is the adaptive step size of the scale- free FTRL algorithm (Orabona and P ´al, 2018, §3) in the style of McMahan and Streeter (2010); Duchi et al. (2011). Scale-free FTRL chooses ηt = α/√Pt s=1 v2s based on a scaling factor α >0, resulting in the update ∆t = −α Pt s=1 vsqPt s=1 v2s . (2.2) Here if the denominator is zero, then we set the output ∆t = 0. The update (2.2) is independent of any constant scaling of loss sequence v1:t, making it a scale-free update. This is beneficial when the magnitude of loss sequence varies across different coordinates. We remark that the analysis of scale-free FTRL is in fact quite subtle, as echoed by McMahan (2017); Orabona and P´al (2018). Using a different proof strategy, we prove a static regret bound (Theorem A.1) of scale-free FTRL that slightly strengthens that of Orabona and P´al (2018). 2.3. Adam Corresponds to Discounted-FTRL Now back to OLU, let us use FTRL to choose the update ∆t. Denoting the coordinate-wise gradients in optimization by g1:t, a na¨ıve approach is to use FTRL directly by setting vt ← gt. Unfortunately, this approach is not a good one because FTRL is designed to achieve low static regret, while OLU requires low dynamic regret. In fact, it is shown by (Jacobsen and Cutkosky, 2022, Theorem 2) that Algorithms of the form (FTRL) are not good dynamic online learners. See also the lower bounds in Theorem 3.3. One could already see intuitively why this is the case: any algorithm in this form does not “forget the past”, as the output ∆t is the (regularized) minimizer of the cumulative loss Lt(x) := Pt s=1 vsx. Therefore, it is only competitive w.r.t. a fixed comparator that minimizes Lt(x), instead of a time-varying comparator sequence. To address this issue, our approach is to “ discount” the losses from the distant past. In particular, we implement this by gradually up-scaling the losses over time. The intuition is that when deciding the output ∆t, the recent losses would have much higher “weights” compared to older ones, which essentially makes the latter negligible. Theorem 2.2 (Informal; see Theorems 3.1 and 3.2) . For some β ∈ (0, 1), the discounted version of scale-free FTRL that internally replaces vt by β−tvt is a good dynamic online learner. Remarkably, plugging this discounted scale-free FTRL into OLU would almost recover Adam. There are just two small issues: in the Adam update, the (scaled) learning rate αt is time-varying, and we need two discounting factors β1 and β2 for the numerator and the denominator separately. It is not hard to fix this last bit, and we end up with an FTRL instance which given the input gt picks vt ← β−t 1 gt , and ηt = αt(β1/β2)t qPt s=1(β−s 2 gs)2 . (2.3) Collecting all the pieces above yields our first main result. Proposition 2.3 (Adam is discounted-FTRL in disguise). For some learning rate αt > 0 and discounting factors β1, β2 ∈ (0, 1], FTRL with (2.3) is equivalent to picking ∆t = −αt Pt s=1 βt−s 1 gsqPt s=1(βt−s 2 gs)2 . Applying it as a coordinate-wiseLEARNER in OLU recovers the Adam optimizer in Adam. Recall that OLU connects the problem of optimization to the well-established problem of dynamic regret minimization. 3Understanding Adam Optimizer via Online Learning of Updates Given Proposition 2.3, we make use of this connection to understand the components of Adam from the dynamic regret perspective. That is the main focus of the next section. Before getting into that, we briefly compare our approach with existing derivations of Adam based on FTRL. 2.4. Comparison with the Previous Approach In fact, Zheng and Kwok (2017) propose a derivation of Adam based on FTRL. However, their approach is quite different than ours, as we detail below. We first briefly summarize the approach of Zheng and Kwok (2017). Their main idea is to consider the “weighted” ver- sion of proximal-FTRL defined as wt = arg min w tX s=1 λs \u0010 ⟨gs, w⟩ + 1 2∥w − ws−1∥2 Qs \u0011 , for some weights {λs} and positive semi-definite matrices {Qs}. Given this, their main observation is that Adam roughly corresponds to this proximal-FTRL with carefully chosen {λs} and {Qs}. Although their motivation to explain Adam with a version of FTRL is similar to ours, we highlight that their approach is different than ours. In fact, our approach overcomes some of the limitations of Zheng and Kwok (2017). • Firstly, their derivation actually needs a heuristic adjust- ment of changing the anchor points of the regularizer from ws−1 to wt−1. A priori, it is not clear why such adjust- ment is needed, and to the best of our knowledge, there is no formal justification given. But with our approach, such an adjustment is naturally derived because under OLU, the online learner chooses the update/increment instead of the iterate. • Secondly, in Zheng and Kwok (2017), in order to recover Adam, they have to choose {ws} and {Qs} carefully, which also lacks justification. One of the main advantages of our approach is the fact that the discounting factors are theoretically justified via the dynamic regret perspective. More specifically, we show that without the discounting factor, FTRL is not a good dynamic learner. 3. Discounted-FTRL as a Dynamic Learner This section provides details on Theorem 2.2, focusing on the special case of Adam where β1, β2 = β for some β ∈ (0, 1], and αt = α for some α >0. From Proposition 2.3 and using the same notation as (2.2), this corresponds to the following coordinate-wise update rule: ∆t = −α Pt s=1 βt−svsqPt s=1(βt−svs)2 , (β-FTRL) and if the denominator is zero, we define ∆t = 0. We call this algorithm β-FTRL. With β = 1 , it exactly recovers scale-free FTRL (2.2) which is shown to be a poor dynamic learner (Jacobsen and Cutkosky, 2022). Therefore, we will focus on β <1 in the dynamic regret analysis. The earlier informal result (Theorem 2.2) is formalized in Theorem 3.1 (for unbounded domain) and Theorem 3.2 (for bounded domain). We provide the simplified versions here, deferring the detailed adaptive version to Theorem B.4. Theorem 3.1 (Dynamic regret of β-FTRL; unbounded domain). For a loss sequence v1:T , consider β-FTRL with β <1 and some constant α >0. Let Mβ := max t∈[1,T] \f\f\fPt s=1 βt−svs \f\f\f qPt s=1(βt−svs)2 . Then, for any comparator sequence u0:T−1 such that |ut| ≤ αMβ for all t, the dynamic regret RT (u0:T−1) is upper bounded by O  \u0000 αM2 β + MβP \u0001 G√1 − β + p 1 − β · αM2 βGT ! . Here, G := maxt∈[1:T] |vt|, and P := PT−1 t=1 |ut − ut−1| is the path length. We sketch the proof of Theorem 3.1 in Subsection 3.3. Theorem 3.1 may not seem straightforward, so let us start with a high level interpretation. First of all, the path lengthP is a standard complexity measure of the comparator u0:T−1 in the literature (Herbster and Warmuth, 2001), which we ex- amine closely in Subsection 3.1. In the context of dynamic online learning, the above bound could be reminiscent of a classical result from (Zinkevich, 2003, Theorem 2): on a do- main of diameter D, the dynamic regret of online gradient descent (OGD) with learning rate η can be bounded as RT (u0:T−1) ≤ O \u0012D2 + DP η + ηG2T \u0013 . (3.1) Intuitively, the choice ofη balances the two conflicting terms on the RHS, and a similar tradeoff remains as a recurring theme in the dynamic online learning literature (Hall and Willett, 2015; Zhang et al., 2018a; Jacobsen and Cutkosky, 2022). In an analogous manner, the discounting factor β in Theorem 3.1 largely serves the similar purpose of balancing conflicting factors. A rigorous discussion is deferred to Subsection 3.2. As a complementary result to Theorem 3.1, we also present a dynamic regret bound for the case of a priori bounded domain, where the outputs of online learner should lie in 4Understanding Adam Optimizer via Online Learning of Updates a bounded domain [−D, D]. In this case, we project the output of β-FTRL to the given domain: ∆t = −clipD  α Pt s=1 βt−svsqPt s=1(βt−svs)2   , (β-FTRLD) where clipD(x) := x min( D |x|, 1). Then, with the same no- tations of G and P as in Theorem 3.1, we get the following result (see Subsection B.5 for details). Theorem 3.2 (Dynamic regret of β-FTRLD; bounded domain). For D >0, consider any comparator sequence u0:T−1 such that |ut| ≤D for all t. Then for any loss sequence v1:T , β-FTRLD with β <1 and α = D has the dynamic regret RT (u0:T−1) upper bounded by O \u0012 DG√1 − β + GP 1 − β + p 1 − βDGT \u0013 . Compared to Theorem 3.1, the main difference is that the regret bound now holds simultaneously for all the loss se- quences v1:T of arbitrary size. The price to pay is the requirement of knowing D, and the multiplying factor on the path length P is slightly worse, i.e., (1 − β)−1/2 → (1 − β)−1. A sneak peek into the details: such a slightly worse factor is due to the projection step breaking the self- bounding property of β-FTRL, which says the discounted gradient sum Pt s=1 βt−svs can be controlled by the maxi- mum update magnitude sup |∆t| times the empirical vari- ance of gradients, i.e., qPt s=1(βt−svs)2. Interested read- ers may compare Subsections B.4 and B.5 for the subtleties. Moving forward, Theorems 3.1 and 3.2 constitute our main results characterizing the dynamic regret of β-FTRL. How- ever, there is still one missing piece. The earlier informal result (Theorem 2.2) states that β-FTRL is a “good” dynamic online learner. However, we have never explained which dynamic regret is good. Actually, the “goodness” criterion in dynamic online learning could be a bit subtle, as the typical sublinear-in-T metric in static online learning becomes vacuous. Next, we briefly provide this important background. 3.1. Basics of Dynamic Online Learning Dynamic online learning is intrinsically challenging. It is well-known that regardless of the algorithm, there exist loss and comparator sequences such that the dynamic regret is at least Ω(T). This is in stark contrast to static regret bounds in OLO, where the standard minimax optimal rate is the sublinear in T, e.g., O \u0000√ T \u0001 . To bypass this issue, the typical approach is throughinstance adaptivity. Each combination of the loss and comparator sequences can be associated to a complexity measure; the larger it is, the harder regret minimization becomes. Al- though it is impossible to guarantee sublinear-in-T regret bounds against the hardest problem instance, one can indeed guarantee a regret bound that depends on such a complex- ity measure. From this perspective, the study of dynamic online learning centers around finding suitable complexity measures and designing adaptive algorithms. Only considering the comparator sequence u0:T−1, the pre- dominant complexity measure is the path length P :=PT−1 t=1 ∥ut − ut−1∥ (Zinkevich, 2003), whose 1D special case is considered in Theorem 3.1 and Theorem 3.2. On a bounded domain with diameter D, the optimal dynamic re- gret bound is O(G √ DP T), which can be achieved through the classical result for OGD (3.1) with the P-dependent learning rate η = G−1p DP/T . On top of that, one could use a model selection approach (Zhang et al., 2018a) to avoid the infeasible oracle tuning (i.e., η depends on the un- known P), at the expense of increased computation. Recent works (Jacobsen and Cutkosky, 2022; Zhang et al., 2023) further extend such results to unbounded domains. The essential “goodness” of this O \u0000 G √ DP T \u0001 bound is due to P ≤ DT . In the worst case the bound is trivially O(DGT ), but if the comparator is easy (i.e., P = O(D)), then it becomes O \u0000 DG √ T \u0001 , recovering the well-known optimal static regret bound. In general, the goodness of a dynamic regret bound is usually measured by the exponents of both P and T (e.g., 1 2 and 1 2 in O \u0000 G √ DP T \u0001 ). Given this background, we now use the dynamic regret results of β-FTRL so far to interpret the role of two key components of Adam, namely the momentum (i.e., aggre- gating past gradients) and the discounting factor β (i.e., exponential moving average). 3.2. Benefits of Momentum and Discounting Factor Recall that a particular strength of the OLU framework is that it establishes one-to-one correspondence between op- timizers and their online learning counterparts. Thus we can compare a variety of optimizers by comparing their corre- sponding online learners, from the perspective of dynamic regret. Notice that β-FTRL from our analysis corresponds to the scaled parameterization of (Adam). Through that, our ultimate goal is to shed light on Adam’s algorithmic components — the momentum and the discounting factor. We first discuss the baseline online learners for this problem: • To understand the momentum, we pick the baselines as a family of “degenerate” online learners that induce non-momentum optimizers, such as SGD and AdaGrad (Duchi et al., 2011). Concretely, for the loss sequence v1:T , this family of LEARNER in OLU has the following generic update rule: for some coordinate-wise learning 5Understanding Adam Optimizer via Online Learning of Updates rate αt[i] > 0 ∀i, it outputs ∆t[i] = −αt[i]vt[i] . (3.2) For example, given a scalar αt > 0, SGD chooses the coordinate-wise learning rate αt[i] independently of the coordinates, i.e., αt[i] = αt , (SGD) while AdaGrad further employs a variance-based precon- ditioning, i.e., αt[i] = αtqPt s=1 vs[i]2 . (AdaGrad) The important observation is that compared to the(FTRL) update rule, the coordinate-wise update ∆t[i] in (3.2) only scales linearly with the most recent observation vt[i], instead of using the entire history v1:t[i]. In other words, from the optimization perspective, this family of algorithms does not make use of the past history of gradi- ents to decide the update direction. • To understand the discounting factor, we pick the baseline as β-FTRL with β = 1 (i.e., no discounting). Alterna- tively, if the domain is bounded, then we use the clipped version of β-FTRLD with β = 1 instead. In other words, such baselines correspond to scale-free FTRL (2.2). In light of Jacobsen and Cutkosky (2022), the case of β = 1 is not a good dynamic online learner, which we discuss more formally below. The following lower bound, inspired by Jacobsen and Cutkosky (2022), shows that the above baselines fail to achieve sublinear dynamic regret for a very benign example of P = O(1). See Subsection C.1 for a proof. Theorem 3.3 (Lower bounds for baselines) . Consider a 2D online linear optimization problem with the bounded domain [−1, 1]2. For any given T, there exist ( i) a loss sequence v1, . . . ,vT ∈ R2 with ∥vt∥ = 1 for all t, and (ii) a comparator sequence u0, . . . ,uT−1 ∈ [−1, 1]2 with the coordinate-wise path length PT−1 t=1 |ut[i] − ut−1[i]| ≤1 for both i = 1, 2, such that the following holds: • For all t, ut−1 ∈ arg minu∈[−1,1]2 ⟨vt, u⟩. • Any “non-momentum” online learner of the form (3.2) has the dynamic regret at least T − 3. • β-FTRLD with β = 1 and D = 1 has the dynamic regret at least (T − 3)/2. The first bullet point says that the constructed comparator se- quence u0:T−1 is the best ones w.r.t.the loss sequence v1:T , therefore the dynamic regret against such u0:T−1 is a good metric to measure the strength of dynamic online learners. Then, the rest of the theorem shows that the two baselines above (corresponding to optimizers without momentum or discounting factor under OLU) cannot guarantee low regret against u0:T−1, thus are not good dynamic online learners. In contrast, Theorem 3.2 shows that β-FTRLD with β <1 is a better dynamic online learner, and we make this very concrete through the following corollary. Again, it suffices to consider the 1D setting. Corollary 3.4. For D >0, consider any comparator se- quence u0:T−1 such that |ut| ≤D for all t. Then, given any constant c >0, β-FTRLD with β = 1 − cT−2/3 > 0 achieves the dynamic regret bound RT (u0:T−1) ≤ O \u0010 DGT 2/3 c 1/2 (1 + c−3/2 P/D) \u0011 , which enjoys a T 2/3 dependency on T. In particular, with the optimal tuning c = Θ \u0000 (P/D) 2/3 \u0001 , the bound becomes O(D 2/3GP 1/3T 2/3). The proof is deferred to Subsection C.2. We emphasize that in the lower bound example of Theorem 3.3, we have P = O(D), so the β <1 case achieves a sublinear dynamic regret bound of O(DGT 2/3). This suggests that in order to design a better dynamic online learner both momentum and discounting factor are necessary. A similar result can be developed for the case of unbounded domain under an assumption regarding the 1D OLO envi- ronment that generates the losses v1:T . Corollary 3.5. Assume the environment is well-behaved in the sense that Mβ ≤ M for all β ∈ (0, 1]. Consider any comparator sequence u0:T−1 such that |ut| ≤αM for all t. Then, given any constant c >0, β-FTRL with parameters α and β = 1 − cT−1 > 0 achieves the dynamic regret bound RT (u0:T−1) ≤ O \u0012 αM2G √ T c 1/2 \u0012 1 + c−1P αM \u0013\u0013 , which enjoys a √ T dependency on T. In particular, with the optimal tuning c = Θ \u0000 P/(αM) \u0001 , the bound becomes O \u0000 α 1/2M 3/2G √ P T \u0001 . In contrast, β-FTRL with the same α but the different β = 1 achieves a dynamic regret bound which is linear inP: RT (u0:T−1) ≤ O \u0010 MGP √ T \u0011 . Essentially, without the discounting factor we can show an O \u0000 P √ T \u0001 dynamic regret bound, but with discounting the bound can be improved to the optimal rate O \u0000√ P T \u0001 , under suitable tuning. This provides another evidence that discounting is helpful for designing a dynamic online learner. We remark that without discounting, the dynamic regret of O \u0000 P √ T \u0001 is unimprovable in light of the lower bound result (Jacobsen and Cutkosky, 2022, Theorem 3). 6Understanding Adam Optimizer via Online Learning of Updates 3.3. Proof Sketch of Theorem 3.1 Finally, we briefly sketch the proof of Theorem 3.1, the dy- namic regret bound of β-FTRL. The proof of Theorem 3.2 mostly follows the same analysis. Our analysis of the dy- namic regret relies on the following “discounted” regret. Definition 3.6 (β-discounted regret). For any discounting factor β ∈ (0, 1], the β-discounted regret is defined as RT;β(u) := TX t=1 βT−tvt(∆t−1 − u) . It is noteworthy that the discounted regret has been consid- ered in the concurrent works (Zhang et al., 2024; Jacobsen and Cutkosky, 2024) to adapt online learners to dynamic environments. Moreover, the discounted regret has found to be useful in designing nonconvex optimization algorithms, as shown in (Zhang and Cutkosky, 2024; Ahn and Cutkosky, 2024). In our work, we propose a generic conversion ap- proach to analyzing the dynamic regret using the discounted regret, called discounted-to-dynamic conversion (Theo- rem B.3), which could be of independent interest. At a high level, our analysis follows the following steps. (Theorem B.2)| {z } Discounted reg. of β-FTRL (Theorem B.3)= = = = = = = = = = =⇒ Discount-to-dynamic (Theorem 3.1)| {z } Dynamic reg. of β-FTRL 1. β-FTRL is the discounted version of scale-free FTRL, and the latter is associated to a static regret bound (Theo- rem A.1). Utilizing this relation, we naturally arrive at a discounted regret bound of β-FTRL (Theorem B.2). In- tuitively, it measures the performance of β-FTRL on an exponentially weighted, “local” look-back window that ends at time T. A particular strength is that the bound is anytime, i.e., it holds for all T simultaneously. 2. Next, consider the dynamic regret over the entire time horizon [1, T]. We can imagine partitioning [1, T] into concatenating subintervals, and on each of them the dy- namic regret can be approximately upper-bounded by suitable aggregations of the above discounted regret bound (modulo certain approximation error) — this is because the discounted regret bound mostly concerns the few recent rounds, so the dynamic regret can be con- trolled by the sum of such local metrics. Formalizing this argument results in the discounted-to-dynamic conver- sion in Theorem B.3: the dynamic regret of an algorithm is expressed using its discounted regret as an equality. Both Theorem 3.1 and 3.2 are obtained by combining these two elements, with slightly different ways of relaxation. 4. Implications for Optimization In this section, we provides the details of Theorem 2.1, which justifies the importance of the dynamic regret guar- antee of LEARNER in OLU, based on its implications for (non-convex) optimization. Recall that in OLU, for the functionF we want to minimize, we choose the update ∆t by the output of LEARNER on the loss sequence g1:t that are stochastic gradients of F. Formally, we assume F : Rd → R is differentiable but not necessarily convex. Following the notations of Cutkosky et al. (2023), given an iterate w and a random variable z, let GRAD (w, z) be the standard stochastic gradient oracle of F at w, satisfying Ez[GRAD (w, z)] = ∇F(w). We first introduce (Cutkosky et al., 2023, Theorem 7) that crucially connects the dynamic regret guarantee to the op- timization guarantee, justifying the importance of the dy- namic regret of LEARNER . Theorem 4.1 (Importance of dynamic regret in OLU). Consider the optimization algorithm (1.1) where • the update ∆t is chosen by LEARNER based on g1:t; • the gradient gt = GRAD (wt−1 + st∆t, zt), where the i.i.d. samples st ∼ Unif([0, 1]) and zt ∼ z. Then, for all T ≥ 0 and any comparator sequence u0:T−1, the iterate wT generated by (1.1) satisfies E[F(wT )]−F(w0) = E \" TX t=1 ⟨gt, ut−1⟩ + RT (u0:T−1) # , where RT (u0:T−1) is the regret bound of LEARNER . The proof is provided in Subsection D.1. The insight is that the nonconvexity can be handled by randomization (through st) at the gradient query, and if F is convex, it suffices to set st = 1 (which is more aligned with practice). Regarding the quantitative result, Theorem 4.1 precisely captures the informal claim from Theorem 2.1: Improving the dynamic regret of LEARNER directly leads to better optimization guarantee. Hence, the effectiveness of β-FTRL as a dynamic online learner (discussed in Section 3) supports the success of its optimizer counterpart, namely the update (Adam). To make this concrete, we revisit the lower bound examples from Theorem 3.3, and discuss the implications. 4.1. Revisiting lower bound example Consider an abstract scenario where we fix the stochastic gradient sequence g1:T to be given the lower bound example 7Understanding Adam Optimizer via Online Learning of Updates of Theorem 3.3. We compare the guarantees in Theorem 4.1, TX t=1 ⟨gt, ut−1⟩ + RT (u0:T−1) . (4.1) Then, the following is a direct corollary of Subsection 3.2. Corollary 4.2. Suppose g1:T and u0:T−1 are chosen as in the 2D example of Theorem 3.3. Then, the following statements hold: • Adam. If LEARNER is β-FTRLD with 1 − β = Θ(T−2/3) and D = 1, then (4.1) = −T + o(T). • No momentum (e.g., SGD/AdaGrad).If LEARNER has the form (3.2), then (4.1) ≥ −3. • No discounting. If LEARNER is β-FTRLD with β = 1 and D = 1, then (4.1) ≥ −1 2 T − 3 2 . Essentially, this result specializes the key insight from Sub- section 3.2, i.e., the benefits of the momentum and the dis- counting factor, to the corresponding optimizers. On the other hand, we acknowledge that the abstract sce- nario of fixing g1:T to be some desired sequence is not en- tirely practical, because (i) they should be stochastic gradi- ents of F and (ii) g1:T depends on the LEARNER of choice. Below, we build on the intuitions of this abstract example and present a concrete classification problem where Adam is more beneficial than the other two baselines. 4.2. Adam Could Be Effective for Sparse and Nonstationary Gradients Inspired by Duchi et al. (2011), we propose a concrete classification scenario for which we see the performance gap described in Corollary 4.2. At a high level, it is designed such that the associated gradient sequence imitates the one from our lower bound construction, Theorem 3.3. Classification of sparse data with small η. Consider the classification of (zi, yi) where zi ∈ Rd is the data vector with its label yi ∈ {±1}. We assume that each data vector zi is sparse. Concretely, we focus on the setting where the dataset consists of coordinate vectors and positive labels,i.e., {(zi, yi)}d i=1 where zi = ei and yi = 1 for i = 1, . . . , d. We consider the following regularized hinge loss ℓ(x) = max(0, 1 − x) + λ|x| for λ <1 , which prevents the classifier from becoming over-confident. See Figure 1 for the landscape of this hinge loss. Then, the training loss is given as F(w) = 1 d dX i=1 ℓ(yi⟨zi, w⟩) , −1 0 1 2 3 40 0.5 1 1.5 2 Figure 1.1D illustration of the regularized hinge loss ℓ(x) = max(0, 1 − x) +λ|x|. We illustrate the case λ = 1/4. Figure 2.Experimental results for the hinge loss classification. (Left) the case of zi = ei. (Right) the case of zi = ciei where ci ∼ Unif[0, 2]. The horizontal dotted line indicates the optimum value of F. All experiments are run for five different random seeds, and we plot the error shades (they are quite small and not conspicuous). and during iterationt, assume the algorithm receives a single data (zi(t), yi(t)), where i(t) is sampled from {1, . . . , d} uniformly at random. For experiments, we initialize at w0 = 0 and use a small learning rate, η = 0 .01, so that each coordinate takes multiple steps to approach the minimum w = 1. Besides, we choose d = 100 and λ = 1/4. Two different settings are considered: 1. Left plot of Figure 2: zi = ei for i = 1, . . . , d. 2. Right plot of Figure 2:zi = ciei, where ci ∼ Unif[0, 2] for i = 1, . . . , d. This setting allows the data vectors to have different magnitudes. From Figure 2, SGD exhibits sluggish progress owing to the sparse nature of stochastic gradients—–that is, only one coordinate is updated at each step. Moreover, setting β = 1 in Adam also results in suboptimal performance once the coordinate-wise iterate wt[i] exceeds 1 (for some coordinate i)—after that, the stochastic gradients point toward other di- rections. In contrast, adopting Adam with β <1 effectively addresses these issues, adeptly managing both the sparsity of updates and the non-stationarity of gradients. Next, we provide a possible qualitative explanation of this gap, from the dynamic regret perspective. Qualitative dynamic regret analysis. Since i(t) is sampled 8Understanding Adam Optimizer via Online Learning of Updates uniformly, it suffices to focus on the first coordinate and consider the 1D setting for simplicity. Then, since learning rate η is chosen small, starting from w0 = 0, the above set- ting could be abstractly thought as generating the following sparse stochastic gradient sequence gt = ( (1 − λ) · I[i(t)=1] if t ≲ τ −λ · I[i(t)=1] if t ≳ τ , (4.2) where τ denotes the first iteration such that wτ > 1. This can be seen as one of the simplest setting of a sparse and non-stationary gradient sequence, mirroring the construc- tion from Theorem 3.3. Now we compare the 1D version of the guarantee (4.1), i.e., the total loss of the LEARNER TX t=1 gtut−1 + RT (u0:T−1) = TX t=1 gt∆t−1 (4.3) for each LEARNER akin to Corollary 4.2: • No momentum. Due to the sparsity in gradient sequence (4.2), having no momentum incurs a large dynamic regret. More specifically, since E|gtgt−1| ≲ 1 d2 , we have E|gt∆t−1| ≲ 1 d2 , for “non-momentum” LEARNER of the form (3.2). There- fore, we have (4.3) = P t gt∆t−1 ≳ − 1 d2 T. • No discounting factor. Due to the non-stationarity in gradient sequence (4.2), having no discounting factor also incurs a large dynamic regret. In particular, β-FTRL with β = 1 generates the update ∆t with the same sign as −P t gt. In a typical run, the sign of −P t gt remains unchanged throughout, but the sign of gt flips once t ≳ τ. Hence, roughly speaking, the update ∆t does not have the “correct” sign (corresponding to the descent direction) after t ≳ τ, leading to (4.3) ≳ −(1 − λ)τ. In contrast, following Corollary 4.2, Adam achieves(4.3) ≤ −(1−λ)τ −λ(T −τ)+ o(T) = −λT −(1−2λ)τ +o(T), improving the above when τ is small. 5. Conclusion and Discussion This work presents a new perspective on the popular Adam optimizer, based on the framework of online learning of updates (OLU) (Cutkosky et al., 2023). Under OLU, our main observation is that Adam corresponds to choosing the dynamic version of FTRL that utilizes the discounting factor. We find this perspective quite advantageous as it gives new insights into the role of Adam’s algorithmic components, such as momentum and the exponential moving average. In fact, our perspective has already inspired a follow-up work by Ahn and Cutkosky (2024), where they show the optimal iteration complexity of Adam for finding stationary points under nonconvex and nonsmooth functions. Their analysis crucially utilizes our perspective that Adam corre- sponds to β-FTRL under OLU. In addition to (Ahn and Cutkosky, 2024), the findings in this work unlock several other important future directions. Below, we list a few of them. • Role of two discounting factors. As an initial effort, this work considers the case of β1 = β2. Given that the default choice in practice is β1 = 0.9 and β2 = 0.999, it would be important to understand the precise effect of choosing β1 < β2. • Other algorithms based on OLU. The framework OLU unlocks a new way to analyze optimization algorithms. As we highlighted in Subsection 3.2, OLU establishes the one-to-one correspondence between other optimizers and their online learning counterparts. The main scope of this work is to provide a better understanding of Adam specifically, and extending our framework to other popu- lar optimizers, such as RMSProp (Tieleman and Hinton, 2012), AdaDelta (Zeiler, 2012), Lion (Chen et al., 2023) etc, is an interesting future direction. We believe that un- derstand them based on our framework would offer new insights for them. • Algorithm design based on OLU. Our current dynamic regret analysis of β-FTRL requires knowledge of the en- vironment. Developing a version of β-FTRL that auto- matically adapts to the environment without prior knowl- edge might lead to more practical algorithms. Moreover, whether one can design practical optimizers based on recent advancements in dynamic online learning (e.g. Ja- cobsen and Cutkosky (2022); Zhang et al. (2023)) would be an important future direction. • Fine-grained analysis of Adam for practical settings. As discussed earlier, Adam has gained significant atten- tion due to its effectiveness in training language models. Recently, Kunstner et al. (2024) investigate key character- istics of the language modeling datasets that might have caused the difficulties in training. In particular, they iden- tify the heavy-tailed imbalance property, where there are a lot more infrequent words/tokens than frequent ones in most language modeling datasets. Further, they demon- strate this property as a main reason why Adam is partic- ularly effective at language modeling tasks (Zhang et al., 2020b). We find their main insights consistent with our claim in Subsection 4.2. The infrequent words in the dataset would likely lead to sparse and non-stationary gradients. Formally investigating this would be also an interesting future direction. 9Understanding Adam Optimizer via Online Learning of Updates Acknowledgements Kwangjun Ahn is indebted to Ashok Cutkosky for several inspiring conversations that led to this project. Kwangjun Ahn was supported by the ONR grant (N00014- 23-1-2299), MIT-IBM Watson, a Vannevar Bush fellowship from Office of the Secretary of Defense, and NSF CAREER award (1846088). Yunbum Kook was supported in part by NSF awards CCF-2007443 and CCF-2134105. Zhiyu Zhang was supported by the funding from Heng Yang. Impact Statement This paper provides a new perspective of understanding the Adam optimizer. This work is theoretical, and we do not see any immediate potential societal consequences. References Jacob D. Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An efficient algorithm for bandit linear optimization. In Conference on Learning Theory (COLT), pages 263–274. Omnipress, 2008. Kwangjun Ahn and Ashok Cutkosky. Adam with model exponential moving average is effective for nonconvex optimization, 2024. Kwangjun Ahn, Xiang Cheng, Minhak Song, Chulhee Yun, Ali Jadbabaie, and Suvrit Sra. Linear attention is (maybe) all you need (to understand Transformer optimization). In International Conference on Learning Representations (ICLR), 2024. Ahmet Alacaoglu, Yura Malitsky, Panayotis Mertikopoulos, and V olkan Cevher. A new regret analysis for Adam- type algorithms. In International Conference on Machine Learning (ICML), pages 202–210. PMLR, 2020. Leon Bottou. Online learning and stochastic approximations. On-Line Learning in Neural Networks, 17(9):142, 1998. Nicol`o Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line learning algo- rithms. Information Theory, IEEE Transactions on, 50 (9):2050–2057, 2004. Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, et al. Symbolic discovery of optimization algorithms. In Advances in Neural Informa- tion Processing Systems (NeurIPS), volume 36, 2023. Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of A class of Adam-type algorithms for non-convex optimization. In International Conference on Learning Representations (ICLR), 2019. Michael Crawshaw, Mingrui Liu, Francesco Orabona, Wei Zhang, and Zhenxun Zhuang. Robustness to unbounded smoothness of generalized SignSGD. In Advances in Neural Information Processing Systems (NeurIPS), vol- ume 35, pages 9955–9968, 2022. Ashok Cutkosky. Artificial constraints and hints for un- bounded online learning. In Conference on Learning Theory (COLT), pages 874–894, 2019. Ashok Cutkosky, Harsh Mehta, and Francesco Orabona. Optimal stochastic non-smooth non-convex optimization through online-to-non-convex conversion. In Interna- tional Conference on Machine Learning (ICML), volume 202, pages 6643–6670. PMLR, 2023. Amit Daniely, Alon Gonen, and Shai Shalev-Shwartz. Strongly adaptive online learning. In International Con- ference on Machine Learning (ICML), pages 1405–1411. PMLR, 2015. Alexandre D´efossez, Leon Bottou, Francis Bach, and Nico- las Usunier. A simple convergence proof of Adam and AdaGrad. Transactions on Machine Learning Research (TMLR), 2022. John Duchi, Elad Hazan, and Yoram Singer. Adaptive sub- gradient methods for online learning and stochastic opti- mization. Journal of Machine Learning Research (JMLR), 12(61):2121–2159, 2011. Geoffrey J. Gordon. Regret bounds for prediction problems. In Conference on Learning Theory (COLT), pages 29–40. ACM, 1999. Zhishuai Guo, Yi Xu, Wotao Yin, Rong Jin, and Tianbao Yang. A novel convergence analysis for algorithms of the Adam family. arXiv preprint arXiv:2112.03459, 2021. Eric C Hall and Rebecca M Willett. Online convex optimiza- tion in dynamic environments. IEEE Journal of Selected Topics in Signal Processing, 9(4):647–662, 2015. Elad Hazan and Satyen Kale. Extracting certainty from un- certainty: Regret bounded by variation in costs. Machine Learning, 80:165–188, 2010. Mark Herbster and Manfred K Warmuth. Tracking the best linear predictor. Journal of Machine Learning Research (JMLR), 1(281-309):10–1162, 2001. Andrew Jacobsen and Ashok Cutkosky. Parameter-free mir- ror descent. In Conference on Learning Theory (COLT), pages 4160–4211. PMLR, 2022. Andrew Jacobsen and Ashok Cutkosky. Online linear re- gression in dynamic environments via discounting. In International Conference on Machine Learning. PMLR, 2024. 10Understanding Adam Optimizer via Online Learning of Updates Kaiqi Jiang, Dhruv Malik, and Yuanzhi Li. How does adap- tive optimization impact local neural network geometry? In Advances in Neural Information Processing Systems (NeurIPS), 2023. Adam Kalai and Santosh Vempala. Efficient algorithms for online decision problems. Journal of Computer and System Sciences (JCSS), 71(3):291–307, 2005. Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2014. Frederik Kunstner, Jacques Chen, Jonathan Wilder Laving- ton, and Mark Schmidt. Noise is not the main factor behind the gap between SGD and Adam on Transformers, but sign descent might be. In International Conference on Learning Representations (ICLR), 2023. Frederik Kunstner, Robin Yadav, Alan Milligan, Mark Schmidt, and Alberto Bietti. Heavy-tailed class imbal- ance and why adam outperforms gradient descent on lan- guage models. arXiv preprint arXiv:2402.19449, 2024. Haochuan Li, Alexander Rakhlin, and Ali Jadbabaie. Con- vergence of Adam under relaxed assumptions. In Advances in Neural Information Processing Systems (NeurIPS), 2023. Xiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with adaptive stepsizes. In International Conference on Artificial Intelligence and Statistics (AISTATS), pages 983–992. PMLR, 2019. H Brendan McMahan. A survey of algorithms and analysis for adaptive online learning. The Journal of Machine Learning Research (JMLR), 18(1):3117–3166, 2017. H. Brendan McMahan and Matthew J. Streeter. Adaptive bound optimization for online convex optimization. In Conference on Learning Theory (COLT), pages 244–256. Omnipress, 2010. Yurii Nesterov. Primal-dual subgradient methods for convex problems. Mathematical Programming, 120(1):221–259, 2009. Francesco Orabona. A modern introduction to online learn- ing. arXiv preprint arXiv:1912.13213, 2019. Francesco Orabona and D´avid P´al. Scale-free online learn- ing. Theoretical Computer Science, 716:50–69, 2018. Yan Pan and Yuanzhi Li. Toward understanding why Adam converges faster than SGD for Transformers. arXiv preprint arXiv:2306.00204, 2023. Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of Adam and beyond. In International Conference on Learning Representations (ICLR), 2018. Shai Shalev-Shwartz and Yoram Singer. Online learn- ing meets optimization in the dual. In Conference on Learning Theory (COLT), volume 4005, pages 423–437. Springer, 2006. Tijmen Tieleman and Geoffrey Hinton. RMSProp: Divide the gradient by a running average of its recent magnitude. COURSERA Neural Networks Mach. Learn, 17, 2012. Tim van Erven. Why FTRL is better than Online Mirror De- scent. Personal Blog Post (Mathematics of Machine learn- ing), 2021. URL https://www.timvanerven. nl/blog/ftrl-vs-omd/#fn:unbounded. Bohan Wang, Jingwen Fu, Huishuai Zhang, Nanning Zheng, and Wei Chen. Closing the gap between the upper bound and lower bound of Adam’s iteration complexity. In Advances in Neural Information Processing Systems (NeurIPS), 2023. Rachel Ward, Xiaoxia Wu, and Leon Bottou. AdaGrad step- sizes: Sharp convergence over nonconvex landscapes. In International Conference on Machine Learning (ICML), pages 6677–6686. PMLR, 2019. Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012. Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates training: A theoretical justification for adaptivity. In International Conference on Learning Representations (ICLR), 2020a. Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank Reddi, Sanjiv Kumar, and Su- vrit Sra. Why are adaptive methods good for attention models? In Advances in Neural Information Process- ing Systems (NeurIPS), volume 33, pages 15383–15393, 2020b. Lijun Zhang, Shiyin Lu, and Zhi-Hua Zhou. Adaptive online learning in dynamic environments. InAdvances in Neural Information Processing Systems (NeurIPS), pages 1330– 1340, 2018a. Lijun Zhang, Tianbao Yang, and Zhi-Hua Zhou. Dynamic regret of strongly adaptive methods. In International Conference on Machine Learning (ICML), pages 5882– 5891. PMLR, 2018b. Qinzi Zhang and Ashok Cutkosky. Random scaling and momentum for non-smooth non-convex optimization. In International Conference on Machine Learning. PMLR, 2024. 11Understanding Adam Optimizer via Online Learning of Updates Yushun Zhang, Congliang Chen, Naichen Shi, Ruoyu Sun, and Zhi-Quan Luo. Adam can converge without any modification on update rules. In Advances in Neural Information Processing Systems (NeurIPS), volume 35, pages 28386–28399, 2022. Zhiyu Zhang, Ashok Cutkosky, and Ioannis Ch Paschalidis. Unconstrained dynamic regret via sparse coding. arXiv preprint arXiv:2301.13349, 2023. Zhiyu Zhang, David Bombara, and Heng Yang. Discounted adaptive online prediction. In International Conference on Machine Learning (ICML). PMLR, 2024. Shuai Zheng and James T Kwok. Follow the moving leader in deep learning. In International Conference on Machine Learning (ICML), pages 4110–4119. PMLR, 2017. Zhiming Zhou, Qingru Zhang, Guansong Lu, Hongwei Wang, Weinan Zhang, and Yong Yu. Adashift: Decorre- lation and convergence of adaptive learning rate methods. In International Conference on Learning Representations (ICLR), 2019. Martin Zinkevich. Online convex programming and gen- eralized infinitesimal gradient ascent. In International Conference on Machine Learning (ICML) , pages 928– 936, 2003. Fangyu Zou, Li Shen, Zequn Jie, Weizhong Zhang, and Wei Liu. A sufficient condition for convergences of Adam and RMSProp. In The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11127– 11135, 2019. 12Understanding Adam Optimizer via Online Learning of Updates A. Analysis of scale-free FTRL Recall from Section 2 that our construction starts with a gradient adaptive FTRL algorithm called scale-free FTRL (Orabona and P´al, 2018). This section presents a self-contained proof of its undiscounted static regret bound. Formally, we consider the 1D OLO problem introduced at the beginning of Section 2. Scale-free FTRL is defined as FTRL with the step size ηt = α/√Pt s=1 v2s, where α >0 is a scaling factor. Equivalently, it has the update rule ∆t = arg min x \" 1 ηt |x|2 + tX s=1 vsx # = −ηt tX s=1 vs = −α Pt s=1 vsqPt s=1 v2s . For well-posedness, if the denominator qPt s=1 v2s = 0, then we set the update to be ∆t = 0. Theorem A.1 (Static regret of scale-free FTRL). For all T > 0, loss sequence v1:T and comparator u ∈ R, scale-free FTRL guarantees the following static regret bound TX t=1 vt(∆t−1 − u) ≤ \u0012u2 2α + √ 2α \u0013vuut TX t=1 v2 t + 2 \u0012 max t∈[1,T] |∆t| \u0013\u0012 max t∈[1,T] |vt| \u0013 . We remark that van Erven (2021) directly applies the clipping technique from (Cutkosky, 2019) to obtain a similar regret bound as Theorem A.1, but in this way the associated algorithm is scale-free FTRL on the “clipped” gradients, rather than scale-FTRL itself. In contrast, we analyze the original scale-free FTRL algorithm for the purpose of explaining Adam (since in practice, Adam does not use the Cutkosky-style clipping on the stochastic gradients). This requires a slightly more involved analysis. Comparison with (Orabona and P´al, 2018). Before proving this theorem, we compare our regret bound with that of scale-free FTRL from (Orabona and P´al, 2018, Theorem 1). Their regret bound in the unconstrained domain setting (which means the domain diameter D defined in their Theorem 1 is infinite) is TX t=1 vt(∆t−1 − u) ≤ O  \u0000 u2 + 1 \u0001 vuut TX t=1 v2 t + √ T max t∈[1,T] |vt|   . Our bound replaces the √ T-factor by the maximum output magnitude (i.e., maxt∈[1,T] |∆t|), and our is better since |∆t| = α \f\f\fPt s=1 vs \f\f\f qPt s=1 v2s ≤ α √ t , which follows from the Cauchy-Schwarz inequality. We need such an improvement because in the discounted setting, the scaled loss sequence will have rapidly growing magnitude, which means this Cauchy-Schwarz step would be quite loose. Our proof makes a nontrivial use of the gradient clipping technique from (Cutkosky, 2019), which is also different from (Orabona and P´al, 2018, Theorem 1) and could be of independent interest. However, we acknowledge that directly modifying the argument of (Orabona and P´al, 2018) might achieve a similar goal. A.1. Proof of Theorem A.1 On the high level, the proof carefully combines the standard FTRL analysis, e.g., (Orabona, 2019, Lemma 7.1), and the gradient clipping technique of (Cutkosky, 2019). Step 1. We start with a preparatory step. Let τ be the index such that vt = 0 for all t ≤ τ, and vτ+1 ̸= 0. Without loss of generality, assume T > τ. Then, TX t=1 vt(∆t−1 − u) = TX t=τ+1 vt(∆t−1 − u) . 13Understanding Adam Optimizer via Online Learning of Updates On the RHS we have ∆τ = 0, and for all t > τ, ∆t is now well-defined by the “nice” gradient adaptive update rule (i.e., the denominator does not cause a problem) ∆t = −ηt tX s=1 vs = −α Pt s=1 vsqPt s=1 v2s . To proceed, for all t > τ, we define Ft(x) = 1 2ηt |x|2 + Pt s=1 vsx, which means that ∆t = arg minx Ft(x). Trivially, at the time index τ, we define Fτ (x) = 0 for all x. Step 2. The main part of the proof starts from the standard FTRL equality (Orabona, 2019, Lemma 7.1), TX t=1 vt(∆t−1 − u) = TX t=1 vt∆t−1 − \u0012 FT (u) − 1 ηT |u|2 \u0013 = 1 ηT |u|2 + T−1X t=τ [Ft(∆t) − Ft+1(∆t+1) + vt+1∆t] + FT (∆T ) − FT (u) ≤ 1 ηT |u|2 + T−1X t=τ [Ft(∆t) − Ft+1(∆t+1) + vt+1∆t] , where the last inequality follows since ∆T = arg minx FT (x). Consider the terms Ft(∆t) − Ft+1(∆t+1) + vt+1∆t in the above sum. Let us define the clipped gradient evt := clip√Pt−1 s=1 v2 s (vt) , where for any D ≥ 0, clipD(x) := x min( D |x|, 1). • For all t > τ, we have Ft(∆t) − Ft+1(∆t+1) + vt+1∆t = Ft(∆t) + vt+1∆t − Ft(∆t+1) − vt+1∆t+1 + 1 2ηt |∆t+1|2 − 1 2ηt+1 |∆t+1|2 ≤ Ft(∆t) + vt+1∆t − Ft(∆t+1) − vt+1∆t+1 ≤ Ft(∆t) + evt+1∆t − Ft(∆t+1) − evt+1∆t+1 + |vt+1 − evt+1|(|∆t| + |∆t+1|) . Following a standard fact of convex functions, e.g., (Orabona, 2019, Lemma 7.8), sinceFt(∆) + evt+1∆ is 1 ηt -strongly convex, it holds that Ft(∆t) + evt+1∆t − Ft(∆t+1) − evt+1∆t+1 ≤ Ft(∆t) + evt+1∆t − min ∆ [Ft(∆) + evt+1∆] ≤ ηt 2 ev2 t+1 . • As for the case of t = τ, since Fτ (∆τ ) = 0 and ∆τ = 0, Fτ (∆τ ) − Fτ+1(∆τ+1) + vτ+1∆τ = −Fτ+1(∆τ+1) = − 1 ητ+1 |∆τ+1|2 − τ+1X s=1 vs∆τ+1 ≤ −vτ+1∆τ+1 ≤ |vτ+1 − evτ+1||∆τ+1| . (evτ+1 = 0) Thus, we obtain the following bound: TX t=1 vt(∆t−1 − u) ≤ u2 2α vuut TX t=1 v2 t + α 2 T−1X t=τ+1 ev2 t+1qPt s=1 v2s + TX t=1 |vt − evt|(|∆t−1| + |∆t|) . 14Understanding Adam Optimizer via Online Learning of Updates Step 3. Finally, consider the two summation terms on the RHS one-by-one. We begin with the first term. ev2 t+1qPt s=1 v2s = √ 2 ev2 t+1q 2 Pt s=1 v2s ≤ √ 2 ev2 t+1q ev2 t+1 + Pt s=1 v2s = 2 √ 2 ev2 t+1 2 q ev2 t+1 + Pt s=1 v2s ≤ 2 √ 2 ev2 t+1q ev2 t+1 + Pt s=1 v2s + qPt s=1 v2s = 2 √ 2   vuutev2 t+1 + tX s=1 v2s − vuut tX s=1 v2s   . Thus, it follows that T−1X t=τ+1 ev2 t+1qPt s=1 v2s ≤ 2 √ 2 T−1X t=τ+1   vuutev2 t+1 + tX s=1 v2s − vuut tX s=1 v2s   ≤ 2 √ 2 T−1X t=τ+1   vuut t+1X s=1 v2s − vuut tX s=1 v2s   ≤ 2 √ 2 vuut TX t=1 v2 t . As for the second summation, we handle it similarly to (Cutkosky, 2019, Theorem 2). Defining Gt = maxs∈[1,t] |vs|, since |∆0| = 0, we have TX t=1 |vt − evt|(|∆t−1| + |∆t|) ≤ 2 \u0012 max t∈[1,T] |∆t| \u0013 TX t=1 |vt − evt| = 2 max \u0012 0, max t∈[1,T] |∆t| \u0013 TX t=1  |vt| − vuut t−1X s=1 v2s   ≤ 2 max \u0012 0, max t∈[1,T] |∆t| \u0013 TX t=1 (Gt − Gt−1) ≤ 2 \u0012 max t∈[1,T] |∆t| \u0013 GT . Combining the two upper bounds above completes the proof. B. Analysis of β-FTRL As discussed in Section 2, Adam corresponds to β-FTRL, the discounted version of scale-free FTRL, through the OLU framework. Thus, quantifying Adam’s performance comes down to analyzing the dynamic regret of β-FTRL. We now present the complete version of Theorem 3.1 (the dynamic regret bound of β-FTRL), fleshing out the proof sketch in Subsection 3.3. A main proof ingredient is our discounted-to-dynamic conversion. As a quick reminder, the formal setting considered is still the 1D OLO problem, where the output of the algorithm is denoted by ∆t ∈ R, and the loss function is denoted by ℓt(x) = vt+1x with vt+1 ∈ R. Step 1: Discounted regret. Our analysis starts with a concept called discounted regret, formalized in Definition 3.6. We recall the definition below for reader’s convenience. Definition B.1 (β-discounted regret). For any discounting factor β ∈ (0, 1], the β-discounted regret is defined as RT;β(u) := TX t=1 βT−tvt(∆t−1 − u) . When β = 1, the β-discounted regret recovers the standard static regret RT (u). The notational difference is simply an extra subscript β in the β-discounted regret, i.e., R·;β. Intuitively, β-FTRL should achieve good β-discounted regret, as long as scale-free FTRL achieves good static regret. This intuition follows from observations that β-FTRL is just scale-free FTRL with the “discounted losses” vt ← β−tvt, and that the β-discounted regret considers the loss sequence β−tvt instead of vt. We formalize this with the proof in Subsection B.1. 15Understanding Adam Optimizer via Online Learning of Updates Theorem B.2 (Discounted regret of β-FTRL). For all T > 0, loss sequence v1:T and comparator u ∈ R, β-FTRL guarantees the β-discounted regret bound RT;β(u) ≤ \u0012u2 2α + √ 2α \u0013vuut TX t=1 (βT−tvt)2 + 2 \u0012 max t∈[1,T] |∆t| \u0013\u0012 max t∈[1,T] |βT−tvt| \u0013 . Step 2: Discounted-to-dynamic conversion. The remaining task is to convert this discounted regret bound to a dynamic regret bound. We accomplish this via a general discounted-to-dynamic conversion, which is of independent interest. The idea is to partition the entire time horizon into subintervals, and then consider static comparators on each of them. To be precise, we consider a partition of [1, T] denoted by SN i=1[ai, bi], where a1 = 1, bi + 1 = ai+1 for all 1 ≤ i < N− 1, and bN = T. Then each partitioned interval [ai, bi] is coupled with an arbitrary fixed comparator ¯ui. We remark that this conversion is independent of the algorithm. For an algorithm A, RA T;β(u) and RA T (u0:T−1) denote its β-discounted regret (Definition 3.6) and its dynamic regret (2.1), respectively. See Subsection B.2 for the proof. Theorem B.3 (Discounted-to-dynamic conversion). Consider an arbitrary 1D OLO algorithm A. For all T >0, loss sequence v1:T and comparator sequence u0:T−1, the dynamic regret of A satisfies RA T (u0:T−1) = βRA T;β(¯uN ) + (1− β) NX i=1 X t∈[ai,bi] RA t;β(¯ui) + β N−1X i=1 \" biX t=1 βbi−tvt ! (¯ui+1 − ¯ui) # + NX i=1 X t∈[ai,bi] vt(¯ui − ut−1) , where SN i=1[ai, bi] is an arbitrary partition of of [1, T], and ¯u1, . . .¯uN ∈ R are also arbitrary. The remarkable aspect of this result is that it is an equality. That is, we do not lose anything through the conversion. Given a discounted regret bound of A, we can make use of this conversion by substituting RA T;β(¯uN ) and RA t;β(¯ui) with their discounted regret bounds, and then taking the infimum on the RHS w.r.t.the partition ∪i∈[N][ai, bi] and the choice of the “approximated comparator sequence” ¯u1, . . . ,¯uN . Step 3: Plugging in β-FTRL. Now we set A in the conversion to β-FTRL. See Subsection B.3 for the proof. Theorem B.4 (Dynamic regret of β-FTRL). Consider β-FTRL with a fixed α >0. Consider any loss sequence v1:T and any comparator sequence u0:T−1 s.t. |ut| ≤U. The dynamic regret (2.1) of the β-FTRL is bounded as RT (u0:T−1) ≤ \u0012U2 2α + √ 2α \u0013\" β q Vβ(v1:T ) + (1− β) TX t=1 q Vβ(v1:t) # + 2β \u0012 max t∈[1,T] |∆t| ·max t∈[1,T] |βT−tvt| \u0013 + 2(1 − β) TX t=1 \u0012 max s∈[1,t] |∆s| ·max s∈[1,t] |βt−svs| \u0013 + VARIATION , where Vβ(v1:t) := Pt s=1(βt−svs)2 is the discounted variance of the losses and VARIATION := inf   β N−1X i=1  biX t=1 βbi−tvt ! (¯ui+1 − ¯ui) + NX i=1 X t∈[ai,bi] vt(¯ui − ut−1)    , and the infimum in VARIATION is taken over all partitions SN i=1[ai, bi] of [1, T] and all choices of {¯ui}i∈N satisfying |¯ui| ≤U. 16Understanding Adam Optimizer via Online Learning of Updates In Theorem B.4, the variation term VARIATION consists of two terms. The first part β N−1X i=1  biX t=1 βbi−tvt ! (¯ui+1 − ¯ui) measures how fast the representative comparators ¯ui’s change across different subintervals, and we hence call it the “inter-partition variation”. The second term NX i=1 X t∈[ai,bi] vt(¯ui − ut−1) measures how different ut’s are from the representative comparators ¯ui’s within each subinterval, and we call it the “intra- partition variation”. A notable strength of the variation term is that it is the infimum over all partitions and ¯ui’s. In other words, the upper bound will automatically adapt to the best choice of partitions and ¯ui’s without knowing them explicitly. For instance, choosing ¯ui = P t∈[ai,bi] vtut−1 P t∈[ai,bi] vt would make the intra-partition variation term zero. Referring to Theorem B.4, we immediately obtain its simplified version in the main text, Theorem 3.1 with the proof in Subsection B.4. For the case of bounded comparators (β-FTRLD), the dynamic regret can be analyzed using almost the same strategy, which leads us to state Theorem 3.2 with the proof in Subsection B.5. B.1. Proof of Theorem B.2 β-FTRL is equivalent to scale-free FTRL with vt ← β−tvt. Therefore, applying Theorem A.1 with vt ← β−tvt leads to TX t=1 β−tvt(∆t−1 − u) ≤ \u0012u2 2α + √ 2α \u0013vuut TX t=1 (β−tvt)2 + 2 \u0012 max t∈[1,T] |∆t| \u0013\u0012 max t∈[1,T] \f\fβ−tvt \f\f \u0013 . Multiplying both sides by βT completes the proof. B.2. Proof of Theorem B.3 Overall, the proof draws inspiration from (Zhang et al., 2018b), where a similar partitioning argument was used to prove a dynamic regret guarantee of a strongly adaptive online learner (Daniely et al., 2015). Throughout the proof, we will omit the superscript A for brevity, since our argument is independent of specific algorithms. We start with a simple fact that connects the dynamic regret to the subinterval static regret. For any partition SN i=1[ai, bi] of [1, T] and any choices of {¯ui}i∈N , RT (u0:T−1) = NX i=1 biX t=ai vt(∆t−1 − ¯ui) + NX i=1 biX t=ai vt(¯ui − ut−1) . (B.1) To handle the static regret on the RHS, we use the following result. Lemma B.5. On any subinterval [a, b] ⊂ [1, T], with any u ∈ R, bX t=a vt(∆t−1 − u) = (1 − β) bX t=a Rt;β(u) + β (Rb;β(u) − Ra−1;β(u)) . Proof. For all t, notice that Rt;β(u) = tX s=1 βt−svs(∆s−1 − u) and Rt−1;β(u) = t−1X s=1 βt−svs(∆s−1 − u) , 17Understanding Adam Optimizer via Online Learning of Updates and thus Rt;β(u) − βRt−1;β(u) = vt(∆t−1 − u) . Summing over t ∈ [a, b], bX t=a vt(∆t−1 − u) = bX t=a Rt;β(u) − β bX t=a Rt−1;β(u) = (1 − β) bX t=a Rt;β(u) − βRa−1;β(u) + βRb;β(u) . Next, applying Lemma B.5 to each [ai, bi] in (B.1) yields: RT (u0:T−1) = (1 − β) NX i=1 X t∈[ai,bi] Rt;β(¯ui) + β NX i=1 [Rbi;β(¯ui) − Rai−1;β(¯ui)] + NX i=1 X t∈[ai,bi] vt(¯ui − ut−1) . Since ai − 1 = bi−1, the second term on the RHS can be rewritten as NX i=1 [Rbi;β(¯ui) − Rai−1;β(¯ui)] = RT;β(¯uN ) + N−1X i=1 [Rbi;β(¯ui) − Rbi;β(¯ui+1)] = RT;β(¯uN ) + N−1X i=1 \" biX t=1 βbi−tvt ! (¯ui+1 − ¯ui) # . Combining everything above completes the proof. B.3. Proof of Theorem B.4 Due to Theorem B.3, for any “approximated comparator sequence” ¯u1, . . . ,¯uN ∈ R with |¯ui| ≤U for i ∈ [N], we have RA T (u0:T−1) = βRA T;β(¯uN ) + (1− β) NX i=1 X t∈[ai,bi] RA t;β(¯ui) + β N−1X i=1 \" biX t=1 βbi−tvt ! (¯ui+1 − ¯ui) # + NX i=1 X t∈[ai,bi] vt(¯ui − ut−1) . Using Theorem B.2 and |¯ui| ≤U, RA T;β(¯uN ) ≤ \u0012¯u2 N 2α + √ 2α \u0013vuut TX t=1 (βT−tvt)2 + 2 \u0012 max t∈[1,T] |∆t| \u0013\u0012 max t∈[1:T] |βT−tvt| \u0013 ≤ \u0012U2 2α + √ 2α \u0013q Vβ(v1:T ) + 2 max t∈[1,T] |∆t| ·max t∈[1:T] |βT−tvt|, and similarly, for any t and ¯ui, RA t;β(¯ui) ≤ \u0012U2 2α + √ 2α \u0013q Vβ(v1:t) + 2 max s∈[1,t] |∆s| ·max s∈[1:t] |βt−svs|. Putting these bounds into the equality and taking the infimum on the RHS (over the partition and the {¯ui}i∈N sequence satisfying |¯ui| ≤U) complete the proof. 18Understanding Adam Optimizer via Online Learning of Updates B.4. Simplification for unbounded domain: Theorem 3.1 Theorem 3.1 follows as a corollary of Theorem B.4 with the partition ST t=1{t}, U = αDβ and ¯ut = ut for all t. With such choices, we have VARIATION ≤ β T−1X t=1  tX s=1 βt−svs ! (ut − ut−1) . Recall that Mβ := maxt∈[1,T] | Pt s=1 βt−svs|√Pt s=1(βt−svs)2 . Hence, it follows that \f\f\f\f\f tX s=1 βt−svs \f\f\f\f\f ≤ Mβ q Vβ(v1:t) . Therefore, the VARIATION term in Theorem B.4 is reduced to VARIATION ≤ βMβ T−1X t=1 q Vβ(v1:t) |ut − ut+1| , and thus the bound becomes (notice that U = αMβ and β <1) RT (u0:T−1) ≤ \u00121 2αM2 β + √ 2α \u0013\"q Vβ(v1:T ) + (1− β) TX t=1 q Vβ(v1:t) # + 2αMβG [1 + (1− β)T] + Mβ T−1X t=1 q Vβ(v1:t) |ut − ut−1| . (B.2) For all t, using β <1 Vβ(v1:t) = tX s=1 (βt−svs)2 ≤ G2 ∞X i=0 β2i = G2 1 − β2 < G2 1 − β . Therefore, RT (u0:T−1) ≤ \u00121 2αM2 β + √ 2α \u0013\u0014 G√1 − β + p 1 − βGT \u0015 + 2αMβG [1 + (1− β)T] + MβGP√1 − β = O \u0012\u0000 α + αM2 β + MβP \u0001 G√1 − β + \u0000 α + αM2 β \u0001p 1 − βGT \u0013 . Hence, we arrive at Theorem 3.1 presented in the main paper. B.5. Simplification for bounded domain Theorem 3.2 For the case of bounded comparators, i.e., |ut| ≤D, we consider β-FTRLD, the D-clipped version of β-FTRL: ∆t = −clipD  α Pt s=1 βt−svsqPt s=1(βt−svs)2   , (β-FTRLD) where clipD(x) = x min( D |x|, 1). With β-FTRLD, since |∆t| ≤D at each step, the following regret bound holds. The proof boils down to verifying that the entire proof strategy of Theorem B.2 goes through even with projection. Theorem B.6 (Discounted regret of β-FTRLD). For allT >0, loss sequence v1:T and comparator |u| ≤D, the discounted regret bound of β-FTRLD is RT;β(u) ≤ \u0012u2 2α + √ 2α \u0013vuut TX t=1 (βT−tvt)2 + 2D \u0012 max t∈[1:T] |βT−tvt| \u0013 . 19Understanding Adam Optimizer via Online Learning of Updates Now based on this discounted regret bound, we prove the claimed dynamic regret bound in Theorem 3.2. Similar to the proof of Theorem 3.1, from Theorem B.4, we choose the partition to be ∪T t=1{t}, and let U = D and ¯ut = ut for all t. With such choices, we have VARIATION ≤ β T−1X t=1 \f\f\f\f\f tX s=1 βt−svs \f\f\f\f\f|ut − ut−1| . Therefore, the upper bound in Theorem B.4 becomes (notice that α = U = D and β <1) RT (u0:T−1) ≤ 2D \"q Vβ(v1:T ) + (1− β) TX t=1 q Vβ(v1:t) # + 2DG [1 + (1− β)T] + β T−1X t=1 \f\f\f\f\f tX s=1 βt−svs \f\f\f\f\f|ut − ut−1| ≤ 4D \"q Vβ(v1:T ) + (1− β) TX t=1 q Vβ(v1:t) # + T−1X t=1 \f\f\f\f\f tX s=1 βt−svs \f\f\f\f\f|ut − ut−1| . Now notice that for all t, since β <1, Vβ(v1:t) = tX s=1 (βt−svs)2 ≤ G2 ∞X i=0 β2i = G2 1 − β2 < G2 1 − β , and \f\f\f\f\f tX s=1 βt−svs \f\f\f\f\f ≤ G ∞X t=0 βt ≤ G 1 − β . Substituting these bounds back to the bound on RT (u0:T−1), we obtain RT (u0:T−1) ≤ 4DG \u0012 1√1 − β + p 1 − β · T \u0013 + GP 1 − β . Therefore, we arrive at Theorem 3.2 presented in the main paper. C. Benefits of momentum and discounting factor This section presents omitted details from Subsection 3.2. The goal is to justify the benefits of Adam’s algorithmic components, namely the momentum and the discounting factor. C.1. Proof of lower bounds (Theorem 3.3) For simplicity, we start by assuming T is a multiple of 4. Consider the following loss sequence: For 1 ≤ t ≤ T/2, vt = ( (1, 0) for t even, (0, 1) for t odd. For T/2 < t≤ T, vt = ( (−1, 0) for t even, (0, −1) for t odd. The comparator sequence u0:T−1 is given as ut = (−1, −1) for 0 ≤ t ≤ T/2 − 1 and ut = (1, 1) for t ≥ T/2. Then, we have PT t=1⟨vt, ut−1⟩ = −T. As for the total loss, • Consider the baseline (3.2). Since vt[i]vt+1[i] = 0 for all t ≥ 1 and i = 1, 2, we have TX t=1 ⟨vt, ∆t−1⟩ = TX t=1 2X i=1 vt[i]∆t−1[i] = − TX t=1 2X i=1 αt−1[i]vt−1[i]vt[i] = 0. 20Understanding Adam Optimizer via Online Learning of Updates • Consider β-FTRLD with β = 1 and D = 1. Recall its coordinate-wise update rule, ∆t[i] = −clip1  α Pt s=1 vs[i]qPt s=1 vs[i]2   . From the loss sequence, it follows that Pt s=1 vs[i] ≥ 0 for all t, and hence, we have −1 ≤ ∆t[i] ≤ 0 for all t ≥ 0 and i = 1, 2. Hence, PT t=1⟨vt, ∆t−1⟩ ≥ −T/2. This completes the proof under the assumption that T is a multiple of 4. For general T, let bT be the largest integer less or equal to T which is a multiple of 4. Then, we define v1: bT and u1: bT−1 as the aforementioned loss and comparator sequences (with T replaced by bT), and this yields lower bounds on PbT t=1⟨vt, ∆t−1 − ut−1⟩. As for the time index satisfying bT < t≤ T, we define vt = (0, 0) and ut−1 = ubT−1. In this way, altogether, PT t=1⟨vt, ∆t−1 − ut−1⟩ = PbT t=1⟨vt, ∆t−1 − ut−1⟩, and the lower bounds for the latter can be applied. C.2. Proof of Corollary 3.4 Using Theorem 3.2 with β = 1 − cT−2/3, RT (u0:T−1) ≲ DG√1 − β + DG 1 − β + p 1 − βDGT (Theorem 3.2) = DGT 1/3 √c + P GT2/3 c + √cDGT 2/3 ≲ DGT 2/3c1/2 \u0010 1 + c−3/2P D \u0011 . With the optimal tuning c = Θ \u0000 (P/D) 2/3 \u0001 , it becomes O(GD 2/3P 1/3T 2/3). C.3. Proof of Corollary 3.5 Consider β < 1 first. Since the environment is well-behaved with constant M, we can invoke Theorem 3.1 with Mβ there replaced by M. Notice that M is independent of β, therefore at the end we may tune β using M. Concretely, using Theorem 3.1 with β = 1 − cT−1, RT (u0:T−1) ≲ \u0000 αM2 + MP \u0001 G√1 − β + p 1 − βαM 2GT (Theorem 3.1) = MG √ T c 1/2 \u0012αM + P c + αM \u0013 ≲ αM2G √ T c 1/2 \u0012 1 + c−1P αM \u0013 . With the optimal tuning c = Θ \u0000 P/(αM) \u0001 , it becomes O(α 1/2M 3/2GP 1/2T 1/2). Next, consider β = 1. We follow the same analysis in Subsection B.4 until (B.2), before plugging in any β. Then, instead of using β <1 there, we plug in β = 1, which yields R[0,T−1](u0:T−1) ≤ \u00121 2αM2 + √ 2α \u0013p V1(v1:T ) + 2αMG + M T−2X t=0 p V1(v1:t+1) |ut − ut+1| ≲ \u0010 αM2 + √ 2α \u0011 G √ T + MG √ T T−2X t=0 |ut − ut+1| ≲ MGP √ T . (T ≫ 1, and P ≫ αM) 21Understanding Adam Optimizer via Online Learning of Updates D. Details on optimization D.1. Proof of Theorem 4.1 Since F is differentiable, the fundamental theorem of calculus implies that for all x, y ∈ Rd, F(y) − F(x) = R1 0⟨∇F(x + t(y − x)), y − x⟩dt. Hence, we have F(wt+1) − F(wt) = Z 1 0 ⟨∇F(wt + s∆t), ∆t⟩ds = Es∼Unif([0,1])⟨∇F(wt + s∆t), ∆t⟩ Now, summing over t and telescoping yield the desired equality. 22",
      "meta_data": {
        "arxiv_id": "2402.01567v2",
        "authors": [
          "Kwangjun Ahn",
          "Zhiyu Zhang",
          "Yunbum Kook",
          "Yan Dai"
        ],
        "published_date": "2024-02-02T17:00:17Z",
        "pdf_url": "https://arxiv.org/pdf/2402.01567v2.pdf"
      }
    },
    {
      "title": "Efficient Non-stationary Online Learning by Wavelets with Applications to Online Distribution Shift Adaptation"
    },
    {
      "title": "TTN: A Domain-Shift Aware Batch Normalization in Test-Time Adaptation",
      "abstract": "This paper proposes a novel batch normalization strategy for test-time\nadaptation. Recent test-time adaptation methods heavily rely on the modified\nbatch normalization, i.e., transductive batch normalization (TBN), which\ncalculates the mean and the variance from the current test batch rather than\nusing the running mean and variance obtained from the source data, i.e.,\nconventional batch normalization (CBN). Adopting TBN that employs test batch\nstatistics mitigates the performance degradation caused by the domain shift.\nHowever, re-estimating normalization statistics using test data depends on\nimpractical assumptions that a test batch should be large enough and be drawn\nfrom i.i.d. stream, and we observed that the previous methods with TBN show\ncritical performance drop without the assumptions. In this paper, we identify\nthat CBN and TBN are in a trade-off relationship and present a new test-time\nnormalization (TTN) method that interpolates the statistics by adjusting the\nimportance between CBN and TBN according to the domain-shift sensitivity of\neach BN layer. Our proposed TTN improves model robustness to shifted domains\nacross a wide range of batch sizes and in various realistic evaluation\nscenarios. TTN is widely applicable to other test-time adaptation methods that\nrely on updating model parameters via backpropagation. We demonstrate that\nadopting TTN further improves their performance and achieves state-of-the-art\nperformance in various standard benchmarks.",
      "full_text": "Published as a conference paper at ICLR 2023 TTN: A D OMAIN -SHIFT AWARE BATCH NORMALIZA - TION IN TEST-TIME ADAPTATION Hyesu Lim1,2∗, Byeonggeun Kim ∗, Jaegul Choo 2, Sungha Choi 1‡ 1Qualcomm AI Research†, 2KAIST ABSTRACT This paper proposes a novel batch normalization strategy for test-time adaptation. Recent test-time adaptation methods heavily rely on the modiﬁed batch normal- ization, i.e., transductive batch normalization (TBN), which calculates the mean and the variance from the current test batch rather than using the running mean and variance obtained from source data,i.e., conventional batch normalization (CBN). Adopting TBN that employs test batch statistics mitigates the performance degra- dation caused by the domain shift. However, re-estimating normalization statistics using test data depends on impractical assumptions that a test batch should be large enough and be drawn from i.i.d. stream, and we observed that the previous meth- ods with TBN show critical performance drop without the assumptions. In this paper, we identify that CBN and TBN are in a trade-off relationship and present a new test-time normalization(TTN) method that interpolates the standardization statistics by adjusting the importance between CBN and TBN according to the domain-shift sensitivity of each BN layer. Our proposed TTN improves model robustness to shifted domains across a wide range of batch sizes and in various realistic evaluation scenarios. TTN is widely applicable to other test-time adap- tation methods that rely on updating model parameters via backpropagation. We demonstrate that adopting TTN further improves their performance and achieves state-of-the-art performance in various standard benchmarks. 1 I NTRODUCTION When we deploy deep neural networks (DNNs) trained on the source domain into test environments (i.e., target domains), the model performance on the target domain deteriorates due to the domain shift from the source domain. For instance, in autonomous driving, a well-trained DNNs model may exhibit signiﬁcant performance degradation at test time due to environmental changes, such as camera sensors, weather, and region (Choi et al., 2021; Lee et al., 2022; Kim et al., 2022b). Test-time adaptation (TTA) has emerged to tackle the distribution shift between source and target domains during test time (Sun et al., 2020; Wang et al., 2020). Recent TTA approaches (Wang et al., 2020; Choi et al., 2022; Liu et al., 2021) address this issue by 1) (re-)estimating normaliza- tion statistics from current test input and 2) optimizing model parameters in unsupervised manner, such as entropy minimization (Grandvalet & Bengio, 2004; Long et al., 2016; Vu et al., 2019) and self-supervised losses (Sun et al., 2020; Liu et al., 2021). In particular, the former focused on the weakness of conventional batch normalization (CBN) (Ioffe & Szegedy, 2015) for domain shift in a test time. As described in Fig. 1(b), when standardizing target feature activations using source statistics, which are collected from the training data, the activations can be transformed into an un- intended feature space, resulting in misclassiﬁcation. To this end, the TTA approaches (Wang et al., 2020; Choi et al., 2022; Wang et al., 2022) have heavily depended on the direct use of test batch statistics to ﬁx such an invalid transformation in BN layers, called transductive BN (TBN) (Nado et al., 2020; Schneider et al., 2020; Bronskill et al., 2020) (see Fig. 1(c)). The approaches utilizing TBN showed promising results but have mainly been assessed in limited evaluation settings (Wang et al., 2020; Choi et al., 2022; Liu et al., 2021). For instance, such evalua- tion settings assume large test batch sizes (e.g., 200 or more) and a single stationary distribution shift ∗Work completed while at Qualcomm Technologies, Inc. ‡Corresponding author. †Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc. 1 arXiv:2302.05155v2  [cs.CV]  18 Feb 2023Published as a conference paper at ICLR 2023 0 20 40 60 80 100 CBN TBN Ours TENT TENT+Ours SWR SWR+Ours Error rate (%) 200 64 16 4 2 1 TTN(Ours) TENT+TTN(Ours) SWR+TTN(Ours) Test batch size (a) Valid output using CBN (b) Invalid output using CBN (c) Valid output using TBN (d) Performance drops in small test batches using TBN ● Class A ● Class B Source mean● Source features Test features⨯ Test meanStandardize Figure 1: Trade-off between CBN & TBN.In conceptual illustrations (a), (b), and (c), the depicted standardization only considers making the feature distribution have a zero mean, disregarding mak- ing it have unit variance. When the source and test distributions are different, and the test batch size is large, (b) test features can be wrongly standardized when using CBN (Ioffe & Szegedy, 2015), but (c) TBN (Nado et al., 2020) can provide a valid output. (d) Error rates (↓) on shifted domains (CIFAR-10-C). TBN and TBN applied (TENT (Wang et al., 2020), SWR (Choi et al., 2022)) meth- ods suffer from severe performance drop when the batch size becomes small, while TTN (Ours) improves overall performance. (i.e., single corruption). Recent studies suggest more practical evaluation scenarios based on small batch sizes (Mirza et al., 2022; Hu et al., 2021; Khurana et al., 2021) or continuously changing data distribution during test time (Wang et al., 2022). We show that the performance of existing methods signiﬁcantly drops once their impractical assumptions of the evaluation settings are violated. For example, as shown in Fig. 1(d), TBN (Nado et al., 2020) and TBN applied methods suffer from severe performance drop when the test batch size becomes small, while CBN is irrelevant to the test batch sizes. We identify that CBN and TBN are in a trade-off relationship (Fig. 1), in the sense that one of each shows its strength when the other falls apart. To tackle this problem, we present a novel test-time normalization (TTN)strategy that controls the trade-off between CBN and TBN by adjusting the importance of source and test batch statistics according to the domain-shift sensitivity of each BN layer. Intuitively, we linearly interpolate be- tween CBN and TBN so that TBN has a larger weight than CBN if the standardization needs to be adapted toward the test data. We optimize the interpolating weight after the pre-training but before the test time, which we refer to as the post-training phase. Speciﬁcally, given a pre-trained model, we ﬁrst estimate channel-wise sensitivity of the afﬁne parameters in BN layers to domain shift by analyzing the gradients from the back-propagation of two input images, clean input and its aug- mented one (simulating unseen distribution). Afterward, we optimize the interpolating weight using the channel-wise sensitivity replacing BN with the TTN layers. It is noteworthy that none of the pre-trained model weights are modiﬁed, but we only train newly added interpolating weight. We empirically show that TTN outperforms existing TTA methods in realistic evaluation settings, i.e., with a wide range of test batch sizes for single, mixed, and continuously changing domain adaptation through extensive experiments on image classiﬁcation and semantic segmentation tasks. TTN as a stand-alone method shows compatible results with the state-of-the-art methods and com- bining our TTN with the baselines even boosts their performance in overall scenarios. Moreover, TTN applied methods ﬂexibly adapt to new target domains while sufﬁciently preserving the source knowledge. No action other than computing per batch statistics (which can be done simultaneously to the inference) is needed in test-time; TTN is compatible with other TTA methods without requir- ing additional computation cost. Our contributions are summarized as follows: • We propose a novel domain-shift aware test-time normalization (TTN) layer that combines source and test batch statistics using channel-wise interpolating weights considering the sensitivity to domain shift in order to ﬂexibly adapt to new target domains while preserving the well-trained source knowledge. 2Published as a conference paper at ICLR 2023 Per-batch        Frozen        Optimize S Standardize Ƹ𝑧 = 𝑧𝑖𝑛 −𝜇 𝜎 T Affine Transform 𝑧𝑜𝑢𝑡 = 𝛾⋅ Ƹ𝑧+𝛽 … … CBN CBN CBN … Pre-train (CBN) (a-1) Post-train (TTN)          Test time (TTN) (a) Overall procedure of train and test phases (b) Comparison of BN layers 𝑧𝑖𝑛 S Ƹ𝑧 T 𝑧𝑜𝑢𝑡 𝛾,𝛽+ 1−𝛼 𝜇𝑠,𝜎𝑠𝜇𝑖𝑛,𝜎𝑖𝑛 𝑧𝑖𝑛 S Ƹ𝑧 T 𝑧𝑜𝑢𝑡 𝜇𝑠,𝜎𝑠 𝛾,𝛽 CBN in test time TBN in test time (b-1) TTN(Ours) in post-train 𝑧𝑖𝑛 S Ƹ𝑧 T 𝑧𝑜𝑢𝑡 𝜇𝑖𝑛,𝜎𝑖𝑛 𝛾,𝛽 𝛼 𝑧𝑖𝑛 S Ƹ𝑧 T 𝑧𝑜𝑢𝑡 𝛾,𝛽+ 1−𝛼 𝜇𝑠,𝜎𝑠𝜇𝑖𝑛,𝜎𝑖𝑛 (b-2) TTN(Ours) in test time 𝛼 Figure 2: Method overview. (a) We introduce an additional training phase between pre-train and test time called (a-1) post-training phase. (b) Our proposed TTN layer combines per-batch statistics and frozen source statistics with interpolating weight α, which is (b-1) optimized in post-training phase and (b-2) ﬁxed in test time. • To show the broad applicability of our proposed TTN, which does not alter training or test- time schemes, we show that adding TTN to existing TTA methods signiﬁcantly improves the performance across a wide range of test batch sizes (from 200 to 1) and in three realistic evaluation scenarios; stationary, continuously changing, and mixed domain adaptation. • We evaluate our method through extensive experiments on image classiﬁcation using CIFAR-10/100-C, and ImageNet-C (Hendrycks & Dietterich, 2018) and semantic segmen- tation task using CityScapes (Cordts et al., 2016), BDD-100K (Yu et al., 2020), Mapil- lary (Neuhold et al., 2017), GTA V (Richter et al., 2016), and SYNTHIA (Ros et al., 2016). 2 M ETHODOLOGY In this section, we describe our method, the Test-Time Normalization(TTN) layer, whose design is suitable for test-time adaptation (TTA) in practical usages out of the large batch size and i.i.d assumptions during a test time. We ﬁrst deﬁne the problem setup in Section 2.1 and present our pro- posed TTN layers in Section 2.2. Finally, we discuss how we optimize TTN layers in Sections 2.3. 2.1 P ROBLEM SETUP Let the train and test data be DS and DT and the corresponding probability distributions be PS and PT, respectively, where DS and DT share the output space, i.e., {yi}∼D S = {yi}∼D T. The covariate shift in TTA is deﬁned asPS(x) ̸= PT(x) where PS(y|x) =PT(y|x) (Quinonero-Candela et al., 2008). A model, fθ, with parameters θ, is trained with a mini-batch, BS = {(xi,yi)}|BS| i=1 , from source data DS, where xi is an example and yi is the corresponding label. During the test, fθ encounters a test batch BT ∼DT, and the objective of TTA is correctly managing the test batch from the different distribution. To simulate more practical TTA, we mainly consider two modiﬁcations: (1) various test batch sizes, |BT|, where small batch size indicates small latency while handling the test data online, and (2) multi, N-target domains, DT = {DT,i}N i=1. Under this setting, each test batch BT is drawn by one of the test domains inDT, where DT may consist of a single target domain, multiple target domains, or mixture of target domains. 2.2 T EST-TIME NORMALIZATION LAYER We denote an input of a BN layer as z ∈RBCHW , forming a mini-batch size of B. The mean and variance of z are µand σ2, respectively, which are computed as follows: µc = 1 BHW B∑ b H∑ h W∑ w zbchw, σ 2 c = 1 BHW B∑ b H∑ h W∑ w (zbchw −µc)2, (1) where µand σ2 are in RC, and C, H, and W stand for the number of channels, dimension of height, and that of width, respectively. Based on µand σ2, the source statistics µs,σ2 s ∈RC are usually estimated with exponential moving average over the training data. 3Published as a conference paper at ICLR 2023 Conv CBN ReLU Conv ReLU Conv CBN ReLU FC… CBN 𝑥′ ℒ𝐶𝐸 (a-1) Gradient of affine parameters 𝛾,𝛽 ∇𝛾 (1),∇𝛽 (1) ∇𝛾 (2),∇𝛽 (2) ∇𝛾 (𝐿),∇𝛽 (𝐿) ∇𝛾′ (1),∇𝛽′ (1) ∇𝛾′ (2),∇𝛽′ (2) ∇𝛾′ (𝐿),∇𝛽′ (𝐿) Conv CBN ReLU Conv ReLU Conv CBN ReLU FC… CBN 𝑥 ℒ𝐶𝐸 (a-2) Prior 𝒜 (b-1) Initialize 𝛼 with prior 𝒜 … 1       0 (a) Obtain prior 𝒜 (b) Optimize 𝛼 Per-batch statistics        Frozen source statistics        Gradient flow Conv ReLU Conv ReLU Conv ReLU FC…𝑥′ ℒ𝐶𝐸 +ℒ𝑀𝑆𝐸 TTNTTNTTN 𝛼 1−𝛼 TTN 𝜇𝑠 (𝑙) 𝜇𝑖𝑛 (𝑙) 𝐶𝑙 𝛼(𝑙,𝑐) + 1−𝛼(𝑙,𝑐) … … ++ 𝛼(𝑙,𝐶𝑙) 1−𝛼(𝑙,𝐶𝑙) (b-2) Optimize 𝛼 C1 C2 C𝐿 Figure 3: Two stages in post-training phase. (a) Given a pre-trained model, which uses CBN, and its training data, we obtain prior knowledge of each BN layer. (a-1) We ﬁrst compute gradients of afﬁne parameters in each BN layer from clean x and augmented input x′and obtain the gradient distance score (Eq. 4). (a-2) For BN layers with larger distance score, we put more importance on current batch statistics than source statistics ( i.e., large α), and we deﬁne prior Aaccordingly (Eq. 5). (b) After obtaining prior A, we substitute BN layers from CBN to TTN.(b-1) Initializing α with prior A, (b-2) we optimize αusing CE and MSE loss (Eq. 6) with augmented training data x′. In BN layers, input z is ﬁrst standardized with statistics µand σ2 and then is scaled and shifted with learnable parameters γ and β in RC. The standardization uses current input batch statistics during training and uses estimated source statistics µs and σ2 s at test time (Fig. 2(b)). To address domain shifts in test time, we adjust the source statistics by combining the source and the test mini-batch statistics (Singh & Shrivastava, 2019; Summers & Dinneen, 2019) with a learnable interpolating weight α∈RC ranges [0,1]. Precisely, TTN standardizes a feature with ˜µ= αµ+ (1−α)µs, ˜σ2 = ασ2 + (1−α)σ2 s + α(1 −α)(µ−µs)2, (2) while using the same afﬁne parameters, γ and β. Note that we have different mixing ratios αc for every layer and channel. 2.3 P OST TRAINING Like Choi et al. (2022), we introduce an additional training phase, the post-training (after pre- training but before testing), to optimize the mixing parameters α in Eq. 2 (Fig. 2(a)). Note that all parameters except αare frozen and we have access to the labeled source data during the post- training. We ﬁrst obtain prior knowledge Aof αby identifying which layers and their channels are sensitive to domain shifts. Then, we optimizeαwith the prior knowledge and an additional objective term. The overall procedure is depicted in Fig. 3 and the pseudocode is provided in appendix A.3. Obtain Prior A. To identify which BN layers and corresponding channels are sensitive to domain shifts, we simulate the domain shifts by augmenting1 the clean image, i.e., original training data, and make a pair of (clean x, domain-shifted x′) images, where the semantic information is shared. To analyze in which layer and channel the standardization statistics should be corrected, we consider the standardized features ˆz(l,c) of z(l,c), for a channel index cat a layer l, whose input is clean x. We compare ˆz(l,c) to that of domain-shifted one, ˆz′(l,c) from x′. Since the pre-trained CBN uses the same µ(l,c) s and σ(l,c) s for both inputs, the difference between ˆz(l,c) and ˆz′(l,c) is caused by the domain discrepancy between xand x′. We argue that if the difference is signiﬁcant, the parameter at (l,c) is sensitive to the domain shift, i.e., intensely affected by the domain shift, and hence the standardization statistics at (l,c) should be adapted towards the shifted input. Drawing inspiration from Choi et al. (2022), we measure the domain-shift sensitivity by comparing gradients. Since the standardized feature ˆz is scaled and shifted by γ and β in each BN layer, we compare the gradients of afﬁne parameters γ and β, ∇γ and ∇β, respectively, to measure the dissimilarity of ˆzand ˆz′. As described in Fig. 3(a-1), we collect the ∇γ and ∇β using cross-entropy 1It is noteworthy that the post-training phase is robust to the choice of data augmentation types. Ablation study results and discussions are provided in the appendix B.4. 4Published as a conference paper at ICLR 2023 loss, LCE. To this end, we introduce a gradient distance score, d(l,c) ∈R for each channel cat layer las follows: s= 1 N N∑ i=1 gi ·g′ i ∥gi∥∥g′ i∥, (3) d(l,c) = 1−1 2 ( s(l,c) γ + s(l,c) β ) , (4) where (g,g′) is (∇(l,c) γ ,∇(l,c) γ′ ) and (∇(l,c) β ,∇(l,c) β′ ) for s(l,c) γ and s(l,c) β , respectively,N is the number of training data, and the resulting d(l,c) ∈[0,1]. Once we obtain sγ and sβ from Eq. 3, we conduct min-max normalization over all s(l,c) γ and s(l,c) β , before computing Eq. 4. To magnify the relative difference, we take the square as a ﬁnal step and denote the result as a prior A(Fig. 3(a-2)): A= [d(1,.),d(2,.),...,d (L,.)]2, (5) where d(l,.) = [d(l,c)]Cl c=1. Optimize α. The goal of optimizing α is to make the combined statistics correctly standardize the features when the input is sampled from an arbitrary target domain. After obtaining the prior A, we replace CBN with TTN layers while keeping the afﬁne parameters. Then, we initialize the interpolating weights α with A, which represents in which layer and channel the standardization statistics need to be adapted using test batch statistics (see Fig. 3(b-1)). To simulate distribution shifts, we use augmented training data. Expecting the model to make consistent predictions either given clean or augmented inputs, we use cross-entropy loss LCE. Furthermore, to prevent αfrom moving too far from the initial value A, we use mean-squared error loss LMSE between αand the prior A, i.e., LMSE = ∥α−A∥2 as a constraint. Total lossLcan be written asL= LCE +λLMSE (6), where λis a weighting hyperparameter (Details are provided in the appendix A.1 & B.1). 3 E XPERIMENTS In image classiﬁcation, we evaluate TTN for corruption robustness in realistic evaluation settings, i.e., where the test batch size can be variant and where the target domain can be either stationary, continuously changing, or mixed with multiple domains. Additionally, we further validate TTN on domain generalization benchmarks incorporating natural domain shifts ( e.g., changes in camera sensors, weather, time, and region) in semantic segmentation. 3.1 E XPERIMENTAL SETUP Given models pre-trained on clean source data, we optimize TTN parameter αwith the augmented source data in the post-training phase. Afterward, we evaluate our post-trained model on the cor- rupted target data. Implementation details are provided in the appendix A.1. Datasets and models. We use corruption benchmark datasets CIFAR-10/100-C and ImageNet-C, which consist of 15 types of common corruptions at ﬁve severity levels (Hendrycks & Dietterich, 2018). Each corruption is applied to test images of the clean datasets (CIFAR-10/100 and Ima- geNet). We use a training set of the clean dataset for post-training and the corrupted dataset for evaluation. As backbone models, we used WideResNet-40-2 (Hendrycks et al., 2019) trained on CIFAR-10/100, and ResNet-50 (He et al., 2016) trained on ImageNet. To validate our method in se- mantic segmentation, we conduct experiments on Cityscapes (Cordts et al., 2016), BDD-100K (Yu et al., 2020), Mapillary (Neuhold et al., 2017), GTA V (Richter et al., 2016), and SYNTHIA (Ros et al., 2016) datasets, in accordance with the experimental setup for domain generalization proposed in RobustNet (Choi et al., 2021). Baselines. To demonstrate that TTN successfully controls the trade-off between CBN and TBN, we compare TTN with (1) AdaptiveBN (Schneider et al., 2020), (2) α-BN (You et al., 2021) and (3) MixNorm (Hu et al., 2021), which combines or takes the moving average of the source and the test batch statistics with a pre-deﬁned hyperparameter ( i.e., a constant α). The following baselines are suggested on top of TBN (Nado et al., 2020); (4) TENT (Wang et al., 2020) optimizes BN afﬁne parameters via entropy minimization. (5) SWR (Choi et al., 2022) updates the entire model parame- ters considering the domain-shift sensitivity. (6) CoTTA (Wang et al., 2022) ensembles the output of 5Published as a conference paper at ICLR 2023 Table 1: Single domain adaptation on corruption benchmark. Error rate ( ↓) averaged over 15 corruptions with severity level 5 using WideResNet-40-2 as a backbone for each test batch size. We used reported results of MixNorm with ﬁxed parameters from the original paper and denoted as ∗. In appendix B.3, we provide variants of TTN, which show stronger performance for small test batch. CIFAR-10-C CIFAR-100-C Method 200 64 16 4 2 1 Avg. 200 64 16 4 2 1 Avg. Source (CBN)18.27 18.27 18.27 18.27 18.27 18.27 18.27 46.75 46.75 46.75 46.75 46.75 46.7546.75 Norm TBN 14.49 15.02 17.10 26.28 35.65 90.00 33.09 39.25 40.21 44.03 59.10 80.65 99.0460.38 AdaptiveBN12.21 12.31 12.89 14.51 15.79 16.14 13.98 36.56 36.85 38.19 41.18 43.2644.0140.01 α-BN 13.78 13.77 13.89 14.54 15.1615.4714.44 39.72 39.85 39.99 41.3442.6645.6441.53 MixNorm∗ 13.85 14.41 14.23 14.60 (B=5) - 15.0914.44 - - - - - - - Ours (TTN)11.6711.8012.13 13.93 15.8317.99 13.8935.5835.8436.7341.0846.6757.7142.27 Optim. TENT 12.08 14.78 16.90 25.61 35.69 90.00 32.51 35.52 39.90 43.78 59.02 80.68 99.0259.65 +Ours (TTN)11.2811.5212.04 13.95 15.8417.9413.77 35.1635.5736.5541.1846.6358.3342.24 SWR 10.26 13.51 16.61 27.33 40.48 90.04 33.04 32.6837.41 43.15 59.90 87.07 99.0559.88 +Ours (TTN)9.92 11.7713.41 18.02 24.0961.5623.13 32.8635.1338.6649.8060.7280.9049.68 Table 2: Continuously changing domain adaptation on corruption benchmark. Error rate (↓) averaged over 15 corruptions with severity level 5 using WideResNet-40-2 as backbone for each test batch size. We omitted ‘Norm’ methods results in this table since they are eqaul to that of Table 1. CIFAR-10-C CIFAR-100-C Method 200 64 16 4 2 1 Avg. 200 64 16 4 2 1 Avg. Source (CBN)18.27 18.27 18.27 18.27 18.27 18.2718.27 46.75 46.75 46.75 46.75 46.75 46.7546.75 Ours (TTN)11.6711.80 12.13 13.9315.8317.9913.89 35.58 35.84 36.73 41.08 46.67 57.71 42.27 Optim. CoTTA 12.46 14.60 21.26 45.69 58.87 90.0040.48 39.75 42.20 52.94 73.69 87.66 98.9965.87 TENT 12.54 13.52 15.69 26.23 35.77 90.0032.29 36.11 37.90 43.78 58.71 81.76 99.0459.55 +Ours (TTN)11.4411.60 12.08 16.1418.3622.4015.33 43.50 37.60 38.28 44.60 54.2980.63 49.82 SWR 11.04 11.53 13.90 23.99 34.02 90.0030.75 34.16 35.79 40.71 58.15 80.55 99.0362.56 +Ours (TTN)10.09 10.5111.28 14.2916.6784.1224.49 33.0934.07 36.15 42.41 53.63 93.08 48.74 augmented test inputs, updates the entire model parameters using a consistency loss between student and teacher models, and stochastically restores the pre-trained model. We refer to TBN, (1), (2), and (3) as normalization-based methods (Norm), the other as optimization-based methods (Optim.), and denote the pre-trained model using CBN as ‘source’. Evaluation scenarios. To show that TTN performs robust on various test batch sizes, we conduct experiments with test batch sizes of 200, 64, 16, 4, 2, and 1. We evaluate our method in three evalu- ation scenarios; single, continuously changing, and mixed domain adaptation. In the single domain adaptation, the model is optimized for one corruption type and then reset before adapting to the subsequent corruption, following the evaluation setting from TENT and SWR. In the continuously changing adaptation (Wang et al., 2022), the model is continuously adapted to 15 corruption types (w/o the reset), which is more realistic because it is impractical to precisely indicate when the data distribution has shifted in the real world. Finally, to simulate the non-stationary target domain where various domains coexist, we evaluate methods in the mixed domain adaptation setting, where a sin- gle batch contains multiple domains. We use a severity level of 5 (Hendrycks & Dietterich, 2018) for all experiments. It is noteworthy that we use a single checkpoint of TTN parameter αfor each dataset across all experimental settings. 3.2 E XPERIMENTS ON IMAGE CLASSIFICATION Tables 1, 2, and 3 show error rates on corruption benchmark datasets in three different evaluation scenarios; single domain, continuously changing, and mixed domain adaptation, respectively. Note that the performance of normalization-based methods in the single (Table 1) and in the continu- ously changing (Table 2) settings are identical. Tables 4 and 5 show the adaptation performance on the source and class imbalanced target domains, respectively. More results and discussions are provided in the appendix B, importantly, including results on ImageNet-C (B.5). Robustness to practical settings. In Table 1, 2, and 3, TTN and TTN applied methods show robust performance over the test batch size ranges from 200 to 1. Comparing with normalization-based baselines, we demonstrate that TTN, which uses channel-wisely optimized combining rateα, shows better results than deﬁning α as a constant hyperparameter, which can be considered as a special 6Published as a conference paper at ICLR 2023 Table 3: Mixed domain adaptation on corruption benchmark. Error rate (↓) of mixed domain with severity level 5 using WideResNet-40-2 as backbone for each test batch size. We used the reported results of MixNorm with ﬁxed parameters from the original paper and denoted them as ∗. CIFAR-10-C CIFAR-100-C Method 200 64 16 4 2 1 Avg. 200 64 16 4 2 1 Avg. Source (CBN)18.27 18.27 18.27 18.27 18.27 18.2718.27 46.75 46.75 46.75 46.75 46.75 46.7546.75 Norm TBN 14.99 15.29 17.38 26.65 35.59 90.0033.31 39.88 40.48 43.73 59.11 80.30 98.9160.40 AdaptiveBN12.62 12.48 12.97 14.59 15.74 16.0214.07 36.88 36.86 38.49 41.43 43.38 44.3140.23 α-BN 13.78 13.78 13.99 14.6115.07 15.2014.41 40.25 40.11 40.47 41.6442.39 43.8141.45 MixNorm∗ 18.80 18.80 18.80 18.80 18.80 18.8018.80 - - - - - - - Ours (TTN)12.1612.1912.3413.9615.5517.8314.00 36.2436.2336.8541.0145.8555.5241.95 Optim. TENT 14.33 14.97 17.30 26.07 35.37 90.0033.01 39.36 40.01 43.33 58.98 80.55 98.9260.19 +Ours (TTN)12.0212.0412.2013.7715.4216.4013.64 36.2936.2336.8941.3846.6557.9542.57 SWR 13.24 13.06 16.57 26.08 38.65 91.0359.54 37.84 37.93 44.37 59.50 78.66 98.9533.10 +Ours (TTN)11.8911.6513.3717.0523.5064.1050.29 36.4936.5139.6046.2058.2084.7623.59 case of TTN; TBN and α-BN corresponds to α = 1and 0.1, respectively. More comparisons with different constant αare provided in the appendix B.2. It is noteworthy that TTN as a stand-alone method favorably compares with optimization-based baselines in all three scenarios. Table 4: Source domain adaptation. Error rate (↓) on CIFAR-10 using WideResNet-40-2. Method Test batch size Avg.200 64 16 4 2 1 Source (CBN)4.92 4.92 4.92 4.92 4.92 4.924.92 Norm TBN 6.41 6.60 8.64 17.65 26.08 90.0025.90Ours (TTN)4.885.115.357.27 9.45 9.96 7.00 Optim. TENT 6.15 6.45 8.61 17.61 26.20 90.0032.2+Ours (TTN)4.935.115.327.22 9.3810.217.02 SWR 5.63 6.01 8.25 17.49 26.32 90.0025.62+Ours (TTN)4.795.025.516.68 7.91 9.34 6.54 Table 5: Class imbalanced target domain. Error rate (↓) averaged over 15 corruptions of CIFAR- 10-C with severity level 5 using WideResNet-40- 2. Details are provided in the appendix A.2. Method Test batch size Avg.200 64 16 4 2 1 Source (CBN)18.27 18.27 18.27 18.27 18.27 18.2718.27 TBN 77.60 76.66 77.72 78.59 77.84 90.0079.74Ours (TTN)35.7535.1334.9232.5128.6017.9930.82 Adopting TTN improves other TTA methods. We compare optimization-based methods with and without TTN layers. Since TENT, SWR, and CoTTA optimize model parameters on top of using TBN layers, they also suffer from performance drops when the test batch size becomes small. Adopting TTN reduces the dependency on large test batch size, i.e., makes robust to small batch size, and even improves their performance when using large test batch. Furthermore, in continual (Table 2) and mixed domain (Table 3) adaptation scenario, TENT and SWR shows higher error rate than in single domain (Table 1) adaptation. We interpret that because they update the model parameters based on the current output and predict the next input batch using the updated model, the model will not perform well if the consecutive batches have different corruption types ( i.e., mixed domain adaptation). Moreover, the error from the previous input batch propagates to the future input stream, and thus they may fall apart rapidly once they have a strongly wrong signal, which can happen in continual adaptation ( i.e., long-term adaptation without resetting). Applying TTN signiﬁcantly accelerates their model performance regardless of the evaluation scenarios. TTN preserves knowledge on source domain. In practice, data driven from the source domain (or a merely different domain) can be re-encountered in test time. We used clean domain test data in the single domain adaptation scenario to show how TTN and other TTA methods adapt to the seen source domain data (but unseen instance). As shown in Table 4, all baseline methods using TBN layers, show performance drops even with large batch sizes. We can conclude that it is still better to rely on source statistics collected from the large training data than using only current input statistics, even if its batch size is large enough to obtain reliable statistics ( i.e., 200). However, since TTN utilizes source statistics while leveraging the current input, TTN itself and TTN adopted methods well preserve the source knowledge compared to the TBN-based methods. With a batch size of 200, we observe that combining the source and a current test batch statistics outperforms the source model (see 3rd row of Table 4). TTN is robust to class imbalanced scenario. Heavily depending on current test batch statistics are especially vulnerable when the class labels are imbalanced (Boudiaf et al., 2022; Gong et al., 2022). To simulate this situation, we sorted test images in class label order and then sampled test batches following the sorted data order. In Table 5, we observe that TTN is more robust to the class imbalanced scenario than utilizing only test batch statistics (i.e., TBN). As explained in Section 3.5, 7Published as a conference paper at ICLR 2023 Table 6: Adaptation on DG benchmarks in semantic segmentation. mIoU(↑) on four unseen domains with test batch size of 2 using ResNet-50 based DeepLabV3+ as a backbone. Method(Cityscapes→) BDD-100K Mapillary GTA V SYNTHIA Cityscapes Source (Chen et al., 2018) 43.50 54.37 43.71 22.78 76.15 Norm TBN 43.12 47.61 42.51 25.71 72.94 Ours (TTN) 47.40 56.92 44.71 26.68 75.09 Optim. TENT 43.30 47.80 43.57 25.92 72.93 + Ours (TTN) 47.89 57.84 46.18 27.29 75.04 SWR 43.40 47.95 42.88 25.97 72.93 + Ours (TTN) 48.85 59.09 46.71 29.16 74.89 we are putting more importance on CBN than TBN, where semantic information is mainly handled, i.e., in deeper layers, so we can understand that TTN is less impacted by skewed label distribution. 3.3 E XPERIMENTS ON SEMANTIC SEGMENTATION We additionally conduct experiments on domain generalization (DG) benchmarks (Choi et al., 2021; Pan et al., 2018) for semantic segmentation, including natural domain shifts ( e.g., Cityscapes→BDD-100K), to demonstrate the broad applicability of TTN. Table 6 shows the results of evaluating the ResNet-50-based DeepLabV3+ (Chen et al., 2018) model trained on the Cityscapes training set using the validation set of real-world datasets such as Cityscapes, BDD-100K, and Map- illary, and synthetic datasets including GTA V and SYNTHIA. We employ a test batch size of 2 for test-time adaptation in semantic segmentation. We observe that even when exploiting test batch statistics for standardization in BN layers (TBN) or updating the model parameters on top of TBN (TENT, SWR) does not improve the model performance (i.e., perform worse than the source model), adopting TTN helps the model make good use of the strength of the test batch statistics. Implemen- tation details and additional results are provided in the appendix A.1 and B.7, respectively. 3.4 A BLATION STUDIES Prior Aregularizes αto be robust to overall test batch sizes. We conduct an ablation study on the importance of each proposed component,i.e., initializing αwith prior A, optimizing αusing CE and MSE losses, and the results are shown in Table 7. Using Afor initialization and MSE loss aims to optimize αfollowing our intuition that we discussed in Section 2.3. Optimizing αusing CE loss improves the overall performance, but without regularizing with MSE loss, αmay overﬁt to large batch size (rows 2 & 3). Initialization with Aor not does not show a signiﬁcant difference, but A provides a better starting point than random initialization when comparing the left and right of the 2nd row. We observe that when using MSE loss, regardless of initialization using A, the optimized αsufﬁciently reﬂects our intuition resulting in a low error rate to overall batch sizes (row 3). Table 7: Ablation study on importance of each component Method Test batch size Avg. Method Test batch size Avg.Init. CE MSE200 64 16 4 2 1 Init. CE MSE200 64 16 4 2 1 - - \u0013 13.36 13.43 13.85 15.50 17.43 20.0715.61 \u0013 - - 13.37 13.43 13.85 15.50 17.44 20.0715.61 - \u0013 - 11.64 11.7312.26 14.46 16.94 19.8814.49 \u0013 \u0013 - 11.73 11.82 12.23 14.18 16.41 19.2714.27 - \u0013 \u0013 11.6411.7812.21 13.97 15.86 18.0013.91 \u0013 \u0013 \u0013 11.67 11.80 12.13 13.93 15.83 17.9913.89 3.5 V ISUALIZATION OF α Fig. 4 shows the visualization of optimized αfor CIFAR-10 using WideResNet-40-2. We observe that α decreases from shallow to deep layers (left to right), which means CBN is more active in deeper layers, and TBN is vice versa. As shown in Table 4 and 6, CBN employing source statistics is superior to TBN when the distribution shift between source and target domains is small. Assum- ing that the αwe obtained is optimal, we can conjecture that CBN is more active ( i.e., αcloser to 0) in deeper layers because domain information causing the distribution shift has been diminished. In contrast, TBN has a larger weight ( i.e., α closer to 1) in shallower layers since the domain in- formation induces a large distribution shift. This interpretation is consistent with the observations of previous studies (Pan et al., 2018; Wang et al., 2021; Kim et al., 2022a) that style information mainly exists in shallower layers, whereas only content information remains in deeper layers. 8Published as a conference paper at ICLR 2023 𝐶𝑙 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Channel-wise 𝛼 Channel mean 𝛼 per layer Channels in all layers1 2661 Figure 4: Optimized α. x- and y-axes indicate all channels in order from shallow to deep layers and the interpolating weight α, respectively. Cl denotes the channel size of layer l. 4 R ELATED WORK Test-time adaptation/training (TTA) aims to adapt models towards test data to overcome the per- formance degradation caused by distribution shifts (Sun et al., 2020; Wang et al., 2020). There are other related problems, unsupervised domain adaptation (UDA) (Sun & Saenko, 2016; Ganin et al., 2016) and source-free domain adaptation (SFDA) (Liang et al., 2020; Huang et al., 2021; Liu et al., 2021). Both UDA and SFDA have access to sufﬁciently large enough unlabeled target datasets, and their objective is to achieve high performance on that particular target domain. Unlike UDA and SFDA, TTA utilizes test data in an online manner. There are two key factors of recent approaches: adapting standardization statistics in normalization layers and adapting model parameters. Normalization in Test Time. Nado et al. (2020) suggested prediction-time BN, which uses test batch statistics for standardization and Schneider et al. (2020) introduced to adapt BN statistics by combining source and test batch statistics considering the the test batch size to mitigate the inter- mediate covariate shift. In this paper, we refer to the former method as TBN. Similarly, You et al. (2021) and Khurana et al. (2021) mixed the statistics using predeﬁned hyperparameter. Also, Mirza et al. (2022) and Hu et al. (2021) adapted the statistics using moving average while augmenting the input to form a pseudo test batch when only a single instance is given. The primary difference with the existing approaches is that we consider the channel-wise domain-shift sensitivity of BN layers to optimize the interpolating weights between CBN and TBN. Concurrently, Zou et al. (2022) pro- posed to adjust the standardization statistics using a learnable calibration strength and showed its effectiveness focusing on the semantic segmentation task. Optimization in Test Time.TENT (Wang et al., 2020), SWR (Choi et al., 2022), and CoTTA (Wang et al., 2022) updated model parameters while using TBN. TENT optimized afﬁne parameters in BN layers using entropy minimization while freezing the others. To maximize the adaptability, SWR updated the entire model parameters minimizing the entropy loss based on the domain-shift sensitivity. To stabilize the adaptation in continuously changing domains, CoTTA used consistency loss between student and teacher models and stochastically restored random parts of the pre-trained model. Liu et al. (2021) and Chen et al. (2022) suggested to update the model through contrastive learning. We focus on correcting the standardization statistics using domain-shift aware interpolating weight α. Similar to Choi et al. (2022), we measure the domain-shift sensitivity by comparing gradients. The principle difference is that we use channel-wise sensitivity when optimizing αin post-training, while SWR used layer-wise sensitivity regularizing the entire model update in test time. 5 C ONCLUSIONS This paper proposes TTN, a novel domain-shift aware batch normalization layer, which combines the beneﬁts of CBN and TBN that have a trade-off relationship. We present a strategy for mixing CBN and TBN based on the interpolating weight derived from the optimization procedure utilizing the sensitivity to domain shift and show that our method signiﬁcantly outperforms other normaliza- tion techniques in various realistic evaluation settings. Additionally, our method is highly practical because it can complement other optimization-based TTA methods. The oracle mixing ratio between CBN and TBN can vary depending on the domain gap difference. However, our proposed method employs a ﬁxed mixing ratio during test time, where the mixing ratio is optimized before model deployment. If we could ﬁnd the optimal mixing ratio according to the distribution shift during test time, we can expect further performance improvement. We consider it as future work. In this regard, our efforts encourage this ﬁeld to become more practical and inspire new lines of research. 9Published as a conference paper at ICLR 2023 REPRODUCIBILITY STATEMENT To ensure the reproducibility of our method, we provide the experimental setup in Section 3.1. Moreover, the details on implementation and evaluation settings can be found in the appendix A.1 and A.2, respectively. The pseudocode for overall training and testing scheme is provided in the appendix A.3. Together with related references and publicly available codes, we believe our paper contains sufﬁcient information for reimplementation. ACKNOWLEDGEMENT We would like to thank Kyuwoong Hwang, Simyung Chang, and Seunghan Yang of the Qualcomm AI Research team for their valuable discussions. REFERENCES Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. John Bronskill, Jonathan Gordon, James Requeima, Sebastian Nowozin, and Richard Turner. Tas- knorm: Rethinking batch normalization for meta-learning. In International Conference on Ma- chine Learning (ICML). PMLR, 2020. Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder- decoder with atrous separable convolution for semantic image segmentation. In Proceedings of the European conference on computer vision (ECCV), 2018. Sungha Choi, Sanghun Jung, Huiwon Yun, Joanne T Kim, Seungryong Kim, and Jaegul Choo. Robustnet: Improving domain generalization in urban-scene segmentation via instance selective whitening. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. Sungha Choi, Seunghan Yang, Seokeon Choi, and Sungrack Yun. Improving test-time adaptation via shift-agnostic weight regularization and nearest source prototypes. In European Conference on Computer Vision (ECCV), 2022. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc ¸ois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural net- works. The journal of machine learning research, 2016. Taesik Gong, Jongheon Jeong, Taewon Kim, Yewon Kim, Jinwoo Shin, and Sung-Ju Lee. Note:robust continual test-time adaptation against temporal correlation. Advances in Neural In- formation Processing Systems (NeurIPS), 2022. Priya Goyal, Piotr Doll´ar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, An- drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In Ad- vances in Neural Information Processing Systems (NeurIPS), 2004. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog- nition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 10Published as a conference paper at ICLR 2023 Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor- ruptions and perturbations. In International Conference on Learning Representations (ICLR), 2018. Dan Hendrycks, Norman Mu, Ekin Dogus Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshmi- narayanan. Augmix: A simple data processing method to improve robustness and uncertainty. In International Conference on Learning Representations (ICLR), 2019. Xuefeng Hu, Gokhan Uzunbas, Sirius Chen, Rui Wang, Ashish Shah, Ram Nevatia, and Ser-Nam Lim. Mixnorm: Test-time adaptation through online normalization estimation. arXiv preprint arXiv:2110.11478, 2021. Jiaxing Huang, Dayan Guan, Aoran Xiao, and Shijian Lu. Model adaptation: Historical contrastive learning for unsupervised domain adaptation without source data. 2021. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. InInternational Conference on Machine Learning (ICML), 2015. Ansh Khurana, Sujoy Paul, Piyush Rai, Soma Biswas, and Gaurav Aggarwal. Sita: Single image test-time adaptation. arXiv preprint arXiv:2112.02355, 2021. Byeonggeun Kim, Seunghan Yang, Jangho Kim, Hyunsin Park, Juntae Lee, and Simyung Chang. Domain Generalization with Relaxed Instance Frequency-wise Normalization for Multi-device Acoustic Scene Classiﬁcation. In Conference of the International Speech Communication Asso- ciation (INTERSPEECH), 2022a. Jin Kim, Jiyoung Lee, Jungin Park, Dongbo Min, and Kwanghoon Sohn. Pin the memory: Learn- ing to generalize semantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022b. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. 2015. Suhyeon Lee, Hongje Seong, Seongwon Lee, and Euntai Kim. Wildnet: Learning domain general- ized semantic segmentation from the wild. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In International Conference on Machine Learning (ICML). PMLR, 2020. Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexan- dre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? Advances in Neural Information Processing Systems (NeurIPS), 2021. Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Unsupervised domain adap- tation with residual transfer networks. In Advances in Neural Information Processing Systems (NeurIPS), 2016. Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. 2017. M Jehanzeb Mirza, Jakub Micorek, Horst Possegger, and Horst Bischof. The norm must go on: Dynamic unsupervised domain adaptation by normalization. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Zachary Nado, Shreyas Padhy, D Sculley, Alexander D’Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robustness under covariate shift. arXiv preprint arXiv:2006.10963, 2020. Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulo, and Peter Kontschieder. The mapillary vistas dataset for semantic understanding of street scenes. In International Conference on Computer Vision (ICCV), 2017. Xingang Pan, Ping Luo, Jianping Shi, and Xiaoou Tang. Two at once: Enhancing learning and generalization capacities via ibn-net. InEuropean Conference on Computer Vision (ECCV), 2018. 11Published as a conference paper at ICLR 2023 Joaquin Quinonero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset shift in machine learning. Mit Press, 2008. Stephan R. Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun. Playing for data: Ground truth from computer games. In European Conference on Computer Vision (ECCV), 2016. German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio M Lopez. The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. Ad- vances in Neural Information Processing Systems (NeurIPS), 2020. Saurabh Singh and Abhinav Shrivastava. Evalnorm: Estimating batch normalization statistics for evaluation. In International Conference on Computer Vision (ICCV), 2019. Cecilia Summers and Michael J Dinneen. Four things everyone should know to improve batch normalization. In International Conference on Learning Representations (ICLR), 2019. Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In European Conference on Computer Vision (ECCV), 2016. Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time train- ing with self-supervision for generalization under distribution shifts. In International Conference on Machine Learning (ICML), 2020. Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick P´erez. Advent: Adver- sarial entropy minimization for domain adaptation in semantic segmentation. InIEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Repre- sentations (ICLR), 2020. Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Yulin Wang, Zanlin Ni, Shiji Song, Le Yang, and Gao Huang. Revisiting locally supervised learning: an alternative to end-to-end training. In International Conference on Learning Representations (ICLR), 2021. Fuming You, Jingjing Li, and Zhou Zhao. Test-time batch statistics calibration for covariate shift. arXiv preprint arXiv:2110.04065, 2021. Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madha- van, and Trevor Darrell. Bdd100k: A diverse driving dataset for heterogeneous multitask learning. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. Yuliang Zou, Zizhao Zhang, Chun-Liang Li, Han Zhang, Tomas Pﬁster, and Jia-Bin Huang. Learn- ing instance-speciﬁc adaptation for cross-domain segmentation. European Conference on Com- puter Vision (ECCV), 2022. 12Published as a conference paper at ICLR 2023 A A PPENDIX : F OR REPRODUCIBILITY This section provides supplemental material for Section 2 and 3.1. A.1 I MPLEMENTATION DETAILS Datasets and Models. For CIFAR-10/100-C, we optimized α using augmented CIFAR-10/100 training set on the pre-trained WideResNet-40-2 (WRN-40) (Hendrycks et al., 2019). For ImageNet- C, we used augmented ImageNet training set (randomly sampled 64000 instances per epoch) on the pre-trained ResNet-50. Augmentation. Following the setting of SWR (Choi et al., 2022), we used color jittering, random invert and random grayscale when obtaining the prior A. When optimizing α, we followed the augmentation choice of CoTTA (Wang et al., 2022), which are color jittering, padding, random afﬁne, gaussian blur, center crop and random horizontal ﬂip. We excluded for gaussian noise to avoid any overlap with corruption type of common corruptions (Hendrycks & Dietterich, 2018). For ImageNet, we only used the same augmentation both for obtaining Aand optimizing αfollowing the SWR augmentation choices. Post-training. When obtaining prior, we used randomly selected 1024 samples from the training set, following the setting of SWR. We used Adam (Kingma & Ba, 2015) optimizer using a learning rate (LR) of 1e-3, which is decayed with cosine schedule (Loshchilov & Hutter, 2017) for 30 epochs and used 200 training batch for CIFAR-10/100. For ImageNet, we lowered LR to 2.5e-4 and used 64 batch size. For semantic segmentation task, we trained TTN using Cityscapes training set, and training batch size of 2. We resized the image height to 800 while preserving the original aspect ratio Cordts et al. (2016). We can terminate the training when MSE loss saturates (We observed that αdoes not show signiﬁcant difference after the MSE loss is saturated). We used the weighting hyperparmeter to MSE loss λas 1. Test time. For AdaptiveBN, which adjusts the interpolating weight using two factors: hyperpa- rameter N and test batch size n, we followed the suggested N, which is empirically obtained best hyperparameter from the original paper, for each n(Figure 11, ResNet architecture from Schneider et al. (2020)). In detail, we set N as 256, 128, 64, 32, and 16 for test batch size 200, 64, 16, 4, 2, and 1, which yields αas 0.44, 0.33, 0.2, 0.11, 0.06, and 0.06, respectively. For optimization-based TTA methods, we followed default setting in TENT, SWR, and CoTTA for test-time adaptation. We used LR of 1e-3 to test batch size of 200 for CIFAR-10/100-C in single domain (TENT, SWR) and continuously changing domain (CoTTA) scenarios. To avoid rapid error accumulation, we lowered LR to 1e-4 for TENT and SWR in continual and mixed do- main scenarios. Moreover, we updated model parameters after accumulating the gradients for 200 samples for CIFAR-10/100-C. In other words, we compute gradients per batch, but update, i.e., optimizer.step(), after seeing 200 data samples. Exceptionally, we used LR of 5e-5 for SWR and SWR+TTN in mixed domain setting. Additionally, in continuously changing and mixed domain scenarios, we used the stable version of SWR, which updates the model parameter based on the frozen source parameters instead of previously updated parameters (original SWR). For semantic segmentation, we set the test batch size as 2 and learning rate for optimization-based methods as 1e-6 for all datasets. For SWR, we set the importance of the regularization term λr as 500. The other hyperparameters are kept the same as Choi et al. (2022) choices. For all test-time adaptation, we used constant learning rate without scheduling. A.2 E VALUATION SCENARIO DETAILS Class imbalanced setting. In the main paper Table 5, we show the results under class imbalanced settings. In the setting, we sorted the test dataset of each corruption in the order of class labels, i.e., from class 0 to 9 for CIFAR-10-C. Then, we comprised test batches following the sorted order. Therefore, most batches consist of single class input data, which leads to biased test batch statistics. For larger batch size, the statistics are more likely to be extremely skewed, and that explains why error rates are higher with larger batch sizes than with the small ones. 13Published as a conference paper at ICLR 2023 A.3 P SEUDOCODE Pseudocode for post-training, i.e., obtaining Aand optimizing α, is provided in Algorithms 1 and 2, respectively, and that for test time is in Algorithm 3. Moreover, we provide PyTorch-friendly pseudocode for obtaining Ain Listing 1. Please see Section 2 for equations and terminologies used in the algorithms. Algorithm 1 Obtain prior A 1: Require: Pre-trained model fθ; source training data DS = (X,Y ) 2: Output: Prior A 3: for all (x,y) in DS do 4: Augment x: x′ 5: Collect gradients (∇γ,∇γ′ ) and (∇β,∇β′ ) from fθ using clean xand augmented x′ 6: end for 7: Compute gradient distance score dusing Eq. 4 8: Deﬁne prior Ausing Eq. 5 Algorithm 2 Post-train 1: Require: Pre-trained model fθ; source training data DS = (X,Y ); step size hyperparameter η; regularization weight hyperparameter λ 2: Output: Optimized interpolating weight α 3: Obtain prior Ausing Algorithm 1 4: Initialize αwith prior A 5: Replace all BN layers of fθ to TTN layers using αand Eq. 2 6: while not done do 7: Sample minibatches BS from DS 8: for all minibatches do 9: Augment all xin BS: x′ 10: Evaluate ∇αLusing fθ given {(x′ i,yi)}|BS| i=1 using Eq. 6 while adapting standardization statistics using Eq. 2 11: Update α←α−η∇αL 12: end for 13: end while Algorithm 3 Inference (Test time) 1: Require: Pre-trained model fθ; optimized α, target test data DT = (X); 2: Replace all BN layers of fθ to TTN layers using αand Eq. 2 3: Sample minibatches BT from DT 4: for all minibatches do 5: Make prediction using fθ given BT while adapting standardization statistics using Eq. 2 6: end for 1 def obtain_prior(model, train_data): 2 # make weight and bias of BN layers requires_grad=True, otherwise False 3 params = {n: p for n, p in model.named_parameters() if p.requires_grad} 4 5 grad_sim = {} 6 for x, y in train_data: 7 # collect gradients for clean and augmented input 8 grad_org = collect_grad(model, x, y, params) 9 grad_aug = collect_grad(model, augment(x), y, params) 10 11 # compute grad similarity 12 for n, p in params.items(): 13 grad_sim[n].data = cosine_sim(grad_org, grad_aug) 14 14Published as a conference paper at ICLR 2023 15 # average over data samples 16 for n, p in params.items(): 17 grad_sim[n].data /= len(train_data) 18 19 # min max normalization 20 max_grad = get_max_value(grad_sim) # scalar 21 min_grad = get_min_value(grad_sim) # scalar 22 grad_dist = {} 23 for n, p in grad_sim.items(): 24 grad_dist[n] = (p - min_grad) / (max_grad - min_grad) 25 26 prior = [] 27 j = 0 28 # integrate gradients of weight(gamma) and bias(beta) for each BN layer 29 for n, p in grad_dist.items(): 30 if \"weight\" in n: 31 prior.append(p) 32 elif \"bias\" in n: 33 prior[j] += p 34 prior[j] /= 2 35 prior[j] = (1-prior[j])**2 36 j += 1 37 38 return prior 39 40 def collect_grad(model, x, y, params): 41 model.zero_grad() 42 out = model(x) 43 loss = ce_loss(out, y) 44 loss.backward() 45 46 grad = {} 47 for n, p in params.items(): 48 grad[n].data = p.data 49 50 return grad Listing 1: PyTorch-friendly pseudo code for obtaining prior B A PPENDIX : E XPERIMENTAL RESULTS B.1 A BLATION STUDIES Channel-wise α. In Table 8, we compared different granularity levels of interpolating weightα, i.e., channel-wise, layer-wise, and a constant value with CIFAR-10-C and backbone WRN-40. We ob- served that channel-wise optimized αshows the best performance. We take average of the channel- wisely optimized α (the 1st row) over channels to make a layer-wise α (the 2nd row), and take average over all channels and layers to make a constant value (the 3rd row). αof the 1st and 2nd rows are visualized in the main paper Figure 4 colored in blue and red, respectively. The constant value α(the 3rd row) is 0.3988. We also optimized αlayer-wisely (the 4th row). Table 8: Ablation study on granularity level of α # Method Test batch size Avg. 200 64 16 4 2 1 1 Channel-wise (Optimized) 11.67 11.80 12.13 13.93 15.83 17.99 13.89 2 Layer-wise (Channel mean) 12.75 12.84 13.16 14.66 16.40 18.82 14.77 3 Constant (Mean) 12.07 12.21 13.05 16.72 20.04 21.26 15.89 4 Layer-wise (Optimized) 13.11 13.21 13.51 14.84 16.46 18.62 14.96 15Published as a conference paper at ICLR 2023 MSE loss strength λ. We empirically set the MSE loss strengthλin Eq. 6 as 1 through the ablation study using CIFAR-10-C and WRN-40 (Table 9). Using the MSE regularizer prevents the learnable αfrom moving too far from the prior, thus letting αfollow our intuition, i.e., putting smaller im- portance on the test batch statistics if the layer or channel is invariant to domain shifts. However, with too strong regularization (λ=10.0), the overall error rates are high, which means the αneeds to be sufﬁciently optimized. On the other hand, with too small regularization, the αmay overﬁt to the training batch size (B=200) and lose the generalizability to the smaller batch size. Table 9: MSE loss strength λ λ Test batch size Avg. 200 64 16 4 2 1 0.0 11.73 11.82 12.23 14.18 16.41 19.27 14.27 0.1 11.65 11.91 12.09 13.84 15.71 18.24 13.91 1.0 11.67 11.80 12.13 13.93 15.83 17.99 13.89 10.0 12.45 12.63 12.82 14.55 16.32 18.56 14.56 B.2 M ORE COMPARISONS ON CIFAR10-C Constant α. Table 10 shows results of simple baseline for normalization-based methods where the αis a constant value ranging from 0 to 1. α= 0 is identical to CBN (Ioffe & Szegedy, 2015), and α = 1 is identical to TBN (Nado et al., 2020). We observe that the lower α, i.e., using less test batch statistics, shows better performance for small test batch sizes. This observation is analogous to the ﬁnding of the previous work (Schneider et al., 2020). Table 10: Constant α. Error rate (↓) averaged over 15 corruptions of CIFAR-10-C (WRN-40). α Test batch size Avg. 200 64 16 4 2 1 0 18.26 18.39 18.26 18.26 18.25 18.25 18.28 0.1 13.95 14.1 14.05 14.65 15.14 15.45 14.56 0.2 12.46 12.66 12.89 14.30 15.53 15.64 13.91 0.3 12.05 12.29 12.72 15.18 17.35 17.42 14.50 0.4 12.13 12.41 13.12 16.69 19.81 20.51 15.78 0.5 12.42 12.78 13.73 18.32 22.52 24.88 17.44 0.6 12.88 13.32 14.48 20.02 25.17 31.97 19.64 0.7 13.37 13.9 15.23 21.75 27.91 46.65 23.14 0.8 13.82 14.37 15.94 23.44 30.59 77.15 29.22 0.9 14.18 14.8 16.58 24.94 33.12 89.81 32.24 1 14.50 15.15 17.10 26.29 35.67 90.00 33.12 B.3 V ARIANTS OF TTN FOR SMALL TEST BATCH SIZE Online TTN. TTN interpolating weight αcan also be adapted during test time. Table 11 shows the results of the TTN online version, which further optimizes the post-trained alpha using the entropy minimization and mean-squared error (MSE) loss between the updated alpha and the initial post- trained alpha. We followed entropy minimization loss used in TENT (Wang et al., 2020), and the MSE loss can be written as LMSE = ∥α−α0∥2, where α0 is the post-trained α. We set the learning rate as 1e-2, 2.5e-3, 5e-4, 1e-4, 5e-5, and 2.5e-5 by linearly decreasing according to the test batch size of 200, 64, 16, 4, 2, and 1 (Goyal et al., 2017). The online TTN shows improvements compared to the ofﬂine TTN in all three evaluation scenarios (single, continuously changing, and mixed). Scaled TTN. Following the observation from Table 10, we lowered the interpolating weight by mul- tiplying a constant scale value, ranging (0,1), to the optimized TTN α. In Table 11, we empirically set the scale value as 0.4. Dynamic training batch size. We observe that using the dynamic batch size in the post-training stage also improves the performance for small test batch sizes (2 or 1), while slightly deteriorating the performance for large test batch sizes (200 or 64). We randomly sampled training batch size from the range of [4,200] for each iteration. Other hyperparameters are kept as the same. 16Published as a conference paper at ICLR 2023 Table 11: TTN variants. Error rate (↓) averaged over 15 corruptions of CIFAR-10-C (WRN-40). Test batch size Avg. Method Eval. setting 200 64 16 4 2 1 TTN (ofﬂine, default) Single & Cont.11.67 11.80 12.13 13.93 15.83 17.99 13.89 TTN (ofﬂine, default) Mixed 12.16 12.19 12.34 13.96 15.55 17.83 14.00 TTN (online) Single 11.39 11.64 11.97 13.70 15.41 17.49 13.60 TTN (online) Cont. 11.67 11.96 12.15 13.90 15.67 17.72 13.85 TTN (online) Mixed 12.04 12.04 12.06 13.90 15.46 17.62 13.85 TTN (scaled) Single & Cont.13.20 13.38 13.35 13.88 14.54 15.17 13.92 TTN (scaled) Mixed 13.17 13.05 13.17 13.74 14.36 15.09 13.76 TTN (dynamic train batch size)Single & Cont.11.82 12.04 12.17 13.60 15.13 17.22 13.66 TTN (dynamic train batch size)Mixed 12.12 12.01 11.91 13.43 14.76 17.23 13.58 B.4 S TUDY ON AUGMENTATION TYPE TTN is robust to the data augmentation. We used data augmentation in the post-training phase to simulate domain shifts. The rationale for the simulation is to expose the model to different input domains. Especially when obtaining the prior, we compare the outputs from shifted domains with the clean (original) domain in order to analyze which part of the model is affected by the domain discrepancy. Therefore, changing the domain itself is what matters, not which domain the input is shifted to. We demonstrated this by conducting ablation studies by varying the augmentation type. We analyzed various augmentation types when obtaining prior and when optimizing alpha and the results are shown in Figure 5 (a) and (b), respectively. We use a true corruption, i.e., one of 15 corruption types in the corruption benchmark, as an augmentation type in the post-training phase to analyze how TTN works if the augmentation type and test corruption type are misaligned or perfectly aligned. Speciﬁcally, we used the true corruption when obtaining the prior while keeping the augmentation choices when optimizing alpha as described in the appendix A.1, and vice versa. Expectedly, the diagonal elements, i.e., where the same corruption type is used both for in post- training and in test time, tend to show the lowest error rate in Figure 5(b). The row-wise standard deviation is lower than 1 in most cases and even lower than 0.5 in Figure 5(a), which means the prior is invariant to the augmentation choice. Moreover, we observe that the average error rate over all elements, 11.74% in ablation for obtaining prior and 11.67% in ablation for optimizing alpha, is almost as same as TTN result 11.67% (See Table 14 and Figure 5). Moreover, we conducted an ablation study on the choice of the augmentation type using CIFAR-10-C and WRN-40 (Table12). We observe that obtaining prior and optimizing alpha steps are robust to the augmentation types. Table 12: Ablation study on augmentation choice. From left to right one augmentation type is added at a time. Default setting, which we used in all experiments, is colored in gray. Prioraugmentation type color jitter + grayscale + invert + gaussian blur + horizontal ﬂip 12.03 11.83 11.67 11.59 11.58 Optimizingαaugmentation type color jitter + padding + afﬁne + gaussian blur + horizontal ﬂip 11.78 11.78 11.7 11.70 11.67 B.5 R ESULTS ON IMAGE NET-C Table 13 shows the experimental results using ResNet-50 (He et al., 2016) on ImageNet- C (Hendrycks & Dietterich, 2018) dataset. We emphasized the effectiveness of our proposed method by showing the signiﬁcant improvement in large-scale dataset experiment. Similar to the results in CIFAR-10/100-C, TTN showed the best performance compared to normalization-based methods (TBN (Nado et al., 2020), AdaptiveBN Schneider et al. (2020), and α-BN (You et al., 2021)) and improved TTA performance when it is applied to optimization-based methods (TENT (Wang et al., 2020) and SWR (Choi et al., 2022)). During post-training, we used LR of 2.5e-4 and batch size of 64. In test time, we used the learning rate of 2.5e-4 for TENT following the setting from the original paper, and used 5e-6 for TENT+TTN. For SWR and SWR+TTN, we used learning rate of 2.5e-4 for B=64, and 2.5e-6 for B=16, 4, 2, and 1. We had to carefully tune the learning rate especially for SWR, since the method updates the 17Published as a conference paper at ICLR 2023 entire model parameters in an unsupervised manner and hence is very sensitive to the learning rate when the test batch size becomes small. Other hyperparameters and details are kept same (See the appendix A.1 for more details). Moreover, to avoid rapid accumulation, we stored gradients for sufﬁciently large test samples and then updated the model parameters (for example, we conducted adaptation in every 64 iterations in the case of batch size of 1) in both TENT and SWR. Table 13: Single domain adaptation on ImageNet-C (ResNet-50). Error rate (↓) averaged over 15 corruption types with severity level 5 is reported for each test batch size. Method Test batch size Avg. Error 64 16 4 2 1 Source (CBN)93.34 93.34 93.34 93.34 93.34 93.34 Norm TBN 74.24 76.81 85.74 95.35 99.86 86.40 AdaptiveBN 77.86 81.47 86.71 90.15 91.11 85.46 α-BN 86.06 86.32 87.16 88.33 90.45 87.66 Ours (TTN) 72.21 73.18 76.98 81.52 88.49 78.48 Optim. TENT 66.56 72.61 93.37 99.46 99.90 86.41 +Ours (TTN)71.42 72.45 76.66 81.89 91.00 78.68 SWR 64.41 74.19 84.30 93.05 99.86 83.16 +Ours (TTN)55.68 69.25 78.48 86.37 94.08 76.77 B.6 E RROR RATES OF EACH CORRUPTION In Table 14, we show the error rates of TTN for each corruption of CIFAR-10-C using the WRN-40 backbone. The averaged results over the corruptions are in Table 1. Table 14: Results of each corruption (CIFAR-10-C) batch sizegauss. shot impulse defocus glass motion zoom snow frost fog brightness contrast elastic pixel. jpeg.Avg. 200 14.81 12.78 17.32 7.37 17.87 8.51 7.23 10.29 9.88 11.29 6.06 8.36 13.42 14.89 14.94 11.6764 14.81 12.81 17.42 7.41 18.21 8.66 7.41 10.44 9.93 11.63 6.11 8.35 13.59 15.18 15.02 11.8016 15.23 13.00 17.98 7.71 18.46 8.95 7.68 10.85 10.17 12.21 6.25 8.54 13.95 15.67 15.3412.134 17.03 15.29 19.75 9.26 20.93 10.01 9.62 12.58 11.85 13.65 7.69 9.25 16.21 18.08 17.7013.932 19.21 16.80 21.86 10.70 23.74 11.39 11.92 14.00 13.39 15.38 8.95 10.13 18.92 20.68 20.4015.831 22.03 19.97 25.24 12.41 26.99 12.22 14.39 14.73 14.60 17.27 10.16 9.51 22.10 24.12 24.0917.99 B.7 S EGMENTATION RESULTS For more comparisons, we add segmentation results for TBN and TTN using test batch size (B) of 4 and 8 (Table 15). The results demonstrate that TTN is robust to the test batch sizes. In other words, the performance difference across the test batch size is small when using TTN (TTN with B=2, 4, and 8). The results are averaged over 3 runs (i.e., using 3 independently optimized α), and the standard deviation is denoted with a ±sign. We omitted the standard deviation for TBN, which is 0.0 for every result since no optimization is required. Since the backbone network is trained with a train batch size of 8, we assume that test batch statistics estimated from the test batch with B=8 are sufﬁciently reliable. Accordingly, TBN with B=8 shows compatible results. However, when B becomes small (i.e., in a more practical scenario), problematic test batch statistics are estimated, and thus TBN suffers from the performance drop while TTN keeps showing robust performance. It is worth noting that TTN outperforms TBN by 3.77% in average accuracy when B=2, i.e., in the most practical evaluation setting, and by 2.54% and 1.99% for B=4 and 8, respectively. Table 15: Adaptation on DG benchmarks in semantic segmentation. mIoU(↑) on four unseen domains with test batch size of 2, 4, and 8 using ResNet-50 based DeepLabV3+ as a backbone. Method(Cityscapes→) BDD-100K Mapillary GTA V SYNTHIA Cityscapes Norm TBN (B=2) 43.12 47.61 42.51 25.71 72.94 Ours (TTN)(B=2) 47.40(±0.02) 56.88(±0.04) 44.69(±0.03) 26.68(±0.01) 75.09(±0.01) TBN (B=4) 45.64 49.17 44.26 25.96 74.29 Ours (TTN)(B=4) 47.72(±0.01) 57.11(±0.01) 45.08(±0.02) 26.52(±0.01) 75.56(±0.01) TBN (B=8) 46.42 49.38 44.81 25.97 75.42 Ours (TTN)(B=8) 47.25(±0.02) 57.28(±0.02) 45.13(±0.03) 26.45(±0.01) 75.82(±0.01) 18Published as a conference paper at ICLR 2023 (b)Augmentation type used for optimizing alpha Test corruption type Error rate. Colored by normalized value (across test corruption type)avg.std.14.550.8612.640.6116.770.927.660.3117.750.738.760.297.490.2510.330.279.710.3212.401.466.110.098.260.4913.350.3714.581.2714.730.3911.67 (a)Augmentation type used for obtaining prior Test corruption type Error rate. Colored by normalized value (across test corruption type) avg.std.15.030.3412.790.2217.240.167.410.0617.910.148.480.087.260.0310.440.149.980.1411.550.106.050.048.570.4013.340.0715.211.5914.830.1111.74 Figure 5: True test corruption as augmentation. Each column represents the augmentation type used (a) when obtaining prior or (b) when optimizing α. Each row represents the test corruption type. The error rate (↓) of CIFAR-10-C with severity level 5 and test batch size 200 using WRN-40 is annotated in each element. Element (i,j) represents the error rate when j-th corruption type is used for augmenting the clean train data during post-training and tested on i-th corruption test set. The heatmap is colored by error rate normalized across the test corruption type (row-wise normalization). 19",
      "meta_data": {
        "arxiv_id": "2302.05155v2",
        "authors": [
          "Hyesu Lim",
          "Byeonggeun Kim",
          "Jaegul Choo",
          "Sungha Choi"
        ],
        "published_date": "2023-02-10T10:25:29Z",
        "pdf_url": "https://arxiv.org/pdf/2302.05155v2.pdf"
      }
    },
    {
      "title": "Four Things Everyone Should Know to Improve Batch Normalization",
      "abstract": "A key component of most neural network architectures is the use of\nnormalization layers, such as Batch Normalization. Despite its common use and\nlarge utility in optimizing deep architectures, it has been challenging both to\ngenerically improve upon Batch Normalization and to understand the\ncircumstances that lend themselves to other enhancements. In this paper, we\nidentify four improvements to the generic form of Batch Normalization and the\ncircumstances under which they work, yielding performance gains across all\nbatch sizes while requiring no additional computation during training. These\ncontributions include proposing a method for reasoning about the current\nexample in inference normalization statistics, fixing a training vs. inference\ndiscrepancy; recognizing and validating the powerful regularization effect of\nGhost Batch Normalization for small and medium batch sizes; examining the\neffect of weight decay regularization on the scaling and shifting parameters\ngamma and beta; and identifying a new normalization algorithm for very small\nbatch sizes by combining the strengths of Batch and Group Normalization. We\nvalidate our results empirically on six datasets: CIFAR-100, SVHN, Caltech-256,\nOxford Flowers-102, CUB-2011, and ImageNet.",
      "full_text": "Published as a conference paper at ICLR 2020 FOUR THINGS EVERYONE SHOULD KNOW TO IMPROVE BATCH NORMALIZATION Cecilia Summers Department of Computer Science University of Auckland cecilia.summers.07@gmail.com Michael J. Dinneen Department of Computer Science University of Auckland mjd@cs.auckland.ac.nz ABSTRACT A key component of most neural network architectures is the use of normalization layers, such as Batch Normalization. Despite its common use and large utility in optimizing deep architectures, it has been challenging both to generically im- prove upon Batch Normalization and to understand the circumstances that lend themselves to other enhancements. In this paper, we identify four improvements to the generic form of Batch Normalization and the circumstances under which they work, yielding performance gains across all batch sizes while requiring no additional computation during training. These contributions include proposing a method for reasoning about the current example in inference normalization statistics, ﬁxing a training vs. inference discrepancy; recognizing and validating the powerful regularization effect of Ghost Batch Normalization for small and medium batch sizes; examining the effect of weight decay regularization on the scaling and shifting parameters γ and β; and identifying a new normalization al- gorithm for very small batch sizes by combining the strengths of Batch and Group Normalization. We validate our results empirically on six datasets: CIFAR-100, SVHN, Caltech-256, Oxford Flowers-102, CUB-2011, and ImageNet. 1 I NTRODUCTION Neural networks have transformed machine learning, forming the backbone of models for tasks in computer vision, natural language processing, and robotics, among many other domains (Krizhevsky & Hinton, 2009; He et al., 2017; Levine et al., 2016; Sutskever et al., 2014; Graves et al., 2013). A key component of many neural networks is the use of normalization layers such as Batch Normaliza- tion (Ioffe & Szegedy, 2015), Group Normalization (Wu & He, 2018), or Layer Normalization (Ba et al., 2016), with Batch Normalization the most commonly used for vision-based tasks. While the true reason why these methods work is still an active area of research (Santurkar et al., 2018), normalization techniques typically serve the purpose of making neural networks more amenable to optimization, allowing the training of very deep networks without the use of careful initializa- tion schemes (Simonyan & Zisserman, 2015; Zhang et al., 2019), custom nonlinearities (Klambauer et al., 2017), or other more complicated techniques (Xiao et al., 2018). Even in situations where training without normalization layers is possible, their usage can still aid generalization (Zhang et al., 2019). In short, normalization layers make neural networks train faster and generalize better. Despite this, it has been challenging to improve normalization layers. In the general case, a new approach would need to be uniformly better than existing normalization methods, which has proven difﬁcult. It has even been difﬁcult to tackle a simpler task: characterizing when speciﬁc changes to common normalization approaches might yield beneﬁts. In all, this has created an environment where approaches such as Batch Normalization are still used as-is, unchanged since their creation. In this work we identify four techniques that everyone should know to improve their usage of Batch Normalization, arguably the most common method for normalization in neural networks. Taken together, these techniques apply in all circumstances in which Batch Normalization is currently used, ranging from large to very small batch sizes, including one method which is even useful when the batch size B = 1, and for each technique we identify the circumstances under which it is expected to be of use. In summary, our contributions are: 1 arXiv:1906.03548v2  [cs.LG]  14 Feb 2020Published as a conference paper at ICLR 2020 1. A way to more effectively use the current example during inference, ﬁxing a discrepancy between training and inference that had been previously overlooked, 2. Identifying Ghost Batch Normalization, a technique designed for very large-batch multi- GPU training (Hoffer et al., 2017), as surprisingly effective even in the medium-batch, single-GPU regime, 3. Recognizing weight decay of the scaling and centering variables γ and β as a valuable source of regularization, an unstudied detail typically neglected, and 4. Proposing a generalization of Batch and Group Normalization in the small-batch setting, effectively making use of cross-example information present in the minibatch even when such information is not enough for effective normalization on its own. Experimentally, we study the most common use-case of Batch Normalization, image classiﬁcation, which is fundamental to most visual problems in machine learning. In total, these four techniques can have a surprisingly large effect, improving accuracy by over 6% on one of our benchmark datasets while only changing the usage of Batch Normalization layers. We have released code at https://github.com/ceciliaresearch/four_things_ batch_norm. 2 R ELATED WORK /BACKGROUND ON NORMALIZATION METHODS Most normalization approaches in neural networks, including Batch Normalization, have the general form of normalizing their inputs xi to have a learnable mean and standard deviation: ˆxi = γ xi −µi√ σ2 i + ϵ + β (1) where γ and β are the learnable parameters, typically initialized to 1 and 0, respectively. Where approaches typically differ is in how the mean µi and variance σ2 i are calculated. Batch Normalization (Ioffe & Szegedy, 2015), the pioneering work in normalization layers, deﬁned µi and σ2 i to be calculated for each channel or feature map separately across a minibatch of data. For example, in a convolutional layer, the mean and variance are computed across all spatial lo- cations and training examples in a minibatch. During inference, these statistics are replaced with an exponential moving average of the mean and variance, making inference behavior independent of inference batch statistics. The effectiveness of Batch Normalization is undeniable, playing a key role in nearly all state-of-the-art convolutional neural networks since its discovery (Szegedy et al., 2016; 2017; He et al., 2016a;b; Zoph & Le, 2017; Zoph et al., 2018; Hu et al., 2018; Howard et al., 2017; Sandler et al., 2018). Despite this, there is still a fairly limited understanding of Batch Normalization’s efﬁcacy — while Batch Normalization’s original motivation was to reduce internal covariate shift during training (Ioffe & Szegedy, 2015), recent work has instead proposed that its true effectiveness stems from making the optimization landscape smoother (Santurkar et al., 2018). One weakness of Batch Normalization is its critical dependence on having a reasonably large batch size, due to the inherent approximation of estimating the mean and variance with a single batch of data. Several works propose methods without this limitation: Layer Normalization (Ba et al., 2016), which has found use in many natural language processing tasks (Vaswani et al., 2017), tackles this by calculating µi and σ2 i over all channels, rather than normalizing each channel independently, but does not calculate statistics across examples in each batch. Instance Normalization (Ulyanov et al., 2016), in contrast, only calculates µi and σ2 i using the information present in each channel, rely- ing on the content of each channel at different spatial locations to provide effective normalization statistics. Group Normalization (Wu & He, 2018) generalizes Layer and Instance Normalization, calculating statistics in “groups” of channels, allowing for stronger normalization power than In- stance Normalization, but still allowing for each channel to contribute signiﬁcantly to the statistics used for its own normalization. The number of normalization groups per normalization layer is typically set to a global constant in group normalization, though alternatives such as specifying the number of channels per group have also been tried (Wu & He, 2018). Besides these most common approaches, many other forms of normalization also exist: Weight Nor- malization (Salimans & Kingma, 2016) normalizes the weights of each layer instead of the inputs, 2Published as a conference paper at ICLR 2020 parameterizing them in terms of a vector giving the direction of the weights and an explicit scale, which must be initialized very carefully. Decorrelated Batch Normalization (Huang et al., 2018) per- forms ZCA whitening in its normalization layer, and Iterative Normalization (Huang et al., 2019) makes it more efﬁcient via a Newton iteration approach. Cho & Lee (2017) analyze the weights in Batch Normalization from the perspective of a Riemannian manifold, yielding new optimization and regularization methods that utilize the manifold’s geometry. Targeting the small batch problem, Batch Renormalization (Ioffe, 2017) uses the moving average of batch statistics to normalize during training, parameterized in such a way that gradients still propa- gate through the minibatch mean and standard deviation, but introduces two new hyperparameters and still suffers somewhat diminished performance in the small-batch setting. Guo et al. (2018) tackle the small batch setting by aggregating normalization statistics over multiple forward passes. Recently, Switchable Normalization (Luo et al., 2019) aims to learn a more effective normalizer by calculating µi and σ2 i as learned weighted combinations of the statistics computed from other normalization methods. While ﬂexible, care must be taken for two reasons: First, as the parame- ters are learned differentiably, they are fundamentally aimed at minimizing the training loss, rather than improved generalization, which typical hyperparameters are optimized for on validation sets. Second, the choice of which normalizers to include in the weighted combination remains important, manifesting in Switchable Normalization’s somewhat worse performance than Group Normalization for small batch sizes. Differentiable Dynamic Normalization (Luo et al., 2019) ﬁxes the latter point, learning an even more ﬂexible normalization layer. Beyond these, there are many approaches we omit for lack of space (Littwin & Wolf, 2018; Deecke et al., 2019; Hoffer et al., 2018; Klambauer et al., 2017; Xiao et al., 2018; Zhang et al., 2019). 3 I MPROVING NORMALIZATION : W HAT EVERYONE SHOULD KNOW In this section we detail four methods for improving Batch Normalization. We also refer readers to the Appendix for a discussion of methods which do not improve normalization layers (sometimes surprisingly so). For clarity, we choose to interleave descriptions of the methods with experimental results, which aids in understanding each of the approaches as they are presented. We experiment with four standard image-centric datasets in this section: CIFAR-100, SVHN, Caltech-256, and ImageNet, and report results on validation datasets in order to fully describe each approach without contaminating test-set results. We give results on test sets, and experimental details in Sec. 4. 3.1 I NFERENCE EXAMPLE WEIGHING Batch Normalization has a disparity in function between training and inference: As previously noted, Batch Normalization calculates its normalization statistics over each minibatch of data separately while training, but during inference a moving average of training statistics is used, simulating the expected value of the normalization statistics. Resolving this disparity is a common theme among methods that have sought to replace Batch Normalization (Ba et al., 2016; Ulyanov et al., 2016; Salimans & Kingma, 2016; Wu & He, 2018; Ioffe, 2017). Here we identify a key component of this training versus inference disparity which can be ﬁxed within the context of Batch Normalization itself, improving it in the general case: when using a moving average during inference, each example does not contribute to its own normalization statistics. To give an example of the effect this has, we consider the output range of Batch Normalization. During training, due to the inclusion of each example in its own normalization statistics, it can be shown1 that the minimum possible output of a Batch Normalization layer is: min x0,...,xB−1 γ x0 −µi√ σ2 i + ϵ + β = −γ √ B−1 +β (2) with a corresponding maximum value ofγ √ B−1+β, where Bis the batch size, and we assume for simplicity that Batch Norm is being applied non-convolutionally. In contrast, during inference the output range of Batch Normalization is unbounded, creating a discrepancy. Morever, this actually happens for real networks: the output range of a network with Batch Normalization is wider during inference than during training (see Sec. B in Appendix). 1Proof in Appendix Sec. A 3Published as a conference paper at ICLR 2020 0.00 0.01 0.02 0.03 0.04 0.05 0.06 77.0 77.2 77.4 77.6 77.8 78.0 78.2 78.4Acc. 0.8 0.9 1.0 1.1 1.2 1.3 Loss 93.6 93.8 94.0 94.2 94.4Top-5 Acc. ResNet-152 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 82.0 82.2 82.4 82.6 82.8 83.0Acc. 0.73 0.74 0.75 0.76 0.77 0.78 0.79 0.80 Loss 96.10 96.15 96.20 96.25 96.30 96.35Top-5 Acc. NasNet-A 0.000 0.004 0.008 0.012 0.016 0.020 73.6 73.8 74.0 74.2 74.4Acc. 1.12 1.13 1.14 1.15 1.16 1.17 1.18 1.19 1.20 Loss 91.75 91.80 91.85 91.90 91.95 92.00 92.05 92.10Top-5 Acc. MobileNetv2 Figure 1: Effect of the example-weighing hyperparameter α on ImageNet for ResNet-152, Mo- bileNetV2, and NASNet-A, measuring top-1 and top-5 accuracies and the cross-entropy loss. Fortunately, once this problem has been realized, it is possible to ﬁx — we need only ﬁgure out how to incorporate example statistics during inference. Denoting mx as the moving average over xand mx2 the corresponding moving average over x2, we apply the following normalization: µi = αE[xi] + (1−α)mx σ2 i = (αE[x2 i ] + (1−α)mx2 ) −µ2 i ˆxi = γ xi −µi√ σ2 i + ϵ + β (3) where αis the contribution of xi to the normalization statistics, and we have reparameterized the variance as σ2 i = E[x2 i ] −E[xi]2. Given this formulation, a natural question is the choice of the parameterα, where α= 0corresponds to the classical inference setting of Batch Normalization and α = 1 replicates the setting of tech- niques which do not use cross-image information in calculating normalization statistics. Intuitively, it would make sense for the optimal value to beα= 1 B . However, this turns out to not be the case — instead, αis a hyperparameter best optimized on a validation set, whose optimal value may depend the model, dataset, and metric being optimized. While counterintuitive, this can be explained by the remaining set of differences between training and inference: for a basic yet fundamental example, the fact that the model has been ﬁt on the training set (also typically with data augmentation) may produce systematically different normalization statistics between training and inference. An advantage of this technique is that we can apply it retroactively to any model trained with Batch Normalization, allowing us to verify its efﬁcacy on a wide variety of models. In Fig. 1 we show the effect of αon the ImageNet ILSVRC 2012 validation set (Russakovsky et al., 2015) for three diverse models: ResNet-152 (He et al., 2016b), MobileNetV2 (Sandler et al., 2018), and NASNet-A Large (Zoph et al., 2018)2. On ResNet-152, for example, proper setting of αcan increase accuracy by up to 0.6%, top-5 accuracy by 0.16%, and loss by a relative 4.7%, which are all quite signiﬁcant given the simplicity of the approach, the competitiveness of ImageNet as a benchmark, and the fact that the improvement is essentially “free” — it involves only modifying the inference behavior of Batch Normalization layers, and does not require any re-training. Across models, the optimal value for α was largest for NASNet-A, the most memory-intensive (and therefore smallest batch size) model of the three. We refer the reader to the Appendix for additional plots with larger ranges of α. Surprisingly, it turns out that this approach can have positive effects on models trained without any cross-image normalization at all, such as models trained with Group Normalization (Wu & He, 2018). We demonstrate this in Fig. 2, where we ﬁnd that adding a tiny amount of information from the moving average statistics can actually result in small improvements, with relatively larger improvements in accuracy on Caltech-256 and cross entropy loss on CIFAR-100 and SVHN. This ﬁnding is extremely surprising, since adding in any information from the moving averages at all represents a clear difference from the training setting of Group Normalization. Similar to the unin- tuitive optimal value for α, we hypothesize that this effect is due to other differences in the settings of training and inference: for example, models are generally trained on images with the application of data augmentation, such as random cropping. During inference, though, images appear unper- turbed, and it might be the case that incorporating information from the moving averages is a way of inﬂuencing the model’s intermediate activations to be more similar to those of data augmented 2Models obtained from (Silberman & Guadarrama). 4Published as a conference paper at ICLR 2020 0.70  0.75  0.80  0.85  0.90  0.95  1.00 73.0 73.2 73.4 73.6 73.8 74.0Acc. CIFAR-100 with Group Normalization 1.30 1.31 1.32 1.33 1.34 1.35 1.36 1.37 1.38 Loss 0.60  0.65  0.70  0.75  0.80  0.85  0.90  0.95  1.00 98.66 98.68 98.70 98.72 98.74Acc. SVHN with Group Normalization 0.056 0.058 0.060 0.062 0.064 0.066 0.068 0.070 Loss 0.800  0.825  0.850  0.875  0.900  0.925  0.950  0.975  1.000 46.2 46.4 46.6 46.8 47.0 47.2Acc. Caltech-256 with Group Normalization 3.15 3.16 3.17 3.18 3.19 3.20 3.21 Loss Figure 2: Effect of the example-weighing hyperparameter αfor models trained with Group Nor- malization on CIFAR-100, SVHN, and Caltech-256. images, which it has been trained on. This mysterious behavior may also point to more general approaches for resolving training-inference discrepancies, and is worthy of further study. Last, we also note very recent work (Singh & Shrivastava, 2019) which examines a similar approach for incorporating the statistics of an example during inference time, using per-layer weights and op- timizing with a more involved procedure that encourages similar outputs to the training distribution. Summary: Inference example weighing resolves one disparity between training and inference for Batch Normalization, is uniformly beneﬁcial across all models and very easy to tune to metrics of interest, and can be used with any model trained with Batch Normalization, even retroactively. 3.2 G HOST BATCH NORMALIZATION FOR MEDIUM BATCH SIZES Ghost Batch Normalization, a technique originally developed for training with very large batch sizes across many accelerators (Hoffer et al., 2017), consists of calculating normalization statistics on dis- joint subsets of each training batch. Concretely, with an overall batch size of Band a “ghost” batch size of B′such that B′evenly divides B, the normalization statistics for example iare calculated as µi = 1 B′ B∑ j=1 xj[ ⌊jB′ B ⌋ = ⌊iB′ B ⌋ ] σ2 i = 1 B′ B∑ j=1 x2 j[ ⌊jB′ B ⌋ = ⌊iB′ B ⌋ ] −µ2 i (4) where [·] is the Iverson bracket, with value 1 if its argument is true and 0 otherwise. Ghost Batch Normalization was previously found to be an important factor in reducing the generalization gap between large-batch and small-batch models (Hoffer et al., 2017), and has since been used by sub- sequent research rigorously studying the large-batch regime (Shallue et al., 2018). Here, we show that it can also be useful in the medium-batch setting3. Why might Ghost Batch Normalization be useful? One reason is its power as a regularizer: due to the stochasticity in normalization statistics caused by the random selection of minibatches during training, Batch Normalization causes the representation of a training example to randomly change every time it appears in a different batch of data. Ghost Batch Normalization, by decreasing the number of examples that the normalization statistics are calculated over, increases the strength of this stochasticity, thereby increasing the amount of regularization. Based on this hypothesis, we would expect to see a unimodal effect of the Ghost Batch Normalization sizeB′on model performance — a large value of B′would offer somewhat diminished performance as a weaker regularizer, a very low value ofB′would have excess regularization and lead to poor performance, and an intermediate value would offer the best tradeoff of regularization strength. We conﬁrm this intuition in Fig. 3. Surprisingly, just using this one simple technique was capable of improving performance by 5.8% on Caltech-256 and 0.84% on CIFAR-100, which is remarkable given it has no additional cost during training. On SVHN, though, where baseline performance is already a very high 98.79% and models do not overﬁt much, usage of Ghost Batch Normalization 3We experiment with batch sizes up to 128 in this work. 5Published as a conference paper at ICLR 2020 2 4 8 16 32 64 128 Ghost Batch Size 72 73 74 75 76Accuracy CIFAR-100 2 4 8 16 32 64 128 Ghost Batch Size 98.625 98.650 98.675 98.700 98.725 98.750 98.775 98.800Accuracy SVHN 2 4 8 16 32 Ghost Batch Size 40.0 42.5 45.0 47.5 50.0 52.5 55.0 57.5Accuracy Caltech-256 Figure 3: Accuracy vs. Ghost Batch Normalization size for CIFAR-100, SVHN, and Caltech-256. CIFAR-100 Caltech-256 0.00  0.01  0.02  0.03  0.04  0.05  0.06  0.07  0.08 54.0 54.5 55.0 55.5 56.0 56.5 57.0Acc. Ghost Batch Size 16 2.7 2.8 2.9 3.0 3.1 3.2 3.3 Loss 0.00  0.02  0.04  0.06  0.08  0.10 54.0 54.5 55.0 55.5 56.0 56.5 57.0 57.5 58.0Acc. Ghost Batch Size 8 2.6 2.7 2.8 2.9 3.0 3.1 3.2 3.3 Loss 0.00  0.02  0.04  0.06  0.08  0.10  0.12  0.14 53 54 55 56 57 58 59 60Acc. Ghost Batch Size 4 2.5 2.6 2.7 2.8 2.9 3.0 3.1 3.2 3.3 3.4 Loss 0.0  0.2  0.4  0.6  0.8  1.0 30 35 40 45 50 55Acc. Ghost Batch Size 2 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 Loss 0.0  0.2  0.4  0.6  0.8  1.0 70.5 71.0 71.5 72.0 72.5 73.0 73.5 74.0Acc. Ghost Batch Size 2 1.0 1.2 1.4 1.6 1.8 2.0 Loss 0.0  0.1  0.2  0.3  0.4 72.0 72.5 73.0 73.5 74.0 74.5 75.0 75.5 76.0Acc. Ghost Batch Size 4 1.1 1.2 1.3 1.4 1.5 1.6 Loss 0.000  0.025  0.050  0.075  0.100  0.125  0.150  0.175  0.200 72.0 72.5 73.0 73.5 74.0 74.5 75.0 75.5 76.0Acc. Ghost Batch Size 8 1.00 1.05 1.10 1.15 1.20 1.25 1.30 1.35 1.40 Loss 0.00  0.02  0.04  0.06  0.08  0.10  0.12  0.14 73.0 73.5 74.0 74.5 75.0 75.5 76.0 76.5 77.0Acc. Ghost Batch Size 16 1.00 1.05 1.10 1.15 1.20 1.25 1.30 Loss SVHN 0.0  0.2  0.4  0.6  0.8  1.0 98.56 98.58 98.60 98.62 98.64 98.66 98.68 98.70Acc. Ghost Batch Size 2 0.064 0.066 0.068 0.070 0.072 0.074 0.076 0.078 0.080 Loss 0.0  0.2  0.4  0.6  0.8  1.0 98.40 98.45 98.50 98.55 98.60 98.65 98.70 98.75Acc. Ghost Batch Size 4 0.066 0.068 0.070 0.072 0.074 Loss 0.0  0.2  0.4  0.6  0.8 98.40 98.45 98.50 98.55 98.60 98.65 98.70 98.75Acc. Ghost Batch Size 8 0.066 0.068 0.070 0.072 0.074 0.076 0.078 0.080 Loss 0.0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8 98.40 98.45 98.50 98.55 98.60 98.65 98.70 98.75 98.80Acc. Ghost Batch Size 16 0.066 0.068 0.070 0.072 0.074 0.076 0.078 0.080 Loss Figure 4: The complementary effects of Inference Example Weighing (Sec. 3.1) and Ghost Batch Normalization (Sec. 3.2) on CIFAR-100, SVHN, and Caltech-256. did not result in an improvement, giving evidence that at least part of its effect is regularization in nature. In practice, B′may be treated as an additional hyperparameter to optimize. As a bonus, Ghost Batch Normalization has a synergistic effect with inference example weighing — it has the effect of making each example more important in calculating its own normalization statistics µi and σ2 i , with greater effect the smallerB′is, precisely the setting that inference example weighing corrects for. We show these results in Fig. 4, where we ﬁnd increasing gain from inference example weighing as B′is made smaller, a gain that compounds from the beneﬁts of Ghost Batch Normalization itself. Interestingly, these examples also demonstrate that accuracy and cross-entropy, the most commonly-used classiﬁcation loss, are only partially correlated, with the optimal values for the inference example weight αsometimes differing wildly between the two (e.g. for SVHN). Summary: Ghost Batch Normalization is beneﬁcial for all but the smallest of batch sizes, has no computational overhead, is straightforward to tune, and can be used in combination with inference example weighing to great effect. 6Published as a conference paper at ICLR 2020 3.3 B ATCH NORMALIZATION AND WEIGHT DECAY Weight decay (Krogh & Hertz, 1992) is a regularization technique that scales the weight of a neural network after each update step by a factor of 1 −δ, and has a complex interaction with Batch Normalization. At ﬁrst, it may even seem paradoxical that weight decay has any effect in a network trained with Batch Normalization, as scaling the weights immediately before a normalization layer by any non-zero constant has mathematically almost no effect on the output of the normalization layer (and no effect at all when ϵ = 0). However, weight decay actually has a subtle effect on the effective learning rate of networks trained with Batch Normalization — without weight decay, the weights in a batch-normalized network grow to have large magnitudes, which has an inverse effect on the effective learning rate, hampering training (Hoffer et al., 2018; van Laarhoven, 2017). Here we turn our attention to the less studied scale and bias parameters common in most normal- ization methods, γ and β. As far as we are aware, the effect of regularization on γ and β has not been studied to any great extent — Wu & He (2018) brieﬂy mention weight decay with these pa- rameters, where weight decay was used when training from scratch, but not ﬁne-tuning, two other papers (Goyal et al., 2017; He et al., 2016a) have this form of weight decay explicitly turned off, and He et al. (2019) encourage disabling weight decay on γ and β, but ultimately ﬁnd diminished performance by doing so. Unlike weight decay on weights in e.g. convolutional layers, which typically directly precede nor- malization layers, weight decay on γ and β can have a regularization effect so long as there is a path in the network between the layer in question and the ultimate output of the network, as if such paths do not pass through another normalization layer, then the weight decay is never “undone” by normalization. This structure is only common in certain types of architectures; for example, Resid- ual Networks (He et al., 2016a;b) have such paths for many of their normalization layers due to the chaining of skip-connections. However, Inception-style networks (Szegedy et al., 2016; 2017) have no residual connections, and despite the fact that each “Inception block” branches into multiple paths, every Batch Normalization layer other than those in the very last block do not have a direct path to the network’s output. We evaluated the effects of weight decay onγand βon CIFAR-100 across 10 runs, where we found that incorporating it improved accuracy by a small but signiﬁcant 0.3% (P = 0.002). Interestingly, even though γhas a multiplicative effect, we did not ﬁnd it mattered whether γwas regularized to 0 or 1 (P = 0.46) — what was important was whether it had weight decay applied at all. We did the same comparison on Caltech-256 with Inception-v3 and ResNet-50 networks, where we found evidence that the network architecture plays a crucial effect: for Inception-v3, incorpo- rating weight decay on γ and β actually hurt performance by 0.13% (mean across 3 trials), while it improved performance for the ResNet-50 network by 0.91%, supporting the hypothesis that the structure of paths between layers and the network’s output are what matter in determining its utility. On SVHN, where the baseline ResNet-18 already had a performance of98.79%, we found a similar pattern as with Ghost Batch Normalization — introducing this regularization produced no change. Summary: Regularization in the form of weight decay on the normalization parameters γ and β can be applied to any normalization layer, but is only effective in architectures with particular connectivity properties like ResNets and in tasks for which models are already overﬁtting. 3.4 G ENERALIZING BATCH AND GROUP NORMALIZATION FOR SMALL BATCHES While Batch Normalization is very effective in the medium to large-batch setting, it still suffers when not enough examples are available to calculate reliable normalization statistics. Although we have shown that techniques such as Inference Example Weighing (Sec. 3.1) can help signiﬁcantly with this, it is still only a partial solution. At the same time, Group Normalization (Wu & He, 2018) was designed for a batch size of B = 1or greater, but ignores all cross-image information. In order to generalize Batch and Group Normalization in the batch size B >1 case, we propose to expand the grouping mechanism of Group Normalization from being over only channels to being over both channels and examples — that is, normalization statistics are calculated bothwithin groups of channels of each example and across examples in groups within each batch 4. 4See submitted code for speciﬁc implementation details. 7Published as a conference paper at ICLR 2020 In principle, this would appear to introduce an additional hyperparameter on top of the number of channel groups used by Group Normalization, both of which would need to be optimized by expensive end-to-end runs of model training. However, in this case we can actually take advantage of the fact that the target batch size is small: if the batch size B is ever large enough that having multiple groups in the example dimension is useful, then it is also large enough to eschew usage of the channel groups from Group Normalization, in a regime where either vanilla Batch Normalization or Ghost Batch Normalization is more effective. Thus, when dealing with a small batch size, in practice we only need to optimize over the same set of hyperparameters as Group Normalization. To demonstrate, we target the extreme setting ofB = 2, and incorporate Inference Example Weigh- ing to all approaches. For CIFAR-100, this approach improves validation set performance over a tuned Group Normalization by0.69% in top-1 accuracy (from 73.91% to 74.60%, average over three runs), and on Caltech-256, performance dramatically improved by 5.0% (from 48.2% to 53.2%, av- erage over two runs). However, this approach has one downside: due to differences in feature statistics across examples, when using only two examples the variability in the normalization statis- tics can still be quite high, even when using multiple channels within each normalization group. As a result, a regularization effect can occur, which may be undesirable for tasks which models are not overﬁtting much. As in Sec. 3.2 and Sec. 3.3, we see this effect in SVHN, where this approach is actually ever so slightly worse than Group Normalization on the validation set (from 98.75% to 98.73%). On such datasets and tasks, it may be more fruitful to invest in higher-capacity models. Summary: Combining Group and Batch Normalization leads to more accurate models in the set- ting of batch sizes B >1, and can have a regularization effect due to Batch Normalization’s vari- ability in statistics when calculated over small batch sizes. 4 A DDITIONAL EXPERIMENTS 4.1 E XPERIMENTAL DETAILS All results in Sec. 3 were performed on the validation datasets of each respective dataset (this section examines test set performance after hyperparameters have been optimized). Of the six datasets we experiment with, only ImageNet (Russakovsky et al., 2015) and Flowers-102 (Nilsback & Zisser- man, 2008) have their own pre-deﬁned validation split, so we constructed validation splits for the other datasets as follows: for CIFAR-100 (Krizhevsky & Hinton, 2009), we randomly took 40,000 of the 50,000 training images for the training split, and the remaining 10,000 as a validation split. For SVHN (Netzer et al., 2011), we similarly split the 604,388 non-test images in a 80-20% split for training and validation. For Caltech-256, no canonical splits of any form are deﬁned, so we used 40 images of each of the 256 categories for training, 10 images for validation, and 30 for testing. For CUB-2011, we used 25% of the given training data as a validation set. The model used for CIFAR-100 and SVHN was ResNet-18 (He et al., 2016b;a) with 64, 128, 256, and 512 ﬁlters across blocks. For Caltech-256, a much larger Inception-v3 (Szegedy et al., 2016) model was used, and we additionally experiment with ResNet-152 (He et al., 2016b) on Flowers-102 and CUB-2011 in Sec. 4.3. All experiments were done on two Nvidia Geforce GTX 1080 Ti GPUs. 4.2 C OMBINING ALL FOUR : I MPROVEMENTS ACROSS BATCH SIZES Here we show the end-to-end effect of these four improvements on the test sets of each dataset, comparing against both Batch and Group Normalization, with a batch size B = 128. We plot results for CIFAR-100 and Caltech-256 in Fig. 5 (a), comparing against Group Normalization and an idealized Batch Normalization with constant performance across batch sizes (simulating if the problematic dependence of Batch Norm on the batch size were completely solved). On CIFAR-100, we see improvements against the best available baseline across all batch sizes. For medium to large batch sizes ( B ≥4), improvements are driven by the combination of Ghost Batch Normalization (Sec. 3.2), Inference Example Weighing (Sec. 3.1), and weight decay intro- duced on γand β(Sec. 3.3). To aid in distinguishing between these effects, we also plot the impact of Ghost Batch Normalization alone, which we ﬁnd particularly impactful as long as long as the batch size isn’t too small ( B >2). Turning to very small batch sizes, for B = 1 improvements 8Published as a conference paper at ICLR 2020 1  2  4  8  16  32  64 Batch size 75 76 77 78 79 80 81 82 83Accuracy CUB-2011 Batch Normalization (ideal) Group Normalization Ghost Batch Normalization Ours 1  2  4  8  16  32 Batch size 44 46 48 50 52 54 56 58Accuracy Caltech-256 Batch Normalization (ideal) Group Normalization Ghost Batch Normalization Ours 1  2  4  8  16  32  64  128 Batch size 74.0 74.5 75.0 75.5 76.0 76.5 77.0 77.5 78.0 78.5Accuracy CIFAR-100 Batch Normalization (ideal) Group Normalization Ghost Batch Normalization Ours 1  2  4  8  16  32  64 Batch size 82 84 86 88 90 92 94 96Accuracy Flowers-102 Batch Normalization (ideal) Group Normalization Ghost Batch Normalization Ours (a) (b) Figure 5: Total performance changes across batch sizes for CIFAR-100 and Caltech-256 (a) when training from scratch, incorporating all proposed improvements to Batch Normalization. On the bot- tom (b) is the same on Flowers-102 and CUB-2011, which employs transfer learning via ﬁne-tuning from ImageNet. Also shown within each plot is the performance of Group Normalization, an ide- alized Batch Normalization that scales perfectly across batch sizes, and Ghost Batch Normalization (Sec. 3.2) by itself, for which the x-axis represents the Ghost Batch Size B′. are due to the introduced weight decay, and for B = 2 the generalization of Batch and Group Normalization leads to the improvement (Sec. 3.4), with some additional effect from weight decay. Improvements on Caltech-256 follow the same trends, but to greater magnitude, with a total increase in performance of 6.5% over Batch Normalization and an increase of 5.9% over Group Normaliza- tion for B = 2. 4.3 T RANSFER LEARNING We also show the applicability of these approaches in the context of transfer learning, which we demonstrate on the Flowers-102 (Nilsback & Zisserman, 2008) and CUB-2011 (Wah et al., 2011) datasets via ﬁne-tuning a ResNet-152 model from ImageNet. These tasks presents several chal- lenges: 1) the Flowers-102 data only contains 10 images per category in the training set (and CUB- 2011 only 30 examples per class), 2) pre-training models on ImageNet is a very strong form of prior knowledge, and despite the small dataset size may heavily reduce the regularization effects of some of the techniques, and 3) we examine the setting of pre-training with generic ImageNet mod- els trained without any of these modiﬁcations, which gives an advantage to both the generic Batch Normalization and Group Normalization, for which pre-trained models exist. We plot results in Fig. 5 (b), where we ﬁnd remarkable qualitative agreement of our non-transfer learning results to this setting, despite the challenges. In total, on Flowers-102 our techniques were able to improve upon Batch Normalization by 2.4% (from 91.0% to 93.4% top-1 accuracy, a 27% relative reduction in error), and upon Group Normalization by 6.1% (from 87.3%, a 48% relative reduction in error). On CUB-2011, which has more training data, we improved upon Batch Normal- ization by 1.4% (from 81.1% to 82.4%) and Group Normalization by 3.8% (from 78.6%). We anticipate that even further improvements might arise by additionally pre-training models with some of these techniques (particularly Ghost Batch Normalization), as we were able to see a large impact (roughly 5%) on Group Normalization by pre-training with a Group Normalization-based model instead of Batch Normalization. 9Published as a conference paper at ICLR 2020 Table 1: Accuracy on CIFAR-100 with non-i.i.d. minibatches. B′refers to the Ghost Batch Nor- malization size (equivalent to the batch size for Batch Normalization and Batch Renormalization), and “Batch Group Norm.” refers to our approach in Sec. 3.4. “Inf. Ex. Weight: Off” refers to using only the moving averages for normalization statistics ( i.e. α = 0), while “On” refers to tuning α based on the validation set. Method B′ Inf. Ex. Weight: Off Inf. Ex. Weight: On Batch Norm 128 40.1 62.2 Batch ReNorm 128 69.0 69.0 Ghost Batch Norm 64 42.3 50.8 32 57.8 70.9 16 64.3 72.2 8 68.7 72.0 4 70.4 71.5 2 68.4 71.4 Batch Group Norm. 2 75.2 76.1 4.4 N ON-I.I.D. MINIBATCHES An implicit assumption in Batch Normalization is that training examples are sampled independently, so that minibatch normalization statistics all follow roughly the same distribution and training statis- tics are faithfully represented in the moving averages. However, in applications where training batches are not sampled i.i.d., such as metric learning (Oh Song et al., 2016; Movshovitz-Attias et al., 2017) or hard negative mining (Shrivastava et al., 2016), violating this assumption may lead to undesired consequences in the model. Here, we test our approaches in this challenging setting. Following Batch Renormalization (Ioffe, 2017), we study the case where examples in a minibatch are sampled from a small number of classes — speciﬁcally, we consider CIFAR-100, and study the extreme case where each minibatch ( B = 128) is comprised of examples from only four random categories (sampled with replacement), each of which is represented with 32 examples in the mini- batch. We present results for Batch Normalization, Batch Renormalization, our generalization of Batch and Group Normalization from Sec. 3.4 (“Batch Group Norm.”), and the full interaction of Ghost Batch Normalization and Inference Example Weighing in Table 1. In this challenging setting, Inference Example Weighing, Ghost Batch Normalization, and Batch Group Norm all have large ef- fect, in many cases halving the error rate of Batch Normalization. For example, Inference Example Weighing was able to reduce the error rate by 20% without any retraining, and tuning Ghost Batch Normalization, even without any inference modiﬁcations, was just as effective as Batch Renormal- ization, a technique partially designed for the non-i.i.d. case. Even further, Batch Group Normaliza- tion was hardly affected at all by the non-i.i.d. training distribution ( 76.1 vs 76.2 for i.i.d.). Last, it is interesting to note that Inference Example Weighing had practically no effect on Batch Renormal- ization (improvement ≤0.1%), conﬁrming Batch Renormalization’s effect in making models more robust to the use of training vs moving average normalization statistics. 5 C ONCLUSION In this work, we have demonstrated four improvements to Batch Normalization that should be known by all who use it. These include: a method for leveraging the statistics of inference examples more effectively in normalization statistics, ﬁxing a discrepancy between training and inference with Batch Normalization; demonstrating the surprisingly powerful effect of Ghost Batch Normalization for improving generalization of models without requiring very large batch sizes; investigating the previously unstudied effect of weight decay on the scaling and shifting parameters γ and β; and introducing a new approach for normalization in the small batch setting, generalizing and leveraging the strengths of both Batch and Group Normalization. In each case, we have done our best to not only demonstrate the effect of the method, but also provide guidance and evidence for precisely which cases in which it may be effective, which we hope will aid in their applicability. 10Published as a conference paper at ICLR 2020 REFERENCES Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Minhyung Cho and Jaehyung Lee. Riemannian approach to batch normalization. In Advances in Neural Information Processing Systems, pp. 5225–5235, 2017. Lucas Deecke, Iain Murray, and Hakan Bilen. Mode normalization. In International Conference on Learning Representations, 2019. Priya Goyal, Piotr Doll´ar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, An- drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recur- rent neural networks. In 2013 IEEE international conference on acoustics, speech and signal processing, pp. 6645–6649. IEEE, 2013. Yong Guo, Qingyao Wu, Chaorui Deng, Jian Chen, and Mingkui Tan. Double forward propagation for memorized batch normalization. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog- nition. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 770–778, 2016a. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European Conference on Computer Vision, pp. 630–645. Springer, 2016b. Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pp. 2961–2969, 2017. Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. Bag of tricks for image classiﬁcation with convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 558–567, 2019. Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generaliza- tion gap in large batch training of neural networks. InAdvances in Neural Information Processing Systems, pp. 1731–1741, 2017. Elad Hoffer, Ron Banner, Itay Golan, and Daniel Soudry. Norm matters: efﬁcient and accurate normalization schemes in deep networks. InAdvances in Neural Information Processing Systems, pp. 2164–2174, 2018. Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7132–7141, 2018. Lei Huang, Dawei Yang, Bo Lang, and Jia Deng. Decorrelated batch normalization. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 791–800, 2018. Lei Huang, Yi Zhou, Fan Zhu, Li Liu, and Ling Shao. Iterative normalization: Beyond standardiza- tion towards efﬁcient whitening. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4874–4883, 2019. Sergey Ioffe. Batch renormalization: Towards reducing minibatch dependence in batch-normalized models. In Advances in Neural Information Processing Systems, pp. 1945–1953, 2017. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In In Proceedings of The 32nd International Conference on Machine Learning, pp. 448–456, 2015. 11Published as a conference paper at ICLR 2020 G¨unter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing neural networks. In Advances in neural information processing systems, pp. 971–980, 2017. Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech- nical report, Citeseer, 2009. Anders Krogh and John A Hertz. A simple weight decay can improve generalization. In Advances in neural information processing systems, pp. 950–957, 1992. Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuo- motor policies. The Journal of Machine Learning Research, 17(1):1334–1373, 2016. Etai Littwin and Lior Wolf. Regularizing by the variance of the activations’ sample-variances. In Advances in Neural Information Processing Systems, pp. 2119–2129, 2018. Ping Luo, Jiamin Ren, and Zhanglin Peng. Differentiable learning-to-normalize via switchable normalization. In International Conference on Learning Representations, 2019. Yair Movshovitz-Attias, Alexander Toshev, Thomas K Leung, Sergey Ioffe, and Saurabh Singh. No fuss distance metric learning using proxies. In Proceedings of the IEEE International Conference on Computer Vision, pp. 360–368, 2017. Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. 2011. M-E. Nilsback and A. Zisserman. Automated ﬂower classiﬁcation over a large number of classes. In Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing , Dec 2008. Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep metric learning via lifted structured feature embedding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4004–4012, 2016. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211–252, 2015. Tim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Advances in Neural Information Processing Systems , pp. 901–909, 2016. Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mo- bilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4510–4520, 2018. Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch nor- malization help optimization? In Advances in Neural Information Processing Systems, pp. 2488– 2498, 2018. Christopher J Shallue, Jaehoon Lee, Joe Antognini, Jascha Sohl-Dickstein, Roy Frostig, and George E Dahl. Measuring the effects of data parallelism on neural network training. arXiv preprint arXiv:1811.03600, 2018. Abhinav Shrivastava, Abhinav Gupta, and Ross Girshick. Training region-based object detectors with online hard example mining. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 761–769, 2016. N. Silberman and S. Guadarrama. Tensorﬂow-slim image classiﬁcation model library. https: //github.com/tensorflow/models/tree/master/research/slim. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations, 2015. 12Published as a conference paper at ICLR 2020 Saurabh Singh and Abhinav Shrivastava. Evalnorm: Estimating batch normalization statistics for evaluation. In Proceedings of the IEEE International Conference on Computer Vision, pp. 3633– 3641, 2019. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pp. 3104–3112, 2014. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethink- ing the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2818–2826, 2016. Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi. Inception-v4, inception-resnet and the impact of residual connections on learning. In Thirty-First AAAI Confer- ence on Artiﬁcial Intelligence, 2017. Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing in- gredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016. Twan van Laarhoven. L2 regularization versus batch and weight normalization. arXiv preprint arXiv:1706.05350, 2017. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pp. 5998–6008, 2017. Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. 2011. Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 3–19, 2018. Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel S Schoenholz, and Jeffrey Penning- ton. Dynamical isometry and a mean ﬁeld theory of cnns: How to train 10,000-layer vanilla convolutional neural networks. In International Conference on Machine Learning, 2018. Hongyi Zhang, Yann N. Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without normalization. In International Conference on Learning Representations, 2019. Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. In Interna- tional Conference on Learning Representations, 2017. Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for scalable image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 8697–8710, 2018. 13Published as a conference paper at ICLR 2020 A P ROOF OF BATCH NORMALIZATION OUTPUT BOUNDS Here we present a proof of Eq. 2. We ﬁrst prove the bound as an inequality and then show that it is tight. Without loss of generality, we assume that x0 is the minimum of {xi}B−1 i=0 and that γ ≥0. Then we want to show that min x0,...,xB−1 γ x0 −µi√ σ2 i + ϵ + β = −γ √ B−1 +β (5) Expanding µi and σ2 i (using the maximum likelihood estimator for σ2 i ), and canceling the scaling and offset terms γand β, we want to show min x0,...,xB−1 x0 −1 B ∑B−1 i=0 xi√ 1 B ∑B−1 i=0 (xi −1 B ∑B−1 j=0 xj)2 + ϵ = − √ B−1 (6) From here we assume without loss of generality that x0 = 0– since the output of Batch Normal- ization is invariant to an additive constant on all xi, we can subtract x0 from all xi and maintain the same value. We also assume that all xi ≥0, then frame the minimum as a bound −1 B ∑B−1 i=0 xi√ 1 B ∑B−1 i=0 (xi −1 B ∑B−1 j=0 xj)2 + ϵ ≥− √ B−1 (7) −1 B B−1∑ i=0 xi ≥− √B−1 B B−1∑ i=0  xi −1 B B−1∑ j=0 xj   2 + ϵ (8) −1 B B−1∑ i=0 xi ≥− √ B−1 B B−1∑ i=0  x2 i −2xi B B−1∑ j=0 xj + 1 B2   B−1∑ j=0 xj   2 + ϵ (9) B−1∑ i=0 xi ≤ √ B−1∑ i=0  (B−1)Bx2 i −2(B−1)xi B−1∑ j=0 xj + B−1 B   B−1∑ j=0 xj   2 + ϵ (10) B−1∑ i=0 xi ≤ √(B−1)B B−1∑ i=0 x2 i −2(B−1) (B−1∑ i=0 xi )2 + (B−1) (B−1∑ i=0 xi )2 + ϵ (11) B−1∑ i=0 xi ≤ √(B−1)B B−1∑ i=0 x2 i −(B−1) (B−1∑ i=0 xi )2 + ϵ (12) (B−1∑ i=0 xi )2 ≤(B−1)B B−1∑ i=0 x2 i −(B−1) (B−1∑ i=0 xi )2 + ϵ (13) B (B−1∑ i=0 xi )2 ≤(B−1)B B−1∑ i=0 x2 i + ϵ (14) (B−1∑ i=0 xi )2 ≤(B−1) B−1∑ i=0 x2 i + ϵ (15) Using the fact that x0 = 0and ϵ> 0, it sufﬁces to show (B−1∑ i=1 xi )2 ≤(B−1) B−1∑ i=1 x2 i (16) 14Published as a conference paper at ICLR 2020 With a change of variables, we have the more general (N−1∑ i=0 xi )2 ≤N N−1∑ i=0 x2 i (17) 1 N2 (N−1∑ i=0 xi )2 ≤ 1 N N−1∑ i=0 x2 i (18) E[x]2 ≤E[x2] (19) E[x2] −E[x]2 ≥0 (20) which is simply an alternate form for the variance of x, which is always non-negative, completing the bound. To show that the bound is tight, we can setx0 = 0and xi = afor all i> 0, where ais a non-negative constant: −1 B ∑B−1 i=0 xi√ 1 B ∑B−1 i=0 (xi −1 B ∑B−1 j=0 xj)2 + ϵ (21) −1 B ∑B−1 i=1 a√ 1 B ( (B−1)2 B2 a2 + ∑B−1 i=1 (a−B−1 B a)2 ) + ϵ (22) −B−1 B a√ 1 B ( (B−1)2 B2 a2 + (B−1)a2 ( 1 −2(B−1) B + (B−1)2 B2 )) + ϵ (23) −(B−1)a B √ a2(B−1) B ( B−1 B2 + 1−2B−1 B + (B−1)2 B2 ) + ϵ (24) −(B−1)a√ a2(B−1) ( B−1 B + B−2B+ 2 +(B−1)2 B ) + ϵ (25) −(B−1)a√ a2(B−1) (B2−2B+1+B−1−B2+2B B ) + ϵ (26) −(B−1)a√ a2(B−1) +ϵ (27) As a→∞ (or if ϵ= 0), then this approaches −(B−1)a a √ (B−1) (28) which is simply − √ (B−1) (29) completing the proof. 15Published as a conference paper at ICLR 2020 5 6 7 8 9 10 11 log(B) 40 20 0 20 40 BN Output RangeBound during training Observed during training Observed during testing Figure 6: Range of output values obtained during inference on the CIFAR-10 test set, compared with the range observed during training and the bound of Eq. 2. See text for details. B E MPIRICAL EVIDENCE OF BATCH NORMALIZATION OUTPUT BOUNDS In Fig. 6 we show the observed output ranges for the last Batch Normalization layer in our CIFAR- 10 network (spatial resolution: 4 ×4), plotting both the range during training and at inference time on the CIFAR-10 test set. Different values of B were obtained by using different Ghost Batch Normalization sizes, keeping in mind that B is determined by the product of the batch size and spatial dimensions. At large values of B, it is unlikely that any network obtains a value even close to the bound of Eq. 2, but as B gets smaller, the output range of the network during training becomes smaller in magnitude, eventually being nearly tight with our bound — for example, for log(B) = 5, the theoretical minimum is −5.57, while the network obtained a minimum of −5.30. However, the maximum and minimum values obtained during inference on the test set show no clear pattern asB changes, and are not subject to the training time bound, which is particularly noticeable for small values of B, where values fall outside the training-time bounds. C N EGATIVE RESULTS : A PPROACHES THAT DIDN ’T WORK . Here we detail a handful of approaches which seemed intuitively promising but ultimately failed to produce positive results. BATCH NORMALIZATION MOVING AVERAGES . In an attempt to resolve the other disparities Batch Normalization has between its training and inference behaviors, we experimented with a handful of different approaches for modifying the moving averages used during inference. First, since examples at inference time do not have data augmentation applied to them, we tried comput- ing the moving averages over examples without data augmentation (implemented by training the model for a few extra epochs over non-augmented examples with a learning rate of 0, but while still updating the moving average variables). This decreased accuracy on CIFAR-100 by roughly half a percent, though it did yield mild improvements to the test set cross-entropy loss. Next, we experimented with calculating the moving averages over the test set, not making use of any of the test labels. Perhaps surprisingly, this behaved very similar to when moving averages were calculated over the training examples (within 0.1% in accuracy and within 1% in cross-entropy), with trends holding regardless of whether data augmentation was applied or not. ADDING BATCH NORMALIZATION -LIKE STOCHASTICITY TO GROUP NORMALIZATION . One of the hypotheses for why Group Normalization generally performs slightly worse than Batch Nor- malization is the regularization effect of Batch Normalization due to random minibatches producing variability in the normalization statistics. Therefore, we tried introducing stochasticity to Group Normalization in a variety of ways, none of which we could get to work well: 1) Adding gaussian 16Published as a conference paper at ICLR 2020 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 64 66 68 70 72 74Acc. 1.2 1.4 1.6 1.8 2.0 2.2 Loss 82 84 86 88 90 92Top-5 Acc. MobileNetv2 0.00 0.02 0.04 0.06 0.08 0.10 72 73 74 75 76 77 78Acc. 0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.2 Loss 90.5 91.0 91.5 92.0 92.5 93.0 93.5 94.0 94.5Top-5 Acc. ResNet-152 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.16 78 79 80 81 82 83Acc. 0.75 0.80 0.85 0.90 0.95 1.00 Loss 94.25 94.50 94.75 95.00 95.25 95.50 95.75 96.00 96.25Top-5 Acc. NasNet-A Figure 7: Effect of the example-weighing hyperparameter αon ImageNet; supplemental version of Fig. 1 with a larger range of α. 0.0  0.2  0.4  0.6  0.8  1.0 15 20 25 30 35 40 45 50Acc. Caltech-256 with Group Normalization 3 4 5 6 7 8 9 10 Loss 0.0  0.2  0.4  0.6  0.8  1.0 90 92 94 96 98 100Acc. SVHN with Group Normalization 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Loss 0.0  0.2  0.4  0.6  0.8  1.0 30 40 50 60 70 80Acc. CIFAR-100 with Group Normalization 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 Loss Figure 8: Effect of the example-weighing hyperparameter αfor models trained with Group Nor- malization on CIFAR-100, SVHN, and Caltech-256; supplemental version of Fig. 2 with a larger range of α. noise to the normalization statistics, where the noise is based on a moving average of the normal- ization statistics, 2) Using random groupings of channels for calculating normalization statistics (optionally only doing randomization a fraction of the time), and 3) changing the number of groups throughout the training procedure, either as increasing or decreasing functions of training steps. MORE PRINCIPLED GROUP SIZE COMPUTATION . As part of generalizing Batch and Group Nor- malization, we examined whether it was possible to determine the number of groups in each nor- malization layer in a more principled way that simply specifying it as a constant throughout the network. For example, one approach we had mild success with was setting the number of elements per group (height ×width ×group size) to a constant, making the number of elements contributing to the normalization statistics uniform across layers. However, we were unable to get any of these ideas to work in a way that generalized properly across datasets. We also tried learning group sizes in a differentiable way with Switchable Normalization, but found that this made models overﬁt too much. D S UPPLEMENTAL INFERENCE EXAMPLE WEIGHING PLOTS In Figures 7, 8, and 9 we present plots corresponding to Figures 1, 2, and 4 of the main text, with larger ranges of the inference weightα. In the main text, we restricted the range ofαto values which showed off the tradeoff of αversus performance at a reasonably local scale, and these ﬁgures show a larger scale for completeness in characterizing model behavior. While this behavior can largely be extrapolated from the behavior for a smaller range of α, there are some interesting trends. On ImageNet 7, we see that only a small amount of inference example weighing is necessary to get most of its beneﬁt, and setting αto larger values corresponds to a regime quite different than in training, smoothly decaying model performance as αbecomes less and less appropriate. Similarly, when applying inference example weighing to Group Normalization (Fig. 8, while performance intuitively decays as αmoves farther and farther away from 1, a surprisingly large range of values for α result in similar performance to Group Normalization, especially on SVHN. Lastly, when comparing the effect of αon models trained with Ghost Batch Normalization (Fig. 9, we clearly see that the optimal value for αis decreasing with respect to the Ghost Batch Normalization size, with the possible unusual exception of optimizing for loss on SVHN. 17Published as a conference paper at ICLR 2020 CIFAR-100 Caltech-256 SVHN 0.0  0.2  0.4  0.6  0.8  1.0 30 40 50 60 70 80Acc. Ghost Batch Size 16 1.0 1.5 2.0 2.5 3.0 Loss 0.0  0.2  0.4  0.6  0.8  1.0 45 50 55 60 65 70 75 80Acc. Ghost Batch Size 8 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 Loss 0.0  0.2  0.4  0.6  0.8  1.0 60 62 64 66 68 70 72 74 76 78Acc. Ghost Batch Size 4 1.05 1.10 1.15 1.20 1.25 1.30 1.35 1.40 1.45 1.50 Loss 0.0  0.2  0.4  0.6  0.8  1.0 70.5 71.0 71.5 72.0 72.5 73.0 73.5 74.0Acc. Ghost Batch Size 2 1.0 1.2 1.4 1.6 1.8 2.0 Loss 0.0  0.2  0.4  0.6  0.8  1.0 0 10 20 30 40 50 60Acc. Ghost Batch Size 16 3 4 5 6 7 Loss 0.0  0.2  0.4  0.6  0.8  1.0 0 10 20 30 40 50 60Acc. Ghost Batch Size 8 3 4 5 6 7 Loss 0.0  0.2  0.4  0.6  0.8  1.0 0 10 20 30 40 50 60Acc. Ghost Batch Size 4 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 Loss 0.0  0.2  0.4  0.6  0.8  1.0 30 35 40 45 50 55Acc. Ghost Batch Size 2 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 Loss 0.0  0.2  0.4  0.6  0.8  1.0 97.4 97.6 97.8 98.0 98.2 98.4 98.6 98.8Acc. Ghost Batch Size 16 0.06 0.08 0.10 0.12 0.14 0.16 Loss 0.0  0.2  0.4  0.6  0.8  1.0 98.0 98.2 98.4 98.6 98.8Acc. Ghost Batch Size 8 0.065 0.070 0.075 0.080 0.085 0.090 0.095 0.100 Loss 0.0  0.2  0.4  0.6  0.8  1.0 98.40 98.45 98.50 98.55 98.60 98.65 98.70 98.75Acc. Ghost Batch Size 4 0.066 0.068 0.070 0.072 0.074 Loss 0.0  0.2  0.4  0.6  0.8  1.0 98.56 98.58 98.60 98.62 98.64 98.66 98.68 98.70Acc. Ghost Batch Size 2 0.064 0.066 0.068 0.070 0.072 0.074 0.076 0.078 0.080 Loss Figure 9: The complementary effects of Inference Example Weighing and Ghost Batch Normaliza- tion on CIFAR-100, SVHN, and Caltech-256; supplemental version of Fig. 4 with a larger range of α. 18",
      "meta_data": {
        "arxiv_id": "1906.03548v2",
        "authors": [
          "Cecilia Summers",
          "Michael J. Dinneen"
        ],
        "published_date": "2019-06-09T01:14:48Z",
        "pdf_url": "https://arxiv.org/pdf/1906.03548v2.pdf"
      }
    },
    {
      "title": "TaskNorm: Rethinking Batch Normalization for Meta-Learning",
      "abstract": "Modern meta-learning approaches for image classification rely on increasingly\ndeep networks to achieve state-of-the-art performance, making batch\nnormalization an essential component of meta-learning pipelines. However, the\nhierarchical nature of the meta-learning setting presents several challenges\nthat can render conventional batch normalization ineffective, giving rise to\nthe need to rethink normalization in this setting. We evaluate a range of\napproaches to batch normalization for meta-learning scenarios, and develop a\nnovel approach that we call TaskNorm. Experiments on fourteen datasets\ndemonstrate that the choice of batch normalization has a dramatic effect on\nboth classification accuracy and training time for both gradient based and\ngradient-free meta-learning approaches. Importantly, TaskNorm is found to\nconsistently improve performance. Finally, we provide a set of best practices\nfor normalization that will allow fair comparison of meta-learning algorithms.",
      "full_text": "TASK NORM : Rethinking Batch Normalization for Meta-Learning John Bronskill * 1 Jonathan Gordon * 1 James Requeima 1 2 Sebastian Nowozin 3 Richard E. Turner1 3 Abstract Modern meta-learning approaches for image clas- siﬁcation rely on increasingly deep networks to achieve state-of-the-art performance, making batch normalization an essential component of meta-learning pipelines. However, the hierarchi- cal nature of the meta-learning setting presents several challenges that can render conventional batch normalization ineffective, giving rise to the need to rethink normalization in this setting. We evaluate a range of approaches to batch normal- ization for meta-learning scenarios, and develop a novel approach that we call TASK NORM . Ex- periments on fourteen datasets demonstrate that the choice of batch normalization has a dramatic effect on both classiﬁcation accuracy and train- ing time for both gradient based- and gradient- free meta-learning approaches. Importantly, TAS- KNORM is found to consistently improve perfor- mance. Finally, we provide a set of best practices for normalization that will allow fair comparison of meta-learning algorithms. 1. Introduction Meta-learning, or learning to learn (Thrun & Pratt, 2012; Schmidhuber, 1987), is an appealing approach for design- ing learning systems. It enables practitioners to construct models and training procedures that explicitly target de- sirable charateristics such as sample-efﬁciency and out-of- distribution generalization. Meta-learning systems have been demonstrated to excel at complex learning tasks such as few-shot learning (Snell et al., 2017; Finn et al., 2017) and continual learning (Nagabandi et al., 2019; Requeima et al., 2019a; Jerfel et al., 2019). Recent approaches to meta-learning rely on increasingly deep neural network based architectures to achieve state-of- *Equal contribution 1University of Cambridge 2Invenia Labs 3Microsoft Research. Correspondence to: John Bronskill <jfb54@cam.ac.uk>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). the-art performance in a range of benchmark tasks (Finn et al., 2017; Mishra et al., 2018; Triantaﬁllou et al., 2020; Requeima et al., 2019a). When constructing very deep net- works, a standard component is the use of normalization layers (NL). In particular, in the image-classiﬁcation do- main, batch normalization (BN; Ioffe, 2017) is crucial to the successful training of very deep convolutional networks. However, as we discuss in Section 3, standard assumptions of the meta-learning scenario violate the assumptions of BN and vice-versa, complicating the deployment of BN in meta-learning. Many papers proposing novel meta-learning approaches employ different forms of BN for the proposed models, and some forms make implicit assumptions that, while improving benchmark performance, may result in potentially undesirable behaviours. Moreover, as we demon- strate in Section 5, performance of the trained models can vary signiﬁcantly based on the form of BN employed, con- founding comparisons across methods. Further, naive adop- tion of BN for meta-learning does not reﬂect the statistical structure of the data-distribution in this scenario. In contrast, we propose a novel variant of BN – TASK NORM – that explicitly accounts for the statistical structure of the data distribution. We demonstrate that by doing so, TASK NORM further accelerates training of models using meta-learning while achieving improved test-time performance. Our main contributions are as follows: • We identify and highlight several issues with BN schemes used in the recent meta-learning literature. • We propose TASK NORM , a novel variant of BN which is tailored for the meta-learning setting. • In experiments with fourteen datasets, we demonstrate that TASK NORM consistently outperforms competing meth- ods, while making less restrictive assumptions than its strongest competitor. 2. Background and Related Work In this section we lay the necessary groundwork for our investigation of batch normalization in the meta-learning scenario. Our focus in this work is on image classiﬁcation. We denote images x∈RC×W×H where W is the image width, Hthe image height, Cthe number of image channels. Each image is associated with a label y ∈ {1,...,M } arXiv:2003.03284v2  [stat.ML]  28 Jun 2020TASK NORM : Rethinking Batch Normalization for Meta-Learning xτ∗myτ∗mψτyτnxτn θ ContextDτ TargetTτ m= 1,...,Mτn= 1,...,Nτ τ=1,... Figure 1.Directed graphical model for multi-task meta-learning. where M is the number of image classes. Finally, a dataset is denoted D= {(xn,yn)}N n=1. 2.1. Meta-Learning We consider the meta-learning classiﬁcation scenario. Rather than a single, large dataset D, we assume access to a dataset D= {τt}K t=1 comprising a large number of training tasks τt, drawn i.i.d. from a distribution p(τ). The data for a task τ consists of a context set Dτ = {(xτ n,yτ n)}Nτ n=1 with Nτ elements with the inputs xτ n and labels yτ n observed, and a target set Tτ = {(xτ∗ m,yτ∗ m)}Mτ m=1 with Mτ elements for which we wish to make predictions. Here the inputs xτ∗are observed and the labels yτ∗are only observed during meta- training (i.e., training of the meta-learning algorithm). The examples from a single task are assumed i.i.d., but examples across tasks are not. Note that the target set examples are drawn from the same set of labels as the examples in the context set. At meta-test time, the meta-learner is required to make predictions for target set inputs of unseen tasks. Often, the assumption is that test tasks will include classes that have not been seen during meta-training, and Dτ will contain only a few observations. The goal of the meta-learner is to process Dτ, and produce a model that can make predictions for any test inputs xτ∗∈Tτ∗associated with the task. Meta-Learning as Hierarchical Probabilistic Modelling A general and useful view of meta-learning is through the perspective of hierarchical probabilistic modelling (Heskes, 2000; Bakker & Heskes, 2003; Grant et al., 2018; Gordon et al., 2019). A standard graphical representation of this modelling approach is presented in Figure 1. Global param- eters θencode information shared across all tasks, while local parameters ψτ encode information speciﬁc to task τ. This model introduces a hierarchy of latent parameters, cor- responding to the hierarchical nature of the data distribution. A general approach to meta-learning is to design inference procedures for the task-speciﬁc parameters ψτ = fφ(Dτ) conditioned on the context set (Grant et al., 2018; Gor- don et al., 2019), where f is parameterized by additional parameters φ. Thus, a meta-learning algorithm deﬁnes a predictive distribution parameterized by θ and φ as p(yτ∗ m|xτ∗ m,fφ (Dτ) ,θ) .This perspective relates to the in- ner and outer loops of meta-learning algorithms (Grant et al., 2018; Rajeswaran et al., 2019): the inner loop uses fφ to provide local updates to ψ, while the outer loop provides predictions for target points. Below, we use this view to summarize a range of meta-learning approaches. Episodic Training The majority of modern meta-learning methods employ episodic training (Vinyals et al., 2016). During meta-training, a task τ is drawn from p(τ) and ran- domly split into a context set Dτ and target set Tτ. The meta-learning algorithm’s inner-loop is then applied to the context set to produce ψτ. With θand ψτ, the algorithm can produce predictions for the target set inputs xτ∗ m. Given a differentiable loss function, and assuming thatfφ is also differentiable, the meta-learning algorithm can then be trained with stochastic gradient descent algorithms. Using log-likelihood as an example loss function, we may express a meta-learning objective for θand φas L(θ,φ) = E p(τ) [Mτ∑ m=1 log p(yτ∗ m|xτ∗ m,fφ (Dτ) ,θ) ] . (1) Common Meta-Learning Algorithms There has been an explosion of meta-learning algorithms proposed in re- cent years. For an in-depth review see Hospedales et al. (2020). Here, we brieﬂy introduce several methods, focus- ing on those that are relevant to our experiments. Arguably the most widely used is the gradient-based approach, the canonical example for modern systems being MAML (Finn et al., 2017). MAML sets θto be the initialization of the neural network parameters. The local parameters ψτ are the network parameters after applying one or more gradient updates based on Dτ. Thus, f in the case of MAML is a gradient-based procedure, which may or may not have additional parameters (e.g., learning rate). Another widely used class of meta-learners are amortized- inference based approaches e.g, VERSA (Gordon et al., 2019) and CNAP S (Requeima et al., 2019a). In these meth- ods, θparameterizes a shared feature extractor, and ψa set of parameters used to adapt the network to the local task, which include a linear classiﬁer and possibly addi- tional parameters of the network. For these models, f is implemented via hyper-networks (Ha et al., 2016) with pa- rameters φ. An important special case of this approach is Prototypical Networks (ProtoNets) (Snell et al., 2017), which replace ψwith nearest neighborhood classiﬁcation in the embedding space of a learned feature extractor gθ. 2.2. Normalization Layers in Deep Learning Normalization layers (NL) for deep neural networks were introduced by Ioffe & Szegedy (2015) to accelerate the train-TASK NORM : Rethinking Batch Normalization for Meta-Learning ing of neural networks by allowing the use of higher learning rates and decreasing the sensitivity to network initialization. Since their introduction, they have proven to be crucial components in the successful training of ever-deeper neural architectures. Our focus is the few-shot image classiﬁcation setting, and as such we concentrate on NLs for 2D convolu- tional networks. The input to a NL is A= (a1,..., aB), a batch of Bimage-shaped activations or pre-activations, to which the NL is applied as a′ n = γ (an −µ√ σ2 + ϵ ) + β, (2) where µand σare the normalization moments, γand βare learned parameters, ϵis a small scalar to prevent division by 0, and operations between vectors are element-wise. NLs differ primarily by how the normalization moments are computed. The ﬁrst such layer – batch normalization (BN) – was introduced by Ioffe & Szegedy (2015). A BN layer distinguishes between training and test modes. At training time, BN computes the moments as µBNc = 1 BHW B∑ b=1 W∑ w=1 H∑ h=1 abwhc, (3) σ2 BNc = 1 BHW B∑ b=1 W∑ w=1 H∑ h=1 (abwhc −µBNc)2. (4) Here, µBN,σ2 BN,γ,β∈RC. As µBN and σ2 BN depend on the batch of observations, BN can be susceptible to failures if the batches at test time differ signiﬁcantly from training batches, e.g., for streaming predictions. To coun- teract this, at training time, a running mean and variance, µr,σr ∈RC, are also computed for each BN layer over all training tasks and stored. At test time, test activations aare normalized using Equation (2) with the statistics µr and σr in place of the batch statistics. Importantly, BN relies on the implicit assumption that Dcomprises i.i.d. samples from some underlying distribution. More recently, additional NLs have been introduced. Many of these methods differ from standard BN in that they nor- malize each instance independently of the remaining in- stances in the batch, making them more resilient to batch distribution shifts at test time. These include instance nor- malization (Ulyanov et al., 2016), layer normalization (Ba et al., 2016), and group normalization (Wu & He, 2018). These are discussed further in Section 3.3. 2.3. Desiderata for Meta-Learning Normalization Layers As modern approaches to meta-learning systems routinely employ deep networks, NLs become essential for efﬁcient training and optimal classiﬁcation performance. For BN in the standard supervised settings, i.i.d. assumptions about the data distribution imply that estimating moments from the training set will provide appropriate normalization statistics for test data. However, this does not hold in the meta- learning scenario, for which data points are only assumed to be i.i.d. within a speciﬁc task. Therefore, the choice of what moments to use when applying a NL to the context and target set data points, during both meta-training and meta-test time, is key. As a result, recent meta-learning approaches employ several normalization procedures that differ according to these de- sign choices. A range of choices are summarized in Figure 2. As we discuss in Section 3 and demonstrate with experi- mental results, some of these have implicit, undesirable assumptions which have signiﬁcant impact on both predic- tive performance and training efﬁciency. We argue that an appropriate NL for the meta-learning scenario requires consideration of the data-generating assumptions associated with the setting. In particular, we propose the following desiderata for a NL when used for meta-learning: 1. Improves speed and stability of training without harming test performance (test set accuracy or log-likelihood); 2. Works well across a range of context set sizes; 3. Is non-transductive, thus supporting inference at meta- test time in a variety of circumstances. A non-transductive meta-learning system makes predictions for a single test set label conditioned only on a single in- put and the context set, while a transductive meta-learning system conditions on additional samples from the test set: p(yτ∗ i |xτ∗ i ,Dτ) non-transductive ; p(yτ∗ i |xτ∗ i=1:m,Dτ) transductive . (5) We argue that there are two key issues with transductive meta-learners. The ﬁrst is that transductive learning is sen- sitive to the distribution over the target set used during meta-training, and as such is less generally applicable than non-transductive learning. For example, transductive learn- ers may fail to make good predictions if target sets contain a different class balance than what was observed during meta-training, or if they are required to make predictions for one example at a time. Transductive learners can also violate privacy constraints. In Table 1 and Appendix D, we provide empirical demonstrations of these failure cases. The second issue is that transductive learners have more information available than non-transductive learners at pre- diction time, which may lead to unfair comparisons. It is worth noting that some meta-learning algorithms are specif- ically designed to leverage transductive inference (e.g., Ren et al., 2018; Liu et al., 2019), though we do not discuss them in this work. In Section 5 we demonstrate that there are signiﬁcant performance differences for a model when trained transductively versus non-transductively.TASK NORM : Rethinking Batch Normalization for Meta-Learning 𝑨 𝑨∗ 𝑁 𝑀𝐵𝑁 𝑨′ 𝑁𝑀𝐵𝑁 𝑨′∗ 𝑨 𝑨′ 𝑨∗ 𝑨′∗ 𝝁𝑟,𝝈𝑟 𝑁𝑁 Conventional Batch Normalization (CBN) 𝑨 𝑨′ 𝑨∗ 𝑨′∗ 𝑁 𝑀𝐵𝑁 𝑁 MetaBN 𝑨 𝑨∗ 𝑁 𝑀𝐵𝑁 𝑨′ 𝑁𝑀𝐵𝑁 𝑨′∗ Transductive Batch Normalization (TBN) 𝑨 𝑨∗ 𝑁 𝑀𝐿𝑁 𝑨′ 𝑁𝑀𝐿𝑁 𝑨′∗ Layer Normalization  (LN) 𝑨 𝑨∗ 𝑁 𝑀𝐼𝑁 𝑨′ 𝑁𝑀𝐼𝑁 𝑨′∗ Instance Normalization (IN) TaskNorm-I, RN𝑨 𝑁 𝑀𝐼𝑁 𝑨′ 𝑀𝐵𝑁 1−𝛼 𝑨∗ 𝑁 𝑀𝐼𝑁 𝑨∗′ 𝛼1−𝛼 𝐵𝐶 Batch Normalization 𝐵𝑁 𝐵𝐶 Layer Normalization 𝐿𝑁 𝐵𝐶 Instance Normalization 𝐼𝑁 TaskNorm-L𝑨 𝑁 𝑀𝐿𝑁 𝑨′ 𝑀𝐵𝑁 1−𝛼 𝑨∗ 𝑁 𝑀𝐿𝑁 𝑨∗′ 𝛼1−𝛼 𝑁 Normalize with moments 𝑀 𝑀𝑋𝑋 Compute 𝑋𝑋 moments 𝐴,𝐴′ : In, Out Context Activations 𝐴∗,𝐴′∗: In, Out Target Activations Meta-train Meta-test Figure 2.A range of options for batch normalization for meta-learning. The cubes on the left depict the dimensions over which different moments are calculated for normalization of 2D convolutional layers. The computational diagrams on the right show how context and target activations are processed for various normalization methods. For all methods except conventional BN (CBN), the processing is identical at meta-train and meta-test time. Cube diagrams are derived from Wu & He (2018). 3. Normalization Layers for Meta-learning In this section, we discuss several normalization schemes that can and have been applied in the recent meta-learning literature, highlighting the modelling assumptions and ef- fects of different design choices. Throughout, we assume that the meta-learning algorithm is constructed such that the context-set inputs are passed through every neural-network module that the target set inputs are passed through at predic- tion time. This implies that moments are readily available from both the context and target set observations for any normalization layer, and is the case for many widely-used meta-learning models (e.g., Finn et al., 2017; Snell et al., 2017; Gordon et al., 2019). To illustrate our arguments, we provide experiments with MAML running simple, but widely used few-shot learning tasks from the Omniglot (Lake et al., 2011) and miniIma- genet (Ravi & Larochelle, 2017) datasets. The results of these experiments are provided in Table 1, and full experi- mental details in Appendix B. 3.1. Conventional Usage of Batch Normalization (CBN) We refer to conventional batch normalization (CBN) as that deﬁned by Ioffe & Szegedy (2015) and as outlined in Sec- tion 2.2. In the context of meta-learning, this involves nor- malizing tasks with computed moments at meta-train time, and using the accumulated running moments to normalize the tasks at meta-test time (see CBN in Figure 2). We highlight two important issues with the use of CBN for meta-learning. The ﬁrst is that, from the graphical model perspective, this is equivalent to lumping µand σwith the global parameters θ, i.e., they are learned from the meta- training set and shared across all tasks at meta-test time. We might expect CBN to perform poorly in meta-learning applications since the running moments are global across all tasks while the task data is only i.i.d. locally within a task, i.e., CBN does not satisfy desiderata 1. This is corroborated by our results (Table 1), where we demonstrate that using CBN with MAML results in very poor predictive performance - no better than chance. The second issue is that, as demonstrated by Wu & He (2018), using small batch sizes leads to inaccurate moments, resulting in signiﬁcant increases in model error. Importantly, the small batch setting may occur often in meta-learning, for example in the 1-shot scenario. Thus, CBN does not satisfy desiderata 2. Despite these issues, CBN is sometimes used, e.g., by Snell et al. (2017), though testing was performed only on Om- niglot and miniImagenet where the distribution of tasks is homogeneous (Triantaﬁllou et al., 2020). In Section 5, we show that Batch renormalization (BRN; Ioffe, 2017) can ex- hibit poor predictive performance in meta-learning scenarios (see Appendix A.1 for further details). 3.2. Transductive Batch Normalization (TBN) Another approach is to do away with the running moments used for normalization at meta-test time, and replace these with context / target set statistics. Here, context / target set statistics are used for normalization, both at meta-train and meta-test time. This is the approach taken by the authors of MAML (Finn et al., 2017),1 and, as demonstrated in our experiments, seems to be crucial to achieve the reported performance. From the graphical model perspective, this implies associating the normalization statistics with neither θnor ψ, but rather with a special set of parameters that is 1See for example (Finn, 2017) for a reference implementation.TASK NORM : Rethinking Batch Normalization for Meta-Learning local for each set (i.e., normalization statistics for Tτ are in- dependent of Dτ). We refer to this approach as transductive batch normalization (TBN; see Figure 2). Unsurprisingly, Nichol et al. (2018) found that using TBN provides a signiﬁcant performance boost in all cases they tested, which is corroborated by our results in Table 1. In other words, TBN achieves desiderata 2, and, as we demon- strate in Section 5, desiderata 1 as well. However, it is transductive. Due to the ubiquity of MAML, many compete- tive meta-learning methods (e.g. Gordon et al., 2019) have adopted TBN. However, in the case of TBN, transductivity is rarely stated as an explicit assumption, and may often confound the comparison among methods (Nichol et al., 2018). Importantly, we argue that to ensure comparisons in experimental papers are rigorous, meta-learning methods that are transductive must be labeled as such. 3.3. Instance-Based Normalization Schemes An additional class of non-transductive NLs are instance- based NLs. Here, both at meta-train and meta-test time, moments are computed separately for each instance, and do not depend on other observations. From a modelling perspective, this corresponds to treating µand σas local at the observation level. As instance-based NLs do not depend on the context set size, they perform equally well across context-set sizes (desiderata 2). However, as we demonstrate in Section 5, the improvements in predictive performance are modest compared to more suitable NLs and they are worse than CBN in terms of training efﬁciency (thus not meeting desiderata 1). Below, we discuss two examples, with a third discussed in Appendix A.2. Layer Normalization (LN; Ba et al., 2016) LN (see Fig- ure 2) has been shown to improve performance compared to CBN in recurrent neural networks, but does not offer the same gains for convolutional neural networks (Ba et al., 2016). The LN moments are computed as: µLNb = 1 HWC W∑ w=1 H∑ h=1 C∑ c=1 abwhc, (6) σ2 LNb = 1 HWC W∑ w=1 H∑ h=1 C∑ c=1 (abwhc −µLNb)2 (7) where µLN,σ2 LN ∈RB. While non-transductive, Table 1 demonstrates that LN falls far short of TBN in terms of accuracy. Further, in Section 5 we demonstrate that LN lacks in training efﬁciency when compared to other NLs. Instance Normalization (IN; Ulyanov et al., 2016) IN (see Figure 2) has been used in a wide variety of image generation applications. The IN moments are computed as: µINbc = 1 HW W∑ w=1 H∑ h=1 abwhc, (8) σ2 INbc = 1 HW W∑ w=1 H∑ h=1 (abwhc −µINbc)2 (9) where µIN,σ2 IN ∈RB×C. Table 1 demonstrates that IN has superior predictive performance to that of LN, but falls considerably short of TBN. In Section 5 we show that IN lacks in training efﬁciency when compared to other NLs. 4. Task Normalization In the previous section, we demonstrated that it is not im- mediately obvious how NLs should be designed for meta- learning applications. We now develop TASK NORM , the ﬁrst NL that is speciﬁcally tailored towards this scenario. TASK NORM is motivated by the view of meta-learning as hi- erarchical probabilistic modelling, discussed in Section 2.1. Given this hierarchical view of the model parameters, the question that arises is, how should we treat the normalization statistics µand σ? Figure 1 implies that the data associated with a task τ are i.i.d. only when conditioning on both θ and ψτ. Thus, the normalization statistics µand σshould be local at the task level, i.e., absorbed into ψτ. Further, the view that ψτ should be inferred conditioned on Dτ implies that the normalization statistics for the target set should be computed directly from the context set. Finally, our desire for a non-transductive scheme implies that any contribution from points in the target should not affect the normalization for other points in the target set, i.e., when computing µ and σfor a particular observation xτ∗∈Tτ, the NL should only have access to Dτ and xτ∗. 4.1. Meta-Batch Normalization (METABN) This perspective leads to our deﬁnition of METABN, which is a simple adaptation of CBN for the meta-learning setting. In METABN, the context set alone is used to compute the normalization statistics for both the context and target sets, both at meta-train and meta-test time (see Figure 2). To our knowledge, METABN has not been described in any publication, but concurrent to this work, it is used in the implementation of Meta-Dataset (Triantaﬁllou et al., 2019). METABN meets almost all of our desiderata, it (i) is non– transductive since the normalization of a test input does not depend on other test inputs in the target set, and (ii) as we demonstrate in Section 5, it improves training speed while maintaining accuracy levels of meta-learning models. How- ever, as we demonstrate in Section 5, METABN performs less well for small context sets. This is because moment estimates will have high-variance when there is little data,TASK NORM : Rethinking Batch Normalization for Meta-Learning and is similar to the difﬁculty of using BN with small-batch training (Wu & He, 2018). To address this issue, we intro- duce the following extension to METABN, which yields our proposed normalization scheme – TASK NORM . 4.2. TASK NORM The key intuition behind TASK NORM is to normalize a task with the context set moments in combination with a set of non-transductive, secondary moments computed from the input being normalized. A blending factor αbetween the two sets of moments is learned during meta-training. The motivation for TASK NORM is as follows: when the context set Dτ is small (e.g. 1-shot or few-shot learning) the context set alone will lead to noisy and inaccurate estimates of the “true” task statistics. In such cases, a secondary set of moments may improve the estimate of the moments, leading to better training efﬁciency and predictive performance in the low data regime. Further, this provides information regarding xτ∗ at prediction time while maintaining non- transductivity. The pooled moments for TASK NORM are computed as: µTN =αµBN + (1−α)µ+, (10) σ2 TN =α ( σ2 BN + (µBN −µTN)2) + (1−α) ( σ2 + + (µ+ −µTN)2) , (11) where µTN,σTN ∈RB×C, µ+, σ2 + are additional mo- ments from a non-transductive NL such as LN or IN com- puted using activations from the example being normalized (see Figure 2), and µBN and σBN are computed from Dτ. Equation (11) is the standard pooled variance when com- bining the variance of two Gaussian estimators. Importantly, we parameterize α= SIGMOID (SCALE |Dτ|+ OFFSET ), where the SIGMOID function ensures that 0 ≤ α ≤ 1, and the scalars SCALE and OFFSET are learned during meta-training. This enables us to learn how much each set should contribute to the estimate of task statistics as a function of the context-set size |Dτ|. Figure 3a depicts the value of αas a function of context set size |Dτ|for a representative set of trained TASK NORM layers. In general, when the context size is suitably large (Nτ >25), αis close to unity, i.e., normalization is carried out entirely with the context set in those layers. When the context size is smaller, there is a mix of the two sets of moments. Allowing each TASK NORM layer to separately adapt to the size of the context set (as opposed to learning a ﬁxed α per layer) is crucial in the meta-learning setting, where we expect the size of Dτ to vary, and are often particularly interested in the “few-shot” regime. Figure 3b plots the line SCALE |Dτ|+ OFFSET for same set of NLs as Figure 3a. The algorithm has learned that theSCALE parameter is non- zero and the OFFSET is almost zero in all cases indicating 100 101 102 Context Set Size 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00Alpha Layer 1 Layer 2 Layer 3 Layer 4 (a) 0 100 200 300 400 500 Context Set Size 0 10 20 30 40 50 60 70SCALE*(Context Size)+OFFSET Layer 1 Layer 2 Layer 3 Layer 4 (b) Figure 3.Plots of: (a) αversus context set size, and (b) αversus SCALE |Dτ|+OFFSET for the ﬁrst NL in each of the four layers in the feature extractor for the TASK NORM -I model. the importance of having αbeing a function of context size. In Appendix E, we provide an ablation study demonstrating the importance of our proposed parameterization ofα. If the context size is ﬁxed, we do not use the full parameterization, but learn a single value for alpha directly. The computational cost of TASK NORM is marginally greater than CBN’s. As a result, per-iteration time increases only slightly. However, as we show in Section 5, TASK NORM converges faster than CBN. In related work, Nam & Kim (2018) deﬁne Batch-Instance Normalization (BIN) that combines the results of CBN and IN with a learned blending factor in order to attenuate unnec- essary styles from images. However, BIN blends the output of the individual CBN and IN normalization operations as opposed to blending the moments. Finally, we note that Reptile (Nichol et al., 2018) uses a non-transductive form of task normalization that involves normalizing examples from the target set one example at a time with the moments of the context set augmented with the single example. We refer to this approach as reptile normalization or RN. It is easy to show that RN is a special case of TASK NORM augmented with IN when α= |Dτ|/(1 +|Dτ|).In Section 5, we show that reptile normalization falls short of TASK NORM , sup- porting the intuition that learning the value ofαis preferable to ﬁxing a value.TASK NORM : Rethinking Batch Normalization for Meta-Learning 5. Experiments In this section, we evaluate TASK NORM along with a range of competitive normalization approaches. 2 The goal of the experiments is to evaluate the following hypotheses: (i) Meta-learning algorithms are sensitive to the choice of NL; (ii) TBN will, in general, outperform non-transductive NLs; and (iii) NLs that consider the meta-learning data assumptions (TASK NORM , METABN, RN) will outperform ones that do not (CBN, BRN, IN, LN, etc.). 5.1. Small Scale Few-Shot Classiﬁcation Experiments We evaluate TASK NORM and a set of NLs using the ﬁrst order MAML and ProtoNets algorithms on the Omniglot and miniImageNet datasets under various way (the number of classes used in each task) and shot (the number of context set examples used per class) conﬁgurations. This setting is smaller scale, and considers only ﬁxed-sized context and target sets. Conﬁguration and training details can be found in Appendix B. Accuracy Table 1 and Table C.1 show accuracy results for various normalization methods on the Omniglot and miniImageNet datasets using the ﬁrst order MAML and the ProtoNets algorithms, respectively. We compute the average rank in an identical manner to Triantaﬁllou et al. (2020). For MAML, TBN is clearly the best method in terms of classiﬁcation accuracy. The best non-transductive approach is TASK NORM that uses IN augmentation ( TASK NORM - I). The two methods using instance-based normalization (LN, IN) do signiﬁcantly less well than methods designed with meta-learning desiderata in mind (i.e. TASK NORM , MetaBN, and RN). The methods using running averages at meta-test time (CBN, BRN) fare the worst. Figure 4a compares the performance of MAML on unseen tasks from miniImageNet when trained with TBN, IN, METABN, and TASK NORM , as a function of the number of shots per class in Dτ, and demonstrates that these trends are consistent across the low-shot range. Note that when meta-testing occurs one example at a time (e.g. in the streaming data scenario) or one class at a time (unbalanced class distribution scenario), accuracy for TBN drops dramatically compared to the case where all the exam- ples are tested at once. This is an important drawback of the transductive approach. All of the other NLs in the table are non-transductive and do not suffer a decrease in accuracy when tested an example at a time or a class at a time. Compared to MAML, the ProtoNets algorithm is much less sensitive to the NL used. Table C.1 indicates that with the exception of IN, all of the normalization methods yield 2Source code is available at https://github.com/ cambridge-mlg/cnaps good performance. We suspect that this is due to the fact that in ProtoNets employs a parameter-less nearest neigh- bor classiﬁer and no gradient steps are taken at meta-test time, reducing the importance of normalization. The top performer is LN which narrowly edges out TaskNorm-L and CBN. Interestingly, TBN is not on top and TASK NORM -I lags as IN is the least effective method. Training Speed Figure 4b plots validation accuracy ver- sus training iteration for the ﬁrst order MAML algorithm training on Omniglot 5-way-5-shot. TBN is the most ef- ﬁcient in terms of training convergence. The best non- transductive method is again TASK NORM -I, which is only marginally worse than TBN and just slightly better than TASK NORM -L. Importantly, TASK NORM -I is superior to ei- ther of MetaBN and IN alone in terms of training efﬁciency. Figure C.1a depicts the training curves for the ProtoNets algorithm. With the exception of IN which converges to a lower validation accuracy, all NLs converge at the the same speed. For the MAML algorithm, the experimental results support our hypotheses. Performance varies signiﬁcantly across NLs. TBN outperformed all methods in terms of classiﬁca- tion accuracy and training efﬁciency, andTASK NORM is the best non-transductive approach. Finally, The meta-learning speciﬁc methods outperformed the more general ones. The picture for ProtoNets is rather different. There is little vari- ability across NLs, TBN lagged the most consistent method LN in terms of accuracy, and the NLs that considered meta- learning needs were not necessarily superior to those that did not. 5.2. Large Scale Few-Shot Classiﬁcation Experiments Next, we evaluate NLs on a demanding few-shot classi- ﬁcation challenge called Meta-Dataset, composed of thir- teen (eight train, ﬁve test) image classiﬁcation datasets (Tri- antaﬁllou et al., 2020). Experiments are carried out with CNAP S, which achieves state-of-the-art performance on Meta-Dataset (Requeima et al., 2019a) and ProtoNets. The challenge constructs few-shot learning tasks by drawing from the following distribution. First, one of the datasets is sampled uniformly; second, the “way” and “shot” are sampled randomly according to a ﬁxed procedure; third, the classes and context / target instances are sampled. As a result, the context size Dτ will vary in the range between 5 and 500 for each task. In the meta-test phase, the iden- tity of the original dataset is not revealed and tasks must be treated independently (i.e. no information can be trans- ferred between them). The meta-training set comprises a disjoint and dissimilar set of classes from those used for meta-test. Details provided in Appendix B and Triantaﬁllou et al. (2020).TASK NORM : Rethinking Batch Normalization for Meta-Learning Table 1.Accuracy results for different few-shot settings on Omniglot and miniImageNet using the MAML algorithm. All ﬁgures are percentages and the ±sign indicates the 95% conﬁdence interval. Bold indicates the highest scores. The numbers after the conﬁguration name indicate the way and shots, respectively. The vertical lines enclose the transductive results. The TBN, examples, and class columns indicate accuracy when tested with all target examples at once, one example at a time, and one class at a time, respectively. All other NLs are non-transductive and yield the same result when tested by example or class. Conﬁguration TBN example class CBN BRN LN IN RN MetaBN TaskNorm-L TaskNorm-I Omniglot-5-1 98.4±0.7 21.6±1.3 21.6 ±1.3 20.1 ±0.0 20.0 ±0.0 83.0 ±1.3 87.4 ±1.2 92.6 ±0.9 91.8 ±0.9 94.0 ±0.8 94.4 ±0.8 Omniglot-5-5 99.2±0.2 22.0±0.5 23.2 ±0.5 20.0 ±0.0 20.0 ±0.0 91.0 ±0.8 93.9 ±0.5 98.2 ±0.2 98.1 ±0.3 98.0 ±0.3 98.6 ±0.2 Omniglot-20-1 90.9±0.5 3.7±0.2 3.7 ±0.2 5.0 ±0.0 5.0 ±0.0 78.1 ±0.7 80.4 ±0.7 89.0 ±0.6 89.6 ±0.5 89.6 ±0.5 90.0±0.5 Omniglot-20-5 96.6±0.2 5.5±0.2 14.5 ±0.3 5.0 ±0.0 5.0 ±0.0 92.3 ±0.2 92.9 ±0.2 96.8±0.2 96.4±0.2 96.4 ±0.2 96.3 ±0.2 miniImageNet-5-145.5±1.8 26.9±1.5 26.9 ±1.5 20.1 ±0.0 20.4 ±0.4 41.2 ±1.6 40.7 ±1.7 40.7 ±1.7 41.6 ±1.6 42.0 ±1.7 42.4 ±1.7 miniImageNet-5-559.7±0.9 30.3±0.7 27.2 ±0.6 20.2 ±0.2 20.7 ±0.5 52.8 ±0.9 54.3 ±0.9 57.6 ±0.9 58.6±0.9 58.1±0.9 58.7±0.9 Average Rank 1.25 - - 8.42 8.58 6.58 5.75 4.00 3.67 3.75 3.00 2 4 6 8 10 Shot 40 45 50 55 60 65Accuracy (%) TBN TaskNorm-I MetaBN IN (a) 0 10000 20000 30000 40000 50000 60000 Iteration 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0Validation Accuracy (%) TBN, CBN, RN TaskNorm-I TaskNorm-L MetaBN LN IN (b) 0 10000 20000 30000 40000 50000 60000 70000 Iteration 101 102 Training Loss TBN, CBN, RN TaskNorm-I TaskNorm-L MetaBN LN IN Baseline (c) Figure 4.(a) Accuracy vs shot for MAML on 5-way miniImagenet classiﬁcation. (b) Plot of validation accuracy versus training iteration using MAML for Omniglot 5-way, 5-shot corresponding to the results in Table 1. (c) Training Loss versus iteration corresponding to the results using the CNAPS algorithm in Table 2. Note that TBN, CBN, and RN all share the same meta-training step. Accuracy The classiﬁcation accuracy results for CNAP S and ProtoNets on Meta-Dataset are shown in Table 2 and Table 3, respectively. In the case of ProtoNets, all the the NLs speciﬁcally designed for meta-learning scenarios out- perform TBN in terms of classiﬁcation accuracy based on their average rank over all the datasets. For CNAP S, both RN and TASK NORM -I meet or exceed the rank of TBN. This may be as |Dτ|(i) is quite large in Meta-Dataset, and (ii) may be imbalanced w.r.t. classes, making prediction harder with transductive NLs. TASK NORM -I comes out as the clear winner ranking ﬁrst in 11 and 10 of the 13 datasets using CNAP S and ProtoNets, respectively. This supports the hypothesis that augmenting the BN moments with a second, instance based set of moments and learn- ing the blending factor αas a function of context set size is superior to ﬁxing α to a constant value (as is the case with RN). With both algorithms, the instance based NLs fall short of the meta-learning speciﬁc ones. However, in the case of CNAP S, they outperform the running average based methods (CBN, BRN), which perform poorly. In the case of ProtoNets, BRN outperforms the instance based meth- ods, and IN fairs the worst of all. In general, ProtoNets is less sensitive to the NL used when compared to CNAP S. The BASELINE column in Table 2 is taken from Requeima et al. (2019a), where the method reported state-of-the-art results on Meta-Dataset. The BASELINE algorithm uses the running moments learned during pre-training of its feature extractor for normalization. Using meta-learning speciﬁc NLs (in particular TASK NORM ) achieves signiﬁcantly im- proved accuracy compared to BASELINE . As an ablation, we have also added an additional variant of TASK NORM that blends the batch moments from the context set with the running moments accumulated dur- ing meta-training that we call TASK NORM -r. TASK NORM - r makes use of the global running moments to augment the local context statistic and it did not perform as well as the TASK NORM variants that employed local moments (i.e. TASK NORM -I and TASK NORM -L). Training Speed Figure 4c plots training loss versus train- ing iteration for the models in Table 2 that use the CNAPS algorithm. The fastest training convergence is achieved by TASK NORM -I. The instance based methods (IN, LN) are the slowest to converge. Note that TASK NORM converges within 60k iterations while BASELINE takes 110k iterationsTASK NORM : Rethinking Batch Normalization for Meta-Learning Table 2.Few-shot classiﬁcation results on META-DATASET using the CNAP S (top) and ProtoNets (bottom) algorithms. Meta-training performed on datasets above the dashed line. Datasets below the dashed line are entirely held out. All ﬁgures are percentages and the ± sign indicates the 95% conﬁdence interval over tasks. Bold indicates the highest scores. Vertical lines in the TBN column indicate that this method is transductive. Numbers in the BASELINE column are from (Requeima et al., 2019a). Dataset TBN Baseline CBN BRN LN IN RN MetaBN TaskNorm-r TaskNorm-L TaskNorm-I ILSVRC 50.2±1.0 51.3 ±1.0 24.8±0.7 19.2 ±0.7 45.5 ±1.1 46.7 ±1.0 49.7 ±1.1 51.3±1.1 49.3±1.0 51.2±1.1 50.6 ±1.1 Omniglot 91.4±0.5 88.0±0.7 47.9 ±1.4 60.0 ±1.6 87.4 ±0.8 79.7 ±1.0 91.0±0.6 90.9 ±0.6 87.8±0.7 90.6 ±0.6 90.7±0.6 Aircraft 81.6 ±0.6 76.8 ±0.8 29.5 ±0.9 56.3 ±0.8 76.5 ±0.8 74.7 ±0.7 82.4 ±0.6 83.9±0.6 81.1±0.7 81.9 ±0.6 83.8±0.6 Birds 74.5±0.8 71.4±0.9 42.1 ±1.0 32.6 ±0.8 67.3 ±0.9 64.9 ±1.0 72.4 ±0.8 73.2 ±0.9 72.8 ±0.9 72.4 ±0.8 74.6±0.8 Textures 59.7 ±0.7 62.5±0.7 37.5±0.7 50.5 ±0.6 60.1 ±0.6 59.7 ±0.7 58.6 ±0.7 58.9 ±0.8 63.2±0.8 57.2±0.7 62.1 ±0.7 Quick Draw 70.8 ±0.8 71.9 ±0.8 44.5 ±1.0 56.7 ±1.0 71.6 ±0.8 68.2 ±0.9 74.3±0.8 74.1 ±0.7 71.6±0.8 74.3±0.8 74.8 ±0.7 Fungi 46.0 ±1.0 46.0 ±1.1 21.1 ±0.8 26.1 ±0.9 39.6 ±1.0 37.8 ±1.0 49.0±1.0 47.9 ±1.0 42.0±1.1 47.1 ±1.1 48.7±1.0 VGG Flower 86.6±0.5 89.2±0.5 79.0±0.7 75.7 ±0.7 84.4 ±0.6 82.6 ±0.6 86.9 ±0.6 85.9 ±0.6 87.7 ±0.6 87.3 ±0.5 89.6±0.6 Trafﬁc Signs 66.6±0.9 60.1±0.9 38.3 ±0.9 38.8 ±1.2 57.3 ±0.8 62.5 ±0.8 66.6±0.8 58.9±0.9 62.7 ±0.8 62.0 ±0.8 67.0±0.7 MSCOCO 41.3 ±1.0 42.0±1.0 14.2±0.7 19.1 ±0.8 32.9 ±1.0 40.8 ±1.0 42.1±1.0 41.6±1.1 40.1 ±1.0 41.6 ±1.0 43.4±1.0 MNIST 92.1 ±0.4 88.6 ±0.5 65.9 ±0.8 82.5 ±0.6 86.8 ±0.5 89.8 ±0.5 91.3 ±0.4 92.1 ±0.4 93.2±0.3 90.5±0.4 92.3±0.4 CIFAR10 70.1±0.8 60.0±0.8 26.1 ±0.7 29.1 ±0.6 55.8 ±0.8 65.9 ±0.8 69.7±0.7 69.6 ±0.8 66.9±0.8 70.3±0.8 69.3 ±0.8 CIFAR100 55.6 ±1.0 48.1 ±1.0 16.7 ±0.8 16.7 ±0.7 37.9 ±1.0 52.9 ±1.0 55.0 ±1.0 54.2 ±1.1 53.0 ±1.1 59.5±1.0 54.6±1.1 Average Rank 3.92 5.58 10.69 10.31 7.96 7.54 3.77 4.04 5.38 4.42 2.38 Table 3.Few-shot classiﬁcation results on META-DATASET using the Prototypical Networks algorithm. Datasets below the dashed line are entirely held out. Meta-training performed on datasets above the dashed line. All ﬁgures are percentages and the ±sign indicates the 95% conﬁdence interval over tasks. Bold indicates the highest scores. Vertical lines in the TBN column indicate that this method is transductive. Dataset TBN CBN BRN LN IN RN MetaBN TaskNorm-r TaskNorm-L TaskNorm-I ILSVRC 44.7±1.0 43.6±1.0 43.0 ±1.0 33.9 ±0.9 32.5 ±0.9 45.1±1.0 44.2 ±1.0 42.7±1.0 45.1±1.1 44.9 ±1.0 Omniglot 90.7±0.6 77.5±1.1 89.1 ±0.7 90.8±0.6 83.4±0.8 90.8±0.6 90.4 ±0.6 88.6±0.7 90.2±0.6 90.6 ±0.6 Aircraft 83.3 ±0.6 77.0 ±0.7 84.4±0.5 73.9±0.7 75.0 ±0.6 80.9 ±0.6 82.3 ±0.6 79.6 ±0.6 81.2 ±0.6 84.7±0.5 Birds 69.6 ±0.9 67.5 ±0.9 69.0 ±0.9 54.1 ±1.0 50.2 ±1.0 68.6 ±0.9 68.6 ±0.8 64.2 ±0.9 68.8 ±0.9 71.0±0.9 Textures 61.2 ±0.7 57.7 ±0.7 58.0 ±0.7 55.8 ±0.7 45.3 ±0.7 64.1 ±0.7 60.5 ±0.7 60.8 ±0.7 63.4 ±0.8 65.9±0.7 Quick Draw 75.0 ±0.8 62.1 ±1.0 74.3 ±0.8 72.5 ±0.8 70.8 ±0.8 75.4 ±0.7 74.2 ±0.7 73.2 ±0.8 75.4 ±0.7 77.5±0.7 Fungi 46.4 ±1.0 43.6 ±1.0 46.5 ±1.0 33.2 ±1.1 29.8 ±1.0 46.7 ±1.0 46.5 ±1.0 42.3 ±1.1 46.5 ±1.0 49.6±1.1 VGG Flower 83.1 ±0.6 82.3 ±0.6 84.5 ±0.6 78.3 ±0.8 69.4 ±0.8 84.4 ±0.7 86.0±0.6 81.1±0.7 82.9 ±0.7 83.2 ±0.6 Trafﬁc Signs 64.0 ±0.8 59.5 ±0.8 65.7 ±0.8 69.1±0.7 60.7±0.8 66.0 ±0.8 63.2 ±0.8 64.9 ±0.8 67.0 ±0.7 65.8 ±0.7 MSCOCO 38.2 ±1.0 36.6 ±1.0 38.4±1.0 30.1±0.9 27.7 ±0.9 37.3 ±1.0 38.6±1.1 35.4±1.0 39.2±1.0 38.5 ±1.0 MNIST 93.4 ±0.4 86.5 ±0.6 91.9 ±0.4 94.0±0.4 87.4±0.5 93.9±0.4 93.9 ±0.4 92.5±0.4 91.9 ±0.4 93.3 ±0.4 CIFAR10 64.7 ±0.8 57.3 ±0.8 60.1 ±0.8 51.5 ±0.8 50.5 ±0.8 62.3 ±0.8 63.0 ±0.8 61.4 ±0.8 66.9±0.8 67.6 ±0.8 CIFAR100 48.0 ±1.1 43.1 ±1.0 43.9 ±1.0 34.0 ±0.9 32.1 ±1.0 47.2 ±1.1 47.0 ±1.0 45.2 ±1.0 51.3±1.1 50.0 ±1.0 Average Rank 4.04 8.19 5.31 7.46 9.58 3.65 3.96 6.73 3.58 2.50 and IN takes 200k. Figure C.1b shows the training curves for the ProtoNets algorithm. The convergence speed trends are very similar to CNAP S, with TASK NORM -I the fastest. Our results demonstrate that TASK NORM is the best ap- proach for normalizing tasks on the large scale Meta-Dataset benchmark in terms of classiﬁcation accuracy and training efﬁciency. Here, we see high sensitivity of performance across NLs. Interestingly, in this setting TASK NORM -I out- performed TBN in classiﬁcation accuracy, as did both RN and METABN. This refutes the hypothesis that TBN will always outperform other methods due to its transductive property, and implies that designing NL methods speciﬁ- cally for meta-learning has signiﬁcant value. In general, the meta-learning speciﬁc methods outperformed more general NLs, supporting our third hypothesis. We suspect the reason that TASK NORM outperforms other methods is due to its ability to adaptively leverage information from bothDτ and xτ∗when computing moments, based on the size of Dτ. 6. Conclusions We have identiﬁed and speciﬁed several issues and chal- lenges with NLs for the meta-learning setting. We have introduced a novel variant of batch normalization – that we call TASK NORM – which is geared towards the meta- learning setting. Our experiments demonstrate that TAS- KNORM achieves performance gains in terms of both classi- ﬁcation accuracy and training speed, sometimes exceeding transductive batch normalization. We recommend that fu- ture work in the few-shot / meta-learning community adopt TASK NORM , and if not, declare the form of normalization used and implications thereof, especially where transductive methods are applied.TASK NORM : Rethinking Batch Normalization for Meta-Learning Acknowledgments The authors would like to thank Elre Oldewage, Will Teb- butt, and the reviewers for their insightful comments and feedback. Richard E. Turner is supported by Google, Ama- zon, ARM, Improbable and EPSRC grants EP/M0269571 and EP/L000776/1. References Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia, Y ., Jozefowicz, R., Kaiser, L., Kudlur, M., Lev- enberg, J., Mané, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V ., Vasudevan, V ., Viégas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y ., and Zheng, X. TensorFlow: Large- scale machine learning on heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software available from tensorﬂow.org. Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Bakker, B. and Heskes, T. Task clustering and gating for Bayesian multitask learning. Journal of Machine Learn- ing Research, 4:83–99, May 2003. Chen, Y . A re-implementation of \"prototypical networks for few-shot learning\". https://github.com/ cyvius96/prototypical-network-pytorch, 2018. Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta-learning for fast adaptation of deep networks. In Precup, D. and Teh, Y . W. (eds.), Proceed- ings of the 34th International Conference on Ma- chine Learning , volume 70 of Proceedings of Ma- chine Learning Research, pp. 1126–1135, International Convention Centre, Sydney, Australia, 06–11 Aug 2017. PMLR. URL http://proceedings.mlr. press/v70/finn17a.html. Finn, C. B. Code for \"Model-agnostic meta-learning for fast adaptation of deep networks\". https://github. com/cbfinn/maml, 2017. Gordon, J., Bronskill, J., Bauer, M., Nowozin, S., and Turner, R. Meta-learning probabilistic inference for prediction. In International Conference on Learning Representations, 2019. URL https://openreview. net/forum?id=HkxStoC5F7. Grant, E., Finn, C., Levine, S., Darrell, T., and Grifﬁths, T. Recasting gradient-based meta-learning as hierarchical bayes. In International Conference on Learning Represen- tations, 2018. URL https://openreview.net/ forum?id=BJ_UL-k0b. Ha, D., Dai, A., and Le, Q. V . Hypernetworks. In In- ternational Conference on Learning Representations , 2016. URL https://openreview.net/forum? id=rkpACe1lx. Heskes, T. Empirical bayes for learning to learn. In Pro- ceedings of the Seventeenth International Conference on Machine Learning, ICML ’00, pp. 367–374, San Fran- cisco, CA, USA, 2000. Morgan Kaufmann Publishers Inc. ISBN 1-55860-707-2. URL http://dl.acm.org/ citation.cfm?id=645529.658133. Hospedales, T., Antoniou, A., Micaelli, P., and Storkey, A. Meta-learning in neural networks: A survey. arXiv preprint arXiv:2004.05439, 2020. Ioffe, S. Batch renormalization: Towards reducing mini- batch dependence in batch-normalized models. In Guyon, I., Luxburg, U. V ., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 30 , pp. 1945– 1953. Curran Associates, Inc., 2017. Ioffe, S. and Szegedy, C. Batch normalization: Accel- erating deep network training by reducing internal co- variate shift. In Bach, F. and Blei, D. (eds.), Proceed- ings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learn- ing Research, pp. 448–456, Lille, France, 07–09 Jul 2015. PMLR. URL http://proceedings.mlr. press/v37/ioffe15.html. Jerfel, G., Grant, E., Grifﬁths, T., and Heller, K. A. Rec- onciling meta-learning and continual learning with on- line mixtures of tasks. In Wallach, H., Larochelle, H., Beygelzimer, A., dÁlché-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Sys- tems 32, pp. 9119–9130. Curran Associates, Inc., 2019. Krizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009. Lake, B., Salakhutdinov, R., Gross, J., and Tenenbaum, J. One shot learning of simple visual concepts. In Proceed- ings of the annual meeting of the cognitive science society, volume 33, 2011. LeCun, Y ., Cortes, C., and Burges, C. MNIST hand- written digit database. AT&T Labs [Online]. Available: http://yann. lecun. com/exdb/mnist, 2:18, 2010.TASK NORM : Rethinking Batch Normalization for Meta-Learning Liu, Y ., Lee, J., Park, M., Kim, S., Yang, E., Hwang, S., and Yang, Y . Learning to propagate labels: trans- ductive propagation network for few-shot learning. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum? id=SyVuRiC5K7. Luo, C., Zhan, J., Xue, X., Wang, L., Ren, R., and Yang, Q. Cosine normalization: Using cosine similarity in- stead of dot product in neural networks. In International Conference on Artiﬁcial Neural Networks, pp. 382–391. Springer, 2018. Mishra, N., Rohaninejad, M., Chen, X., and Abbeel, P. A simple neural attentive meta-learner. In In- ternational Conference on Learning Representations , 2018. URL https://openreview.net/forum? id=B1DmUzWAW. Nagabandi, A., Finn, C., and Levine, S. Deep online learn- ing via meta-learning: Continual adaptation for model- based RL. In International Conference on Learning Representations, 2019. URL https://openreview. net/forum?id=HyxAfnA5tm. Nam, H. and Kim, H.-E. Batch-instance normalization for adaptively style-invariant neural networks. InAdvances in Neural Information Processing Systems, pp. 2558–2567, 2018. Nichol, A., Achiam, J., and Schulman, J. On ﬁrst-order meta-learning algorithms. arXiv preprint arXiv:1803.02999, 2018. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Rai- son, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. Pytorch: An imperative style, high-performance deep learning library. In Wal- lach, H., Larochelle, H., Beygelzimer, A., d’ Alché-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural In- formation Processing Systems 32, pp. 8024–8035. Curran Associates, Inc., 2019. Rajeswaran, A., Finn, C., Kakade, S. M., and Levine, S. Meta-learning with implicit gradients. In Wallach, H., Larochelle, H., Beygelzimer, A., d’ Alché-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Informa- tion Processing Systems 32, pp. 113–124. Curran Asso- ciates, Inc., 2019. Ravi, S. and Larochelle, H. Optimization as a model for few- shot learning. In Proceedings of the International Confer- ence on Learning Representations, 2017. URL https: //openreview.net/pdf?id=rJY0-Kcll. Ren, M., Ravi, S., Triantaﬁllou, E., Snell, J., Swersky, K., Tenenbaum, J. B., Larochelle, H., and Zemel, R. S. Meta- learning for semi-supervised few-shot classiﬁcation. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum? id=HJcSzz-CZ. Requeima, J., Gordon, J., Bronskill, J., Nowozin, S., and Turner, R. E. Fast and ﬂexible multi-task classiﬁcation us- ing conditional neural adaptive processes. In Wallach, H., Larochelle, H., Beygelzimer, A., dÁlché-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 32 , pp. 7957–7968. Curran Asso- ciates, Inc., 2019a. Requeima, J., Gordon, J., Bronskill, J., Nowozin, S., and Turner, R. E. Code for \"Fast and ﬂexible multi-task clas- siﬁcation using conditional neural adaptive processes\". https://github.com/cambridge-mlg/cnaps, 2019b. Salimans, T. and Kingma, D. P. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Advances in neural information pro- cessing systems, pp. 901–909, 2016. Schmidhuber, J. Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta- ... hook. PhD thesis, Technische Universität München, 1987. Singh, S. and Krishnan, S. Filter response normalization layer: Eliminating batch dependence in the training of deep neural networks. arXiv preprint arXiv:1911.09737, 2019. Snell, J. Code for the nips 2017 paper \"prototypical networks for few-shot learning\". https://github. com/jakesnell/prototypical-networks, 2017. Snell, J., Swersky, K., and Zemel, R. Prototypical networks for few-shot learning. In Guyon, I., Luxburg, U. V ., Ben- gio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Pro- cessing Systems 30, pp. 4077–4087. Curran Associates, Inc., 2017. Thrun, S. and Pratt, L. Learning to learn. Springer Science & Business Media, 2012. Triantaﬁllou, E., Zhu, T., Dumoulin, V ., Lamblin, P., Xu, K., Goroshin, R., Gelada, C., Swersky, K., Man- zagol, P.-A., and Larochelle, H. Code for \"Meta- dataset: A dataset of datasets for learning to learn from few examples\". https://github.com/ google-research/meta-dataset, 2019.TASK NORM : Rethinking Batch Normalization for Meta-Learning Triantaﬁllou, E., Zhu, T., Dumoulin, V ., Lamblin, P., Evci, U., Xu, K., Goroshin, R., Gelada, C., Swersky, K., Man- zagol, P.-A., and Larochelle, H. Meta-dataset: A dataset of datasets for learning to learn from few examples. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum? id=rkgAGAVKPr. Ulyanov, D., Vedaldi, A., and Lempitsky, V . Instance nor- malization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016. Vinyals, O., Blundell, C., Lillicrap, T., kavukcuoglu, k., and Wierstra, D. Matching networks for one shot learning. In Lee, D. D., Sugiyama, M., Luxburg, U. V ., Guyon, I., and Garnett, R. (eds.), Advances in Neural Information Pro- cessing Systems 29, pp. 3630–3638. Curran Associates, Inc., 2016. Wu, Y . and He, K. Group normalization. InProceedings of the European Conference on Computer Vision (ECCV), pp. 3–19, September 2018.TASK NORM : Rethinking Batch Normalization for Meta-Learning A. Additional Normalization Layers Here we discuss various additional NLs that are relevant to meta-learning. A.1. Batch Renormalization (BRN) Batch renormalization (BRN; Ioffe, 2017) is intended to mitigate the issue of non-identically distributed and/or small batches while retaining the training efﬁciency and stability of CBN. In BRN, the CBN algorithm is augmented with an afﬁne transform with batch-derived parameters which cor- rect for the batch statistics being different from the overall population. The normalized activations of a BRN layer are computed as follows: a′ n = γ ( r (an −µBN σBN + ϵ ) + d ) + β, where r=stop_grad ( clip[1/rmax,rmax] (σBN σr )) , d=stop_grad ( clip[−dmax,dmax] (µBN −µr σr )) . Here stop_grad(·) denotes a gradient blocking operation, and clip[a,b] denotes an operation returning a value in the range [a,b]. Like CBN, BRN is not well suited to the meta-learning scenario as it does not map directly to the hierarchical form of meta-learning models. In Section 5, we show that using BRN can improve predictive performance compared to CBN, but still performs signiﬁcantly worse than competitive approaches. Table 1 shows that batch renormalization performs poorly when using MAML. A.2. Group Normalization (GN) A key insight of Wu & He (2018) is that CBN performance suffers with small batch sizes. The goal of Group Normal- ization (GN; Wu & He, 2018) is thus to address the problem of normalization of small batch sizes, which, among other matters, is crucial for training large models in a data-parallel fashion. This is achieved by dividing the image channels into a number of groups Gand subsequently computing the moments for each group. GN is equivalent to LN when there is only a single group (G= 1) and equivalent to IN when the number of groups is equal to the number of channels in the layer (G= C). A.3. Other NLs There exist additional NLs including Weight Normaliza- tion (Salimans & Kingma, 2016), Cosine Normalization (Luo et al., 2018), Filter Response Normalization (Singh & Krishnan, 2019), among many others. Weight normalization reparameterizes weight vectors in a neural network to improve the conditioning for optimization. Weight normalization is non-transductive, but we don’t con- sider this approach further in this work as we focus on NLs that modify activations as opposed to weights. Filter Response Normalization (FRN) is another non- transductive NL that performs well for all batch sizes. How- ever we did not include it in our evaluation as FRN also encompasses the activation function as an essential part of normalization making it difﬁcult to be a drop in replacement for CBN in pre-trained networks as is the case for some of our experiments. Cosine normalization replaces the dot-product calculation in neural networks with cosine similarity for improved per- formance. We did not consider this method further in our work as it is not a simple drop-in replacement for CBN in pre-existing networks such as the ResNet-18 we use in our experiments. B. Experimental Details In this section, we provide the experimental details re- quired to reproduce our experiments. The experiments using MAML (Finn et al., 2017) were implemented in TensorFlow (Abadi et al., 2015), the Prototypical Networks experiments were implemented in Pytorch (Paszke et al., 2019), and the experiments using CNAP S (Requeima et al., 2019a) were implemented using a combination of TensorFlow (Abadi et al., 2015) and Pytorch. All experiments were executed on NVIDIA Tesla P100-16GB GPUs. B.1. MAML Experiments We evaluate MAML using a range of normalization layers on: 1. Omniglot (Lake et al., 2011): a few-shot learning dataset consisting of 1623 handwritten characters (each with 20 instances) derived from 50 alphabets. 2. miniImageNet (Vinyals et al., 2016): a dataset of 60,000 color images that is sub-divided into 100 classes, each with 600 instances. For all the MAML experiments, we used the codebase pro- vided by the MAML authors (Finn, 2017) with only small modiﬁcations to enable additional normalization techniques. Note that we used the ﬁrst-order approximation version of MAML for all experiments. MAML was invoked with the command lines as speciﬁed in the main.py ﬁle in the MAML codebase. No hyper-parameter tuning was per- formed and we took the results from a single run. All modelsTASK NORM : Rethinking Batch Normalization for Meta-Learning were trained for 60,000 iterations and then tested. No early stopping was used. We did not select the model based on val- idation accuracy or other criteria. The MAML code employs ten gradient steps at test time and computes classiﬁcation accuracy after each step. We report the maximum accuracy across those ten steps. To generate the plot in Figure 4a, we use the same command line as Omniglot-5-1, but vary the update batch size from one to ten. B.2. CNAP S Experiments We evaluate CNAP S using a range of normalization lay- ers on a demanding few-shot classiﬁcation challenge called Meta-Dataset (Triantaﬁllou et al., 2020). Meta-Dataset is composed of ten (eight train, two test) image classiﬁcation datasets. We augment Meta-Dataset with three additional held-out datasets: MNIST (LeCun et al., 2010), CIFAR10 (Krizhevsky & Hinton, 2009), and CIFAR100 (Krizhevsky & Hinton, 2009). The challenge constructs few-shot learn- ing tasks by drawing from the following distribution. First, one of the datasets is sampled uniformly; second, the “way” and “shot” are sampled randomly according to a ﬁxed pro- cedure; third, the classes and context / target instances are sampled. Where a hierarchical structure exists in the data (ILSVRC or OMNIGLOT ), task-sampling respects the hier- archy. In the meta-test phase, the identity of the original dataset is not revealed and the tasks must be treated inde- pendently (i.e. no information can be transferred between them). Notably, the meta-training set comprises a disjoint and dissimilar set of classes from those used for meta-test. Full details are available in Triantaﬁllou et al. (2020). For all the CNAP S experiments, we use the code provided by the the CNAP S authors (Requeima et al., 2019b) with only small modiﬁcations to enable additional normalization techniques. We follow an identical dataset conﬁguration and training process as prescribed in Requeima et al. (2019b). To generate results in Table 2, we used the following CNAP S options: FiLM feature adaptation, a learning rate of 0.001, and TBN, CBN, BRN, and RN used 70,000 training itera- tions, IN used 200,000 iterations, LN used 110,000 itera- tions, and TASK NORM used 60,000 iterations. The CNAP S code generates two models: fully trained and best validation. We report the better of the two. We performed no hyper- parameter tuning and report the test results from the ﬁrst run. Note that CBN, TBN, and RN share the same trained model. They differ only in how meta-testing is done. B.3. Prototypical Networks Experiments We evaluate the Prototypical Networks (Snell et al., 2017) algorithm with a range of NLs using the same Omniglot, miniImageNet, and Meta-Dataset benchmarks. For Omniglot, we used the codebase created by the Pro- totypical Networks authors (Snell, 2017). For miniIma- geNet, we used the a different codebase ((Chen, 2018)) as the ﬁrst codebase did not support miniImageNet. Only small modiﬁcations were made to the two codebases to enable additional NLs. For Omniglot and miniImageNet, we set hyper-parameters as prescribed in (Snell et al., 2017). Early stopping was employed and the model that produced the best validation was used for testing. For Meta-Dataset, we use the code provided by the the CNAP S authors (Requeima et al., 2019b) with only small modiﬁcations to enable additional normalization techniques and a new classiﬁer adaptation layer to generate the linear classiﬁer weights per equation (8) in (Snell et al., 2017). We follow an identical dataset conﬁguration and training process as prescribed in Requeima et al. (2019b). To gen- erate results in Table 3, we used the following CNAP S options: no feature adaptation, a learning rate of 0.001, 60,000 training iterations for all NLs, and the pretrained feature extractor weights were not frozen and allowed to update during meta-training. C. Additional Classiﬁcation Results Table C.1 shows the classiﬁcation accuracy results for the ProtoNets algorithm on the Omniglot and miniImageNet datasets. Figure C.1a and Figure C.1b show the training curves for the ProtoNets algorithm on Omniglot and Meta- Dataset, respectively. D. Additional Transduction Tests A non-transductive meta-learning system makes predictions for a single test set label conditioned only on a single input and the context set. A transductive meta-learning system also conditions on additional samples from the test set. Table D.2 demonstrates failure modes for transductive learn- ing. In addition to reporting the classiﬁcation accuracy re- sults when the target set is evaluated all at once (ﬁrst column of results for each NL), we report the classiﬁcation accuracy when meta-testing is performed one target-set example at a time (second column of results for each NL), and one target-set class at a time (third column of results for each NL). Table D.2 demonstrates that classiﬁcation accuracy drops dramatically for TBN when testing is performed one example or one class at a time. Importantly, in the case of TASK NORM -I (or any non- transductive NL – i.e. all of NLs evaluated in this work apart from TBN), the evaluation results are identical whether they are meta-tested on the entire target set at once, one example at a time, or one class at a time. This shows that transductive learning is sensitive to the distribution over the target set used during meta-training, demonstrating that transductive learning is less generally applicable than non-transductiveTASK NORM : Rethinking Batch Normalization for Meta-Learning Table C.1.Accuracy results for different few-shot settings on Omniglot and miniImageNet using the Prototypical Networks algorithm. All ﬁgures are percentages and the ±sign indicates the 95% conﬁdence interval. Bold indicates the highest scores. The numbers after the conﬁguration name indicate the way and shots, respectively. The vertical lines in the TBN column indicate that this method is transductive. Conﬁguration TBN CBN BRN LN IN RN MetaBN TaskNorm-L TaskNorm-I Omniglot-5-1 98.4 ±0.2 98.5±0.2 98.5 ±0.2 98.7 ±0.2 93.7±0.4 98.0 ±0.2 98.4 ±0.2 98.6±0.2 98.4±0.2 Omniglot-5-5 99.6±0.1 99.6 ±0.1 99.6 ±0.1 99.7 ±0.1 98.8±0.1 99.6±0.1 99.6 ±0.1 99.6 ±0.1 99.6 ±0.1 Omniglot-20-1 94.5 ±0.2 94.5 ±0.2 94.6 ±0.2 94.9±0.2 83.5±0.3 94.1 ±0.2 94.5 ±0.2 95.0±0.2 93.4±0.2 Omniglot-20-5 98.6±0.1 98.6 ±0.1 98.6 ±0.1 98.7 ±0.1 96.3±0.1 98.6±0.1 98.6 ±0.1 98.7 ±0.1 98.6 ±0.1 miniImageNet-5-1 45.9 ±0.6 47.8±0.6 46.3±0.6 47.5±0.6 30.4±0.5 39.7 ±0.5 42.6 ±0.6 47.5±0.6 43.2±0.6 miniImageNet-5-5 65.5 ±0.5 66.7±0.5 64.7±0.5 66.3±0.5 48.8±0.5 63.1 ±0.5 64.6 ±0.5 65.3 ±0.5 63.9 ±0.5 Average Rank 4.58 3.25 4.33 2.75 9.00 6.67 5.25 3.08 6.08 0 10000 20000 30000 40000 50000 Iteration 0.6 0.7 0.8 0.9Validation Accuracy (%) TBN, CBN, RN TaskNorm-I TaskNorm-L MetaBN LN IN (a) 0 10000 20000 30000 40000 50000 60000 Iteration 101 Training Loss TBN, CBN, RN TaskNorm-I TaskNorm-L MetaBN LN IN (b) Figure C.1.(a) Plot of validation accuracy versus training iteration using ProtoNets for Omniglot 20-way, 1-shot corresponding to the results in Table C.1. (b) Training Loss versus iteration corresponding to the results using the ProtoNets algorithm on META-DATASET in Table 3. Note that TBN, CBN, and RN all share the same meta-training step. learning. In particular, transductive learners may fail to make good predictions if target sets contains a different class balance than what was observed during meta-training, or if they are required to make predictions for one example at a time (e.g. in streaming applications). E. Ablation Study: Choosing the best parameterization for α There are a number of possibilities for the parameterization of the TASK NORM blending parameter α. We consider four different conﬁgurations for each NL: 1. αis learned separately for each channel (i.e. channel speciﬁc) as an independent parameter. 2. αis learned shared across all channels as an indepen- dent parameter. 3. αis learned separately for each channel (i.e. channel speciﬁc) as a function of context set size (i.e. α = SIGMOID (SCALE |Dτ|+ OFFSET )). 4. αis learned shared across all channels as a function of context set size (i.e. α = SIGMOID (SCALE |Dτ|+ OFFSET )). Accuracy Table E.3 and Table E.4 show classiﬁcation accuracy for the various parameterizations for MAML and the CNAP S algorithms, respectively using the TASK NORM - I NL. When using the MAML algorithm, there are only two op- tions to evaluate as the context size is ﬁxed for each con- ﬁguration of dataset, shot, and way and thus we need only evaluate the independent options (1 and 2 above). Table E.3 indicates that the classiﬁcation accuracy for the channel spe- ciﬁc and shared parameterizations are nearly identical, but the shared parameterization is better in the Omniglot-5-1 benchmark and hence has the best ranking overall.TASK NORM : Rethinking Batch Normalization for Meta-Learning Table D.2.Few-shot classiﬁcation results for TBN and TASK NORM -I on META-DATASET using the CNAP S algorithm. For each NL, the ﬁrst column of results \"All\" reports accuracy when meta-testing is performed on the entire target set at once. The second column of results \"Example\" reports accuracy when meta-testing is performed one example at a time. The third column of results \"Class\" reports accuracy when meta-testing is performed one class at a time. All ﬁgures are percentages and the ±sign indicates the 95% conﬁdence interval over tasks. Meta-training is performed on datasets above the dashed line, while datasets below the dashed line are entirely held out. TBN T ASK NORM -I Dataset All Example Class All Example Class ILSVRC 50.2 ±1.0 9.5 ±0.3 11.8 ±0.4 50.4 ±1.1 50.4 ±1.1 50.4 ±1.1 Omniglot 91.4 ±0.5 7.5 ±0.4 9.6 ±0.4 91.3 ±0.6 91.3 ±0.6 91.3 ±0.6 Aircraft 81.6 ±0.6 11.8 ±0.4 14.4 ±0.4 83.8 ±0.6 83.8 ±0.6 83.8 ±0.6 Birds 74.5 ±0.8 7.6 ±0.4 8.4 ±0.4 74.4 ±0.9 74.4 ±0.9 74.4 ±0.9 Textures 59.7 ±0.7 17.0 ±0.2 18.1 ±0.4 61.1 ±0.7 61.1 ±0.7 61.1 ±0.7 Quick Draw 70.8 ±0.8 5.6 ±0.4 8.8 ±0.4 74.7 ±0.7 74.7 ±0.7 74.7 ±0.7 Fungi 46.0 ±1.0 5.0 ±0.3 6.5 ±0.4 50.6 ±1.1 50.6 ±1.1 50.6 ±1.1 VGG Flower 86.6 ±0.5 11.2 ±0.4 12.6 ±0.4 87.8 ±0.5 87.8 ±0.5 87.8 ±0.5 Trafﬁc Signs 66.6 ±0.9 6.0 ±0.3 8.1 ±0.4 64.8 ±0.8 64.8 ±0.8 64.8 ±0.8 MSCOCO 41.3 ±1.0 6.1 ±0.3 7.9 ±0.4 42.2 ±1.0 42.2 ±1.0 42.2 ±1.0 MNIST 92.1 ±0.4 14.4 ±0.3 19.3 ±0.4 91.3 ±0.4 91.3 ±0.4 91.3 ±0.4 CIFAR10 70.1 ±0.8 14.4 ±0.3 16.4 ±0.4 70.0 ±0.8 70.0 ±0.8 70.0 ±0.8 CIFAR100 55.6 ±1.0 5.6 ±0.3 7.7 ±0.4 54.6 ±1.0 54.6 ±1.0 54.6 ±1.0 Table E.3.Few-shot classiﬁcation results for two αparameteriza- tions on Omniglot and miniImageNet using the MAML algorithm. All ﬁgures are percentages and the ±sign indicates the 95% conﬁ- dence interval over tasks. Bold indicates the highest scores. Independent Conﬁguration Channel Speciﬁc Shared Omniglot-5-1 90.7 ±1.0 94.4±0.8 Omniglot-5-5 98.3 ±0.2 98.6±0.2 Omniglot-20-1 90.6±0.5 90.0 ±0.5 Omniglot-20-5 96.4±0.2 96.3 ±0.2 miniImageNet-5-1 42.6±1.8 42.4 ±1.7 miniImageNet-5-5 58.8±0.9 58.7 ±0.9 Average Rank 1.67 1.33 When using the CNAP S algorithm on the Meta-Dataset benchmark, the best parameterization option in terms of classiﬁcation accuracy is αshared across channels as a func- tion of context size. One justiﬁcation for having α be a function of context size can be seen in Figure 3b. Here we plot the line SCALE |Dτ|+ OFFSET on a linear scale for a representative set of NLs in the ResNet-18 used in the CNAP S algorithm. The algorithm has learned that the SCALE parameter is non-zero and the OFFSET is almost zero in all cases. If a constant αwould lead to better accu- racy, we would see the opposite (i.e the SCALE parameter would be at or near zero and the OFFSET parameter being some non-zero value). From Table E.4 we can also see that accuracy is better when the parameterization is a shared α opposed to having a channel-speciﬁc α. Training Speed Figure E.2a and Figure E.2b show the learning curves for the various parameterization options using the MAML and the CNAP S algorithms, respectively with a TASK NORM -I NL. For the MAML algorithm the training efﬁciency of the shared and channel speciﬁc parameterizations are almost identical. For the CNAP S algorithm, Figure E.2b indicates the training efﬁciency of the independent parameterization is considerably worse than the functional one. The two functional representations for the CNAPs algorithm have almost identical training curves. Based on Figure E.2a and Figure E.2b, we conclude that the training speed of the functional parameterization is superior to that of the independent parameterization and that there is little or no difference in the training speeds between the functional, shared parameterization and the functional, channel speciﬁc parameterization. In summary, the best parameterization for α when it is learned shared across channels as a function of context set size (option 4, above). We use this parameterization in all of the CNAP S experiments in the main paper. For the MAML experiments, the functional parameterization is meaningless given that all the test conﬁgurations have a ﬁxed context size. In that case, we used the independent, shared across channels parameterization for αfor the experiments in the main paper.TASK NORM : Rethinking Batch Normalization for Meta-Learning Table E.4.Few-shot classiﬁcation results for various αparameterizations on META-DATASET using the CNAP S algorithm. All ﬁgures are percentages and the ±sign indicates the 95% conﬁdence interval over tasks. Bold indicates the highest scores. Meta-training performed on datasets above the dashed line, while datasets below the dashed line are entirely held out. Independent Functional Dataset Channel Speciﬁc Shared Channel Speciﬁc Shared ILSVRC 45.3 ±1.0 49.6±1.1 49.8 ±1.1 50.6 ±1.1 Omniglot 90.8±0.6 90.9 ±0.6 90.1 ±0.6 90.7 ±0.6 Aircraft 82.3 ±0.7 84.6±0.6 84.4 ±0.6 83.8 ±0.6 Birds 70.1 ±0.9 73.2 ±0.9 73.1 ±0.9 74.6±0.8 Textures 54.8 ±0.7 58.5 ±0.7 61.0 ±0.8 62.1±0.7 Quick Draw 73.0 ±0.8 73.9±0.7 74.2 ±0.7 74.8 ±0.7 Fungi 43.8 ±1.0 47.6±1.0 48.0 ±1.0 48.7 ±1.0 VGG Flower 85.9 ±0.6 86.3 ±0.5 86.5 ±0.7 89.6±0.6 Trafﬁc Signs 62.6 ±0.8 62.6 ±0.8 60.1 ±0.8 67.0±0.7 MSCOCO 38.3 ±1.1 40.9 ±1.0 40.2 ±1.0 43.4±1.0 MNIST 92.6±0.4 91.7±0.4 91.1 ±0.4 92.3±0.4 CIFAR10 65.7 ±0.9 67.7 ±0.8 67.3 ±0.9 69.3±0.8 CIFAR100 48.1 ±1.2 52.1 ±1.1 53.3±1.0 54.6 ±1.1 Average Rank 3.5 2.5 2.5 1.5 0 10000 20000 30000 40000 50000 60000 Iteration 0.70 0.75 0.80 0.85 0.90 0.95 1.00Validation Accuracy (%)Independent, Channel Specific Independent, Shared (a) 0 10000 20000 30000 40000 50000 60000 70000 Iteration 101 4 × 100 6 × 100 2 × 101 Training Loss Independent, Channel Specific Independent, Shared Functional, Channel Specific Functional, Shared (b) Figure E.2.(a) Plots of validation accuracy versus training iteration corresponding to the parameterization experiments using the MAML algorithm in Table E.3. (b) Plot of training loss versus iteration corresponding to the parameterization experiments using the CNAPS algorithm in Table E.4.",
      "meta_data": {
        "arxiv_id": "2003.03284v2",
        "authors": [
          "John Bronskill",
          "Jonathan Gordon",
          "James Requeima",
          "Sebastian Nowozin",
          "Richard E. Turner"
        ],
        "published_date": "2020-03-06T15:43:27Z",
        "venue": "Proceedings of Machine Learning and Systems 2020, 4683-4694",
        "pdf_url": "https://arxiv.org/pdf/2003.03284v2.pdf"
      }
    },
    {
      "title": "Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization",
      "abstract": "Batch Normalization (BN) is one of the most widely used techniques in Deep\nLearning field. But its performance can awfully degrade with insufficient batch\nsize. This weakness limits the usage of BN on many computer vision tasks like\ndetection or segmentation, where batch size is usually small due to the\nconstraint of memory consumption. Therefore many modified normalization\ntechniques have been proposed, which either fail to restore the performance of\nBN completely, or have to introduce additional nonlinear operations in\ninference procedure and increase huge consumption. In this paper, we reveal\nthat there are two extra batch statistics involved in backward propagation of\nBN, on which has never been well discussed before. The extra batch statistics\nassociated with gradients also can severely affect the training of deep neural\nnetwork. Based on our analysis, we propose a novel normalization method, named\nMoving Average Batch Normalization (MABN). MABN can completely restore the\nperformance of vanilla BN in small batch cases, without introducing any\nadditional nonlinear operations in inference procedure. We prove the benefits\nof MABN by both theoretical analysis and experiments. Our experiments\ndemonstrate the effectiveness of MABN in multiple computer vision tasks\nincluding ImageNet and COCO. The code has been released in\nhttps://github.com/megvii-model/MABN.",
      "full_text": "Published as a conference paper at ICLR 2020 TOWARDS STABILIZING BATCH STATISTICS IN BACK - WARD PROPAGATION OF BATCH NORMALIZATION Junjie Yan1,2∗, Ruosi Wan 3∗, Xiangyu Zhang 3†, Wei Zhang 1,2, Yichen Wei 3, Jian Sun 3 1 Shanghai Key Laboratory of Intelligent Information Processing 2 School of Computer Science, Fudan University 3 Megvii Technology. {jjyan17, weizh}@fudan.edu.cn, {wanruosi, zhangxiangyu, weiyichen, sunjian}@megvii.com. ABSTRACT Batch Normalization (BN) is one of the most widely used techniques in Deep Learning ﬁeld. But its performance can awfully degrade with insufﬁcient batch size. This weakness limits the usage of BN on many computer vision tasks like de- tection or segmentation, where batch size is usually small due to the constraint of memory consumption. Therefore many modiﬁed normalization techniques have been proposed, which either fail to restore the performance of BN completely, or have to introduce additional nonlinear operations in inference procedure and in- crease huge consumption. In this paper, we reveal that there are two extra batch statistics involved in backward propagation of BN, on which has never been well discussed before. The extra batch statistics associated with gradients also can severely affect the training of deep neural network. Based on our analysis, we propose a novel normalization method, named Moving Average Batch Normal- ization (MABN). MABN can completely restore the performance of vanilla BN in small batch cases, without introducing any additional nonlinear operations in inference procedure. We prove the beneﬁts of MABN by both theoretical anal- ysis and experiments. Our experiments demonstrate the effectiveness of MABN in multiple computer vision tasks including ImageNet and COCO. The code has been released in https://github.com/megvii-model/MABN. 1 I NTRODUCTION Batch Normalization (BN) (Ioffe & Szegedy, 2015) is one of the most popular techniques for train- ing neural networks. It has been widely proven effective in many applications, and become the indispensable part of many state of the art deep models. Despite the success of BN, it’s still challenging to utilize BN when batch size is extremely small 1. The batch statistics with small batch size are highly unstable, leading to slow convergence during training and bad performance during inference. For example, in detection or segmentation tasks, the batch size is often limited to 1 or 2 per GPU due to the requirement of high resolution inputs or complex structure of the model. Directly computing batch statistics without any modiﬁcation on each GPU will make performance of the model severely degrade. To address such issues, many modiﬁed normalization methods have been proposed. They can be roughly divided into two categories: some of them try to improve vanilla BN by correcting batch statistics (Ioffe, 2017; Singh & Shrivastava, 2019), but they all fail to completely restore the perfor- mance of vanilla BN; Other methods get over the instability of BN by using instance-level normal- ization (Ulyanov et al., 2016; Ba et al., 2016; Wu & He, 2018), therefore models can avoid the affect ∗Equal Contribution. Work was done when Junjie Yan was an intern at Megvii Technology. †Corresponding author. 1In the context of this paper, we use ”batch size/normalization batch size” to refer the number of samples used to compute statistics unless otherwise stated. We use ”gradient batch size” to refer the number of samples used to update weights. 1 arXiv:2001.06838v2  [cs.CV]  8 Apr 2020Published as a conference paper at ICLR 2020 of batch statistics. This type of methods can restore the performance in small batch cases to some extent. However, instance-level normalization hardly meet industrial or commercial needs so far, for this type of methods have to compute instance-level statistics both in training and inference, which will introduce additional nonlinear operations in inference procedure and dramatically increase con- sumption Shao et al. (2019). While vanilla BN uses the statistics computed over the whole training data instead of batch of samples when training ﬁnished. Thus BN is a linear operator and can be merged with convolution layer during inference procedure. Figure 1(a) shows with ResNet-50 (He et al., 2016), instance-level normalization almost double the inference time compared with vanilla BN. Therefore, it’s a tough but necessary task to restore the performance of BN in small batch training without introducing any nonlinear operations in inference procedure. In this paper, we ﬁrst analysis the formulation of vanilla BN, revealing there are actually not only 2 but 4 batch statistics involved in normalization during forward propagation (FP) as well as backward propagation (BP). The additional 2 batch statistics involved in BP are associated with gradients of the model, and have never been well discussed before. They play an important role in regularizing gradients of the model during BP. In our experiments (see Figure 2), variance of the batch statistics associated with gradients in BP, due to small batch size, is even larger than that of the widely- known batch statistics (mean, variance of feature maps). We believe the instability of batch statistics associated with gradients is one of the key reason why BN performs poorly in small batch cases. Based on our analysis, we propose a novel normalization method named Moving Average Batch Normalization (MABN). MABN can completely get over small batch issues without introducing any nonlinear manipulation in inference procedure. The core idea of MABN is to replace batch statistics with moving average statistics. We substitute batch statistics involved in BP and FP with different type of moving average statistics respectively, and theoretical analysis is given to prove the beneﬁts. However, we observed directly using moving average statistics as substitutes for batch statistics can’t make training converge in practice. We think the failure takes place due to the occasional large gradients during training, which has been mentioned in Ioffe (2017). To avoid training collapse, we modiﬁed the vanilla normalization form by reducing the number of batch statistics, centralizing the weights of convolution kernels, and utilizing renormalizing strategy. We also theoretically prove the modiﬁed normalization form is more stable than vanilla form. MABN shows its effectiveness in multiple vision public datasets and tasks, including Ima- geNet (Russakovsky et al., 2015), COCO (Lin et al., 2014). All results of experiments show MABN with small batch size ( 1 or 2) can achieve comparable performance as BN with regular batch size (see Figure 1(b)). Besides, it has same inference consumption as vanilla BN (see Figure 1(a)). We also conducted sufﬁcient ablation experiments to verify the effectiveness of MABN further. (a)  (b) Figure 1: (a) Throughout (iterations per second) in inference procedure using different Normaliza- tion methods. The implementation details can be seen in appendix B.2. (b)ImageNet classiﬁcation validation error vs. batch sizes. 2 R ELATED WORK Batch normalization (BN) (Ioffe & Szegedy, 2015) normalizes the internal feature maps of deep neu- ral network using channel-wise statistics (mean, standard deviation) along batch dimension. It has 2Published as a conference paper at ICLR 2020 been widely proven effectively in most of tasks. But the vanilla BN heavily relies on sufﬁcient batch size in practice. To restore the performance of BN in small batch cases, many normalization tech- niques have been proposed: Batch Renormalization (BRN) (Ioffe, 2017) introduces renormalizing parameters in BN to correct the batch statistics during training, where the renormalizing parame- ters are computed using moving average statistics; Unlike BRN, EvalNorm (Singh & Shrivastava, 2019) corrects the batch statistics during inference procedure. Both BRN and EvalNorm can restore the performance of BN to some extent, but they all fail to get over small batch issues completely. Instance Normalization (IN) (Ulyanov et al., 2016), Layer Normalization (LN) (Ba et al., 2016), and Group normalization (GN) (Wu & He, 2018) all try to avoid the effect of batch size by utiliz- ing instance level statistics. IN uses channel-wise statistics per instance instead of per batch, while LN uses instance-level statistics along channel dimension. But IN and LN shows no superiority to vanilla BN in most of cases. GN divides all channels in predeﬁned groups, and uses group-wise statistics per instance. It can restore the performance of vanilla BN very well in classiﬁcation and detection tasks. But it have to introduce extra nonlinear manipulations in inference procedure and severely increase inference consumption, as we have pointed out in Section 1. SyncBN (Peng et al., 2018) handle the small batch issues by computing the mean and variance across multiple GPUs. This method doesn’t essentially solve the problem, and requires a lot of resource. Online Normal- ization Chiley et al. (2019) modiﬁes BP by using moving average statistics, so they can set batch size as 1 without degradation of performance, but Online Normalization still have to use instance-level normalization to cooperate with modiﬁcation in BP, so its inference efﬁciency is much lower than original BN. Apart from operating on feature maps, some works exploit to normalize the weights of convolution: Weight Standardization (Qiao et al., 2019) centralizes weight at ﬁrst before divides weights by its standard deviation. It still has to combine with GN to handle small batch cases. 3 S TATISTICS IN BATCH NORMALIZATION 3.1 R EVIEW OF BATCH NORMALIZATION First of all, let’s review the formulation of batch Normalization (Ioffe & Szegedy, 2015): assume the input of a BN layer is denoted asX ∈RB×p, where Bdenotes the batch size, pdenotes number of features. In training procedure, the normalized feature maps Y at iteration tis computed as: Y = X −µBt σBt , (1) where batch statistics µBt and σ2 Bt are the sample mean and sample variance computed over the batch of samples Bt at iteration t: µBt = 1 B ∑ b Xb,:, σ 2 Bt = 1 B ∑ b (Xb,: −µBt)2. (2) Besides, a pair of parameters γ, βare used to scale and shift normalized value Y : Z = Y γ+ β. (3) The scaling and shifting part is added in all normalization form by default, and will be omitted in the following discussion for simplicity. As Ioffe & Szegedy (2015) demonstrated, the batch statisticsµBt,σ2 Bt are both involved in backward propagation (BP). We can derive the formulation of BP in BN as follows: let Ldenote the loss, Θt denote the set of the whole learnable parameters of the model at iteration t. Given the partial gradients ∂L ∂Y ⏐⏐⏐⏐ Θt,Bt , the partial gradients ∂L ∂X ⏐⏐⏐⏐ Θt,Bt is computed as ∂L ∂X ⏐⏐⏐⏐ Θt,Bt = 1 σBt ( ∂L ∂Y ⏐⏐⏐⏐ Θt,Bt −gBt −Y ·ΨBt) (4) where ·denotes element-wise production, gBt and ΨBt are computed as gBt = 1 B ∑ b ∂L ∂Yb,: ⏐⏐⏐⏐ Θt,Bt , ΨBt = 1 B ∑ b Yb,: · ∂L ∂Yb,: ⏐⏐⏐⏐ Θt,Bt , (5) 3Published as a conference paper at ICLR 2020 It can be seen from (5) that gBt and ΨBt are also batch statistics involved in BN during BP. But they have never been well discussed before. 3.2 I NSTABILITY OF BATCH STATISTICS According to Ioffe & Szegedy (2015), the ideal normalization is to normalize feature mapsX using expectation and variance computed over the whole training data set: Y = X −EX√ Var[X] . (6) But it’s impractical when using stochastic optimization. Therefore, Ioffe & Szegedy (2015) uses mini-batches in stochastic gradient training, each mini-batch produces estimates the mean and vari- ance of each activation. Such simpliﬁcation makes it possible to involve mean and variance in BP. From the derivation in section 3.1, we can see batch statistics µBt, σ2 Bt are the Monte Carlo (MC) estimators of population statistics E[X|Θt], Var[X|Θt] respectively at iteration t. Similarly, batch statistics gBt, ΨBt are MC estimators of population statistics E[ ∂L ∂Yb,: |Θt], E[Yb,: · ∂L ∂Yb,: |Θt] at it- eration t. E[ ∂L ∂Yb,: |Θt], E[Yb,: · ∂L ∂Yb,: |Θt] are computed over the whole data set. They contain the information how the mean and the variance of population will change as model updates, so they play an important role to make trade off between the change of individual sample and population. There- fore, it’s crucial to estimate the population statistics precisely, in order to regularize the gradients of the model properly as weights update. It’s well known the variance of MC estimator is inversely proportional to the number of samples, hence the variance of batch statistics dramatically increases when batch size is small. Figure 2 shows the change of batch statistics from a speciﬁc normalization layer of ResNet-50 during training on ImageNet. Regular batch statistics (orange line) are regarded as a good approximation for population statistics. We can see small batch statistics (blue line) are highly unstable, and contains notable error compared with regular batch statistics during training. In fact, the bias of gBt and ΨBt in BP is more serious than that ofµBt and σ2 Bt (see Figure 2(c), 2(d)). The instability of small batch statistics can worsen the capacity of the models in two aspects: ﬁrstly the instability of small batch statistics will make training unstable, resulting in slow convergence; Secondly the instability of small batch can produce huge difference between batch statistics and population statistics. Since the model is trained using batch statistics while evaluated using population statistics, the difference between batch statistics and population statistics will cause inconsistency between training and inference procedure, leading to bad performance of the model on evaluation data. (a) µB  (b) σ2 B  (c) gB  (d) ΨB Figure 2: Plot of batch statistics from layer1.0.bn1 in ResNet-50 during training. The formulation of these batch statistics (µB, σ2 B, gB, ΨB) have been shown in Section 3.1. Blue line represents the small batch statistic (|B|= 2) to compute, while orange line represents the regular batch statistics( |B|= 32). The x-axis represents the iterations, while the y-axis represents thel2 norm of these statistics in each ﬁgures. Notice the mean of gand Ψ is close to zero, hence l2 norm of gBand ΨBessentially represent their standard deviation. 4 M OVING AVERAGE BATCH NORMALIZATION Based on the discussion in Section 3.2, the key to restore the performance of BN is to solve the instability of small batch statistics. Therefore we considered two ways to handle the instability of 4Published as a conference paper at ICLR 2020 small batch statistics: using moving average statistics to estimate population statistics, and reducing the number of statistics by modifying the formulation of normalization. 4.1 S UBSTITUTE BATCH STATISTICS BY MOVING AVERAGE STATISTICS . Moving average statistics seem to be a suitable substitute for batch statistics to estimate population statistics when batch is small. We consider two types of moving average statistics: simple mov- ing average statistics (SMAS)2 and exponential moving average statistics (EMAS)3. The following theorem shows under mild conditions, SMAS and EMAS are more stable than batch statistics: Theorem 1 Assume there exists a sequence of random variable (r.v.) {ξt}∞ t=1, which are indepen- dent, uniformly bounded, i.e. ∀t,|ξt|<C , and have uniformly bounded density. Deﬁne: St = 1 m t∑ i=t−m+1 ξi, E t = (1 −α) t∑ i=1 αt−iξi, (7) where m∈R+. If the sequence {ξt}∞ t=1 satisﬁes ∃ξ,∀ϵ∈R, lim t→∞ P(ξt ≤ϵ) = P(ξ≤ϵ), (8) then we have E(Et) = E(ξ) + o(1), Var (Et) = (1 −α2t)(1 −α) 1 + α Var(ξ) + o(1); (9) If the sequence {ξt}∞ t=1 satisﬁes lim t→∞ sup λ |P(ξt−1 <λ) −P(ξt <λ)|= 0, (10) then we have E(St) = E(ξt) + o(1), Var (St) = Var(ξt) m + o(1); (11) The proof of theorem 1 can be seen in appendix A.1. Theorem 1 not only proves moving aver- age statistics have lower variance compared with batch statistics, but also reveals that with large momentum α, EMAS is better than SMAS with lower variance. However, using SMAS and EMAS request different conditions: Condition (8) means the sequence of the given statistics need to weakly converge to a speciﬁc random variable. For {µBt}∞ t=1, {σ2 Bt}∞ t=1, they converge to the ”ﬁnal” batch statistics µB, σ2 B (when training ﬁnished), hence condition (8) is satisﬁed, EMAS can be applied to replace {µBt}∞ t=1, {σ2 Bt}∞ t=1; Unfortunately {gBt}∞ t=1, {ΨBt}∞ t=1 don’t share the same property, EMAS is not suitable to take replace of {gBt}∞ t=1, {ΨBt}∞ t=1. However, under the assumption that learning rate is extremely small, the difference between the distribution of ξt−1 and ξt is tiny, thus condition (10) is satisﬁed, we can use SMAS to replace {gBt}∞ t=1, {ΨBt}∞ t=1. In a word, we can use EMAS ˆµt, ˆσ2 t to replace µBt, σ2 Bt, and use SMAS ¯gt, ¯Ψt to replace gBt, ΨBt in (1) and (4), where ˆµt = αˆµt−1 + (1 −α)µBt, ˆσ2 t = αˆσ2 t−1 + (1 −α)σ2 Bt, (12) ¯gt = 1 m m∑ s=1 gBt−m+s, ¯Ψt = 1 m m∑ s=1 ΨBt−m+s. (13) Notice neither of SMAS and EMAS is the unbiased substitute for batch statistics, but the bias can be extremely small comparing with expectation and variance of batch statistics, which is proven by equation 11 in theorem 1, our experiments also prove the effectiveness of moving average statistics as substitutes for small batch statistics (see Figure 3, 4 in appendix B.1). 2The exponential moving average (EMA) for a series{Yt}∞ t=1 is calculated as: St = α·Yt +(1 −α)·St−1. 3The simple moving average (SMA) for a series {Yt}∞ t=1 is calculated as: St = ∑t s=t−M+1 Ys M . 5Published as a conference paper at ICLR 2020 Relation to Batch Renormalization Essentially, Batch Renormalization (BRN) (Ioffe, 2017) re- places batch statistics µBt, σ2 Bt with EMAS ˆµt, ˆσ2 t both in FP (1) and BP (4). The formulation of BRN during training is written as: Y = X −µBt σBt , ˆY = r·Y + d (14) where r = clip[1/λ,λ](σBt ˆσt ), d = clip[−d,d](µBt−ˆµt ˆσt ). Based on our analysis, BRN successfully eliminates the effect of small batch statistics µBt and σ2 Bt by EMAS, but the small batch statistics associated with gradients gBt and ΨBt remains during backward propagation, preventing BRN from completely restoring the performance of vanilla BN. 4.2 S TABILIZING NORMALIZATION BY REDUCING THE NUMBER OF STATISTICS To further stabilize training procedure in small batch cases, we consider normalizing feature maps X using EX2 instead of EX and Var(X). The formulation of normalization is modiﬁed as: Y = X χBt , Z = Y ·γ+ β, (15) where χ2 Bt = 1 B ∑ bX2 b,:. Given ∂L ∂Y , the backward propagation is: ∂L ∂X ⏐⏐⏐⏐ Θt,Bt = 1 χBt ( ∂L ∂Y ⏐⏐⏐⏐ Θt,Bt −Y ·ΨBt). (16) The beneﬁts of the modiﬁcation seems obvious: there’s only two batch statistics left during FP and BP, which will introduce less instability into the normalization layer compared with vanilla normalizing form. In fact we can theoretically prove the beneﬁts of the modiﬁcation by following theorem: Theorem 2 If the following assumptions hold: 1. Var[ˆσ] = o(1), Var[ˆχ] = o(1); 2. Cov({∂L ∂y,y},{gB,ΨB}) = o(1); 3. Ey= o(1); Then we have: Var [∂L ∂x ⏐⏐⏐⏐ modified ] ≤Var [∂L ∂x ⏐⏐⏐⏐ vanilla ] −Var[gB] ˆσ2 (17) The proof can be seen in appendix A.2. According to (17), Var[∂L/∂X ⏐⏐ vanilla] is larger than that of Var[∂L/∂X ⏐⏐ modified], the gap is at least Var[gB]/ˆσ2, which mainly caused by the variance of gB/ˆσ. So the modiﬁcation essentially reduces the variance of the gradient by eliminating the batch statistics gBduring BP. Since gBt is a Monte Carlo estimator, the gap is inversely proportional to batch size. This can also explain why the improvement of modiﬁcation is signiﬁcant in small batch cases, but modiﬁed BN shows no superiority to vanilla BN within sufﬁcient batch size (see ablation study in section 5.1). Centralizing weights of convolution kernel Notice theorem 2 relies on assumption 3. The vanilla normalization naturally satisﬁes Ey = 0 by centralizing feature maps, but the modiﬁed normaliza- tion doesn’t necessarily satisfy assumption 3. To deal with that, inspired by Qiao et al. (2019), we ﬁnd centralizing weights W ∈Rq×p of convolution kernels, named as Weight Centralization (WC) can be a compensation for the absence of centralizing feature maps in practice: ¯W = 1 p ∑ i W:i, Xoutput = (W − ¯W)Xinput, (18) 6Published as a conference paper at ICLR 2020 where Xinput, Xoutput are the input and output of the convolution layer respectively. We conduct further ablation study to clarify the effectiveness of WC (see Table 4 in appendix B.2). It shows that WC has little beneﬁts to vanilla normalization, but it can signiﬁcantly improve the performance of modiﬁed normalization. We emphasize that weight centralization is only a practical remedy for the absence of centralizing feature maps. The theoretical analysis remains as a future work. Clipping and renormalizing strategy. In practice, we ﬁnd directly substituting batch statistics by moving average statistics in normalization layer will meet collapse during training. Therefore we take use of the clipping and renormalizing strategy from BRN (Ioffe, 2017). All in all, the formulation of proposed method MABN is: Y = X ¯χt , ˆY = r·Y (19) ∂L ∂Y ⏐⏐⏐⏐ Θt,Bt = r·∂L ∂ˆY ⏐⏐⏐⏐ Θt,Bt , ∂L ∂X ⏐⏐⏐⏐ Θt,Bt = 1 ¯χt ( ∂L ∂Y ⏐⏐⏐⏐ Θt,Bt −Y ⊙¯Ψt) (20) where the EMAS ˆχt is computed as ˆχt = αˆχt−1 + (1 −α)χBt, SMAS ¯χt is deﬁned as ¯χ2 t = 1 m ∑m s=1 χ2 Bt−m+s, SMAS ¯Ψt is deﬁned as (13). The renormalizing parameter is set as r= clip[1/λ,λ]( ¯χt ˆχt ). 5 E XPERIMENTS This section presents main results of MABN on ImageNet (Russakovsky et al., 2015), COCO (Lin et al., 2014). Further experiment results on ImangeNet, COCO and Cityscapes (Cordts et al., 2016) can be seen in appendix B.2, B.3, B.4 resepectively. We also evaluate the computational overhead and memory footprint of MABN, the results is shown in appendix B.5. 5.1 I MAGE CLASSIFICATION IN IMAGENET We evaluate the proposed method on ImageNet (Russakovsky et al., 2015) classiﬁcation datatsets with 1000 classes. All classiﬁcation experiments are conducted with ResNet-50 (He et al., 2016). More implementation details can be found in the appendix B.2. BN (Regular) BN (Small) BRN (Small) MABN (Small, m= 16) val error 23.41 35.22 30 .29 23 .58 ∆ (vs BN(Regular)) - 11.81 6 .88 0.17 Table 1: Comparison of top-1 error rate (%) of ResNet-50 on ImageNet Classiﬁcation. The gradient batch size is 32 per GPU. Regular means normalization batch size is 32, while Small means normalization batch size is 2. Comparison with other normalization methods. Our baseline is BN using small ( |B|= 2) or regular (|B|= 32) batch size, and BRN (Ioffe, 2017) with small batch size. We don’t present the per- formance of instance-level normalization counterpart on ImageNet, because they are not linear-type method during inference time, and they also failed to restore the performance of BN (over +0.5%), according to Wu & He (2018). Table 1 shows vanilla BN with small batch size can severely worsen the performance of the model( +11.81%); BRN (Ioffe, 2017) alleviates the issue to some extent, but there’s still remaining far from complete recovery(+6.88%); While MABN almost completely restore the performance of vanilla BN(+0.17%). We also compared the performance of BN, BRN and MABN when varying the batch size (see Figure 1(b)). BN and BRN are heavily relies on the batch size of training, though BRN performs better than 7Published as a conference paper at ICLR 2020 vanilla BN. MABN can always retain the best capacity of ResNet-50, regardless of batch size during training. Experiment Number Vanilla Normalization Modiﬁed Normalization EMAS in FP SMAS in BP Top-1 Error (%) 1⃝ ✓ 23.41 (BN, regular) 2⃝ ✓ 23.53 (regular) 3⃝ ✓ 35.22 (BN) 4⃝ ✓ ✓ 30.29 (BRN) 5⃝ ✓ ✓ ✓ - 6⃝ ✓ 29.68 7⃝ ✓ ✓ 27.03 8⃝ ✓ ✓ ✓ 23.58 (MABN) Table 2: Ablation study on ImageNet Classiﬁcation with ResNet-50. The normalization batch size is 2 in all experiments otherwise stated. The memory size is 16 and momentum is 0.98 when using SMAS, otherwise the momentum is 0.9. ”-” means the training can’t converge. Ablation study on ImageNet. We conduct ablation experiments on ImageNet to clarify the con- tribution of each part of MABN (see table 2). With vanilla normalization form, replacing batch statistics in FP with EMAS (as BRN) will restore the performance to some extents( −4.93%, com- paring 3⃝and 4⃝), but there’s still a huge gap ( +6.88%, comparing 1⃝and 4⃝) from complete restore. Directly using SMAS in BP with BRN will meet collapse during training ( 5⃝), no matter how we tuned hyperparameters. We think it’s due to the instability of vanilla normalization structure in small cases, so we modify the formulation of normalization shown in section 4.2. The modiﬁed normalization even slightly outperforms BRN in small batch cases (comparing 4⃝and 6⃝). How- ever, modiﬁed normalization shows no superiority to vanilla form (comparing1⃝and 2⃝), which can be interpreted by the result of theorem 2. With EMAS in FP, modiﬁed normalization signiﬁcantly reduces the error rate further (comparing 6⃝and 7⃝), but still fail to restore the performance com- pletely (+3.62%, comparing 1⃝and 7⃝). Applying SMAS in BP ﬁnally ﬁlls the rest of gap, almost completely restore the performance of vanilla BN in small batch cases ( +0.17 ,comparing 1⃝and 8⃝). 5.2 D ETECTION AND SEGMENTATION IN COCO FROM SCRATCH We conduct experiments on Mask R-CNN (He et al., 2017) benchmark using a Feature Pyramid Network(FPN) (Lin et al., 2017a) following the basic setting in He et al. (2017). We train the networks from scratch (He et al., 2018) for 2×times. Only the backbone contains normalization layers. More implementation details and experiment results can be seen in the appendix B.3. APbbox APbbox 50 APbbox 75 APmask APmask 50 APmask 75 BN 30.41 48.47 32 .70 27.91 45.79 29 .33 BRN 31.93 50.95 34 .48 29.16 48.16 30 .69 SyncBN 34.81 55.18 37 .69 31.69 51.86 33 .68 MABN 34.85 54.97 38 .00 31.61 51.88 33 .64 Table 3: Comparison of Average Precision(AP) of Mask-RCNN on COCO Detection and Segmen- tation. The gradients batch size is 16. The normalization batch size of SyncBN is 16, while that of BN, BRN and MABN are both 2. The momentum of BRN and MABN are both 0.98, while the momentum of BN and SyncBN are both 0.9. The buffer size (m) is 16). 8Published as a conference paper at ICLR 2020 Table 3 shows the result of MABN compared with vanilla BN, BRN and SyncBN (Peng et al., 2018). It can be seen that MABN outperforms vanilla BN and BRN by a clear margin and get comparable performance with SyncBN. Quite different from Imagenet experiments, we update the parameters every single batch (with Bnorm = 2 ). With such a complex pipeline, MABN still achieves a comparable performance as SyncBN. 6 C ONCLUSION This paper reveals the existence of the batch statisticsgBand ΨBinvolved in backward propagation of BN, and analysis their inﬂuence to training process. This discovery provides a new perspective to understand why BN always fails in small batch cases. Based on our analysis, we propose MABN to deal with small batch training problem. MABN can completely restore the performance of vanilla BN in small batch cases, and is extraordinarily efﬁcient compared with its counterpart like GN. Our experiments on multiple computer vision tasks (classiﬁcation, detection, segmentation) have shown the remarkable performance of MABN. 9Published as a conference paper at ICLR 2020 ACKNOWLEDGEMENT This research was partially supported by National Key RD Program of China (No. 2017YFA0700800), Beijing Academy of Artiﬁcial Intelligence (BAAI), and NSFC under Grant No. 61473091. REFERENCES Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence , 40(4): 834–848, 2017. Vitaliy Chiley, Ilya Sharapov, Atli Kosson, Urs Koster, Ryan Reece, Soﬁa Samaniego de la Fuente, Vishal Subbiah, and Michael James. Online Normalization for Training Neural Networks. arXiv e-prints, art. arXiv:1905.05894, May 2019. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. Sam Gross and Michael Wilber. Training and investigating residual nets, 2016. URL https: //github.com/facebook/fb.resnet.torch. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international conference on computer vision, pp. 1026–1034, 2015. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog- nition. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 770–778, 2016. Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pp. 2961–2969, 2017. Kaiming He, Ross Girshick, and Piotr Doll ´ar. Rethinking imagenet pre-training. arXiv preprint arXiv:1811.08883, 2018. Sergey Ioffe. Batch renormalization: Towards reducing minibatch dependence in batch-normalized models. In Advances in neural information processing systems, pp. 1945–1953, 2017. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. InInternational Conference on Machine Learning, pp. 448–456, 2015. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pp. 740–755. Springer, 2014. Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In Proceedings of the IEEE conference on com- puter vision and pattern recognition, pp. 2117–2125, 2017a. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll ´ar. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision , pp. 2980–2988, 2017b. 10Published as a conference paper at ICLR 2020 Chao Peng, Tete Xiao, Zeming Li, Yuning Jiang, Xiangyu Zhang, Kai Jia, Gang Yu, and Jian Sun. Megdet: A large mini-batch object detector. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6181–6189, 2018. Siyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan Yuille. Weight standardization. arXiv preprint arXiv:1903.10520, 2019. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211–252, 2015. Wenqi Shao, Tianjian Meng, Jingyu Li, Ruimao Zhang, Yudian Li, Xiaogang Wang, and Ping Luo. Ssn: Learning sparse switchable normalization via sparsestmax. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 443–451, 2019. Saurabh Singh and Abhinav Shrivastava. Evalnorm: Estimating batch normalization statistics for evaluation. arXiv preprint arXiv:1904.06031, 2019. Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing in- gredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016. Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 3–19, 2018. Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2881–2890, 2017. 11Published as a conference paper at ICLR 2020 A S KETCH OF PROOF A.1 P ROOF OF THEOREM 1 If the condition (8) is satisﬁed, i.e. {ξt}∞ t=1 weakly converge to ξ. Since {ξt}∞ t=1 has uniformly bounded density, we have: lim t→∞ Eξt = Eξ (21) lim t→∞ Var[ξt] = Var[ξ] (22) Since {ξt}∞ t=1 are independently, hence we have: Var[Et] = Var[(1 −α) ∑ i=1 t∑ i=1 αt−iξi] = (1 −α)2 t∑ i=1 α2(t−i)Var[ξi] = (1 −α)2 t∑ i=1 α2(t−i)Var[ξ] + (1−α)2 t∑ i=1 α2(t−i)(Var[ξi] −Var[ξ]) = (1 −α2t)(1 −α) 1 + α Var[ξ] + o(1) (23) as t→∞. Hence (9) has been proven. If the condition (10) is satisﬁed. Since {ξt}∞ t=1 is uniformly bounded, then ∃C ∈R+, ∀, |ξt|< C. As t→∞, We have ⏐⏐Eξt−1 −Eξt|= | ∫ x∈[−C,C] xpt−1(x)dx− ∫ x∈[−C,C] xpt(x)dx ⏐⏐ = ⏐⏐ ∫ x∈[−C,C] x(pt−1(x) −pt(x))dx ⏐⏐ = ⏐⏐x(Ft−1(x) −Ft(x)) ⏐⏐C −C − ∫ x∈[−C,C] (Ft−1(x) −Ft(x))dx ⏐⏐ ≤ ∫ x∈[−C,C] |Ft−1(x) −Ft(x)|dx ≤2C·sup x |Ft−1(x) −Ft(x)| = 2C·sup x |P(ξt−1 <x) −P(ξt <x)| = o(1) (24) Similarly, we have |Eξ2 t−1 −Eξ2 t|= ⏐⏐ ∫ x∈[−C,C] x2(pt−1(x) −pt(x))dx ⏐⏐ = ⏐⏐x2(Ft−1(x) −Ft(x)) ⏐⏐C −C − ∫ x∈[−C,C] 2x(Ft−1(x) −Ft(x))dx ⏐⏐ ≤ ∫ x∈[−C,C] 2|x||Ft−1(x) −Ft(x)|dx ≤4C2 ·sup x |Ft−1(x) −Ft(x)| = o(1) (25) Therefore combining (24) and (25), we have |Var[ξt−1] −Var[ξt]|≤| Eξ2 t−1 −Eξ2 t|+ |(Eξt−1)2 −(Eξt)2| = o(1) (26) 12Published as a conference paper at ICLR 2020 For a ﬁxed memory size m, as t→∞, we have Var(St) = Var[ 1 m m−1∑ i=0 ξt−i] = 1 m m−1∑ i=0 Var[ξt−i] = 1 m m−1∑ i=0 (Var[ξt] + o(1)) = Var[ξt] + o(1) (27) Therefore, (11) has been proven. A.2 P ROOF OF THEOREM 2 Without loss of generality, given the backward propagation of two normalizing form of a single input xwith batch B: ∂L ∂x ⏐⏐⏐⏐ vanilla = 1 ˆσ[∂L ∂y −gB−y·ΨB], ∂L ∂x ⏐⏐⏐⏐ modified = 1 ˆχ[∂L ∂y −y·ΨB], (28) where gB, ΨBare the batch statistics, and ˆσ, ˆχare the EMAS, deﬁned as before. We omitted the subscript tfor simplicity. Then the variance of partial gradients w.r.t. inputs xis written as Var[∂L ∂x ⏐⏐⏐⏐ vanilla ] = Var [1 ˆσ[∂L ∂y −gB−y·ΨB] ] (29) = 1 ˆσ2 [ Var [∂L ∂y −y·ΨB ] + Var [ gB ] + 2Cov [∂L ∂y −y·ΨB,gB ]] (30) = 1 ˆσ2 [ Var [∂L ∂y −y·ΨB ] + Var [ gB ]]] (31) ≥ 1 ˆχ2 Var [∂L ∂y −y·ΨB ] + Var [ gB ˆσ2 (32) = Var [∂L ∂x ⏐⏐⏐⏐ modified ] + Var[gB] ˆσ2 (33) where (30) is satisﬁed due to assumption 1. The variance of ˆσis so small that ˆσcan be regarded as a ﬁxed number; (31) is satisﬁed because Cov [∂L ∂y −y·ΨB,gB ] = Cov[∂L ∂y,gB] −Cov [ y·ΨB,gB ] (34) = Cov[∂L ∂y,gB] −E[yΨB(gB−EgB)] + E[yΨB]E[gB−EgB](35) Due to assumption 2, the correlation between individual sample and batch statistics is close to 0, hence we have Cov[∂L ∂y,gB] = 0 (36) E[yΨB(gB−EgB)] = EyE[ΨB(gB−EgB)] (37) E[yΨB] = EyEΨB (38) Besides, Eyis close to 0 according to assumption 3, hence Cov [∂L ∂y −y·ΨB,gB ] = 0. (39) (32) is satisﬁed due to the deﬁnition of ˆχand ˆσ, we have ˆχ2 = ˆσ2 + ˆµ2. (40) Similar to ˆσ, the variance of ˆχ is also too small that ˆχ can be regarded as a ﬁxed number due to assumption 1, so (33) is satisﬁed. 13Published as a conference paper at ICLR 2020 B E XPERIMENTS B.1 S TATISTICS ANALYSIS We analyze the difference between small batch statistics (|B|= 2) and regular batch statistics (|B|= 32) with the modiﬁed formulation of normalization (15) shown in Section 4.2. (a) χ2 B  (b) ΨB Figure 3: Plot of batch statistics from layer1.0.bn1 in ResNet-50 with a modiﬁed structure during training. The formulation of these batch statistics (χ2 B, ΨB) is shown in section 4.2, 3.1 respectively. Blue line represents the small batch statistic (|B|= 2), while orange line represents the regular batch statistics (|B|= 32). We use the small batch statistics to update the network parameters. (a) χ2 B  (b) ΨB Figure 4: Plot of batch statistics from layer1.0.bn1 in ResNet-50 with MABN. The formulation of these batch statistics ( χ2 B, ΨB) is shown in section 4.2, 3.1 respectively. Blue line represents the SMA batch statistic(2+30), while orange line represents the regular batch statistics(32). We use the moving average batch statistics to update the network parameters. Figure 3 illustrates the change of small batch statistics and regular batch statistics in FP and BP respectively. The variance of small batch statistics is much higher than the regular one. However, when we use SMAS as a approximation for regular batch statistics, the gap between SMAS and regular batch statistics is not obvious as shown in Figure 4. 14Published as a conference paper at ICLR 2020 B.2 E XPERIMENTS ON IMAGE NET Implementation details. All experiments on ImageNet are conducted across 8 GPUs. We train models with a gradient batch size ofBg = 32 images per GPU. To simulate small batch training, we split the samples on each GPU into Bg/|B|groups where |B|denotes the normalization batch size. The batch statistics are computed within each group individually. All weights from convolutions are initialized as He et al. (2015). We use 1 to initialize all γ and 0 to initialize all β in normalization layers. We use a weight decay of 10−4 for all weight layers including γ and β (following Wu & He (2018)). We train 600,000 iterations (approximately equal to 120 epoch when gradient batch size is 256) for all models, and divide the learning rate by 10 at 150,000, 300,000 and 450,000 iterations. The data augmentation follows Gross & Wilber (2016). The models are evaluated by top-1 classiﬁcation error on center crops of 224 ×224 pixels in the validation set. In vanilla BN or BRN, the momentumα= 0.9, in MABN, the momentum α= 0.98. Additional ablation studies. Table 4 shows the additional ablation results. We test all possible combination of all three kinds of statistics (SMAS, EMAS, BS) in FP and BP. The experiments results strongly prove our theoretical analysis in section 4.3. Besides, we verify the necessity of centralizing weights with modiﬁed normalization form. Experiment number w/o centralizing feature maps X Centralizing weights W FP statistics BP statistics Top-1 Error (%) 1⃝ ✓ ✓ EMAS SMAS 23.58(MABN) 2⃝ ✓ ✓ SMAS SMAS 26.63 3⃝ ✓ ✓ EMAS EMAS 24.83 4⃝ ✓ ✓ EMAS BS 27.03 5⃝ ✓ ✓ BS BS 29.68 6⃝ ✓ EMAS SMAS 25.45 7⃝ ✓ EMAS BS 29.57 8⃝ ✓ BS BS 32.95 9⃝ BS BS 35.22 10⃝ ✓ BS BS 34.27 11⃝ ✓ BS BS 23.35(regular) Table 4: Further ablation study on ImageNet with ResNet-50. The normalization batch size is 2 in all experiments. The buffer size ( m) is 16 and momentum is 0.98 when using SMA statistics, otherwise the momentum is 0.9. BS means vanilla batch statistics. B.3 E XPERIMENTS ON COCO Implementation details. We train the Mask-RCNN pipeline from scratch with MABN. We train the model on 8 GPUs, with 2 images per GPU. We train our model using COCO 2014 train and trainval35k dataset. We evaluate the model on COCO 2014 minival dataset.We set the momentum α= 0.98 for all MABN layers. We report the standard COCO mericsAPbbox, APbbox 75 , APbbox 50 for bounding box detection and APmask, APmask 50 , APmask 75 for instance segmentation. Other basic settings follow He et al. (2017). MABN used on heads. We build mask-rcnn baseline using a Feature Pyramid Network(FPN)(Lin et al., 2017a) backbone. The base model is ResNet-50. We train the models for 2×iterations. We use 4conv1fc instead of 2fc as the box head. Both backbone and heads contain normalization layers. We replace all normalization layers in each experiments. While training models with MABN, we use batch statistics in normalization layers on head during ﬁrst 10,000 iterations. Table 5 shows the result. The momentum are set to 0.98 in BRN and MABN. 15Published as a conference paper at ICLR 2020 APbbox APbbox 50 APbbox 75 APmask APmask 50 APmask 75 BN 32.38 50.44 35 .47 29.07 47.68 30 .75 BRN 34.07 52.66 37 .12 30.98 50.03 32 .93 SyncBN 36.81 56.23 40 .08 33.11 53.46 35 .28 MABN 36.50 55.79 40 .17 32.69 52.78 34 .71 Table 5: Comparision of Average Precision(AP) of Mask-RCNN on COCO Detection and Segmen- tation. The gradients batch size is 16. The normalization batch size of SyncBN is 16, while that of BN, BRN, MABN are both 2, the buffer size (m) of MABN is 32. Training from pretrained model. We compare the performance of MABN and SyncBN when training model based on ImageNet pretrained weights for 2x iterations. The results are shown in Table APbbox APbbox 50 APbbox 75 APmask APmask 50 APmask 75 SyncBN 38.25 57.81 42 .01 34.22 54.97 36 .34 MABN 38.42 58.19 41 .99 34.12 55.10 36 .12 Table 6: Comparision of Average Precision(AP) of Mask-RCNN on COCO Detection and Segmen- tation. The gradients batch size is 16. The normalization batch size of SyncBN is 16, while that of BN, BRN, MABN are both 2, the buffer size (m) of MABN is 32. Training from scratch for one-stage model. We also compare MABN and SyncBN based on one-stage pipeline. We build on retinanet(Lin et al., 2017b) benchmark. We train the model from scratch for 2×iterations. The results are shown in Table 7. APbbox APbbox 50 APbbox 75 SyncBN 29.80 46.21 31 .47 MABN 29.52 45.69 31 .14 Table 7: Comparison of Average Precision(AP) of retinanet on COCO Detection. The gradients batch size is 16. The normalization batch size of SyncBN is 16, while that of MABN is 2. All experiment results shows MABN can get comparable as SyncBN, and signiﬁcantly outperform BN on COCO. B.4 S EMANTIC SEGMENTATION IN CITYSCAPES We evaluate semantic segmentation in Cityscapes(Cordts et al., 2016). It contains 5,000 high qual- ity pixel-level ﬁnely annotated images collected from 50 cities in different seasons. We conduct experiments on PSPNET baseline and follow the basic settings mentioned in Zhao et al. (2017). For fair comparison, our backbone network is ResNet-101 as in Chen et al. (2017). Since we cen- tralize weights of convolutional kernel to use MABN, we have to re-pretrain our backbone model on Imagenet dataset. During ﬁne-tuning process, we linearly increase the learning rate for 3 epoch (558 iterations) at ﬁrst. Then we follow the ”poly” learning schedule as Zhao et al. (2017). Table 8 shows the result of MABN compared with vanilla BN, BRN and SyncBN. The buffer size (m) of MABN is 16, the modementum of MABN and BRN is 0.98. Since the statistics (mean and variance) is more stable in a pre-trained model than a random initial- ized one, the gap between vanilla BN and SyncBN is not signiﬁcant (+ 1.41%). However, MABN 16Published as a conference paper at ICLR 2020 pretrain Top-1 mIoU BN 21.74 77.11 BRN 21.74 77.30 SyncBN 21.74 78.52 MABN 21.70 78.20 Table 8: Results on Cityscapes testing set. still outperforms vanilla BN by a clear margin.(+1.09%). Besides, BRN shows no obvious superior- ity to vanilla BN(+0.19%) on Cityscapes dataset. B.5 C OMPUTATIONAL OVERHEAD We compare the computational overhead and memory footprint of BN, GN and MABN. We use maskrcnn with resnet50 and FPN as benchmark. We compute the theoretical FLOPS during infer- ence and measure the inference speed when a single image (3×224×224) goes through the backbone (resnet50 + FPN). We assume BN and MABN can be absorbed in convolution layer during infer- ence. GN can not be absorbed in convolution layer, so its FLOPS is larger than BN and MABN. Besides GN includes division and sqrt operation during inference, therefore it’s much slower than BN and MABN during inference time. We also monitor the training process of maskrcnn on COCO (8 GPUs, 2 images per GPU), and show its memory footprint and training speed. Notice we have not optimized the implementation of MABN, so its training speed is a little slower than BN and GN. FLOPS (M) Memory (GB) Training Speed (iter/s) Inference Speed (iter/s) BN 3123.75 58.875 2.35 12.73 GN 3183.28 58.859 2.22 6.34 MABN 3123.75 60.609 1.81 12.73 Table 9: Computational overhead and memory footprint of BN, GN and MABN. 17",
      "meta_data": {
        "arxiv_id": "2001.06838v2",
        "authors": [
          "Junjie Yan",
          "Ruosi Wan",
          "Xiangyu Zhang",
          "Wei Zhang",
          "Yichen Wei",
          "Jian Sun"
        ],
        "published_date": "2020-01-19T14:41:22Z",
        "pdf_url": "https://arxiv.org/pdf/2001.06838v2.pdf"
      }
    },
    {
      "title": "MetaNorm: Learning to Normalize Few-Shot Batches Across Domains"
    },
    {
      "title": "The Entropy Enigma: Success and Failure of Entropy Minimization",
      "abstract": "Entropy minimization (EM) is frequently used to increase the accuracy of\nclassification models when they're faced with new data at test time. EM is a\nself-supervised learning method that optimizes classifiers to assign even\nhigher probabilities to their top predicted classes. In this paper, we analyze\nwhy EM works when adapting a model for a few steps and why it eventually fails\nafter adapting for many steps. We show that, at first, EM causes the model to\nembed test images close to training images, thereby increasing model accuracy.\nAfter many steps of optimization, EM makes the model embed test images far away\nfrom the embeddings of training images, which results in a degradation of\naccuracy. Building upon our insights, we present a method for solving a\npractical problem: estimating a model's accuracy on a given arbitrary dataset\nwithout having access to its labels. Our method estimates accuracy by looking\nat how the embeddings of input images change as the model is optimized to\nminimize entropy. Experiments on 23 challenging datasets show that our method\nsets the SoTA with a mean absolute error of $5.75\\%$, an improvement of\n$29.62\\%$ over the previous SoTA on this task. Our code is available at\nhttps://github.com/oripress/EntropyEnigma",
      "full_text": "The Entropy Enigma: Success and Failure of Entropy Minimization Ori Press 1 Ravid Shwartz-Ziv 2 Yann LeCun2 3 Matthias Bethge 1 Abstract Entropy minimization (EM) is frequently used to increase the accuracy of classification models when they’re faced with new data at test time. EM is a self-supervised learning method that op- timizes classifiers to assign even higher proba- bilities to their top predicted classes. In this pa- per, we analyze why EM works when adapting a model for a few steps and why it eventually fails after adapting for many steps. We show that, at first, EM causes the model to embed test im- ages close to training images, thereby increasing model accuracy. After many steps of optimiza- tion, EM makes the model embed test images far away from the embeddings of training images, which results in a degradation of accuracy. Build- ing upon our insights, we present a method for solving a practical problem: estimating a model’s accuracy on a given arbitrary dataset without hav- ing access to its labels. Our method estimates accuracy by looking at how the embeddings of input images change as the model is optimized to minimize entropy. Experiments on 23 challenging datasets show that our method sets the SoTA with a mean absolute error of 5.75%, an improvement of 29.62% over the previous SoTA on this task. Our code is available at: https://github. com/oripress/EntropyEnigma 1. Introduction Practitioners commonly employ model adaptation strategies to enhance classifier performance on real-world data, which often differs significantly from training data. Unsupervised losses play a crucial role in adapting models to images corrupted by noise, such as snow or motion blur, or images from domains not seen in training, such as paintings or computer rendered images. Entropy minimization (EM) is a 1University of T ¨ubingen, T ¨ubingen AI Center, Germany 2New York University 3Meta AI, FAIR. Correspondence to: <ori.press@bethgelab.org>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). Test Time Adaptation (TTA) method that can improve the accuracy of a model on new datasets, without the need for additional labeled training data. EM adapts classifiers by iteratively increasing the probabilities assigned to the most likely classes while diminishing those of the others, and is an integral part of many recent TTA methods (Wang et al., 2020; Mummadi et al., 2021; Rusak et al., 2022b; Goyal et al., 2022; Niu et al., 2022; Cho et al., 2023; Niu et al., 2023; Press et al., 2023; D¨obler et al., 2024; Marsden et al., 2024). In this paper, we analyze EM to understand how it works, when and why it fails, and how to use it to predict model accuracy. The initial intuition behind using entropy minimization, given by Wang et al. (2020) was based on the observation that models tend to be more accurate on images for which they make predictions with higher confidence. The logi- cal extension of this observation was to encourage models to bolster their confidence on such images. However, our analysis reveals this intuition to be only partly true. Remark- ably, even when we construct datasets by excluding samples initially classified correctly — effectively creating datasets with a 100% classification error rate at the start — entropy minimization performance remains largely intact. Our analysis uncovers that during entropy minimization, embeddings of images from the input dataset tend to form distinct clusters. The distances between samples within each cluster diminish, creating more defined groupings, while the centers of these clusters gradually move apart, a phe- nomenon akin to neural collapse (Papyan et al., 2020; Han et al., 2021; Ben-Shaul et al., 2023). At first, embeddings of the input images not only cluster, but also stay close to the embeddings of original training images. Only after nu- merous optimization steps do these embeddings begin to diverge, distancing themselves from the embeddings of the clean training data (Fig. 1). We show this divergence to be intricately tied to a reduction in the model’s accuracy. Drawing from our insights, we present a method designed to estimate the accuracy of a given model on any dataset, with- out labels. This task is notably difficult, because in some cases in-distribution accuracy is tied to out-of-distribution (OOD) accuracy (Miller et al., 2021), while in other cases it is not (Teney et al., 2022). Our approach, termed Weighted Flips (WF), works in conjunction with TTA methods as they 1 arXiv:2405.05012v2  [cs.CV]  12 May 2024The Entropy Enigma: Success and Failure of Entropy Minimization Figure 1.Understanding the successes and failures of EM through clustering embedding dynamics. After a few iterations of adaptation (left), EM improves the accuracy of pretrained classifiers by embedding the input test data near mean embeddings of classes from the training data, marked by stars. Eventually, after many iterations (right), EM fails, because it embeds input test data far from where training data is embedded. We show the t-SNE embeddings of 16-class-Imagenet (Geirhos et al., 2018), throughout adaptation to Gaussian Noise 3 (Hendrycks & Dietterich, 2019). adapt to input data, with minimal added overhead. Using approximations of cluster consistency, WF estimates the ac- curacy of the network by measuring how the predictions of a fixed set of images change: the more they change, the lower the consistency of the clusters and the lower the predicted accuracy. We validate the efficacy of our method across an extensive array of 23 ImageNet-scale (Deng et al., 2009) datasets, encompassing diverse challenges, such as random adversarial noises, hard images, and datasets featuring OOD classes. WF surpasses the prior state-of-the-art methods by a substantial margin of 29.62%, setting a new benchmark in the accuracy estimation domain. 2. The Mystery of Entropy Minimization EM has been validated as effective in semi-supervised settings, with pioneering work by (Grandvalet & Ben- gio, 2004) and subsequent advancements, such as Tent (Wang et al., 2020), which demonstrated EM’s ability to enhance the accuracy of pre-trained classifiers on unlabeled ImageNet-scale (Deng et al., 2009) datasets. EM operates by iteratively optimizing the model to minimize the en- tropy of the output classification probabilities, denoted by H(ˆy) = −P c p(ˆyc) logp(ˆyc), where ˆy is the logits vector and p(ˆyc) is the probability assigned to class c. This ap- proach inherently boosts the likelihood of the most probable classes while diminishing that of the others. Wang et al. (2020) observed a correlation between lower output entropy and accuracy, indicating that images with low entropy out- puts are more likely to be classified correctly. Subsequent studies, including (Niu et al., 2022; Press et al., 2023; Mars- den et al., 2024), have built on this foundation, assigning more weight to lower-entropy samples, and even ignoring high-entropy samples entirely. To assess the influence of correctly classified images on EM’s effectiveness, we tested the effects of omitting im- ages that were initially correctly classified by the model. If such images are pivotal in EM’s ability to enhance classi- fier performance, we expect a notable decline in the EM efficacy. For this purpose, we utilized ImageNet-C (Hendrycks & Dietterich, 2019) Gaussian Noise level 3, dividing it into training and holdout sets. The training set was replicated seven times, systematically omitting images for which the ground truth label lay somewhere in the pre-trained model’s top-k predictions, for ( k ∈ [1, 2, 3, 5, 10, 20, 50]). Con- cretely, for k = 1 , all accurately classified images were excluded, and for k = 2, images whose label ranked within the top two predictions were removed, and so forth. Each altered training set was used to adapt a Tented model. The model’s accuracy was then evaluated on the holdout set, with evaluations every ten iterations, spanning a total of 1,000 iterations. The experiment results (see Figure 2) are revealing, under- scoring the robustness of EM. Notably, EM’s effectiveness endures even when images initially classified correctly are excluded. For instance, removing all initially correctly classified im- ages before adaptation produces an increase in accuracy comparable to not removing any images, with gains of 10.50% and 12.38%, respectively. Even more remarkable is the persistence of this trend: with k = 10, the model still 2The Entropy Enigma: Success and Failure of Entropy Minimization Figure 2.EM remains effective even when initially correctly classified images are excluded. Accuracy gain per iteration on a holdout set, as Tent adapts to its inputs. Surprisingly, the per- formance gain on the holdout set is high, even when we exclude top-k samples from the training set. When top-k = 0, no images are excluded. registers a notable accuracy improvement of 7.88%. This observation is particularly striking given the nature of the excluded images – they are not just numerous, but also rep- resent the highest quality, being those the network is most certain about. Specifically, images excluded atk = 1, which constitute 45% of the dataset, have an average entropy of 1.85, markedly lower than the original dataset’s average entropy of 2.84. Additionally, we also tested the effects of removing images according on their initial entropy level, and found similar results (see Appendix G). These findings intriguingly sug- gest that the model’s accuracy and entropy on individual images may not be as pivotal to EM’s success in enhancing classifier performance as previously thought. It reveals a nuanced dimension of EM’s functionality and hints at the presence of deeper mechanisms, which we will investigate next. 3. Phases of Entropy Minimization: Clustering Dynamics and Embedding Alignment We analyze the evolution of input data embeddings as EM progresses through its iterations. At first, EM causes the model to increase in accuracy, which we refer to as the first phase, followed by a decrease in accuracy, which we refer to as the second phase. The number of EM iterations needed for the model to reach its maximum accuracy (the end of the first phase, and the beginning of the second) is varied and depends on the input data. In the first phase, these embeddings align closely with the embeddings of samples from the original training distribu- tion. However, in the second phase, this alignment starts to deteriorate; the embeddings drift progressively further from the training distribution, disrupting the initial alignment, as conceptually depicted in Figure 3. Figure 3.The two-phase clustring paradigm explains EM be- havior. Intuitive visualization of EM’s phases. In the first phase (success), input test data becomes more clustered, aligning closely with the mean embeddings of corresponding classes from the train- ing data (the colored stars). In the second phase (failure), these clusters diverge from the mean embeddings. To examine the clustering process across the two phases of the EM, we focus on two measures: (1) the quality of the clusters and (2) their alignment with the original training data distribution. For evaluating cluster quality, we ran k-means on the embeddings and computed the Silhouette score (Rousseeuw, 1987), a widely recognized metric for measuring cluster quality. The Silhouette score gauges how closely an embedding corresponds to its own cluster in contrast to neighboring clusters, with a high score indicating distinct and well-separated clusters. To quantify the alignment between clusters and embeddings of the original training distribution, we looked at mean embeddings for the classes in the ImageNet validation set, alongside the centroids of clusters found by k-means. We use the Hungarian method (Kuhn, 1955) to find a matching between mean class embeddings and centroids, which mini- mizes the average distance between each assigned pair of (class embedding, centroid). Henceforth, we refer to this average of distances as “Shift distance”. As ImageNet contains many similar fine-grained classes, we restrict our analyses to the 16 classes outlined in (Geirhos et al., 2018), which represent approximately 20% of the total images. Consequently, we use k = 16 when we cluster the embeddings using k-means. This focused approach allowed for a detailed and controlled examination of clustering be- haviors within the framework of EM. We now examine changes in the Silhouette score and Shift distance as Tent adapts to the input data, over 50,000 it- erations using a ResNet-50 (He et al., 2016). Figure 4 showcases the comparative Silhouette scores and Shift dis- 3The Entropy Enigma: Success and Failure of Entropy Minimization Figure 4.Two-phase behavior during the EM adaption predicts accuracy.Differences in Silhouette score, Shift distance, and accuracy for Tent adaptation. Each point corresponds to a test dataset; each dataset appears twice: once in blue, corresponding to phase 1 (success, ∆ Acc ≥ 0), and once in orange, corresponding to phase 2 (failure, ∆ Acc < 0). Left: In both phases, and across almost all datasets, the Silhouette score of embeddings increases, corresponding to a better-clustered embedding space. Right: In the first phase, input data embeddings are kept close to training image embeddings, while in the second phase, they drift away, exhibiting large Shift distance changes. The datasets used are IN-C, IN-C and IN-3DCC. tances for both phases, incorporating findings from three diverse datasets: IN-C (Hendrycks & Dietterich, 2019), IN- C (Mintun et al., 2021), and IN-3DCC (Kar et al., 2022). Our findings distill into two primary insights: First, a pos- itive change in Silhouette score, indicative of enhanced clustering, is observed in both phases for more than 98% of cases. Notably, during the initial phase, a positive cor- relation exists between changes in Silhouette score and ac- curacy (ρ = 0.70, significant at α = 0.05). Second, Shift distances minimally change (and sometimes diminish, sig- nifying closer proximity to training data embeddings) in the first phase, they notably grow larger in the second phase. During this latter phase, a substantial negative correlation emerges between changes in Shift distance and accuracy (ρ = −0.79, significant at α = 0.05). Synthesizing these results reveals a nuanced picture: EM bolsters accuracy by clustering the embedded data into more concentrated clusters. This strategy remains efficacious as long as these embeddings align closely with the embeddings of the training data. However, as input data embeddings diverge from the training distribution, the classifier’s ac- curacy diminishes. This intricate interplay offers a deeper understanding of EM’s operation and its dependency on the spatial dynamics of data embeddings. We discuss the connection between EM and clustering in more detail in Appendix A. 4. Estimating Dataset Accuracy Leveraging our understanding of EM, we tackle a critical challenge in TTA settings: estimating the accuracy of a classification model on a given dataset. Ideally, one might resort to the metrics used in this paper, namely Silhouette score or Shift distance, for this purpose. However, these metrics encounter practical hurdles: the Silhouette score depends on clustering, which varies across datasets due to differences in class distributions or the total number of classes, and calculating the Shift distance is impossible, as accessing the training data (in order to calculate mean embedding vectors per class) is forbidden in most TTA settings (Wang et al., 2020; Niu et al., 2022; Yuan et al., 2023). 4.1. Label Flipping Due to the difficulties of measuring these scores in prac- tice, we take a different approach. We look at the number of images for which the model’s prediction changes some- where between the initial and the final iteration of the EM (“label flips”). According to our hypothesis, the number of label flips is correlated with the pre-trained model’s ac- curacy on the dataset. Our reasoning is as follows: there exists a tight correlation between accuracy and Silhouette score at iteration 0 — the higher the accuracy, the better clustered the input data, shown in Figure 5. Therefore, we do not expect EM, which works by clustering its inputs, to significantly change an already well-clustered set of embed- dings. It follows that there will likely be only a few label flips. Conversely, given a dataset with a low accuracy, its 4The Entropy Enigma: Success and Failure of Entropy Minimization Figure 5.Label flips are strongly correlated with Silhouette score. Silhouette score at the initial iteration and the total number of label flips at the final iteration are correlated for datasets in IN-C, IN-C, and IN-3DCC. Both metrics are correlated with accuracy, but measuring label flips is easier and more practical. image embeddings will likely be badly clustered initially, which leads EM to change them significantly, resulting in many label flips. We demonstrate the validity of this reasoning by adapt- ing the state-of-the-art TTA method, Rdumb (Press et al., 2023), to IN-C, IN-C, and IN-3DCC. Initially, we used the pre-trained model to classify 1,000 input images and then recorded the total number of label flips after adaptation. The model is adapted for 1,000 iterations because Rdumb resets itself every 1,000 iterations. We find a strong correlation between accuracy and label flips, seen in Figure 5. 4.2. Weighted Flips We now describe the Weighted Flips (WF) method of con- verting the count of label flips into a dataset accuracy es- timate. Instead of just counting the number of flips, we additionally consider the classifier’s initial confidence in its predictions for each image; images initially classified with high confidence that later flip should contribute more significantly than those with lower initial confidence. We then compute the WF as: W F= X i 1{flip}(i) · ci where 1flip (i) is 1 if image i’s label flipped and0 otherwise, and ci is the confidence percentile of imagei. Utilizing pairs of weighted flips and accuracy ((WF, accuracy)k) from IN- Validation and ImageNet-C holdout noises, we interpolate the weighted-flips-to-accuracy function, f (refer to Figure 6). To estimate the accuracy of a model on an unfamiliar dataset, we adapt the model to it using RDumb (for details, see Appendix J), measuring flips on the first 1,000 input images. After adaptation, we count and weigh the flips, estimating the model’s accuracy as f(WF). Importantly, WF is versatile and can work with a range of TTA methods (see Appendix E), and f can be interpolated in a variety of different ways (see Appendix B). In Appendix D.1, D.2, we present ablation studies on the effects of varying end iterations and holdout set sizes on performance. 4.3. Experimental Setting Accuracy estimation methods must yield robust estimates across diverse and challenging datasets to be considered reliable. In our evaluation, we probe the effectiveness of our proposed method using an extensive selection of popular ImageNet-scale classification datasets. This includes all classification datasets from the Shift-Happens benchmark1. Our chosen datasets encompass a wide spectrum, from various types of noise (IN-C, IN-C, IN-3DCC, CCC) and domain shifts (IN-R, IN-V2, IN-D), to adversarial noises (Patch-IN, BG Challenge, IN-Obfuscations), and even im- ages featuring classes not present in ImageNet (NINCO). Several datasets provide multiple splits of a similar nature, the results of which we average, except for ImageNet-D (Rusak et al., 2022a), which encompasses a variety of dis- tinct domains. The CCC dataset (Press et al., 2023) is par- ticularly expansive, containing 27 splits with 7.5M images each; for practicality, we only include the initial 25k images from each split in our analysis. Altogether, our evaluation spans 326 individual dataset splits. We briefly describe the other methods tested alongside ours: • AC (Hendrycks & Gimpel, 2016): Computes the dataset-wide average confidence for the top-predicted class in each image. • DoC (Guillory et al., 2021): Builds upon AC by as- sessing the variance in mean confidence between the validation and OOD sets, demonstrating consistent en- hancements in performance. • ATC (Garg et al., 2022): Estimates accuracy by deter- mining the fraction of unlabeled data samples where the model’s confidence exceeds a learned threshold. • COT (Lu et al., 2023): Estimates accuracy by applying Optimal Transport to quantify the disparity between OOD and in-distribution model outputs. 1https://github.com/ shift-happens-benchmark/icml-2022 5The Entropy Enigma: Success and Failure of Entropy Minimization Figure 6.Fitting and accuracy prediction using the WF method. Left: Fitting f: using the noises in IN-C Holdout and ImageNet- Validation, we fit pairs of (weighted flips, accuracy), shown in red. The black curve shows the function resulting from interpolating the points, f(x) = 0.00036x2 − 0.32x + 75.66. Right: With our weighted-flips-to-accuracy function f, we can estimate the accuracy of a model across the six splits from (Rusak et al., 2022a). We use the same f function and show that it works across different architectures, without refitting. 4.4. Results Looking at Table 1 reveals that our WF method consistently stands out as the best estimator across a broad spectrum of ImageNet-scale datasets. WF sets a new benchmark by achieving an average estimation error of just 5.75%, signifi- cantly outperforming the nearest competitor, COT, reducing the relative error by 29.62%. This exemplary performance of WF is not limited to average cases; even in the most challenging scenarios of worst-case performance, WF main- tains its superiority, cutting the error by 29.74% compared to COT. Furthermore, WF demonstrates remarkable consis- tency as an estimator. In 18 of the 23 datasets evaluated, it either leads the pack or comes a close second. This is in stark contrast to the performance of COT, which, despite being second-best, only achieves top-two rankings in 12 datasets. The persistent effectiveness of WF across diverse conditions underscores its reliability and superiority in ac- curacy estimation. Practicality of WF: Beyond its top-tier performance, WF stands out for its practicality. It operates concurrently with the EM process, requiring only three parameters that define the weighted-flips-to-accuracy function, f. This process adds minimal computational overhead, requiring only 20 additional forward passes for every 1,000 Rdumb iteration steps. Lastly, WF is effective even when only a small num- ber of samples are available, see Appendix C. Versatility across Models and Architectures: To demon- strate the adaptability of the WF method, we tested it across various models and architectures, employing the same weighted-flips-to-accuracy function, f, used in our primary experiments (Table 1). Testing encompassed dif- ferent ResNet variants, including models enhanced with noise augmentation techniques, such as ANT (Rusak et al., 2020), AugMix (Hendrycks et al., 2019), and DeepAugment (Hendrycks et al., 2021a). Additionally, we evaluated a ResNext-101 (Xie et al., 2017), ViTB-16 (Dosovitskiy et al., 2010), and MaxViT-T (Tu et al., 2022). The mean absolute errors between estimated and actual accuracies are reported in Table 2. Remarkably, 5 of the 8 models tested achieved a lower mean absolute error than the baseline model, RN-50, showing that f maintains its efficacy across different model architectures. When f is refitted on the architecture that WF is evaluated on, performance improves (see Appendix F). Robustness to Dataset Choice: In Table 1, we derived the weighted-flips-to-accuracy function f using IN-C holdout and ImageNet validation noises. We further validated the robustness of the WF method by fitting f using a subset of the 23 datasets and then assessing its performance on the remaining datasets. As an added challenge, we excluded datasets used in the original configuration: IN-C Holdout and ImageNet-Validation. For each subset size, we repeated the fitting and evaluation process 50 times. The results, plot- ted in Figure 7, illustrate that the WF method consistently outperforms COT across almost all subset sizes, reinforc- ing its resilience and reliability across a broad spectrum of datasets. 6The Entropy Enigma: Success and Failure of Entropy Minimization Table 1.Mean Absolute Error between estimated accuracy, and true accuracy on a ResNet-50 model, for 4 estimation methods (AC, DoC, ATC, COT) (Hendrycks & Gimpel, 2016; Guillory et al., 2021; Garg et al., 2022; Lu et al., 2023), and ours. Our method (WF) is consistently either best or second best, with the best average and worst-case performance across many different OOD datasets. Best results are in bold; second best are underlined, {.} indicates how many splits are in each dataset, when there are more than 1. Datasets AC DoC ATC COT WF (ours) Noises IN-C {75} (Hendrycks & Dietterich, 2019) 10.06 6.61 7.44 2.23 4.79 IN-C {50} (Mintun et al., 2021) 19.48 15.96 12.16 3.17 7.35 IN-3DCC {60} (Kar et al., 2022) 11.83 3.44 8.15 3.02 3.66 CCC {27} (Press et al., 2023) 15.51 11.95 6.05 2.04 2.80 Domain Shifts Stylized (Geirhos et al., 2019) 31.63 28.08 7.36 12.18 3.81 IN-V2 {3} (Recht et al., 2019) 5.58 2.41 0.45 2.68 4.70 IN-Sketch (Wang et al., 2019) 22.34 18.78 0.15 4.23 1.71 IN-R (Hendrycks et al., 2021a) 23.21 19.65 0.37 2.44 1.88 IN-D (Rusak et al., 2022a) Real 10.56 7.00 1.35 27.54 3.18 Painting 17.40 13.85 3.27 7.49 2.12 Clipart 21.27 17.72 1.62 4.52 3.37 Sketch 24.43 20.87 0.61 0.71 5.44 Infograph 54.12 50.57 36.26 3.44 3.63 Quickdraw 32.67 29.11 4.13 1.60 2.57 Cartoon & Drawing {2} (Salvador & Oberman, 2022) 15.69 12.13 4.42 1.62 13.25 Adversarial Noises BG Challenge {8} (Xiao et al., 2020) 10.54 7.37 4.88 19.68 6.92 IN-A (Hendrycks et al., 2021b) 45.12 41.57 20.51 30.38 21.61 IN-C Patch {75} (Gu et al., 2022) 4.37 0.16 4.42 2.57 1.60 IN-Hard (Taesiri et al., 2023) 29.71 26.15 6.73 15.33 3.64 Patch-IN {10} (Pintor et al., 2023) 8.06 5.11 5.11 10.13 8.87 IN-Obfuscations {3} (Stimberg et al., 2023) 99.90 96.34 99.90 0.12 4.58 OOD/Other ObjectNet (Barbu et al., 2019) 34.59 31.03 9.43 10.40 2.74 NINCO (Bitterwolf et al., 2023) 50.29 46.74 26.97 20.28 18.07 Average 26.02 22.29 11.81 8.17 5.75 Worst Case 99.90 96.34 99.90 30.38 21.61 Average (Worst Case Excluded) 22.66 18.92 7.81 7.16 5.03 5. Related Work To the best of our knowledge, the first time EM was shown to be useful for improving a classifier’s accuracy was in (Grandvalet & Bengio, 2004). They showed how EM can be applied to a logistic regressor, and found it to be ben- eficial in cases where the data was corrupted by outliers. Following this, Lee et al. (2013) proposed pseudo labeling as a means of improving classification accuracy on MNIST. Interestingly, t-SNE is used to show that pseudo labeling works partly by encouraging the model’s embeddings to be better clustered, and away from the decision boundaries of the model. Moreoever, it is stated that pseudo labeling is equivalent to entropy regularization (Grandvalet & Bengio, 2004). Although this might be true in the settings consid- ered then, pseudo labeling was shown to be less effective (and thus not equivalent) on larger-scale datasets, by Tent. Unlike previous work, we demonstrate that EM clusters by measuring the Silhouette score of the clusters themselves, allowing us to empirically evaluate ImageNet scale datasets. Additionally, we show what happens when EM fails, which is not discussed in prior work, with the exception of (Oliver et al., 2018), which shows how EM fails to adapt to a toy “two moons” dataset, because the model increases the mag- nitude of its output logits. This isn’t the case in most TTA settings, as the final layer of the model isn’t trained. Minimizing entropy at test time was popularized by Tent 7The Entropy Enigma: Success and Failure of Entropy Minimization Table 2.Mean Absolute Error between estimated accuracy and true accuracy, across different architectures. Using the same weighted-flips to-accuracy function, f, works across different architectures and models, without need for finetuning. For each model and dataset, the task is to estimate the accuracy of that model on the dataset. Best results are in bold; second best are underlined. AugMix: ♢ ANT: ‡ DeepAugment: ♠ (Hendrycks et al., 2019; Rusak et al., 2020; Hendrycks et al., 2021a) Datasets RN-50 RN-18 RN-34 RN-50 ‡ RN-50 ♢ RN-50 ♢♠ RNXt-101 RNXt-101 ♠ ViT-B/16 MaxViT-T IN-C 4.79 7.21 6.04 5.39 5.02 4.81 5.35 4.12 8.34 6.73 IN-C 7.35 7.90 6.77 6.84 6.60 6.48 5.60 5.63 6.59 4.97 IN-3DCC 3.66 3.58 3.89 3.20 3.07 2.98 7.23 4.37 7.19 6.79 IN-V2 4.70 4.11 3.37 3.67 5.06 5.00 6.47 5.54 4.44 6.08 IN-D Real 3.18 2.83 0.38 2.72 6.59 3.30 4.24 0.61 1.02 3.40 Painting 2.12 5.36 0.78 0.59 7.62 2.51 12.02 1.12 3.02 0.60 Clipart 3.37 1.59 4.42 0.32 6.19 2.19 7.24 0.53 12.82 4.52 Sketch 5.44 1.53 3.93 6.18 9.73 3.60 10.75 1.89 11.04 10.88 Infograph 3.63 1.76 3.67 3.74 6.78 0.28 6.37 2.34 9.27 9.13 Quickdraw 2.57 2.34 2.24 2.20 2.53 1.27 2.27 1.21 2.36 2.31 Average 4.08 3.82 3.55 3.49 5.92 3.10 6.76 2.74 6.61 5.54 Figure 7.WF outperforms other methods across almost all sub- set sizes. Mean Absolute Error of WF when using a weighted-flips- to-accuracy function f to fit on random subsets of the 23 datasets in Table 1. For each point on the x-axis, we sample 50 fitting datasets for WF, and plot the average and the standard deviation of the MAE. For the other methods, we plot average MAE across all datasets . (Wang et al., 2020), which demonstrated the effectiveness of EM on large-scale datasets, such as ImageNet-C. Entropy minimization is ideal for domain adaptation: it can be used on a trained model, without retraining, and doesn’t require balancing a proxy loss with a classification loss, as in (Gidaris et al., 2018; Sun et al., 2020; Gandelsman et al., 2022). Though many prior works use losses that are based on en- tropy (Wang et al., 2020; Rusak et al., 2022b; Goyal et al., 2022; Mummadi et al., 2021; Wang et al., 2022; Niu et al., 2022; Cho et al., 2023; Press et al., 2023; Niu et al., 2023; D¨obler et al., 2024; Marsden et al., 2024), little is known as to why it works. Additionally, entropy minimization, when used in TTA settings, is effective for only a limited num- ber of iterations, before the classifier degrades to chance accuracy, shown in (Press et al., 2023). Interestingly, this degradation of accuracy, named “collapse”, differs from classical definitions of catastrophic forgetting in continual learning (De Lange et al., 2021), in that the task itself does not change. A plethora of methods have been used for adapting a trained classifier to out-of-domain data: from using an auxiliary loss to help learn the test domain (Sun et al., 2019; 2020; Gandelsman et al., 2022) through simply re-estimating the mean and variance statistics (Schneider et al., 2020; Nado et al., 2020) to using image augmentations (Wang et al., 2022; Song et al., 2023; Chakrabarty et al., 2023). However, for their simplicity and success, entropy minimization-based methods are still the most widely used and successful in settings most relevant to this work. Works that follow Tent improve EM by modifying the loss to be more robust to label noise (Rusak et al., 2022b) or smoother (Mummadi et al., 2021), or by adjusting the tem- perature of the output distribution (Goyal et al., 2022). While testing on long sequences of images, both (Wang et al., 2022) and (Niu et al., 2022) show that Tent degrades in accuracy, the more iterations it does. (Press et al., 2023) show that this is in fact true for all TTA methods apart from EATA (Niu et al., 2022), which uses an L2 regularizer to con- strain the adapting model’s weights to be close to those of 8The Entropy Enigma: Success and Failure of Entropy Minimization the pretrained model. (Niu et al., 2023) study the effects of batch size, label shifts and other factors on adaptation; they propose a method to stabilize adaptation. Similarly, (D¨obler et al., 2024) also test entropy minimization-based methods in real-world conditions, and propose a new method based on a diversity and a weighted entropy loss. Entropy has also been used in semi-supervised settings: (Sohn et al., 2020) propose augmentation and an entropy loss to train a classifier when only a few labels are available. Analyzing which labels flip during training has been studied in (Toneva et al., 2018), which explored which samples are forgotten during training. Another work, (Deng et al., 2022) looked at how to reduce the amount of times a label flips dur- ing training. The agreement/disagreement between different models on ID data was shown to be linearly correlated to OOD accuracy and has been recently used to estimate accu- racy in (Miller et al., 2021; Jiang et al., 2021; Baek et al., 2022; Kim et al., 2023). These works are beyond the scope of this work, as they require access to multiple models and ID data, which is disallowed in most TTA settings (Wang et al., 2020; Niu et al., 2022; Yuan et al., 2023). 6. Conclusion While EM is a cornerstone in many TTA methods, the me- chanics of its success have remained enigmatic. This study sheds light on the transformative journey of input data em- beddings under the EM adaption. It reveals a biphasic clus- tering process, where alignment with the training data’s embedding clusters bolsters accuracy, followed by a subse- quent phase where excessive divergence diminishes it. Our work goes beyond deciphering the mystery behind en- tropy minimization; it also utilizes this knowledge to signif- icantly refine the precision of model accuracy predictions in TTA contexts. This dual achievement underscores the potential of deep analytical approaches in enhancing the efficacy and applicability of machine learning models. Acknowledgements We thank Ofir Press for helpful insights and feedback. The authors thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting Ori Press. Matthias Bethge is a member of the Machine Learning Cluster of Excellence, funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Founda- tion) under Germany’s Excellence Strategy – EXC number 2064/1 – Project number 390727645 and acknowledges support by the German Research Foundation (DFG): SFB 1233, Robust Vision: Inference Principles and Neural Mech- anisms, TP 4, Project No: 276693517. This work was supported by the T¨ubingen AI Center. The authors declare no conflicts of interests. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel needs to be highlighted here specifically. References Amini, M.-R. and Gallinari, P. Semi-supervised logistic regression. In ECAI, volume 2, pp. 11, 2002. Baek, C., Jiang, Y ., Raghunathan, A., and Kolter, J. Z. Agreement-on-the-line: Predicting the performance of neural networks under distribution shift.Advances in Neu- ral Information Processing Systems , 35:19274–19289, 2022. Barbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gut- freund, D., Tenenbaum, J., and Katz, B. Objectnet: A large-scale bias-controlled dataset for pushing the lim- its of object recognition models. Advances in neural information processing systems, 32, 2019. Ben-Shaul, I., Shwartz-Ziv, R., Galanti, T., Dekel, S., and LeCun, Y . Reverse engineering self-supervised learning. arXiv preprint arXiv:2305.15614, 2023. Bitterwolf, J., M¨uller, M., and Hein, M. In or out? fixing imagenet out-of-distribution detection evaluation. arXiv preprint arXiv:2306.00826, 2023. Chakrabarty, G., Sreenivas, M., and Biswas, S. Santa: Source anchoring network and target alignment for con- tinual test time adaptation. Transactions on Machine Learning Research, 2023. Cho, Y ., Kim, Y ., and Lee, D. Beyond entropy: Style transfer guided single image continual test-time adaptation. arXiv preprint arXiv:2311.18270, 2023. De Lange, M., Aljundi, R., Masana, M., Parisot, S., Jia, X., Leonardis, A., Slabaugh, G., and Tuytelaars, T. A continual learning survey: Defying forgetting in classifi- cation tasks. IEEE transactions on pattern analysis and machine intelligence, 44(7):3366–3385, 2021. Dempster, A. P., Laird, N. M., and Rubin, D. B. Maximum likelihood from incomplete data via the em algorithm. Journal of the royal statistical society: series B (method- ological), 39(1):1–22, 1977. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009. 9The Entropy Enigma: Success and Failure of Entropy Minimization Deng, X., Xiao, Y ., Long, B., and Zhang, Z. Reducing flip- ping errors in deep neural networks. InProceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 6506–6514, 2022. D¨obler, M., Marencke, F., Marsden, R. A., and Yang, B. Diversity-aware buffer for coping with temporally corre- lated data streams in online test-time adaptation. arXiv preprint arXiv:2401.00989, 2024. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. arxiv 2020. arXiv preprint arXiv:2010.11929, 2010. Gandelsman, Y ., Sun, Y ., Chen, X., and Efros, A. Test-time training with masked autoencoders. Advances in Neural Information Processing Systems, 35:29374–29385, 2022. Garg, S., Balakrishnan, S., Lipton, Z. C., Neyshabur, B., and Sedghi, H. Leveraging unlabeled data to pre- dict out-of-distribution performance. arXiv preprint arXiv:2201.04234, 2022. Geirhos, R., Temme, C. R., Rauber, J., Sch¨utt, H. H., Bethge, M., and Wichmann, F. A. Generalisation in humans and deep neural networks. Advances in neural information processing systems, 31, 2018. Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wich- mann, F. A., and Brendel, W. Imagenet-trained CNNs are biased towards texture; increasing shape bias im- proves accuracy and robustness. In International Confer- ence on Learning Representations, 2019. URL https: //openreview.net/forum?id=Bygh9j09KX. Gidaris, S., Singh, P., and Komodakis, N. Unsupervised rep- resentation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018. Goyal, S., Sun, M., Raghunathan, A., and Kolter, J. Z. Test time adaptation via conjugate pseudo-labels. Advances in Neural Information Processing Systems, 35:6204–6218, 2022. Grandvalet, Y . and Bengio, Y . Semi-supervised learning by entropy minimization. Advances in neural information processing systems, 17, 2004. Gu, J., Tresp, V ., and Qin, Y . Evaluating model robustness to patch perturbations. In ICML 2022 Shift Happens Workshop, 2022. Guillory, D., Shankar, V ., Ebrahimi, S., Darrell, T., and Schmidt, L. Predicting with confidence on unseen distri- butions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1134–1144, 2021. Han, X., Papyan, V ., and Donoho, D. L. Neural collapse under mse loss: Proximity to and dynamics on the central path. arXiv preprint arXiv:2106.02073, 2021. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions and perturba- tions. arXiv preprint arXiv:1903.12261, 2019. Hendrycks, D. and Gimpel, K. A baseline for detecting misclassified and out-of-distribution examples in neural networks. arXiv preprint arXiv:1610.02136, 2016. Hendrycks, D., Mu, N., Cubuk, E. D., Zoph, B., Gilmer, J., and Lakshminarayanan, B. Augmix: A simple data processing method to improve robustness and uncertainty. arXiv preprint arXiv:1912.02781, 2019. Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M., et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 8340–8349, 2021a. Hendrycks, D., Zhao, K., Basart, S., Steinhardt, J., and Song, D. Natural adversarial examples. CVPR, 2021b. Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pp. 448– 456. pmlr, 2015. Jiang, Y ., Nagarajan, V ., Baek, C., and Kolter, J. Z. As- sessing generalization of sgd via disagreement. arXiv preprint arXiv:2106.13799, 2021. Kar, O. F., Yeo, T., Atanov, A., and Zamir, A. 3d common corruptions and data augmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18963–18974, 2022. Kim, E., Sun, M., Raghunathan, A., and Kolter, Z. Reliable test-time adaptation via agreement-on-the-line. arXiv preprint arXiv:2310.04941, 2023. Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009. Kuhn, H. W. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):83– 97, 1955. 10The Entropy Enigma: Success and Failure of Entropy Minimization Kullback, S. and Leibler, R. A. On information and suf- ficiency. The annals of mathematical statistics , 22(1): 79–86, 1951. Lee, D.-H. et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural net- works. In Workshop on challenges in representation learning, ICML, volume 3, pp. 896. Atlanta, 2013. Lu, Y ., Wang, Z., Zhai, R., Kolouri, S., Campbell, J., and Sycara, K. Predicting out-of-distribution er- ror with confidence optimal transport. arXiv preprint arXiv:2302.05018, 2023. Marsden, R. A., D ¨obler, M., and Yang, B. Universal test- time adaptation through weight ensembling, diversity weighting, and prior correction. In Proceedings of the IEEE/CVF Winter Conference on Applications of Com- puter Vision, pp. 2555–2565, 2024. Miller, J. P., Taori, R., Raghunathan, A., Sagawa, S., Koh, P. W., Shankar, V ., Liang, P., Carmon, Y ., and Schmidt, L. Accuracy on the line: on the strong correlation between out-of-distribution and in-distribution generalization. In International Conference on Machine Learning, pp. 7721– 7735. PMLR, 2021. Mintun, E., Kirillov, A., and Xie, S. On interaction between augmentations and corruptions in natural corruption ro- bustness. Advances in Neural Information Processing Systems, 34:3571–3583, 2021. Mummadi, C. K., Hutmacher, R., Rambach, K., Levinkov, E., Brox, T., and Metzen, J. H. Test-time adaptation to distribution shift by confidence maximization and input transformation. arXiv preprint arXiv:2106.14999, 2021. Nado, Z., Padhy, S., Sculley, D., D’Amour, A., Lakshmi- narayanan, B., and Snoek, J. Evaluating prediction-time batch normalization for robustness under covariate shift. arXiv preprint arXiv:2006.10963, 2020. Niu, S., Wu, J., Zhang, Y ., Chen, Y ., Zheng, S., Zhao, P., and Tan, M. Efficient test-time model adaptation with- out forgetting. In International conference on machine learning, pp. 16888–16905. PMLR, 2022. Niu, S., Wu, J., Zhang, Y ., Wen, Z., Chen, Y ., Zhao, P., and Tan, M. Towards stable test-time adaptation in dynamic wild world. arXiv preprint arXiv:2302.12400, 2023. Oliver, A., Odena, A., Raffel, C., Cubuk, E., and Goodfel- low, I. Realistic evaluation of semi-supervised learning algortihms. In International conference on learning rep- resentations, pp. 1–15, 2018. Papyan, V ., Han, X., and Donoho, D. L. Prevalence of neural collapse during the terminal phase of deep learn- ing training. Proceedings of the National Academy of Sciences, 117(40):24652–24663, 2020. Pintor, M., Angioni, D., Sotgiu, A., Demetrio, L., Demontis, A., Biggio, B., and Roli, F. Imagenet-patch: A dataset for benchmarking machine learning robustness against adversarial patches. Pattern Recognition, 134:109064, 2023. Poland, W. B. and Shachter, R. D. Mixtures of gaussians and minimum relative entropy techniques for modeling continuous uncertainties. In Uncertainty in Artificial Intelligence, pp. 183–190. Elsevier, 1993. Press, O., Schneider, S., K ¨ummerer, M., and Bethge, M. Rdumb: A simple approach that questions our progress in continual test-time adaptation. Advances in Neural Information Processing Systems, 36, 2023. Recht, B., Roelofs, R., Schmidt, L., and Shankar, V . Do imagenet classifiers generalize to imagenet? In Interna- tional conference on machine learning, pp. 5389–5400. PMLR, 2019. Rousseeuw, P. J. Silhouettes: a graphical aid to the inter- pretation and validation of cluster analysis. Journal of computational and applied mathematics, 20:53–65, 1987. Rusak, E., Schott, L., Zimmermann, R. S., Bitterwolf, J., Bringmann, O., Bethge, M., and Brendel, W. A simple way to make neural networks robust against diverse im- age corruptions. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part III 16, pp. 53–69. Springer, 2020. Rusak, E., Schneider, S., Gehler, P. V ., Bringmann, O., Bren- del, W., and Bethge, M. Imagenet-d: A new challenging robustness dataset inspired by domain adaptation. In ICML 2022 Shift Happens Workshop, 2022a. Rusak, E., Schneider, S., Pachitariu, G., Eck, L., Gehler, P. V ., Bringmann, O., Brendel, W., and Bethge, M. If your data distribution shifts, use self-learning. Transactions on Machine Learning Research, 2022b. Salvador, T. and Oberman, A. M. Imagenet-cartoon and imagenet-drawing: two domain shift datasets for ima- genet. In ICML 2022 Shift Happens Workshop, 2022. Schneider, S., Rusak, E., Eck, L., Bringmann, O., Bren- del, W., and Bethge, M. Improving robustness against common corruptions by covariate shift adaptation. Ad- vances in neural information processing systems , 33: 11539–11551, 2020. 11The Entropy Enigma: Success and Failure of Entropy Minimization Sohn, K., Berthelot, D., Carlini, N., Zhang, Z., Zhang, H., Raffel, C. A., Cubuk, E. D., Kurakin, A., and Li, C.-L. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. Advances in neural informa- tion processing systems, 33:596–608, 2020. Song, J., Lee, J., Kweon, I. S., and Choi, S. Ecotta: Memory- efficient continual test-time adaptation via self-distilled regularization. In Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition , pp. 11920–11929, 2023. Stimberg, F., Chakrabarti, A., Lu, C.-T., Hazimeh, H., Stretcu, O., Qiao, W., Liu, Y ., Kaya, M., Rashtchian, C., Fuxman, A., et al. Benchmarking robustness to adversar- ial image obfuscations. arXiv preprint arXiv:2301.12993, 2023. Sun, Y ., Tzeng, E., Darrell, T., and Efros, A. A. Unsuper- vised domain adaptation through self-supervision. arXiv preprint arXiv:1909.11825, 2019. Sun, Y ., Wang, X., Liu, Z., Miller, J., Efros, A., and Hardt, M. Test-time training with self-supervision for generaliza- tion under distribution shifts. In International conference on machine learning, pp. 9229–9248. PMLR, 2020. Taesiri, M. R., Nguyen, G., Habchi, S., Bezemer, C.-P., and Nguyen, A. Zoom is what you need: An empirical study of the power of zoom and spatial biases in image classification. arXiv preprint arXiv:2304.05538, 2023. Teney, D., Lin, Y ., Oh, S. J., and Abbasnejad, E. Id and ood performance are sometimes inversely correlated on real-world datasets. arXiv preprint arXiv:2209.00613, 2022. Toneva, M., Sordoni, A., Combes, R. T. d., Trischler, A., Bengio, Y ., and Gordon, G. J. An empirical study of example forgetting during deep neural network learning. arXiv preprint arXiv:1812.05159, 2018. Tu, Z., Talebi, H., Zhang, H., Yang, F., Milanfar, P., Bovik, A., and Li, Y . Maxvit: Multi-axis vision transformer. In European conference on computer vision, pp. 459–479. Springer, 2022. Wang, D., Shelhamer, E., Liu, S., Olshausen, B., and Darrell, T. Tent: Fully test-time adaptation by entropy minimiza- tion. arXiv preprint arXiv:2006.10726, 2020. Wang, H., Ge, S., Lipton, Z., and Xing, E. P. Learning ro- bust global representations by penalizing local predictive power. In Advances in Neural Information Processing Systems, pp. 10506–10518, 2019. Wang, Q., Fink, O., Van Gool, L., and Dai, D. Continual test- time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7201–7211, 2022. Xiao, K., Engstrom, L., Ilyas, A., and Madry, A. Noise or signal: The role of image backgrounds in object recogni- tion. ArXiv preprint arXiv:2006.09994, 2020. Xie, S., Girshick, R., Doll´ar, P., Tu, Z., and He, K. Aggre- gated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vi- sion and pattern recognition, pp. 1492–1500, 2017. Yuan, L., Xie, B., and Li, S. Robust test-time adaptation in dynamic scenarios. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15922–15932, 2023. 12The Entropy Enigma: Success and Failure of Entropy Minimization A. The Relationship between Entropy Minimization and Clustering In this section, we explain the connection between entropy minimization and the Expectation-Maximization algorithm (Dempster et al., 1977) with a mixture of Gaussians and show how the iterative entropy minimization objective leads to a clustering process similar to the Expectation-Maximization algorithm. In the Expectation-Maximization algorithm for clustering, the latent variables represent the cluster assignments, and the algorithm alternates between estimating the cluster assignments (E-step) and updating the cluster parameters (M-step). The convergence of the EM algorithm in this setting has been formally established (Dempster et al., 1977). Poland & Shachter (1993) showed that for a random variableX with a given distribution and the mixture of random variables Y that derive from it, the objective of minimizing the “relative entropy” between X and Y generalizes the objective of the Expectation Maximization algorithm: to maximize the likelihood of the observations x drawn from Y ’s distribution. In our setting, the iterative entropy minimization process corresponds to the Expectation Maximization algorithm, as iterative entropy minimization can also be seen as a form of “self-training” with minimization of the relative entropy (the DKL (Kullback & Leibler, 1951)) of the pseudo-labels (the model’s predictions) (Grandvalet & Bengio, 2004). The forward pass of our training process serves two purposes: (1) it sets the “observations”, which are the model’s predictions, and (2) it acts as the E-step of the algorithm, estimating the distribution given the model parameters (the clustering assignment). The backpropagation step, which updates the model parameters (the cluster parameters), serves as the M-step and maximizes the likelihood under the current pseudo-label estimates (Amini & Gallinari, 2002). It is important to note that in our setting, the entropy minimization procedure involves changing both X and Y in each iteration, which may be different from the original Expectation Maximization algorithm. Using these insights, we can provide a better explanation for the two-phase clustering phenomenon observed in our experiments. In the initial “success” phase, where the change in the embeddings is relatively small during the process, the entropy minimization effectively performs Expectation Maximization unsupervised clustering in the model’s embedding space, guided by the smart initialization provided by the pre-trained model. The E-step estimates the pseudo-labels based on the current embedding structure, while the M-step updates the model to refine the embeddings and increase intra-cluster similarity. This process leads to the formation of well-separated clusters, as reflected by the increasing Silhouette score. However, as the Expectation Maximization algorithm continues over many iterations in the “failure” phase or if there is bad initialization, it starts to overfit the model to the specific characteristics of the ”new” test data. Unlike the regular Expectation Maximization algorithm, in our case, the data distribution (the observations) changes over time, which leads to a drift in the embeddings away from the initialized representations learned from the training data. This overfitting effect, which might even converge to a global minimum, is captured by the increasing Shift distance between the test data embeddings and the training class embeddings. To support this explanation, we also provide visualizations of the prediction space to illustrate the clustering process and the eventual drift from the training embeddings. We used a mixture of Gaussians, and trained a GMM with the Expectation Maximization algorithm using maximum likelihood, where the means are initialized based on random samples. The covariance is used as the identity matrix, with the input samples being trainable and optimizing their location. In Figure 8, each dot represents a sample colored by its original class, where the Xs are the centroids at each iteration. As we can see, with the “smart initialization” of the cluster centers, the points converge to the “right” clusters based on the original cluster centers. However, when we start the cluster centers with some shift, namely there is “wrong” initialization, the clusters start with good clustering but then converge to wrong solutions where they mix points with different classes. 13The Entropy Enigma: Success and Failure of Entropy Minimization Figure 8.Top: With the “smart initialization” of the cluster centers, the points converge to the “right” clusters based on the original cluster centers. Middle: When we start the cluster centers with some shift, namely there is “wrong” initialization, the clusters start with good clustering but then converge to wrong solutions where they mix points with different classes. Bottom: For reference, we also show the regular Expectation Maximization algorithm on the shifted dataset. The X’s represent cluster centroids at each iteration. 14The Entropy Enigma: Success and Failure of Entropy Minimization B. Different Parameterizations of f In this section, we test the different ways of parameterizing the weighted-flips-to-accuracy function, f. Firstly, we look at the effects of not weighing each flip, and then we look at linear and cubic interpolations between flips and accuracy (as opposed to quadratic interpolation, used in the rest of the paper). Our results in Table 3 show that the optimalf is a weighted and interpolated quadratically, with the other variations not far behind. Importantly, all variations off perform better than the second best performing method, COT (Lu et al., 2023). Table 3.Mean Absolute Error between estimated accuracy, and true accuracy on a ResNet-50 model, for weighted and unweighted flips-to-accuracy functions, that are either linear, quadratic, or cubic interpolations of points. Datasets Unweighted Linear Unweighted Quadratic Weighted Linear Weighted Quadratic Weighted Cubic Noises IN-C {75} (Hendrycks & Dietterich, 2019) 4.95 5.04 5.94 4.79 5.23 IN-C {50} (Mintun et al., 2021) 7.19 7.36 7.94 7.35 7.01 IN-3DCC {60} (Kar et al., 2022) 4.10 4.12 4.33 3.66 4.25 CCC {27} (Press et al., 2023) 2.97 3.22 4.8 2.80 4.34 Domain Shifts Stylized (Geirhos et al., 2019) 7.12 7.12 7.12 3.81 7.12 IN-V2 {3} (Recht et al., 2019) 3.55 3.71 5.42 4.70 4.03 IN-Sketch (Wang et al., 2019) 1.11 1.32 2.64 4.23 0.23 IN-R (Hendrycks et al., 2021a) 1.43 1.67 3.01 1.88 0.52 IN-D (Rusak et al., 2022a) Real 3.39 3.16 2.04 3.18 4.70 Painting 2.07 1.94 0.34 2.20 0.85 Clipart 2.78 3.08 5.12 3.37 2.44 Sketch 6.12 6.95 12.89 5.44 12.38 Infograph 7.28 8.76 10.35 3.63 10.35 Quickdraw 0.79 0.79 0.79 2.57 0.79 Cartoon & Drawing {2} (Salvador & Oberman, 2022) 13.60 13.76 14.34 13.25 12.96 Adversarial Noises BG Challenge {8} (Xiao et al., 2020) 7.19 7.36 7.33 6.92 8.50 IN-A (Hendrycks et al., 2021b) 23.70 23.53 20.39 21.61 22.91 IN-C Patch {75} (Gu et al., 2022) 1.95 2.00 2.42 1.60 1.48 IN-Hard (Taesiri et al., 2023) 5.27 4.92 0.72 3.64 3.49 Patch-IN {10} (Pintor et al., 2023) 7.42 7.55 9.02 8.87 7.98 IN-Obfuscations {3} (Stimberg et al., 2023) 0.20 0.10 0.10 4.58 0.10 OOD/Other ObjectNet (Barbu et al., 2019) 6.81 6.81 6.81 2.74 6.81 NINCO (Bitterwolf et al., 2023) 20.20 19.85 14.98 18.07 17.73 Average 6.14 6.27 6.47 5.75 6.36 Worst Case 23.70 23.53 20.39 21.61 22.91 Average (Worst Case Excluded) 5.34 5.48 5.84 5.03 5.60 15The Entropy Enigma: Success and Failure of Entropy Minimization C. WF with Limited Data To further test WF’s ability in a challenging setting, we look at how it performs under memory and data constraints. To this end, we test WF in the following scenarios: (1) WF is only allowed to store 100 samples for calculating flips, and (2) when whole dataset is limited to 100 samples for flip calculation and 1,000 samples for adaptation). We note that previous work assumes the existence of at least 2,000 test samples (Niu et al., 2022). In both cases, we use the original weighted-flips-to-accuracy function, f, by multiplying the the weighted flips calculated on 100 samples by 10, and plugging the output into f. Even with only using 100 samples, WF is able to best the original implementation by a bit. Surprisingly, even with limited data and memory, WF manages to remain competitive with unconstrained methods, and is significantly ahead of COT, when it is constrained in a similar manner. Table 4.WF is effective in memory constrained settings. Without finetuning or refitting f, WF beats the original implementation, when only using 100 samples to calculate weighted flips (WF limited mem). In the limited memory/data setting, WF gets access to only 1000 samples in total, 100 of which are used for flip calculations. In this setting, COT gets access to 1,000 input samples and 1,000 in distribution samples. Best results are in bold; second best are underlined, {.} indicates how many splits are in each dataset, when there are more than 1. Datasets COT original WF original WF limited mem COT limited mem/data WF limited mem/data Noises IN-C {75} (Hendrycks & Dietterich, 2019) 2.23 4.79 7.52 36.67 6.52 IN-C {50} (Mintun et al., 2021) 3.17 7.35 8.34 40.55 4.60 IN-3DCC {60} (Kar et al., 2022) 3.02 3.66 3.97 34.44 4.31 CCC {27} (Press et al., 2023) 2.04 2.80 3.71 26.67 4.92 Domain Shifts Stylized (Geirhos et al., 2019) 12.18 3.81 3.37 38.84 2.50 IN-V2 {3} (Recht et al., 2019) 2.68 4.70 4.00 43.96 3.80 IN-Sketch (Wang et al., 2019) 4.23 1.71 1.68 12.46 3.39 IN-R (Hendrycks et al., 2021a) 2.44 1.88 3.03 14.99 12.03 IN-D (Rusak et al., 2022a) Real 27.54 3.18 1.73 41.52 6.51 Painting 7.49 2.12 0.71 26.21 18.44 Clipart 4.52 3.37 5.91 15.98 8.10 Sketch 0.71 5.44 6.30 12.65 4.50 Infograph 3.44 3.63 1.24 4.57 2.51 Quickdraw 1.60 2.57 2.80 0.06 2.46 Cartoon & Drawing {2} (Salvador & Oberman, 2022) 1.62 13.25 16.48 33.25 13.44 Adversarial Noises BG Challenge {8} (Xiao et al., 2020) 19.68 6.92 5.84 32.84 10.15 IN-A (Hendrycks et al., 2021b) 30.38 21.61 16.75 15.30 29.15 IN-C Patch {75} (Gu et al., 2022) 2.57 1.60 1.98 47.03 1.92 IN-Hard (Taesiri et al., 2023) 15.33 3.64 0.65 5.83 14.73 Patch-IN {10} (Pintor et al., 2023) 10.13 8.87 9.09 49.68 9.81 IN-Obfuscations {3} (Stimberg et al., 2023) 0.12 4.58 4.67 0.09 8.93 OOD/Other ObjectNet (Barbu et al., 2019) 10.40 2.74 0.29 2.44 2.74 NINCO (Bitterwolf et al., 2023) 20.28 18.07 20.24 13.05 35.68 Average 8.17 5.75 5.67 23.87 9.40 Worst Case 30.38 21.61 20.24 49.68 35.68 Average (Worst Case Excluded) 7.16 5.03 5.00 22.70 8.21 16The Entropy Enigma: Success and Failure of Entropy Minimization D. Weighted Flips Ablations D.1. Stopping Iteration Ablations WF measures the amount of weighted flips from iteration 0 to iteration 1,000. This is done because RDumb resets the model to its pretrained state every 1,000 iterations, in order to avoid collapse (Press et al., 2023). Here, we look at how measuring weighted flips before iteration 1,000 affects the performance of WF. Interestingly, using 500 iterations increases performance by a relative 26.89% as opposed to the 1,000 iterations used in the rest of the paper. Stopping Iteration Datasets 1000 500 250 100 50 IN-C 4.79 4.88 4.99 5.48 5.67 IN-C 7.35 7.48 7.84 8.96 10.10 IN-3DCC 3.66 3.32 3.37 3.00 3.06 IN-V2 4.70 4.59 4.93 5.11 5.49 IN-D Real 3.18 0.36 0.96 2.82 5.01 Painting 2.12 1.21 4.02 11.28 14.83 Clipart 3.37 0.31 3.75 9.30 14.85 Sketch 5.44 2.49 0.33 5.61 9.26 Infograph 3.63 3.46 0.09 4.37 7.53 Quickdraw 2.57 1.73 7.55 17.54 25.35 Average 4.08 2.98 3.78 7.35 10.12 Figure 9. Left: Mean Absolute Error between estimated accuracy and true accuracy, when measuring weighted flips between iteration 0 and various stopping iterations. Right: For different stopping iterations, interpolating between the points in the holdout set yields different weighted-flips-to-accuracy functions. D.2. Holdout Set Size Ablations Holdout Set Size Datasets 1000 500 250 100 50 IN-C 4.79 4.77 5.52 6.07 7.62 IN-C 7.35 5.91 6.74 7.06 8.40 IN-3DCC 3.66 5.10 4.34 5.17 5.22 IN-V2 4.70 5.13 5.48 3.47 9.10 IN-D Real 3.18 3.50 1.78 0.57 4.21 Painting 2.12 5.38 1.92 6.93 8.57 Clipart 3.37 6.69 7.96 8.79 7.70 Sketch 5.44 10.23 7.39 4.87 4.10 Infograph 3.63 6.15 0.88 3.05 3.77 Quickdraw 2.57 0.71 9.87 38.86 31.66 Average 4.08 5.36 5.19 8.48 9.04 Figure 10.Left: Mean Absolute Error between estimated accuracy and true accuracy, when measuring weighted flips on sets of images of different sizes. Right: For different holdout set sizes, interpolating between the points in the holdout set yields different weighted-flips-to- accuracy functions. f can only output values that are between 0 and 100. 17The Entropy Enigma: Success and Failure of Entropy Minimization E. WF with other TTA Methods WF esimates the accuracy of a dataset as RDumb (Press et al., 2023) is used to adapt to it. In this section, we show that WF work with a variety of different EM methods. To further showcase the versatility of WF, we do not finetune any method, and use the original weighted-flips-to-accuracy function f, for all experiments in Table 5. Table 5. Mean Absolute Error between estimated accuracy and true accuracy, when adapting to data using a ResNet-50 backbone and different TTA methods: Tent (Wang et al., 2020), RPL (Rusak et al., 2022b), and CPL (Goyal et al., 2022). In all cases, the original weighted-flips-to-accuracy function f is used, highlighting the versatility of WF. Datasets RDumb Tent RPL CPL IN-C 4.79 6.75 6.85 5.14 IN-C 7.35 9.68 7.20 7.44 IN-3DCC 3.66 2.92 3.99 3.72 IN-V2 4.70 3.80 3.82 4.42 IN-D Real 3.18 5.15 5.15 0.31 Painting 2.12 7.59 7.59 0.03 Clipart 3.37 6.98 7.11 2.57 Sketch 5.44 3.30 3.52 3.86 Infograph 3.63 2.29 2.24 3.40 Quickdraw 2.57 2.24 2.24 2.37 Average 4.08 5.07 4.97 3.33 F. Additional Vision Transformer Experiments To further analyze WF and the second best method, COT, we add additionally analysis using a ViT-B/16 model. The task is to estimate the accuracy of a ViT-B/16 on a variety of datasets. We compare between using the original weighted-flips-accuracy function, f, which was interpolated using data from a ResNet-50, and interpolating the function using ViT-B/16 data points. In both cases, the datasets used to interpolate are the same. Additionally, we compare to COT on this task. Table 6. Mean Absolute Error between estimated accuracy and true accuracy, when estimating the accuracy of a ViT-B/16 on different datasets. Datasets WF WF (new f) COT IN-C 8.34 1.64 22.24 IN-C 6.59 1.48 25.37 IN-3DCC 7.19 1.87 18.43 IN-V2 4.44 3.63 21.29 IN-D Real 1.02 7.27 37.21 Painting 3.02 7.06 19.13 Clipart 12.82 0.25 13.58 Sketch 11.04 1.90 5.74 Infograph 9.27 3.34 1.16 Quickdraw 2.36 13.04 0.28 Average 6.61 4.15 16.44 18The Entropy Enigma: Success and Failure of Entropy Minimization G. Omitting Samples by Top-k Accuracy/Entropy Level In addition to removing samples by Top-k accuracy, we also analyze the effects of removing samples according to their initial entropy level. We find that both experiments exhibit similar behaviour: it is possible to remove many Top- k/low entropy samples, without significantly affecting the accuracy gain of Tent (on a holdout set of Gaussian Noise 3). Figure 11.Left: Average entropy across top-k samples for different values of k. The percentages shown are the fraction of images out of the whole dataset. The original dataset, Gaussian Noise 3, has an average entropy of 2.84. Right: The relative size of the datasets, when top-k samples are removed. Figure 12.Left: Accuracy gain per iteration on a holdout set, as Tent adapts to its inputs. Each line corresponds to a different experiment where we remove samples based on their initial entropy level. Similarly to Figure 2, it’s possible to remove low entropy samples while barely hurting performance. When entropy ≤ 0, no images are excluded. Right: The relative size of the datasets and their average entropy, when samples with a entropy level≤ k are removed. 19The Entropy Enigma: Success and Failure of Entropy Minimization H. Silhouette score, Shift distance, and Accuracy Throughout Entropy Minimization In Figure 4, we looked at the changes of Silhouette scores/Shift distances for each phase in EM. Here, we show how these scores, along with accuracy, change in every iteration of Tent. For each one of the datasets analyzed, we group noises based on severity level, and plot their averages and standard deviations, for every iteration. Figure 13.Changes in Silhouette scores, Shift distances, and Accuracies as Tent adapts to its inputs. We group together noises by severity level, and average the data for every iteration. 20The Entropy Enigma: Success and Failure of Entropy Minimization I. WF on CIFAR10/100 WF is not as effecitve on CIFAR10 (Krizhevsky et al., 2009) as it is on ImageNet (Deng et al., 2009) scale datasets. CIFAR10 is an outlier in entropy minimization: for example, Press et al. (2023) showed that Tent doesn’t degrade in accuracy, even after 100 million CIFAR10 images seen. We nonetheless run our method on CIFAR10. On average, we see only 0-5 label flips per dataset on C10-C. This is far from what we see ImageNet-scale datasets we tested. Like in the paper, we interpolate a weighted-flips-to-accuracy function f on the holdout set and get: f(x) = −249.36x2 − 87.39x + 77.01 which has a MAE of 16.64 on the C10 validation set. We repeat this for CIFAR100 and get: f(x) = 0.000322x2 − 0.287x + 99.54 which has a MAE of 9.10 on the C100 validation set. Apart from refitting f, we did not tune any other parameter in these two experiments. J. RDumb WF uses RDumb (Press et al., 2023) to estimate accuracy. We go over the implementation of the method in brief. RDumb is based on ETA (Niu et al., 2022), wherein the model is reset to its pretrained state every 1,000 iterations. Rdumb optimizes the BatchNorm (Ioffe & Szegedy, 2015) parameters, Θ of a given classifier f. The loss optimized is entropy, with two filtration steps: the first, in which samples with high entropy are filtered out, and the second, in which samples that produce logits similar to previous samples are filtered out. For a sample x, the first filtration is given by: Sent(x) = 1 exp[E(x; Θ)− E0] · IE(x;Θ)<E0 (x), with E0 = 0.4 × ln103. The second filtration is given by: Sdiv(x) = I{cos(fo(x),m−1)<ϵ}(x) where cos() is the cosine similarity, and mt is an exponential moving average of the logits of previously seen samples at iteration t: mt = ( y1, if t = 1 αyt + (1 − α)mt−1, if t >1 and yt is the average model prediction on a batch of inputs at step t, and α = 0.9. Put together with entropy minimization, the optimization formula becomes: min ˆΘ −Sent(x) · Sdiv(x) X y∈C fΘ(y|x) logfΘ(y|x) RDumb uses a SGD with a learning rate of 2.5 × 10−4, and a batch size of 64, and is reset to its pre-trained state every 1,000 iterations. 21The Entropy Enigma: Success and Failure of Entropy Minimization K. Software Licenses • ImageNet-C (Hendrycks & Dietterich, 2019)Apache License 2.0 https://github.com/hendrycks/robustness • ImageNet-R (Hendrycks et al., 2021a) MIT License https://github.com/hendrycks/imagenet-r • ImageNet-3D-CC (Kar et al., 2022): CC-BY-NC 4.0 License https://github.com/EPFL-VILAB/3DCommonCorruptions • ImageNet- C (Mintun et al., 2021): MIT License https://github.com/facebookresearch/augmentation-corruption • ImageNet-V2 (Recht et al., 2019): MIT License https://github.com/modestyachts/ImageNetV2 • Backgrounds Challenge (Xiao et al., 2020): https://github.com/MadryLab/backgrounds_challenge • CCC (Press et al., 2023): MIT License https://github.com/oripress/CCC • Stylized ImageNet (Geirhos et al., 2019): MIT License https://github.com/rgeirhos/Stylized-ImageNet • NINCO (Bitterwolf et al., 2023): MIT License https://github.com/j-cb/NINCO • ImageNet-D (Rusak et al., 2022a): Apache License 2.0 https://github.com/bethgelab/robustness • ObjectNet (Barbu et al., 2019): MIT License https://objectnet.dev/ • Shift Happens Benchmark: Apache License 2.0 https://github.com/shift-happens-benchmark/ icml-2022 22",
      "meta_data": {
        "arxiv_id": "2405.05012v2",
        "authors": [
          "Ori Press",
          "Ravid Shwartz-Ziv",
          "Yann LeCun",
          "Matthias Bethge"
        ],
        "published_date": "2024-05-08T12:26:15Z",
        "pdf_url": "https://arxiv.org/pdf/2405.05012v2.pdf"
      }
    },
    {
      "title": "Tent: Fully Test-Time Adaptation by Entropy Minimization",
      "abstract": "A model must adapt itself to generalize to new and different data during\ntesting. In this setting of fully test-time adaptation the model has only the\ntest data and its own parameters. We propose to adapt by test entropy\nminimization (tent): we optimize the model for confidence as measured by the\nentropy of its predictions. Our method estimates normalization statistics and\noptimizes channel-wise affine transformations to update online on each batch.\nTent reduces generalization error for image classification on corrupted\nImageNet and CIFAR-10/100 and reaches a new state-of-the-art error on\nImageNet-C. Tent handles source-free domain adaptation on digit recognition\nfrom SVHN to MNIST/MNIST-M/USPS, on semantic segmentation from GTA to\nCityscapes, and on the VisDA-C benchmark. These results are achieved in one\nepoch of test-time optimization without altering training.",
      "full_text": "Published as a conference paper at ICLR 2021 TENT : F ULLY TEST-TIME ADAPTATION BY ENTROPY MINIMIZATION Dequan Wang1∗, Evan Shelhamer2∗†, Shaoteng Liu1, Bruno Olshausen1, Trevor Darrell1 dqwang@cs.berkeley.edu, shelhamer@google.com UC Berkeley1 Adobe Research2 ABSTRACT A model must adapt itself to generalize to new and different data during testing. In this setting of fully test-time adaptation the model has only the test data and its own parameters. We propose to adapt by test entropy minimization (tent 1): we optimize the model for conﬁdence as measured by the entropy of its predictions. Our method estimates normalization statistics and optimizes channel-wise afﬁne transformations to update online on each batch. Tent reduces generalization error for image classiﬁcation on corrupted ImageNet and CIFAR-10/100 and reaches a new state-of-the-art error on ImageNet-C. Tent handles source-free domain adapta- tion on digit recognition from SVHN to MNIST/MNIST-M/USPS, on semantic segmentation from GTA to Cityscapes, and on the VisDA-C benchmark. These results are achieved in one epoch of test-time optimization without altering training. 1 I NTRODUCTION Deep networks can achieve high accuracy on training and testing data from the same distribution, as evidenced by tremendous benchmark progress (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; He et al., 2016). However, generalization to new and different data is limited (Hendrycks & Dietterich, 2019; Recht et al., 2019; Geirhos et al., 2018). Accuracy suffers when the training (source) data differ from the testing (target) data, a condition known as dataset shift(Quionero-Candela et al., 2009). Models can be sensitive to shifts during testing that were not known during training, whether natural variations or corruptions, such as unexpected weather or sensor degradation. Nevertheless, it can be necessary to deploy a model on different data distributions, so adaptation is needed. During testing, the model must adapt given only its parameters and the target data. Thisfully test-time adaptation setting cannot rely on source data or supervision. Neither is practical when the model ﬁrst encounters new testing data, before it can be collected and annotated, as inference must go on. Real-world usage motivates fully test-time adaptation by data, computation, and task needs: 1. Availability. A model might be distributed without source data for bandwidth, privacy, or proﬁt. 2. Efﬁciency. It might not be computationally practical to (re-)process source data during testing. 3. Accuracy. A model might be too inaccurate without adaptation to serve its purpose. To adapt during testing we minimize the entropy of model predictions. We call this objective the test entropy and name our method tent after it. We choose entropy for its connections to error and shift. Entropy is related to error, as more conﬁdent predictions are all-in-all more correct (Figure 1). Entropy is related to shifts due to corruption, as more corruption results in more entropy, with a strong rank correlation to the loss for image classiﬁcation as the level of corruption increases (Figure 2). To minimize entropy, tent normalizes and transforms inference on target data by estimating statistics and optimizing afﬁne parameters batch-by-batch. This choice of low-dimensional, channel-wise feature modulation is efﬁcient to adapt during testing, even for online updates. Tent does not restrict or alter model training: it is independent of the source data given the model parameters. If the model can be run, it can be adapted. Most importantly, tent effectively reduces not just entropy but error. ∗Equal contribution. †Work done at Adobe Research; the author is now at DeepMind. 1Please see the project page at https://github.com/DequanWang/tent for the code and more. 1 arXiv:2006.10726v3  [cs.LG]  18 Mar 2021Published as a conference paper at ICLR 2021 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Entropy 0 20 40 60 80Error (%) Figure 1: Predictions with lower entropy have lower error rates on corrupted CIFAR-100-C. Certainty can serve as supervision during testing. 0.2 0.3 0.4 0.5 0.6 Entropy 0.2 0.4 0.6 0.8 1.0 1.2Loss = 0.61 original noise blur digital weather  level level Figure 2: More corruption causes more loss and entropy on CIFAR-100-C. Entropy can estimate the degree of shift without training data or labels. Our results evaluate generalization to corruptions for image classiﬁcation, to domain shift for digit recognition, and to simulation-to-real shift for semantic segmentation. For context with more data and optimization, we evaluate methods for robust training, domain adaptation, and self-supervised learning given the labeled source data. Tent can achieve less error given only the target data, and it improves on the state-of-the-art for the ImageNet-C benchmark. Analysis experiments support our entropy objective, check sensitivity to the amount of data and the choice of parameters for adaptation, and back the generality of tent across architectures. Our contributions • We highlight the setting of fully test-time adaptation with only target data and no source data. To emphasize practical adaptation during inference we benchmark with ofﬂine and online updates. • We examine entropy as an adaptation objective and propose tent: a test-time entropy minimization scheme to reduce generalization error by reducing the entropy of model predictions on test data. • For robustness to corruptions, tent reaches 44.0% error on ImageNet-C, better than the state-of- the-art for robust training (50.2%) and the strong baseline of test-time normalization (49.9%). • For domain adaptation, tent is capable of online and source-free adaptation for digit classiﬁcation and semantic segmentation, and can even rival methods that use source data and more optimization. 2 S ETTING : F ULLY TEST-TIME ADAPTATION Adaptation addresses generalization from source to target. A model fθ(x) with parameters θtrained on source data and labels xs,ys may not generalize when tested on shifted target data xt. Table 1 summarizes adaptation settings, their required data, and types of losses. Our fully test-time adaptation setting uniquely requires only the model fθ and unlabeled target data xt for adaptation during inference. Existing adaptation settings extend training given more data and supervision. Transfer learning by ﬁne-tuning (Donahue et al., 2014; Yosinski et al., 2014) needs target labels to (re-)train with a supervised loss L(xt,yt). Without target labels, our setting denies this supervised training. Domain adaptation (DA) (Quionero-Candela et al., 2009; Saenko et al., 2010; Ganin & Lempitsky, 2015; Tzeng et al., 2015) needs both the source and target data to train with a cross-domain loss L(xs,xt). Test-time training (TTT) (Sun et al., 2019b) adapts during testing but ﬁrst alters training to jointly optimize its supervised loss L(xs,ys) and self-supervised loss L(xs). Without source, our setting denies joint training across domains (DA) or losses (TTT). Existing settings have their purposes, but do not cover all practical cases when source, target, or supervision are not simultaneously available. Unexpected target data during testing requires test-time adaptation. TTT and our setting adapt the model by optimizing an unsupervised loss during testing L(xt). During training, TTT jointly optimizes this same loss on source data L(xs) with a supervised loss L(xs,ys), to ensure the parameters θare shared across losses for compatibility with adaptation by L(xt). Fully test-time adaptation is independent of the training data and training loss given the parameters θ. By not changing training, our setting has the potential to require less data and computation for adaptation. 2Published as a conference paper at ICLR 2021 Table 1: Adaptation settings differ by their data and therefore losses during training and testing. Of the source s and target t data xand labels y, our fully test-time setting only needs the target data xt. setting source data target data train loss test loss ﬁne-tuning - xt,yt L(xt,yt) - domain adaptation xs, ys xt L(xs,ys) + L(xs,xt) - test-time training xs, ys xt L(xs,ys) + L(xs) L(xt) fully test-time adaptation - xt - L(xt)     = f (     ; θ)  Loss (   ,      ) θ (a) training <latexit sha1_base64=\"YDIW6Mi/jc4gnv843zXedRTilJU=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49mF654lbdGcgy8XJSgRyNXvmr249ZGnGFTFJjOp6boJ9RjYJJPil1U8MTykZ0wDuWKhpx42ezUyfkxCp9EsbalkIyU39PZDQyZhwFtjOiODSL3lT8z+ukGF74mVBJilyx+aIwlQRjMv2b9IXmDOXYEsq0sLcSNqSaMrTplGwI3uLLy6R1VvVqVc+7qVXql3keRTiCYzgFD86hDtfQgCYwGMAzvMKbI50X5935mLcWnHzmEP7A+fwBBSaOGg==</latexit> x s <latexit sha1_base64=\"1psCF/1OyjuZ4TwBu21voNG0XaI=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8eK1hbaWDbbSbt0swm7GyGE/gQvHhTEq3/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dkorq2vrG+XNytb2zu5edf/gQcepYthisYhVJ6AaBZfYMtwI7CQKaRQIbAfj66nffkKleSzvTZagH9Gh5CFn1FjpLnvU/WrNrbszkGXiFaQGBZr96ldvELM0QmmYoFp3PTcxfk6V4UzgpNJLNSaUjekQu5ZKGqH289mpE3JilQEJY2VLGjJTf0/kNNI6iwLbGVEz0oveVPzP66YmvPRzLpPUoGTzRWEqiInJ9G8y4AqZEZkllClubyVsRBVlxqZTsSF4iy8vk/ZZ3Tuve97tea1xVeRRhiM4hlPw4AIacANNaAGDITzDK7w5wnlx3p2PeWvJKWYO4Q+czx8GrY4b</latexit> y s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"1psCF/1OyjuZ4TwBu21voNG0XaI=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8eK1hbaWDbbSbt0swm7GyGE/gQvHhTEq3/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dkorq2vrG+XNytb2zu5edf/gQcepYthisYhVJ6AaBZfYMtwI7CQKaRQIbAfj66nffkKleSzvTZagH9Gh5CFn1FjpLnvU/WrNrbszkGXiFaQGBZr96ldvELM0QmmYoFp3PTcxfk6V4UzgpNJLNSaUjekQu5ZKGqH289mpE3JilQEJY2VLGjJTf0/kNNI6iwLbGVEz0oveVPzP66YmvPRzLpPUoGTzRWEqiInJ9G8y4AqZEZkllClubyVsRBVlxqZTsSF4iy8vk/ZZ3Tuve97tea1xVeRRhiM4hlPw4AIacANNaAGDITzDK7w5wnlx3p2PeWvJKWYO4Q+czx8GrY4b</latexit> y s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"YDIW6Mi/jc4gnv843zXedRTilJU=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49mF654lbdGcgy8XJSgRyNXvmr249ZGnGFTFJjOp6boJ9RjYJJPil1U8MTykZ0wDuWKhpx42ezUyfkxCp9EsbalkIyU39PZDQyZhwFtjOiODSL3lT8z+ukGF74mVBJilyx+aIwlQRjMv2b9IXmDOXYEsq0sLcSNqSaMrTplGwI3uLLy6R1VvVqVc+7qVXql3keRTiCYzgFD86hDtfQgCYwGMAzvMKbI50X5935mLcWnHzmEP7A+fwBBSaOGg==</latexit> x s (b) fully test-time adaptation θ    = f (    ; θ+Δ)  Entropy (    ) <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t <latexit sha1_base64=\"m/ZzdjACtPk7VVv8qMUMxHkn5f0=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49YK9ccavuDGSZeDmpQI5Gr/zV7ccsjbhCJqkxHc9N0M+oRsEkn5S6qeEJZSM64B1LFY248bPZqRNyYpU+CWNtSyGZqb8nMhoZM44C2xlRHJpFbyr+53VSDC/8TKgkRa7YfFGYSoIxmf5N+kJzhnJsCWVa2FsJG1JNGdp0SjYEb/HlZdI6q3q1qufd1Cr1yzyPIhzBMZyCB+dQh2toQBMYDOAZXuHNkc6L8+58zFsLTj5zCH/gfP4ABquOGw==</latexit> x t <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t <latexit sha1_base64=\"m/ZzdjACtPk7VVv8qMUMxHkn5f0=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49YK9ccavuDGSZeDmpQI5Gr/zV7ccsjbhCJqkxHc9N0M+oRsEkn5S6qeEJZSM64B1LFY248bPZqRNyYpU+CWNtSyGZqb8nMhoZM44C2xlRHJpFbyr+53VSDC/8TKgkRa7YfFGYSoIxmf5N+kJzhnJsCWVa2FsJG1JNGdp0SjYEb/HlZdI6q3q1qufd1Cr1yzyPIhzBMZyCB+dQh2toQBMYDOAZXuHNkc6L8+58zFsLTj5zCH/gfP4ABquOGw==</latexit> x t <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t     = f (     ; θ)  Loss (   ,      ) θ (a) training <latexit sha1_base64=\"YDIW6Mi/jc4gnv843zXedRTilJU=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49mF654lbdGcgy8XJSgRyNXvmr249ZGnGFTFJjOp6boJ9RjYJJPil1U8MTykZ0wDuWKhpx42ezUyfkxCp9EsbalkIyU39PZDQyZhwFtjOiODSL3lT8z+ukGF74mVBJilyx+aIwlQRjMv2b9IXmDOXYEsq0sLcSNqSaMrTplGwI3uLLy6R1VvVqVc+7qVXql3keRTiCYzgFD86hDtfQgCYwGMAzvMKbI50X5935mLcWnHzmEP7A+fwBBSaOGg==</latexit> x s <latexit sha1_base64=\"1psCF/1OyjuZ4TwBu21voNG0XaI=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8eK1hbaWDbbSbt0swm7GyGE/gQvHhTEq3/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dkorq2vrG+XNytb2zu5edf/gQcepYthisYhVJ6AaBZfYMtwI7CQKaRQIbAfj66nffkKleSzvTZagH9Gh5CFn1FjpLnvU/WrNrbszkGXiFaQGBZr96ldvELM0QmmYoFp3PTcxfk6V4UzgpNJLNSaUjekQu5ZKGqH289mpE3JilQEJY2VLGjJTf0/kNNI6iwLbGVEz0oveVPzP66YmvPRzLpPUoGTzRWEqiInJ9G8y4AqZEZkllClubyVsRBVlxqZTsSF4iy8vk/ZZ3Tuve97tea1xVeRRhiM4hlPw4AIacANNaAGDITzDK7w5wnlx3p2PeWvJKWYO4Q+czx8GrY4b</latexit> y s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"1psCF/1OyjuZ4TwBu21voNG0XaI=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8eK1hbaWDbbSbt0swm7GyGE/gQvHhTEq3/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dkorq2vrG+XNytb2zu5edf/gQcepYthisYhVJ6AaBZfYMtwI7CQKaRQIbAfj66nffkKleSzvTZagH9Gh5CFn1FjpLnvU/WrNrbszkGXiFaQGBZr96ldvELM0QmmYoFp3PTcxfk6V4UzgpNJLNSaUjekQu5ZKGqH289mpE3JilQEJY2VLGjJTf0/kNNI6iwLbGVEz0oveVPzP66YmvPRzLpPUoGTzRWEqiInJ9G8y4AqZEZkllClubyVsRBVlxqZTsSF4iy8vk/ZZ3Tuve97tea1xVeRRhiM4hlPw4AIacANNaAGDITzDK7w5wnlx3p2PeWvJKWYO4Q+czx8GrY4b</latexit> y s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"YDIW6Mi/jc4gnv843zXedRTilJU=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49mF654lbdGcgy8XJSgRyNXvmr249ZGnGFTFJjOp6boJ9RjYJJPil1U8MTykZ0wDuWKhpx42ezUyfkxCp9EsbalkIyU39PZDQyZhwFtjOiODSL3lT8z+ukGF74mVBJilyx+aIwlQRjMv2b9IXmDOXYEsq0sLcSNqSaMrTplGwI3uLLy6R1VvVqVc+7qVXql3keRTiCYzgFD86hDtfQgCYwGMAzvMKbI50X5935mLcWnHzmEP7A+fwBBSaOGg==</latexit> x s (b) fully test-time adaptation θ    = f (    ; θ+Δ)  Entropy (    ) <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t <latexit sha1_base64=\"m/ZzdjACtPk7VVv8qMUMxHkn5f0=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49YK9ccavuDGSZeDmpQI5Gr/zV7ccsjbhCJqkxHc9N0M+oRsEkn5S6qeEJZSM64B1LFY248bPZqRNyYpU+CWNtSyGZqb8nMhoZM44C2xlRHJpFbyr+53VSDC/8TKgkRa7YfFGYSoIxmf5N+kJzhnJsCWVa2FsJG1JNGdp0SjYEb/HlZdI6q3q1qufd1Cr1yzyPIhzBMZyCB+dQh2toQBMYDOAZXuHNkc6L8+58zFsLTj5zCH/gfP4ABquOGw==</latexit> x t <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t <latexit sha1_base64=\"m/ZzdjACtPk7VVv8qMUMxHkn5f0=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49YK9ccavuDGSZeDmpQI5Gr/zV7ccsjbhCJqkxHc9N0M+oRsEkn5S6qeEJZSM64B1LFY248bPZqRNyYpU+CWNtSyGZqb8nMhoZM44C2xlRHJpFbyr+53VSDC/8TKgkRa7YfFGYSoIxmf5N+kJzhnJsCWVa2FsJG1JNGdp0SjYEb/HlZdI6q3q1qufd1Cr1yzyPIhzBMZyCB+dQh2toQBMYDOAZXuHNkc6L8+58zFsLTj5zCH/gfP4ABquOGw==</latexit> x t <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t Figure 3: Method overview. Tent does not alter training (a), but minimizes the entropy of predictions during testing (b) over a constrained modulation ∆, given the parameters θand target data xt. 3 M ETHOD : T EST ENTROPY MINIMIZATION VIA FEATURE MODULATION We optimize the model during testing to minimize the entropy of its predictions by modulating its features. We call our method tent for test entropy. Tent requires a compatible model, an objective to minimize (Section 3.1), and parameters to optimize over (Section 3.2) to fully deﬁne the algorithm (Section Section 3.3). Figure 3 outlines our method for fully test-time adaptation. The model to be adapted must be trained for the supervised task, probabilistic, and differentiable. No supervision is provided during testing, so the model must already be trained. Measuring the entropy of predictions requires a distribution over predictions, so the model must be probabilistic. Gradients are required for fast iterative optimization, so the model must be differentiable. Typical deep networks for supervised learning satisfy these model requirements. 3.1 E NTROPY OBJECTIVE Our test-time objective L(xt) is to minimize the entropy H(ˆy) of model predictions ˆy= fθ(xt). In particular, we measure the Shannon entropy (Shannon, 1948), H(ˆy) = −∑ cp(ˆyc) logp(ˆyc) for the probability ˆyc of class c. Note that optimizing a single prediction has a trivial solution: assign all probability to the most probable class. We prevent this by jointly optimizing batched predictions over parameters that are shared across the batch. Entropy is an unsupervised objective because it only depends on predictions and not annotations. However, as a measure of the predictions it is directly related to the supervised task and model. In contrast, proxy tasks for self-supervised learning are not directly related to the supervised task. Proxy tasks derive a self-supervised label y′from the input xt without the task label y. Examples of these proxies include rotation prediction (Gidaris et al., 2018), context prediction (Doersch et al., 2015), and cross-channel auto-encoding (Zhang et al., 2017). Too much progress on a proxy task could interfere with performance on the supervised task, and self-supervised adaptation methods have to limit or mix updates accordingly (Sun et al., 2019b;a). As such, care is needed to choose a proxy compatible with the domain and task, to design the architecture for the proxy model, and to balance optimization between the task and proxy objectives. Our entropy objective does not need such efforts. 3.2 M ODULATION PARAMETERS The model parameters θare a natural choice for test-time optimization, and these are the choice of prior work for train-time entropy minimization (Grandvalet & Bengio, 2005; Dhillon et al., 2020; Carlucci et al., 2017). However, θis the only representation of the training/source data in our setting, and altering θcould cause the model to diverge from its training. Furthermore, f can be nonlinear and θcan be high dimensional, making optimization too sensitive and inefﬁcient for test-time usage. 3Published as a conference paper at ICLR 2021 IN OUT+ <latexit sha1_base64=\"FGMSn1olAms3UkJ+mUM6lRBkJrw=\">AAAB6HicbVDLSgNBEOyNryS+oh69DAZBEMKuKHoMevGYgHlgsoTZSW8yZvbBzKwYlnyBFw+K5OoP+C/e/BqdJB40saChqOqmu8uLBVfatj+tzNLyyupaNpdf39jc2i7s7NZVlEiGNRaJSDY9qlDwEGuaa4HNWCINPIENb3A18Rv3KBWPwhs9jNENaC/kPmdUG6l63CkU7ZI9BVkkzg8plnPx+Pb94avSKXy0uxFLAgw1E1SplmPH2k2p1JwJHOXbicKYsgHtYcvQkAao3HR66IgcGqVL/EiaCjWZqr8nUhooNQw80xlQ3Vfz3kT8z2sl2r9wUx7GicaQzRb5iSA6IpOvSZdLZFoMDaFMcnMrYX0qKdMmm7wJwZl/eZHUT0rOaemsatK4hBmysA8HcAQOnEMZrqECNWCA8AjP8GLdWU/WqzWetWasn5k9+APr7RuTUJCF</latexit> \u0000 <latexit sha1_base64=\"8eHH7cr25vA7s0zJYYCDPQNSaT0=\">AAAB7XicbVDLSgNBEOyNrxhfUY+KDAbBU9gVQb0FvXhMwDwgWcLsZDYZM7OzzMwKYcnRuxcPinj1F/Id3vwGf8LJ46CJBQ1FVTfdXUHMmTau++VklpZXVtey67mNza3tnfzuXk3LRBFaJZJL1QiwppxFtGqY4bQRK4pFwGk96N+M/foDVZrJ6M4MYuoL3I1YyAg2Vqq1ulgI3M4X3KI7AVok3owUSoejyvfj0ajczn+2OpIkgkaGcKx103Nj46dYGUY4HeZaiaYxJn3cpU1LIyyo9tPJtUN0YpUOCqWyFRk0UX9PpFhoPRCB7RTY9PS8Nxb/85qJCS/9lEVxYmhEpovChCMj0fh11GGKEsMHlmCimL0VkR5WmBgbUM6G4M2/vEhqZ0XvvHhVsWlcwxRZOIBjOAUPLqAEt1CGKhC4hyd4gVdHOs/Om/M+bc04s5l9+APn4wd3ypLI</latexit> ⇥ <latexit sha1_base64=\"r9CoIRh1LwyAxszWUWZZpZEIYvU=\">AAAB7XicbVA9TwJBEJ3DL8Av1NLmIjGxIndGoyXRxhIT+YhwIXvLHqzs7V5254yE8B9sLDDG1tL/Yuev0QUsFHzJJC/vzWRmXpgIbtDzPp3M0vLK6lo2l1/f2NzaLuzs1oxKNWVVqoTSjZAYJrhkVeQoWCPRjMShYPWwfznx6/dMG67kDQ4SFsSkK3nEKUEr1VrIY2bahaJX8qZwF4n/Q4rlXDK+fX/4qrQLH62OomnMJFJBjGn6XoLBkGjkVLBRvpUalhDaJ13WtFQSuyQYTq8duYdW6biR0rYkulP198SQxMYM4tB2xgR7Zt6biP95zRSj82DIZZIik3S2KEqFi8qdvO52uGYUxcASQjW3t7q0RzShaAPK2xD8+ZcXSe245J+UTq9tGhcwQxb24QCOwIczKMMVVKAKFO7gEcbw7CjnyXlxXmetGednZg/+wHn7Btf2kwo=</latexit> \u0000 <latexit sha1_base64=\"icKTvSnYuWAwxCN4MXaVcPxJrUE=\">AAAB7HicbVBNS8NAEN34WetX1aMiwSJ4KokI6q3oxWMLpi20oWy2k3bpZhN2J0IJPXr24kERr/6G/g5v/gb/hNuPg7Y+GHi8N8PMvCARXKPjfFlLyyura+u5jfzm1vbObmFvv6bjVDHwWCxi1QioBsEleMhRQCNRQKNAQD3o3479+gMozWN5j4ME/Ih2JQ85o2gkrxUA0nah6JScCexF4s5IsXw0qn4/Ho8q7cJnqxOzNAKJTFCtm66ToJ9RhZwJGOZbqYaEsj7tQtNQSSPQfjY5dmifGqVjh7EyJdGeqL8nMhppPYgC0xlR7Ol5byz+5zVTDK/8jMskRZBsuihMhY2xPf7c7nAFDMXAEMoUN7farEcVZWjyyZsQ3PmXF0ntvORelK6rJo0bMkWOHJITckZccknK5I5UiEcY4eSJvJBXS1rP1pv1Pm1dsmYzB+QPrI8ftLWSVw==</latexit> \u0000 <latexit sha1_base64=\"6pSYsGji0D9Bm0vY9by0e43+pZo=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgxbArAfUW9OIxAfOAZAmzk95kzOzsMjMrhJAv8OJBEa9+kjf/xkmyB00saCiquunuChLBtXHdbye3tr6xuZXfLuzs7u0fFA+PmjpOFcMGi0Ws2gHVKLjEhuFGYDtRSKNAYCsY3c381hMqzWP5YMYJ+hEdSB5yRo2V6he9Ysktu3OQVeJlpAQZar3iV7cfszRCaZigWnc8NzH+hCrDmcBpoZtqTCgb0QF2LJU0Qu1P5odOyZlV+iSMlS1pyFz9PTGhkdbjKLCdETVDvezNxP+8TmrCa3/CZZIalGyxKEwFMTGZfU36XCEzYmwJZYrbWwkbUkWZsdkUbAje8surpHlZ9irlm3qlVL3N4sjDCZzCOXhwBVW4hxo0gAHCM7zCm/PovDjvzseiNedkM8fwB87nD3htjL0=</latexit> ÷ <latexit sha1_base64=\"KLNiQjydwC+UjsLtIanox9T+rq8=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoN6KXjxWsB/QhrLZbNqlu5uwuymU0L/gxYMiXv1D3vw3btoctPXBwOO9GWbmBQln2rjut1Pa2Nza3invVvb2Dw6PqscnHR2nitA2iXmsegHWlDNJ24YZTnuJolgEnHaDyX3ud6dUaRbLJzNLqC/wSLKIEWxyaRCy6bBac+vuAmideAWpQYHWsPo1CGOSCioN4Vjrvucmxs+wMoxwOq8MUk0TTCZ4RPuWSiyo9rPFrXN0YZUQRbGyJQ1aqL8nMiy0nonAdgpsxnrVy8X/vH5qohs/YzJJDZVkuShKOTIxyh9HIVOUGD6zBBPF7K2IjLHCxNh4KjYEb/XlddK5qnuN+u1jo9a8K+IowxmcwyV4cA1NeIAWtIHAGJ7hFd4c4bw4787HsrXkFDOn8AfO5w8aWY5N</latexit> µ <latexit sha1_base64=\"lbHwl5bkUbenc+Yo+u8yNzpxsy0=\">AAAB6nicbVDLSgNBEOyNrxhfUY+KDAbBU9gVQb0FvXhM0DwgWcLsZDYZMjO7zMwKYcnRoxcPinj1I/Id3vwGf8LJ46CJBQ1FVTfdXUHMmTau++VklpZXVtey67mNza3tnfzuXk1HiSK0SiIeqUaANeVM0qphhtNGrCgWAaf1oH8z9usPVGkWyXsziKkvcFeykBFsrHTXEkk7X3CL7gRokXgzUigdjirfj0ejcjv/2epEJBFUGsKx1k3PjY2fYmUY4XSYayWaxpj0cZc2LZVYUO2nk1OH6MQqHRRGypY0aKL+nkix0HogAtspsOnpeW8s/uc1ExNe+imTcWKoJNNFYcKRidD4b9RhihLDB5Zgopi9FZEeVpgYm07OhuDNv7xIamdF77x4VbFpXMMUWTiAYzgFDy6gBLdQhioQ6MITvMCrw51n5815n7ZmnNnMPvyB8/EDTj2RiQ==</latexit> \u0000 <latexit sha1_base64=\"xnrzB72KzfqBMQ17s1zlsxQWR+k=\">AAAB7XicbZDLSgMxFIbP1Fsdb1WXboJFcFVmRFAXYtGNywr2Au1QMmmmjU0yQ5IRytB3cONCETcufBT3bsS3Mb0stPWHwMf/n0POOWHCmTae9+3kFhaXllfyq+7a+sbmVmF7p6bjVBFaJTGPVSPEmnImadUww2kjURSLkNN62L8a5fV7qjSL5a0ZJDQQuCtZxAg21qq1NOsK3C4UvZI3FpoHfwrFiw/3PHn7civtwmerE5NUUGkIx1o3fS8xQYaVYYTTodtKNU0w6eMubVqUWFAdZONph+jAOh0Uxco+adDY/d2RYaH1QIS2UmDT07PZyPwva6YmOg0yJpPUUEkmH0UpRyZGo9VRhylKDB9YwEQxOysiPawwMfZArj2CP7vyPNSOSv5x6ezGK5YvYaI87ME+HIIPJ1CGa6hAFQjcwQM8wbMTO4/Oi/M6Kc05055d+CPn/Qf/xpJs</latexit> <latexit sha1_base64=\"9MzbukliF0G5U4WyINCTJmMNjA8=\">AAACNnicdVBNS8NAFNz4bf2KevSyWAQFLUlR9CiK4EWoYFuhiWWz3dSlu0nYfVFL6K/y4u/w1osHRbz6E9y0PWjVgYVhZh773gSJ4Bocp29NTE5Nz8zOzRcWFpeWV+zVtZqOU0VZlcYiVtcB0UzwiFWBg2DXiWJEBoLVg85p7tfvmNI8jq6gmzBfknbEQ04JGKlpX3gyxZ5gIRCl4nvsSQK3QZCd9RoPTfB3sad5W5Kb8j+h7Xx+D5vszk3Zb9pFp+QMgH8Td0SKaIRK0372WjFNJYuACqJ1w3US8DOigFPBegUv1SwhtEParGFoRCTTfjY4u4e3jNLCYazMiwAP1O8TGZFad2VgkvnCetzLxb+8RgrhkZ/xKEmBRXT4UZgKDDHOO8QtrhgF0TWEUMXNrpjeEkUomKYLpgR3/OTfpFYuuQcl53K/eHwyqmMObaBNtI1cdIiO0TmqoCqi6BH10St6s56sF+vd+hhGJ6zRzDr6AevzC4nRq7w=</latexit> µ  E [ x t ] , \u0000 2  E [( µ \u0000 x t ) 2 ] <latexit sha1_base64=\"5uCFLjsyhVlotMr43Rw1BdZFk0s=\">AAACYXicbZFLS+RAFIUrGZ/tK+Ms3RQ2gqC0iSgzy2bcuHTAVqHTNDfVN21hVRKqbmamCf0nZzcbN/4RKzH4vlBw+O659TiVFEpaCsP/nv9lYXFpeWW1s7a+sbkVfN2+snlpBA5ErnJzk4BFJTMckCSFN4VB0InC6+TurO5f/0ZjZZ5d0qzAkYZpJlMpgBwaB3/jKWgNPFaYEhiT/+EtOeBxAYYkqFgD3UqqzudHL6wxHfI4QXo73YBPh59RbRkH3bAXNsU/iqgVXdbWxTj4F09yUWrMSCiwdhiFBY2qekuhcN6JS4sFiDuY4tDJDDTaUdUkNOd7jkx4mhu3MuINfT1RgbZ2phPnrO9r3/dq+FlvWFL6Y1TJrCgJM/F0UFoqTjmv4+YTaVCQmjkBwkh3Vy5uwYAg9ykdF0L0/skfxdVxLzrthb9Ouv2fbRwrbIftsn0Wse+sz87ZBRswwe69BW/D2/Qe/FU/8LefrL7Xznxjb8rfeQSpH7dZ</latexit> \u0000  \u0000 + @ H / @\u0000 , \u0000  \u0000 + @ H / @\u0000 normalization transformation Figure 4: Tent modulates features during testing by estimating normalization statistics µ,σ and optimizing transformation parameters γ,β. Normalization and transformation apply channel-wise scales and shifts to the features. The statistics and parameters are updated on target data without use of source data. In practice, adapting γ,β is efﬁcient because they make up <1% of model parameters. For stability and efﬁciency, we instead only update feature modulations that are linear (scales and shifts), and low-dimensional (channel-wise). Figure 4 shows the two steps of our modulations: normalization by statistics and transformation by parameters. Normalization centers and standardizes the input xinto ¯x= (x−µ)/σby its mean µand standard deviation σ. Transformation turns ¯xinto the output x′= γ¯x+ βby afﬁne parameters for scale γand shift β. Note that the statistics µ,σ are estimated from the data while the parameters γ,β are optimized by the loss. For implementation, we simply repurpose the normalization layers of the source model. We update their normalization statistics and afﬁne parameters for all layers and channels during testing. 3.3 A LGORITHM Initialization The optimizer collects the afﬁne transformation parameters {γl,k,βl,k}for each normalization layer land channel kin the source model. The remaining parameters θ\\{γl,k,βl,k} are ﬁxed. The normalization statistics {µl,k,σl,k}from the source data are discarded. Iteration Each step updates the normalization statistics and transformation parameters on a batch of data. The normalization statistics are estimated for each layer in turn, during the forward pass. The transformation parameters γ,β are updated by the gradient of the prediction entropy ∇H(ˆy), during the backward pass. Note that the transformation update follows the prediction for the current batch, and so it only affects the next batch (unless forward is repeated). This needs just one gradient per point of additional computation, so we use this scheme by default for efﬁciency. Termination For online adaptation, no termination is necessary, and iteration continues as long as there is test data. For ofﬂine adaptation, the model is ﬁrst updated and then inference is repeated. Adaptation may of course continue by updating for multiple epochs. 4 E XPERIMENTS We evaluate tent for corruption robustness on CIFAR-10/CIFAR-100 and ImageNet, and for domain adaptation on digit adaptation from SVHN to MNIST/MNIST-M/USPS. Our implementation is in PyTorch (Paszke et al., 2019) with the pycls library (Radosavovic et al., 2019). Datasets We run on image classiﬁcation datasets for corruption and domain adaptation conditions. For large-scale experiments we choose ImageNet (Russakovsky et al., 2015), with 1,000 classes, a training set of 1.2 million, and a validation set of 50,000. For experiments at an accessible scale we choose CIFAR-10/CIFAR-100 (Krizhevsky, 2009), with 10/100 classes, a training set of 50,000, and a test set of 10,000. For domain adaptation we choose SVHN (Netzer et al., 2011) as source and MNIST (LeCun et al., 1998)/MNIST-M (Ganin & Lempitsky, 2015)/USPS (Hull, 1994) as targets, with ten classes for the digits 0–9. SVHN has color images of house numbers from street views with a training set of 73,257 and test set of 26,032. MNIST/MNIST-M/USPS have handwritten digits with a training sets of 60,000/60,000/7,291 and test sets of 10,000/10,000/2,007. Models For corruption we use residual networks (He et al., 2016) with 26 layers (R-26) on CIFAR- 10/100 and 50 layers (R-50) on ImageNet. For domain adaptation we use the R-26 architecture. For fair comparison, all methods in each experimental condition share the same architecture. Our networks are equipped with batch normalization (Ioffe & Szegedy, 2015). For the source model without adaptation, the normalization statistics are estimated during training on the source data. For all test-time adaptation methods, we estimate these statistics during testing on the target data, as done in concurrent work on adaptation by normalization (Schneider et al., 2020; Nado et al., 2020). 4Published as a conference paper at ICLR 2021 Table 2: Corruption benchmark on CIFAR-10-C and CIFAR-100-C for the highest severity. Tent has least error, with less optimization than domain adaptation (RG, UDA-SS) and test-time training (TTT), and improves on test-time norm (BN). Method Source Target Error (%) C10-C C100-C Source train 40.8 67.2 RG train train 18.3 38.9 UDA-SS train train 16.7 47.0 TTT train test 17.5 45.0 BN test 17.3 42.6 PL test 15.7 41.2 Tent (ours) test 14.3 37.3 originalgaussshot impulsedefocus glassmotionzoomsnowfrostfog bright contrastelasticpixeljpeg 0 25 50 75Error (%) source 59.5% norm 49.9% tent 44.0% ANT 50.2% Figure 5: Corruption benchmark on ImageNet-C: error for each type averaged over severity levels. Tent improves on the prior state-of-the-art, adver- sarial noise training (Rusak et al., 2020), by fully test-time adaptation without altering training. Optimization We optimize the modulation parameters γ,β following the training hyperparameters for the source model with few changes. On ImageNet we optimize by SGD with momentum; on other datasets we optimize by Adam (Kingma & Ba, 2015). We lower the batch size (BS) to reduce memory usage for inference, then lower the learning rate (LR) by the same factor to compensate (Goyal et al., 2017). On ImageNet, we set BS = 64 and LR = 0.00025, and on other datasets we set BS = 128 and LR = 0.001.We control for ordering by shufﬂing and sharing the order across methods. Baselines We compare to domain adaptation, self-supervision, normalization, and pseudo-labeling: • source applies the trained classiﬁer to the test data without adaptation, • adversarial domain adaptation (RG) reverses the gradients of a domain classiﬁer on source and target to optimize for a domain-invariant representation (Ganin & Lempitsky, 2015), • self-supervised domain adaptation (UDA-SS) jointly trains self-supervised rotation and position tasks on source and target to optimize for a shared representation (Sun et al., 2019a), • test-time training (TTT) jointly trains for supervised and self-supervised tasks on source, then keeps training the self-supervised task on target during testing (Sun et al., 2019b), • test-time normalization (BN) updates batch normalization statistics (Ioffe & Szegedy, 2015) on the target data during testing (Schneider et al., 2020; Nado et al., 2020), • pseudo-labeling (PL) tunes a conﬁdence threshold, assigns predictions over the threshold as labels, and then optimizes the model to these pseudo-labels before testing (Lee, 2013). Only test-time normalization (BN), pseudo-labeling (PL), and tent (ours) are fully test-time adaptation methods. See Section 2 for an explanation and contrast with domain adaptation and test-time training. 4.1 R OBUSTNESS TO CORRUPTIONS To benchmark robustness to corruption, we make use of common image corruptions (see Appendix A for examples). The CIFAR-10/100 and ImageNet datasets are turned into the CIFAR-10/100-C and ImageNet-C corruption benchmarks by duplicating their test/validation sets and applying 15 types of corruptions at ﬁve severity levels (Hendrycks & Dietterich, 2019). Tent improves more with less data and computation.Table 2 reports errors averaged over corrup- tion types at the severest level of corruption. On CIFAR-10/100-C we compare all methods, including those that require joint training across domains or losses, given the convenient sizes of these datasets. Adaptation is ofﬂine for fair comparison with ofﬂine baselines. Tent improves on the fully test-time adaptation baselines (BN, PL) but also the domain adaptation (RG, UDA-SS) and test-time training (TTT) methods that need several epochs of optimization on source and target. Tent consistently improves across corruption types.Figure 5 plots the error for each corruption type averaged over corruption levels on ImageNet-C. We compare the most efﬁcient methods—source, normalization, and tent—given the large scale of the source data (>1 million images) needed by other methods and the 75 target combinations of corruption types and levels. Tent and BN adapt online to rival the efﬁciency of inference without adaptation. Tent reaches the least error for most corruption types without increasing the error on the original data. 5Published as a conference paper at ICLR 2021 Table 3: Digit domain adaptation from SVHN to MNIST/MNIST-M/USPS. Source-free adaptation is not only feasible, but more efﬁcient. Tent always improves on normalization (BN), and in 2/3 cases achieves less error than domain adaptation (RG, UDA-SS) without joint training on source & target. Method Source Target Epochs Error (%) Source + Target MNIST MNIST-M USPS Source train - 18.2 39.7 19.3 RG train train 10 + 10 15.0 33.4 18.9 UDA-SS train train 10 + 10 11.1 22.2 18.4 BN test 0 + 1 15.7 39.7 18.0 Tent (ours) test 0 + 1 10.0 37.0 16.3 Tent (ours) test 0 + 10 8.2 36.8 14.4 Tent reaches a new state-of-the-art without altering training.The state-of-the-art methods for robustness extend training with adversarial noise (ANT) (Rusak et al., 2020) for 50.2% error or mixtures of data augmentations (AugMix) (Hendrycks et al., 2020) for 51.7% error. Combined with stylization from external images (SIN) (Geirhos et al., 2019), ANT+SIN reaches 47.4%. Tent reaches a new state-of-the-art of 44.0% by online adaptation and 42.3% by ofﬂine adaptation. It improves on ANT for all types except noise, on which ANT is trained. This requires just one gradient per test point, without more optimization on the training set (ANT, AugMix) or use of external images (SIN). Among fully test-time adaptation methods, tent reduces the error beyond test-time normalization for 18% relative improvement. In concurrent work, Schneider et al. (2020) report 49.3% error for test-time normalization, for which tent still gives 14% relative improvement. 4.2 S OURCE -FREE DOMAIN ADAPTATION We benchmark digit adaptation (Ganin & Lempitsky, 2015; Tzeng et al., 2015; 2017; Shu et al., 2018) for shifts from SVHN to MNIST/MNIST-M/USPS. Recall that unsupervised domain adaptation makes use the labeled source data and unlabeled target data, while our fully test-time adaptation setting denies use of source data. Adaptation is ofﬂine for fair comparison with ofﬂine baselines. Tent adapts to target without source.Table 3 reports the target errors for domain adaptation and fully test-time adaptation methods. Test-time normalization (BN) marginally improves, while adversarial domain adaptation (RG) and self-supervised domain adaptation (UDA-SS) improve more by joint training on source and target. Tent always has lower error than the source model and BN, and it achieves the lowest error in 2/3 cases, even in just one epoch and without use of source data. While encouraging for fully test-time adaptation, unsupervised domain adaptation remains necessary for the highest accuracy and harder shifts. For SVHN-to-MNIST, DIRT-T (Shu et al., 2018) achieves a remarkable 0.6% error 2. For MNIST-to-SVHN, a difﬁcult shift with source-only error of 71.3%, DIRT-T reaches45.5% and UDA-SS reaches 38.7%. Tent fails on this shift and increases error to 79.8%. In this case success presently requires joint optimization over source and target. Tent needs less computation, but still improves with more.Tent adapts efﬁciently on target data alone with just one gradient per point. RG & UDA-SS also use the source data (SVHN train), which is ∼7×the size of the target data (MNIST test), and optimize for 10 epochs. Tent adapts with ∼80× less computation. With more updates, tent reaches 8.2% error in 10 epochs and 6.5% in 100 epochs. With online updates, tent reaches 12.5% error in one epoch and 8.4% error in 10 epochs. Tent scales to semantic segmentation.To show scalability to large models and inputs, we evaluate semantic segmentation (pixel-wise classiﬁcation) on a domain shift from a simulated source to a real target. The source is GTA (Richter et al., 2017), a video game in an urban environment, and the target is Cityscapes (Cordts et al., 2016), an urban autonomous driving dataset. The model is HRNet-W18, a fully convolutional network (Shelhamer et al., 2017) with high-resolution architecture (Wang et al., 2020). The target intersection-over-union scores (higher is better) are source 28.8%, BN 31.4%, and tent 35.8% with ofﬂine optimization by Adam. For adaptation to a single image, tent reaches 36.4% in 10 iterations with episodic optimization. See the appendix for a qualitative example (Appendix B). 2We exclude DIRT-T from our experiments because of incomparable differences in architecture and model selection. DIRT-T tunes with labeled target data, but we do not. Please refer to Shu et al. (2018) for more detail. 6Published as a conference paper at ICLR 2021 Figure 6: Tent reduces the entropy and loss. We plot changes in entropy∆Hand loss ∆Lfor all of CIFAR-100-C. Change in entropy rank-correlates with change in loss: note the dark diagonal and the rank correlation coefﬁcient of 0.22. (a) Source (b) BN  (c) Tent (d) Oracle  Figure 7: Adapted features on CIFAR-100-C with Gaussian noise (front) and reference features without corruption (back). Corruption shifts fea- tures away from the reference, but BN reduces the shifts. Tent instead shifts features more, and closer to an oracle that optimizes on target labels. Tent scales to the VisDA-C challenge.To show adaptation on a more difﬁcult benchmark, we evaluate on the VisDA-C challenge (Peng et al., 2017). The task is object recognition for 12 classes where the source data is synthesized by rendering 3D models and the target data is collected from real scenes. The validation error for our source model (ResNet-50, pretrained on ImageNet) is 56.1%, while tent reaches 45.6%, and improves to 39.6% by updating all layers except for the ﬁnal classiﬁer as done by Liang et al. (2020). Although ofﬂine source-free adaptation by model adaptation (Li et al., 2020) or SHOT (Liang et al., 2020) can reach lower error with more computation and tuning, tent can adapt online during testing. 4.3 A NALYSIS Tent reduces entropy and error.Figure 6 veriﬁes tent does indeed reduce the entropy and the task loss (softmax cross-entropy). We plot changes in entropy and loss on CIFAR-100-C for all 75 corruption type/level combinations. Both axes are normalized by the maximum entropy of a prediction (log 100) and clipped to ±1. Most points have lower entropy and error after adaptation. Tent needs feature modulation.We ablate the normalization and transformation steps of feature modulation. Not updating normalization increases errors, and can fail to improve over BN and PL. Not updating transformation parameters reduces the method to test-time normalization. Updating only the last layer of the model can improve but then degrades with further optimization. Updating the full model parameters θnever improves over the unadapted source model. Tent generalizes across target data.Adaptation could be limited to the points used for updates. We check that adaptation generalizes across points by adapting on target train and not target test. Test errors drop: CIFAR-100-C error goes from 37.3% to 34.2% and SVHN-to-MNIST error goes from 8.2% to 6.5%. (Train is larger than test; when subsampling to the same size errors differ by <0.1%.) Therefore the adapted modulation is not point speciﬁc but general. Tent modulation differs from normalization.Modulation normalizes and transforms features. We examine the combined effect. Figure 7 contrasts adapted features on corrupted data against reference features on uncorrupted data. We plot features from the source model, normalization, tent, and an oracle that optimizes on the target labels. Normalization makes features more like the reference, but tent does not. Instead, tent makes features more like the oracle. This suggests a different and task-speciﬁc effect. See the appendix for visualizations of more layers (Appendix C). 7Published as a conference paper at ICLR 2021 Tent adapts alternative architectures.Tent is architecture agnostic in principle. To gauge its generality in practice, we evaluate new architectures based on self-attention (SAN) (Zhao et al., 2020) and equilibrium solving (MDEQ) (Bai et al., 2020) for corruption robustness on CIFAR-100-C. Table 4 shows that tent reduces error with the same settings as convolutional residual networks. Table 4: Tent adapts alternative architectures on CIFAR-100-C without tuning. Results are error (%). SAN-10 (pair) SAN-10 (patch) MDEQ (large) Source BN Tent Source BN Tent Source BN Tent 55.3 39.7 36.7 48.0 31.8 29.2 53.3 44.9 41.7 5 R ELATED WORK We relate tent to existing adaptation, entropy minimization, and feature modulation methods. Train-Time AdaptationDomain adaptation jointly optimizes on source and target by cross-domain losses L(xs,xt) to mitigate shift. These losses optimize feature alignment (Gretton et al., 2009; Sun et al., 2017), adversarial invariance (Ganin & Lempitsky, 2015; Tzeng et al., 2017), or shared proxy tasks (Sun et al., 2019a). Transduction (Gammerman et al., 1998; Joachims, 1999; Zhou et al., 2004) jointly optimizes on train and test to better ﬁt speciﬁc test instances. While effective in their settings, neither applies when joint use of source/train and target/test is denied. Tent adapts on target alone. Recent “source-free” methods (Li et al., 2020; Kundu et al., 2020; Liang et al., 2020) also adapt without source data. Li et al. (2020); Kundu et al. (2020) rely on generative modeling and optimize multiple models with multiple losses. Kundu et al. (2020); Liang et al. (2020) also alter training. Tent does not need generative modeling, nor does it alter training, and so it can deployed more generally to adapt online with much more computational efﬁciency. SHOT (Liang et al., 2020) adapts by informa- tion maximization (entropy minimization and diversity regularization), but differs in its other losses and its parameterization. These source-free methods optimize ofﬂine with multiple losses for multiple epochs, which requires more tuning and computation than tent, but may achieve more accuracy with more computation. Tent optimizes online with just one loss and an efﬁcient parameterization of modulation to emphasize fully test-time adaptation during inference. We encourage examination of each of these works on the frontier of adaptation without source data. Chidlovskii et al. (2016) are the ﬁrst to motivate adaptation without source data for legal, commercial, or technical concerns. They adapt predictions by applying denoising auto-encoders while we adapt models by entropy minimization. We share their motivations, but the methods and experiments differ. Test-Time AdaptationTent adapts by test-time optimization and normalization to update the model. Test-time adaptation of predictions, through which harder and uncertain cases are adjusted based on easier and certain cases (Jain & Learned-Miller, 2011), provides inspiration for certainty-based model adaptation schemes like our own. Test-time training (TTT) (Sun et al., 2019b) also optimizes during testing, but differs in its loss and must alter training. TTT relies on a proxy task, such as recognizing rotations of an image, and so its loss depends on the choice of proxy. (Indeed, its authors caution that the proxy must be “both well-deﬁned and non-trivial in the new domain”). TTT alters training to optimize this proxy loss on source before adapting to target. Tent adapts without proxy tasks and without altering training. Normalizing feature statistics is common for domain adaptation (Gretton et al., 2009; Sun et al., 2017). For batch normalization Li et al. (2017); Carlucci et al. (2017) separate source and target statistics during training. Schneider et al. (2020); Nado et al. (2020) estimate target statistics during testing to improve generalization. Tent builds on test-time normalization to further reduce generalization error. Entropy MinimizationEntropy minimization is a key regularizer for domain adaptation (Carlucci et al., 2017; Shu et al., 2018; Saito et al., 2019; Roy et al., 2019), semi-supervised learning (Grandvalet & Bengio, 2005; Lee, 2013; Berthelot et al., 2019), and few-shot learning (Dhillon et al., 2020). Regularizing entropy penalizes decisions at high densities in the data distribution to improve accuracy for distinct classes (Grandvalet & Bengio, 2005). These methods regularize entropy during training in concert with other supervised and unsupervised losses on additional data. Tent is the ﬁrst to minimize 8Published as a conference paper at ICLR 2021 entropy during testing, for adaptation to dataset shifts, without other losses or data. Entropic losses are common; our contribution is to exhibit entropy as the sole lossfor fully test-time adaptation. Feature ModulationModulation makes a model vary with its input. We optimize modulations that are simpler than the full model for stable and efﬁcient adaptation. We modulate channel-wise afﬁne transformations, for their effectiveness in tandem with normalization (Ioffe & Szegedy, 2015; Wu & He, 2018), and for their ﬂexibility in conditioning for different tasks (Perez et al., 2018). These normalization and conditioning methods optimize the modulation during training by a supervised loss, but keep it ﬁxed during testing. We optimize the modulation during testing by an unsupervised loss, so that it can adapt to different target data. 6 D ISCUSSION Tent reduces generalization error on shifted data by test-time entropy minimization. In minimizing entropy, the model adapts itself to feedback from its own predictions. This is truly self-supervised self-improvement. Self-supervision of this sort is totally deﬁned by the supervised task, unlike proxy tasks designed to extract more supervision from the data, and yet it remarkably still reduces error. Nevertheless, errors due to corruption and other shifts remain, and therefore more adaptation is needed. Next steps should pursue test-time adaptation on more and harder types of shift, over more general parameters, and by more effective and efﬁcient losses. Shifts Tent reduces error for a variety of shifts including image corruptions, simple changes in appearance for digits, and simulation-to-real discrepancies. These shifts are popular as standardized benchmarks, but other real-world shifts exist. For instance, the CIFAR 10.1 and ImageNetV2 test sets (Recht et al., 2018; 2019), made by reproducing the dataset collection procedures, entail natural but unknown shifts. Although error is higher on both sets, indicating the presence of shift, tent does not improve generalization. Adversarial shifts (Szegedy et al., 2014) also threaten real-world usage, and attackers keep adapting to defenses. While adversarial training (Madry et al., 2018) makes a difference, test-time adaptation could help counter such test-time attacks. Parameters Tent modulates the model by normalization and transformation, but much of the model stays ﬁxed. Test-time adaptation could update more of the model, but the issue is to identify parameters that are both expressive and reliable, and this may interact with the choice of loss. TTT adapts multiple layers of features shared by supervised and self-supervised models and SHOT adapts all but the last layer(s) of the model. These choices depend on the model architecture, the loss, and tuning. For tent modulation is reliable, but the larger shift on VisDA is better addressed by the SHOT parameterization. Jointly adapting the input could be a more general alternative. If a model can adapt itself on target, then perhaps its input gradients might optimize spatial transformations or image translations to reduce shift without source data. Losses Tent minimizes entropy. For more adaptation, is there an effective loss for general but episodic test-time optimization? Entropy is general across tasks but limited in scope. It needs batches for optimization, and cannot update episodically on one point at a time. TTT can do so, but only with the right proxy task. For less computation, is there an efﬁcient loss for more local optimization? Tent and TTT both require full (re-)computation of the model for updates because they depend on its predictions. If the loss were instead deﬁned on the representation, then updates would require less forward and backward computation. Returning to entropy speciﬁcally, this loss may interact with calibration (Guo et al., 2017), as better uncertainty estimation could drive better adaptation. We hope that the fully test-time adaptation setting can promote new methods for equipping a model to adapt itself, just as tent yields a new model with every update. ACKNOWLEDGMENTS We thank Eric Tzeng for discussions on domain adaptation, Bill Freeman for comments on the experiments, Yu Sun for consultations on test-time training, and Kelsey Allen for feedback on the exposition. We thank the anonymous reviewers of ICLR 2021 for their feedback, which certainly improved the latest adaptation of the paper. 9Published as a conference paper at ICLR 2021 REFERENCES Shaojie Bai, Vladlen Koltun, and J Zico Kolter. Multiscale deep equilibrium models. arXiv preprint arXiv:2006.08656, 2020. David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. In NeurIPS, 2019. Fabio Maria Carlucci, Lorenzo Porzi, Barbara Caputo, Elisa Ricci, and Samuel Rota Bulo. Autodial: Automatic domain alignment layers. In 2017 IEEE International Conference on Computer Vision (ICCV), pp. 5077–5085. IEEE, 2017. Boris Chidlovskii, Stephane Clinchant, and Gabriela Csurka. Domain adaptation in the absence of source domain data. In SIGKDD, pp. 451–460, 2016. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016. Guneet Singh Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano Soatto. A baseline for few-shot image classiﬁcation. In ICLR, 2020. Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context prediction. In ICCV, 2015. J. Donahue, Y . Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In ICML, 2014. A Gammerman, V V ovk, and V Vapnik. Learning by transduction. InUAI, 1998. Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In ICML, 2015. Robert Geirhos, Carlos RM Temme, Jonas Rauber, Heiko H Schütt, Matthias Bethge, and Felix A Wichmann. Generalisation in humans and deep neural networks. In NeurIPS, 2018. Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and Wieland Brendel. Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. In International Conference on Learning Representations, 2019. Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. In ICLR, 2018. Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In NeurIPS, 2005. A. Gretton, AJ. Smola, J. Huang, M. Schmittfull, KM. Borgwardt, and B. Schölkopf. Covariate shift and local learning by distribution matching. In Dataset Shift in Machine Learning, pp. 131–160. MIT Press, Cambridge, MA, USA, 2009. Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In ICML, 2017. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, June 2016. Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In ICLR, 2019. Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. Augmix: A simple data processing method to improve robustness and uncertainty. In ICLR, 2020. Jonathan J. Hull. A database for handwritten text recognition research. TPAMI, 1994. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015. 10Published as a conference paper at ICLR 2021 Vidit Jain and Erik Learned-Miller. Online domain adaptation of a pre-trained cascade of classiﬁers. In CVPR, 2011. Thorsten Joachims. Transductive inference for text classiﬁcation using support vector machines. In ICML, 1999. Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiﬁcation with deep convolutional neural networks. NeurIPS, 25, 2012. Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009. Jogendra Nath Kundu, Naveen Venkat, R Venkatesh Babu, et al. Universal source-free domain adaptation. In CVPR, pp. 4544–4553, 2020. Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. Dong-Hyun Lee. Pseudo-label: The simple and efﬁcient semi-supervised learning method for deep neural networks. In ICML Workshop on challenges in representation learning, 2013. Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, and Si Wu. Model adaptation: Unsupervised domain adaptation without source data. In CVPR, June 2020. Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou. Revisiting batch normalization for practical domain adaptation. In ICLRW, 2017. Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In ICML, 2020. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018. Zachary Nado, Shreyas Padhy, D Sculley, Alexander D’Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robustness under covariate shift. arXiv preprint arXiv:2006.10963, 2020. Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. NeurIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019. Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko. VisDA: The visual domain adaptation challenge. arXiv preprint arXiv:1710.06924, 2017. Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. In AAAI, 2018. Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset shift in machine learning. MIT Press, Cambridge, MA, USA, 2009. Ilija Radosavovic, Justin Johnson, Saining Xie, Wan-Yen Lo, and Piotr Dollár. On network design spaces for visual recognition. In ICCV, 2019. Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do cifar-10 classiﬁers generalize to cifar-10? arXiv preprint arXiv:1806.00451, 2018. Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet classiﬁers generalize to ImageNet? In ICML, 2019. Stephan R Richter, Zeeshan Hayder, and Vladlen Koltun. Playing for benchmarks. In ICCV, 2017. Subhankar Roy, Aliaksandr Siarohin, Enver Sangineto, Samuel Rota Bulo, Nicu Sebe, and Elisa Ricci. Unsuper- vised domain adaptation using feature-whitening and consensus loss. In CVPR, 2019. 11Published as a conference paper at ICLR 2021 Evgenia Rusak, Lukas Schott, Roland S Zimmermann, Julian Bitterwolf, Oliver Bringmann, Matthias Bethge, and Wieland Brendel. A simple way to make neural networks robust against diverse image corruptions. In ECCV, 2020. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. ImageNet large scale visual recognition challenge. IJCV, 2015. Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to new domains. In European conference on computer vision, pp. 213–226. Springer, 2010. Kuniaki Saito, Donghyun Kim, Stan Sclaroff, Trevor Darrell, and Kate Saenko. Semi-supervised domain adaptation via minimax entropy. In ICCV, 2019. Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improv- ing robustness against common corruptions by covariate shift adaptation. arXiv preprint arXiv:2006.16971, 2020. C.E. Shannon. A mathematical theory of communication. Bell system technical journal, 27, 1948. Evan Shelhamer, Jonathan Long, and Trevor Darrell. Fully convolutional networks for semantic segmentation. PAMI, 2017. Rui Shu, Hung H Bui, Hirokazu Narui, and Stefano Ermon. A dirt-t approach to unsupervised domain adaptation. In ICLR, 2018. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015. Baochen Sun, Jiashi Feng, and Kate Saenko. Correlation alignment for unsupervised domain adaptation. In Domain Adaptation in Computer Vision Applications, pp. 153–171. Springer, 2017. Yu Sun, Eric Tzeng, Trevor Darrell, and Alexei A Efros. Unsupervised domain adaptation through self- supervision. arXiv preprint arXiv:1909.11825, 2019a. Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A Efros, and Moritz Hardt. Test-time training for out-of-distribution generalization. arXiv preprint arXiv:1909.13231, 2019b. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. 2014. Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. In ICCV, 2015. Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In CVPR, 2017. Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, et al. Deep high-resolution representation learning for visual recognition. PAMI, 2020. Yuxin Wu and Kaiming He. Group normalization. In ECCV, 2018. Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In NeurIPS, 2014. Richard Zhang, Phillip Isola, and Alexei A Efros. Split-brain autoencoders: Unsupervised learning by cross- channel prediction. In CVPR, 2017. Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition. In CVPR, 2020. Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, and Bernhard Schölkopf. Learning with local and global consistency. NeurIPS, 2004. 12Published as a conference paper at ICLR 2021 APPENDIX This supplement summarizes the image corruptions used in our experiments, highlights a qualitative example of instance-wise adaptation for semantic segmentation, and visualizes feature shifts across more layers. A R OBUSTNESS TO CORRUPTIONS In Section 4.1 we evaluate methods on a common image corruptions benchmark. Table 2 reports errors on the most severe level of corruption, level 5, and Figure 5 reports errors for each corruption type averaged across each of the levels 1–5. We summarize these corruptions types by example in Figure 8. Gaussian Noise  Shot Noise  Impulse Noise  Defocus Blur  Frosted Glass Blur Motion Blur  Zoom Blur  Snow  Frost  Fog Brightness  Contrast  Elastic  Pixelate  JPEG Figure 8: Examples of each corruption type in the image corruptions benchmark. While synthetic, this set of corruptions aims to represent natural factors of variation like noise, blur, weather, and digital imaging effects. This ﬁgure is reproduced from Hendrycks & Dietterich (2019). B S OURCE -FREE ADAPTATION FOR SEMANTIC SEGMENTATION Figure 9 shows a qualitative result on source-free adaptation for semantic segmentation (pixel-wise classiﬁcation) with simulation-to-real (sim-to-real) shift. For this sim-to-real condition, the source data is simulated while the target data is real. Our source data is GTA Richter et al. (2017), a visually-sophisticated video game set in an urban environment, and our target data is Cityscapes Cordts et al. (2016), an urban autonomous driving dataset. The supervised model is HRnet-W18, a fully convolutional network Shelhamer et al. (2017) in the high-resolution network family Wang et al. (2020). For this qualitative example, we run tent on a single image for multiple iterations, because an image is in effect a batch of pixels. This demonstrates adaptation to a target instance, without any further access to the target domain through usage of multiple images from the target distribution. 13Published as a conference paper at ICLR 2021 image label source-only tent, iteration 1 tent, iteration 5 tent, iteration 10 Figure 9: Adaptation for semantic segmentation with simulation-to-real shift from GTA Richter et al. (2017) to Cityscapes Cordts et al. (2016). Tent only uses the target data, and optimizes over a single image as a dataset of pixel-wise predictions. This episodic optimization in effect ﬁts a custom model to each image of the target domain. In only 10 iterations our method suppresses noise (see the completion of the street segment, in purple) and recovers missing classes (see the motorcycle and rider, center). 14Published as a conference paper at ICLR 2021 C F EATURE SHIFTS ACROSS LAYERS AND METHODS (a) Source (b) BN (c) Tent (d) Oracle Layer 2 Layer 5 Layer 8 Layer 11 Layer 14 Layer 18 Layer 20 Layer 23 Layer 26 Figure 10: Adapted features on CIFAR-100-C with Gaussian noise (front) and reference features without corruption (back). Corruption shifts the source features from the reference. BN shifts the features back to be more like the reference. Tent shifts features to be less like the reference, and more like an oracle that optimizes on target labels. 15",
      "meta_data": {
        "arxiv_id": "2006.10726v3",
        "authors": [
          "Dequan Wang",
          "Evan Shelhamer",
          "Shaoteng Liu",
          "Bruno Olshausen",
          "Trevor Darrell"
        ],
        "published_date": "2020-06-18T17:55:28Z",
        "pdf_url": "https://arxiv.org/pdf/2006.10726v3.pdf"
      }
    },
    {
      "title": "Diversify, Contextualize, and Adapt: Efficient Entropy Modeling for Neural Image Codec",
      "abstract": "Designing a fast and effective entropy model is challenging but essential for\npractical application of neural codecs. Beyond spatial autoregressive entropy\nmodels, more efficient backward adaptation-based entropy models have been\nrecently developed. They not only reduce decoding time by using smaller number\nof modeling steps but also maintain or even improve rate--distortion\nperformance by leveraging more diverse contexts for backward adaptation.\nDespite their significant progress, we argue that their performance has been\nlimited by the simple adoption of the design convention for forward adaptation:\nusing only a single type of hyper latent representation, which does not provide\nsufficient contextual information, especially in the first modeling step. In\nthis paper, we propose a simple yet effective entropy modeling framework that\nleverages sufficient contexts for forward adaptation without compromising on\nbit-rate. Specifically, we introduce a strategy of diversifying hyper latent\nrepresentations for forward adaptation, i.e., using two additional types of\ncontexts along with the existing single type of context. In addition, we\npresent a method to effectively use the diverse contexts for contextualizing\nthe current elements to be encoded/decoded. By addressing the limitation of the\nprevious approach, our proposed framework leads to significant performance\nimprovements. Experimental results on popular datasets show that our proposed\nframework consistently improves rate--distortion performance across various\nbit-rate regions, e.g., 3.73% BD-rate gain over the state-of-the-art baseline\non the Kodak dataset.",
      "full_text": "Diversify, Contextualize, and Adapt: Efficient Entropy Modeling for Neural Image Codec Jun-Hyuk Kim∗ Seungeon Kim Won-Hee Lee Dokwan Oh Samsung Advanced Institute of Technology {jh131.kim, se2.kim, why_wh.lee, dokwan.oh}@samsung.com Abstract Designing a fast and effective entropy model is challenging but essential for prac- tical application of neural codecs. Beyond spatial autoregressive entropy models, more efficient backward adaptation-based entropy models have been recently de- veloped. They not only reduce decoding time by using smaller number of modeling steps but also maintain or even improve rate–distortion performance by leveraging more diverse contexts for backward adaptation. Despite their significant progress, we argue that their performance has been limited by the simple adoption of the design convention for forward adaptation: using only a single type of hyper latent representation, which does not provide sufficient contextual information, especially in the first modeling step. In this paper, we propose a simple yet effective entropy modeling framework that leverages sufficient contexts for forward adaptation with- out compromising on bit-rate. Specifically, we introduce a strategy of diversifying hyper latent representations for forward adaptation, i.e., using two additional types of contexts along with the existing single type of context. In addition, we present a method to effectively use the diverse contexts for contextualizing the current elements to be encoded/decoded. By addressing the limitation of the previous approach, our proposed framework leads to significant performance improvements. Experimental results on popular datasets show that our proposed framework con- sistently improves rate–distortion performance across various bit-rate regions, e.g., 3.73% BD-rate gain over the state-of-the-art baseline on the Kodak dataset. 1 Introduction Most neural image codecs [ 8, 9, 11, 15, 17, 18] first transform an image into a quantized latent representation. It is then encoded into a bitstream via an entropy coding algorithm, which relies on a learned probability model known as the entropy model. According to the Shannon’s source coding theorem, the minimum expected length of a bitstream is equal to the entropy of the source. Thus, accurately modeling entropy of the quantized latent representation is crucial. Entropy models estimate a joint probability distribution over the elements of the quantized latent representation. Generally, it is assumed that all elements follow conditionally independent probability distributions. To satisfy this, the probability distributions are modeled in context-adaptive manners, which is key to accurate entropy modeling [18]. Recent methods are based on the joint backward and forward adaptation where the probability distributions adapt by leveraging contexts in two different ways: directly using previously encoded/decoded elements (i.e., backward adaptation), and extracting and utilizing an additional hyper latent representation (i.e., forward adaptation). Here, the type of contexts leveraged can be diverse depending on the spatial range they cover. First, each element has dependencies with other elements in the same spatial location along the channel dimension. Since the channel-wise dependencies correspond to the local image area (e.g., a 16 × 16 patch), we denote ∗Corresponding author 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2411.05832v1  [cs.CV]  6 Nov 2024them as the “local” context. Second, dependencies exist among spatially adjacent elements, and we refer to them as the “regional” context. Lastly, long-range spatial dependencies span the entire image area, referred to as the “global” context. For the backward adaptation, the modeling order, i.e., which elements are modeled first, is an important factor, and the key lies in how effectively we can utilize diverse contexts in the modeling process. Early studies employ spatial autoregressive (AR) models that access regional context including the most spatially adjacent elements. However, they suffer from significantly slow decoding times due to the inevitably large number of modeling steps, which is equal to the spatial dimensions [18]. To enhance efficiency in entropy modeling, several attempts reduce the number of modeling steps while leveraging diverse contexts: a 10-step channel-wise AR model [17], a 2-step spatial non-AR model with a checkerboard pattern [8], and a 4-step non-AR model that operates across spatial and channel dimensions using a quadtree partition [14]. Figure 1: DCA diversifies the hyper latent repre- sentations and contextualizes the current elements by leveraging the diverse hyper latent representa- tions along with the previous elements. As a result, the probability distributionsadapt effectively, lead- ing to accurate entropy modeling. Entropy models based on the efficient backward adaptation methods have led to significant im- provements. However, they are still limited in fully leveraging contexts for forward adaptation. Since they use multiple neural layers with down- sampling and upsampling for modeling hyper latent representation, they can only access the regional context. This limits the performance improvement due to the insufficient contexts (Figure 1a). In particular, this limitation is exac- erbated at the first step where only forward adap- tation is utilized due to the absence of previous elements (Figure 6). Therefore, it is necessary to develop effective forward adaptation in synergy with the efficient backward adaptation. In this paper, we propose a simple yet effec- tive entropy modeling framework, called DCA (Diversify, Contextualize, and Adapt), leverag- ing sufficient contexts for forward adaptation without compromising on bit-rate (Figure 1b). Building on the quadtree partition-based back- ward adaptation [ 14], we introduce a strategy of diversification, i.e., extracting local, regional, and global hyper latent representations unlike only a single regional one in the previous ap- proach. Note that simply using more contexts for forward adaptation does not guarantee performance improvements because forward adaptation requires additional bit allocation unlike backward adap- tation. Then, we propose how to effectively utilize the diverse contexts along with the previously modeled elements for contextualizing the current elements to be encoded/decoded. To consider step- wise different situations, e.g., increased number of previous elements over steps, our contextualization method is designed to utilize each hyper latent representation separately in a step-adaptive manner. Additionally, our contextualization method proceeds in the sequence of regional, global, and local hyper latent representations. Similarly to backward adaptation, we empirically observe that modeling order also matters in forward adaptation. Our main contributions are summarized as follows: • We propose a strategy of diversifying contexts for forward adaptation by extracting three different hyper latent representations, i.e., local, regional, and global ones. This strategy can provide sufficient contexts for forward adaptation without compromising on bit-rate. • We introduce how to effectively leverage the diverse contexts, i.e., previously modeled elements and the three hyper latent representations. We empirically show that the modeling order of three types of contexts affects the performance. • Through the diversification and contextualization methods, our DCA effectively adapts, resulting in significant performance improvements. For example, DCA achieves 3.73% BD-rate gain over the state-of-the-art method [14] on the Kodak dataset. 22 Related work Joint backward and forward adaptation. Ballé et al. [3] propose a scale hyperprior for forward adaptation. A hyper latent representation is extracted and utilized for inferring local scale parameters of the parameterized entropy model. Minnen et al. [18] extend the hyperprior model by using an additional mean hyperprior, and introduce joint backward and forward adaptation by combining the extended hyperprior model with a spatial autoregressive (AR) model. A patch matching-based non-local referring model [20] and a multi-head attention-based global hyperprior [11] are proposed to enrich contexts for backward and forward adaptation, respectively. Efficient backward adaptation. To address the slow decoding times of spatial AR-based entropy models, several studies have proposed group-wise backward adaptation methods. They first divide the quantized latent representation into multiple groups and then process them in a group-wise manner, resulting in improved efficiency. He et al.[8] propose dividing the quantized latent representation into two groups using the checkerboard pattern, which is further improved by incorporating Transformer- based modules [21]. While they apply a group-wise modeling in spatial dimension, Minnen and Singh [17] introduce a channel-wise AR model that divides the quantized latent representation into ten groups along channel dimension. Some studies [22, 23] improve this model by applying Swin Transformer [16]. Based on the channel-wise AR model, He et al. [9] optimize the channel division and combine it with the checkerboard-based model. Recently, Li et al. [14] propose a quadtree partition-based backward adaptation that divides the quantized latent representation into four groups considering both channel and spatial dimensions. In this paper, we propose a novel fast and effective entropy model that achieves better rate–distortion performance by diversifying not only the quantized latent representation but also the hyper latent representations for backward and forward adaptation, respectively. 3 Methods We provide an overview of the proposed methods in Figure 2. The analysis transform fa(·) and the synthesis transform fs(·) (the gray blocks in Figure 2) are learned to find an effective mapping between an input image x and a quantized latent representation ˆy, i.e., ˆy = ⌊fa(x)⌉ and ˆx = fs(ˆy), where ⌊·⌉ is a round operation and ˆx is the decoded image. For the analysis and synthesis transforms, we adopt the same model structure as in the ELIC-sm model [9] due to its efficiency. All other components are learned to model a prior probability distribution on the quantized latent representation ˆy, i.e., the entropy model pˆy. The learned entropy model is utilized in the process of entropy coding, for which we employ the asymmetric numeral systems [6]. Here, our goal is to design a fast and effective learned entropy model. Figure 3: Example of the quadtree partition-based backward adaptation for ˆy ∈ R4×4×320. For sim- plicity, channel dimensions are represented via different colors. ˆyi means the elements to be en- coded/decoded at the i-th step. For modeling the current elements ˆyi, all the previous modeled el- ements ˆy<i are used. For example, the elements corresponding to the red arrow leverage diverse contexts including elements across different chan- nels at the same spatial location (local context de- noted as L) and spatially adjacent four elements of the same channel (regional context denoted as R). Quadtree partition. We build our entropy model on the joint forward and backward adapta- tion where the quadtree partition is used, which is formulated as follows [14]: p(ˆy) = p(ˆz)×p(ˆy|ˆz) = p(ˆz)× 4Y i=1 p(ˆyi|ˆy<i, ˆz) where ˆy is the quantized latent representation, ˆz is the quantized regional hyper latent repre- sentation, ˆyi is the elements to be modeled at the i-th step, and ˆy<i is all the previous mod- eled elements before the i-th step. At each step, one-fourth of the total elements are modeled. The method partitions the quantized latent rep- resentation into four groups along the channel dimension, and then divides each group into non-overlapping 2×2 patches along the spatial dimension. The entropy modeling proceeds over 3Figure 2: Overview of the neural image codec with the proposed entropy model, referred to as DCA. DCA can be employed by any analysis and synthesis transforms fa(·) and fs(·). DCA is an adaptive entropy model consisting of two main stages: diversify (Section 3.1) and contextualize (Section 3.2). First, given the latent representation y, DCA extracts diverse hyper latent representations ˆzl, ˆzr, and ˆzg, and then encodes them into the bitstreams using learned factorized entropy models, which are omitted in this figure for simplicity. Second, contextualization proceeds over four steps. By using the three features ϕl, ϕr, and ϕg (from the three hyper latent representations, respectively) and all the previously encoded/decoded elements before the i-th step, i.e., ˆy<i, DCA contextualizes the current elements to be encoded/decoded, i.e., ˆyi, and finally obtains adaptive distribution parameters µi and σi for probability modeling. Using the learned adaptive probability model, the quantized latent representation ˆy are encoded into a bitstream. four steps, with each step modeling different elements as shown in Figure 3. This quadtree partition- based method uses diverse contexts for backward adaptation, capturing dependencies from both spatial and channel dimensions. Motivation. Recent studies to efficient modeling of backward adaptation have made significant advancements in terms of optimizing the rate–distortion–computation trade-off; however, there is still a gap between their assumptions in the probability modeling and actual data, leaving room for further performance enhancement. The assumptions are as follows: 1) All elements of ˆz are independent; 2) All elements of ˆyi are conditionally independent given ˆy<i and ˆz. Here, the more the actual data deviates from the assumptions, the lower the accuracy of the entropy modeling. At the first modeling step, the elements are modeled conditioned only on the quantized hyper latent representation, i.e., p(ˆy1|ˆz), resulting in a hyperprior model known for deviating from to the second assumption [18]. This can be more problematic because the state-of-the-art methods process a relatively large number of elements at the first step in order to complete the overall modeling with a minimal number of steps. We also empirically show that this problem actually occurs in Figure 6. One straightforward solution is to increase the number of steps so that fewer elements are modeled in the first step. However, this leads to slower modeling speeds, which conflict with the goal of our paper, i.e., developing a fast and effective entropy model. Another simple approach is to provide more quantized regional hyper latent representation ˆz when modeling the quantized latent representation ˆy1. However, paradoxically, this approach can introduce another issue due to the first assumption. Since all elements of hyper latent representation are the same type of information (i.e., regional context), there is a relatively high likelihood of dependencies among the elements. Therefore, to meet both assumptions, the newly added hyper latent representation is required to be independent from the existing regional hyper latent representation. This is why our proposed diversification method using 4local, regional, and global hyper latent representations is needed. In this paper, we propose a fast and effective entropy model, called DCA, which consists of three main stages: diversifying the hyper latent representations, contextualizing the elements targeted for probability modeling, and ultimately adapting the probability distribution of the elements to the given contexts. 3.1 Diversify The proposed DCA aims to diversify the information that the hyper latent representations contain. Specifically, given the latent representation y ∈ RH×W×C, where H, W, and C are the height, width, and the number of channels, respectively, DCA extracts three different types of hyper latent representations depending on the range they cover: a local hyper latent representationˆzl ∈ RH×W×Cl, a regional hyper latent representation ˆzr ∈ R H 4 ×W 4 ×Cr , and a global hyper latent representation ˆzg ∈ RN× C N . The whole process is illustrated in the orange blocks of Figure 2. Local context. To model remaining dependencies along channel dimension at each spatial location, which correspond to a 16 × 16 local patch in the image domain, we introduce local hyper analysis and synthesis transforms, la(·) and ls(·), based on Swin Transformer (SwinT) [16]. The local hyper analysis transform la(·) analyzes local information in the latent representation, followed by the quantization operation to obtain a local hyper latent representation ˆzl. The local synthesis transform ls(·) synthesizes the local features ϕl ∈ RH×W×2C for contextualization from the local hyper latent representation ˆzl. Each transform proceeds in the order of a Patch Split block, a SwinT block, and a Patch Merge block. The Patch Split block serves the function of shifting all channel-wise elements at each spatial location to a 2 × 2 spatial resolution, consisting of the depth-to-space, layer normalization, and linear layers in sequence. The SwinT block then captures dependencies between elements within each non-overlapping window of the input, producing an output of the same size as the input. By setting the window size to 2 × 2 in conjunction with the use of the Split block, we enforce the local hyper transforms to focus only on the local image area. The Patch Merge block performs the opposite function of the Patch Split block, containing the layer normalization, linear, and space-to-depth layers in sequence. Regional context. While the receptive field of the local hyper transforms is limited to the local image area (i.e., 16×16 patches), regional hyper analysis transformra(·) and regional hyper synthesis transform rs(·) model remaining dependencies between elements distributed across a relatively wide image area. The regional hyper analysis transform ra(·) analyzes regional information in the latent representation and yields a regional hyper latent representation ˆzr after quantization. From the extracted regional hyper latent representation ˆzr, the regional synthesis transform rs(·) generates the regional features ϕr ∈ RH×W×2C for contextualization. To do this, we stack multiple layers with the downsampling and upsampling operations for the regional hyper analysis and synthesis transforms, respectively. We adopt the same structure as the previous work [22], which is based on SwinT. Specifically, the regional hyper analysis transform ra(·) conducts a Patch Merge block, five SwinT blocks, a Patch Merge block, and a SwinT block. The regional hyper synthesis transform rs(·) is constructed in the opposite order of the hyper analysis transform ra(·), using the Patch Split block instead of the Patch Merge block. Global context. Lastly, to capture remaining dependencies between elements across the whole image area, we construct global hyper analysis and synthesis transforms ga(·) and gs(·) by adopting model structure of the global hyperprior model of Informer [11]. The global hyper analysis transform ga(·) extracts globally abstracted information from the latent representation using a Transformer block with cross-attention and a 1 × 1 convolutional layer. After quantization, it obtains a global hyper latent representation ˆzg. Using a 1 × 1 convolutional layer, the global synthesis transform gs(·) infers the global features ϕg ∈ RN×2C for contextualization. 3.2 Contextualize Diverse contexts, i.e., previously modeled elements (i.e., ˆy<i) and hyper latent representations (i.e., ˆzl, ˆzr, and ˆzg) can be used for adapting probability distributions. Here, an important research question 5emerges: How can we effectively leverage the diverse contexts? First, to consider step-wise varied situations, e.g., increased previously encoded/decoded elements over modeling steps, we propose a step-adaptive utilization of the three hyper latent representations. In other words, instead of applying a combined set of the three hyper latent representations, each hyper latent representation is adaptively leveraged at each step. In addition, we use regional, global, and local information sequentially. We argue that modeling order is crucial for forward adaptation, which is already known to be a key factor for backward adaptation. The green part of Figure 2 illustrates our proposed contextualization model c(·) at the i-th step. First, the previously modeled ˆy<i and the regional feature ϕr are combined based on the same structure as in the previous approach [14], consisting of concatenation, a 1 × 1 convolutional layer, and three DepthConv Blocks. The DepthConv Block employs depth-wise separable convolutional layers for more efficient implementation. Second, the global feature ϕg is combined with the output of the last DepthConv Block using the Transformer block with cross-attention [11]. Finally, we combine the output of the Transformer block with the local feature ϕl using concatenation followed by three 1 × 1 convolutional layers, yielding the distribution parameters µi and σi. To make our contextualization model c(·) more efficient in terms of the number of parameters, all layers share weights across the four steps except for the initial 1 × 1 convolutional layer. Discussion on modeling order. According to the study on theoretical understanding of masked autoencoder via hierarchical latent variable models, the semantic level of the learned representation varies with the masking ratio [ 12]. Specifically, extremely large or small masking ratios lead to low-level detailed information such as texture, while non-extreme masking ratios result in high- level semantic information. Inspired by this, we can infer that the local and global hyper latent representations correspond to relatively low-level information because they are extracted via limited utilization of the latent representation. The receptive field of the local hyper latent representation is limited by 1×1 convolutional layers. While the receptive field of the global hyper latent representation is whole image area, its attention mechanism selectively use the latent representation. Through the same reasoning, we can infer that regional hyper latent representation corresponds to relatively high-level information. Since different type of contexts has different characteristics, we argue that the modeling order is important for effective entropy modeling. In Figure 11, we empirically confirm that modeling higher-level information (i.e., regional context) first and lower-level information (i.e., local and global contexts) later is more effective than the opposite. In addition, the order between global and local is shown to be not influential. 3.3 Adapt We design an adaptive entropy model on the quantized latent representation ˆy where each element is assumed to follow the Gaussian distribution, and each distribution parameters are obtained from the previous diversification and contextualization stages. Following the previous works [3, 18], we formulate our entropy model as follows: pˆy(ˆy) = Y i \u0010 N \u0000 µi, σ2 i \u0001 ∗ U \u0000 −1 2, 1 2 \u0001\u0011 (ˆyi), (1) where µi and σi are the mean and scale of the Gaussian distribution for each element ˆyi, respectively. The transforms and entropy model are jointly trained in an end-to-end manner by minimizing the expected length of the bitstream (rate) and the expected distortion between the original image and the decoded image, d(·, ·). When a learned entropy model precisely matches the actual probability distribution, the entropy coding algorithm achieves the minimum rate. Therefore, we minimize the cross-entropy between the two distributions. We use mean squared error (MSE) for measuring image distortion. The objective function for our method is as follows: L = Ex∼px \u0002 −log2 pˆy(ˆy) − log2 pˆzl(ˆzl) − log2 pˆzr (ˆzr) − log2 pˆzg (ˆzg) + λ · d(x, ˆx) \u0003 , (2) where px is the distribution of the training dataset, the entropy models pˆzl, pˆzr , and pˆzg are the non-parametric fully factorized entropy models [2], and λ is the Lagrange multiplier that determines weighting between rate and distortion. As the value increases, a model is trained in a direction that reduces information loss, and consequently leads to a higher bit-rate. 60.2 0.4 0.6 0.8 Bit-rate (bpp) 28 30 32 34 36 38PSNR (dB) B (AR) + F (R) (Minnen et al., 2018) B (AR) + F (GL) (Kim et al., 2022) B (ChARM) + F (R) (Minnen & Singh, 2020) B (Quadtree) + F (R) (Li et al., 2023) B (Quadtree) + F (RGL) (Ours, DCA) (a) 0.1 0.2 0.3 0.4 0.5 0.6 Bit-rate (bpp) 30 32 34 36 38PSNR (dB) B (AR) + F (R) (Minnen et al., 2018) B (AR) + F (GL) (Kim et al., 2022) B (ChARM) + F (R) (Minnen & Singh, 2020) B (Quadtree) + F (R) (Li et al., 2023) B (Quadtree) + F (RGL) (Ours, DCA) (b) Figure 4: Performance comparison with latest entropy models on the two benchmark datasets: (a) Kodak and (b) Tecnick. For clear comparisons, we denote each method as follows. B and F mean backward and forward adaptation, respectively, and the corresponding methods are written in parentheses. For backward adaptation, AR, ChARM, and Quadtree represent spatial autoregressive model, channel-wise autoregressive model, and qaudtree partition-based model, respectively. For forward adaptation, L, R, G mean local, regional, and global hyper latent representations, respectively. 4 Experiments We use a PyTorch [19] based open-source library and evaluation platform, CompressAI [4], which has been widely used for developing and evaluating neural image codecs. Training. We set our model parameters as follows: C = 320, Cl = 10, Cr = 192, and N = 8. We train our models corresponding six different bit-rates. We use 300,000 images randomly sampled from the OpenImages [13] dataset. We construct a batch size of 16 with 256×256 patches randomly cropped from different training images. All models are trained for 100 epochs using the Adam optimizer. The learning rate is set to 10−4 up to 90 epoch, and then decreases to 10−5. We use PyTorch v1.9.0, CUDA v11.1, CuDNN v8.0.5, and all experiments are conducted using a single NVIDIA A100 GPU. Evaluation. We evaluate our method on the two popular datasets: Kodak [7] and Tecnick [1]. The Kodak dataset consists of 24 images with a resolution of either 768×512 or 512×768 pixels. The Tecnick dataset is composed of 100 images with a resolution of 1200×1200 pixels. We evaluate our method in terms of rate–distortion performance. For this, we calculate the bits per pixel (bpp) after the encoding phase, and measure distortion between the decoded image and the original image using the peak signal-to-noise ratio (PSNR). 4.1 Comparison with state-of-the-art methods We compare the proposed entropy model, DCA, with state-of-the-art entropy models. DCA can be combined with any transforms; in this paper, DCA is implemented with transforms that have the same structure as in ELIC-sm [9]. For the comparison, we further train image compression methods with four different entropy models [11, 14, 17, 18]. For a fair comparison, they are also implemented with transforms that have the same structure as in ELIC-sm [9]. Rate–Distortion. Figures 4a and 4b show the rate–distortion performance on the Kodak and Tecnick datasets, respectively. The proposed DCA consistently achieves the best rate–distortion performance across all bit-rate regions and two benchmark datasets. Specifically, DCA achieves 11.96% average rate savings over VTM-12.1 on the Kodak dataset, while the second (Lie et al. 2023) [14] and third best (Minnen & Singh, 2020) [17] methods obtain 8.55% and 4.86%, respectively. Complexity. We also evaluate DCA in terms of efficiency. To this end, we provide the de- coding time, the number of model parameters, and Bjøntegaard delta rate (BD-rate) [ 5] in Fig- ure 5. Decoding time is measured on the Kodak dataset using a single NVIDIA V100 GPU. 7Table 1: Performance comparison with state-of-the-art entropy models on Kodak. Methods BD-rate ( %) ↓ Decoding time (ms) ↓ # Parameters (M) ↓ Baseline (CVPR’23) [14] 0.00 67.05 32.64 LIC-TCM (CVPR’23) [15] −0.72 139.04 55.19 MLIC++ (ICMLW’23) [10] 5.76 242.61 107.80 DCA (Ours) −3.73 82.05 37.89 0.00 0.05 0.10 0.15 0.20 12 10 8 6 4 2 0 2 BD-rate (%) 53.1 M 32.6 M 37.9 M 8 9 26.0 M 21.9 M B (AR) + F (R) (Minnen et al., 2018) B (AR) + F (GL) (Kim et al., 2022) B (ChARM) + F (R) (Minnen & Singh, 2020) B (Quadtree) + F (R) (Li et al., 2023) B (Quadtree) + F (RGL) (Ours, DCA) 0.0 0.2 0.4 0.6 0.8 1.0 Decoding time (s) 0.0 0.2 0.4 0.6 0.8 1.0 Figure 5: Performance comparison with latest en- tropy models on the Kodak dataset in terms of decoding time, BD-rate, and model size. Decoding time is measured on a NVIDIA V100 GPU. BD- rate means average rate savings over VTM-12.1. The size of the circle is determined proportionally to the number of model parameters, and the spe- cific numbers are written to the left of the circles. BD-rate means the average bit-rate savings com- pared to a baseline while maintaining the same quality of decoded images. We set VTM-12.1 as the baseline, calculate BD-rate for each im- age in the Kodak dataset, and average them. As shown in Figure 5, our DCA achieves better rate–distortion–computation trade-off than the AR models [11, 18] and the ChARM model [17]. Even compared to the quadtree-based entropy model [14], DCA improves performance signif- icantly, i.e., 3.73% BD-rate gain, without com- promising efficiency as much as possible. Using the same structure of transforms, we ad- ditionally compare DCA with two state-of-the- art entropy models (Table 1), which shows that DCA improves performance most efficiently. It is worth noting that performance improvements are significantly difficult to achieve when there are constraints on compute and memory usage, and this is the achievement of our DCA. Figure 6: Illustration of normalized latent representations ¯yi across four steps using both the baseline and proposed DCA. Each sub-figure includes the minimum and maximum values of normalized latent representations. Notably, the baseline exhibits a broader range of values at the first modeling step, resulting in a higher bit-rate. More examples are provided in the appendix. Probability modeling. To further show the role of the proposed DCA, we measure the normalized latent representation for each step, i.e., ¯yi = (yi −µi)/σi. It provides a standardized measure of how far the latent representation deviates from the predicted mean in terms of estimated standard deviations. Smaller values indicate that a learned entropy model estimates the true probability distribution more accurately. In Figure 6, we compare the result with that of the baseline model [14], which does not leverage diverse contexts for forward adaptation. Here, we have an interesting observation: While the existing work shows significantly high values at the first modeling step, DCA demonstrates consistent modeling performance across four steps. At the first step, since only forward adaptation is possible, the previous approach can utilize only a limited amount of context. On the other hand, DCA enables sufficient contextualization through diverse hyper latent representations, addressing the limitation of previous approach, which has difficulty effectively adapting to various situations. 8Figure 7: Qualitative comparison of the decoded images by the proposed DCA and VTM-12.1. 0.30 0.32 0.34 Bit-rate (bpp) 32.2 32.3 32.4PSNR (dB) 0.64 0.66 0.68 0.70 Bit-rate (bpp) 35.825 35.850 35.875 35.900 35.925PSNR (dB) R R + G R + G + L Large R G + L Figure 8: Analysis of forward context diversity. R, G, L denote the regional, global, and local forward contexts, respectively. “Large R” means using larger amount of the regional context. 0.300 0.302 0.304 0.306 Bit-rate (bpp) 32.37 32.38 32.39 32.40PSNR (dB) 0.884 0.886 0.888 0.890 Bit-rate (bpp) 37.75 37.76 37.77 37.78PSNR (dB) B (LR) + F (LRG) B (LRG) + F (LRG) Figure 9: Analysis of backward context diversity. B and F mean backward and forward contexts, respectively. L, R, G denote the local, regional, and global contexts, respectively. Qualitative results. We provide visual results in Figure 7, showing the decoded image from DCA has better visual quality and a higher PSNR value than that of VTM-12.1, under the same bit-rate. 4.2 Model analysis We conduct detailed analyses of DCA. To do this, we train different variants of DCA depending on various aspects for analysis. All results are shown in two different bit-rate regions (Figures 8 to 12). Analysis of diversification. To validate the effectiveness of diversifying contexts for forward adaptation, we compare three different methods depending on the context diversity (Figure 8): one using regional context (“R”), one using both regional and global contexts (“R + G”), and one using regional, global, and local contexts altogether (“R + G + L”). We use two additional models: one using a larger regional context (“Large R”) and the other using both global and local contexts (“G + L”). The comparison among the first three methods show diversifying forward contexts is effective in both bit-rate regions. Through the results showing that the “Large R” method does not contribute to performance improvement, we once again demonstrate the effectiveness of our diversification. The last one is decomposing regional context into global and local ones rather than diversifying, which is equal to simply adopting the forward adaptation of Informer [11]. The result shows the decomposing approach significantly decreases compression efficiency, and diversifying is more effective. In addition, while our focus lies on diversifying forward context, someone might be curious about whether the context utilized for the quadtree-based backward adaptation is diverse enough. To verify this, we conduct experiments additionally extracting global information from the backward context. Figure 9 demonstrates that there is no distinguishable advantage between two: one simply using the quadtree-based method [14] for backward adaptation (“B (LR) + F (LRG)”) and the other using additional global information for backward adaptation (“B (LRG) + F (LRG)”). This implies that backward adaptation is favorable when focusing on local and regional contexts. Analysis of contextualization. To show the effectiveness of our contextualization approach, we first compare two different methods in Figure 10. “R + G (Step-independent)” combines regional and global information in advance and utilizes the combined one regardless of the step. The other adaptively utilizes regional and global information separately for each step, “R + G (Step-adaptive)”. 90.300 0.305 0.310 0.315 Bit-rate (bpp) 32.30 32.32 32.34 32.36 32.38PSNR (dB) 0.640 0.645 0.650 0.655 0.660 Bit-rate (bpp) 35.850 35.875 35.900 35.925 35.950PSNR (dB) R R + G (Step-independent) R + G (Step-adaptive) Figure 10: Analysis of how to contextualize. R and G denote the regional and global contexts, respectively. “Step-independent” first combines R and G and then utilizes them for all steps, while “Step-adaptive” does not pre-combine them and utilizes each separately for all steps. 0.300 0.305 0.310 0.315 Bit-rate (bpp) 32.30 32.32 32.34 32.36 32.38PSNR (dB) 0.640 0.645 0.650 0.655 0.660 Bit-rate (bpp) 35.850 35.875 35.900 35.925 35.950PSNR (dB) R R G L  R L G  G L R  L G R Figure 11: Analysis of contextualization order. R, G, L denote the regional, global, and local for- ward contexts, respectively.→ means the order. For example. the “R→G→L” method utilizes R, G, and L sequentially. As a reference, we use the model utilizing only regional context, i.e., “R”. The result shows that both are effective and our step-adaptive approach is more beneficial for boosting performance. In addition, we analyze the effectiveness of our modeling order for contextualization in Figure 11. Four different ordering methods and one reference method are used for the comparison. Our ordering approach (“R→G→L”) achieves the best rate–distortion performance, and the methods are catego- rized into two groups based on the performance in both bit-rate regions. Models that prioritize the regional context (“R→G→L” and “R→L→G”) show better performance compared to those that do not prioritize it (“G→L→R” and “L→G→R”). We observe that the method using the opposite mod- eling order of the proposed sequence (“L→G→R”) even exhibits a performance decline compared to the baseline (“R”) in a lower bit-rate region. 0.300 0.305 0.310 0.315 Bit-rate (bpp) 32.30 32.32 32.34 32.36 32.38PSNR (dB) 0.640 0.645 0.650 0.655 0.660 Bit-rate (bpp) 35.850 35.875 35.900 35.925 35.950PSNR (dB) B (CNN) + F (CNN) B (CNN) + F (Attention) B (Attention) + F (Attention) Figure 12: Analysis of model architecture for backward and forward adaptation. B and F mean backward and forward adaptation, respectively. Analysis of architecture. Applying attention mechanisms on various tasks is one of the most actively researched topics. We analyze the effect of the architecture combination for forward and backward adaptation in DCA. Figure 12 compares three different methods: one without attention (“B (CNN) + F (CNN)”), another applying attention only to forward adaptation (“B (CNN) + F (Attnen- tion)”), and the last applying attention for both (“B (Attention) + F (Attention)”). The results show that CNN and attention are effective for forward and backward adaptation, respectively. We infer that focusing on local and regional information is preferable in backward adaptation; thus, a CNN with a locality inductive bias may be more effective. 5 Conclusion In this paper, we proposed a fast and effective entropy modeling framework, DCA, which diversifies forward contexts by extracting local, regional, and global information, and contextualizes current elements with the diverse forward and backward contexts. We demonstrated that our DCA improves rate–distortion performance significantly compared to previous approach without compromising efficiency as much as possible. Furthermore, we provided diverse insights into entropy modeling by conducting a comprehensive and in-depth analysis of the design aspects of DCA. Limitation and future works. To address the limitation of the state-of-the-art entropy models, we focused on paving a novel framework with diverse contexts rather than designing neural architec- tures. Therefore, DCA can be limited by the architectural designs that are inspired by the existing works [11, 14, 22]. In the future, we expect that it would be further improved by neural architectures especially designed for the diverse contexts. In addition, it is worth exploring alternative criteria for diversification beyond the spatial range the contexts covers (i.e., local, regional, and global contexts). 10References [1] N. Asuni and A. Giachetti. TESTIMAGES: a large-scale archive for testing visual devices and basic image processing algorithms. In Proceedings of the Smart Tools and Apps for Graphics (STAG), 2014. [2] J. Ballé, V . Laparra, and E. P. Simoncelli. End-to-end optimized image compression. InProceedings of the International Conference on Learning Representations (ICLR), 2017. [3] J. Ballé, D. Minnen, S. Singh, S. J. Hwang, and N. Johnston. Variational image compression with a scale hyperprior. In Proceedings of the International Conference on Learning Representations (ICLR), 2018. [4] J. Bégaint, F. Racapé, S. Feltman, and A. Pushparaja. CompressAI: a PyTorch library and evaluation platform for end-to-end compression research. arXiv preprint arXiv:2011.03029, 2020. [5] G. Bjøntegaard. Calculation of average PSNR differences between RD-curves. VCEG-M33, 2001. [6] J. Duda. Asymmetric numeral systems: entropy coding combining speed of Huffman coding with compres- sion rate of arithmetic coding. arXiv preprint arXiv:1311.2540, 2013. [7] R. Franzen. Kodak lossless true color image suite. http://r0k.us/graphics/kodak/, 1999. [8] D. He, Y . Zheng, B. Sun, Y . Wang, and H. Qin. Checkerboard context model for efficient learned image compression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. [9] D. He, Z. Yang, W. Peng, R. Ma, H. Qin, and Y . Wang. ELIC: Efficient learned image compression with unevenly grouped space-channel contextual adaptive coding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [10] W. Jiang and R. Wang. MLIC++: Linear complexity multi-reference entropy modeling for learned image compression. In Proceedings of the International Conference on Machine Learning (ICML) Workshop, 2023. [11] J.-H. Kim, B. Heo, and J.-S. Lee. Joint global and local hierarchical priors for learned image compression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [12] L. Kong, M. Q. Ma, G. Chen, E. P. Xing, Y . Chi, L.-P. Morency, and K. Zhang. Understanding masked autoencoders via hierarchical latent variable models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [13] I. Krasin, T. Duerig, N. Alldrin, V . Ferrari, S. Abu-El-Haija, A. Kuznetsova, H. Rom, J. Uijlings, S. Popov, A. Veit, S. Belongie, V . Gomes, A. Gupta, C. Sun, G. Chechik, D. Cai, Z. Feng, D. Narayanan, and K. Murphy. OpenImages: A public dataset for large-scale multi-label and multi-class image classification. Dataset available from https://github.com/openimages, 2017. [14] J. Li, B. Li, and Y . Lu. Neural video compression with diverse contexts. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [15] J. Liu, H. Sun, and J. Katto. Learned image compression with mixed Transformer-CNN architectures. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [16] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and B. Guo. Swin Transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. [17] D. Minnen and S. Singh. Channel-wise autoregressive entropy models for learned image compression. In Proceedings of the IEEE International Conference on Image Processing (ICIP), 2020. [18] D. Minnen, J. Ballé, and G. Toderici. Joint autoregressive and hierarchical priors for learned image compression. In Advances in Neural Information Processing Systems (NeurIPS), 2018. [19] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. PyTorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems (NeurIPS), 2019. [20] Y . Qian, Z. Tan, X. Sun, M. Lin, D. Li, Z. Sun, L. Hao, and R. Jin. Learning accurate entropy model with global reference for image compression. In Proceedings of the International Conference on Learning Representations (ICLR), 2021. 11[21] Y . Qian, X. Sun, M. Lin, Z. Tan, and R. Jin. Entroformer: A transformer-based entropy model for learned image compression. In Proceedings of the International Conference on Learning Representations (ICLR), 2022. [22] Y . Zhu, Y . Yang, and T. Cohen. Transformer-based transform coding. InProceedings of the International Conference on Learning Representations (ICLR), 2022. [23] R. Zou, C. Song, and Z. Zhang. The devil is in the details: Window-based attention for image compression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 12A Additional results We provide the normalized latent representations of images from the Kodak dataset in Figure 13. Figure 13: Illustrations of normalized latent representations ¯yi across four steps for Kodak images using both the baseline [14] and proposed DCA. Each sub-figure includes the minimum and maximum values of normalized latent representations. Notably, the baseline exhibits a broader range of values at the first modeling step, resulting in a higher bit-rate. We provide an in-depth runtime analysis by sub-systems in Table 2. Table 2: Runtime (ms) of DCA. Total encoding/decoding time includes the ANS entropy coding. Transforms Entropy model (DCA) Total fa fs la ls ra rs ga gs c Encoding Decoding 3.46 1.54 0.98 0.96 5.32 4.78 0.63 0.08 14.75 117.49 82.05 13",
      "meta_data": {
        "arxiv_id": "2411.05832v1",
        "authors": [
          "Jun-Hyuk Kim",
          "Seungeon Kim",
          "Won-Hee Lee",
          "Dokwan Oh"
        ],
        "published_date": "2024-11-06T04:30:04Z",
        "pdf_url": "https://arxiv.org/pdf/2411.05832v1.pdf"
      }
    },
    {
      "title": "Entropy Minimization In Emergent Languages",
      "abstract": "There is growing interest in studying the languages that emerge when neural\nagents are jointly trained to solve tasks requiring communication through a\ndiscrete channel. We investigate here the information-theoretic complexity of\nsuch languages, focusing on the basic two-agent, one-exchange setup. We find\nthat, under common training procedures, the emergent languages are subject to\nan entropy minimization pressure that has also been detected in human language,\nwhereby the mutual information between the communicating agent's inputs and the\nmessages is minimized, within the range afforded by the need for successful\ncommunication. That is, emergent languages are (nearly) as simple as the task\nthey are developed for allow them to be. This pressure is amplified as we\nincrease communication channel discreteness. Further, we observe that stronger\ndiscrete-channel-driven entropy minimization leads to representations with\nincreased robustness to overfitting and adversarial attacks. We conclude by\ndiscussing the implications of our findings for the study of natural and\nartificial communication systems.",
      "full_text": "Entropy Minimization In Emergent Languages Eugene Kharitonov 1 Rahma Chaabouni 1 2 Diane Bouchacourt 1 Marco Baroni 1 3 Abstract There is growing interest in studying the lan- guages that emerge when neural agents are jointly trained to solve tasks requiring communication through a discrete channel. We investigate here the information-theoretic complexity of such lan- guages, focusing on the basic two-agent, one- exchange setup. We ﬁnd that, under common training procedures, the emergent languages are subject to an entropy minimization pressure that has also been detected in human language, whereby the mutual information between the com- municating agent’s inputs and the messages is minimized, within the range afforded by the need for successful communication. That is, emer- gent languages are (nearly) as simple as the task they are developed for allow them to be. This pressure is ampliﬁed as we increase communica- tion channel discreteness. Further, we observe that stronger discrete-channel-driven entropy min- imization leads to representations with increased robustness to overﬁtting and adversarial attacks. We conclude by discussing the implications of our ﬁndings for the study of natural and artiﬁcial communication systems. 1. Introduction There has recently been much interest in the analysis of the communication systems arising when deep network agents that interact to accomplish a goal are allowed to exchange language-like discrete messages (Lazaridou et al., 2016; Havrylov & Titov, 2017; Choi et al., 2018; Lazaridou et al., 2018; Li & Bowling, 2019; Chaabouni et al., 2020). Under- standing the emergent protocol is important if we want to eventually develop agents capable of interacting with each other and with us through language (Mikolov et al., 2016; 1Facebook AI Research, Paris, France 2Cognitive Machine Learning (ENS - EHESS - PSL - CNRS - INRIA) 3Catalan Insti- tute for Research and Advanced Studies, Barcelona, Spain. Corre- spondence to: Eugene Kharitonov <kharitonov@fb.com>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). Chevalier-Boisvert et al., 2019). The pursuit might also provide comparative evidence about how core properties of human language have evolved (Kirby, 2002; Hurford, 2014; Harding Graesser et al., 2019). While earlier stud- ies reported ways in which deep agent protocols radically depart from human language (Kottur et al., 2017; Boucha- court & Baroni, 2018; Chaabouni et al., 2019; Lowe et al., 2019), we show here that emergent communication shares an important property of the latter, namely a tendency to- wards entropy minimization. Converging evidence shows that efﬁciency pressures are at work in language and other biological communication systems (Ferrer i Cancho et al., 2013; Gibson et al., 2019). One particular aspect of communicative efﬁciency, robustly observed across many semantic domains, is the tendency to minimize lexicon entropy, to the extent allowed by the coun- teracting need for accuracy (Zaslavsky et al., 2018; 2019). For example, while most languages distinguish grandmoth- ers from grandfathers, few have separate words for mother- and father-side grandmothers, as the latter distinction makes communication only slightly more accurate at the cost of an increase in lexicon complexity (Kemp & Regier, 2012). We show here, in two separate games designed to precisely mea- sure such property, that the protocol evolved by interacting deep agents is subject to the same complexity minimiza- tion pressure. Entropy minimization in natural language has been con- nected to the Information Bottleneck principle (Tishby et al., 1999). In turn, complexity reduction due to the Information Bottleneck provides a beneﬁcial regularization effect on learned representations (Fischer, 2019; Alemi et al., 2016; Achille & Soatto, 2018a;b). It is difﬁcult to experimentally verify the presence of such effect in human language, but we can look for it in our computational simulations. We conﬁrm that, when relaxing channel discreteness, the en- tropy minimization property no longer holds, and the system becomes less robust against overﬁtting and adversarial noise. This in turn raises intriguing questions about the origin of discreteness in human language, that we return to in the conclusion. arXiv:1905.13687v3  [cs.CL]  26 Jun 2020Entropy Minimization In Emergent Languages 2. General framework We establish our results in the context of signaling games (Lewis, 1969), as introduced to the current language emergence literature by Lazaridou et al. (2016) and adopted in several later studies (Havrylov & Titov, 2017; Boucha- court & Baroni, 2018; Lazaridou et al., 2018). There are two agents, Sender and Receiver, provided with individual in- puts at the beginning of each episode. Sender sends a single message to Receiver, and Receiver has to perform an action based on its own input and the received message. Impor- tantly, there is no direct supervision on the message protocol. We consider agents that are deterministic functions of their inputs (after training). As an example, consider the task of communicating a n-bit number, sampled uniformly at random from 0...2n −1. The full number is shown to Sender, and its k (0 ≤k ≤n) least-signiﬁcant bits are also revealed to Receiver. Receiver has to output the full number, based on the message from Sender and its own input. Would Sender transmit the en- tire number through its message? In this case, the protocol would be “complex,” encodingnbits. Alternatively, Sender could only encode the bits that Receiver does not know, and let Receiver ﬁll in the rest by itself. This emergent protocol would be “simple,” encoding only strictly necessary infor- mation. We ﬁnd experimentally that, once the agents are successfully trained to jointly solve the task, the emergent protocol minimizes the entropy of the messages or, equiva- lently in our setup, the mutual information between Sender’s input and messages. In other words, the agents consistently approximate the simplest successful protocol (in the current example, the one transmitting ≈n−kbits). We can connect the entropies of Sender and Receiver in- puts is and ir, messages m, Receiver’s output (the chosen action) o, and ground-truth outputs lby standard inequali- ties (Cover & Thomas, 2012).1 Denoting Sender’s computa- tion as a function S : S(is) =m, and Receiver as function R: R(m,ir) =o, we obtain: H(is) ≥H(S(is)) =H(m) ≥H(m|ir) ≥ ≥H(R(m,ir)|ir) =H(o|ir) ≈H(l|ir), (1) where the last relation stems from the fact that after success- ful training o≈l. Note that, since agents are deterministic after training, H(m) = I(is; m). We can then use these quantities interchangeably. Our empirical measurements indicate that the entropy of the messages min the emergent protocol tends to approach the lower bound: H(m) →H(l|ir), even if the upper bound H(is) is far. that Receiver needs is reduced without chang- ing other parameters, the emergent protocol becomes sim- 1We also use the fact that that H(x) ≥ H(g(x)) for any dis- crete r.v. xand function g. pler (lower entropy). In other words, the emergent protocol adapts to minimize the information that passes through it. Code for our experiments is publicly available at github.com/facebookresearch/EGG/ as a part of the EGG framework (Kharitonov et al., 2019). 3. Methodology 3.1. Games We study two signaling games. In Guess Number, the agents are trained to recover an integer-representing vector with uniform Bernoulli-distributed components. This simple setup gives us full control over the amount of information needed to solve the task. The second game, Image Classi- ﬁcation, employs more naturalistic data, as the agents are jointly trained to classify pairs of MNIST digits (LeCun et al., 1998b). Guess Number We draw an 8-bit integer 0 ≤z ≤255 uniformly at random, by sampling its 8 bits independently from the uniform Bernoulli distribution. All bits are revealed to Sender as an 8-dimensional binary vector is. The last k bits are revealed to Receiver ( 0 ≤k ≤8) as its input ir. Sender outputs a single-symbol message mto Receiver. In turn, Receiver outputs a vector othat recovers all the bits of zand should be equal to is. In this game, Sender has a linear layer that maps the input vector is to a hidden representation of size 10, followed by a leaky ReLU activation. Next is a linear layer followed by a softmax over the vocabulary. Receiver linearly maps both its input ir and the message to 10-dimensional vectors, concatenates them, applies a fully connected layer with output size 20, followed by a leaky ReLU. Finally, another linear layer and a sigmoid nonlinearity are applied. When training with REINFORCE and the Stochastic Computation graph approach (see Sec. 3.2), we increase the hidden layer sizes threefold, as this leads to a more robust convergence. Image Classiﬁcation In this game, the agents are jointly trained to classify 28x56 images of two MNIST digits, stacked side-by-side (more details in Supplementary). Un- like Guess Number, Receiver has no side input. Instead, we control the informational complexity of Receiver’s task by controlling the size of its output space, i.e., the number of labels we assign to the images. To do so, we group all two- digit sequences 00..99 into Nl ∈{2,4,10,20,25,50,100} equally-sized classes. In Sender, input images are embedded by a LeNet-1 in- stance (LeCun et al., 1990) into 400-dimensional vectors. These embedded vectors are passed to a fully connected layer, followed by a softmax selecting a vocabulary sym- bol. Receiver embeds the received messages into 400- dimensional vectors, passed to a fully connected layer withEntropy Minimization In Emergent Languages a softmax activation returning the class probabilities. We report hyperparameter grids in Supplementary. In the following experiments, we ﬁx vocabulary to 1024 symbols (experiments with other vocabulary sizes, multi-symbol mes- sages, and larger architectures are reported in Supplemen- tary). No parts of the agents are pre-trained or shared. The loss being optimized depends on the chosen gradient estima- tion method (see Sec. 3.2). We denote it L(o,l), and it is a function of Receiver’s outputoand the ground-truth output l. When training in Guess Number with REINFORCE, we use a 0/1 loss: the agents get zero loss only when all bits of zare correctly recovered. When training with Gumbel- Softmax relaxation or the Stochastic Computation Graph approach, we use binary cross-entropy (Guess Number) and negative log-likelihood (Image Classiﬁcation). 3.2. Training with discrete channel Training to communicate with discrete messages is non- trivial, as we cannot back-propagate through the messages. Current language emergence work mostly uses Gumbel- Softmax relaxation (e.g., Havrylov & Titov, 2017) or RE- INFORCE (e.g., Lazaridou et al., 2016) to get gradient esti- mates. We also explore the Stochastic Computation Graph optimization approach. We plug the obtained gradient esti- mates into Adam (Kingma & Ba, 2014). Gumbel-Softmax relaxation Samples from the Gumbel- Softmax distribution (a) are reperameterizable, hence allow gradient-based training, and (b) approximate samples from the corresponding Categorical distribution (Maddison et al., 2016; Jang et al., 2016). To get a sample that approximates an n-dimensional Categorical distribution with probabilities pi, we draw ni.i.d. samples gi from Gumbel(0,1) and use them to calculate a vector ywith components: yi = exp[(gi + logpi)/τ]∑ j exp[(gj + logpj)/τ], (2) where τ is the temperature hyperparameter. As τ tends to 0, the samples yget closer to one-hot samples; as τ →+∞, the components yi become uniform. During training, we use these relaxed samples as messages from Sender, making the entire Sender/Receiver setup differentiable. REINFORCE by Williams (1992) is a standard reinforce- ment learning algorithm. In our setup, it estimates the gradi- ent of the expectation of the loss L(o,l) w.r.t. the parameter vector θas follows: Eis,ir Em∼S(is),o∼R(m,ir) [(L(o; l) −b)∇θlog Pθ(m,o)] (3) The expectations are estimated by sampling mfrom Sender and, after that, sampling ofrom Receiver. We use the run- ning mean baseline b(Greensmith et al., 2004; Williams, 1992) as a control variate. We adopt the common trick to add an entropy regularization term (Williams & Peng, 1991; Mnih et al., 2016) that favors higher entropy. We impose entropy regularization on the outputs of the agents with coefﬁcients λs (Sender) and λr (Receiver). Stochastic Computation Graph (SCG) In our setup, the gradient estimate approach of Schulman et al. (2015) re- duces to computing the gradient of the surrogate function: Eis,ir Em∼S(is) [L(o; l) +sg(L(o; l) −b) logPθ(m)] , (4) where sgdenotes stop-gradient operation. We do not sample Receiver actions: Its parameter gradients are obtained with standard backpropagation (ﬁrst term in Eq. 4). Sender’s messages are sampled, and its gradient is calculated akin to REINFORCE (second term in Eq. 4). Again, we apply entropy-favoring regularization on Sender’s output (with coefﬁcient λs) and use the mean baseline. Role of entropy regularization As we mentioned above, when training with REINFORCE and SCG, we include a (standard) entropy regularization term in the loss which explicitly maximizes entropy of Sender’s output. Clearly, this term is at odds with the entropy minimization effect we observe. In our experiments, we found that high values ofλs (the parameter controlling Sender’s entropy regularization) prevent communication success; on the other hand, a small non-zero λs is crucial for successful training. In Sec. 4 we investigate the effect of λs on entropy minimization.2 3.3. Experimental protocol In Guess Number, we use all 28 possible inputs for train- ing, early stopping and analysis. In Image Classiﬁcation, we train on random image pairs from the MNIST training data, and use image pairs from the MNIST held-out set for validation. We select the runs that achieved a high level of performance (training accuracy above 0.99 for Guess Number and validation accuracy above 0.98 for Image Clas- siﬁcation), thus studying typical agent behavior provided they succeeded at the game. At test time, we select the Sender’s message symbol greed- ily, hence the messages are discrete and Sender represents a (deterministic) function S of its input is, m = S(i). Calculating the entropy H(m) of the distribution of dis- crete messages mis straightforward. In Guess Number, we enumerate all 256 possible values of zas inputs, obtain messages and calculate entropy H(m). For Image Classiﬁ- cation, we sample image pairs from the held-out set. The upper bound on H(m) is as follow: Hmax = 8bits (bounded by H(is)) in Guess Number, and Hmax = 10 2The parameter λr, that controls Receiver’s entropy regulariza- tion, does not inﬂuence the observed effect.Entropy Minimization In Emergent Languages bits (bounded by vocabulary size) in Image Classiﬁcation. Its lower bound is equal to Hmin = H(l|ir) = 8−kbits for Guess number. In Image Classiﬁcation, communica- tion can only succeed if H(m) is not less than H(l), i.e., Hmin = H(l) = log2 Nl, with Nl the number of equally- sized classes we split the images into. 4. Experiments 4.1. Entropy minimization Guess Number In Figure 1, the horizontal axes span the number of bits of z that Receiver lacks, 8 −k. The ver- tical axis reports the information content of the protocol, measured by messages entropy H(m). Each integer on the horizontal axis corresponds to a game conﬁguration, and for each such conﬁguration we aggregate multiple (suc- cessful) runs with different hyperparameters and random seeds. Hmin indicates the minimal amount of bits Sender has to send in a particular conﬁguration for the task to be solvable. The upper bound (not shown) is Hmax = 8bits. Across hyperparameters and random seeds, trainings with Gumbel-Softmax and SCG have success rate above 50%. With REINFORCE success rate is approximately 20%. Consider ﬁrst the conﬁgurations where Receiver’s input is insufﬁcient to answer correctly (at least one binary digit hidden, k≤7). From Figure 1a, we observe that the trans- mitted information is strictly monotonically increasing with the number of binary digits hidden from Receiver. Thus, even if Sender sees the very same input in all conﬁgura- tions, a more nuanced protocol is only developed when it is necessary. Moreover, the entropy H(m) (equivalently: the transmitted information) stays close to the lower bound. This entropy minimization property holds for all the consid- ered training approaches across all conﬁgurations. Consider next the conﬁguration where Receiver is getting the whole integer zas its input (k= 8, the leftmost conﬁgu- ration in Figure 1, corresponding to 0 on x axis). Based on the observations above, one would expect that the protocol would approach zero entropy in this case (as no informa- tion needs to be transmitted). However, the measurements indicate that the protocol is encoding considerably more information. It turns out that this information is entirely ignored by Receiver. To demonstrate this, we fed all pos- sible distinct inputs to Sender, retrieved the corresponding messages, and shufﬂed them to destroy any information about the inputs they might carry. The shufﬂed messages were then passed to Receiver alongside its own (un-shufﬂed) inputs. The overall performance was not affected by this manipulation, conﬁrming the hypothesis that Receiver ig- nores the messages. We conclude that in this case there is no entropy minimization pressure on Sender simply be- cause there is no communication. The full experiment is in Supplementary. We further consider the effect of various hyperparameters. In Figure 1b, we split the results obtained with Gumbel- Softmax by relaxation temperature. As discussed in Sec. 3.2, lower temperatures more closely approximate discrete com- munication, hence providing a convenient control of the level of discreteness imposed during training (recall that at test time we enforce full discreteness by selecting the symbol greedily). The ﬁgure shows that lower tempera- tures consistently lead to lower H(m). This implies that, as we increase the “level of discreteness” at training, we get stronger entropy minimization pressure. In Figures 1c & 1d, we report H(m) when training with Stochastic Graph Optimization and REINFORCE across degrees of entropy regularization. We report curves corre- sponding to λs values which converged in more than three conﬁgurations. With REINFORCE, we see a weak tendency for a higher λs to trigger a higher entropy in the protocol. However, message entropy stays generally close to the lower bound even in presence of strong exploration, which favors higher entropy in Sender’s output distribution. Image Classiﬁcation As the models are more complex, we only had consistent success when training with Gumbel- Softmax (success rate is approximately 80%). In Figure 2a we aggregate all successful runs. The information encoded by the protocol grows as Receiver’s output requires more information. However, in all conﬁgurations, the transmit- ted information stays well below the 10-bit upper bound and tends to be close to Hmin. A natural interpretation is that Sender prefers to take charge of image classiﬁcation and directly pass information about the output label, rather than sending along a presumably more information-heavy description of the input. In Figure 2b, we split the runs by temperature. Again, we see that lower temperatures consis- tently lead to stronger entropy minimization pressures. Summarizing, when communicating through a discrete chan- nel, there is consistent pressure for the emergent protocol to encode as little information as necessary. This holds across games, training methods and hyperparameters. When train- ing with Gumbel-Softmax, temperature controls the strength of this pressure, conﬁrming the relation between entropy minimization and discreteness. 4.2. Evolution of message entropy during training To gain further insights into the minimization trend, we studied the evolution of message entropy during training. We observed that the initial entropy of Sender can be both higher and lower than the minimum entropy Hmin required for solving the task. Further, we measured how the en- tropy of the messages changes after each training epoch by applying the same procedure as above, i.e., feeding theEntropy Minimization In Emergent Languages 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits Gumbel-Softmax Stoch. computation REINFORCE Hmin (a) All three training approaches. 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits  = 1.0  = 1.25  = 1.5 Hmin (b) Training with Gumbel-Softmax relaxation. 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits s = 0.01 s = 0.025 s = 0.05 s = 0.075 s = 0.1 Hmin (c) Training with Stochastic Computation Graph. 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits s = 0.01 s = 0.025 s = 0.05 s = 0.075 s = 0.1 s = 0.5 Hmin (d) Training with REINFORCE. Figure 1.Guess Number: entropy of the messages m. Shaded regions represent one standard error of the mean (SEM). entire dataset to Sender and selecting the message symbol greedily. When message entropy starts higher than Hmin, it falls close to it during the training. Similarly, when it starts lower than Hmin, it increases during training. This experiment is reported in Supplementary. Thus, information minimization is not simply due to the difﬁculty of discov- ering a higher-entropy protocol during learning, but also due to the complexity of maintaining mutual coordination between the agents. 4.3. Representation discreteness and robustness The entropy minimization effect indicates that a discrete rep- resentation will only store as much information as necessary to solve the task. This emergent behavior resembles the In- formation Bottleneck principle (Tishby et al., 1999; Achille & Soatto, 2018a). The fact that lower training-time temper- atures in Gumbel-Softmax optimization correlate with both higher discreteness and a tighter bottleneck (see Sec. 3.3) makes us further conjecture that discreteness is causally connected to the emergent bottleneck. The Information Bottleneck principle has also been claimed to govern en- tropy minimization in natural language (Zaslavsky et al., 2018; 2019). Bottleneck effects in neural agents and natural language might be due to the same cause, namely communi- cation discreteness. Further, we hypothesize that the emergent discrete bottle- neck might have useful properties, since existing (continu- ous) architectures that explicitly impose a bottleneck pres- sure are more robust to overﬁtting (Fischer, 2019) and ad- versarial attacks (Alemi et al., 2016; Fischer, 2019). We test whether similar regularization properties also emerge in our computational simulations (without any explicit pressure imposed through the cost function), and whether they are correlated with communication channel discreteness. If this connection exists, it also suggests that discreteness might be “beneﬁcial” to human languages for the same reasons. 4.3.1. R OBUSTNESS TO OVER -FITTING To assess our hypotheses, we consider the Image Classiﬁ- cation game (Nl = 10) in presence of randomly-shufﬂed training labels (the test set is untouched) (Zhang et al., 2016). This task allows us to explore whether the discrete commu- nication bottleneck is associated to robustness to overﬁtting, and whether the latter depends on discreteness level (con- trolled by the temperature τ of Gumbel-Softmax). We use the same architecture as above. The agents are trained withEntropy Minimization In Emergent Languages 1 2 3 4 5 6 log2Nl 1 2 3 4 5 6 7H(m), bits All runs Hmin (a) Successful runs pooled together. 1 2 3 4 5 6 log2Nl 1 2 3 4 5 6 7H(m), bits =0.75 =1.0 =1.5 =2.0 Hmin (b) Successful runs grouped by temperature. Figure 2.Image Classiﬁcation: entropy of the messages min function of log number of target classes, Nl. Shaded regions mark SEM. Gumbel-Softmax relaxation; at test-time the communication is fully discrete. We also consider two baseline architectures without the discrete channel. In Linear, the fully connected output layer of Sender is directly connected to the linear embedding input of Receiver. Softmax (SM) places a softmax activation (with temperature) after Sender’s output layer and passes the result to Receiver. We vary temperature and proportion of training examples with shufﬂed labels. We use temperatures τ = 1.0 and τ = 10.0 (the agents reach a test accuracy of 0.98 when trained with these temperatures on the original training set). SM with τ = 1.0 and τ = 10.0 behave similarly, hence we only report SM with τ = 1.0. Figure 3a shows training accuracy when all labels are shuf- ﬂed. Linear and SM ﬁt the random labels almost perfectly within the ﬁrst 150 epochs. With τ = 10.0, GS achieves 0.8 accuracy within 200 epochs. When GS with τ = 1.0 is considered, the agents only start to improve over random guessing after 150 epochs, and accuracy is well below 0.2 after 200 epochs. As expected, test set performance is at chance level (Figure 3b). In the next experiment, we shufﬂe labels for a randomly selected half of the training instances. Train and test accuracies are shown in Figures 3c and 3d, respectively. All models initially ﬁt the true-label exam- ples (train accuracy ≈0.5, test accuracy ≈0.97). With more training, the baselines and GS with τ = 10.0 start (over)ﬁtting the random labels, too: train accuracy grows, while test accuracy falls. In contrast, GS with τ = 1.0 does not ﬁt random labels, and its test accuracy stays high. Note that SM patterns with Linear and high-temperature GS, showing that the training-time discretization noise in GS is instrumental for robustness to over-ﬁtting. We interpret the results as follows. To fully exploit their joint capacity for “successful” over-ﬁtting, the agents need to coordinate label memorization. This requires passing large amounts of information through the channel. With a low temperature (more closely approximating a discrete channel), this is hard, due to a stronger entropy minimiza- tion pressure. To test the hypothesis, we run an experiment where all labels are shufﬂed and a layer of size 400x400 is either added to Sender (just before the channel) or to Re- ceiver (just after the channel). We predict that, with higherτ (less discrete, less entropy minimization pressure), the train- ing curves will be close, as the extra capacity can be used for memorization equally easy in both cases. With lower τ (more discrete, more pressure), the accuracy curves will be more distant, as the extra capacity can only be successfully exploited for memorization when placed before the channel. Figures 3e & 3f bear out the prediction. 4.3.2. R OBUSTNESS TO ADVERSARIAL EXAMPLES We study next robustness of agents equipped with a relaxed discrete channel against adversarial attacks. We use the same architectures as in the preceding experiment. We train agents with different random seeds and imple- ment white-box attacks on the trained models, varying tem- perature τ and the allowed perturbation norm, ϵ. We use the standard Fast Gradient Sign Method of (Goodfellow et al., 2014). The original image is is perturbed to i∗ s along the direction that maximizes the loss of Receiver’s output o= R(S(is)) w.r.t. the ground-truth class l: i∗ s = clip[is + ϵ·sign[∇is L(o,l)] ,0,1] , (5) where ϵcontrols the L∞norm of the perturbation. Under an attack with a ﬁxed ϵ, a more robust method will have a higher accuracy. To avoid numerical stability issues akin to those reported by (Carlini & Wagner, 2016), all computa- tions are done in 64-bit ﬂoats. We experiment with two approaches of getting gradients forEntropy Minimization In Emergent Languages 0 50 100 150 200 Epochs 0.0 0.2 0.4 0.6 0.8 1.0Train accuracy GS, =1.0 GS, =10.0 Linear SM, =1.0 (a) All train labels are shufﬂed. 0 50 100 150 200 Epochs 0.0 0.2 0.4 0.6 0.8 1.0Test accuracy GS, =1.0 GS, =10.0 Linear SM, =1.0  (b) All train labels are shufﬂed. 0 50 100 150 200 Epochs 0.0 0.2 0.4 0.6 0.8 1.0Train accuracy GS, =1.0 GS, =10.0 Linear SM, =1.0 (c) Half of train labels are shufﬂed. 0 50 100 150 200 Epochs 0.0 0.2 0.4 0.6 0.8 1.0Test accuracy GS, =1.0 GS, =10.0 Linear SM, =1.0  (d) Half of train labels are shufﬂed. 0 200 400 600 800 1000 Epochs 0.0 0.2 0.4 0.6 0.8 1.0Train accuracy =10.0, before channel =10.0, after channel (e) All labels shufﬂed; Additional layer before channel vs. after channel 0 200 400 600 800 1000 Epochs 0.0 0.2 0.4 0.6 0.8 1.0Train accuracy =1.0, before channel =1.0, after channel (f) All labels shufﬂed; Additional layer before channel vs. after chan- nel Figure 3.Learning in presence of random labels. GS (SM) denotes models trained with Gumbel-Softmax (Softmax) channel. Linear are models with the channel removed.Entropy Minimization In Emergent Languages 0.00 0.05 0.10 0.15 0.20 0.25 Perturbation norm,  0.2 0.4 0.6 0.8 1.0Accuracy GS, =0.1 GS, =1.0 GS, =10.0 (a) Robustness vs. temp. τ. 0.00 0.05 0.10 0.15 0.20 0.25 Perturbation norm,  0.0 0.2 0.4 0.6 0.8 1.0Accuracy GS, =0.1 SM, =1.0 Linear (b) Comparison to the baselines. Figure 4.Robustness to adversarial examples: higher accuracy given ﬁxed ϵimplies more robustness. the attack. Under the ﬁrst approach, the gradient ∇is L(o,l) is estimated using the standard Gumbel-Softmax relaxation. It is possible, however, that the randomization that Gumbel- Softmax uses internally reduces the usefulness of gradients used for the attack. Hence we also experiment with a setup that is easier for an adversary: after training (and during the attack), we replace the Gumbel-Softmax by a softmax non-linearity with the same temperature. We found that performance in these two setups is virtually the same, indi- cating that the obtained robustness results are independent from the randomization in the channel. Rather, they are due to emergence of well-separated “categories” during training. As in the preceding experiment, SM behaves similarly with different temperatures (we experimented with τ ∈ {0.1,1.0,10.0}): we only report results with τ = 1.0. Fig- ure 4a shows that, as temperature decreases, the accuracy drop also decreases. The highest robustness is achieved with τ = 0.1. Comparison with the baselines (Figure 4b) conﬁrms that relaxed discrete training with τ = 0.1 im- proves robustness. In sum, increased channel discreteness makes it harder to transmit large amounts of information, and leads to in- creased robustness against over-ﬁtting and adversarial ex- amples. Discreteness brings about a bottleneck that has beneﬁcial properties, which might ultimately provide a mo- tivation for why an emergent communication system should evolve towards discreteness. 5. Related Work We brieﬂy reviewed studies of emergent deep agent commu- nication and entropy minimization in human language in the introduction. We are not aware of earlier work that looks for this property in emergent communication, although Evti- mova et al. (2018) used information theory to study protocol development during learning, and, closer to us, K˚ageb¨ack et al. (2018) studied the effect of explicitly adding a com- plexity minimization term to the cost function of an emer- gent color-naming system. Discrete representations are explored in many places (e.g., van den Oord et al., 2017; Jang et al., 2016; Rolfe, 2016). However, these works focus on ways to learn good discrete representations, rather than analyzing the properties of rep- resentations that are independently emerging on the side. Furthermore, our study extends to agents communicating with variable-length messages, produced and consumed by GRU (Cho et al., 2014) and Transformer (Vaswani et al., 2017) cells (see Supplementary). The sequential setup is speciﬁc to language, clearly distinguished from the settings studied in generic sparse-representation work. Other studies, inspired by the Information Bottleneck prin- ciple, control the complexity of neural representations by regulating their information content (Strouse & Schwab, 2017; Fischer, 2019; Alemi et al., 2016; Achille & Soatto, 2018a;b). While they externally impose the bottleneck, we observe that the latter is an intrinsic feature of learning to communicate through a discrete channel. 6. Discussion Entropy minimization is pervasive in human language, where it constitutes a speciﬁc facet of the more general pressure towards communication efﬁciency. We found that the same property consistently characterizes the protocol emerging in simulations where two neural networks learn to solve a task jointly through a discrete communication code. In a comparative perspective, we hypothesize that entropy minimization is a general property of discrete communica- tion, independent of speciﬁc biological constraints humans are subject to. In particular, our analysis tentatively estab- lishes a link between this property and the inherent difﬁculty of encoding information in discrete form (cf. the effect of adding a layer before or after the communication bottleneck in the over-ﬁtting experiment).Entropy Minimization In Emergent Languages Exploring entropy minimization in computational simula- tions provides a ﬂexibility we lack when studying humans. For example, we uncovered here initial evidence that the communication bottleneck is acting as a good regularizer, making the joint agent system more robust to noise and adversarial examples. This leads to an intriguing conjec- ture on the origin of language. Its discrete nature is often traced back to the fact that it allows us to produce an in- ﬁnite number of expressions by combining a ﬁnite set of primitives (e.g., Berwick & Chomsky, 2016). However, it is far from clear that the need to communicate an inﬁnite number of concepts could have provided the initial pressure to develop a discrete code. More probably, once such code independently emerged, it laid the conditions to develop an inﬁnitely expressive language (Bickerton, 2014; Collier et al., 2014). Our work suggests that, because of its inherent regularizing effect, discrete coding is advantageous already when communication is about a limited number of concepts, providing an alternative explanation for its origin. In the future, we would like to study more continuous seman- tic domains, such as color maps, where perfect accuracy is not easily attainable, nor desirable. Will the networks ﬁnd an accuracy/complexity trade-off similar to those at- tested in human languages? Will other core language prop- erties claimed to be related to this trade-off, such as Zipﬁan frequency distributions (Ferrer i Cancho & D ´ıaz-Guilera, 2007), concurrently emerge? We would also like to compare the performance of human subjects equipped with novel con- tinuous vs. discrete communication protocols, adopting the methods of experimental semiotics (Galantucci, 2009). We expect discrete protocols to be more general and robust. Our results have implications for the efforts to evolve agents interacting with each other and with humans through a dis- crete channel. First, because of entropy minimization, we should not agents to develop a richer protocol than the sim- plest one ensuring accurate communication. For example, Bouchacourt & Baroni (2018) found that agents trained to discriminate pairs of natural images depicting instances of about 500 high-level categories, such as cats and dogs, de- veloped a lexicon that does not denote such categories, but low-level properties of the images themselves. This makes sense from an entropy-minimization perspective, as talking about the 500 high-level categories demands log2 500 bits of information, whereas many low-level strategies (e.g., dis- criminating average pixel intensity in the images) will only require transmitting a few bits. To have agents developing rich linguistic protocols, we must face them with varied challenges that truly demand them. Second, the focus on a discrete protocol is typically moti- vated by the goal to develop machines eventually able to communicate with humans. Indeed, discrete messages are not required in multi-agent scenarios where no human in the loop is foreseen (Sukhbaatar et al., 2016). Our results sug- gest that, long before agents reach the level of complexity necessary to converse with humans, there are independent reasons to encourage discreteness, as it leads to simpler protocols and it provides a source of robustness in a noisy world. An exciting direction for future applied work will be to test the effectiveness of discrete communication as a general form of representation learning. Acknowledgements The authors thank Emmanuel Dupoux for discussions and the anonymous reviewers for their feed- back. References Achille, A. and Soatto, S. Information dropout: Learning optimal representations through noisy computation. IEEE TPAMI, 40(12):2897–2905, 2018a. Achille, A. and Soatto, S. Emergence of invariance and disentanglement in deep representations. The Journal of Machine Learning Research, 19(1):1947–1980, 2018b. Alemi, A. A., Fischer, I., Dillon, J. V ., and Murphy, K. Deep variational information bottleneck. arXiv preprint arXiv:1612.00410, 2016. Berwick, R. and Chomsky, N. Why Only Us: Language and Evolution. MIT Press, Cambridge, MA, 2016. Bickerton, D. More than Nature Needs: Language, Mind, and Evolution. Harvard University Press, Cambridge, MA, 2014. Bouchacourt, D. and Baroni, M. How agents see things: On visual representations in an emergent language game. In EMNLP, 2018. Carlini, N. and Wagner, D. Defensive distillation is not robust to adversarial examples. arXiv preprint arXiv:1607.04311, 2016. Chaabouni, R., Kharitonov, E., Dupoux, E., and Baroni, M. Anti-efﬁcient encoding in emergent communication. In NeurIPS, 2019. Chaabouni, R., Kharitonov, E., Bouchacourt, D., Dupoux, E., and Baroni, M. Compositionality and generalization in emergent languages. In ACL, 2020. Chevalier-Boisvert, M., Bahdanau, D., Lahlou, S., Willems, L., Saharia, C., Nguyen, T. H., and Bengio, Y . BabyAI: A platform to study the sample efﬁciency of grounded language learning. In ICLR, 2019. Cho, K., Van Merri ¨enboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y . Learn- ing phrase representations using rnn encoder-decoderEntropy Minimization In Emergent Languages for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014. Choi, E., Lazaridou, A., and de Freitas, N. Compositional obverter communication learning from raw visual input. arXiv preprint arXiv:1804.02341, 2018. Collier, K., Bickel, B., van Schaik, C., Manser, M., and Townsend, S. Language evolution: Syntax before phonol- ogy? Proceedings of the Royal Society B: Biological Sciences, 281(1788):1–7, 2014. Cover, T. M. and Thomas, J. A. Elements of Information Theory. John Wiley & Sons, 2012. Evtimova, K., Drozdov, A., Kiela, D., and Cho, K. Emergent communication in a multi-modal, multi-step referential game. In ICLR, 2018. Ferrer i Cancho, R. and D´ıaz-Guilera, A. The global minima of the communicative energy of natural communication systems. Journal of Statistical Mechanics: Theory and Experiment, 2007(06):P06009, 2007. Ferrer i Cancho, R., Hern´andez-Fern´andez, A., Lusseau, D., Agoramoorthy, G., Hsu, M., and Semple, S. Compression as a universal principle of animal behavior. Cognitive Science, 37(8):1565–1578, 2013. Fischer, I. The conditional entropy bottleneck, 2019. URL https://openreview.net/forum? id=rkVOXhAqY7. Galantucci, B. Experimental semiotics: A new approach for studying communication as a form of joint action. Topics in Cognitive Science, 1(2):393–410, 2009. Gibson, E., Piantadosi, R. F. S., Dautriche, I., Mahowald, K., Bergen, L., and Levy, R. How efﬁciency shapes human language. Trends in Cognitive Science, 2019. In press. Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain- ing and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. Greensmith, E., Bartlett, P. L., and Baxter, J. Variance reduc- tion techniques for gradient estimates in reinforcement learning. JMLR, 5(Nov):1471–1530, 2004. Harding Graesser, L., Cho, K., and Kiela, D. Emergent lin- guistic phenomena in multi-agent communication games. In EMNLP, 2019. Havrylov, S. and Titov, I. Emergence of language with multi- agent games: Learning to communicate with sequences of symbols. In NIPS, 2017. Hurford, J. The Origins of Language. Oxford University Press, Oxford, UK, 2014. Jang, E., Gu, S., and Poole, B. Categorical repa- rameterization with Gumbel-Softmax. arXiv preprint arXiv:1611.01144, 2016. K˚ageb¨ack, M., Dubhashi, D., and Sayeed, A. DeepColor: Reinforcement learning optimizes information efﬁciency and well-formedness in color name partitioning. In Pro- ceedings of CogSci, pp. 1895–1900, Austin, TX, 2018. Kemp, C. and Regier, T. Kinship categories across lan- guages reﬂect general communicative principles. Science, 336(6084):1049–1054, 2012. Kharitonov, E., Chaabouni, R., Bouchacourt, D., and Ba- roni, M. EGG: a toolkit for research on Emergence of lanGuage in Games. In EMNLP: System Demonstrations, 2019. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Kirby, S. Natural language from artiﬁcial life. Artiﬁcial life, 8(2):185–215, 2002. Kottur, S., Moura, J. M., Lee, S., and Batra, D. Natural lan- guage does not emerge “naturally” in multi-agent dialog. arXiv preprint arXiv:1706.08502, 2017. Lazaridou, A., Peysakhovich, A., and Baroni, M. Multi- agent cooperation and the emergence of (natural) lan- guage. arXiv preprint arXiv:1612.07182, 2016. Lazaridou, A., Hermann, K. M., Tuyls, K., and Clark, S. Emergence of linguistic communication from referential games with symbolic and pixel input. arXiv preprint arXiv:1804.03984, 2018. LeCun, Y ., Boser, B. E., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W. E., and Jackel, L. D. Hand- written digit recognition with a back-propagation network. In NIPS, 1990. LeCun, Y ., Bottou, L., Bengio, Y ., and Haffner, P. Gradient- based learning applied to document recognition. Proceed- ings of the IEEE, 86(11):2278–2324, 1998a. LeCun, Y ., Bottou, L., Bengio, Y ., Haffner, P., et al. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998b. Lewis, D. Convention harvard university press. Cambridge, MA, 1969. Li, F. and Bowling, M. Ease-of-teaching and language structure from emergent communication. In NeurIPS. 2019.Entropy Minimization In Emergent Languages Lowe, R., Foerster, J., Boureau, Y ., Pineau, J., and Dauphin, Y . On the pitfalls of measuring emergent communica- tion. In Proceedings of AAMAS, pp. 693–701, Montreal, Canada, 2019. Maddison, C. J., Mnih, A., and Teh, Y . W. The concrete distribution: A continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016. Mikolov, T., Joulin, A., and Baroni, M. A roadmap towards machine intelligence. In International Conference on In- telligent Text Processing and Computational Linguistics, pp. 29–61. Springer, 2016. Mnih, V ., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., and Kavukcuoglu, K. Asyn- chronous methods for deep reinforcement learning. In ICML, 2016. Rolfe, J. T. Discrete variational autoencoders.arXiv preprint arXiv:1609.02200, 2016. Schulman, J., Heess, N., Weber, T., and Abbeel, P. Gradient estimation using stochastic computation graphs. In NIPS, 2015. Strouse, D. and Schwab, D. J. The deterministic information bottleneck. Neural computation, 29(6):1611–1630, 2017. Sukhbaatar, S., Szlam, A., and Fergus, R. Learning mul- tiagent communication with backpropagation. In NIPS. 2016. Tishby, N., Pereira, F., and Bialek, W. The information bot- tleneck method. In Proceedings of the 37th Annual Aller- ton Conference on Communication, Control and Comput- ing. University of Illinois Press, 1999. van den Oord, A., Vinyals, O., and Kavukcuoglu, K. Neural discrete representation learning. In NIPS, 2017. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., Kaiser, L., and Polosukhin, I. Attention is all you need. In NIPS, 2017. Williams, R. J. Simple statistical gradient-following algo- rithms for connectionist reinforcement learning. Machine learning, 8(3-4):229–256, 1992. Williams, R. J. and Peng, J. Function optimization using connectionist reinforcement learning algorithms. Con- nection Science, 3(3):241–268, 1991. Zaslavsky, N., Kemp, C., Regier, T., and Tishby, N. Ef- ﬁcient compression in color naming and its evolution. Proceedings of the National Academy of Sciences , 115 (31):7937–7942, 2018. Zaslavsky, N., Regier, T., Tishby, N., and Kemp, C. Se- mantic categories of artifacts and animals reﬂect efﬁcient coding. In Proceedings of CogSci, pp. 1254–1260, Mon- treal, Canada, 2019. Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. Understanding deep learning requires rethinking general- ization. arXiv preprint arXiv:1611.03530, 2016. 7. How much does Receiver rely on messages in Guess Number? We supplement the experiments of Section 3 of the main text by studying the degree to which Receiver relies on messages in Guess Number. In particular, we show that when Receiver has the full input (is = ir), it ignores the messages. We measure the degree to which Receiver relies on the messages from Sender by constructing a setup where we break communication, but still let Receiver rely on its own input. More precisely, we ﬁrst enumerate all test inputs for Sender is and Receiver ir. We obtain messages that correspond to Sender’s inputs, and shufﬂe them. Next, we feed the shufﬂed messages alongside Receiver’s own (un- shufﬂed) inputs and compute accuracy, as a measure of Receiver’s dependence on the messages. This procedure preserves the marginal distribution of Sender’s messages, but destroys all the information Sender transmits. Without messages, Receiver, given k input bits, can only reach an accuracy of 28−k. In Figure 5, we report results ag- gregated by training method. Receiver is extremely close to the accuracy’s higher bound in all conﬁgurations. Moreover, when Receiver gets the entire input, the drop in accuracy after shufﬂing is tiny, proving that Receiver’s reliance on the message is minimal in that setting. 8. Inﬂuence of architecture choices 8.1. Does vocabulary size affect the results? We repeat the same experiments as in Section 3 of the main text while varying vocabulary size. Note that, to make Guess Number solvable across each conﬁguration, the vocabulary has to contain at least 256 symbols. Similarly, for Image Classiﬁcation, vocabulary size must be of at least 100. We tried vocabulary sizes of 256, 1024, 4096 for Guess Number, and 512, 1024, 2048 for Image Classiﬁcation. The results are reported in Figures 6 (Guess Number) and 7 (Image Classiﬁcation). We observe that there is little qualitative variation over vocabulary size, hence the conclusions we had in Section 3 of the main paper are robust to variations of this parameter.Entropy Minimization In Emergent Languages 0 2 4 6 8 Binary digits hidden 0.0 0.2 0.4 0.6 0.8 1.0Accuracy Gumbel-Softmax Stoch. computation REINFORCE upper bound Figure 5.Guess Number: Receiver’s dependence on messages, measured as performance drop under message intervention. 8.2. Does Receiver’s capacity affect the results? One potential confounding variable is the capacity of Re- ceiver. Indeed, if Receiver is very simple, then, for the task to be solved, Sender would have to calculate the answer itself and feed it to Receiver. To investigate this, we repeat the Image Classiﬁcation experiment from Section 4.1 of the main paper while controlling the power of Receiver’s ar- chitecture: we put two additional fully-connected 400x400 hidden layers between the input embedding and the output layer, while in Section 4, Receiver had a single hidden layer. In Figure 8, we compare the results obtained with these two variations of Receiver. The reported entropy minimiza- tion effect holds: even in presence of additional layers, the entropy of messages H(m) is far from the upper-bound Hmax = 10 bits and closely follows the lower bound, Hmin = log2 Nl. Thus, again, a more nuanced protocol only appears when it is needed. Finally, we see that results for both architectures are close, although in three out of seven task setups (the number of classes Nl is 2, 10, and 20) a deeper model results in a slightly higher entropy of the protocol, on average. Overall, we conclude that Receiver’s capacity does not play a major role in the entropy mini- mization effect and the latter also takes place with a more powerful Receiver. 8.3. What if communication takes place through sequences of symbols? We also experiment with Guess Number in a setup where the agents communicate via variable-length messages. The general architecture of the agents is same as in Section 3, but we append GRU agents (Cho et al., 2014). Sender GRU is unrolled to generate the message. The message is produced until the GRU outputs a special eos token or until the max- imal length is reached. In the latter case, eos is appended to the message. The produced message is consumed by a Receiver’s GRU unit and the hidden state corresponding to eos is used by Receiver as input to further processing. When Receiver has additional inputs (in the Guess Num- ber game), these inputs are used as initial hidden state of the GRU cell. We use the Stochastic Computation Graph estimator as described in Section 3.2, as it provided fastest convergence. We consider the entire variable-length message as the real- ization of a random variablemwhen calculating the entropy of the messages, H(m). The results are reported in Fig- ure 9, arranged in function of maximal message length and vocabulary size. As before, we aggregate the successful runs according to the entropy regularization coefﬁcient λs applied to Sender’s output layer. From Figure 9 we observe that the results are in line with those obtained in the one-symbol scenario. Entropy mini- mization still holds: a more nuanced (high-entropy) protocol only develops when more digits are hidden from Receiver, which hence requires more information to perform the task. The approximation to the lower bound is however less tight as the overall number of possible messages grows (higher maximum length and/or vocabulary size). There is also a weak tendency for lower λs to encourage a tighter bottle- neck. In preliminary experiments, we have similar results when the variable-length communication is performed via Trans- former cells (Vaswani et al., 2017) instead of GRUs (not reported here). 9. Two-digit MNIST dataset As discussed in Section 3, to ensure high output informa- tional complexity in the Image Classiﬁcation task, we use a two-digit variant of the MNIST dataset (LeCun et al., 1998a). We construct it as follows. When iterating over the original MNIST dataset, we take a batch band (a) select the ﬁrst |b|/2 and last |b|/2 images, refer to them as b1 and b2, respectively; (b) create a new batch where the ith image from b1 is placed to the left of the ith image from b2 and then vice versa. As a result, we obtain a new stream of images, where each MNIST digit is seen twice, on the left and on the right side. Note that not all possible pairwise combinations of the original images are generated (there are 600002 of those in the training set alone) and the exact combinations change across epochs. As labels, we use the depicted two-digit number modulo Nl, where Nl is the re- quired number of classes. All pixels are scaled into [0, 1]. We use this same process to generate training and test sets, based on the training and test images of the original MNIST dataset, respectively.Entropy Minimization In Emergent Languages 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits  = 1.0  = 1.25  = 1.5 Hmin (a) V ocab. size: 256, Gumbel-Softmax 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits  = 1.0  = 1.25  = 1.5 Hmin (b) V ocab. size: 1024, Gumbel-Softmax 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits  = 1.0  = 1.25  = 1.5 Hmin (c) V ocab. size: 4096, Gumbel-Softmax 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits s = 0.01 s = 0.025 s = 0.05 s = 0.075 s = 0.1 Hmin (d) V ocab. size: 256, Stoch. Computation Graph approach 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits s = 0.01 s = 0.025 s = 0.05 s = 0.075 s = 0.1 Hmin (e) V ocab. size: 1024, Stoch. Computation Graph approach 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits s = 0.01 s = 0.025 s = 0.05 s = 0.075 s = 0.1 Hmin (f) V ocab. size: 4096, Stoch. Computation Graph approach 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits s = 0.01 s = 0.025 s = 0.05 s = 0.075 s = 0.1 s = 0.5 Hmin (g) V ocab. size: 256, REINFORCE 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits s = 0.01 s = 0.025 s = 0.05 s = 0.075 s = 0.1 s = 0.5 Hmin (h) V ocab. size: 1024, REINFORCE 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits s = 0.01 s = 0.025 s = 0.05 s = 0.075 s = 0.1 s = 0.5 Hmin (i) V ocab. size: 4096, REINFORCE Figure 6.Guess Number: Entropy of the messages m, depending on vocabulary size, training method, and relaxation temperature τ (when trained with Gumbel-Softmax) or Sender’s entropy regularization coefﬁcientλs. Shaded regions mark standard deviation. 1 2 3 4 5 6 log2Nl 1 2 3 4 5 6 7H(m), bits =0.75 =1.0 =1.5 =2.0 Hmin (a) V ocab. size: 512 1 2 3 4 5 6 log2Nl 1 2 3 4 5 6 7H(m), bits =0.75 =1.0 =1.5 =2.0 Hmin (b) V ocab. size: 1024 1 2 3 4 5 6 log2Nl 1 2 3 4 5 6 7H(m), bits =0.75 =1.0 =1.5 =2.0 Hmin (c) V ocab. size: 2048 Figure 7.Image Classiﬁcation: entropy of the messages H(m) across vocabulary sizes. Successful runs are pooled together. Shaded regions mark standard deviation.Entropy Minimization In Emergent Languages 1 2 3 4 5 6 log2Nl 1 2 3 4 5 6 7H(m), bits One hidden layer Three hidden layers Hmin Figure 8.Image Classiﬁcation: entropy of the messages H(m) across Receiver model sizes. Successful runs are pooled together. Shaded regions mark standard deviation. 10. Hyperparameters In our experiments, we used the following hyperparameter grids. Guess Number (Gumbel-Softmax) V ocab. size: [256, 1024, 4096]; temperature, τ: [0.5, 0.75, 1.0, 1.25, 1.5]; learning rate: [0.001, 0.0001]; max. number of epochs: 250; random seeds: [0, 1, 2, 3]; batch size: 8; early stopping thr.: 0.99; bits shown to Receiver: [0, 1, 2, 3, 4, 5, 6, 7, 8]. Guess Number (REINFORCE) V ocab. size: [256, 1024, 4096]; Sender entropy regularization coef., λs: [0.01, 0.025, 0.05, 0.1, 0.5, 1.0]; Receiver entropy regularization coef., λr: [0.01, 0.1, 0.5, 1.0]; learning rate: [0.0001, 0.001, 0.01]; max. number of epochs: 1000; random seeds: [0, 1, 2, 3]; batch size: 2048; early stopping thr.: 0.99; bits shown to Receiver: [0, 1, 2, 3, 4, 5, 6, 7, 8]. Guess Number (Stochastic Computation Graph ap- proach): V ocab. size: [256, 1024, 4096]; Sender entropy regularization coef., λs: [0.01, 0.05, 0.1, 0.25]; learning rate: [0.0001, 0.001]; max. number of epochs: 1000; ran- dom seeds: [0, 1, 2, 3]; batch size: 2048; early stopping thr.: 0.99; bits shown to Receiver: [0, 1, 2, 3, 4, 5, 6, 7, 8]. Image Classiﬁcation experiments V ocab. size: [512, 1024, 2048]; temperature, τ: [0.5, 0.75, 1.0, 1.5, 2.0]; learning rate: [0.001], max. number of epochs: 100; random seeds: [0, 1, 2]; batch size: 32; early stopping thr.: 0.98; number of classes: [2, 4, 10, 20, 25, 50, 100]. Fitting random labels experiments V ocab. size: 1024; temperature, τ: [1.0, 10.0]; learning rate: 0.0001, max. number of epochs: 200; random seeds: [0, 1, 2, 3, 4]; batch size: 32; early stopping thr.: ∞; prob. of label corruption: [0.0, 0.5, 1.0]. Adversarial attack experiments V ocab. size: 1024; tem- perature, τ: [0.1, 1.0, 10.0]; learning rate: 0.0001, max. number of epochs: 200; random seeds: [0, 1, 2, 3, 4]; batch size: 32; early stopping thr.: 0.98. 11. Evolution of message entropy during training In this Section, we aim to gain additional insight into de- velopment of the communication protocol by measuring its entropy during training. We concentrate on Guess Number and use the same experimental runs summarized in Figure 1 of the main text. For each game conﬁguration (that is, number of bits hidden from Receiver), we randomly select one successful run and plot the evolution of Sender message entropy and accuracy over training epochs.3 We also plot entropy and accuracy curves for a randomly selected failed run, to verify to what extent entropy development depends on task success. We report results for runs where training was performed with Gumbel-Softmax relaxation and with the Stochastic Graph Computation approach in Figures 10 and 11, respectively. The reported entropy and accuracy values are calculated in evaluation mode, where Sender’s output is selected greedily, without sampling. A higher entropy of such deterministic Sender indicates that the latter can encode more information about inputs in its messages. From these results, we ﬁrstly observe that the initial entropy of Sender’s messages (before training) can be both higher than required for communication success (Figures 10a and 11a) and lower (the rest). When it starts higher than needed, it generally falls closer to the minimum level required for the solution. When the initial value is low, it increases during training. The failed runs can have message entropy above (Figures 10a, 10b & 11a) and below (e.g. Figures 10c, 10d & 11d) successful runs, suggesting that there is no systematic relation between degree of entropy and task success. The fact that the entropy can be reduced with no decrease in accuracy or even with accuracy growth (e.g. Figure 10a, red line, epochs 5..30) indicates that the tendency to discover new messages (increasing entropy) is counter-balanced by the complexity of mutual coordination with Receiver when entropy is larger. In our interpretation, it is this interplay that serves as a source of the natural bottleneck. Finally, while in some runs the entropy is effectively in- creased w.r.t. its initialization level, the resulting protocol’s entropy is at, or slightly above the lower bound of what the task allows. In this sense, we argue that the reported effect 3We exclude the conﬁguration in which Receiver sees the entire input, as it is a degenerate case of non-communication, as discussed in Section 4 of the main text.Entropy Minimization In Emergent Languages 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits =0.05 =0.1 =0.01 Hmin (a) Max length: 5, vocabulary size: 16 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits =0.05 =0.1 =0.01 Hmin (b) Max length: 10, vocabulary size: 16 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits =0.05 =0.1 =0.01 Hmin (c) Max length: 5, vocabulary size: 64 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits =0.05 =0.1 =0.01 Hmin (d) Max length: 10, vocabulary size: 64 Figure 9.Guess Number: Entropy of the emergent protocol when communication is performed with variable-length messages. Shaded regions mark standard deviation.Entropy Minimization In Emergent Languages can be correctly denoted as a “minimization” result. References Achille, A. and Soatto, S. Information dropout: Learning optimal representations through noisy computation. IEEE TPAMI, 40(12):2897–2905, 2018a. Achille, A. and Soatto, S. Emergence of invariance and disentanglement in deep representations. The Journal of Machine Learning Research, 19(1):1947–1980, 2018b. Alemi, A. A., Fischer, I., Dillon, J. V ., and Murphy, K. Deep variational information bottleneck. arXiv preprint arXiv:1612.00410, 2016. Berwick, R. and Chomsky, N. Why Only Us: Language and Evolution. MIT Press, Cambridge, MA, 2016. Bickerton, D. More than Nature Needs: Language, Mind, and Evolution. Harvard University Press, Cambridge, MA, 2014. Bouchacourt, D. and Baroni, M. How agents see things: On visual representations in an emergent language game. In EMNLP, 2018. Carlini, N. and Wagner, D. Defensive distillation is not robust to adversarial examples. arXiv preprint arXiv:1607.04311, 2016. Chaabouni, R., Kharitonov, E., Dupoux, E., and Baroni, M. Anti-efﬁcient encoding in emergent communication. In NeurIPS, 2019. Chaabouni, R., Kharitonov, E., Bouchacourt, D., Dupoux, E., and Baroni, M. Compositionality and generalization in emergent languages. In ACL, 2020. Chevalier-Boisvert, M., Bahdanau, D., Lahlou, S., Willems, L., Saharia, C., Nguyen, T. H., and Bengio, Y . BabyAI: A platform to study the sample efﬁciency of grounded language learning. In ICLR, 2019. Cho, K., Van Merri ¨enboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y . Learn- ing phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014. Choi, E., Lazaridou, A., and de Freitas, N. Compositional obverter communication learning from raw visual input. arXiv preprint arXiv:1804.02341, 2018. Collier, K., Bickel, B., van Schaik, C., Manser, M., and Townsend, S. Language evolution: Syntax before phonol- ogy? Proceedings of the Royal Society B: Biological Sciences, 281(1788):1–7, 2014. Cover, T. M. and Thomas, J. A. Elements of Information Theory. John Wiley & Sons, 2012. Evtimova, K., Drozdov, A., Kiela, D., and Cho, K. Emergent communication in a multi-modal, multi-step referential game. In ICLR, 2018. Ferrer i Cancho, R. and D´ıaz-Guilera, A. The global minima of the communicative energy of natural communication systems. Journal of Statistical Mechanics: Theory and Experiment, 2007(06):P06009, 2007. Ferrer i Cancho, R., Hern´andez-Fern´andez, A., Lusseau, D., Agoramoorthy, G., Hsu, M., and Semple, S. Compression as a universal principle of animal behavior. Cognitive Science, 37(8):1565–1578, 2013. Fischer, I. The conditional entropy bottleneck, 2019. URL https://openreview.net/forum? id=rkVOXhAqY7. Galantucci, B. Experimental semiotics: A new approach for studying communication as a form of joint action. Topics in Cognitive Science, 1(2):393–410, 2009. Gibson, E., Piantadosi, R. F. S., Dautriche, I., Mahowald, K., Bergen, L., and Levy, R. How efﬁciency shapes human language. Trends in Cognitive Science, 2019. In press. Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain- ing and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. Greensmith, E., Bartlett, P. L., and Baxter, J. Variance reduc- tion techniques for gradient estimates in reinforcement learning. JMLR, 5(Nov):1471–1530, 2004. Harding Graesser, L., Cho, K., and Kiela, D. Emergent lin- guistic phenomena in multi-agent communication games. In EMNLP, 2019. Havrylov, S. and Titov, I. Emergence of language with multi- agent games: Learning to communicate with sequences of symbols. In NIPS, 2017. Hurford, J. The Origins of Language. Oxford University Press, Oxford, UK, 2014. Jang, E., Gu, S., and Poole, B. Categorical repa- rameterization with Gumbel-Softmax. arXiv preprint arXiv:1611.01144, 2016. K˚ageb¨ack, M., Dubhashi, D., and Sayeed, A. DeepColor: Reinforcement learning optimizes information efﬁciency and well-formedness in color name partitioning. In Pro- ceedings of CogSci, pp. 1895–1900, Austin, TX, 2018. Kemp, C. and Regier, T. Kinship categories across lan- guages reﬂect general communicative principles. Science, 336(6084):1049–1054, 2012.Entropy Minimization In Emergent Languages 0 50 100 150 200 250 epoch 0.0 0.5 1.0 1.5 2.0 2.5H(m), bits 0 50 100 150 200 250 epoch 0.0 0.2 0.4 0.6 0.8 1.0Accuracy (a) Binary digits hidden: 2 0 50 100 150 200 250 epoch 0 1 2 3 4 5H(m), bits 0 50 100 150 200 250 epoch 0.0 0.2 0.4 0.6 0.8 1.0Accuracy (b) Binary digits hidden: 4 0 50 100 150 200 250 epoch 0 2 4 6H(m), bits 0 50 100 150 200 250 epoch 0.0 0.2 0.4 0.6 0.8 1.0Accuracy (c) Binary digits hidden: 6 0 50 100 150 200 250 epoch 2 4 6 8H(m), bits 0 50 100 150 200 250 epoch 0.0 0.2 0.4 0.6 0.8 1.0Accuracy (d) Binary digits hidden: 8 Figure 10.Evolution of H(m) over training epochs. Gumbel Softmax-based optimization, Guess Number. For each game conﬁguration, speciﬁed by the number of bits Receiver lacks, we sample one successful (black line) and one failed (red line) training trajectory. The blue line marks Hmin, minimal entropy for a successful solution.Entropy Minimization In Emergent Languages 0 50 100 150 200 250 epoch 0 2 4 6H(m), bits 0 50 100 150 200 250 epoch 0.2 0.4 0.6 0.8 1.0Accuracy (a) Binary digits hidden: 2 0 50 100 150 200 250 epoch 2 4 6H(m), bits 0 50 100 150 200 250 epoch 0.0 0.2 0.4 0.6 0.8 1.0Accuracy (b) Binary digits hidden: 4 0 50 100 150 200 250 epoch 2 3 4 5 6H(m), bits 0 50 100 150 200 250 epoch 0.0 0.2 0.4 0.6 0.8 1.0Accuracy (c) Binary digits hidden: 6 0 50 100 150 200 250 epoch 4 6 8H(m), bits 0 50 100 150 200 250 epoch 0.0 0.2 0.4 0.6 0.8 1.0Accuracy (d) Binary digits hidden: 8 Figure 11.Evolution of H(m) over training epochs. Stochastic Computation Graph-based optimization, Guess Number. For each game conﬁguration, speciﬁed by the number of bits Receiver lacks, we sample one successful (black line) and one failed (red line) training trajectory. The blue line marks Hmin, minimal entropy for a successful solution.Entropy Minimization In Emergent Languages Kharitonov, E., Chaabouni, R., Bouchacourt, D., and Ba- roni, M. EGG: a toolkit for research on Emergence of lanGuage in Games. In EMNLP: System Demonstrations, 2019. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Kirby, S. Natural language from artiﬁcial life. Artiﬁcial life, 8(2):185–215, 2002. Kottur, S., Moura, J. M., Lee, S., and Batra, D. Natural lan- guage does not emerge “naturally” in multi-agent dialog. arXiv preprint arXiv:1706.08502, 2017. Lazaridou, A., Peysakhovich, A., and Baroni, M. Multi- agent cooperation and the emergence of (natural) lan- guage. arXiv preprint arXiv:1612.07182, 2016. Lazaridou, A., Hermann, K. M., Tuyls, K., and Clark, S. Emergence of linguistic communication from referential games with symbolic and pixel input. arXiv preprint arXiv:1804.03984, 2018. LeCun, Y ., Boser, B. E., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W. E., and Jackel, L. D. Hand- written digit recognition with a back-propagation network. In NIPS, 1990. LeCun, Y ., Bottou, L., Bengio, Y ., and Haffner, P. Gradient- based learning applied to document recognition. Proceed- ings of the IEEE, 86(11):2278–2324, 1998a. LeCun, Y ., Bottou, L., Bengio, Y ., Haffner, P., et al. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998b. Lewis, D. Convention harvard university press. Cambridge, MA, 1969. Li, F. and Bowling, M. Ease-of-teaching and language structure from emergent communication. In NeurIPS. 2019. Lowe, R., Foerster, J., Boureau, Y ., Pineau, J., and Dauphin, Y . On the pitfalls of measuring emergent communica- tion. In Proceedings of AAMAS, pp. 693–701, Montreal, Canada, 2019. Maddison, C. J., Mnih, A., and Teh, Y . W. The concrete distribution: A continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016. Mikolov, T., Joulin, A., and Baroni, M. A roadmap towards machine intelligence. In International Conference on In- telligent Text Processing and Computational Linguistics, pp. 29–61. Springer, 2016. Mnih, V ., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., and Kavukcuoglu, K. Asyn- chronous methods for deep reinforcement learning. In ICML, 2016. Rolfe, J. T. Discrete variational autoencoders.arXiv preprint arXiv:1609.02200, 2016. Schulman, J., Heess, N., Weber, T., and Abbeel, P. Gradient estimation using stochastic computation graphs. In NIPS, 2015. Strouse, D. and Schwab, D. J. The deterministic information bottleneck. Neural computation, 29(6):1611–1630, 2017. Sukhbaatar, S., Szlam, A., and Fergus, R. Learning mul- tiagent communication with backpropagation. In NIPS. 2016. Tishby, N., Pereira, F., and Bialek, W. The information bot- tleneck method. In Proceedings of the 37th Annual Aller- ton Conference on Communication, Control and Comput- ing. University of Illinois Press, 1999. van den Oord, A., Vinyals, O., and Kavukcuoglu, K. Neural discrete representation learning. In NIPS, 2017. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., Kaiser, L., and Polosukhin, I. Attention is all you need. In NIPS, 2017. Williams, R. J. Simple statistical gradient-following algo- rithms for connectionist reinforcement learning. Machine learning, 8(3-4):229–256, 1992. Williams, R. J. and Peng, J. Function optimization using connectionist reinforcement learning algorithms. Con- nection Science, 3(3):241–268, 1991. Zaslavsky, N., Kemp, C., Regier, T., and Tishby, N. Ef- ﬁcient compression in color naming and its evolution. Proceedings of the National Academy of Sciences , 115 (31):7937–7942, 2018. Zaslavsky, N., Regier, T., Tishby, N., and Kemp, C. Se- mantic categories of artifacts and animals reﬂect efﬁcient coding. In Proceedings of CogSci, pp. 1254–1260, Mon- treal, Canada, 2019. Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. Understanding deep learning requires rethinking general- ization. arXiv preprint arXiv:1611.03530, 2016.",
      "meta_data": {
        "arxiv_id": "1905.13687v3",
        "authors": [
          "Eugene Kharitonov",
          "Rahma Chaabouni",
          "Diane Bouchacourt",
          "Marco Baroni"
        ],
        "published_date": "2019-05-31T15:54:41Z",
        "pdf_url": "https://arxiv.org/pdf/1905.13687v3.pdf"
      }
    },
    {
      "title": "Generalization Guarantees for Sparse Kernel Approximation with Entropic Optimal Features",
      "abstract": "Despite their success, kernel methods suffer from a massive computational\ncost in practice. In this paper, in lieu of commonly used kernel expansion with\nrespect to $N$ inputs, we develop a novel optimal design maximizing the entropy\namong kernel features. This procedure results in a kernel expansion with\nrespect to entropic optimal features (EOF), improving the data representation\ndramatically due to features dissimilarity. Under mild technical assumptions,\nour generalization bound shows that with only $O(N^{\\frac{1}{4}})$ features\n(disregarding logarithmic factors), we can achieve the optimal statistical\naccuracy (i.e., $O(1/\\sqrt{N})$). The salient feature of our design is its\nsparsity that significantly reduces the time and space cost. Our numerical\nexperiments on benchmark datasets verify the superiority of EOF over the\nstate-of-the-art in kernel approximation.",
      "full_text": "Generalization Guarantees for Sparse Kernel Approximation with Entropic Optimal Features Liang Ding, Rui Tuo, Shahin Shahrampour Texas A&M University E-mail: {ldingaa,ruituo,shahin}@tamu.edu Abstract Despite their success, kernel methods suffer from a massive computational cost in practice. In this paper, in lieu of commonly used kernel expansion with respect toNinputs, we develop a novel optimal design maximizing the entropy among kernel features. This procedure results in a kernel expansion with respect to entropic optimal features (EOF), improving the data representation dramatically due to features dissimilarity. Under mild technical assumptions, our generalization bound shows that with only O(N 1 4 ) features (disregarding logarithmic factors), we can achieve the optimal statistical accuracy (i.e., O(1/ √ N)). The salient feature of our design is its sparsity that signiﬁcantly reduces the time and space cost. Our numerical experiments on benchmark datasets verify the superiority of EOF over the state-of-the-art in kernel approximation. 1 Introduction Kernel methods are powerful tools in describing nonlinear data models. However, despite their success in various machine learning tasks, kernel methods always suffer from scalability issues, especially when the learning task involves matrix inversion (e.g., kernel ridge regression). This is simply due to the fact that for a dataset of size N, the inversion step requires O(N3) time cost. To tackle this problem, a great deal of research has been dedicated to the approximation of kernels using low-rank surrogates (1–3). By approximating the kernel, these methods deal with a linear problem, potentially solvable in a linear time with respect to N (see e.g. (4) for linear Support Vector Machines (SVM)). In the approximation of kernel with a ﬁnite number of features, one fundamental question is how to select the features. As an example, in supervised learning, we are interested to identify features that lead to low out-of-sample error. This question has been studied in the context of random features, which 1 arXiv:2002.04195v1  [cs.LG]  11 Feb 2020is an elegant method for kernel approximation ( 3). Most of the works in this area improve the out-of- sample performance by modifying the stochastic oracle from which random features are sampled (5–7). Nevertheless, these methods deal with dense feature matrices (due to randomness) and still require a large number of features to learn the data subspace. Decreasing the number of features directly affects the time and space costs, and to achieve that we must choose features that are as distinct as possible (to better span the space). Focusing on explicit features, we aim to achieve this goal in the current work. 1.1 Our Contributions In this paper, we study low-rank kernel approximation by ﬁnding a set of mutually orthogonal features with nested and compact supports. We ﬁrst theoretically characterize a condition (based on the Sturm-Liouville problem), which allows us to obtain such features. Then, we propose a novel optimal design method that maximizes the metric entropy among those features. The problem is formulated as a combinatorial optimization with a constraint on the number of features used for approximation. The optimization is generally NP-hard but yields closed-form solutions for speciﬁc numbers of features. The algorithm, dubbed entropic optimal features (EOF), can use these features for supervised learning. The construction properties of features (orthogonality, compact support, and nested support) result in a sparse approximation saving dramatically on time and space costs. We establish a generalization bound for EOF that shows with only O(N 1 4 ) features (disregarding logarithmic factors), we can achieve the optimal statistical accuracy (i.e., O(1/ √ N)). Our numerical experiments on benchmark datasets verify the superiority of EOF over the state-of-the-art in kernel approximation. While we postpone the exhaustive literature review to Section 6, none of the previous works has approached the problem from the entropy maximization perspective, which is the unique distinction of the current work. 2 Preliminaries on Kernel Methods Kernel methods map ﬁnite-dimensional data to a potentially inﬁnite dimensional feature space. Any element f in the reproducing kernel Hilbert space (RKHS) of k, denoted by Hk, has the following 2representation: f = ∞∑ i=1 ⟨f,gi⟩kgi, (1) where ⟨·,·⟩k RKHS inner product induced by k and {gi}is any feature set (i.e., orthonormal basis) that spans the space Hk. In general, the kernel trick relies on the observation that the inner product ⟨k(·,x),k(·,x′)⟩k = k(x,x′) with x,x′∈RD (reproducing property), so k(x,x′) is cheap to compute without the need to calculate the inner product. In this case, the feature set selected in equation (1) is {k(·,x) : x ∈RD}and the target function can be written as ∑ i cik(·,xi). Under mild conditions, by the Representer Theorem, it is guaranteed that any solution of the risk minimization problem assumes the form f(·) = ∑N i=1 cik(·,xi), where N is the number of training data points. However, this representation introduces a massive time cost ofO(N3) and a memory cost of O(N2) in the training. Further, the feature space {k(·,x) : x ∈RD}may not cover Hk from an optimal sense. To be more speciﬁc, there might be another set of features {gi}M i=1 with M ≪N such that {k(·,x) : x ∈X}⊂{ gi}M i=1 where X ∈RN×D is the input data. To address the aforementioned problem, (3) propose a random approximation of k(x,x′) k(x,x′) ≈zT(x)z(x′) (2) where z(x) = [ζ1(x),...,ζ M (x)] is a random vector. This decomposes the feature k(·,x) into a linear combination of random low-rank features {ζi}to approximate the original target function ∑N i=1 cik(·,xi) by ∑M i=1 αiζi. This idea resolves the computational issue of the algorithm, but due to random selection of the features, the method does not offer the best candidate features for reconstructing the target function. Furthermore, in supervised learning the goal is to ﬁnd a mapping from inputs to outputs, and an optimal kernel approximation does not necessarily result in an optimal target function. The reason is simply that we require the features that best represent the underlying data model (or target function) rather than the kernel function. 33 Kernel Feature Selection In this paper, we propose an algorithm that uses a sparse representation to attain a high prediction accuracy with a low computational cost. The key ingredient is to ﬁnd an expansion: f = ∞∑ i=1 ⟨f,gi⟩kgi (3) such that features {gi}satisfy the following properties: 1. Compact support: supt [gi] is compact. 2. Nested support: supt [gi] = ⋃ j∈I supt[gj] for some ﬁnite set I. 3. Orthogonality: ⟨gi,gj⟩k = δij where δij denotes the Kronecker delta. Properties 1-2 ensure low time cost for the algorithm by promoting sparsity. To be more speciﬁc, given any ﬁnite set {gi}M i=1 and any data point x, gi(x) = 0 for a large number of gi ∈{gi}M i=1. Property 3 provides a better expansion of Hk. In general, this problem may be intractable; however, we will prove later in Theorem 2 that whenk satisﬁes the following condition, then a feature set {φi}that satisﬁes properties 1-3 does exist: Condition 1. Let kernel kbe of the following product form: k(x,x′) = D∏ d=1 p(min{xd,x′ d})q(max{xd,x′ d}) where pand qare the independent solutions of the Sturm-Liouville problem on the interval [a,b] for any a,b ∈[−∞,∞]: d dxα(x)dy dx + β(x)y= 0, and they satisfy the following boundary conditions: c11p′(a) + c12p(a) = 0 c21q′(b) + c22q(b) = 0 4with cij ≥0 for i,j = 1 ,2 and the operator d dxα(x) d dx + β(x) is an elliptic operator that satisﬁes Lax-Milgram Theorem (see section 6 of (8)). We provide two commonly used kernels that satisfy condition 1: k(x,x′) = e−ω∥x−x′∥1 k(x,x′) = D∏ d=1 [ωmin{xd,x′ d}+ 1]. The ﬁrst one is the Laplace kernel and the second one is the kernel associated to weighted Sobolev space (9). Let zl,i = i2−l for any l,i ∈N. Then, when the dimension D = 1, features associated to Laplace kernel satisfying properties 1-3 are as follows: φl,i(x) =    sinh ω|x−zl,i+1| sinh ω2−l if x∈(zl,i,zl,i+1] sinh ω|x−zl,i−1| sinh ω2−l if x∈[zl,i−1,zl,i] 0 otherwise (4) and features associated to the weighted Sobolev space kernel are as follows: φl,i(x) = max { 0,1 −|x−zl,i| 2−l } where (l,i) is the index of features. We now start from 1-D kernel to construct a feature space that satisﬁes properties 1-3: Theorem 1. Suppose k is a kernel that satisﬁes Condition 1. Let Zl = {zl,i = i2−l : i = 1,2l −1} and let Bl = {i = 1 ,··· ,2l −1 : i is odd}. We then deﬁne the following function on the interval [zl,i−1,zl,i+1] = [(i−1)2−l,(i+ 1)2−l]: φl,i(x) =    q(x)pl,i+1−p(x)ql,i+1 ql,ipl,i+1−pl,iql,i+1 if x∈(zl,i, zl,i+1] p(x)ql,i−1−q(x)pl,i−1 pl,iql,i−1−ql,ipl,i−1 if x∈[zl,i−1, zl,i] 0 otherwise . (5) where pl,i = p(zl,i) = p(i2−l) and ql,i = q(zl,i) = q(i2−l). Then the following feature set is an orthogonal basis of the RKHS of k, Hk, that satisﬁes property 1-3 on the unit interval [0,1]: {φl,i : l∈N,i ∈Bl}. 5The theorem above characterizes the set of features that satisfy Condition 1 when the input is scalar. To extend the idea to D-dimensional space, we only need to take the tensor product form of the 1-dimensional kernel, as described by the consequent theorem: Theorem 2. Suppose kis a kernel that satisﬁes Condition 1. For any l ∈ND, we deﬁne the Cartesian product of sets as follows: Zl = ×D d=1Zld = {zl,i = (zl1,i1 ,··· ,zlD,iD ) : zld,id ∈Zld} Bl = ×D d=1 = {i ∈ND : id ∈Bld}. We then deﬁne the following function on the hypercube×D d=1[zld,id−1,zld,id+1] = ×D d=1[(id −1)2−ld,(id + 1)2−ld]: φl,i(x) = D∏ d=1 φld,id(xd) (6) where the function φld,id is deﬁned in Theorem 1. Then the following feature set is an orthogonal basis of the RKHS of k, Hk, that satisﬁes property 1-3 on the unit cube [0,1]D: {φl,i : l ∈ND,i ∈Bl}. The proof of Theorem 1 is given in the supplementary material. Theorem 2 can be derived from Theorem 1, because the kernel is simply the tensor product of 1-dimensional kernel in Theorem 1. Corollary 3.For any kernelksatisﬁes condition 1 and let φl,i be the function deﬁned in Theorem 2. Then we have the following expansion for k: k(x,x′) = ∑ l∈ND ∑ i∈Bl φl,i(x)φl,i(x′) ⟨φl,i,φl,i⟩k (7) where ⟨·,·⟩k is the inner product induced by k. Proof. We only need to substitute f(·) in equation (3) by k(x,·), then according to the reproducing property of kwe can have the result. 6Corollary 3 is the direct result of Theorem 2. So we can have the following sparse approximation for the value k(x,x′): k(x,x′) ≈zT(x)z(x′) where z(x) = [φl,i(x) ||φl,i||k ] (l,i)∈S for some set S. We will show in section 4.1 that most entries on z(x) are zero. Form this perspective, the expansion (7) is analogous to the random feature (2) except that the above z(x) is nonrandom. We now use the RKHS of the following kernel on[0,1] as an example: k(x,x′) = min{x,x′}[1 −max{x,x′}]. The RKHS associated to kis the ﬁrst order Sobolev space with zero boundary conditions: Hk = { f : ∫ 1 0 [f′(s)]2ds< ∞,f(0) = f(1) = 0 } . In this example, the feature functions given by Theorem 1 coincide with a wavelet basis in Hk. Consider the mother wavelet given by the triangular function: φ(d) = max{0,1 −|d|}. Then for any l∈N, i= 1,··· ,2l −1, direct calculations show that φl,i(x) = φ (x−i2−l 2−l ) . (8) Now it is easy to verify that the features {φl,i : l∈N,i is odd}satisfy the desired properties 1-3: 1. supt [φl,i] = [(i−1)2−l,(i+ 1)2−l]. 2. supt [φl,i] = supt[φl+1,2i−1] ∪supt[φl+1,2i+1]. 3. ∫1 0 φ′ l,iφ′ n,jds= 2l+1δ(l,i),(n,j). 7Figure 1: Top two panels: W2 = {φl,i : l = 2}and W3 = {φl,i : l = 3}; lower two panels: nested structure for the representation of a function f ∈Hk. 0 0.2 0.4 0.6 0.8 10 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 W 2 0 0.2 0.4 0.6 0.8 10 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 W 3 0 0.2 0.4 0.6 0.8 10 0.05 0.1 0.15 0.2 0.25 0.3 , 1,1 , 2,1 , 2,2 0 0.2 0.4 0.6 0.8 10 0.05 0.1 0.15 0.2 0.25 0.3 , 1,1 , 2,1 , 2,2 , 3,1 , 3,2 , 3,3 , 3,4 Figure 2: 2-D tensor product of wavelet features with compact support φ[1,2],[11] and φ[1,2],[13] Figure 1 illustrates the compact and nested supports of these wavelet features. The compact support properties can lead to a signiﬁcant improvement in time cost. Consider the evaluation of f(x) = ∑ |l|≤n αl,iφl,i(x). The compact support property implies that φl,i(x) = 0 for most (l,i)’s, so that the computational cost of evaluating f(x) can be much lower than the total number of features. In Section 4.1, we will leverage this property of the basis functions to propose an efﬁcient algorithm for learning. This goal cannot be achieve when the basis functions are not compactly supported, such as the random features. Figure 2 shows the example of the tensor product of the wavelet feature deﬁned in (8). It is a 2- dimensional extension of the wavelet feature and according to Theorem 2, the features satisfy properties 81-3 in the RKHS induced by the following kernel: k(x,x′) = D∏ d=1 min{xd,x′ d}[1 −max{xd,x′ d}], which is the mixed Sobolev space of ﬁrst order with zero boundary condition on [0,1]D.We refer the reader to (10) for more details on mixed order Sobolev space. In view of Theorem 2, we can now lift a data point fromx ∈RD to a ﬁnite dimensional space spanned by features with compact and nested supports. As a result, the evaluation of x on a large number of features is zero, yielding a sparse and efﬁcient representation. 4 Entropic Optimal Design In the previous section, we provide conditions under which we can ﬁnd features with compact and nested supports. We now present an optimization criterion to select the best ﬁnite set of features with the maximum metric entropy. The intuition behind this choice is that we favor a set of features that are different from each other as much as possible, so that we can reconstruct the underlying model by a moderate amount of features. To formulate the optimization problem, we need to introduce some notation. First we introduce the covering number of an operator between two Banach spaces. Let ε> 0 and A,B be Banach spaces with unit balls BA and BB, respectively. The covering number of an operator T : A →B is deﬁned as N(T,ε) = inf n∈N { n: ∃{bi ∈B}n i=1 s.t. T(BA) ⊆ n⋃ i=1 (bi + εBB) } . The metric entropy of T is then deﬁned as Ent[T,ε] := log N(T,ε). Now, let Hk be the RKHS associated to kernel kwith the inner product ⟨·,·⟩k, and let PS be the projection operator from Hk to the following ﬁnite dimensional subspace FS = {φl,i : (l,i) ∈S}, where φl,i is deﬁned in Theorem 2 and dim(PS) = |S|. Our goal is to ﬁnd the optimal set S∗(with cardinality at most M), whose corresponding feature set maximizes the entropy. This is equivalent to 9solving the following optimization problem: sup S Ent[PS,ε] s.t. |S|≤ M. (9) Following the lines in the proof of Theorem 2 (in the supplementary), we can show that the features in FS are mutually orthogonal with Hilbert norm: ||φl,i||2 Hk =: C−1 l,i , (10) where Cl,i →∞ as |l|→∞ (see lemma 1 in Supplementary Material). We ﬁrst multiply φl,i by C 1 2 l,i to normalize the feature. For any function f ∈Hk, we then have PSf = ∑ (l,i)∈S Cl,i⟨f,φl,i⟩kφl,i. As a result, the entropic optimization problem (9) is equivalent to searching an M-dimensional Euclidean space with the largest unit ball, which can be characterized as follows max S ∑ (l,i)∈S Cl,i s.t.|S|≤ M. This optimization problem is called the Knapsack problem and, in general, is NP-hard (11). However, for some speciﬁc values of M, closed form solutions exist. Consider the Laplace kernel here as an example. For Laplace kernel k(x,x′) = e−ω∥x−y∥1 , from direct calculation, the constant is: Cl,i = D∏ d=1 sinh(ω2−ld). In this case, Cl = Cl,i is independent of i and for any |l|<|l′|, the value Cl >Cl′. Therefore, we can derive that when M = |{l : |l|<n}|for some n, the optimal set S∗ n is S∗ n = {(l,i) : |l|≤ n,i ∈Bl} (11) because for any Cl ∈S∗ n and any Cl′ ̸∈S∗ n, Cl >Cl′. It turns out the set S∗ n is equivalent to the Sparse Grid design (10). 104.1 Algorithm: Entropic Optimal Features With the aforementioned theorems, we can now describe the algorithm to compute the regression function associated to a kernel that satisﬁes Condition 1. Suppose the set S∗ n given by equation (11) is the index set associated to the feature functions that maximizes the entropy optimization problem (9). So given a speciﬁc input x, we aim to compute the vector z(x) = [Cl,iφl,i(x)](l,i)∈S∗n =: [zl,i(x)](l,i)∈S∗n where Cl,i is the coeffecient deﬁned in (10), z(x) is the approximation that satisﬁes k(x,x′) ≈z(x)Tz(x′) in Corollary 3 with φl,i the feature function deﬁned in equation (6). We call z(x) the entropic optimal feature (EOF). According to properties 1-3, the supports of{φl,i : (l,i) ∈S∗ n}are either disjoint or nested. Therefore, only a small amount of entries on z(x) are non-zero. To be more speciﬁc, given any l ∈ND and input x, the supports of {φl,i : i ∈Bl}are disjoint so we can immediately compute the unique non-zero entry zl,i(x). Algorithm 1 shows how to explicitly compute the EOF z(x) at a data point x. Note that ⌈·⌉,⌊·⌋ denote the ceiling and ﬂoor operations, respectively. Algorithm 1Entropic Optimal Features (EOF) Input: point x, S∗ n Initialize z(x) = [zl,i(x)](l,i)∈S∗n = 0 while |l|≤ n+ D−1 do for d= 1 to Ddo id = { ⌈xd 2−ld ⌉if ⌈xd 2−ld ⌉ is odd ⌊xd 2−ld ⌋if ⌊xd 2−ld ⌋ is odd end for zl,i(x) = Cl,iφl,i(x) end while The dimension of the vector z(x) given nlevels is O(2nnD−1) (10). The number of non-zero elements 11for z(x) after running Algorithm 1 is: ∑ |l|≤n+D−1 1 = n+D−1∑ i=D ∑ |l|=i 1 = n+D−1∑ i=D (i−1 D−1 ) = (n+ D−1 D ) = O(nD), which means fraction of non-zeros to the whole vector in z(x) grows with O( n 2n ) as a function of level n. Time Complexity of EOF in Regression:Based on above, if we ﬁx M as the size of z(x), the number of non-zero entries on z(x) is O(logD M). Since we evaluate z(x) for each training data, the feature matrix has O(nlogD M) non-zero elements, resulting in a training cost of O(Nlog2D M), which is smaller than O(NM2) of random features (3), especially when Dis moderate. 5 Generalization Bound In this section, we present the generalization bound for EOF when it is used in supervised learning. Let us deﬁne the approximated target function as ˆf := argmin f∈FM 1 N N∑ j=1 L(yi,f(xi)) + λ∥f∥2 k, given independent and identically distributed samples{(xi,yi)}N i=1, where FM denotes the space spanned by the ﬁrst M EOFs; L is a loss function; and λ is a tuning parameter that may depend on n. We denote by R(f) := Ex,y[L(y,f(x))] the true risk. The goal is to bound the generalization error R( ˆf) − inff∈Hk R(f). We use the following assumptions to establish the bound: Assumption 1. There exists f0 ∈Hk so that inff∈Hk R(f) = R(f0). Assumption 2. The function my(·) := L(y,·) is twice differentiable for all y. Furthermore, my(·) is strongly convex. 12Assumption 3. The density function of input x is uniformly bounded away from inﬁnity. The outputs are uniformly bounded. Assumption 1 allows inﬁmum to be achieved in the RKHS. This is not ensured automatically since we deal with a potentially inﬁnite-dimensional RKHS Hk, that is possibly universal (see Remark 2 of (12)). Assumption 2 is true for common loss functions including least squares for regression (my(y′) = (y−y′)2) and logistic regression for classiﬁcation (my(y′) = log[1 + exp(−yy′)]). The bounded output constraint of Assumption 3 is also common in supervised learning. The generalization bound is given by the following theorem. Theorem 4. Suppose Assumptions 1-3 are fulﬁlled. If the tuning parameter is choosing to have λ ∼ N−1/2, then R( ˆf) −inf f R(f) ≤Op(N−1/2) + CM−2 log4D−4 M, for some C >0. The constants may depend on ∥f0∥k. The theorem above shows that with O(N 1 4 ) EOFs, the optimal statistical accuracy O(1/ √ N) is achieved up to logarithmic factors. Compared to random features for kernel approximation, this result improves the generalization bound. For random features, the number of required features to achieve the optimal rate is O( √ N) in the case of ridge regression (12). 6 Related Literature We provide related works for kernel approximation from different perspectives: Random Features (Randomized Kernel Approximation):Randomized features was introduced as an elegant approach for Monte Carlo approximation of shift-invariant kernels (3), and it was later extended for Quasi Monte Carlo approximation (13). Several methods consider improving the time cost of random features, decreasing it by a linear factor of the input dimension (see e.g., Fast-food (14, 15)). Quadrature- based random features are also shown to boost kernel approximation (16). The generalization properties 13of random features have been studied for ℓ1-regularized risk minimization (17) and ridge regression (12), improving the initial generalization bound of (18). (19) develop orthogonal random features (ORF) to boost the variance of kernel approximation. ORF is shown to provide optimal kernel estimator in terms of mean-squared error ( 20). A number of recent works have considered data-dependent sampling of random features to improve kernel approximation. Examples consist of (21) on compact nonlinear feature maps, (15,22) on approximation of shift-invariant/translation-invariant kernels, and (23) on data-dependent approximation using greedy approaches (e.g., Frank-Wolfe). Furthermore, data-dependent sampling has been used to improve generalization in supervised learning (5, 7) through target kernel alignment. Deterministic Kernel Approximation:The studies on ﬁnding low-rank surrogates for kernels date back two decades (1, 2). As an example, the celebrated Nystr ¨om method (24, 25) samples a subset of training data for approximating a low-rank kernel matrix. The Nystr¨om method has been further improved in (26) and more recently used for approximation of indeﬁnite kernels (27). Explicit feature maps have also proved to provide efﬁcient kernel approximation. The works of (28–30) have proposed low-dimensional Taylor expansions of Gaussian kernel for improving the time cost of learning. (31) further study explicit feature maps for additive homogeneous kernels. Sparse Approximation Using Greedy Methods:Sparse approximation literature has mostly focused on greedy methods. (32) have developed a matching pursuit algorithm where kernels are the dictionary elements. The work of (33) focuses on sparse regression and classiﬁcation models using Mercer kernels, and (34) considers sparse regression with multiple kernels. Classical matching pursuit was developed for regression, but further extensions to logistic regression (35) and smooth loss functions (36) have also been studied. (37) propose a greedy reconstruction technique for regression by empirically ﬁtting squared error residuals. (38) also use greedy methods for sparse approximation using multiple kernels. Our approach is radically different from the prior work in the sense that we characterize a set of features that maximize the entropy. Our feature construction and entropy optimization techniques are novel and have not been explored in the kernel approximation literature. 147 Numerical Experiments Benchmark Algorithm: We now compare EOF with the following random-feature benchmark algo- rithms on several datasets from the UCI Machine Learning Repository: 1) RKS(18) with approximated Laplace kernel feature z(x) = 1√ M [cos(xTγm + bm)]M m=1, where {γm}M m=1 are sampled from a Cauchy distribution multiplied by σ, and {bm}M m=1 are sampled from the uniform distribution on [0,2π]. 2) ORF (19) with approximated Gaussian kernel feature z(x) = 1√ M [cos(xTγm + bm)]M m=1, with [γ1 γ2 ···γm] = σSQ where S is a diagonal matrix, with diagonal entries sampled i.i.d. from the χ-distribution with ddegrees and Q is the orthogonal matrix obtained from the QR decomposition of a matrix G with normally distributed entries. Note that ORF approximates a Gaussian kernel. 3) LKRF(5) with approximated Laplace kernel feature z(x) = 1√ M [cos(xTγm + bm)]M m=1, with ﬁrst a larger number M0 random features are sampled and then re-weighted by solving a kernel alignment optimization. The top M random features would be used in the training. 4) EERF(7), with approximated Laplace kernel feature z(x) = 1√ M [cos(xTγm + bm)]M m=1,where ﬁrst a larger number M0 random features are sampled and then re-weighted according to a score function. The top M random features would appear in the training. Experiment Setup:We also use approximated Laplace kernel feature z(x) = [φl,i(x)](l,i)∈S∗n where φl,i = ∏D d=1 φld,id with φld,id deﬁned as equation (4). To determine the value of σused in RKS, EERF, LKRF and ORF we choose the value of σ−1 for each dataset to be the mean distance of the 50th ℓ2 nearest neighbor (19). We then calculate the corresponding ωfor EOF associated to σ. The number of features in EOF is a function of dimension Dand level n, so it is not possible to calculate them for any M. To resolve this issue, for any given M, we select the set S∗ n deﬁned in (11) that satisﬁes ⏐⏐S∗ n−1 ⏐⏐<M ≤ ⏐⏐S∗ n ⏐⏐ and randomly select M pairs of (l,i) ∈S∗ n to have a random set SM . We then use the following 15Table 1: Input dimension, number of training samples, and number of test samples are denoted by D, Ntrain, and Ntest, respectively D ATA SET TASK D N TRAIN N TEST MNIST C LASSIFICATION 32 20000 10000 E LECTRICAL G RIDS S TABILITY C LASSIFICATION 13 7000 3000 S UPERCONDUCTIVITY R EGRESSION 81 15000 6263 E NERGY E FFICIENCY R EGRESSION 8 512 256 approximated feature: zM (x) := [φl,i(x)](l,i)∈SM . This is equivalent to randomly select M rows from the feature z(x) = [φl,i(x)](l,i)∈S∗n. We let M0 = 10M for LKRF and EERF, then for any M, we compare the performance of different algorithms. Datasets: In Table 1, we report the number of training samples Ntrain and test samples Ntest used for each dataset. For the MNIST data set, we map the original 784−dimensional data to a 32−dimensional space using an auto-encoder. If the training and test samples are not provided separately for a dataset, we split it randomly. We standardize the data as follows: we scale each input to the unit interval [0,1] and the responses in regression to be inside [−1,1]. Comparison: For a ﬁxed number of features, we perform 50 simulation runs for each algorithm on each data set. We then report the average test error (with standard errors) in Fig. 3 where the plot line is the mean error of an algorithm and the error bar reﬂects the standard deviation of the error. Throughout our experiments, we can see that EOF consistently improves the test error compared to other randomized- feature algorithms. This is speciﬁcally visible when the gap between SM and S∗ n becomes very small and, due to the optimality of S∗ n, EOF outperforms any random feature algorithm. In Table 2, we also compare the time complexity and space complexity. We deﬁne the feature matrix F := [z(xi)]N i=1, 16Figure 3: Comparison of the test error of EOF (this work) versus benchmark algorithms inclduing RKS, EERF, LKRF and ORF. 20 30 40 50 60 70 80 Number of Features (M) 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1 Test Error MNIST RKS EERF LKRF ORF EOF(this work) 20 25 30 35 40 45 50 55 60 Number of Features (M) 0 0.02 0.04 0.06 0.08 0.1 Test Error Electrical Grid Stability RKS EERF LKRF ORF EOF(this work) 80 90 100 110 120 130 140 150 160 Number of Features (M) 0.074 0.076 0.078 0.08 0.082 0.084 0.086 0.088 0.09 Test Error Superconductivity RKS EERF LKRF ORF EOF(this work) 10 15 20 25 30 35 40 45 50 55 60 Number of Features (M) 0 0.02 0.04 0.06 0.08 0.1 0.12Test Error Energy Efficiency RKS EERF LKRF ORF EOF(this work) which is an M ×N matrix with M the number of features and N the number of data. Due to the sparse structure of EOF, we can also see that the number of non-zero entries of the F associated to EOF is smaller than other methods. When both the dimension Dand the size of data N are large, the sparsity of EOF becomes more obvious as shown in the case of MNIST. The time cost of running EOF is also quite impressive. It is consistently better than EERF and LKRF and comparable and slightly slower than RKS. In fact, the major time for EOF is spent on feature matrix construction. For random features, due to high efﬁciency of matrix operations in Matlab, feature construction is fast. However, for EOF the feature construction via matrix operations is not possible in an efﬁcient way. We observed that after the feature matrix construction, EOF is the fastest method in training. For example, if we only count the training time (excluding feature construction) as the time cost, in kernel ridge regression on the dataset Superconductivity, the comparison between RKS and EOF is as follows: The run time is obtained on a MacPro with a 4-core, 3.3 GHz Intel Core i5 CPU and 8 GB of RAM (2133Mhz). 17Table 2: Time and Space Complexity Comparison MNIST Method M M0 Ttrain nnz(F) RKS 80 1.64 1.6 × 106 EERF 80 800 4.43 1.6 × 106 LKRF 80 800 3.07 1.6 × 106 ORF 80 1.21 1.6 × 106 EOF 80 2048 2.45 2.5 × 105 Superconductivity Method M M0 Ttrain nnz(F) RKS 160 0.10 2.4 × 106 EERF 160 1600 0.45 2.4 × 106 LKRF 160 1600 0.37 2.4 × 106 ORF 160 0.13 2.4 × 106 EOF 160 161 0.14 1.2 × 106 Electrical Grids Stability Method M M0 Ttrain nnz(F) RKS 60 0.04 4.2 × 105 EERF 60 600 0.14 4.2 × 105 LKRF 60 600 0.13 4.2 × 105 ORF 60 0.06 4.2 × 105 EOF 60 338 0.08 1.3 × 105 Energy Efﬁciency Method M M0 Ttrain nnz(F) RKS 60 0.01 6.1 × 103 EERF 60 600 0.05 6.1 × 103 LKRF 60 600 0.06 6.1 × 103 ORF 60 0.02 6.1 × 103 EOF 60 128 0.03 1.0 × 103 Table 3: Comparison on RKS and EOF in pure training excluding feature construction. M= 80 M= 100 M= 120 M= 140 M= 160 RKS 2×10−3 3×10−3 4×10−3 5×10−3 6×10−3 EOF 2×10−3 2×10−3 2×10−3 2×10−3 2×10−3 8 Conclusion We provide a method to construct a set of mutually orthogonal features (with nested and small supports) and select the best M of them that maximize the entropy of the associated projector. The nested and compact support of feature functions greatly reduces the time and space cost for feature matrix operations. The orthogonality and entropic optimality reduces dramatically the error of approximation. We have provided generalization error bound which indicates that only O(N 1 4 ) features are needed to achieve 18the O(N−1 2 ) optimal accuracy. Future directions include generalizing this method to a broader class of kernels. Supplementary Material A Proof of Theorem 1 The kernel function: k(x,y) = p(min{x,y})q(max{x,y}) is in fact the Green’s function of the Sturm-Liouville operator (39) L:= d dxα(x) d dx + β(x) So the inner product product induced by kis ⟨f,g⟩k = ∫ 1 0 fLgdx For any l∈N and i̸= j, the supports of φl,i and φl,j are [(i−1)2−l,(i+ 1)2−l] and [(j−1)2−l,(j+ 1)2−l]respectively. This two supports are disjoint because both iand j are odd so ⟨φl,i,φl,j⟩k = 0 if i̸= j. For any l,n ∈N and any i,j, the supports supt[φl,i] and supt[φn,j] are either disjoint or nested. If they are disjoint, then ⟨φn,j,φn,j⟩k = 0. If they are nested, , without loss of generality assume l>n and i≤j2l−n, then because both pand qsatisfy: Lp= Lq= 0 so ⟨φl,i,φn,j⟩k = ∫ (i+1)2−l (i−1)2−l φn,jLφl,idx = ∫ (i+1)2−l (i−1)2−l φn,jLp(x)ql,i−1 −q(x)pi,l−1 pl,iql,i−1 −ql,ipl,i−1 dx = 0. 19As a result, we have ⟨φl,i,φn,j⟩k = λl,iδ(l,i),(n,j) where λl,i is a function of land i. B Proof of Theorem 4 We need the following lemmas. Lemma 5. Denote fM = argminf∈FM ∥f0 −f∥k. Then we have R(fM ) −R(f0) ≤CM−2 log4D−4 M∥f0∥2 k, for some constant C. Proof. According to Assumption 2, we can see that R(fM ) −R(f0) = E[m′′ y(u∗)(fM (x) −f0(x))2] In view of Assumption 3, we only need to prove ∥fM −f0∥2 L2 = CM−2 log4D−4 M∥f0∥2 k for any f0 ∈Hk we then can ﬁnish the proof. Let M = |{(l,i) : |l|≤ n,i ∈Bl}|. According to theorem 2, we have the following expansion: ∥fM −f0∥L2 = ∥ ∑ |l|>n ∑ i∈Bl ⟨f0, φl,i ∥φl,i∥k ⟩k φl,i(·) ∥φl,i∥k ∥L2 = ∥ ∑ |l|>n ∑ i∈Bi ∫ Sl,i f0(s)Lφl,i(s)ds φl,i(·) ∥φl,i∥2 k ∥L2 . where Sl,i is the support of φl.i. We let v(·)l := ∑ i∈Bi ∫ Sl,i f0(s)Lφl,i(s)ds φl,i(·) ∥φl,i∥2 k . 20Our ﬁrst goal is to estimate vl. From theorem 2 of ( 40) or direct calculation based on the property of Green’s function, we can see that for anyf ∈Hk: ∫ Sl,i f(s)Lφl,i(s)ds = [ D⨂ d=1 ∆ld,id]f where ∆ld,idf := αld,idf ⏐⏐ xd=zld,id −βld,id−1f ⏐⏐ xd=zld,id−1 −βld,id+1f ⏐⏐ xd=zld,id αl,i = pl,i+1ql,i−1 −pl,i−1ql,i+1 [pl,i+1ql,i −pl,iql,i+1][pl,i1ql,i−1 −pl,i−1ql,i] βl,i = 1 pl,i+1ql,i −pl,iql,i+1 and ⨂denotes the tensor product of the ∆l,i operators. Since bouth q and pare the solution of the SL-equation, therefore, p,q are twice differentiable. We have 1 pl,i+1ql,i −pl,iql,i+1 = 2l [pl,i+1ql,i −pl,iql,i]/2−l −[pl,iql,i+1 −pl,iql,i]/2−l ∼ 2l p′ l,iql,i −pl,iq′ l,i we notice that p′ l,iql,i −pl,iq′ l,i is the Wronskian of the SL-operator, which is bounded away from 0. Therefore, ∆ld,id acting on f has the following approximation: ∆ld,idf ∼ [2f ⏐⏐ xd=zld,id −f ⏐⏐ xd=zld,id−1 −f ⏐⏐ xd=zld,id+1 ] 2−l ≤C max j=1,−1 { |f ⏐⏐ xd=zld,id+j −f ⏐⏐ xd=zld,id | 2−l }. As a result, ⨂D d=1 ∆ld,id acting on f has the following approximation: D⨂ d=1 ∆ld,idf ≤C D∏ d=1 max j=1,−1 { |f ⏐⏐ xd=zld,id+j −f ⏐⏐ xd=zld,id | 2−l }. 21From the same reasoning, we can see that ∥φl,i∥2 k = D∏ d=1 αld,id ∼2|l|. We also Taylor expandφld,id for each 1 ≤d≤Dup to second order and from direct calculation, we can have φld,id(x) ∼max{0,1 −|x−zld,id| 2−ld }+ O(2−ld). This gives us the approximation up to second order: ∥φl,i∥2 L2 = ∫ Sl,i D∏ d=1 φ2 ld,id(sd)ds ∼ ∫ Sl,i D∏ d=1 [max{0,1 −|s−zld,id| 2−ld }]2s = (2 3 )D2−|l|= (1 3 )DV ol(Sl,i). Therefore, we can have the following estimate for vl: ∥vl∥L2 = ∥ ∑ i∈Bi ∫ Sl,i f0(s)Lφl,i(s)ds φl,i(·) ∥φl,i∥2 k ∥L2 ≤ ⏐⏐⏐2−2|l|C ∑ i∈Bi [ D⨂ d=1 ∆ld,idf]2V ol(Sl,i) ⏐⏐⏐ 1 2 ∼2−|l|∥ D∏ d=1 ∂ ∂xd f0∥L2 ∼2−|l|∥f0∥k where the second line is from the fact that supports of {φl,i : i ∈Bl}are disjoint, the third line is from the Riemann integral approximation and the last line is from the energy estimate assumption of SL-operator 22(see, for instance, section 6.2.2 of (8)). Finally, we have: ∥f0 −fM ∥L2 ≤ ∑ |l|>n ∥vl∥L2 ∼∥f0∥k ∑ |l|>n 2−|l| = ∥f0∥k ∑ i>n 2−i ∑ |l|=i 1 = ∥f0∥k ∑ n>i 2−i (i−1 d−1 ) ∼∥f0∥k2−nnD−1 where the identity of the last line can be veriﬁed in (41). From (10) we also have M = O(2nnD−1) we can substitute this identity to the previous equation to have the ﬁnal result. The (ϵ,L∞)-covering number of a function space F, denoted as N(ϵ,F,∥·∥L∞), is deﬁned as the smallest number N0, so that there exist centers f1,...,f N0 , and for each f ∈F, there exists fi so that ∥f −fi∥L∞ <ϵ. Lemma 6. The covering number of the unit ball of Hk, denoted as F:= {f ∈Hk : ∥f∥k ≤1}, is bounded as follows: N(ϵ,F,∥·∥L∞) = O(1 εlogD−1 2 1 ε) Proof. When k(x,y) = e−ω∥x−y∥1 or k(x,y) = ∏D d=1 min{xd,yd}, then Hk is equivalent to the Sobolev space of mixed ﬁrst derivative H1 mix([0,1]D) (41). According to 6.6 of (42), we can immediately derive the result. When kernel kis different than these two, the energy property of an SL-operator requires that ⟨f,f ⟩k = ∫ [0,1]D f(x)[ D∏ d=1 L]f(x)d(x) ≤C ∫ [0,1]D | D∏ d=1 ∂ ∂xd f|2dx 23which means Hk can be embedded on H1 mix. Therefore, the covering number of Hk must be bounded by that of H1 mix. Lemma 7 shows the the function classes associated with the learning problem are Donsker. We refer to (43) for the deﬁnition and properties of Donsker classes. Lemma 7. Let P be the probability measure of (x,y). The space GR is P-Donsker for each R> 0. Proof. In view of Theorem 2.5.6 of (43), it sufﬁces to prove that ∫ ∞ 0 √ log N[](ϵ,GR,∥·∥L2(P))dϵ< ∞, where N[](ϵ,GR,∥·∥ L2(P)) is the covering number with bracketing deﬁned as follows. For function g : RD ×R →R, its L2(P) norm is deﬁned as [E[g(x,y)]2]1/2. Given functions gL,gU such that gL(u,v) ≤gU (u,v) for each (u,v), deﬁne the bracket [gL,gU ] as the set of functions {g : gL(u,v) ≤ g(u,v) ≤gU (u,v)}. The covering number with bracketing N[](ϵ,GR,∥·∥L2(P)) is the smallest number N0 so that there exist brackets [gL,1,gU,1],..., [gL,N0 ,gU,N0 ], such that ∪N0 i=1[gL,i,gU,i] ⊃ GR, and ∥gU,i −gL,i∥L2(P) ≤ϵfor all i. Let FR = {f : ∥f∥k <R}. We start with the centers f1,...,f N0 with N0 = N(ϵ,FR,∥·∥L∞) = N(ϵ/R,F1,∥·∥L∞) so that for each f ∈FR, there exists fi =: ξ(f) such that ∥f−fi∥L∞ <ϵ. To bound the covering number with bracketing, we need to construct the associated brackets. The reproduction property implies that ∥f∥L∞ ≤∥f∥k. Then for any f ∈FR, by mean value theorem, |L(y,f(x)) −L(y,ξ(f)(x))| ≤ sup |u|<R ⏐⏐⏐⏐ ∂L ∂u(y,u) ⏐⏐⏐⏐ϵ=: S(y)ϵ. Now we deﬁne gL,i(u,v) = L(v,fi(u)) −S(v)ϵ and gU,i(u,v) = L(v,fi(u)) + S(v)ϵ. Clearly gL,i ≤gU,i and ∥gU,i −gL,i∥L2(P) = 2ϵ[E[S(y)]2]1/2, 24which is a multiple of ϵaccording to Assumptions 2-3. Besides, (12) implies that for all f such that ∥f −fi∥L∞ <ϵ, L(v,f(u)) ∈[gL,i,gU,i]. So we invoke Lemma 6 to ﬁnd that N[](2ϵ[E[S(y)]2]1/2,∥·∥L2(P)) = O(1 εlogD−1 2 1 ε), which implies the desired result. To bound the generalization error, we observe R( ˆf) −R(f0) = { R( ˆf) − 1 N N∑ i=1 L(yi, ˆf(xi)) } + { 1 N N∑ i=1 L(yi, ˆf(xi)) − 1 N N∑ i=1 L(yi,fM (xi)) } + { 1 N N∑ i=1 L(yi,fM (xi)) −R(fM ) } + {R(fM ) −R(f0)}=: I1 + I2 + I3 + I4. We will bound I1 and I3 with a uniform error bound of empirical processes. For I2, we have I2 ≤λ∥fM ∥2 k −λ∥ˆf∥2 k ≤λ∥f0∥2 K = O(N−1/2)∥f0∥2 k, where the ﬁrst inequality follows from the optimality condition 1 N N∑ i=1 L(yi, ˆf(xi)) + λ∥ˆf∥2 k ≤ 1 N N∑ i=1 L(yi,fM (xi)) + λ∥fM ∥2 k. The term I4 is bounded by Lemma 5. Now we turn to I1 and I3. To show that I1 = Op(N−1/2) and I3 = Op(N−1/2), it sufﬁces to show that the functions L(y, ˆf(x)) and L(y,fM (x)) fall in a Donsker class (43) with probability arbitrarily close to one. For L(y,fM (x)), this is clearly true in view of Lemma 7 and the fact that ∥fM ∥k ≤∥f0∥k. Therefore, I3 = Op(N−1/2) For L(y, ˆf(x)), it sufﬁces to prove that ∥ˆf∥k = Op(1). To show this result, we start with the optimality condition 1 N N∑ i=1 L(yi, ˆf(xi)) + λ∥ˆf∥2 k ≤ 1 N N∑ i=1 L(yi,fM (xi)) + λ∥fM ∥2 k. 25In view of Assumption 2, we can write L(y,f(x)) −L(y,f0(x)) = W ·(f(x) −f0(x)) + m′′ y(u∗)(f(x) −f0(x))2, where W = m′ y(f0(x)), and u∗lies between f(x) and f0(x). Assumptions 2 and 3 implies that the derivative and the expectation are interchangeable, so that 0 = (Emy(f0(X)))′= Em′ y(f0(X)) = EW. We then invoke Assumption 2 to ﬁnd λ∥ˆf∥2 k ≤− 1 N N∑ i=1 Wi( ˆf(xi) −f0(xi)) + { 1 N N∑ i=1 L(yi,fM (xi)) − 1 N N∑ i=1 L(yi,f0(xi)) } − V( ˆf(xi) −f0(xi))2 + λ∥f0∥2 k =: J1 + J2 + J3 + J4, (12) for some V >0 due to the strong convexity of my(·). For the ﬁrst term, we have J1 ≤ (∥ˆf∥k + 1) sup f∈Hk 1 N N∑ i=1 −Wi f(xi) −f0(xi) ∥f∥K + 1 = ( ∥ˆf∥k + 1)Op(N−1/2), where the last step follows from the fact that EWi = 0, Wi is bounded, and Lemma 3.4.3 of (43) and the fact that ∥f−f0∥k/(∥f∥k +1) = O(1). Clearly, we haveJ2 = I3 +Op(N−1/2) = Op(N−1/2) according to the central limit theorem. The third term is clearly non-positive. We also have J4 = Op(N−1/2) by assumption for λ. Now we conclude from (12) that λ∥ˆf∥2 k ≤∥ˆf∥kOp(N−1/2) + Op(N−1/2), which implies ∥ˆf∥k = Op(1). This completes the proof. 26References and Notes 1. A. J. Smola and B. Sch ¨okopf, “Sparse greedy matrix approximation for machine learning,” in Proceedings of the Seventeenth International Conference on Machine Learning, 2000, pp. 911–918. 2. S. Fine and K. Scheinberg, “Efﬁcient SVM training using low-rank kernel representations,”Journal of Machine Learning Research, vol. 2, no. Dec, pp. 243–264, 2001. 3. A. Rahimi and B. Recht, “Random features for large-scale kernel machines,” inAdvances in neural information processing systems, 2008, pp. 1177–1184. 4. T. Joachims, “Training linear SVM’s in linear time,” in Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2006, pp. 217–226. 5. A. Sinha and J. C. Duchi, “Learning kernels with random features,” inAdvances In Neural Information Processing Systems, 2016, pp. 1298–1306. 6. H. Avron, M. Kapralov, C. Musco, C. Musco, A. Velingker, and A. Zandieh, “Random fourier features for kernel ridge regression: Approximation bounds and statistical guarantees,” in Proceedings of the 34th International Conference on Machine Learning-Volume 70, 2017, pp. 253–262. 7. S. Shahrampour, A. Beirami, and V . Tarokh, “On data-dependent random features for improved generalization in supervised learning,” in Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018. 8. L. C. Evans, Partial differential equations. Providence, R.I.: American Mathematical Society, 2010. 9. J. Dick, F. Kuo, and I. Sloan, “High-dimensional integration: The quasi-monte carlo way,” Acta Numerica, vol. 22, 05 2013. 10. H.-J. Bungartz and M. Griebel, “Sparse grids,”In: Acta Numerica. Vol. 13, pp. 147-269, vol. 13, 05 2004. 2711. H. Kellerer, U. Pferschy, and D. Pisinger, Knapsack Problems, 01 2004. 12. A. Rudi and L. Rosasco, “Generalization properties of learning with random features,” inAdvances in Neural Information Processing Systems, 2017, pp. 3218–3228. 13. J. Yang, V . Sindhwani, H. Avron, and M. Mahoney, “Quasi-monte carlo feature maps for shift-invariant kernels,” inInternational Conference on Machine Learning, 2014, pp. 485–493. 14. Q. Le, T. Sarl´os, and A. Smola, “Fastfood-approximating kernel expansions in loglinear time,” in International Conference on Machine Learning, vol. 85, 2013. 15. Z. Yang, A. Wilson, A. Smola, and L. Song, “A la carte–learning fast kernels,” inArtiﬁcial Intelligence and Statistics, 2015, pp. 1098–1106. 16. M. Munkhoeva, Y . Kapushev, E. Burnaev, and I. Oseledets, “Quadrature-based features for kernel approximation,” in Advances in Neural Information Processing Systems, 2018, pp. 9147–9156. 17. I. E.-H. Yen, T.-W. Lin, S.-D. Lin, P. K. Ravikumar, and I. S. Dhillon, “Sparse random feature algorithm as coordinate descent in hilbert space,” in Advances in Neural Information Processing Systems, 2014, pp. 2456–2464. 18. A. Rahimi and B. Recht, “Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning,” in Advances in Neural Information Processing Systems, 2009, pp. 1313– 1320. 19. X. Y . Felix, A. T. Suresh, K. M. Choromanski, D. N. Holtmann-Rice, and S. Kumar, “Orthogonal random features,” in Advances in Neural Information Processing Systems, 2016, pp. 1975–1983. 20. K. Choromanski, M. Rowland, T. Sarl´os, V . Sindhwani, R. Turner, and A. Weller, “The geometry of random features,” in International Conference on Artiﬁcial Intelligence and Statistics, 2018, pp. 1–9. 2821. F. X. Yu, S. Kumar, H. Rowley, and S.-F. Chang, “Compact nonlinear maps and circulant extensions,” arXiv preprint arXiv:1503.03893, 2015. 22. J. B. Oliva, A. Dubey, A. G. Wilson, B. P´oczos, J. Schneider, and E. P. Xing, “Bayesian nonparametric kernel-learning,” inArtiﬁcial Intelligence and Statistics, 2016, pp. 1078–1086. 23. R. Agrawal, T. Campbell, J. Huggins, and T. Broderick, “Data-dependent compression of random features for large-scale kernel approximation,” inThe 22nd International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2019, pp. 1822–1831. 24. C. Williams and M. Seeger, “Using the Nystr¨om method to speed up kernel machines,” inAdvances in Neural Information Processing Systems, 2001. 25. P. Drineas and M. W. Mahoney, “On the Nystr ¨om method for approximating a gram matrix for improved kernel-based learning,”Journal of Machine Learning Research, vol. 6, no. Dec, pp. 2153– 2175, 2005. 26. K. Zhang, I. W. Tsang, and J. T. Kwok, “Improved Nystr ¨om low-rank approximation and error analysis,” in International Conference on Machine Learning. ACM, 2008, pp. 1232–1239. 27. D. Oglic and T. G ¨artner, “Scalable learning in reproducing kernel krein spaces,” in International Conference on Machine Learning, 2019, pp. 4912–4921. 28. C. Yang, R. Duraiswami, and L. Davis, “Efﬁcient kernel machines using the improved fast gauss transform,” inProceedings of the 17th International Conference on Neural Information Processing Systems, 2004, pp. 1561–1568. 29. J.-W. Xu, P. P. Pokharel, K.-H. Jeong, and J. C. Principe, “An explicit construction of a reproducing gaussian kernel Hilbert space,” inIEEE International Conference on Acoustics, Speech and Signal Processing, vol. 5, 2006. 2930. A. Cotter, J. Keshet, and N. Srebro, “Explicit approximations of the gaussian kernel,”arXiv preprint arXiv:1109.4603, 2011. 31. A. Vedaldi and A. Zisserman, “Efﬁcient additive kernels via explicit feature maps,”IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, no. 3, pp. 480–492, 2012. 32. P. Vincent and Y . Bengio, “Kernel matching pursuit,”Machine Learning, vol. 48, no. 1-3, pp. 165–187, 2002. 33. P. B. Nair, A. Choudhury, and A. J. Keane, “Some greedy learning algorithms for sparse regression and classiﬁcation with mercer kernels,” Journal of Machine Learning Research, vol. 3, no. Dec, pp. 781–801, 2002. 34. V . Sindhwani and A. C. Lozano, “Non-parametric group orthogonal matching pursuit for sparse learning with multiple kernels,” in Advances in Neural Information Processing Systems, 2011, pp. 2519–2527. 35. A. Lozano, G. Swirszcz, and N. Abe, “Group orthogonal matching pursuit for logistic regression,” in Artiﬁcial Intelligence and Statistics, 2011, pp. 452–460. 36. F. Locatello, R. Khanna, M. Tschannen, and M. Jaggi, “A uniﬁed optimization view on generalized matching pursuit and frank-wolfe,” inArtiﬁcial Intelligence and Statistics, 2017, pp. 860–868. 37. D. Oglic and T. G¨artner, “Greedy feature construction,” inAdvances in Neural Information Processing Systems, 2016, pp. 3945–3953. 38. S. Shahrampour and V . Tarokh, “Learning bounds for greedy approximation with explicit feature maps from multiple kernels,” in Advances in Neural Information Processing Systems , 2018, pp. 4695–4706. 3039. V . F. Zaitsev and A. D. Polyanin,Handbook of Exact Solutions for Ordinary Differential Equations, 2nd ed. CRC Press, 2002. 40. L. Ding and X. Zhang, “Scalable stochastic kriging with markovian covariances,” 2018. 41. L. Ding, S. Mak, and C.-F. Wu, “Bdrygp: a new gaussian process model for incorporating boundary information,” 08 2019. 42. D. Dung, V . Temlyakov, and T. Ullrich, “Hyperbolic cross approximation,” 01 2016. 43. A. W. van der Vaart and J. A. Wellner,Weak Convergence and Empirical Processes. Springer, 1996. 31",
      "meta_data": {
        "arxiv_id": "2002.04195v1",
        "authors": [
          "Liang Ding",
          "Rui Tuo",
          "Shahin Shahrampour"
        ],
        "published_date": "2020-02-11T04:12:31Z",
        "pdf_url": "https://arxiv.org/pdf/2002.04195v1.pdf"
      }
    }
  ]
}