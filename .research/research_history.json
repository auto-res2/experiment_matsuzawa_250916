{
  "research_topic": "Graph Attention Networkの学習の高速化",
  "queries": [
    "efficient GAT training",
    "GAT training acceleration",
    "parallel GAT training",
    "sparse GAT training",
    "quantized GAT training"
  ],
  "research_study_list": [
    {
      "title": "Bandit Samplers for Training Graph Neural Networks",
      "abstract": "Several sampling algorithms with variance reduction have been proposed for\naccelerating the training of Graph Convolution Networks (GCNs). However, due to\nthe intractable computation of optimal sampling distribution, these sampling\nalgorithms are suboptimal for GCNs and are not applicable to more general graph\nneural networks (GNNs) where the message aggregator contains learned weights\nrather than fixed weights, such as Graph Attention Networks (GAT). The\nfundamental reason is that the embeddings of the neighbors or learned weights\ninvolved in the optimal sampling distribution are changing during the training\nand not known a priori, but only partially observed when sampled, thus making\nthe derivation of an optimal variance reduced samplers non-trivial. In this\npaper, we formulate the optimization of the sampling variance as an adversary\nbandit problem, where the rewards are related to the node embeddings and\nlearned weights, and can vary constantly. Thus a good sampler needs to acquire\nvariance information about more neighbors (exploration) while at the same time\noptimizing the immediate sampling variance (exploit). We theoretically show\nthat our algorithm asymptotically approaches the optimal variance within a\nfactor of 3. We show the efficiency and effectiveness of our approach on\nmultiple datasets.",
      "full_text": "Bandit Samplers for Training Graph Neural Networks Ziqi Liu∗ Ant Financial Services Group ziqiliu@antfin.com Zhengwei Wu∗ Ant Financial Services Group zejun.wzw@antfin.com Zhiqiang Zhang Ant Financial Services Group lingyao.zzq@antfin.com Jun Zhou Ant Financial Services Group jun.zhoujun@antfin.com Shuang Yang Ant Financial Services Group shuang.yang@antfin.com Le Song Ant Financial Services Group Georgia Institute of Technology lsong@cc.gatech.edu Yuan Qi Ant Financial Services Group yuan.qi@antfin.com Abstract Several sampling algorithms with variance reduction have been proposed for ac- celerating the training of Graph Convolution Networks (GCNs). However, due to the intractable computation of optimal sampling distribution, these sampling algorithms are suboptimal for GCNs and are not applicable to more general graph neural networks (GNNs) where the message aggregator contains learned weights rather than ﬁxed weights, such as Graph Attention Networks (GAT). The funda- mental reason is that the embeddings of the neighbors or learned weights involved in the optimal sampling distribution arechanging during the training and not known a priori, but only partially observed when sampled, thus making the derivation of an optimal variance reduced samplers non-trivial. In this paper, we formulate the optimization of the sampling variance as an adversary bandit problem, where the rewards are related to the node embeddings and learned weights, and can vary constantly. Thus a good sampler needs to acquire variance information about more neighbors (exploration) while at the same time optimizing the immediate sampling variance (exploit). We theoretically show that our algorithm asymptotically ap- proaches the optimal variance within a factor of 3. We show the efﬁciency and effectiveness of our approach on multiple datasets. 1 Introduction Graph neural networks [ 15, 11] have emerged as a powerful tool for representation learning of graph data in irregular or non-euclidean domains [3, 23]. For instance, graph neural networks have demonstrated state-of-the-art performance on learning tasks such as node classiﬁcation, link and graph property prediction, with applications ranging from drug design [ 8], social networks [ 11], transaction networks [16], gene expression networks [9], and knowledge graphs [19]. One major challenge of training GNNs comes from the requirements of heavy ﬂoating point operations and large memory footprints, due to the recursive expansions over the neighborhoods. For a minibatch with a single vertex vi, to compute its embedding h(L) i at the L-th layer, we have to expand its neighborhood from the (L−1)-th layer to the 0-th layer, i.e. L-hops neighbors. That will soon cover ∗Equal Contribution. Preprint. Under review. arXiv:2006.05806v2  [cs.LG]  11 Jun 2020a large portion of the graph if particularly the graph is dense. One basic idea of alleviating such “neighbor explosion” problem was to sample neighbors in a top-down manner, i.e. sample neighbors in the l-th layer given the nodes in the (l+ 1)-th layer recursively. Several layer sampling approaches [11, 6, 14, 25] have been proposed to alleviate above “neighbor explosion” problem and improve the convergence of training GCNs, e.g. with importance sampling. However, the optimal sampler [14], q⋆ ij = αij∥h(l) j ∥2 ∑ k∈Ni αik∥h(l) k ∥2 for vertex vi, to minimize the variance of the estimator ˆh(l+1) i involves all its neighbors’ hidden embeddings, i.e.{ˆh(l) j |vj ∈Ni}, which is infeasible to be computed because we can only observe them partially while doing sampling. Existing approaches [6, 14, 25] typically compromise the optimal sampling distribution via approximations, which may impede the convergence. Moreover, such approaches are not applicable to more general cases where the weights or kernelsαij’s are not known a priori, but are learned weights parameterized by attention functions [22]. That is, both the hidden embeddings and learned weights involved in the optimal sampler constantly vary during the training process, and only part of the unnormalized attention values or hidden embeddings can be observed while do sampling. Present work. We derive novel variance reduced samplers for training of GCNs and attentive GNNs with a fundamentally different perspective. That is, different with existing approaches that need to compute the immediate sampling distribution, we maintain nonparametric estimates of the sampler instead, and update the sampler towards optimal variance after we acquire partial knowledges about neighbors being sampled, as the algorithm iterates. To fulﬁl this purpose, we formulate the optimization of the samplers as a bandit problem, where the regret is the gap between expected loss (negative reward) under current policy (sampler) and expected loss with optimal policy. We deﬁne the reward with respect to each action, i.e. the choice of a set of neighbors with sample size k, as the derivatives of the sampling variance, and show the variance of our samplers asymptotically approaches the optimal variance within a factor of3. Under this problem formulation, we propose two bandit algorithms. The ﬁrst algorithm based on multi-armed bandit (MAB) chooses k < Karms (neighbors) repeatedly. Our second algorithm based on MAB with multiple plays chooses a combinatorial set of neighbors with size konly once. To summarize, ( 1) We recast the sampler for GNNs as a bandit problem from a fundamentally different perspective. It works for GCNs and attentive GNNs while existing approaches apply only to GCNs. ( 2) We theoretically show that the regret with respect to the variance of our estimators asymptotically approximates the optimal sampler within a factor of 3 while no existing approaches optimize the sampler. ( 3) We empirically show that our approachs are way competitive in terms of convergence and sample variance, compared with state-of-the-art approaches on multiple public datasets. 2 Problem Setting Let G= (V,E) denote the graph with N nodes vi ∈V, and edges (vi,vj) ∈E. Let the adjacency matrix denote as A∈RN×N. Assuming the feature matrix H(0) ∈RN×D(0) with h(0) i denoting the D(0)-dimensional feature of node vi. We focus on the following simple but general form of GNNs: h(l+1) i = σ ( N∑ j=1 α(vi,vj) h(l) j W(l) ) , l = 0,...,L −1 (1) where h(l) i is the hidden embedding of node vi at the l-th layer, ααα= (α(vi,vj)) ∈RN×N is a kernel or weight matrix, W(l) ∈RD(l)×D(l+1) is the transform parameter on the l-th layer, and σ(·) is the activation function. The weight α(vi,vj), or αij for simplicity, is non-zero only if vj is in the 1-hop neighborhood Ni of vi. It varies with the aggregation functions [ 3, 23]. For example, (1) GCNs [8, 15] deﬁne ﬁxed weights as ααα= ˜D−1 ˜Aor ααα= ˜D−1 2 ˜A˜D−1 2 respectively, where ˜A= A+I, and ˜Dis the diagonal node degree matrix of ˜A. (2) The attentive GNNs [22, 17] deﬁne a learned weight α(vi,vj) by attention functions: α(vi,vj) = ˜α(vi,vj;θ)∑ vk∈Ni ˜α(vi,vk;θ) , where the unnormalized attentions ˜α(vi,vj; θ) = exp(ReLU(aT[Whi∥Whj])), are parameterized by θ= {a,W}. Different 2from GCNs, the learned weights αij ∝˜αij can be evaluated only given all the unnormalized weights in the neighborhood. The basic idea of layer sampling approaches [11, 6, 14, 25] was to recast the evaluation of Eq. (1) as ˆh(l+1) i = σ ( N(i) Epij [ ˆh(l) j ] W(l) ) , (2) where pij ∝αij, and N(i) =∑ jαij. Hence we can evaluate each node vi at the (l+ 1)-th layer, using a Monte Carlo estimator with sampled neighbors at the l-th layer. Without loss of generality, we assume pij = αij and N(i) = 1that meet the setting of attentive GNNs in the rest of this paper. To further reduce the variance, let us consider the following importance sampling ˆh(l+1) i = σW(l) ( ˆµ(l) i ) = σW(l) ( Eqij [αij qij ˆh(l) j ]) , (3) where we use σW(l) (·) to include transform parameter W(l) into the function σ(·) for conciseness. As such, one can ﬁnd an alternative sampling distribution qi = (qij1 ,...,q ij|Ni|) to reduce the variance of an estimator, e.g. a Monte Carlo estimator ˆµ(l) i = 1 k ∑k s=1 αijs qijs ˆh(l) js , where js ∼qi. Take expectation overqi, we deﬁne the variance of ˆµ(l) i = αijs qijs ˆh(l) js at step tand (l+ 1)-th layer to be: Vt(qi) =E [ˆµ(l) i (t) −µ(l) i (t)  2] = E [αijs(t) qijs h(l) js (t) − ∑ j∈Ni αij(t)h(l) j (t)  2] . (4) Note that αij and h(vj) that are inferred during training may vary over steps t’s. We will explicitly include step tand layer lonly when it is necessary. By expanding Eq. (4) one can write V(qi) as the difference of two terms. The ﬁrst is a function of qi, which we refer to as the effective variance: Ve(qi) = ∑ j∈Ni 1 qij α2 ij∥hj∥2 , (5) while the second does not depend on qi, and we denote it by Vc = ∑ j∈Ni αijhj  2 . The optimal sampling distribution [6, 14] at (l+ 1)-th layer for vertex ithat minimizes the variance is: q⋆ ij = αij∥h(l) j ∥2 ∑ k∈Ni αik∥h(l) k ∥2 . (6) However, evaluating this sampling distribution is infeasible because we cannot have all the knowledges of neighbors’ embeddings in the denominator of Eq.(6). Moreover, the αij’s in attentive GNNs could also vary during the training procedure. Existing layer sampling approaches based on importance sampling just ignore the effects of norm of embeddings and assume theαij’s are ﬁxed during training. As a result, the sampling distribution is suboptimal and only applicable to GCNs where the weights are ﬁxed. Note that our derivation above follows the setting of node-wise sampling approaches [11], but the claim remains to hold for layer-wise sampling approaches [6, 14, 25]. 3 Related Works We summarize three types of works for training graph neural networks. First, several “layer sampling” approaches [11, 6, 14, 25] have been proposed to alleviate the “neigh- bor explosion” problems. Given a minibatch of labeled vertices at each iteration, such approaches sample neighbors layer by layer in a top-down manner. Particularly, node-wise samplers [11] ran- domly sample neighbors in the lower layer given each node in the upper layer, while layer-wise samplers [6, 14, 25] leverage importance sampling to sample neighbors in the lower layer given all the nodes in upper layer with sample sizes of each layer be independent of each other. Empirically, the layer-wise samplers work even worse [5] compared with node-wise samplers, and one can set an appropriate sample size for each layer to alleviate the growth issue of node-wise samplers. In this paper, we focus on optimizing the variance in the vein of layer sampling approaches. Though the 3derivation of our bandit samplers follows the node-wise samplers, it can be extended to layer-wise. We leave this extension as a future work. Second, Chen et al. [5] proposed a variance reduced estimator by maintaining historical embeddings of each vertices, based on the assumption that the embeddings of a single vertex would be close to its history. This estimator uses a simple random sampler and works efﬁcient in practice at the expense of requiring an extra storage that is linear with number of nodes. Third, two “graph sampling” approaches [7, 24] ﬁrst cut the graph into partitions [ 7] or sample into subgraphs [24], then they train models on those partitions or subgraphs in a batch mode [ 15]. They show that the training time of each epoch may be much faster compared with “layer sampling” approaches. We summarize the drawbacks as follows. First, the partition of the original graph could be sensitive to the training problem. Second, these approaches assume that all the vertices in the graph have labels, however, in practice only partial vertices may have labels [12, 16]. GNNs Architecture.For readers who are interested in the works related to the architecture of GNNs, please refer to the comprehensive survey [23]. Existing sampling approaches works only on GCNs, but not on more advanced architectures like GAT [22]. 4 Variance Reduced Samplers as Bandit Problems We formulate the optimization of sampling variance as a bandit problem. Our basic idea is that instead of explicitly calculating the intractable optimal sampling distribution in Eq. (6) at each iteration, we aim to optimize a sampler or policy Qt i for each vertex iover the horizontal steps 1 ≤t≤T, and make the variance of the estimator following this sampler asymptotically approach the optimum Q⋆ i = argmin Qi ∑T t=1 Vt e(Qi), such that ∑T t=1 Vt e(Qt i) ≤c∑T t=1 Vt e(Q⋆ i) for some constant c >1. Each action of policy Qt i is a choice of any subset of neighbors Si ⊂Ni where Si ∼Qt i. We denote Qi,Si(t) as the probability of the action that vi chooses Si at t. The gap to be minimized between effective variance and the oracle is Vt e(Qt i) −Vt e(Q⋆ i) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. (7) Note that the function Vt e(Qt i) is convex w.r.t Qt i, hence for Qt i and Q⋆ i we have the upper bound derived on right hand of Eq. (7). We deﬁne this upper bound as regret at t, which means the expected loss (negative reward) with policy Qt i minus the expected loss with optimal policy Q⋆ i. Hence the reward w.r.t choosing Si at tis the negative derivative of the effective varianceri,Si(t) = −∇Qi,Si(t)Vt e(Qt i). In the following, we adapt this bandit problem in the adversary bandit setting [1] because the rewards vary as the training proceeds and do not follow a priori ﬁxed distribution [4]. We leave the studies of other bandits as a future work. We show in section 6 that with this regret the variances of our estimators asymptotically approach the optimal variance within a factor of 3. Our samplers sample 1-element subset of neighbors k times or a k-element subset of neighbors once from the alternative sampling distribution qt i = (qij1 (t),...,q ij|Ni|(t)) for each vertex vi. We instantiate above framework under two bandit settings.(1) In the adversary MAB setting [1], we deﬁne the sampler Qt i as qt i, that samples exact anarm (neighbor) vjs ⊂Nifrom qt i. In this case the set Siis the 1-element subset {vjs}. To have a sample size ofkneighbors, we repeat this action ktimes. After we collected krewards rijs(t) =−∇qi,js(t)Vt e(qt i) we update qt i by EXP3 [1]. (2) In the adversary MAB with multiple plays setting [21], it uses an efﬁcient k-combination sampler (DepRound [10]) Qi to sample any k-element subset Si ⊂Ni that satisﬁes ∑ Si:j∈Si Qi,Si = qij,∀vj ∈Ni, where qij corresponds to the alternative probability of sampling vj. As such, it allows us to select from a set of (|Ni| k ) distinct subsets of armsfrom |Ni|arms at once. The selection can be done in O(|Ni|). After we collected the reward −∇Qi,Si(t)Vt e(Qt i), we update qt i by EXP3.M [21]. Discussions. We have to select a sample size of k neighbors in GNNs. Note that in the rigorous bandit setting, exact one action should be made and followed by updating the policy. In adversary MAB, we do the selection ktimes and update the policy, hence strictly speaking applying MAB to our problem is not rigorous. Applying MAB with multiple plays to our problem is rigorous because it allows us to sample k neighbors at once and update the rewards together. For readers who are interested in EXP3, EXP3.M and DepRound, please ﬁnd them in Appendix A. 4Algorithm 1Bandit Samplers for Training GNNs. Require: step T, sample size k, number of layers L, node features H(0), adjacency matrix A. 1: Initialize: qij(1) = 1/|Ni|if j ∈Ni else 0, wij(1) = 1if j ∈Ni else 0. 2: for t= 1to T do 3: Read a minibatch of labeled vertices at layer L. 4: Use sampler qt i or DepRound(k,qt i) to sample neighbors top-down with sample size k. 5: Forward GNN model via estimators deﬁned in Eq. (8) or Proposition 1. 6: Backpropagation and update GNN model. 7: for each vi in the 1-st layer do 8: Collect vi’sksampled neighbors vj ∈St i, and rewards rt i = {rij(t) :vj ∈St i}. 9: Update qt+1 i and wt+1 i by EXP3(qt i,wt i,rt i,St i) or EXP3.M(qt i,wt i,rt i,St i). 10: end for 11: end for 12: return GNN model. 5 Algorithms The framework of our algorithm is: (1) use a sampler Qt i to sample k arms from the alternative sampling distribution qt i for any vertex vi, (2) establish the unbiased estimator, (3) do feedforward and backpropagation, and ﬁnally (4) calculate the rewards and update the alternative sampling distribution with a proper bandit algorithm. We show this framework in Algorithm 1. Note that the variance w.r.tqi in Eq. (4) is deﬁned only at the (l+ 1)-th layer, hence we should maintain multiple qi’s at each layer. In practice, we ﬁnd that maintain a single qi and update it only using rewards from the 1-st layer works well enough. The time complexity of our algorithm is same with any node-wise approaches [11]. In addition, it requires a storage in O(|E|) to maintain nonparametric estimates qi’s. It remains to instantiate the estimators, variances and rewards related to our two bandit settings. We name our ﬁrst algorithm GNN-BS under adversary MAB setting, and the second GNN-BS.M under adversary MAB with multiple plays setting. We ﬁrst assume the weights αij’s are ﬁxed, then extend to attentive GNNs that αij(t)’s change. 5.1 GNN-BS: Graph Neural Networks with Bandit Sampler In this setting, we choose 1 arm and repeat ktimes. We have the following Monte Carlo estimator ˆµi = 1 k k∑ s=1 αijs qijs ˆhjs, js ∼qi. (8) This yields the variance V(qi) = 1 k Eqi [ αijs qijs hjs −∑ j∈Ni αijhj  2] . Following Eq. (5) and Eq. (7), we have the reward of vi picking neighbor vj at step tas rij(t) =−∇qij(t)Vt e(qt i) = α2 ij k·qij(t)2 ∥hj(t)∥2. (9) 5.2 GNN-BS.M: Graph Neural Networks with Multiple Plays Bandit Sampler Given a vertex vi, an important property of DepRound is that it satisﬁes ∑ Si:j∈Si Qi,Si = qij,∀vj ∈ Ni, where Si ⊂Ni is any subset of size k. We have the following unbiased estimator. Proposition 1. ˆµi = ∑ js∈Si αijs qijs hjs is the unbiased estimator of µi = ∑ j∈Ni αijhj given that Si is sampled from qi using the DepRound sampler Qi, where Si is the selected k-subset neighbors of vertex i. The effective variance of this estimator is Ve(Qi) =∑ Si⊂Ni Qi,Si∥∑ js∈Si αijs qijs hjs∥2. Since the derivative of this effective variance w.r.t Qi,Si does not factorize, we instead have the following approximated effective variance using Jensen’s inequality. Proposition 2. The effective variance can be approximated by Ve(Qi) ≤∑ js∈Ni αijs qijs ∥hjs∥2. 5Table 1: Dataset summary. “s” dontes multi-class task, and “m” denotes multi-label task. Dateset V E Degree # Classes # Features # train # validation # test Cora 2,708 5 ,429 2 7 (s) 1,433 1 ,208 500 1 ,000 Pubmed 19,717 44 ,338 3 3 (s) 500 18 ,217 500 1 ,000 PPI 56,944 818 ,716 15 121 (m) 50 44 ,906 6 ,514 5 ,524 Reddit 232,965 11,606,919 50 41 (s) 602 153 ,932 23 ,699 55 ,334 Flickr 89,250 899 ,756 10 7 (s) 500 44 ,625 22 ,312 22 ,313 Proposition 3. The negative derivative of the approximated effective variance ∑ js∈Ni αijs qijs ∥hjs∥2 w.r.tQi,Si, i.e. the reward of vi choosing Si at tis ri,Si(t) =∑ js∈Si αijs qijs(t)2 ∥hjs(t)∥2. Follow EXP3.M we use the reward w.r.t each arm asrij(t) = αij qij(t)2 ∥hj(t)∥2,∀j ∈Si. Our proofs rely on the property of DepRound introduced above. 5.3 Extension to Attentive GNNs In this section, we extend our algorithms to attentive GNNs. The issue remained is that the attention value αij can not be evaluated with only sampled neighborhoods, instead, we can only compute the unnormalized attentions ˜αij. We deﬁne the adjusted feedback attention values as follows: α′ ij = ∑ j∈Si qij · ˜αij∑ j∈Si ˜αij , (10) where ˜αij’s are the unnormalized attention values that can be obviously evaluated when we have sampled (vi,vj). We use ∑ j∈Si qij as a surrogate of ∑ j∈Si ˜αij ∑ j∈Ni ˜αij so that we can approximate the truth attention values αij by our adjusted attention values α′ ij. 6 Regret Analysis As we described in section 4, the regret is deﬁned as ⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. By choosing the reward as the negative derivative of the effective variance, we have the following theorem that our bandit sampling algorithms asymptotically approximate the optimal variance within a factor of 3. Theorem 1. Using Algorithm 1 with η= 0.4 and δ= √ (1−η)η4k5 ln(n/k) Tn4 to minimize the effective variance with respect to {Qt i}1≤t≤T, we have T∑ t=1 Vt e(Qt i) ≤3 T∑ t=1 Vt e(Q⋆ i) + 10 √ Tn4 ln(n/k) k3 (11) where T ≥ln(n/k)n2(1 −η)/(kη2), n= |Ni|. Our proof follows [18] by upper and lower bounding the potential function. The upper and lower bounds are the functions of the alternative sampling probability qij(t) and the reward rij(t) respec- tively. By multiplying the upper and lower bounds by the optimal sampling probability q⋆ i and using the reward deﬁnition in (9), we have the upper bound of the effective variance. The growth of this regret is sublinear in terms of T. The regret decreases in polynomial as sample size kgrows. Note that the number of neighbors nis always well bounded in pratical graphs, and can be considered as a moderate constant number. Compared with existing layer sampling approaches [11, 6, 25] that have a ﬁxed variance given the speciﬁc estimators, this is the ﬁrst work optimizing the sampling variance of GNNs towards optimum. We will empirically show the sampling variances in experiments. 7 Experiments In this section, we conduct extensive experiments compared with state-of-the-art approaches to show the advantage of our training approaches. We use the following rule to name our approaches: GNN 6architecture plus bandit sampler. For example, GCN-BS, GAT-BSand GP-BS denote the training approaches for GCN, GAT [22] and GeniePath [17] respectively. The major purpose of this paper is to compare the effects of our samplers with existing training algorithms, so we compare them by training the same GNN architecture. We use the following architectures unless otherwise stated. We ﬁx the number of layers as 2 as in [15] for all comparison algorithms. We set the dimension of hidden embeddings as 16 for Cora and Pubmed, and 256 for PPI, Reddit and Flickr. For a fair comparison, we do not use the normalization layer [2] particularly used in some works [5, 24]. For attentive GNNs, we use the attention layer proposed in GAT. we set the number of multi-heads as 1 for simplicity. We report results on 5 benchmark data that include Cora [20], Pubmed [20], PPI [11], Reddit [11], and Flickr [24]. We follow the standard data splits, and summarize the statistics in Table 1. Table 2: Comparisons on the GCN architecture: testing Micro F1 scores. Method Cora Pubmed PPI Reddit Flickr GraphSAGE 0.731(±0.014) 0.890(±0.002) 0.689(±0.005) 0.949(±0.001) 0.494(±0.001) FastGCN 0.827(±0.001) 0.895(±0.005) 0.502(±0.003) 0.825(±0.006) 0.500(±0.001) LADIES 0.843(±0.003) 0.880(±0.006) 0.574(±0.003) 0.932(±0.001) 0.465(±0.007) AS-GCN 0.830(±0.001) 0.888(±0.006) 0.599(±0.004) 0.890(±0.013) 0.506(±0.012) S-GCN 0.828(±0.001) 0.893(±0.001) 0.744(±0.003) 0.943(±0.001) 0.501(±0.002) ClusterGCN 0.807(±0.006) 0.887(±0.001) 0.853(±0.001) 0.938(±0.002) 0.418(±0.002) GraphSAINT 0.815(±0.012) 0.899(±0.002) 0.787(±0.003) 0.965(±0.001) 0.507(±0.001) GCN-BS 0.855(±0.005) 0.903(±0.001) 0.905(±0.003) 0.957(±0.000) 0.513(±0.001) Table 3: Comparisons on the attentive GNNs architecture: testing Micro F1 scores. Method Cora Pubmed PPI Reddit Flickr AS-GAT 0.813(±0.001) 0.884(±0.003) 0.566(±0.002) NA 0.472(±0.012) GraphSAINT-GAT0.773(±0.036) 0.886(±0.016) 0.789(±0.001) 0.933(±0.012) 0.470(±0.002) GAT-BS 0.857(±0.003) 0.894(±0.001) 0.841(±0.001) 0.962(±0.001) 0.513(±0.001) GAT-BS.M 0.857(±0.003) 0.894(±0.000) 0.867(±0.003) 0.962(±0.000) 0.513(±0.001) GP-BS 0.811(±0.002) 0.890(±0.003) 0.958(±0.001) 0.964(±0.000) 0.507(±0.000) GP-BS.M 0.811(±0.001) 0.892(±0.001) 0.965(±0.001) 0.964(±0.000) 0.507(±0.000) We summarize the comparison algorithms as follows. (1) GraphSAGE [11] is a node-wise layer sampling approach with a random sampler. (2) FastGCN [6], LADIES [ 25], and AS-GCN [ 14] are layer sampling approaches based on importance sampling. (3) S-GCN [5] can be viewed as an optimization solver for training of GCN based on a simply random sampler. (4) ClusterGCN [7] and GraphSAINT [24] are “graph sampling” techniques that ﬁrst partition or sample the graph into small subgraphs, then train each subgraph using the batch algorithm [15]. (5) The open source algorithms that support the training of attentive GNNs are AS-GCN and GraphSAINT. We denote them as AS-GAT and GraphSAINT-GAT. We do grid search for the following hyperparameters in each algorithm, i.e., the learning rate {0.01,0.001}, the penalty weight on theℓ2-norm regularizers {0,0.0001,0.0005,0.001}, the dropout rate {0,0.1,0.2,0.3}. By following the exsiting implementations2, we save the model based on the best results on validation, and restore the model to report results on testing data in Section 7.1. For the sample size k in GraphSAGE, S-GCN and our algorithms, we set 1 for Cora and Pubmed, 5 for Flickr, 10 for PPI and reddit. We set the sample size in the ﬁrst and second layer for FastGCN and AS-GCN/AS-GAT as 256 and 256 for Cora and Pubmed, 1,900 and 3,800 for PPI, 780 and 1,560 for Flickr, and 2,350 and 4,700 for Reddit. We set the batch size of all the layer sampling approaches and S-GCN as 256 for all the datasets. For ClusterGCN, we set the partitions according 2Checkout: https://github.com/matenure/FastGCN or https://github.com/huangwb/AS-GCN 7to the suggestions [7] for PPI and Reddit. We set the number of partitions for Cora and Pubmed as 10, for ﬂickr as 200 by doing grid search. We set the architecture of GraphSAINT as “0-1-1”3 which means MLP layer followed by two graph convolution layers. We use the “rw” sampling strategy that reported as the best in their original paper to perform the graph sampling procedure. We set the number of root and walk length as the paper suggested. 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Cora GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 0 20 40 60 80 100 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2 4 6 8 10 Epoch 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.350 0.375 0.400 0.425 0.450 0.475 0.500 0.525Micro F1 on validation Flickr GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Cora GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 0 5 10 15 20 25 30 35 40 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 2 4 6 8 10 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT Figure 1: The convergence on validation in terms of epochs. 7.1 Results on Benchmark Data We report the testing results on GCN and attentive GNN architectures in Table 2 and Table 3 respectively. We run the results of each algorithm3 times and report the mean and standard deviation. The results on the two layer GCN architecture show that our GCN-BS performs the best on most of datasets. The results on the two layer attentive GNN architecture show the superiority of our algorithms on training more complex GNN architectures. GraphSAINT or AS-GAT cannot compute the softmax of learned weights, but simply use the unnormalized weights to perform the aggregation. As a result, most of results from AS-GAT and GraphSAINT-GAT in Table 3 are worse than their results in Table 2. Thanks to the power of attentive structures in GNNs, our algorithms perform better results on PPI and Reddit compared with GCN-BS, and signiﬁcantly outperform the results from AS-GAT and GraphSAINT-GAT. 7.2 Convergence In this section, we analyze the convergences of comparison algorithms on the two layer GCN and attentive GNN architectures in Figure 1 in terms of epoch. We run all the algorithms3 times and show the mean and standard deviation. Our approaches consistently converge to better results with faster 3Checkout https://github.com/GraphSAINT/ for more details. 8rates and lower variances in most of datasets like Pubmed, PPI, Reddit and Flickr compared with the state-of-the-art approaches. The GNN-BS algorithms perform very similar to GNN-BS.M, even though strictly speaking GNN-BS does not follow the rigorous MAB setting. Furthermore, we show a huge improvement on the training of attentive GNN architectures compared with GraphSAINT-GAT and AS-GAT. The convergences on validation in terms of timing (seconds), compared with layer sampling approaches, in Appendix C.1 show the similar results. We further give a discussion about timing among layer sampling approaches and graph sampling approaches in Appendix C.2. 3 5 7 10 Sample Size 0.4 0.6 0.8 1.0Micro F1 on test FastGCN LADIES AS-GCN GraphSAGE S-GCN GCN-BS AS-GAT GP-BS 3 5 7 10 Sample Size 0.5 1.0 1.5 2.0Variance FastGCN LADIES AS-GCN GraphSAGE S-GCN GCN-BS AS-GAT GP-BS Figure 2: Comparisons on PPI by varying the sample sizes: ( left) F1 score, (right) sample variances. 7.3 Sample Size Analysis We analyze the sampling variances and accuracy as sample size varies using PPI data. Note that existing layer sampling approaches do not optimize the variances once the samplers are speciﬁed. As a result, their variances are simply ﬁxed [25], while our approaches asymptotically appoach the optimum. For comparison, we train our models until convergence, then compute the average sampling variances. We show the results in Figure 2. The results are grouped into two categories, i.e. results for GCNs and attentive GNNs respectively. The sampling variances of our approaches are smaller in each group, and even be smaller than the variances of S-GCN that leverages a variance reduction solver. This explains the performances of our approaches on testing Micro F1 scores. We also ﬁnd that the overall sampling variances of node-wise approaches are way better than those of layer-wise approaches. 8 Conclusions In this paper, we show that the optimal layer samplers based on importance sampling for training general graph neural networks are computationally intractable, since it needs all the neighbors’ hidden embeddings or learned weights. Instead, we re-formulate the sampling problem as a bandit problem that requires only partial knowledges from neighbors being sampled. We propose two algorithms based on multi-armed bandit and MAB with multiple plays, and show the variance of our bandit sampler asymptotically approaches the optimum within a factor of 3. Furthermore, our algorithms are not only applicable to GCNs but more general architectures like attentive GNNs. We empirically show that our algorithms can converge to better results with faster rates and lower variances compared with state-of-the-art approaches. References [1] P. Auer, N. Cesa-Bianchi, Y . Freund, and R. E. Schapire. The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):48–77, 2002. [2] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [3] P. W. Battaglia, J. B. Hamrick, V . Bapst, A. Sanchez-Gonzalez, V . Zambaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018. [4] G. Burtini, J. Loeppky, and R. Lawrence. A survey of online experiment design with the stochastic multi-armed bandit. arXiv preprint arXiv:1510.00757, 2015. 9[5] J. Chen, J. Zhu, and L. Song. Stochastic training of graph convolutional networks with variance reduction. arXiv preprint arXiv:1710.10568, 2017. [6] J. Chen, T. Ma, and C. Xiao. Fastgcn: fast learning with graph convolutional networks via importance sampling. arXiv preprint arXiv:1801.10247, 2018. [7] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh. Cluster-gcn: An efﬁcient algorithm for training deep and large graph convolutional networks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 257–266, 2019. [8] H. Dai, B. Dai, and L. Song. Discriminative embeddings of latent variable models for structured data. In International conference on machine learning, pages 2702–2711, 2016. [9] A. Fout, J. Byrd, B. Shariat, and A. Ben-Hur. Protein interface prediction using graph convo- lutional networks. In Advances in Neural Information Processing Systems, pages 6530–6539, 2017. [10] R. Gandhi, S. Khuller, S. Parthasarathy, and A. Srinivasan. Dependent rounding and its applications to approximation algorithms. Journal of the ACM (JACM), 53(3):324–360, 2006. [11] W. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, pages 1024–1034, 2017. [12] B. Hu, Z. Zhang, C. Shi, J. Zhou, X. Li, and Y . Qi. Cash-out user detection based on attributed heterogeneous information network with a hierarchical attention mechanism. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 946–953, 2019. [13] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020. [14] W. Huang, T. Zhang, Y . Rong, and J. Huang. Adaptive sampling towards fast graph represen- tation learning. In Advances in Neural Information Processing Systems , pages 4558–4567, 2018. [15] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. [16] Z. Liu, C. Chen, X. Yang, J. Zhou, X. Li, and L. Song. Heterogeneous graph neural networks for malicious account detection. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management, pages 2077–2085. ACM, 2018. [17] Z. Liu, C. Chen, L. Li, J. Zhou, X. Li, L. Song, and Y . Qi. Geniepath: Graph neural networks with adaptive receptive paths. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 4424–4431, 2019. [18] F. Salehi, L. E. Celis, and P. Thiran. Stochastic optimization with bandit sampling. arXiv preprint arXiv:1708.02544, 2017. [19] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. Van Den Berg, I. Titov, and M. Welling. Modeling relational data with graph convolutional networks. In European Semantic Web Conference, pages 593–607. Springer, 2018. [20] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad. Collective classiﬁca- tion in network data. AI magazine, 29(3):93–93, 2008. [21] T. Uchiya, A. Nakamura, and M. Kudo. Algorithms for adversarial bandit problems with multiple plays. In International Conference on Algorithmic Learning Theory, pages 375–389. Springer, 2010. [22] P. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y . Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017. [23] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S. Yu. A comprehensive survey on graph neural networks. arXiv preprint arXiv:1901.00596, 2019. 10[24] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . Prasanna. Graphsaint: Graph sampling based inductive learning method. arXiv preprint arXiv:1907.04931, 2019. [25] D. Zou, Z. Hu, Y . Wang, S. Jiang, Y . Sun, and Q. Gu. Layer-dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, pages 11247–11256, 2019. 11A Algorithms Algorithm 2EXP3(qt i,wt i,rt i,St i). Require: η= 0.4, sample size k, neighbor size n= |Ni|, δ= √ (1 −η)η4k5 ln(n/k)/(Tn4). 1: Set ˆrij(t) =rij(t)/qij(t) if j ∈St i else 0 wij(t+ 1) =wij(t) exp(δˆrij(t)/n) 2: Set qij(t+ 1)←(1 −η) wij(t+1)∑ j∈Ni wij(t+1) + η n, for j ∈Ni Algorithm 3EXP3.M(qt i,wt i,rt i,St i) Require: η = 0.4, sample size k, neighbor size n = |Ni|, δ = √ (1 −η)η4k5 ln(n/k)/(Tn4), Ut i = ∅. 1: For j ∈Ni set ˆrij(t) = {rij(t)/qij(t) if j ∈St i 0 otherwise wij(t+ 1) = {wij(t) exp(δˆrij(t)/n) if j /∈Ut i wij(t) otherwise 2: if maxj∈Ni wij(t+ 1)≥( 1 k −η n) ∑ j∈Ni wij(t+ 1)/(1 −η) then 3: Decide at so as to satisfy at∑ wij(t+1)≥at at + ∑ wij(t+1)<at wij(t+ 1)= (1 k −η n)/(1 −η) 4: Set Ut+1 i = {j : wij(t+ 1)≥at} 5: else 6: Set Ut+1 i = ∅ 7: end if 8: Set w′ ij(t+ 1) = {wij(t+ 1) if j ∈Ni\\Ut+1 i at if j ∈Ut i 9: Set qij(t+ 1) =k ( (1 −η) w′ ij(t+1)∑ j∈Ni w′ ij(t+1) + η n ) for j ∈Ni Algorithm 4DepRound(k,(q1,q2,...,q K)) 1: Input: Sample size k(k<K ), sample distribution (q1,q2,...,q K) with ∑K i=1 qi = k 2: Output: Subset of [K] with kelements 3: while there is an iwith 0 <qi <1 do 4: Choose distinct iand jwith 0 <qi <1 and 0 <qj <1 5: Set β = min{1 −qi,qj}and γ = min{qi,1 −qj} 6: Update qi and qj as (qi,qj) = { (qi + β,qj −β) with probability γ β+γ (qi −γ,qj + γ) with probability β β+γ 7: end while 8: return {i: qi = 1,1 ≤i≤K} 12B Proofs Proposition 1. ˆµi = ∑ js∈Si αijs qijs hjs is the unbiased estimator of µi = ∑ j∈Ni αijhj given that Si is sampled from qi using the DepRound sampler Qi, where Si is the selected k-subset neighbors of vertex i. Proof. Let us denote Qi,Si as the probability of vertex vi choosing any k-element subset Si ⊂Ni from the K-element set Ni using DepRound sampler Qi. This sampler follows the alternative sampling distribution qi = (qij1 ,...,q ijK ) where qijs denotes the alternative probability of sampling neighbor vjs. This sampler is guaranteed to satisfy ∑ Si:j∈Si Qi,Si = qij, i.e. the sum over the probabilities of all subsets Si that contains element jequals the probability qij. E[ˆµi] =E  ∑ js∈Si αijs qijs hjs   (12) = ∑ Si⊂Ni Qi,Si ∑ js∈Si αijs qijs hjs (13) = ∑ j∈Ni ∑ Si:j∈Si Qi,Si αij qij hj (14) = ∑ j∈Ni αij qij hj ∑ Si:j∈Si Qi,Si (15) = ∑ j∈Ni αij qij hjqij (16) = ∑ j∈Ni αijhj (17) Proposition 2. The effective variance can be approximated by Ve(Qi) ≤∑ js∈Ni αijs qijs ∥hjs∥2. Proof. The variance is V(Qi) =E    ∑ js∈Si αijs qijs hjs − ∑ j∈Ni αijhj  2  = ∑ Si⊂Ni Qi,Si  ∑ js∈Si αijs qijs hjs  2 −  ∑ j∈Ni αijhj  2 . Therefore the effective variance has following upper bound: Ve(Qi) = ∑ Si⊂Ni Qi,Si  ∑ js∈Si αijs qijs hjs  2 ≤ ∑ Si⊂Ni Qi,Si ∑ js∈Si αijs  hjs qijs  2 (Jensen′sInequality ) = ∑ js∈Ni ∑ Si:js∈Si Qi,Siαijs  hjs qijs  2 = ∑ js∈Ni αijs q2 ijs ∥hjs∥2 ∑ Si:js∈Si Qi,Si = ∑ js∈Ni αijs qijs ∥hjs∥2 13Proposition 3. The negative derivative of the approximated effective variance ∑ js∈Ni αijs qijs ∥hjs∥2 w.r.tQi,Si, i.e. the reward of vi choosing Si at t, is ri,Si(t) =∑ js∈Si αijs qijs(t)2 ∥hjs(t)∥2. Proof. Deﬁne the upper bound as ˆVe(Qi) =∑ js∈Ni αijs qijs ∥hjs∥2, then its derivative is ∇Qi,Si ˆVe(Qi) =∇Qi,Si ∑ js∈Ni αijs qijs ∥hjs∥2 = ∇Qi,Si ∑ js∈Ni αijs∑ S′ i:js∈S′ i Qi,S′ i ∥hjs∥2 = ∇Qi,Si ∑ js∈Si αijs∑ S′ i:js∈S′ i Qi,S′ i ∥hjs∥2 = − ∑ js∈Si αjs q2 ijs ∥hjs∥2 (chainrule) Before we give the proof of Theorem 1, we ﬁrst prove the following Lemma 1 that will be used later. Lemma 1. For any real value constantη≤1 and any valid distributions Qt i and Q⋆ i we have (1 −2η)Vt e(Qt i) −(1 −η)Vt e(Q⋆ i) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩+ η⟨Q⋆ i,∇Qt i Vt e(Qt i)⟩ (18) Proof. The function Vt e(Q) is convex with respect to Q, hence for any Qt i and Q⋆ i we have Vt e(Qt i) −Vt e(Q⋆ i) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. (19) Multiplying both sides of this inequality by 1 −η, we have (1 −η)Vt e(Qt i) −(1 −η)Vt e(Q⋆ i) (20) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩−η⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. (21) In the following, we prove this Lemma in our two bandit settings: adversary MAB setting and adversary MAB with multiple plays setting. In adversary MAB setting, we have ⟨Qt i,∇Qt i Vt e(Qt i)⟩= − ∑ j∈Ni qij(t) α2 ij k·qij(t)2 ∥hj(t)∥2 (22) = −Vt e(Qt i) (23) In adversary MAB with multiple plays setting , we use the approximated effective variance∑ js∈Ni αijs qijs ∥hjs∥2 derived in Proposition 2. For notational simplicity, we denote the approxi- mated effective variance as Ve in the following. We have ⟨Qt i,∇Qt i Vt e(Qt i)⟩= − ∑ Si⊂Ni Qt i,Si ∑ js∈Si αijs qijs(t)2 ∥hjs∥2 (24) = − ∑ js∈Ni αijs qijs(t)2 ∥hjs∥2 ∑ Si:js∈Si Qt i,Si (25) = − ∑ js∈Ni αijs qijs(t)∥hjs∥2 (26) = −Vt e(Qt i). (27) The equation (24) holds because of Proposition 3. 14At last, we conclude the proof (1 −η)Vt e(Qt i) −(1 −η)Vt e(Q⋆ i) (28) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩−η⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩ (29) = ⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩+ η⟨Q⋆ i,∇Qt i Vt e(Qt i)⟩+ ηVt e(Qt i). (30) Theorem 1. Using Algorithm 1 with η= 0.4 and δ = √ (1 −η)η4k5 ln(n/k)/(Tn4) to minimize effective variance with respect to {Qt i}1≤t≤T, we have T∑ t=1 Vt e(Qt i) ≤3 T∑ t=1 Vt e(Q⋆ i) + 10 √ Tn4 ln(n/k) k3 (31) where T ≥ln(n/k)n2(1 −η)/(kη2) and n= |Ni|. Proof. First we explain why condition T ≥ln(n/k)n2(1 −η)/(kη2) ensures that δˆrij(t) ≤1, δˆrij(t) = √ (1 −η)η4k5 ln(n/k) Tn4 ·αij(t) q3 ij(t) ∥hj(t)∥2 (32) ≤ √ (1 −η)η4k5 ln(n/k) Tn4 · n3 k3η3 (33) ≤1 (34) Assuming ∥hj(t)∥≤ 1, inequality (33) holds because αij(t) ≤1 and qij(t) ≥kη/n. Then replace T by the condition, we get δˆrij(t) ≤1. Let Wi(t), W′ i(t) denote ∑ j∈Ni wij(t), ∑ j∈Ni w′ ij(t) respectively. Then for any t= 1,2,...,T , Wi(t+ 1) Wi(t) = ∑ j∈Ni\\Ut i wij(t+ 1) Wi(t) + ∑ j∈Ut i wij(t+ 1) Wi(t) (35) = ∑ j∈Ni\\Ut i wij(t) Wi(t) ·exp(δˆrij(t)) + ∑ j∈Ut i wij(t) Wi(t) (36) ≤ ∑ j∈Ni\\Ut i wij(t) Wi(t) [ 1 +δˆrij(t) + (δˆrij(t))2] + ∑ j∈Ut i wij(t) Wi(t) (37) = 1 +W′ i (t) Wi(t) ∑ j∈Ni\\Ut i wij(t) W′ i (t) [ δˆrij(t) + (δˆrij(t))2] (38) = 1 +W′ i (t) Wi(t) ∑ j∈Ni\\Ut i qij(t)/k−η/n 1 −η [ δˆrij(t) + (δˆrij(t))2] (39) ≤1 + δ k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (40) Inequality (37) uses ea ≤1 +a+ a2 for a≤1. Equality (39) holds because of update equation of qij(t) deﬁned in EXP3.M. Inequality (40) holds because W′ i (t) Wi(t) ≤1. Since 1 +x≤ex for x≥0, we have ln Wi(t+ 1) Wi(t) ≤ δ k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (41) 15If we sum, for 1 ≤t≤T, we get the following telescopic sum ln Wi(T + 1) Wi(1) = T∑ t=1 ln Wi(t+ 1) Wi(t) (42) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (43) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)ˆr2 ij(t) (44) On the other hand, for any subset Scontaining k elements, ln Wi(T + 1) Wi(1) ≥ln ∑ j∈S wij(T + 1) Wi(1) (45) ≥ ∑ j∈S ln wij(T + 1) k −ln n k (46) ≥δ k ∑ j∈S ∑ t:j /∈Ut i ˆrij(t) −ln n k (47) The inequality (46) uses the fact that ∑ j∈S wij(T + 1)≥k( ∏ j∈S wij(T + 1))1/k The equation (47) uses the fact that wij(T + 1) = exp(δ ∑ t:j /∈Ut i ˆrij(t)) From (44) and (47), we get δ k ∑ j∈S ∑ t:j /∈Ut i ˆrij(t) −ln n k ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (48) And we have the following inequality δ k ∑ j∈S ∑ t:j∈Ut i rij(t) = δ k ∑ j∈S ∑ t:j∈Ut i qij(t)ˆrij(t) (49) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ut i qij(t)ˆrij(t) (50) The equality (49) holds beacuse rij(t) =qijˆrij(t) when j ∈St i and Ut i ⊆St i bacause qt ij = 1for all j ∈Ut i. Then add inequality (50) in (48) we have δ k ∑ j∈S ∑ t:j∈Ut i rij(t) +δ k ∑ j∈S ∑ t:j /∈Ut i ˆrij(t) −ln n k (51) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)ˆr2 ij(t) (52) Given qij(t) we have E[ˆr2 ij(t)] =r2 ij(t)/qij(t), hence, taking expectation of (51) yields that δ k T∑ t=1 ∑ j∈S rij(t) −ln n k ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)rij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni r2 ij(t) (53) 16By multiplying (53) by Q⋆ i,S and summing over S, we get δ k T∑ t=1 ∑ S⊂Ni Q⋆ i,S ∑ j∈S rij(t) −ln n k ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)rij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni r2 ij(t) (54) As ∑ j∈Ni qij(t)rij(t) = ∑ j∈Ni ∑ Si:j∈Si Qt i,Sirij(t) (55) = ∑ Si⊂Ni Qt i,Si ∑ j∈Si rij(t) (56) = − ∑ Si⊂Ni Qt i,Si∇Qt i,Si Vt e(Qt i,Si) (57) = −⟨Qt i,∇Qt i Vt e(Qt i)⟩ (58) By plugging (58) in (54) and rearranging it, we ﬁnd T∑ t=1 ⟨Qt i −Q⋆ i ,∇Qt i Vt e(Qt i)⟩+ η T∑ t=1 ⟨Q⋆ i ,∇Qt i Vt e(Qt i)⟩ (59) ≤δ T∑ t=1 ∑ j∈Ni r2 ij(t) +(1 −η)k δ ln(n/k) Using Lemma 1, we have (1 −2η) T∑ t=1 Vt e(Qt i) −(1 −η) T∑ t=1 Vt e(Q⋆ i ) ≤δ T∑ t=1 ∑ j∈Ni r2 ij(t) +(1 −η)k δ ln(n/k) (60) Finally, we know that ∑ j∈Ni r2 ij(t) = ∑ j∈Ni αij(t)2 qij(t)4 (61) ≤ ∑ j∈Ni αij(t) n4 k4η4 (becauseqij(t) ≥kη/n) (62) = n4 k4η4 (63) By setting η= 0.4 and δ= √ (1 −η)η4k5 ln(n/k)/(Tn4), we get the upper bound. C Experiments C.1 Convergences We show the convergences on validation in terms of timing (seconds) in Figure 3 and Figure 4. Basically, our algorithms converge to much better results in nearly same duration compared with other “layer sampling” approaches. Note that we cannot complete the training of AS-GAT on Reddit because of memory issues. C.2 Discussions on Timings between Layer Sampling and Graph Sampling Paradigms Note that the comparisons of timing between “graph sampling” and “layer sampling” paradigms have been studied recently in [ 7, 24]. As a result, we do not compare the timing with “graph sampling” approaches. Under certain conditions, the graph sampling approaches should be faster than layer sampling approaches. That is, graph sampling approaches are designed for graph data that all vertices have labels. Under such condition, the ﬂoating point operations analyzed in [7] are 170.25 0.50 0.75 1.00 Time 0.2 0.4 0.6 0.8Micro F1 on validation Cora GCN-BS GraphSAGE FastGCN AS-GCN LADIES 5 10 15 20 Time 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GCN-BS GraphSAGE FastGCN AS-GCN LADIES 0 1000 2000 3000 Time 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GCN-BS GraphSAGE FastGCN AS-GCN LADIES 500 1000 1500 Time 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GCN-BS GraphSAGE FastGCN AS-GCN LADIES 100 200 300 400 Time 0.40 0.45 0.50Micro F1 on validation Flickr GCN-BS GraphSAGE FastGCN AS-GCN LADIES Figure 3: The convergence in timing (seconds) on GCNs. 0.5 1.0 1.5 2.0 Time 0.2 0.4 0.6 0.8Micro F1 on validation Cora GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 0 10 20 30 Time 0.6 0.7 0.8 0.9Micro F1 on validation Pubmed GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 0 1000 2000 Time 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 100 200 300 Time 0.44 0.46 0.48 0.50Micro F1 on validation Flickr GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT Figure 4: The convergence in timing (seconds) on attentive GNNs. maximally utilized compared with the “layer sampling” paradigm. However, in practice, there are large amount of graph data with labels only on some types of vertices, such as the graphs in [ 16]. “Graph sampling” approaches are not applicable to cases where only partial vertices have labels. To summarize, the “layer sampling” approaches are more ﬂexible and general compared with “graph sampling” approaches in many cases. C.3 Results on OGB We report our results on OGB protein dataset [13]. We set the learning rate as 1e-3, batch size as 256, the dimension of hidden embeddings as 64, sample size as 10 and epochs as 200. We save the model based on the best results on validation and report results on testing data. We run the experiment 10 times with different random seeds to compute the average and standard deviation of the results. Our result of GP-BS on protein dataset performs the best4 until we submitted this paper. Please ﬁnd our implementations at https://github.com/xavierzw/ogb-geniepath-bs. Dateset Mean Std #experiments ogbn-proteins 0.78253 0.00352 10 4Please refer to https://ogb.stanford.edu/docs/leader_nodeprop/. 18",
      "meta_data": {
        "arxiv_id": "2006.05806v2",
        "authors": [
          "Ziqi Liu",
          "Zhengwei Wu",
          "Zhiqiang Zhang",
          "Jun Zhou",
          "Shuang Yang",
          "Le Song",
          "Yuan Qi"
        ],
        "published_date": "2020-06-10T12:48:37Z",
        "pdf_url": "https://arxiv.org/pdf/2006.05806v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the computational intractability of optimal variance-reduced samplers for training Graph Neural Networks (GNNs), particularly those with learned weights (e.g., Graph Attention Networks), where neighbor embeddings and learned weights change dynamically and are only partially observed. It proposes a novel formulation of sampler optimization as an adversary bandit problem. Key contributions include theoretically showing that the proposed algorithms asymptotically approach the optimal variance within a factor of 3, and empirically demonstrating superior performance in terms of convergence, faster training rates, and lower sampling variances compared to state-of-the-art approaches on various GNN architectures and datasets.",
        "methodology": "The core methodology involves formulating the optimization of sampling variance as an adversary bandit problem. The rewards are defined as the negative derivatives of the effective sampling variance. Two bandit algorithms are proposed: GNN-BS, which operates under the Multi-Armed Bandit (MAB) setting, sampling individual neighbors `k` times and updating the sampling distribution `q_t_i` using the EXP3 algorithm; and GNN-BS.M, which uses the MAB with multiple plays setting, employing the DepRound `k`-combination sampler to select a `k`-element subset of neighbors once, and updating `q_t_i` with EXP3.M. For attentive GNNs, adjusted feedback attention values are introduced to approximate the true attention values based on sampled unnormalized attentions.",
        "experimental_setup": "Extensive experiments were conducted on five public benchmark datasets (Cora, Pubmed, PPI, Reddit, Flickr) and the OGB protein dataset. The proposed methods (GCN-BS, GAT-BS, GP-BS, and their .M variants) were evaluated on GCN, Graph Attention Networks (GAT), and GeniePath (GP) architectures, typically with two layers. Comparison algorithms included GraphSAGE, FastGCN, LADIES, AS-GCN, S-GCN, ClusterGCN, and GraphSAINT for GCNs, and AS-GAT and GraphSAINT-GAT for attentive GNNs. Hyperparameters (learning rate, L2 regularization, dropout rate, sample size `k`) were tuned via grid search. Performance was measured primarily by Micro F1 scores, and convergence rates (epochs and time) and sample variances were also analyzed. Results were reported as mean and standard deviation over multiple runs (3 for benchmarks, 10 for OGB).",
        "limitations": "The GNN-BS algorithm, which repeatedly samples `k` times in the Multi-Armed Bandit (MAB) setting, is noted to be \"strictly speaking not rigorous\" within the MAB framework. For GNN-BS.M, the effective variance used is an approximation derived via Jensen's inequality (Proposition 2) rather than the exact value. The theoretical regret analysis (Theorem 1) includes an assumption that `||h_j(t)|| <= 1`. The current derivation of bandit samplers primarily follows node-wise sampling approaches, with its extension to layer-wise sampling left as future work. A practical simplification noted is maintaining a single `q_i` and updating it only from rewards of the first layer, which works well in practice but is not theoretically optimal across all layers.",
        "future_research_directions": "The paper suggests two primary directions for future research. Firstly, extending the proposed bandit samplers to work with layer-wise sampling approaches, as the current derivation focuses on node-wise samplers. Secondly, exploring other bandit problem settings beyond the adversary bandit setting adopted in this work."
      }
    },
    {
      "title": "Are GATs Out of Balance?",
      "abstract": "While the expressive power and computational capabilities of graph neural\nnetworks (GNNs) have been theoretically studied, their optimization and\nlearning dynamics, in general, remain largely unexplored. Our study undertakes\nthe Graph Attention Network (GAT), a popular GNN architecture in which a node's\nneighborhood aggregation is weighted by parameterized attention coefficients.\nWe derive a conservation law of GAT gradient flow dynamics, which explains why\na high portion of parameters in GATs with standard initialization struggle to\nchange during training. This effect is amplified in deeper GATs, which perform\nsignificantly worse than their shallow counterparts. To alleviate this problem,\nwe devise an initialization scheme that balances the GAT network. Our approach\ni) allows more effective propagation of gradients and in turn enables\ntrainability of deeper networks, and ii) attains a considerable speedup in\ntraining and convergence time in comparison to the standard initialization. Our\nmain theorem serves as a stepping stone to studying the learning dynamics of\npositive homogeneous models with attention mechanisms.",
      "full_text": "Are GATs Out of Balance? Nimrah Mustafa∗ nimrah.mustafa@cispa.de Aleksandar Bojchevski† a.bojchevski@uni-koeln.de Rebekka Burkholz∗ burkholz@cispa.de ∗CISPA Helmholtz Center for Information Security, 66123 Saarbrücken, Germany †University of Cologne, 50923 Köln, Germany Abstract While the expressive power and computational capabilities of graph neural net- works (GNNs) have been theoretically studied, their optimization and learning dynamics, in general, remain largely unexplored. Our study undertakes the Graph Attention Network (GAT), a popular GNN architecture in which a node’s neighbor- hood aggregation is weighted by parameterized attention coefficients. We derive a conservation law of GAT gradient flow dynamics, which explains why a high portion of parameters in GATs with standard initialization struggle to change dur- ing training. This effect is amplified in deeper GATs, which perform significantly worse than their shallow counterparts. To alleviate this problem, we devise an initialization scheme that balances the GAT network. Our approach i) allows more effective propagation of gradients and in turn enables trainability of deeper networks, and ii) attains a considerable speedup in training and convergence time in comparison to the standard initialization. Our main theorem serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms. 1 Introduction A rapidly growing class of model architectures for graph representation learning are Graph Neural Networks (GNNs) [20] which have achieved strong empirical performance across various applications such as social network analysis [8], drug discovery [27], recommendation systems [61], and traffic forecasting [50]. This has driven research focused on constructing and assessing specific architectural designs [59] tailored to various tasks. On the theoretical front, mostly the expressive power [51, 26] and computational capabilities [45] of GNNs have been studied. However, there is limited understanding of the underlying learning mechanics. We undertake a classical variant of GNNs, the Graph Attention Network (GAT) [48, 9]. It overcomes the limitation of standard neighborhood representation averaging in GCNs [29, 22] by employing a self-attention mechanism [47] on nodes. Attention performs a weighted aggregation over a node’s neighbors to learn their relative importance. This increases the expressiveness of the neural network. GAT is a popular model that continues to serve as strong baseline. However, a major limitation of message-passing GNNs in general, and therefore GATs as well, is severe performance degradation when the depth of the network is even slightly increased. SOTA results are reportedly achieved by models of 2 or 3 layers. This problem has largely been attributed to over-smoothing [35], a phenomenon in which node representations become indistinguishable when multiple layers are stacked. To relieve GNNs of over-smoothing, practical techniques inspired by classical deep neural networks tweak the training process, e.g. by normalization [11, 64, 65, 66] or regularization [38, 42, 55, 67], and impose architectural changes such as skip connections (dense 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2310.07235v2  [cs.LG]  25 Oct 2023and residual) [34, 53] or offer other engineering solutions [ 33] or their combinations [ 12]. Other approaches to overcome over-smoothing include learning CNN-like spatial operators from random paths in the graph instead of the point-wise graph Laplacian operator [ 17] and learning adaptive receptive fields (different ‘effective’ neighborhoods for different nodes) [63, 36, 52]. Another problem associated with loss of performance in deeper networks is over-squashing [1], but we do not discuss it since non-attentive models are more susceptible to it, which is not our focus. Alternative causes for the challenged trainability of deeper GNNs beyond over-smoothing is hampered signal propagation during training (i.e. backpropagation) [25, 62, 13]. To alleviate this problem for GCNs, a dynamic addition of skip connections to vanilla-GCNs, which is guided by gradient flow, and a topology-aware isometric initialization are proposed in [ 25]. A combination of orthogonal initialization and regularization facilitates gradient flow in [21]. To the best of our knowledge, these are the only works that discuss the trainability issue of deep GNNs from the perspective of signal propagation, which studies a randomly initialized network at initialization. Here, we offer an insight into a mechanism that allows these approaches to improve the trainability of deeper networks that is related to the entire gradient flow dynamics. We focus in particular on GAT architectures and the specific challenges that are introduced by attention. Our work translates the concept of neuronwise balancedness from traditional deep neural networks to GNNs, where a deeper understanding of gradient dynamics has been developed and norm balanced- ness is a critical assumption that induces successful training conditions. One reason is that the degree of initial balancedness is conserved throughout the training dynamics as described by gradient flow (i.e. a model of gradient descent with infinitesimally small learning rate or step size). Concretely, for fully connected feed-forward networks and deep convolutional neural networks with continuous homogenous activation functions such as ReLUs, the difference between the squared l2-norms of incoming and outgoing parameters to a neuron stays approximately constant during training [ 15, 32]. Realistic optimization elements such as finite learning rates, batch stochasticity, momentum, and weight decay break the symmetries induced by these laws. Yet, gradient flow equations can be extended to take these practicalities into account [31]. Inspired by these advances, we derive a conservation law for GATs with positive homogeneous activation functions such as ReLUs. As GATs are a generalization of GCNs, most aspects of our insights can also be transferred to other architectures. Our consideration of the attention mechanism would also be novel in the context of classic feed-forward architectures and could have intriguing implications for transformers that are of independent interest. In this work, however, we focus on GATs and derive a relationship between model parameters and their gradients, which induces a conservation law under gradient flow. Based on this insight, we identify a reason for the lack of trainability in deeper GATs and GCNs, which can be alleviated with a balanced parameter initialization. Experiments on multiple benchmark datasets demonstrate that our proposal is effective in mitigating the highlighted trainability issues, as it leads to considerable training speed-ups and enables significant parameter changes across all layers. This establishes a causal relationship between trainability and parameter balancedness also empirically, as our theory predicts. The main theorem presented in this work serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms such as transformers [47] and in particular, vision transformers which require depth more than NLP transformers[49]. Our contributions are as follows. • We derive a conservation law of gradient flow dynamics for GATs, including its variations such as shared feature weights and multiple attention heads. • This law offers an explanation for the lack of trainability of deeper GATs with standard initialization. • To demonstrate empirically that our theory has established a causal link between balancedness and trainability, we propose a balanced initialization scheme that improves the trainability of deeper networks and attains considerable speedup in training and convergence time, as our theory predicts. 2 Theory: Conservation laws in GATs Preliminaries For a graph G = (V, E) with node set V and edge set E ⊆ V × V , where the neighborhood of a node v is given as N(v) = {u|(u, v) ∈ E}, a GNN layer computes each node’s representation by aggregating over its neighbors’ representation. In GATs, this aggregation is weighted by parameterized attention coefficients αuv, which indicate the importance of node u for v. 2Table 1: Attributes of init. schemes Init. Feat. Attn. Bal.? Xav Xavier Xavier No XavZ Xavier Zero No BalX Xavier Zero Yes BalO LLortho Zero Yes Figure 1: Params. to balance for neuron i ∈ [nl] Given input representations hl−1 v for all nodes v ∈ V , a GAT 1 layer l transforms those to: hl v = ϕ( X u∈N(v) αl uv · Wl shl−1 u ), where (1) αl uv = exp((al)⊤ · LeakyReLU(Wl shl−1 u + Wl t hl−1 v ))P u′∈N(v) exp((al)⊤ · LeakyReLU(Wlshl−1 u′ + Wl t hl−1v ))) . (2) We consider ϕ to be a positively homogeneous activation functions (i.e ϕ(x) =xϕ′(x) and conse- quently, ϕ(ax) =aϕ(x) for positive scalars a), such as a ReLU ϕ(x) = max{x, 0} or LeakyReLU ϕ(x) = max{x, 0} + −α max{−x, 0}. The feature transformation weights Ws and Wt for source and target nodes, respectively, may also be shared such that W = Ws = Wt. Definition 2.1. Given training data {(xm, ym)}M m=1 ⊂ Rd × Rp for M ≤ V , let f : Rd → Rp be the function represented by a network constructed by stacking L GAT layers as defined in Eq. (1) and (2) with W = Ws = Wt and h0 m = g(xm). Each layer l ∈ [L] of size nl has associated parameters: a feature weight matrix Wl ∈ Rnl×nl−1 and an attention vector al ∈ Rnl, where n0 = d and nL = p. Given a differentiable loss function ℓ : Rd × Rp → R, the loss L = 1/M PM i=1 ℓ(f(xm), ym) is used to update model parameters w ∈ {Wl, al}L l=1 with learning rate γ by gradient descent, i.e., wt+1 = wt − γ∇wL, where ∇wL = [∂L/∂w1, . . . ,∂L/∂w|w|] and w0 is set by the initialization scheme. For an infinitesimal γ → 0, the dynamics of gradient descent behave similarly to gradient flow defined by dw/dt = −∇wL, where t is the continuous time index. We useW[i, :], W[:, i], and a[i] to denote the ith row of W, column of W, and entry ofa, respectively. Note that Wl[i, :] is the vector of weights incoming to neuron i ∈ [nl] and Wl+1[:, i] is the vector of weights outgoing from the same neuron. For the purpose of discussion, let i ∈ [dl], j∈ [dl−1], and k ∈ [dl+1], as depicted in Fig. 1. ⟨·, ·⟩ denotes the scalar product. Conservation Laws We focus the following exposition on weight-sharing GATs, as this variant has the practical benefit that it requires fewer parameters. Yet, similar laws also hold for the non-weight- sharing case, as we state and discuss in the supplement. For brevity, we also defer the discussion of laws for the multi-headed attention mechanism to the supplement. Theorem 2.2 (Structure of gradients). Given the feature weight and attention parameters Wl and al of a layer l in a GAT network as described in Def. 2.1, the structure of the gradients for layer l ∈ [L − 1] in the network is conserved according to the following law: ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. (3) Corollary 2.3 (Norm conservation of weights incoming and outgoing at every neuron). Given the gradient structure of Theorem 2.2 and positive homogeneity of the activation ϕ in Eq. (1), gradient flow in the network adheres to the following invariance for l ∈ [L − 1]: d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 = d dt \u0010\r\rWl+1[: i] \r\r2\u0011 , (4) 1This definition of GAT is termed GATv2 in [9]. Hereafter, we refer to GATv2 as GAT. 3(a) δ with GD  (b) δ with Adam  (c) c with Xavier  (d) c with BalO Figure 2: Visualizing conservation laws: (a),(b) show δ ≈ 0 (can not empirically be exactly zero due to finite γ = 0.1) from Eq. (7) for hidden layers when L = 3. (c),(d) show the value of c from Eq. (5), which is determined by the initialization and should be 0 when the network is balanced, for L = 10 trained with GD. With Xav., the network is unbalanced and hence c ̸= 0for all neurons. With BalO, c = 0exactly at initialization for all neurons by design but during training, it varies slightly andc ≈ 0 as the network becomes slightly unbalanced due to the finite γ = 0.1. As both δ and c approximately meet their required value, we conclude that the derived law holds for practical purposes as well. It then directly follows by integration over time that the l2-norms of feature and attention weights incoming to and outgoing from a neuron are preserved such that \r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2 − \r\rWl+1[: i] \r\r2 = c, (5) where c = d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 at initialization (i.e. t = 0). We denote the ‘degree of balancedness’ by c and call an initialization balanced if c = 0. Corollary 2.4 (Norm conservation across layers). Given Eq. (4), the invariance of gradient flow at the layer level for l ∈ [L − 1] by summing over i ∈ [nl] is: d dt \u0010\r\rWl\r\r2 F − \r\ral\r\r2\u0011 = d dt \u0010\r\rWl+1\r\r2 F \u0011 . (6) Remark 2.5. Similar conservation laws also hold for the original less expressive GAT version [48] as well as for the vanilla GCN [29] yielded by fixing al = 0. Insights We first verify our theory and then discuss its implications in the context of four different initializations for GATs, which are summarized in Table 1. Xavier (Xav.) initialization [19] is the standard default for GATs. We describe the Looks-Linear-Orthogonal (LLortho) initialization later. In order to empirically validate Theorem 2.2, we rewrite Equation 3 to define δ as: δ = ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al, ∇alL⟩ − ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ = 0. (7) We observe how the value of δ varies during training in Fig. 2 for both GD and Adam optimizers with γ = 0.1. Although the theory assumes infinitesimal learning rates due to gradient flow, it still holds sufficiently (δ ≈ 0) in practice for normally used learning rates as we validate in Fig. 2. Furthermore, although the theory assumes vanilla gradient descent optimization, it still holds for the Adam optimizer (Fig. 2b). This is so because the derived law on the level of scalar products between gradients and parameters (see Theorem 2.2) still holds, as it states a relationship between gradients and parameters that is independent of the learning rate or precise gradient updates. Having verified Theorem 2.2, we use it to deduce an explanation for a major trainability issue of GATs with Xavier initialization that is amplified with increased network depth. The main reason is that the last layer has a comparatively small average weight norm, as E \r\rWL[:, i] \r\r2 = 2nL/(nL + nL−1) << 1, where the number of outputs is smaller than the layer width nL << nL−1. In contrast, E \r\rWL−1[i, :] \r\r2 = 2nL−1/(nL−1 + nL−2) = 1 and EaL−1[i]2 = 2/(1 +nL−1). In consequence, the right-hand side of our derived conservation law nL−2X j=1 (WL−1 ij )2 ∇WL−1 ij L WL−1 ij − (aL−1 i )2 ∇aL−1 i L aL−1 i = nLX k=1 (WL ki)2 ∇WL ki L WL ki (8) 4(a) Frac. of sig. params with relative change > 0.05  (b) Rel. grad. norms of feat. wghts. & training curve Figure 3: For L = 5, a high fraction of parameters change, and gradients propagate to earlier layers for both the unbalanced and balanced initialization Xav. and BalO initialization achieve 75.5% and 79.9% test accuracy, respectively. Note that the balanced initialization in (b) is able to train faster. (a) Frac. of sig. params with relative change > 0.5  (b) Rel. grad. norms of feat. wghts. & training curve Figure 4: For L = 10, unbalanced initialization is unable to train the model as relative gradients are extremely small, and the parameters (and loss) do not change. However, the balanced initialization is able to propagate gradients, change parameters, and thus attain training loss → 0. Xav. and Bal O achieve 39.3% and 80.2% test accuracy, respectively. Note that the balanced initialization in (b) is also able to train faster than the 5 layer network with Xavier initialization in Fig. 3b. is relatively small, as PnL k=1(WL ki)2 << 1, on average, while the weights on the left-hand side are orders of magnitude larger. This implies that the relative gradients on the left-hand side are likely relatively small to meet the equality unless all the signs of change are set in such a way that they satisfy the equality, which is highly unlikely during training. Therefore, relative gradients in layer L −1 and accordingly also the previous layers of the same dimension can only change in minor ways. The amplification of this trainability issue with depth can also be explained by the recursive substitu- tion of Theorem 2.2 in Eq. (8) that results in a telescoping series yielding: n1X j=1 n0X m=1 W(1) jm 2 ∇W(1) jm L W(1) jm − L−1X l=1 nlX o=1 a(l) o 2 ∇a(l) o L a(l) o = nL−1X i=1 nLX k=1 W(L) ki 2 ∇W(L) ki L W(L) ki (9) Generally, 2n1 < n0 and thus E \r\rW1[j :] \r\r2 = 2n1/(n1 + n0) < 1. Since the weights in the first layer and the gradients propagated to the first layer are both small, gradients of attention parameters of the intermediate hidden layers must also be very small in order to satisfy Eq. (9). Evidently, the problem aggravates with depth where the same value must now be distributed over the parameters and gradients of more layers. Note that the norms of the first and last layers are usually not arbitrary but rather determined by the input and output scale of the network. Analogously, we can see see how a balanced initialization would mitigate the problem. Equal weight norms \r\rWL−1[i, :] \r\r2 and \r\rWL[:, i] \r\r2 in Eq. (8) (as the attention parameters are set to 0 during balancing, see Procedure 2.6) would allow larger relative gradients in layerL − 1, as compared to the imbalanced case, that can enhance gradient flow in the network to earlier layers. In other words, gradients on both sides of the equation have equal room to drive parameter change. 5Figure 5: Relative gradient norms of feature (left axis, solid) and of attention (right axis, stylized) parameters for l ∈ [1, 5, 10] and L = 10, sampled every 25 epochs. Test accuracy is at the top. Both attention and feature gradients at the first, middle, and last layer of the network with both balanced initializations are much larger than with unbalanced initialization (note axis scales). To visualize the trainability issue, we study the relative change |(w∗−w0)/w∗|. of trained network parameters (w∗) w.r.t. their initial value. In order to observe meaningful relative change, we only con- sider parameters with a significant contribution to the model output (w∗ ≥ 10−4). We also plot the rel- ative gradient norms of the feature weights across all layers to visualize their propagation. We display these values forL = 5and L = 10in Fig. 3 and 4, respectively, and observe a stark contrast, similar to the trends in relative gradient norms of selected layers of a 10 layer GAT in Fig. 5. In all experiments, GAT was trained with the specified initialization on Cora using GD with γ = 0.1 for 5000 epochs. Interestingly, the attention parameters change most in the first layer and increasingly more towards the last layer, as seen in Fig. 3a and 4a. This is consistently observed in both the 5 and 10 layer networks if the initialization is balanced, but with unbalanced initialization, the 10 layer network is unable to produce the same effect. As our theory defines a coarse-level conservation law, such fine-grained training dynamics cannot be completely explained by it. Since the attention parameters are set to 0 in the balanced initializations (see Procedure 2.6), as ablation, we also observe the effect of initializing attention and feature parameters with zero and Xavier, respectively. From Eq.(2), al = 0leads to αuv = 1/|N(v)|, implying that all neighbors u of the node v are equally important at initialization. Intuitively, this allows the network to learn the importance of neighbors without any initially introduced bias and thus avoids the ‘chicken and egg’ problem that arises in the initialization of attention over nodes[30]. Although this helps generalization in some cases (see Fig. 6), it alone does not help the trainability issue as seen in Fig. 4a. It may in fact worsen it (see Fig. 5), which is explained by Eq. 8. These observations underline the further need for parameter balancing. Procedure 2.6 (Balancing). Based on Eq. (5) from the norm preservation law 2.3, we note that in order to achieve balancedness, (i.e. set c = 0in Eq.(5)), the randomly initialized parameters Wl and al must satisfy the following equality for l ∈ [L]: \r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2 = \r\rWl+1[: i] \r\r2 This can be achieved by scaling the randomly initialized weights as follows: 1. Set al = 0 for l ∈ [L]. 2. Set W1[i, :] = W1[i,:] ∥W1[i,:]∥ √βi, for i ∈ [n1] where βi is a hyperparameter 3. Set Wl+1[:, i] = Wl+1[:,i] ∥Wl+1[:,i]∥ \r\rWl[i, :] \r\r for i ∈ [nl] and l ∈ [L − 1] In step 2, βi determines the initial squared norm of the incoming weights to neuron i in the first layer. It can be any constant thus also randomly sampled from a normal or uniform distribution. We explain why we set βi = 2for i ∈ [n1] in the context of an orthogonal initialization. Remark 2.7. This procedure balances the network starting from l = 1towards l = L. Yet, it could also be carried out in reverse order by first defining \r\rWL[:, i] \r\r2 = βi for i ∈ [nL−1]. Balanced Orthogonal Initialization The feature weights Wl for l ∈ [L] need to be initialized randomly before balancing, which can simply be done by using Xavier initialization. However, existing works have pointed out benefits of orthogonal initialization to achieve initial dynamical 6(a) Test accuracy (higher is better).  (b) Epochs to best model (lower is better). Figure 6: GAT trained on Cora using SGD isometry for DNNs and CNNs [37, 44, 23], as it enables training very deep architectures. In line with these findings, we initialize Wl, before balancing, to be an orthogonal matrix with a looks-linear (LL) mirrored block structure [46, 6], that achieves perfect dynamical isometry in perceptrons with ReLU activation functions. Due to the peculiarity of neighborhood aggregation, the same technique does not induce perfect dynamical isometry in GATs (or GNNs). Exploring how dynamical isometry can be achieved or approximated in general GNNs could be an interesting direction for future work. Nevertheless, using a (balanced) LL-orthogonal initialization enhances trainaibility, particularly of deeper models. We defer further discussion on the effects of orthogonal initialization, such as with identity matrices, to the appendix. We outline the initialization procedure as follows: Let ∥ and |= denote row-wise and column- wise matrix concatenation, respectively. Let us draw a random orthonormal submatrix Ul and define W1 = [U1 ∥ −U1] where U1 ∈ R n1 2 ×n0 , Wl = [[Ul ∥ −Ul] |= [−Ul ∥ Ul]] where Ul ∈ R nl 2 × nl−1 2 for l = {2, . . . , L− 1}, and WL = [UL |= −UL] where UL ∈ RnL× nl−1 2 . Since UL is an orthonormal matrix, \r\rWL[:, i] \r\r2 = 2by definition, and by recursive application of Eq. (2.3 (with al = 0), balancedness requires that \r\rW1[i :, ] \r\r2 = 2. Therefore, we set βi = 2for i ∈ n1. 3 Experiments The main purpose of our experiments is to verify the validity of our theoretical insights and deduce an explanation for a major trainability issue of GATs that is amplified with increased network depth. Based on our theory, we understand the striking observation in Figure 4a that the parameters of default Xavier initialized GATs struggle to change during training. We demonstrate the validity of our theory with the nature of our solution. As we balance the parameter initialization (see Procedure 2.6), we allow the gradient signal to pass through the network layers, which enables parameter changes during training in all GAT layers. In consequence, balanced initialization schemes achieve higher generalization performance and significant training speed ups in comparison with the default Xavier initialization. It also facilitates training deeper GAT models. To analyze this effect systematically, we study the different GAT initializations listed in Table 1 on generalization (in % accuracy) and training speedup (in epochs) using nine common benchmark datasets for semi-supervised node classification tasks. We defer dataset details to the supplement. We use the standard provided train/validation/test splits and have removed the isolated nodes from Citeseer. We use the Pytorch Geometric framework and run our experiments on either Nvidia T4 Tensor Core GPU with 15 GB RAM or Nvidia GeForce RTX 3060 Laptop GPU with 6 GB RAM. We allow each network to train, both with SGD and Adam, for 5000 epochs (unless it converges earlier, i.e. achieves training loss ≤ 10−4) and select the model state with the highest validation accuracy. For each experiment, the mean ±95% confidence interval over five runs is reported for accuracy and epochs to the best model. All reported results use ReLU activation, weight sharing and no biases, unless stated otherwise. No weight sharing leads to similar trends, which we therefore omit. SGD We first evaluate the performance of the different initialization schemes of GAT models using vanilla gradient descent. Since no results have been previously reported using SGD on these datasets, to the best of our knowledge, we find that setting a learning rate of 0.1, 0.05 and 0.005 for L = [2, 5], L = [10, 20], and L = 40, respectively, allows for reasonably stable training on Cora, Citeseer, and Pubmed. For the remaining datasets, we set the learning rate to 0.05, 0.01, 0.005 and 0.0005 for 7Table 2: Mean accuracy(%) ± 95% CI over five runs of GAT with width= 64trained using SGD. Init. L = 2 L = 5 L = 10 L = 20 L = 40 Citeseer Xav 71.82 ± 2.73 58 .40 ± 2.25 24 .70 ± 8.90 19 .23 ± 1.54 18 .72 ± 1.15 BalX 71.62 ± 0.80 68 .83 ± 1.62 64 .13 ± 1.57 54 .88 ± 7.95 42 .63 ± 17.47 BalO 72.02 ± 0.63 70 .63 ± 0.60 68 .83 ± 1.68 68 .70 ± 1.18 63 .40 ± 1.43 Pubmed Xav 77.26 ± 1.39 70 .68 ± 2.16 67 .32 ± 10.70 36 .52 ± 11.50 27 .20 ± 13.99 BalX 78.02 ± 0.73 75.66 ± 1.81 77.60 ± 1.56 76.44 ± 1.70 75 .74 ± 2.94 BalO 77.68 ± 0.45 76.62 ± 1.59 77.04 ± 2.14 78.20 ± 0.61 77 .80 ± 1.41 Actor Xav 27.32 ± 0.59 24 .60 ± 0.93 24 .08 ± 0.80 22 .29 ± 3.26 19 .46 ± 5.75 BalX 26.00 ± 0.59 23 .93 ± 1.42 24.21 ± 0.78 24 .74 ± 1.14 23.88 ± 0.97 BalO 26.59 ± 1.03 24 .61 ± 0.77 24.17 ± 0.62 24 .24 ± 1.05 23.93 ± 1.53 Chameleon Xav 52.81 ± 1.37 54.21 ± 1.05 30 .31 ± 5.96 22 .19 ± 2.04 22 .28 ± 3.15 BalX 51.18 ± 1.94 54.21 ± 0.82 52 .11 ± 3.72 51.89 ± 1.89 38 .64 ± 10.31 BalO 50.00 ± 3.07 53 .95 ± 1.81 51 .84 ± 3.21 52.72 ± 0.13 44 .30 ± 1.61 Cornell Xav 42.70 ± 2.51 41.08 ± 2.51 42.70 ± 1.34 25.41 ± 14.64 22 .70 ± 13.69 BalX 41.08 ± 6.84 35 .14 ± 11.82 41 .08 ± 2.51 40 .00 ± 4.93 37.84 ± 5.62 BalO 42.16 ± 1.64 43.24 ± 2.12 36.76 ± 5.02 35.68 ± 3.29 36.22 ± 3.42 Squirrel Xav 35.20 ± 0.44 40 .96 ± 0.92 21 .65 ± 1.52 20 .23 ± 1.69 19 .67 ± 0.29 BalX 35.95 ± 1.69 40.98 ± 0.87 38.98 ± 1.49 38.35 ± 1.07 25 .38 ± 4.62 BalO 35.83 ± 0.92 42.52 ± 1.19 38.85 ± 1.36 39.15 ± 0.44 26 .57 ± 1.83 Texas Xav 60.00 ± 1.34 60 .54 ± 3.42 58 .92 ± 1.34 49 .73 ± 20.97 17 .84 ± 26.98 BalX 60.54 ± 1.64 61 .62 ± 2.51 61 .62 ± 2.51 58 .92 ± 2.51 56.22 ± 1.34 BalO 60.00 ± 1.34 57 .30 ± 1.34 56 .76 ± 0.00 58 .38 ± 4.55 57.30 ± 3.91 Wisconsin Xav 51.37 ± 6.04 51.37 ± 8.90 51 .76 ± 3.64 43 .14 ± 25.07 31 .76 ± 31.50 BalX 49.80 ± 8.79 54.51 ± 4.19 47.84 ± 7.16 50.59 ± 10.49 41.18 ± 1.54 BalO 49.80 ± 4.24 55 .69 ± 3.64 51.76 ± 5.68 49.02 ± 4.36 48.24 ± 4.52 L = [2, 5], L= 10, L = 20, and L = 40, respectively. Note that we do not perform fine-tuning of the learning rate or other hyperparameters with respect to performance and use the same settings for all initializations to allow fair comparison. Figure 6 highlights that models initialized with balanced parameters, Bal O, consistently achieve the best accuracy and significantly speed up training, even on shallow models of 2 layers. In line with our theory, the default Xavier (Xav) initialization hampers model training (see also Figure 4a). Interestingly, the default Xavier (Xav) initialized deeper models tend to improve in performance with width but cannot compete with balanced initialization schemes. For example, the accuracy achieved by Xav for L = 10increases from 24.7% to 68.7% when the width is increased from 64 to 512. We hypothesize that the reason why width aids generalization performance is that a higher number of (almost random) features supports better models. This would also be in line with studies of overparameterization in vanilla feed-forward architectures, where higher width also aids random feature models [2, 4, 57, 43]. The observation that width overparameterized models enable training deeper models may be of independent interest in the context of how overparameterization in GNNs may be helpful. However, training wider and deeper (hence overall larger) models is computationally inefficient, even for datasets with the magnitude of Cora. In contrast, the BalO initialized model for L = 10is already able to attain 79.7% even with width= 64and improves to 80.9% with width= 512. Primarily the 8(a) Test accuracy.  (b) Epochs to convergence. Figure 7: GAT with 64 hidden dimensions trained using Adam. parameter balancing must be responsible for the improved performance, as it is not sufficient to simply initialize the attention weights to zero, as shown in Figure 4a and 5. For the remaining datasets, we report only the performance of networks with 64 hidden dimension in Table 2 for brevity. Since we have considered the XavZ initialization only as an ablation study and find it to be ineffective (see Figure 6a), we do not discuss it any further. Note that for some datasets (e.g.Pubmed, Wisconsin), deeper models (e.g. L = 40) are unable to train at all with Xavier initialization, which explains the high variation in test accuracy across multiple runs as, in each run, the model essentially remains a random network. The reduced performance even with balanced initialization at higher depth may be due to a lack of convergence within5000 epochs. Nevertheless, the fact that the drop in performance is not drastic, but rather gradual as depth increases, indicates that the network is being trained, i.e. trainability is not an issue. As a stress test, we validate that GAT networks with balanced initialization maintain their trainability at even larger depths of 64 and 80 layers (see Fig. 9 in appendix). In fact, the improved performance and training speed-up of models with balanced initialization as opposed to models with standard initialization is upheld even more so for very deep networks. Apart from Cora, Citeseer and Pubmed, the other datasets are considered heterophilic in nature and standard GNN models do not perform very well on them. State-of-the-art performance on these datasets is achieved by specialized models. We do not compare with them for two reasons: i) they do not employ an attention mechanism, which is the focus of our investigations, but comprise various special architectural elements and ii) our aim is not to outperform the SOTA, but rather highlight a mechanism that underlies the learning dynamics and can be exploited to improve trainability. We also demonstrate the effectiveness of a balanced initialization in training deep GATs in comparison to LipschitzNorm [14], a normalization technique proposed specifically for training deep GNNs with self-attention layers such as GAT (see Table 6 in appendix). Adam Adam, the most commonly used optimizer for GNNs, stabilizes the training dynamics and can compensate partially for problematic initializations. However, the drop in performance with depth, though smaller, is inevitable with unbalanced initialization. As evident from Figure 7, BalO initialized models achieve higher accuracy than or at par with Xavier initialization and converge in fewer epochs. As we observe with SGD, this difference becomes more prominent with depth, despite the fact that Adam itself significantly improves the trainability of deeper networks over SGD. Our argument of small relative gradients (see Eq.(8)) also applies to Adam. We have used the initial learning rates reported in the literature [9]: 0.005 for Cora and Citeseer, and 0.01 for Pubmed for the 2 and 5 layer networks. To allow stable training of deeper networks, we reduce the initial learning rate by a factor 0.1 for the 10 and 20 layer networks on all three datasets. Architectural variations We also consider other architectural variations such as employing ELUs instead of ReLUs, using multiple attention heads, turning off weight sharing, and addition of standard elements such as weight decay and dropout. In all cases, the results follow a similar tendency as already reported. Due to a lack of space, we defer the exact results to the appendix. These variations further increase the performance of networks with our initialization proposals, which can therefore be regarded as complementary. Note that residual skip connections between layers are also supported 9in a balanced initialization provided their parameters are initialized with zero. However, to isolate the contribution of the initialization scheme and validate our theoretical insights, we have focused our exposition on the vanilla GAT version. Limitations The derived conservation law only applies to the self-attention defined in the original GAT and GATv2 models, and their architectural variations such as ωGAT[18] (see Fig. 10 in appendix). Note that the law also holds for the non-attentive GCNs (see Table 7 in appendix) which are a special case of GATs (where the attention parameters are simply zero). Modeling different kinds of self-attention such as the dot-product self-attention in [28] entails modification of the conservation law, which has been left for future work. 4 Discussion GATs [9] are powerful graph neural network models that form a cornerstone of learning from graph- based data. The dynamic attention mechanism provides them with high functional expressiveness, as they can flexibly assign different importance to neighbors based on their features. However, as we slightly increase the network depth, the attention and feature weights face difficulty changing during training, which prevents us from learning deeper and more complex models. We have derived an explanation of this issue in the form of a structural relationship between the gradients and parameters that are associated with a feature. This relationship implies a conservation law that preserves a sum of the respective squared weight and attention norms during gradient flow. Accordingly, if weight and attention norms are highly unbalanced as is the case in standard GAT initialization schemes, relative gradients for larger parameters do not have sufficient room to increase. This phenomenon is similar in nature to the neural tangent kernel (NTK) regime [ 24, 54], where only the last linear layer of a classic feed-forward neural network architecture can adapt to a task. Conservation laws for basic feed-forward architectures [15, 3, 32] do not require an infinite width assumption like NTK-related theory and highlight more nuanced issues for trainability. Furthermore, they are intrinsically linked to implicit regularization effects during gradient descent [41]. Our results are of independent interest also in this context, as we incorporate the attention mechanism into the analysis, which has implications for sequence models and transformers as well. One of these implications is that an unbalanced initialization hampers the trainability of the involved parameters. Yet, the identification of the cause already contains an outline for its solution. Balancing the initial norms of feature and attention weights leads to more effective parameter changes and significantly faster convergence during training, even in shallow architectures. To further increase the trainability of feature weights, we endow them with a balanced orthogonal looks-linear structure, which induces perfect dynamical isometry in perceptrons [10] and thus enables signal to pass even through very deep architectures. Experiments on multiple benchmark datasets have verified the validity of our theoretical insights and isolated the effect of different modifications of the initial parameters on the trainability of GATs. 5 Acknowledgements We gratefully acknowledge funding from the European Research Council (ERC) under the Horizon Europe Framework Programme (HORIZON) for proposal number 101116395 SPARSE-ML. 10References [1] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. In International Conference on Learning Representations, 2021. [2] Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient descent for deep linear neural networks. In International Conference on Learning Representations, 2019. [3] Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach, 2018. [4] Jimmy Ba, Murat A. Erdogdu, Taiji Suzuki, Denny Wu, and Tianzong Zhang. Generalization of two-layer neural networks: An asymptotic viewpoint. In International Conference on Learning Representations, 2020. [5] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016. [6] David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams. The shattered gradients problem: If resnets are the answer, then what is the question? In International Conference on Machine Learning, 2018. [7] Peter L. Bartlett, David P. Helmbold, and Philip M. Long. Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks. In International Conference on Machine Learning, 2018. [8] T. Bian, X. Xiao, T. Xu, P. Zhao, W. Huang, Y . Rong, and J. Huang. Rumor detection on social media with bi-directional graph convolutional networks. In AAAI Conference on Artificial Intelligence, 2020. [9] Shaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? In International Conference on Learning Representations, 2022. [10] Rebekka Burkholz and Alina Dubatovka. Initialization of ReLUs for dynamical isometry. In Advances in Neural Information Processing Systems, volume 32, 2019. [11] Tianle Cai, Shengjie Luo, Keyulu Xu, Di He, Tie-Yan Liu, and Liwei Wang. Graphnorm: A principled approach to accelerating graph neural network training. In International Conference on Machine Learning, 2021. [12] Tianlong Chen, Kaixiong Zhou, Keyu Duan, Wenqing Zheng, Peihao Wang, Xia Hu, and Zhangyang Wang. Bag of tricks for training deeper graph neural networks: A comprehensive benchmark study. In IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [13] Weilin Cong, Morteza Ramezani, and Mehrdad Mahdavi. On provable benefits of depth in training graph convolutional networks. In Advances in Neural Information Processing Systems, 2021. [14] George Dasoulas, Kevin Scaman, and Aladin Virmaux. Lipschitz normalization for self-attention layers with application to graph neural networks. In International Conference on Machine Learning, 2021. [15] S. S. Du, W. Hu, and J. D. Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. In Advances in Neural Information Processing Systems, 2018. [16] Moshe Eliasof, Eldad Haber, and Eran Treister. Pde-gcn: Novel architectures for graph neural networks motivated by partial differential equations. In Advances in Neural Information Processing Systems, 2021. [17] Moshe Eliasof, Eldad Haber, and Eran Treister. pathgcn: Learning general graph spatial operators from paths. In International Conference on Machine Learning, 2022. [18] Moshe Eliasof, Lars Ruthotto, and Eran Treister. Improving graph neural networks with learnable propagation operators. In International Conference on Machine Learning, 2023. 11[19] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In International Conference on Artificial Intelligence and Statistics, volume 9, pages 249–256, May 2010. [20] M. Gori, G. Monfardini, and F. Scarselli. A new model for learnig in graph domains. In IEEE International Joint Conference on Neural Networks, 2005. [21] Kai Guo, Kaixiong Zhou, Xia Hu, Yu Li, Yi Chang, and Xin Wang. Orthogonal graph neural networks. In AAAI Conference on Artificial Intelligence, 2022. [22] William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, 2018. [23] Wei Hu, Lechao Xiao, and Jeffrey Pennington. Provable benefit of orthogonal initialization in optimizing deep linear networks. In International Conference on Machine Learning, 2020. [24] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems , 2018. [25] Ajay Jaiswal, Peihao Wang, Tianlong Chen, Justin F. Rousseau, Ying Ding, and Zhangyang Wang. Old can be gold: Better gradient flow can make vanilla-gcns great again. In Advances in Neural Information Processing Systems, 2022. [26] Stefanie Jegelka. Theory of graph neural networks: Representation and learning. In The International Congress of Mathematicians, 2022. [27] S. Kearnes, K. McCloskey, M. Berndl, V . Pande, and P. Riley. Molecular graph convolutions: Moving beyond fingerprints. In Journal of Computer-Aided Molecular Design, 2016. [28] Dongkwan Kim and Alice Oh. How to find your friendly neighborhood: Graph attention design with self-supervision. In International Conference on Learning Representations, 2021. [29] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017. [30] Boris Knyazev, Graham W. Taylor, and Mohamed R. Amer. Understanding attention and generalization in graph neural networks. In Advances in Neural Information Processing Systems, 2019. [31] D. Kunin, J. Sagastuy-Brena, S. Ganguli, D.L.K. Yamins, and H. Tanaka. Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics. InInternational Conference on Learning Representations, 2021. [32] Thien Le and Stefanie Jegelka. Training invariances and the low-rank phenomenon: beyond linear networks. In International Conference on Learning Representations, 2022. [33] Guohao Li, Matthias Müller, Bernard Ghanem, and Vladlen Koltun. Training graph neural networks with 1000 layers. In International Conference on Machine Learning, 2022. [34] Guohao Li, Matthias Müller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as cnns? In International Conference on Computer Vision, 2019. [35] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In AAAI Conference on Artificial Intelligence, 2018. [36] Meng Liu, Hongyang Gao, and Shuiwang Ji. Towards deeper graph neural networks. In SIGKDD International Conference on Knowledge Discovery and Data Mining, 2020. [37] Dmytro Mishkin and Jiri Matas. All you need is a good init. In International Conference on Learning Representations, 2016. [38] Pál András Papp, Karolis Martinkus, Lukas Faber, and Roger Wattenhofer. Dropgnn: Ran- dom dropouts increase the expressiveness of graph neural networks. In Advances in Neural Information Processing Systems, 2021. 12[39] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geo- metric graph convolutional networks. In International Conference on Learning Representations, 2020. [40] Oleg Platonov, Denis Kuznedelev, Michael Diskin, Artem Babenko, and Liudmila Prokhorenkova. A critical look at the evaluation of gnns under heterophily: are we really making progress? In International Conference on Learning Representations, 2023. [41] Noam Razin and Nadav Cohen. Implicit regularization in deep learning may not be explainable by norms. In Advances in Neural Information Processing Systems, 2020. [42] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional networks on node classification. In International Conference on Learning Representations, 2020. [43] Karthik Abinav Sankararaman, Soham De, Zheng Xu, W. Ronny Huang, and Tom Goldstein. The impact of neural network overparameterization on gradient confusion and stochastic gradient descent. In International Conference on Machine Learning, 2020. [44] Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In International Conference on Learning Representations, 2014. [45] F. Scarselli, M. Gori, A.C Tsoi, M. Hagenbuchner, and G. Monfardini. Computational capabili- ties of graph neural networks. In IEEE Transactions on Neural Networks, 2009. [46] Wenling Shang, Kihyuk Sohn, Diogo Almeida, and Honglak Lee. Understanding and im- proving convolutional neural networks via concatenated rectified linear units. In International Conference on Machine Learning, 2016. [47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa- tion Processing Systems, 2017. [48] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018. [49] Peihao Wang, Wenqing Zheng, Tianlong Chen, and Zhangyang Wang. Anti-oversmoothing in deep vision transformers via the fourier domain analysis: From theory to practice. In International Conference on Learning Representations, 2022. [50] Z. Wu, S. Pan, G. Long, J. Jiang, and C. Zhang. Graph wavenet for deep spatial-temporal graph modeling. In International Joint Conference on Artificial Intelligence, 2019. [51] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. [52] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In International Conference on Machine Learning, 2018. [53] Keyulu Xu, Mozhi Zhang, Stefanie Jegelka, and Kenji Kawaguchi. Optimization of graph neural networks: Implicit acceleration by skip connections and more depth. In International Conference on Machine Learning, 2021. [54] Greg Yang and Edward J. Hu. Tensor programs iv: Feature learning in infinite-width neural networks. In Marina Meila and Tong Zhang, editors, International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 11727–11737, 2021. [55] Han Yang, Kaili Ma, and James Cheng. Rethinking graph regularization for graph neural networks. In Advances in Neural Information Processing Systems, 2021. 13[56] Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. In International Conference on Machine Learning, 2016. [57] Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for under- standing neural networks. In Advances in Neural Information Processing Systems, 2019. [58] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform bad for graph representation? In Advances in Neural Information Processing Systems, 2021. [59] Jiaxuan You, Rex Ying, and Jure Leskovec. Design space for graph neural networks. In Advances in Neural Information Processing Systems, 2021. [60] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J. Kim. Graph transformer networks. In Advances in Neural Information Processing Systems, 2020. [61] M. Zhang and Y . Chen. Inductive matrix completion based on graph neural networks. In International Conference on Learning Representations, 2020. [62] Wentao Zhang, Zeang Sheng, Ziqi Yin, Yuezihan Jiang, Yikuan Xia, Jun Gao, Zhi Yang, and Bin Cui. Model degradation hinders deep graph neural networks. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022. [63] Wentao Zhang, Mingyu Yang, Zeang Sheng, Yang Li, Wen Ouyang, Yangyu Tao, Zhi Yang, and Bin Cui. Node dependent local smoothing for scalable graph learning. In Advances in Neural Information Processing Systems, 2021. [64] Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. In International Conference on Learning Representations, 2020. [65] Kaixiong Zhou, Xiao Huang, Yuening Li, Daochen Zha, Rui Chen, and Xia Hu. Towards deeper graph neural networks with differentiable group normalization. In Advances in Neural Information Processing Systems, 2020. [66] Kuangqi Zhou, Yanfei Dong, Kaixin Wang, Wee Sun Lee, Bryan Hooi, Huan Xu, and Jiashi Feng. Understanding and resolving performance degradation in graph convolutional networks. In Conference on Information and Knowledge Management, 2021. [67] Difan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, and Quanquan Gu. Layer- dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, 2019. 14Appendices 6 Proofs of Theorems Notation. Scalar and element-wise products are denoted ⟨, ⟩ and ⊙, respectively. Boldface lowercase and uppercase symbols represent vectors and matrices, respectively. Our proof of Theorem 2.2 utilizes a rescale invariance that follows from Noether’s theorem, as stated by [31]. Note that we could also derive the gradient structure directly from the derivatives, but the rescale invariance is easier to follow. Definition 6.1 (Rescale invariance). The loss L(θ) is rescale invariant with respect to disjoint subsets of the parameters θ1 and θ2 if for every λ >0 we have L(θ) =L((λθ1, λ−1θ2, θd)), where θ = (θ1, θ2, θd). We frequently utilize the following relationship. Lemma 6.2 (Gradient structure due to rescale invariance [31]). The rescale invariance of L enforces the following geometric constraint on the gradients of the loss with respect to its parameters: ⟨θ1, ∇θ1 L⟩ − ⟨θ2, ∇θ2 L⟩ = 0. (10) We first use this rescale invariance to prove our main theorem for a GAT with shared feature transformation weights and a single attention head, as described in Def. 2.1. The underlying principle generalizes to other GAT versions as well, as we exemplify with two other variants. Firstly, we study the case of unshared weights for feature transformation of source and target nodes. Secondly, we discuss multiple attention heads. Proof of Theorem 2.2 Our main proof strategy relies on identifying a multiplicative rescale invariance in GATs that allows us to apply Lemma 6.2. We identify rescale invariances for every neuron i at layer l that induce the stated gradient structure. Specifically, we define the components of in-coming weights to the neuron as θ1 = {w | w ∈ Wl[i, : ]} and the union of all out-going edges (regarding features and attention) as θ2 = {w | w ∈ Wl+1[: , i]} ∪ {al[i]}. It is left to show that these parameters are invariant under rescaling. Let us, therefore, evaluate the GAT loss at λθ1 and λ−1θ2 and show that it remains invariant for any choice of λ >0. Note that the only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and αl uv. We denote the scaled network components with a tilde resulting in ˜hl u[i], ˜hl+1 v [k], and ˜αl uv As we show, parameters of upper layers remain unaffected, as ˜hl+1 v [k] coincides with its original non-scaled variant ˜hl+1 v [k] =hl+1 v [k]. Let us start with the attention coefficients. Note that ˜al[i] =λ−1al[i] and ˜Wl[i, j] =λWl[i, j]. This implies that ˜αl uv = exp(el uv)P u′∈N(v) exp(eluv) = αl uv , because (11) ˜el uv = (al)⊤ · ϕ2(Wl(hl−1 u + hl−1 v )) =el uv, (12) which follows from the positive homogeneity of ϕ2 that allows ˜el uv = λ−1al[i]ϕ2( nl−1X j λWl[i, j](hl−1 u [j] +hl−1 v [j]) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) (13) = λ−1λal[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) (14) = el uv. (15) 15Since ˜αl uv = αl uv, it follows that ˜hu l [i] =ϕ1   X z∈N(u) αl zu nl−1X j λWl[i, j]hl−1 z [j]   = λϕ1   X z∈N(u) αl zu nl−1X j Wl[i, j]hl−1 z [j]   = λhl u[i] In the next layer, we therefore have ˜hl+1 v [k] =ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]˜hl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]λhl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i Wl+1[k, i]hl u[i]   = hl+1 v [k] Thus, the output node representations of the network remain unchanged, and according to Def. 6.1, the loss L is rescale-invariant. Consequently, as per Lemma 6.2, the constraint in Eq.(10) can be written as: ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ − ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ = 0. which can be rearranged to Eq.((2.2): ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. thus proving Theorem 2.2. Proof of Corollary 2.3 Given that gradient flow applied on loss L is captured by the differential equation dw dt = −∇wL (16) which implies: d dt \r\rWl[i, :] \r\r2 = 2⟨Wl[i, :], d dtWl[i, :]⟩ = −2⟨Wl[i, :], ∇Wl[i,:]L⟩ (17) substituting in Eq.(4) similarly for gradient flow of Wl+1[:, i] and al[i], as done in Eq.(17) yields Theorem 2.2: d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 = d dt \u0010\r\rWl+1[: i] \r\r2\u0011 . −2⟨Wl[i, :], ∇Wl[i,:]L⟩ −(−2)⟨al[i], ∇al[i]L⟩ = −2⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. Therefore, Eq.(4) and consequently Eq.(5) in Corollary 2.3 hold. Summing over i ∈ [nl] on both sides of Eq.(4) yields Corollary 2.4. 16Theorem 6.3 (Structure of gradients for GAT without weight-sharing). Let a GAT network as defined by Def. 2.1 consisting of L layers be given. The feature transformation parameters Wl s and Wl t of the source and target nodes of an edge and attention parametersal of a GAT layer l are defined according to Eq.(1) and (2). Then the gradients for layer l ∈ [L − 1] in the network are governed by the following law: ⟨Wl s[i, :], ∇Wls[i,:]L⟩ + ⟨Wl t [i, :], ∇Wl t [i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1 s [:, i], ∇Wl+1 s [:,i]L⟩ (18) Proof. The proof is analogous to the derivation of Theorem 2.2. We follow the same principles and define the disjoint subsets θ1 and θ2 of the parameter set θ, associated with a neuron i in layer l accordingly, as follows: θ1 = {w|w ∈ Wl s[i, :]} ∪ {w|w ∈ Wl t [i, :]} θ2 = {w|w ∈ Wl+1 s [:, i]} ∪ {al[i]} Then, the invariance of node representations follows similarly to the proof of Theorem 2.2. The only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and αl uv. We denote the scaled network components with a tilde resulting in ˜hl u[i], ˜hl+1 v [k], and ˜αl uv As we show, parameters of upper layers remain unaffected, as ˜hl+1 v [k] coincides with its original non-scaled variant ˜hl+1 v [k] =hl+1 v [k]. Let us start with the attention coefficients. Note that ˜al[i] =λ−1al[i], ˜Ws l [i, j] = λWl s[i, j] and ˜Wl t [i, j] =λWl t [i, j]. This implies that ˜αl uv = exp(el uv)P u′∈N(v) exp(eluv) = αl uv , because (19) ˜el uv = (al)⊤ · ϕ2(Wl shl−1 u + Wl t hl−1 v ) =el uv, (20) which follows from the positive homogeneity of ϕ2 that allows ˜el uv = λ−1al[i]ϕ2( nl−1X j λWl s[i, j]hl−1 u [j] +λWl t [i, j]hl−1 v [j] (21) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j]) (22) = λ−1λal[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j] (23) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j]) (24) = el uv. (25) Since ˜αl uv = αl uv, it follows that ˜hu l [i] =ϕ1   X z∈N(u) αl zu nl−1X j λWl s[i, j]hl−1 z [j]   = λϕ1   X z∈N(u) αl zu nl−1X j Wl s[i, j]hl−1 z [j]   = λhl u[i] 17In the next layer, we therefore have ˜hv l+1 [k] =ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1 s [k, i]˜hl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1 s [k, i]λhl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i Wl+1 s [k, i]hl u[i]   = hl+1 v [k] Thus, the application of Lemma 6.2 derives Theorem 6.3. Theorem 6.4 (Structure of gradients for GAT with multi-headed attention) . Given the feature transformation parameters Wl k and attention parameters al k of an attention head k ∈ [K] in a GAT layer of a L layer network. Then the gradients of layer l ∈ [L − 1] respect the law: KX k ⟨Wl k[i, :], ∇Wl k[i,:]L⟩ − KX k ⟨al k[i], ∇al k[i]L⟩ = KX k ⟨Wl+1 k [:, i], ∇Wl+1 k [:,i]L⟩. (26) Proof. Each attention head is independently defined by Eq.1 and Eq.2, and thus Theorem 2.2 holds for each head, separately. The aggregation of multiple heads in a layer is done over node representations of each head in either one of two ways: Concatenation: hl v =∥K k ϕ(P u∈N(v) αkuv l · Wl khl−1 u ) , or Average: hl v = 1 K PK k ϕ(P u∈N(v) αkuv l · Wl khl−1 u ) Thus, the rescale invariance and consequent structure of gradients of the parameters in each head are not altered and Eq.(6.2) holds by summation of the conservation law over all heads. For the most general case of a GAT layer without weight-sharing and with multi-headed attention, each term in Theorem 6.3 is summed over all heads. Further implications analogous to Corollary 2.3, and 2.4 can be derived for these variants by similar principles. 7 Training Dynamics In the main paper, we have shared observations regarding the learning dynamics of GAT networks of varying depths with standard initialization and or balanced initialization schemes. Here, we report additional figures for more varied depths to complete the picture. All figures in this section correspond to a L layer GAT network trained on Cora. For SGD, the learning rate is set to γ = 0.1, γ = 0.05 and γ = 0.01 for L = 5, L = 10and L = 20, respectively. For Adam, γ = 0.005 and γ = 0.0001 for L = 5and L = 10, respectively. Relative change of a feature transformation parameter w is defined as |(w∗−w0)/w∗| where w0 is the initialized value and w∗ is the value for the model with maximum validation accuracy during training. Absolute change |a∗ − a0| is used for attention parameters a since attention parameters are initialized with 0 in the balanced initialization. We view the fraction of parameters that remain unchanged during training separately in Figure 8, and examine the layer-wise distribution of change in parameters in Figure 11 without considering the unchanged parameters. To analyze how gradients vary in the network during training, we define the relative gradient norm of feature transformation and attention parameters for a layer l as ∥∇WlWl∥F/∥Wl∥F and ∥∇alal∥/∥al∥, respectively. Figures 3b and 4b, and Figure 12 depict relative gradient norms for training under SGD and Adam respectively. 18(a) L = 5  (b) L = 10  (c) L = 20 Figure 8: Layer-wise fraction of feature transformation parameters Wl and attention parameters al with zero relative and absolute change, respectively, trained with SGD. For L = 2, no parameters with zero change existed. A small number of feature weights do not change in a 5 layer unbalanced network initialized with Xavier, but this fraction becomes significantly large when depth is increased to L = 10. Note that W1 contains a much larger number of parameters compared to the intermediate layers (specifically in this case, though it is generally common). At L = 20, nearly all w ∈ Wl with an unbalanced initialization (Xav. and Xav+ZeroAtt) struggle to change during training, whereas the balanced Xavier and LL-orthogonal initialization are able to drive change in most w ∈ Wl and all a ∈ al parameters, allowing the network to train. 8 Additional Results The results of the main paper have focused on the vanilla GAT having ReLU activation between consecutive layers, a single attention head, and shared weights for feature transformation of the source and target nodes, optimized with vanilla gradient descent (or Adam) without any regularization. Here, we present additional results for training with architectural variations, comparing a balanced initialization with a normalization scheme focused on GATs, and discussing the impact of an orthogonal initialization and the applicability of the derived conservation law to other message- passing GNNs (MPGNNs). Training variations To understand the impact of different architectural variations, common regular- ization strategies such as dropout and weight decay, and different activation functions, we conducted additional experiments. For a 10-layer network with width 64, Table 3 and 4 report our results for SGD and Adam, respectively. The values of hyperparameters were taken from [48]. Table 3: Mean test accuracy(%) ± 95% CI over five runs of GAT trained on Cora using SGD Variation L Xav Bal X BalO None (Vanilla GAT) 5 73 .00 ± 3.02 76 .96 ± 2.21 79.48 ± 0.43 10 25 .48 ± 18.13 77 .72 ± 1.49 79.46 ± 1.34 attention heads = 8 5 73 .56 ± 2.71 77 .44 ± 1.54 79.58 ± 0.53 10 25 .50 ± 18.18 77 .02 ± 2.76 79.06 ± 0.73 activation = ELU 5 75 .68 ± 1.80 79 .20 ± 1.07 79.64 ± 0.36 10 73 .02 ± 2.27 78.64 ± 1.72 47.76 ± 7.39 dropout = 0.6 5 42 .14 ± 15.97 79 .18 ± 1.17 81.00 ± 0.62 10 24 .90 ± 9.50 30 .94 ± 1.04 44.40 ± 1.84 weight decay = 0.0005 5 67 .26 ± 6.30 77 .36 ± 1.74 79.56 ± 0.48 10 18 .78 ± 11.96 76 .56 ± 2.91 79.40 ± 1.15 weight sharing = False 5 70 .80 ± 7.00 77 .28 ± 1.45 79.82 ± 0.63 10 19 .54 ± 14.03 76 .04 ± 1.77 79.06 ± 0.32 We find that the balanced initialization performs similar in most variations, outperforming the unbalanced standard initialization by a substantial margin. Dropout generally does not seem to be helpful to the deeper network ( L = 10), regardless of the initialization. From Table 3 and 4, it is evident that although techniques like dropout and weight decay may aid optimization, they are 19Table 4: Mean test accuracy(%) ± 95% CI over five runs of GAT trained on Cora using Adam Variation L Xav Bal X BalO None (Vanilla GAT) 5 76 .18 ± 1.61 79 .38 ± 2.24 80.20 ± 0.57 10 70 .86 ± 3.99 75 .72 ± 2.35 79.62 ± 1.27 attention heads = 8 5 75 .62 ± 1.74 78 .54 ± 1.06 79.56 ± 0.85 10 70 .94 ± 2.76 75 .48 ± 2.48 79.74 ± 1.10 activation = ELU 5 76 .56 ± 1.72 78 .72 ± 1.02 79.56 ± 0.64 10 75 .30 ± 1.42 78.48 ± 1.79 76.52 ± 0.97 dropout = 0.6 5 76 .74 ± 1.44 78 .42 ± 1.28 79.92 ± 0.48 10 32 .48 ± 6.99 31 .76 ± 1.33 76.34 ± 0.95 weight decay = 0.0005 5 75 .10 ± 2.05 78 .52 ± 1.41 79.80 ± 0.63 10 28 .74 ± 12.04 74 .68 ± 3.06 79.70 ± 1.14 weight sharing = False 5 76 .56 ± 1.72 78 .72 ± 1.02 79.56 ± 0.64 10 71 .12 ± 2.23 73 .32 ± 1.23 79.46 ± 1.16 alone not enough to enable the trainability of deeper network GATs and thus are complementary to balancedness. Note that ELU is not a positively homogeneous activation function, which is an assumption in out theory. In practice, however, it does not impact the Xavier-balanced initialization (Bal X). However, the Looks-Linear orthogonal structure is specific to ReLUs. Therefore, the orthogonality and balancedness of the Bal O initialization are negatively impacted by ELU, although the Adam optimizer seems to compensate for it to some extent. In addition to increasing the trainability of deeper networks, the balanced initialization matches the state-of-the-art performance of Xavier initialized 2 layer GAT, given the architecture and optimization hyperparameters as reported in [48]. We used an existing GAT implementation and training script2 of Cora that follows the original GAT(v1)[48] paper and added our balanced initialization to the code. As evident from Table 5, the balanced initialization matches SOTA performance of GAT(v1) on Cora (83-84%) (on a version of the dataset with duplicate edges). GAT(v2) achieved 78.5% accuracy on Pubmed but the corresponding hyperparameter values were not reported. Hence, we transferred them from [48]. This way, we are able to match the state-of-the-art performance of GAT(v2) on Pubmed, as shown in Table 5. Table 5: GAT performance on Cora and Pubmed: mean test accuracy(%) ± 95% CI over five runs. . Xav Bal X BalO Cora 84.50 ± 0.52 84.58 ± 0.65 84.55 ± 0.47 Pubmed 78.38 ± 0.77 78 .52 ± 0.54 78.56 ± 0.20 Comparison with Lipshitz Normalization A feasible comparison can be carried out with [14] that proposes a Lipschitz normalization technique aimed at improving the performance of deeper GATs in particular. We use their provided code to reproduce their experiments on Cora, Citeseer, and Pubmed for 2,5,10,20 and 40-layer GATs with Lipschitz normalization and compare them with LLortho+Bal initialization in Table 6. Note that Lipschitz normalization has been shown to outperform other previous normalization techniques for GNNs such as pair-norm [64] and layer-norm [5]. Impact of orthogonal initialization In the main paper, we have advocated and presented results for using a balanced LL-orthogonal initialization. Here, we discuss two special cases of orthogonal initialization (and their balanced versions): identity matrices that have also been used for GNNs in 2https://github.com/Diego999/pyGAT 20Table 6: Comparing a balanced LL-orthogonal initialization to Lipschitz normalization applied with a standard (imbalanced) Xavier initialization: the balanced initialization results in a much higher accuracy as the depth of the network increases. Dataset Cora Citeseer Pubmed Layers Lip. Norm. Bal O Init. Lip. Norm. Bal O Init. Lip. Norm. Bal O Init. 2 82.1 79.5 65 .4 67.7 74.8 76.0 5 77 .1 80.2 63.0 67.7 73.7 75.7 10 78 .0 79.6 43.6 67.4 52.8 76.9 20 72 .2 77.3 18.2 66.3 23.3 77.3 40 12 .9 75.9 18.1 63.2 36.6 77.5 [18, 16], and matrices with looks-linear structure using an identity submatrix (LLidentity) since a looks-linear structure would be more effective for ReLUs [10]. In line with [18], identity and LLidentity are used to initialize the hidden layers while Xavier is used to initialize the first and last layers. A network initialized with identity is balanced by adjusting the weights of the first and last layer to have norm1 (as identity matrices have row-wise and column-wise norm). A network with LLidentity initialization is balanced to have norm 2 in the first and last layer, similar to LLortho initialization. We compare the performance using these four base initializations (one standard Xavier, and three orthogonal cases) and their balanced counterparts in Fig. 9. We observe that balancing an (LL-)orthogonal initialization results in an improvement in the general- ization ability of the network in most cases and speeds up training, particularly for deeper networks. However, note that an (LL-)orthogonal initialization itself also has a positive effect on trainability in particular of deep networks. Contributing to this fact is the mostly-balanced nature of an (LL- )orthogonal initialization i.e. given hidden layers of equal dimensions the network is balanced in all layers except the first and last layers (assuming zero attention parameters), which allows the model to train, as opposed to the more severely imbalanced standard Xavier initialization. This further enforces the key takeaway of our work that norm imbalance at initialization hampers the trainability of GATs. In addition, the LLortho+Bal initialization also speeds up training over the LLortho initialization even in cases in which the generalization performance of the model is at par for both initializations. Figure 9: GAT network with varying initialization trained using SGD. Note that identity initializations have also been explored in the context of standard feed-forward neural networks. While they tend to work in practice, the lack of induced feature diversity can be problematic from a theoretical point of view (see e.g. for a counter-example [7]). However, potentially due to the reasons regarding feature diversity and ReLU activation discussed above, the balanced looks-linear random orthogonal initialization (LLortho+Bal.) outperforms initialization with identity matrices (see Fig. 9). In most cases, the balanced versions outperform the imbalanced version of the base initialization and the performance gap becomes exceedingly large as network depth increases. 21Applicability to other MPGNNs As GAT is a generalization of the GCN, all theorems are also applicable to GCNs (where the attention parameters are simply0). We provide additional experiments in Table 7, comparing the performance of GCN models of varying depths with imbalanced and balanced initializations trained on Cora using the SGD optimizer. We use the same hyperparameters as reported for GAT. As expected, the trainability benefits of a balanced initialization are also evident in deeper GCN networks. Table 7: GCN with varying initialization trained using SGD. Dataset Depth Xavier Xavier+Bal LLortho LLortho+Bal Cora 2 77 .8 ± 0.9 80 .5 ± 0.5 78 .0 ± 0.3 80.9 ± 0.4 5 73 .2 ± 3.4 78 .3 ± 0.8 80.3 ± 0.8 79.6 ± 1.0 10 24 .1 ± 4.5 77 .6 ± 2.0 80 .0 ± 1.1 80.0 ± 0.9 20 14 .4 ± 11.2 62 .8 ± 3.6 78 .7 ± 0.5 78.8 ± 1.5 40 13 .4 ± 0.9 65 .9 ± 7.3 28 .3 ± 16.2 77.1 ± 0.9 64 9 .8 ± 5.4 33 .0 ± 13.4 27 .3 ± 12.7 76.7 ± 1.3 80 12 .4 ± 19.3 33 .8 ± 12.9 38 .9 ± 21.3 77.1 ± 1.3 Citeseer 2 66 .6 ± 20.0 71 .3 ± 1.8 66 .0 ± 3.2 72.3 ± 0.9 5 60 .9 ± 12.3 66 .9 ± 15.0 69 .0 ± 6.4 70.1 ± 1.8 10 23 .8 ± 36.8 66 .0 ± 5.9 70.6 ± 0.9 69.8 ± 10.9 20 16 .4 ± 18.2 47 .9 ± 10.0 67 .0 ± 8.6 69.7 ± 4.5 40 13 .9 ± 56.8 37 .3 ± 92.8 44 .8 ± 6.8 64.7 ± 13.6 64 13 .8 ± 41.4 29 .5 ± 15.0 37 .3 ± 79.6 66.3 ± 0.5 80 12 .4 ± 42.7 25 .8 ± 3.6 30 .1 ± 21.8 64.1 ± 3.2 The underlying principle of a balanced network initialization holds in general. However, adapting the balanced initialization to different methods entails modification of the conservation law derived for GATs to correspond to the specific architecture of the other method. For example, the proposed balanced initialization can be applied directly to a more recent variant of GAT, ωGAT [18], for which the same conservation law holds and also benefits from a balanced initialization. We conduct additional experiments to verify this. The results in Fig. 10 follow a similar pattern as for GATs: a balanced orthogonal initialization with looks linear structure (LLortho+Bal) of ωGAT performs the best, particularly by a wide margin at much higher depths (64 and 80 layers). Figure 10: ωGAT network with varying trained on Cora. In this work, we focus our exposition on GATs and take the first step in modeling the training dynamics of attention-based models for graph learning. An intriguing direction for future work is to derive modifications in the conservation law for other attention-based models utilizing the dot-product self-attention mechanism such as SuperGAT [28] and other architectures based on Transformers[47], 22that are widely used in Large Language Models (LLMs) and have recently also been adapted for graph learning [58, 60]. 9 Datasets Summary We used nine benchmark datasets for the transductive node classification task in our experiments, as summarized in Table 8. For the Platenoid datasets (Cora, Citeseer, and Pubmed) [56], we use the graphs provided by Pytorch Geometric (PyG), in which each link is saved as an undirected edge in both directions. However, the number of edges is not exactly double (for example, 5429 edges are reported in the raw Cora dataset, but the PyG dataset has 10556 < (2 × 5429)) edges as duplicate edges have been removed in preprocessing. We also remove isolated nodes in the Citeseer dataset. The WebKB (Cornell, Texas, and Wisconsin), Wikipedia (Squirrel and Chameleon) and Actor datasets [39], are used from the replication package provided by [40], where duplicate edges are removed. All results in this paper are according to the dataset statistics reported in Table 8. Table 8: Summary of datasets used in experiments. Dataset Cora Cite. Pubmed Cham. Squi. Actor Corn. Texas Wisc. # Nodes 2708 3327 19717 2277 5201 7600 183 183 251 # Edges 10556 9104 88648 31371 198353 26659 277 279 450 # Features 1433 3703 500 2325 2089 932 1703 1703 1703 # Classes 7 6 3 5 5 5 5 5 5 # Train 140 120 60 1092 2496 3648 87 87 120 # Validation 500 500 500 729 1664 2432 59 59 80 # Test 1000 1000 1000 456 1041 1520 37 37 51 23(a) L = 10: The balanced initialization allows larger changes in a higher number of parameters in Wl across all layers l with the highest margin in l = 1. The change distribution for the parameters in al is missing for l = 5and l ∈ [7, 10] because these parameters remain unchanged (see Fig.8). (b) L = 20: The same pattern as in L = 10is seen. However, the difference between the trainability of models with unbalanced and balanced initialization becomes even more prominent. No attention parameters change at all with standard Xavier initialization (and hence their change distribution is not visible in the plot). Figure 11: Distributions of the relative and absolute change in the values of feature transformation parameters Wl and attention parameters al, respectively, when trained using SGD with unbalanced (Xav. and Xav+ZeroAtt) and balanced (Xav+Bal and LLortho+Bal) initialization. The black markers in each standard box-whisker plot denote the mean. In general, larger changes occur in attention parameters at later layers closer to the output of the network, whereas feature parameters change more in the earlier layers at the input of the network. We observe this also from the perspective of relative gradients, as higher gradients cause higher parameter changes. 24(a) L = 5: test accuracy (left to right) is 71.6%, 76.2%, 77.3%, and 80.5%. (b) L = 10: test accuracy (left to right) is 63.2%, 68.8%, 71.8%, and 78.0%. Figure 12: Layer-wise relative gradient norms of GAT parameters trained with Adam, which in itself improves trainability over SGD, especially for L = 10(see Fig.4b) even with standard initialization. However, with a balanced initialization, the training is faster and the model also generalizes better, as evident from the significant margin in test accuracy. A general noticeable trend, particularly with balanced initialization, is that the attention parameters have higher gradients at the end of the network (and thus change more in the later layers (see Fig. 11)) whereas the feature parameters have high gradients at the start of the network. Also, it can be seen that the feature parameters begin to change a few epochs before the attention parameters change noticeably, while the attention parameters continue to change for a few more epochs after the gradients of feature parameters have dropped. We speculate a representational benefit could drive this behaviour, i.e. with depth, the learned representation of neighbors becomes increasingly informative for a node and thus leads to a higher activity of the attention mechanism. 25",
      "meta_data": {
        "arxiv_id": "2310.07235v2",
        "authors": [
          "Nimrah Mustafa",
          "Aleksandar Bojchevski",
          "Rebekka Burkholz"
        ],
        "published_date": "2023-10-11T06:53:05Z",
        "pdf_url": "https://arxiv.org/pdf/2310.07235v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper investigates the optimization and learning dynamics of Graph Attention Networks (GATs), particularly addressing why deep GATs with standard initialization struggle to train and perform worse than shallow counterparts. It derives a conservation law of GAT gradient flow dynamics, which explains that a significant portion of parameters in GATs with standard initialization struggle to change during training, an effect amplified in deeper networks. To counter this, the authors propose a balanced initialization scheme that allows more effective gradient propagation, enabling the trainability of deeper networks and achieving considerable speedup in training and convergence time. The main theorem also serves as a foundation for studying the learning dynamics of positive homogeneous models with attention mechanisms, including Transformers.",
        "methodology": "The core methodology involves translating the concept of neuron-wise balancedness from traditional deep neural networks to GNNs. The authors derive a conservation law for GATs (including variations like shared feature weights and multiple attention heads) with positive homogeneous activation functions (e.g., ReLU, LeakyReLU). This derivation is based on a rescale invariance property of the loss function, which imposes a geometric constraint on the gradients (Lemma 6.2). Based on this insight, a balanced initialization scheme (Procedure 2.6) is devised: 1) attention parameters (al) are set to zero, and 2) feature weights (Wl) are scaled recursively to ensure their squared l2-norms balance across layers, effectively setting the 'degree of balancedness' (c) to zero at initialization. A specific variant, Balanced Orthogonal (BalO) initialization, uses a looks-linear (LL) mirrored block structure for Wl to promote dynamical isometry, and then applies the balancing procedure.",
        "experimental_setup": "The research focuses on the semi-supervised node classification task, utilizing nine common benchmark datasets: Cora, Citeseer, Pubmed, Actor, Chameleon, Cornell, Squirrel, Texas, and Wisconsin. Experiments are conducted using the Pytorch Geometric framework on Nvidia T4 or RTX 3060 GPUs. Models are trained for up to 5000 epochs with SGD and Adam optimizers, with learning rates fine-tuned for different depths and datasets. The model state with the highest validation accuracy is selected, and results (mean ±95% confidence interval) are reported over five runs. The GAT architecture typically uses ReLU activation, weight sharing, and no biases, with a hidden dimension of 64. Four initialization schemes are compared: Xavier (Xav), Xavier with Zero Attention (XavZ, an ablation), Balanced Xavier (BalX), and Balanced Looks-Linear Orthogonal (BalO). Ablation studies include variations in multi-headed attention, ELU activation, dropout, weight decay, and disabling weight sharing. The proposed balanced initialization is also compared against Lipschitz Normalization [14].",
        "limitations": "The derived conservation law is specifically applicable to the self-attention mechanisms found in the original GAT, GATv2, and ωGAT models. It requires modification for other types of self-attention, such as dot-product self-attention used in models like SuperGAT. The theoretical derivations assume gradient flow with infinitesimally small learning rates, although empirical results show the law holds sufficiently well for practical finite learning rates and optimizers like Adam. The assumption of positive homogeneous activation functions means that non-homogeneous functions like ELU can negatively impact the performance of the BalO initialization, though Adam may compensate. Furthermore, the Looks-Linear orthogonal initialization does not perfectly induce dynamical isometry in GATs (or GNNs) due to the nature of neighborhood aggregation. The study also explicitly avoids comparison with state-of-the-art models on heterophilic datasets, as its primary goal is to highlight learning dynamics rather than achieve top performance.",
        "future_research_directions": "Future research could involve exploring how dynamical isometry can be achieved or better approximated in general Graph Neural Networks. Another promising direction is to derive modifications to the conservation law for other attention-based models, particularly those utilizing dot-product self-attention mechanisms like SuperGAT, and extending this analysis to other Transformer-based architectures, including those used in Large Language Models (LLMs) and graph learning (e.g., Vision Transformers which benefit greatly from depth). Further investigation into how width overparameterization in GNNs aids generalization performance and enables the training of deeper models is also suggested."
      }
    },
    {
      "title": "Dynamic Rescaling for Training GNNs"
    },
    {
      "title": "GATE: How to Keep Out Intrusive Neighbors",
      "abstract": "Graph Attention Networks (GATs) are designed to provide flexible neighborhood\naggregation that assigns weights to neighbors according to their importance. In\npractice, however, GATs are often unable to switch off task-irrelevant\nneighborhood aggregation, as we show experimentally and analytically. To\naddress this challenge, we propose GATE, a GAT extension that holds three major\nadvantages: i) It alleviates over-smoothing by addressing its root cause of\nunnecessary neighborhood aggregation. ii) Similarly to perceptrons, it benefits\nfrom higher depth as it can still utilize additional layers for (non-)linear\nfeature transformations in case of (nearly) switched-off neighborhood\naggregation. iii) By down-weighting connections to unrelated neighbors, it\noften outperforms GATs on real-world heterophilic datasets. To further validate\nour claims, we construct a synthetic test bed to analyze a model's ability to\nutilize the appropriate amount of neighborhood aggregation, which could be of\nindependent interest.",
      "full_text": "GATE: How to Keep Out Intrusive Neighbors Nimrah Mustafa 1 Rebekka Burkholz 1 Abstract Graph Attention Networks (GATs) are designed to provide flexible neighborhood aggregation that assigns weights to neighbors according to their importance. In practice, however, GATs are often unable to switch off task-irrelevant neigh- borhood aggregation, as we show experimen- tally and analytically. To address this challenge, we propose GATE, a GAT extension that holds three major advantages: i) It alleviates over- smoothing by addressing its root cause of un- necessary neighborhood aggregation. ii) Simi- larly to perceptrons, it benefits from higher depth as it can still utilize additional layers for (non- )linear feature transformations in case of (nearly) switched-off neighborhood aggregation. iii) By down-weighting connections to unrelated neigh- bors, it often outperforms GATs on real-world heterophilic datasets. To further validate our claims, we construct a synthetic test bed to an- alyze a model’s ability to utilize the appropri- ate amount of neighborhood aggregation, which could be of independent interest. 1. Introduction Graph neural networks (GNNs) (Gori et al., 2005) are a standard class of models for machine learning on graph- structured data that utilize node feature and graph struc- ture information jointly to achieve strong empirical per- formance, particularly on node classification tasks. In- put graphs to GNNs stem from various domains of real- world systems such as social (Bian et al., 2020), com- mercial (Zhang & Chen, 2020), academic (Hamaguchi et al., 2017), economic (Monken et al., 2021), biochem- ical(Kearnes et al., 2016), physical (Shlomi et al., 2021), and transport (Wu et al., 2019) networks that are diverse in their node feature and graph structure properties. 1CISPA Helmholtz Center for Information Security, 66123 Saarbr¨ucken, Germany. Correspondence to: Nimrah Mustafa <nimrah.mustafa@cispa.de>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). The message-passing mechanism of GNNs (Kipf & Welling, 2017; Xu et al., 2019) involves two key steps: a transformation of the node features, and the aggregation of these transformed features from a node’s neighborhood to update the node’s representation during training. While this has proven to be largely successful in certain cases, it gen- erally introduces some problems for learning with GNNs, the most notorious of which is over-smoothing (Li et al., 2018). The enforced use of structural information in ad- dition to node features may be detrimental to learning the node classification task, as shown by recent results where state-of-the-art GNNs perform the same as or worse than multi-layer perceptrons (MLPs) (Gomes et al., 2022; Yan et al., 2022; Ma et al., 2022). One such task is where node labels can be easily determined by informative node fea- tures and require no contribution from the neighborhood. Here, standard neighborhood aggregation, as in most GNN architectures, would impair model performance, particu- larly with an increase in model depth. A popular standard GNN architecture that, in principle, tries to resolve this problem is the Graph Attention Net- work (GAT) (Veliˇckovi´c et al., 2018; Brody et al., 2022). By design, neighborhood aggregation in GATs is charac- terized by learnable coefficients intended to assign larger weights to more important neighboring nodes (including the node itself) in order to learn better node representations. Therefore, in the above example, GATs should ideally re- sort to assigning near-zero importance to neighbor nodes, effectively switching off neighborhood aggregation. How- ever, we find that, counter-intuitively, GATs are unable to do this in practice and continue to aggregate the uninforma- tive features in the neighborhood which impedes learning. One may ask why one would employ a GAT (or any GNN architecture) if an MLP suffices. In practice, we do not know whether neighborhood aggregation (of raw features or features transformed by a perceptron or MLP), would be beneficial or not beforehand. This raises a pertinent ques- tion for the GNN research community: How much neigh- borhood aggregation is needed for a given task? . Ideally, it is what we would want a model to learn. Otherwise, the right task-specific architecture would need to be identified by time and resource-intensive manual tuning. We address the challenge faced by GAT to effectively de- 1 arXiv:2406.00418v2  [cs.LG]  29 Jul 2024GATE: How to Keep Out Intrusive Neighbors termine how well a node is represented by its own features compared to the features of neighboring nodes, i.e., dis- tinguish between the relative importance of available node features and graph structure information for a given task. Firstly, we provide an intuitive explanation for the problem based on a conservation law of GAT gradient flow dynam- ics derived by Mustafa & Burkholz (2023). Building on this insight, we present GATE, an extension of the GAT architecture that can switch neighborhood aggregation on and off as necessary. This allows our proposed architecture to gain the following advantages over GAT: 1. It alleviates the notorious over-smoothing problem by addressing the root cause of unnecessarily repeated neighborhood aggregation. 2. It allows the model to benefit from more meaningful representations obtained solely by deeper non-linear transformations, similarly to perceptrons, in layers with little to no neighborhood aggregation. 3. It often outperforms GATs on real-world heterophilic datasets by weighing down unrelated neighbors. 4. It offers interpretable learned self-attention coeffi- cients, at the node level, that are indicative of the rel- ative importance of feature and structure information in the locality of the node. In order to validate these claims, we construct a syn- thetic test bed of two opposite types of learning problems for node classification where label-relevant information is completely present only in a node’s i) own features and ii) neighboring nodes’ features (see Fig. 2). GATE is able to adapt to both cases as necessary. On real-world datasets, GATE performs competitively on homophilic datasets and is substantially better than GAT on heterophilic datasets. Furthermore, up to our knowledge, it achieves a new state of the art on the relatively large OGB-arxiv dataset (Hu et al., 2021) (i.e., 79.57 ± 0.84% test accuracy). In sum- mary, our contributions are as follows: • We identify and experimentally demonstrate a struc- tural limitation of GAT, i.e., its inability to switch off neighborhood aggregation. • We propose GATE, an extension of GAT, that over- comes this limitation and, in doing so, unlocks several benefits of the architecture. • We update an existing conservation law relating the structure of gradients in GAT to GATE. • We construct a synthetic test bed to validate our claims, which could be of independent interest to mea- sure progress in developing adaptive neighborhood aggregation schemes. Figure 1: MLP only performs node feature transforma- tions, whereas GAT also always aggregates over the neigh- borhood. With the ability to switch off neighborhood ag- gregation, GATE can learn to emulate MLP behavior and potentially interleave effective perceptron and standard lay- ers in a flexible manner. This allows for more expressive power that we find to benefit real-world tasks (see Table 3). 2. Related Work To relieve GNNs from the drawbacks of unnecessarily re- peated neighborhood aggregation in deeper models, ini- tial techniques were inspired by classical deep learning of MLPs such as normalization (Cai et al., 2021; Zhao & Akoglu, 2020; Zhou et al., 2020; 2021) and regularization (Papp et al., 2021; Rong et al., 2020; Yang et al., 202; Zou et al., 2019). More recently, the need for deeper models and architectural changes to limit neighborhood aggregation as necessary has been recognized. Some approaches use linear combi- nations of initial features and current layer representation (Gasteiger et al., 2019), add skip connections and identity mapping (Chen et al., 2020; Cong et al., 2021), combine representations of all previous layers at the last layer (Xu et al., 2018), aggregate information from a node-wise de- fined range of k-hop neighbors(Liu et al., 2020), and limit the number of aggregation iterations based on node influ- ence scores (Zhang et al., 2021). However, these architec- tures are not flexible enough to utilize additional network layers to simulate perceptron behavior, which, as we find, benefits heterophilic tasks. (Ma et al., 2023) discuss ‘good’ and ‘bad’ heterophily, which are also task-dependent. Other contemporary works for general GNNs propose the use of bilevel optimization to determine a node’s strategic discrete action to a received message (Finkelshtein et al., 2023) and a variational inference framework for adaptive message passing (Errica et al., 2023). While these non- attentive architectures improve message passing in GNNs, we focus on identifying and explaining a structural limita- tion of self-attention in GAT, that continues to be used as a strong baseline architecture. An orthogonal line of research uses graph structural learn- ing (Yang et al., 2019; Stretcu et al., 2019; Franceschi et al., 2020) to amend the input graph structure such that neighborhood aggregation benefits the given task. Such 2GATE: How to Keep Out Intrusive Neighbors approaches are difficult to scale, more susceptible to over- smoothing, and potentially destroy any inherent informa- tion in the original graph structure. On the contrary, a standard GNN architecture equipped to selectively per- form neighborhood aggregation avoids these pitfalls. Self- supervision of the attention mechanism has also been pro- posed (Wang et al., 2019; Kim & Oh, 2021). Methods such as graph rewiring (Deac et al., 2022) to overcome problems such as over-squashing (Alon & Yahav, 2021) are comple- mentary and may also be combined with GATE. While we focus our insights on GAT, architectures based on GAT such as ωGAT (Eliasof et al., 2023) also suffer from the same problem (see Fig. 14 in Appendix C). This further confirms that the universal problem with GAT has been correctly identified. In general, recent works direct ef- fort to understand the current limitations of graph attention (Lee et al., 2023; Fountoulakis et al., 2023). 3. Architecture Notation Consider a graph G = ( V, E) with node set V and edge set E ⊆ V × V, where for a node v ∈ V the neighborhood is N(v) = {u|(u, v) ∈ E} and input features are h0 v. A GNN layer updates each node’s repre- sentation by aggregating over its neighbors’ representation and combining it with its own features. The aggregation and combination steps can be performed together by in- troducing self-loops in G such that, ∀v ∈ V, (v, v) ∈ E. We assume the presence of self-loops in G unless spec- ified otherwise. In GATs, this aggregation is weighted by parameterized attention coefficients αuv, which indi- cate the importance of node u for v. A network is con- structed by stacking L layers, defined as follows, using a non-linear activation function ϕ that is homogeneous (i.e ϕ(x) = xϕ′(x)) and consequently, ϕ(ax) = aϕ(x) for positive scalars a) such as ReLU ϕ(x) = max {x, 0} or LeakyReLU ϕ(x) = max{x, 0} + −α max{−x, 0}. GAT Given input representations hl−1 v for v ∈ V, a GAT 1 layer l ∈ [L] transforms those to: hl v = ϕ   X u∈N(v) αl uv · Wl shl−1 u  , where (1) αl uv = exp \u0000 el uv \u0001 P u′∈N(v) exp \u0000 el u′v \u0001, and (2) el uv = \u0000 al\u0001⊤ · ϕ \u0000 Wl shl−1 u + Wl thl−1 v \u0001 (3) The feature transformation weights Ws and Wt for source and target nodes, respectively, may also be shared such that Ws = Wt. We denote this variant of GAT by GATS. 1Throughout, we refer to GATv2 (Brody et al., 2022) as GAT. GATE In addition, we propose GATE, a GAT variant that flexibly weights the importance of node features and neigh- borhood features. A GATE layer is also defined by Eq. (1) and (2) but modifies euv in Eq. (3) to Eq. (4). el uv = \u0000 1u̸=val s + 1u=val t \u0001⊤ ·ϕ \u0000 Ulhl−1 u + Vlhl−1 v \u0001 . (4) We denote euv in Eq. (3) and (4) as el vv if u = v. For GATE, Wl s in Eq. (1) is denoted as Wl. A weight- sharing variant of GATE, GATE S, is characterized by all feature transformation parameters being shared in a layer (i.e. Wl = Ul = Vl). Note that, then, for a d-dimensional layer, GATE adds onlyd more parameters to GAT. We next present theoretical insights into the reasoning be- hind the inability of GATs to switch off neighborhood ag- gregation, which is rooted in norm constraints imposed by the inherent conservation law for GATs. The gradients of GATE fulfill an updated conservation law (Thoerem 4.3) that enables switching off neighborhood aggregation in a parameter regime with well-trainable attention. 4. Theoretical Insights For simplicity, we limit our discussion here to GATs with weight sharing. We derive similar arguments for GATs without weight sharing in Appendix A.1. The following conservation law was recently derived for GATs to explain trainability issues of standard initialization schemes. Even with improved initializations, we argue that this law lim- its the effective expressiveness of GATs and their ability to switch off neighborhood aggregation when necessary. Theorem 4.1 (Thm. 2.2 by Mustafa & Burkholz (2023)) . The parameters θ of a layer l ∈ [L − 1] in a GAT network and their gradients ∇θL w.r.t. loss L fulfill: ⟨Wl [i,:], ∇Wl [i,:] ⟩ = ⟨Wl+1 [:,i] , ∇Wl+1 [:,i] ⟩ + ⟨al [i], ∇al [i] ⟩. (5) Intuitively, this equality limits the budget for the relative change of parameters and imposes indirectly a norm con- straint on the parameters. Under gradient flow that assumes infinitesimally small learning rates, this law implies that the relationship \r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2 − \r\rWl+1[: i] \r\r2 = c stays constant during training, where c is defined by the initial norms. Other gradient-based optimizers fulfill this norm balance also approximately. Note that the norms\r\rWl[i, :] \r\r generally do not assume arbitrary values but are determined by the required scale of the output. Deeper models are especially less flexible in varying these norms as deviations could lead to exploding or diminishing out- puts and/or gradients. In consequence, the norms of the attention parameters are also bounded. Furthermore, a pa- rameter becomes harder to change during training when its 3GATE: How to Keep Out Intrusive Neighbors magnitude increases. This can be seen by transforming the law with respect to the relative change of a parameter de- fined as ∆θ = ∇θL/θ for θ ̸= 0 or ∆θ = 0 for θ = 0. nl+1X j=1 Wl ij 2 ∆Wl ij = nl+2X k=1 Wl+1 ki 2 ∆Wl+1 ki + al i 2 ∆al i. (6) The higher the magnitude of an attention parameter (al i)2, the smaller will be the relative change ∆al i and vice versa. This restricts the attention mechanism in the network to a less-trainable regime without converging to a meaningful model. We next explain why large \r\ral\r\r are required to switch off neighborhood aggregation in a layer. Insight 4.2 (Effective expressiveness of GATs). GATs are challenged to switch off neighborhood aggregation during training, as this requires the model to enter a less trainable regime with large attention parameters ∥a∥2 >> 1. An intuitive derivation of this insight is presented in the appendix. Here, we outline the main argument based on the observation that to make the contribution of neighbor j insignificant relative to node i, we require αij/αii << 1. We use relativeαij/αii instead of αij and αii to cancel out normalization constants and simplify the analysis. Our key observation is that, given an insignificant link (i, j), its relative contribution to its two neighborhoods αij/αii << 1 and αji/αjj << 1 are affected in oppo- site ways by a feature f of the attention parameters a, i.e. if a[f] contributes to reducing αij/αii, it automatically in- creases αji/αjj . However, we require multiple features that contribute to reducing only αij without strengthen- ing αji that may only be possible in a high dimensional space requiring large norms of a. Yet, the norms ∥a∥2 are constrained by the parameter initialization and cannot in- crease arbitrarily due to the derived conservation law. Note that to switch off all neighborhood aggregation, we require αij/αii << 1, ∀ j ∈ N(i), further complicating the task. To address this challenge, we modify the GAT architecture by GATE that learns separate attention parameters for the node and the neighborhood contribution. As its conserva- tion law indicates, it can switch off neighborhood aggrega- tion in the well-trainable parameter regime. Theorem 4.3 (Structure of GATE gradients). The param- eters and gradients of a GATE network w.r.t. to loss L for layer l ∈ [L − 1] are conserved according to the following laws. Given Θ(θ) = ⟨θ, ∇θL⟩, it holds that: Θ(Wl [i,:]) − Θ(al+1 s [i]) − Θ(al+1 t [i]) = Θ(Wl+1 [:,i] ). (7) and, if additional independent matrices Ul and Vl are trainable, it also holds that: Θ(al s[i]) + Θ(al t[i]) = Θ(Ul [i,:]) + Θ(Vl [i,:]). (8) The proof is provided in the appendix. We utilize this the- orem for two purposes. Firstly, it induces an initialization that enables at least the initial trainability of the network. Similarly to GAT (Mustafa & Burkholz, 2023), we initial- ize all a parameters with zero and matrices W with ran- dom orthogonal looks-linear structure in GATE. This also ensures that we have no initial inductive bias or preference for specific neighbor or node features. As an ablation, we also verify that the initialization of the attention parameters in GAT with zero alone can not switch off neighborhood aggregation in GAT (see Fig. 7 in Appendix C). Secondly, the conservation law leads to the insight that a GATE network is more easily capable of switching off neighborhood aggregation or node feature contributions in comparison with GAT. Insight 4.4 (GATE is able to switch off neighborhood ag- gregation.). GATE can flexibly switch off neighborhood aggregation or node features in the well-trainable regime of the attention parameters. This insight follows immediately from the related conser- vation law for GATE that shows that at and as can inter- change the available budget for relative change among each other. Furthermore, the contribution of neighbors and the nodes are controlled separately. We show how the respec- tive switch-off can be achieved with relatively small atten- tion parameter norms that correspond to the well-trainable regime in Appendix A.3. To verify these insights in exper- iments, we next design synthetic data generators that can test the ability of GNNs to take structural infromation into account in a task-appropriate manner. 5. Experiments We validate the ability of GATE to perform the appropri- ate amount of neighborhood aggregation, as relevant for the given task and input graph, on both synthetic and real- world graphs. In order to gauge the amount of neighbor- hood aggregation, we study the distribution of αvv values (over the nodes) at various epochs during training and lay- ers in the network. This serves as a fair proxy since ∀ v ∈ V, αvv = 1 − P u∈N(v),u̸=v αuv. Thus, αvv = 1 implies no neighborhood aggregation (i.e. only hv is used) whereas αvv = 0 implies only neighborhood aggregation (i.e. hv is not used). Figure 2 shows an exemplary con- struction of both these cases. We defer a discussion of the experimental setup to Appendix B. 5.1. Synthetic Test Bed We construct the synthetic test bed as a node classification task for two types of problems: self-sufficient learning and neighbor-dependent learning. In the self-sufficient learning problem, complete label-relevant information is present in 4GATE: How to Keep Out Intrusive Neighbors (a) No neighborhood contribution required. (b) Only neighborhood contribution required. Figure 2: Examples of synthetic input graphs constructed for learning tasks that are (a) self-sufficient and can be bet- ter solved by switching off neighborhood aggregation, i.e. αvv = 1 and (b) neighbor-dependent that benefit from ig- noring the node’s own features, i.e.αvv = 0. In both cases, ∀ v ∈ V, P u∈N(v),u̸=v αuv + αvv = 1. These represent opposite ends of the spectrum whereas real-world tasks of- ten lie in between and require αii ∈ [0, 1]. GATE’s atten- tion mechanism is more flexible than GAT’s in learning the level of neighborhood aggregation required for a task. a node’s own features. On the contrary, in the neighbor- dependent learning problem, label-relevant information is present in the node features of the k-hop neighbors. We discuss both cases in detail, beginning with the simpler one. Learning self-sufficient node labels In order to model this task exactly, we generate an Erd ˝os–R´enyi (ER) graph structure G with N = 1000 nodes and edge probabil- ity p = 0 .01. Node labels yv are assigned uniformly at random from C = [2 , 8] classes. Input node features h0 v are generated as one-hot encoded node labels in both cases, i.e., h0 v = 1yv . Nodes are divided randomly into train/validation/test split with a 2 : 1 : 1 ratio. We also use a real-world graph structure of the Cora dataset. Two cases using this graph structure are tested: i) using the original node labels consisting of7 classes, and ii) randomized labels of 7 classes. Input node features are generated as one-hot encoding of node labels in both cases. The standard splits of Cora are used. Table 1: Self-sufficient learning: S, Cand L denote graph structure, number of label classes, and number of net- work layers, respectively. Original (Orig.) and randomized (Rand.) labels are used for the Cora structure. In all cases, 100% train accuracy is achieved except in ones marked with ∗ and GATE eventually achieves 100% test accuracy a few epochs later except in one marked with ‡. GAT S and GAT models marked with † also eventually achieve 100% test accuracy. GATE S behaves similarly to GATE and achieves 100% train and test accuracy. . S C L Test Acc.@Epoch of Min. Train Loss GATS GAT GATE Cora O,7 1 99.1@215† 97.7@166† 99.0@127 2 93.4@218 94.5@158 99.6@35 5 85.9@92 85.5@72 98.4@36‡ R,7 1 99.4@263 † 99.8@268† 100@104 2 61.7@2088 ∗ 52.8@341∗ 99.9@36 5 35.1@609 32.1@1299 99.9@23 ER(p = 0.01) R,2 1 100@341 † 100@182† 100@1313 2 99.2@100 † 99.2@119† 99.6@79 5 64.0@7778 ∗ 99.6@239 100@45 R,8 1 88.8@9578 ∗ 98.4@3290 99.2@1755 2 90.4@2459 ∗ 94.8@2237 99.6@44 5 23.6@8152 26.0@8121 100@28 As evident in Table 1, GAT is unable to perfectly learn this task whereas GATE easily achieves100% train and test ac- curacy, and often in fewer training epochs. In line with the homophilic nature of Cora, GAT achieves reasonably good accuracy when the original labels of the Cora graph structure are used as neighborhood aggregation is relatively less detrimental, particularly in a single-layer model. Nevertheless, in the same case, GATE generalizes better than GAT with an increase in model depth. This in- dicates that over-smoothing, a major cause of performance degradation with model depth in GNNs, is also alleviated due to reduced neighborhood aggregations (see Fig. 3). On the contrary, random labels pose a real challenge to GAT. Since the neighborhood features are fully uninforma- tive about a node’s label in the randomized case, aggrega- tion over such a neighborhood distorts the fully informa- tive features of the node itself. This impedes the GAT net- work from learning the task, as it is unable to effectively switch off aggregation (see Fig. 3), whereas GATE is able to adapt to the required level of neighborhood aggregation (i.e. none, in this case). Interestingly, note that a single 5GATE: How to Keep Out Intrusive Neighbors (a) GAT with original labels  (b) GAT with random labels (c) GATE with original labels  (d) GATE with random labels Figure 3: Distribution of αvv against training epoch for self-sufficient learning problem using Cora structure and input node features as the one-hot encoding of labels for 1 and 2 layer models. Due to space limitation, we defer the plots of 5 layer networks to Fig. 8 in Appendix C. layer GAT in the case of random labels can almost, though not completely, switch off neighborhood aggregation (see Fig. 3) and achieve (near) perfect accuracy in the simpler cases. This is in line with our theoretical analysis (see In- sight 4.2), as the norms of a single-layer model are not con- strained and thus the attention parameters have more free- dom to change. Overall, the accuracy of GAT worsens drastically along two dimensions simultaneously: i) an increase in the depth of the model (due to increased unnecessary aggregation), and ii) an increase in the complexity of the task (due to an in- crease in the number of classes in an ER graph and conse- quently in node neighborhoods). In the interest of space, we defer results for GATE S to Fig. 9 in Appendix C as aggregation patterns similar to GATE are observed. Having established that GATE excels GAT in avoiding task-irrelevant neighborhood aggregation, it is also impor- tant to verify whether GATE can perform task-relevant neighborhood aggregation when required, and as much as required. We answer this question next by studying the be- havior of GATE, in comparison to GAT, on a synthetically constructed neighbor-dependent learning problem. Learning neighbor-dependent node labels To model this task, we generate an ER graph structure with N = 1000 nodes and edge probability p = 0 .01. Input node features h0 v ∈ Rd are sampled from a multivariate normal distribution N(0d, Id). For simplicity, d = 2. This input graph G is fed to a random GAT network Mk with k layers of width d. Note that this input graph G has no self-loops on nodes (i.e. v /∈ N(v)).The parameters of Mk are initialized with the standard Xavier (Glorot & Ben- gio, 2010) initialization. Thus, for each node v, the node embedding output by Mk, hMk v is effectively a function f Table 2: Neighbor-dependent learning: k and L denote the number of aggregation steps of the random GAT used for label generation and the number of layers of the evaluated network, respectively. Mean test accuracy ±95% confi- dence interval over 5 runs is reported. In all cases, a GATE variant outperforms the GAT variants. We further analyze one experimental run in detail in Table 8 in Appendix C. k L GATS GAT GATE S GATE 1 1 93 .6±1.3 92 .3±1.3 96.4 ± 0.7 93.5±1.3 2 93 .5±0.7 92 .7±2.7 97.9 ± 0.8 94.6±2.1 3 88 .2±4.9 91 .8±3.4 92 .1±4.6 94.0 ± 1.5 2 2 90 .4±1.3 87 .7±1.6 93.8 ± 0.5 88.7±2.5 3 82 .2±4.5 88 .9±2.1 85 .8±2.5 93.4 ± 3.3 4 84 .0±5.0 83 .0±4.8 89.2 ± 2.3 87.8±2.4 3 3 84 .3±3.2 83 .8±2.7 87 .5±1.8 88.6 ± 2.0 4 71 .4±3.9 75 .9±7.6 89.2 ± 1.0 89.0±0.5 5 80 .2±4.8 83 .9±2.2 86 .1±0.8 87.8 ± 1.6 of the k-hop neighboring nodes of node v represented by a random GAT network. Let Nk(v) denote the set of k-hop neighbors of v and v /∈ Nk(v). Finally, we run K-means clustering on the neighborhood aggregated representation of nodes hMk v to divide nodes into C clusters. For simplicity, we set C = 2. This cluster- ing serves as the node labels (i.e. yv = argc∈[C](v ∈ c) for our node classification task. Thus, the label yv of a node v to be learned is highly dependent on the input features of the neighboring nodes h0 u ∈ Nk(v) rather than the node’s own input features h0 v. The generated input data and the real decision boundary for varying k are shown in Fig. 4. Corresponding results in Table 2 and Fig. 5 exhibit that GATE can better detect the amount of necessary neighborhood aggregation than GAT. However, this task is more challenging than the previous one, and GATE too can not achieve perfect100% test accu- racy. This could be attributed to data points close to the real decision boundary which is not crisply defined (see Fig. 4). 5.2. Real-World Data To demonstrate that the ability of GATE to switch-off neighborhood aggregation has real application relevance, we evaluate GATE on relatively large-scale real-world node classification tasks, namely on five heterophilic benchmark datasets (Platonov et al., 2023) (see Table 3) and three OGB datasets (Hu et al., 2021) (see Table 5). We defer results and discussion on five small-scale datasets with varying homophily levels to Table 7 in Appendix C. To analyze 6GATE: How to Keep Out Intrusive Neighbors (a) k = 1  (b) k = 2  (c) k = 3 Figure 4: (a)-(c): Distribution of node labels of a synthetic dataset, with neighbor-dependent node labels, based on nodes’ own random features (left) and neighbors’ features aggregated k times (right). Figure 5: Distribution of αvv against training epoch for the neighbor-dependent learning problem withk = 3. Rows: GAT (top) and GATE (bottom) architecture. Columns (left to right): 3, 4, and 5 layer models. While GAT is unable to switch off neighborhood aggregation in any layer, only 3 layers of the 4 and 5 layer models perform neighborhood aggregation. the potential benefits of combining MLP and GAT layers in GATE, we compare its behavior with GAT and MLP. We argue that the better performance of GATE, by a large margin in most cases, can be attributed to down-weighting unrelated neighbors, leveraging deeper non-linear feature transformations, and reducing over-smoothing. While we focus our exposition on the neighborhood aggre- gation perspective of GATs, we also consider the FAGCN architecture (Bo et al., 2021), which relies on a similar at- tention mechanism and, in theory, could switch off neigh- borhood aggregation when positive and negative contribu- tions of neighbors cancel out. In contrast to GATE, it re- quires tuning a hyperparameter ϵ, which controls the con- tribution of raw node features to each layer. Furthermore, on our synthetic tasks, we find that, like GAT, FAGCN is also unable to limit neighborhood contribution. We also provide a detailed qualitative and quantitative discussion comparing GATE and FAGCN using the synthetic testbed in Appendix C. Next, we analyze GATE’s ability to mix MLP and GAT layers. To this end, we evaluate a GNN architecture con- structed by alternately placing GAT and MLP layers in the network that we denote by MLP +GAT on various het- erophilic tasks. The purpose of this experiment is twofold. Firstly, we observe in Table 3 that MLP+GAT outperforms both GAT and MLP in most cases. This highlights the benefit of only performing non-linear transformations on raw or aggregated neighborhood features without immedi- ate further neighborhood aggregation to learn potentially more complex features. Secondly, we find GATE to outper- form MLP+GAT (see Table 3). This illustrates that rigidly embedding MLP layers in a GNN with arbitrary predefined roles is not ideal as the appropriate degree and placement of neighborhood aggregation is unknown a-priori. In con- trast, GATE offers more flexibility to learn intricate com- binations of GNN layers and nonlinear feature transforma- tions that define more adequate models for a given task, as exemplified in Fig. 6. The distributions ofαvv in Fig. 6 across layers in GATE re- veal information about the relative importance of node fea- ture and graph structure at the node level, which allows us to analyze the question to which degree graph information is helpful for a task. For example, in Fig. 6, we observe that the αvv values are mostly lower in the minesweeper dataset than the roman-empire dataset. This indicates that aggregation, particularly over the input node features and the final layer’s learned representations, is more beneficial compared to the node’s own features for the minesweeper dataset. On the other hand, for roman-empire, the model has a higher preference to utilize features of the node it- self (as most values of αvv approach 1) over features of the neighbors. This aligns with the homophily levels, 0.05 and 0.68, of the roman-empire and minesweeper datasets, re- spectively. A similar analysis for datasets Texas and Actor can be found in Fig. 13 in Appendix C. We also observe in Fig. 6 that when neighborhood aggrega- 7GATE: How to Keep Out Intrusive Neighbors Table 3: We report mean test accuracy ±95% confidence interval for roman-empire and amazon-ratings and AUC-ROC for the other three datasets over the standard 10 splits, following (Platonov et al., 2023). All architectures were run with networks of depth 5 and 10 layers. The better performance for each architecture is shown with the number of network layers used in parentheses. GATE outperforms GAT and other baselines on all datasets, mostly by a significant margin. roman-empire amazon-ratings questions minesweeper tolokers GAT 26.10 ± 1.25 (5) 45 .58 ± 0.41 (10) 57 .72 ± 1.58 (5) 50 .83 ± 0.41 (5) 63 .57 ± 1.03 (10) MLP 65.12 ± 0.25 (5) 43 .26 ± 0.34 (5) 59 .44 ± 0.94 (10) 50 .74 ± 0.56 (5) 62 .67 ± 1.06 (10) MLP+GAT 70.83 ± 0.39 (5) 45 .25 ± 0.17 (10) 59 .12 ± 1.57 (10) 60 .07 ± 1.11 (5) 65 .85 ± 0.64 (10) FAGCN 67.55 ± 0.81 (5) 42 .85 ± 0.83 (10) 60 .38 ± 1.21 (5) 63 .38 ± 0.91 (10) 60 .89 ± 1.12 (5) GATE 75.55 ± 0.30 (5) 45.73 ± 0.24(10) 62.95 ± 0.71 (5) 66.14 ± 1.57 (5) 66.63 ± 1.15(10) (a) roman-empire, GAT:28.96% test accuracy.  (b) roman-empire, GATE: 75.94% test accuracy. (c) Minesweeper, GAT:50.50% test AUROC.  (d) Minesweeper, GATE:67.57% test AUROC. Figure 6: Distribution of αvv against training epoch for a run of 5 layer networks on real-world heterophilic tasks. As ex- pected, GAT is unable to significantly vary neighborhood aggregation whereas GATE exhibits varying aggregation patterns across layers and tasks. These could be interpreted to indicate the inherent importance of raw node features relative to their neighborhoods for a given task. We defer the plots of the 10-layer models for tolokers dataset to Fig. 11 in Appendix C. tion takes place, the level of aggregation across all nodes, as indicated by the shape of αvv distribution, varies over network layers. This is expected as different nodes need different levels of aggregation depending on where they are situated in the graph topology. For example, peripheral nodes would require more aggregation than central nodes to obtain a similar amount of information. Therefore, as already observed with purposefully constructed synthetic data, GATE offers a more interpretable model than GAT in a real-world setting. While we focus our evaluation in Table 3 on comparison with the most relevant baselines such as attention-based ar- chitectures, we next present a more extensive comparison with 14 other baseline architectures in Table 4. For the re- sults reported in Table 3, we conduct experiments in a sim- ple setting without additional elements that may impact the performance such as skip connections, normalization, etc., to isolate the effect of the architecture and evaluate solely the impact of GATE’s ability to switch off neighborhood aggregation on real-world data. However, for the results in Table 4, we adopt the original codebase of (Platonov et al., 2023), which utilizes such elements to evaluate the performance of baseline GNNs and architectures specifi- cally designed for heterophilic datasets. We evaluate GATE in the same settings optimized for their experiments. For easy comparison, we replicate their results from Table 4 in (Platonov et al., 2023). We observe in Table 4 that while GATE outperforms GAT (and other baselines) significantly, GATE has comparable performance to GAT-sep, a variant of GAT, despite GATE being more parameter efficient by an order of magnitude. More specifically, GAT-sep and GATE introduce d2 and d additional parameters, respectively, in a layer. By corre- spondingly adapting GATE, we find GATE-sep to achieve the best performance in most cases. Therefore, additional techniques generally employed to boost performance are compatible and complementary to GATE. GATE’s ability to benefit from depth in terms of generaliza- tion is demonstrated on OGB datasets (see Table 5). In par- 8GATE: How to Keep Out Intrusive Neighbors Table 4: An extensive comparison of GATE with baseline GNNs using the experimental setup of (Platonov et al., 2023). Accuracy is reported for roman-empire and amazon-ratings, and ROC AUC is reported for the remaining three datasets. roman-empire amazon-ratings minesweeper tolokers questions GATE 89.51 ± 0.49 52.49 ± 0.46 92.82 ± 0.90 84.62 ± 0.69 78.46 ± 1.17 GAT 80.87 ± 0.30 49.09 ± 0.63 92.01 ± 0.68 83.70 ± 0.47 77.43 ± 1.20 GATE-sep 89.78 ± 0.54 54.51 ± 0.38 94.18 ± 0.43 84.48 ± 0.57 78.20 ± 1.00 GAT-sep 88.75 ± 0.41 52.70 ± 0.62 93.91 ± 0.35 83.78 ± 0.43 76.79 ± 0.71 GT 86.51 ± 0.73 51.17 ± 0.66 91.85 ± 0.76 83.23 ± 0.64 77.95 ± 0.68 GT-sep 87.32 ± 0.39 52.18 ± 0.80 92.29 ± 0.47 82.52 ± 0.92 78.05 ± 0.93 GCN 73.69 ± 0.74 48.70 ± 0.63 89.75 ± 0.52 83.64 ± 0.67 76.09 ± 1.27 SAGE 85.74 ± 0.67 53.63 ± 0.39 93.51 ± 0.57 82.43 ± 0.44 76.44 ± 0.62 H2GCN 60.11 ± 0.52 36.47 ± 0.23 89.71 ± 0.31 73.35 ± 1.01 63.59 ± 1.46 CPGNN 63.96 ± 0.62 39.79 ± 0.77 52.03 ± 5.46 73.36 ± 1.01 65.96 ± 1.95 GPR-GNN 64.85 ± 0.27 44.88 ± 0.34 86.24 ± 0.61 72.94 ± 0.97 55.48 ± 0.91 FSGNN 79.92 ± 0.56 52.74 ± 0.83 90.08 ± 0.70 82.76 ± 0.61 78.86 ± 0.92 GloGNN 59.63 ± 0.69 36.89 ± 0.14 51.08 ± 1.23 73.39 ± 1.17 65.74 ± 1.19 FAGCN 65.22 ± 0.56 44.12 ± 0.30 88.17 ± 0.73 77.75 ± 1.05 77.24 ± 1.26 GBK-GNN 74.57 ± 0.47 45.98 ± 0.71 90.85 ± 0.58 81.01 ± 0.67 74.47 ± 0.86 JacobiConv 71.14 ± 0.42 43.55 ± 0.48 89.66 ± 0.40 68.66 ± 0.65 73.88 ± 1.16 ResNet 65.88 ± 0.38 45.90 ± 0.52 50.89 ± 1.39 72.95 ± 1.06 70.34 ± 0.76 ResNet+SGC 73.90 ± 0.51 50.66 ± 0.48 70.88 ± 0.90 80.70 ± 0.97 75.81 ± 0.96 ResNet+adj 52.25 ± 0.40 51.83 ± 0.57 50.42 ± 0.83 78.78 ± 1.11 75.77 ± 1.24 Table 5: Mean test accuracy ±95% confidence interval (and number of network layers). We replicate the results for GAT reported by (Brody et al., 2022). GATE leverages deeper networks to substantially outperform GAT. OGB- GAT GATE arxiv 71.87 ± 0.16 (3) 79.57 ± 0.84 (12) products 80.63 ± 0.46 (3) 86.24 ± 1.01 (8) mag 32.61 ± 0.29 (2) 35.29 ± .36 (5) ticular, GATE improves the SOTA test accuracy (78.03%) on the arxiv dataset achieved by a model using embed- dings learned by a language model instead of raw node features(Duan et al., 2023), as reported on the OGB leader- board. While the better performance of deeper models with limited neighborhood aggregation in certain layers indi- cates reduced over-smoothing, we also verify this insight quantitatively (see Table 9 in Appendix C). Our experimental code is available at https:// github.com/RelationalML/GATE.git. 6. Conclusion We experimentally illustrate a structural limitation of GAT that disables the architecture, in practice, to switch off task- irrelevant neighborhood aggregation. This obstructs GAT from achieving its intended potential. Based on insights from an existing conservation law of gradient flow dynam- ics in GAT, we have explained the source of this problem. To verify that we have identified the correct issue, we re- solve it with a modification of GAT, which we call GATE, and derive the corresponding modified conservation law. GATE holds multiple advantages over GAT, as it can lever- age the benefits of depth as in MLPs, offer interpretable, learned self-attention coefficients, and adapt the model to the necessary degree of neighborhood aggregation for a given task. We verify this on multiple synthetic and real- world tasks, where GATE significantly outperforms GAT and also achieves a new SOTA test accuracy on the OGB- arxiv dataset. Therefore, we argue that GATE is a suitable candidate to answer highly debated questions related to the importance of a given graph structure for standard tasks. 9GATE: How to Keep Out Intrusive Neighbors Acknowledgements We gratefully acknowledge funding from the European Research Council (ERC) under the Horizon Europe Framework Programme (HORIZON) for proposal number 101116395 SPARSE-ML. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. References Alon, U. and Yahav, E. On the bottleneck of graph neural networks and its practical implications. In International Conference on Learning Representations, 2021. Bian, T., Xiao, X., Xu, T., Zhao, P., Huang, W., Rong, Y ., and Huang, J. Rumor detection on social media with bi- directional graph convolutional networks. In AAAI Con- ference on Artificial Intelligence, 2020. Bo, D., Wang, X., Shi, C., and Shen, H. Beyond low- frequency information in graph convolutional networks. In AAAI Conference on Artificial Intelligence, 2021. Brody, S., Alon, U., and Yahav, E. How attentive are graph attention networks? In International Conference on Learning Representations, 2022. Burkholz, R. and Dubatovka, A. Initialization of ReLUs for dynamical isometry. In Advances in Neural Information Processing Systems, volume 32, 2019. Cai, C. and Wang, Y . A note on over-smoothing for graph neural networks. In Graph Representation Learning Workshop, International Conference on Machine Learn- ing, 2020. Cai, T., Luo, S., Xu, K., He, D., Liu, T.-Y ., and Wang, L. Graphnorm: A principled approach to accelerating graph neural network training. In International Conference on Machine Learning, 2021. Chen, M., Wei, Z., Huang, Z., Ding, B., and Li, Y . Simple and deep graph convolutional networks. InInternational Conference on Machine Learning, 2020. Cong, W., Ramezani, M., and Mahdavi, M. On provable benefits of depth in training graph convolutional net- works. In Advances in Neural Information Processing Systems, 2021. Deac, A., Lackenby, M., and Veliˇckovi´c, P. Expander graph propagation. In Learning on Graphs Conference, 2022. Duan, K., Liu, Q., Chua, T.-S., Yan, S., Ooi, W. T., Xie, Q., and He, J. Simteg: A frustratingly simple approach improves textual graph learning, 2023. Eliasof, M., Ruthotto, L., and Treister, E. Improving graph neural networks with learnable propagation operators. In International Conference on Machine Learning, 2023. Errica, F., Christiansen, H., Zaverkin, V ., Maruyama, T., Niepert, M., and Alesiani, F. Adaptive message passing: A general framework to mitigate oversmoothing, over- squashing, and underreaching, 2023. Finkelshtein, B., Huang, X., Bronstein, M., and ˙Ismail ˙Ilkan Ceylan. Cooperative graph neural networks, 2023. Fountoulakis, K., Levi, A., Yang, S., Baranwal, A., and Jagannath, A. Graph attention retrospective. In Journal of Machine Learning Research, 2023. Franceschi, L., Niepert, M., Pontil, M., and He, X. Learn- ing discrete structures for graph neural networks. In In- ternational Conference on Machine Learning, 2020. Gasteiger, J., Bojchevski, A., and G ¨unnemann, S. Predict then propagate: Graph neural networks meet personal- ized pagerank. In International Conference on Learning Representations, 2019. Glorot, X. and Bengio, Y . Understanding the difficulty of training deep feedforward neural networks. In Interna- tional Conference on Artificial Intelligence and Statis- tics, volume 9, pp. 249–256, May 2010. Gomes, D., Ruelens, F., Efthymiadis, K., Nowe, A., and Vrancx, P. When are graph neural networks better than structure agnostic methods? In Neural Information Pro- cessing Systems Workshop ICBINB, 2022. Gori, M., Monfardini, G., and Scarselli, F. A new model for learnig in graph domains. InIEEE International Joint Conference on Neural Networks, 2005. Hamaguchi, T., Oiwa, H., Shimbo, M., and Matsumoto, Y . Knowledge transfer for out-of-knowledge-base entities : A graph neural network approach. In International Joint Conference on Artificial Intelligence, 2017. Hamilton, W. L., Ying, R., and Leskovec, J. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, 2018. Hu, W., Fey, M., Zitnik, M., Dong, Y ., Ren, H., Liu, B., Catasta, M., and Leskovec, J. Open graph benchmark: Datasets for machine learning on graphs, 2021. 10GATE: How to Keep Out Intrusive Neighbors Kearnes, S., McCloskey, K., Berndl, M., Pande, V ., and Riley, P. Molecular graph convolutions: Moving beyond fingerprints. In Journal of Computer-Aided Molecular Design, 2016. Kim, D. and Oh, A. How to find your friendly neighbor- hood: Graph attention design with self-supervision. In International Conference on Learning Representations , 2021. Kipf, T. N. and Welling, M. Semi-supervised classifica- tion with graph convolutional networks. InInternational Conference on Learning Representations, 2017. Lee, S. Y ., Bu, F., Yoo, J., and Shin, K. Towards deep attention in graph neural networks: Problems and reme- dies. In International Conference on Machine Learning, 2023. Li, Q., Han, Z., and Wu, X.-M. Deeper insights into graph convolutional networks for semi-supervised learning. In AAAI Conference on Artificial Intelligence, 2018. Liu, M., Gao, H., and Ji, S. Towards deeper graph neu- ral networks. In SIGKDD International Conference on Knowledge Discovery and Data Mining, 2020. Ma, Y ., Liu, X., Shah, N., and Tang, J. Is homophily a necessity for graph neural networks? In International Conference on Learning Representations, 2022. Ma, Y ., Liu, X., Shah, N., and Tang, J. Is homophily a necessity for graph neural networks? In International Conference on Learning Representations, 2023. Monken, A., Haberkorn, F., Gopinatha, M., Freeman, L., and Batarseh, F. A. Graph neural networks for modeling causality in international trade. In AAAI Conference on Artificial Intelligence, 2021. Mustafa, N. and Burkholz, R. Are GATS out of balance? In Advances in Neural Information Processing Systems, 2023. Papp, P. A., Martinkus, K., Faber, L., and Wattenhofer, R. Dropgnn: Random dropouts increase the expressiveness of graph neural networks. In Advances in Neural Infor- mation Processing Systems, 2021. Pei, H., Wei, B., Chang, K. C.-C., Lei, Y ., and Yang, B. Geom-gcn: Geometric graph convolutional networks. In International Conference on Learning Representations , 2020. Platonov, O., Kuznedelev, D., Diskin, M., Babenko, A., and Prokhorenkova, L. A critical look at the evalua- tion of gnns under heterophily: are we really making progress? In International Conference on Learning Rep- resentations, 2023. Rong, Y ., Huang, W., Xu, T., and Huang, J. Dropedge: To- wards deep graph convolutional networks on node clas- sification. In International Conference on Learning Rep- resentations, 2020. Shlomi, J., Battaglia, P., and Vlimant, J.-R. Graph neural networks in particle physics. 2021. Stretcu, O., Viswanathan, K., Movshovitz-Attias, D., Pla- tanios, E., Ravi, S., and Tomkins, A. Graph agreement models for semi-supervised learning. In Advances in Neural Information Processing Systems, 2019. Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Li`o, P., and Bengio, Y . Graph attention networks. In International Conference on Learning Representations , 2018. Wang, G., Ying, R., Huang, J., and Leskovec, J. Improving graph attention networks with large margin-based con- straints. In Graph Representation Learning Workshop, Neural Information Processing Systems, 2019. Wu, Z., Pan, S., Long, G., Jiang, J., and Zhang, C. Graph wavenet for deep spatial-temporal graph modeling. In International Joint Conference on Artificial Intelligence, 2019. Xu, K., Li, C., Tian, Y ., Sonobe, T., Kawarabayashi, K.-i., and Jegelka, S. Representation learning on graphs with jumping knowledge networks. In International Confer- ence on Machine Learning, 2018. Xu, K., Hu, W., Leskovec, J., and Jegelka, S. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. Yan, Y ., Hashemi, M., Swersky, K., Yang, Y ., and Koutra, D. Two sides of the same coin: Heterophily and over- smoothing in graph convolutional neural networks. In IEEE International Conference on Data Mining, 2022. Yang, H., Ma, K., and Cheng, J. Rethinking graph regular- ization for graph neural networks. InAdvances in Neural Information Processing Systems, 202. Yang, L., Kang, Z., Cao, X., Jin, D., Yang, B., and Guo, Y . Topology optimization based graph convolutional net- work. In International Joint Conference on Artificial In- telligence, 2019. Zhang, M. and Chen, Y . Inductive matrix completion based on graph neural networks. In International Conference on Learning Representations, 2020. Zhang, W., Yang, M., Sheng, Z., Li, Y ., Ouyang, W., Tao, Y ., Yang, Z., and Cui, B. Node dependent local smooth- ing for scalable graph learning. In Advances in Neural Information Processing Systems, 2021. 11GATE: How to Keep Out Intrusive Neighbors Zhao, L. and Akoglu, L. Pairnorm: Tackling oversmooth- ing in gnns. In International Conference on Learning Representations, 2020. Zhou, K., Huang, X., Li, Y ., Zha, D., Chen, R., and Hu, X. Towards deeper graph neural networks with differen- tiable group normalization. In Advances in Neural Infor- mation Processing Systems, 2020. Zhou, K., Dong, Y ., Wang, K., Lee, W. S., Hooi, B., Xu, H., and Feng, J. Understanding and resolving performance degradation in graph convolutional networks. InConfer- ence on Information and Knowledge Management, 2021. Zou, D., Hu, Z., Wang, Y ., Jiang, S., Sun, Y ., and Gu, Q. Layer-dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, 2019. 12GATE: How to Keep Out Intrusive Neighbors A. Theoretical Derivations A.1. Derivation of Insight 4.2 Statement (Restated Insight 4.2) . GATs are challenged to switch off neighborhood aggregation during training, as this would require the model to enter a less trainable regime with large attention parameters ∥a∥2 >> 1. We have to distinguish GATs with and without weight sharing in our analysis. GATs with weight sharing: To investigate the ability of a GAT to switch off neighborhood aggregation, let us focus on a link(i, j) that should neither contribute to the feature transformation of i nor j. This implies that we need to find attention parameters a (and potentially feature transformations W) so that αij/αii << 1 with αij/αii = exp ( eij − eii). This implies that we require eij − eii << 0 and thus aT ϕ (W (hi + hj)) − 2aT ϕ (W (hi)) << 0. Since we also require αij/αjj << 1, it follows from adding both inequalities that aT [ϕ (W (hi + hj)) − (ϕ (Whi) + ϕ (Whj))] << 0. This inequality can only be fulfilled if there exists at least one feature f for which ∆fij ; = a[f] [ϕ (W[f, :] (hi + hj)) − (ϕ (W[f, :]hi) + ϕ (W[f, :]hi))] fulfills ∆fij << 0. Yet, note that if both ϕ (W[f, :]hi) and ϕ (W[f, :]hj) are positive or both are negative, we just get ∆fij = 0 because of the definition of a LeakyReLU. Thus, there must exist at least one feature f so that without loss of generality ϕ (W[f, :]hi) < 0 and ϕ (W[f, :]hj) > 0. It follows that if a[f] > 0 that 0 > a[f]ϕ (W[f, :]hi) >> a[f] (ϕ (W[f, :] (hi + hj)) − ϕ (W[f, :]hj)) > a[f] (ϕ (W[f, :] (hi + hj)) − 2ϕ (W[f, :]hj)) also receives a negative contribution that makes αij/αjj smaller. Yet, what happens to αij/αii? By distinguishing two cases, namely W[f, :] (hi + hj) > 0 or W[f, :] (hi + hj) < 0 and computing a[f] [ϕ (W (hi + hj)) − 2ϕ (W[f, :]hj)] > 0 we find the feature contribution to be positive. If a[f] < 0, then 0 > a[f]ϕ (W[f, :]hj) >> a[f] (ϕ (W[f, :] (hi + hj)) − ϕ (W[f, :]hi)) > a[f] (ϕ (W[f, :] (hi + hj)) − 2ϕ (W[f, :]hi)) and αij/αjj is reduced. Similarly, we can derive that at the same time αij/αii is increased, however. This implies that any feature that contributes to reducing ∆fij automatically increases one feature while it increases another. We therefore need multiple featuresf to contribute to reducing either αij/αii or αij/αjj to compensate for other increases. This implies, in order to switch off neighborhood aggregation, we would need a high dimensional space of features that cater to switching off specific links without strengthening others. Furthermore, they would need large absolute values of a[f] and norms of W[f, :] or exploding feature vectors h to achieve this. Yet, all these norms are constrained by the derived conservation law and therefore prevent learning a representation that switches off full neighborhoods. GATs without weight sharing: The flow of argumentation without weight sharing is very similar to the one above with weight sharing. Yet, we have to distinguish more cases. 13GATE: How to Keep Out Intrusive Neighbors Similarly to before, we require αij/αjj << 1 and αji/αii << 1. It follows from adding both related inequalities that aT [ϕ (Wshi + Wthj) + ϕ (Wshj + Wthi) − ϕ ((Ws + Wt) hi) − ϕ ((Ws + Wt) hj)] << 0. This implies that for at least one feature f, we require a[f][ϕ \u0000 Ws[f, :]hi + Wt[f, :]hj \u0001 + ϕ \u0000 Ws[f, :]hj + Wt[f, :]hi \u0001 − ϕ ((Ws[f, :] + Wt[f, :]) hi) − ϕ ((Ws[f, :] + Wt[f, :]) hj)] << 0. (9) Again, our goal is to show that this feature automatically decreases the contribution of one feature while it increases another. As argued above, switching off neighborhood aggregation would therefore need a high dimensional space of features that cater to switching off specific links without strengthening others. Furthermore, they would need large absolute values of a[f] and norms of W[f, :] or exploding feature vectors h to achieve this. Our derived norm constraints, however, prevent learning such a model representation. Concretely, without loss of generality, we therefore have to show that if a[f][ϕ \u0000 Ws[f, :]hi + Wt[f, :]hj \u0001 − ϕ \u0000\u0000 Ws[f, :] + Wt[f, :] \u0001 hj \u0001 < 0, (10) at the same time, we receive a[f][ϕ \u0000 Ws[f, :]hj + Wt[f, :]hi \u0001 − ϕ \u0000\u0000 Ws[f, :] + Wt[f, :] \u0001 hi \u0001 > 0, (11) (or vice versa). In principle, we have to show this for 16 different cases of pre-activation sign configurations for the four terms in Eq. (9). Yet, since the argument is symmetric with respect to exchanging i and j, only 8 different cases remain. Two trivial cases are identical signs for all four terms. These are excluded, as the left hand side (LHS) of Eq. (9) would become zero and thus not contribute to our goal to switch off neighborhood aggregation. In the following, we will discuss the remaining six cases. Please note that for the remainder of this derivation α >0 denotes the slope of the leakyReLU and not the attention weights αij. 1. Case (+ − ++): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi > 0, and (Ws[f, :] + Wt[f, :]) hj > 0. From this assumption and the fact that ϕ is a leakyReLU it follows that the LHS of Eq. (9) be- comes: a[f][ϕ \u0000 Ws[f, :]hi + Wt[f, :]hj \u0001 + ϕ \u0000 Ws[f, :]hj + Wt[f, :]hi \u0001 − ϕ \u0000\u0000 Ws[f, :] + Wt[f, :] \u0001 hi \u0001 − ϕ \u0000\u0000 Ws[f, :] + Wt[f, :] \u0001 hj \u0001 ] = a[f](α −1)[Ws[f, :]hj + Wt[f, :]hi]. Since α −1 < 0 and [Ws[f, :]hj + Wt[f, : ]hi] < 0 according to our assumption, Eq. (9) demands a[f] < 0. To switch off neighborhood aggregation, we would need to be able to make the LHS of Eq. (10) and Eq. (11) Eq. (11) negative. Yet, a negative a[f] leads to a positive LHS of Eq. (11). Thus, the assumed sign configuration cannot support switching off neighborhood aggregation. 2. Case (+ − −−): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi < 0, and (Ws[f, :] + Wt[f, :]) hj < 0. The LHS of Eq. (9) becomes a[f](1 −α)[Ws[f, :]hi + Wt[f, :]hj], which demands a[f] < 0. Accordingly, the LHS of Eq. (10) is clearly negative, while the LHS of Eq. (11) is a[f]αWs[f, :](hj − hi) > 0. The last inequality follows from our assumptions that implyWs[f, :]hj < Ws[f, :]hi by combining the assumptions (Ws[f, :] + Wt[f, :]) hj < 0 and Ws[f, :]hi + Wt[f, :]hj > 0. Again, this result implies that the considered sign configuration does not support switching off neighborhood aggregation. 3. Case (+ + + −): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi > 0, (Ws[f, :] + Wt[f, :]) hi > 0, and (Ws[f, :] + Wt[f, :]) hj < 0. The LHS of Eq. (9) becomes a[f](1 −α)[Ws[f, :]hj + Wt[f, :]hj], which demands a[f] > 0. Accordingly, the LHS of Eq. (10) becomes positive, which hampers switching-off neighborhood aggregation as discussed. 14GATE: How to Keep Out Intrusive Neighbors 4. Case (− − −+): Let us assume that Ws[f, :]hi + Wt[f, :]hj < 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi < 0, and (Ws[f, :] + Wt[f, :]) hj > 0. The LHS of Eq. (9) becomes a[f](α −1)[Ws[f, :]hj + Wt[f, :]hj], which demands a[f] > 0. Accordingly, the LHS of Eq. (10) becomes clearly negative. However, the LHS of Eq. (11) is positive, asa[f]αWs[f, :](hj − hi) > 0. The last inequality follows from our assumptions that imply Ws[f, :]hj > Ws[f, :]hi by combining the assumptions (Ws[f, :] + Wt[f, :]) hj > 0 and Ws[f, :]hi + Wt[f, :]hj < 0. Again, this analysis implies that the considered sign configuration does not support switching off neighborhood aggregation. 5. Case (+ − +−): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi > 0, and (Ws[f, :] + Wt[f, :]) hj < 0. According to our assumptions the LHS of Eq. (10) can only be negative if a[f] < 0. Yet, the LHS of Eq. (11) can only be negative if a[f] > 0. Thus, this case clearly cannot contribute to switching off neighborhood aggregation. 6. Case (+ − −+): Let us assume that Ws[f, :]hi + Wt[f, :]hj > 0, Ws[f, :]hj + Wt[f, :]hi < 0, (Ws[f, :] + Wt[f, :]) hi < 0, and (Ws[f, :] + Wt[f, :]) hj > 0. Eq. (9) becomes a[f](1 − α)Ws[f, :] (hi − hj) < 0. At the same time, the LHS of Eq. (10) simplifies to a[f]Ws[f, : ](hi − hj) and the LHS of Eq. (11) is a[f]αWs[f, :](hj − hi) > 0. Hence, a negative Eq. (9) leads to a positive Eq. (11). Accordingly, the last possible sign configuration also does not support switching off neighborhood aggregation, which concludes our derivation. A.2. Proof of Theorem 4.3 Statement (Restated Theorem 4.3). The gradients and parameters of GATE for layer l ∈ [L − 1] are conserved according to the following laws: ⟨Wl[i, :], ∇Wl[i,:]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ + ⟨al+1 s [i], ∇al+1 s [i]L⟩ + ⟨al+1 t [i], ∇al+1 t [i]L⟩. (12) and, if additional independent matrices Ul and Vl are trainable, it also holds ⟨al s[i], ∇als[i]L⟩ + ⟨al t[i], ∇al t[i]L⟩ = ⟨Ul[i, :], ∇Ul[i,:]L⟩ + ⟨V l[i, :], ∇V l[i,:]L⟩. (13) The proof is analogous to the derivation of Theorem 2.2 by (Mustafa & Burkholz, 2023) that is restated in this work as Theorem 4.1. For ease, we replicate their notation and definitions here. Statement (Rescale invariance: Def 5.1 by Mustafa & Burkholz (2023)). The loss L(θ) is rescale-invariant with respect to disjoint subsets of the parametersθ1 and θ2 if for everyλ >0 we haveL(θ) = L((λθ1, λ−1θ2, θd)), where θ = (θ1, θ2, θd). Statement (Gradient structure due to rescale invariance Lemma 5.2 in (Mustafa & Burkholz, 2023)). The rescale invariance of L enforces the following geometric constraint on the gradients of the loss with respect to its parameters: ⟨θ1, ∇θ1L⟩ − ⟨θ2, ∇θ2L⟩ = 0. (14) We first consider the simpler case of GATES, i.e. W = U = V Theorem A.1 (Structure of GATES gradients). The gradients and parameters of GATES for layer l ∈ [L−1] are conserved according to the following laws: ⟨Wl[i, :], ∇Wl[i,:]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ + ⟨al s[i], ∇als[i]L⟩ + ⟨al t[i], ∇al t[i]L⟩. (15) Following a similar strategy to (Mustafa & Burkholz, 2023), we identify rescale invariances for every neuron i at layer l that induce the stated gradient structure. Given the following definition of disjoint subsets θ1 and θ2 of the parameter set θ, associated with neuron i in layer l, 15GATE: How to Keep Out Intrusive Neighbors θ1 = {x|x ∈ Wl[i, :]} θ2 = {w|w ∈ Wl+1[:, i]} ∪ {al s[i]} ∪ {al t[i]} We show that the loss of GATES remains invariant for any λ >0. The only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and αl uv. The scaled network parameters are denoted with a tilde as ˜as l[i] = λ−1al s[i], ˜at l[i] = λ−1al t[i], and ˜Wl[i, j] = λWl[i, j], and the corresponding networks components scaled as a result are denoted by ˜hl u[i], ˜hl+1 v [k], and ˜αl uv. We show that the parameters of upper layers remain unaffected, as ˜hl+1 v [k] coincides with its original non-scaled variant ˜hl+1 v [k] = hl+1 v [k]. Also recall Eq. (4) for W = U = V as: el uv = ((1 − quv)al s + (quv)al t)⊤ · ϕ(Wlhl−1 u + Wlhl−1 v ) where quv = 1 if u = v and quv = 0 if u ̸= v. For simplicity, we rewrite this as: el uv,u̸=v = (al s)⊤ · ϕ(Wlhl−1 u + Wlhl−1 v ) (16) el uv,u=v = (al t)⊤ · ϕ(Wlhl−1 u + Wlhl−1 v ) (17) We show that ˜αl uv = exp(˜el uv)P u′∈N(v) exp(˜eluv) = αl uv , because (18) ˜el uv,u̸=v = el uv,u̸=v , and ˜el uv,u=v = el uv,u=v (19) which follows from the positive homogeneity of ϕ that allows ˜el uv,u=v = λ−1al s[i]ϕ( nl−1X j λWl[i, j](hl−1 u [j] + hl−1 v [j]) + nlX i′̸=i al s[i′]ϕ( nl−1X j Wl[i′, j](hl−1 u [j] + hl−1 v [j]) (20) = λ−1λal s[i]ϕ( nl−1X j Wl[i, j](hl−1 u [j] + hl−1 v [j]) + nlX i′̸=i al s[i′]ϕ( nl−1X j Wl[i′, j](hl−1 u [j] + hl−1 v [j]) (21) = el uv,u̸=v. (22) and similarly, 16GATE: How to Keep Out Intrusive Neighbors ˜el uv,u=v = λ−1al t[i]ϕ( nl−1X j λWl[i, j](hl−1 u [j] + hl−1 v [j]) + nlX i′̸=i al t[i′]ϕ( nl−1X j Wl[i′, j](hl−1 u [j] + hl−1 v [j]) (23) = λ−1λal t[i]ϕ( nl−1X j Wl[i, j](hl−1 u [j] + hl−1 v [j]) + nlX i′̸=i al t[i′]ϕ( nl−1X j Wl[i′, j](hl−1 u [j] + hl−1 v [j]) (24) = el uv,u=v. (25) Since ˜αl uv = αl uv, it follows that ˜hu l [i] = ϕ1   X z∈N(u) αl zu nl−1X j λWl[i, j]hl−1 z [j]   = λϕ1   X z∈N(u) αl zu nl−1X j Wl[i, j]hl−1 z [j]   = λhl u[i]. In the next layer, we therefore have ˜hl+1 v [k] = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]˜hl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]λhl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i Wl+1[k, i]hl u[i]   = hl+1 v [k]. Thus, the output node representations of the network remain unchanged, and the loss L is rescale-invariant. Next consider the case that Wl, Ul, and V l are independent matrices. Similarly to the previous reasoning, we see that if we scale ˜Wl[i, :] = Wl[i, :]λ, then also scaling ˜Wl+1[:, i] = Wl+1[:, i]λ−1 and ˜al+1 s [i] = al+1 s [i]λ−1 and ˜al+1 t [i] = al+1 t [i]λ−1 will keep the GATE layer unaltered. In this case, we obtain an additional rescaling relationship between al s, al t and Ul, V l. A rescaling of the form ˜as l[i] = λ−1al s[i], ˜at l[i] = λ−1al t[i] could be compensated by ˜Ul[i, :] = Ul[i, :]λ and ˜V l[i, :] = V l[i, :]λ. It follows immediately that ˜euv = euv. A.3. Derivation of Insight 4.4 Following the analysis in A.1, in contrast to GAT, αij/αii << 1 can be easily realized in GATE with as[f] < 0 and at[f] > 0 for all or only a subset of the features. Note that for the non-weight-sharing case, U and V in GATE would simply correspond to Ws and Wt, respectively, in GATE and the same line of reasoning holds. Large norms are usually not required to create a notable difference in size between eii and eij. 17GATE: How to Keep Out Intrusive Neighbors B. Experimental Settings Our complete experimental setup is described as follows. Non-linearity For GATS and GAT networks, we substitute ϕ in Eq. (3) with LeakyReLU as defined in the standard architecture. For GATE, we substitute ϕ in Eq. (4) with ReLU in order to be able to interpret the sign of as and at parameters as contributing positively or negatively to neighborhood aggregation. MLP, MLP +GAT , and FAGCN also all use ReLU after every hidden layer. Network Width We vary the depth of GAT and GATE networks in all our experiments as specified. For synthetic datasets, the network width is fixed to 64 in all cases. For OGB datasets, we use the hidden dimensions used by (Brody et al., 2022). For the remaining datasets, the network width is also fixed to 64. Initialization The feature transformation parameter matrices, i.e.,W, U, and V are initialized randomly with an orthog- onal looks-linear structure (Burkholz & Dubatovka, 2019) for MLP, MLP+GAT , GAT(S) and GATE(S). The parameters a in GAT(S) use Xavier initialization (Glorot & Bengio, 2010), as is the standard. In GATE(S), as and at are initialized to 0 to initially give equal weights to the features of a node itself and its neighboring nodes. Optimization Synthetic, OGB, and remaining real-world tasks are run for a maximum of 10000, 2000, 5000 epochs, respectively, using the Adam optimizer. To isolate the effect of the architecture and study the parameter dynamics during training as best as possible, we do not use any additional elements such as weight decay and dropout regularization. We also do not perform any hyperparameter optimization. However, the learning rate is adjusted for different real-world datasets to enable stable training of models as specified in Table 6. Nevertheless, for a fair comparison, the same learning rate is used for a given problem across all architectures. For all synthetic data, a learning rate of 0.005 is used. Real-world datasets use their standard train/test/validation splits, i.e. those provided by Pytorch Geometric for Planetoid datasets Cora and Citeseer, by OGB framework for OGB datasets, and by (Platonov et al., 2023) for all remaining real-world datasets. Code Our experimental code and synthetic data generators are available at https://github.com/ RelationalML/GATE.git. Table 6: Details of real-world datasets used in experiments. dataset # nodes # edges # features # classes learning rate used for L layer networks ogb-arxiv 169, 343 2 , 315, 598 128 40 L = [12] : 0.001 ogb-products 2, 449, 029 123 , 718, 152 100 47 L = [8] : 0.001 ogb-mag 736, 389 10 , 792, 672 128 349 L = [5] : 0.005 roman-empire 22, 662 32 , 927 300 18 L = [5] : 0.001, L= [10] : 0.0005 amazon-ratings 24, 492 93 , 050 300 5 L = [5] : 0.001, L= [10] : 0.0005 questions 48, 921 153 , 540 301 2 L = [5] : 0.001, L= [10] : 0.0005 minesweeper 10, 000 39 , 402 7 2 L = [5] : 0.001, L= [10] : 0.0005 tolokers 11, 758 519 , 000 10 2 L = [5] : 0.001, L= [10] : 0.0005 cora 2, 708 10 , 556 1 , 433 7 L = [2, 5] : 0.005, L= [10, 20] : 0.0005 citeseer 3, 327 9 , 104 3 , 703 6 L = [2, 5] : 0.001, L= [10, 20] : 0.0001 actor 7, 600 26 , 659 932 5 L = [2, 5, 10, 20] : 0.005 texas 183 279 1 , 703 5 L = [2, 5] : 0.01, L= [10, 20] : 0.0005 wisconsin 251 450 1 , 703 5 L = [2, 5] : 0.01, L= [10, 20] : 0.005 18GATE: How to Keep Out Intrusive Neighbors C. Additional Results Smaller Real-World Datasets We evaluate GAT and GATE on five small-scale real-world datasets with varying ho- mophily levels β as defined in (Pei et al., 2020) and report results in Table 7. Higher values ofβ indicate higher homophily, i.e. similar nodes (with the same label) tend to be connected. We note that a 2-layer network of a baseline method for het- erophilic datasets, Geom-GCN (Pei et al., 2020), attains test accuracy (%) of64.1, 67.6, and 31.6 for Wisconsin, Texas, and Actor datasets, respectively, which is in line with that achieved by GATE. Except for Citeseer, the best overall performance for each dataset is achieved on a shallow model. This is not surprising as these datasets are small-scale and potentially prone to over-fitting in large models, particularly since we do not use any skip connections or regularization to retain model performance. Furthermore, the three heterophilic datasets have been recently shown to be problematic (Platonov et al., 2023). Therefore, a better evaluation of GATE is on relatively large-scale OGB datasets (Hu et al., 2021) and more recent heterophilic datasets (Platonov et al., 2023) that can exploit the flexibility of GATE. Although GATE is more pa- rameterized than GAT, it usually requires fewer training epochs and generalizes better, in addition to other advantages over GAT as discussed in the paper. Table 7: Test accuracy (%) of GAT and GATE models for network depthL on small-scale real-world datasets with varying homophily levels β. Entries marked with * indicate models that achieve 100% training accuracy and stable test accuracy. Otherwise, test accuracy at max. validation accuracy is reported. Data β L = 2 L = 5 L = 10 L = 20 GAT GATE S GATE GAT GATE S GATE GAT GATE S GATE GAT GATE S GATE Wisc. 0.21 62 .7* 80.4 70.5* 51.0 70.5 60.7* 45.1 62.7 58.8 47 .1 62.7 60.7 Texas 0.11 56 .7* 67.6* 67.6* 51.4 67.6* 67.6* 56.7* 62.2* 62.3* 59.4* 62.1* 64.9 Actor 0.24 27 .1 32.2 31.6 25 .4 27 .5 29.2 25.3 27 .4 27.9 24.5 24 .6 29.4 Cora 0.83 80 .0 81.0* 80.8 79 .8 80.8* 80.4 77 .6 80.0* 79.2 77 .7 77 .2* 79.0 Cite. 0.71 68 .0 67 .6* 68.3 67.2 68.7* 67.8 66 .9 67.6* 67.6 68.2 67 .1* 69.2 Initialization of attention parameters in GAT We show in Fig. 7 that setting the initial value of attention parameters as and at in GATE to zero is, in fact, not what enables neighborhood aggregation but rather the separation ofa into as and at as discussed in Insight 4.4. Figure 7: Distribution of αvv against training epoch for self-sufficient learning problem using the Cora structure with random labels, where input node features are a one-hot encoding of node labels for GAT with attention parameters a initialized to zero. Left to right:1, 2 and 5 layer models that achieve test accuracy of100%, 52.7%, and 36.2%, respectively, which is similar to the results obtained by standard Xavier initialization of attention parameters in GAT. Further analysis of experiments We present the analysis of α coefficients learned for some experiments in the main paper that were deferred to the appendix due to space limitations. 19GATE: How to Keep Out Intrusive Neighbors (a) GAT with original labels  (b) GAT with random labels (c) GATE with original labels  (d) GATE with random labels Figure 8: Distribution of αvv against training epoch for self-sufficient learning problem using Cora structure and input node features as the one-hot encoding of labels for 5 layer models. (a) original labels (b) Random labels Figure 9: Distribution of αvv against training epoch for the self-sufficient learning problem using Cora graph structure with original (top) and random (bottom) node labels and input node features as a one-hot encoding of labels. Left to right: 1, 2, and 5 layer GATES models that all 100% test accuracy except in the case of 5 layer model using original labels. In this case, although a training accuracy if 100% is achieved at 32 epochs with test accuracy 97.3%, a maximum test accuracy of 98.4% is reached at 7257 epochs. Training the model to run to 15000 epochs only increases it to 98.4%. An increased learning rate did not improve this case. However, we also run the GAT model for 15000 epochs for this case, and it achieves 85.9% test accuracy at epoch 47 where the model achieves 100% accuracy and only achieves a maximum test accuracy of 89.3% briefly at epoch 8. 20GATE: How to Keep Out Intrusive Neighbors Figure 10: Distribution of αvv against training epoch for the neighbor-dependent learning problem withk = 1. Rows: GAT (top) and GATE (bottom) architecture. Columns (left to right): 1, 2, and 3 layer models. While GAT is unable to switch off neighborhood aggregation in any layer, only 1 layer of the 2 and 3 layer models perform neighborhood aggregation. Table 8: Neighbor-dependent learning: k and L denote the number of aggregation steps of the random GAT used for label generation and the number of layers of the evaluated network, respectively. Entries marked with * identify models where 100% train accuracy is not achieved. Underlined entries identify the model with the highest train accuracy at the epoch of max. test accuracy. This provides an insight into how similar the function represented by the trained model is to the function used to generate node labels, i.e. whether the model is simply overfitting to the train data or really learning the task. Higher training and test accuracy simultaneously indicate better learning. In this regard, the difference in train accuracy at max. test accuracy between GATE and GATS or GAT is only 0.4, 1.0 and 0.6 for the settings (k = 1, L= 3), (k = 2, L= 4) and (k = 3, L= 3), respectively. k L Test Acc. @ Epoch of Max. Train Acc. Max Test Acc. @ Epoch GATS GAT GATE GAT S GAT GATE 1 1 92.0@2082* 91.2@6830* 93.2@3712* 93.2@1421 92.0@9564 93.6@3511 2 89.6@8524* 88.0@8935 91.2@942 91.6@5188 92.8@4198 95.6@111 3 86.4@9180* 88.8@997 92.8@618 91.2@6994 92.8@437 97.2@82 2 2 88.8@6736* 89.6@3907 88.8@467 93.2@151 93.2@95 92.0@105 3 82.0@7612 89.2@1950 91.6@370 91.6@1108 93.2@856 95.2@189 4 84.8@4898 82.4@739 87.2@639 88.0@1744 88.4@423 90.4@447 3 3 80.8@8670 80.4@737 85.2@391 86.4@1578 88.8@285 92.0@47 4 78.0@3012 80.4@767 89.6@480 86.8@1762 85.6@469 91.6@139 5 80.0@6611 74.4@1701 86.0@447 85.6@921 83.6@1098 91.2@243 21GATE: How to Keep Out Intrusive Neighbors (a) Tolokers, GAT:61.6% test AUROC.  (b) Tolokers, GATE:69.2% test AUROC. Figure 11: Distribution of αvv against training epoch for one run of 10 layer networks on real-world heterophilic task. Figure 12: Distribution of αvv against training epoch of 2-layer (left) and 5-layer (right) GAT networks for heterophilic datasets Texas (top) and Actor (bottom) 2-layer modes. Despite having connections to unrelated neighbors, GAT is unable to switch off neighborhood aggregation. Figure 13: Distribution of αvv, against training epoch of 2-layer (left) and 5-layer (right) GATE networks for heterophilic datasets Texas (top) and Actor (bottom), across layers could be interpreted to indicate the inherent importance of raw node features relative to their neighborhoods. For instance, in the case of Texas, GATE carries out little to no neighborhood aggregation in the first layer over input node features. Instead, aggregation is mainly done over node features transformed in earlier layers that effectuate non-linear feature learning as in perceptrons. However, in the case of Actor, GATE prefers most of the neighborhood aggregation to occur over the input node features, indicating that they are more informative for the task at hand. 22GATE: How to Keep Out Intrusive Neighbors Over-smoothing analysis In the main paper, we have already established the superior performance of GATE, compared to GAT, on several tasks. Intuitively, this can partially be attributed to reduced over-smoothing as its root cause, unnecessary neighborhood aggregation, is alleviated. Here, we verify this insight quantitatively. A widely accepted measure of over-smoothing is the Dirichlet energy (DE) (Cai & Wang, 2020). However, Eliasof et al. (2023) propose a modification of DE to measure GAT energy EGAT , that we use to evaluate over-smoothing in our experiments (see Table 9). We note that the notion of ‘over-smoothing’ is itself task-dependent. It is difficult to determine the optimal degree of smoothing for a task and the threshold that determines ‘over’-smoothing. This merits an in-depth analysis and curation of task-dependent smoothness measures that are not our focus. To show that GATE reduced over- smoothing relative to GAT, it suffices that a decrease in smoothing and an increase in accuracy occur simultaneously. Table 9: The measures Einput, EGAT and EGATE denote the smoothness of input node features, node features at the last layer L of the trained GAT and GATE models, respectively. Two cases are considered: All node pairs and only adjacent node pairs to measure smoothing at the global graph and local node level. Higher values indicate less smoothing. Node representations learned by GATE achieve higher test accuracy on all these tasks, as reported in the main paper, and are simultaneously less smooth than GAT in most cases, indicating that GATE potentially alleviates over-smoothing in GATs. (a) Synthetic self-sufficient task: varying graph structure and label distribution, node features as one-hot encoding of labels, L = 5. Experiment setting All node pairs Adjacent node pairs Einput EGAT EGATE Einput EGAT EGATE Cora, Original Labels 6.016 e + 06 1 .281 e + 08 2.037 e + 09 2.006 e + 03 7 .903 e + 03 1.971 e + 05 Cora, Random Labels 6.283 e + 06 3 .472 e + 09 3.747 e + 09 9.080 e + 03 9 .306 e + 05 3.966 e + 06 ER(p = .01), 2 Classes 4.994 e + 05 2 .701 e + 06 4.478 e + 07 5.042 e + 03 2 .272 e + 04 3.229 e + 05 ER(p = .01), 8 Classes 8.745 e + 05 3 .350 e + 07 2.615 e + 08 8.694 e + 03 1 .762 e + 05 1.960 e + 06 (b) Synthetic neighbor-dependent task: graph structure, node features, generator parameter k, and label distribution as in Section 5. Experiment setting All node pairs Adjacent node pairs Einput EGAT EGATE Einput EGAT EGATE k = 1, L= 3 1 .953 e + 06 5.306 e + 07 5.095 e + 07 1 .957 e + 05 4.234 e + 05 3.610 e + 05 k = 2, L= 4 1 .975 e + 06 1 .193 e + 07 2.198 e + 07 2.012 e + 04 1 .016 e + 05 1.939 e + 05 k = 3, L= 5 1 .951 e + 06 1 .645 e + 07 1.053 e + 08 1.966 e + 04 1 .408 e + 05 9.096 e + 05 (c) Real-world tasks. Dataset L All node pairs Adjacent node pairs Einput EGAT EGATE Einput EGAT EGATE roman-empire 5 1 .274 e + 09 1 .002 e + 11 7.491 e + 11 7.878 e + 04 2 .441 e + 06 4.009 e + 07 amazon-ratings 10 3 .844 e + 08 1 .187 e + 10 2.272 e + 10 4.933 e + 04 3 .848 e + 05 7.430 e + 05 minesweeper 5 6 .869 e + 07 1 .386 e + 09 2.531 e + 10 2.628 e + 04 1 .946 e + 05 7.017 e + 06 tolokers 10 1 .391 e + 08 1 .044 e + 11 1.042 e + 11 3.423 e + 05 1 .249 e + 08 1.397 e + 08 cora 10 5 .088 e + 05 1 .437 e + 07 2.783 e + 08 6.490 e + 02 5 .959 e + 02 1.226 e + 04 citeseer 10 3 .463 e + 05 2 .916 e + 05 1.126 e + 07 2.360 e + 02 2 .426 e + 00 1.030 e + 02 texas 10 1 .945 e + 06 3 .280 e + 04 3.877 e + 04 1.758 e + 04 8 .487 e + 01 9.695 e + 01 actor 10 2 .612 e + 08 1.800 e + 07 1.364 e + 07 1 .237 e + 05 2.810 e + 03 2.215 e + 03 wisconsin 10 4 .438 e + 06 1 .057 e + 07 1.008 e + 08 3.299 e + 04 3 .765 e + 04 7.363 e + 08 23GATE: How to Keep Out Intrusive Neighbors Comparison with other GNNs Other GNN architectures could potentially switch off neighborhood aggregation, as we show here. However, they are less flexible in assigning different importance to neighbors, suffer from over-smoothing, or come at the cost of an increased parameter count by increasing the size of the hidden dimensions (e.g. via a concatena- tion operation). We evaluate the performance of three such architectures that, in principle, employ different aggregation methods, which are likely to be capable of switching off neighborhood aggregation, on synthetic datasets empirically and discuss their ability or inability to switch off neighborhood aggregation qualitatively as follows. 1. ωGAT (Eliasof et al., 2023) introduces an additional feature-wise layer parameter ω that can, in principle, switch off neighborhood aggregation by setting ω parameters to 0, in addition to the attention mechanism based on GAT. How- ever, in practice, as we verify on our synthetic dataset in Figure 14, it is unable to effectively switch off neighborhood aggregation. Although it outperforms GAT, it is still substantially worse than GATE, especially for the deeper model due to unnecessary neighborhood aggregations. Another architecture based on graph attention, superGAT(Kim & Oh, 2021), falls under the paradigm of structural learning as it uses a self-supervised attention mechanism essentially for link prediction between nodes, and therefore its comparison with GATE is infeasible. 2. GraphSAGE (Hamilton et al., 2018) uses the concatenation operation to combine the node’s own representation with the aggregated neighborhood representation. Therefore, it is usually (but not always) able to switch off the neigh- borhood aggregation for the synthetic datasets designed for the self-sufficient learning task (see Table 10). Mostly, GATE performs better on the neighbor-dependent task, in particular for deeper models, where the performance of GraphSAGE drops likely due to over-smoothing (see Table 11). 3. FAGCN (Bo et al., 2021) requires a slightly more detailed analysis. Authors of FAGCN state in the paper that: ‘When αG ij ≈ 0, the contributions of neighbors will be limited, so the raw features will dominate the node representations.’ where αG ij defined in the paper can be considered analogous to αij in GAT, though they are defined differently. Thus, from an expressivity point of view, FAGCN should be able to assign parameters such that all αG ij = 0. However, we empirically observe on synthetic datasets designed for the self-sufficient learning task, values of αG ij do not, in fact, approach zero. Despite being unable to switch off neighborhood aggregation, FAGCN, in its default implementation, achieves 100% test accuracy on the task. We discover this is so because FAGCN introduces direct skip connections of non-linearly transformed raw node features to every hidden layer. Given the simplicity of the one-hot encoded features in the datasets and the complete dependence of the label on these features, FAGCN is able to represent the desired function. In order to better judge its ability to switch off neighborhood aggregation by setting αG ij = 0, we remove this skip connection. From an expressivity point of view, FAGCN should still be able to achieve 100% test accuracy by using only the (non-)linear transformations of raw features initially and performing no neighborhood aggregation in the hidden layers. However, we find that FAGCN was unable to emulate this behavior in practice. For a fair comparison of the differently designed attention mechanism in FAGCN with GATE, we introduce self-loops in the data so FAGCN may also receive a node’s own features in every hidden layer. Even then, FAGCN fails to achieve perfect test accuracy as shown in Table 10. Therefore, we suspect the attention mechanism in FAGCN may also be susceptible to the trainability issues we have identified for the attention mechanism in GAT. Nevertheless, the capacity of FAGCN to learn negative associations with neighboring nodes is complementary to GATE and both could be combined. It would be interesting to derive conservation laws inherent to other architectures such as FAGCN and GraphSAGE and study how they govern the behaviour of parameters. Furthermore, by design, FAGCN does not perform any non-linear transformations of aggregated neighborhood features which may be necessary in some tasks, such as our synthetic dataset for the neighbor-dependent learning task. As Table 11 shows, GATE outperforms FAGCN on such a task. Lastly, we would like to emphasize that our aim is to provide insights into the attention mechanism of GAT and understand its limitations. While it should be able to flexibly assign importance to neighbors and the node itself without the need for concatenated representation or explicit skip connections of the raw features to every layer, it is currently unable to do so in practice. In order to verify our identification of trainability issues, we modify the GAT architecture to enable the trainability of attention parameters which control the trade-off between node features and structural information. 24GATE: How to Keep Out Intrusive Neighbors Figure 14: Distribution of αvv against training epoch for self-sufficient learning problem using the Cora structure with random labels, where input node features are a one-hot encoding of node labels, for theωGAT architecture for the1, 2 and 5 layer models that achieve test accuracy of 100%, 98.5%, and 49.3%, respectively. Table 10: Self-sufficient learning: S, Cand L denote graph structure, number of label classes, and number of network layers, respectively. Original (Orig.) and Randomized (Rand.) labels are used for the Cora structure. The FAGCN model is implemented without skip connections from the input layer to every other layer and without any self-loops in input data, whereas FAGCN* denotes the model also without skip connections but with self-loops introduced for all nodes in input data. Structure C L Max. Test Accuracy (%) GAT GATE FAGCN FAGCN* SAGE Cora O,7 1 100 100 90.1 97 .6 100 2 94 .6 100 94.2 94 .9 98 .8 5 88 .5 99.7 87.1 89 .1 92 .4 R,7 1 100 100 61.6 97 .8 100 2 57 .0 100 69.2 70 .5 100 5 36 .7 100 21.2 36 .7 99 .6 ER (p = 0.01) R,2 1 100 100 100 100 100 2 100 100 100 100 100 5 99 .6 100 96.4 99 .2 100 R,8 1 99 .2 100 86.4 98 .8 100 2 97 .6 100 86.0 91 .6 100 5 38 .4 100 31.6 40 .4 100 25GATE: How to Keep Out Intrusive Neighbors Table 11: Neighbor-dependent learning: k and L denote the number of hops aggregated in the neighborhood to generate labels, and the number of layers of the evaluated network, respectively. k L Max Test Accuracy (%) @ Epoch GAT GATE SAGE FAGCN 1 1 92@9564 93.6 @ 3511 93.2@2370 93 .2@1618 2 92 .8@4198 95.6 @ 111 95.6@723 94 .1@1455 3 92 .8@437 97.2 @ 82 96.8@100 81 .2@573 2 2 93.2 @ 95 92.0@105 90 .8@199 90 .4@170 3 93 .2@856 95.2 @ 189 94.4@113 88 .8@283 4 88 .4@423 90 .4@447 92.4 @ 139 87.6@549 3 3 88 .8@285 92.0 @ 47 87.6@45 89 .2@528 4 85 .6@469 91.6 @ 139 88@60 89 .2@3191 5 83 .6@1098 91.2 @ 243 86.0@35 88 .8@205 26",
      "meta_data": {
        "arxiv_id": "2406.00418v2",
        "authors": [
          "Nimrah Mustafa",
          "Rebekka Burkholz"
        ],
        "published_date": "2024-06-01T12:31:15Z",
        "pdf_url": "https://arxiv.org/pdf/2406.00418v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper identifies and experimentally demonstrates a structural limitation of Graph Attention Networks (GATs): their inability to switch off task-irrelevant neighborhood aggregation, which leads to over-smoothing. To address this, the authors propose GATE, an extension of GAT that can flexibly switch neighborhood aggregation on and off as needed. GATE is shown to alleviate over-smoothing, benefit from deeper architectures by utilizing layers for non-linear feature transformations, and often outperforms GATs on real-world heterophilic datasets by down-weighting unrelated neighbors. It also offers interpretable learned self-attention coefficients. Furthermore, GATE achieves a new state-of-the-art test accuracy on the OGB-arxiv dataset. A synthetic test bed is also constructed to analyze a model's ability to utilize the appropriate amount of neighborhood aggregation.",
        "methodology": "The methodology builds on theoretical insights derived from a conservation law of GAT gradient flow dynamics, which explains why GATs struggle to switch off neighborhood aggregation due to norm constraints on their attention parameters (requiring a less trainable regime with large attention parameter norms). GATE modifies the GAT attention mechanism (Eq. 3 to Eq. 4) by introducing separate attention parameters, `a_s` (self-node) and `a_t` (neighbor), for the node's own features and its neighbors' contributions, allowing flexible weighting. This architectural change, supported by an updated conservation law for GATE gradients (Theorem 4.3), enables the model to switch off neighborhood aggregation in a well-trainable parameter regime. The attention mechanism uses ReLU as the non-linear activation function for interpretability, and attention parameters `a_s`, `a_t` are initialized to zero, while feature transformation matrices (`W`, `U`, `V`) use an orthogonal looks-linear structure.",
        "experimental_setup": "The evaluation includes both synthetic and real-world graphs. The synthetic test bed consists of two node classification problems: self-sufficient learning (label-relevant information only in node's own features) and neighbor-dependent learning (label-relevant information in k-hop neighbors' features). These use Erdős–Rényi (ER) graphs or Cora graph structures, with one-hot encoded or multivariate normal features. Real-world datasets include five heterophilic benchmarks (roman-empire, amazon-ratings, questions, minesweeper, tolokers) and three OGB datasets (OGB-arxiv, OGB-products, OGB-mag), along with smaller datasets (Cora, Citeseer, Actor, Texas, Wisconsin) with varying homophily. Baselines include GAT, MLP, MLP+GAT, FAGCN, and several other GNN architectures. Metrics are test accuracy and AUC-ROC. Models are trained using the Adam optimizer for up to 10000 epochs (synthetic), 2000 (OGB), or 5000 (other real-world), without weight decay or dropout regularization to isolate architectural effects. Learning rates are adjusted per dataset. Model depth and distribution of self-attention coefficients (α_vv) are analyzed, and over-smoothing is quantitatively assessed using a modified Dirichlet energy (GAT energy).",
        "limitations": "The paper highlights GAT's inherent structural limitation in inability to switch off task-irrelevant neighborhood aggregation. For GATE, the neighbor-dependent synthetic task proves challenging, and GATE does not achieve perfect 100% test accuracy, possibly due to a not crisply defined decision boundary. For smaller real-world datasets, deeper models are noted to be prone to overfitting without additional elements like skip connections or regularization. The authors also acknowledge that the notion of 'over-smoothing' is task-dependent and a universally optimal degree of smoothing is difficult to determine; their focus is on demonstrating a relative reduction in smoothing compared to GAT.",
        "future_research_directions": "The synthetic test bed developed in this research is suggested to be of independent interest for measuring progress in developing adaptive neighborhood aggregation schemes. The authors also propose that graph rewiring methods, which are complementary to GATE, could be combined with it. Another suggested direction is to derive conservation laws inherent to other GNN architectures, such as FAGCN and GraphSAGE, to better understand how they govern parameter behavior. A more in-depth analysis and curation of task-dependent smoothness measures are also implicitly suggested due to the current challenges in defining 'over-smoothing'."
      }
    },
    {
      "title": "GraphSAINT: Graph Sampling Based Inductive Learning Method",
      "abstract": "Graph Convolutional Networks (GCNs) are powerful models for learning\nrepresentations of attributed graphs. To scale GCNs to large graphs,\nstate-of-the-art methods use various layer sampling techniques to alleviate the\n\"neighbor explosion\" problem during minibatch training. We propose GraphSAINT,\na graph sampling based inductive learning method that improves training\nefficiency and accuracy in a fundamentally different way. By changing\nperspective, GraphSAINT constructs minibatches by sampling the training graph,\nrather than the nodes or edges across GCN layers. Each iteration, a complete\nGCN is built from the properly sampled subgraph. Thus, we ensure fixed number\nof well-connected nodes in all layers. We further propose normalization\ntechnique to eliminate bias, and sampling algorithms for variance reduction.\nImportantly, we can decouple the sampling from the forward and backward\npropagation, and extend GraphSAINT with many architecture variants (e.g., graph\nattention, jumping connection). GraphSAINT demonstrates superior performance in\nboth accuracy and training time on five large graphs, and achieves new\nstate-of-the-art F1 scores for PPI (0.995) and Reddit (0.970).",
      "full_text": "Published as a conference paper at ICLR 2020 GraphSAINT: G RAPH SAMPLING BASED INDUCTIVE LEARNING METHOD Hanqing Zeng∗ University of Southern California zengh@usc.edu Hongkuan Zhou∗ University of Southern California hongkuaz@usc.edu Ajitesh Srivastava University of Southern California ajiteshs@usc.edu Rajgopal Kannan US Army Research Lab rajgopal.kannan.civ@mail.mil Viktor Prasanna University of Southern California prasanna@usc.edu ABSTRACT Graph Convolutional Networks (GCNs) are powerful models for learning repre- sentations of attributed graphs. To scale GCNs to large graphs, state-of-the-art methods use variouslayer samplingtechniques to alleviate the “neighbor explosion” problem during minibatch training. We propose GraphSAINT, a graph sampling based inductive learning method that improves training efﬁciency and accuracy in a fundamentally different way. By changing perspective, GraphSAINT constructs minibatches by sampling the training graph, rather than the nodes or edges across GCN layers. Each iteration, a complete GCN is built from the properly sampled subgraph. Thus, we ensure ﬁxed number of well-connected nodes in all layers. We further propose normalization technique to eliminate bias, and sampling algorithms for variance reduction. Importantly, we can decouple the sampling from the for- ward and backward propagation, and extend GraphSAINT with many architecture variants (e.g., graph attention, jumping connection). GraphSAINT demonstrates superior performance in both accuracy and training time on ﬁve large graphs, and achieves new state-of-the-art F1 scores for PPI (0.995) and Reddit (0.970). 1 I NTRODUCTION Recently, representation learning on graphs has attracted much attention, since it greatly facilitates tasks such as classiﬁcation and clustering (Wu et al., 2019; Cai et al., 2017). Current works on Graph Convolutional Networks (GCNs) (Hamilton et al., 2017; Chen et al., 2018b; Gao et al., 2018; Huang et al., 2018; Chen et al., 2018a) mostly focus on shallow models (2 layers) on relatively small graphs. Scaling GCNs to larger datasets and deeper layers still requires fast alternate training methods. In a GCN, data to be gathered for one output node comes from its neighbors in the previous layer. Each of these neighbors in turn, gathers its output from the previous layer, and so on. Clearly, the deeper we back track, the more multi-hop neighbors to support the computation of the root. The number of support nodes (and thus the training time) potentially grows exponentially with the GCN depth. To mitigate such “neighbor explosion”, state-of-the-art methods use variouslayer sampling techniques. The works by Hamilton et al. (2017); Ying et al. (2018a); Chen et al. (2018a) ensure that only a small number of neighbors (typically from 2 to 50) are selected by one node in the next layer. Chen et al. (2018b) and Huang et al. (2018) further propose samplers to restrict the neighbor expansion factor to 1, by ensuring a ﬁxed sample size in all layers. While these methods signiﬁcantly speed up training, they face challenges in scalability, accuracy or computation complexity. ∗Equal contribution 1 arXiv:1907.04931v4  [cs.LG]  16 Feb 2020Published as a conference paper at ICLR 2020 Present work We present GraphSAINT (Graph SAmpling based INductive learning meThod) to efﬁciently train deep GCNs. GraphSAINT is developed from a fundamentally different way of minibatch construction. Instead of building a GCN on the full training graph and then sampling across the layers, we sample the training graph ﬁrst and then build a full GCN on the subgraph. Our method is thus graph samplingbased. Naturally, GraphSAINT resolves “neighbor explosion”, since every GCN of the minibatches is a small yet complete one. On the other hand, graph sampling based method also brings new challenges in training. Intuitively, nodes of higher inﬂuence on each other should have higher probability to form a subgraph. This enables the sampled nodes to “support” each other without going outside the minibatch. Unfortunately, such strategy results in non-identical node sampling probability, and introduces bias in the minibatch estimator. To address this issue, we develop normalization techniques so that the feature learning does not give preference to nodes more frequently sampled. To further improve training quality, we perform variance reduction analysis, and design light-weight sampling algorithms by quantifying “inﬂuence” of neighbors. Experiments on GraphSAINT using ﬁve large datasets show signiﬁcant performance gain in both training accuracy and time. We also demonstrate the ﬂexibility of GraphSAINT by integrating our minibatch method with popular GCN architectures such as JK-net (Xu et al., 2018) and GAT (Veliˇckovi´c et al., 2017). The resulting deep models achieve new state-of-the-art F1 scores on PPI (0.995) and Reddit (0.970). 2 R ELATED WORK A neural network model that extends convolution operation to the graph domain is ﬁrst proposed by Bruna et al. (2013). Further, Kipf & Welling (2016); Defferrard et al. (2016) speed up graph convolution computation with localized ﬁlters based on Chebyshev expansion. They target relatively small datasets and thus the training proceeds in full batch. In order to scale GCNs to large graphs, layer sampling techniques (Hamilton et al., 2017; Chen et al., 2018b; Ying et al., 2018a; Chen et al., 2018a; Gao et al., 2018; Huang et al., 2018) have been proposed for efﬁcient minibatch training. All of them follow the three meta steps: 1. Construct a complete GCN on the full training graph. 2. Sample nodes or edges of each layer to form minibatches. 3. Propagate forward and backward among the sampled GCN. Steps (2) and (3) proceed iteratively to update the weights via stochastic gradient descent. The layer sampling algorithm of GraphSAGE (Hamilton et al., 2017) performs uniform node sampling on the previous layer neighbors. It enforces a pre-deﬁned budget on the sample size, so as to bound the minibatch computation complexity. Ying et al. (2018a) enhances the layer sampler of Hamilton et al. (2017) by introducing an importance score to each neighbor. The algorithm presumably leads to less information loss due to weighted aggregation. S-GCN (Chen et al., 2018a) further restricts neighborhood size by requiring only two support nodes in the previous layer. The idea is to use the historical activations in the previous layer to avoid redundant re-evaluation. FastGCN (Chen et al., 2018b) performs sampling from another perspective. Instead of tracking down the inter-layer connections, node sampling is performed independently for each layer. It applies importance sampling to reduce variance, and results in constant sample size in all layers. However, the minibatches potentially become too sparse to achieve high accuracy. Huang et al. (2018) improves FastGCN by an additional sampling neural network. It ensures high accuracy, since sampling is conditioned on the selected nodes in the next layer. Signiﬁcant overhead may be incurred due to the expensive sampling algorithm and the extra sampler parameters to be learned. Instead of sampling layers, the works of Zeng et al. (2018) and Chiang et al. (2019) build mini- batches from subgraphs. Zeng et al. (2018) proposes a speciﬁc graph sampling algorithm to ensure connectivity among minibatch nodes. They further present techniques to scale such training on shared-memory multi-core platforms. More recently, ClusterGCN (Chiang et al., 2019) proposes graph clustering based minibatch training. During pre-processing, the training graph is partitioned into densely connected clusters. During training, clusters are randomly selected to form minibatches, and intra-cluster edge connections remain unchanged. Similar to GraphSAINT, the works of Zeng et al. (2018) and Chiang et al. (2019) do not sample the layers and thus “neighbor explosion” is avoided. Unlike GraphSAINT, both works are heuristic based, and do not account for bias due to the unequal probability of each node / edge appearing in a minibatch. Another line of research focuses on improving model capacity. Applying attention on graphs, the architectures of Veliˇckovi´c et al. (2017); Zhang et al. (2018); Lu et al. (2019) better capture neighbor features by dynamically adjusting edge weights. Klicpera et al. (2018) combines PageRank with GCNs to enable efﬁcient information propagation from many hops away. To develop deeper models, 2Published as a conference paper at ICLR 2020 “skip-connection” is borrowed from CNNs (He et al., 2015; Huang et al., 2017) into the GCN context. In particular, JK-net Xu et al. (2018) demonstrates signiﬁcant accuracy improvement on GCNs with more than two layers. Note, however, that JK-net (Xu et al., 2018) follows the same sampling strategy as GraphSAGE (Hamilton et al., 2017). Thus, its training cost is high due to neighbor explosion. In addition, high order graph convolutional layers (Zhou, 2017; Lee et al., 2018; Abu-El-Haija et al., 2019) also help propagate long-distance features. With the numerous architectural variants developed, the question of how to train them efﬁciently via minibatches still remains to be answered. 3 P ROPOSED METHOD : GraphSAINT Graph sampling based method is motivated by the challenges in scalability (in terms of model depth and graph size). We analyze the bias (Section 3.2) and variance (Section 3.3) introduced by graph sampling, and thus, propose feasible sampling algorithms (Section 3.4). We show the applicability of GraphSAINT to other architectures, both conceptually (Section 4) and experimentally (Section 5.2). In the following, we deﬁne the problem of interest and the corresponding notations. A GCN learns representation of an un-directed, attributed graph G(V,E), where each node v ∈V has a length-f attribute xv. Let A be the adjacency matrix and ˜A be the normalized one (i.e., ˜A = D−1A, and D is the diagonal degree matrix). Let the dimension of layer-ℓinput activation be f(ℓ). The activation of node v is x(ℓ) v ∈Rf(ℓ) , and the weight matrix is W(ℓ) ∈Rf(ℓ)×f(ℓ+1) . Note that xv = x(1) v . Propagation rule of a layer is deﬁned as follows: x(ℓ+1) v = σ (∑ u∈V ˜Av,u ( W(ℓ) )T x(ℓ) u ) (1) where ˜Av,u is a scalar, taking an element of ˜A. And σ(·) is the activation function (e.g., ReLU). We use subscript “s” to denote parameterd of the sampled graph (e.g.,Gs, Vs, Es). GCNs can be applied under inductive and transductive settings. While GraphSAINT is applicable to both, in this paper, we focus on inductive learning. It has been shown that inductive learning is especially challenging (Hamilton et al., 2017) — during training, neither attributes nor connections of the test nodes are present. Thus, an inductive model has to generalize to completely unseen graphs. 3.1 M INIBATCH BY GRAPH SAMPLING 0 1 2 3 4 5 6 8 9 7 0 1 2 3 5 7 0 1 2 3 5 7 0 1 2 3 5 7 Gs = SAMPLE(G) Full GCN on Gs Figure 1: GraphSAINT training algorithm GraphSAINT follows the design philosophy of directly sampling the training graph G, rather than the corresponding GCN. Our goals are to 1. extract appropriately connected subgraphs so that little information is lost when propagating within the subgraphs, and 2. combine information of many subgraphs together so that the training process overall learns good representation of the full graph. Figure 1 and Algorithm 1 illustrate the training algorithm. Before training starts, we perform light-weight pre-processing on Gwith the given sampler SAMPLE. The pre-processing estimates the probability of a node v∈V and an edge e∈E being sampled by SAMPLE. Such probability is later used to normalize the subgraph neighbor aggregation and the minibatch loss (Section 3.2). Afterwards, 3Published as a conference paper at ICLR 2020 Algorithm 1GraphSAINT training algorithm Input: Training graph G(V,E,X); Labels Y ; Sampler SAMPLE; Output: GCN model with trained weights 1: Pre-processing: Setup SAMPLE parameters; Compute normalization coefﬁcients α, λ. 2: for each minibatch do 3: Gs (Vs,Es) ←Sampled sub-graph of Gaccording to SAMPLE 4: GCN construction on Gs. 5: {yv |v∈Vs}← Forward propagation of {xv |v∈Vs}, normalized by α 6: Backward propagation from λ-normalized loss L(yv,yv). Update weights. 7: end for training proceeds by iterative weight updates via SGD. Each iteration starts with an independently sampled Gs (where |Vs| ≪|V|). We then build a full GCN on Gs to generate embedding and calculate loss for every v∈Vs. In Algorithm 1, node representation is learned by performing node classiﬁcation in the supervised setting, and each training node vcomes with a ground truth label yv. Intuitively, there are two requirements for SAMPLE: 1. Nodes having high inﬂuence on each other should be sampled in the same subgraph. 2. Each edge should have non-negligible probability to be sampled. For requirement (1), an ideal SAMPLE would consider the joint information from node connections as well as attributes. However, the resulting algorithm may have high complexity as it would need to infer the relationships between features. For simplicity, we deﬁne “inﬂuence” from the graph connectivity perspective and design topology based samplers. Requirement (2) leads to better generalization since it enables the neural net to explore the full feature and label space. 3.2 N ORMALIZATION A sampler that preserves connectivity characteristic of Gwill almost inevitably introduce bias into minibatch estimation. In the following, we present normalization techniques to eliminate biases. Analysis of the complete multi-layer GCN is difﬁcult due to non-linear activations. Thus, we analyze the embedding of each layer independently. This is similar to the treatment of layers independently by prior work (Chen et al., 2018b; Huang et al., 2018). Consider a layer-(ℓ+ 1)node vand a layer-ℓ node u. If vis sampled (i.e., v∈Vs), we can compute the aggregated feature of vas: ζ(ℓ+1) v = ∑ u∈V ˜Av,u αu,v ( W(ℓ) )T x(ℓ) u 1 u|v = ∑ u∈V ˜Av,u αu,v ˜x(ℓ) u 1 u|v, (2) where ˜x(ℓ) u = ( W(ℓ))T x(ℓ) u , and 1 u|v ∈{0,1}is the indicator function given vis in the subgraph (i.e., 1 u|v = 0if v∈Vs ∧(u,v) ̸∈Es; 1 u|v = 1if (u,v) ∈Es; 1 u|v not deﬁned if v̸∈Vs). We refer to the constant αu,v as aggregator normalization. Deﬁne pu,v = pv,u as the probability of an edge (u,v) ∈E being sampled in a subgraph, and pv as the probability of a node v∈V being sampled. Proposition 3.1. ζ(ℓ+1) v is an unbiased estimator of the aggregation ofvin the full(ℓ+ 1)th GCN layer, ifαu,v = pu,v pv . i.e.,E ( ζ(ℓ+1) v ) = ∑ u∈V ˜Av,u ˜x(ℓ) u . Assuming that each layer independently learns an embedding, we use Proposition 3.1 to normalize feature propagation of each layer of the GCN built byGraphSAINT. Further, let Lv be the loss on vin the output layer. The minibatch loss is calculated as Lbatch = ∑ v∈Gs Lv/λv, where λv is a constant that we term loss normalization. We set λv = |V|· pv so that: E(Lbatch) = 1 |G| ∑ Gs∈G ∑ v∈Vs Lv λv = 1 |V| ∑ v∈V Lv. (3) Feature propagation within subgraphs thus requires normalization factors αand λ, which are com- puted based on the edge and node probability pu,v, pv. In the case of random node or random edge samplers, pu,v and pv can be derived analytically. For other samplers in general, closed form expression is hard to obtain. Thus, we perform pre-processing for estimation. Before training starts, 4Published as a conference paper at ICLR 2020 we run the sampler repeatedly to obtain a set of N subgraphs G. We setup a counter Cv and Cu,v for each v∈V and (u,v) ∈E, to count the number of times the node or edge appears in the subgraphs of G. Then we set αu,v = Cu,v Cv = Cv,u Cv and λv = Cv N . The subgraphs Gs ∈G can all be reused as minibatches during training. Thus, the overhead of pre-processing is small (see Appendix D.2). 3.3 V ARIANCE We derive samplers for variance reduction. Letebe the edge connectingu, v, and b(ℓ) e = ˜Av,u ˜x(ℓ−1) u + ˜Au,v ˜x(ℓ−1) v . It is desirable that variance of all estimators ζ(ℓ) v is small. With this objective, we deﬁne: ζ = ∑ ℓ ∑ v∈Gs ζ(ℓ) v pv = ∑ ℓ ∑ v,u ˜Av,u pvαu,v ˜x(ℓ) u 1 v1 u|v = ∑ ℓ ∑ e b(ℓ) e pe 1 (ℓ) e . (4) where 1 e = 1if e∈Es; 1 e = 0if e̸∈Es. And 1 v = 1if v∈Vs; 1 v = 0if v̸∈Vs. The factor pu in the ﬁrst equality is present so that ζis an unbiased estimator of the sum of all node aggregations at all layers: E(ζ) =∑ ℓ ∑ v∈VE ( ζ(ℓ) v ) . Note that 1 (ℓ) e = 1 e,∀ℓ, since once an edge is present in the sampled graph, it is present in all layers of our GCN. We deﬁne the optimal edge sampler to minimize variance for every dimension of ζ. We restrict ourselves to independent edge sampling. For each e∈E, we make independent decision on whether it should be in Gs or not. The probability of including eis pe. We further constrain ∑pe = m, so that the expected number of sampled edges equals to m. The budget mis a given sampling parameter. Theorem 3.2. Under independent edge sampling with budgetm, the optimal edge probabilities to minimize the sum of variance of eachζ’s dimension is given by:pe = m∑ e′ ∑ ℓ b(ℓ) e′  ∑ ℓ b(ℓ) e . To prove Theorem 3.2, we make use of the independence among graph edges, and the dependence among layer edges to obtain the covariance of 1 (ℓ) e . Then using the fact that sum of pe is a constant, we use the Cauchy-Schwarz inequality to derive the optimal pe. Details are in Appendix A. Note that calculating b(ℓ) e requires computing ˜x(ℓ−1) v , which increases the complexity of sampling. As a reasonable simpliﬁcation, we ignore ˜x(ℓ) v to make the edge probability dependent on the graph topology only. Therefore, we choose pe ∝ ˜Av,u + ˜Au,v = 1 deg(u) + 1 deg(v) . The derived optimal edge sampler agrees with the intuition in Section 3.1. If two nodes u, v are connected and they have few neighbors, then uand vare likely to be inﬂuential to each other. In this case, the edge probability pu,v = pv,u should be high. The above analysis on edge samplers also inspires us to design other samplers, which are presented in Section 3.4. Remark We can also apply the above edge sampler to perform layer sampling. Under the indepen- dent layer sampling assumption of Chen et al. (2018b), one would sample a connection ( u(ℓ),v(ℓ+1)) with probability p(ℓ) u,v ∝ 1 deg(u) + 1 deg(v) . For simplicity, assume a uniform degree graph (of degree d). Then p(ℓ) e = p. For an already sampled u(ℓ) to connect to layer ℓ+ 1, at least one of its edges has to be selected by the layer ℓ+ 1sampler. Clearly, the probability of an input layer node to “survive” the Lnumber of independent sampling process is ( 1 −(1 −p)d )L−1 . Such layer sampler potentially returns an overly sparse minibatch for L> 1. On the other hand, connectivity within a minibatch of GraphSAINT never drops with GCN depth. If an edge is present in layer ℓ, it is present in all layers. 3.4 S AMPLERS Based on the above variance analysis, we present several light-weight and efﬁcient samplers that GraphSAINT has integrated. Detailed sampling algorithms are listed in Appendix B. Random node sampler We sample |Vs|nodes from Vrandomly, according to a node probability distribution P(u) ∝ ˜A:,u  2 . This sampler is inspired by the layer sampler of Chen et al. (2018b). 5Published as a conference paper at ICLR 2020 Random edge sampler We perform edge sampling as described in Section 3.3. Random walk based samplersAnother way to analyze graph sampling based multi-layer GCN is to ignore activations. In such case, Llayers can be represented as a single layer with edge weights given by B = ˜AL. Following a similar approach as Section 3.3, if it were possible to pick pairs of nodes (whether or not they are directly connected in the original ˜A) independently, then we would set pu,v ∝Bu,v + Bv,u, where Bu,v can be interpreted as the probability of a random walk to start at uand end at v in Lhops (and Bv,u vice-versa). Even though it is not possible to sample a subgraph where such pairs of nodes are independently selected, we still consider a random walk sampler with walk length Las a good candidate for L-layer GCNs. There are numerous random walk based samplers proposed in the literature (Ribeiro & Towsley, 2010; Leskovec & Faloutsos, 2006; Hu & Lau, 2013; Li et al., 2015). In the experiments, we implement a regular random walk sampler (with rroot nodes selected uniformly at random and each walker goes hhops), and also a multi-dimensional random walk sampler deﬁned in Ribeiro & Towsley (2010). For all the above samplers, we return the subgraph induced from the sampled nodes. The induction step adds more connections into the subgraph, and empirically helps improve convergence. 4 D ISCUSSION Extensions GraphSAINT admits two orthogonal extensions. First, GraphSAINT can seamlessly integrate other graph samplers. Second, the idea of training by graph sampling is applicable to many GCN architecture variants: 1. Jumping knowledge(Xu et al., 2018): since our GCNs constructed during training are complete, applying skip connections to GraphSAINT is straightforward. On the other hand, for some layer sampling methods (Chen et al., 2018b; Huang et al., 2018), extra modiﬁcation to their samplers is required, since the jumping knowledge architecture requires layer-ℓ samples to be a subset of layer-(ℓ−1) samples∗. 2. Attention (Veliˇckovi´c et al., 2017; Fey, 2019; Zhang et al., 2018): while explicit variance reduction is hard due to the dynamically updated attention values, it is reasonable to apply attention within the subgraphs which are considered as representatives of the full graph. Our loss and aggregator normalizations are also applicable†. 3. Others: To support high order layers (Zhou, 2017; Lee et al., 2018; Abu-El-Haija et al., 2019) or even more complicated networks for the task of graph classiﬁcation (Ying et al., 2018b), we replace the full adjacency matrix A with the (normalized) one for the subgraph As to perform layer propagation. Comparison GraphSAINT enjoys: 1. high scalability and efﬁciency, 2. high accuracy, and 3. low training complexity. Point (1) is due to the signiﬁcantly reduced neighborhood size compared with Hamilton et al. (2017); Ying et al. (2018a); Chen et al. (2018a). Point (2) is due to the better inter- layer connectivity compared with Chen et al. (2018b), and unbiased minibatch estimator compared with Chiang et al. (2019). Point (3) is due to the simple and trivially parallelizable pre-processing compared with the sampling of Huang et al. (2018) and clustering of Chiang et al. (2019). 5 E XPERIMENTS Setup Experiments are under the inductive, supervised learning setting. We evaluate GraphSAINT on the following tasks: 1. classifying protein functions based on the interactions of human tissue proteins (PPI), 2. categorizing types of images based on the descriptions and common properties of online images (Flickr), 3. predicting communities of online posts based on user comments (Reddit), 4. categorizing types of businesses based on customer reviewers and friendship (Yelp), and 5. classifying product categories based on buyer reviewers and interactions (Amazon). For PPI, we use the small version for the two layer convergence comparison (Table 2 and Figure 2), since Hamilton et al. (2017) and Chen et al. (2018a) report accuracy for this version in their original papers. We use the large version for additional comparison with Chiang et al. (2019) to be consistent with its reported accuracy. All datasets follow “ﬁxed-partition” splits. Appendix C.2 includes further details. ∗The skip-connection design proposed by Huang et al. (2018) does not have such “subset” requirement, and thus is compatible with both graph sampling and layer sampling based methods. †When applying GraphSAINT to GAT (Veliˇckovi´c et al., 2017), we remove the softmax step which normalizes attention values within the same neighborhood, as suggested by Huang et al. (2018). See Appendix C.3. 6Published as a conference paper at ICLR 2020 Table 1: Dataset statistics (“m” stands formulti-class classiﬁcation, and “s” for single-class.) Dataset Nodes Edges Degree Feature Classes Train / Val / Test PPI 14,755 225,270 15 50 121 (m) 0.66 / 0.12 / 0.22 Flickr 89,250 899,756 10 500 7 (s) 0.50 / 0.25 / 0.25 Reddit 232,965 11,606,919 50 602 41 (s) 0.66 / 0.10 / 0.24 Yelp 716,847 6,977,410 10 300 100 (m) 0.75 / 0.10 / 0.15 Amazon 1,598,960 132,169,734 83 200 107 (m) 0.85 / 0.05 / 0.10 PPI (large version) 56,944 818,716 14 50 121 (m) 0.79 / 0.11 / 0.10 We open source GraphSAINT‡. We compare with six baselines: 1. vanilla GCN (Kipf & Welling, 2016), 2. GraphSAGE (Hamilton et al., 2017), 3. FastGCN (Chen et al., 2018b), 4. S-GCN (Chen et al., 2018a), 5. AS-GCN (Huang et al., 2018), and 6. ClusterGCN (Chiang et al., 2019). All baselines are executed with their ofﬁcially released code (see Appendix C.3 for downloadable URLs and commit numbers). Baselines and GraphSAINT are all implemented in Tensorﬂow with Python3. We run experiments on a NVIDIA Tesla P100 GPU (see Appendix C.1 for hardware speciﬁcation). 5.1 C OMPARISON WITH STATE-OF-THE -ART Table 2 and Figure 2 show the accuracy and convergence comparison of various methods. All results correspond to two-layer GCN models (for GraphSAGE, we use its mean aggregator). For a given dataset, we keep hidden dimension the same across all methods. We describe the detailed architecture and hyperparameter search procedure in Appendix C.3. The mean and conﬁdence interval of the accuracy values in Table 2 are measured by three runs under the same hyperparameters. The training time of Figure 2 excludes the time for data loading, pre-processing, validation set evaluation and model saving. Our pre-processing incurs little overhead in training time. See Appendix D.2 for cost of graph sampling. For GraphSAINT, we implement the graph samplers described in Section 3.4. In Table 2, “Node” stands for random node sampler; “Edge” stands for random edge sampler; “RW” stands for random walk sampler; “MRW” stands for multi-dimensional random walk sampler. Table 2: Comparison of test set F1-micro score with state-of-the-art methods Method PPI Flickr Reddit Yelp Amazon GCN 0.515 ±0.006 0.492 ±0.003 0.933 ±0.000 0.378 ±0.001 0.281 ±0.005 GraphSAGE 0.637 ±0.006 0.501 ±0.013 0.953 ±0.001 0.634 ±0.006 0.758 ±0.002 FastGCN 0.513 ±0.032 0.504 ±0.001 0.924 ±0.001 0.265 ±0.053 0.174 ±0.021 S-GCN 0.963 ±0.010 0.482 ±0.003 0.964 ±0.001 0.640 ±0.002 — ‡ AS-GCN 0.687 ±0.012 0.504 ±0.002 0.958 ±0.001 — ‡ — ‡ ClusterGCN 0.875 ±0.004 0.481 ±0.005 0.954 ±0.001 0.609 ±0.005 0.759 ±0.008 GraphSAINT-Node 0.960±0.001 0.507 ±0.001 0.962 ±0.001 0.641 ±0.000 0.782 ±0.004 GraphSAINT-Edge 0.981±0.007 0.510 ±0.002 0.966±0.001 0.653±0.003 0.807 ±0.001 GraphSAINT-RW 0.981±0.004 0.511±0.001 0.966±0.001 0.653±0.003 0.815±0.001 GraphSAINT-MRW 0.980±0.006 0.510 ±0.001 0.964 ±0.000 0.652 ±0.001 0.809 ±0.001 Table 3: Additional comparison with ClusterGCN (test set F1-micro score) PPI (large version) Reddit 2 ×512 5 ×2048 2 ×128 4 ×128 ClusterGCN 0.903 ±0.002 0.994 ±0.000 0.954 ±0.001 0.966 ±0.001 GraphSAINT 0.941±0.003 0.995±0.000 0.966±0.001 0.970±0.001 ‡Open sourced code: https://github.com/GraphSAINT/GraphSAINT ‡The codes throw runtime error on the large datasets (Yelp or Amazon). 7Published as a conference paper at ICLR 2020 0 20 40 600.4 0.6 0.8 1 Validation F1-micro PPI 0 10 20 30 40 0.44 0.46 0.48 0.5 0.52 Flickr 0 50 100 1500.9 0.92 0.94 0.96 0.98 Reddit 0 200 400 600 800 0.25 0.45 0.65 Yelp 0 200 400 0.2 0.4 0.6 0.8 Training time (second) Amazon GCN GraphSAGE FastGCN* S-GCN AS-GCN ClusterGCN GraphSAINT *: CPU execution time -RW Figure 2: Convergence curves of 2-layer models on GraphSAINT and baselines Clearly, with appropriate graph samplers, GraphSAINT achieves signiﬁcantly higher accuracy on all datasets. For GraphSAINT-Node, we use the same node probability as FastGCN. Thus, the accuracy improvement is mainly due to the switching from layer sampling to graph sampling (see “Remark” in Section 3.3). Compared with AS-GCN, GraphSAINT is signiﬁcantly faster. The sampler of AS-GCN is expensive to execute, making its overall training time even longer than vanilla GCN. We provide detailed computation complexity analysis on the sampler in Appendix D.2. For S-GCN on Reddit, it achieves similar accuracy as GraphSAINT, at the cost of over 9×longer training time. The released code of FastGCN only supports CPU execution, so its convergence curve is dashed. Table 3 presents additional comparison with ClusterGCN. We useL×f to specify the architecture, where Land f denote GCN depth and hidden dimension, respectively. The four architectures are the ones used in the original paper (Chiang et al., 2019). Again, GraphSAINT achieves signiﬁcant accuracy improvement. To train models with L> 2 often requires additional architectural tweaks. ClusterGCN uses its diagonal enhancement technique for the 5-layer PPI and 4-layer Reddit models. GraphSAINT uses jumping knowledge connection (Xu et al., 2018) for 4-layer Reddit. Evaluation on graph samplers From Table 2, random edge and random walk based samplers achieve higher accuracy than the random node sampler. Figure 3 presents sensitivity analysis on parameters of “RW”. We use the same hyperparameters (except the sampling parameters) and network architecture as those of the “RW” entries in Table 2. We ﬁx the length of each walker to2 (i.e., GCN depth), and vary the number of roots rfrom 250 to 2250. For PPI, increasing rfrom 250 to 750 signiﬁcantly improves accuracy. Overall, for all datasets, accuracy stabilizes beyond r= 750. 5.2 GraphSAINT ON ARCHITECTURE VARIANTS AND DEEP MODELS In Figure 4, we train a 2-layer and a 4-layer model of GAT (Veliˇckovi´c et al., 2017) and JK-net (Xu et al., 2018), by using minibatches of GraphSAGE and GraphSAINT respectively. On the two 4-layer architectures, GraphSAINT achieves two orders of magnitude speedup than GraphSAGE, indicating much better scalability on deep models. From accuracy perspective, 4-layer GAT-SAGE and JK- SAGE do not outperform the corresponding 2-layer versions, potentially due to the smoothening effect caused by the massive neighborhood size. On the other hand, with minibatches returned by our edge sampler, increasing model depth of JK-SAINT leads to noticeable accuracy improvement (from 0.966 of 2-layer to 0.970 of 4-layer). Appendix D.1 contains additional scalability results. 6 C ONCLUSION We have presented GraphSAINT, a graph sampling based training method for deep GCNs on large graphs. We have analyzed bias and variance of the minibatches deﬁned on subgraphs, and proposed 8Published as a conference paper at ICLR 2020 0 1,000 2,0000.4 0.6 0.8 1 Number of walkers Test F1-micro PPI Flickr Reddit Yelp Amazon Figure 3: Sensitivity analysis 100 102 104 0.93 0.94 0.95 0.96 0.97 Training time (second) Validation F1-micro GAT 100 102 104 JK-net GraphSAINT 2-layer GraphSAINT 4-layer GraphSAGE 2-layer GraphSAGE 4-layer Figure 4: GraphSAINT with JK-net and GAT (Reddit) normalization techniques and sampling algorithms to improve training quality. We have conducted extensive experiments to demonstrate the advantage of GraphSAINT in accuracy and training time. An interesting future direction is to develop distributed training algorithms using graph sampling based minibatches. After partitioning the training graph in distributed memory, sampling can be performed independently on each processor. Afterwards, training on the self-supportive subgraphs can signiﬁcantly reduce the system-level communication cost. To ensure the overall convergence quality, data shufﬂing strategy for the graph nodes and edges can be developed together with each speciﬁc graph sampler. Another direction is to perform algorithm-system co-optimization to accelerate the training of GraphSAINT on heterogeneous computing platforms (Zeng et al., 2018; Zeng & Prasanna, 2019). The resolution of “neighbor explosion” by GraphSAINT not only reduces the training computation complexity, but also improves hardware utilization by signiﬁcantly less data trafﬁc to the slow memory. In addition, task-level parallelization is easy since the light-weight graph sampling is completely decoupled from the GCN layer propagation. ACKNOWLEDGEMENT This material is based on work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract Number FA8750-17-C-0086 and National Science Foundation (NSF) under Contract Numbers CCF-1919289 and OAC-1911229. Any opinions, ﬁndings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reﬂect the views of DARPA or NSF. REFERENCES Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Hrayr Harutyunyan, Nazanin Alipourfard, Kristina Lerman, Greg Ver Steeg, and Aram Galstyan. Mixhop: Higher-order graph convolution architec- tures via sparsiﬁed neighborhood mixing. arXiv preprint arXiv:1905.00067, 2019. Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. CoRR, abs/1312.6203, 2013. URL http://arxiv.org/abs/ 1312.6203. HongYun Cai, Vincent W. Zheng, and Kevin Chen-Chuan Chang. A comprehensive survey of graph embedding: Problems, techniques and applications. CoRR, abs/1709.07604, 2017. URL http://arxiv.org/abs/1709.07604. Jianfei Chen, Jun Zhu, and Le Song. Stochastic training of graph convolutional networks with variance reduction. In ICML, pp. 941–949, 2018a. Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: Fast learning with graph convolutional networks via importance sampling. In International Conference on Learning Representations (ICLR), 2018b. Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster-gcn: An ef- ﬁcient algorithm for training deep and large graph convolutional networks. CoRR, abs/1905.07953, 2019. URL http://arxiv.org/abs/1905.07953. 9Published as a conference paper at ICLR 2020 Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral ﬁltering. In Advances in Neural Information Processing Systems, pp. 3844–3852, 2016. Matthias Fey. Just jump: Dynamic neighborhood aggregation in graph neural networks. CoRR, abs/1904.04849, 2019. URL http://arxiv.org/abs/1904.04849. Hongyang Gao, Zhengyang Wang, and Shuiwang Ji. Large-scale learnable graph convolutional networks. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD ’18, pp. 1416–1424, New York, NY , USA, 2018. ACM. ISBN 978-1-4503-5552-0. Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems 30, pp. 1024–1034. 2017. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CoRR, abs/1512.03385, 2015. URL http://arxiv.org/abs/1512.03385. Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735–1780, 1997. Pili Hu and Wing Cheong Lau. A survey and taxonomy of graph sampling. arXiv preprint arXiv:1308.5865, 2013. Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700–4708, 2017. Wenbing Huang, Tong Zhang, Yu Rong, and Junzhou Huang. Adaptive sampling towards fast graph representation learning. In Advances in Neural Information Processing Systems, pp. 4558–4567, 2018. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Thomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. CoRR, abs/1609.02907, 2016. URL http://arxiv.org/abs/1609.02907. Johannes Klicpera, Aleksandar Bojchevski, and Stephan Günnemann. Personalized embedding propa- gation: Combining neural networks on graphs with personalized pagerank. CoRR, abs/1810.05997, 2018. URL http://arxiv.org/abs/1810.05997. John Boaz Lee, Ryan A. Rossi, Xiangnan Kong, Sungchul Kim, Eunyee Koh, and Anup Rao. Higher- order graph convolutional networks. CoRR, abs/1809.07697, 2018. URL http://arxiv.org/ abs/1809.07697. Jure Leskovec and Christos Faloutsos. Sampling from large graphs. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 631–636. ACM, 2006. R. Li, J. X. Yu, L. Qin, R. Mao, and T. Jin. On random walk based graph sampling. In 2015 IEEE 31st International Conference on Data Engineering, pp. 927–938, April 2015. doi: 10.1109/ICDE. 2015.7113345. Haonan Lu, Seth H. Huang, Tian Ye, and Xiuyan Guo. Graph star net for generalized multi-task learning. CoRR, abs/1906.12330, 2019. URL http://arxiv.org/abs/1906.12330. Bruno Ribeiro and Don Towsley. Estimating and sampling graphs with multidimensional random walks. In Proceedings of the 10th ACM SIGCOMM conference on Internet measurement, pp. 390–403. ACM, 2010. Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017. 10Published as a conference paper at ICLR 2020 Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. A comprehensive survey on graph neural networks. CoRR, abs/1901.00596, 2019. URL http: //arxiv.org/abs/1901.00596. Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. arXiv preprint arXiv:1806.03536, 2018. Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure Leskovec. Graph convolutional neural networks for web-scale recommender systems. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD ’18, 2018a. ISBN 978-1-4503-5552-0. Rex Ying, Jiaxuan You, Christopher Morris, Xiang Ren, William L. Hamilton, and Jure Leskovec. Hierarchical graph representation learning with differentiable pooling. In Proceedings of the 32Nd International Conference on Neural Information Processing Systems, NIPS’18, pp. 4805–4815, USA, 2018b. Curran Associates Inc. URL http://dl.acm.org/citation.cfm?id= 3327345.3327389. Hanqing Zeng and Viktor Prasanna. GraphACT: Accelerating GCN training on CPU-FPGA hetero- geneous platforms. arXiv preprint arXiv:2001.02498, 2019. Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor K. Prasanna. Accurate, efﬁcient and scalable graph embedding. CoRR, abs/1810.11899, 2018. URL http: //arxiv.org/abs/1810.11899. Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King, and Dit-Yan Yeung. Gaan: Gated atten- tion networks for learning on large and spatiotemporal graphs. arXiv preprint arXiv:1803.07294, 2018. Zhenpeng Zhou. Graph convolutional networks for molecules. CoRR, abs/1706.09916, 2017. URL http://arxiv.org/abs/1706.09916. A P ROOFS Proof of Proposition 3.1.Under the condition that vis sampled in a subgraph: E ( ζ(ℓ+1) v ) =E (∑ u∈V ˜Av,u αu,v ˜x(ℓ) u 1 u|v ) = ∑ u∈V ˜Av,u αu,v ˜x(ℓ) u E ( 1 u|v ) = ∑ u∈V ˜Av,u αu,v ˜x(ℓ) u P((u,v) sampled|vsampled) = ∑ u∈V ˜Av,u αu,v ˜x(ℓ) u P((u,v) sampled) P(vsampled) = ∑ u∈V ˜Av,u αu,v ˜x(ℓ) u pu,v pv (5) where the second equality is due to linearity of expectation, and the third equality (conditional edge probability) is due to the initial condition that vis sampled in a subgraph. It directly follows that, when αu,v = pu,v pv , E ( ζ(ℓ+1) v ) = ∑ u∈V ˜Av,u ˜x(ℓ) u 11Published as a conference paper at ICLR 2020 Proof of Theorem 3.2.Below, we use Cov (·) to denote covariance and Var(·) to denote variance. For independent edge sampling as deﬁned in Section 3.3, Cov ( 1 (ℓ1) e1 ,1 (ℓ2) e2 ) = 0,∀e1 ̸= e2. And for a full GCN on the subgraph, Cov ( 1 (ℓ1) e ,1 (ℓ2) e ) = pe −p2 e. To start the proof, we ﬁrst assume that the b(ℓ) e is one dimensional (i.e., a scalar) and denote it by b(ℓ) e . Now, Var(ζ) = ∑ e,ℓ ( b(ℓ) e pe )2 Var ( 1 (ℓ) e ) + 2 ∑ e,ℓ1<ℓ2 b(ℓ1) e b(ℓ2) e p2e Cov ( 1 (ℓ1) e ,1 (ℓ2) e ) = ∑ e,ℓ ( b(ℓ) e )2 pe − ∑ e,ℓ ( b(ℓ) e )2 + 2 ∑ e,ℓ1<ℓ2 b(ℓ1) e b(ℓ2) e p2e ( pe −p2 e ) = ∑ e (∑ ℓ b(ℓ) e )2 pe − ∑ e (∑ ℓ b(ℓ) e )2 (6) Let a given constant m= ∑ e pe be the expected number of sampled edges. By Cauchy-Schwarz in- equality: ∑ e ( ∑ ℓ b(ℓ) e ) 2 pe m= ∑ e (∑ ℓ b(ℓ) e√pe )2 ∑ e (√pe )2 ≥ (∑ e,ℓ b(ℓ) e )2 . The equality is achieved when ⏐⏐⏐ ∑ ℓ b(ℓ) e√pe ⏐⏐⏐∝√pe. i.e., variance is minimized when pe ∝ ⏐⏐⏐∑ ℓ b(ℓ) e ⏐⏐⏐. It directly follows that: pe = m ∑ e′ ⏐⏐⏐∑ ℓ b(ℓ) e′ ⏐⏐⏐ ⏐⏐⏐⏐⏐ ∑ ℓ b(ℓ) e ⏐⏐⏐⏐⏐ For the multi-dimensional case of b(ℓ) e , following similar steps as above, it is easy to show that the optimal edge probability to minimize ∑ i Var(ζi) (where iis the index for ζ’s dimensions) is: pe = m ∑ e′ ∑ ℓ b(ℓ) e′   ∑ ℓ b(ℓ) e  B S AMPLING ALGORITHM Algorithm 2 lists the four graph samplers we have integrated into GraphSAINT. The naming of the samplers follows that of Table 2. Note that the sampling parameters nand mspecify a budget rather than the actual number of nodes and edges in the subgraph Gs. Since certain nodes or edges in the training graph Gmay be repeatedly sampled under a single invocation of the sampler, we often have |Vs|<n for node and MRW samplers, |Vs|<2mfor edge sampler, and |Vs|<r ·hfor RW sampler. Also note that the edge sampler presented in Algorithm 2 is an approximate version of the independent edge sampler deﬁned in Section 3.4. Complexity (excluding the subgraph induction step) of the original version in Section 3.4 is O(|E|), while complexity of the approximate one is O(m). When m≪|E|, the approximate version leads to identical accuracy as the original one, for a given m. C D ETAILED EXPERIMENTAL SETUP C.1 H ARDWARE SPECIFICATION AND ENVIRONMENT We run our experiments on a single machine with Dual Intel Xeon CPUs (E5-2698 v4 @ 2.2Ghz), one NVIDIA Tesla P100 GPU (16GB of HBM2 memory) and 512GB DDR4 memory. The code is written in Python 3.6.8 (where the sampling part is written with Cython 0.29.2). We use Tensorﬂow 1.12.0 on CUDA 9.2 with CUDNN 7.2.1 to train the model on GPU. Since the subgraphs are sampled independently, we run the sampler in parallel on 40 CPU cores. 12Published as a conference paper at ICLR 2020 Algorithm 2Graph sampling algorithms by GraphSAINT Input: Training graph G(V,E); Sampling parameters: node budget n; edge budget m; number of roots r; random walk length h Output: Sampled graph Gs (Vs,Es) 1: function NODE (G,n) ⊿Node sampler 2: P(v) := ˜A:,v  2 /∑ v′∈V ˜A:,v′  2 3: Vs ←nnodes randomly sampled (with replacement) from Vaccording to P 4: Gs ←Node induced subgraph of Gfrom Vs 5: end function 6: function EDGE (G,m) ⊿Edge sampler (approximate version) 7: P((u,v)) := ( 1 deg(u) + 1 deg(v) ) /∑ (u′,v′)∈E ( 1 deg(u′) + 1 deg(v′) ) 8: Es ←medges randomly sampled (with replacement) from Eaccording to P 9: Vs ←Set of nodes that are end-points of edges in Es 10: Gs ←Node induced subgraph of Gfrom Vs 11: end function 12: function RW(G,r,h) ⊿Random walk sampler 13: Vroot ←rroot nodes sampled uniformly at random (with replacement) from V 14: Vs ←Vroot 15: for v∈Vroot do 16: u←v 17: for d= 1to hdo 18: u←Node sampled uniformly at random from u’s neighbor 19: Vs ←Vs ∪{u} 20: end for 21: end for 22: Gs ←Node induced subgraph of Gfrom Vs 23: end function 24: function MRW(G,n,r) ⊿Multi-dimensional random walk sampler 25: VFS ←rroot nodes sampled uniformly at random (with replacement) from V 26: Vs ←VFS 27: for i= r+ 1to ndo 28: Select u∈VFS with probability deg(u)/∑ v∈VFS deg(v) 29: u′←Node randomly sampled from u’s neighbor 30: VFS ←(VFS \\{u}) ∪{u′} 31: Vs ←Vs ∪{u} 32: end for 33: Gs ←Node induced subgraph of Gfrom Vs 34: end function 13Published as a conference paper at ICLR 2020 100 101 102 103 104 105 10−6 10−4 10−2 100 Degree P(degree ≥k) PPI Flickr Reddit Yelp Amazon Figure 5: Degree Distribution C.2 A DDITIONAL DATASET DETAILS Here we present the detailed procedures to prepare the Flickr, Yelp and Amazon datasets. The Flickr dataset originates from NUS-wide §. The SNAP website ¶collected Flickr data from four different sources including NUS-wide, and generated an un-directed graph. One node in the graph represents one image uploaded to Flickr. If two images share some common properties (e.g., same geographic location, same gallery, comments by the same user, etc.), there is an edge between the nodes of these two images. We use as the node features the 500-dimensional bag-of-word representation of the images provided by NUS-wide. For labels, we scan over the 81 tags of each image and manually merged them to 7 classes. Each image belongs to one of the 7 classes. The Yelp dataset is prepared from the rawjson data of businesses, users and reviews provided in the open challenge website∥. For nodes and edges, we scan the friend list of each user in the raw json ﬁle of users. If two users are friends, we create an edge between them. We then ﬁlter out all the reviews by each user and separate the reviews into words. Each review word is converted to a 300-dimensional vector using the Word2Vec model pre-trained on GoogleNews∗∗. The word vectors of each node are added and normalized to serve as the node feature (i.e., xv). As for the node labels, we scan the raw json ﬁle of businesses, and use the categories of the businesses reviewed by a user vas the multi-class label of v. For the Amazon dataset, a node is a product on the Amazon website and an edge (u,v) is created if products uand vare bought by the same customer. Each product contains text reviews (converted to 4-gram) from the buyer. We use SVD to reduce the dimensionality of the 4-gram representation to 200, and use the obtained vectors as the node feature. The labels represent the product categories (e.g., books, movies, shoes). Figure 5 shows the degree distribution of the ﬁve graphs. A point (k,p) in the plot means the probability of a node having degree at least kis p. C.3 A DDITIONAL DETAILS IN EXPERIMENTAL CONFIGURATION Table 4 summarizes the URLs to download the baseline codes. The optimizer for GraphSAINT and all baselines is Adam (Kingma & Ba, 2014). For all baselines and datasets, we perform grid search on the hyperparameter space deﬁned by: •Hidden dimension: {128,256,512} §http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm ¶https://snap.stanford.edu/data/web-flickr.html ∥https://www.yelp.com/dataset ∗∗https://code.google.com/archive/p/word2vec/ 14Published as a conference paper at ICLR 2020 Table 4: URLs and commit number to run baseline codes Baseline URL Commit Vanilla GCN github.com/williamleif/GraphSAGE a0fdef GraphSAGE github.com/williamleif/GraphSAGE a0fdef FastGCN github.com/matenure/FastGCN b8e6e6 S-GCN github.com/thu-ml/stochastic_gcn da7b78 AS-GCN github.com/huangwb/AS-GCN 5436ec ClusterGCNgithub.com/google-research/google-research/tree/master/cluster_gcn99021e Table 5: Training conﬁguration of GraphSAINT for Table 2 Sampler Dataset Training Sampling Learning rate Dropout Node budget Edge budget Roots Walk length Node PPI 0.01 0.0 6000 — — — Flickr 0.01 0.2 8000 — — — Reddit 0.01 0.1 8000 — — — Yelp 0.01 0.1 5000 — — — Amazon 0.01 0.1 4500 — — — Edge PPI 0.01 0.1 — 4000 — — Flickr 0.01 0.2 — 6000 — — Reddit 0.01 0.1 — 6000 — — Yelp 0.01 0.1 — 2500 — — Amazon 0.01 0.1 — 2000 — — RW PPI 0.01 0.1 — — 3000 2 Flickr 0.01 0.2 — — 6000 2 Reddit 0.01 0.1 — — 2000 4 Yelp 0.01 0.1 — — 1250 2 Amazon 0.01 0.1 — — 1500 2 MRW PPI 0.01 0.1 8000 — 2500 — Flickr 0.01 0.2 12000 — 3000 — Reddit 0.01 0.1 8000 — 1000 — Yelp 0.01 0.1 2500 — 1000 — Amazon 0.01 0.1 4500 — 1500 — •Dropout: {0.0,0.1,0.2,0.3} • Learning rate: {0.1,0.01,0.001,0.0001} The hidden dimensions used for Table 2, Figure 2, Figure 3 and Figure 4 are: 512 for PPI, 256 for Flickr, 128 for Reddit, 512 for Yelp and 512 for Amazon. All methods terminate after a ﬁxed number of epochs based on convergence. We save the model producing the highest validation set F1-micro score, and reload it to evaluate the test set accuracy. For vanilla GCN and AS-GCN, we set the batch size to their default value 512. For GraphSAGE, we use the mean aggregator with the default batch size 512. For S-GCN, we set the ﬂag -cv -cvd (which stand for “control variate” and “control variate dropout”) with pre-computation of the ﬁrst layer aggregation. According to the paper (Chen et al., 2018a), such pre-computation signiﬁcantly reduces training time without affecting accuracy. For S-GCN, we use the default batch size 1000, and for FastGCN, we use the default value 400. For ClusterGCN, its batch size is determined by two parameters: the cluster size and the number of clusters per batch. We sweep the cluster size from 500 to 10000 with step 500, and the number of clusters per batch from {1,10,20,40}to determine the optimal conﬁguration for each dataset / architecture. Considering that for ClusterGCN, the cluster structure may be sensitive to the cluster size, and for FastGCN, the minibatch connectivity may increase with the sample size, we present additional experimental results to reveal the relation between accuracy and batch size in Appendix D.3. 15Published as a conference paper at ICLR 2020 Table 6: Training conﬁguration of GraphSAINT for Table 3 Arch. Sampler Dataset Training Sampling Learning rate Dropout Node budget Edge budget Roots Walk length 2×512 MRW PPI (large) 0.01 0.1 1500 — 300 — 5×2048 RW PPI (large) 0.01 0.1 — — 3000 2 2×128 Edge Reddit 0.01 0.1 — 6000 — — 4×128 Edge Reddit 0.01 0.2 — 11000 — — Table 7: Training conﬁguration of GraphSAINT for Figure 4 (Reddit) 2-layer GAT-SAINT 4-layer GAT-SAINT 2-layer JK-SAINT 4-layer JK-SAINT Hidden dimension 128 128 128 128 AttentionK 8 8 — — Aggregation⨁ — — Concat. Concat. Sampler RW RW Edge Edge (root: 3000; length: 2) (root: 2000; length: 4) (budget: 6000) (budget: 11000) Learning rate 0.01 0.01 0.01 0.01 Dropout 0.2 0.2 0.1 0.2 Conﬁguration of GraphSAINT to reproduce Table 2 results is shown in Table 5. Conﬁguration of GraphSAINT to reproduce Table 3 results is shown in Table 6. Below we describe the conﬁguration for Figure 4. The major difference between a normal GCN and a JK-net (Xu et al., 2018) is that JK-net has an additional ﬁnal layer that aggregates all the output hidden features of graph convolutional layers 1 to L. Mathematically, the additional aggregation layer outputs the ﬁnal embedding xJK as follows: xJK = σ ( WT JK · L⨁ ℓ=1 x(ℓ) v ) (7) where based on Xu et al. (2018), ⨁is the vector aggregation operator: max-pooling, concatenation or LSTM (Hochreiter & Schmidhuber, 1997) based aggregation. The graph attention of GAT (Veli ˇckovi´c et al., 2017) calculates the edge weights for neighbor aggregation by an additional neural network. With multi-head ( K) attention, the layer- (ℓ−1) features propagate to layer-(ℓ) as follows: x(ℓ) v =  K k=1 σ   ∑ u∈neighbor(v) αk u,vWkx(ℓ−1) v   (8) where ∥is the vector concatenation operation, and the coefﬁcient αis calculated with the attention weights ak by: αk u,v = LeakyReLU (( ak)T [ Wkxu∥Wkxv ]) (9) Note that the αcalculation is slightly different from the original equation in Veliˇckovi´c et al. (2017). Namely, GAT-SAINT does not normalize αby softmax across all neighbors of v. We make such modiﬁcation since under the minibatch setting, node vdoes not see all its neighbors in the training graph. The removal of softmax is also seen in the attention design of Huang et al. (2018). Note that during the minibatch training, GAT-SAINT further applies another edge coefﬁcient on top of attention for aggregator normalization. Table 7 shows the conﬁguration of the GAT-SAINT and JK-SAINT curves in Figure 4. 16Published as a conference paper at ICLR 2020 2 3 4 5 60 2 4 6 8 GCN depth Normalized training time GraphSAINT: Reddit S-GCN: Reddit GraphSAINT: Yelp S-GCN: Yelp Figure 6: Comparison of training efﬁciency PPI Flickr Reddit Yelp Amazon 0 0.5 1 1.5 Fraction of training time Node Edge RW MRW Figure 7: Fraction of training time on sampling D A DDITIONAL EXPERIMENTS D.1 T RAINING EFFICIENCY ON DEEP MODELS We evaluate the training efﬁciency for deeper GCNs. We only compare with S-GCN, since implemen- tations for other layer sampling based methods have not yet supported arbitrary model depth. The batch size and hidden dimension are the same as Table 2. On the two large graphs (Reddit and Yelp), we increase the number of layers and measure the average time per minibatch execution. In Figure 6, training cost of GraphSAINT is approximately linear with GCN depth. Training cost of S-GCN grows dramatically when increasing the depth. This reﬂects the “neighbor explosion” phenomenon (even though the expansion factor of S-GCN is just 2). On Yelp, S-GCN gives “out-of-memory” error for models beyond 5 layers. D.2 C OST OF SAMPLING AND PRE-PROCESSING Cost of graph samplers ofGraphSAINT Graph sampling introduces little training overhead. Let ts be the average time to sample one subgraph on a multi-core machine. Let tt be the average time to perform the forward and backward propagation on one minibatch on GPU. Figure 7 shows the ratio ts/tt for various datasets. The parameters of the samplers are the same as Table 2. For Node, Edge and RW samplers, we observe that time to sample one subgraph is in most cases less than 25% of the training time. The MRW sampler is more expensive to execute. Regarding the complete pre-processing procedure, we repeatedly run the sampler for N = 50·|V| /|Vs|times before training, to estimate the node and edge probability as discussed in Section 3.2 (where |Vs|is the average subgraph size). These sampled subgraphs are reused as training minibatches. Thus, if training runs for more than N iterations, the pre-processing is nearly zero-cost. Under the setting of Table 2, pre-processing on PPI and Yelp and Amazon does not incur any overhead in training time. Pre-processing on Flickr and Reddit (with RW sampler) takes less than 40% and 15% of their corresponding total training time. Cost of layers sampler of AS-GCNAS-GCN uses an additional neural network to estimate the conditional sampling probability for the previous layer. For a node v already sampled in layer ℓ, features of layer-(ℓ−1) corresponding to all v’s neighbors need to be fed to the sampling neural network to obtain the node probability. For sake of analysis, assume the sampling network is a single layer MLP, whose weight WMLP has the same shape as the GCN weights W(ℓ). Then we can show, for a L-layer GCN on a degree-dgraph, per epoch training complexity of AS-GCN is approximately γ = (d·L) /∑L−1 ℓ=0 dℓ times that of vanilla GCN. For L = 2, we have γ ≈2. This explains the observation that AS-GCN is slower than vanilla GCN in Figure 2. Additional, Table 8 shows the training time breakdown for AS-GCN. Clearly, its sampler is much more expensive than the graph sampler of GraphSAINT. 17Published as a conference paper at ICLR 2020 Table 8: Per epoch training time breakdown for AS-GCN Dataset Sampling time (sec) Forward / Backward propagation time (sec) PPI 1.1 0.2 Flickr 5.3 1.1 Reddit 20.7 3.5 Cost of clustering of ClusterGCNClusterGCN uses the highly optimized METIS software††to perform clustering. Table 9 summarizes the time to obtain the clusters for the ﬁve graphs. On the large and dense Amazon graph, the cost of clustering increase dramatically. The pre-processing time of ClusterGCN on Amazon is more than 4×of the total training time. On the other hand, the sampling cost of GraphSAINT does not increase signiﬁcantly for large graphs (see Figure 7). Table 9: Clustering time of ClusterGCN PPI Flickr Reddit Yelp Amazon Time (sec) 2.2 11.6 40.0 106.7 2254.2 Taking into account the pre-processing time, sampling time and training time altogether, we sum- marize the total convergence time of GraphSAINT and ClusterGCN in Table 10 (corresponding to Table 2 conﬁguration). On graphs that are large and dense (e.g., Amazon), GraphSAINT achieves signiﬁcantly faster convergence. Note that both the sampling of GraphSAINT and clustering of ClusterGCN can be performed ofﬂine. Table 10: Comparison of total convergence time (pre-processing + sampling + training, unit: second) PPI Flickr Reddit Yelp Amazon GraphSAINT-Edge 91.0 7.0 16.6 273.9 401.0 GraphSAINT-RW 103.6 7.5 17.2 310.1 425.6 ClusterGCN 163.2 12.9 55.3 256.0 2804.8 D.3 E FFECT OF BATCH SIZE Table 11 shows the change of test set accuracy with batch sizes. For each row of Table 11, we ﬁx the batch size, tune the other hyperparameters according to Appendix C.3, and report the highest test set accuracy achieved. For GraphSAGE, S-GCN and AS-GCN, their default batch sizes (512,1000 and 512, respectively) lead to the highest accuracy on all datasets. For FastGCN, increasing the default batch size (from 400 to 4000) leads to noticeable accuracy improvement. For ClusterGCN, different datasets correspond to different optimal batch sizes. Note that the accuracy in Section 5.1 is already tuned by identifying the optimal batch size on a per graph basis. For FastGCN, intuitively, increasing batch size may help with accuracy improvement since the minibatches may become better connected. Such intuition is veriﬁed by the rows of 400 and 2000. However, increasing the batch size from 2000 to 4000 does not further improve accuracy signiﬁcantly. For ClusterGCN, the optimal batch size depends on the cluster structure of the training graph. For PPI, small batches are better, while for Amazon, batch size does not have signiﬁcant impact on accuracy. For GraphSAGE, overly large batches may have negative impact on accuracy due to neighbor explosion. Approximately, GraphSAGE expand 10×more neighbors per layer. For a 2-layer GCN, a size 2 ×103 minibatch would then require the support of 2 ×105 nodes from the ††http://glaros.dtc.umn.edu/gkhome/metis/metis/download ∗Default batch size ¶The training does not converge. ‡The codes throw runtime error on the large datasets (Yelp or Amazon). 18Published as a conference paper at ICLR 2020 Table 11: Test set F1-micro for the baselines under various batch sizes Method Batch size PPI Flickr Reddit Yelp Amazon GraphSAGE 256 0.600 0.474 0.934 0.563 0.428 512∗ 0.637 0.501 0.953 0.634 0.758 1024 0.610 0.482 0.935 0.632 0.705 2048 0.625 0.374 0.936 0.563 0.447 FastGCN 400∗ 0.513 0.504 0.924 0.265 0.174 2000 0.561 0.506 0.934 0.255 0.196 4000 0.564 0.507 0.934 0.260 0.195 S-GCN 500 0.519 0.462 — ¶ — ¶ — ‡ 1000∗ 0.963 0.482 0.964 0.640 — ‡ 2000 0.646 0.482 0.949 0.614 — ‡ 4000 0.804 0.482 0.949 0.594 — ‡ 8000 0.694 0.481 0.950 0.613 — ‡ AS-GCN 256 0.682 0.504 0.950 — ‡ — ‡ 512∗ 0.687 0.504 0.958 — ‡ — ‡ 1024 0.687 0.502 0.951 — ‡ — ‡ 2048 0.670 0.502 0.952 — ‡ — ‡ ClusterGCN 500 0.875 0.481 0.942 0.604 0.752 1000 0.831 0.478 0.947 0.602 0.756 1500 0.865 0.480 0.954 0.602 0.752 2000 0.828 0.469 0.954 0.609 0.759 2500 0.849 0.476 0.954 0.598 0.745 3000 0.840 0.473 0.954 0.607 0.754 3500 0.846 0.473 0.952 0.602 0.754 4000 0.853 0.472 0.949 0.605 0.756 input layer. Note that the full training graph size of Reddit is just around 1.5 ×105. Thus, no matter which nodes are sampled in the output layer, GraphSAGE would almost always propagate features within the full training graph for initial layers. We suspect this would lead to difﬁculties in learning. For S-GCN, with batch size of 500, it fails to learn properly on Reddit and Yelp. The accuracy ﬂuctuates in a region of very low value, even after appropriate hyperparameter tuning. For AS-GCN, its accuracy is not sensitive to the batch size, since AS-GCN addresses neighbor explosion and also ensures good inter-layer connectivity within the minibatch. 19",
      "meta_data": {
        "arxiv_id": "1907.04931v4",
        "authors": [
          "Hanqing Zeng",
          "Hongkuan Zhou",
          "Ajitesh Srivastava",
          "Rajgopal Kannan",
          "Viktor Prasanna"
        ],
        "published_date": "2019-07-10T21:11:13Z",
        "pdf_url": "https://arxiv.org/pdf/1907.04931v4.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "GraphSAINT proposes a novel graph sampling based inductive learning method to efficiently train deep Graph Convolutional Networks (GCNs) on large graphs. It addresses the 'neighbor explosion' problem by constructing minibatches through sampling the training graph first, then building a complete GCN on the sampled subgraph. Key contributions include: 1) ensuring a fixed number of well-connected nodes in all layers by changing the minibatch construction perspective; 2) developing normalization techniques to eliminate bias introduced by non-identical node sampling probabilities; 3) designing light-weight sampling algorithms for variance reduction; and 4) decoupling sampling from forward/backward propagation, allowing flexible integration with various GCN architectures (e.g., graph attention, jumping connection). GraphSAINT demonstrates superior accuracy and training time on five large datasets, achieving new state-of-the-art F1 scores for PPI (0.995) and Reddit (0.970).",
        "methodology": "GraphSAINT's core methodology involves constructing minibatches by first sampling a subgraph from the full training graph, and then building a full GCN on this sampled subgraph for each iteration. This fundamentally differs from layer sampling approaches. The process includes: 1) **Pre-processing** to estimate node (pv) and edge (pu,v) sampling probabilities by repeatedly running the chosen sampler; 2) **Minibatch Training** where, in each iteration, an independently sampled subgraph (Gs) is used to build a full GCN; 3) **Normalization** using aggregator normalization (αu,v = pu,v / pv) for feature propagation and loss normalization (λv = |V| * pv) for the minibatch loss, ensuring unbiased estimators; 4) **Variance Reduction** via optimal edge sampling probabilities derived from an analysis, which are simplified to be topology-dependent (e.g., pe ∝ 1/deg(u) + 1/deg(v)); 5) **Samplers** including Random Node, Random Edge, Random Walk (RW), and Multi-Dimensional Random Walk (MRW) samplers, all inducing subgraphs from sampled nodes/edges; and 6) **Architectural Flexibility**, allowing seamless integration with architectures like Jumping Knowledge Networks (JK-net) and Graph Attention Networks (GAT) due to complete GCNs on subgraphs.",
        "experimental_setup": "Experiments were conducted in an inductive, supervised learning setting on five large-scale graph datasets: PPI (protein-protein interaction), Flickr (image network), Reddit (online post communities), Yelp (business categorization), and Amazon (product categorization), all using fixed-partition train/validation/test splits. The evaluation metric used was F1-micro score. GraphSAINT was compared against six baselines: vanilla GCN, GraphSAGE, FastGCN, S-GCN, AS-GCN, and ClusterGCN. All models were implemented in TensorFlow with Python 3, running on a single machine equipped with dual Intel Xeon CPUs and an NVIDIA Tesla P100 GPU (sampler parallelized on 40 CPU cores). Hyperparameters (hidden dimension, dropout, learning rate) were tuned using a grid search. Comparisons included accuracy, training time, and convergence curves for 2-layer GCNs, with additional evaluations on deeper (4-layer and 5-layer) models and architectural variants like GAT and JK-net.",
        "limitations": "The paper acknowledges several limitations. The derivation of optimal edge sampling probabilities for variance reduction uses a reasonable simplification (ignoring feature dependencies) to make them topology-dependent, which is not the fully optimal solution. For general samplers, deriving closed-form expressions for node and edge sampling probabilities is challenging, necessitating pre-processing (repeated sampling) for estimation. Explicit variance reduction for attention-based GCNs (like GAT) is noted as difficult due to dynamically updated attention values. The Multi-Dimensional Random Walk (MRW) sampler is observed to be more computationally expensive than other proposed samplers. Additionally, the paper highlights issues with baselines, such as GraphSAGE potentially suffering from accuracy degradation with overly large batches due to neighbor explosion, S-GCN failing to converge or incurring 'out-of-memory' errors on large/deep settings, AS-GCN having an expensive sampler that increases overall training time, and ClusterGCN's clustering pre-processing cost being significant for very large and dense graphs.",
        "future_research_directions": "Future research directions include: 1) developing distributed training algorithms for GraphSAINT, where the training graph is partitioned across distributed memory, sampling is performed independently on each processor, and training on self-supportive subgraphs reduces communication cost, potentially requiring new data shuffling strategies; and 2) pursuing algorithm-system co-optimization to accelerate GraphSAINT training on heterogeneous computing platforms (e.g., CPU-FPGA). This leverages GraphSAINT's advantage in resolving 'neighbor explosion' to reduce computation, improve hardware utilization through less data traffic, and facilitate task-level parallelization due to the decoupling of graph sampling and GCN layer propagation."
      }
    },
    {
      "title": "GACT: Activation Compressed Training for Generic Network Architectures",
      "abstract": "Training large neural network (NN) models requires extensive memory\nresources, and Activation Compressed Training (ACT) is a promising approach to\nreduce training memory footprint. This paper presents GACT, an ACT framework to\nsupport a broad range of machine learning tasks for generic NN architectures\nwith limited domain knowledge. By analyzing a linearized version of ACT's\napproximate gradient, we prove the convergence of GACT without prior knowledge\non operator type or model architecture. To make training stable, we propose an\nalgorithm that decides the compression ratio for each tensor by estimating its\nimpact on the gradient at run time. We implement GACT as a PyTorch library that\nreadily applies to any NN architecture. GACT reduces the activation memory for\nconvolutional NNs, transformers, and graph NNs by up to 8.1x, enabling training\nwith a 4.2x to 24.7x larger batch size, with negligible accuracy loss. We\nimplement GACT as a PyTorch library at\nhttps://github.com/LiuXiaoxuanPKU/GACT-ICML.",
      "full_text": "GACT: Activation Compressed Training for Generic Network Architectures Xiaoxuan Liu, Lianmin Zheng, Dequan Wang, Yukuo Cen, Weize Chen, Xu Han, Jianfei Chen, Zhiyuan Liu, Jie Tang, Joey Gonzalez, Michael Mahoney, Alvin Cheung University of California, Berkeley xiaoxuan liu@berkeley.edu, jianfeic@tsinghua.edu.cn Abstract Training large neural network (NN) models requires extensive memory resources, and Activation Compressed Training (ACT) is a promising approach to reduce training memory footprint. This paper presents GACT, an ACT framework to support a broad range of machine learning tasks for generic NN architectures with limited domain knowledge. By analyzing a linearized version of ACT’s approximate gradient, we prove the convergence of GACT without prior knowledge on operator type or model architecture. To make training stable, we propose an algorithm that decides the compression ratio for each tensor by estimating its impact on the gradient at run time. We implement GACT as a PyTorch library that readily applies to any NN architecture. GACT reduces the activation memory for convolutional NNs, transformers, and graph NNs by up to 8.1×, enabling training with a 4.2× to 24.7× larger batch size, with negligible accuracy loss. We implement GACT as a PyTorch library at https://github.com/LiuXiaoxuanPKU/GACT-ICML. 1 Introduction In recent years, we have witnessed the trend of using larger and larger neural network (NN) models to deliver improved accuracy and generalization in various machine learning tasks [1, 2]. However, training these models requires a considerable amount of on-device GPU memory. Unfortunately, the increase of GPU memory capacity has been relatively slow, leading to a fundamental barrier to the development of large NN models. Activation Compressed Training (ACT) is a promising approach to reduce the memory footprint of models during training. As all layers’ activations need to be kept in the memory for computing the gradients during training, ACT reduces memory consumption by compressing these saved activations. Prior work [3, 4, 5, 6] has shown the eﬀectiveness of ACT by reducing activation footprint by up to 12 ×with 2-bit activations. Although ACT has already demonstrated impressive compression capabilities, previous work on ACT is restricted to speciﬁc NN architectures. For example, ActNN [ 5] is a quantization framework for convolutional NNs only; Mesa [ 7] proposes a per head/layer quantization method for vision transformers; and AC-GC [ 6] derives convergence error bound for diﬀerent types of operators separately. Developing a generic ACT framework is challenging. Theoretically, convergence guarantees must be made without assumptions on the network architecture. Algorithmically, the framework should ﬁnd eﬀective compression strategies for all kinds of networks automatically. From the system perspective, the framework should support arbitrary NN operations, including user-deﬁned ones. In this work, we propose GACT, a general framework for ACT that is agnostic to the NN architecture. Neither specialized mathematical derivations nor customized implementation is needed to support diﬀerent operators. To enable this, we develop a general convergence theory by analyzing the stochastic gradient (SG) introduced by ACT. We show that the SG can be well approximated by a linearized version, which is unbiased to stochastic compressors. The variance of the linearized gradient has a particularly simple structure that allows a numerical algorithm to predict the variance given a compression strategy. Then, we generate the strategy by approximately solving an integer program. 1 arXiv:2206.11357v4  [cs.LG]  3 Sep 2022We implement our method as a library based on PyTorch that can be quickly integrated into real-world machine learning systems. The library also provides several optimization levels to explore the trade-oﬀ between memory and speed. We demonstrate the ﬂexibility and eﬃciency of GACT on various tasks, including image classiﬁcation, object detection, text, and graph node classiﬁcation. Our evaluation shows that GACT can reduce activation memory by up to 8.1 ×, enabling training with a 24.7 ×larger batch size on the same GPU. In sum, our main contributions are as follows: • We propose a general convergence theory for ACT. • We develop an algorithm that automatically estimates the sensitivity of each compressed tensor and selects the optimal compression strategy. • We build eﬃcient implementation of GACT in PyTorch with an easy-to-use API that can also be combined with other memory-saving techniques seamlessly. 2 Related Work Activation Compressed Training.ACT has been applied to convolutional NNs using diﬀerent compressors, such as quantizers [3, 4, 5], JPEG [8], or scientiﬁc data compression algorithms [ 9, 6]. ACT is also applied to transformers [7] and graph NNs [10]. However, the existing theory for ACT [ 3, 4, 5, 6] relies on the case-by-case analysis of speciﬁc network operators, such as convolution, ReLU, and batch normalization. It also requires dedicated implementations for each operator. On the contrary, GACT focuses on the generality of activation compressed training, not a speciﬁc quantizer design, which is the main topic of previous work. Instead of assuming that the network is a stack of layers, GACT formulates the problem as a computational graph of operators. This is general enough to cover transformers [11], graph NNs [12], second-order derivatives, and unknown future architectures. Reduced Precision Training. Apart from ACT, reduced precision training [13, 14, 15, 16, 17, 18] performs calculations directly on low precision data, reducing the computation cost and memory footprint simultaneously. To achieve this, specialized kernels are used to calculate on low precision data. In contrast, ACT only considers storage, and it can thus use more ﬂexible compression strategies and achieve a much better compression ratio with the same accuracy loss. Memory-Eﬃcient Training. Gradient checkpointing [19, 20] trades computation for memory by dropping some of the activations in the forward pass from memory and recomputing them in the backward pass. Swapping [21, 22, 23, 24] oﬄoads activation or model parameters to an external memory (e.g., CPU memory). Recent work [25] explores the possibility of combining the gradient checkpointing and swapping. All these methods save memory by storing fewer tensors on the GPU. In contrast, GACT compresses the saved tensors and is complementary to these approaches. Moreover, the generality of GACT enables easy combination with these methods, which we explore in this paper. 3 Formulation We ﬁrst present the mathematical formulation of our activation compressed training (ACT) framework. As we would like to develop a general ACT algorithm, applicable to a wide range of NN architectures, we make minimal assumptions on our formulation. Throughout the paper, we deﬁne the variance of a vector x as Var [x] = E [ ∥x∥2 ] −∥E[x]∥2. 3.1 Activation Compressed Training In this work, we abstract the forward propagation as two functions ℓ(x; θ) and h(x; θ). Both take a datum x and the model parameter θ as the input. The loss function ℓ(x; θ) outputs the loss of the network θ on datum x. The context function h(x; θ) outputs tensors to be stored in the memory for computing the gradients, which are referred as the context. Assume that the context consists of L tensors, where each tensor h(l)(x; θ) is represented by a ﬂattened Dl-dimensional vector. Denote h(x; θ) = ( h(l)(x; θ))L l=1. Our notations are somewhat unconventional in the sense that we do not explicitly deﬁne each layer’s activation. We do not even assume that there is a NN. It could be any computational graph that saves context tensors. Given a dataset X = {xn}N n=1, deﬁne the batch loss L(θ) := 1 N ∑N n=1 ℓ(x; θ). The dataset can be equivalently represented as an empirical data distribution pX(x) := 1 N ∑N n=1 δ(x−xn), where δ is the Dirac 2Type\tequation\there. ConvBNConv……ReLU ReLU……BNConvConv Compressed context tensors (GPU) ℓ(𝑥;𝜃) Pack hook Unpackhook context ℎ(𝑥;𝜃) Swap outSwap in 𝑄(ℎ(𝑥;𝜃)) Forward Computation Graph Backward Computation Graph CPU Memory bits 𝑔(𝑄ℎ(𝑥;𝜃);𝜃) Adaptive AlgorithmCompressorGACT Decompressor 𝑥,\t gradient𝑔(𝑄ℎ(𝑥;𝜃);𝜃) (𝜃)model Figure 1: The architecture of GACT. delta function. The batch loss can be written as L(θ) = EX[ℓ(x; θ)], where EX denotes for taking expectation over pX. The network is trained with stochastic gradient descent (SGD) [ 26]. Starting from an initial model θ0, at the t-th iteration, SGD updates the model with: θt+1 ←θt −η∇θℓ(x; θt), (1) where η is a learning rate, and the SG ∇θℓ(x; θ) is computed on a random datum x ∼pX. Notice that EX[∇θℓ(x; θ)] = ∇θL(θ), i.e., the SG is an unbiased estimator of the batch gradient ∇θL(θ). Crucially, the SG can be written in the form ∇θℓ(x; θt) = g(h(x; θt); θt). In other words, the back propagation only depends on the forward propagation through the context h(x; θt). The entire context must be kept in memory for computing the gradients. The context dominates the memory consumption in many applications. ACT reduces the training memory footprint by compressing the context. Let Q(h) be a compressor, which converts h to compact formats while keeping Q(h) ≈h. Then, ACT computes the gradient with compressed context: θt+1 ←θt −ηg(Q(h(x; θt)); θt). (2) We refer to g(Q(h(x; θt); θt) as the activation compressed (AC) gradient. ACT is signiﬁcantly more memory eﬃcient then the plain SGD, Eq. (1), since it only needs to store a compressed version of the context. Suppose the original context h(x; θt) consists of 32-bit ﬂoating point tensors, and Q(·) is a compressor which quantizes tensors to 2-bit integers, ACT will reduce the context memory by 16 ×. Fig. 1 illustrates the computational graph of ACT with these notations. In the following presentation, we might denote h(x,θ) simply by h when there is no confusion. 3.2 Convergence of ACT ACT is a lossy approximation of SGD, as it uses an approximate gradient g(Q(h); θ). Therefore, some kind of theoretical guarantee is required for ACT to be useful. Fortunately, analyzing ACT is made signiﬁcantly simpler by introducing an unbiased stochastic compressor Q(·), such that EQ[Q(x)] = x for any x. EQ[·] means taking expectation over the compressor. In this way, g(Q(h); θ) can be viewed as a stochastic estimator of the batch gradient ∇L(θ), but the randomness comes not only from the datum x but also the compressor Q(·). Therefore, ACT is still an SGD algorithm. Standard analytical tools for SGD [ 27] are applicable for studying ACT. SGD algorithms have particular good properties when the SG is unbiased. In our case, this means EQ[g(Q(h); θ)] = g(h; θ). However, the SG is biased general, even when the stochastic compressor itself is unbiased.1 The key technique in this work is to construct an unbiased approximation of the AC gradient by linearizing 1Consider the exampleg(h) =I(h ≥0.5), whereh ∈[0, 1] and its AC gradientg(Q(h)) =I(Q(h) ≥0.5) with the compressor Q(h) ∼Bernoulli(h). Then, E [g(Q(h))] =P(Q(h) = 1) =h ̸= g(h). 3the gradient function g(·; θ). Consider the ﬁrst-order Taylor expansion of g(·; θ) at h: ˆg(Q(h); h,θ) := g(h; θ) + J(h,θ)∆h, (3) where J(h,θ) := ∂g(h;θ) ∂h is a Jacobian matrix, ∆ h := Q(h) −h is the compression error. We further denote ˆgxθ(Q(h); h) := ˆg(Q(h); h,θ)|h=h(x;θ) and Jxθ(h) := J(h,θ)|h=h(x;θ) for short. Since E[∆h(x; θ)] = 0, ˆgxθ(Q(h); h) is an unbiased SG, Furthermore, the approximation error is small: Proposition 1. Assuming that g(h; θ) is twice diﬀerentiable w.r.t. h, and the second order derivative is bounded, then E[∥g(Q(h); θ) −ˆgxθ(Q(h); h)∥2] = O(VarQ[∆h]). Since ∆h itself is unbiased, VarQ[∆h] = EQ [ ∥∆h∥2 ] is simply the expected compression error. Prop. 1 implies that the linearization error is bounded by the compression error. The linearized gradient ˆg is accurate if the compression is accurate. Using ˆg as a bridge, we arrive in the following convergence theorem: Theorem 1. Assume that: A1. L(θ) is a continuous diﬀerentiable, ∇L(θ) is β-Lipschitz continuous. A2. L(θ) is bounded below by L∗. A3. g(h; θ) is diﬀerentiable w.r.t. h and ∃b> 0, s.t. ∀θ,E∥g(Q(h(x; θ)); θ) −ˆgxθ(Q(h); h)∥≤ b. A4. ∃σ2 >0, s.t., ∀θ, Var [ˆgxθ(Q(h); h)] ≤σ2. Then, for all η <1 2β, if we run ACT deﬁned as Eq. (2) for T iterations, then we have min t=0,...,T−1 E [ ∥∇L(θt)∥2 ] ≤4(L(θ0) −L∗) ηT + 3b2 + ηβσ2 Remark: The analytical technique used in Thm. 1 is rather standard, see Thm. 4.8 in [ 27]. However, we consider the variance term σ2 of the linearized gradient, rather than the SG itself. This formulation brings better analytical properties and an adaptive algorithm for determining the compression scheme, as we shall see soon in Sec. 4. The convergence of ACT is aﬀected by both the linearization error (A3) and the variance of the unbiased gradient ˆg(·; θ) (A4). The latter is characterized as: Proposition 2. Var [ˆgxθ(Q(h); h)] = VarX[g(h; θ)] + EX[VarQ[ˆgxθ(Q(h); h)]] , where the second term on the RHS equals to EX[VarQ[Jxθ(h)∆h]] = O(VarQ[∆h]) . Prop. 2 separates the variance from diﬀerent noise sources. VarX[g(h(x,θ); θ)] is the variance raised by random sampling of data (“sampling variance”). EX[VarQ[Jxθ(h)∆h(x,θ)]] is the variance raised by compression. Now, the convergence in Thm. 1 is depicted by 3 b2 + ηβσ2. By Prop. 1, b2 = O(VarQ[∆h]2). By Prop. 2, σ2 = O(1) +O(VarQ[∆h]), since the sampling variance is not aﬀected by compression. Therefore, when the compression is accurate (∆ h →0), the impact of the linearization error is negligible, and the variance of the unbiased gradient dominates. ACT behaves as if the AC gradient is unbiased. 4 Adapting the Compression Rate In a network, some context tensors (such as those stored for computing the cross entropy loss) are extremely sensitive, a small amount of compression would result in diverged training, while other tensors are quite robust to compression. Therefore, we must apply diﬀerent amounts of compression for each context tensor. As a general framework, we have no prior knowledge of the users’ model architecture, so we designed an algorithm to infer the sensitivity for each context tensor and determine their compression rate automatically. There is a tradeoﬀ between the compression error and the storage requirement. We represent the storage requirement of the compressed context in bits per dimension . We assume that bl bits/dim. are used for compression h(l), and Qbl(h(l)) be the compression result. Let b = (bl)L l=1 be a compression scheme, Qb(h) := {Qbl(h(l))}L l=1, and ∆bh= Qb(h) −h. 44.1 Structure of Variance As discussed in Sec. 3.2, when the compression is relatively accurate, the variance plays the main role in determining the convergence. Therefore, we would like to investigate how the compression scheme would impact the variance. Formally, we are interested in: V(b; h,θ) := VarQ[ˆg(Qb(h); h,θ)] . Once V(b,h; θ) is known, we can ﬁnd the minimum variance compression scheme under a given total bits budget B, by solving the integer programming problem: min b V(b; h(x; θ),θ), s.t. L∑ i=1 blDl ≤B, (4) where Dl is the dimensionality of h(l). To proceed, we need the following assumptions on the compressor Qb(·): Assumption B1: The compressed result is element-wise uncorrelated. That is, for anyi̸= j, Cov [Qb(h)i,Qb(h)j] = 0. Assumption B2: For compressing h(l)(x; θ) to bl bits/dim., the compression error can be written in the form Var [ ∆blh(l)(x; θ)j ] ≤Rlj(x; θ)S(bl), where S(bl) is a known function. This isolates the eﬀect of bl through the unary factor S(bl). Both assumptions can be achieved by a stochastic rounding quantizer [28], where Rlj(x; θ) = 1 4 ( maxkh(l) k −minkh(l) k )2 and S(b) = (2bl −1)−2. See Appendix A.4 for the derivations. The following theorem reveals the structure of the variance: Theorem 2. Under assumptions B1, B2, there exists a family of functions {cl(h,θ)}L l=1, such that the compression variance can be written in the form V(b; h,θ) ≤ L∑ l=1 cl(h,θ)S(bl). (5) 4.2 Computing Sensitivity Thm. 2 reveals two good properties of the variance: (1) the impact of compressing diﬀerent context tensors simply sums up, without aﬀecting each other; and (2) the compression scheme only impacts the variance through S(bl). Both properties are brought about by linearization. Since S(·) is a known function, we only need to know cl(h,θ) to solve problem Eq. (4). cl(h,θ) can be understood as the sensitivity of the AC gradient to the compression of the l-th tensor. We can compute cl(h,θ) numerically by leveraging the idempotence of compressing a tensor: Assumption B3: If h= Q(h′) for some h′with non-zero probability, then Q(h) = h and VarQ[Q(h)] = 0. Let Q¬(l) b (h) = {Qb1 (h(1)),...,h (l),...,Q bL(h(L))}be some tensors, where every tensor except h(l) is compressed. Plug h= Q¬(l) b (h) into Eq. (5), and use B3, we have V(b; Q¬(l) b (h),θ) ≤cl(Q¬(l) b (h),θ)S(bl). The left hand side can be approximated by taking ˆg(Qb(h); h,θ) ≈g(Qb(h); θ). Assume that cl(·,θ) is reasonably continuous, we have cl(h,θ) ≈VarQ[g(Qb(h); θ)] |h=Q¬(l) b (h)/S(bl). The variance can be replaced by empirical variance. Alg. 1 illustrates this idea. To compute VarQ[g(Qb(h); θ)] at h= Q¬(l) b (h), we keep the random seeds ﬁxed for all the compressors except the l-th one. We compute the empirical variance by two evaluations of g(Qb(h); θ), which are two NN iterations (forward + backward propagation). Finally, we assume thatc(h,θ) remains stable for diﬀerent mini-batches h, and along the training trajectory (θt). Therefore, we maintain a cl for each tensor l, which is updated by periodically running Alg. 1. Eq. (4) is approximately solved by the O(Llog2 L) greedy algorithm [5]. Another useful feature of this approach is predicting failure (in an a posteriori manner). If the compression variance V(b; h,θ) is dominating the overall gradient variance Var [g(Q(h); θt)], compression is adding too much noise to the gradient, and the convergence might be aﬀected. The overall gradient variance can be 5Algorithm 1 Numerical algorithm for computing cl(h,θ). Require: A gradient evaluation function g(·; θ) Require: A series of L+ 1 random seeds (rl)L+1 l=1 . Require: Any compression scheme b= (bl)L l=1 ∀l, seed Q(l) with rl g0 ←g(Qb(h); θ) {First iteration} ∀l, seed Q(l) with rl seed Q(l) with rL+1 g1 ←g(Qb(h); θ) {Second iteration, with another seed } Return 1 2 ∥g0 −g1∥2 /S(bl) 1importtorch2 importgact3 4model = ... # user defined model5 controller = gact.controller(model, opt_level='L2')6  controller.install_hook()78  # trainingloop9forepochin...10 foriterin...11......12 # instruct gact how to perform forward and backward13deffwdbwdprop():14output = model(data)15loss =loss_func(output,target)16optimizer.zero_grad()17loss.backward()1819controller.iterate(fwdbwdprop) Figure 2: Usage example of GACT computed by maintaining a running mean of the gradient. If V(b; θ)/Var [ˆg(Q(h); θt)] is too large, we can raise an alert to the user to increase the storage budget. 5 System Implementation We implemented GACT as a lightweight library in PyTorch. Users can use GACT for any NN architecture with several lines of code change. GACT uses low-level PyTorch hooks to capture context tensors, so it supports arbitrary operators, including custom operators deﬁned by users. We implemented eﬃcient CUDA kernels to infer tensor sensitivity and to perform compression during run time. GACT uses the same per-group quantizer in ActNN [5] as the compressor. However, GACT diﬀers from ActNN in several aspects. ActNN relies on manual analytical deduction to compute the sensitivity for diﬀerent operators, while GACT infers tensor sensitivity automatically, as described in Sec. 4.2. Moreover, ActNN performs layer-level quantization. It has to implement an activation compressed version for each operator and substitute operators during the training (e.g., replace torch.nn.Conv2d with actnn.Conv2d). In contrast, GACT runs at tensor level and uses a single hook interface to compress saved tensors for all operators. 5.1 General API As shown in Fig. 2, the interface of GACT is straightforward and intuitive, requiring the user to (i) initialize the GACT controller and specify an optimization level (Line 5); (ii) install hooks (Line 6); and (iii) instruct GACT how to perform forward and backward propagation (Lines 13-17) and pass it as a function (fwdbwdprop) to the controller (Line 19). We require users to specify (iii) because GACT needs to numerically run the forward and backward pass to infer tensor sensitivity. Although fwdbwdprop is passed to the controller every iteration, it is only called internally every adapt interval iterations when tensor sensitivity changes. As shown in Sec. 6.1, tensor sensitivity stabilizes quickly after the ﬁrst several epochs, adapt interval can thus be set to a large number, introducing negligible impact on training speed. 65.2 System Architecture Fig. 1 shows an overview of GACT. The GACT controller has three modules: Adaptivate Algorithm; Compressor; and Decompressor. In the forward pass, the controller uses PyTorch pack hook to capture all context tensors. Then Adaptive Algorithm infers tensor sensitivity based on gradients and assigns higher bits to more sensitive tensors, as described in Sec. 4.2. The bits information is used to instruct Compressor to perform quantization. In the backward pass, Decompressor dequantizes context tensors and uses unpack hook to send the dequantized results back to the PyTorch’s auto diﬀerentiation engine. The controller is also responsible for swapping quantized tensors to the CPU and prefetching them back during the backward propagation if swapping is enabled. 5.3 Identifying Tensors to Quantize The pack hook and unpack hook process all types of context tensors, including activation, parameters trained by the optimizer, and training states such as running mean/variance used by batch normalization. To guarantee that only the activations are quantized, we ﬁlter out saved parameters by recording the data pointers of all the model parameters before training, and we skip quantization if the input tensor pointer exists in the parameter pointer set. Similarly, GACT does not quantize training states by checking if the input tensor requires gradients. However, using hooks blindly disables some memory-saving optimization. For example, in a transformer’s self-attention layer, the keys, query, value tensors are all calculated from the same input tensor. The saved objects of the three operations thus all refer to the same tensor. In this case, PyTorch triggers the pack hook three times. If we perform quantization blindly, we waste computation resources and introduce extra memory consumption because the same underlying tensor is quantized and saved more than once. GACT avoids duplication by generating footprints for each input context tensor. We use the CUDA data pointer, sampled data points, and the tensor statistics (e.g., sum) as the footprint. GACT manages all quantized context tensors and uses the footprint to diﬀerentiate them. If a tensor is already quantized, GACT will skip quantization and return previous results directly. 5.4 Parallel Swap and Prefetch To further reduce activation memory, we combine GACT with swapping. All compressed tensors are oﬄoaded to the CPU during the forward pass and swapped back in the backward pass. Here, we replace the original tensor with quantized activation, as data movement is more expensive than computation. Swapping the original tensor saves the quantization overhead but adds more data movement cost between CPU and GPU. As shown in Sec. 6.4, quantization overhead is much smaller than copying full-precision data to CPU in modern GPU architecture. Furthermore, we create two new streams (swap in/out) to parallelize the computation and swapping operation to reduce the swap overhead. The forward computation and swap-out process happen in parallel during the forward pass. During the backward pass, in each layer the swap-in stream is responsible for prefetching the compressed activation of the previous layer to avoid synchronization overhead. We leverage the CUDA event to ensure tasks in diﬀerent streams are executed in the correct order. 5.5 Other Memory Optimizations Gradient checkpointing. Gradient checkpointing [ 19] works by dividing the NN into segments. The algorithm only stores the inputs of each segment and recomputes the dropped activations segment by segment during backpropagation. The memory consumption is thus the cost of storing the inputs of all segments plus the maximum memory cost to backpropagate each segment. When combined with gradient checkpointing, GACT can reduce the memory consumption of both parts. GACT reduces the memory consumption of the ﬁrst part by quantizing the segment inputs. Moreover, the activations saved during the recompute phase are also quantized, reducing the memory cost of the second part. Combining GACT with gradient checkpointing might introduce more training noise because the recompute starts from quantized segment inputs, making the forward pass of recompute phase not exact. However, in Sec. 6.4, we show the noise introduced by forwarding from the quantized tensors is negligible. Memory eﬃcient self-attention. When the batch size is very large, the single layer after dequantization occupies a large amount of memory and prevents the batch size from increasing further. We observe this 7Table 1: Optimization levels for GACT. Level Compression Strategy Bits L0 Do not compress 32 L1 per-group quantization with auto-precision 4 L2 L1 + swapping/prefetching 4 CB1 L1 + gradient checkpointing 4 CB2 CB1 + eﬃcient self-attention 4 relu conv *pool conv *conv *pool conv *conv *pool conv *conv *pool linear *drop linear *drop linear loss 1 2 4 8 32bits 10−1 100 101 cl (a) Inferred per-tensor cl (line) and bits/dim. (bar) for VGG-11. Layers with * have a preceding ReLU layer with shared context. drop=dropout, loss=cross entropy loss. 1 2 4 8 bits/dim. 10−2 10−1 100 101 grad. var. uniform adapt (b) Gradient variance. 0 50 100 epoch 10−2 100 102 104 cl (c) Evolution of the per- tensor sensitivity. Each line iscl for a tensor. Figure 3: Eﬀectiveness of the adaptive algorithm. problem in transformer-based models where self-attention has quadratic space complexity in terms of sequence length. To reduce the memory footprint of the self-attention layer, we implement the algorithm introduced in [29] that achieves linear space complexity, and combines it with GACT. 5.6 Optimization level To exploit the trade-oﬀ between memory saving and training speed, GACT provides several optimization levels. Higher levels can save more memory but with more overhead. Tab. 1 lists these optimization levels. L1 uses per-group quantization with the adaptive algorithm. L2 combines per-group quantization with swapping and prefetching. For transformer-based models, CB1 combines GACT with gradient checkpointing. CB2 further reduces the peak memory by adding eﬃcient self-attention to CB1. 6 Experiments We ﬁrst demonstrate the eﬀectiveness of the GACT adaptive algorithm. We further apply GACT to a wide range of machine learning tasks, including image classiﬁcation, object detection, text, and graph node classiﬁcation. We compare the training accuracy and activation compression rate for full precision, adaptive 4/3/2 (using GACT to adaptively decide quantization bits with an average of 4/3/2 bit) and ﬁx-4 bit (quantizating all tensors uniformly with 4 bits). Next, we study the trade-oﬀ between compression rate and training throughput and compare GACT with other state-of-the-art memory-saving methods. Lastly, we demonstrate the ﬂexibility of GACT by exploring the possibility of combining it with other memory optimization methods (CB1, CB2 as listed in Table 1). We use open-source model implementations for all tasks. 6.1 Compression Strategy We ﬁrst test the eﬀectiveness of our adaptive compression rate algorithm for training VGG-11 [ 30] on ImageNet. Fig. 3(a) plots the inferred per-tensor sensitivity cl and the corresponding optimal bits/dim. GACT assigns more bits to more sensitive layers. The context tensor saved by the cross-entropy loss operator is most sensitive. A small amount of compression leads to a huge gradient variance. This makes sense since the loss is the ﬁrst operator to back-propagate through, where the error accumulates. Therefore, GACT assigns 32 bits/dim. for the tensors in the classiﬁcation head. With the adaptive algorithm, GACT with an average of 4 bits/dim. achieves smaller gradient variance than uniformly assigning 8 bits/dim. for all the tensors, as shown in Fig. 3(b). Finally, Fig. 3(c) shows that the sensitivity cl(h; θt) remains stable during training.Therefore, periodically updating cl at a large interval is reasonable, and this introduces negligible impact on training speed. 8Table 2: For classiﬁcation, we train VGG11 [ 30], ResNet-50 [31], and Swin-Tiny [ 32] on ImageNet [ 33]. For object detection, we train RetinaNet [ 34], Faster R-CNN [35] on Coco [ 36]. We report accuracy on validation sets (Div. indicates diverge) and the compression rate of context tensors (numbers in brackets) for both tasks. Task Model FP32 GACT Adapt 4bit (L1) GACTAdapt 2bit Cls. VGG11 68.75 68.77 (2.84×) 68.49 (3.34×) ResNet-50 77.29 76.96 (6.69×) 76.13 (11.39×) Swin-tiny 81.18 80.92 (7.44×) 77.91 (13.73×) Det. Faster RCNN37.4 37.0 (4.86×) 36.1 (6.81×) RetinaNet 36.5 36.3 (3.11×) Div. 6.2 Optimization level We apply GACT on various computer vision tasks, including image classiﬁcation and object detection, as shown in Fig. 2. We also vary the average bits used by the adaptive algorithm to explore the memory accuracy trade-oﬀ. On both tasks, GACT L1 achieves comparable ( <0.5% accuracy drop) or even better results than the full precision training, while reducing activation memory by up to 7.44 ×. Here, we list the accuracy of FP32 as the strongest accuracy baseline. For other lossy methods we consider in Sec. 6.3, the accuracy is no better than FP32, and we list their training accuracy in Appendix C. Notice that here GACT Adapt 2bit diverges on the detection task. This is because, as shown in Sec.3.2, although ACT has unbiased gradients, the compression error and learning rate aﬀect the convergence. When using 2 bit, the compression error is large and the learning rate has to be reduced accordingly to guarantee convergence. However, we do not want to slow training by decreasing the learning rate. All experiments are run with the same learning rate as the full precision. Therefore when compression error is large, the training diverges. Furthermore, we observe that the memory reduction varies among networks because GACT does not quantize intermediate states, and the size of intermediate states diﬀers between networks. For example, in VGG11, when the batch size is 128, GACT reduces the saved tensor size from 5889MB to 2080MB, among which 78% (1494MB) is used to store the intermediate index for the max-pooling layer that is not quantized by GACT. Next, we demonstrate the ﬂexibility of GACT by applying it to a wider variety of natural language processing (NLP) and graph machine learning (Graph) tasks. We run multiple seeds for each task, and we report the mean ±std of accuracy/F1 across runs as shown in Tab. 3. We include the detailed experimental setup in Appendix B. For both NLP and Graph tasks, GACT L1 achieves comparable training results with FP32, introducing less than 0.3% accuracy/F1-score drop, while reducing activation memory by 4.18 ×to 7.93×. Moreover, the results are stable across runs, introducing similar accuracy variance as FP32. We also show the training results of ﬁx-4bit quantization, where all tensors are uniformly quantized with 4 bits. As shown in Tab. 3, ﬁx-4 bit quantization causes signiﬁcant accuracy/F1-score loss on various graph models. For Bert-large, ﬁxed-4 bit quantization works ﬁne because all the context tensors have similar sensitivity. On the other hand, GACT L1, using a similar amount of memory as always quantizing each layer to 4 bits, still performs on par with full precision training on all the models. This shows the necessity of using adaptive algorithms to assign bits based on tensor sensitivity for stabilized training. Moreover, for Bert-large and three graph models (GCN/GAT/GCNII), GACT converges and gives lossless results with 3 bits. Remarkably, across all the graph models, training with 2-bit GACT causes little accuracy loss ( <1%). This shows the robustness of our adaptive algorithm. 6.3 Memory Saving and Computational Overhead Settings and baselines. We implement the benchmark with PyTorch 1.10 and measure the memory saving and overhead of GACT on an AWS g4dn.4xlarge instance, which has a 16GB NVIDIA T4 GPU and 64GB CPU memory. On ResNet-50, we compare with ActNN [ 5], a dedicated quantization framework for convolutional NNs, and DTR [ 21], a state-of-the-art rematerialization method for dynamic graphs. “swap” is a simple swapping strategy that swaps all activations to the CPU. For Bert-large, we also show the results on Mesa [ 7], a memory-saving resource-eﬃcient training framework for transformers, and ZeRO-Oﬄoad [ 37], a highly optimized system for training large-scale language models. Gradient checkpointing uses the default checkpointing policy provided by the transformer library [ 38], where only the input to each transformer block is saved before the backward pass. On Swin-tiny, we only include Mesa and swap because other baselines 9Table 3: Accuracy and activation compression rate for NLP and Graph tasks. Accuracy that drops > 1% is in italic font. Model Dataset FP32 Fix 4bit GACT Adapt 4bit (L1)GACT Adapt 3bitGACT Adapt 2bit GCN Flickr 51.17±0.19 50.93±0.16 (7.56×) 51.08±0.18 (7.93×) 51.14±0.18 (11.34×) 51.20±0.18 (17.56×) Reddit 95.33±0.07 94.42±0.11 (7.55×) 95.32±0.07 (7.90×) 95.31±0.07 (9.70×) 95.34±0.06 (13.68×) Yelp 39.86±0.94 39.85±1.22 (5.94×) 40.06±0.74 (6.42×) 40.21±0.82 (7.46×) 39.89±1.45 (9.00×) ogbn-arxiv71.51±0.65 68.61±0.77 (7.54×) 71.35±0.36 (8.09×) 70.82±0.95 (10.45×) 70.87±0.66 (13.75×) GAT Flickr 52.40±0.28 35.24±11.90 (4.23×) 52.26±0.31 (4.34×) 51.68±1.13 (5.04×) 51.62±1.19 (5.46×) Reddit 95.95±0.06 59.37±11.48 (4.12×) 96.02±0.09 (4.29×) 95.96±0.06 (4.64×) 95.82±0.06 (5.24×) Yelp 52.41±0.69 36.09±13.70 (4.04×) 52.18±0.38 (4.18×) 51.63±0.83 (4.53×) 51.15±0.53 (5.24×) ogbn-arxiv71.68±0.54 54.64±5.62 (5.04×) 71.80±0.47 (5.09×) 71.47±0.50 (6.14×) 71.21±0.68 (6.98×) GCNII Flickr 52.37±0.16 52.28±0.16 (4.84×) 52.31±0.16 (4.91×) 52.36±0.16 (5.54×) 52.23±0.15 (6.44×) Reddit 96.32±0.24 86.50±1.08 (4.51×) 96.11±0.22 (4.52×) 96.01±0.33 (5.16×) 95.54±0.29 (5.92×) Yelp 62.33±0.20 62.21±0.22 (5.26×) 62.28±0.26 (5.34×) 62.53±0.36 (6.29×) 62.33±0.37 (7.28×) ogbn-arxiv72.52±0.12 44.57±5.01 (6.54×) 72.28±0.35 (6.74×) 72.22±0.28 (7.98×) 71.74±0.26 (10.24×) Bert- large MNLI 86.74±0.24 85.98±0.16 (7.55×) 86.61±0.11 (7.38×) 86.68±0.08 (9.13×) 84.24±0.74 (12.87×) SST-2 93.69±0.30 93.46±0.23 (7.55×) 93.54±0.52 (7.30×) 93.20±0.37 (9.05×) 91.90±1.04 (12.91×) MRPC 88.20±0.02 87.36±0.19 (7.55×) 87.90±0.10 (7.40×) 87.69±0.07 (9.19×) 82.54±0.38 (12.91×) QNLI 92.29±0.14 92.34±0.07 (7.55×) 92.44±0.07 (7.42×) 92.43±0.31 (9.19×) 90.74±0.13 (12.95×) Table 4: Largest models GACT can train with 16G GPU memory. In ResNet (batch size=64), D (depth): number of layers, W (width): base width of the bottleneck block, R (resolution): width and height of input images. In Bert-large (batch size=16) and GCN, D (depth): number of transformer/gcn blocks, W (width): hidden size. Dim Maximum Value Throughput (TFLOPS) FP L1 L2 FP L1 L2 ResNet- 152 D 160 460 1124 0.43 0.47 0.41 W 88 304 320 0.44 0.89 0.6 R 232 548 716 0.41 0.39 0.44 Bert- large D 32 56 64 0.67 0.56 0.53 W 1280 1488 2032 0.68 0.61 0.60 GCN D 24 152 240 0.20 0.14 0.15 W 2464 3948 4244 0.36 0.38 0.40 lack the support for this network. Results. We compare the training throughput of GACT against other memory saving systems in Fig. 4. On ResNet-50, GACT achieves similar throughput as ActNN (ActNN optimization L5 is not listed because it optimizes PyTorch memory allocation, which is unrelated to quantization and can also be applied to GACT), but ActNN enables training with a larger batch size. This is expected because ActNN implements eﬃcient, customized layers for diﬀerent operators in convolutional NNs. For Bert-large, Zero-oﬄoad fails quickly because it only oﬄoads optimizer states that occupy a small portion of total memory to CPU. GACT L1 outperforms Mesa because Mesa only compresses tensors to 8 bit. When the batch is bigger, the activation size of each segment becomes the memory bottleneck and prevents gradient checkpointing from increasing the batch size. Moreover, combining GACT with gradient checkpointing and eﬃcient self-attention further reduces the peak memory, increasing the batch size by up to 24.7 ×. Meanwhile, it introduces a small throughput overhead compared with the original gradient checkpointing. Across all the network architectures, GACT enables training with a 4.2 ×to 24.9×larger batch size under the same memory budget. Network scaling. With GACT, we can construct larger models or train with a higher image resolution. Tab. 4 compares the largest model we can train against full precision. With the same batch size and memory budget, GACT can scale a ResNet-152 to 7.0 ×deeper, 3.6 ×wider or 3.0 ×higher resolution. Similarly, Bert-large can be scaled to 2.0 ×deeper or 1.6×wider. In GCN, GACT enables training 10.0 ×deeper and 1.7×wider network. Overall, GACT maintains 75% - 136% original training throughput. 6.4 Other Optimizations We evaluate the idea of combining GACT with swapping on Bert-large-cased. As shown in Tab. 5, swapping compressed tensors is faster than swapping the original ones because communication between CPU and GPU is more time-consuming than computation. Combining GACT with swapping increases training speed by 10(a) 0 200 400 600 800 Batch Size 0 50 100Training Throughput 4.3× L0 L1 L2 ResNet-50 DTR Swap ActNN GACT (b) 0 100 200 300 400 500 600 Batch Size 0 10 20Training Throughput 24.7× L0 L1 L2 CB1 CB2 Bert-large ZeroOff Swap Mesa CKPT GACT (c) 0 100 200 300 400 500 600 Batch Size 0 25 50 75Training Throughput 5.6× L0 L1 L2 Swin-tiny Mesa Swap GACT Figure 4: Training throughput vs batch size. Red cross mark means out-of-memory. The shaded yellow region denotes the batch sizes with full precision training given the memory budget. CKPT: Gradient checkpointing, ZeroOﬀ: ZeRO-Oﬄoad. Table 5: Swap and prefetch speed/memory on Bert-large. Algorithm Speed (sequence/s) Peak Mem. (MB) Total Mem. (MB) FP32 16.41 9573 9527 FP32 + swap 6.02 5215 5093 GACT swap 12.95 5426 5325 GACT swap + prefetch14.02 5426 5324 up to 2.3×. Notice here that the peak memory use of “GACT swap” is slightly higher than “FP32 + swap” because GACT does not quantize and swap intermediate states such as running mean/var of BatchNorm layer. Moreover, prefetch increases the speed by about 7% with negligible memory overhead. We next demonstrate combining GACT with gradient checkpointing (CB1). Gradient checkpointing is performed at the beginning of each transformer block, thus avoiding saving tensors generated within the block. We then apply GACT with gradient checkpointing, where the saved tensors are quantized with 4 bits. As shown in Tab. 6, the accuracy is unaﬀected. We also compare the activation memory and peak memory of CB1 and CB2 in Tab. 7. AM2 denotes the peak activation memory, which is the size of saved tensors after reforwarding the ﬁrst transformer block. When batch size = 288, compared with gradient checkpointing on full precision (FP32), CB1 and CB2 reduce the peak activation size by 4.7 ×and 5.4×respectively. 7 Conclusion This paper presents GACT, an ACT framework for generic NN architectures. We prove the convergence of GACT without prior knowledge about operator type or network architecture by analyzing a linearized Table 6: Accuracy of Bert-large-cased on SST-2 and QNLI datasets Algorithm SST-2 QNLI Algorithm SST-2 QNLI FP32 93.58 92.42 CB1 93.81 92.26 11Table 7: Memory use of diﬀerent algorithms on Bert-large. AM1: Activation size before backward, AM2: Activation size after reforwading the ﬁrst transformer block. When batch size = 288, L0 runs out of memory, and therefore it is not listed below. Batch Size Algorithm AM1(MB) AM2(MB) Peak Mem.(MB) 16 L0 4434 - 9573 FP32 + CKPT210 394 5541 CB1 37 99 5286 CB2 31 79 5269 288 FP32 + CKPT3783 7092 12885 CB1 515 1497 8251 CB2 486 1307 8102 approximation of ATC’s gradients. With the adaptive algorithm, GACT achieves negligible accuracy loss on various tasks, reducing activation memory by up to 8.1 ×and enabling training with up to 24.7 ×batch size compared with full precision training. Acknowledgements This work was supported by the National Key Research and Development Project of China (No. 2021ZD0110502); NSF of China Project (No. 62106120), by the National Science Foundation through grants IIS-1955488, IIS-2027575, CCF-1723352, ARO W911NF2110339, ONR N00014-21-1-2724, and DOE award DE-SC0016260. We would also like to acknowledge partial support from DARPA, IARPA, the Sloan Foundation, NSF, and ONR. Our conclusions do not necessarily reﬂect the position or the policy of our sponsors, and no oﬃcial endorsement should be inferred. References [1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018. [2] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and eﬃcient sparsity. arXiv preprint arXiv:2101.03961 , 2021. [3] Ayan Chakrabarti and Benjamin Moseley. Backprop with approximate activations for memory-eﬃcient network training. arXiv preprint arXiv:1901.07988 , 2019. [4] Fangcheng Fu, Yuzheng Hu, Yihan He, Jiawei Jiang, Yingxia Shao, Ce Zhang, and Bin Cui. Don’t waste your bits! squeeze activations and gradients for deep neural networks via tinyscript. In International Conference on Machine Learning, pages 3304–3314. PMLR, 2020. [5] Jianfei Chen, Lianmin Zheng, Zhewei Yao, Dequan Wang, Ion Stoica, Michael W Mahoney, and Joseph E Gonzalez. Actnn: Reducing training memory footprint via 2-bit activation compressed training. In International Conference on Machine Learning , 2021. [6] R David Evans and Tor Aamodt. AC-GC: Lossy activation compression with guaranteed convergence. Advances in Neural Information Processing Systems , 34, 2021. [7] Zizheng Pan, Peng Chen, Haoyu He, Jing Liu, Jianfei Cai, and Bohan Zhuang. Mesa: A memory-saving training framework for transformers. arXiv preprint arXiv:2111.11124 , 2021. [8] R David Evans, Lufei Liu, and Tor M Aamodt. Jpeg-act: accelerating deep learning via transform-based lossy compression. In 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA), pages 860–873. IEEE, 2020. [9] Sian Jin, Guanpeng Li, Shuaiwen Leon Song, and Dingwen Tao. A novel memory-eﬃcient deep learning training framework via error-bounded lossy compression. In 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming , pages 485–487, 2021. 12[10] Anonymous. EXACT: Scalable graph neural networks training via extreme activation compression. In Submitted to The Tenth International Conference on Learning Representations , 2022. under review. [11] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, / Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998–6008, 2017. [12] Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. arXiv preprint arXiv:1609.02907 , 2016. [13] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. In International Conference on Learning Representations, 2018. [14] Shuang Wu, Guoqi Li, Feng Chen, and Luping Shi. Training and inference with integers in deep neural networks. In International Conference on Learning Representations, 2018. [15] Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan. Training deep neural networks with 8-bit ﬂoating point numbers. In Advances in Neural Information Processing Systems, pages 7675–7684, 2018. [16] Ron Banner, Itay Hubara, Elad Hoﬀer, and Daniel Soudry. Scalable methods for 8-bit training of neural networks. In Advances in Neural Information Processing Systems , pages 5145–5153, 2018. [17] Jianfei Chen, Yu Gai, Zhewei Yao, Michael W Mahoney, and Joseph E Gonzalez. A statistical framework for low-bitwidth training of deep neural networks. In Advances in neural information processing systems , 2020. [18] Xiao Sun, Naigang Wang, Chia-Yu Chen, Jiamin Ni, Ankur Agrawal, Xiaodong Cui, Swagath Venkatara- mani, Kaoutar El Maghraoui, Vijayalakshmi Viji Srinivasan, and Kailash Gopalakrishnan. Ultra-low precision 4-bit training of deep neural networks. In Advances in Neural Information Processing Systems , volume 33, 2020. [19] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174 , 2016. [20] Paras Jain, Ajay Jain, Aniruddha Nrusimha, Amir Gholami, Pieter Abbeel, Kurt Keutzer, Ion Stoica, and Joseph E Gonzalez. Checkmate: Breaking the memory wall with optimal tensor rematerialization. arXiv preprint arXiv:1910.02653 , 2019. [21] Marisa Kirisame, Steven Lyubomirsky, Altan Haan, Jennifer Brennan, Mike He, Jared Roesch, Tianqi Chen, and Zachary Tatlock. Dynamic tensor rematerialization. arXiv preprint arXiv:2006.09616 , 2020. [22] Chien-Chin Huang, Gu Jin, and Jinyang Li. Swapadvisor: Pushing deep learning beyond the gpu memory limit via smart swapping. In Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems , pages 1341–1355, 2020. [23] Linnan Wang, Jinmian Ye, Yiyang Zhao, Wei Wu, Ang Li, Shuaiwen Leon Song, Zenglin Xu, and Tim Kraska. Superneurons: Dynamic GPU memory management for training deep neural networks. In 23rd ACM SIGPLAN symposium on principles and practice of parallel programming , pages 41–53, 2018. [24] Xuan Peng, Xuanhua Shi, Hulin Dai, Hai Jin, Weiliang Ma, Qian Xiong, Fan Yang, and Xuehai Qian. Capuchin: Tensor-based gpu memory management for deep learning. In Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems , pages 891–905, 2020. 13[25] Olivier Beaumont, Lionel Eyraud-Dubois, and Alena Shilova. Eﬃcient combination of rematerialization and oﬄoading for training dnns. Advances in Neural Information Processing Systems , 34, 2021. [26] L´ eon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMP- STAT’2010, pages 177–186. Springer, 2010. [27] L´ eon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. SIAM Review, 60(2):223–311, 2018. [28] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during propagations. In Advances in neural information processing systems, pages 3123–3131, 2015. [29] Markus N Rabe and Charles Staats. Self-attention does not need o(n2) memory. arXiv preprint arXiv:2112.05682, 2021. [30] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations, 2015. [31] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE conference on computer vision and pattern recognition , pages 770–778, 2016. [32] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. International Conference on Computer Vision (ICCV) , 2021. [33] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In IEEE conference on computer vision and pattern recognition , pages 248–255. Ieee, 2009. [34] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll´ ar. Focal loss for dense object detection. In International Conference on Computer Vision (ICCV) , pages 2980–2988, 2017. [35] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. Advances in neural information processing systems , 28:91–99, 2015. [36] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740–755. Springer, 2014. [37] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. Zero-oﬄoad: Democratizing billion-scale model training. arXiv preprint arXiv:2101.06840, 2021. [38] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R´ emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations , pages 38–45, Online, October 2020. Association for Computational Linguistics. [39] Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. Graphsaint: Graph sampling based inductive learning method. arXiv preprint arXiv:1907.04931 , 2019. [40] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020. 14[41] Petar Veliˇ ckovi´ c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903 , 2017. [42] Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph convolutional networks. In International Conference on Machine Learning , pages 1725–1735. PMLR, 2020. [43] Yukuo Cen, Zhenyu Hou, Yan Wang, Qibin Chen, Yizhen Luo, Xingcheng Yao, Aohan Zeng, Shiguang Guo, Peng Zhang, Guohao Dai, Yu Wang, Chang Zhou, Hongxia Yang, and Jie Tang. Cogdl: Toolkit for deep learning on graphs. arXiv preprint arXiv:2103.00959 , 2021. [44] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , pages 353–355, 2018. [45] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 4171–4186, 2019. 15A Proof of Theorems A.1 Theorem 1: Convergence of ACT Assume that: A1. L(θ) is a continuous diﬀerentiable, ∇L(θ) is β-Lipschitz continuous. . A2. L(θ) is bounded below by L∗. A3. g(h; θ) is diﬀerentiable w.r.t. h and ∃b> 0, s.t. ∀θ,E∥g(Q(h(x,θ)); θ) −ˆg(h(x,θ); θ)∥≤ b. A4. ∃σ2 >0, s.t., ∀θ, Var [ˆg(h(x,θ)] ≤σ2. Then, for all η <1 2β, if we run ACT deﬁned as Eq. (2) for T iterations, then we have min t=0,...,T−1 E [ ∥∇L(θt)∥2 ] ≤4(L(θ0) −L∗) ηT + 3b2 + ηβσ2 Proof. Denote m:= ∇θL(θt), ϵ:= ˆg(h(x,θt); θt) −m, d:= g(Q(h(x; θt)); θt) −ˆg(h(x,θt); θt). Then, by A3 and A4, we have E[ϵ] = E[g(h(x,θt); θt) −∇θL(θt)] + E[⟨J(x,θt),∆Q(h(x,θt))⟩] = ⟨J(x,θt),E[∆Q(h(x,θt))]⟩= 0. (6) E [ ∥ϵ∥2 ] = ∥E[ϵ]∥2 + Var [ϵ] = Var [ˆg(h(x,θt); θt)] ≤σ2. (7) E[∥d∥] ≤b. (8) By the deﬁnitions, the ACT dynamics can be written as θt+1 ←θt −η(m+ d+ ϵ). By A1, we have L(θt+1) ≤L(θt) −η⟨m,m + d+ ϵ⟩+ βη2 2 ∥m+ d+ ϵ∥2 . (9) By Eq.s (6,8) E[⟨m,m + d+ ϵ⟩] ≥∥m∥2 −∥m∥∥d∥+ ⟨m,E[ϵ]⟩≥∥ m∥2 −∥m∥b. (10) By Eq.s (6,7,8), and ∥x+ y∥2 ≤2 ∥x∥2 + 2∥y∥2, E [ ∥m+ d+ ϵ∥2 ] = E [ ∥m+ d∥2 ] + Var [ϵ] ≤2E[∥m∥]2 + 2E[∥d∥]2 + Var [ϵ] = 2E[∥m∥]2 + 2b2 + σ2. (11) Taking expectation on both sides of Eq. (9), plug in Eq.s (10, 11), and use η <1 2β, we have E[L(θt+1)] ≤L(θt) −η(∥m∥2 −∥m∥b) + βη2 2 (2E[∥m∥]2 + 2b2 + σ2). =L(θt) −(η−βη2) ∥m∥2 + η∥m∥b+ βη2 2 (2b2 + σ2) =L(θt) −η 2 ∥m∥2 + η∥m∥b+ βη2 2 (2b2 + σ2). Completing the squares, E[L(θt+1)] ≤L(θt) −η 2(∥m∥−b)2 + βη2 2 (2b2 + σ2). Take expectation on both sides and sum up for t= 0,...,T −1, E[L(θT)] −L(θ0) ≤−η 2 T−1∑ t=0 E(∥∇L(θt)∥−b)2 + βη2T 2 (2b2 + σ2). Reorganize the terms, Et [ E(∥∇L(θt)∥−b)2 ] ≤2(L(θ0) −L(θT)) ηT + ηβ(2b2 + σ2). Let t∗= argmintE[∥∇L(θt)∥], and use A1, we have E(∥∇L(θt∗)∥−b)2 ≤2(L(θ0) −L∗) ηT + ηβ(2b2 + σ2). 16Use (a+ b)2 ≤2a2 + 2b2, we have E [ ∥∇L(θt∗)∥2 ] ≤4(L(θ0) −L∗) ηT + (2βη+ 2)b2 + ηβσ2 ≤4(L(θ0) −L∗) ηT + 3b2 + ηβσ2. A.2 Proposition 1: The Linearization Error Proof. Consider the gradient function g(Q(h(x; θ); θ)), whose output is a P-dimensional vector. Since it is twice diﬀerentiable, we construct the Taylor’s expansion at h(x; θ) with Lagrange remainder: ∃H1,...,H P,s.t., ∀i, gi(Q(h(x; θ)); θ) = gi(h(x,θ); θ) + Ji(x,θ)∆h(x,θ) + ∆h(x,θ)⊤Hi∆h(x,θ), where Ji(h(x; θ),θ) := ∂gi(h(x;θ);θ) ∂h . By the assumption, there exists P >0, such that the linearization error is ∥g(Q(h(x; θ)); θ) −ˆg(h(x; θ); h(x; θ),θ)∥1 = P∑ i=1 ∆h(x,θ)⊤Hi∆h(x,θ) ≤γP ∥∆h(x,θ)∥2 . Taking expectation, E[∥g(Q(h(x; θ)); h(x; θ),θ) −ˆg(h(x; θ); θ)∥2] ≤E[∥g(Q(h(x; θ)); θ) −ˆg(h(x; θ); h(x; θ),θ)∥1] ≤γPVar [∆h(x,θ)] = O(Var [∆h(x,θ)]). A.3 Proposition 2: The Order of the Variance The following proposition is convenient for isolating the diﬀerent noise sources. Proposition A. (Law of Total Variance) Var [X] = E[Var [X |Y]] + Var [E[X |Y]] . Proof. By deﬁnition Var [ˆg(h(x; θt); h(x; θ),θt)] = Var [g(h(x,θ); θ)] + Var [J(h(x; θ),θ)∆h(x,θ)] , where Var [g(h(x,θ); θ)] is the noise introduced by subsampling the data x. By law of total variance, Var [J(h(x; θ),θ)∆h(x,θ)] = EX[VarQ[J(h(x; θ); θt)∆h(x,θ)]] + VarX[EQ[J(h(x; θ); θt)∆h(x,θ)]]   =0 , where VarQ[J(h(x; θ); θt)∆h(x,θ)] =EQ [ ∥J(h(x; θ); θt)∆h(x,θ)∥2 ] ≤EQ [ ∥J(h(x; θ); θt)∥2 ∥∆h(x,θ)∥2 ] = ∥J(h(x; θ); θt)∥2 EQ [ ∥∆h(x,θ)∥2 ] = O(Var [∆h(x,θ)]) . A.4 Proposition 3: The Structure of the Variance Before investigating the structure of VarQ[J(x; θt)∆h(x,θ)], let’s do some recap: the parameter θt is a P-dimensional vector; the context diﬀerence ∆ h(x,θ) is a D-dimensional vector, and J(x; θt) is a P ×D matrix. Recall that ∆ h(x,θ) is the concatenation of L-vectors, ∆h(l)(x,θ), and let J(l)(x,θ) := ∂g ∂h(l) g ( (h(l)(x; θ))L l=1,θ ) , which is a P ×Dl matrix. Furthermore, let h(l) j (x,θ) be the j-th dimension, and J(l) j (x,θ) be its j-th column. To proceed, we need to make the following assumptions to the compressor Q(·) : RD →RD: B1: The compressed result is element-wise uncorrelated. That is, for any i̸= j, Cov [Q(h)i,Q(h)j] = 0. B2: For compressing a vector h to b bits, the compression variance of each dimension can be written in the form Var [Q(h)j] ≤Rj(h)S(b), where S(·) is a known function. Both assumptions can be achieved by a stochastic rounding [28] quantizer, where Q(h)j = { T−1 h,b (⌈Th,b(hj)⌉) w.p. Th,b(hj) −⌊Th,b(hj)⌋ T−1 h,b (⌊Th,b(hj)⌋) otherwise , 17where Th,b(hj) = (2b−1) hj−minj h maxj h−minj h. Since each dimension is quantized independently, B1 is met. Moreover, Var [Q(h)j] ≤1 4 (maxjh−minjh (hj −minjh) )2 (2b −1)−2 = Rj(h)S(b), where Rj(h) = 1 4 (maxjh−minjh (hj −minjh) )2 , S (b) = (2b −1)−2. Proof. By deﬁnition, J(h; θ)∆h= L∑ l=1 Dl∑ j=1 J(l) j (h; θt)∆h(l) j . Using Assumption B1, we have VarQ[J(h; θ)∆h] = EQ    L∑ l=1 Dl∑ j=1 J(l) j (h; θt)∆h(l) j  2  = L∑ l=1 Dl∑ j=1 EQ [J(l) j (h; θt)∆h(l) j  2] . = L∑ l=1 Dl∑ j=1 J(l) j (h; θt)  2 VarQ [ ∆h(l) j ] Using Assumption B2, we have VarQ[J(h; θ)∆h] ≤ L∑ l=1 Dl∑ j=1 J(l) j (h; θt)  2 Rl(h)S(bl) = L∑ l=1 cl(h,θ)S(bl), where cl(θ,h) := Rl(h) J(l)(h; θt) 2 F. B Experiment Setup B.1 Node classiﬁcation task on graphs We conduct experiments on four node classiﬁcation datasets with standard splits, including Flickr, Reddit, Yelp from GraphSAINT [39], and ogbn-arxiv from Open Graph Benchmark (OGB) [ 40]. The four datasets cover extensive downstream applications with diﬀerent scales. We use accuracy as the evaluation metric for multi-class classiﬁcation and micro-F1 for multi-label classiﬁcation. We run ten seeds (0 to 9) and report the average accuracy across runs. We evaluate GACT on three representative GNN models, including GCN [ 12], GAT [41], and GCNII [ 42] under the full-batch training setting. All three models are implemented by CogDL [ 43], a toolkit for graph neural networks. B.2 Text classiﬁcation task We select four largest datasets, MNLI, QQP, SST-2, and QNLI, from the GLUE benchmark [ 44]. The four datasets cover diﬀerent aspects of natural language understanding, including sentiment classiﬁcation, natural language inference and paraphrase detection. We use the mainstream transformer implementation [ 38] to train Bert-large [45]. We run three seeds (42, 43, 44) and report F1 for QQP, accuracy for the others. C Training Accuracy of Baselines For all the baselines we compared in Sec. 6.3, only ActNN, Mesa, and ZeRO-Oﬄoad are lossy methods. All other methods are lossless and have the same training accuracy as FP32. For ResNet-50 on ImageNet, the training accuracy for FP32, GACT, ActNN L2, and ActNN L3 are 77.3, 77.0, 77.4, and 76.9. For Bert-Large on SST-2, the accuracy for FP32, GACT, Mesa, and ZeRO-Oﬄoad are 93.7, 93.5, 93.8, and 93.3. For Swin-tiny on ImageNet, the training accuracy for FP32, GACT, and Mesa are 81.2, 81.0, and 81.3 respectively. 18",
      "meta_data": {
        "arxiv_id": "2206.11357v4",
        "authors": [
          "Xiaoxuan Liu",
          "Lianmin Zheng",
          "Dequan Wang",
          "Yukuo Cen",
          "Weize Chen",
          "Xu Han",
          "Jianfei Chen",
          "Zhiyuan Liu",
          "Jie Tang",
          "Joey Gonzalez",
          "Michael Mahoney",
          "Alvin Cheung"
        ],
        "published_date": "2022-06-22T20:06:23Z",
        "pdf_url": "https://arxiv.org/pdf/2206.11357v4.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper introduces GACT, a generic Activation Compressed Training (ACT) framework designed to support a broad range of machine learning tasks across various neural network architectures without requiring specialized domain knowledge. GACT addresses the problem of extensive memory requirements for training large neural networks by compressing activations. The key contributions include a novel general convergence theory for ACT, an adaptive algorithm that automatically estimates the sensitivity of each tensor to compression and selects an optimal compression strategy at runtime, and an efficient PyTorch library implementation with an easy-to-use API. GACT achieves significant memory reduction (up to 8.1x) and enables training with substantially larger batch sizes (4.2x to 24.7x) with negligible accuracy loss across diverse tasks including image classification, object detection, text, and graph node classification.",
        "methodology": "GACT abstracts forward propagation into loss and context functions, training models with stochastic gradient descent (SGD) using compressed context. The theoretical foundation involves analyzing a linearized version of ACT's approximate gradient, proving convergence without prior knowledge of operator type or model architecture. It relies on an unbiased stochastic compressor and a first-order Taylor expansion of the gradient function to show that linearization error is bounded by compression error. To adapt the compression rate for each tensor, GACT proposes an algorithm that estimates tensor sensitivity by analyzing its impact on gradient variance. This is achieved by formulating an integer programming problem to minimize variance under a total bits budget, assuming element-wise uncorrelated compressed results and a specific structure for compression error (e.g., using a stochastic rounding quantizer). Tensor sensitivity (`cl`) is numerically computed using empirical variance from two gradient evaluations with controlled random seeds. The framework is implemented as a PyTorch library using low-level hooks to capture and compress context tensors, supporting arbitrary and custom operators. It includes optimizations like parallel swap and prefetch for offloading compressed tensors to CPU, integration with gradient checkpointing, and memory-efficient self-attention for transformers.",
        "experimental_setup": "GACT was evaluated across diverse machine learning tasks and models: Image classification (VGG-11, ResNet-50, Swin-Tiny on ImageNet), object detection (RetinaNet, Faster R-CNN on Coco), natural language processing (Bert-large on GLUE benchmark datasets: MNLI, QQP, SST-2, QNLI), and graph neural networks (GCN, GAT, GCNII on Flickr, Reddit, Yelp, ogbn-arxiv datasets). Evaluation metrics included accuracy (for most tasks) and micro-F1 score (for multi-label classification and QQP). Baselines compared against included full precision (FP32), fixed-bit quantization, and other memory-saving methods like ActNN, DTR, simple swapping, Mesa, and ZeRO-Offload, and Gradient Checkpointing. Experiments were conducted on an AWS g4dn.4xlarge instance (16GB NVIDIA T4 GPU, 64GB CPU memory) using PyTorch 1.10. Various GACT optimization levels (L0-L2, CB1, CB2) were tested, and results were averaged over multiple seeds (10 for graph tasks, 3 for text tasks).",
        "limitations": "GACT Adapt 2-bit could diverge in object detection tasks (e.g., RetinaNet) because large compression errors might necessitate reduced learning rates, which were not applied to avoid slowing training. GACT does not quantize all intermediate states, such as max-pooling layer indices or BatchNorm running mean/variance, which can limit overall memory reduction in certain network architectures (e.g., 78% of saved tensor size in VGG-11 was not quantized). While designed to be negligible, combining GACT with gradient checkpointing can introduce training noise due to recomputation starting from quantized segment inputs. The adaptive algorithm relies on the observation that tensor sensitivity (`cl`) remains stable during training, which allows for periodic updates but is an assumption that might not hold universally. The theoretical convergence guarantees and variance structure derivations rely on specific assumptions about the compressor's behavior (element-wise uncorrelated results, specific error structure, idempotence).",
        "future_research_directions": "Not mentioned"
      }
    },
    {
      "title": "GACT: Activation Compressed Training for Generic Network Architectures",
      "abstract": "Training large neural network (NN) models requires extensive memory\nresources, and Activation Compressed Training (ACT) is a promising approach to\nreduce training memory footprint. This paper presents GACT, an ACT framework to\nsupport a broad range of machine learning tasks for generic NN architectures\nwith limited domain knowledge. By analyzing a linearized version of ACT's\napproximate gradient, we prove the convergence of GACT without prior knowledge\non operator type or model architecture. To make training stable, we propose an\nalgorithm that decides the compression ratio for each tensor by estimating its\nimpact on the gradient at run time. We implement GACT as a PyTorch library that\nreadily applies to any NN architecture. GACT reduces the activation memory for\nconvolutional NNs, transformers, and graph NNs by up to 8.1x, enabling training\nwith a 4.2x to 24.7x larger batch size, with negligible accuracy loss. We\nimplement GACT as a PyTorch library at\nhttps://github.com/LiuXiaoxuanPKU/GACT-ICML.",
      "full_text": "GACT: Activation Compressed Training for Generic Network Architectures Xiaoxuan Liu, Lianmin Zheng, Dequan Wang, Yukuo Cen, Weize Chen, Xu Han, Jianfei Chen, Zhiyuan Liu, Jie Tang, Joey Gonzalez, Michael Mahoney, Alvin Cheung University of California, Berkeley xiaoxuan liu@berkeley.edu, jianfeic@tsinghua.edu.cn Abstract Training large neural network (NN) models requires extensive memory resources, and Activation Compressed Training (ACT) is a promising approach to reduce training memory footprint. This paper presents GACT, an ACT framework to support a broad range of machine learning tasks for generic NN architectures with limited domain knowledge. By analyzing a linearized version of ACT’s approximate gradient, we prove the convergence of GACT without prior knowledge on operator type or model architecture. To make training stable, we propose an algorithm that decides the compression ratio for each tensor by estimating its impact on the gradient at run time. We implement GACT as a PyTorch library that readily applies to any NN architecture. GACT reduces the activation memory for convolutional NNs, transformers, and graph NNs by up to 8.1×, enabling training with a 4.2× to 24.7× larger batch size, with negligible accuracy loss. We implement GACT as a PyTorch library at https://github.com/LiuXiaoxuanPKU/GACT-ICML. 1 Introduction In recent years, we have witnessed the trend of using larger and larger neural network (NN) models to deliver improved accuracy and generalization in various machine learning tasks [1, 2]. However, training these models requires a considerable amount of on-device GPU memory. Unfortunately, the increase of GPU memory capacity has been relatively slow, leading to a fundamental barrier to the development of large NN models. Activation Compressed Training (ACT) is a promising approach to reduce the memory footprint of models during training. As all layers’ activations need to be kept in the memory for computing the gradients during training, ACT reduces memory consumption by compressing these saved activations. Prior work [3, 4, 5, 6] has shown the eﬀectiveness of ACT by reducing activation footprint by up to 12 ×with 2-bit activations. Although ACT has already demonstrated impressive compression capabilities, previous work on ACT is restricted to speciﬁc NN architectures. For example, ActNN [ 5] is a quantization framework for convolutional NNs only; Mesa [ 7] proposes a per head/layer quantization method for vision transformers; and AC-GC [ 6] derives convergence error bound for diﬀerent types of operators separately. Developing a generic ACT framework is challenging. Theoretically, convergence guarantees must be made without assumptions on the network architecture. Algorithmically, the framework should ﬁnd eﬀective compression strategies for all kinds of networks automatically. From the system perspective, the framework should support arbitrary NN operations, including user-deﬁned ones. In this work, we propose GACT, a general framework for ACT that is agnostic to the NN architecture. Neither specialized mathematical derivations nor customized implementation is needed to support diﬀerent operators. To enable this, we develop a general convergence theory by analyzing the stochastic gradient (SG) introduced by ACT. We show that the SG can be well approximated by a linearized version, which is unbiased to stochastic compressors. The variance of the linearized gradient has a particularly simple structure that allows a numerical algorithm to predict the variance given a compression strategy. Then, we generate the strategy by approximately solving an integer program. 1 arXiv:2206.11357v4  [cs.LG]  3 Sep 2022We implement our method as a library based on PyTorch that can be quickly integrated into real-world machine learning systems. The library also provides several optimization levels to explore the trade-oﬀ between memory and speed. We demonstrate the ﬂexibility and eﬃciency of GACT on various tasks, including image classiﬁcation, object detection, text, and graph node classiﬁcation. Our evaluation shows that GACT can reduce activation memory by up to 8.1 ×, enabling training with a 24.7 ×larger batch size on the same GPU. In sum, our main contributions are as follows: • We propose a general convergence theory for ACT. • We develop an algorithm that automatically estimates the sensitivity of each compressed tensor and selects the optimal compression strategy. • We build eﬃcient implementation of GACT in PyTorch with an easy-to-use API that can also be combined with other memory-saving techniques seamlessly. 2 Related Work Activation Compressed Training.ACT has been applied to convolutional NNs using diﬀerent compressors, such as quantizers [3, 4, 5], JPEG [8], or scientiﬁc data compression algorithms [ 9, 6]. ACT is also applied to transformers [7] and graph NNs [10]. However, the existing theory for ACT [ 3, 4, 5, 6] relies on the case-by-case analysis of speciﬁc network operators, such as convolution, ReLU, and batch normalization. It also requires dedicated implementations for each operator. On the contrary, GACT focuses on the generality of activation compressed training, not a speciﬁc quantizer design, which is the main topic of previous work. Instead of assuming that the network is a stack of layers, GACT formulates the problem as a computational graph of operators. This is general enough to cover transformers [11], graph NNs [12], second-order derivatives, and unknown future architectures. Reduced Precision Training. Apart from ACT, reduced precision training [13, 14, 15, 16, 17, 18] performs calculations directly on low precision data, reducing the computation cost and memory footprint simultaneously. To achieve this, specialized kernels are used to calculate on low precision data. In contrast, ACT only considers storage, and it can thus use more ﬂexible compression strategies and achieve a much better compression ratio with the same accuracy loss. Memory-Eﬃcient Training. Gradient checkpointing [19, 20] trades computation for memory by dropping some of the activations in the forward pass from memory and recomputing them in the backward pass. Swapping [21, 22, 23, 24] oﬄoads activation or model parameters to an external memory (e.g., CPU memory). Recent work [25] explores the possibility of combining the gradient checkpointing and swapping. All these methods save memory by storing fewer tensors on the GPU. In contrast, GACT compresses the saved tensors and is complementary to these approaches. Moreover, the generality of GACT enables easy combination with these methods, which we explore in this paper. 3 Formulation We ﬁrst present the mathematical formulation of our activation compressed training (ACT) framework. As we would like to develop a general ACT algorithm, applicable to a wide range of NN architectures, we make minimal assumptions on our formulation. Throughout the paper, we deﬁne the variance of a vector x as Var [x] = E [ ∥x∥2 ] −∥E[x]∥2. 3.1 Activation Compressed Training In this work, we abstract the forward propagation as two functions ℓ(x; θ) and h(x; θ). Both take a datum x and the model parameter θ as the input. The loss function ℓ(x; θ) outputs the loss of the network θ on datum x. The context function h(x; θ) outputs tensors to be stored in the memory for computing the gradients, which are referred as the context. Assume that the context consists of L tensors, where each tensor h(l)(x; θ) is represented by a ﬂattened Dl-dimensional vector. Denote h(x; θ) = ( h(l)(x; θ))L l=1. Our notations are somewhat unconventional in the sense that we do not explicitly deﬁne each layer’s activation. We do not even assume that there is a NN. It could be any computational graph that saves context tensors. Given a dataset X = {xn}N n=1, deﬁne the batch loss L(θ) := 1 N ∑N n=1 ℓ(x; θ). The dataset can be equivalently represented as an empirical data distribution pX(x) := 1 N ∑N n=1 δ(x−xn), where δ is the Dirac 2Type\tequation\there. ConvBNConv……ReLU ReLU……BNConvConv Compressed context tensors (GPU) ℓ(𝑥;𝜃) Pack hook Unpackhook context ℎ(𝑥;𝜃) Swap outSwap in 𝑄(ℎ(𝑥;𝜃)) Forward Computation Graph Backward Computation Graph CPU Memory bits 𝑔(𝑄ℎ(𝑥;𝜃);𝜃) Adaptive AlgorithmCompressorGACT Decompressor 𝑥,\t gradient𝑔(𝑄ℎ(𝑥;𝜃);𝜃) (𝜃)model Figure 1: The architecture of GACT. delta function. The batch loss can be written as L(θ) = EX[ℓ(x; θ)], where EX denotes for taking expectation over pX. The network is trained with stochastic gradient descent (SGD) [ 26]. Starting from an initial model θ0, at the t-th iteration, SGD updates the model with: θt+1 ←θt −η∇θℓ(x; θt), (1) where η is a learning rate, and the SG ∇θℓ(x; θ) is computed on a random datum x ∼pX. Notice that EX[∇θℓ(x; θ)] = ∇θL(θ), i.e., the SG is an unbiased estimator of the batch gradient ∇θL(θ). Crucially, the SG can be written in the form ∇θℓ(x; θt) = g(h(x; θt); θt). In other words, the back propagation only depends on the forward propagation through the context h(x; θt). The entire context must be kept in memory for computing the gradients. The context dominates the memory consumption in many applications. ACT reduces the training memory footprint by compressing the context. Let Q(h) be a compressor, which converts h to compact formats while keeping Q(h) ≈h. Then, ACT computes the gradient with compressed context: θt+1 ←θt −ηg(Q(h(x; θt)); θt). (2) We refer to g(Q(h(x; θt); θt) as the activation compressed (AC) gradient. ACT is signiﬁcantly more memory eﬃcient then the plain SGD, Eq. (1), since it only needs to store a compressed version of the context. Suppose the original context h(x; θt) consists of 32-bit ﬂoating point tensors, and Q(·) is a compressor which quantizes tensors to 2-bit integers, ACT will reduce the context memory by 16 ×. Fig. 1 illustrates the computational graph of ACT with these notations. In the following presentation, we might denote h(x,θ) simply by h when there is no confusion. 3.2 Convergence of ACT ACT is a lossy approximation of SGD, as it uses an approximate gradient g(Q(h); θ). Therefore, some kind of theoretical guarantee is required for ACT to be useful. Fortunately, analyzing ACT is made signiﬁcantly simpler by introducing an unbiased stochastic compressor Q(·), such that EQ[Q(x)] = x for any x. EQ[·] means taking expectation over the compressor. In this way, g(Q(h); θ) can be viewed as a stochastic estimator of the batch gradient ∇L(θ), but the randomness comes not only from the datum x but also the compressor Q(·). Therefore, ACT is still an SGD algorithm. Standard analytical tools for SGD [ 27] are applicable for studying ACT. SGD algorithms have particular good properties when the SG is unbiased. In our case, this means EQ[g(Q(h); θ)] = g(h; θ). However, the SG is biased general, even when the stochastic compressor itself is unbiased.1 The key technique in this work is to construct an unbiased approximation of the AC gradient by linearizing 1Consider the exampleg(h) =I(h ≥0.5), whereh ∈[0, 1] and its AC gradientg(Q(h)) =I(Q(h) ≥0.5) with the compressor Q(h) ∼Bernoulli(h). Then, E [g(Q(h))] =P(Q(h) = 1) =h ̸= g(h). 3the gradient function g(·; θ). Consider the ﬁrst-order Taylor expansion of g(·; θ) at h: ˆg(Q(h); h,θ) := g(h; θ) + J(h,θ)∆h, (3) where J(h,θ) := ∂g(h;θ) ∂h is a Jacobian matrix, ∆ h := Q(h) −h is the compression error. We further denote ˆgxθ(Q(h); h) := ˆg(Q(h); h,θ)|h=h(x;θ) and Jxθ(h) := J(h,θ)|h=h(x;θ) for short. Since E[∆h(x; θ)] = 0, ˆgxθ(Q(h); h) is an unbiased SG, Furthermore, the approximation error is small: Proposition 1. Assuming that g(h; θ) is twice diﬀerentiable w.r.t. h, and the second order derivative is bounded, then E[∥g(Q(h); θ) −ˆgxθ(Q(h); h)∥2] = O(VarQ[∆h]). Since ∆h itself is unbiased, VarQ[∆h] = EQ [ ∥∆h∥2 ] is simply the expected compression error. Prop. 1 implies that the linearization error is bounded by the compression error. The linearized gradient ˆg is accurate if the compression is accurate. Using ˆg as a bridge, we arrive in the following convergence theorem: Theorem 1. Assume that: A1. L(θ) is a continuous diﬀerentiable, ∇L(θ) is β-Lipschitz continuous. A2. L(θ) is bounded below by L∗. A3. g(h; θ) is diﬀerentiable w.r.t. h and ∃b> 0, s.t. ∀θ,E∥g(Q(h(x; θ)); θ) −ˆgxθ(Q(h); h)∥≤ b. A4. ∃σ2 >0, s.t., ∀θ, Var [ˆgxθ(Q(h); h)] ≤σ2. Then, for all η <1 2β, if we run ACT deﬁned as Eq. (2) for T iterations, then we have min t=0,...,T−1 E [ ∥∇L(θt)∥2 ] ≤4(L(θ0) −L∗) ηT + 3b2 + ηβσ2 Remark: The analytical technique used in Thm. 1 is rather standard, see Thm. 4.8 in [ 27]. However, we consider the variance term σ2 of the linearized gradient, rather than the SG itself. This formulation brings better analytical properties and an adaptive algorithm for determining the compression scheme, as we shall see soon in Sec. 4. The convergence of ACT is aﬀected by both the linearization error (A3) and the variance of the unbiased gradient ˆg(·; θ) (A4). The latter is characterized as: Proposition 2. Var [ˆgxθ(Q(h); h)] = VarX[g(h; θ)] + EX[VarQ[ˆgxθ(Q(h); h)]] , where the second term on the RHS equals to EX[VarQ[Jxθ(h)∆h]] = O(VarQ[∆h]) . Prop. 2 separates the variance from diﬀerent noise sources. VarX[g(h(x,θ); θ)] is the variance raised by random sampling of data (“sampling variance”). EX[VarQ[Jxθ(h)∆h(x,θ)]] is the variance raised by compression. Now, the convergence in Thm. 1 is depicted by 3 b2 + ηβσ2. By Prop. 1, b2 = O(VarQ[∆h]2). By Prop. 2, σ2 = O(1) +O(VarQ[∆h]), since the sampling variance is not aﬀected by compression. Therefore, when the compression is accurate (∆ h →0), the impact of the linearization error is negligible, and the variance of the unbiased gradient dominates. ACT behaves as if the AC gradient is unbiased. 4 Adapting the Compression Rate In a network, some context tensors (such as those stored for computing the cross entropy loss) are extremely sensitive, a small amount of compression would result in diverged training, while other tensors are quite robust to compression. Therefore, we must apply diﬀerent amounts of compression for each context tensor. As a general framework, we have no prior knowledge of the users’ model architecture, so we designed an algorithm to infer the sensitivity for each context tensor and determine their compression rate automatically. There is a tradeoﬀ between the compression error and the storage requirement. We represent the storage requirement of the compressed context in bits per dimension . We assume that bl bits/dim. are used for compression h(l), and Qbl(h(l)) be the compression result. Let b = (bl)L l=1 be a compression scheme, Qb(h) := {Qbl(h(l))}L l=1, and ∆bh= Qb(h) −h. 44.1 Structure of Variance As discussed in Sec. 3.2, when the compression is relatively accurate, the variance plays the main role in determining the convergence. Therefore, we would like to investigate how the compression scheme would impact the variance. Formally, we are interested in: V(b; h,θ) := VarQ[ˆg(Qb(h); h,θ)] . Once V(b,h; θ) is known, we can ﬁnd the minimum variance compression scheme under a given total bits budget B, by solving the integer programming problem: min b V(b; h(x; θ),θ), s.t. L∑ i=1 blDl ≤B, (4) where Dl is the dimensionality of h(l). To proceed, we need the following assumptions on the compressor Qb(·): Assumption B1: The compressed result is element-wise uncorrelated. That is, for anyi̸= j, Cov [Qb(h)i,Qb(h)j] = 0. Assumption B2: For compressing h(l)(x; θ) to bl bits/dim., the compression error can be written in the form Var [ ∆blh(l)(x; θ)j ] ≤Rlj(x; θ)S(bl), where S(bl) is a known function. This isolates the eﬀect of bl through the unary factor S(bl). Both assumptions can be achieved by a stochastic rounding quantizer [28], where Rlj(x; θ) = 1 4 ( maxkh(l) k −minkh(l) k )2 and S(b) = (2bl −1)−2. See Appendix A.4 for the derivations. The following theorem reveals the structure of the variance: Theorem 2. Under assumptions B1, B2, there exists a family of functions {cl(h,θ)}L l=1, such that the compression variance can be written in the form V(b; h,θ) ≤ L∑ l=1 cl(h,θ)S(bl). (5) 4.2 Computing Sensitivity Thm. 2 reveals two good properties of the variance: (1) the impact of compressing diﬀerent context tensors simply sums up, without aﬀecting each other; and (2) the compression scheme only impacts the variance through S(bl). Both properties are brought about by linearization. Since S(·) is a known function, we only need to know cl(h,θ) to solve problem Eq. (4). cl(h,θ) can be understood as the sensitivity of the AC gradient to the compression of the l-th tensor. We can compute cl(h,θ) numerically by leveraging the idempotence of compressing a tensor: Assumption B3: If h= Q(h′) for some h′with non-zero probability, then Q(h) = h and VarQ[Q(h)] = 0. Let Q¬(l) b (h) = {Qb1 (h(1)),...,h (l),...,Q bL(h(L))}be some tensors, where every tensor except h(l) is compressed. Plug h= Q¬(l) b (h) into Eq. (5), and use B3, we have V(b; Q¬(l) b (h),θ) ≤cl(Q¬(l) b (h),θ)S(bl). The left hand side can be approximated by taking ˆg(Qb(h); h,θ) ≈g(Qb(h); θ). Assume that cl(·,θ) is reasonably continuous, we have cl(h,θ) ≈VarQ[g(Qb(h); θ)] |h=Q¬(l) b (h)/S(bl). The variance can be replaced by empirical variance. Alg. 1 illustrates this idea. To compute VarQ[g(Qb(h); θ)] at h= Q¬(l) b (h), we keep the random seeds ﬁxed for all the compressors except the l-th one. We compute the empirical variance by two evaluations of g(Qb(h); θ), which are two NN iterations (forward + backward propagation). Finally, we assume thatc(h,θ) remains stable for diﬀerent mini-batches h, and along the training trajectory (θt). Therefore, we maintain a cl for each tensor l, which is updated by periodically running Alg. 1. Eq. (4) is approximately solved by the O(Llog2 L) greedy algorithm [5]. Another useful feature of this approach is predicting failure (in an a posteriori manner). If the compression variance V(b; h,θ) is dominating the overall gradient variance Var [g(Q(h); θt)], compression is adding too much noise to the gradient, and the convergence might be aﬀected. The overall gradient variance can be 5Algorithm 1 Numerical algorithm for computing cl(h,θ). Require: A gradient evaluation function g(·; θ) Require: A series of L+ 1 random seeds (rl)L+1 l=1 . Require: Any compression scheme b= (bl)L l=1 ∀l, seed Q(l) with rl g0 ←g(Qb(h); θ) {First iteration} ∀l, seed Q(l) with rl seed Q(l) with rL+1 g1 ←g(Qb(h); θ) {Second iteration, with another seed } Return 1 2 ∥g0 −g1∥2 /S(bl) 1importtorch2 importgact3 4model = ... # user defined model5 controller = gact.controller(model, opt_level='L2')6  controller.install_hook()78  # trainingloop9forepochin...10 foriterin...11......12 # instruct gact how to perform forward and backward13deffwdbwdprop():14output = model(data)15loss =loss_func(output,target)16optimizer.zero_grad()17loss.backward()1819controller.iterate(fwdbwdprop) Figure 2: Usage example of GACT computed by maintaining a running mean of the gradient. If V(b; θ)/Var [ˆg(Q(h); θt)] is too large, we can raise an alert to the user to increase the storage budget. 5 System Implementation We implemented GACT as a lightweight library in PyTorch. Users can use GACT for any NN architecture with several lines of code change. GACT uses low-level PyTorch hooks to capture context tensors, so it supports arbitrary operators, including custom operators deﬁned by users. We implemented eﬃcient CUDA kernels to infer tensor sensitivity and to perform compression during run time. GACT uses the same per-group quantizer in ActNN [5] as the compressor. However, GACT diﬀers from ActNN in several aspects. ActNN relies on manual analytical deduction to compute the sensitivity for diﬀerent operators, while GACT infers tensor sensitivity automatically, as described in Sec. 4.2. Moreover, ActNN performs layer-level quantization. It has to implement an activation compressed version for each operator and substitute operators during the training (e.g., replace torch.nn.Conv2d with actnn.Conv2d). In contrast, GACT runs at tensor level and uses a single hook interface to compress saved tensors for all operators. 5.1 General API As shown in Fig. 2, the interface of GACT is straightforward and intuitive, requiring the user to (i) initialize the GACT controller and specify an optimization level (Line 5); (ii) install hooks (Line 6); and (iii) instruct GACT how to perform forward and backward propagation (Lines 13-17) and pass it as a function (fwdbwdprop) to the controller (Line 19). We require users to specify (iii) because GACT needs to numerically run the forward and backward pass to infer tensor sensitivity. Although fwdbwdprop is passed to the controller every iteration, it is only called internally every adapt interval iterations when tensor sensitivity changes. As shown in Sec. 6.1, tensor sensitivity stabilizes quickly after the ﬁrst several epochs, adapt interval can thus be set to a large number, introducing negligible impact on training speed. 65.2 System Architecture Fig. 1 shows an overview of GACT. The GACT controller has three modules: Adaptivate Algorithm; Compressor; and Decompressor. In the forward pass, the controller uses PyTorch pack hook to capture all context tensors. Then Adaptive Algorithm infers tensor sensitivity based on gradients and assigns higher bits to more sensitive tensors, as described in Sec. 4.2. The bits information is used to instruct Compressor to perform quantization. In the backward pass, Decompressor dequantizes context tensors and uses unpack hook to send the dequantized results back to the PyTorch’s auto diﬀerentiation engine. The controller is also responsible for swapping quantized tensors to the CPU and prefetching them back during the backward propagation if swapping is enabled. 5.3 Identifying Tensors to Quantize The pack hook and unpack hook process all types of context tensors, including activation, parameters trained by the optimizer, and training states such as running mean/variance used by batch normalization. To guarantee that only the activations are quantized, we ﬁlter out saved parameters by recording the data pointers of all the model parameters before training, and we skip quantization if the input tensor pointer exists in the parameter pointer set. Similarly, GACT does not quantize training states by checking if the input tensor requires gradients. However, using hooks blindly disables some memory-saving optimization. For example, in a transformer’s self-attention layer, the keys, query, value tensors are all calculated from the same input tensor. The saved objects of the three operations thus all refer to the same tensor. In this case, PyTorch triggers the pack hook three times. If we perform quantization blindly, we waste computation resources and introduce extra memory consumption because the same underlying tensor is quantized and saved more than once. GACT avoids duplication by generating footprints for each input context tensor. We use the CUDA data pointer, sampled data points, and the tensor statistics (e.g., sum) as the footprint. GACT manages all quantized context tensors and uses the footprint to diﬀerentiate them. If a tensor is already quantized, GACT will skip quantization and return previous results directly. 5.4 Parallel Swap and Prefetch To further reduce activation memory, we combine GACT with swapping. All compressed tensors are oﬄoaded to the CPU during the forward pass and swapped back in the backward pass. Here, we replace the original tensor with quantized activation, as data movement is more expensive than computation. Swapping the original tensor saves the quantization overhead but adds more data movement cost between CPU and GPU. As shown in Sec. 6.4, quantization overhead is much smaller than copying full-precision data to CPU in modern GPU architecture. Furthermore, we create two new streams (swap in/out) to parallelize the computation and swapping operation to reduce the swap overhead. The forward computation and swap-out process happen in parallel during the forward pass. During the backward pass, in each layer the swap-in stream is responsible for prefetching the compressed activation of the previous layer to avoid synchronization overhead. We leverage the CUDA event to ensure tasks in diﬀerent streams are executed in the correct order. 5.5 Other Memory Optimizations Gradient checkpointing. Gradient checkpointing [ 19] works by dividing the NN into segments. The algorithm only stores the inputs of each segment and recomputes the dropped activations segment by segment during backpropagation. The memory consumption is thus the cost of storing the inputs of all segments plus the maximum memory cost to backpropagate each segment. When combined with gradient checkpointing, GACT can reduce the memory consumption of both parts. GACT reduces the memory consumption of the ﬁrst part by quantizing the segment inputs. Moreover, the activations saved during the recompute phase are also quantized, reducing the memory cost of the second part. Combining GACT with gradient checkpointing might introduce more training noise because the recompute starts from quantized segment inputs, making the forward pass of recompute phase not exact. However, in Sec. 6.4, we show the noise introduced by forwarding from the quantized tensors is negligible. Memory eﬃcient self-attention. When the batch size is very large, the single layer after dequantization occupies a large amount of memory and prevents the batch size from increasing further. We observe this 7Table 1: Optimization levels for GACT. Level Compression Strategy Bits L0 Do not compress 32 L1 per-group quantization with auto-precision 4 L2 L1 + swapping/prefetching 4 CB1 L1 + gradient checkpointing 4 CB2 CB1 + eﬃcient self-attention 4 relu conv *pool conv *conv *pool conv *conv *pool conv *conv *pool linear *drop linear *drop linear loss 1 2 4 8 32bits 10−1 100 101 cl (a) Inferred per-tensor cl (line) and bits/dim. (bar) for VGG-11. Layers with * have a preceding ReLU layer with shared context. drop=dropout, loss=cross entropy loss. 1 2 4 8 bits/dim. 10−2 10−1 100 101 grad. var. uniform adapt (b) Gradient variance. 0 50 100 epoch 10−2 100 102 104 cl (c) Evolution of the per- tensor sensitivity. Each line iscl for a tensor. Figure 3: Eﬀectiveness of the adaptive algorithm. problem in transformer-based models where self-attention has quadratic space complexity in terms of sequence length. To reduce the memory footprint of the self-attention layer, we implement the algorithm introduced in [29] that achieves linear space complexity, and combines it with GACT. 5.6 Optimization level To exploit the trade-oﬀ between memory saving and training speed, GACT provides several optimization levels. Higher levels can save more memory but with more overhead. Tab. 1 lists these optimization levels. L1 uses per-group quantization with the adaptive algorithm. L2 combines per-group quantization with swapping and prefetching. For transformer-based models, CB1 combines GACT with gradient checkpointing. CB2 further reduces the peak memory by adding eﬃcient self-attention to CB1. 6 Experiments We ﬁrst demonstrate the eﬀectiveness of the GACT adaptive algorithm. We further apply GACT to a wide range of machine learning tasks, including image classiﬁcation, object detection, text, and graph node classiﬁcation. We compare the training accuracy and activation compression rate for full precision, adaptive 4/3/2 (using GACT to adaptively decide quantization bits with an average of 4/3/2 bit) and ﬁx-4 bit (quantizating all tensors uniformly with 4 bits). Next, we study the trade-oﬀ between compression rate and training throughput and compare GACT with other state-of-the-art memory-saving methods. Lastly, we demonstrate the ﬂexibility of GACT by exploring the possibility of combining it with other memory optimization methods (CB1, CB2 as listed in Table 1). We use open-source model implementations for all tasks. 6.1 Compression Strategy We ﬁrst test the eﬀectiveness of our adaptive compression rate algorithm for training VGG-11 [ 30] on ImageNet. Fig. 3(a) plots the inferred per-tensor sensitivity cl and the corresponding optimal bits/dim. GACT assigns more bits to more sensitive layers. The context tensor saved by the cross-entropy loss operator is most sensitive. A small amount of compression leads to a huge gradient variance. This makes sense since the loss is the ﬁrst operator to back-propagate through, where the error accumulates. Therefore, GACT assigns 32 bits/dim. for the tensors in the classiﬁcation head. With the adaptive algorithm, GACT with an average of 4 bits/dim. achieves smaller gradient variance than uniformly assigning 8 bits/dim. for all the tensors, as shown in Fig. 3(b). Finally, Fig. 3(c) shows that the sensitivity cl(h; θt) remains stable during training.Therefore, periodically updating cl at a large interval is reasonable, and this introduces negligible impact on training speed. 8Table 2: For classiﬁcation, we train VGG11 [ 30], ResNet-50 [31], and Swin-Tiny [ 32] on ImageNet [ 33]. For object detection, we train RetinaNet [ 34], Faster R-CNN [35] on Coco [ 36]. We report accuracy on validation sets (Div. indicates diverge) and the compression rate of context tensors (numbers in brackets) for both tasks. Task Model FP32 GACT Adapt 4bit (L1) GACTAdapt 2bit Cls. VGG11 68.75 68.77 (2.84×) 68.49 (3.34×) ResNet-50 77.29 76.96 (6.69×) 76.13 (11.39×) Swin-tiny 81.18 80.92 (7.44×) 77.91 (13.73×) Det. Faster RCNN37.4 37.0 (4.86×) 36.1 (6.81×) RetinaNet 36.5 36.3 (3.11×) Div. 6.2 Optimization level We apply GACT on various computer vision tasks, including image classiﬁcation and object detection, as shown in Fig. 2. We also vary the average bits used by the adaptive algorithm to explore the memory accuracy trade-oﬀ. On both tasks, GACT L1 achieves comparable ( <0.5% accuracy drop) or even better results than the full precision training, while reducing activation memory by up to 7.44 ×. Here, we list the accuracy of FP32 as the strongest accuracy baseline. For other lossy methods we consider in Sec. 6.3, the accuracy is no better than FP32, and we list their training accuracy in Appendix C. Notice that here GACT Adapt 2bit diverges on the detection task. This is because, as shown in Sec.3.2, although ACT has unbiased gradients, the compression error and learning rate aﬀect the convergence. When using 2 bit, the compression error is large and the learning rate has to be reduced accordingly to guarantee convergence. However, we do not want to slow training by decreasing the learning rate. All experiments are run with the same learning rate as the full precision. Therefore when compression error is large, the training diverges. Furthermore, we observe that the memory reduction varies among networks because GACT does not quantize intermediate states, and the size of intermediate states diﬀers between networks. For example, in VGG11, when the batch size is 128, GACT reduces the saved tensor size from 5889MB to 2080MB, among which 78% (1494MB) is used to store the intermediate index for the max-pooling layer that is not quantized by GACT. Next, we demonstrate the ﬂexibility of GACT by applying it to a wider variety of natural language processing (NLP) and graph machine learning (Graph) tasks. We run multiple seeds for each task, and we report the mean ±std of accuracy/F1 across runs as shown in Tab. 3. We include the detailed experimental setup in Appendix B. For both NLP and Graph tasks, GACT L1 achieves comparable training results with FP32, introducing less than 0.3% accuracy/F1-score drop, while reducing activation memory by 4.18 ×to 7.93×. Moreover, the results are stable across runs, introducing similar accuracy variance as FP32. We also show the training results of ﬁx-4bit quantization, where all tensors are uniformly quantized with 4 bits. As shown in Tab. 3, ﬁx-4 bit quantization causes signiﬁcant accuracy/F1-score loss on various graph models. For Bert-large, ﬁxed-4 bit quantization works ﬁne because all the context tensors have similar sensitivity. On the other hand, GACT L1, using a similar amount of memory as always quantizing each layer to 4 bits, still performs on par with full precision training on all the models. This shows the necessity of using adaptive algorithms to assign bits based on tensor sensitivity for stabilized training. Moreover, for Bert-large and three graph models (GCN/GAT/GCNII), GACT converges and gives lossless results with 3 bits. Remarkably, across all the graph models, training with 2-bit GACT causes little accuracy loss ( <1%). This shows the robustness of our adaptive algorithm. 6.3 Memory Saving and Computational Overhead Settings and baselines. We implement the benchmark with PyTorch 1.10 and measure the memory saving and overhead of GACT on an AWS g4dn.4xlarge instance, which has a 16GB NVIDIA T4 GPU and 64GB CPU memory. On ResNet-50, we compare with ActNN [ 5], a dedicated quantization framework for convolutional NNs, and DTR [ 21], a state-of-the-art rematerialization method for dynamic graphs. “swap” is a simple swapping strategy that swaps all activations to the CPU. For Bert-large, we also show the results on Mesa [ 7], a memory-saving resource-eﬃcient training framework for transformers, and ZeRO-Oﬄoad [ 37], a highly optimized system for training large-scale language models. Gradient checkpointing uses the default checkpointing policy provided by the transformer library [ 38], where only the input to each transformer block is saved before the backward pass. On Swin-tiny, we only include Mesa and swap because other baselines 9Table 3: Accuracy and activation compression rate for NLP and Graph tasks. Accuracy that drops > 1% is in italic font. Model Dataset FP32 Fix 4bit GACT Adapt 4bit (L1)GACT Adapt 3bitGACT Adapt 2bit GCN Flickr 51.17±0.19 50.93±0.16 (7.56×) 51.08±0.18 (7.93×) 51.14±0.18 (11.34×) 51.20±0.18 (17.56×) Reddit 95.33±0.07 94.42±0.11 (7.55×) 95.32±0.07 (7.90×) 95.31±0.07 (9.70×) 95.34±0.06 (13.68×) Yelp 39.86±0.94 39.85±1.22 (5.94×) 40.06±0.74 (6.42×) 40.21±0.82 (7.46×) 39.89±1.45 (9.00×) ogbn-arxiv71.51±0.65 68.61±0.77 (7.54×) 71.35±0.36 (8.09×) 70.82±0.95 (10.45×) 70.87±0.66 (13.75×) GAT Flickr 52.40±0.28 35.24±11.90 (4.23×) 52.26±0.31 (4.34×) 51.68±1.13 (5.04×) 51.62±1.19 (5.46×) Reddit 95.95±0.06 59.37±11.48 (4.12×) 96.02±0.09 (4.29×) 95.96±0.06 (4.64×) 95.82±0.06 (5.24×) Yelp 52.41±0.69 36.09±13.70 (4.04×) 52.18±0.38 (4.18×) 51.63±0.83 (4.53×) 51.15±0.53 (5.24×) ogbn-arxiv71.68±0.54 54.64±5.62 (5.04×) 71.80±0.47 (5.09×) 71.47±0.50 (6.14×) 71.21±0.68 (6.98×) GCNII Flickr 52.37±0.16 52.28±0.16 (4.84×) 52.31±0.16 (4.91×) 52.36±0.16 (5.54×) 52.23±0.15 (6.44×) Reddit 96.32±0.24 86.50±1.08 (4.51×) 96.11±0.22 (4.52×) 96.01±0.33 (5.16×) 95.54±0.29 (5.92×) Yelp 62.33±0.20 62.21±0.22 (5.26×) 62.28±0.26 (5.34×) 62.53±0.36 (6.29×) 62.33±0.37 (7.28×) ogbn-arxiv72.52±0.12 44.57±5.01 (6.54×) 72.28±0.35 (6.74×) 72.22±0.28 (7.98×) 71.74±0.26 (10.24×) Bert- large MNLI 86.74±0.24 85.98±0.16 (7.55×) 86.61±0.11 (7.38×) 86.68±0.08 (9.13×) 84.24±0.74 (12.87×) SST-2 93.69±0.30 93.46±0.23 (7.55×) 93.54±0.52 (7.30×) 93.20±0.37 (9.05×) 91.90±1.04 (12.91×) MRPC 88.20±0.02 87.36±0.19 (7.55×) 87.90±0.10 (7.40×) 87.69±0.07 (9.19×) 82.54±0.38 (12.91×) QNLI 92.29±0.14 92.34±0.07 (7.55×) 92.44±0.07 (7.42×) 92.43±0.31 (9.19×) 90.74±0.13 (12.95×) Table 4: Largest models GACT can train with 16G GPU memory. In ResNet (batch size=64), D (depth): number of layers, W (width): base width of the bottleneck block, R (resolution): width and height of input images. In Bert-large (batch size=16) and GCN, D (depth): number of transformer/gcn blocks, W (width): hidden size. Dim Maximum Value Throughput (TFLOPS) FP L1 L2 FP L1 L2 ResNet- 152 D 160 460 1124 0.43 0.47 0.41 W 88 304 320 0.44 0.89 0.6 R 232 548 716 0.41 0.39 0.44 Bert- large D 32 56 64 0.67 0.56 0.53 W 1280 1488 2032 0.68 0.61 0.60 GCN D 24 152 240 0.20 0.14 0.15 W 2464 3948 4244 0.36 0.38 0.40 lack the support for this network. Results. We compare the training throughput of GACT against other memory saving systems in Fig. 4. On ResNet-50, GACT achieves similar throughput as ActNN (ActNN optimization L5 is not listed because it optimizes PyTorch memory allocation, which is unrelated to quantization and can also be applied to GACT), but ActNN enables training with a larger batch size. This is expected because ActNN implements eﬃcient, customized layers for diﬀerent operators in convolutional NNs. For Bert-large, Zero-oﬄoad fails quickly because it only oﬄoads optimizer states that occupy a small portion of total memory to CPU. GACT L1 outperforms Mesa because Mesa only compresses tensors to 8 bit. When the batch is bigger, the activation size of each segment becomes the memory bottleneck and prevents gradient checkpointing from increasing the batch size. Moreover, combining GACT with gradient checkpointing and eﬃcient self-attention further reduces the peak memory, increasing the batch size by up to 24.7 ×. Meanwhile, it introduces a small throughput overhead compared with the original gradient checkpointing. Across all the network architectures, GACT enables training with a 4.2 ×to 24.9×larger batch size under the same memory budget. Network scaling. With GACT, we can construct larger models or train with a higher image resolution. Tab. 4 compares the largest model we can train against full precision. With the same batch size and memory budget, GACT can scale a ResNet-152 to 7.0 ×deeper, 3.6 ×wider or 3.0 ×higher resolution. Similarly, Bert-large can be scaled to 2.0 ×deeper or 1.6×wider. In GCN, GACT enables training 10.0 ×deeper and 1.7×wider network. Overall, GACT maintains 75% - 136% original training throughput. 6.4 Other Optimizations We evaluate the idea of combining GACT with swapping on Bert-large-cased. As shown in Tab. 5, swapping compressed tensors is faster than swapping the original ones because communication between CPU and GPU is more time-consuming than computation. Combining GACT with swapping increases training speed by 10(a) 0 200 400 600 800 Batch Size 0 50 100Training Throughput 4.3× L0 L1 L2 ResNet-50 DTR Swap ActNN GACT (b) 0 100 200 300 400 500 600 Batch Size 0 10 20Training Throughput 24.7× L0 L1 L2 CB1 CB2 Bert-large ZeroOff Swap Mesa CKPT GACT (c) 0 100 200 300 400 500 600 Batch Size 0 25 50 75Training Throughput 5.6× L0 L1 L2 Swin-tiny Mesa Swap GACT Figure 4: Training throughput vs batch size. Red cross mark means out-of-memory. The shaded yellow region denotes the batch sizes with full precision training given the memory budget. CKPT: Gradient checkpointing, ZeroOﬀ: ZeRO-Oﬄoad. Table 5: Swap and prefetch speed/memory on Bert-large. Algorithm Speed (sequence/s) Peak Mem. (MB) Total Mem. (MB) FP32 16.41 9573 9527 FP32 + swap 6.02 5215 5093 GACT swap 12.95 5426 5325 GACT swap + prefetch14.02 5426 5324 up to 2.3×. Notice here that the peak memory use of “GACT swap” is slightly higher than “FP32 + swap” because GACT does not quantize and swap intermediate states such as running mean/var of BatchNorm layer. Moreover, prefetch increases the speed by about 7% with negligible memory overhead. We next demonstrate combining GACT with gradient checkpointing (CB1). Gradient checkpointing is performed at the beginning of each transformer block, thus avoiding saving tensors generated within the block. We then apply GACT with gradient checkpointing, where the saved tensors are quantized with 4 bits. As shown in Tab. 6, the accuracy is unaﬀected. We also compare the activation memory and peak memory of CB1 and CB2 in Tab. 7. AM2 denotes the peak activation memory, which is the size of saved tensors after reforwarding the ﬁrst transformer block. When batch size = 288, compared with gradient checkpointing on full precision (FP32), CB1 and CB2 reduce the peak activation size by 4.7 ×and 5.4×respectively. 7 Conclusion This paper presents GACT, an ACT framework for generic NN architectures. We prove the convergence of GACT without prior knowledge about operator type or network architecture by analyzing a linearized Table 6: Accuracy of Bert-large-cased on SST-2 and QNLI datasets Algorithm SST-2 QNLI Algorithm SST-2 QNLI FP32 93.58 92.42 CB1 93.81 92.26 11Table 7: Memory use of diﬀerent algorithms on Bert-large. AM1: Activation size before backward, AM2: Activation size after reforwading the ﬁrst transformer block. When batch size = 288, L0 runs out of memory, and therefore it is not listed below. Batch Size Algorithm AM1(MB) AM2(MB) Peak Mem.(MB) 16 L0 4434 - 9573 FP32 + CKPT210 394 5541 CB1 37 99 5286 CB2 31 79 5269 288 FP32 + CKPT3783 7092 12885 CB1 515 1497 8251 CB2 486 1307 8102 approximation of ATC’s gradients. With the adaptive algorithm, GACT achieves negligible accuracy loss on various tasks, reducing activation memory by up to 8.1 ×and enabling training with up to 24.7 ×batch size compared with full precision training. Acknowledgements This work was supported by the National Key Research and Development Project of China (No. 2021ZD0110502); NSF of China Project (No. 62106120), by the National Science Foundation through grants IIS-1955488, IIS-2027575, CCF-1723352, ARO W911NF2110339, ONR N00014-21-1-2724, and DOE award DE-SC0016260. We would also like to acknowledge partial support from DARPA, IARPA, the Sloan Foundation, NSF, and ONR. Our conclusions do not necessarily reﬂect the position or the policy of our sponsors, and no oﬃcial endorsement should be inferred. References [1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018. [2] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and eﬃcient sparsity. arXiv preprint arXiv:2101.03961 , 2021. [3] Ayan Chakrabarti and Benjamin Moseley. Backprop with approximate activations for memory-eﬃcient network training. arXiv preprint arXiv:1901.07988 , 2019. [4] Fangcheng Fu, Yuzheng Hu, Yihan He, Jiawei Jiang, Yingxia Shao, Ce Zhang, and Bin Cui. Don’t waste your bits! squeeze activations and gradients for deep neural networks via tinyscript. In International Conference on Machine Learning, pages 3304–3314. PMLR, 2020. [5] Jianfei Chen, Lianmin Zheng, Zhewei Yao, Dequan Wang, Ion Stoica, Michael W Mahoney, and Joseph E Gonzalez. Actnn: Reducing training memory footprint via 2-bit activation compressed training. In International Conference on Machine Learning , 2021. [6] R David Evans and Tor Aamodt. AC-GC: Lossy activation compression with guaranteed convergence. Advances in Neural Information Processing Systems , 34, 2021. [7] Zizheng Pan, Peng Chen, Haoyu He, Jing Liu, Jianfei Cai, and Bohan Zhuang. Mesa: A memory-saving training framework for transformers. arXiv preprint arXiv:2111.11124 , 2021. [8] R David Evans, Lufei Liu, and Tor M Aamodt. Jpeg-act: accelerating deep learning via transform-based lossy compression. In 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA), pages 860–873. IEEE, 2020. [9] Sian Jin, Guanpeng Li, Shuaiwen Leon Song, and Dingwen Tao. A novel memory-eﬃcient deep learning training framework via error-bounded lossy compression. In 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming , pages 485–487, 2021. 12[10] Anonymous. EXACT: Scalable graph neural networks training via extreme activation compression. In Submitted to The Tenth International Conference on Learning Representations , 2022. under review. [11] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, / Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998–6008, 2017. [12] Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. arXiv preprint arXiv:1609.02907 , 2016. [13] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. In International Conference on Learning Representations, 2018. [14] Shuang Wu, Guoqi Li, Feng Chen, and Luping Shi. Training and inference with integers in deep neural networks. In International Conference on Learning Representations, 2018. [15] Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan. Training deep neural networks with 8-bit ﬂoating point numbers. In Advances in Neural Information Processing Systems, pages 7675–7684, 2018. [16] Ron Banner, Itay Hubara, Elad Hoﬀer, and Daniel Soudry. Scalable methods for 8-bit training of neural networks. In Advances in Neural Information Processing Systems , pages 5145–5153, 2018. [17] Jianfei Chen, Yu Gai, Zhewei Yao, Michael W Mahoney, and Joseph E Gonzalez. A statistical framework for low-bitwidth training of deep neural networks. In Advances in neural information processing systems , 2020. [18] Xiao Sun, Naigang Wang, Chia-Yu Chen, Jiamin Ni, Ankur Agrawal, Xiaodong Cui, Swagath Venkatara- mani, Kaoutar El Maghraoui, Vijayalakshmi Viji Srinivasan, and Kailash Gopalakrishnan. Ultra-low precision 4-bit training of deep neural networks. In Advances in Neural Information Processing Systems , volume 33, 2020. [19] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174 , 2016. [20] Paras Jain, Ajay Jain, Aniruddha Nrusimha, Amir Gholami, Pieter Abbeel, Kurt Keutzer, Ion Stoica, and Joseph E Gonzalez. Checkmate: Breaking the memory wall with optimal tensor rematerialization. arXiv preprint arXiv:1910.02653 , 2019. [21] Marisa Kirisame, Steven Lyubomirsky, Altan Haan, Jennifer Brennan, Mike He, Jared Roesch, Tianqi Chen, and Zachary Tatlock. Dynamic tensor rematerialization. arXiv preprint arXiv:2006.09616 , 2020. [22] Chien-Chin Huang, Gu Jin, and Jinyang Li. Swapadvisor: Pushing deep learning beyond the gpu memory limit via smart swapping. In Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems , pages 1341–1355, 2020. [23] Linnan Wang, Jinmian Ye, Yiyang Zhao, Wei Wu, Ang Li, Shuaiwen Leon Song, Zenglin Xu, and Tim Kraska. Superneurons: Dynamic GPU memory management for training deep neural networks. In 23rd ACM SIGPLAN symposium on principles and practice of parallel programming , pages 41–53, 2018. [24] Xuan Peng, Xuanhua Shi, Hulin Dai, Hai Jin, Weiliang Ma, Qian Xiong, Fan Yang, and Xuehai Qian. Capuchin: Tensor-based gpu memory management for deep learning. In Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems , pages 891–905, 2020. 13[25] Olivier Beaumont, Lionel Eyraud-Dubois, and Alena Shilova. Eﬃcient combination of rematerialization and oﬄoading for training dnns. Advances in Neural Information Processing Systems , 34, 2021. [26] L´ eon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMP- STAT’2010, pages 177–186. Springer, 2010. [27] L´ eon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. SIAM Review, 60(2):223–311, 2018. [28] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during propagations. In Advances in neural information processing systems, pages 3123–3131, 2015. [29] Markus N Rabe and Charles Staats. Self-attention does not need o(n2) memory. arXiv preprint arXiv:2112.05682, 2021. [30] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations, 2015. [31] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE conference on computer vision and pattern recognition , pages 770–778, 2016. [32] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. International Conference on Computer Vision (ICCV) , 2021. [33] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In IEEE conference on computer vision and pattern recognition , pages 248–255. Ieee, 2009. [34] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll´ ar. Focal loss for dense object detection. In International Conference on Computer Vision (ICCV) , pages 2980–2988, 2017. [35] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. Advances in neural information processing systems , 28:91–99, 2015. [36] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740–755. Springer, 2014. [37] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. Zero-oﬄoad: Democratizing billion-scale model training. arXiv preprint arXiv:2101.06840, 2021. [38] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R´ emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations , pages 38–45, Online, October 2020. Association for Computational Linguistics. [39] Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. Graphsaint: Graph sampling based inductive learning method. arXiv preprint arXiv:1907.04931 , 2019. [40] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020. 14[41] Petar Veliˇ ckovi´ c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903 , 2017. [42] Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph convolutional networks. In International Conference on Machine Learning , pages 1725–1735. PMLR, 2020. [43] Yukuo Cen, Zhenyu Hou, Yan Wang, Qibin Chen, Yizhen Luo, Xingcheng Yao, Aohan Zeng, Shiguang Guo, Peng Zhang, Guohao Dai, Yu Wang, Chang Zhou, Hongxia Yang, and Jie Tang. Cogdl: Toolkit for deep learning on graphs. arXiv preprint arXiv:2103.00959 , 2021. [44] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , pages 353–355, 2018. [45] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 4171–4186, 2019. 15A Proof of Theorems A.1 Theorem 1: Convergence of ACT Assume that: A1. L(θ) is a continuous diﬀerentiable, ∇L(θ) is β-Lipschitz continuous. . A2. L(θ) is bounded below by L∗. A3. g(h; θ) is diﬀerentiable w.r.t. h and ∃b> 0, s.t. ∀θ,E∥g(Q(h(x,θ)); θ) −ˆg(h(x,θ); θ)∥≤ b. A4. ∃σ2 >0, s.t., ∀θ, Var [ˆg(h(x,θ)] ≤σ2. Then, for all η <1 2β, if we run ACT deﬁned as Eq. (2) for T iterations, then we have min t=0,...,T−1 E [ ∥∇L(θt)∥2 ] ≤4(L(θ0) −L∗) ηT + 3b2 + ηβσ2 Proof. Denote m:= ∇θL(θt), ϵ:= ˆg(h(x,θt); θt) −m, d:= g(Q(h(x; θt)); θt) −ˆg(h(x,θt); θt). Then, by A3 and A4, we have E[ϵ] = E[g(h(x,θt); θt) −∇θL(θt)] + E[⟨J(x,θt),∆Q(h(x,θt))⟩] = ⟨J(x,θt),E[∆Q(h(x,θt))]⟩= 0. (6) E [ ∥ϵ∥2 ] = ∥E[ϵ]∥2 + Var [ϵ] = Var [ˆg(h(x,θt); θt)] ≤σ2. (7) E[∥d∥] ≤b. (8) By the deﬁnitions, the ACT dynamics can be written as θt+1 ←θt −η(m+ d+ ϵ). By A1, we have L(θt+1) ≤L(θt) −η⟨m,m + d+ ϵ⟩+ βη2 2 ∥m+ d+ ϵ∥2 . (9) By Eq.s (6,8) E[⟨m,m + d+ ϵ⟩] ≥∥m∥2 −∥m∥∥d∥+ ⟨m,E[ϵ]⟩≥∥ m∥2 −∥m∥b. (10) By Eq.s (6,7,8), and ∥x+ y∥2 ≤2 ∥x∥2 + 2∥y∥2, E [ ∥m+ d+ ϵ∥2 ] = E [ ∥m+ d∥2 ] + Var [ϵ] ≤2E[∥m∥]2 + 2E[∥d∥]2 + Var [ϵ] = 2E[∥m∥]2 + 2b2 + σ2. (11) Taking expectation on both sides of Eq. (9), plug in Eq.s (10, 11), and use η <1 2β, we have E[L(θt+1)] ≤L(θt) −η(∥m∥2 −∥m∥b) + βη2 2 (2E[∥m∥]2 + 2b2 + σ2). =L(θt) −(η−βη2) ∥m∥2 + η∥m∥b+ βη2 2 (2b2 + σ2) =L(θt) −η 2 ∥m∥2 + η∥m∥b+ βη2 2 (2b2 + σ2). Completing the squares, E[L(θt+1)] ≤L(θt) −η 2(∥m∥−b)2 + βη2 2 (2b2 + σ2). Take expectation on both sides and sum up for t= 0,...,T −1, E[L(θT)] −L(θ0) ≤−η 2 T−1∑ t=0 E(∥∇L(θt)∥−b)2 + βη2T 2 (2b2 + σ2). Reorganize the terms, Et [ E(∥∇L(θt)∥−b)2 ] ≤2(L(θ0) −L(θT)) ηT + ηβ(2b2 + σ2). Let t∗= argmintE[∥∇L(θt)∥], and use A1, we have E(∥∇L(θt∗)∥−b)2 ≤2(L(θ0) −L∗) ηT + ηβ(2b2 + σ2). 16Use (a+ b)2 ≤2a2 + 2b2, we have E [ ∥∇L(θt∗)∥2 ] ≤4(L(θ0) −L∗) ηT + (2βη+ 2)b2 + ηβσ2 ≤4(L(θ0) −L∗) ηT + 3b2 + ηβσ2. A.2 Proposition 1: The Linearization Error Proof. Consider the gradient function g(Q(h(x; θ); θ)), whose output is a P-dimensional vector. Since it is twice diﬀerentiable, we construct the Taylor’s expansion at h(x; θ) with Lagrange remainder: ∃H1,...,H P,s.t., ∀i, gi(Q(h(x; θ)); θ) = gi(h(x,θ); θ) + Ji(x,θ)∆h(x,θ) + ∆h(x,θ)⊤Hi∆h(x,θ), where Ji(h(x; θ),θ) := ∂gi(h(x;θ);θ) ∂h . By the assumption, there exists P >0, such that the linearization error is ∥g(Q(h(x; θ)); θ) −ˆg(h(x; θ); h(x; θ),θ)∥1 = P∑ i=1 ∆h(x,θ)⊤Hi∆h(x,θ) ≤γP ∥∆h(x,θ)∥2 . Taking expectation, E[∥g(Q(h(x; θ)); h(x; θ),θ) −ˆg(h(x; θ); θ)∥2] ≤E[∥g(Q(h(x; θ)); θ) −ˆg(h(x; θ); h(x; θ),θ)∥1] ≤γPVar [∆h(x,θ)] = O(Var [∆h(x,θ)]). A.3 Proposition 2: The Order of the Variance The following proposition is convenient for isolating the diﬀerent noise sources. Proposition A. (Law of Total Variance) Var [X] = E[Var [X |Y]] + Var [E[X |Y]] . Proof. By deﬁnition Var [ˆg(h(x; θt); h(x; θ),θt)] = Var [g(h(x,θ); θ)] + Var [J(h(x; θ),θ)∆h(x,θ)] , where Var [g(h(x,θ); θ)] is the noise introduced by subsampling the data x. By law of total variance, Var [J(h(x; θ),θ)∆h(x,θ)] = EX[VarQ[J(h(x; θ); θt)∆h(x,θ)]] + VarX[EQ[J(h(x; θ); θt)∆h(x,θ)]]   =0 , where VarQ[J(h(x; θ); θt)∆h(x,θ)] =EQ [ ∥J(h(x; θ); θt)∆h(x,θ)∥2 ] ≤EQ [ ∥J(h(x; θ); θt)∥2 ∥∆h(x,θ)∥2 ] = ∥J(h(x; θ); θt)∥2 EQ [ ∥∆h(x,θ)∥2 ] = O(Var [∆h(x,θ)]) . A.4 Proposition 3: The Structure of the Variance Before investigating the structure of VarQ[J(x; θt)∆h(x,θ)], let’s do some recap: the parameter θt is a P-dimensional vector; the context diﬀerence ∆ h(x,θ) is a D-dimensional vector, and J(x; θt) is a P ×D matrix. Recall that ∆ h(x,θ) is the concatenation of L-vectors, ∆h(l)(x,θ), and let J(l)(x,θ) := ∂g ∂h(l) g ( (h(l)(x; θ))L l=1,θ ) , which is a P ×Dl matrix. Furthermore, let h(l) j (x,θ) be the j-th dimension, and J(l) j (x,θ) be its j-th column. To proceed, we need to make the following assumptions to the compressor Q(·) : RD →RD: B1: The compressed result is element-wise uncorrelated. That is, for any i̸= j, Cov [Q(h)i,Q(h)j] = 0. B2: For compressing a vector h to b bits, the compression variance of each dimension can be written in the form Var [Q(h)j] ≤Rj(h)S(b), where S(·) is a known function. Both assumptions can be achieved by a stochastic rounding [28] quantizer, where Q(h)j = { T−1 h,b (⌈Th,b(hj)⌉) w.p. Th,b(hj) −⌊Th,b(hj)⌋ T−1 h,b (⌊Th,b(hj)⌋) otherwise , 17where Th,b(hj) = (2b−1) hj−minj h maxj h−minj h. Since each dimension is quantized independently, B1 is met. Moreover, Var [Q(h)j] ≤1 4 (maxjh−minjh (hj −minjh) )2 (2b −1)−2 = Rj(h)S(b), where Rj(h) = 1 4 (maxjh−minjh (hj −minjh) )2 , S (b) = (2b −1)−2. Proof. By deﬁnition, J(h; θ)∆h= L∑ l=1 Dl∑ j=1 J(l) j (h; θt)∆h(l) j . Using Assumption B1, we have VarQ[J(h; θ)∆h] = EQ    L∑ l=1 Dl∑ j=1 J(l) j (h; θt)∆h(l) j  2  = L∑ l=1 Dl∑ j=1 EQ [J(l) j (h; θt)∆h(l) j  2] . = L∑ l=1 Dl∑ j=1 J(l) j (h; θt)  2 VarQ [ ∆h(l) j ] Using Assumption B2, we have VarQ[J(h; θ)∆h] ≤ L∑ l=1 Dl∑ j=1 J(l) j (h; θt)  2 Rl(h)S(bl) = L∑ l=1 cl(h,θ)S(bl), where cl(θ,h) := Rl(h) J(l)(h; θt) 2 F. B Experiment Setup B.1 Node classiﬁcation task on graphs We conduct experiments on four node classiﬁcation datasets with standard splits, including Flickr, Reddit, Yelp from GraphSAINT [39], and ogbn-arxiv from Open Graph Benchmark (OGB) [ 40]. The four datasets cover extensive downstream applications with diﬀerent scales. We use accuracy as the evaluation metric for multi-class classiﬁcation and micro-F1 for multi-label classiﬁcation. We run ten seeds (0 to 9) and report the average accuracy across runs. We evaluate GACT on three representative GNN models, including GCN [ 12], GAT [41], and GCNII [ 42] under the full-batch training setting. All three models are implemented by CogDL [ 43], a toolkit for graph neural networks. B.2 Text classiﬁcation task We select four largest datasets, MNLI, QQP, SST-2, and QNLI, from the GLUE benchmark [ 44]. The four datasets cover diﬀerent aspects of natural language understanding, including sentiment classiﬁcation, natural language inference and paraphrase detection. We use the mainstream transformer implementation [ 38] to train Bert-large [45]. We run three seeds (42, 43, 44) and report F1 for QQP, accuracy for the others. C Training Accuracy of Baselines For all the baselines we compared in Sec. 6.3, only ActNN, Mesa, and ZeRO-Oﬄoad are lossy methods. All other methods are lossless and have the same training accuracy as FP32. For ResNet-50 on ImageNet, the training accuracy for FP32, GACT, ActNN L2, and ActNN L3 are 77.3, 77.0, 77.4, and 76.9. For Bert-Large on SST-2, the accuracy for FP32, GACT, Mesa, and ZeRO-Oﬄoad are 93.7, 93.5, 93.8, and 93.3. For Swin-tiny on ImageNet, the training accuracy for FP32, GACT, and Mesa are 81.2, 81.0, and 81.3 respectively. 18",
      "meta_data": {
        "arxiv_id": "2206.11357v4",
        "authors": [
          "Xiaoxuan Liu",
          "Lianmin Zheng",
          "Dequan Wang",
          "Yukuo Cen",
          "Weize Chen",
          "Xu Han",
          "Jianfei Chen",
          "Zhiyuan Liu",
          "Jie Tang",
          "Joey Gonzalez",
          "Michael Mahoney",
          "Alvin Cheung"
        ],
        "published_date": "2022-06-22T20:06:23Z",
        "pdf_url": "https://arxiv.org/pdf/2206.11357v4.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "GACT proposes a generic Activation Compressed Training (ACT) framework to reduce memory footprint for training large neural networks across various architectures (convolutional NNs, transformers, graph NNs) without requiring domain-specific knowledge or customized implementations. The key contributions include a general convergence theory for ACT based on a linearized approximate gradient, an adaptive algorithm that automatically estimates tensor sensitivity and selects optimal compression ratios at runtime for stable training, and an efficient PyTorch library implementation. GACT achieves up to 8.1x activation memory reduction, enabling 4.2x to 24.7x larger batch sizes with negligible accuracy loss on diverse machine learning tasks.",
        "methodology": "GACT abstracts forward propagation into loss and context functions, compressing context tensors using an unbiased stochastic compressor Q(h). It formulates a general convergence theory by analyzing the stochastic gradient (SG) of ACT, approximating it with an unbiased linearized version (first-order Taylor expansion) whose variance has a simple structure. The framework employs an adaptive algorithm to determine the compression rate for each tensor by estimating its impact on the gradient. This algorithm approximately solves an integer programming problem to minimize variance under a total bits budget, leveraging properties of stochastic rounding quantizers (element-wise uncorrelated results, specific error form). Tensor sensitivity is numerically computed by evaluating empirical variance across two network iterations while holding other compression seeds fixed. GACT is implemented as a PyTorch library using low-level hooks to capture context tensors, supporting arbitrary operators, and employs per-group quantization. It also includes optimizations like parallel swapping of compressed tensors to/from CPU and can be combined with gradient checkpointing and memory-efficient self-attention.",
        "experimental_setup": "Experiments were conducted on an AWS g4dn.4xlarge instance (16GB NVIDIA T4 GPU, 64GB CPU memory) using PyTorch 1.10. Tasks included image classification (VGG-11, ResNet-50, Swin-Tiny on ImageNet), object detection (RetinaNet, Faster R-CNN on COCO), natural language processing (Bert-large on GLUE benchmark datasets: MNLI, QQP, SST-2, QNLI), and graph node classification (GCN, GAT, GCNII on Flickr, Reddit, Yelp, ogbn-arxiv). Evaluation metrics were accuracy (for most tasks) and F1-score (for QQP and multi-label graph classification), reporting mean ± std across multiple seeds. Baselines included Full Precision (FP32), ActNN, DTR, simple swapping ('swap'), Mesa, ZeRO-Offload, and Gradient Checkpointing. GACT was tested with different optimization levels (L0-L2, CB1, CB2) and average bit-widths (4, 3, 2-bit adaptive compression) against uniform fixed-bit quantization (fix-4bit). Network scaling capabilities were also benchmarked by increasing depth, width, or resolution.",
        "limitations": "GACT Adapt 2bit can diverge on certain tasks (e.g., object detection) if the compression error is too large, implying a need for a reduced learning rate to guarantee convergence, which was not applied in experiments to maintain training speed. The memory reduction achieved by GACT can vary across different network architectures because it does not quantize all intermediate states (e.g., max-pooling indices), and the size of these unquantized states differs. Combining GACT with gradient checkpointing might introduce slight training noise due to recomputing from quantized segment inputs, although experiments indicated this noise was negligible. The theoretical convergence and adaptive algorithm rely on specific assumptions about the compressor and gradient function (e.g., element-wise uncorrelated compressed results, twice differentiability of g(h; θ) with bounded second-order derivative).",
        "future_research_directions": "Not mentioned"
      }
    },
    {
      "title": "Generative Teaching Networks: Accelerating Neural Architecture Search by Learning to Generate Synthetic Training Data",
      "abstract": "This paper investigates the intriguing question of whether we can create\nlearning algorithms that automatically generate training data, learning\nenvironments, and curricula in order to help AI agents rapidly learn. We show\nthat such algorithms are possible via Generative Teaching Networks (GTNs), a\ngeneral approach that is, in theory, applicable to supervised, unsupervised,\nand reinforcement learning, although our experiments only focus on the\nsupervised case. GTNs are deep neural networks that generate data and/or\ntraining environments that a learner (e.g. a freshly initialized neural\nnetwork) trains on for a few SGD steps before being tested on a target task. We\nthen differentiate through the entire learning process via meta-gradients to\nupdate the GTN parameters to improve performance on the target task. GTNs have\nthe beneficial property that they can theoretically generate any type of data\nor training environment, making their potential impact large. This paper\nintroduces GTNs, discusses their potential, and showcases that they can\nsubstantially accelerate learning. We also demonstrate a practical and exciting\napplication of GTNs: accelerating the evaluation of candidate architectures for\nneural architecture search (NAS), which is rate-limited by such evaluations,\nenabling massive speed-ups in NAS. GTN-NAS improves the NAS state of the art,\nfinding higher performing architectures when controlling for the search\nproposal mechanism. GTN-NAS also is competitive with the overall state of the\nart approaches, which achieve top performance while using orders of magnitude\nless computation than typical NAS methods. Speculating forward, GTNs may\nrepresent a first step toward the ambitious goal of algorithms that generate\ntheir own training data and, in doing so, open a variety of interesting new\nresearch questions and directions.",
      "full_text": "GENERATIVE TEACHING NETWORKS : ACCELERATING NEURAL ARCHITECTURE SEARCH BY LEARNING TO GENERATE SYNTHETIC TRAINING DATA Felipe Petroski Such, Aditya Rawal, Joel Lehman, Kenneth O. Stanley∗& Jeff Clune∗ Uber AI Labs ABSTRACT This paper investigates the intriguing question of whether we can create learning algorithms that automatically generate training data, learning environments, and curricula in order to help AI agents rapidly learn. We show that such algorithms are possible via Generative Teaching Networks (GTNs), a general approach that is, in theory, applicable to supervised, unsupervised, and reinforcement learning, although our experiments only focus on the supervised case. GTNs are deep neu- ral networks that generate data and/or training environments that a learner (e.g. a freshly initialized neural network) trains on for a few SGD steps before being tested on a target task. We then differentiate through the entire learning process via meta-gradients to update the GTN parameters to improve performance on the target task. GTNs have the beneﬁcial property that they can theoretically gener- ate any type of data or training environment, making their potential impact large. This paper introduces GTNs, discusses their potential, and showcases that they can substantially accelerate learning. We also demonstrate a practical and excit- ing application of GTNs: accelerating the evaluation of candidate architectures for neural architecture search (NAS), which is rate-limited by such evaluations, enabling massive speed-ups in NAS. GTN-NAS improves the NAS state of the art, ﬁnding higher performing architectures when controlling for the search pro- posal mechanism. GTN-NAS also is competitive with the overall state of the art approaches, which achieve top performance while using orders of magnitude less computation than typical NAS methods. Speculating forward, GTNs may repre- sent a ﬁrst step toward the ambitious goal of algorithms that generate their own training data and, in doing so, open a variety of interesting new research questions and directions. 1 I NTRODUCTION AND RELATED WORK Access to vast training data is now common in machine learning. However, to effectively train neural networks (NNs) does not require using all available data. For example, recent work in cur- riculum learning (Graves et al., 2017), active learning (Konyushkova et al., 2017; Settles, 2010) and core-set selection (Sener & Savarese, 2018; Tsang et al., 2005) demonstrates that a surrogate dataset can be created by intelligently sampling a subset of training data, and that such surrogates enable competitive test performance with less training effort. Being able to more rapidly determine the per- formance of an architecture in this way could particularly beneﬁt architecture search, where training thousands or millions of candidate NN architectures on full datasets can become prohibitively ex- pensive. From this lens, related work in learning-to-teach has shown promise. For example, the learning to teach (L2T) (Fan et al., 2018) method accelerates learning for a NN learner (hereafter, just learner) through reinforcement learning, by learning how to subsample mini-batches of data. A key insight in this paper is that the surrogate data need not be drawn from the original data distribution (i.e. they may not need to resemble the original data). For example, humans can learn new skills from reading a book or can prepare for a team game like soccer by practicing skills, such as passing, dribbling, juggling, and shooting. This paper investigates the question of whether we can train a data-generating network that can produce synthetic data that effectively and efﬁciently ∗co-senior authors. Corresponding authors: {felipe.such,kstanley,jeffclune}@uber.com 1 arXiv:1912.07768v1  [cs.LG]  17 Dec 2019teaches a target task to a learner. Related to the idea of generating data, Generative Adversarial Networks (GANs) can produce impressive high-resolution images (Goodfellow et al., 2014; Brock et al., 2018), but they are incentivized to mimic real data (Goodfellow et al., 2014), instead of being optimized to teach learners more efﬁciently than real data. Another approach for creating surrogate training data is to treat the training data itself as a hyper- parameter of the training process and learn it directly. Such learning can be done through meta- gradients (also called hyper-gradients), i.e. differentiating through the training process to optimize a meta-objective. This approach was described in Maclaurin et al. (2015), where 10 synthetic training images were learned using meta-gradients such that when a network is trained on these images, the network’s performance on the MNIST validation dataset is maximized. In recent work concurrent with our own, Wang et al. (2019b) scaled this idea to learn 100 synthetic training examples. While the 100 synthetic examples were more effective for training than 100 original (real) MNIST training examples, we show that it is difﬁcult to scale this approach much further without the regularity across samples provided by a generative architecture (Figure 2b, green line). Being able to very quickly train learners is particularly valuable for neural architecture search (NAS), which is exciting for its potential to automatically discover high-performing architectures, which otherwise must be undertaken through time-consuming manual experimentation for new domains. Many advances in NAS involve accelerating the evaluation of candidate architectures by training a predictor of how well a trained learner would perform, by extrapolating from previously trained architectures (Luo et al., 2018; Liu et al., 2018a; Baker et al., 2017). This approach is still expensive because it requires many architectures to be trained and evaluated to train the predictor. Other approaches accelerate training by sharing training across architectures, either through shared weights (e.g. as in ENAS; Pham et al. (2018)), or Graph HyperNetworks (Zhang et al., 2018). We propose a scalable, novel, meta-learning approach for creating synthetic data called Generative Teaching Networks (GTNs). GTN training has two nested training loops: an inner loop to train a learner network, and an outer-loop to train a generator network that produces synthetic training data for the learner network. Experiments presented in Section 3 demonstrate that the GTN approach produces synthetic data that enables much faster learning, speeding up the training of a NN by a fac- tor of 9. Importantly, the synthetic data in GTNs is not only agnostic to the weight initialization of the learner network (as in Wang et al. (2019b)), but is also agnostic to the learner’sarchitecture. As a result, GTNs are a viable method for accelerating evaluation of candidate architectures in NAS. Indeed, controlling for the search algorithm (i.e. using GTN-produced synthetic data as a drop-in replacement for real data when evaluating a candidate architecture’s performance), GTN-NAS im- proves the NAS state of the art by ﬁnding higher-performing architectures than comparable methods like weight sharing (Pham et al., 2018) and Graph HyperNetworks (Zhang et al., 2018); it also is competitive with methods using more sophisticated search algorithms and orders of magnitude more computation. It could also be combined with those methods to provide further gains. One promising aspect of GTNs is that they make very few assumptions about the learner. In contrast, NAS techniques based on shared training are viable only if the parameterizations of the learners are similar. For example, it is unclear how weight-sharing or HyperNetworks could be applied to architectural search spaces wherein layers could be either convolutional or fully-connected, as there is no obvious way for weights learned for one layer type to inform those of the other. In contrast, GTNs are able to create training data that can generalize between such diverse types of architectures. GTNs also open up interesting new research questions and applications to be explored by future work. Because they can rapidly train new architectures, GTNs could be used to create NNs on- demand that meet speciﬁc design constraints (e.g. a given balance of performance, speed, and en- ergy usage) and/or have a speciﬁc subset of skills (e.g. perhaps one needs to rapidly create a compact network capable of three particular skills). Because GTNs can generate virtually any learning en- vironment, they also one day could be a key to creating AI-generating algorithms, which seek to bootstrap themselves from simple initial conditions to powerful forms of AI by creating an open- ended stream of challenges (learning opportunities) while learning to solve them (Clune, 2019). 2 M ETHODS The main idea in GTNs is to train a data-generating network such that a learner network trained on data it rapidly produces high accuracy in a target task. Unlike a GAN, here the two networks 2cooperate (rather than compete) because their interests are aligned towards having the learner per- form well on the target task when trained on data produced by the GTN. The generator and the learner networks are trained with meta-learning via nested optimization that consists of inner and outer training loops (Figure 1a). In the inner-loop, the generator G(z,y) takes Gaussian noise ( z) and a label ( y) as input and outputs synthetic data (x). Optionally, the generator could take only noise as input and produce both data and labels as output (Appendix F). The learner is then trained on this synthetic data for a ﬁxed number of inner-loop training steps with any optimizer, such as SGD or Adam (Kingma & Ba, 2014): we use SGD with momentum in this paper. SI Equation 1 deﬁnes the inner-loop SGD with momentum update for the learner parameters θt. We sample zt (noise vectors input to the generator) from a unit-variance Gaussian andyt labels for each generated sample) uniformly from all available class labels. Note that both zt and yt are batches of samples. We can also learn a curriculum directly by additionally optimizing zt directly (instead of sampling it randomly) and keeping yt ﬁxed throughout all of training. The inner-loop loss function ℓinner can be cross-entropy for classiﬁcation problems or mean squared error for regression problems. Note that the inner-loop objective does not depend on the outer- loop objective and could even be parameterized and learned through meta-gradients with the rest of the system (Houthooft et al., 2018). In the outer-loop, the learner θT (i.e. the learner parameters trained on synthetic data after the T inner-loop steps) is evaluated on the realtraining data, which is used to compute the outer-loop loss (aka meta-training loss). The gradient of the meta-training loss with respect to the generator is computed by backpropagating through the entire inner-loop learning process. While computing the gradients for the generator we also compute the gradients of hyper- parameters of the inner-loop SGD update rule (its learning rate and momentum), which are updated after each outer-loop at no additional cost. To reduce memory requirements, we leverage gradient- checkpointing (Griewank & Walther, 2000) when computing meta-gradients. The computation and memory complexity of our approach can be found in Appendix D. (1) Noise Inner-loop Generator Learner (4) Meta-loss Real  Data (2) Data (3) SGD Step (5) Gradient of Meta-loss w.r.t. Generator Outer-loop (a) Overview of Generative Teaching Networks Without WN With WN 0.0 0.2 0.4 0.6 0.8 1.0 1.2Validation Loss (b) GTN stability with WN 0 500 1000 1500 2000 Outer-loop Iterations 0.850 0.875 0.900 0.925 0.950 0.975 1.000Test Accuracy No Curriculum All Shuffled Shuffled Batch Full Curriculum (c) GTN curricula comparison Figure 1: (a) Generative Teaching Network (GTN) Method. The numbers in the ﬁgure reﬂect the order in which a GTN is executed. Noise is fed as an input to the Generator (1), which uses it to gen- erate new data (2). The learner is trained (e.g. using SGD or Adam) to perform well on the generated data (3). The trained learner is then evaluated on the real training data in the outer-loop to compute the outer-loop meta-loss (4). The gradients of the generator parameters are computed w.r.t. to the meta-loss to update the generator (5). Both a learned curriculum and weight normalization sub- stantially improve GTN performance. (b) Weight normalization improves meta-gradient training of GTNs, and makes the method much more robust to different hyperparameter settings. Each boxplot reports the ﬁnal loss of 20 runs obtained during hyperparameter optimization with Bayesian Opti- mization (lower is better). (c) shows a comparison between GTNs with different types of curricula. The GTN method with the most control over how samples are presented performs the best. A key motivation for this work is to generate synthetic data that is learner agnostic, i.e. that gener- alizes across different potential learner architectures and initializations. To achieve this objective, at the beginning of each new outer-loop training, we choose a new learner architecture according to a predeﬁned set and randomly initialize it (details in Appendix A). Meta-learning with Weight Normalization. Optimization through meta-gradients is often unsta- ble (Maclaurin et al., 2015). We observed that this instability greatly complicates training because of its hyperparameter sensitivity, and training quickly diverges if they are not well-set. Combining the gradients from Evolution Strategies (Salimans et al., 2017) and backpropagation using inverse variance weighting (Fleiss, 1993; Metz et al., 2019) improved stability in our experiments, but op- timization still consistently diverged whenever we increased the number of inner-loop optimization 3steps. To mitigate this issue, we introduce applying weight normalization (Salimans & Kingma, 2016) to stabilize meta-gradient training by normalizing the generator and learner weights. Instead of updating the weights (W) directly, we parameterize them as W = g·V/∥V∥and instead update the scalar gand vector V. Weight normalization eliminates the need for (and cost of) calculating ES gradients and combining them with backprop gradients, simplifying and speeding up the algorithm. We hypothesize that weight normalization will help stabilize meta-gradient training more broadly, although future work is required to test this hypothesis in meta-learning contexts besides GTNs. The idea is that applying weight normalization to meta-learning techniques is analogous to batch normalization for deep networks (Ioffe & Szegedy, 2015). Batch normalization normalizes the forward propagation of activations in a long sequence of parameterized operations (a deep NN). In meta-gradient training both the activations and weights result from a long sequence of parameterized operations and thus both should be normalized. Results in section 3.1 support this hypothesis. Learning a Curriculum with Generative Teaching Networks. Previous work has shown that a learned curriculum can be more effective than training from uniformly sampled data (Graves et al., 2017). A curriculum is usually encoded with indexes to samples from a given dataset, rendering it non-differentiable and thereby complicating the curriculum’s optimization. With GTNs however, a curriculum can be encoded as a series of input vectors to the generator (i.e. instead of sampling the zt inputs to the generator from a Gaussian distribution, a sequence of zt inputs can be learned). A curriculum can thus be learned by differentiating through the generator to optimize this sequence (in addition to the generator’s parameters). Experiments conﬁrm that GTNs more effectively teach learners when optimizing such a curriculum (Section 3.2). Accelerating NAS with Generative Teaching Networks.Since GTNs can accelerate learner train- ing, we propose harnessing GTNs to accelerate NAS. Rather than evaluating each architecture in a target task with a standard training procedure, we propose evaluating architectures with a meta- optimized training process (that generates synthetic data in addition to optimizing inner-loop hyper- parameters). We show that doing so signiﬁcantly reduces the cost of running NAS (Section 3.4). The goal of these experiments is to ﬁnd a high-performing CNN architecture for the CIFAR10 image-classiﬁcation task (Krizhevsky et al., 2009) with limited compute costs. We use the same architecture search-space, training procedure, hyperparameters, and code from Neural Architecture Optimization (Luo et al., 2018), a state-of-the-art NAS method. The search space consists of the topology of two cells: a reduction cell and a convolutional cell. Multiple copies of such cells are stacked according to a predeﬁned blueprint to form a full CNN architecture (see Luo et al. (2018) for more details). The blueprint has two hyperparameters N and F that control how many times the convolutional cell is repeated (depth) and the width of each layer, respectively. Each cell contains B = 5nodes. For each node within a cell, the search algorithm has to choose two inputs as well as two operations to apply to those inputs. The inputs to a node can be previous nodes or the outputs of the last two layers. There are 11 operations to choose from (Appendix C). Following Luo et al. (2018), we report the performance of our best cell instantiated with N = 6,F = 36after the resulting architecture is trained for a signiﬁcant amount of time (600 epochs). Since evaluating each architecture in those settings (named ﬁnal evaluation from now on) is time consuming, Luo et al. (2018) uses a surrogate evaluation (named search evaluation) to estimate the performance of a given cell wherein a smaller version of the architecture ( N = 3,F = 32) is trained for less epochs (100) on real data. We further reduce the evaluation time of each cell by replacing the training data in the search evaluation with GTN synthetic data, thus reducing the training time per evaluation by 300x (which we call GTN evaluation). While we were able to train GTNs directly on the complex architectures from the NAS search space, training was prohibitively slow. Instead, for these experiments, we optimize our GTN ahead of time using proxy learners described in Appendix A.2, which are smaller fully-convolutional networks (this meta-training took 8h on one p6000 GPU). Interestingly, although we never train our GTN on any NAS architectures, because of generalization, synthetic data from GTNs were still effective for training them. 3 R ESULTS We ﬁrst demonstrate that weight normalization signiﬁcantly improves the stability of meta-learning, an independent contribution of this paper (Section 3.1). We then show that training with synthetic data is more effective when learning such data jointly with a curriculum that orders its presentation 4to the learner (Section 3.2). We next show that GTNs can generate a synthetic training set that enables more rapid learning in a few SGD steps than real training data in two supervised learning domains (MNIST and CIFAR10) and in a reinforcement learning domain (cart-pole, Appendix H). We then apply GTN-synthetic training data for neural architecture search to ﬁnd high performing architectures for CIFAR10 with limited compute, outperforming comparable methods like weight sharing (Pham et al., 2018) and Graph HyperNetworks (Zhang et al., 2018) (Section 3.4). We uniformly split the usual MNIST training set into training (50k) and validation sets (10k). The training set was used for inner-loop training (for the baseline) and to compute meta-gradients for all the treatments. We used the validation set for hyperparameter tuning and report accuracy on the usual MNIST test set (10k images). We followed the same procedure for CIFAR10, resulting in training, validation, and test sets with 45k, 5k, and 10k examples, respectively. Unless otherwise speciﬁed, we ran each experiment 5 times and plot the mean and its 95% conﬁdence intervals from (n=1,000) bootstrapping. Appendix A describes additional experimental details. 3.1 I MPROVING STABILITY WITH WEIGHT NORMALIZATION To demonstrate the effectiveness of weight normalization for stabilizing and robustifying meta- optimization, we compare the results of running hyperparameter optimization for GTNs with and without weight normalization on MNIST. Figure 1b shows the distribution of the ﬁnal performance obtained for 20 runs during hyperparameter tuning, which reﬂects how sensitive the algorithms are to hyperparameter settings. Overall, weight normalization substantially improved robustness to hyperparameters and ﬁnal learner performance, supporting the initial hypothesis. 3.2 I MPROVING GTN S WITH A CURRICULUM We experimentally evaluate four different variants of GTNs, each with increasing control over the ordering of the zcodes input to the generator, and thus the order of the inputs provided to the learner. The ﬁrst variant (called GTN - No Curriculum), trains a generator to output synthetic training data by sampling the noise vector zfor each sample independently from a Gaussian distribution. In the next three GTN variants, the generator is provided with a ﬁxed set of input samples (instead of a noise vector). These input samples are learned along with the generator parameters during GTN training. The second GTN variant (called GTN - All Shufﬂed ) learns a ﬁxed set of 4,096 input samples that are presented in a random order without replacement (thus learning controls the data, but not the order in which they are presented). The third variant (calledGTN - Shufﬂed Batch) learns 32 batches of 128 samples each (so learning controls which samples coexist within a batch), but the order in which the batches are presented is randomized (without replacement). Finally, the fourth variant (calledGTN - Full Curriculum) learns a deterministic sequence of 32 batches of 128 samples, giving learning full control. Learning such a curriculum incurs no additional computational expense, as learning the zt tensor is computationally negligible and avoids the cost of repeatedly sampling new Gaussian z codes. We plot the test accuracy of a learner (with random initial weights and architecture) as a function of outer-loop iterations for all four variants in Figure 1c. Although GTNs - No curriculum can seemingly generate endless data (see Appendix G), it performs worse than the other three variants with a ﬁxed set of generator inputs. Overall, training the GTN with exact ordering of input samples (GTN - Full Curriculum) outperforms all other variants. While curriculum learning usually refers to training on easy tasks ﬁrst and increasing their difﬁculty over time, our curriculum goes beyond presenting tasks in a certain order. Speciﬁcally, GTN - Full Curriculum learns both the order in which to present samples and the speciﬁc group of samples to present at the same time. The ability to learn a full curriculum improves GTN performance. For that reason, we adopt that approach for all GTN experiments. 3.3 GTN S FOR SUPERVISED LEARNING To explore whether GTNs can generate training data that helps networks learn rapidly, we compare to 3 treatments for MNIST classiﬁcation. 1)Real Data - Training learners with random mini-batches of real data, as is ubiquitous in SGD. 2) Dataset Distillation - Training learners with synthetic data, where training examples are directly encoded as tensors optimized by the meta-objective, as in Wang et al. (2019b). 3) GTN - Our method where the training data presented to the learner is generated by a neural network. Note that all three methods meta-optimize the inner-loop hyperparameters (i.e. the learning rate and momentum of SGD) as part of the meta-optimization. 5We emphasize that producing state-of-the-art (SOTA) performance (e.g. on MNIST or CIFAR) when training with GTN-generated data is not important for GTNs. Because the ultimate aim for GTNs is to accelerate NAS (Section 3.4), what matters ishow well and inexpensively we can identify architectures that achieve high asymptotic accuracy when later trained on the full (real) training set. A means to that end is being able to train architectures rapidly, i.e. with very few SGD steps, because doing so allows NAS to rapidly identify promising architectures. We are thus interested in “few-step accuracy (i.e. accuracy after a few–e.g. 32 or 128–SGD steps). Besides, there are many reasons not to expect SOTA performance with GTNs (Appendix B). Figure 2a shows that the GTN treatment signiﬁcantly outperforms the other ones ( p <0.01) and trains a learner to be much more accurate whenin the few-step performance regime. Speciﬁcally, for each treatment the ﬁgure shows the test performance of a learner following 32 inner-loop training steps with a batch size of 128. We would not expect training on synthetic data to produce higher accuracy than unlimited SGD steps on real data, but here the performance gain comes because GTNs can compress the real training data by producing synthetic data that enables learners to learn more quickly than on real data. For example, the original dataset might contain many similar images, where only a few of them would be sufﬁcient for training (and GTN can produce just these few). GTN could also combine many different things that need to be learned about images into one image. Figure 2b shows the few-step performance of a learner from each treatment after 2000 total outer- loop iterations (∼1 hour on a p6000 GPU). For reference, Dataset Distillation (Wang et al., 2019b) reported 79.5% accuracy for a randomly initialized network (using 100 synthetic images vs. our 4,096) and L2T (Fan et al., 2018) reported needing 300x more training iterations to achieve >98% MNIST accuracy. Surprisingly, although recognizable as digits and effective for training, GTN- generated images (Figure 2c) were not visually realistic (see Discussion). 0 500 1000 1500 2000 Outer-loop Iterations 0.90 0.92 0.94 0.96 0.98 1.00Test Accuracy GTN Real Data Dataset Distillation (a) Meta-training curves 0 10 20 30 Inner-loop Iterations 0.90 0.92 0.94 0.96 0.98 1.00Test Accuracy GTN Real Data Dataset Distillation (b) Training curves  (c) GTN-generated samples Figure 2: Teaching MNIST with GTN-generated images. (a) MNIST test set few-step accuracy across outer-loop iterations for different sources of inner-loop training data. The inner-loop consists of 32 SGD steps and the outer-loop optimizes MNIST validation accuracy. Our method (GTN) outperforms the two controls (dataset distillation and samples from real data). (b) For the ﬁnal meta-training iteration, across inner-loop training, accuracy on the MNIST test set when inner-loop training on different data sources. (c) 100 random samples from the trained GTN. Samples are often recognizable as digits, but are not realistic (see Discussion). Each column contains samples from a different digit class, and each row is taken from different inner-loop iterations (evenly spaced from the 32 total iterations, with early iterations at the top). 3.4 A RCHITECTURE SEARCH WITH GTN S We next test the beneﬁts of GTN for NAS (GTN-NAS) in CIFAR10, a domain where NAS has previously shown signiﬁcant improvements over the best architectures produced by armies of hu- man scientists. Figure 3a shows the few-step training accuracy of a learner trained with either GTN-synthetic data or real (CIFAR10) data over meta-training iterations. After 8h of meta-training, training with GTN-generated data was signiﬁcantly faster than with real data, as in MNIST. To explore the potential for GTN-NAS to accelerate CIFAR10 architecture search, we investigated the Spearman rank correlation (across architectures sampled from the NAS search space) between accelerated GTN-trained network performance (GTN evaluation) and the usual more expensive per- formance metric used during NAS (search evaluation). A correlation plot is shown in Figure 3c; note that a strong correlation implies we can train architectures using GTN evaluation as an inexpensive surrogate. We ﬁnd that GTN evaluation enables predicting the performance of an architecture efﬁ- 6ciently. The rank-correlation between 128 steps of training with GTN-synthetic data vs. 100 epochs of real data is 0.3606. The correlation improves to 0.5582 when considering the top 50% of archi- tectures recommended by GTN evaluation scores, which is important because those are the ones that search would select. This improved correlation is slightly stronger than that from 3 epochs of training with real data (0.5235), a ∼9×cost-reduction per trained model. 20 40 60 80 100 120 Inner-loop Iterations 0.3 0.4 0.5 0.6 0.7Training Accuracy GTN Real Data (a) CIFAR10 inner-loop training  (b) CIFAR10 GTN samples 0.1 0.2 0.3 0.4 0.5 GTN Predicted Performance 0.90 0.91 0.92 0.93 0.94 0.95Real Data Predicted Perf.  (c) CIFAR10 correlation Figure 3: Teaching CIFAR10 with GTN-generated images. (a) CIFAR10 training set performance of the ﬁnal learner (after 1,700 meta-optimization steps) across inner-loop learning iterations. (b) Samples generated by GTN to teach CIFAR10 are unrecognizable, despite being effective for train- ing. Each column contains a different class, and each row is taken from the same inner-loop iteration (evenly spaced from all 128 iterations, early iterations at the top). (c) Correlation between perfor- mance prediction using GTN-data vs. Real Data. When considering the top half of architectures (as ranked by GTN evaluation), correlation between GTN evaluation and search evaluation is strong (0.5582 rank-correlation), suggesting that GTN-NAS has potential to uncover high performing ar- chitectures at a signiﬁcantly lower cost. Architectures shown are uniformly sampled from the NAS search space. The top 10% of architectures according to the GTN evaluation (blue squares)– those likely to be selected by GTN-NAS–have high true asymptotic accuracy. Architecture search methods are composed of several semi-independent components, such as the choice of search space, search algorithm, and proxy evaluation of candidate architectures. GTNs are proposed as an improvement to this last component, i.e. as a new way to quickly evaluate a new architecture. Thus we test our method under the standard search space for CIFAR10, using a simple form of search (random search) for which there are previous benchmark results. In particular, we ran an architecture search experiment where we evaluated 800 randomly generated architectures trained with GTN-synthetic data. We present the performance after ﬁnal evaluation of the best architecture found in Table 1. This experimental setting is similar to that of Zhang et al. (2018). Highlighting the potential of GTNs as an improved proxy evaluation for architectures, we achieve state-of-the-art results when controlling for search algorithm (the choice of which is orthogonal to our contribution). While it is an apples-to-oranges comparison, GTN-NAS is competitive even with methods that use more advanced search techniques than random search to propose architectures (Appendix E). GTN is compatible with such techniques, and would likely improve their performance, an interesting area of future work. Furthermore, because of the NAS search space, the modules GTN found can be used to create even larger networks. A further test of whether GTNs predictions generalize is if such larger networks would continue performing better than architectures generated by the real- data control, similarly scaled. We tried F=128 and show it indeed does perform better (Table 1), suggesting additional gains can be had by searching post-hoc for the correct F and N settings. 4 D ISCUSSION , FUTURE WORK , AND CONCLUSION The results presented here suggest potential future applications and extensions of GTNs. Given the ability of GTNs to rapidly train new models, they are particularly useful when training many independent models is required (as we showed for NAS). Another such application would be to teach networks on demand to realize particular trade-offs between e.g. accuracy, inference time, and memory requirements. While to address a range of such trade-offs would ordinarily require training many models ahead of time and selecting amongst them (Elsken et al., 2019), GTNs could instead rapidly train a new network only when a particular trade-off is needed. Similarly, agents with unique combinations of skills could be created on demand when needed. 7Table 1: Performance of different architecture search methods. Our results report mean ±SD of 5 evaluations of the same architecture with different initializations. It is common to report scores with and without Cutout (DeVries & Taylor, 2017), a data augmentation technique used during training. We found better architectures compared to other methods that reduce architecture evaluation speed and were tested with random search (Random Search+WS and Random Search+GHN). Increasing the width of the architecture found (F=128) further improves performance. Because each NAS method ﬁnds a different architecture, the number of parameters differs. Each method ran once. Model Error(%) #params GPU Days Random Search + GHN (Zhang et al., 2018) 4.3 ±0.1 5.1M 0.42 Random Search + Weight Sharing (Luo et al., 2018) 3.92 3.9M 0.25 Random Search + Real Data (baseline) 3.88 ±0.08 12.4M 10 Random Search + GTN (ours) 3.84 ±0.06 8.2M 0.67 Random Search + Real Data + Cutout (baseline) 3.02 ±0.03 12.4M 10 Random Search + GTN + Cutout (ours) 2.92 ±0.06 8.2M 0.67 Random Search + Real Data + Cutout (F=128) (baseline) 2.51 ±0.13 151.7M 10 Random Search + GTN + Cutout (F=128) (ours) 2.42 ±0.03 97.9M 0.67 Interesting questions are raised by the lack of similarity between the synthetic GTN data and real MNIST and CIFAR10 data. That unrealistic and/or unrecognizable images can meaningfully affect NNs is reminiscent of the ﬁnding that deep neural networks are easily fooled by unrecognizable images (Nguyen et al., 2015). It is possible that if neural network architectures were functionally more similar to human brains, GTNs’ synthetic data might more resemble real data. However, an alternate (speculative) hypothesis is that the human brain might also be able to rapidly learn an arbitrary skill by being shown unnatural, unrecognizable data (recalling the novel Snow Crash). The improved stability of training GTNs from weight normalization naturally suggests the hypoth- esis that weight normalization might similarly stabilize, and thus meaningfully improve, any tech- niques based on meta-gradients (e.g. MAML (Finn et al., 2017), learned optimizers (Metz et al., 2019), and learned update rules (Metz et al., 2018)). In future work, we will more deeply investigate how consistently, and to what degree, this hypothesis holds. Both weight sharing and GHNs can be combined with GTNs by using the shared weights or Hyper- Network for initialization of proposed learners and then ﬁne-tuning on GTN-produced data. GTNs could also be combined with more intelligent ways to propose which architecture to sample next such as NAO (Luo et al., 2018). Many other extensions would also be interesting to consider. GTNs could be trained for unsupervised learning, for example by training a useful embedding function. Additionally, they could be used to stabilize GAN training and prevent mode collapse (Appendix I shows encouraging initial results). One particularly promising extension is to introduce a closed- loop curriculum (i.e. one that responds dynamically to the performance of the learner throughout training), which we believe could signiﬁcantly improve performance. For example, a recurrent GTN that is conditioned on previous learner outputs could adapt its samples to be appropriately easier or more difﬁcult depending on an agent’s learning progress, similar in spirit to the approach of a human tutor. Such closed-loop teaching can improve learning (Fan et al., 2018). An additional interesting direction is having GTNs generate training environments for RL agents. Appendix H shows this works for the simple RL task of CartPole. That could be either for a pre- deﬁned target task, or could be combined with more open-ended algorithms that attempt to con- tinuously generate new, different, interesting tasks that foster learning (Clune, 2019; Wang et al., 2019a). Because GTNs can encode any possible environment, they (or something similar) may be necessary to have truly unconstrained, open-ended algorithms (Stanley et al., 2017). If techniques could be invented to coax GTNs to produce recognizable, human-meaningful training environments, the technique could also produce interesting virtual worlds for us to learn in, play in, or explore. This paper introduces a new method called Generative Teaching Networks, wherein data genera- tors are trained to produce effective training data through meta-learning. We have shown that such an approach can produce supervised datasets that yield better few-step accuracy than an equivalent amount of real training data, and generalize across architectures and random initializations. We leverage such efﬁcient training data to create a fast NAS method that generates state-of-the-art ar- chitectures (controlling for the search algorithm). While GTNs may be of particular interest to the 8ﬁeld of architecture search (where the computational cost to evaluate candidate architectures often limits the scope of its application), we believe that GTNs open up an intriguing and challenging line of research into a variety of algorithms that learn to generate their own training data. 5 A CKNOWLEDGEMENTS For insightful discussions and suggestions, we thank the members of Uber AI Labs, especially Theofanis Karaletsos, Martin Jankowiak, Thomas Miconi, Joost Huizinga, and Lawrence Murray. REFERENCES Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pa- chocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dexterous in-hand manipulation. arXiv preprint arXiv:1808.00177, 2018. Bowen Baker, Otkrist Gupta, Ramesh Raskar, and Nikhil Naik. Accelerating neural architecture search using performance prediction. arXiv preprint arXiv:1705.10823, 2017. Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high ﬁdelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016. Jeff Clune. Ai-gas: Ai-generating algorithms, an alternate paradigm for producing general artiﬁcial intelligence. arXiv preprint arXiv:1905.10985, 2019. Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017. Gamaleldin F. Elsayed, Ian J. Goodfellow, and Jascha Sohl-Dickstein. Adversarial reprogramming of neural networks. CoRR, abs/1806.11146, 2018. URL http://arxiv.org/abs/1806. 11146. Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Efﬁcient multi-objective neural architecture search via lamarckian evolution. In International Conference on Learning Representations, 2019. Yang Fan, Fei Tian, Tao Qin, Xiang-Yang Li, and Tie-Yan Liu. Learning to teach. arXiv preprint arXiv:1805.03643, 2018. Chelsea Finn and Sergey Levine. Meta-learning and universality: Deep representations and gradient descent can approximate any learning algorithm. arXiv preprint arXiv:1710.11622, 2017. Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the 34th International Conference on Machine Learning- Volume 70, pp. 1126–1135. JMLR. org, 2017. JL Fleiss. Review papers: The statistical basis of meta-analysis. Statistical methods in medical research, 2(2):121–145, 1993. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor- mation processing systems, pp. 2672–2680, 2014. Alex Graves, Marc G. Bellemare, Jacob Menick, R´emi Munos, and Koray Kavukcuoglu. Automated curriculum learning for neural networks. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pp. 1311–1320, 2017. Andreas Griewank and Andrea Walther. Algorithm 799: revolve: an implementation of check- pointing for the reverse or adjoint mode of computational differentiation. ACM Transactions on Mathematical Software (TOMS), 26(1):19–45, 2000. 9Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international conference on computer vision, pp. 1026–1034, 2015. Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. CoRR, abs/1503.02531, 2015. Rein Houthooft, Yuhua Chen, Phillip Isola, Bradly Stadie, Filip Wolski, OpenAI Jonathan Ho, and Pieter Abbeel. Evolved policy gradients. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 5405–5414. Curran Associates, Inc., 2018. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Ksenia Konyushkova, Raphael Sznitman, and Pascal Fua. Learning active learning from data. In NIPS, 2017. Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009. Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy. Progressive neural architecture search. In Proceed- ings of the European Conference on Computer Vision (ECCV), pp. 19–34, 2018a. Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055, 2018b. Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu. Neural architecture optimization. In Advances in neural information processing systems, pp. 7816–7827, 2018. Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectiﬁer nonlinearities improve neural net- work acoustic models. In Proc. icml, volume 30, pp. 3, 2013. Dougal Maclaurin, David Duvenaud, and Ryan P. Adams. Gradient-based hyperparameter opti- mization through reversible learning. In Proceedings of the 32Nd International Conference on In- ternational Conference on Machine Learning - Volume 37, ICML’15, pp. 2113–2122. JMLR.org, 2015. Luke Metz, Niru Maheswaranathan, Brian Cheung, and Jascha Sohl-Dickstein. Meta-learning up- date rules for unsupervised representation learning. arXiv preprint arXiv:1804.00222, 2018. Luke Metz, Niru Maheswaranathan, Jeremy Nixon, Daniel Freeman, and Jascha Sohl-dickstein. Learned optimizers that outperform on wall-clock and validation loss, 2019. V olodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pp. 1928–1937, 2016. Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High con- ﬁdence predictions for unrecognizable images. In In Computer Vision and Pattern Recognition (CVPR ’15), 2015. Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. Efﬁcient neural architecture search via parameters sharing. In Jennifer Dy and Andreas Krause (eds.),Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 4095–4104, Stockholmsmssan, Stockholm Sweden, 10–15 Jul 2018. PMLR. Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for image classiﬁer architecture search. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence , volume 33, pp. 4780–4789, 2019. 10Tim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Advances in Neural Information Processing Systems , pp. 901–909, 2016. Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In NIPS, 2016. Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017. Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach. In International Conference on Learning Representations, 2018. Burr Settles. Active learning literature survey. Technical report, 2010. Hava T Siegelmann and Eduardo D Sontag. On the computational power of neural nets. Journal of computer and system sciences, 50(1):132–150, 1995. Akash Srivastava, Lazar Valkov, Chris Russell, Michael U. Gutmann, and Charles Sutton. Veegan: Reducing mode collapse in gans using implicit variational learning. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 3308–3318. Curran Associates, Inc., 2017. Kenneth O. Stanley, Joel Lehman, and Lisa Soros. Open-endedness: The last grand challenge youve never heard of. O’Reilly Online, 2017. URL https://www.oreilly.com/ideas/ open-endedness-the-last-grand-challenge-youve-never-heard-of . Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013. Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Do- main randomization for transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) , pp. 23–30. IEEE, 2017. Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J Gordon. An empirical study of example forgetting during deep neural network learning. arXiv preprint arXiv:1812.05159, 2018. Ivor Tsang, James Kwok, and Pak-Ming Cheung. Core vector machines: Fast svm training on very large data sets. Journal of Machine Learning Research, 6:363–392, 04 2005. Rui Wang, Joel Lehman, Jeff Clune, and Kenneth O Stanley. Paired open-ended trailblazer (poet): Endlessly generating increasingly complex and diverse learning environments and their solutions. arXiv preprint arXiv:1901.01753, 2019a. Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A. Efros. Dataset distillation, 2019b. Chris Zhang, Mengye Ren, and Raquel Urtasun. Graph hypernetworks for neural architecture search. arXiv preprint arXiv:1810.05749, 2018. Barret Zoph and Quoc V . Le. Neural architecture search with reinforcement learning. 2017. URL https://arxiv.org/abs/1611.01578. 11APPENDIX A A DDITIONAL EXPERIMENTAL DETAILS The outer loop loss function is domain speciﬁc. In the supervised experiments on MNIST and CIFAR, the outer loop loss was cross-entropy for logistic regression on real MNIST or CIFAR data. The inner-loop loss matches the outer-loop loss, but with synthetic data instead of real data. Appendix H describes the losses for the RL experiments. The following equation deﬁnes the inner-loop SGD with momentum update for the learner parame- ters θt. θt+1 = θt −α ∑ 0≤t′≤t βt−t′ ∇ℓinner(G(zt′ ,yt′ ),yt′ ,θt′ ), (1) where αand β are the learning rate and momentum hyperparameters, respectively. zt is a batch of noise vectors that are input to the generator and are sampled from a unit-variance Gaussian.yt are a batch of labels for each generated sample/input and are sampled uniformly from all available class labels. Instead of randomly samplingzt, we can also learn a curriculum by additionally optimizingzt directly and keeping yt ﬁxed throughout all of training. Results for both approaches (and additional curriculum ablations) are reported in Section 3.2. A.1 MNIST E XPERIMENTS : For the GTN training for MNIST we sampled architectures from a distribution that produces ar- chitectures with convolutional (conv) and fully-connectd (FC) layers. All architectures had 2 conv layers, but the number of ﬁlters for each layer was sampled uniformly from the ranges U([32,128]) and U([64,256]), respectively. After each conv layer there is a max pooling layer for dimensionality reduction. After the last conv layer, there is a fully-connected layer with number of ﬁlters sampled uniformly from the range U([64,256]). We used Kaiming Normal initialization (He et al., 2015) and LeakyReLUs (Maas et al., 2013) (with α= 0.1). We use BatchNorm (Ioffe & Szegedy, 2015) for both the generator and the learners. The BatchNorm momentum for the learner was set to 0 (meta-training consistently converged to small values and we saw no signiﬁcant gain from learning the value). The generator consisted of 2 FC layers (1024 and 128 ∗H/4 ∗H/4 ﬁlters, respectively, where H is the ﬁnal width of the synthetic image). After the last FC layer there are 2 conv layers. The ﬁrst conv has 64 ﬁlters. The second conv has 1 ﬁlter followed by a Tanh. We found it particularly important to normalize (mean of zero and variance of one) all datasets. Hyperparameters are shown in Table 2. Hyperparameter Value Learning Rate 0.01 Initial LR 0.02 Initial Momentum 0.5 Adam Beta 1 0.9 Adam Beta 2 0.999 Size of latent variable 128 Inner-Loop Batch Size 128 Outer-Loop Batch Size 128 Table 2: Hyperparameters for MNIST experiments A.2 CIFAR10 E XPERIMENTS : For GTN training for CIFAR-10, the template architecture is a small learner with 5 convolutional layers followed by a global average pooling and an FC layer. The second and fourth convolution had stride=2 for dimensionality reduction. The number of ﬁlters of the ﬁrst conv layer was sam- pled uniformly from the range U([32,128]) while all others were sampled uniformly from the range U([64,256]). Other details including the generator architecture were the same as the MNIST exper- iments, except the CIFAR generator’s second conv layer had 3 ﬁlters instead of 1. Hyperparameters 12used can be found in Table 3. For CIFAR10 we augmented the real training set when training GTNs with random crops and horizontal ﬂips. We do not add weight normalization to the ﬁnal architectures found during architecture search, but we do so when we train architectures with GTN-generated data during architecture search to provide an estimate of their asymptotic performance. Hyperparameter Value Learning Rate 0.002 Initial LR 0.02 Initial Momentum 0.5 Adam Beta 1 0.9 Adam Beta 2 0.9 Adam ϵ 1e-5 Size of latent variable 128 Inner-loop Batch Size 128 Outer-loop Batch Size 256 Table 3: Hyperparameters for CIFAR10 experiments APPENDIX B R EASONS GTN S ARE NOT EXPECTED TO PRODUCE SOTA ACCURACY VS . ASYMPTOTIC PERFORMANCE WHEN TRAINING ON REAL DATA There are three reasons not to expect SOTA accuracy levels for the learners trained on synthetic data: (1) we train for very few SGD steps (32 or 128 vs. tens of thousands), (2) SOTA performance results from architectures explicitly designed (with much human effort) to achieve record accuracy, whereas GTN produces compressed training data optimized to generalize across diverse architectures with the aim of quickly evaluating a new architecture’s potential, and (3) SOTA methods often use data outside of the benchmark dataset and complex data-augmentation schemes. APPENDIX C C ELL SEARCH SPACE When searching for the operations in a CNN cell, the 11 possible operations are listed below. • identity • 1 ×1 convolution • 3 ×3 convolution • 1 ×3 + 3×1 convolution • 1 ×7 + 7×1 convolution • 2 ×2 max pooling • 3 ×3 max pooling • 5 ×5 max pooling • 2 ×2 average pooling • 3 ×3 average pooling • 5 ×5 average pooling APPENDIX D C OMPUTATION AND MEMORY COMPLEXITY With the traditional training of DNNs with back-propagation, the memory requirements are pro- portional to the size of the network because activations during the forward propagation have to be stored for the backward propagation step. With meta-gradients, the memory requirement also grows with the number of inner-loop steps because all activations and weights have to be stored for the 132nd order gradient to be computed. This becomes impractical for large networks and/or many inner- loop steps. To reduce the memory requirements, we utilize gradient-checkpointing (Griewank & Walther, 2000) by only storing the computed weights of learner after each inner-loop step and re- computing the activations during the backward pass. This trick allows us to compute meta-gradients for networks with 10s of millions of parameters over hundreds of inner-loop steps in a single GPU. While in theory the computational cost of computing meta-gradients with gradient-checkpointing is 4x larger than computing gradients (and 12x larger than forward propagation), in our experiments it is about 2.5x slower than gradients through backpropagation due to parallelism. We could further reduce the memory requirements by utilizing reversable hypergradients (Maclaurin et al., 2015), but, in our case, we were not constrained by the number of inner-loop steps we can store in memory. APPENDIX E E XTENDED NAS RESULTS In the limited computation regime (less than 1 day of computation), the best methods were, in order, GHN, ENAS, GTN, and NAONet with a mean error of 2.84%, 2.89%, 2.92%, and 2.93%, respectively. A 0.08% difference on CIFAR10 represents 8 out of the 10k test samples. For that reason, we consider all of these methods as state of the art. Note that out of the four, GTN is the only one relying on Random Search for architecture proposal. Table 4: Performance of different architecture search methods. Search with our method required 16h total, of which 8h were spent training the GTN and 8h were spent evaluating 800 architectures with GTN-produced synthetic data. Our results report mean ±SD of 5 evaluations of the same architec- ture with different initializations. It is common to report scores with and without Cutout (DeVries & Taylor, 2017), a data augmentation technique used during training.We found better architectures compared to other methods using random search (Random-WS and GHN-Top) and are competitive with algorithms that beneﬁt from more advanced search methods (e.g. NAONet and ENAS employ non-random architecture proposals for performance gains; GTNs could be combined with such non- random proposals, which would likely further improve performance). Increasing the width of the architecture found (F=128) further improves performance. Model Error(%) #params Random GPU Days NASNet-A (Zoph & Le, 2017) 3.41 3.3M \u0017 2000 AmoebaNet-B + Cutout (Real et al., 2019) 2.13 34.9M \u0017 3150 DARTS + Cutout (Liu et al., 2018b) 2.83 4.6M \u0017 4 NAONet + Cutout (Luo et al., 2018) 2.48 10.6M \u0017 200 NAONet-WS (Luo et al., 2018) 3.53 2.5M \u0017 0.3 NAONet-WS + Cutout (Luo et al., 2018) 2.93 2.5M \u0017 0.3 ENAS (Pham et al., 2018) 3.54 4.6M \u0017 0.45 ENAS + Cutout (Pham et al., 2018) 2.89 4.6M \u0017 0.45 GHN Top-Best + Cutout (Zhang et al., 2018) 2.84 ±0.07 5.7M \u0017 0.84 GHN Top (Zhang et al., 2018) 4.3 ±0.1 5.1M ✓ 0.42 Random-WS (Luo et al., 2018) 3.92 3.9M ✓ 0.25 Random Search + Real Data (baseline) 3.88 ±0.08 12.4M ✓ 10 RS + Real Data + Cutout (baseline) 3.02 ±0.03 12.4M ✓ 10 RS + Real Data + Cutout (F=128) (baseline) 2.51 ±0.13 151.7M ✓ 10 Random Search + GTN (ours) 3.84 ±0.06 8.2M ✓ 0.67 Random Search + GTN + Cutout (ours) 2.92 ±0.06 8.2M ✓ 0.67 RS + GTN + Cutout (F=128) (ours) 2.42 ±0.03 97.9M ✓ 0.67 APPENDIX F C ONDITIONED GENERATOR VS . XY-G ENERATOR Our experiments in the main paper conditioned the generator to create data with given labels, by concatenating a one-hot encoded label to the input vector. We also explored an alternative approach where the generator itself produced a target probability distribution to label the data it generates. Because more information is encoded into a soft label than a one-hot encoded one, we expected an improved training set to be generated by this variant. Indeed, such a “dark knowledge” dis- tillation setup has been shown to perform better than learning from labels (Hinton et al., 2015). 14However, the results in Figure 4 indicate that jointly generating both images and their soft labels under-performs generating only images, although the result could change with different hyperpa- rameter values and/or innovations that improve the stability of training. 0 500 1000 1500 2000 2500 3000 3500 Outer-loop Steps 0.800 0.825 0.850 0.875 0.900 0.925 0.950 0.975 1.000Validation Accuracy Real Data GTN DK Figure 4: Comparison between a conditional generator and a generator that outputs an image/label pair. We expected the latter “dark knowledge” approach to outperform the conditional generator, but that does not seem to be the case. Because initialization and training of the dark knowledge variant were more sensitive, we believe a more rigorous tuning of the process could lead to a different result. APPENDIX G GTN GENERATES (SEEMINGLY ) ENDLESS DATA While optimizing images directly (i.e. optimizing a ﬁxed tensor of images) would result in a ﬁxed number of samples, optimizing a generator can potentially result in an unlimited amount of new samples. We tested this generative capability by generating more data during evaluation (i.e. with no change to the meta-optimization procedure) in two ways. In the ﬁrst experiment, we increase the amount of data in each inner-loop optimization step by increasing the batch size (which results in lower variance gradients). In the second experiment, we keep the number of samples per batch ﬁxed, but increase the number of inner-loop optimization steps for which a new network is trained. Both cases result in an increased amount of training data. If the GTN generator has overﬁt to the number of inner-loop optimization steps during meta-training and/or the batch size, then we would not expect performance to improve when we have the generator produce more data. However, an alternate hypothesis is that the GTN is producing a healthy distribution of training data, irrespective of exactly how it is being used. Such a hypothesis would be supported by performance increase in these experiments. Figure 5a shows performance as a function of increasing batch size (beyond the batch size used during meta-optimization, i.e. 128). The increase in performance of GTN means that we can sample larger training sets from our generator (with diminishing returns) and that we are not limited by the choice of batch size during training (which is constrained due to both memory and computation requirements). Figure 5b shows the results of generating more data by increasing the number of inner-loop opti- mization steps. Generalization to more inner-loop optimization steps is important when the number of inner-loop optimization steps used during meta-optimization is not enough to achieve maximum performance. This experiment also tests the generalization of the optimizer hyperparameters be- cause they were optimized to maximize learner performance after a ﬁxed number of steps. There is an increase in performance of the learner trained on GTN-generated data as the number of inner- loop optimization steps is increased, demonstrating that the GTN is producing generally useful data instead of overﬁtting to the number of inner-loop optimization steps during training (Figure 5b). Extending the conclusion from Figure 2b, in the very low data regime, GTN is signiﬁcantly better than training on real data (p< 0.05). However, as more inner-loop optimization steps are taken and thus more unique data is available to the learner, training on the real data becomes more effective than learning from synthetic data (p< 0.05) (see Figure 5b). 15150 200 250 300 350 400 450 500 Inner loop batch size 0.915 0.920 0.925 0.930 0.935 0.940 0.945 0.950Validation Accuracy Real Data GTN (a) Increasing inner-loop batch size 20 40 60 80 100 120 Inner-loop Steps 0.92 0.93 0.94 0.95 0.96 0.97Validation Accuracy Real Data GTN (b) Increasing inner-loop optimization steps Figure 5: (a) The left ﬁgure shows that even though GTN was meta-trained to generate synthetic data of batch size 128, sampling increasingly larger batches results in improved learner performance (the inner-loop optimization steps are ﬁxed to 16). (b) The right ﬁgure shows that increasing the number of inner-loop optimization steps (beyond the 16 steps used during meta-training) improves learner performance. The performance gain with real data is larger in this setting. This improvement shows that GTNs do not overﬁt to a speciﬁc number of inner-loop optimization steps. Figure 6: GTN samples w/o curriculum. Another interesting test for our generative model is to test the distribution of learners after they have trained on the synthetic data. We want to know, for instance, if training on synthetic samples from one GTN results in a functionally similar set of learner weights regardless of learner initialization (this phenomena can be called learner mode collapse). Learner mode collapse would prevent the performance gains that can be achieved through ensembling diverse learners. We tested for learner mode collapse by evaluating the performance (on held-out data and held-out architecture) of an en- semble of 32 randomly initialized learners that are trained on independent batches from the same GTN. To construct the ensemble, we average the predicted probability distributions across the learn- ers to compute a combined prediction and accuracy. The results of this experiment can be seen in Figure 7, which shows that the combined performance of an ensemble is better (on average) than an individual learner, providing additional evidence that the distribution of synthetic data is healthy and allows ensembles to be harnessed to improve performance, as is standard with networks trained on real data. APPENDIX H GTN FOR RL To demonstrate the potential of GTNs for RL, we tested our approach with a small experiment on the classic CartPole test problem (see Brockman et al. (2016) for details on the domain. We conducted this experiment before the discovery that weight normalization improves GTN training, so these experiments do not feature it; it might further improve performance. For this experiment, the meta-objective the GTN is trained with is the advantage actor-critic formulation: log π(a|θπ)(R− V(s; θv)) (Mnih et al., 2016). The state-value V is provided by a separate neural network trained to estimate the average state-value for the learners produced so far during meta-training. The learners train on synthetic data via a single-step of SGD with a batch size of 512 and a mean squared error 160 500 1000 1500 2000 2500 3000 3500 Outer-loop Steps 0.86 0.88 0.90 0.92 0.94 0.96 0.98 1.00Validation Accuracy Real Data - single Real Data - ensemble GTN - single GTN - ensemble Figure 7: Performance of an ensemble of GTN learners vs. individual GTN learners. Ensembling a set of neural networks that each had different weight initializations, but were trained on data from the same GTN substantially improves performance. This result provides more evidence that GTNs generate a healthy distribution of training data and are not somehow forcing the learners to all learn a functionally equivalent solution. regression loss, meaning the inner loop is supervised learning. The outer-loop is reinforced because the simulator is non-differentiable. We could have also used an RL algorithm in the inner loop. In that scenario the GTN would have to learn to produce an entire synthetic world an RL agent would learn in. Thus, it would create the initial state and then iteratively receive actions and generate the next state and optionally a reward. For example, a GTN could learn to produce an entire MDP that an agent trains on, with the meta-objective being that the trained agent then performs well on a target task. We consider such synthetic (PO)MDPs an exciting direction for future research. The score on CartPole is the number of frames out of 200 for which the pole is elevated. Both GTN and an A2C (Mnih et al., 2016) control effectively solve the problem (Figure 8). Interestingly, training GTNs takes the same number of simulator steps as training a single learner with policy- gradients (Figure 8). Incredibly, however, once trained, the synthetic data from a GTN can be used to train a learner to maximum performance in a single SGD step! While that is unlikely to be true for harder target RL tasks, these results suggest that the speed-up for architecture search from using GTNs in the RL domain can be even greater than in supervised domain. The CartPole experiments feature a single-layer neural network with 64 hidden units and a tanh activation function for both the policy and the value network. The inner-loop batch size was 512 and the number of inner-loop training iterations was 1. The observation space of this environment consists of a real-valued vector of size 4 (Cart position, Cart velocity, Pole position, Pole velocity). The action space consists of 2 discrete actions (move left or move right). The outer loop loss is the reward function for the target domain (here, pole-balancing). The inner loop loss is mean squared error (i.e. the network is doing supervised learning on the state-action mapping pairs provided by the GTN). APPENDIX I S OLVING MODE COLLAPSE IN GAN S WITH GTN S We created an implementation of generative adversarial networks (GANs) (Goodfellow et al., 2014) and found they tend to generate the same class of images (e.g. only 1s, Figure 9), which is a common training pathology in GANs known as mode collapse (Srivastava et al., 2017). While there are tech- niques to prevent mode collapse (e.g. minibatch discrimination and historical averaging (Salimans et al., 2016)), we hypothesized that combining the ideas behind GTNs and GANs might provide a different, additional technique to help combat mode collapse. The idea is to add a discriminator to the GTN forcing the data it generates to both be realistic and help a learner perform well on the meta-objective of classifying MNIST. The reason this approach should help prevent mode collapse is that if the generator only produces one class of images, a learner trained on that data will not be able to classify all classes of images. This algorithm (GTN-GAN) was able to produce realistic images with no identiﬁable mode collapse (Figure 10). GTNs offer a different type of solution to the issue of mode collapse than the many that have been proposed, adding a new tool to our toolbox 170 20000 40000 60000 80000 100000 Environment Steps 25 50 75 100 125 150 175 200Reward A2C Agent A2C + GTN Agent Figure 8: An A2C Agent control trains a single policy throughout all of training, while the GTN method starts with a new, randomly initialized network at each iteration and produces the plotted performance after a single step of SGD . This plot is difﬁcult to parse because of that difference: it compares the accumulated performance of A2C across all environment steps up to that point vs. the performance achieved with GTN data in a single step of SGD from a single batch of synthetic data. Thus, at the 100,000 th step of training, GTNs enable training a newly initialized network to the given performance (of around 190) 100,000 times faster with GTN synthetic data than with A2C from scratch. With GTNs, we can therefore train many new, high-performing agents quickly. That would be useful in many ways, such as greatly accelerating architecture search algorithms for RL. Of course, these results are on a simple problem, and (unlike our supervised learning experiments) have not yet shown that the GTN data works with different architectures, but these results demonstrate the intriguing potential of GTNs for RL. One reason we might expect even larger speedups for RL vs. supervised learning is because a major reason RL is sample inefﬁcient is because it requires exploration to ﬁgure out how to solve the problem. However, once that exploration has been done, the GTN can produce data to efﬁciently teach that solution to a new architecture. RL thus represents an exciting area of future research for GTNs. Performing that research is beyond the scope of this paper, but we highlight the intriguing potential here to inspire such future work. for solving that problem. Note we do not claim this approach is better than other techniques to prevent mode collapse, only that it is an interesting new type of option, perhaps one that could be productively combined with other techniques. Figure 9: Images generated by a basic GAN on MNIST before and after mode collapse. The left image shows GAN-produced images early in GAN training and the right image shows GAN samples later in training after mode collapse has occurred due to training instabilities. APPENDIX J A DDITIONAL MOTIVATION There is an additional motivation for GTNs that involves long-term, ambitious research goals: GTN is a step towards algorithms that generate their own training environments, such that agents trained in them eventually solve tasks we otherwise do not know how to train agents to solve (Clune, 2019). It is important to pursue such algorithms because our capacity to conceive of effective training en- vironments on our own as humans is limited, yet for our learning algorithms to achieve their full potential they will ultimately need to consume vast and complex curricula of learning challenges 18Figure 10: Images generated by a GTN with an auxiliary GAN loss. Combining GTNs with GANs produces far more realistic images than GTNs alone (which produced alien, unrecognizable images, Figure 6). The combination also stabilizes GAN training, preventing mode collapse. and data. Algorithms for generating curricula, such as the the paired open-ended trailblazer (POET) algorithm (Wang et al., 2019a), have proven effective for achieving behaviors that would otherwise be out of reach, but no algorithm yet can generate completely unconstrained training conditions. For example, POET searches for training environments within a highly restricted preconceived space of problems. GTNs are exciting because they can encode a rich set of possible environments with min- imal assumptions, ranging from labeled data for supervised learning to (in theory) entire complex virtual RL domains (with their own learned internal physics). Because RNNs are Turing-complete (Siegelmann & Sontag, 1995), GTNs should be able to theoretically encode all possible learning environments. Of course, while what is theoretically possible is different from what is achievable in practice, GTNs give us an expressive environmental encoding to begin exploring what potential is unlocked when we can learn to generate sophisticated learning environments. The initial results presented here show that GTNs can be trained end-to-end with gradient descent through the entire learning process; such end-to-end learning has proven highly scalable before, and may similarly in the future enable learning expressive GTNs that encode complex learning environments. APPENDIX K O N THE REALISM OF IMAGES There are two phenomenon related to the recognizability of the GTN-generated images that are interesting. (1) Many of the images generated by GTNs are unrecognizable (e.g. as digits), yet a network trained on them still performs well on a real, target task (e.g. MNIST). (2) Some conditions increase the realism (recognizability) of the images. We will focus on the MNIST experiments because that is where we have conducted experiments to investigate this phenomenon. Figure 12 shows all of the images generated by a GTN with a curriculum. Most of the images do not resemble real MNIST digits, and many are alien and unrecognizable. Interestingly, there is a qualitative change in the recognizability of the images at the very end of the curriculum (the last 4-5 rows, which show the last two training batches). Both phenomena are interesting, and we do not have satisfactory explanations for either. Here we present many hypothesis we have generated that could explain these phenomenon. We also present a few experiments we have done to shed light on these issues. A more detailed investigation is an interesting area for future research. Importantly, the recognizable images at the end of the curriculum are not required to obtain high performance on MNIST. The evidence for that fact is in Figure 2, which shows that the performance of a learner trained on GTN-data is already high after around 23 inner-loop iterations, before the network has seen the recognizable images in the last 4-5 rows (which are shown in the last two training batches, i.e. training iterations 31 and 32). Thus, a network can learn to get over 98% accuracy on MNIST training only on unrecognizable images. At a high level, there are three possible camps of explanation for these phenomenon. Camp 1. Performance would be higher with higher realism, but optimization difﬁculties (e.g. vanishing/exploding gradients) prevent learning a generator that produces such higher- performing, more realistic images.Evidence in favor of this camp of hypotheses is that the realistic images come at the end of the curriculum, where the gradient ﬂow is easiest (as gradients do not have to ﬂow back through multiple inner-loop steps of learning). A prediction of this hypothesis is that as we improve our ability to train GTNs, the images will become more realistic. 19Camp 2. Performance is higher with lower realism (at least when not late in the curriculum), which is why unrealistic images are produced. There are at least two reasons why unrealistic images could generate higher performance. (A) Compression enables faster learning (i.e. learning with fewer samples). Being able to produce unrealistic images allows much more information to be packed into a single training example. For example, imagine a single image that could teach a network about many different styles of the digit 7 all at the same time (and/or different translations, rotations, and scales of a 7). It is well known that data augmentation improves performance because it teaches a network, for example, that the same image at different locations in the image is of the same class. It is conceivable that a single image could do something similar by showing multiple 7s at different locations. (B) Unrealistic images allow better generalization. When trying to produce high performance with very few samples, the risk of performance loss due to overﬁtting is high. A small set of realistic images may not have enough variation in non-essential aspects of the image (e.g. the background color) that allow a network to reliably learn the class of interest in a way that will generalize to instances of that class not in the training set (e.g. images of that class with a background color not in the training set). With the ability to produce unrealistic images (e.g. 7s against many different artiﬁcial backdrops, such as by adding seemingly random noise to the background color), GTNs could prevent the network from overﬁtting to spurious correlations in the training set (e.g. background color). In other words, GTNs couldlearn to produce something similar to domain randomization (Tobin et al., 2017; Andrychowicz et al., 2018) to improve generalization, an exciting prospect. Camp 3. It makes no difference on performance whether the images are realistic, but there are more unrealistic images that are effective than realistic ones, explaining why they tend to be produced. This hypothesis is in line with the fact that deep neural networks are easily fooled (Nguyen et al., 2015) and susceptible to adversarial examples (Szegedy et al., 2013). The idea is that images that are unrecognizeable to us are surprisingly meaningful to (i.e. impactful on) DNNs. This hypothesis is also in line with the fact that images can be generated that hack a trained DNN to cause it to perform other functions it was not trained to do (e.g. to perform a different function entirely, such as hacking an ImageNet classiﬁcation network to perfom a counting task like counting the number of occurences of Zebras in an image) (Elsayed et al., 2018). This hypothesis is also in line with recent research into meta-learning, showing that an initial weight vector can be carefully chosen such that it will produce a desired outcome (including implementing any learning algorithm) once subjected to data and SGD (Finn et al., 2017; Finn & Levine, 2017). One thing not explained by this hypothesis is why images at the end of the curriculum are more recognizable. Within this third camp of hypotheses is the possibility that the key features required to recognize a type of image (e.g. a 7) could be broken up across images. For example, one image could teach a network about the bottom half of a 7 and another about the top half. Recognizing either on its own is evidence for a seven, and if across a batch or training dataset the network learned to associate both features with the class 7, there is no reason that both the top half and bottom half ever have to co-occur. That could lead to unrealistic images with partial features. One prediction of this hypothesis (although one not exclusive to this hypothesis), is that averaging all of the images for each class across the entire GTN-produced training set should reveal recognizable digits. The idea is that no individual image contains a full seven, but on average the images combine to produce sevens (and the other digits). Figure 11 shows the results of this experiment. On average the digits are recognizable. This result is also consistent with Camp 1 of hypotheses: perhaps performance would increase further if the images were individually more recognizable. It is also consistent with Camp 2: perhaps the network is forced to combine many sevens into each image, making them individually unrecognizeable, but recognizable as 7s on average. Additionally, in line with Camp 2, if the network has learned to produce something like domain randomization, it could add variation across the dataset in the background (making each individual image less recognizable), but Hypothesis 2 would predict that, on average, the aspects of the image that do not matter (e.g. the background) average out to a neutral value or the true dataset mean (for MNIST, black), whereas the true class information (e.g. the digit itself) would be recognizable on average, exactly as we see in Figure 11. Thus, the average images shed light on the overall subject, but do not provide conclusive results regarding which camp of hypotheses is correct. An additional experiment we performed was to see if the alien images somehow represent the edges of the decision boundaries between images. The hypothesis is that images in the center of a cluster (e.g. a Platonic, archetypal 7) are not that helpful to establish neural network decision boundaries 20between classes, and thus GTN does not need to produce many of them. Instead, it might bene- ﬁt by generating mostly edge cases to establish the decision boundaries, which is why the digits are mostly difﬁcult to recognize. To rephrase this hypothesis in the language of support vector machines, the GTN could be mostly producing the support vectors of each class, instead of more recognizable images well inside of each class (i.e. instead of producing many Platonic images with a high margin from the decision boundary). A prediction of this hypothesis is that the unrecog- nizable GTN-generated images should be closer to the decision boundaries than the recognizable GTN-generated images. To test this hypothesis, we borrow an idea and technique from Toneva et al. (2018), which argues that one way to identify images near (or far) from a decision boundary is to count the number of times that, during the training of a neural network, images in the training set have their classiﬁcation labels change. The intuition is that Platonic images in the center of a class will not have their labels change often across training, whereas images near the boundaries between classes will change labels often as the decision boundaries are updated repeatedly during training. We trained a randomly initialized network on real images (the results are qualitatively the same if the network is trained on the GTN-produced images). After each training step we classify the images in Figure 12 with the network being trained. We then rank the synthetic images from Figure 12 on the frequency that their classiﬁcation changed between adjacent SGD steps. Figure 15 presents these images reordered (in row-major order) according to the number of times the output label for that image changed during training. The recognizable images are all tightly clustered in this analysis, showing that there is a strong relationship between how recognizable an image is and how often its label changes during training. Interestingly, the images are not all the way at one end of the spectrum. However, keep in mind that many images in this sorted list are tied with respect to the number of changes (with ties broken randomly), and the number of ﬂips does not go up linearly with each row of the image. Figure 14 shows the number of label ﬂips vs. the order in this ranked list. The recognizable images on average have 2.0 label ﬂips (Figure 14, orange horizontal line), meaning that they are towards the extreme of images whose labels do not change often. This result is in line with the hypothesis that these are Platonic images well inside the class boundary. However, there are also many unrecognizable images whose labels do not ﬂip often, which is not explained by this hypothesis. Overall, this analysis suggests the discovery of something interesting, although much future work needs to be done to probe this question further. Why are images only realistic at the end of the curriculum? Separate from, but related to, the question of why most images are unrecognizable, is why the recognizable images are only produced at the end of the curriculum. We have come up with a few different hypotheses, but we do not know which is correct. (1) The gradients ﬂow best to those samples, and thus they become the most realistic (in line with Camp 1 of hypotheses above). (2) It helps performance for some reason to have realistic images right at the end of training, but realism does not help (Camp 3) or even hurts (Camp 2) earlier in the curriculum. For example, perhaps the Platonic images are the least likely to change the decision boundaries, allowing them to be used for ﬁnal ﬁne-tuning of the decision boundaries (akin to an annealed learning rate). In line with this hypothesis is that, when optimization cannot create a deterministic curriculum, realism seems to be higher on average (Figure 13). (3) The effect is produced by the decision to take the batch normalization (Ioffe & Szegedy, 2015) statistics from the ﬁnal batch of training. Batch normalization is a common technique to improve training. Following normal batch norm procedures, during inner-loop training the batch norm statistics (mean and variance) are computed per batch. However, during inner-loop testing/inference, the statistics are instead computed from the training set. In our experiments, we calculate these statistics from the last batch in the curriculum. Thus, if it helps performance on the meta-training test set (the inner loop test set performance the GTN is being optimized for) to have the statistics of that batch match the statistics of the target data set (which contains real images), there could be a pressure for those images to be more realistic. Contrary to this hypothesis, however, is the fact that realism increases in the last two batches of the curriculum, not just the last batch (most visible in Figure 2, which shows sample from each batch in a separate row). Another hypothesis (consistent with Camp 1 and Camp 3), is that producing ﬁrst unrealistic then realistic images might reﬂect how neural networks learn (e.g. ﬁrst learning low-level ﬁlters before moving to more complex examples). However, that hypothesis would presumably predict a gradual increase in realism across the curriculum, instead of realism only sharply increasing in the last few batches. Finally, we did not observe this phenomenon in the CIFAR experiments with a full 21curriculum: the last few batches are not realistic in that experiment (Figure 3b). We do not know why the results on this front are different between MNIST and CIFAR experiments. In short, we do not have a good understanding for why realism increases towards the end of the curriculum. Shedding more light on this issue is an interesting area for future research. Figure 11: Pixel-wise mean per class of all GTN-generated images from the full curriculum treat- ment. 22Figure 12: All images generated by the full-curriculum GTN. The images are shown in the order they are presented to the network, with the ﬁrst batch of images in the curriculum in the top row and the last batch of data in the last row. The batch size does not correspond to the number of samples per row, so batches wrap from the right side of one row to the left side of the row below. 23Figure 13: A sample of images generated by the no-curriculum GTN. 240 1000 2000 3000 4000 Image Index 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0Label Switching Frequency Figure 14: The number of times a class label changes across training for each GTN-generated sample (Y-axis) vs. the rank of that sample when ordered by that same statistic (X-axis). A relatively small fraction of the samples ﬂip labels many times (in line with the idea that they are near the class decision boundaries), whereas most samples change labels only a few times (i.e. once the are learned, they stay learned, in line with them being more canonical examples). The orange line shows the average number of class changes for recognizable images (those in the red box in Figure 15). While not the images with the least number of ﬂips, these recognizable images are towards the end of the spectrum of images whose labels do not change often, in line with the hypothesis that they are more canonical class exemplars. 25Figure 15: All images generated by the full-curriculum GTN ordered by the frequency that their labels change during training. Highlighted is a dense region of realistic images that we manually identiﬁed. 26",
      "meta_data": {
        "arxiv_id": "1912.07768v1",
        "authors": [
          "Felipe Petroski Such",
          "Aditya Rawal",
          "Joel Lehman",
          "Kenneth O. Stanley",
          "Jeff Clune"
        ],
        "published_date": "2019-12-17T00:57:50Z",
        "pdf_url": "https://arxiv.org/pdf/1912.07768v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper introduces Generative Teaching Networks (GTNs), a novel meta-learning approach for automatically generating synthetic training data, learning environments, and curricula to accelerate AI agent learning. The main contributions include demonstrating that GTNs can substantially accelerate neural network training (e.g., a 9x speed-up on MNIST) and applying them to Neural Architecture Search (NAS). GTN-NAS finds higher-performing architectures compared to state-of-the-art methods when controlling for the search proposal mechanism, and is competitive with overall SOTA approaches using significantly less computation. A key finding is that GTNs generate synthetic data that is agnostic to the learner's weight initialization and architecture, making it suitable for NAS. Additionally, the paper shows that applying weight normalization significantly improves the stability of meta-gradient training for GTNs, and that learning a curriculum for data presentation further enhances GTN performance.",
        "methodology": "GTNs employ a meta-learning approach with two nested training loops. In the inner loop, a generator network G(z,y) takes Gaussian noise (z) and a label (y) as input (or just noise for data and labels) to produce synthetic data. A learner network is then trained on this synthetic data for a fixed number of SGD steps. In the outer loop, the trained learner is evaluated on real target task data to compute a meta-loss. Meta-gradients are then computed by backpropagating through the entire inner-loop learning process to update the GTN's parameters (both the generator and inner-loop hyperparameters like learning rate and momentum). To ensure learner-agnostic data generation, a new learner architecture and random initialization are used at the start of each outer-loop training. Weight normalization is applied to generator and learner weights to stabilize meta-gradient training, reducing hyperparameter sensitivity and the need for complex gradient combination techniques. A curriculum for data presentation can be learned by optimizing the sequence of input vectors (zt) to the generator directly, in addition to the generator's parameters. Gradient checkpointing is utilized to reduce memory requirements during meta-gradient computation.",
        "experimental_setup": "Experiments were conducted in supervised learning domains (MNIST and CIFAR10) and a reinforcement learning domain (CartPole). For MNIST and CIFAR10, the datasets were split into training (50k/45k), validation (10k/5k), and test (10k/10k) sets. Learner architectures for GTN training on MNIST consisted of convolutional and fully-connected layers with varied filter counts, using Kaiming Normal initialization, LeakyReLUs, and BatchNorm. For CIFAR10, proxy learners were small fully-convolutional networks. The generator architectures were multi-layered FC and convolutional networks. Inner-loop training used SGD with momentum, while outer-loop meta-optimization used Adam. For NAS, the CIFAR10 domain utilized the search space and setup from Neural Architecture Optimization (Luo et al., 2018). Architecture evaluation was accelerated by replacing standard training data with GTN-synthetic data (GTN evaluation), reducing training time per evaluation by 300x compared to real data search evaluation. GTN meta-training on proxy learners took 8 hours on one p6000 GPU. Performance was measured using few-step accuracy, test accuracy, and Spearman rank correlation for NAS, with results averaged over 5 runs and 95% confidence intervals from bootstrapping (n=1,000). Hyperparameter tuning was performed using Bayesian Optimization. CartPole experiments used a single-layer neural network with 64 hidden units and a tanh activation, employing an Advantage Actor-Critic formulation for the outer-loop meta-objective.",
        "limitations": "The current experiments primarily focus on the supervised learning case, with preliminary and limited exploration in reinforcement learning. While GTNs significantly improve learning stability with weight normalization, meta-gradient optimization can still be unstable and hyperparameter-sensitive. Training GTNs directly on complex NAS architectures proved prohibitively slow, necessitating the use of smaller proxy learners for meta-training, despite the generalization observed. A notable limitation is that GTN-generated images, especially for CIFAR10, were often unrealistic and unrecognizable, raising questions about the nature of effective training data for neural networks. The research does not aim to achieve state-of-the-art asymptotic performance with synthetic data, but rather focuses on rapid learning in the few-step regime. The preliminary CartPole experiments did not incorporate weight normalization. Furthermore, there is an incomplete understanding of why generated images are often unrecognizable yet effective, and why realism changes (or doesn't) across the learned curriculum in different datasets.",
        "future_research_directions": "Future work could extend GTNs to unsupervised learning tasks, such as training useful embedding functions. GTNs also show promise in stabilizing GAN training and preventing mode collapse. A particularly promising direction is to introduce closed-loop curricula, where GTNs dynamically adapt generated samples based on learner performance, potentially through recurrent networks, similar to a human tutor. GTNs could be applied to generate training environments for reinforcement learning agents, including entire synthetic (PO)MDPs, possibly combined with open-ended algorithms like POET to foster continuous learning. Another avenue is to combine GTNs with existing NAS techniques, such as weight sharing or HyperNetworks for initialization, or more intelligent search algorithms, to achieve further performance gains. A deeper investigation into the broader applicability of weight normalization for stabilizing other meta-gradient-based techniques (e.g., MAML, learned optimizers) is also warranted. Finally, GTNs could enable the on-demand creation of neural networks tailored to specific design constraints (e.g., balancing accuracy, inference time, and memory) or possessing unique combinations of skills, and could potentially be coaxed to produce recognizable, human-meaningful virtual training environments."
      }
    },
    {
      "title": "Accelerating Transformer Pre-training with 2:4 Sparsity",
      "abstract": "Training large transformers is slow, but recent innovations on GPU\narchitecture give us an advantage. NVIDIA Ampere GPUs can execute a\nfine-grained 2:4 sparse matrix multiplication twice as fast as its dense\nequivalent. In the light of this property, we comprehensively investigate the\nfeasibility of accelerating feed-forward networks (FFNs) of transformers in\npre-training. First, we define a ``flip rate'' to monitor the stability of a\n2:4 training process. Utilizing this metric, we propose three techniques to\npreserve accuracy: to modify the sparse-refined straight-through estimator by\napplying the masked decay term on gradients, to determine a feasible decay\nfactor in warm-up stage, and to enhance the model's quality by a dense\nfine-tuning procedure near the end of pre-training. Besides, we devise two\ntechniques to practically accelerate training: to calculate transposable 2:4\nmasks by convolution, and to accelerate gated activation functions by reducing\nGPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm\nachieves similar convergence to dense training algorithms on several\ntransformer pre-training tasks, while actual acceleration can be observed on\ndifferent shapes of transformer block apparently. Our toolkit is available at\nhttps://github.com/huyz2023/2by4-pretrain.",
      "full_text": "Accelerating Transformer Pre-training with 2:4 Sparsity Yuezhou Hu1 Kang Zhao Weiyu Huang 1 Jianfei Chen 1 Jun Zhu 1 Abstract Training large transformers is slow, but recent innovations on GPU architecture give us an ad- vantage. NVIDIA Ampere GPUs can execute a fine-grained 2:4 sparse matrix multiplication twice as fast as its dense equivalent. In the light of this property, we comprehensively investigate the feasibility of accelerating feed-forward networks (FFNs) of transformers in pre-training. First, we define a “flip rate” to monitor the stability of a 2:4 training process. Utilizing this metric, we propose three techniques to preserve accuracy: to modify the sparse-refined straight-through es- timator by applying the masked decay term on gradients, to determine a feasible decay factor in warm-up stage, and to enhance the model’s quality by a dense fine-tuning procedure near the end of pre-training. Besides, we devise two tech- niques to practically accelerate training: to calcu- late transposable 2:4 masks by convolution, and to accelerate gated activation functions by reducing GPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm achieves similar con- vergence to dense training algorithms on several transformer pre-training tasks, while actual ac- celeration can be observed on different shapes of transformer block apparently. Our toolkit is avail- able at https://github.com/huyz2023/ 2by4-pretrain. 1. Introduction Pre-training large-scale transformers is hard, for its intensive computation and time-consuming process (Anthony et al., 2020). To accelerate training, sparsity-based methods have recently emerged as a promising solution, and one of the hardware-friendly sparse patterns is 2:4 sparsity. In a 2:4 sparse matrix, every four consecutive elements contain two 1Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Center, Tsinghua-Bosch Joint ML Center, THBI Lab, Tsinghua University. Correspondence to: Jianfei Chen <jianfeic@tsinghua.edu.cn>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). zeros. Within a tensor core, a 2:4 sparse matrix multiplica- tion (2:4-spMM) could be 2x faster than its dense equivalent on NVIDIA Ampere architecture GPUs. Some works use 2:4 sparsity for accelerating training (Hubara et al., 2021; Lu et al., 2023; McDanel et al., 2022; Chmiel et al., 2023). However, they mainly target on con- volutional neural networks (CNNs) (Hubara et al., 2021; McDanel et al., 2022), whose architecture, optimizer and training procedure are different from transformers. Whether these 2:4 sparse training methods are capable for transform- ers remains under-explored. In practice, we find two bar- riers: 1) Low accuracy. The hyperparameters in some accuracy preserving techniques for transformers vary sig- nificantly from that for CNNs, which is ineffective if trans- planted directly. Remarkably, simply halving the inner di- mensionality of a feed-forward network can also reduce the same amount of computational cost, but provides bet- ter performance than most of proposed 2:4 sparse training methods. 2) Inefficiency. All previous works on 2:4 training stay on simulation, and do not provide actual acceleration results. Besides, they don’t focus on other key operations be- yond matrix multiplication that affect the practical time cost, such as overheads of pruning and activation functions. They usually lead to substantial mismatches between simulation and actual acceleration performance. In this work, we aim to propose an end-to-end acceleration method for pre-training transformers based on 2:4 sparsity. Here are our major contributions: • We propose three accuracy-preserving techniques (two for masked decay and one for dense fine-tune) for 2:4 training. First, we propose to apply the masked decay on gradients rather than on weight. Second, we show that the feasible masked decay factor on transformers may be very small (100x smaller than it has been reported on CNNs) and devise a method to quickly determine an available decay factor. Besides, our analysis demonstrates that employing a dense fine-tuning stage at the end of pre- training, rather than at the beginning, can enhance the quality of transformers. • We analyze practical factors affecting the 2:4 training speed of transformers, which is rarely considered by pre- vious works. We identify two speed bottlenecks: prun- ing overhead and gated activation functions’ overhead. 1 arXiv:2404.01847v3  [cs.LG]  27 Oct 2024Accelerating Transformer Pre-training with 2:4 Sparsity We proposed kernel-level accelerated methods to address each of these bottlenecks. • To the best of our knowledge, this is the first report on end-to-end acceleration on pre-training transformers (Fig- ure 7, Table 11). Experiments show that transformers pre-trained using our proposed sparse training scheme are comparable or even superior in accuracy to those trained with dense training methods (Table 5, 6). 2. Related Work Existing sparsity-based methods can be classified into two categories: accelerating inference and accelerating training. For training acceleration, they can be further grouped by whether 2:4 sparsity is involved. Sparsity for Inference Acceleration Early methods in- clude one-shot pruning (Han et al., 2015; 2016; Lee et al., 2018; Mishra et al., 2021). Later methods (Evci et al., 2021; Zhou et al., 2021; Lasby et al., 2023) suggest using dynamic sparse training (DST). Particularly, Zhou et al. (2021) pro- poses sparse-refined straight-through estimator (SR-STE) for 2:4 inference. Iterative magnitude-based pruning (IMP) methods (Chen et al., 2020; 2021; You et al., 2022), orig- inated from the winning lottery ticket theory (Frankle & Carbin, 2019; Frankle et al., 2020), can also be viewed as a DST approach. All these methods only speedup the forward pass. They are insufficient to accelerate training. 2:4 Semi-Structured Sparsity for Training Acceleration Accelerating training by 2:4 sparsity is hard, because both the forward and backward passes need to be accelerated. On some GPUs involving sparse tensor cores, 2:4-spMMs perform 2x faster than dense GEMMs (Mishra et al., 2021; BUSATO & POOL). In light of this, (Hubara et al., 2021) firstly proposes a transposable N:M mask to accelerate both output activations and input gradients computation in back- ward pass. Zhang et al. (2023) improve transposable mask to bi-directional mask (Bi-Mask) to further boost mask di- versity. To accelerate calculating weight gradient via 2:4- spMM, an unbiased minimum-variance estimator (MVUE) is introduced (Chmiel et al., 2023). In addition, Xu et al. (2022) also achieve fully sparse training of CNNs using spatial similarity. However, all these works do not report end-to-end training speedups on 2:4 sparse tensor cores, and they are built for CNNs. Practical 2:4 training acceleration on transformers has not been reported so far. Other Structured Sparsity for Training Acceleration Structured sparsity means channel-wise pruning to dense networks. For instance, training a large model and then compressing it to be thinner or shallower seems effective (Li et al., 2020; Zhou et al., 2020), given a fixed accuracy requirement. However, it’s not memory-efficient due to the larger model’s redundancy. In addition, low-rank adaption proves to be an effective method to reduce fine-tuning costs (Hu et al., 2023), but it can’t accelerate the pre-training. 3. Preliminary In this section, we first present the mathematical formula- tions of dense training and fully sparse training. Afterward, we revisit the related methods which are helpful to achieve fully sparse training with 2:4 sparsity, including SR-STE (Zhou et al., 2021), transposable N: M mask (Hubara et al., 2021), and MVUE (Chmiel et al., 2023). 3.1. Dense Training Problem Formulation Dense training solves an opti- mization problem minw L(w), where L is a loss function, w ∈ RD is the collection of dense weights of all layers, flat- tened to a vector. The loss is optimized by gradient descent optimization algorithms such as SGD, Adam (Kingma & Ba, 2017) and AdamW (Loshchilov & Hutter, 2019). GEMMs of a Linear Layer in Dense Training In each training step, a single linear layer performs three general matrix multiplications (GEMMs): Z = XW⊤, ∇X = ∇ZW, ∇W = ∇⊤ ZX, (1) where X, W and Z are input activations, weights, and out- put activations, with shape X, ∇X ∈ Rp×q, W, ∇W ∈ Rr×q, and Z, ∇Z ∈ Rp×r. Here, the three GEMMs com- putes output activations, input activation gradients, and weight gradients, respectively. Without loss of generality, we assume the input X to be a 2D matrix rather than a 3D tensor. In the feed-forward networks of a transformer, this can be done by simply flattening the input tensors’ first two axes, i.e., axes of batch size and sequence length. 3.2. Fully Sparse Training with 2:4 Sparsity GEMMs can be accelerated with structured sparsity. Partic- ularly, 2:4 sparsity (Mishra et al., 2021) is a semi-structured sparsity pattern supported on NVIDIA Ampere architec- tures. A 2:4 sparse matrix partitions its elements into groups of four numbers, where each group has exactly two zeros. Depending on the direction of partition, there are row-wise 2:4 sparse matrix and column-wise 2:4 sparse matrix; see Appendix A.1. With such sparsity, a GEMM C = AB can be accelerated by 2x with the 2:4-spMM kernel if either A is row-wise 2:4 sparse, or B is column-wise 2:4 sparse. To accelerate training, each GEMM in Equation (1) should have one 2:4 sparse operand. In general, weights and out- put activation gradients are selected to be pruned due to relatively lower pruning-induced loss (Chmiel et al., 2023). 2Accelerating Transformer Pre-training with 2:4 Sparsity That is, Z = XSwt(W⊤), (2) ∇X = ∇ZSw(W), (3) ∇W = Sz(∇⊤ Z)X. (4) In Equations (2) to (4), Swt, Sw, and Sz represent the prun- ing functions of W⊤, W, and ∇⊤ Z. They take dense matri- ces as input, and outputs 2:4 sparse matrices. By intuition, a pruning function picks out the 2 elements with the max magnitudes in the adjoining 4 elements and zero out the rest. With hardware support, computing Equations (2) to (4) can be theoretically 2x faster than Equation (1). This method use 2:4-spMMs for all matrix multiplications in forward and backward propagation, so we call it fully sparse training (FST). Note that Equation (4) contains a straight-through estimator (STE), which we will explain later. Transposable Masks Hubara et al. (2021) suggest that a weight matrix and its transpose can be simply pruned by multiplying binary masks, i.e., Swt(W⊤) =W⊤ ⊙ Mwt, S w(W) =W ⊙ Mw, where Mwt, Mw ∈ {0, 1}p×q are 2:4 sparse, and ⊙ is element-wise product. To utilize 2:4-spMM, the two binary masks should be mutually transposable: Mwt = M⊤ w, (5) which they call as transposable masks (same as our defina- tion in Section 5.1). In this manner, the backward pass share the same sparse weight matrix with the forward pass. The authors also propose a 2-approximation method for generat- ing such masks with claimed low computational complexity. Minimum-Variance Unbiased Estimator Chmiel et al. (2023) propose to calculate the 2:4 sparse masks of neural gradients by MVUE, i.e., Sz(∇⊤ Z) = MVUE(∇⊤ Z). (6) Compared to the commonly used minimum square error esti- mation, MVUE guarantees unbiasedness and minimizes the variance of the sparsified gradients, which is more favorable for promoting the convergence of training. 3.3. Optimization Strategies for Sparse Training The optimization of a sparse network is difficult as it has non- differentiable pruning functions. The optimization objective can be formulated as minw L(˜ w). The network makes prediction with a sparse weight vector ˜ w= m(w) ⊙ w, where the mask m(w) ∈ {0, 1}D is the concatenation of masks for each layer. If a layer is not sparsified, then the corresponding mask is an all-one matrix. Computing the gradient is tricky since the mask m is dynamically com- puted based on the dense weight w: by chain rule we have ∇wL(˜ w) =∂ ˜w ∂w ∇˜ wL(˜ w), where ∂ ˜w ∂w is a Jacobian matrix. However, ˜w is not differentiable with w since it includes a non-differentiable mask-computing-function m(·) in it. Thus, it takes some skills to estimate the gradients and up- date the parameters. STE As ˜w is an approximation of w, a straight-through estimator (STE, Bengio et al. (2013)) directly passes the gradient of ˜w to w: ∇wL(˜ w) ← ∇˜ wL(˜ w). (7) SR-STE There is a problem with STE: only a portion of the weights in a layer participate in the forward calculation, but all the weights receive gradients. This indicates that the gradients associated with masked weights1 might be inac- curate. To suppress those inaccurate gradients, Zhou et al. (2021) proposes sparse-refined straight-through estimator (SR-STE) which adds a decay term when updating: wt ← wt−1 − γ(∇wLt(˜ wt−1) +λW (m(wt−1)) ⊙ wt−1), (8) where γ stands for the learning rate, λW is the decay fac- tor, and m(wt−1) denotes the logical not operation of m(wt−1). This decay term alleviates the change of weight mask. With SR-STE, the optimization target becomes min w L(˜ w) +λW 2 ∥w ⊙ m(w)∥2 2. (9) 4. Accuracy Preserving Techniques While the methods reviewed in Section 3 can successfully perform FST on small-scale models such as ResNet and DenseNet, it is not clear whether they can be directly ap- plied to pre-train large transformers. It is challenging for FST to preserve the accuracy of dense training, since the weights and masks need to be learned jointly, which is a non- differentiable, combinatorial optimization problem. More- over, unlike inference acceleration methods, FST has no pre-trained dense model to start with. In this section, we pro- pose three practical techniques to improve the convergence of FST for transformers: transformer-specific masked decay, Fast decay factor determination and dense fine-tuning. 4.1. Flip Rate: Stability of Training Inspired by previous work (Zhou et al., 2021; You et al., 2022), we define a “flip rate” to measure how frequently the mask vector changes after one optimizer step. This metric could be used to monitor whether the network connection is stable during training. 3Accelerating Transformer Pre-training with 2:4 Sparsity Figure 1.Flip rates change throughout the training of differentλW on Transformer-base. Note that these models utilize an identical learning rate schedule. Table 1.Training results of different λW on Transformer-base. As λW increases from 0 to 2e-4, accuracy first rises and then drops, which means that λW should be neither too big nor too small to reach the optimal results. λW AVG EPOCH LOSS VAL LOSS TEST BLEU DENSE 4.558 3.978 26.15 0 (STE) 4.76 4.164 24.98 6E-7 4.684 4.079 25.68 6E-6 4.626 4.033 25.81 2E-6 4.64 4.041 25.94 2E-5 4.642 4.049 25.74 2E-4 4.662 4.06 25.62 Definition 4.1. Suppose wt is a D-dimensional weight vector at time t, and the flip rate rt is defined as the change in proportion of the mask vector after an optimizer step: rt = ∥m(wt) − m(wt−1)∥1/D ∈ [0, 1]. The larger rt is, the more unstable the network connections become. You et al. (2022) suggest that a sparse neural network acts differently in different training phases. In the early phase of training, it eagerly explores different connection modes, which means the masks vector change rapidly over time. Later, the masks gradually become stable, and the network turns itself to fine-tune weight values. In terms of flip rate, we hypothesize that A healthy training process comes with the flip rate rt rising at the beginning of training and then gradually fading to 0. We measure flip rate change for dense training, STE and SR-STE with different λW in Figure 1. For dense training, we compute the flip rate by pruning the dense weight in each iteration, despite the pruned weight is never used for training. In terms of flip rate, dense training is healthy: itsrt exactly increases first before declines. If a training process 1Unlike some relevant literature, we use “masked weights” and “pruned weights” to denote the weights that are set to 0. consistently has higher flip rate than dense training, which we call as “flip rate explosion”, it may suffer from a loss in final accuracy due to unstable training; see Table 1. In practice, STE suffers from a flip rate explosion, while SR- STE takes effect by “freezing” masks of weights: by adding a decay term, it decrease the number of flips. This inhibition effect is related to the decay factor of SR-STE: the larger λW is, the stronger the inhibition of flips is, and the smaller flip rate goes. In this section, all methods we propose involve our ultimate principle: the peak of the curve should be sufficiently high to fully explore different connection modes, and the tail should be sufficiently low for the optimization process to converge. 4.2. Transformer-Specific Masked Decay Based on our insights on flip rate, we propose a method to suppress the frequent change of masks during FST for transformers, which we call masked decay. Unlike Equation (8) which imposes regularization directly on weights, we propose to add masked decay on gradients, i.e., gt ← ∇wLt(˜ wt−1) +λW (m(wt−1) ⊙ wt−1). (10) On SGD, applying decay on weights and on gradients are equivalent, but on popular optimizers like Adam and AdamW they aren’t. Specifically, Adam updates weights by wt ← wt−1 − γ(β1ut−1 + (1− β1)gt) (1 − βt 1)(√ˆvt + ϵ) (11) where u and v are the first and second order momentum of w. Compared to Equation (8), the masked decay regu- larization term in Equation (10) would be later normalized by √ˆvt + ϵ in Equation (11), before it is subtracted from weights. In this way, each dimension receives a different intensity of decay (“masked decay”). More specifically, weights with larger gradients get smaller decay intensity, and vice versa. In FST, we periodically prune weights by their magnitudes. STE may cause the network to fall into such “dilemma points”, where a portion of pruned weights and unpruned weights have nearly the same L1 norm. Thus, the network consistently oscillate between two possible masks m1 and m2, and is unlikely to jump out the dilemma itself. To illustrate this, we split each weight matrix by small 4 × 4 blocks. We count each block’s cumulative flip number and measure the ”L1 norm gap” by gi = ∥m1 ⊙ wi∥1 − ∥m2 ⊙ wi∥1, where wi is the i-th 4 × 4 weights, m1 ⊙ wi and m2 ⊙ wi have the first and second largest L1-norm among different pruning binary masks. The selected mask is most likely to oscillate between m1 and m2, especially when gi is small. In STE, there exists more 4 × 4 blocks 4Accelerating Transformer Pre-training with 2:4 Sparsity Figure 2.Scatter plots of cumulative flip number and L1 norm gap gi on every 4 × 4 block. All results are selected on Transformer- base, with epoch=20. (a) shows the result of dense model. (b)-(d) shows that of masked decaying on gradients, no decaying, and masked decaying on weights. Also, we do it on purpose to choose an extremely large λW for SR-STE. Figure 3.Applying masked decay on weights takes no effect to inhibit flip rate on BERT-base (compared to applying directly on gradient). Table 2.Optimal λW for multiple models. MODEL OPTIMAL λW RESNET18 (Z HOU ET AL ., 2021) 2 E-4 BERT-BASE 6E-6 TRANSFORMER -BASE 1E-6 DEIT-TINY 2E-3 GPT-2 124M 6 E-5 350M 2 E-4 774M 2 E-4 1558M 6 E-5 with high flip num and low ”L1 norm gap”; see Figure 2. This results in overall flip rate explosion of STE. On these occasions, we argue that an evenly masked de- cay applied on weights is insufficient to save the training from such “traps”. The weights don’t differentiate them- selves after an update, so masks may oscillate back. By normalizing the weight gradients with √ˆvt + ϵ, our masked decay amplifies the regularization strength for the dimen- sion with smaller gradient, pushing it towards zero. Then, the regularized dimension can no longer compete with other dimensions. So we effectively break the tie and push the training process out of the trap, towards a “healthier” state. The comparison results between our masked decay defined in Equation (10) and the conventional counterpart in Equa- tion (8) are shown in Figure 3. Results show that applying masked decay on weights takes no effect to inhibit flip rate explosion of STE, while applying on gradients works fine. 4.3. Fast Decay Factor Determination The determination of the decay factor λW in Equation (10) is non-trivial: if λW is excessively large, then the “peak” of the flip rate curve is not high enough; ifλW is too small, the “tail” of the curve is not low enough. Both do not provide a healthy training process. Besides, we find that λW values for CNNs and other small-scale networks differ significantly from those for transformers, while on transformers, optimal λW can span up to three orders of magnitude (Table 2). As pre-training large transformers is costly, grid searching for λW with the final accuracy is impractical, so it is vital to determine a feasible λW as quickly as possible. To quickly determine λW , here we propose a test-based method: 1) Grid search on the warm-up stage of training. For each λW value in a candidate set, sample a corresponding flip rate of the sparse network from a small number of training steps. Note that sampling in early training stage is enough to obtain a representative flip rate specific to a sparse network. 2) Comparison with the dense counterparts. Suppose rt0 to be the standard flip rate on the dense network at time t0 and r ′ t0 to be the sparse network’s flip rate. Their ratio is µ = r ′ t0/rt0 . We suggest that a feasibleλW should have µ ∈ [0.60, 0.95] and the sparse network may suffer from an accuracy drop if µ ≥ 1. 4.4. Dense Fine-Tuning To better improve accuracy, we suggest using a “dense fine- tuning” procedure at the end of training. Formally, we select a switch point ts. FST is performed while t ≤ ts, and dense training is switched to if t > ts. Why Choose Dense Fine-Tuning Instead of Dense Pre- training? While previous work (Han et al., 2017) suggest to switch between sparse and dense training stages, some recent works like STEP (Lu et al., 2023) utilize dense pre- training rather than dense fine-tuning, which means a dense network is initially trained for a period of time before being switched to a sparse one. However, we argue that dense pre- training is meaningless in our FST process. As described in 5Accelerating Transformer Pre-training with 2:4 Sparsity Figure 4.Dense fine-tuning versus dense pre-training on BERT- base Section 4.1, the peak of the flip rate curve should be suffi- ciently high to explore connection modes, so what matters most to the flip rate is the magnitudes of weights, which are the key to determine if connections are built or demol- ished. In this regard, both FST and dense pre-training are capable of delivering proper gradient magnitudes, so dense pre-training is a waste. The precise gradients are generally more necessary in the later stages of training, where the flip rate of the dense network comes to its tail. Figure 4 visual- izes the loss curve of pre-training BERT-base, where dense pre-train obtains nearly the same result as the naive SR-STE method. From this, we propose the following insight: If dense pre-training of tα steps provides slight improve- ment of accuracy, then moving the tα dense steps to the end gives far more improvement than dense pre-training. As for the specific position of the switch point in training, STEP (Lu et al., 2023) suggests that the dense pre-training occupy 10% to 50% of the total steps. Likewise, we deter- mine that our dense fine-tuning takes up the last 1/6 of total steps for balance training efficiency and accuracy. 5. Training Acceleration Techniques For transformers, the forward pass of FST involves prun- ing weights in FFNs with transposable 2:4 masks and then performing normal forward propagation. During backward propagation in FST, the gradients of input activations and weight gradients in FFNs are derived by Equation (3) and (4), respectively. Note that we also utilize MVUE to prune gradients of output activations, i.e., Equation (6). Compared to dense training, our FST replaces all the GEMMs in FFNs with 2:4-spMMs that theoretically perform 2x faster than their dense counterparts on GPUs within sparse tensor cores. In addition to speeding up the most time-consuming GEMMs in FFNs, there are three major operations that also have non-negligible impacts on training speed: 1) Pruning. In FST, pruning includes two steps: finding a mask that satisfies the 2:4 sparse patterns and then enforc- ing the mask to the corresponding dense matrices. In our case, we find that the time cost of finding transposable masks is time-consuming. 2) Activation functions. In transformers, SwiGLU and GEGLU (Shazeer, 2020) are popular. These two acti- vation functions involve a gate mechanism to regulate activations. This mechanism easily induces the GPU L2 cache misses, thus decreasing the computing speed. 3) Updating optimizer states. The excessive update fre- quency can introduce additional time overheads. Below, we show our methods to accelerate these operations, the main workflow of which is shown in Appendix B. 5.1. Fast Computation of Transposable Masks Problem Formulation We aim to find such a mask matrix M ∈ {0, 1}r×q for every W ∈ Rr×q in the FFN layer that 1) each adjoining 4 × 4 block contains 8 non-zero positions; each row and column in the block occupies 2 non-zero elements exactly; 2) maxM ∥M ⊙ W∥1. Then M would be our targeting transposable mask. As described in Equation (5), both a transposable mask itself and its transposition conform to the format of 2:4 sparsity. Previous 2-approximation algorithm (Hubara et al., 2021) consists of two steps: sort elements, and pick elements out of the array. They claim that the procedure has less computational complexity. However, in practice, the sorting and picking process contains too many jumps in its control flow, and may be fatal to modern GPU architecture. To make full use of the GPUs’ parallel computation capability (SIMD and SIMT), we convert the transposable mask-search process into a convolution operation which traverse all the masks to obtain the optimal one in three steps: 1) Create a convolutional kernel in the shape of 4 × 4 × nt, where nt denotes the number of transposable masks. In the case of 2:4 sparsity, mask diversity nt = 90. These mask blocks for 2:4 sparsity can be selected by exhaus- tively inspecting all potential masks offline. 2) Calculate the index matrix via Algorithm 1. The index matrix denotes which 4 × 4 mask in the convolutional kernel is the optimal mask that retains most of the weight norms after being applied to weights. Algorithm 1 transposable mask search Input: mask pattern m′, weight matrix W 1. W = abs(W) 2. out = conv2d(W, m′, stride= 4, padding= 0) 3. index = argmax(out, dim= 2) return index 3) Replace all the elements in the index matrix by the cor- responding 4 × 4 block, which is the desired mask. 6Accelerating Transformer Pre-training with 2:4 Sparsity Figure 5.Transposable mask search Figure 6.left: adapted method; right: intuitive method Table 3.Throughput of two transposable search kernels on RTX3090 (TB/s). INPUT METHOD 2-APPROX OURS FP16 FP32 FP16 FP32 3072 × 768 18.5 36.4 69.2 104.7 4096 × 1024 22.5 38.4 91.9 131.5 5120 × 1280 22.6 44.4 91 128.2 1024 × 1600 22.8 44.8 95 134.5 8192 × 2048 23 45.1 99.4 142.9 16384 × 4096 23.2 45.4 100.1 144.8 30768 × 8192 23.2 45.5 100.9 145.1 Table 4.Throughput of two GEGLU implementations on RTX3090 with fp16 column-major input tensors (TB/s). INPUT METHOD INTUITIVE OURS 32 × 512 × 768 18.4 55.5 32 × 512 × 1024 19.9 55.7 32 × 512 × 1280 18.2 55.9 32 × 512 × 1600 18.4 55.9 32 × 512 × 2048 19.5 56 32 × 512 × 4096 11.8 56.1 32 × 512 × 8192 12.1 56.2 Notably, step (1) is executed offline. Step (2) and (3) are fre- quently performed during FST. The workflow of our method is shown in Figure 5. Compared to the 2-approximation al- gorithm, our method is up to about 5 times faster (Table 3). 5.2. Acceleration of Gated Activation Functions Activation functions with gated mechanisms are widely used in transformers such as GLM (Du et al., 2022) and LLaMA (Touvron et al., 2023). Typical gated activation functions involve SwiGLU and GEGLU. The bottleneck of such activation functions is that the gate operations easily incur GPU L2 cache miss. Take GEGLU as an example: GEGLU(X, U, V, b, c) = GELU(XU⊤ +b)⊙(XV⊤ + c), where X ∈ Rp×q, U, V ∈ Rr×q, b, c ∈ Rr. In prac- tice, this function is composed of three steps: 1) Concatenate U and V into a new weight matrix W ∈ R2r×q, and b, c into a new bias vector d ∈ R2r. 2) Directly calculate Z = XW⊤ + d ∈ Rp×2r as a com- pressed matrix. 3) Split the Z in the second dimension intoZ1, Z2 ∈ Rp×r. Calculate GELU(Z1) ⊙ Z2. Different from dense model, where output activations are row-major matrices, in FST, the output activations are column-major; see Appendix A.2. This property results in the third step being extremely time-consuming if conven- tionally Z is accessed along the row dimension. To illustrate, Figure 6 shows that in a column-major matrix Z, accessing along the column accords with array layout. Thus, adjacent elements loaded into the GPU cache can be probably hit. By contrast, accessing along the row does not fully utilize the efficiency of GPU cache. In light of this, we carefully imple- ment a GEGLU kernel where elements are accessed along the column dimension. In this way, GEGLU is performed 5 times faster than the naive counterpart; see Table 4. 5.3. Other Implementation Details Reducing Updating Frequency We find that a 2:4 mask doesn’t change a lot after one optimization step, and it is not necessary to update a mask frequently. For the sake of efficiency, we update the transposable masks of weights every l optimizer steps. We usually take l = 40in practice. Utilities For 2:4-spMMs, we use CUTLASS (Thakkar et al., 2023). Other GPU kernels are implemented in Triton, including transposable mask search kernel, pruning kernel, MVUE kernel, GEGLU kernel, and masked decay kernel. 6. Experiments In this section, we validate the proposed training speedup methods on several transformers, including BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019), Transformer- 2Results reported in the original paper; see https: //github.com/facebookresearch/deit/blob/ main/README_deit.md. 3DeiT-base dense model using the original recipe. 7Accelerating Transformer Pre-training with 2:4 Sparsity Table 5.GLUE scores of different 2:4 training methods with BERT. METHODLOSS AVG SCORECOLA MNLI MNLIEXTRAMRPC QNLI QQP RTE SST-2 STS-B DENSE 2.066979.8±0.4 45.3±1.1 82.6±0.2 83.4±0.1 78.8±1.7/86.1±1 89 .3±0.2 90.3±0.1/87.1±0 55.8±0.9 91±0.5 83 .7±1/83.7±1HALF 2.128077.9±0.4 37.2±1.3 82.4±0.1 83±0.3 75 .1±1.4/84.2±0.7 88 .8±0.3 89.9±0.1/86.6±0.1 51.2±2.4 92.1±0.5 82.1±0.5/82.3±0.4STEP 2.1179 77.7±0.1 40.4±1.4 82.2±0.1 82.8±0.1 74.5±0.7/83.5±0.4 88 .3±0.4 90.2±0.1/87±0.1 50.8±2.1 92.3±0.3 79.7±1.2/80.7±0.6BI-MASK2.117677.7±0.3 38.3±0.7 82.3±0.1 83±0.1 74 .3±0.7/83±0.6 88 .3±0.3 90.2±0.1/86.9±0.1 53.1±1.4 90.9±0.3 80.9±0.7/81.7±0.4OURS 2.0968 79.6±0.6 44.4±1.9 82.6±0.2 83±0.1 80.9±0.7/87.4±0.4 88.4±0.3 90.3±0.1/87±0.1 54.3±1 91.2±0.4 82.9±2.1/83±1.7 Table 6.GLUE scores with different model sizes on GPT-2 models. PARAMSMETHODVAL LOSSAVGSCORECOLA MNLI MRPC QNLI QQP RTE SST-2 STS-B WNLI 124M DENSE 2.907 73.9±1.1 44.6±0.9 82±0.1 78.3±1.3/84.8±1 88 .4±0.2 90±0 86 .5±0/61.3±1.5 91 .9±0.2 77.3±3.2/77.9±2.9 24.3±7.1OURS 2.952 74.3±0.5 44.8±1.3 81.5±0.2 77.5±1.8/84.2±1.3 87.8±0.1 89.5±0.1 85.9±0.1/66±1 90.6±0.4 80±0.8/80.3±0.5 23.9±6.4 350M DENSE 2.618 76.3±0.1 54.3±0.4 85.1±0.1 80.7±1/86.6±0.7 90 .7±0.1 91±0.1 87.8±0.1/64.9±1.7 93.5±0.4 81.7±1.2/82.2±0.8 17.6±3.2OURS 2.688 77.1±0.2 51.8±1.8 84.3±0.1 80.6±1.3/86.5±0.8 90.4±0.2 90.7±0.1 87.5±0.1/66.7±1.3 93.3±0.4 83.4±1.1/83.5±1.1 26.4±4 774M DENSE 2.493 76.2±0.4 57.5±2 86.1±0.1 80.3±1.3/86.4±0.9 91.4±0.2 91.1±0.1 88±0.1/67.7±2.6 94 .6±0.4 77.3±3.3/78.4±2.9 15.1±2.3OURS 2.564 77.1±0.4 55.9±0.9 85.6±0.2 81.2±0.6/87±0.4 91.4±0.1 91±0.1 87.8±0.1/71.5±0.7 94.2±0.4 81.8±1.3/82.3±1.2 15.8±1.2 1558MDENSE 2.399 76.5±0.5 55.3±2 87 ±0.1 79 ±1/85.3±0.8 91 .8±0.3 91.3±0.1 88.3±0.1/73.3±2 95 .9±0.3 78.5±2.4/79.2±2.5 13±1.3OURS 2.489 77.1±0.5 56.4±3 86.6±0.1 80±0.4/86.1±0.3 91.9±0.1 91.4±0.1 88.4±0.1/75±1.8 95.2±0.4 80.6±1.1/81.1±1.3 12.7±1.1 Table 7.SQuAD scores on GPT-2 models. PARAMS METHOD EM F1 124M DENSE 67.6 78.8 OURS 67.5 78 .5 350M DENSE 73.2 83.6 OURS 71.9 82 .4 774M DENSE 74.3 84.9 OURS 74.3 84 .6 Table 8.Experimental results for DeiT. SIZE METHOD ACC@1 A CC@5 DEIT-TINY ORIGINAL 2 72.2 91.1 DENSE 3 72.9 91.6 OURS 70.4 90 .1 DEIT-SMALL ORIGINAL 79.9 90.5 DENSE 79.9 94.5 BI-MASK 77.6 - OURS 79.2 94.8 DEIT-BASE ORIGINAL 81.8 95.6 DENSE 81.0 95.0 OURS 81.3 95 .4 Table 9.Experimental results for Transformer-base. METHOD AVG EPOCH LOSS TEST BLEU VAL BLEU VAL LOSS DENSE 4.558 26.15 26.56 3.982 HALF 4.659 26.12 26.36 4.041 STEP 4.692 25.27 25.85 4.082 OURS 4.649 26 .48 26 .78 3 .977 base for machine translation (Vaswani et al., 2023), and DeiT (Touvron et al., 2021b). For BERT, we use Cramming (Geiping & Goldstein, 2022) to pre-train a 16-layer BERT model with the sequence length of 512 on the C4 dataset (Raffel et al., 2019). For GPT-2, we use nanoGPT (Karpathy, 2023) to pre-train GPT-2 124M, 355M, 774M, and 1.5B on OpenWebText (Gokaslan & Cohen, 2019). Both BERT and GPT-2 models are estimated on GLUE (Wang et al., 2018). For DeiT (Touvron et al., 2021a), we pre-train DeiT-tiny on ImageNet-1K dataset (Deng et al., 2009). Besides, we use fairseq (Ott et al., 2019) to train Transformer-base on the WMT 14 En-De dataset (Bojar et al., 2014) and measure the BLEU (Papineni et al., 2002) score of the trained model. Of note, we use n to denote the length of sequences, d to denote the input and output dimensions of each trans- former block, dff to denote the inner dimensions of the FFNs in each transformer block, h to denote the number of heads, and N to denote the micro-batch size on each device. The pre-training and evaluation scripts are pub- licly available at https://github.com/thu-ml/ 2by4-pretrain-acc-examples . 6.1. Accuracy Results To investigate the effect of different 2:4 sparse training meth- ods, we pre-train a sparse BERT-base model on the C4 dataset using two sparse training methods: STEP (Lu et al., 2023) and Bi-Mask (Zhang et al., 2023). Besides, we also pre-train a dense BERT-base and a ‘Half’ BERT-base for comparison. Of note, ‘Half’ denotes a smaller yet still dense BERT-base model. To create Half model, we simply reduce the dff of each FFN layer in the original BERT-base by half while maintaining the original value of d. Theoretically, this adjustment halves the floating operations (FLOPs) of the original FFN layer as well. Except for the FFN layers, the shapes of the rest layers remain unaltered. All the pre-trained models are measured on GLUE bench- mark (WNLI excluded). Surprisingly, Table 5 shows that despite having identical FLOPs, the 2:4-sparse BERT-base trained with STEP and Bi-Mask shows inferior average scores compared to the Half model. The Half model attains 8Accelerating Transformer Pre-training with 2:4 Sparsity Table 10.Experimental results of masked decay, MVUE, and dense fine-tuning (FT) with BERT-Base. For decay term, we use both techniques in Sections 4.2 and 4.3. MASKED DECAY MVUE D ENSE FT L OSS AVG SCORE % % % 2.1553 77.6 ± 0.2 ! % % 2.1096 79.2 ± 0.2 ! ! % 2.1172 78.4 ± 0.3 ! % ! 2.0896 79.4 ± 0.2 ! ! ! 2.0968 79 .6 ±0.6 Table 11.Actual pre-train speed up on the whole network. PARAMETERS BATCH SIZE SPEEDUP 124M 16 1.18 350M 8 1.2 774M 4 1.21 Figure 7.Result of acceleration ratio S of different batch sizes and embedding Sizes. (a) shows the acceleration of a FFN layer. (b)-(d) shows the acceleration of a transformer block when n = 2048, 1024, 512. an average score of 77.9 on GLUE tests, while STEP and Bi-Mask only reach 77.7 due to the weaknesses in MRPC, QNLI, and STSB. By comparison, BERT-base trained in our proposed training method achieves 79.6 on GLUE, which significantly outperforms other sparse training methods and is comparable with the dense baseline, i.e., 79.8. Besides, we pre-train GPT-2 models with proposed meth- ods. Table 6 and 7 shows that our method for model sizes of 124M, 350M, 775M and 1558M achieves lossless scores compared with dense baselines. Similarly, DeiT and Transformer-base trained with our method also reach com- parable results to dense training; see Table 8 and 9. For GPT-2 and BERT, the training loss curves are sketched in Appendix C. Ablation Study We aim to investigate the effect of masked decay, MVUE and dense fine-tuning introduced in Section 4.2, 3.2, and 4.4. The 16-layer BERT-base is used for ablation study. Results in Table 10 show that: 1) The dense fine-tuning procedure helps to improve accuracy on GLUE by 2 points at most ; 2) MVUE leads to insignifi- cant, controllable accuracy loss; 3) By combining all these techniques together, 2:4 sparse training for transformers achieves comparable accuracy results as dense training. 6.2. Speedup Results The training acceleration techniques proposed in Section 5 are evaluated using GPT-2 models and RTX3090 GPUs. FP16 mixed precision training is used on all models. The practical speedups of a single FFN layer, a single trans- former block, and the entire network, compared to their re- spective dense counterparts, are reported. All the measured datum contain both forward and backward propagation. Feed-forward Network Layers For a single FFN layer, we fix n = 2048and change d. Results in Figure 7 show that a FFN layer can be accelerated up to 1.7x faster than its corresponding dense layer. Transformer Block We measure the acceleration ratio of a transformer block when n = 512, 1024, 2048. Results in Figure 7 show that in most cases, a transformer block can be accelerated to 1.3x faster via 2:4 sparsity. To illustrate this, a detailed profile result is given in Appendix D. End-to-end Acceleration Finally, we test the practical speedups of training GPT-2 models. Results in Table 11 show that our training method conducts up to 1.2x faster than the dense training on a single RTX3090. 7. Conclusions In this study, we are the first to propose accelerating the pre-training of transformers by 2:4 sparsity. We analyze the limitations of previous 2:4 training methods, including the impropriety in choosing positions and determining values of the masked decay factor, speed bottleneck incurred by computing transposable masks and gated activation func- tions. We propose a series of techniques to tackle them. Our training method is validated on DeiT, BERT, Transformer- base and GPT-2 models. In particular, we have attained 1.2x end-to-end training acceleration for the GPT-2 774M model without losing its accuracy. 9Accelerating Transformer Pre-training with 2:4 Sparsity Acknowledgements We would like to thank Ziteng Wang, Bingrui Li and Haocheng Xi for valuable discussions and help on the training large transformers. This work was supported by the National Key Research and Development Pro- gram of China (No. 2021ZD0110502), NSFC Projects (Nos. 62376131, 62061136001, 62106123, 62076147, U19A2081, 61972224), Tsinghua Institute for Guo Qiang, and the High Performance Computing Center, Tsinghua University. J.Z is also supported by the XPlorer Prize. Impact Statement Our proposed efficient algorithm can be used to accelerate pre-training large-scale transformers like GLM (Du et al., 2022), LLaMA (Touvron et al., 2023), etc. Recently, large transformers have exhibited remarkable efficacy in various fields such as natural language processing, computer vision, and speech recognition. However, the pre-training stage of large transformers is computationally intensive and time- consuming. For instance, pre-training a GPT-4 can span several months, even using a supercomputer equipped with thousands of GPUs. Thus, acceleration approaches are nec- essary. Our fully sparse training approach of transformers can potentially accelerate the FFN layers of a model by the- oretical 2x faster, without loss of accuracy. Thus, it can be potentially used to save energy and reduce carbon footprint. But this work can also be used to accelerate baleful software, like software that generates malicious contents, which may have a negative impact on human society. References Anthony, L. F. W., Kanding, B., and Selvan, R. Carbon- tracker: Tracking and predicting the carbon footprint of training deep learning models, 2020. Bengio, Y ., L´eonard, N., and Courville, A. Estimating or propagating gradients through stochastic neurons for conditional computation, 2013. Bojar, O., Buck, C., Federmann, C., Haddow, B., Koehn, P., Leveling, J., Monz, C., Pecina, P., Post, M., Saint- Amand, H., Soricut, R., Specia, L., and Tamchyna, A. Findings of the 2014 workshop on statistical machine translation. In WMT@ACL, 2014. URL https://api. semanticscholar.org/CorpusID:15535376. BUSATO, F. and POOL, J. Exploiting nvidia ampere struc- tured sparsity with cusparselt [online]. 2020 [visited on 2021-10-10]. Chen, T., Frankle, J., Chang, S., Liu, S., Zhang, Y ., Wang, Z., and Carbin, M. The lottery ticket hypothesis for pre- trained bert networks, 2020. Chen, X., Cheng, Y ., Wang, S., Gan, Z., Wang, Z., and Liu, J. Earlybert: Efficient bert training via early-bird lottery tickets, 2021. Chmiel, B., Hubara, I., Banner, R., and Soudry, D. Min- imum variance unbiased n:m sparsity for the neural gradients. In The Eleventh International Conference on Learning Representations, 2023. URL https:// openreview.net/forum?id=vuD2xEtxZcj. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vi- sion and Pattern Recognition, pp. 248–255, 2009. doi: 10.1109/CVPR.2009.5206848. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding, 2019. Du, Z., Qian, Y ., Liu, X., Ding, M., Qiu, J., Yang, Z., and Tang, J. Glm: General language model pretraining with autoregressive blank infilling, 2022. Evci, U., Gale, T., Menick, J., Castro, P. S., and Elsen, E. Rigging the lottery: Making all tickets winners, 2021. Frankle, J. and Carbin, M. The lottery ticket hypothesis: Finding sparse, trainable neural networks, 2019. Frankle, J., Dziugaite, G. K., Roy, D. M., and Carbin, M. Stabilizing the lottery ticket hypothesis, 2020. Geiping, J. and Goldstein, T. Cramming: Training a lan- guage model on a single gpu in one day, 2022. Gokaslan, A. and Cohen, V . Openwebtext cor- pus. http://Skylion007.github.io/ OpenWebTextCorpus, 2019. Han, S., Pool, J., Tran, J., and Dally, W. J. Learning both weights and connections for efficient neural networks, 2015. Han, S., Mao, H., and Dally, W. J. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding, 2016. Han, S., Pool, J., Narang, S., Mao, H., Gong, E., Tang, S., Elsen, E., Vajda, P., Paluri, M., Tran, J., Catanzaro, B., and Dally, W. J. Dsd: Dense-sparse-dense training for deep neural networks, 2017. Hu, Z., Lan, Y ., Wang, L., Xu, W., Lim, E.-P., Lee, R. K.-W., Bing, L., and Poria, S. Llm-adapters: An adapter fam- ily for parameter-efficient fine-tuning of large language models. arXiv preprint arXiv:2304.01933, 2023. 10Accelerating Transformer Pre-training with 2:4 Sparsity Hubara, I., Chmiel, B., Island, M., Banner, R., Naor, S., and Soudry, D. Accelerated sparse neural training: A provable and efficient method to find n:m transposable masks, 2021. Karpathy, A. nanogpt. https://github.com/ karpathy/nanoGPT/, 2023. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization, 2017. Lasby, M., Golubeva, A., Evci, U., Nica, M., and Ioannou, Y . Dynamic sparse training with structured sparsity, 2023. Lee, N., Ajanthan, T., and Torr, P. H. Snip: Single-shot network pruning based on connection sensitivity. arXiv preprint arXiv:1810.02340, 2018. Li, Z., Wallace, E., Shen, S., Lin, K., Keutzer, K., Klein, D., and Gonzalez, J. Train big, then compress: Rethinking model size for efficient training and inference of trans- formers. In International Conference on machine learn- ing, pp. 5958–5968. PMLR, 2020. Loshchilov, I. and Hutter, F. Decoupled weight decay regu- larization, 2019. Lu, Y ., Agrawal, S., Subramanian, S., Rybakov, O., Sa, C. D., and Yazdanbakhsh, A. Step: Learning n:m struc- tured sparsity masks from scratch with precondition, 2023. McDanel, B., Dinh, H., and Magallanes, J. Accelerating dnn training with structured data gradient pruning, 2022. Mishra, A., Latorre, J. A., Pool, J., Stosic, D., Stosic, D., Venkatesh, G., Yu, C., and Micikevicius, P. Accelerating sparse deep neural networks, 2021. Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of NAACL- HLT 2019: Demonstrations, 2019. Papineni, K., Roukos, S., Ward, T., and Zhu, W. J. Bleu: a method for automatic evaluation of machine translation. 10 2002. doi: 10.3115/1073083.1073135. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsu- pervised multitask learners. 2019. URL https: //api.semanticscholar.org/CorpusID: 160025533. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv e-prints, 2019. Shazeer, N. Glu variants improve transformer, 2020. Thakkar, V ., Ramani, P., Cecka, C., Shivam, A., Lu, H., Yan, E., Kosaian, J., Hoemmen, M., Wu, H., Kerr, A., Nicely, M., Merrill, D., Blasig, D., Qiao, F., Majcher, P., Springer, P., Hohnerbach, M., Wang, J., and Gupta, M. CUTLASS, January 2023. URL https://github. com/NVIDIA/cutlass. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jegou, H. Training data-efficient image trans- formers & amp; distillation through attention. In Interna- tional Conference on Machine Learning, volume 139, pp. 10347–10357, July 2021a. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J ´egou, H. Training data-efficient image trans- formers & distillation through attention, 2021b. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation lan- guage models. arXiv preprint arXiv:2302.13971, 2023. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need, 2023. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. Glue: A multi-task benchmark and anal- ysis platform for natural language understanding. In BlackboxNLP@EMNLP, 2018. URL https://api. semanticscholar.org/CorpusID:5034059. Xu, W., He, X., Cheng, K., Wang, P., and Cheng, J. Towards fully sparse training: Information restoration with spatial similarity. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 2929–2937, 2022. You, H., Li, C., Xu, P., Fu, Y ., Wang, Y ., Chen, X., Baraniuk, R. G., Wang, Z., and Lin, Y . Drawing early-bird tickets: Towards more efficient training of deep networks, 2022. Zhang, Y ., Luo, Y ., Lin, M., Zhong, Y ., Xie, J., Chao, F., and Ji, R. Bi-directional masks for efficient n:m sparse training, 2023. Zhou, A., Ma, Y ., Zhu, J., Liu, J., Zhang, Z., Yuan, K., Sun, W., and Li, H. Learning n:m fine-grained structured sparse neural networks from scratch, 2021. Zhou, D., Ye, M., Chen, C., Meng, T., Tan, M., Song, X., Le, Q., Liu, Q., and Schuurmans, D. Go wide, then narrow: Efficient training of deep thin networks. In In- ternational Conference on Machine Learning, pp. 11546– 11555. PMLR, 2020. 11Accelerating Transformer Pre-training with 2:4 Sparsity A. 2:4-spMM A.1. 2:4 Sparsity Examples of row-wise, column-wise and transposable 2:4 sparse matrix are shown in Figure 8. Note that transposable 2:4 sparsity aligns with both row-wise and column-wise 2:4 sparsity. Figure 8.Row-wise 2:4, column-wise and transposable 2:4 sparse matrix. A.2. Array Layout The array layout of different types of matrix multiplications are listed in Table 12, which explains why output activations and activation gradients are column-major matrices in FST. Table 12.Array layout of MN. Here S denotes that the matrix is in row-wise 2:4 sparsity, R denotes row-major dense matrix, and C denotes column-major dense matrix. M N S S ⊤ R C S % % R R S⊤ % % % % R % C R R C % C R R B. Workflow The main workflow of a single linear layer in FST process is depicted in Figure 9. Figure 9.2:4 sparse training iteration for a layer on a single batch. 12Accelerating Transformer Pre-training with 2:4 Sparsity C. Training Loss Curve For BERT-base and GPT-2, we depict training loss curve in Figure 10. Figure 10.Left: train loss of GPT-2; right: train loss of BERT. D. Profiling result To explain how we reach 1.3x block speedup, we profile our code and break down the time costs as shown in the table below; see Table 13. Table 13.Time costs of each part of our network and the dense model in one iteration per layer. m denotes the accumulation steps over micro batches. Our method is evaluated on GPT-2, with batch size 16, sequence length 1024, embedding dimension 1024 and heads number 16. DENSE (MS/EXEC ) SPARSE (MS/EXEC ) ACCELERATION RATIO S FREQUENCY (EXEC /ITER ) FFN LINEAR FWD GEMM 12173.8 7305.78 1.666324472 - BWD GEMM 23295 14080.82 1.654378083 - MVUE+ PRUNE 0 171.4 - - TOTAL 23295 14252.22 1.634482207 - TOTAL 35468.8 21558 1.645273216 - OTHERS 4 FWD 167 118.17 - - BWD 65.5 20.03 - - TOTAL 232.5 138.2 - - TOTAL FWD 12340.8 7423.95 1.662295678 - BWD 23360.5 14272.25 1.636777663 - TOTAL 35701.3 21696.2 1.645509352 - OTHERS FWD 6874.3 7090.55 - - BWD 13920.7 14117.45 - - TOTAL 20795 21208 - - TOTAL FWD 19215.1 14514.5 1.323855455 - BWD 37281.2 28389.7 1.313194574 - TOTAL 56496.3 42904.2 1.316801152 - MASKED DECAY 0 45.2 - 1 m PRUNE WEIGHTS 0 320.3 - 1 m TRANSPOSABLE MASK SEARCH 0 634.8 - 1 40m 4All functions in FFN except linear layers, i.e., activation function and dropout. 13",
      "meta_data": {
        "arxiv_id": "2404.01847v3",
        "authors": [
          "Yuezhou Hu",
          "Kang Zhao",
          "Weiyu Huang",
          "Jianfei Chen",
          "Jun Zhu"
        ],
        "published_date": "2024-04-02T11:12:42Z",
        "venue": "Proceedings of the 41st International Conference on Machine\n  Learning (2024), in Proceedings of Machine Learning Research 235:19531-19543",
        "pdf_url": "https://arxiv.org/pdf/2404.01847v3.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the challenge of accelerating the pre-training of large transformers by leveraging the 2:4 sparse matrix multiplication capability of NVIDIA Ampere GPUs. It identifies and overcomes limitations of previous 2:4 sparse training methods, which were primarily designed for CNNs and suffered from low accuracy and practical inefficiency. The key contributions include proposing three accuracy-preserving techniques (masked decay on gradients, fast decay factor determination, and dense fine-tuning near the end of pre-training) and two practical acceleration techniques (convolution-based transposable 2:4 mask calculation and GPU L2 cache miss reduction for gated activations). This work is the first to report end-to-end acceleration for pre-training transformers with 2:4 sparsity, achieving comparable or superior accuracy to dense training and up to 1.2x actual end-to-end speedup on GPT-2 models.",
        "methodology": "The core methodology involves fully sparse training (FST) where General Matrix Multiplications (GEMMs) in Feed-Forward Networks (FFNs) are replaced with 2:4 sparse matrix multiplications (2:4-spMMs). To preserve accuracy, a 'flip rate' metric is introduced to monitor mask stability. A transformer-specific masked decay is applied to *gradients* (rather than weights), where the decay term is normalized by Adam/AdamW optimizers to provide differential regularization, helping to resolve 'dilemma points' and stabilize training. A fast, test-based method is proposed to determine the optimal decay factor (λW) during the warm-up stage, by comparing the sparse network's flip rate ratio to its dense counterpart (µ ∈ [0.60, 0.95]). Dense fine-tuning is implemented in the last 1/6 of pre-training steps. For acceleration, transposable 2:4 masks are computed efficiently using a convolution operation (up to 5x faster than previous methods) and gated activation functions (like GEGLU) are accelerated by a custom kernel that optimizes GPU L2 cache utilization (up to 5x faster). Mask update frequency is also reduced (every 40 optimizer steps). CUTLASS is used for 2:4-spMMs, and custom kernels are implemented in Triton.",
        "experimental_setup": "The proposed methods were validated across various transformer models: BERT (16-layer), GPT-2 (124M, 355M, 774M, 1.5B parameters), Transformer-base, and DeiT-tiny. Pre-training was conducted on the C4 dataset for BERT, OpenWebText for GPT-2, ImageNet-1K for DeiT-tiny, and WMT 14 En-De for Transformer-base. Evaluation benchmarks included GLUE (for BERT and GPT-2), SQuAD (for GPT-2), ImageNet-1K (ACC@1, ACC@5 for DeiT), and BLEU score (for Transformer-base). All experiments utilized NVIDIA RTX3090 GPUs with FP16 mixed precision training. Baselines for comparison included dense training, a 'Half' BERT model (with FFN inner dimensionality halved), STEP, and Bi-Mask. Ablation studies were performed on BERT-base to evaluate the impact of masked decay, MVUE, and dense fine-tuning.",
        "limitations": "The primary acceleration benefits of the method are inherently dependent on specific hardware architectures, specifically NVIDIA Ampere GPUs and their tensor cores. While individual FFN layers showed up to 1.7x speedup, the observed end-to-end training speedup for the entire network was up to 1.2x, indicating that other parts of the training pipeline still present bottlenecks or that the integration isn't perfectly efficient. Additionally, the authors note a potential for negative societal impact, as the acceleration techniques could be used to speed up the development of 'baleful software' that generates malicious content.",
        "future_research_directions": "Future research directions include applying the proposed efficient algorithm to accelerate the pre-training of other large-scale transformer models, such as GLM and LLaMA. There is also potential for further optimizing the remaining bottlenecks in the overall training pipeline to achieve even greater end-to-end speedups beyond the current 1.2x. Continued efforts to explore the method's capability in saving energy and reducing the carbon footprint associated with training large language models are also implied."
      }
    },
    {
      "title": "Transformer Quality in Linear Time",
      "abstract": "We revisit the design choices in Transformers, and propose methods to address\ntheir weaknesses in handling long sequences. First, we propose a simple layer\nnamed gated attention unit, which allows the use of a weaker single-head\nattention with minimal quality loss. We then propose a linear approximation\nmethod complementary to this new layer, which is accelerator-friendly and\nhighly competitive in quality. The resulting model, named FLASH, matches the\nperplexity of improved Transformers over both short (512) and long (8K) context\nlengths, achieving training speedups of up to 4.9$\\times$ on Wiki-40B and\n12.1$\\times$ on PG-19 for auto-regressive language modeling, and 4.8$\\times$ on\nC4 for masked language modeling.",
      "full_text": "Transformer Quality in Linear Time Weizhe Hua* 1 2 Zihang Dai * 2 Hanxiao Liu * 2 Quoc V . Le2 Abstract We revisit the design choices in Transformers, and propose methods to address their weaknesses in handling long sequences. First, we propose a simple layer named gated attention unit, which allows the use of a weaker single-head atten- tion with minimal quality loss. We then propose a linear approximation method complementary to this new layer, which is accelerator-friendly and highly competitive in quality. The resulting model, named FLASH3, matches the perplexity of improved Transformers over both short (512) and long (8K) context lengths, achieving training speedups of up to 4.9×on Wiki-40B and 12.1× on PG-19 for auto-regressive language modeling, and 4.8×on C4 for masked language modeling. 1. Introduction Transformers (Vaswani et al., 2017) have become the new engine of state-of-the-art deep learning systems, leading to many recent breakthroughs in language (Devlin et al., 2018; Brown et al., 2020) and vision (Dosovitskiy et al., 2020). Although they have been growing in model size, most Trans- formers are still limited to short context size due to their quadratic complexity over the input length. This limitation prevents Transformer models from processing long-term information, a critical property for many applications. Many techniques have been proposed to speedup Transform- ers over extended context via more efﬁcient attention mech- anisms (Child et al., 2019; Dai et al., 2019; Rae et al., 2019; Choromanski et al., 2020; Wang et al., 2020; Katharopoulos et al., 2020; Beltagy et al., 2020; Zaheer et al., 2020; Kitaev et al., 2020; Roy et al., 2021; Jaegle et al., 2021). Despite the linear theoretical complexity for some of those methods, vanilla Transformers still remain as the dominant choice in *Equal contribution 1Cornell University 2Google Re- search, Brain Team. Correspondence to: Weizhe Hua <wh399@cornell.edu>, Zihang Dai <zihangd@google.com>, Hanxiao Liu <hanxiaol@google.com>. Proceedings of the39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s). 0 5 10 15 20 25 30 TPU-core-days -2.8 -3.0 -3.2 Neg. log pplx FLASH TFM+ + TFM 4.9x 25.6x Speedup Length over TFM over TFM++ 512 1.8 × 1.2× 1024 9.0 × 1.3× 2048 8.9 × 1.6× 4096 13.1 × 2.7× 8192 25.6 × 4.9× Figure 1: TPU-v4 training speedup of FLASH relative to the vanilla Transformer (TFM) and an augmented Transformer (TFM++) for auto-regressive language modeling on Wiki- 40B — All models are comparable in size at around 110M and trained for 125K steps with 218 tokens per batch. state-of-the-art systems. Here we examine this issue from a practical perspective, and ﬁnd existing efﬁcient attention methods suffer from at least one of the following drawbacks: • Inferior Quality. Our studies reveal that vanilla Trans- formers, when augmented with several simple tweaks, can be much stronger than the common baselines used in the literature (see Transformer vs. Transformer++ in Figure 1). Existing efﬁcient attention methods often incur signiﬁcant quality drop compared to augmented Trans- formers, and this drop outweighs their efﬁciency beneﬁts. • Overhead in Practice . As efﬁcient attention methods often complicate Transformer layers and require extensive memory re-formatting operations, there can be a nontrivial gap between their theoretical complexity and empirical speed on accelerators such as GPUs or TPUs. • Inefﬁcient Auto-regressive Training. Most attention lin- earization techniques enjoy fast decoding during infer- ence, but can be extremely slow to train on auto-regressive tasks such as language modeling. This is primarily due to their RNN-style sequential state updates over a large number of steps, making it infeasible to fully leverage the strength of modern accelerators during training. 3FLASH = Fast Linear Attention with a Single Head arXiv:2202.10447v2  [cs.LG]  27 Jun 2022Input Concat V softmax(QK+B) Gated Linear Unit  + Multi-Head Self-Attention Gated Attention Unit (Ours) Input Dense Dense Dense Dense Q K V V relu2(QK+B) Input Dense Dense Dense V Q & K def scale_offset(x): gamma = var(x.shape[=1:]) beta = var(x.shape[=1:]) return x ∗ gamma + beta def attn(x, v, s=128): z = dense(x, s) q, k = scale_offset(z), scale_offset(z) qk = tf.einsum(/quotesingle.ts1bns,bms→bnm/quotesingle.ts1, q, k) a = relu(qk + rel_pos_bias(q, k)) ∗∗ 2 return tf.einsum(/quotesingle.ts1bnm,bme→bne/quotesingle.ts1, a, v) def gated_attn_unit(x, d=768, e=1536): shortcut, x = x, norm(x) u, v = dense(x, e), dense(x, e) x = u ∗ attn(x, v) return dense(x, d) + shortcut Figure 2: (a) An augmented Transformer layer which consists of two blocks: Gated Linear Unit (GLU) and Multi-Head Self-Attention (MHSA), (b) Our proposed Gated Attention Unit (GAU), (c) Pseudocode for Gated Attention Unit. Skip connection and input normalization over the residual branch are omitted in (a), (b) for brevity. We address the above issues by developing a new model fam- ily that, for the ﬁrst time, not only achieves parity with fully augmented Transformers in quality, but also truly enjoys lin- ear scalability over the context size on modern accelerators. Unlike existing efﬁcient attention methods which directly aim to approximate the multi-head self-attention (MHSA) in Transformers, we start with a new layer design which nat- urally enables higher-quality approximation. Speciﬁcally, our model, named FLASH, is developed in two steps: First, we propose a new layer that is more desirable for effective approximation. We introduce a gating mechanism to alleviate the burden of self-attention, resulting in the Gated Attention Unit(GAU) in Figure 2. As compared to Transformer layers, each GAU layer is cheaper, and more importantly, its quality relies less on the precision of atten- tion. In fact, GAU with a small single-head, softmax-free attention is as performant as Transformers. While GAU still suffers from quadratic complexity over the context size, it weakens the role of attention hence allows us to carry out approximation later with minimal quality loss. We then propose an efﬁcient method to approximate the quadratic attention in GAU, leading to a layer variant with linear complexity over the context size. The key idea is to ﬁrst group tokens into chunks, then using precise quadratic attention within a chunk and fast linear attention across chunks, as illustrated in Figure 4. We further describe how an accelerator-efﬁcient implementation can be naturally de- rived from this formulation, achieving linear scalability in practice with only a few lines of code change. We conduct extensive experiments to demonstrate the efﬁ- cacy of FLASH over a variety of tasks (masked and auto- regressive language modeling), datasets (C4, Wiki-40B, PG- 19) and model scales (110M to 500M). Remarkably, FLASH is competitive with fully-augmented Transformers (Trans- former++) in quality across a wide range of context sizes of practical interest (512–8K), while achieving linear scala- bility on modern hardware accelerators. For example, with comparable quality, FLASH achieves a speedup of 1.2×– 4.9×for language modeling on Wiki-40B and a speedup of 1.0×–4.8×for masked language modeling on C4 over Transformer++. As we further scale up to PG-19 (Rae et al., 2019), FLASH reduces the training cost of Transformer++ by up to 12.1×and achieves signiﬁcant gain in quality. 2. Gated Attention Unit Here we present Gated Attention Unit (GAU), a simpler yet more performant layer than Transformers. While GAU still has quadratic complexity over the context length, it is more desirable for the approximation method to be presented in Section 3. We start with introducing related layers: Vanilla MLP. Let X ∈RT×d be the representations over T tokens. The output for Transformer’s MLP can be formu- lated as O= φ(XWu)Wo where Wu ∈Rd×e, Wo ∈Re×d. Here ddenotes the model size, edenotes the expanded inter- mediate size, and φis an element-wise activation function. Gated Linear Unit (GLU). This is an improved MLP augmented with gating (Dauphin et al., 2017). GLU has been proven effective in many cases (Shazeer, 2020; Narang et al., 2021) and is used in state-of-the-art Transformer language models (Du et al., 2021; Thoppilan et al., 2022). U = φu(XWu), V = φv(XWv) ∈RT×e (1) O= (U ⊙V)Wo ∈RT×d (2) where ⊙stands for element-wise multiplication. In GLU, each representation ui is gated by another representation vi associated with the same token.0.0 0.2 0.4 Step/sec/TPU-core 3.1 3.0 2.9 2.8 2.7 Neg. log pplx 42M 110M 335M 41M 105M 316M LM (Wiki-40B) MHSA+MLP MHSA+GLU GAU 0.0 0.5 1.0 Steps/sec/TPU-core 1.7 1.6 1.5 1.4 1.3 1.2 Neg. log pplx 42M 110M 335M 41M 105M 316M Masked LM (C4) MHSA+MLP MHSA+GLU GAU Layer Type # of Layers d MHSA+MLP 8+8 512 12+12 768 24+24 1024 MHSA+GLU 8+8 512 12+12 768 24+24 1024 GAU 15 512 22 768 46 1024 Figure 3: GAU vs. Transformers for auto-regressive and masked language modeling on short context length (512). Gated Attention Unit (GAU). The key idea is to formu- late attention and GLU as a uniﬁed layer and to share their computation as much as possible (Figure 2). This not only results in higher param/compute efﬁciency, but also natu- rally enables a powerful attentive gating mechanism. Specif- ically, GAU generalizes Eq. (2) in GLU as follows: O= (U ⊙ˆV)Wo where ˆV = AV (3) where A∈RT×T contains token-token attention weights. Unlike GLU which always uses vi to gate ui (both asso- ciated with the same token), our GAU replaces vi with a potentially more relevant representation ˆvi = ∑ j aijvj “re- trieved” from all available tokens using attention. The above will reduce to GLU when Ais an identity matrix. Consistent with the ﬁndings in Liu et al. (2021), the presence of gating allows the use of a much simpler/weaker attention mechanism than MHSA without quality loss: Z = φz(XWz) ∈RT×s (4) A= relu2 ( Q(Z)K(Z)⊤+ b ) ∈RT×T (5) Modiﬁcations PPLX (LM/MLM) Params (M) original GAU 16.78 / 4.23 105 relu2 − →softmax 17.04 / 4.31 105 single-head − →multi-head 17.76 / 4.48 105 no gating 17.45 / 4.58 131 Table 1: Impact of various modiﬁcations on GAU. where Z is a shared representation ( s ≪d)4, Qand K are two cheap transformations that apply per-dim scalars and offsets to Z (similar to the learnable variables in Lay- erNorms), and bis the relative position bias. We also ﬁnd the softmax in MHSA can be simpliﬁed as a regular activa- tion function in the case of GAU5. The GAU layer and its 4Unless otherwise speciﬁed, we set s=128 in this work. 5We use squared ReLU (So et al., 2021) throughout this paper, which empirically works well on language tasks. Modiﬁcations PPLX (LM/MLM) Params (M) original MHSA 16.87 / 4.35 110 softmax − →relu2 17.15 / 4.77 110 multi-head − →single-head 17.89 / 4.73 110 add gating 17.25 / 4.43 106 Table 2: Impact of various modiﬁcations on MHSA. pseudocode are illustrated in Figure 2. Unlike Transformer’s MHSA which comes with4d2 param- eters, GAU’s attention introduces only a single small dense matrix Wz with dsparameters on top of GLU (scalars and offsets in Qand Kare negligible). By setting e = 2dfor GAU, this compact design allows us to replace each Trans- former block (MLP/GLU + MHSA) with two GAUs while retaining similar model size and training speed. GAU vs. Transformers. Figure 3 shows that GAUs are competitive with Transformers (MSHA + MLP/GLU) on TPUs across different models sizes. Note these experiments are conducted over a relatively short context size (512). We will see later in Section 4 that GAUs are in fact even more performant when the context length is longer, thanks to their reduced capacity in attention. Layer Ablations. In Table 1 & 2 we show that both GAUs and Transformers are locally optimal on their own. 3. Fast Linear Attention with GAU There are two observations from Section 2 that motivate us to extend GAU to modeling long sequences: • First, the gating mechanism in GAU allows the use of a weaker (single-headed, softmax-free) attention with- out quality loss. If we further adapt this intuition into modeling long sequences with attention, GAU could also boost the effectiveness of approximate (weak) attention mechanisms such as local, sparse and linearized attention.• In addition, the number of attention modules is naturally doubled with GAU — recall MLP+MHSA ≈2×GAU in terms of cost (Section 2). Since approximate attention usu- ally requires more layers to capture full dependency (Dai et al., 2019; Child et al., 2019), this property also makes GAU more appealing in handling long sequences. With this intuition in mind, we start by reviewing some related work on modeling long sequences with attention, and then show how we enable GAU to achieve Transformer- level quality in linear time on long sequences. 3.1. Existing Linear-Complexity Variants Partial Attention. A popular class of methods tries to approximate the full attention matrix with different partial/s- parse patterns, including local window (Dai et al., 2019; Rae et al., 2019), local+sparse (Child et al., 2019; Li et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020), axial (Ho et al., 2019; Huang et al., 2019), learnable patterns through hashing (Kitaev et al., 2020) or clustering (Roy et al., 2021). Though not as effective as full attention, these variants are usually able to enjoy quality gains from scaling to longer sequences. However, the key problem with this class of methods is that they involve extensive irregular or regular memory re-formatting operations such as gather, scatter, slice and concatenation, which are not friendly to modern accelerators of massive parallelism, particularly specialized ASICs like TPU. As a result, their practical beneﬁts (speed and RAM efﬁciency), if any, largely depend on the choice of accelerator and usually fall behind the theoretical analysis. Hence, in this work, we deliberately minimize the number of memory re-formatting operations in our model. Linear Attention. Alternatively, another popular line of research linearizes the attention computation by decompos- ing the attention matrix and then re-arranging the order of matrix multiplications (Choromanski et al., 2020; Wang et al., 2020; Katharopoulos et al., 2020; Peng et al., 2021). Schematically, the linear attention can be expressed as ˆVlin = Q ( K⊤V )    Rd×d approx −−−→ˆVquad = Softmax ( QK⊤)    RT×T V where Q,K,V ∈RT×d are the query, key and value rep- resentations, respectively. Re-arranging the computation reduces the complexity w.r.tT from quadratic to linear. Another desirable property of linear attention is itsconstant6 computation and memory for each auto-regressive decoding step at inference time. To see that, deﬁne Mt = K⊤ :t V:t and notice that the computation of Mt can be fully incremental: Mt = Mt−1 + KtV⊤ t (6) 6Constant is with respective to the sequence length T. cumsum cumsum Figure 4: (top) Quadratic attention, (mid) Linear attention, (bottom) Proposed mixed chunk attention with a chunk size (C) of 2 (Cis always greater than or equal to 128 in our ex- periments). Our method signiﬁcantly reduces the compute in quadratic attention (red links), while requiring substan- tially less RNN-style steps (green squares) in conventional linear attention. This means we only need to maintain a cache with constant O(d2) memory and whenever a new input arrives at time stamp t, only constant O(d2) computation is required to accumulate KtV⊤ t into Mt−1 and get Mt. On the contrary, full quadratic attention requires linear O(Td) computation and memory for each decoding step, as each new input has to attend to all the previous steps. However, on the other hand, re-arranging the computation in linear attention leads to a severe inefﬁciency during auto- regressive training. As shown in Fig. 4 (mid), due to the causal constraint for auto-regressive training, the query vec- tor at each time step Qt corresponds to a different cache value Mt = K⊤ :t V:t. This requires the model to compute and cache T different values {Mt}T t=1 instead of only one value K⊤V in the non-autoregressive case. In theory, the sequence {Mt}T t=1 can be obtained in O(Td2) by ﬁrst com- puting {KtV⊤ t }T t=1 and then performing a large cumulative sum (cumsum) over T tokens. But in practice, the cumsum introduces an RNN-style sequential dependencyof T steps, where an O(d2) state needs to be processed each step. The sequential dependency not only limits the degree of paral- lelism, but more importantly requires T memory accessin the loop, which usually costs much more time than comput- ing the element-wise addition on modern accelerators. As a result, there exists a considerable gap between the theoreti- cal complexity and actual running time. In practice, we ﬁnd that directly computing the full quadratic attention matrix iseven faster than the re-arranged (linearized) version on both TPUs (Figure 6(a)) and GPUs (Appendix C.1). 3.2. Our Method: Mixed Chunk Attention Based on the strengths and weaknesses of existing linear- complexity attentions, we propose mixed chunk attention, which merges the beneﬁts from both partial attention and linear attention. The high-level idea is illustrated in Figure 4. Below we reformulate GAU to incorporate this idea. Preparation. The input sequence is ﬁrst chunked into G non-overlapping chunks of size C, i.e. [T] →[T/C × C]. Then, Ug ∈RC×e, Vg ∈RC×e and Zg ∈RC×s are produced for each chunk gfollowing the GAU formulation in Eq. (1) and Eq. (4). Next, four types of attention heads Qquad g , Kquad g , Qlin g , Klin g are produced from Zg by applying per-dim scaling and offset (this is very cheap). We will describe how GAU’s attention can be efﬁciently approximated using a local attention plus a global attention. Note all the major tensors Ug, Vg and Zg are shared be- tween the two components. The only additional parameters introduced over the original GAU are the per-dim scalars and offsets for generating Qlin g and Klin g (4×sparameters). Local Attention per Chunk. First, a local quadratic at- tention is independently applied to each chunk of length C to produce part of the pre-gating state: ˆVquad g = relu2 ( Qquad g Kquad g ⊤ + b ) Vg. The complexity of this part is O(G×C2 ×d) =O(TCd), which is linear in T given that Cremains constant. Global Attention across Chunks. In addition, a global linear attention mechanism is employed to capture long- range interaction across chunks Non-Causal: ˆVlin g = Qlin g ( G∑ h=1 Klin h ⊤ Vh ) , (7) Causal: ˆVlin g = Qlin g (g−1∑ h=1 Klin h ⊤ Vh ) . (8) Note the summations in Eq. (7) and Eq. (8) are performed at the chunk level. For the causal (auto-regressive) case, this reduces the number of elements in the cumsum in token- level linear attention by a factor of C(a typical Cis 256 in our experiments), leading to a signiﬁcant training speedup. Finally, ˆVquad g and ˆVlin g are added together, followed by gat- ing and a post-attention projection analogous to Eq. (3): Og = [ Ug ⊙ ( ˆVquad g + ˆVlin g )] Wo. def _global_linear_attn(q, k, v, causal): if causal: kv = tf.einsum(/quotesingle.ts1bgcs,bgce→bgse/quotesingle.ts1, k, v) kv = tf.cumsum(kv, axis=1, exclusive=True) return tf.einsum(/quotesingle.ts1bgcs,bgse→bgce/quotesingle.ts1, q, kv) else: kv = tf.einsum(/quotesingle.ts1bgcs,bgce→bse/quotesingle.ts1, k, v) return tf.einsum(/quotesingle.ts1bgcs,bse→bgce/quotesingle.ts1, q, kv) def _local_quadratic_attn(q, k, v, causal): qk = tf.einsum(/quotesingle.ts1bgns,bgms→bgnm/quotesingle.ts1, q, k) a = relu(qk + rel_pos_bias(q, k)) ∗∗ 2 a = causal_mask(a) if causal else a return tf.einsum(/quotesingle.ts1bgnm,bgme→bgne/quotesingle.ts1, a, v) def attn(x, v, causal, s=128): # x: [B x G x C x D]; v: [B x G x C x E] z = dense(x, s) v_quad = _local_quadratic_attn( scale_offset(z), scale_offset(z), v, causal) v_lin = _global_linear_attn( scale_offset(z), scale_offset(z), v, causal) return v_quad + v_lin Code 1: Pseudocode for mixed chunk attention. The mixed chunk attention is simple to implement and the corresponding pseudocode is given in Code 1. 3.2.1. D ISCUSSIONS Fast Auto-regressive Training. Importantly, as depicted in Fig. 4 (bottom), thanks to chunking, the sequential de- pendency in the auto-regressive case reduces from T steps in the standard linear attention to G = T/C steps in the chunked version in Eq. (8). Therefore, we observe the auto- regressive training becomes dramatically faster with the chunk size is in {128,256,512}. With the inefﬁciency of auto-regressive training eliminated, the proposed model still enjoys the constant per-step decoding memory and compu- tation of O(Cd2), where the additional constant C comes from the local quadratic attention. On Non-overlapping Local Attention. Chunks in our method does not overlap with each other. In theory, in- stead of using the non-overlapping local attention, any par- tial attention variant could be used as a substitute while keeping the chunked linear attention ﬁxed. As a concrete example, we explored allowing each chunk to additionally attends to its nearby chunks, which essentially makes the local attention overlapping, similar to Longformer (Belt- agy et al., 2020) and BigBird (Zaheer et al., 2020). While overlapping local attention consistently improves quality, it also introduces many memory re-formatting operations that clearly harm the actual running speed. In our preliminary experiments with language modeling on TPU, we found the cost-beneﬁt trade-off of using overlapping local attention may not be as good as adding more layers in terms of both memory and speed. In general, we believe the optimal par- tial attention variant is task-speciﬁc, while non-overlapping local attention is always a strong candidate when combined with the choice of chunked linear attention.0 10 20 30 Training time -3.4 -3.2 -3. -2.8Neg. log perplexity FLASH-Quad FLASH Transformer Transformer++ Combiner Performer 26 27 28 Latency per step (ms) 29 210 211 212 213 Context length (a) Per-step training latency 0 2 4 6 -1.5 -2.5 -3.5 TPU-core-days Neg. log pplx (b) Context length = 512 0 2 4 6 -1.5 -2.5 -3.5 TPU-core-days Neg. log pplx (c) Context length = 1024 0 5 -1.5 -2.5 -3.5 TPU-core-days Neg. log pplx (d) Context length = 2048 0 5 10 15 -1.5 -2.5 -3.5 TPU-core-days Neg. log pplx (e) Context length = 4096 0 10 20 -1.5 -2.5 -3.5 -5.5 TPU-core-days Neg. log pplx (f) Context length = 8192 Figure 5: Masked language modeling validation-set results on the C4 dataset — All models are comparable in size at around 110M (i.e., BERT-Base scale) and trained for 125K steps with 218 tokens per batch. The quality is measured in negative log perplexity. Connections to Combiner. Similar to our method, Com- biner (Ren et al., 2021) also splits the sequence into non- overlapping chunks and utilizes quadratic local attention within each chunk. The key difference lies in how the long- range information is summarized and combined with the local information (e.g., our mixed chunk attention allows larger effective memory per chunk hence leads to better quality). See Appendix A for detailed discussions. 4. Experiments We focus on two of our models that have different com- plexities with respect to the context length. The quadratic- complexity model FLASH-Quad refers to a stack of GAUs whereas the linear-complexity model named FLASH con- sists of both GAUs and the proposed mixed chunk attention. To demonstrate their efﬁcacy and general applicability, we evaluate them on both bidirectional and auto-regressive se- quence modeling tasks over multiple large-scale datasets. Baselines. First of all, the vanilla Transformer (Vaswani et al., 2017) with GELU activation function (Hendrycks & Gimpel, 2016) is included as a standard baseline for calibra- tion. Despite of being a popular baseline in the literature, we ﬁnd that RoPE (Su et al., 2021) and GLU (Shazeer, 2020) can lead to signiﬁcant performance boosts. We there- fore also include Transformer + RoPE (Transformer+) and Transformer + RoPE + GLU (Transformer++) as two much stronger baselines with quadratic complexity. To demonstrate the advantages of our models on long se- quences, we further compare our models with two notable linear-complexity Transformer variants—Performer (Choro- manski et al., 2020) and Combiner (Ren et al., 2021), where Performer is a representative linear attention method and Combiner (using a chunked attention design similar to ours) has shown superior cost-beneﬁt trade-off over many other approaches (Ren et al., 2021). To get the best performance, we use the rowmajor-axial variant of Combiner (Combiner- Axial) and the ReLU-kernel variant of Performer. Both models are also augmented with RoPE. For fair comparison, all models are implemented in the same codebase to ensure identical tokenizer and hyper-parameters for training and evaluation. The per-step training latencies of all models are measured using TensorFlow Proﬁler. See Appendix B for detailed settings and model speciﬁcations. 4.1. Bidirectional Language Modeling In BERT (Devlin et al., 2018), masked language modeling (MLM) reconstructs randomly masked out tokens in the input sequence. We pretrain and evaluate all models on0 10 20 30 Training time -3.4 -3.2 -3. -2.8Neg. log perplexity FLASH-Quad FLASH Transformer Transformer++ Combiner Performer 29 210 211 212 213 26 28 210 Context length Latency per step (ms) (a) Per-step training latency 0 4 -2.8 -3.0 -3.2 -3.4 16 20 TPU-core-days Neg. log pplx (b) Context length = 512 0 4 -2.8 -3.0 -3.2 -3.4 16 20 TPU-core-days Neg. log pplx (c) Context length = 1024 0 4 8 -2.8 -3.0 -3.2 -3.4 16 20 TPU-core-days Neg. log pplx (d) Context length = 2048 0 10 20 -2.8 -3.0 -3.2 -3.4 TPU-core-days Neg. log pplx (e) Context length = 4096 0 10 20 30 -2.8 -3.0 -3.2 -3.4 TPU-core-days Neg. log pplx (f) Context length = 8192 Figure 6: Auto-regressive language modeling validation-set results on the Wiki-40B dataset — All models are sized around 110M (i.e., BERT-Base scale) and trained for 125K steps with 218 tokens per batch. The quality is measured in negative log perplexity. the C4 dataset (Raffel et al., 2020). We consistently train each model with 218 tokens per batch for 125K steps, while varying the context length on a wide range including 512, 1024, 2048, 4096, and 8192. The quality of each model is reported in perplexity as a proxy metric for the performance on downstream tasks. The training speed of each model (i.e., training latency per step) is measured with 64 TPU-v4 cores, and the total training cost is reported in TPU-v4-core-days. Figure 5(a) shows the latency of each training step for all models at different context lengths. Results for Trans- former+ are omitted for brevity as it lies in between Trans- former and Transformer++. Across all the six models, laten- cies for Combiner, Performer, and FLASH remain roughly constant as the context length increases, demonstrating lin- ear complexity with respect to context length. FLASH-Quad is consistently faster than Transformer and Transformer++ for all context lengths. In particular, FLASH-Quad is 2 × as fast as Transformer++ when the context length increases to 8192. More importantly, as shown in Figures 5(b)-5(f), for all sequence lengths ranging from 512 to 8192, our mod- els always achieve the best quality (i.e., lowest perplexity) under the same computational resource. In particular, if the goal is to match Transformer++’s ﬁnal perplexity at step 125K, FLASH-Quad and FLASH can reduce the train- ing cost by 1.1×–2.5×and 1.0×–4.8×, respectively. It is worth noting that, to the best of our knowledge, FLASH is the only linear-complexity model that achieves perplexity competitive with the fully-augmented Transformers and its quadratic-complexity counterpart. See Appendix C.2 for a detailed quality and speed comparison of all models. 4.2. Auto-regressive Language Modeling For auto-regressive language modeling, we focus on the Wiki-40B (Guo et al., 2020) and PG-19 (Rae et al., 2019) datasets, which consist of clean English Wikipedia pages and books extracted from Project Gutenberg, respectively. It is worth noting that the average document length in PG-19 is 69K words, making it ideal for evaluating model perfor- mance over long context lengths. We train and evaluate all models with 218 tokens per batch for 125K steps, with context lengths ranging from 512 to 8K for Wiki-40B and 1K to 8K for PG-19. We report token-level perplexity for Wiki-40B and word-level perplexity for PG-19. Figure 6(a) shows that FLASH-Quad and FLASH achieve the lowest latency among quadratic and linear complexity models, respectively. We compare the quality and training cost trade-offs of all models on Wiki40-B over increasingTable 3: Auto-regressive language models on the PG-19 dataset — Latency (Lat.) is measured with 64 TPU-v4 cores. Model Context Length 1024 2048 4096 8192 PPLX Lat. Speedup* PPLX Lat. Speedup* PPLX Lat. Speedup* PPLX Lat. Speedup* Transformer+ 44.45 282 1.00 × 43.14 433 1.00 × 42.80 698 1.00 × 43.27 1292 1.00 × Transformer++ 44.47 292 – 43.18 441 – 43.13 712 – 43.26 1272 1.21 × Combiner 46.04 386 – 44.68 376 – 43.99 374 – 44.12 407 – FLASH-Quad 43.40 231 2.18 × 42.01 273 3.29× 41.46 371 3.59× 41.68 560 5.23× FLASH 44.06 234 1.66× 42.17 237 3.85 × 40.72 234 6.75 × 41.07 250 12.12 × * Measured based on time taken to match Transformer+’s ﬁnal quality (at step 125K) on TPU. – Indicates that the speciﬁc model fails to achieve the same perplexity as Transformer+. context lengths in Figures 6(b)-6(f). Similar to the ﬁndings on MLM tasks, our models dominate all other models in terms of quality-training speed for all sequence lengths. Speciﬁcally, FLASH-Quad reduces the training time of Transformer++ by 1.2×to 2.5×and FLASH cuts the com- pute cost by 1.2×to 4.9×while reaching a similar perplex- ity as Transformer++. Between our own models, FLASH closely tracks the perplexity of FLASH-Quad and starts to achieve a better perplexity-cost trade-off when the con- text length goes beyond 2048. Detailed quality and speed comparisons for all models are included in Appendix C.2. For PG-19, following Rae et al., an increased model scale of roughly 500M parameters (see Table 10) is used for all mod- els in comparison. The results are summarized in Table 3. Compared to the numbers in Wiki-40B, FLASH achieves a more pronounced improvements in perplexity and train- ing time over the augmented Transformers on PG-19. For example, with a context length of 8K, FLASH-Quad and FLASH are able to reach the ﬁnal perplexity (at 125K-step) of Transformer+ in only 55K and 55K steps, yielding 5.23× and 12.12×of speedup, respectively. We hypothesize that the increased gains over Transformer+ arise from the long- range nature of PG-19 (which consists of books). Similar to our previous experiments, FLASH achieves a lower perplex- ity than all of the full-attention Transformer variants while being signiﬁcantly faster, demonstrating the effectiveness of our efﬁcient attention design. 4.3. Fine-tuning To demonstrate the effectiveness of FLASH over down- stream tasks, we ﬁne-tune our pre-trained models on the TriviaQA dataset (Joshi et al., 2017). Passages in Trivi- aQA can span multiple documents, which challenges the capability of the models in handling long contexts. For a fair and meaningful comparison, we pretrain all models on English Wikipedia (same domain as TriviaQA) with a context length of 4096 and a batch size of 64 for 125k steps. For ﬁne-tuning, we sweep over three different learn- ing rates, including 1e−4, 7e−5, and 5e−5, and report the best validation-set F1 score across these runs. Table 4: Results on TrivialQA with context length 4096 — “PT“ stands for pre-training and “FT“ stands for ﬁne-tuning. All models are comparable in size at around 110M. sstands for the head size of the single-head attention. For FLASH, “ﬁrst-to-all” means that we also let the ﬁrst token in each chunk to attend to the entire sequence using a single-head softmax attention. Latency (Lat.) is measured with 32 TPU-v4 cores. Model PT FT PT / FT PPLX F1 Lat. reduction Transformer+ 3.48 74.2 1.00 ×/ 1.00× Combiner 3.51 67.2 2.78×/ 2.75× FLASH-Quads=128 3.24 72.7 1.89 ×/ 1.79× FLASH-Quads=512 3.12 74.8 1.76×/ 1.67× FLASHs=512 3.23 73.3 2.61 ×/ 2.60× FLASHs=512 + ﬁrst-to-all 3.24 73.9 2.78×/ 2.69× We observe that the ﬁne-tuning results of the FLASH fam- ily can beneﬁt from several minor changes in the model conﬁguration. As shown in Table 4, increasing the head size of FLASH-Quad from 128 to 512 leads to a signiﬁcant boost of 2.1 point in the F1 score with negligible impact on speed. We further identify several other tweaks that improve the linear FLASH variant speciﬁcally, including using a small chunk size (128), disabling gradient clipping during ﬁnetuning, using softmax instead of squared ReLU for the [CLS] token, and (optionally) allowing the ﬁrst to- ken in each chunk to attend to the entire sequence using softmax. With those changes, FLASHs=512 achieves compa- rable quality to Transformer+ (0.3 difference in F1 is within the range of variance) while being 2.8×and 2.7×as fast as Transformer+ in pretraining and ﬁne-tuning, respectively. 4.4. Ablation Studies Signiﬁcance of quadratic & linear components. To bet- ter understand the efﬁcacy of FLASH, we ﬁrst study how much the local quadratic attention and the global linear atten- tion contribute to the performance individually. To this end,-3.2 -3.0 -2.8 FLASH FLASH (LocalOnly) FLASH (GlobalOnly) MC-TFM++ 0 2 4 6 8 -4.6 -4.4 0 2 4 6 8 0 2 4 6 8 0 2 4 6 8 TPU-core-days Neg. log pplx (a) Context length = 1024 (b) Context length = 2048 (c) Context length = 4096 (d) Context length = 8192 Figure 7: Ablation study of the proposed FLASH architecture. we create FLASH (LocalOnly) and FLASH (GlobalOnly) by only keeping the local quadratic attention and the global linear attention in FLASH, respectively. In FLASH (Glob- alOnly), we reduce the chunk size from 256 to 64 to produce more local summaries for the global linear attention. In Fig- ure 7 we see a signiﬁcant gap between the full model and the two variants, suggesting that the linear and global attention are complementary to each other — both are critical to the quality of the proposed mixed chunk attention. Signiﬁcance of GAU. Here we study the importance of using GAU in FLASH. To achieve this,we apply the same idea of mixed chunk attention to Transformer++. We re- fer to this variant as MC-TFM++ (MC stands for mixed chunk) which uses quadratic MHSA within each chunk and multi-head linear attention across chunks. Effectively, MC- TFM++ has the same linear complexity as FLASH, but the core for MC-TFM++ is Transformer++ instead of GAU. Figure 7 shows that FLASH outperforms MC-TFM++ by a large margin (more than 2×speedup when the sequence length is greater than 2048), conﬁrming the importance of GAU in our design. We further look into the perplexity in- crease due to our approximation method in Table 5, showing that the quality loss due to approximation is substantially smaller when going from FLASH-Quad to FLASH than go- ing from TFM++ to MC-TFM++. This indicates that mixed chunk attention is more compatible with GAU than MHSA, which matches our intuition that GAU is more beneﬁcial to weaker/approximate attention mechanisms. Impact of Chunk Size. The choice of chunk size can affect both the quality and the training cost of FLASH. We observe that, in general, larger chunk sizes perform better as the context length increases. For example, setting the chunk size to 512 is clearly preferable to the default chunk size (C=256) when the context length exceeds 1024. In practice, hyperparameter search over the chunk size can be performed to optimize the performance of FLASH further, although we did not explore such option in our experiments. More detailed analysis can be found in Appendix C.3. Table 5: Perplexity increases when mixed chunk attention is applied to GAU (→FLASH) or to TFM++ (→MC-TFM++) — Results are reported for MLM and LM with increasing context lengths from 512 to 8192. MLM on C4 512 1024 2048 4096 8192 FLASH-Quad→FLASH 0.0 0.05 0.06 0.07 0.07 TFM++→MC-TFM++ 0.36 0.37 0.49 0.48 0.43 LM on Wiki-40B 512 1024 2048 4096 8192 FLASH-Quad→FLASH -0.05 0.06 0.22 0.30 0.11 TFM++→MC-TFM++ 0.54 0.75 0.86 0.90 0.87 5. Conclusion We have presented FLASH, a practical solution to address the quality and empirical speed issues of existing efﬁcient Transformer variants. This is achieved by designing a per- formant layer (gated linear unit) and by combining it with an accelerator-efﬁcient approximation strategy (mixed chunk attention). Experiments on bidirectional and auto-regressive language modeling tasks show that FLASH is as good as fully-augmented Transformers in quality (perplexity), while being substantially faster to train than the state-of-the-art. A future work is to investigate the scaling laws of this new model family and the performance on downstream tasks. Acknowledgements The authors would like to thank Gabriel Bender, John Blitzer, Maarten Bosma, Andrew Brock, Ed Chi, Hanjun Dai, Yann N. Dauphin, Pieter-Jan Kindermans and David So for their useful feedback. Weizhe Hua was supported in part by the Facebook fellowship. References Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Beltagy, I., Peters, M. E., and Cohan, A. Long- former: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Child, R., Gray, S., Radford, A., and Sutskever, I. Gen- erating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Choromanski, K., Likhosherstov, V ., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. Dai, Z., Yang, Z., Yang, Y ., Carbonell, J., Le, Q. V ., and Salakhutdinov, R. Transformer-xl: Attentive language models beyond a ﬁxed-length context. arXiv preprint arXiv:1901.02860, 2019. Dauphin, Y . N., Fan, A., Auli, M., and Grangier, D. Lan- guage modeling with gated convolutional networks. In Proceedings of the 34th International Conference on Ma- chine Learning - Volume 70, ICML’17, pp. 933–941. JMLR.org, 2017. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding. arXiv preprint arXiv:1810.04805, 2018. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale.arXiv preprint arXiv:2010.11929, 2020. Du, N., Huang, Y ., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y ., Krikun, M., Zhou, Y ., Yu, A. W., Firat, O., et al. Glam: Efﬁcient scaling of language models with mixture- of-experts. arXiv preprint arXiv:2112.06905, 2021. Elfwing, S., Uchibe, E., and Doya, K. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural Networks, 107:3–11, 2018. Gehring, J., Auli, M., Grangier, D., Yarats, D., and Dauphin, Y . N. Convolutional sequence to sequence learning. In Proceedings of the 34th International Conference on Ma- chine Learning - Volume 70, ICML’17, pp. 1243–1252. JMLR.org, 2017. Guo, M., Dai, Z., Vrandecic, D., and Al-Rfou, R. Wiki-40b: Multilingual language model dataset. In LREC 2020, 2020. Hendrycks, D. and Gimpel, K. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. Ho, J., Kalchbrenner, N., Weissenborn, D., and Salimans, T. Axial attention in multidimensional transformers. arXiv preprint arXiv:1912.12180, 2019. Huang, Z., Wang, X., Huang, L., Huang, C., Wei, Y ., and Liu, W. Ccnet: Criss-cross attention for semantic segmen- tation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 603–612, 2019. Jaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zisserman, A., and Carreira, J. Perceiver: General perception with it- erative attention. In International Conference on Machine Learning, pp. 4651–4664. PMLR, 2021. Joshi, M., Choi, E., Weld, D., and Zettlemoyer, L. Trivi- aQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1601–1611, Vancouver, Canada, July 2017. Association for Compu- tational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pp. 5156–5165. PMLR, 2020. Kitaev, N., Kaiser, Ł., and Levskaya, A. Reformer: The efﬁcient transformer. arXiv preprint arXiv:2001.04451, 2020. Li, S., Jin, X., Xuan, Y ., Zhou, X., Chen, W., Wang, Y .- X., and Yan, X. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecast- ing. Advances in Neural Information Processing Systems, 32:5243–5253, 2019. Liu, H., Dai, Z., So, D. R., and Le, Q. V . Pay attention to mlps. NeurIPS, 2021. Narang, S., Chung, H. W., Tay, Y ., Fedus, W., Fevry, T., Matena, M., Malkan, K., Fiedel, N., Shazeer, N., Lan, Z., et al. Do transformer modiﬁcations transfer across implementations and applications? arXiv preprint arXiv:2102.11972, 2021. Nguyen, T. Q. and Salazar, J. Transformers without tears: Improving the normalization of self-attention. CoRR, abs/1910.05895, 2019. URL http://arxiv.org/ abs/1910.05895. Peng, H. et al. Random feature attention. In ICLR, 2021. Rae, J. W., Potapenko, A., Jayakumar, S. M., and Lillicrap, T. P. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019.Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., and Liu, P. J. Ex- ploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020. URL http://jmlr. org/papers/v21/20-074.html. Ramachandran, P., Zoph, B., and Le, Q. V . Searching for activation functions. CoRR, abs/1710.05941, 2017. URL http://arxiv.org/abs/1710.05941. Ren, H., Dai, H., Dai, Z., Yang, M., Leskovec, J., Schuur- mans, D., and Dai, B. Combiner: Full attention trans- former with sparse computation cost. In Beygelzimer, A., Dauphin, Y ., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum? id=MQQeeDiO5vv. Roy, A., Saffar, M., Vaswani, A., and Grangier, D. Efﬁcient content-based sparse attention with routing transform- ers. Transactions of the Association for Computational Linguistics, 9:53–68, 2021. Shazeer, N. GLU variants improve transformer. CoRR, abs/2002.05202, 2020. URL https://arxiv.org/ abs/2002.05202. So, D. R., Ma ´nke, W., Liu, H., Dai, Z., Shazeer, N., and Le, Q. V . Primer: Searching for efﬁcient transformers for language modeling. NeurIPS, 2021. Su, J., Lu, Y ., Pan, S., Wen, B., and Liu, Y . Roformer: Enhanced transformer with rotary position embedding, 2021. Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kul- shreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L., Du, Y ., et al. Lamda: Language models for dialog appli- cations. arXiv preprint arXiv:2201.08239, 2022. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten- tion is all you need. In Advances in neural information processing systems, pp. 5998–6008, 2017. Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Al- berti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., et al. Big bird: Transformers for longer se- quences. In NeurIPS, 2020.A. Connections to Combiner To capture long-term information, Combiner (Ren et al., 2021) additionally summarizes each chunk into summary key and value vectors Ksum,V sum ∈RT/C ×d and concatenate them into the local quadratic attention, i.e. ˆVg = Softmax ( Q[Kg; Ksum] ) [Vg; Vsum]. Effectively, Combiner compresses each chunk of C vectors into a single vector of O(d), whereas our chunked linear attention part compresses each chunk into a matrix Klin h ⊤ Vh of size O(sd) which is stimes larger. In other words, less compression is done in chunked linear attention, allowing increased memory hence a potential advantage over Combiners. Another difference lies in how the compressed long-term information from different chunks are combined, where Combiner reuses the quadratic attention whereas our chunked linear attention simply performs (cumulative) sum. However, it is straightforward to incorporate what Combiner does in our proposed method by constructing an extra[T/C ×T/C] attention matrix to combine the chunk summaries, e.g. Alin = relu2 ( QsumKsum⊤+ bsum ) , ˆVlin g = Qlin g [T/C∑ h=1 alin gh ( Klin h ⊤ Vh )] . We indeed brieﬂy experimented with this variant and found it helpful. But it clearly complicates the overall model design, and more importantly requires the model to store and attend to all chunk summaries. As a result, the auto-regressive decoding complexity will increase to O((C+ T/C)d2) which is length-dependent and no longer constant. Hence, we do not include this feature in our default conﬁguration. B. Experimental Setup B.1. Hyperparameters Bidirectional Language Modeling. Hyperparameters for the MLM task on C4 are listed in Table 6. All models are implemented, trained, and evaluated using the same codebase to guarantee fair comparison. Table 6: Hyperparameters for MLM pretraining on C4. MLM Results (Figure 5) Data C4 Sequence length 512 - 8192 Tokens per batch 218 Batch size 218/Sequence length Number of steps 125K Warmup steps 10K Peak learning rate 7e-4 Learning rate decay Linear Optimizer AdamW Adam ϵ 1e-6 Adam (β1,β2) (0.9, 0.999) Weight decay 0.01 Local gradient clipping* 0.1 Chunk size 256 Hidden dropout 0 GELU dropout 0 Attention dropout (if applicable) 0 * Applied to all models except the vanilla Transformer. Auto-regressive Language Modeling. Hyperparameters for the LM tasks on Wiki-40B and PG-19 are listed in Table 7. All models are implemented, trained, and evaluated using the same codebase to guarantee fair comparison.Table 7: Hyperparameters for LM pretraining on Wiki-40B and PG-19. LM Results (Figure 6) LM Results (Table 3) Data Wiki-40B PG-19 Sequence length 512 - 8192 1024 - 8192 Tokens per batch 218 Batch size 218/Sequence length Number of steps 125K Warmup steps 10K Peak learning rate 7e-4 Learning rate decay Linear Optimizer AdamW Adam ϵ 1e-6 Adam (β1,β2) (0.9, 0.999) Weight decay 0.01 Local gradient clipping* 0.1 Hidden dropout 0 GELU dropout 0 Attention dropout (if applicable) 0 Chunk size 256 512 * Applied to all models except the vanilla Transformer. B.2. Model Speciﬁcations Detailed speciﬁcations of all models used in our experiments are summarized in Tables 8, 9, and 10. In the experiments, SiLU/Swish (Elfwing et al., 2018; Hendrycks & Gimpel, 2016; Ramachandran et al., 2017) is used as the nonlinearity for FLASH-Quad and FLASH, as it slightly outperforms GELU (Hendrycks & Gimpel, 2016) in our models. It is also worth noting that we use ScaleNorm for some masked language models because ScaleNorm runs slightly faster than LayerNorm on TPU-v4 without compromising the quality of the model. Table 8: Model conﬁgurations for MLM experiments on the C4 dataset in Section 4. FLASH-Quad FLASH Transformer Transformer+ Transformer++ Combiner Performer # of attention heads 1 1 12 12 12 12 12 Attention kernel relu2 relu2 softmax softmax softmax softmax relu Attention type Quadratic Mixed Chunk Quadratic Quadratic Quadratic Rowmajor-Axial Linear FFN type GAU1 GAU1 MLP MLP GLU MLP MLP Activation2 SiLU/Swish SiLU/Swish GELU GELU GELU GELU GELU Norm. type3 ScaleNorm ScaleNorm LayerNorm ScaleNorm ScaleNorm ScaleNorm ScaleNorm Absolute position emb. ScaledSin4 ScaledSin4 Learnable5 ScaledSin4 ScaledSin4 ScaledSin4 ScaledSin4 Relative position emb. RoPE RoPE – RoPE RoPE RoPE RoPE # of layers 24 24 12+12 6 12+126 12+126 12+126 12+126 Hidden size 768 768 768 768 768 768 768 Expansion rate 2 2 4 4 4 4 4 Chunk size – 256 – – – 256 – Params (M) 112 112 110 110 110 124 110 1 FLASH-Quad and FLASH combines the attention and feed-forward network into one module named GAU. 2 SiLU/Swish are proposed by Elfwing et al. (2018); Hendrycks & Gimpel (2016); Ramachandran et al. (2017). 3 ScaleNorm and LayerNorm are proposed by Nguyen & Salazar (2019) and Ba et al. (2016), respectively. 4 ScaleSin re-scales sinusoidal position embedding (Vaswani et al., 2017) with a linearnable scalar for stability. 5 The learnable position embedding is proposed by Gehring et al. (2017). 6 The model is consist of 12 attention layers and 12 FFN layers. C. Additional Experimental Results Here, we provide full results on the training speed of different language models using a Nvidia V100 GPU (in Table 11) and the ablation study of chunk size for FLASH (in Figure 8).Table 9: Model conﬁgurations for LM experiments on the Wiki-40B dataset in Section 4. FLASH-Quad FLASH Transformer Transformer+ Transformer++ Combiner Performer # of attention heads 1 1 12 12 12 12 12 Attention kernel relu2 relu2 softmax softmax softmax softmax relu Attention type Quadratic Mixed Chunk Quadratic Quadratic Quadratic Rowmajor-Axial Linear FFN type GAU1 GAU1 MLP MLP GLU MLP MLP Activation2 SiLU/Swish SiLU/Swish GELU GELU GELU GELU GELU Norm. type LayerNorm LayerNorm LayerNorm LayerNorm LayerNorm LayerNorm LayerNorm Absolute position emb. ScaledSin3 ScaledSin3 Learnable4 ScaledSin3 ScaledSin3 ScaledSin3 ScaledSin3 Relative position emb. RoPE RoPE – RoPE RoPE RoPE RoPE # of layers 24 24 12+12 5 12+125 12+125 12+125 12+125 Hidden size 768 768 768 768 768 768 768 Expansion rate 2 2 4 4 4 4 4 Chunk size – 256 – – – 256 – Params (M) 112 112 110 110 110 124 110 1 FLASH-Quad and FLASH combines the attention and feed-forward network into one module named GAU. 2 SiLU/Swish are proposed by Elfwing et al. (2018); Hendrycks & Gimpel (2016); Ramachandran et al. (2017). 3 ScaleSin re-scales sinusoidal position embedding (Vaswani et al., 2017) with a linearnable scalar for stability. 4 The learnable position embedding is proposed by Gehring et al. (2017). 5 The model is consist of 12 attention layers and 12 FFN layers. Table 10: Model conﬁgurations for LM experiments on the PG-19 dataset in Section 4. FLASH-Quad FLASH Transformer+ Transformer++ Combiner # of attention heads 1 1 16 16 16 Attention kernel relu2 relu2 softmax softmax softmax Attention type Quadratic Mixed Chunk Quadratic Quadratic Rowmajor-Axial FFN type GAU1 GAU1 MLP GLU MLP Activation2 SiLU/Swish SiLU/Swish GELU GELU GELU Norm. type LayerNorm LayerNorm LayerNorm LayerNorm LayerNorm Absolute position emb. ScaledSin3 ScaledSin3 ScaledSin3 ScaledSin3 ScaledSin3 Relative position emb. RoPE RoPE RoPE RoPE RoPE # of layers 72 72 36+36 4 36+364 36+364 Hidden size 1024 1024 1024 1024 1024 Expansion rate 2 2 4 4 4 Chunk size – 512 – – 512 Params (M) 496 496 486 486 562 1 FLASH-Quad and FLASH combines the attention and feed-forward network into one module named GAU. 2 SiLU/Swish are proposed by Elfwing et al. (2018); Hendrycks & Gimpel (2016); Ramachandran et al. (2017). 3 ScaleSin re-scales sinusoidal position embedding (Vaswani et al., 2017) with a linearnable scalar for stability. 4 The model is consist of 36 attention layers and 36 FFN layers. Table 11: Comparison of latency for each training step of auto-regressive language modeling on Wiki-40B using a single Nvidia Tesla V100 GPU — Latency is reported in millisecond. OOM stands for the CUDA out of memory error. Performer-Matmul implements the cumulative sum (cumsum) using matrix multiplication. Context length ×Batch size Model 512 ×4 1024 ×2 2048 ×1 4096 ×1 Transformer++ 222.4 243.9 315.0 OOM Performer 823.0 827.4 799.8 OOM Performer-Matmul 697.4 701.7 688.9 OOM FLASH 254.4 235.0 242.8 452.9C.1. Auto-regressive Training on GPU We observe that the inefﬁciency of auto-regressive training is not limited to hardware accelerators such as TPUs. As shown in Table 11, Performer has the largest latency among the three models because it requires to perform cumsum over all tokens sequentially. In contrast, the proposed FLASH achieves the lowest latency when the context length is over 1024, suggesting the effectiveness of the proposed mixed chunk attention mechanism. C.2. Tabular MLM and LM Results We summarize the experimental results of MLM on C4 and LM on Wiki-40B in Tables 12 and 13. Table 12: Bidirectional/masked language models on the C4 dataset. The best perplexity (PPLX) on the validation set is reported. Training latency is measured with 64 TPU-v4 cores. Model Context Length 512 1024 2048 4096 8192 PPLX Latency PPLX Latency PPLX Latency PPLX Latency PPLX Latency Transformer 4.517 47.7 4.436 63.9 4.196 90.9 4.602 142.5 4.8766 252.7 Transformer+ 4.283 48.8 4.151 64.4 4.032 91.5 3.989 142.9 3.986 252.9 Transformer++ 4.205 47.6 4.058 64.6 3.920 91.6 3.876 143.4 3.933 252.1 Performer 5.897 37.2 6.324 37.6 8.032 39.1 12.622 36.9 102.980 40.9 Combiner 4.449 67.2 4.317 66.4 4.238 66.4 4.195 68.3 4.225 77.3 FLASH-Quad 4.176 43.7 3.964 50.1 3.864 61.7 3.828 84.9 3.830 132.1 FLASH 4.172 51.2 4.015 50.1 3.928 51.4 3.902 50.7 3.897 59.9 Table 13: Auto-regressive language models on the Wiki-40B dataset. The best perplexity (PPLX) on the validation set is reported. Training latency is measured with 64 TPU-v4 cores. Model Context Length 512 1024 2048 4096 8192 PPLX Latency PPLX Latency PPLX Latency PPLX Latency PPLX Latency Transformer 17.341 54.0 19.808 70.9 18.154 96.3 17.731 149.1 18.254 260.7 Transformer+ 16.907 55.6 15.999 70.3 15.653 96.1 15.515 149.3 15.478 261.9 Transformer++ 16.835 54.7 15.943 70.9 15.489 96.6 15.282 149.2 15.254 261.0 Performer 18.989 1439.7 18.520 1386.9 18.547 1518.9 18.987 1526.7 19.923 1526.8 Combiner 17.338 75.5 16.710 74.4 16.344 71.8 16.171 71.7 16.119 77.9 FLASH-Quad 16.633 54.1 15.879 59.5 15.305 71.3 14.955 96.1 14.998 141.3 FLASH 16.581 57.2 15.935 56.9 15.525 56.7 15.259 57.0 15.109 62.5 C.3. Ablation Study of Chunk Size The choice of chunk size can have an impact on both the quality and the training cost of FLASH. In the extreme case where chunk size equals the context length, FLASH falls back to FLASH-Quad and loses the scalability to long context lengths. In the other extreme case where chunk size is equal to one, the proposed attention module becomes a linear attention, which suffers from inefﬁcient auto-regressive training. Figure 8 shows the tradeoff between the quality and training cost of four different chunk sizes for context lengths from 1K to 8K. D. Pseudocode For FLASH-Quad and FLASH We show the detailed implementation of FLASH-Quad and FLASH in Codes 6 and 8.(a) Context length = 1024 (b) Context length = 2048 (c) Context length = 4096 (d) Context length = 8192 Figure 8: Ablation study of the chunk size (C) of FLASH for context lengths from 1K to 8K. def _get_scaledsin(embeddings): \"\"\"Create sinusoidal position embedding with a scaling factor.\"\"\" hidden_size = int(embeddings.shape[=1]) pos = tf.range(tf.shape(embeddings)[1]) pos = tf.cast(pos, tf.float32) half_d = hidden_size // 2 freq_seq = tf.cast(tf.range(half_d), tf.float32) / float(half_d) inv_freq = 10000 ∗∗ =freq_seq sinusoid = tf.einsum(/quotesingle.ts1s,d→sd/quotesingle.ts1, pos, inv_freq) scaledsin = tf.concat([tf.sin(sinusoid), tf.cos(sinusoid)], axis==1) scalar = tf.get_variable( /quotesingle.ts1scaledsin_scalar/quotesingle.ts1, shape=(), initializer=tf.constant_initializer(1 / hidden_size ∗∗ 0.5)) scaledsin ∗= scalar return scaledsin Code 2: Pseudocode for ScaledSin absolute position embedding. def rope(x, axis): \"\"\"RoPE position embedding.\"\"\" shape = x.shape.as_list() if isinstance(axis, int): axis = [axis] spatial_shape = [shape[i] for i in axis] total_len = 1 for i in spatial_shape: total_len ∗= i position = tf.reshape( tf.cast(tf.range(total_len, delta=1.0), tf.float32), spatial_shape) for i in range(axis[=1] + 1, len(shape) = 1, 1): position = tf.expand_dims(position, axis==1) half_size = shape[=1] // 2 freq_seq = tf.cast(tf.range(half_size), tf.float32)/float(half_size) inv_freq = 10000 ∗∗ =freq_seq sinusoid = tf.einsum(/quotesingle.ts1...,d→...d/quotesingle.ts1, position, inv_freq) sin = tf.sin(sinusoid) cos = tf.cos(sinusoid) x1, x2 = tf.split(x, 2, axis==1) return tf.concat([x1 ∗ cos = x2 ∗ sin, x2 ∗ cos + x1 ∗ sin], axis==1) Code 3: Pseudocode for RoPE.WEIGHT_INITIALIZER = tf.random_normal_initializer(stddev=0.02) def rel_pos_bias(n): \"\"\"Relative position bias.\"\"\" if n < 512: # Construct Toeplitz matrix directly when the sequence length is less than 512. w = tf.get_variable( /quotesingle.ts1weight/quotesingle.ts1, shape=[2 ∗ n = 1], dtype=tf.float32, initializer=WEIGHT_INITIALIZER) t = tf.pad(w, [[0, n]]) t = tf.tile(t, [n]) t = t[..., :=n] t = tf.reshape(t, [n, 3 ∗ n = 2]) r = (2 ∗ n = 1) // 2 t = t[..., r:=r] else: # Construct Toeplitz matrix using RoPE when the sequence length is over 512. a = tf.get_variable( /quotesingle.ts1a/quotesingle.ts1, shape=[128], dtype=dtype, initializer=WEIGHT_INITIALIZER) b = tf.get_variable( /quotesingle.ts1b/quotesingle.ts1, shape=[128], dtype=dtype, initializer=WEIGHT_INITIALIZER) a = rope(tf.tile(a[None, :], [n, 1]), axis=0) b = rope(tf.tile(b[None, :], [n, 1]), axis=0) t = tf.einsum(/quotesingle.ts1mk,nk→mn/quotesingle.ts1, a, b) return t Code 4: Pseudocode for relative position bias. def norm(x, begin_axis==1, eps=1e=5, norm_type=/quotesingle.ts1layer_norm/quotesingle.ts1): \"\"\"Normalization layer.\"\"\" shape = x.shape.as_list() axes = list(range(len(shape)))[begin_axis:] if norm_type == /quotesingle.ts1layer_norm/quotesingle.ts1: mean, var = tf.nn.moments(x, axes, keepdims=True) x = (x = mean) ∗ tf.rsqrt(var + eps) gamma = tf.get_variable( /quotesingle.ts1gamma/quotesingle.ts1, shape=x.shape.as_list()[begin_axis:], initializer=tf.initializers.ones()) beta = tf.get_variable( /quotesingle.ts1beta/quotesingle.ts1, shape=x.shape.as_list()[begin_axis:], initializer=tf.initializers.zeros()) return gamma ∗ x + beta elif norm_type == /quotesingle.ts1scale_norm/quotesingle.ts1: mean_square =tf.reduce_mean(tf.math.square(x), axes, keepdims=True) x = x ∗ tf.rsqrt(mean_square + eps) scalar = tf.get_variable(/quotesingle.ts1scalar/quotesingle.ts1, shape=(), initializer=tf.constant_initializer(1.0)) return scale ∗ x Code 5: Pseudocode for LayerNorm and ScaleNorm.WEIGHT_INITIALIZER = tf.random_normal_initializer(stddev=0.02) def GAU(x, causal, norm_type=/quotesingle.ts1layer_norm/quotesingle.ts1, expansion_factor=2): \"\"\"GAU block. Input shape: batch size x sequence length x model size \"\"\" seq_len = tf.shape(x)[1] d = int(x.shape[=1]) e = int(d ∗ expansion_factor) shortcut, x = x, norm(x, begin_axis==1, norm_type=norm_type) s = 128 uv = tf.layers.dense(x, 2 ∗ e + s, kernel_initializer=WEIGHT_INITIALIZER, bias_initializer=/quotesingle.ts1zeros/quotesingle.ts1) u, v, base = tf.split(tf.nn.silu(uv), [e, e, s], axis==1) # Generate Query (q) and Key (k) from base. gamma = tf.get_variable(/quotesingle.ts1gamma/quotesingle.ts1, shape=[2, s], initializer=WEIGHT_INITIALIZER) beta = tf.get_variable(/quotesingle.ts1beta/quotesingle.ts1, shape=[2, s], initializer=tf.initializers.zeros()) base = tf.einsum(/quotesingle.ts1...r,hr→...hr/quotesingle.ts1, base, gamma) + beta base = rope(base, axis=1) q, k = tf.unstack(base, axis==2) # Calculate the quadratic attention. qk = tf.einsum(/quotesingle.ts1bnd,bmd→bnm/quotesingle.ts1, q, k) bias = rel_pos_bias(seq_len) kernel = tf.math.square(tf.nn.relu(qk / seq_len + bias)) # Apply the causal mask for auto=regressive tasks. if causal: causal_mask = tf.linalg.band_part( tf.ones([seq_len, seq_len], dtype=x.dtype), num_lower==1, num_upper=0) kernel ∗= causal_mask x = u ∗ tf.einsum(/quotesingle.ts1bnm,bme→bne/quotesingle.ts1, kernel, v) x = tf.layers.dense(x, d, kernel_initializer=WEIGHT_INITIALIZER, bias_initializer=/quotesingle.ts1zeros/quotesingle.ts1) return x + shortcut Code 6: Pseudocode for GAU (FLASH-Quad). def segment_ids_to_mask(segment_ids, causal=False): \"\"\"Generate the segment mask from the segment ids. The segment mask is used to remove the attention between tokens in different documents. \"\"\" min_ids, max_ids = tf.reduce_min(segment_ids, axis==1), tf.reduce_max(segment_ids, axis==1) # 1.0 indicates in the same group and 0.0 otherwise mask = tf.logical_and( tf.less_equal(min_ids[:, :, None], max_ids[:, None, :]), tf.greater_equal(max_ids[:, :, None], min_ids[:, None, :])) mask = tf.cast(mask, tf.float32) if causal: g = tf.shape(min_ids)[1] causal_mask = 1.0 = tf.linalg.band_part( tf.ones([g, g], dtype=tf.float32), num_lower=0, num_upper==1) mask ∗= causal_mask mask = tf.math.divide_no_nan(mask, tf.reduce_sum(mask, axis==1, keepdims=True)) return mask Code 7: Pseudocode for generating segment mask.WEIGHT_INITIALIZER = tf.random_normal_initializer(stddev=0.02) def FLASH(x, causal, segment_ids, norm_type=/quotesingle.ts1layer_norm/quotesingle.ts1, expansion_factor=2): \"\"\"FLASH block. Input shape: batch size x num chunks x chunk length x model size \"\"\" _, g, n, d = x.shape.as_list() e = int(d ∗ expansion_factor) shortcut, x = x, norm(x, begin_axis==1, norm_type=norm_type) s = 128 uv = tf.layers.dense(x, 2 ∗ e + s, kernel_initializer=WEIGHT_INITIALIZER, bias_initializer=/quotesingle.ts1zeros/quotesingle.ts1) u, v, base = tf.split(tf.nn.silu(uv), [e, e, s], axis==1) # Generate Query and Key for both quadratic and linear attentions. gamma = tf.get_variable(/quotesingle.ts1gamma/quotesingle.ts1, shape=[4, s], initializer=WEIGHT_INITIALIZER) beta = tf.get_variable(/quotesingle.ts1beta/quotesingle.ts1, shape=[4, s], initializer=tf.initializers.zeros()) base = tf.einsum(/quotesingle.ts1...r,hr→...hr/quotesingle.ts1, base, gamma) + beta base = rope(base, axis=[1, 2]) quad_q, quad_k, lin_q, lin_k = tf.unstack(base, axis==2) if causal: # Linear attention part. lin_kv = tf.einsum(/quotesingle.ts1bgnk,bgne→bgke/quotesingle.ts1, lin_k, v) / tf.cast(n, x.dtype) mask = segment_ids_to_mask(segment_ids, causal=True) cum_lin_kv = tf.einsum(/quotesingle.ts1bhke,bgh→bgke/quotesingle.ts1, lin_kv, mask) linear = tf.einsum(/quotesingle.ts1bgnk,bgke→bgne/quotesingle.ts1, lin_q, cum_lin_kv) # Quadratic attention part. quad_qk = tf.einsum(/quotesingle.ts1bgnk,bgmk→bgnm/quotesingle.ts1, quad_q, quad_k) bias = rel_pos_bias(n) kernel = tf.math.square(tf.nn.relu(quad_qk / n + bias)) # Apply the causal mask for auto=regressive tasks. causal_mask = tf.linalg.band_part(tf.ones([n, n], dtype=x.dtype), num_lower==1, num_upper=0) quadratic = tf.einsum(/quotesingle.ts1bgnm,bgme→bgne/quotesingle.ts1, kernel ∗ causal_mask, v) else: # Linear attention part lin_kv = tf.einsum(/quotesingle.ts1bgnk,bgne→bgke/quotesingle.ts1, lin_k, v) / tf.cast(n, x.dtype) mask = segment_ids_to_mask(segment_ids) lin_kv = tf.einsum(/quotesingle.ts1bhke,bgh→bgke/quotesingle.ts1, lin_kv, mask) linear = tf.einsum(/quotesingle.ts1bgnk,bgke→bgne/quotesingle.ts1, lin_q, lin_kv) # Quadratic attention part quad_qk = tf.einsum(/quotesingle.ts1bgnk,bgmk→bgnm/quotesingle.ts1, quad_q, quad_k) bias = rel_pos_bias(n) kernel = tf.math.square(tf.nn.relu((quad_qk / n + bias)) quadratic = tf.einsum(/quotesingle.ts1bgnm,bgme→bgne/quotesingle.ts1, kernel, v) x = u ∗ (quadratic + linear) x = tf.layers.dense(x, d, kernel_initializer=WEIGHT_INITIALIZER, bias_initializer=/quotesingle.ts1zeros/quotesingle.ts1) return x + shortcut Code 8: Pseudocode for FLASH.",
      "meta_data": {
        "arxiv_id": "2202.10447v2",
        "authors": [
          "Weizhe Hua",
          "Zihang Dai",
          "Hanxiao Liu",
          "Quoc V. Le"
        ],
        "published_date": "2022-02-21T18:59:38Z",
        "pdf_url": "https://arxiv.org/pdf/2202.10447v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the limitations of Transformers in handling long sequences, specifically their quadratic complexity, inferior quality of existing efficient variants, practical overhead on accelerators, and inefficient auto-regressive training. It proposes FLASH (Fast Linear Attention with a Single Head), a new model that achieves quality comparable to fully augmented Transformers while offering linear scalability and significant training speedups. Key contributions include the Gated Attention Unit (GAU) layer for more effective approximation and a mixed chunk attention mechanism for linear complexity.",
        "methodology": "The FLASH model is developed in two steps: First, the Gated Attention Unit (GAU) layer is introduced, which incorporates a gating mechanism to reduce the reliance on attention precision, allowing for a simpler, weaker (single-head, softmax-free) attention without quality loss. This makes the layer more amenable to approximation. Second, the quadratic attention in GAU is approximated using a mixed chunk attention method. This involves grouping tokens into non-overlapping chunks, applying precise quadratic attention within each chunk, and employing fast linear attention across chunks. This approach significantly reduces the sequential dependency during auto-regressive training from T steps to T/C steps (where C is chunk size), making it highly accelerator-efficient.",
        "experimental_setup": "The models were evaluated on masked language modeling (MLM) and auto-regressive language modeling (LM) tasks, including fine-tuning for question answering (TriviaQA). Datasets used were C4 (MLM), Wiki-40B (LM), and PG-19 (LM, known for long documents). Model scales ranged from 110M to ~500M parameters. Experiments were conducted across various context lengths (512 to 8192 tokens). Baselines included vanilla Transformer, Transformer+ (with RoPE), Transformer++ (with RoPE + GLU), Performer, and Combiner. Training was performed for 125K steps with 2^18 tokens per batch on TPU-v4 cores, with additional GPU benchmarks (Nvidia V100). Quality was measured by negative log perplexity (for LM/MLM) and F1 score (for TriviaQA).",
        "limitations": "While the paper focuses on overcoming existing limitations, an implicit trade-off is made regarding the choice of non-overlapping local attention. Although practical and accelerator-friendly, the paper notes that overlapping local attention could theoretically improve quality but introduces memory re-formatting operations that harm actual running speed. This suggests that the current non-overlapping design prioritizes practical speed over potentially marginal theoretical quality gains from more complex local attention patterns. Additionally, the default configuration for chunked linear attention maintains a constant decoding complexity of O(Cd^2) per step, implicitly accepting that more complex chunk summary combinations (like those explored in Combiner) would lead to length-dependent decoding complexity (O((C+T/C)d^2)), which was avoided.",
        "future_research_directions": "Future work will involve investigating the scaling laws of the new FLASH model family and evaluating its performance on various downstream tasks beyond the presented language modeling and QA tasks."
      }
    },
    {
      "title": "Dynamic Rescaling for Training GNNs"
    },
    {
      "title": "Bandit Samplers for Training Graph Neural Networks",
      "abstract": "Several sampling algorithms with variance reduction have been proposed for\naccelerating the training of Graph Convolution Networks (GCNs). However, due to\nthe intractable computation of optimal sampling distribution, these sampling\nalgorithms are suboptimal for GCNs and are not applicable to more general graph\nneural networks (GNNs) where the message aggregator contains learned weights\nrather than fixed weights, such as Graph Attention Networks (GAT). The\nfundamental reason is that the embeddings of the neighbors or learned weights\ninvolved in the optimal sampling distribution are changing during the training\nand not known a priori, but only partially observed when sampled, thus making\nthe derivation of an optimal variance reduced samplers non-trivial. In this\npaper, we formulate the optimization of the sampling variance as an adversary\nbandit problem, where the rewards are related to the node embeddings and\nlearned weights, and can vary constantly. Thus a good sampler needs to acquire\nvariance information about more neighbors (exploration) while at the same time\noptimizing the immediate sampling variance (exploit). We theoretically show\nthat our algorithm asymptotically approaches the optimal variance within a\nfactor of 3. We show the efficiency and effectiveness of our approach on\nmultiple datasets.",
      "full_text": "Bandit Samplers for Training Graph Neural Networks Ziqi Liu∗ Ant Financial Services Group ziqiliu@antfin.com Zhengwei Wu∗ Ant Financial Services Group zejun.wzw@antfin.com Zhiqiang Zhang Ant Financial Services Group lingyao.zzq@antfin.com Jun Zhou Ant Financial Services Group jun.zhoujun@antfin.com Shuang Yang Ant Financial Services Group shuang.yang@antfin.com Le Song Ant Financial Services Group Georgia Institute of Technology lsong@cc.gatech.edu Yuan Qi Ant Financial Services Group yuan.qi@antfin.com Abstract Several sampling algorithms with variance reduction have been proposed for ac- celerating the training of Graph Convolution Networks (GCNs). However, due to the intractable computation of optimal sampling distribution, these sampling algorithms are suboptimal for GCNs and are not applicable to more general graph neural networks (GNNs) where the message aggregator contains learned weights rather than ﬁxed weights, such as Graph Attention Networks (GAT). The funda- mental reason is that the embeddings of the neighbors or learned weights involved in the optimal sampling distribution arechanging during the training and not known a priori, but only partially observed when sampled, thus making the derivation of an optimal variance reduced samplers non-trivial. In this paper, we formulate the optimization of the sampling variance as an adversary bandit problem, where the rewards are related to the node embeddings and learned weights, and can vary constantly. Thus a good sampler needs to acquire variance information about more neighbors (exploration) while at the same time optimizing the immediate sampling variance (exploit). We theoretically show that our algorithm asymptotically ap- proaches the optimal variance within a factor of 3. We show the efﬁciency and effectiveness of our approach on multiple datasets. 1 Introduction Graph neural networks [ 15, 11] have emerged as a powerful tool for representation learning of graph data in irregular or non-euclidean domains [3, 23]. For instance, graph neural networks have demonstrated state-of-the-art performance on learning tasks such as node classiﬁcation, link and graph property prediction, with applications ranging from drug design [ 8], social networks [ 11], transaction networks [16], gene expression networks [9], and knowledge graphs [19]. One major challenge of training GNNs comes from the requirements of heavy ﬂoating point operations and large memory footprints, due to the recursive expansions over the neighborhoods. For a minibatch with a single vertex vi, to compute its embedding h(L) i at the L-th layer, we have to expand its neighborhood from the (L−1)-th layer to the 0-th layer, i.e. L-hops neighbors. That will soon cover ∗Equal Contribution. Preprint. Under review. arXiv:2006.05806v2  [cs.LG]  11 Jun 2020a large portion of the graph if particularly the graph is dense. One basic idea of alleviating such “neighbor explosion” problem was to sample neighbors in a top-down manner, i.e. sample neighbors in the l-th layer given the nodes in the (l+ 1)-th layer recursively. Several layer sampling approaches [11, 6, 14, 25] have been proposed to alleviate above “neighbor explosion” problem and improve the convergence of training GCNs, e.g. with importance sampling. However, the optimal sampler [14], q⋆ ij = αij∥h(l) j ∥2 ∑ k∈Ni αik∥h(l) k ∥2 for vertex vi, to minimize the variance of the estimator ˆh(l+1) i involves all its neighbors’ hidden embeddings, i.e.{ˆh(l) j |vj ∈Ni}, which is infeasible to be computed because we can only observe them partially while doing sampling. Existing approaches [6, 14, 25] typically compromise the optimal sampling distribution via approximations, which may impede the convergence. Moreover, such approaches are not applicable to more general cases where the weights or kernelsαij’s are not known a priori, but are learned weights parameterized by attention functions [22]. That is, both the hidden embeddings and learned weights involved in the optimal sampler constantly vary during the training process, and only part of the unnormalized attention values or hidden embeddings can be observed while do sampling. Present work. We derive novel variance reduced samplers for training of GCNs and attentive GNNs with a fundamentally different perspective. That is, different with existing approaches that need to compute the immediate sampling distribution, we maintain nonparametric estimates of the sampler instead, and update the sampler towards optimal variance after we acquire partial knowledges about neighbors being sampled, as the algorithm iterates. To fulﬁl this purpose, we formulate the optimization of the samplers as a bandit problem, where the regret is the gap between expected loss (negative reward) under current policy (sampler) and expected loss with optimal policy. We deﬁne the reward with respect to each action, i.e. the choice of a set of neighbors with sample size k, as the derivatives of the sampling variance, and show the variance of our samplers asymptotically approaches the optimal variance within a factor of3. Under this problem formulation, we propose two bandit algorithms. The ﬁrst algorithm based on multi-armed bandit (MAB) chooses k < Karms (neighbors) repeatedly. Our second algorithm based on MAB with multiple plays chooses a combinatorial set of neighbors with size konly once. To summarize, ( 1) We recast the sampler for GNNs as a bandit problem from a fundamentally different perspective. It works for GCNs and attentive GNNs while existing approaches apply only to GCNs. ( 2) We theoretically show that the regret with respect to the variance of our estimators asymptotically approximates the optimal sampler within a factor of 3 while no existing approaches optimize the sampler. ( 3) We empirically show that our approachs are way competitive in terms of convergence and sample variance, compared with state-of-the-art approaches on multiple public datasets. 2 Problem Setting Let G= (V,E) denote the graph with N nodes vi ∈V, and edges (vi,vj) ∈E. Let the adjacency matrix denote as A∈RN×N. Assuming the feature matrix H(0) ∈RN×D(0) with h(0) i denoting the D(0)-dimensional feature of node vi. We focus on the following simple but general form of GNNs: h(l+1) i = σ ( N∑ j=1 α(vi,vj) h(l) j W(l) ) , l = 0,...,L −1 (1) where h(l) i is the hidden embedding of node vi at the l-th layer, ααα= (α(vi,vj)) ∈RN×N is a kernel or weight matrix, W(l) ∈RD(l)×D(l+1) is the transform parameter on the l-th layer, and σ(·) is the activation function. The weight α(vi,vj), or αij for simplicity, is non-zero only if vj is in the 1-hop neighborhood Ni of vi. It varies with the aggregation functions [ 3, 23]. For example, (1) GCNs [8, 15] deﬁne ﬁxed weights as ααα= ˜D−1 ˜Aor ααα= ˜D−1 2 ˜A˜D−1 2 respectively, where ˜A= A+I, and ˜Dis the diagonal node degree matrix of ˜A. (2) The attentive GNNs [22, 17] deﬁne a learned weight α(vi,vj) by attention functions: α(vi,vj) = ˜α(vi,vj;θ)∑ vk∈Ni ˜α(vi,vk;θ) , where the unnormalized attentions ˜α(vi,vj; θ) = exp(ReLU(aT[Whi∥Whj])), are parameterized by θ= {a,W}. Different 2from GCNs, the learned weights αij ∝˜αij can be evaluated only given all the unnormalized weights in the neighborhood. The basic idea of layer sampling approaches [11, 6, 14, 25] was to recast the evaluation of Eq. (1) as ˆh(l+1) i = σ ( N(i) Epij [ ˆh(l) j ] W(l) ) , (2) where pij ∝αij, and N(i) =∑ jαij. Hence we can evaluate each node vi at the (l+ 1)-th layer, using a Monte Carlo estimator with sampled neighbors at the l-th layer. Without loss of generality, we assume pij = αij and N(i) = 1that meet the setting of attentive GNNs in the rest of this paper. To further reduce the variance, let us consider the following importance sampling ˆh(l+1) i = σW(l) ( ˆµ(l) i ) = σW(l) ( Eqij [αij qij ˆh(l) j ]) , (3) where we use σW(l) (·) to include transform parameter W(l) into the function σ(·) for conciseness. As such, one can ﬁnd an alternative sampling distribution qi = (qij1 ,...,q ij|Ni|) to reduce the variance of an estimator, e.g. a Monte Carlo estimator ˆµ(l) i = 1 k ∑k s=1 αijs qijs ˆh(l) js , where js ∼qi. Take expectation overqi, we deﬁne the variance of ˆµ(l) i = αijs qijs ˆh(l) js at step tand (l+ 1)-th layer to be: Vt(qi) =E [ˆµ(l) i (t) −µ(l) i (t)  2] = E [αijs(t) qijs h(l) js (t) − ∑ j∈Ni αij(t)h(l) j (t)  2] . (4) Note that αij and h(vj) that are inferred during training may vary over steps t’s. We will explicitly include step tand layer lonly when it is necessary. By expanding Eq. (4) one can write V(qi) as the difference of two terms. The ﬁrst is a function of qi, which we refer to as the effective variance: Ve(qi) = ∑ j∈Ni 1 qij α2 ij∥hj∥2 , (5) while the second does not depend on qi, and we denote it by Vc = ∑ j∈Ni αijhj  2 . The optimal sampling distribution [6, 14] at (l+ 1)-th layer for vertex ithat minimizes the variance is: q⋆ ij = αij∥h(l) j ∥2 ∑ k∈Ni αik∥h(l) k ∥2 . (6) However, evaluating this sampling distribution is infeasible because we cannot have all the knowledges of neighbors’ embeddings in the denominator of Eq.(6). Moreover, the αij’s in attentive GNNs could also vary during the training procedure. Existing layer sampling approaches based on importance sampling just ignore the effects of norm of embeddings and assume theαij’s are ﬁxed during training. As a result, the sampling distribution is suboptimal and only applicable to GCNs where the weights are ﬁxed. Note that our derivation above follows the setting of node-wise sampling approaches [11], but the claim remains to hold for layer-wise sampling approaches [6, 14, 25]. 3 Related Works We summarize three types of works for training graph neural networks. First, several “layer sampling” approaches [11, 6, 14, 25] have been proposed to alleviate the “neigh- bor explosion” problems. Given a minibatch of labeled vertices at each iteration, such approaches sample neighbors layer by layer in a top-down manner. Particularly, node-wise samplers [11] ran- domly sample neighbors in the lower layer given each node in the upper layer, while layer-wise samplers [6, 14, 25] leverage importance sampling to sample neighbors in the lower layer given all the nodes in upper layer with sample sizes of each layer be independent of each other. Empirically, the layer-wise samplers work even worse [5] compared with node-wise samplers, and one can set an appropriate sample size for each layer to alleviate the growth issue of node-wise samplers. In this paper, we focus on optimizing the variance in the vein of layer sampling approaches. Though the 3derivation of our bandit samplers follows the node-wise samplers, it can be extended to layer-wise. We leave this extension as a future work. Second, Chen et al. [5] proposed a variance reduced estimator by maintaining historical embeddings of each vertices, based on the assumption that the embeddings of a single vertex would be close to its history. This estimator uses a simple random sampler and works efﬁcient in practice at the expense of requiring an extra storage that is linear with number of nodes. Third, two “graph sampling” approaches [7, 24] ﬁrst cut the graph into partitions [ 7] or sample into subgraphs [24], then they train models on those partitions or subgraphs in a batch mode [ 15]. They show that the training time of each epoch may be much faster compared with “layer sampling” approaches. We summarize the drawbacks as follows. First, the partition of the original graph could be sensitive to the training problem. Second, these approaches assume that all the vertices in the graph have labels, however, in practice only partial vertices may have labels [12, 16]. GNNs Architecture.For readers who are interested in the works related to the architecture of GNNs, please refer to the comprehensive survey [23]. Existing sampling approaches works only on GCNs, but not on more advanced architectures like GAT [22]. 4 Variance Reduced Samplers as Bandit Problems We formulate the optimization of sampling variance as a bandit problem. Our basic idea is that instead of explicitly calculating the intractable optimal sampling distribution in Eq. (6) at each iteration, we aim to optimize a sampler or policy Qt i for each vertex iover the horizontal steps 1 ≤t≤T, and make the variance of the estimator following this sampler asymptotically approach the optimum Q⋆ i = argmin Qi ∑T t=1 Vt e(Qi), such that ∑T t=1 Vt e(Qt i) ≤c∑T t=1 Vt e(Q⋆ i) for some constant c >1. Each action of policy Qt i is a choice of any subset of neighbors Si ⊂Ni where Si ∼Qt i. We denote Qi,Si(t) as the probability of the action that vi chooses Si at t. The gap to be minimized between effective variance and the oracle is Vt e(Qt i) −Vt e(Q⋆ i) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. (7) Note that the function Vt e(Qt i) is convex w.r.t Qt i, hence for Qt i and Q⋆ i we have the upper bound derived on right hand of Eq. (7). We deﬁne this upper bound as regret at t, which means the expected loss (negative reward) with policy Qt i minus the expected loss with optimal policy Q⋆ i. Hence the reward w.r.t choosing Si at tis the negative derivative of the effective varianceri,Si(t) = −∇Qi,Si(t)Vt e(Qt i). In the following, we adapt this bandit problem in the adversary bandit setting [1] because the rewards vary as the training proceeds and do not follow a priori ﬁxed distribution [4]. We leave the studies of other bandits as a future work. We show in section 6 that with this regret the variances of our estimators asymptotically approach the optimal variance within a factor of 3. Our samplers sample 1-element subset of neighbors k times or a k-element subset of neighbors once from the alternative sampling distribution qt i = (qij1 (t),...,q ij|Ni|(t)) for each vertex vi. We instantiate above framework under two bandit settings.(1) In the adversary MAB setting [1], we deﬁne the sampler Qt i as qt i, that samples exact anarm (neighbor) vjs ⊂Nifrom qt i. In this case the set Siis the 1-element subset {vjs}. To have a sample size ofkneighbors, we repeat this action ktimes. After we collected krewards rijs(t) =−∇qi,js(t)Vt e(qt i) we update qt i by EXP3 [1]. (2) In the adversary MAB with multiple plays setting [21], it uses an efﬁcient k-combination sampler (DepRound [10]) Qi to sample any k-element subset Si ⊂Ni that satisﬁes ∑ Si:j∈Si Qi,Si = qij,∀vj ∈Ni, where qij corresponds to the alternative probability of sampling vj. As such, it allows us to select from a set of (|Ni| k ) distinct subsets of armsfrom |Ni|arms at once. The selection can be done in O(|Ni|). After we collected the reward −∇Qi,Si(t)Vt e(Qt i), we update qt i by EXP3.M [21]. Discussions. We have to select a sample size of k neighbors in GNNs. Note that in the rigorous bandit setting, exact one action should be made and followed by updating the policy. In adversary MAB, we do the selection ktimes and update the policy, hence strictly speaking applying MAB to our problem is not rigorous. Applying MAB with multiple plays to our problem is rigorous because it allows us to sample k neighbors at once and update the rewards together. For readers who are interested in EXP3, EXP3.M and DepRound, please ﬁnd them in Appendix A. 4Algorithm 1Bandit Samplers for Training GNNs. Require: step T, sample size k, number of layers L, node features H(0), adjacency matrix A. 1: Initialize: qij(1) = 1/|Ni|if j ∈Ni else 0, wij(1) = 1if j ∈Ni else 0. 2: for t= 1to T do 3: Read a minibatch of labeled vertices at layer L. 4: Use sampler qt i or DepRound(k,qt i) to sample neighbors top-down with sample size k. 5: Forward GNN model via estimators deﬁned in Eq. (8) or Proposition 1. 6: Backpropagation and update GNN model. 7: for each vi in the 1-st layer do 8: Collect vi’sksampled neighbors vj ∈St i, and rewards rt i = {rij(t) :vj ∈St i}. 9: Update qt+1 i and wt+1 i by EXP3(qt i,wt i,rt i,St i) or EXP3.M(qt i,wt i,rt i,St i). 10: end for 11: end for 12: return GNN model. 5 Algorithms The framework of our algorithm is: (1) use a sampler Qt i to sample k arms from the alternative sampling distribution qt i for any vertex vi, (2) establish the unbiased estimator, (3) do feedforward and backpropagation, and ﬁnally (4) calculate the rewards and update the alternative sampling distribution with a proper bandit algorithm. We show this framework in Algorithm 1. Note that the variance w.r.tqi in Eq. (4) is deﬁned only at the (l+ 1)-th layer, hence we should maintain multiple qi’s at each layer. In practice, we ﬁnd that maintain a single qi and update it only using rewards from the 1-st layer works well enough. The time complexity of our algorithm is same with any node-wise approaches [11]. In addition, it requires a storage in O(|E|) to maintain nonparametric estimates qi’s. It remains to instantiate the estimators, variances and rewards related to our two bandit settings. We name our ﬁrst algorithm GNN-BS under adversary MAB setting, and the second GNN-BS.M under adversary MAB with multiple plays setting. We ﬁrst assume the weights αij’s are ﬁxed, then extend to attentive GNNs that αij(t)’s change. 5.1 GNN-BS: Graph Neural Networks with Bandit Sampler In this setting, we choose 1 arm and repeat ktimes. We have the following Monte Carlo estimator ˆµi = 1 k k∑ s=1 αijs qijs ˆhjs, js ∼qi. (8) This yields the variance V(qi) = 1 k Eqi [ αijs qijs hjs −∑ j∈Ni αijhj  2] . Following Eq. (5) and Eq. (7), we have the reward of vi picking neighbor vj at step tas rij(t) =−∇qij(t)Vt e(qt i) = α2 ij k·qij(t)2 ∥hj(t)∥2. (9) 5.2 GNN-BS.M: Graph Neural Networks with Multiple Plays Bandit Sampler Given a vertex vi, an important property of DepRound is that it satisﬁes ∑ Si:j∈Si Qi,Si = qij,∀vj ∈ Ni, where Si ⊂Ni is any subset of size k. We have the following unbiased estimator. Proposition 1. ˆµi = ∑ js∈Si αijs qijs hjs is the unbiased estimator of µi = ∑ j∈Ni αijhj given that Si is sampled from qi using the DepRound sampler Qi, where Si is the selected k-subset neighbors of vertex i. The effective variance of this estimator is Ve(Qi) =∑ Si⊂Ni Qi,Si∥∑ js∈Si αijs qijs hjs∥2. Since the derivative of this effective variance w.r.t Qi,Si does not factorize, we instead have the following approximated effective variance using Jensen’s inequality. Proposition 2. The effective variance can be approximated by Ve(Qi) ≤∑ js∈Ni αijs qijs ∥hjs∥2. 5Table 1: Dataset summary. “s” dontes multi-class task, and “m” denotes multi-label task. Dateset V E Degree # Classes # Features # train # validation # test Cora 2,708 5 ,429 2 7 (s) 1,433 1 ,208 500 1 ,000 Pubmed 19,717 44 ,338 3 3 (s) 500 18 ,217 500 1 ,000 PPI 56,944 818 ,716 15 121 (m) 50 44 ,906 6 ,514 5 ,524 Reddit 232,965 11,606,919 50 41 (s) 602 153 ,932 23 ,699 55 ,334 Flickr 89,250 899 ,756 10 7 (s) 500 44 ,625 22 ,312 22 ,313 Proposition 3. The negative derivative of the approximated effective variance ∑ js∈Ni αijs qijs ∥hjs∥2 w.r.tQi,Si, i.e. the reward of vi choosing Si at tis ri,Si(t) =∑ js∈Si αijs qijs(t)2 ∥hjs(t)∥2. Follow EXP3.M we use the reward w.r.t each arm asrij(t) = αij qij(t)2 ∥hj(t)∥2,∀j ∈Si. Our proofs rely on the property of DepRound introduced above. 5.3 Extension to Attentive GNNs In this section, we extend our algorithms to attentive GNNs. The issue remained is that the attention value αij can not be evaluated with only sampled neighborhoods, instead, we can only compute the unnormalized attentions ˜αij. We deﬁne the adjusted feedback attention values as follows: α′ ij = ∑ j∈Si qij · ˜αij∑ j∈Si ˜αij , (10) where ˜αij’s are the unnormalized attention values that can be obviously evaluated when we have sampled (vi,vj). We use ∑ j∈Si qij as a surrogate of ∑ j∈Si ˜αij ∑ j∈Ni ˜αij so that we can approximate the truth attention values αij by our adjusted attention values α′ ij. 6 Regret Analysis As we described in section 4, the regret is deﬁned as ⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. By choosing the reward as the negative derivative of the effective variance, we have the following theorem that our bandit sampling algorithms asymptotically approximate the optimal variance within a factor of 3. Theorem 1. Using Algorithm 1 with η= 0.4 and δ= √ (1−η)η4k5 ln(n/k) Tn4 to minimize the effective variance with respect to {Qt i}1≤t≤T, we have T∑ t=1 Vt e(Qt i) ≤3 T∑ t=1 Vt e(Q⋆ i) + 10 √ Tn4 ln(n/k) k3 (11) where T ≥ln(n/k)n2(1 −η)/(kη2), n= |Ni|. Our proof follows [18] by upper and lower bounding the potential function. The upper and lower bounds are the functions of the alternative sampling probability qij(t) and the reward rij(t) respec- tively. By multiplying the upper and lower bounds by the optimal sampling probability q⋆ i and using the reward deﬁnition in (9), we have the upper bound of the effective variance. The growth of this regret is sublinear in terms of T. The regret decreases in polynomial as sample size kgrows. Note that the number of neighbors nis always well bounded in pratical graphs, and can be considered as a moderate constant number. Compared with existing layer sampling approaches [11, 6, 25] that have a ﬁxed variance given the speciﬁc estimators, this is the ﬁrst work optimizing the sampling variance of GNNs towards optimum. We will empirically show the sampling variances in experiments. 7 Experiments In this section, we conduct extensive experiments compared with state-of-the-art approaches to show the advantage of our training approaches. We use the following rule to name our approaches: GNN 6architecture plus bandit sampler. For example, GCN-BS, GAT-BSand GP-BS denote the training approaches for GCN, GAT [22] and GeniePath [17] respectively. The major purpose of this paper is to compare the effects of our samplers with existing training algorithms, so we compare them by training the same GNN architecture. We use the following architectures unless otherwise stated. We ﬁx the number of layers as 2 as in [15] for all comparison algorithms. We set the dimension of hidden embeddings as 16 for Cora and Pubmed, and 256 for PPI, Reddit and Flickr. For a fair comparison, we do not use the normalization layer [2] particularly used in some works [5, 24]. For attentive GNNs, we use the attention layer proposed in GAT. we set the number of multi-heads as 1 for simplicity. We report results on 5 benchmark data that include Cora [20], Pubmed [20], PPI [11], Reddit [11], and Flickr [24]. We follow the standard data splits, and summarize the statistics in Table 1. Table 2: Comparisons on the GCN architecture: testing Micro F1 scores. Method Cora Pubmed PPI Reddit Flickr GraphSAGE 0.731(±0.014) 0.890(±0.002) 0.689(±0.005) 0.949(±0.001) 0.494(±0.001) FastGCN 0.827(±0.001) 0.895(±0.005) 0.502(±0.003) 0.825(±0.006) 0.500(±0.001) LADIES 0.843(±0.003) 0.880(±0.006) 0.574(±0.003) 0.932(±0.001) 0.465(±0.007) AS-GCN 0.830(±0.001) 0.888(±0.006) 0.599(±0.004) 0.890(±0.013) 0.506(±0.012) S-GCN 0.828(±0.001) 0.893(±0.001) 0.744(±0.003) 0.943(±0.001) 0.501(±0.002) ClusterGCN 0.807(±0.006) 0.887(±0.001) 0.853(±0.001) 0.938(±0.002) 0.418(±0.002) GraphSAINT 0.815(±0.012) 0.899(±0.002) 0.787(±0.003) 0.965(±0.001) 0.507(±0.001) GCN-BS 0.855(±0.005) 0.903(±0.001) 0.905(±0.003) 0.957(±0.000) 0.513(±0.001) Table 3: Comparisons on the attentive GNNs architecture: testing Micro F1 scores. Method Cora Pubmed PPI Reddit Flickr AS-GAT 0.813(±0.001) 0.884(±0.003) 0.566(±0.002) NA 0.472(±0.012) GraphSAINT-GAT0.773(±0.036) 0.886(±0.016) 0.789(±0.001) 0.933(±0.012) 0.470(±0.002) GAT-BS 0.857(±0.003) 0.894(±0.001) 0.841(±0.001) 0.962(±0.001) 0.513(±0.001) GAT-BS.M 0.857(±0.003) 0.894(±0.000) 0.867(±0.003) 0.962(±0.000) 0.513(±0.001) GP-BS 0.811(±0.002) 0.890(±0.003) 0.958(±0.001) 0.964(±0.000) 0.507(±0.000) GP-BS.M 0.811(±0.001) 0.892(±0.001) 0.965(±0.001) 0.964(±0.000) 0.507(±0.000) We summarize the comparison algorithms as follows. (1) GraphSAGE [11] is a node-wise layer sampling approach with a random sampler. (2) FastGCN [6], LADIES [ 25], and AS-GCN [ 14] are layer sampling approaches based on importance sampling. (3) S-GCN [5] can be viewed as an optimization solver for training of GCN based on a simply random sampler. (4) ClusterGCN [7] and GraphSAINT [24] are “graph sampling” techniques that ﬁrst partition or sample the graph into small subgraphs, then train each subgraph using the batch algorithm [15]. (5) The open source algorithms that support the training of attentive GNNs are AS-GCN and GraphSAINT. We denote them as AS-GAT and GraphSAINT-GAT. We do grid search for the following hyperparameters in each algorithm, i.e., the learning rate {0.01,0.001}, the penalty weight on theℓ2-norm regularizers {0,0.0001,0.0005,0.001}, the dropout rate {0,0.1,0.2,0.3}. By following the exsiting implementations2, we save the model based on the best results on validation, and restore the model to report results on testing data in Section 7.1. For the sample size k in GraphSAGE, S-GCN and our algorithms, we set 1 for Cora and Pubmed, 5 for Flickr, 10 for PPI and reddit. We set the sample size in the ﬁrst and second layer for FastGCN and AS-GCN/AS-GAT as 256 and 256 for Cora and Pubmed, 1,900 and 3,800 for PPI, 780 and 1,560 for Flickr, and 2,350 and 4,700 for Reddit. We set the batch size of all the layer sampling approaches and S-GCN as 256 for all the datasets. For ClusterGCN, we set the partitions according 2Checkout: https://github.com/matenure/FastGCN or https://github.com/huangwb/AS-GCN 7to the suggestions [7] for PPI and Reddit. We set the number of partitions for Cora and Pubmed as 10, for ﬂickr as 200 by doing grid search. We set the architecture of GraphSAINT as “0-1-1”3 which means MLP layer followed by two graph convolution layers. We use the “rw” sampling strategy that reported as the best in their original paper to perform the graph sampling procedure. We set the number of root and walk length as the paper suggested. 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Cora GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 0 20 40 60 80 100 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2 4 6 8 10 Epoch 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.350 0.375 0.400 0.425 0.450 0.475 0.500 0.525Micro F1 on validation Flickr GCN-BS GraphSAGE FastGCN LADIES AS-GCN ClusterGCN GraphSAINT-GCN S-GCN 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Cora GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Epoch 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 0 5 10 15 20 25 30 35 40 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT AS-GAT 2 4 6 8 10 Epoch 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GP-BS.M GAT-BS.M GP-BS GAT-BS GraphSAINT-GAT Figure 1: The convergence on validation in terms of epochs. 7.1 Results on Benchmark Data We report the testing results on GCN and attentive GNN architectures in Table 2 and Table 3 respectively. We run the results of each algorithm3 times and report the mean and standard deviation. The results on the two layer GCN architecture show that our GCN-BS performs the best on most of datasets. The results on the two layer attentive GNN architecture show the superiority of our algorithms on training more complex GNN architectures. GraphSAINT or AS-GAT cannot compute the softmax of learned weights, but simply use the unnormalized weights to perform the aggregation. As a result, most of results from AS-GAT and GraphSAINT-GAT in Table 3 are worse than their results in Table 2. Thanks to the power of attentive structures in GNNs, our algorithms perform better results on PPI and Reddit compared with GCN-BS, and signiﬁcantly outperform the results from AS-GAT and GraphSAINT-GAT. 7.2 Convergence In this section, we analyze the convergences of comparison algorithms on the two layer GCN and attentive GNN architectures in Figure 1 in terms of epoch. We run all the algorithms3 times and show the mean and standard deviation. Our approaches consistently converge to better results with faster 3Checkout https://github.com/GraphSAINT/ for more details. 8rates and lower variances in most of datasets like Pubmed, PPI, Reddit and Flickr compared with the state-of-the-art approaches. The GNN-BS algorithms perform very similar to GNN-BS.M, even though strictly speaking GNN-BS does not follow the rigorous MAB setting. Furthermore, we show a huge improvement on the training of attentive GNN architectures compared with GraphSAINT-GAT and AS-GAT. The convergences on validation in terms of timing (seconds), compared with layer sampling approaches, in Appendix C.1 show the similar results. We further give a discussion about timing among layer sampling approaches and graph sampling approaches in Appendix C.2. 3 5 7 10 Sample Size 0.4 0.6 0.8 1.0Micro F1 on test FastGCN LADIES AS-GCN GraphSAGE S-GCN GCN-BS AS-GAT GP-BS 3 5 7 10 Sample Size 0.5 1.0 1.5 2.0Variance FastGCN LADIES AS-GCN GraphSAGE S-GCN GCN-BS AS-GAT GP-BS Figure 2: Comparisons on PPI by varying the sample sizes: ( left) F1 score, (right) sample variances. 7.3 Sample Size Analysis We analyze the sampling variances and accuracy as sample size varies using PPI data. Note that existing layer sampling approaches do not optimize the variances once the samplers are speciﬁed. As a result, their variances are simply ﬁxed [25], while our approaches asymptotically appoach the optimum. For comparison, we train our models until convergence, then compute the average sampling variances. We show the results in Figure 2. The results are grouped into two categories, i.e. results for GCNs and attentive GNNs respectively. The sampling variances of our approaches are smaller in each group, and even be smaller than the variances of S-GCN that leverages a variance reduction solver. This explains the performances of our approaches on testing Micro F1 scores. We also ﬁnd that the overall sampling variances of node-wise approaches are way better than those of layer-wise approaches. 8 Conclusions In this paper, we show that the optimal layer samplers based on importance sampling for training general graph neural networks are computationally intractable, since it needs all the neighbors’ hidden embeddings or learned weights. Instead, we re-formulate the sampling problem as a bandit problem that requires only partial knowledges from neighbors being sampled. We propose two algorithms based on multi-armed bandit and MAB with multiple plays, and show the variance of our bandit sampler asymptotically approaches the optimum within a factor of 3. Furthermore, our algorithms are not only applicable to GCNs but more general architectures like attentive GNNs. We empirically show that our algorithms can converge to better results with faster rates and lower variances compared with state-of-the-art approaches. References [1] P. Auer, N. Cesa-Bianchi, Y . Freund, and R. E. Schapire. The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):48–77, 2002. [2] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [3] P. W. Battaglia, J. B. Hamrick, V . Bapst, A. Sanchez-Gonzalez, V . Zambaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018. [4] G. Burtini, J. Loeppky, and R. Lawrence. A survey of online experiment design with the stochastic multi-armed bandit. arXiv preprint arXiv:1510.00757, 2015. 9[5] J. Chen, J. Zhu, and L. Song. Stochastic training of graph convolutional networks with variance reduction. arXiv preprint arXiv:1710.10568, 2017. [6] J. Chen, T. Ma, and C. Xiao. Fastgcn: fast learning with graph convolutional networks via importance sampling. arXiv preprint arXiv:1801.10247, 2018. [7] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh. Cluster-gcn: An efﬁcient algorithm for training deep and large graph convolutional networks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 257–266, 2019. [8] H. Dai, B. Dai, and L. Song. Discriminative embeddings of latent variable models for structured data. In International conference on machine learning, pages 2702–2711, 2016. [9] A. Fout, J. Byrd, B. Shariat, and A. Ben-Hur. Protein interface prediction using graph convo- lutional networks. In Advances in Neural Information Processing Systems, pages 6530–6539, 2017. [10] R. Gandhi, S. Khuller, S. Parthasarathy, and A. Srinivasan. Dependent rounding and its applications to approximation algorithms. Journal of the ACM (JACM), 53(3):324–360, 2006. [11] W. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, pages 1024–1034, 2017. [12] B. Hu, Z. Zhang, C. Shi, J. Zhou, X. Li, and Y . Qi. Cash-out user detection based on attributed heterogeneous information network with a hierarchical attention mechanism. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 946–953, 2019. [13] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020. [14] W. Huang, T. Zhang, Y . Rong, and J. Huang. Adaptive sampling towards fast graph represen- tation learning. In Advances in Neural Information Processing Systems , pages 4558–4567, 2018. [15] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. [16] Z. Liu, C. Chen, X. Yang, J. Zhou, X. Li, and L. Song. Heterogeneous graph neural networks for malicious account detection. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management, pages 2077–2085. ACM, 2018. [17] Z. Liu, C. Chen, L. Li, J. Zhou, X. Li, L. Song, and Y . Qi. Geniepath: Graph neural networks with adaptive receptive paths. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 4424–4431, 2019. [18] F. Salehi, L. E. Celis, and P. Thiran. Stochastic optimization with bandit sampling. arXiv preprint arXiv:1708.02544, 2017. [19] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. Van Den Berg, I. Titov, and M. Welling. Modeling relational data with graph convolutional networks. In European Semantic Web Conference, pages 593–607. Springer, 2018. [20] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad. Collective classiﬁca- tion in network data. AI magazine, 29(3):93–93, 2008. [21] T. Uchiya, A. Nakamura, and M. Kudo. Algorithms for adversarial bandit problems with multiple plays. In International Conference on Algorithmic Learning Theory, pages 375–389. Springer, 2010. [22] P. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y . Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017. [23] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S. Yu. A comprehensive survey on graph neural networks. arXiv preprint arXiv:1901.00596, 2019. 10[24] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . Prasanna. Graphsaint: Graph sampling based inductive learning method. arXiv preprint arXiv:1907.04931, 2019. [25] D. Zou, Z. Hu, Y . Wang, S. Jiang, Y . Sun, and Q. Gu. Layer-dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, pages 11247–11256, 2019. 11A Algorithms Algorithm 2EXP3(qt i,wt i,rt i,St i). Require: η= 0.4, sample size k, neighbor size n= |Ni|, δ= √ (1 −η)η4k5 ln(n/k)/(Tn4). 1: Set ˆrij(t) =rij(t)/qij(t) if j ∈St i else 0 wij(t+ 1) =wij(t) exp(δˆrij(t)/n) 2: Set qij(t+ 1)←(1 −η) wij(t+1)∑ j∈Ni wij(t+1) + η n, for j ∈Ni Algorithm 3EXP3.M(qt i,wt i,rt i,St i) Require: η = 0.4, sample size k, neighbor size n = |Ni|, δ = √ (1 −η)η4k5 ln(n/k)/(Tn4), Ut i = ∅. 1: For j ∈Ni set ˆrij(t) = {rij(t)/qij(t) if j ∈St i 0 otherwise wij(t+ 1) = {wij(t) exp(δˆrij(t)/n) if j /∈Ut i wij(t) otherwise 2: if maxj∈Ni wij(t+ 1)≥( 1 k −η n) ∑ j∈Ni wij(t+ 1)/(1 −η) then 3: Decide at so as to satisfy at∑ wij(t+1)≥at at + ∑ wij(t+1)<at wij(t+ 1)= (1 k −η n)/(1 −η) 4: Set Ut+1 i = {j : wij(t+ 1)≥at} 5: else 6: Set Ut+1 i = ∅ 7: end if 8: Set w′ ij(t+ 1) = {wij(t+ 1) if j ∈Ni\\Ut+1 i at if j ∈Ut i 9: Set qij(t+ 1) =k ( (1 −η) w′ ij(t+1)∑ j∈Ni w′ ij(t+1) + η n ) for j ∈Ni Algorithm 4DepRound(k,(q1,q2,...,q K)) 1: Input: Sample size k(k<K ), sample distribution (q1,q2,...,q K) with ∑K i=1 qi = k 2: Output: Subset of [K] with kelements 3: while there is an iwith 0 <qi <1 do 4: Choose distinct iand jwith 0 <qi <1 and 0 <qj <1 5: Set β = min{1 −qi,qj}and γ = min{qi,1 −qj} 6: Update qi and qj as (qi,qj) = { (qi + β,qj −β) with probability γ β+γ (qi −γ,qj + γ) with probability β β+γ 7: end while 8: return {i: qi = 1,1 ≤i≤K} 12B Proofs Proposition 1. ˆµi = ∑ js∈Si αijs qijs hjs is the unbiased estimator of µi = ∑ j∈Ni αijhj given that Si is sampled from qi using the DepRound sampler Qi, where Si is the selected k-subset neighbors of vertex i. Proof. Let us denote Qi,Si as the probability of vertex vi choosing any k-element subset Si ⊂Ni from the K-element set Ni using DepRound sampler Qi. This sampler follows the alternative sampling distribution qi = (qij1 ,...,q ijK ) where qijs denotes the alternative probability of sampling neighbor vjs. This sampler is guaranteed to satisfy ∑ Si:j∈Si Qi,Si = qij, i.e. the sum over the probabilities of all subsets Si that contains element jequals the probability qij. E[ˆµi] =E  ∑ js∈Si αijs qijs hjs   (12) = ∑ Si⊂Ni Qi,Si ∑ js∈Si αijs qijs hjs (13) = ∑ j∈Ni ∑ Si:j∈Si Qi,Si αij qij hj (14) = ∑ j∈Ni αij qij hj ∑ Si:j∈Si Qi,Si (15) = ∑ j∈Ni αij qij hjqij (16) = ∑ j∈Ni αijhj (17) Proposition 2. The effective variance can be approximated by Ve(Qi) ≤∑ js∈Ni αijs qijs ∥hjs∥2. Proof. The variance is V(Qi) =E    ∑ js∈Si αijs qijs hjs − ∑ j∈Ni αijhj  2  = ∑ Si⊂Ni Qi,Si  ∑ js∈Si αijs qijs hjs  2 −  ∑ j∈Ni αijhj  2 . Therefore the effective variance has following upper bound: Ve(Qi) = ∑ Si⊂Ni Qi,Si  ∑ js∈Si αijs qijs hjs  2 ≤ ∑ Si⊂Ni Qi,Si ∑ js∈Si αijs  hjs qijs  2 (Jensen′sInequality ) = ∑ js∈Ni ∑ Si:js∈Si Qi,Siαijs  hjs qijs  2 = ∑ js∈Ni αijs q2 ijs ∥hjs∥2 ∑ Si:js∈Si Qi,Si = ∑ js∈Ni αijs qijs ∥hjs∥2 13Proposition 3. The negative derivative of the approximated effective variance ∑ js∈Ni αijs qijs ∥hjs∥2 w.r.tQi,Si, i.e. the reward of vi choosing Si at t, is ri,Si(t) =∑ js∈Si αijs qijs(t)2 ∥hjs(t)∥2. Proof. Deﬁne the upper bound as ˆVe(Qi) =∑ js∈Ni αijs qijs ∥hjs∥2, then its derivative is ∇Qi,Si ˆVe(Qi) =∇Qi,Si ∑ js∈Ni αijs qijs ∥hjs∥2 = ∇Qi,Si ∑ js∈Ni αijs∑ S′ i:js∈S′ i Qi,S′ i ∥hjs∥2 = ∇Qi,Si ∑ js∈Si αijs∑ S′ i:js∈S′ i Qi,S′ i ∥hjs∥2 = − ∑ js∈Si αjs q2 ijs ∥hjs∥2 (chainrule) Before we give the proof of Theorem 1, we ﬁrst prove the following Lemma 1 that will be used later. Lemma 1. For any real value constantη≤1 and any valid distributions Qt i and Q⋆ i we have (1 −2η)Vt e(Qt i) −(1 −η)Vt e(Q⋆ i) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩+ η⟨Q⋆ i,∇Qt i Vt e(Qt i)⟩ (18) Proof. The function Vt e(Q) is convex with respect to Q, hence for any Qt i and Q⋆ i we have Vt e(Qt i) −Vt e(Q⋆ i) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. (19) Multiplying both sides of this inequality by 1 −η, we have (1 −η)Vt e(Qt i) −(1 −η)Vt e(Q⋆ i) (20) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩−η⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩. (21) In the following, we prove this Lemma in our two bandit settings: adversary MAB setting and adversary MAB with multiple plays setting. In adversary MAB setting, we have ⟨Qt i,∇Qt i Vt e(Qt i)⟩= − ∑ j∈Ni qij(t) α2 ij k·qij(t)2 ∥hj(t)∥2 (22) = −Vt e(Qt i) (23) In adversary MAB with multiple plays setting , we use the approximated effective variance∑ js∈Ni αijs qijs ∥hjs∥2 derived in Proposition 2. For notational simplicity, we denote the approxi- mated effective variance as Ve in the following. We have ⟨Qt i,∇Qt i Vt e(Qt i)⟩= − ∑ Si⊂Ni Qt i,Si ∑ js∈Si αijs qijs(t)2 ∥hjs∥2 (24) = − ∑ js∈Ni αijs qijs(t)2 ∥hjs∥2 ∑ Si:js∈Si Qt i,Si (25) = − ∑ js∈Ni αijs qijs(t)∥hjs∥2 (26) = −Vt e(Qt i). (27) The equation (24) holds because of Proposition 3. 14At last, we conclude the proof (1 −η)Vt e(Qt i) −(1 −η)Vt e(Q⋆ i) (28) ≤⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩−η⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩ (29) = ⟨Qt i −Q⋆ i,∇Qt i Vt e(Qt i)⟩+ η⟨Q⋆ i,∇Qt i Vt e(Qt i)⟩+ ηVt e(Qt i). (30) Theorem 1. Using Algorithm 1 with η= 0.4 and δ = √ (1 −η)η4k5 ln(n/k)/(Tn4) to minimize effective variance with respect to {Qt i}1≤t≤T, we have T∑ t=1 Vt e(Qt i) ≤3 T∑ t=1 Vt e(Q⋆ i) + 10 √ Tn4 ln(n/k) k3 (31) where T ≥ln(n/k)n2(1 −η)/(kη2) and n= |Ni|. Proof. First we explain why condition T ≥ln(n/k)n2(1 −η)/(kη2) ensures that δˆrij(t) ≤1, δˆrij(t) = √ (1 −η)η4k5 ln(n/k) Tn4 ·αij(t) q3 ij(t) ∥hj(t)∥2 (32) ≤ √ (1 −η)η4k5 ln(n/k) Tn4 · n3 k3η3 (33) ≤1 (34) Assuming ∥hj(t)∥≤ 1, inequality (33) holds because αij(t) ≤1 and qij(t) ≥kη/n. Then replace T by the condition, we get δˆrij(t) ≤1. Let Wi(t), W′ i(t) denote ∑ j∈Ni wij(t), ∑ j∈Ni w′ ij(t) respectively. Then for any t= 1,2,...,T , Wi(t+ 1) Wi(t) = ∑ j∈Ni\\Ut i wij(t+ 1) Wi(t) + ∑ j∈Ut i wij(t+ 1) Wi(t) (35) = ∑ j∈Ni\\Ut i wij(t) Wi(t) ·exp(δˆrij(t)) + ∑ j∈Ut i wij(t) Wi(t) (36) ≤ ∑ j∈Ni\\Ut i wij(t) Wi(t) [ 1 +δˆrij(t) + (δˆrij(t))2] + ∑ j∈Ut i wij(t) Wi(t) (37) = 1 +W′ i (t) Wi(t) ∑ j∈Ni\\Ut i wij(t) W′ i (t) [ δˆrij(t) + (δˆrij(t))2] (38) = 1 +W′ i (t) Wi(t) ∑ j∈Ni\\Ut i qij(t)/k−η/n 1 −η [ δˆrij(t) + (δˆrij(t))2] (39) ≤1 + δ k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (40) Inequality (37) uses ea ≤1 +a+ a2 for a≤1. Equality (39) holds because of update equation of qij(t) deﬁned in EXP3.M. Inequality (40) holds because W′ i (t) Wi(t) ≤1. Since 1 +x≤ex for x≥0, we have ln Wi(t+ 1) Wi(t) ≤ δ k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (41) 15If we sum, for 1 ≤t≤T, we get the following telescopic sum ln Wi(T + 1) Wi(1) = T∑ t=1 ln Wi(t+ 1) Wi(t) (42) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (43) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)ˆr2 ij(t) (44) On the other hand, for any subset Scontaining k elements, ln Wi(T + 1) Wi(1) ≥ln ∑ j∈S wij(T + 1) Wi(1) (45) ≥ ∑ j∈S ln wij(T + 1) k −ln n k (46) ≥δ k ∑ j∈S ∑ t:j /∈Ut i ˆrij(t) −ln n k (47) The inequality (46) uses the fact that ∑ j∈S wij(T + 1)≥k( ∏ j∈S wij(T + 1))1/k The equation (47) uses the fact that wij(T + 1) = exp(δ ∑ t:j /∈Ut i ˆrij(t)) From (44) and (47), we get δ k ∑ j∈S ∑ t:j /∈Ut i ˆrij(t) −ln n k ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni\\Ut i qij(t)ˆr2 ij(t) (48) And we have the following inequality δ k ∑ j∈S ∑ t:j∈Ut i rij(t) = δ k ∑ j∈S ∑ t:j∈Ut i qij(t)ˆrij(t) (49) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ut i qij(t)ˆrij(t) (50) The equality (49) holds beacuse rij(t) =qijˆrij(t) when j ∈St i and Ut i ⊆St i bacause qt ij = 1for all j ∈Ut i. Then add inequality (50) in (48) we have δ k ∑ j∈S ∑ t:j∈Ut i rij(t) +δ k ∑ j∈S ∑ t:j /∈Ut i ˆrij(t) −ln n k (51) ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)ˆrij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)ˆr2 ij(t) (52) Given qij(t) we have E[ˆr2 ij(t)] =r2 ij(t)/qij(t), hence, taking expectation of (51) yields that δ k T∑ t=1 ∑ j∈S rij(t) −ln n k ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)rij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni r2 ij(t) (53) 16By multiplying (53) by Q⋆ i,S and summing over S, we get δ k T∑ t=1 ∑ S⊂Ni Q⋆ i,S ∑ j∈S rij(t) −ln n k ≤ δ k(1 −η) T∑ t=1 ∑ j∈Ni qij(t)rij(t) + δ2 k(1 −η) T∑ t=1 ∑ j∈Ni r2 ij(t) (54) As ∑ j∈Ni qij(t)rij(t) = ∑ j∈Ni ∑ Si:j∈Si Qt i,Sirij(t) (55) = ∑ Si⊂Ni Qt i,Si ∑ j∈Si rij(t) (56) = − ∑ Si⊂Ni Qt i,Si∇Qt i,Si Vt e(Qt i,Si) (57) = −⟨Qt i,∇Qt i Vt e(Qt i)⟩ (58) By plugging (58) in (54) and rearranging it, we ﬁnd T∑ t=1 ⟨Qt i −Q⋆ i ,∇Qt i Vt e(Qt i)⟩+ η T∑ t=1 ⟨Q⋆ i ,∇Qt i Vt e(Qt i)⟩ (59) ≤δ T∑ t=1 ∑ j∈Ni r2 ij(t) +(1 −η)k δ ln(n/k) Using Lemma 1, we have (1 −2η) T∑ t=1 Vt e(Qt i) −(1 −η) T∑ t=1 Vt e(Q⋆ i ) ≤δ T∑ t=1 ∑ j∈Ni r2 ij(t) +(1 −η)k δ ln(n/k) (60) Finally, we know that ∑ j∈Ni r2 ij(t) = ∑ j∈Ni αij(t)2 qij(t)4 (61) ≤ ∑ j∈Ni αij(t) n4 k4η4 (becauseqij(t) ≥kη/n) (62) = n4 k4η4 (63) By setting η= 0.4 and δ= √ (1 −η)η4k5 ln(n/k)/(Tn4), we get the upper bound. C Experiments C.1 Convergences We show the convergences on validation in terms of timing (seconds) in Figure 3 and Figure 4. Basically, our algorithms converge to much better results in nearly same duration compared with other “layer sampling” approaches. Note that we cannot complete the training of AS-GAT on Reddit because of memory issues. C.2 Discussions on Timings between Layer Sampling and Graph Sampling Paradigms Note that the comparisons of timing between “graph sampling” and “layer sampling” paradigms have been studied recently in [ 7, 24]. As a result, we do not compare the timing with “graph sampling” approaches. Under certain conditions, the graph sampling approaches should be faster than layer sampling approaches. That is, graph sampling approaches are designed for graph data that all vertices have labels. Under such condition, the ﬂoating point operations analyzed in [7] are 170.25 0.50 0.75 1.00 Time 0.2 0.4 0.6 0.8Micro F1 on validation Cora GCN-BS GraphSAGE FastGCN AS-GCN LADIES 5 10 15 20 Time 0.70 0.75 0.80 0.85 0.90Micro F1 on validation Pubmed GCN-BS GraphSAGE FastGCN AS-GCN LADIES 0 1000 2000 3000 Time 0.4 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GCN-BS GraphSAGE FastGCN AS-GCN LADIES 500 1000 1500 Time 0.6 0.7 0.8 0.9Micro F1 on validation Reddit GCN-BS GraphSAGE FastGCN AS-GCN LADIES 100 200 300 400 Time 0.40 0.45 0.50Micro F1 on validation Flickr GCN-BS GraphSAGE FastGCN AS-GCN LADIES Figure 3: The convergence in timing (seconds) on GCNs. 0.5 1.0 1.5 2.0 Time 0.2 0.4 0.6 0.8Micro F1 on validation Cora GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 0 10 20 30 Time 0.6 0.7 0.8 0.9Micro F1 on validation Pubmed GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 0 1000 2000 Time 0.5 0.6 0.7 0.8 0.9Micro F1 on validation PPI GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT 100 200 300 Time 0.44 0.46 0.48 0.50Micro F1 on validation Flickr GP-BS.M GP-BS GAT-BS.M GAT-BS AS-GAT Figure 4: The convergence in timing (seconds) on attentive GNNs. maximally utilized compared with the “layer sampling” paradigm. However, in practice, there are large amount of graph data with labels only on some types of vertices, such as the graphs in [ 16]. “Graph sampling” approaches are not applicable to cases where only partial vertices have labels. To summarize, the “layer sampling” approaches are more ﬂexible and general compared with “graph sampling” approaches in many cases. C.3 Results on OGB We report our results on OGB protein dataset [13]. We set the learning rate as 1e-3, batch size as 256, the dimension of hidden embeddings as 64, sample size as 10 and epochs as 200. We save the model based on the best results on validation and report results on testing data. We run the experiment 10 times with different random seeds to compute the average and standard deviation of the results. Our result of GP-BS on protein dataset performs the best4 until we submitted this paper. Please ﬁnd our implementations at https://github.com/xavierzw/ogb-geniepath-bs. Dateset Mean Std #experiments ogbn-proteins 0.78253 0.00352 10 4Please refer to https://ogb.stanford.edu/docs/leader_nodeprop/. 18",
      "meta_data": {
        "arxiv_id": "2006.05806v2",
        "authors": [
          "Ziqi Liu",
          "Zhengwei Wu",
          "Zhiqiang Zhang",
          "Jun Zhou",
          "Shuang Yang",
          "Le Song",
          "Yuan Qi"
        ],
        "published_date": "2020-06-10T12:48:37Z",
        "pdf_url": "https://arxiv.org/pdf/2006.05806v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the challenge of suboptimal sampling algorithms for training Graph Convolution Networks (GCNs) and their inapplicability to more general Graph Neural Networks (GNNs) like Graph Attention Networks (GAT). This is due to the intractable computation of optimal sampling distributions, which involve dynamically changing node embeddings and learned weights. The main contributions are: (1) Formulating the GNN sampling problem as an adversary bandit problem, making it applicable to both GCNs and attentive GNNs. (2) Providing theoretical guarantees that the proposed bandit sampler asymptotically approaches the optimal variance within a factor of 3. (3) Demonstrating empirical superiority in terms of convergence, faster training rates, and lower sampling variances compared to state-of-the-art approaches on multiple public datasets.",
        "methodology": "The core methodology involves reformulating the optimization of sampling variance as an adversary bandit problem, moving away from explicitly computing intractable optimal sampling distributions. Instead, the approach maintains nonparametric estimates of the sampler and iteratively updates it based on partial knowledge acquired from sampled neighbors. The 'reward' for each action (choosing a set of neighbors) is defined as the negative derivative of the sampling variance. Two bandit algorithms are proposed: (1) GNN-BS, based on the adversary Multi-Armed Bandit (MAB) setting, where a single neighbor is chosen k times and the sampler is updated by EXP3. (2) GNN-BS.M, based on MAB with multiple plays, which uses an efficient k-combination sampler (DepRound) to select a k-element subset of neighbors once, updating the sampler with EXP3.M. For attentive GNNs, where attention values (αij) change, adjusted feedback attention values (α'ij) are introduced using sampled unnormalized attentions and sampling probabilities.",
        "experimental_setup": "Experiments were conducted on five benchmark datasets: Cora, Pubmed, PPI, Reddit, and Flickr, following standard data splits. The evaluation involved GCN and attentive GNN architectures (GAT and GeniePath), with the number of layers fixed at 2 and varying hidden embedding dimensions (16 for Cora/Pubmed, 256 for PPI/Reddit/Flickr). Hyperparameters like learning rate, L2-norm regularizers, and dropout rate were optimized via grid search. Comparison algorithms included GraphSAGE, FastGCN, LADIES, AS-GCN, S-GCN, ClusterGCN, and GraphSAINT, along with their attentive GNN variants (AS-GAT, GraphSAINT-GAT). Specific sample sizes for node-wise and layer-wise samplers were configured per dataset. Performance was measured using Micro F1 scores, and convergence rates were analyzed in terms of epochs and training time. Sample size analysis was performed on the PPI dataset, comparing F1 scores and sampling variances. Additional results on the OGB protein dataset were reported for GP-BS.",
        "limitations": "Existing layer-wise sampling approaches have been empirically shown to perform worse than node-wise samplers. While the proposed derivation follows node-wise samplers, the extension to layer-wise approaches is left as future work. The rigorous application of the Multi-Armed Bandit (MAB) setting (GNN-BS) by repeating single-arm selections `k` times is acknowledged as 'not strictly rigorous' compared to the multiple-play setting (GNN-BS.M), although practical performance is similar. The theoretical proof relies on an assumption that node embedding norms (∥hj(t)∥) are less than or equal to 1. The paper also highlights that 'graph sampling' approaches are not applicable in scenarios where only partial vertices have labels, a limitation inherent to graph sampling methods rather than the proposed bandit samplers.",
        "future_research_directions": "The paper suggests extending the proposed bandit sampling framework to layer-wise sampling approaches for GNNs. Additionally, exploring other bandit settings beyond the adversarial bandit framework used in this paper is identified as a potential area for future research."
      }
    },
    {
      "title": "Graph Ladling: Shockingly Simple Parallel GNN Training without Intermediate Communication",
      "abstract": "Graphs are omnipresent and GNNs are a powerful family of neural networks for\nlearning over graphs. Despite their popularity, scaling GNNs either by\ndeepening or widening suffers from prevalent issues of unhealthy gradients,\nover-smoothening, information squashing, which often lead to sub-standard\nperformance. In this work, we are interested in exploring a principled way to\nscale GNNs capacity without deepening or widening, which can improve its\nperformance across multiple small and large graphs. Motivated by the recent\nintriguing phenomenon of model soups, which suggest that fine-tuned weights of\nmultiple large-language pre-trained models can be merged to a better minima, we\nargue to exploit the fundamentals of model soups to mitigate the aforementioned\nissues of memory bottleneck and trainability during GNNs scaling. More\nspecifically, we propose not to deepen or widen current GNNs, but instead\npresent a data-centric perspective of model soups tailored for GNNs, i.e., to\nbuild powerful GNNs. By dividing giant graph data, we build multiple\nindependently and parallelly trained weaker GNNs (soup ingredient) without any\nintermediate communication, and combine their strength using a greedy\ninterpolation soup procedure to achieve state-of-the-art performance. Compared\nto concurrent distributed GNN training works such as Jiong et. al. 2023, we\ntrain each soup ingredient by sampling different subgraphs per epoch and their\nrespective sub-models are merged only after being fully trained (rather than\nintermediately so). Moreover, we provide a wide variety of model soup\npreparation techniques by leveraging state-of-the-art graph sampling and graph\npartitioning approaches that can handle large graphs. Codes are available at:\n\\url{https://github.com/VITA-Group/graph_ladling}.",
      "full_text": "Graph Ladling: Shockingly Simple Parallel GNN Training without Intermediate Communication Ajay Jaiswal 1 Shiwei Liu 1 Tianlong Chen 1 Ying Ding 1 Zhangyang Wang1 Abstract Graphs are omnipresent and GNNs are a powerful family of neural networks for learning over graphs. Despite their popularity, scaling GNNs either by deepening or widening suffers from prevalent is- sues of unhealthy gradients, over-smoothening, information squashing, which often lead to sub- standard performance. In this work, we are in- terested in exploring a principled way to scale GNNs capacity without deepening or widening, which can improve its performance across mul- tiple small and large graphs. Motivated by the recent intriguing phenomenon of model soups, which suggest that fine-tuned weights of multiple large-language pre-trained models can be merged to a better minima, we argue to exploit the fun- damentals of model soups to mitigate the afore- mentioned issues of memory bottleneck and train- ability during GNNs scaling. More specifically, we propose not to deepen or widen current GNNs, but instead present a data-centric perspective of model soups tailored for GNNs, i.e., to build powerful GNNs. By dividing giant graph data, we build multiple independently and parallelly trained weaker GNNs (soup ingredient) without any intermediate communication, and combine their strength using a greedy interpolation soup procedure to achieve state-of-the-art performance. Compared to concurrent distributed GNN train- ing works such as (Zhu et al., 2023), we train each soup ingredient by sampling different sub- graphs per epoch and their respective sub-models are merged only after being fully trained (rather than intermediately so). Moreover, we provide a wide variety of model soup preparation techniques by leveraging state-of-the-art graph sampling and graph partitioning approaches that can handle large graphs. Extensive experiments across many *Equal contribution 1University of Texas at Austin. Correspon- dence to: Ajay Jaiswal <ajayjaiswal@utexas.edu>. Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s). real-world small and large graphs, illustrate the effectiveness of our approach and point towards a promising orthogonal direction for GNN scal- ing. Codes are available at: https://github. com/VITA-Group/graph_ladling. 1. Introduction Graphs represent a myriad of real-world data from social networks, knowledge graphs, gene expression networks, etc. Graph neural networks (GNNs) (Kipf & Welling, 2017; Def- ferrard et al., 2016; Veliˇckovi´c et al., 2017; You et al., 2020; Gao et al., 2018; Chiang et al., 2019; Zheng et al., 2021b; Chen et al., 2018; Duan et al., 2022; Thekumparampil et al., 2018), which use message passing (MP) strategy at their core for aggregating knowledge from neighbors, have been widely accepted as powerful algorithmic tools for learning over graphs. Although message passing provides GNNs superior performance over traditional MLPs, the nature of evolving massive topological structures prevents MP-based GNNs from scaling to industrial-grade graph applications, and the majority of state-of-the-art GNNs are only tested on small graph datasets. Additionally, due to the prevalent issues such as unhealthy gradients, over-smoothening and squashing (Li et al., 2018; NT & Maehara, 2019; Alon & Yahav, 2021; Jaiswal et al., 2022; Liu et al., 2021) while training GNNs, increasing model capacity either by deep- ening (stacking more layers) or widening (increasing neigh- borhood coverage) often lead to sub-standard performance. Previously, conforming to the empirical scaling laws (Ka- plan et al., 2020), where the final model quality has been found to have a power-law relationship with the amount of data, model size, and compute time; several works (Li et al., 2021; Jaiswal et al., 2022; Zhou et al., 2021b) have attempted to scale GNNs (up to 1000 layers) assuming that processing larger graphs would likely benefit from more parameters. Unlike conventional deep neural networks, exploiting scale to revamp information absorption is not straight-forward for GNNs, and numerous existing works rely on architectural changes, regularization & normaliza- tion, better initialization (Li et al., 2019; Chen et al., 2020; Li et al., 2018; Liu et al., 2020; Rong et al., 2020; Huang et al., 1 arXiv:2306.10466v2  [cs.LG]  24 Aug 2023Graph Ladling: Shockingly Simple Parallel GNN Training without Intermediate Communication 2020; Zhao & Akoglu, 2020; Zhou et al., 2021a; Jaiswal et al., 2022) for improving the trainability and try to over- come astonishingly high memory footprints by mini-batch training, i.e. sampling a smaller set of nodes or partitioning large graphs (Hamilton et al., 2017; Chen et al., 2018; Zou et al., 2019; Chiang et al., 2019; Zeng et al., 2019). While these methods are a step in the right direction, they do not scale well as the models become deeper or wider, since memory consumption is still dependent on the number of layers. We are interested in exploring an orthogonal step: Does there exist a principled way to scale GNNs capac- ity without deepening or widening, which can improve its performance across small and large graphs? Recently, for large pre-trained models with many appli- cations in computer vision (Han et al., 2020; Li et al., 2023; Mao et al., 2022; Jaiswal et al., 2021a; Zheng et al., 2021a) and natural language processing (Talmor et al., 2018; Jaiswal et al., 2021b; Zheng et al., 2023; Liu et al., 2023; Chen et al., 2023; Jaiswal et al., 2023), several works (Worts- man et al., 2022b; Ilharco et al., 2022; Juneja et al., 2022) investigate the intriguing phenomenon of “model soup”, and have shown that weights of multiple dense fine-tuned mod- els (candidate ingredients of soup) can be merged together into better generalizable solutions lying in low error basins. Despite enormous attention in NLP, it is unexplored for GNNs, presumably due to traditional wisdom that unlike large pre-trained transformers in NLP, current state-of-the- art GNN’s model capacity is under-parameterized apropos of gigantic graphs. Despite some recent works (W AN, 2022; Lin et al., 2022) illustrating the benefits of GNNs ensem- bling, they exhibit high computational cost during inference which worsens in the context of large graphs. Motivated by the mergability of multiple fine-tuned models illustrated by model soups, in this work, we raise the research question: Is it possible to leverage the fundamentals of model soups to handle the aforementioned issues of memory bottleneck and trainability, during scaling of GNNs? To this end, we propose not to deepen or widen current GNNs, but instead explore a data-centric perspective of di- viding ginormous graph data to build independently and parallelly trained multiple comparatively weaker models without any intermediate communication of model weights, and merge them together using greedy interpolation soup procedure to achieve state-of-the-art performance. Our work draws motivation from recent advancements in parallel train- ing of pretrained language models (LMs). For example, Branch-Merge-Train (BTM) (Li et al., 2022) learns a set of independent expert LMs specializing in different domains followed by averaging to generalize to multiple domains, and Lo-Fi (Wortsman et al., 2022a) illustrates the futility of communication overhead in data-parallel multi-node fine- tuning of LMs. Although these techniques seem to work for large pre-trained LMs, it is still unclear and unexplored if they will work for comparatively much smaller GNNs in the training-from-scratch regime. Moreover, GNNs deal with graph-structured relational data unlike independent samples in LMs and have their own unique set of challenges in their trainability, which makes it interesting to under- stand if soup phenomenon will help or deteriorate GNNs performance. To our surprise, we found that independently trained GNNs from scratch can be smoothly aggregated using our greedy soup interpolation procedure to create a better generalizable GNN that performs exceptionally well. It suggests that lin- ear scaling of GNNs either by deepening or widening is not necessarily the right approach towards building high-quality generalizable GNNs, and model soups can be an alternative. Note that, unlike the conventional model soup, we explore greedy weight interpolation for graph models, and in a well- motivated data-centric perspective (where it matters the most) considering the exploding size of real-world graphs. More specifically, in our work, we firstly illustrate easy adaptability of model soups across multiple SOTA GNN architectures trained on multiple small and large graphs (un- explored till now) and secondly present a novel data-centric perspective of model soups for large graph-structured re- lational data within constraints of available computational resource. Moreover, we extend current state-of-the-art graph sampling and partitioning techniques to facilitate the train- ing of candidate soup ingredients which can be seamlessly combined at end for better generalization. We also compare our recipe with distributed GNN training, e.g., concurrent work (Zhu et al., 2023), in Section 4.1. Our primary contributions can be summarized as: ■ We illustrate the harmonious adaptability of model soups for graph-structured data and experimentally val- idate its performance benefits across multiple GNN architectures and graph scales. Our experiments reveal orthogonal knowledge stored in the candidate mod- els which can be surprisingly aggregated during soup preparation using our greedy interpolation procedure. ■ We present a novel data-centric perspective of model soups tailored for GNNs and carefully study the ben- efits of independent and parallel training of candidate models and their mergability in scenarios without the luxury of having computation resources to process en- tire graph, by extending state-of-the-art (SOTA) graph sampling and partitioning algorithms. ■ Our extensive experiments across multiple large- scale and small-scale graphs {Cora, Citeseer, PubMed, Flickr, Reddit, OGBN-ArXiv, OGBN-products} using multiple GNN architec- tures {GCN, JKNet, DAGNN, APPNP, SGC, GraphSAGE, CluterGCN, GraphSAINT} 2Graph Ladling: Shockingly Simple Parallel GNN Training without Intermediate Communication validates the effectiveness of our approach. 2. Methodology 2.1. Preliminaries Graph Neural Networks (GNNs) exploit an iterative mes- sage passing (MP) scheme to capture the structural infor- mation within nodes’ neighborhoods. Let G = (V, E) be a simple and undirected graph with vertex set V, edge set E and .Let A ∈ RN×N be the associated adjacency matrix, such that Aij = 1 if (i, j) ∈ E, and Aij = 0 otherwise. N = |V| is the number of nodes. Let X ∈ RN×P0 be the node feature matrix, whose i-th row represents a P0- dimension feature vector for the i-th node. To facilitate understanding, a unified formulation of MP with K layers is presented as follows: X(K) = A(K−1)σ(A(K−2)σ(· · ·σ(A(0)X(0) W(0))) · ··)W(K−2))W(K−1) (1) where σ is an activation function (e.g. ReLU), A(l) is the adjacency matrix at l-th layer, and W(l) is the feature trans- formation weight matrix at l-th layer which will be learnt for the downstream tasks. 2 4 8 10 16 32 Layer Count 66 67 68 69 70 71 72 73T est AccuracyGCN GCNII GPRGNN 8 16 256 512 1024 2048 Neighbourhood sample Size 69.0 69.2 69.4 69.6 69.8 70.0 70.2 70.4 70.6Accuracy~51 iter/s ~46 iter/s ~19 iter/s ~17 iter/s ~12 iter/s ~11 iter/s Figure 1.(Left) Performance comparison of GCN, GCNII, and GPRGNN on OGBN-arxiv dataset with increasing layer count. (Right) Performance comparison of 2-layer GraphSAGE on OGBN-arxiv dataset with increasing neighbor sampling thresh- old. Results illustrate that deepening and widening the model capacity doesn’t necessarily help in improved performance. 2.2. GNNs and Issues with Model Explosion Despite the enormous success of GNNs in learning high- quality node representations, many state-of-the-art GNNs are shallow due to notorious challenges in their trainability, resulting in sub-standard performance. Recently, we have been observing a massive explosion in the topological struc- ture of real-world graph data, which raises demand to in- crease the model capacity to facilitate capturing long-range dependencies, and relaxing the widely-adopted restrictions to learn from limited neighbors subjected to resource con- straints due to the design issues of message passing strategy. Several works (Li et al., 2021; Jaiswal et al., 2022; Zhou et al., 2021b) have attempted the empirical scaling law from NLP, which suggests that over-parametrized models have better generalization capabilities, and proposed to similarly scale GNNs by stacking layers citing their increased abil- ity to capture long-term dependencies. However, scaling GNNs capacity either by deepening to capture long-range dependencies or by widening to increase neighborhood ag- gregation is inhibited by the prevalent issues of unhealthy gradients during back-propagation leading to poor trainabil- ity, over-smoothening causing feature collapse, and informa- tion squashing due to overload and distortion of messages being propagated from distant nodes. Figure 1 illustrates the effect of state-of-the-art GNNs scal- ing from the perspective of deepening and widening on OGBN-arxiv. It can be clearly observed that vanilla- GCN, GCNII and GPRGNN, all suffer significant perfor- mance from drop with increasing model capacity. Note that this substandard performance still exists in GCNII and GPRGNN, which are equipped with SOTA architectural modifications (eg. skip-connections), regularization, and normalization. Additionally, as shown in Figure 1(right), increasing the breadth of message passing to capture more and more neighborhood information for GraphSAGE on OGBN-arxiv does not necessarily help in improving the performance rather increases the memory footprints and reduce throughput (iterations/sec). To this end, we argue that model explosion is not necessarily the only solution towards building high-quality generaliz- able GNNs and propose an orthogonal direction inspired by the recent distributed and parallel training platform of model soups. We propose to build independently and parallelly trained multiple comparatively weaker models without any intermediate communication, and merge their knowledge to create a better generalizable GNN. In the next sections, we will explain how model soups can be easily adapted for many state-of-the-art GNNs, and how can we smoothly extend the current graph sampling and partitioning to create candidate ingredients for the model soup. Algorithm 1 Greedy Interpolation Soup Procedure Input: Soup Ingredients M = {M1, M2, ..., MK} Msorted ← SORTV alAcc(M) soup ← Msorted[0] for i = 1to K do for α in linspace(0, 1, step) do if ValAcc(interpolate(soup, Mi, α)) ≥ ValAcc(soup) then soup ← interpolate(soup, Mi, α) end if end for end for 3Graph Ladling: Shockingly Simple Parallel GNN Training without Intermediate Communication 2.3. Model Soups and current state-of-the-art GNNs In this section, we discuss about the harmonious adaptation of model soups for various state-of-the-art GNNs. Although model soups have been recently receiving ample attention for parallel training of large language pre-trained models, it is still unclear and unexplored if they will work for compar- atively much smaller GNNs trained from scratch. We for the first time observed that unlike using a pre-trained ini- tialization in LLMs, GNNs optimized independently from the same random initialization organically lie in the same basin of the error landscape, and they can be smoothly aggregated using linear interpolation across their weight parameters, with significant performance gain. More specifically, we propose to create K instances (soup ingredients) of the same GNN architecture of interest, shar- ing a common random initialization to ensure that the op- timization trajectory does not deviate significantly during training facilitating smooth mergability. Subject to resource availability, GNN instances are trained independently in isolation across multiple GPUs using the hyperparameter configuration set {h1, h2, ..., hK} designed with slight vari- ations in the learning rate, weight decay, seed values, etc. Once the soup ingredients are ready, we start preparing the soup using our greedy interpolation soup procedure as illustrated in Algorithm 1, which in general follows (Worts- man et al., 2022b). The greedy soup sequentially adds each model as a potential ingredient in the soup, and only keeps the model in the soup if it leads to improving performance on the validation set. For merging two ingredients, we perform a search for an optimal interpolation ratio α ∈ [0, 1] that helps in performance gain, else the ingredient is discarded. In comparison with several state-of-the-art GNNs, we ex- perimentally illustrate that GNN soups prepared using in- gredients having exactly the same model configuration, significantly perform better without any requirement of increasing model depth or width. For example, a 4- layer GCNII model soup prepared using 50 candidate in- gredients, beats GCNII with a similar configuration on PubMed by 1.12 ± 0.36. Our experiments across multi- ple GNN architectures and datasets ranging from small graphs to large graphs {Cora, Citeseer, PubMed, OGBN-ArXiv, OGBN-products} strongly validate the universal effectiveness of model soups for GNNs. 2.4. Data-Centric Model soups and Large Graph Training Paradigms In this section, we discuss our approach for preparing data- centric model soups in scenarios when we have resource constraints to perform message passing with the entire graph data in memory, by leveraging the SOTA graph sampling and partitioning mechanisms. As MP requires nodes ag- Algorithm 2 Model soup with Graph Sampling Input: ingredient count: Ic; Graph G, sampling ratio: s; gpu count: Gc; Hyperparameter Settings: H, Sampling Criterion: S(node, edge, layer) candidate queue ← {M1, M2, ..., MIc} soup ← None for i = 1to range(( Ic Gc ) + 1)do {M1, M2, ..., MGc} ←Dequeue Gc ingredients and distribute across available GPUs Train all distributed Mi in parallel with mini-batch sampling sample(G, s, S) and setting hi ∈ H soup ← greedy weight interpolation ({M1, M2, ..., MGc}∪ soup) end for Return soup gregating information from their neighbors, the integral graph structures are inevitably preserved during forward and backward propagation, thus occupying considerable running memory and time. Following Equation 1, the key bottleneck relies inA(l)X(l) for vanilla MP, requiring entire sparse adjacency matrix to be present in one GPU during training, which becomes challenging with growing topol- ogy of real-world graphs. Recently, numerous efforts with regards to graph sampling (Hamilton et al., 2017; Zou et al., 2019; Chen et al., 2018) and partitioning (Chiang et al., 2019; Zeng et al., 2019) have been proposed for approxi- mating the iterative full-batch MP to reduce the memory consumption for training within one single GPU to mitigate the aforementioned issue and scale up GNNs. Provided the formulation of Equation 1; graph sampling and partitioning paradigms pursue an optimal way to perform batch training, such that each batch will meet the memory constraint of a single GPU for message passing. We restate a unified illustration to elucidate the formulation of graph sampling and partitioning used for training our model soup ingredient Mi as follows: X(K) B0 [Mi] = ˜A (K−1) B1 σ( ˜A (K−2) B2 σ(···σ( ˜A (0) BK X(0) BK [Mi] W(0)[Mi])) · ··)W(K−2)[Mi])W(K−1)[Mi] (2) where ˜A (l) indicate the adjacency matrix for the l-th layer sampled from the full graph, Bl is the set of nodes sampled at l-th layer, Mi and W(l)[Mi] indicate the i-th candidate ingredient and weight associated with its l-th layer. This mini-batch training approach combined with a sampling and partitioning strategy significantly helps in alleviating time consumption and memory usage which grow expo- nentially with the GNN depth. It is worth noting that we have explored extension to scalable infrastructure topics like distributed training with multiple GPUs to alleviate the expenses of training ingredients for model soup. 4Graph Ladling: Shockingly Simple Parallel GNN Training without Intermediate Communication 2.4.1. M ODEL SOUP WITH GRAPH SAMPLING Given a large graphG = {V, E}, we explore three categories of widely-used sampling strategies: node-wise sampling, edge-wise sampling, and layer-wise sampling; to facilitate the training of candidates in our data-centric model soup. Node-wise Sampling : Node-wise sampling approaches propose to sample nodes from the sampling space N(v), which is 1-hop neighborhood of v as Bl+1 = S v∈Bl {x|x ∼ Q · PN(v)}, where Q denotes the number of samples, and P is the sampling distribution. In our work, we have used GraphSAGE node sampling whereP is the uniform distribu- tion. GraphSAGE sampling strategy fixes the conventional issue of “node explosion” by clipping the neighborhood sample count to Q for each node. Edge-wise Sampling : Edge-wise sampling proposes to sample edges from the sampling space E(vK) which is K-hop neighborhood of v as Bl+1 = S v∈Bl {x|x ∼ Q · PE(vk)}, where Bl+1 denotes the set of nodes induced due to edge-sampling and P is the uniform distribution. In our work, we have used uniform distribution for sampling a fixed amount of edges for each mini-batch training. Layer-wise Sampling : Bl+1 = {x|x ∼ Q · PN(Bl)}, where N(Bl) =S v∈Bl denotes the 1-hop neighbourhood of all the nodes in Bl In our work, following FastGCN, the sampling distribution P is designed using importance sampling where the probability for node u of being sampled is p(u) ∝ ||A(u, :)||2. Experimentally, we found that layer- wise induced adjacency matrix is usually sparser than the others, which accounts sub-optimal performance in practice. As shown in Algorithm 2, for our data-centric model soup, we first initialize our soup ingredients using a common ran- dom initialization. Depending upon the resource availability, we dequeue our ingredients across different GPUs, and train them in parallel with mini-batch sampling defined by a sam- pling strategy (node, edge, layer) and sampling ratio. Our model soup is prepared using greedy interpolation proce- dure (Algorithm 1) in an “available and added” fashion once the candidate ingredients are ready to be added to the soup. 2.4.2. M ODEL SOUP WITH GRAPH PARTITIONING Despite the success of sampling-based approaches, they still suffer from the issue of neighborhood explosion even with restricted node sampling when GNNs go deep leading to memory bottleneck. It has been observed that the efficiency of a mini-batch algorithm can be characterized by the notion of “embedding utilization”, which is proportional to the number of links between nodes in one batch or within-batch links (Chiang et al., 2019). Such findings motivated the design of batches using graph clustering algorithms that aim to construct partitions of nodes so that there are more graph links between nodes in the same partition than nodes in different partitions. More specifically, in graph partitioning, Algorithm 3 Model soup with Graph Partitioning Input: ingredient count: Ic; Graph G, sampling ratio: s; gpu count: Gc; Hyperparameter Settings: H, Epoch Count: E Partition the graph G into fixed K clusters V = {V1, V2, ..., VK} using METIS and save. candidate queue ← {M1, M2, ..., MIc} soup ← None for i = 1to (range(( Ic Gc ) + 1)do {M1, M2, ..., MGc} ←Dequeue Gc ingredients and distribute across available GPUs for For All distributed Mi in parallel do for iter in range(E) do Randomly choose q clusters {t1, t2, ..., tq} ∈ V Form a subgraph ˜G with nodes ˜V = {Vt1 , Vt2 , ..., Vtk } and edges A˜V,˜V Train Mi with subgraph ˜G setting hi ∈ H end for end for soup ← greedy weight interpolation ({M1, M2, ..., MGc}∪ soup) end for Return soup all the GNN layers share the same subgraph induced from the entire graph based on a partitioning strategy PG as BK = BK−1 = ... = B0 = {x|x ∼ Q · PG}, such that the sampled nodes are confined in the induced subgraph. In our work, we propose a novel and efficient strategy for graph partition-based model soup, building upon the Clus- terGCN and show that it can outperform ClusterGCN by > 1% on two large graphs OGBN-arxiv and Flickr. We adopted the multi-clustering framework proposed in ClusterGCN (METIS) to partition the graph G into fixed K clusters V = {V1, V2, ..., VK} which will be used to train each candidate ingredient of the model soup. We propose to save the cluster partition to ensure that there is no overhead of expensive graph partitioning again and again for improv- ing the efficiency while training model soup ingredients. Once the clusters are decided, each candidate ingredient chooses q (hyperparameter) vertex partitions independently and forms a subgraph that can be used for training. This partitioning strategy allows us to train comparatively deeper GNNs wrt. sampling approaches. Once the candidate in- gredients are trained using hyperparameter settings H, we prepare our model soup in an “available and added” strat- egy using greedy interpolation procedure (Algorithm 1) as shown in Algorithm 3. 5Graph Ladling: Shockingly Simple Parallel GNN Training without Intermediate Communication 0 5 10 15 20 25 30 Ingredient Number 94.0 94.5 95.0 95.5 96.0 96.5 97.0Test Accuracy Reddit 0 5 10 15 20 25 30 Ingredient Number 67 68 69 70 71 72 73Test Accuracy OGBN-ArXiv 0 5 10 15 20 25 30 Ingredient Number 76.0 76.5 77.0 77.5 78.0 78.5 79.0 79.5 80.0Test Accuracy OGBN-products Figure 2.Plot illustrating individual performance of each participating candidate ingredient of our data-partition soup. Red dashed line indicates the performance of the model soup generated using our greedy interpolation procedure. Table 1.Performance comparison of model soups generated using 50 candidate ingredients independently trained on the benchmarking datasets wrt. vanilla performance of several SOTA GNNs. Note that our ingredients have exactly the same architectural design as their vanilla counterpart. Results reported for vanilla GNNs are averaged across 30 independent runs. Dataset GCN GCNII JKNet DAGNN APPNP SGC VanillaOur SoupVanillaOur SoupVanillaOur SoupVanillaOur SoupVanillaOur SoupVanillaOur Soup Cora 82.39 83.52 82.19 82.77 79.06 79.89 83.39 83.45 83.64 83.70 79.31 80.22 Citeseer 71.36 72.01 72.97 73.56 66.98 68.01 73.05 73.76 72.13 73.15 72.22 73.53 PubMed 79.56 80.22 78.06 79.44 77.24 77.42 80.58 80.92 80.30 80.62 78.06 78.94 ArXiv 68.53 69.45 72.56 72.92 66.41 67.63 71.22 72.01 66.95 67.37 61.98 63.15 3. Experiments and Analysis 3.1. Dataset and Experimental Setup Our experiments are conducted on two GPU servers equipped with RTX A6000 and RTX 3090 GPUs. The hyper- parameters for soup ingredients corresponding to different datasets training are selected via our built-in efficient param- eter sweeping functionalities from pre-defined ranges nearby our optimal setting in Appendix B. For our small-scale ex- periments, we use three standard citation network datasets: Cora, Citeseer, and Pubmed, while our large-scale experiments are conducted with four popular large-scale open benchmarks: Flickr, Reddit, OGBN-ArXiv, and OGBN-products (Appendix A). For our evaluation on our chosen datasets, we have closely followed the data split settings and metrics reported by the recent benchmark (Duan et al., 2022; Chen et al., 2021). We consider variations in the key hyperparameters{random seed, batch size, learning rate, weight decay, and dropout rate} for generating the candidate ingredient models in our model soup. Unless explicitly spec- ified, we have used 50 candidate ingredients for small-scale graphs and 30 candidate ingredients for large-scale graphs experiments and our model soup is prepared using interpo- lation hyperparameter α ∈ [0 − 1] with a step size of 0.01 in Algorithm 1. For comparison with SOTA models, we have adopted the official implementation (Appendix C) of JKNet, DAGNN, APPNP, GCNII, SGC, ClusterGCN, and GraphSAINT. Note that while generating our model soup, we make sure to use exactly the same architectural design for our candidate ingredients to ensure a fair comparison. 3.2. Model Soups and SOTA GNNs In this section, we conduct a systematic and extensive study to understand the harmonious adaptation of model soups for state-of-the-art GNNs on popular graph benchmarks Cora, Citeseer, Pubmed, and OGBN-ArXiv. Note that al- though model soups have recently attracted significant atten- tion for large pre-trained language models, it is still unclear and unexplored if they can work for comparatively much smaller graph neural networks trained from scratch which learn from graph-structured data with relational properties unlike NLP and vision datasets having independent train- ing samples. Model soups provide an orthogonal way of increasing model capacity without deepening or widening GNNs which brings many unwanted trainability issues in GNNs. Table 1 illustrates the performance summarization of our model soup generated using 50 candidate ingredients independently trained on the benchmarking datasets with respect to the vanilla performance of several SOTA GNNs following the exact same architectural design (4 layers with 256 hidden dimension) to ensure a fair comparison. The results reported for vanilla GNNs within Table 1 are aver- aged across 30 independent runs using different seed values selected at random from [1 − 100] without replacement. We first observe that across all datasets and GNN archi- tecture, model soups outputs significantly outperform their vanilla counterpart. More noticeably, it improves GCN performance on Cora by 1.13%, GCNII performance on PubMed by 1.38%, JKNet, DAGNN, and SGC performance on OGBN-ArXiv by 1.22%,0.79%,1.17% respectively, and 6Graph Ladling: Shockingly Simple Parallel GNN Training without Intermediate Communication Table 2.Illustration of the current state of various fancy architectural and regularization modifications recently proposed to facilitate deepening of GCNs and help in improving their performance. Note that our model soup prepared by combining the strength of 50 candidate ingredients of 2-layer GCNs can significantly outperform all these fancy methods. Category Settings Cora Citeseer PubMed 2 16 32 2 16 32 2 16 32 Vanilla-GCN - 81.10 21.49 21.22 71.46 19.59 20.29 79.76 39.14 38.77 Skip Residual 74.73 20.05 19.57 66.83 20.77 20.90 75.27 38.84 38.74 Connection Initial 79.00 78.61 78.74 70.15 68.41 68.36 77.92 77.52 78.18 Jumping 80.98 76.04 75.57 69.33 58.38 55.03 77.83 75.62 75.36 Dense 77.86 69.61 67.26 66.18 49.33 41.48 72.53 69.91 62.99 Identity 82.98 67.23 40.57 68.25 56.39 35.28 79.09 79.55 73.74 Normalization BatchNorm 69.91 61.20 29.05 46.27 26.25 21.82 67.15 58.00 55.98 PairNorm 74.43 55.75 17.67 63.26 27.45 20.67 75.67 71.30 61.54 NodeNorm 79.87 21.46 21.48 68.96 18.81 19.03 78.14 40.92 40.93 CombNorm 80.00 55.64 21.44 68.59 18.90 18.53 78.11 40.93 40.90 Random Dropping DropNode 77.10 27.61 27.65 69.38 21.83 22.18 77.39 40.31 40.38 DropEdge 79.16 28.00 27.87 70.26 22.92 22.92 78.58 40.61 40.50 Our Model Soup (2-layer GCN)83.47±0.32 72.11±0.14 80.30±0.25 Table 3.Performance Comaprison of GradientGCN (Jaiswal et al., 2022) soup with 50 candidate ingredients on three WebKB datasets (Texas, Wisconsin, Cornell), and Actor dataset. Texas Wisconsin Cornell Actors GCN 60.12±4.22 52.94±3.99 54.05±7.11 25.46±1.43 SGC 56.41±4.25 51.29±6.44 58.57±3.44 26.17±1.15 GCNII 64.28±2.93 59.19±9.07 58.51±1.66 30.95±1.04 JKNet 61.08±6.23 52.76±5.69 57.30±4.95 28.80±0.97 APPNP 60.68±4.50 54.24±5.94 58.43±3.74 28.65±1.28 GradientGCN69.19±6.56 70.31±4.75 74.16±6.48 34.28±1.12 Ours 70.45±2.77 72.01±3.89 75.90±4.76 34.69±0.51 APPNP performance on Citeseer by 1.02%. Moreover, Ta- ble 2 illustrates the current state of various fancy architec- tural and regularization modifications recently proposed to facilitate deepening of GCNs and help in improving their performance. It can be clearly observed that our model soup prepared by combining the strength of 50 candidate ingredi- ents of 2-layer GCNs can significantly outperform all these fancy methods, bolstering our claim that model explosion by deepening and widening is necessarily not the only and right direction for building high-quality generalizable GNNs. 3.3. Data-Centric Model Soup with Graph Sampling and Graph Partitioning In this section, we provide experimental results for prepar- ing data-centric model soup in scenarios when we do not have the luxury of resources to perform message passing on the entire graph, by leveraging the SOTA graph sampling (node, edge, layer) and partitioning mechanisms to prepare the candidate ingredients of the soup. Table 4 illustrates the performance comparison of state-of-the-art graph sam- pling approaches GraphSAGE, FastGCN, and LADIES with respect to our node, edge, and layer-wise sampled soup pre- pared using Algorithm 2. Results of GraphSAGE, FastGCN, and LADIES are reported as mean across 30 independent runs while our soup results are reported as the mean of 5 independent runs. We additionally report the performance of graph ensemble prepared using the 30 independent runs of best-performing baseline (GraphSAGE). It can be clearly observed that our Node sampled soup performs best and comfortably outperforms all the baselines along with the graph ensemble which has a hefty inference cost, by signifi- cant margins. Similar to the sub-standard performance of layer-wise sampling approaches FastGCN and LADIES, our Layer sampled soup has comparatively low (although better than FastGCN and LADIES) performance, possibly because layer-wise induced adjacency matrix is usually sparser than the others, which accounts for its sub-optimal performance. In Table 6, we attempted to analyze the activation memory usage of activations and the hardware throughput (higher is better) of GraphSAGE (neighborhood sample size of 40) with respect to our data-centric model soup using node sam- pling equivalent to one-fouth of GraphSAGE (i.e., neigh- borhood sample size of 10) on OGBN-products. We found that with comparatively less memory requirement, our ap- proach can ∼ 1% better performance, eliciting the high potential in improving GNNs performance by combining the strength of multiple weak models. Next, Table 5 illus- trates the performance comparison of our Graph Partition Soup with respect to two well-known graph partitioning GNNs: ClusterGCN and GraphSAINT. Results of Clus- terGCN and GraphSAINT are reported as mean across 30 independent runs while our soup results are reported as mean of 5 independent runs each generated using 30 candidate ingredients using Algorithm 3. We also present the perfor- mance of graph ensemble prepared using the 30 independent runs of the best-performing baseline (ClusterGCN). Across all benchmarking datasets (Flicket, Reddit, OGBN-ArXiv, and OGBN-products), our data partition soup outperforms both GraphSAINT and ClusterGCN. More noticeably, our method beats ClusterGCN with a similar architecture design by > 1% margin on Flickr and OGBN-ArXiv dataset. In addition, Figure 2 illustrates the individual performance of each participating candidate ingredient of our data-partition 7Graph Ladling: Shockingly Simple Parallel GNN Training without Intermediate Communication Table 4.Performance comparison of state-of-the-art graph sampling approaches GraphSAGE, FastGCN, and LADIES wrt. our node, edge, and layer-wise sampled soup prepared using Algorithm 3. Results of GraphSAGE, FastGCN, and LADIES are reported as mean across 30 independent runs while our soup results are reported as mean of 5 independent runs, where each run uses 50 candidate ingredients. Method Flickr Reddit OGBN-ArXiv OGBN-products N=89,250 E=899,756 N=232,965 E=11,606,919 N=169,343 E=1,166,243 N= 2,449,029 E=61,859,140 GraphSAGE 53.63 ±0.13% 96.50 ±0.03% 71.55 ±0.41% 80.61 ±0.16% FastGCN 49.89 ±0.62% 79.50 ±1.22% 66.10 ±1.06% 73.46 ±0.20% LADIES 50.04 ±0.39% 86.96 ±0.37% 62.78 ±0.89% 75.31 ±0.56% GraphSAGE Ensemble 53.71 96.61 71.58 80.85 Node Sampled Soup 54.47±0.13 % 97.28±0.08 % 72.83±0.21 % 81.34±0.28 % Edge Sampled Soup 52.91±0.56 % 92.66±0.34 % 70.45±0.29 % 75.58±0.45 % Layer Sampled Soup 51.08±0.22 % 86.01±0.17 % 68.30±0.54 % 74.95±0.38 % Table 5.Performance comparison of our Graph Partition Soup with respect to two well-known graph partitioning GNNs: ClusterGCN and GraphSAINT. Results of ClusterGCN and GraphSAINT are reported as mean across 30 independent runs while our soup results are reported as mean of 5 independent runs, where each run uses 30 candidate ingredients. Method Flickr Reddit OGBN-ArXiv OGBN-products N=89,250 E=899,756 N=232,965 E=11,606,919 N=169,343 E=1,166,243 N= 2,449,029 E=61,859,140 ClusterGCN 51.20 ±0.13% 95.68 ±0.03% 71.29 ±0.44% 78.62 ±0.61% GraphSAINT 51.81 ±0.17% 95.62 ±0.05% 70.55 ±0.26% 75.36 ±0.34% ClusterGCN Ensemble 51.49 95.70 71.43 78.74 Our Data Partition Soup52.23±0.12 % 96.41±0.08 % 72.35±0.19 % 79.34±0.28 % Table 6.The memory usage of activations and the hardware throughput (higher is better) of GraphSAGE with respect to our data-centric model soup using node sampling equivalent to one- fouth of GraphSAGE on OGBN-products. Act. Memory(MB) Throughput(iter/sec) Accuracy GraphSAGE 415.94 37.69 79.5 ±0.36 Ours 369.51 44.30 80.42±0.41 Table 7.Performance comparison of our data partition soup on OGBN-ArXiv dataset, with varying candidate ingredient counts. Ingredient Count10 20 30 50 100 Performance 71.426 71.691 72.353 72.388 72.814 soup, and it can be clearly observed that there is orthogo- nal knowledge stored in the learned weights of these net- works. This gets elucidated by merging candidates and thereby improving overall performance which demonstrates the strength of our data-centric model soups. 3.4. Effect of Ingredient Count on Data-Centric Soup In this section, we try to understand the strength of increas- ing ingredient count to our final data-centric model soup performance. Table 7 illustrates the performance compar- ison of our data partition soup on OGBN-ArXiv dataset, with varying candidate ingredient counts. It can be clearly observed that increasing candidates generally lead to better performance, thanks to the greedy interpolation procedure. However, due to the increase in computational and time cost, we restrict the number of ingredients to 30 for all experi- ments related to large graphs, and 50 for small graphs. 3.5. Does intermediate communication benefit soup? In this section, we attempt to answer another abaltion ques- tion: How does intermediate communication across candi- date ingredients (i.e. souping at intervals during training) benefit the performance of the final model soup? To this end, we prepared a data partition model soup of GCNs using OGBN-ArXiv dataset, where we executed souping across candidate ingredients at regular intervals of 100 epochs during training. To our surprise, we found that the fi- nal soup performance is −0.745% less compared to our communication-free approach presumably due to a lack of diversity and orthogonal knowledge across the candidate ingredients. Moreover, intermediate communication incurs additional soup preparation overhead along with a new com- munication interval hyperparameter for optimization. 4. Background Work Model soups (Wortsman et al., 2022b) proposed a greedy mechanism for averaging weights of multiple fine-tuned large language models with varying hyperparameters uni- formly. They found that averaging many fine-tuned vision models improve out-of-domain generalization. Recently, (Li et al., 2022) introduced branch-train-merge which is at the intersection of model combination and distributed training. They consider the case where the training data is partitioned into different textual domains, then train an individual ex- pert model for each domain. Merging all of these experts 8Graph Ladling: Shockingly Simple Parallel GNN Training without Intermediate Communication via weight averaging or ensembling to outperform the dense baseline of training one large model on all of the data. Lo-fi (Wortsman et al., 2022a) proposes splitting up a large lan- guage model fine-tuning job into multiple smaller jobs, and dedicating its fine-tuning across multiple nodes in isolation. Unlike standard data-parallel multi-node finetuning where gradients between nodes are communicated at each step, Lo-fi removes all communication between nodes during fine-tuning. (Jin et al., 2022) propose a dataless knowledge fusion method and study the problem of merging individual models built on different training data sets to obtain a single model that performs well both across all data set domains and can generalize on out-of-domain data. Despite many recent advances in model parameter merg- ing, to the surprise, it has not been explored for GNNs, where it can benefit the most, given the high demand for distributed and parallel training for graph data. Our work differs from recent works in model soups as: firstly, unlike over-parameterized large language models, we explore com- paratively under-parameterized GNNs; secondly, we work with graph-structured relational data instead of indepen- dent training samples in NLP or vision; thirdly, we explore model merging on GNNs trained from scratch rather in the fine-tuning settings. GNNs have their own set of unique training challenges and it was unexplored if the model soup mechanism will be beneficial or hurt the performance. 4.1. Comparison to Related Work and Concurrent Ideas in Distributed GNN Training Our algorithm is mainly inspired by a GNN ensembling perspective, and its aim is to optimize model accuracy with or without distributed data parallelism, by interpolating in- dividually trained GNN model weights. Meanwhile, several recent or concurrent works have contributed to improving speed and accuracy for distributed data-parallel GNN train- ing with efficient model averaging or randomized partitions. (Ramezani et al., 2021) proposed a communication-efficient distributed GNN training technique (LLCG), which first trains a GNN on local machine data by ignoring the de- pendency between nodes among different machines, then sends the locally trained model to the server for periodic model averaging. However, LLCG heavily relies on the global Server Corrections module on the server to refine the locally learned models. CoFree-GNN (Cao et al., 2023) presents another communication-free GNN training frame- work, that utilizes a Vertex Cut partitioning, i.e., rather than partitioning the graph by cutting the edges between parti- tions, the Vertex Cut partitions the edges and duplicates the node information to preserve the graph structure. One concurrent and independent work by (Zhu et al., 2023) proposed a distributed training framework that assembles independent trainers, each of which asynchronously learns a local model on locally-available parts of the training graph. In this way, they only conducted periodic (time- based) model aggregation to synchronize the local models. Note that, unlike their periodic weight averaging of the par- ticipating trainers, our work performs averaging only after the complete training of candidates. Meanwhile, our candi- dates are trained by sampling different graph clusters from the input graph every epoch, to facilitate the diversity de- mand for model soups from the input graph. In comparison, each candidate model in (Zhu et al., 2023) only trains on lo- calized subgraph assigned by randomized node/super-node partitions, without having access to entire input graph. 5. Conclusion In this work, we explore a principled way to scale GNN capacity without deepening or widening which can improve its performance across multiple small and large graph. We present a data-centric perspective of model soups to build powerful GNNs by dividing giant graph data to build in- dependently and parallelly trained multiple comparatively weaker GNNs without any intermediate communication, and combining their strength using a greedy interpola- tion soup procedure to achieve state-of-the-art performance. Moreover, we provide a wide variety of model soup prepa- ration techniques by leveraging SOTA graph sampling and graph partitioning approaches. Our future work will aim to develop a theoretical framework to explain the benefits of data-centric GNN soups. Acknowledgement Z. Wang is in part supported by US Army Research Office Young Investigator Award W911NF2010240 and the NSF AI Institute for Foundations of Machine Learning (IFML). We also appreciate the helpful discussions with Jiong Zhu. References Mage: Automatic diagnosis of autism spectrum disorders using multi-atlas graph convolutional networks and en- semble learning. Neurocomputing, 469:346–353, 2022. ISSN 0925-2312. Alon, U. and Yahav, E. On the bottleneck of graph neural networks and its practical implications. ArXiv, abs/2006.05205, 2021. Cao, K., Deng, R., Wu, S., Huang, E. W., Subbian, K., and Leskovec, J. Communication-free distributed gnn training with vertex cut. arXiv preprint arXiv:2308.03209, 2023. Chen, J., Ma, T., and Xiao, C. Fastgcn: fast learning with graph convolutional networks via importance sampling. arXiv preprint arXiv:1801.10247, 2018. 9Graph Ladling: Shockingly Simple Parallel GNN Training without Intermediate Communication Chen, M., Wei, Z., Huang, Z., Ding, B., and Li, Y . Sim- ple and deep graph convolutional networks. ArXiv, abs/2007.02133, 2020. Chen, T., Zhou, K., Duan, K., Zheng, W., Wang, P., Hu, X., and Wang, Z. Bag of tricks for training deeper graph neu- ral networks: A comprehensive benchmark study. arXiv preprint arXiv:2108.10521, 2021. Chen, T., Zhang, Z., JAISW AL, A. K., Liu, S., and Wang, Z. Sparse moe as the new dropout: Scaling dense and self- slimmable transformers. In The Eleventh International Conference on Learning Representations, 2023. Chiang, W.-L., Liu, X., Si, S., Li, Y ., Bengio, S., and Hsieh, C.-J. Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks. In Proceed- ings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 257–266, 2019. Defferrard, M., Bresson, X., and Vandergheynst, P. Con- volutional neural networks on graphs with fast localized spectral filtering. Advances in neural information pro- cessing systems, 29, 2016. Duan, K., Liu, Z., Wang, P., Zheng, W., Zhou, K., Chen, T., Hu, X., and Wang, Z. A comprehensive study on large-scale graph training: Benchmarking and rethink- ing. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track , 2022. URL https://openreview.net/forum? id=2QrFr_U782Z. Gao, H., Wang, Z., and Ji, S. Large-scale learnable graph convolutional networks. Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Dis- covery & Data Mining, 2018. Hamilton, W., Ying, Z., and Leskovec, J. Inductive repre- sentation learning on large graphs. Advances in neural information processing systems, 30, 2017. Han, K., Wang, Y ., Chen, H., Chen, X., Guo, J., Liu, Z., Tang, Y ., Xiao, A., Xu, C., Xu, Y ., Yang, Z., Zhang, Y ., and Tao, D. A survey on visual transformer. ArXiv, abs/2012.12556, 2020. Huang, W., Rong, Y ., Xu, T., Sun, F., and Huang, J. Tackling over-smoothing for general graph convolutional networks. ArXiv, abs/2008.09864, 2020. Ilharco, G., Wortsman, M., Gadre, S. Y ., Song, S., Hajishirzi, H., Kornblith, S., Farhadi, A., and Schmidt, L. Patching open-vocabulary models by interpolating weights. arXiv preprint arXiv:2208.05592, 2022. Jaiswal, A., Li, T., Zander, C., Han, Y ., Rousseau, J. F., Peng, Y ., and Ding, Y . Scalp-supervised contrastive learning for cardiopulmonary disease classification and localization in chest x-rays using patient metadata. In 2021 IEEE International Conference on Data Mining (ICDM) , pp. 1132–1137. IEEE, 2021a. Jaiswal, A., Tang, L., Ghosh, M., Rousseau, J., Peng, Y ., and Ding, Y . Radbert-cl: Factually-aware contrastive learning for radiology report classification. Proceedings of machine learning research, 158:196–208, 2021b. Jaiswal, A., Wang, P., Chen, T., Rousseau, J. F., Ding, Y ., and Wang, Z. Old can be gold: Better gradient flow can make vanilla-gcns great again. arXiv preprint arXiv:2210.08122, 2022. Jaiswal, A., Chen, T., Rousseau, J. F., Peng, Y ., Ding, Y ., and Wang, Z. Attend who is weak: Pruning-assisted medical image localization under sophisticated and im- plicit imbalances. In Proceedings of the IEEE/CVF Win- ter Conference on Applications of Computer Vision, pp. 4987–4996, 2023. Jin, X., Ren, X., Preotiuc-Pietro, D., and Cheng, P. Data- less knowledge fusion by merging weights of language models. arXiv preprint arXiv:2212.09849, 2022. Juneja, J., Bansal, R., Cho, K., Sedoc, J., and Saphra, N. Lin- ear connectivity reveals generalization strategies. arXiv preprint arXiv:2205.12411, 2022. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Kipf, T. and Welling, M. Semi-supervised classification with graph convolutional networks. ArXiv, abs/1609.02907, 2017. Klicpera, J., Bojchevski, A., and G ¨unnemann, S. Predict then propagate: Graph neural networks meet personalized pagerank. In ICLR, 2019. Li, G., M¨uller, M., Thabet, A. K., and Ghanem, B. Can gcns go as deep as cnns? ArXiv, abs/1904.03751, 2019. Li, G., M ¨uller, M., Ghanem, B., and Koltun, V . Training graph neural networks with 1000 layers. In International conference on machine learning, pp. 6437–6449. PMLR, 2021. Li, M., Gururangan, S., Dettmers, T., Lewis, M., Althoff, T., Smith, N. A., and Zettlemoyer, L. Branch-train-merge: Embarrassingly parallel training of expert language mod- els. ArXiv, abs/2208.03306, 2022. 10Graph Ladling: Shockingly Simple Parallel GNN Training without Intermediate Communication Li, Q., Han, Z., and Wu, X.-M. Deeper insights into graph convolutional networks for semi-supervised learning. In Thirty-Second AAAI conference on artificial intelligence, 2018. Li, T., Shetty, S., Kamath, A., Jaiswal, A., Jiang, X., Ding, Y ., and Kim, Y . Cancergpt: Few-shot drug pair synergy prediction using large pre-trained language models.arXiv preprint arXiv:2304.10946, 2023. Lin, Q., Yu, S., Sun, K., Zhao, W., Alfarraj, O., Tolba, A., and Xia, F. Robust graph neural networks via ensemble learning. Mathematics, 10(8), 2022. doi: 10.3390/math10081300. Liu, H., Yang, Y ., and Wang, X. Overcoming catastrophic forgetting in graph neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pp. 8653–8661, 2021. Liu, M., Gao, H., and Ji, S. Towards deeper graph neural networks. Proceedings of the 26th ACM SIGKDD Inter- national Conference on Knowledge Discovery & Data Mining, 2020. Liu, S., Chen, T., Zhang, Z., Chen, X., Huang, T., Jaiswal, A., and Wang, Z. Sparsity may cry: Let us fail (cur- rent) sparse neural networks together! arXiv preprint arXiv:2303.02141, 2023. Mao, Z., Jaiswal, A., Wang, Z., and Chan, S. H. Sin- gle frame atmospheric turbulence mitigation: A bench- mark study and a new physics-inspired transformer model. ArXiv, abs/2207.10040, 2022. NT, H. and Maehara, T. Revisiting graph neural networks: All we have is low-pass filters. ArXiv, abs/1905.09550, 2019. Ramezani, M., Cong, W., Mahdavi, M., Kandemir, M. T., and Sivasubramaniam, A. Learn locally, correct glob- ally: A distributed algorithm for training graph neural networks. arXiv preprint arXiv:2111.08202, 2021. Rong, Y ., Huang, W., Xu, T., and Huang, J. Dropedge: Towards deep graph convolutional networks on node clas- sification. In ICLR, 2020. Talmor, A., Herzig, J., Lourie, N., and Berant, J. Common- senseqa: A question answering challenge targeting com- monsense knowledge. arXiv preprint arXiv:1811.00937, 2018. Thekumparampil, K. K., Wang, C., Oh, S., and Li, L.- J. Attention-based graph neural network for semi- supervised learning. arXiv preprint arXiv:1803.03735, 2018. Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., and Bengio, Y . Graph attention networks.arXiv preprint arXiv:1710.10903, 2017. Wortsman, M., Gururangan, S., Li, S., Farhadi, A., Schmidt, L., Rabbat, M., and Morcos, A. S. lo-fi: distributed fine-tuning without communication. arXiv preprint arXiv:2210.11948, 2022a. Wortsman, M., Ilharco, G., Gadre, S. Y ., Roelofs, R., Gontijo-Lopes, R., Morcos, A. S., Namkoong, H., Farhadi, A., Carmon, Y ., Kornblith, S., et al. Model soups: averaging weights of multiple fine-tuned mod- els improves accuracy without increasing inference time. In International Conference on Machine Learning , pp. 23965–23998. PMLR, 2022b. Wu, F., Souza, A., Zhang, T., Fifty, C., Yu, T., and Wein- berger, K. Simplifying graph convolutional networks. In International conference on machine learning, pp. 6861– 6871. PMLR, 2019. Xu, K., Li, C., Tian, Y ., Sonobe, T., Kawarabayashi, K.- i., and Jegelka, S. Representation learning on graphs with jumping knowledge networks. In International Con- ference on Machine Learning , pp. 5453–5462. PMLR, 2018. Yang, L., Luo, L., Xin, L., Zhang, X., and Zhang, X. Dagnn: Demand-aware graph neural networks for session-based recommendation. arXiv preprint arXiv:2105.14428 , 2021. You, Y ., Chen, T., Wang, Z., and Shen, Y . L2-gcn: Layer- wise and learned efficient training of graph convolutional networks. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2124–2132, 2020. Zeng, H., Zhou, H., Srivastava, A., Kannan, R., and Prasanna, V . Graphsaint: Graph sampling based inductive learning method. arXiv preprint arXiv:1907.04931, 2019. Zhao, L. and Akoglu, L. Pairnorm: Tackling oversmoothing in gnns. ArXiv, abs/1909.12223, 2020. Zheng, M., Gao, P., Zhang, R., Wang, X., Li, H., and Dong, H. End-to-end object detection with adaptive clustering transformer. ArXiv, abs/2011.09315, 2021a. Zheng, W., Huang, E. W., Rao, N., Katariya, S., Wang, Z., and Subbian, K. Cold brew: Distilling graph node rep- resentations with incomplete or missing neighborhoods. arXiv preprint arXiv:2111.04840, 2021b. Zheng, W., Sharan, S., Jaiswal, A. K., Wang, K., Xi, Y ., Xu, D., and Wang, Z. Outline, then details: Syntactically guided coarse-to-fine code generation. arXiv preprint arXiv:2305.00909, 2023. 11Graph Ladling: Shockingly Simple Parallel GNN Training without Intermediate Communication Zhou, K., Dong, Y ., Wang, K., Lee, W. S., Hooi, B., Xu, H., and Feng, J. Understanding and resolving performance degradation in deep graph convolutional networks. Pro- ceedings of the 30th ACM International Conference on Information & Knowledge Management, 2021a. Zhou, K., Huang, X., Zha, D., Chen, R., Li, L., Choi, S.-H., and Hu, X. Dirichlet energy constrained learning for deep graph neural networks. Advances in Neural Information Processing Systems, 34:21834–21846, 2021b. Zhu, J., Reganti, A., Huang, E., Dickens, C., Rao, N., Sub- bian, K., and Koutra, D. Simplifying distributed neu- ral network training on massive graphs: Randomized partitions improve model aggregation. arXiv preprint arXiv:2305.09887, 2023. Zou, D., Hu, Z., Wang, Y ., Jiang, S., Sun, Y ., and Gu, Q. Layer-dependent importance sampling for training deep and large graph convolutional networks. In NeurIPS, 2019. 12Graph Ladling: Shockingly Simple Parallel GNN Training without Intermediate Communication A. Dataset Details Table 10 provided provides the detailed properties and download links for all adopted datasets. We adopt the following benchmark datasets since i) they are widely applied to develop and evaluate GNN models, especially for deep GNNs studied in this paper; ii) they contain diverse graphs from small-scale to large-scale or from homogeneous to heterogeneous; iii) they are collected from different applications including citation network, social network, etc. Table 8.Graph datasets statistics and download links. Dataset Nodes Edges Classes Download Links Cora 2,708 5,429 7 https://github.com/kimiyoung/planetoid/raw/master/data Citeseer 3,327 4,732 6 https://github.com/kimiyoung/planetoid/raw/master/data PubMed 19,717 44,338 3 https://github.com/kimiyoung/planetoid/raw/master/data OGBN-ArXiv 169,343 1,166,243 40 https://ogb.stanford.edu/ Flickr 89,250 899,756 7 PyTorchGeometic:https://arxiv.org/abs/1907.04931 Reddit 232,965 11,606,919 41 PyTorchGeometic:https://arxiv.org/abs/1706.02216 OGBN-products 2,449,029 61,859,140 47 https://ogb.stanford.edu/ B. Experimental setting of our large-scale datasets Table 9.The searched optimal hyperparameters for all tested methods for data-centric soup. Method Flickr Reddit OGBN-products Split: 0.50/0.25/0.25 Split: 0.66 / 0.10 / 0.24 Split: 0.10 / 0.02 / 0.88 GraphSAGE(Hamilton et al., 2017) LR: 0.0001, WD: 0.0001, DP: 0.5, LR: 0.0001, WD: 0.0 DP: 0.2, LR: 0.001, WD: 0.0 DP: 0.5, EP: 50, HD: 512, #L: 4, BS: 1000 EP: 50, HD: 512, #L: 4, BS: 1000 EP: 50, HD: 512, #L: 4, BS: 1000 FastGCN(Chen et al., 2018) LR: 0.001, WD: 0.0002, DP: 0.1 LR: 0.01, WD: 0.0 DP: 0.5, LR: 0.01, WD: 0.0 DP: 0.2 EP: 50, HD: 512, #L: 2, BS: 5000 EP: 50, HD: 256, #L: 2, BS: 5000 EP: 50, HD: 256, #L: 2, BS: 5000 LADIES(Zou et al., 2019) LR: 0.001, WD: 0.0002, DP: 0.1, LR: 0.01, WD: 0.0001 DP: 0.2, LR: 0.01, WD: 0.0 DP: 0.2 EP: 50, HD: 512, #L: 2, BS: 5000 EP: 50, HD: 512, #L: 2, BS: 5000 EP: 30, HD: 256, #L: 2, BS: 5000 ClusterGCN(Chiang et al., 2019) LR: 0.001, WD: 0.0002, DP: 0.2, LR: 0.0001, WD: 0.0 DP: 0.5 LR: 0.001, WD: 0.0001 DP: 0.2, EP: 30, HD: 256, #L: 2, BS: 5000 EP: 50, HD: 256, #L: 4, BS: 2000 EP: 40, HD: 128, #L: 4, BS: 2000 GraphSAINT(Zeng et al., 2019) LR: 0.001, WD: 0.0004, DP: 0.2 LR: 0.01, WD: 0.0002 DP: 0.7 LR: 0.01, WD: 0.0 DP: 0.2, EP: 50, HD: 512, #L: 4, BS: 5000 EP: 30, HD: 128, #L: 2, BS: 5000 EP: 40, HD: 128, #L: 2, BS: 5000 C. Code adaptation URL for our baselines Table 10.Method and their official implementation used in our work. Method Download URL JKNet(Xu et al., 2018) https://github.com/mori97/JKNet-dgl DAGNN(Yang et al., 2021) https://github.com/vthost/DAGNN APPNP(Klicpera et al., 2019) https://github.com/gasteigerjo/ppnp GCNII(Chen et al., 2020) https://github.com/chennnM/GCNII SGC(Wu et al., 2019) https://github.com/Tiiiger/SGC ClusterGCN(Chiang et al., 2019) https://github.com/benedekrozemberczki/ClusterGCN GraphSAINT(Zeng et al., 2019) https://github.com/GraphSAINT/GraphSAINT 13",
      "meta_data": {
        "arxiv_id": "2306.10466v2",
        "authors": [
          "Ajay Jaiswal",
          "Shiwei Liu",
          "Tianlong Chen",
          "Ying Ding",
          "Zhangyang Wang"
        ],
        "published_date": "2023-06-18T03:33:46Z",
        "pdf_url": "https://arxiv.org/pdf/2306.10466v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the challenge of scaling Graph Neural Networks (GNNs) without deepening or widening, which often leads to issues like unhealthy gradients, over-smoothening, and information squashing. It proposes 'Graph Ladling,' a data-centric adaptation of model soups for GNNs, where multiple independent and parallel 'weaker' GNNs (soup ingredients) are trained on divided graph data without any intermediate communication. Their strengths are then combined using a greedy interpolation soup procedure to achieve state-of-the-art performance across various small and large graphs. The work demonstrates that model soups offer an orthogonal direction for GNN scaling, allowing aggregation of orthogonal knowledge stored in candidate models.",
        "methodology": "The core methodology involves adapting 'model soups' for GNNs trained from scratch. K instances of the same GNN architecture (soup ingredients), sharing a common random initialization, are trained independently and in parallel on multiple GPUs, with slight variations in hyperparameters (e.g., learning rate, weight decay, seed). After independent training, a greedy interpolation soup procedure (Algorithm 1) is used: ingredients are sorted by validation accuracy, and then sequentially merged. For each ingredient, an optimal interpolation ratio (alpha between 0 and 1) is searched, and the ingredient is only kept if it improves validation set performance. For large graphs, a data-centric approach extends state-of-the-art graph sampling (node-wise, edge-wise, layer-wise) and graph partitioning (e.g., METIS-based clustering from ClusterGCN) mechanisms. In sampling (Algorithm 2), ingredients are trained with mini-batch sampling. In partitioning (Algorithm 3), the graph is pre-partitioned into clusters, and each ingredient trains on a subgraph formed by randomly selected clusters. A key aspect is the absence of intermediate communication of model weights during individual ingredient training; merging occurs only post-training.",
        "experimental_setup": "Experiments were conducted on two GPU servers equipped with RTX A6000 and RTX 3090 GPUs. The datasets included three small-scale citation networks (Cora, Citeseer, PubMed) and four large-scale open benchmarks (Flickr, Reddit, OGBN-ArXiv, OGBN-products). The study used multiple GNN architectures for comparison, including GCN, GCNII, JKNet, DAGNN, APPNP, SGC, GraphSAGE, ClusterGCN, and GraphSAINT. Hyperparameters for soup ingredients were selected via efficient parameter sweeping, with variations in random seed, batch size, learning rate, weight decay, and dropout rate. Model soups were typically prepared with 50 candidate ingredients for small-scale graphs and 30 for large-scale graphs. The greedy interpolation used an alpha (α) ranging from 0 to 1 with a step size of 0.01. Baselines used official implementations, and candidate ingredients maintained the exact same architectural design as their vanilla counterparts for fair comparison. Performance was evaluated using test accuracy, with vanilla GNN results averaged across 30 independent runs and soup results averaged across 5 independent runs (each with multiple ingredients).",
        "limitations": "The computational and time costs increase with a higher number of ingredients, necessitating a practical restriction on the ingredient count. Layer-wise sampling approaches, including the Layer sampled soup, showed comparatively lower performance due to the typically sparser nature of layer-wise induced adjacency matrices. An ablation study revealed that intermediate communication across candidate ingredients during training resulted in reduced final soup performance, possibly due to a lack of diversity and orthogonal knowledge among ingredients, and incurred additional overhead. The paper also notes the current absence of a theoretical framework to fully explain the observed benefits of data-centric GNN soups.",
        "future_research_directions": "Future work will aim to develop a theoretical framework to explain the benefits and underlying mechanisms of data-centric GNN soups."
      }
    },
    {
      "title": "Are GATs Out of Balance?",
      "abstract": "While the expressive power and computational capabilities of graph neural\nnetworks (GNNs) have been theoretically studied, their optimization and\nlearning dynamics, in general, remain largely unexplored. Our study undertakes\nthe Graph Attention Network (GAT), a popular GNN architecture in which a node's\nneighborhood aggregation is weighted by parameterized attention coefficients.\nWe derive a conservation law of GAT gradient flow dynamics, which explains why\na high portion of parameters in GATs with standard initialization struggle to\nchange during training. This effect is amplified in deeper GATs, which perform\nsignificantly worse than their shallow counterparts. To alleviate this problem,\nwe devise an initialization scheme that balances the GAT network. Our approach\ni) allows more effective propagation of gradients and in turn enables\ntrainability of deeper networks, and ii) attains a considerable speedup in\ntraining and convergence time in comparison to the standard initialization. Our\nmain theorem serves as a stepping stone to studying the learning dynamics of\npositive homogeneous models with attention mechanisms.",
      "full_text": "Are GATs Out of Balance? Nimrah Mustafa∗ nimrah.mustafa@cispa.de Aleksandar Bojchevski† a.bojchevski@uni-koeln.de Rebekka Burkholz∗ burkholz@cispa.de ∗CISPA Helmholtz Center for Information Security, 66123 Saarbrücken, Germany †University of Cologne, 50923 Köln, Germany Abstract While the expressive power and computational capabilities of graph neural net- works (GNNs) have been theoretically studied, their optimization and learning dynamics, in general, remain largely unexplored. Our study undertakes the Graph Attention Network (GAT), a popular GNN architecture in which a node’s neighbor- hood aggregation is weighted by parameterized attention coefficients. We derive a conservation law of GAT gradient flow dynamics, which explains why a high portion of parameters in GATs with standard initialization struggle to change dur- ing training. This effect is amplified in deeper GATs, which perform significantly worse than their shallow counterparts. To alleviate this problem, we devise an initialization scheme that balances the GAT network. Our approach i) allows more effective propagation of gradients and in turn enables trainability of deeper networks, and ii) attains a considerable speedup in training and convergence time in comparison to the standard initialization. Our main theorem serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms. 1 Introduction A rapidly growing class of model architectures for graph representation learning are Graph Neural Networks (GNNs) [20] which have achieved strong empirical performance across various applications such as social network analysis [8], drug discovery [27], recommendation systems [61], and traffic forecasting [50]. This has driven research focused on constructing and assessing specific architectural designs [59] tailored to various tasks. On the theoretical front, mostly the expressive power [51, 26] and computational capabilities [45] of GNNs have been studied. However, there is limited understanding of the underlying learning mechanics. We undertake a classical variant of GNNs, the Graph Attention Network (GAT) [48, 9]. It overcomes the limitation of standard neighborhood representation averaging in GCNs [29, 22] by employing a self-attention mechanism [47] on nodes. Attention performs a weighted aggregation over a node’s neighbors to learn their relative importance. This increases the expressiveness of the neural network. GAT is a popular model that continues to serve as strong baseline. However, a major limitation of message-passing GNNs in general, and therefore GATs as well, is severe performance degradation when the depth of the network is even slightly increased. SOTA results are reportedly achieved by models of 2 or 3 layers. This problem has largely been attributed to over-smoothing [35], a phenomenon in which node representations become indistinguishable when multiple layers are stacked. To relieve GNNs of over-smoothing, practical techniques inspired by classical deep neural networks tweak the training process, e.g. by normalization [11, 64, 65, 66] or regularization [38, 42, 55, 67], and impose architectural changes such as skip connections (dense 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2310.07235v2  [cs.LG]  25 Oct 2023and residual) [34, 53] or offer other engineering solutions [ 33] or their combinations [ 12]. Other approaches to overcome over-smoothing include learning CNN-like spatial operators from random paths in the graph instead of the point-wise graph Laplacian operator [ 17] and learning adaptive receptive fields (different ‘effective’ neighborhoods for different nodes) [63, 36, 52]. Another problem associated with loss of performance in deeper networks is over-squashing [1], but we do not discuss it since non-attentive models are more susceptible to it, which is not our focus. Alternative causes for the challenged trainability of deeper GNNs beyond over-smoothing is hampered signal propagation during training (i.e. backpropagation) [25, 62, 13]. To alleviate this problem for GCNs, a dynamic addition of skip connections to vanilla-GCNs, which is guided by gradient flow, and a topology-aware isometric initialization are proposed in [ 25]. A combination of orthogonal initialization and regularization facilitates gradient flow in [21]. To the best of our knowledge, these are the only works that discuss the trainability issue of deep GNNs from the perspective of signal propagation, which studies a randomly initialized network at initialization. Here, we offer an insight into a mechanism that allows these approaches to improve the trainability of deeper networks that is related to the entire gradient flow dynamics. We focus in particular on GAT architectures and the specific challenges that are introduced by attention. Our work translates the concept of neuronwise balancedness from traditional deep neural networks to GNNs, where a deeper understanding of gradient dynamics has been developed and norm balanced- ness is a critical assumption that induces successful training conditions. One reason is that the degree of initial balancedness is conserved throughout the training dynamics as described by gradient flow (i.e. a model of gradient descent with infinitesimally small learning rate or step size). Concretely, for fully connected feed-forward networks and deep convolutional neural networks with continuous homogenous activation functions such as ReLUs, the difference between the squared l2-norms of incoming and outgoing parameters to a neuron stays approximately constant during training [ 15, 32]. Realistic optimization elements such as finite learning rates, batch stochasticity, momentum, and weight decay break the symmetries induced by these laws. Yet, gradient flow equations can be extended to take these practicalities into account [31]. Inspired by these advances, we derive a conservation law for GATs with positive homogeneous activation functions such as ReLUs. As GATs are a generalization of GCNs, most aspects of our insights can also be transferred to other architectures. Our consideration of the attention mechanism would also be novel in the context of classic feed-forward architectures and could have intriguing implications for transformers that are of independent interest. In this work, however, we focus on GATs and derive a relationship between model parameters and their gradients, which induces a conservation law under gradient flow. Based on this insight, we identify a reason for the lack of trainability in deeper GATs and GCNs, which can be alleviated with a balanced parameter initialization. Experiments on multiple benchmark datasets demonstrate that our proposal is effective in mitigating the highlighted trainability issues, as it leads to considerable training speed-ups and enables significant parameter changes across all layers. This establishes a causal relationship between trainability and parameter balancedness also empirically, as our theory predicts. The main theorem presented in this work serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms such as transformers [47] and in particular, vision transformers which require depth more than NLP transformers[49]. Our contributions are as follows. • We derive a conservation law of gradient flow dynamics for GATs, including its variations such as shared feature weights and multiple attention heads. • This law offers an explanation for the lack of trainability of deeper GATs with standard initialization. • To demonstrate empirically that our theory has established a causal link between balancedness and trainability, we propose a balanced initialization scheme that improves the trainability of deeper networks and attains considerable speedup in training and convergence time, as our theory predicts. 2 Theory: Conservation laws in GATs Preliminaries For a graph G = (V, E) with node set V and edge set E ⊆ V × V , where the neighborhood of a node v is given as N(v) = {u|(u, v) ∈ E}, a GNN layer computes each node’s representation by aggregating over its neighbors’ representation. In GATs, this aggregation is weighted by parameterized attention coefficients αuv, which indicate the importance of node u for v. 2Table 1: Attributes of init. schemes Init. Feat. Attn. Bal.? Xav Xavier Xavier No XavZ Xavier Zero No BalX Xavier Zero Yes BalO LLortho Zero Yes Figure 1: Params. to balance for neuron i ∈ [nl] Given input representations hl−1 v for all nodes v ∈ V , a GAT 1 layer l transforms those to: hl v = ϕ( X u∈N(v) αl uv · Wl shl−1 u ), where (1) αl uv = exp((al)⊤ · LeakyReLU(Wl shl−1 u + Wl t hl−1 v ))P u′∈N(v) exp((al)⊤ · LeakyReLU(Wlshl−1 u′ + Wl t hl−1v ))) . (2) We consider ϕ to be a positively homogeneous activation functions (i.e ϕ(x) =xϕ′(x) and conse- quently, ϕ(ax) =aϕ(x) for positive scalars a), such as a ReLU ϕ(x) = max{x, 0} or LeakyReLU ϕ(x) = max{x, 0} + −α max{−x, 0}. The feature transformation weights Ws and Wt for source and target nodes, respectively, may also be shared such that W = Ws = Wt. Definition 2.1. Given training data {(xm, ym)}M m=1 ⊂ Rd × Rp for M ≤ V , let f : Rd → Rp be the function represented by a network constructed by stacking L GAT layers as defined in Eq. (1) and (2) with W = Ws = Wt and h0 m = g(xm). Each layer l ∈ [L] of size nl has associated parameters: a feature weight matrix Wl ∈ Rnl×nl−1 and an attention vector al ∈ Rnl, where n0 = d and nL = p. Given a differentiable loss function ℓ : Rd × Rp → R, the loss L = 1/M PM i=1 ℓ(f(xm), ym) is used to update model parameters w ∈ {Wl, al}L l=1 with learning rate γ by gradient descent, i.e., wt+1 = wt − γ∇wL, where ∇wL = [∂L/∂w1, . . . ,∂L/∂w|w|] and w0 is set by the initialization scheme. For an infinitesimal γ → 0, the dynamics of gradient descent behave similarly to gradient flow defined by dw/dt = −∇wL, where t is the continuous time index. We useW[i, :], W[:, i], and a[i] to denote the ith row of W, column of W, and entry ofa, respectively. Note that Wl[i, :] is the vector of weights incoming to neuron i ∈ [nl] and Wl+1[:, i] is the vector of weights outgoing from the same neuron. For the purpose of discussion, let i ∈ [dl], j∈ [dl−1], and k ∈ [dl+1], as depicted in Fig. 1. ⟨·, ·⟩ denotes the scalar product. Conservation Laws We focus the following exposition on weight-sharing GATs, as this variant has the practical benefit that it requires fewer parameters. Yet, similar laws also hold for the non-weight- sharing case, as we state and discuss in the supplement. For brevity, we also defer the discussion of laws for the multi-headed attention mechanism to the supplement. Theorem 2.2 (Structure of gradients). Given the feature weight and attention parameters Wl and al of a layer l in a GAT network as described in Def. 2.1, the structure of the gradients for layer l ∈ [L − 1] in the network is conserved according to the following law: ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. (3) Corollary 2.3 (Norm conservation of weights incoming and outgoing at every neuron). Given the gradient structure of Theorem 2.2 and positive homogeneity of the activation ϕ in Eq. (1), gradient flow in the network adheres to the following invariance for l ∈ [L − 1]: d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 = d dt \u0010\r\rWl+1[: i] \r\r2\u0011 , (4) 1This definition of GAT is termed GATv2 in [9]. Hereafter, we refer to GATv2 as GAT. 3(a) δ with GD  (b) δ with Adam  (c) c with Xavier  (d) c with BalO Figure 2: Visualizing conservation laws: (a),(b) show δ ≈ 0 (can not empirically be exactly zero due to finite γ = 0.1) from Eq. (7) for hidden layers when L = 3. (c),(d) show the value of c from Eq. (5), which is determined by the initialization and should be 0 when the network is balanced, for L = 10 trained with GD. With Xav., the network is unbalanced and hence c ̸= 0for all neurons. With BalO, c = 0exactly at initialization for all neurons by design but during training, it varies slightly andc ≈ 0 as the network becomes slightly unbalanced due to the finite γ = 0.1. As both δ and c approximately meet their required value, we conclude that the derived law holds for practical purposes as well. It then directly follows by integration over time that the l2-norms of feature and attention weights incoming to and outgoing from a neuron are preserved such that \r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2 − \r\rWl+1[: i] \r\r2 = c, (5) where c = d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 at initialization (i.e. t = 0). We denote the ‘degree of balancedness’ by c and call an initialization balanced if c = 0. Corollary 2.4 (Norm conservation across layers). Given Eq. (4), the invariance of gradient flow at the layer level for l ∈ [L − 1] by summing over i ∈ [nl] is: d dt \u0010\r\rWl\r\r2 F − \r\ral\r\r2\u0011 = d dt \u0010\r\rWl+1\r\r2 F \u0011 . (6) Remark 2.5. Similar conservation laws also hold for the original less expressive GAT version [48] as well as for the vanilla GCN [29] yielded by fixing al = 0. Insights We first verify our theory and then discuss its implications in the context of four different initializations for GATs, which are summarized in Table 1. Xavier (Xav.) initialization [19] is the standard default for GATs. We describe the Looks-Linear-Orthogonal (LLortho) initialization later. In order to empirically validate Theorem 2.2, we rewrite Equation 3 to define δ as: δ = ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al, ∇alL⟩ − ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ = 0. (7) We observe how the value of δ varies during training in Fig. 2 for both GD and Adam optimizers with γ = 0.1. Although the theory assumes infinitesimal learning rates due to gradient flow, it still holds sufficiently (δ ≈ 0) in practice for normally used learning rates as we validate in Fig. 2. Furthermore, although the theory assumes vanilla gradient descent optimization, it still holds for the Adam optimizer (Fig. 2b). This is so because the derived law on the level of scalar products between gradients and parameters (see Theorem 2.2) still holds, as it states a relationship between gradients and parameters that is independent of the learning rate or precise gradient updates. Having verified Theorem 2.2, we use it to deduce an explanation for a major trainability issue of GATs with Xavier initialization that is amplified with increased network depth. The main reason is that the last layer has a comparatively small average weight norm, as E \r\rWL[:, i] \r\r2 = 2nL/(nL + nL−1) << 1, where the number of outputs is smaller than the layer width nL << nL−1. In contrast, E \r\rWL−1[i, :] \r\r2 = 2nL−1/(nL−1 + nL−2) = 1 and EaL−1[i]2 = 2/(1 +nL−1). In consequence, the right-hand side of our derived conservation law nL−2X j=1 (WL−1 ij )2 ∇WL−1 ij L WL−1 ij − (aL−1 i )2 ∇aL−1 i L aL−1 i = nLX k=1 (WL ki)2 ∇WL ki L WL ki (8) 4(a) Frac. of sig. params with relative change > 0.05  (b) Rel. grad. norms of feat. wghts. & training curve Figure 3: For L = 5, a high fraction of parameters change, and gradients propagate to earlier layers for both the unbalanced and balanced initialization Xav. and BalO initialization achieve 75.5% and 79.9% test accuracy, respectively. Note that the balanced initialization in (b) is able to train faster. (a) Frac. of sig. params with relative change > 0.5  (b) Rel. grad. norms of feat. wghts. & training curve Figure 4: For L = 10, unbalanced initialization is unable to train the model as relative gradients are extremely small, and the parameters (and loss) do not change. However, the balanced initialization is able to propagate gradients, change parameters, and thus attain training loss → 0. Xav. and Bal O achieve 39.3% and 80.2% test accuracy, respectively. Note that the balanced initialization in (b) is also able to train faster than the 5 layer network with Xavier initialization in Fig. 3b. is relatively small, as PnL k=1(WL ki)2 << 1, on average, while the weights on the left-hand side are orders of magnitude larger. This implies that the relative gradients on the left-hand side are likely relatively small to meet the equality unless all the signs of change are set in such a way that they satisfy the equality, which is highly unlikely during training. Therefore, relative gradients in layer L −1 and accordingly also the previous layers of the same dimension can only change in minor ways. The amplification of this trainability issue with depth can also be explained by the recursive substitu- tion of Theorem 2.2 in Eq. (8) that results in a telescoping series yielding: n1X j=1 n0X m=1 W(1) jm 2 ∇W(1) jm L W(1) jm − L−1X l=1 nlX o=1 a(l) o 2 ∇a(l) o L a(l) o = nL−1X i=1 nLX k=1 W(L) ki 2 ∇W(L) ki L W(L) ki (9) Generally, 2n1 < n0 and thus E \r\rW1[j :] \r\r2 = 2n1/(n1 + n0) < 1. Since the weights in the first layer and the gradients propagated to the first layer are both small, gradients of attention parameters of the intermediate hidden layers must also be very small in order to satisfy Eq. (9). Evidently, the problem aggravates with depth where the same value must now be distributed over the parameters and gradients of more layers. Note that the norms of the first and last layers are usually not arbitrary but rather determined by the input and output scale of the network. Analogously, we can see see how a balanced initialization would mitigate the problem. Equal weight norms \r\rWL−1[i, :] \r\r2 and \r\rWL[:, i] \r\r2 in Eq. (8) (as the attention parameters are set to 0 during balancing, see Procedure 2.6) would allow larger relative gradients in layerL − 1, as compared to the imbalanced case, that can enhance gradient flow in the network to earlier layers. In other words, gradients on both sides of the equation have equal room to drive parameter change. 5Figure 5: Relative gradient norms of feature (left axis, solid) and of attention (right axis, stylized) parameters for l ∈ [1, 5, 10] and L = 10, sampled every 25 epochs. Test accuracy is at the top. Both attention and feature gradients at the first, middle, and last layer of the network with both balanced initializations are much larger than with unbalanced initialization (note axis scales). To visualize the trainability issue, we study the relative change |(w∗−w0)/w∗|. of trained network parameters (w∗) w.r.t. their initial value. In order to observe meaningful relative change, we only con- sider parameters with a significant contribution to the model output (w∗ ≥ 10−4). We also plot the rel- ative gradient norms of the feature weights across all layers to visualize their propagation. We display these values forL = 5and L = 10in Fig. 3 and 4, respectively, and observe a stark contrast, similar to the trends in relative gradient norms of selected layers of a 10 layer GAT in Fig. 5. In all experiments, GAT was trained with the specified initialization on Cora using GD with γ = 0.1 for 5000 epochs. Interestingly, the attention parameters change most in the first layer and increasingly more towards the last layer, as seen in Fig. 3a and 4a. This is consistently observed in both the 5 and 10 layer networks if the initialization is balanced, but with unbalanced initialization, the 10 layer network is unable to produce the same effect. As our theory defines a coarse-level conservation law, such fine-grained training dynamics cannot be completely explained by it. Since the attention parameters are set to 0 in the balanced initializations (see Procedure 2.6), as ablation, we also observe the effect of initializing attention and feature parameters with zero and Xavier, respectively. From Eq.(2), al = 0leads to αuv = 1/|N(v)|, implying that all neighbors u of the node v are equally important at initialization. Intuitively, this allows the network to learn the importance of neighbors without any initially introduced bias and thus avoids the ‘chicken and egg’ problem that arises in the initialization of attention over nodes[30]. Although this helps generalization in some cases (see Fig. 6), it alone does not help the trainability issue as seen in Fig. 4a. It may in fact worsen it (see Fig. 5), which is explained by Eq. 8. These observations underline the further need for parameter balancing. Procedure 2.6 (Balancing). Based on Eq. (5) from the norm preservation law 2.3, we note that in order to achieve balancedness, (i.e. set c = 0in Eq.(5)), the randomly initialized parameters Wl and al must satisfy the following equality for l ∈ [L]: \r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2 = \r\rWl+1[: i] \r\r2 This can be achieved by scaling the randomly initialized weights as follows: 1. Set al = 0 for l ∈ [L]. 2. Set W1[i, :] = W1[i,:] ∥W1[i,:]∥ √βi, for i ∈ [n1] where βi is a hyperparameter 3. Set Wl+1[:, i] = Wl+1[:,i] ∥Wl+1[:,i]∥ \r\rWl[i, :] \r\r for i ∈ [nl] and l ∈ [L − 1] In step 2, βi determines the initial squared norm of the incoming weights to neuron i in the first layer. It can be any constant thus also randomly sampled from a normal or uniform distribution. We explain why we set βi = 2for i ∈ [n1] in the context of an orthogonal initialization. Remark 2.7. This procedure balances the network starting from l = 1towards l = L. Yet, it could also be carried out in reverse order by first defining \r\rWL[:, i] \r\r2 = βi for i ∈ [nL−1]. Balanced Orthogonal Initialization The feature weights Wl for l ∈ [L] need to be initialized randomly before balancing, which can simply be done by using Xavier initialization. However, existing works have pointed out benefits of orthogonal initialization to achieve initial dynamical 6(a) Test accuracy (higher is better).  (b) Epochs to best model (lower is better). Figure 6: GAT trained on Cora using SGD isometry for DNNs and CNNs [37, 44, 23], as it enables training very deep architectures. In line with these findings, we initialize Wl, before balancing, to be an orthogonal matrix with a looks-linear (LL) mirrored block structure [46, 6], that achieves perfect dynamical isometry in perceptrons with ReLU activation functions. Due to the peculiarity of neighborhood aggregation, the same technique does not induce perfect dynamical isometry in GATs (or GNNs). Exploring how dynamical isometry can be achieved or approximated in general GNNs could be an interesting direction for future work. Nevertheless, using a (balanced) LL-orthogonal initialization enhances trainaibility, particularly of deeper models. We defer further discussion on the effects of orthogonal initialization, such as with identity matrices, to the appendix. We outline the initialization procedure as follows: Let ∥ and |= denote row-wise and column- wise matrix concatenation, respectively. Let us draw a random orthonormal submatrix Ul and define W1 = [U1 ∥ −U1] where U1 ∈ R n1 2 ×n0 , Wl = [[Ul ∥ −Ul] |= [−Ul ∥ Ul]] where Ul ∈ R nl 2 × nl−1 2 for l = {2, . . . , L− 1}, and WL = [UL |= −UL] where UL ∈ RnL× nl−1 2 . Since UL is an orthonormal matrix, \r\rWL[:, i] \r\r2 = 2by definition, and by recursive application of Eq. (2.3 (with al = 0), balancedness requires that \r\rW1[i :, ] \r\r2 = 2. Therefore, we set βi = 2for i ∈ n1. 3 Experiments The main purpose of our experiments is to verify the validity of our theoretical insights and deduce an explanation for a major trainability issue of GATs that is amplified with increased network depth. Based on our theory, we understand the striking observation in Figure 4a that the parameters of default Xavier initialized GATs struggle to change during training. We demonstrate the validity of our theory with the nature of our solution. As we balance the parameter initialization (see Procedure 2.6), we allow the gradient signal to pass through the network layers, which enables parameter changes during training in all GAT layers. In consequence, balanced initialization schemes achieve higher generalization performance and significant training speed ups in comparison with the default Xavier initialization. It also facilitates training deeper GAT models. To analyze this effect systematically, we study the different GAT initializations listed in Table 1 on generalization (in % accuracy) and training speedup (in epochs) using nine common benchmark datasets for semi-supervised node classification tasks. We defer dataset details to the supplement. We use the standard provided train/validation/test splits and have removed the isolated nodes from Citeseer. We use the Pytorch Geometric framework and run our experiments on either Nvidia T4 Tensor Core GPU with 15 GB RAM or Nvidia GeForce RTX 3060 Laptop GPU with 6 GB RAM. We allow each network to train, both with SGD and Adam, for 5000 epochs (unless it converges earlier, i.e. achieves training loss ≤ 10−4) and select the model state with the highest validation accuracy. For each experiment, the mean ±95% confidence interval over five runs is reported for accuracy and epochs to the best model. All reported results use ReLU activation, weight sharing and no biases, unless stated otherwise. No weight sharing leads to similar trends, which we therefore omit. SGD We first evaluate the performance of the different initialization schemes of GAT models using vanilla gradient descent. Since no results have been previously reported using SGD on these datasets, to the best of our knowledge, we find that setting a learning rate of 0.1, 0.05 and 0.005 for L = [2, 5], L = [10, 20], and L = 40, respectively, allows for reasonably stable training on Cora, Citeseer, and Pubmed. For the remaining datasets, we set the learning rate to 0.05, 0.01, 0.005 and 0.0005 for 7Table 2: Mean accuracy(%) ± 95% CI over five runs of GAT with width= 64trained using SGD. Init. L = 2 L = 5 L = 10 L = 20 L = 40 Citeseer Xav 71.82 ± 2.73 58 .40 ± 2.25 24 .70 ± 8.90 19 .23 ± 1.54 18 .72 ± 1.15 BalX 71.62 ± 0.80 68 .83 ± 1.62 64 .13 ± 1.57 54 .88 ± 7.95 42 .63 ± 17.47 BalO 72.02 ± 0.63 70 .63 ± 0.60 68 .83 ± 1.68 68 .70 ± 1.18 63 .40 ± 1.43 Pubmed Xav 77.26 ± 1.39 70 .68 ± 2.16 67 .32 ± 10.70 36 .52 ± 11.50 27 .20 ± 13.99 BalX 78.02 ± 0.73 75.66 ± 1.81 77.60 ± 1.56 76.44 ± 1.70 75 .74 ± 2.94 BalO 77.68 ± 0.45 76.62 ± 1.59 77.04 ± 2.14 78.20 ± 0.61 77 .80 ± 1.41 Actor Xav 27.32 ± 0.59 24 .60 ± 0.93 24 .08 ± 0.80 22 .29 ± 3.26 19 .46 ± 5.75 BalX 26.00 ± 0.59 23 .93 ± 1.42 24.21 ± 0.78 24 .74 ± 1.14 23.88 ± 0.97 BalO 26.59 ± 1.03 24 .61 ± 0.77 24.17 ± 0.62 24 .24 ± 1.05 23.93 ± 1.53 Chameleon Xav 52.81 ± 1.37 54.21 ± 1.05 30 .31 ± 5.96 22 .19 ± 2.04 22 .28 ± 3.15 BalX 51.18 ± 1.94 54.21 ± 0.82 52 .11 ± 3.72 51.89 ± 1.89 38 .64 ± 10.31 BalO 50.00 ± 3.07 53 .95 ± 1.81 51 .84 ± 3.21 52.72 ± 0.13 44 .30 ± 1.61 Cornell Xav 42.70 ± 2.51 41.08 ± 2.51 42.70 ± 1.34 25.41 ± 14.64 22 .70 ± 13.69 BalX 41.08 ± 6.84 35 .14 ± 11.82 41 .08 ± 2.51 40 .00 ± 4.93 37.84 ± 5.62 BalO 42.16 ± 1.64 43.24 ± 2.12 36.76 ± 5.02 35.68 ± 3.29 36.22 ± 3.42 Squirrel Xav 35.20 ± 0.44 40 .96 ± 0.92 21 .65 ± 1.52 20 .23 ± 1.69 19 .67 ± 0.29 BalX 35.95 ± 1.69 40.98 ± 0.87 38.98 ± 1.49 38.35 ± 1.07 25 .38 ± 4.62 BalO 35.83 ± 0.92 42.52 ± 1.19 38.85 ± 1.36 39.15 ± 0.44 26 .57 ± 1.83 Texas Xav 60.00 ± 1.34 60 .54 ± 3.42 58 .92 ± 1.34 49 .73 ± 20.97 17 .84 ± 26.98 BalX 60.54 ± 1.64 61 .62 ± 2.51 61 .62 ± 2.51 58 .92 ± 2.51 56.22 ± 1.34 BalO 60.00 ± 1.34 57 .30 ± 1.34 56 .76 ± 0.00 58 .38 ± 4.55 57.30 ± 3.91 Wisconsin Xav 51.37 ± 6.04 51.37 ± 8.90 51 .76 ± 3.64 43 .14 ± 25.07 31 .76 ± 31.50 BalX 49.80 ± 8.79 54.51 ± 4.19 47.84 ± 7.16 50.59 ± 10.49 41.18 ± 1.54 BalO 49.80 ± 4.24 55 .69 ± 3.64 51.76 ± 5.68 49.02 ± 4.36 48.24 ± 4.52 L = [2, 5], L= 10, L = 20, and L = 40, respectively. Note that we do not perform fine-tuning of the learning rate or other hyperparameters with respect to performance and use the same settings for all initializations to allow fair comparison. Figure 6 highlights that models initialized with balanced parameters, Bal O, consistently achieve the best accuracy and significantly speed up training, even on shallow models of 2 layers. In line with our theory, the default Xavier (Xav) initialization hampers model training (see also Figure 4a). Interestingly, the default Xavier (Xav) initialized deeper models tend to improve in performance with width but cannot compete with balanced initialization schemes. For example, the accuracy achieved by Xav for L = 10increases from 24.7% to 68.7% when the width is increased from 64 to 512. We hypothesize that the reason why width aids generalization performance is that a higher number of (almost random) features supports better models. This would also be in line with studies of overparameterization in vanilla feed-forward architectures, where higher width also aids random feature models [2, 4, 57, 43]. The observation that width overparameterized models enable training deeper models may be of independent interest in the context of how overparameterization in GNNs may be helpful. However, training wider and deeper (hence overall larger) models is computationally inefficient, even for datasets with the magnitude of Cora. In contrast, the BalO initialized model for L = 10is already able to attain 79.7% even with width= 64and improves to 80.9% with width= 512. Primarily the 8(a) Test accuracy.  (b) Epochs to convergence. Figure 7: GAT with 64 hidden dimensions trained using Adam. parameter balancing must be responsible for the improved performance, as it is not sufficient to simply initialize the attention weights to zero, as shown in Figure 4a and 5. For the remaining datasets, we report only the performance of networks with 64 hidden dimension in Table 2 for brevity. Since we have considered the XavZ initialization only as an ablation study and find it to be ineffective (see Figure 6a), we do not discuss it any further. Note that for some datasets (e.g.Pubmed, Wisconsin), deeper models (e.g. L = 40) are unable to train at all with Xavier initialization, which explains the high variation in test accuracy across multiple runs as, in each run, the model essentially remains a random network. The reduced performance even with balanced initialization at higher depth may be due to a lack of convergence within5000 epochs. Nevertheless, the fact that the drop in performance is not drastic, but rather gradual as depth increases, indicates that the network is being trained, i.e. trainability is not an issue. As a stress test, we validate that GAT networks with balanced initialization maintain their trainability at even larger depths of 64 and 80 layers (see Fig. 9 in appendix). In fact, the improved performance and training speed-up of models with balanced initialization as opposed to models with standard initialization is upheld even more so for very deep networks. Apart from Cora, Citeseer and Pubmed, the other datasets are considered heterophilic in nature and standard GNN models do not perform very well on them. State-of-the-art performance on these datasets is achieved by specialized models. We do not compare with them for two reasons: i) they do not employ an attention mechanism, which is the focus of our investigations, but comprise various special architectural elements and ii) our aim is not to outperform the SOTA, but rather highlight a mechanism that underlies the learning dynamics and can be exploited to improve trainability. We also demonstrate the effectiveness of a balanced initialization in training deep GATs in comparison to LipschitzNorm [14], a normalization technique proposed specifically for training deep GNNs with self-attention layers such as GAT (see Table 6 in appendix). Adam Adam, the most commonly used optimizer for GNNs, stabilizes the training dynamics and can compensate partially for problematic initializations. However, the drop in performance with depth, though smaller, is inevitable with unbalanced initialization. As evident from Figure 7, BalO initialized models achieve higher accuracy than or at par with Xavier initialization and converge in fewer epochs. As we observe with SGD, this difference becomes more prominent with depth, despite the fact that Adam itself significantly improves the trainability of deeper networks over SGD. Our argument of small relative gradients (see Eq.(8)) also applies to Adam. We have used the initial learning rates reported in the literature [9]: 0.005 for Cora and Citeseer, and 0.01 for Pubmed for the 2 and 5 layer networks. To allow stable training of deeper networks, we reduce the initial learning rate by a factor 0.1 for the 10 and 20 layer networks on all three datasets. Architectural variations We also consider other architectural variations such as employing ELUs instead of ReLUs, using multiple attention heads, turning off weight sharing, and addition of standard elements such as weight decay and dropout. In all cases, the results follow a similar tendency as already reported. Due to a lack of space, we defer the exact results to the appendix. These variations further increase the performance of networks with our initialization proposals, which can therefore be regarded as complementary. Note that residual skip connections between layers are also supported 9in a balanced initialization provided their parameters are initialized with zero. However, to isolate the contribution of the initialization scheme and validate our theoretical insights, we have focused our exposition on the vanilla GAT version. Limitations The derived conservation law only applies to the self-attention defined in the original GAT and GATv2 models, and their architectural variations such as ωGAT[18] (see Fig. 10 in appendix). Note that the law also holds for the non-attentive GCNs (see Table 7 in appendix) which are a special case of GATs (where the attention parameters are simply zero). Modeling different kinds of self-attention such as the dot-product self-attention in [28] entails modification of the conservation law, which has been left for future work. 4 Discussion GATs [9] are powerful graph neural network models that form a cornerstone of learning from graph- based data. The dynamic attention mechanism provides them with high functional expressiveness, as they can flexibly assign different importance to neighbors based on their features. However, as we slightly increase the network depth, the attention and feature weights face difficulty changing during training, which prevents us from learning deeper and more complex models. We have derived an explanation of this issue in the form of a structural relationship between the gradients and parameters that are associated with a feature. This relationship implies a conservation law that preserves a sum of the respective squared weight and attention norms during gradient flow. Accordingly, if weight and attention norms are highly unbalanced as is the case in standard GAT initialization schemes, relative gradients for larger parameters do not have sufficient room to increase. This phenomenon is similar in nature to the neural tangent kernel (NTK) regime [ 24, 54], where only the last linear layer of a classic feed-forward neural network architecture can adapt to a task. Conservation laws for basic feed-forward architectures [15, 3, 32] do not require an infinite width assumption like NTK-related theory and highlight more nuanced issues for trainability. Furthermore, they are intrinsically linked to implicit regularization effects during gradient descent [41]. Our results are of independent interest also in this context, as we incorporate the attention mechanism into the analysis, which has implications for sequence models and transformers as well. One of these implications is that an unbalanced initialization hampers the trainability of the involved parameters. Yet, the identification of the cause already contains an outline for its solution. Balancing the initial norms of feature and attention weights leads to more effective parameter changes and significantly faster convergence during training, even in shallow architectures. To further increase the trainability of feature weights, we endow them with a balanced orthogonal looks-linear structure, which induces perfect dynamical isometry in perceptrons [10] and thus enables signal to pass even through very deep architectures. Experiments on multiple benchmark datasets have verified the validity of our theoretical insights and isolated the effect of different modifications of the initial parameters on the trainability of GATs. 5 Acknowledgements We gratefully acknowledge funding from the European Research Council (ERC) under the Horizon Europe Framework Programme (HORIZON) for proposal number 101116395 SPARSE-ML. 10References [1] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. In International Conference on Learning Representations, 2021. [2] Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient descent for deep linear neural networks. In International Conference on Learning Representations, 2019. [3] Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach, 2018. [4] Jimmy Ba, Murat A. Erdogdu, Taiji Suzuki, Denny Wu, and Tianzong Zhang. Generalization of two-layer neural networks: An asymptotic viewpoint. In International Conference on Learning Representations, 2020. [5] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016. [6] David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams. The shattered gradients problem: If resnets are the answer, then what is the question? In International Conference on Machine Learning, 2018. [7] Peter L. Bartlett, David P. Helmbold, and Philip M. Long. Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks. In International Conference on Machine Learning, 2018. [8] T. Bian, X. Xiao, T. Xu, P. Zhao, W. Huang, Y . Rong, and J. Huang. Rumor detection on social media with bi-directional graph convolutional networks. In AAAI Conference on Artificial Intelligence, 2020. [9] Shaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? In International Conference on Learning Representations, 2022. [10] Rebekka Burkholz and Alina Dubatovka. Initialization of ReLUs for dynamical isometry. In Advances in Neural Information Processing Systems, volume 32, 2019. [11] Tianle Cai, Shengjie Luo, Keyulu Xu, Di He, Tie-Yan Liu, and Liwei Wang. Graphnorm: A principled approach to accelerating graph neural network training. In International Conference on Machine Learning, 2021. [12] Tianlong Chen, Kaixiong Zhou, Keyu Duan, Wenqing Zheng, Peihao Wang, Xia Hu, and Zhangyang Wang. Bag of tricks for training deeper graph neural networks: A comprehensive benchmark study. In IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [13] Weilin Cong, Morteza Ramezani, and Mehrdad Mahdavi. On provable benefits of depth in training graph convolutional networks. In Advances in Neural Information Processing Systems, 2021. [14] George Dasoulas, Kevin Scaman, and Aladin Virmaux. Lipschitz normalization for self-attention layers with application to graph neural networks. In International Conference on Machine Learning, 2021. [15] S. S. Du, W. Hu, and J. D. Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. In Advances in Neural Information Processing Systems, 2018. [16] Moshe Eliasof, Eldad Haber, and Eran Treister. Pde-gcn: Novel architectures for graph neural networks motivated by partial differential equations. In Advances in Neural Information Processing Systems, 2021. [17] Moshe Eliasof, Eldad Haber, and Eran Treister. pathgcn: Learning general graph spatial operators from paths. In International Conference on Machine Learning, 2022. [18] Moshe Eliasof, Lars Ruthotto, and Eran Treister. Improving graph neural networks with learnable propagation operators. In International Conference on Machine Learning, 2023. 11[19] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In International Conference on Artificial Intelligence and Statistics, volume 9, pages 249–256, May 2010. [20] M. Gori, G. Monfardini, and F. Scarselli. A new model for learnig in graph domains. In IEEE International Joint Conference on Neural Networks, 2005. [21] Kai Guo, Kaixiong Zhou, Xia Hu, Yu Li, Yi Chang, and Xin Wang. Orthogonal graph neural networks. In AAAI Conference on Artificial Intelligence, 2022. [22] William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, 2018. [23] Wei Hu, Lechao Xiao, and Jeffrey Pennington. Provable benefit of orthogonal initialization in optimizing deep linear networks. In International Conference on Machine Learning, 2020. [24] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems , 2018. [25] Ajay Jaiswal, Peihao Wang, Tianlong Chen, Justin F. Rousseau, Ying Ding, and Zhangyang Wang. Old can be gold: Better gradient flow can make vanilla-gcns great again. In Advances in Neural Information Processing Systems, 2022. [26] Stefanie Jegelka. Theory of graph neural networks: Representation and learning. In The International Congress of Mathematicians, 2022. [27] S. Kearnes, K. McCloskey, M. Berndl, V . Pande, and P. Riley. Molecular graph convolutions: Moving beyond fingerprints. In Journal of Computer-Aided Molecular Design, 2016. [28] Dongkwan Kim and Alice Oh. How to find your friendly neighborhood: Graph attention design with self-supervision. In International Conference on Learning Representations, 2021. [29] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017. [30] Boris Knyazev, Graham W. Taylor, and Mohamed R. Amer. Understanding attention and generalization in graph neural networks. In Advances in Neural Information Processing Systems, 2019. [31] D. Kunin, J. Sagastuy-Brena, S. Ganguli, D.L.K. Yamins, and H. Tanaka. Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics. InInternational Conference on Learning Representations, 2021. [32] Thien Le and Stefanie Jegelka. Training invariances and the low-rank phenomenon: beyond linear networks. In International Conference on Learning Representations, 2022. [33] Guohao Li, Matthias Müller, Bernard Ghanem, and Vladlen Koltun. Training graph neural networks with 1000 layers. In International Conference on Machine Learning, 2022. [34] Guohao Li, Matthias Müller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as cnns? In International Conference on Computer Vision, 2019. [35] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In AAAI Conference on Artificial Intelligence, 2018. [36] Meng Liu, Hongyang Gao, and Shuiwang Ji. Towards deeper graph neural networks. In SIGKDD International Conference on Knowledge Discovery and Data Mining, 2020. [37] Dmytro Mishkin and Jiri Matas. All you need is a good init. In International Conference on Learning Representations, 2016. [38] Pál András Papp, Karolis Martinkus, Lukas Faber, and Roger Wattenhofer. Dropgnn: Ran- dom dropouts increase the expressiveness of graph neural networks. In Advances in Neural Information Processing Systems, 2021. 12[39] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geo- metric graph convolutional networks. In International Conference on Learning Representations, 2020. [40] Oleg Platonov, Denis Kuznedelev, Michael Diskin, Artem Babenko, and Liudmila Prokhorenkova. A critical look at the evaluation of gnns under heterophily: are we really making progress? In International Conference on Learning Representations, 2023. [41] Noam Razin and Nadav Cohen. Implicit regularization in deep learning may not be explainable by norms. In Advances in Neural Information Processing Systems, 2020. [42] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional networks on node classification. In International Conference on Learning Representations, 2020. [43] Karthik Abinav Sankararaman, Soham De, Zheng Xu, W. Ronny Huang, and Tom Goldstein. The impact of neural network overparameterization on gradient confusion and stochastic gradient descent. In International Conference on Machine Learning, 2020. [44] Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In International Conference on Learning Representations, 2014. [45] F. Scarselli, M. Gori, A.C Tsoi, M. Hagenbuchner, and G. Monfardini. Computational capabili- ties of graph neural networks. In IEEE Transactions on Neural Networks, 2009. [46] Wenling Shang, Kihyuk Sohn, Diogo Almeida, and Honglak Lee. Understanding and im- proving convolutional neural networks via concatenated rectified linear units. In International Conference on Machine Learning, 2016. [47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa- tion Processing Systems, 2017. [48] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018. [49] Peihao Wang, Wenqing Zheng, Tianlong Chen, and Zhangyang Wang. Anti-oversmoothing in deep vision transformers via the fourier domain analysis: From theory to practice. In International Conference on Learning Representations, 2022. [50] Z. Wu, S. Pan, G. Long, J. Jiang, and C. Zhang. Graph wavenet for deep spatial-temporal graph modeling. In International Joint Conference on Artificial Intelligence, 2019. [51] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. [52] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In International Conference on Machine Learning, 2018. [53] Keyulu Xu, Mozhi Zhang, Stefanie Jegelka, and Kenji Kawaguchi. Optimization of graph neural networks: Implicit acceleration by skip connections and more depth. In International Conference on Machine Learning, 2021. [54] Greg Yang and Edward J. Hu. Tensor programs iv: Feature learning in infinite-width neural networks. In Marina Meila and Tong Zhang, editors, International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 11727–11737, 2021. [55] Han Yang, Kaili Ma, and James Cheng. Rethinking graph regularization for graph neural networks. In Advances in Neural Information Processing Systems, 2021. 13[56] Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. In International Conference on Machine Learning, 2016. [57] Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for under- standing neural networks. In Advances in Neural Information Processing Systems, 2019. [58] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform bad for graph representation? In Advances in Neural Information Processing Systems, 2021. [59] Jiaxuan You, Rex Ying, and Jure Leskovec. Design space for graph neural networks. In Advances in Neural Information Processing Systems, 2021. [60] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J. Kim. Graph transformer networks. In Advances in Neural Information Processing Systems, 2020. [61] M. Zhang and Y . Chen. Inductive matrix completion based on graph neural networks. In International Conference on Learning Representations, 2020. [62] Wentao Zhang, Zeang Sheng, Ziqi Yin, Yuezihan Jiang, Yikuan Xia, Jun Gao, Zhi Yang, and Bin Cui. Model degradation hinders deep graph neural networks. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022. [63] Wentao Zhang, Mingyu Yang, Zeang Sheng, Yang Li, Wen Ouyang, Yangyu Tao, Zhi Yang, and Bin Cui. Node dependent local smoothing for scalable graph learning. In Advances in Neural Information Processing Systems, 2021. [64] Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. In International Conference on Learning Representations, 2020. [65] Kaixiong Zhou, Xiao Huang, Yuening Li, Daochen Zha, Rui Chen, and Xia Hu. Towards deeper graph neural networks with differentiable group normalization. In Advances in Neural Information Processing Systems, 2020. [66] Kuangqi Zhou, Yanfei Dong, Kaixin Wang, Wee Sun Lee, Bryan Hooi, Huan Xu, and Jiashi Feng. Understanding and resolving performance degradation in graph convolutional networks. In Conference on Information and Knowledge Management, 2021. [67] Difan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, and Quanquan Gu. Layer- dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, 2019. 14Appendices 6 Proofs of Theorems Notation. Scalar and element-wise products are denoted ⟨, ⟩ and ⊙, respectively. Boldface lowercase and uppercase symbols represent vectors and matrices, respectively. Our proof of Theorem 2.2 utilizes a rescale invariance that follows from Noether’s theorem, as stated by [31]. Note that we could also derive the gradient structure directly from the derivatives, but the rescale invariance is easier to follow. Definition 6.1 (Rescale invariance). The loss L(θ) is rescale invariant with respect to disjoint subsets of the parameters θ1 and θ2 if for every λ >0 we have L(θ) =L((λθ1, λ−1θ2, θd)), where θ = (θ1, θ2, θd). We frequently utilize the following relationship. Lemma 6.2 (Gradient structure due to rescale invariance [31]). The rescale invariance of L enforces the following geometric constraint on the gradients of the loss with respect to its parameters: ⟨θ1, ∇θ1 L⟩ − ⟨θ2, ∇θ2 L⟩ = 0. (10) We first use this rescale invariance to prove our main theorem for a GAT with shared feature transformation weights and a single attention head, as described in Def. 2.1. The underlying principle generalizes to other GAT versions as well, as we exemplify with two other variants. Firstly, we study the case of unshared weights for feature transformation of source and target nodes. Secondly, we discuss multiple attention heads. Proof of Theorem 2.2 Our main proof strategy relies on identifying a multiplicative rescale invariance in GATs that allows us to apply Lemma 6.2. We identify rescale invariances for every neuron i at layer l that induce the stated gradient structure. Specifically, we define the components of in-coming weights to the neuron as θ1 = {w | w ∈ Wl[i, : ]} and the union of all out-going edges (regarding features and attention) as θ2 = {w | w ∈ Wl+1[: , i]} ∪ {al[i]}. It is left to show that these parameters are invariant under rescaling. Let us, therefore, evaluate the GAT loss at λθ1 and λ−1θ2 and show that it remains invariant for any choice of λ >0. Note that the only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and αl uv. We denote the scaled network components with a tilde resulting in ˜hl u[i], ˜hl+1 v [k], and ˜αl uv As we show, parameters of upper layers remain unaffected, as ˜hl+1 v [k] coincides with its original non-scaled variant ˜hl+1 v [k] =hl+1 v [k]. Let us start with the attention coefficients. Note that ˜al[i] =λ−1al[i] and ˜Wl[i, j] =λWl[i, j]. This implies that ˜αl uv = exp(el uv)P u′∈N(v) exp(eluv) = αl uv , because (11) ˜el uv = (al)⊤ · ϕ2(Wl(hl−1 u + hl−1 v )) =el uv, (12) which follows from the positive homogeneity of ϕ2 that allows ˜el uv = λ−1al[i]ϕ2( nl−1X j λWl[i, j](hl−1 u [j] +hl−1 v [j]) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) (13) = λ−1λal[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) (14) = el uv. (15) 15Since ˜αl uv = αl uv, it follows that ˜hu l [i] =ϕ1   X z∈N(u) αl zu nl−1X j λWl[i, j]hl−1 z [j]   = λϕ1   X z∈N(u) αl zu nl−1X j Wl[i, j]hl−1 z [j]   = λhl u[i] In the next layer, we therefore have ˜hl+1 v [k] =ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]˜hl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]λhl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i Wl+1[k, i]hl u[i]   = hl+1 v [k] Thus, the output node representations of the network remain unchanged, and according to Def. 6.1, the loss L is rescale-invariant. Consequently, as per Lemma 6.2, the constraint in Eq.(10) can be written as: ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ − ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ = 0. which can be rearranged to Eq.((2.2): ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. thus proving Theorem 2.2. Proof of Corollary 2.3 Given that gradient flow applied on loss L is captured by the differential equation dw dt = −∇wL (16) which implies: d dt \r\rWl[i, :] \r\r2 = 2⟨Wl[i, :], d dtWl[i, :]⟩ = −2⟨Wl[i, :], ∇Wl[i,:]L⟩ (17) substituting in Eq.(4) similarly for gradient flow of Wl+1[:, i] and al[i], as done in Eq.(17) yields Theorem 2.2: d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 = d dt \u0010\r\rWl+1[: i] \r\r2\u0011 . −2⟨Wl[i, :], ∇Wl[i,:]L⟩ −(−2)⟨al[i], ∇al[i]L⟩ = −2⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. Therefore, Eq.(4) and consequently Eq.(5) in Corollary 2.3 hold. Summing over i ∈ [nl] on both sides of Eq.(4) yields Corollary 2.4. 16Theorem 6.3 (Structure of gradients for GAT without weight-sharing). Let a GAT network as defined by Def. 2.1 consisting of L layers be given. The feature transformation parameters Wl s and Wl t of the source and target nodes of an edge and attention parametersal of a GAT layer l are defined according to Eq.(1) and (2). Then the gradients for layer l ∈ [L − 1] in the network are governed by the following law: ⟨Wl s[i, :], ∇Wls[i,:]L⟩ + ⟨Wl t [i, :], ∇Wl t [i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1 s [:, i], ∇Wl+1 s [:,i]L⟩ (18) Proof. The proof is analogous to the derivation of Theorem 2.2. We follow the same principles and define the disjoint subsets θ1 and θ2 of the parameter set θ, associated with a neuron i in layer l accordingly, as follows: θ1 = {w|w ∈ Wl s[i, :]} ∪ {w|w ∈ Wl t [i, :]} θ2 = {w|w ∈ Wl+1 s [:, i]} ∪ {al[i]} Then, the invariance of node representations follows similarly to the proof of Theorem 2.2. The only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and αl uv. We denote the scaled network components with a tilde resulting in ˜hl u[i], ˜hl+1 v [k], and ˜αl uv As we show, parameters of upper layers remain unaffected, as ˜hl+1 v [k] coincides with its original non-scaled variant ˜hl+1 v [k] =hl+1 v [k]. Let us start with the attention coefficients. Note that ˜al[i] =λ−1al[i], ˜Ws l [i, j] = λWl s[i, j] and ˜Wl t [i, j] =λWl t [i, j]. This implies that ˜αl uv = exp(el uv)P u′∈N(v) exp(eluv) = αl uv , because (19) ˜el uv = (al)⊤ · ϕ2(Wl shl−1 u + Wl t hl−1 v ) =el uv, (20) which follows from the positive homogeneity of ϕ2 that allows ˜el uv = λ−1al[i]ϕ2( nl−1X j λWl s[i, j]hl−1 u [j] +λWl t [i, j]hl−1 v [j] (21) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j]) (22) = λ−1λal[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j] (23) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j]) (24) = el uv. (25) Since ˜αl uv = αl uv, it follows that ˜hu l [i] =ϕ1   X z∈N(u) αl zu nl−1X j λWl s[i, j]hl−1 z [j]   = λϕ1   X z∈N(u) αl zu nl−1X j Wl s[i, j]hl−1 z [j]   = λhl u[i] 17In the next layer, we therefore have ˜hv l+1 [k] =ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1 s [k, i]˜hl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1 s [k, i]λhl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i Wl+1 s [k, i]hl u[i]   = hl+1 v [k] Thus, the application of Lemma 6.2 derives Theorem 6.3. Theorem 6.4 (Structure of gradients for GAT with multi-headed attention) . Given the feature transformation parameters Wl k and attention parameters al k of an attention head k ∈ [K] in a GAT layer of a L layer network. Then the gradients of layer l ∈ [L − 1] respect the law: KX k ⟨Wl k[i, :], ∇Wl k[i,:]L⟩ − KX k ⟨al k[i], ∇al k[i]L⟩ = KX k ⟨Wl+1 k [:, i], ∇Wl+1 k [:,i]L⟩. (26) Proof. Each attention head is independently defined by Eq.1 and Eq.2, and thus Theorem 2.2 holds for each head, separately. The aggregation of multiple heads in a layer is done over node representations of each head in either one of two ways: Concatenation: hl v =∥K k ϕ(P u∈N(v) αkuv l · Wl khl−1 u ) , or Average: hl v = 1 K PK k ϕ(P u∈N(v) αkuv l · Wl khl−1 u ) Thus, the rescale invariance and consequent structure of gradients of the parameters in each head are not altered and Eq.(6.2) holds by summation of the conservation law over all heads. For the most general case of a GAT layer without weight-sharing and with multi-headed attention, each term in Theorem 6.3 is summed over all heads. Further implications analogous to Corollary 2.3, and 2.4 can be derived for these variants by similar principles. 7 Training Dynamics In the main paper, we have shared observations regarding the learning dynamics of GAT networks of varying depths with standard initialization and or balanced initialization schemes. Here, we report additional figures for more varied depths to complete the picture. All figures in this section correspond to a L layer GAT network trained on Cora. For SGD, the learning rate is set to γ = 0.1, γ = 0.05 and γ = 0.01 for L = 5, L = 10and L = 20, respectively. For Adam, γ = 0.005 and γ = 0.0001 for L = 5and L = 10, respectively. Relative change of a feature transformation parameter w is defined as |(w∗−w0)/w∗| where w0 is the initialized value and w∗ is the value for the model with maximum validation accuracy during training. Absolute change |a∗ − a0| is used for attention parameters a since attention parameters are initialized with 0 in the balanced initialization. We view the fraction of parameters that remain unchanged during training separately in Figure 8, and examine the layer-wise distribution of change in parameters in Figure 11 without considering the unchanged parameters. To analyze how gradients vary in the network during training, we define the relative gradient norm of feature transformation and attention parameters for a layer l as ∥∇WlWl∥F/∥Wl∥F and ∥∇alal∥/∥al∥, respectively. Figures 3b and 4b, and Figure 12 depict relative gradient norms for training under SGD and Adam respectively. 18(a) L = 5  (b) L = 10  (c) L = 20 Figure 8: Layer-wise fraction of feature transformation parameters Wl and attention parameters al with zero relative and absolute change, respectively, trained with SGD. For L = 2, no parameters with zero change existed. A small number of feature weights do not change in a 5 layer unbalanced network initialized with Xavier, but this fraction becomes significantly large when depth is increased to L = 10. Note that W1 contains a much larger number of parameters compared to the intermediate layers (specifically in this case, though it is generally common). At L = 20, nearly all w ∈ Wl with an unbalanced initialization (Xav. and Xav+ZeroAtt) struggle to change during training, whereas the balanced Xavier and LL-orthogonal initialization are able to drive change in most w ∈ Wl and all a ∈ al parameters, allowing the network to train. 8 Additional Results The results of the main paper have focused on the vanilla GAT having ReLU activation between consecutive layers, a single attention head, and shared weights for feature transformation of the source and target nodes, optimized with vanilla gradient descent (or Adam) without any regularization. Here, we present additional results for training with architectural variations, comparing a balanced initialization with a normalization scheme focused on GATs, and discussing the impact of an orthogonal initialization and the applicability of the derived conservation law to other message- passing GNNs (MPGNNs). Training variations To understand the impact of different architectural variations, common regular- ization strategies such as dropout and weight decay, and different activation functions, we conducted additional experiments. For a 10-layer network with width 64, Table 3 and 4 report our results for SGD and Adam, respectively. The values of hyperparameters were taken from [48]. Table 3: Mean test accuracy(%) ± 95% CI over five runs of GAT trained on Cora using SGD Variation L Xav Bal X BalO None (Vanilla GAT) 5 73 .00 ± 3.02 76 .96 ± 2.21 79.48 ± 0.43 10 25 .48 ± 18.13 77 .72 ± 1.49 79.46 ± 1.34 attention heads = 8 5 73 .56 ± 2.71 77 .44 ± 1.54 79.58 ± 0.53 10 25 .50 ± 18.18 77 .02 ± 2.76 79.06 ± 0.73 activation = ELU 5 75 .68 ± 1.80 79 .20 ± 1.07 79.64 ± 0.36 10 73 .02 ± 2.27 78.64 ± 1.72 47.76 ± 7.39 dropout = 0.6 5 42 .14 ± 15.97 79 .18 ± 1.17 81.00 ± 0.62 10 24 .90 ± 9.50 30 .94 ± 1.04 44.40 ± 1.84 weight decay = 0.0005 5 67 .26 ± 6.30 77 .36 ± 1.74 79.56 ± 0.48 10 18 .78 ± 11.96 76 .56 ± 2.91 79.40 ± 1.15 weight sharing = False 5 70 .80 ± 7.00 77 .28 ± 1.45 79.82 ± 0.63 10 19 .54 ± 14.03 76 .04 ± 1.77 79.06 ± 0.32 We find that the balanced initialization performs similar in most variations, outperforming the unbalanced standard initialization by a substantial margin. Dropout generally does not seem to be helpful to the deeper network ( L = 10), regardless of the initialization. From Table 3 and 4, it is evident that although techniques like dropout and weight decay may aid optimization, they are 19Table 4: Mean test accuracy(%) ± 95% CI over five runs of GAT trained on Cora using Adam Variation L Xav Bal X BalO None (Vanilla GAT) 5 76 .18 ± 1.61 79 .38 ± 2.24 80.20 ± 0.57 10 70 .86 ± 3.99 75 .72 ± 2.35 79.62 ± 1.27 attention heads = 8 5 75 .62 ± 1.74 78 .54 ± 1.06 79.56 ± 0.85 10 70 .94 ± 2.76 75 .48 ± 2.48 79.74 ± 1.10 activation = ELU 5 76 .56 ± 1.72 78 .72 ± 1.02 79.56 ± 0.64 10 75 .30 ± 1.42 78.48 ± 1.79 76.52 ± 0.97 dropout = 0.6 5 76 .74 ± 1.44 78 .42 ± 1.28 79.92 ± 0.48 10 32 .48 ± 6.99 31 .76 ± 1.33 76.34 ± 0.95 weight decay = 0.0005 5 75 .10 ± 2.05 78 .52 ± 1.41 79.80 ± 0.63 10 28 .74 ± 12.04 74 .68 ± 3.06 79.70 ± 1.14 weight sharing = False 5 76 .56 ± 1.72 78 .72 ± 1.02 79.56 ± 0.64 10 71 .12 ± 2.23 73 .32 ± 1.23 79.46 ± 1.16 alone not enough to enable the trainability of deeper network GATs and thus are complementary to balancedness. Note that ELU is not a positively homogeneous activation function, which is an assumption in out theory. In practice, however, it does not impact the Xavier-balanced initialization (Bal X). However, the Looks-Linear orthogonal structure is specific to ReLUs. Therefore, the orthogonality and balancedness of the Bal O initialization are negatively impacted by ELU, although the Adam optimizer seems to compensate for it to some extent. In addition to increasing the trainability of deeper networks, the balanced initialization matches the state-of-the-art performance of Xavier initialized 2 layer GAT, given the architecture and optimization hyperparameters as reported in [48]. We used an existing GAT implementation and training script2 of Cora that follows the original GAT(v1)[48] paper and added our balanced initialization to the code. As evident from Table 5, the balanced initialization matches SOTA performance of GAT(v1) on Cora (83-84%) (on a version of the dataset with duplicate edges). GAT(v2) achieved 78.5% accuracy on Pubmed but the corresponding hyperparameter values were not reported. Hence, we transferred them from [48]. This way, we are able to match the state-of-the-art performance of GAT(v2) on Pubmed, as shown in Table 5. Table 5: GAT performance on Cora and Pubmed: mean test accuracy(%) ± 95% CI over five runs. . Xav Bal X BalO Cora 84.50 ± 0.52 84.58 ± 0.65 84.55 ± 0.47 Pubmed 78.38 ± 0.77 78 .52 ± 0.54 78.56 ± 0.20 Comparison with Lipshitz Normalization A feasible comparison can be carried out with [14] that proposes a Lipschitz normalization technique aimed at improving the performance of deeper GATs in particular. We use their provided code to reproduce their experiments on Cora, Citeseer, and Pubmed for 2,5,10,20 and 40-layer GATs with Lipschitz normalization and compare them with LLortho+Bal initialization in Table 6. Note that Lipschitz normalization has been shown to outperform other previous normalization techniques for GNNs such as pair-norm [64] and layer-norm [5]. Impact of orthogonal initialization In the main paper, we have advocated and presented results for using a balanced LL-orthogonal initialization. Here, we discuss two special cases of orthogonal initialization (and their balanced versions): identity matrices that have also been used for GNNs in 2https://github.com/Diego999/pyGAT 20Table 6: Comparing a balanced LL-orthogonal initialization to Lipschitz normalization applied with a standard (imbalanced) Xavier initialization: the balanced initialization results in a much higher accuracy as the depth of the network increases. Dataset Cora Citeseer Pubmed Layers Lip. Norm. Bal O Init. Lip. Norm. Bal O Init. Lip. Norm. Bal O Init. 2 82.1 79.5 65 .4 67.7 74.8 76.0 5 77 .1 80.2 63.0 67.7 73.7 75.7 10 78 .0 79.6 43.6 67.4 52.8 76.9 20 72 .2 77.3 18.2 66.3 23.3 77.3 40 12 .9 75.9 18.1 63.2 36.6 77.5 [18, 16], and matrices with looks-linear structure using an identity submatrix (LLidentity) since a looks-linear structure would be more effective for ReLUs [10]. In line with [18], identity and LLidentity are used to initialize the hidden layers while Xavier is used to initialize the first and last layers. A network initialized with identity is balanced by adjusting the weights of the first and last layer to have norm1 (as identity matrices have row-wise and column-wise norm). A network with LLidentity initialization is balanced to have norm 2 in the first and last layer, similar to LLortho initialization. We compare the performance using these four base initializations (one standard Xavier, and three orthogonal cases) and their balanced counterparts in Fig. 9. We observe that balancing an (LL-)orthogonal initialization results in an improvement in the general- ization ability of the network in most cases and speeds up training, particularly for deeper networks. However, note that an (LL-)orthogonal initialization itself also has a positive effect on trainability in particular of deep networks. Contributing to this fact is the mostly-balanced nature of an (LL- )orthogonal initialization i.e. given hidden layers of equal dimensions the network is balanced in all layers except the first and last layers (assuming zero attention parameters), which allows the model to train, as opposed to the more severely imbalanced standard Xavier initialization. This further enforces the key takeaway of our work that norm imbalance at initialization hampers the trainability of GATs. In addition, the LLortho+Bal initialization also speeds up training over the LLortho initialization even in cases in which the generalization performance of the model is at par for both initializations. Figure 9: GAT network with varying initialization trained using SGD. Note that identity initializations have also been explored in the context of standard feed-forward neural networks. While they tend to work in practice, the lack of induced feature diversity can be problematic from a theoretical point of view (see e.g. for a counter-example [7]). However, potentially due to the reasons regarding feature diversity and ReLU activation discussed above, the balanced looks-linear random orthogonal initialization (LLortho+Bal.) outperforms initialization with identity matrices (see Fig. 9). In most cases, the balanced versions outperform the imbalanced version of the base initialization and the performance gap becomes exceedingly large as network depth increases. 21Applicability to other MPGNNs As GAT is a generalization of the GCN, all theorems are also applicable to GCNs (where the attention parameters are simply0). We provide additional experiments in Table 7, comparing the performance of GCN models of varying depths with imbalanced and balanced initializations trained on Cora using the SGD optimizer. We use the same hyperparameters as reported for GAT. As expected, the trainability benefits of a balanced initialization are also evident in deeper GCN networks. Table 7: GCN with varying initialization trained using SGD. Dataset Depth Xavier Xavier+Bal LLortho LLortho+Bal Cora 2 77 .8 ± 0.9 80 .5 ± 0.5 78 .0 ± 0.3 80.9 ± 0.4 5 73 .2 ± 3.4 78 .3 ± 0.8 80.3 ± 0.8 79.6 ± 1.0 10 24 .1 ± 4.5 77 .6 ± 2.0 80 .0 ± 1.1 80.0 ± 0.9 20 14 .4 ± 11.2 62 .8 ± 3.6 78 .7 ± 0.5 78.8 ± 1.5 40 13 .4 ± 0.9 65 .9 ± 7.3 28 .3 ± 16.2 77.1 ± 0.9 64 9 .8 ± 5.4 33 .0 ± 13.4 27 .3 ± 12.7 76.7 ± 1.3 80 12 .4 ± 19.3 33 .8 ± 12.9 38 .9 ± 21.3 77.1 ± 1.3 Citeseer 2 66 .6 ± 20.0 71 .3 ± 1.8 66 .0 ± 3.2 72.3 ± 0.9 5 60 .9 ± 12.3 66 .9 ± 15.0 69 .0 ± 6.4 70.1 ± 1.8 10 23 .8 ± 36.8 66 .0 ± 5.9 70.6 ± 0.9 69.8 ± 10.9 20 16 .4 ± 18.2 47 .9 ± 10.0 67 .0 ± 8.6 69.7 ± 4.5 40 13 .9 ± 56.8 37 .3 ± 92.8 44 .8 ± 6.8 64.7 ± 13.6 64 13 .8 ± 41.4 29 .5 ± 15.0 37 .3 ± 79.6 66.3 ± 0.5 80 12 .4 ± 42.7 25 .8 ± 3.6 30 .1 ± 21.8 64.1 ± 3.2 The underlying principle of a balanced network initialization holds in general. However, adapting the balanced initialization to different methods entails modification of the conservation law derived for GATs to correspond to the specific architecture of the other method. For example, the proposed balanced initialization can be applied directly to a more recent variant of GAT, ωGAT [18], for which the same conservation law holds and also benefits from a balanced initialization. We conduct additional experiments to verify this. The results in Fig. 10 follow a similar pattern as for GATs: a balanced orthogonal initialization with looks linear structure (LLortho+Bal) of ωGAT performs the best, particularly by a wide margin at much higher depths (64 and 80 layers). Figure 10: ωGAT network with varying trained on Cora. In this work, we focus our exposition on GATs and take the first step in modeling the training dynamics of attention-based models for graph learning. An intriguing direction for future work is to derive modifications in the conservation law for other attention-based models utilizing the dot-product self-attention mechanism such as SuperGAT [28] and other architectures based on Transformers[47], 22that are widely used in Large Language Models (LLMs) and have recently also been adapted for graph learning [58, 60]. 9 Datasets Summary We used nine benchmark datasets for the transductive node classification task in our experiments, as summarized in Table 8. For the Platenoid datasets (Cora, Citeseer, and Pubmed) [56], we use the graphs provided by Pytorch Geometric (PyG), in which each link is saved as an undirected edge in both directions. However, the number of edges is not exactly double (for example, 5429 edges are reported in the raw Cora dataset, but the PyG dataset has 10556 < (2 × 5429)) edges as duplicate edges have been removed in preprocessing. We also remove isolated nodes in the Citeseer dataset. The WebKB (Cornell, Texas, and Wisconsin), Wikipedia (Squirrel and Chameleon) and Actor datasets [39], are used from the replication package provided by [40], where duplicate edges are removed. All results in this paper are according to the dataset statistics reported in Table 8. Table 8: Summary of datasets used in experiments. Dataset Cora Cite. Pubmed Cham. Squi. Actor Corn. Texas Wisc. # Nodes 2708 3327 19717 2277 5201 7600 183 183 251 # Edges 10556 9104 88648 31371 198353 26659 277 279 450 # Features 1433 3703 500 2325 2089 932 1703 1703 1703 # Classes 7 6 3 5 5 5 5 5 5 # Train 140 120 60 1092 2496 3648 87 87 120 # Validation 500 500 500 729 1664 2432 59 59 80 # Test 1000 1000 1000 456 1041 1520 37 37 51 23(a) L = 10: The balanced initialization allows larger changes in a higher number of parameters in Wl across all layers l with the highest margin in l = 1. The change distribution for the parameters in al is missing for l = 5and l ∈ [7, 10] because these parameters remain unchanged (see Fig.8). (b) L = 20: The same pattern as in L = 10is seen. However, the difference between the trainability of models with unbalanced and balanced initialization becomes even more prominent. No attention parameters change at all with standard Xavier initialization (and hence their change distribution is not visible in the plot). Figure 11: Distributions of the relative and absolute change in the values of feature transformation parameters Wl and attention parameters al, respectively, when trained using SGD with unbalanced (Xav. and Xav+ZeroAtt) and balanced (Xav+Bal and LLortho+Bal) initialization. The black markers in each standard box-whisker plot denote the mean. In general, larger changes occur in attention parameters at later layers closer to the output of the network, whereas feature parameters change more in the earlier layers at the input of the network. We observe this also from the perspective of relative gradients, as higher gradients cause higher parameter changes. 24(a) L = 5: test accuracy (left to right) is 71.6%, 76.2%, 77.3%, and 80.5%. (b) L = 10: test accuracy (left to right) is 63.2%, 68.8%, 71.8%, and 78.0%. Figure 12: Layer-wise relative gradient norms of GAT parameters trained with Adam, which in itself improves trainability over SGD, especially for L = 10(see Fig.4b) even with standard initialization. However, with a balanced initialization, the training is faster and the model also generalizes better, as evident from the significant margin in test accuracy. A general noticeable trend, particularly with balanced initialization, is that the attention parameters have higher gradients at the end of the network (and thus change more in the later layers (see Fig. 11)) whereas the feature parameters have high gradients at the start of the network. Also, it can be seen that the feature parameters begin to change a few epochs before the attention parameters change noticeably, while the attention parameters continue to change for a few more epochs after the gradients of feature parameters have dropped. We speculate a representational benefit could drive this behaviour, i.e. with depth, the learned representation of neighbors becomes increasingly informative for a node and thus leads to a higher activity of the attention mechanism. 25",
      "meta_data": {
        "arxiv_id": "2310.07235v2",
        "authors": [
          "Nimrah Mustafa",
          "Aleksandar Bojchevski",
          "Rebekka Burkholz"
        ],
        "published_date": "2023-10-11T06:53:05Z",
        "pdf_url": "https://arxiv.org/pdf/2310.07235v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This research addresses the trainability issues and performance degradation of Graph Attention Networks (GATs) in deeper architectures when using standard initialization. The main contributions include: 1) deriving a novel conservation law of GAT gradient flow dynamics, which provides a theoretical explanation for why a significant portion of GAT parameters struggle to change during training, especially in deeper networks with standard initialization; 2) proposing a balanced initialization scheme that enables more effective gradient propagation, thereby allowing the successful training of deeper GATs; and 3) demonstrating empirically that this balanced initialization considerably speeds up training and convergence while improving generalization performance, thus establishing a causal link between parameter balancedness and network trainability.",
        "methodology": "The methodology is primarily theoretical and experimental. Theoretically, the authors derive a conservation law for GAT gradient flow dynamics by identifying a multiplicative rescale invariance in GATs, similar to concepts used in traditional deep neural networks. This law relates the scalar product of weights and their gradients, leading to a norm conservation principle for incoming and outgoing weights around a neuron. Based on this, a 'degree of balancedness' is defined, and an initialization is considered balanced if this degree is zero. Experimentally, the paper proposes a balancing procedure: 1) setting attention parameters to zero, and 2) scaling feature weights layer-wise to ensure the norms of incoming and outgoing weights are equal. This balancing procedure is applied on top of both Xavier initialization (BalX) and a novel Looks-Linear Orthogonal initialization (BalO), which aims to achieve initial dynamical isometry for feature weights.",
        "experimental_setup": "The GAT models, primarily GATv2 with ReLU activation, weight sharing, and no biases, were evaluated on nine benchmark datasets for semi-supervised node classification: Cora, Citeseer, Pubmed, Actor, Chameleon, Cornell, Squirrel, Texas, and Wisconsin. Experiments were conducted using the Pytorch Geometric framework on Nvidia T4 or RTX 3060 GPUs. Training involved SGD and Adam optimizers for up to 5000 epochs, with learning rates adapted for different depths and datasets. Performance was measured by mean test accuracy and epochs to the best model (convergence speed-up), with 95% confidence intervals over five runs. The study compared standard Xavier initialization against balanced Xavier (BalX) and balanced Looks-Linear Orthogonal (BalO) initializations, and included ablation studies like Xavier with zero attention (XavZ). Additional experiments covered architectural variations (ELU, multi-headed attention, no weight sharing, dropout, weight decay), comparisons with Lipschitz normalization, and applicability to GCNs.",
        "limitations": "The derived conservation law is specific to the self-attention mechanism in the original GAT, GATv2, and ωGAT models; it requires modification for other types of self-attention, such as dot-product self-attention. The theoretical framework assumes positively homogeneous activation functions, which means non-homogeneous functions like ELU can negatively impact the specialized orthogonal initialization (BalO), although Adam might compensate. Practical training with finite learning rates and optimizers like Adam only approximately adheres to the conservation law. Moreover, for some datasets, deeper models with Xavier initialization fail to train entirely, leading to high result variability, and even with balanced initialization, very deep models (e.g., L=40) might not fully converge within the set epoch limit. The study also acknowledges that for heterophilic datasets, specialized GNN models (not using attention) might achieve better performance, but comparison was not the focus.",
        "future_research_directions": "Future research directions include extending the study of learning dynamics to other positive homogeneous models with attention mechanisms, particularly Transformers and Vision Transformers. Further work could involve deriving modifications to the conservation law for different self-attention mechanisms, such as dot-product self-attention in models like SuperGAT and Transformer-based graph learning architectures. Another promising area is exploring how dynamical isometry can be achieved or approximated in general GNNs, building on the observation that the current LL-orthogonal technique doesn't induce perfect isometry in GATs. The paper also suggests investigating the role of overparameterization in GNNs to understand how increased width might aid generalization in deeper models."
      }
    },
    {
      "title": "GBA: A Tuning-free Approach to Switch between Synchronous and Asynchronous Training for Recommendation Models",
      "abstract": "High-concurrency asynchronous training upon parameter server (PS)\narchitecture and high-performance synchronous training upon all-reduce (AR)\narchitecture are the most commonly deployed distributed training modes for\nrecommendation models. Although synchronous AR training is designed to have\nhigher training efficiency, asynchronous PS training would be a better choice\nfor training speed when there are stragglers (slow workers) in the shared\ncluster, especially under limited computing resources. An ideal way to take\nfull advantage of these two training modes is to switch between them upon the\ncluster status. However, switching training modes often requires tuning\nhyper-parameters, which is extremely time- and resource-consuming. We find two\nobstacles to a tuning-free approach: the different distribution of the gradient\nvalues and the stale gradients from the stragglers. This paper proposes Global\nBatch gradients Aggregation (GBA) over PS, which aggregates and applies\ngradients with the same global batch size as the synchronous training. A\ntoken-control process is implemented to assemble the gradients and decay the\ngradients with severe staleness. We provide the convergence analysis to reveal\nthat GBA has comparable convergence properties with the synchronous training,\nand demonstrate the robustness of GBA the recommendation models against the\ngradient staleness. Experiments on three industrial-scale recommendation tasks\nshow that GBA is an effective tuning-free approach for switching. Compared to\nthe state-of-the-art derived asynchronous training, GBA achieves up to 0.2%\nimprovement on the AUC metric, which is significant for the recommendation\nmodels. Meanwhile, under the strained hardware resource, GBA speeds up at least\n2.4x compared to synchronous training.",
      "full_text": "GBA: A Tuning-free Approach to Switch between Synchronous and Asynchronous Training for Recommendation Models Wenbo Su∗, Yuanxing Zhang∗, Yufeng Cai, Kaixu Ren, Pengjie Wang, Huimin Yi, Yue Song, Jing Chen1, Hongbo Deng, Jian Xu, Lin Qu1, Bo Zheng† Alibaba Group {vincent.swb, yuanxing.zyx, baike.cyf, kaixu.rkx, pengjie.wpj, huimin.yhm, yue.song, dhb167148, xiyu.xj, bozheng}@alibaba-inc.com 1{gongcheng.cj, xide.ql}@taobao.com Abstract High-concurrency asynchronous training upon parameter server (PS) architecture and high-performance synchronous training upon all-reduce (AR) architecture are the most commonly deployed distributed training modes for recommendation models. Although synchronous AR training is designed to have higher training efﬁciency, asynchronous PS training would be a better choice for training speed when there are stragglers (slow workers) in the shared cluster, especially under limited computing resources. An ideal way to take full advantage of these two training modes is to switch between them upon the cluster status. However, switch- ing training modes often requires tuning hyper-parameters, which is extremely time- and resource-consuming. We ﬁnd two obstacles to a tuning-free approach: the different distribution of the gradient values and the stale gradients from the stragglers. This paper proposes Global Batch gradients Aggregation (GBA) over PS, which aggregates and applies gradients with the same global batch size as the synchronous training. A token-control process is implemented to assemble the gradients and decay the gradients with severe staleness. We provide the conver- gence analysis to reveal that GBA has comparable convergence properties with the synchronous training, and demonstrate the robustness of GBA the recommenda- tion models against the gradient staleness. Experiments on three industrial-scale recommendation tasks show that GBA is an effective tuning-free approach for switching. Compared to the state-of-the-art derived asynchronous training, GBA achieves up to 0.2% improvement on the AUC metric, which is signiﬁcant for the recommendation models. Meanwhile, under the strained hardware resource, GBA speeds up at least 2.4x compared to synchronous training. 1 Introduction Nowadays, recommendation models with a large volume of parameters and high computational complexity have become the mainstream in the deep learning communities [12]. Accelerating the training of these recommendation models is a trending issue, and recently synchronous training upon high-performance computing (HPC) has dominated the training speed records [ 16, 15, 29]. The resource requirements of the synchronous training upon AR are more rigorous than the asynchronous ∗∗These authors contributed equally to this work. ††Corresponding author 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2205.11048v2  [cs.LG]  9 Oct 2022training upon PS [1]. In a shared training cluster with dynamic status [2], the synchronous training would be retarded by a few straggling workers. Thus, its training speed may be even much slower than the high-concurrency asynchronous training. Should it be possible to switch the training mode according to the cluster status, we will have access to making full use of the limited hardware resources. Switching the training mode for a speciﬁc model usually demands tuning of the hyper-parameters for guarantees of accuracy. Re-tuning the hyper-parameters is common in the one-shot training workloads (e.g., the general CV or NLP workloads) [18]. However, it is not applicable for the continual learning or the lifelong training of the recommendation models [9], as tuning would be highly time- and resource-consuming. When switching the training mode of representative recommendation models, we confront three main challenges from our shared cluster: 1) Model accuracy may suffer from a sudden drop after switching, requiring the model to be retrained on a large amount of data to reach the comparable accuracy before switching; 2) The distribution of gradient values is different between synchronous training and asynchronous training, making the models under two training modes difﬁcult to reach the same accuracy by tuning the hyper-parameters;3) The cluster status imposes staleness on the asynchronous training, and staleness negatively impacts the aggregation of gradients, especially for the dense parameters. We conduct a systematic investigation of the training workloads of recommendation models to tackle the above challenges. It is found that when the global batch size (i.e., the actual batch size of gradient aggregation) is the same, the distribution of gradient values of asynchronous training tends to be similar to that of synchronous training. Besides, we notice that due to the high sparsity, the embedding parameters in recommendation models are less frequently updated than the dense parameters, leading to a stronger tolerance for staleness than the general CV or NLP deep learning models. Based on these insights, we propose Global Batch gradients Aggregation (GBA), which ensures the model keeps the same global batch size when switched between the synchronous and asynchronous training. GBA is implemented by a token-control mechanism, which resorts to bounding the staleness and making gradient aggregation [11]. The mechanism suppresses the staleness following a staleness decay strategy over the token index. The faster nodes would take more tokens without waiting, and thereby GBA trains as fast as the asynchronous mode. Furthermore, the convergence analysis shows that GBA has comparable convergence properties with the synchronous mode, even under high staleness for recommendation models. We conduct an extensive evaluation on three continual learning of recommendation tasks. The results reveal that GBA performs well on both accuracy and efﬁciency with the same hyper-parameters. Particularly, GBA improves the AUC metric by 0.2% on average compared to the state-of-the-art training modes of asynchronous training. Besides, GBA presents at least 2.4x speedup over the synchronous AR training in the cluster with strained hardware resources. To the best of our knowledge, this is the ﬁrst work to approach switching between synchronous and asynchronous training without tuning the hyper-parameters. GBA has been deployed in our shared training cluster. The tuning-free switching enables our users to dynamically change the training modes between GBA and the synchronous HPC training for the continual learning tasks. The overall training efﬁciency of these training workloads is thereby signiﬁcantly improved, and the hardware utilization within the cluster is also raised by a large margin. 2 Related Work Distributed training mode. PS [19] and AR [16] are two mainstream architectures for the training workloads of recommendation models, accompanied by the asynchronous training and synchronous training, respectively. Researchers are enthusiastic about the pipeline, communication, and compu- tation optimization for the AR architecture of recommender systems [29]. Meanwhile, to improve the training efﬁciency of the PS architecture, researchers propose a category of semi-synchronous training mode [11]. For example, Hop-BS [ 22] restricts the gradient updates under the bounded staleness, and Hop-BW [ 22] ignores the gradients from the stragglers with the well-shufﬂed and redundancy data. Recently, a category of decentralized training has been proposed in many studies to scale out the AR architecture. Local all-reduce [23], local update [24], exponential graph [26], and many topology-aware solutions have proven promising in the NLP and CV tasks. However, owing to the sparsity in recommendation models, the inconsistent parameters among workers and the dropped gradients of the scarce IDs would intolerably degrade the accuracy. Besides, these training modes 20:00 12:00 23:00 Time in a day 0 0.5 1.0Normalized value CPU Util. Sync. Async. Prague SwarmAdam Figure 1: Normalized QPS of four training modes in training YouTubeDNN models in a shared cluster, with CPU utilization in a day. 0.8043DeepFMYouTubeDNN AUC Training progress Switching 0.7680 Figure 2: The AUC on the validation set of Criteo-4GB and Private by every 5% progress during training, switching at 50% progress. hardly consider the requirements to switch to another training mode according to the cluster status, though switching is beneﬁcial to improve the training efﬁciency in the shared training clusters. Staleness and noisy gradients. The indeterminate or even inferior model accuracy of asynchronous training is mainly attributed to the staleness [6] and the noisy gradients [25] caused by the small batch. Although prior research has pointed out that the converged giant model is less sensitive to staleness [7], staleness is still a negative factor in the accuracy of the continual recommendation training. Many efforts have been put into controlling staleness via Taylor expansion [30], weighted penalty [33], etc. Recent works present a large-batch training paradigm with specially-designed optimizers to scale the gradients before updating [27], and point out that it can reach the best accuracy by merely adjusting batch size [10, 28]. There are also attempts to change gradient aggregation strategies during the asynchronous training to achieve stable model accuracy [20]. GBA generalizes the staleness control paradigm to the recommendation workloads by token-control mechanism, which ﬁnds the balance between bounding staleness and ignoring gradients. GBA runs with the same global batch size as the synchronous mode, ensuring the effective switching between GBA and synchronous training without tuning hyper-parameters. 3 Preliminaries 3.1 Distributed Training of Recommendation Models Recommendation models usually comprise two modules: the sparse module contains the embedding layers with the embedding parameters, mapping the categorical IDs into numerical space; the dense module contains the computational blocks with the dense parameters, such as attention and MLP [3, 31], to exploit the feature interactions. The main difference between the two kinds of parameters is the occurrence ratio in each training batch. Each training batch needs all the dense parameters, yet only a tiny amount of embedding parameters are required according to the feature IDs in the data shard. The latest development of recommendation models introduces high complexity and a large volume of parameters, making distributed training essential to improve training efﬁciency. The synchronous HPC training mode usually adopts the AR architecture, where the dense parameters are replicated, and the embedding parameters are partitioned on each worker. HPC should be deployed by monopolizing a few high-performance workers and making full use of the associated resources, which may be retarded by the slow workers [17]. PS architecture is usually coupled with asynchronous high concurrency training where the parameters are placed on PSs, and the workers are responsible for the computation. On the one hand, the high concurrency mechanism activates the fragmentary resources in the training cluster by deploying hundreds of workers. On the other hand, the asynchronous training brings in gradient staleness, which occurs when the gradient is calculated based on the parameters of an old version and applied to the parameters of a new version. 3.2 Observations and Insights within a Shared Training Cluster We investigate the training workloads of recommendation models from a shared training cluster to observe the obstacles and necessities of switching training modes. 33 6 9 12 L2-norm of Gradients ( ×10−5 ) 0 0.12 0.25Proportion Sync. BSP-1K BSP-2K BSP-4K BSP-6K Figure 3: The distribution of L2-norm of gra- dients from the synchronous training and BSP with various size of aggregation. 1 20 50 80 Occurences per 100 batches 10 −7 10 −4 10 −1 Proportion Figure 4: The skewed distribution of ID occur- rences across batches, reﬂecting the frequency that an ID gets updated. Observation 1: Cluster status determines the performance of training modes. Figure 1 illus- trates the average CPU utilization within a real shared cluster, and the corresponding samples/queries per second (QPS) of a YouTubeDNN [ 5] model by the synchronous and asynchronous training mode. The utilization and QPS are normalized by their maximal value, respectively. When the cluster is relatively vacant, models trained in the synchronous mode can fully occupy the hardware resources, satisfying HPC conditions and presenting high efﬁciency. When there are plenty of hetero- geneous workloads in the cluster, slow workers dominate the training speed, making the asynchronous training mode run much faster than the synchronous mode. We also implement two approaches of local all-reduce3. Since the status of each device in the cluster is constantly changing, the local all-reduce-based mode would not work well when confronting resource shortages. Observation 2: Directly switching training mode brings sudden drop on accuracy. We run DeepFM [21] over Criteo-4GB [14] (few parameters, fast convergence) and YouTubeDNN on Private dataset (trillions of parameters, slow convergence) in the shared cluster. We tune the hyper-parameters from scratch for the best model accuracy of both asynchronous and synchronous mode, and denote the two sets of hyper-parameters as set A and set S, respectively. After training in one training mode, we evaluate the tendency of the training AUC after switching to the other training mode with setA or set S. Figure 2 illustrates that after switching from the synchronous mode to the asynchronous mode, the AUC encounters sudden drop and even decreases to 0.5. The AUC drop also appears in the opposite-side switching, indicating that this condition is irrelevant to whether the model had been converged. These observations imply that directly switching the training mode requires heavy effort in re-tuning the hyper-parameter. Inherently, training modes would lead to different convergence or minima owing to the difference in batch size, learning rate and many other factors, which have already received in-depth theoretical research [? ? ]. We provide theoretical analysis to explain the sudden drop in Appendix D. We then probe into the insights from asynchronously training recommendation models. Insight 1: Distribution of the gradient values is related to the aggregated batch size.We attempt to investigate the reason for observation 2 from the gradient aspect. We implement asynchronous bulk synchronous parallel (BSP) on the YouTubeDNN recommendation task, which asynchronously aggregates K gradients from workers before applying the values to the parameters. Here, we set K to 100, the same as the number of workers. Besides, we compare the synchronous training in 6.4K local batch size (64 workers). Figure 3 plots the distribution of the L2-norm of gradient values from the synchronous training and BSP with various local batch sizes. It is evident that the batch size determines the mean and variance of the distribution. The distribution of BSP resembles synchronous training when the aggregation size is similar (i.e., BSP-4K). The result suggests that the same aggregation size could lead to a similar distribution of gradient values. However, there is still a gap in model accuracy after equalizing the global batch size between the asynchronous training and the synchronous training, mainly induced by gradient staleness. Insight 2: The gradient staleness imposes different impact on the embedding parameters and the dense parameters. Due to the skewed distribution, most IDs would merely appear in a small number of batches, as depicted in Fig. 4. It means that in the recommendation models, only a tiny portion of IDs would be involved in every single batch, and the embedding parameters are less 3SwarmAdam is a variant of SwarmSGD [24] with Adam optimizer. It is uncommon to use SwarmAdam and Prague [23] in recommendation models as they may lead to accuracy loss which is not tolerable in the business. 4PS Worker 𝟎 Worker 𝑵𝒂 −𝟏 Worker 𝟐 Worker 𝟏 PS …… 2 1 1 … 2 ∇ ∇ ∇ … ∇ Gradient Buffer 𝑀 −10 token gradient (partition) 1 2 … 2 1 1 … 2 ∇ ∇ ∇ … ∇ Gradient Buffer 𝑀 −10 token 1 2 … 0 0 … 0 1 1 … 1 2 2 … 2 … 𝐝0 𝐝1 𝐝2 … 𝐝𝑄−1 Token List (Size 𝑄) Data List (Size 𝑄) worker sends token  and gradient to PSdata and token are  sent to worker  𝑀 gradients in gradient buffer would be aggregated Size 𝑀 Size 𝑀 Size 𝑀 Dense Parameters Staleness 2 ∇ 1 ∇ 1 ∇ … … 2 ∇ … Embedding Parameters Non-UpdateUpdate gradient (partition) token gradient Figure 5: Illustration of the token-control mechanism in GBA: everyMgradients would be aggregated in the buffer before the PSs apply them to the parameters; workers report gradients to the PSs along with a token indicating the degree of data staleness. frequently updated than the dense parameters. Therefore, the embedding parameters tend to be more robust on the gradient staleness than the dense parameters (for example, considering a worker in training, there could be ﬁve updates for the dense parameters, yet only two updates for the embedding of a speciﬁc ID). 4 Global Batch based Gradients Aggregation 4.1 Training Recommendation Models with GBA Switching the distributed training mode for recommendation models should get rid of tuning the hyper-parameters. We introduce the concept of global batch size, which is deﬁned as the actual batch size when gradients are aggregated and applied, and propose GBA for the tuning-free switching. We denote the local batch size, i.e., the actual batch size on each worker and the number of workers, as Bs and Ns for the synchronous training, Ba and Na for the asynchronous training. Then, the global batch size in synchronous training, denoted by Gs, can be calculated as Bs ×Ns. Following Insight 1, GBA remains the global batch size unchanged when we switch the distributed training model from synchronous training to asynchronous training. For each step, all the dense parameters would be updated, and only a small number of embedding parameters would be updated. Then the dense parameters and embedding parameters obtain different gradient staleness during training. Hence, we deﬁne the data staleness as the uniﬁed staleness in training recommendation models. The data staleness describes the gap between the global step when the worker begins to ingest a data batch and the global step when the calculated gradient is applied. Obviously, the data staleness in the synchronous training mode is constantly zero. Based on data staleness, we implement GBA by a token-control mechanism on the PS architecture to cope with the sparsity and the dynamic cluster status. Figure 5 illustrates the architecture of the proposed token-control mechanism. Over the canonical PS, we prepare a queue called data list to arrange the data (addresses) by batches. Given a dataset D, suppose we can split it into Qbatches of size Ba, denoted by D= (d0,d1,..., dQ−1). Meanwhile, we establish another queue called token list to yield the token of each individual batch. The token list contains Qtokens, denoted by (t0,t1,...,t Q−1), each attached to one batch in the data list to indicate the global step when this batch is sent to a worker. The token value starts from zero, and each token value repeats M times in the token list. Here, M is the number of batches we use to aggregate gradients. Under this setting, we can deduce that there will be K = ⌈Q M⌉gradient updates during the training. Then we set ti = ⌊i K⌋,∀i∈{0,1,...,Q −1}to ensure that the token list yields the token value in ascending order. Apart from the two queues, we also employ a gradient buffer to receive the gradients calculated by the workers with the corresponding tokens of the gradients. To be consistent with the tokens, the capacity of the gradient buffer is set to M, and therefore the PSs would aggregate M gradients before applying them to the variables. Note that each PS maintains an individual gradient buffer to deal with the gradients of the corresponding partitions of the variables. 5During the training process, a worker would pull the parameters from PS, a batch from the data list, and a token from the token list simultaneously before ingesting the data and computing the gradient locally. When a worker completes calculating the gradient of a batch, the gradient and the corresponding token are sent to the gradient buffer on PS. Then, the worker immediately proceeds to work on the next batch. In this way, the fast workers can keep working without waiting for the slow ones. When the gradient buffer reaches the capacity of M pairs of gradients and tokens, all the gradients are aggregated to apply once, and at the same time, the buffer will be cleared. This is what we call ﬁnishing a global step, and thereby the global batch size in GBA can be calculated as Ga = Ba×M. According to the design, we aim to keep the global batch size consistent in switching, that is, Gs = Ga. Hence, we can set the size of the gradient buffer to be M = Bs×Ns Ba . We would use M workers in GBA, i.e., Na = M, to avoid the intrinsic gradient staleness led by the inconsistency between the number of workers and the number of batches to aggregate. At the update of global step k, denote τ(m,k) the m-th token in the gradient buffer. When we aggregate the gradients in the gradient buffer, we shall decay the gradients that suffer from severe staleness. GBA could employ different staleness decay strategies to mitigate the negative impact from the staleness according to the token index, and in this work we deﬁne it as: f(τ(m,k),k) = {0, k −τ(m,k) >ι 1, k −τ(m,k) ≤ι, (1) where ιis the threshold of tolerance. If f(τ(m,k),k) = 0, we exclude the m-th gradient in the buffer due to the severe staleness; otherwise, we aggregate the gradient. As we can see, tokens help identify whether the corresponding gradients are stale and how many stale steps are behind the current global step. In this case, although the token is designed over the data staleness, the negative impact from the canonical gradient staleness can also be mitigated. 4.2 Convergence Analysis We have seen much research on the convergence analysis of the synchronous and asynchronous training. Following the assumptions and convergence analysis in Dutta et al. [6], the expectation of error after ksteps of gradient updates in the synchronous training can be deduced by: E[F(wk)] −F∗≤ ηLσ2 2cNsBs + (1 −ηc)k(F(w0) −F∗− ηLσ2 2cNsBs ), (2) where wk denotes the parameter in step k, ηdenotes learning rate, Lis the Lipschitz constant and σ denotes the variance of gradients. F(w) is the empirical risk function that is strongly convex with parameter c. E[F(wk)] −F∗is the expected gap of the risk function from its optimal value, used as the error after ksteps. As mentioned in Eqn. (2), The ﬁrst term in the right, i.e. ηLσ2 2c(NsBs) , would be the error ﬂoor, and (1 −ηc) is the decay rate. The proposed GBA is derived upon the asynchronous gradient aggregation. We assume that, for some γ ≤1, γ ≥ζE[||∇F(wk) −∇F(wτ(m,k))||2 2] E[||∇F(wk)||2 2] . (3) Here, γis a measure of gradients impact induced by the staleness; smaller value of γindicates that staleness makes less accuracy deterioration of the model. Besides, ζindicates the average probability that any parameter in the model would be both updated in step kand step τ(m,k). Intuitively, ζ would be far below 1 in the recommendation models due to the strong sparsity. Then, the error of GBA after ksteps of aggregated updates would become (Appendix A presents the proof): E[F(wk)] −F∗≤ ηLσ2 2cγ′MBa + (1 −ηγ′c)k(E[F(w0)] −F∗− ηLσ2 2cγ′MBa ), (4) where γ′= 1 −γ+ p0 2 and p0 is a lower bound on the conditional probability that the token equals to the global step, i.e., τ(m,k) = k. Equation (4) proves the convergence of GBA. Considering the error ﬂoors of Eqn. (2) and Eqn. (4), M×Ba should be set close to Ns×Bs to make GBA tuning-free. It is exactly the global batch size we use in GBA, consistent with our main idea of keeping global batch size unchanged. Recall that with the embedding parameters, ζ <1 makes γlower than the training of general CV or NLP models. Consequently, the error ﬂoor remains low in GBA. 6(d) Criteo (to Sync.) (e) Alimama (to Sync.) (f) Private (to Sync.) Test day (a) Criteo (from Sync.) (b) Alimama (from Sync.) (c) Private (from Sync.)Test day 0.785 0.644 0.653 0.777 0.653 0.636 0.770 1stdayLast dayAverageSync.+0.0011-0.0002+0.0002Hop-BW-0.0012-0.0046-0.0025Hop-BS-0.0015-0.0979-0.0716BSP-0.0017-0.0045-0.0034Async.-0.1513-0.1542-0.1518 1stdayLast dayAverageSync.+0.0011+0.0001+0.0002Hop-BW-0.0060-0.0021-0.0036Hop-BS-0.0018-0.0005-0.0009BSP-0.0079-0.0021-0.0040Async.-0.0080-0.0980-0.0875 (g) Diff. from GBA (from Sync.) (h) Diff. from GBA (to Sync.) Figure 6: The AUC tendencies on the test days of the three datasets after inheriting a base model: (a-c) from the synchronous training modes and switching to the compared training modes; (d-f) from the compared training modes and switching to the synchronous training modes; (g-h) AUC difference between GBA and the other training modes after switching from/to synchronous training. 5 Evaluation 5.1 Settings Table 5.1: Settings of the three continual recommendation tasks by the compared training modes. Task Modeldescription Data partsSampleper dayOptimizer Learningrate # of workersLocal batchsize Privatehyper-param. Criteo(DeepFM) 19M(x40K) FLOPS45B params.16 avg. dim. 12 days (base)11 days (eval)190M Adagrad (Async.)Adam (Others)0.006 (Async.)0.0011 (Others)32 (Sync.)100 (Others) 5K (Async.)12.8K (GBA)40K (Others) Hop-BS (b1=2)BSP (b2=20)Hop-BW (b3=20)GBA (ι=3) Alimama(DIEN) 112M(x3K) FLOPS160B params.19 avg. dim. 5 days (base)3 days (eval)90M Adagrad (Async.)Adam (Others)0.008 (Async.)0.0015 (Others)32 (Sync.)128 (Others) 1K (Async.)0.75K (GBA)3K (Others) Hop-BS (b1=2)BSP (b2=20)Hop-BW (b3=20)GBA (ι=4) Private(YouTubeDNN) 746M(x6.4K) FLOPS1.9T params.24 avg. dim. 14 days (base)8 days (eval)2B Adagrad (Async.)Adam (Others)0.001 (Async.)0.0006 (Others)64 (Sync.)400 (Others) 1K (Async.)1K (GBA)6.4K (Others) Hop-BS (b1=2)BSP (b2=50)Hop-BW (b3=100)GBA (ι=4) We conduct systematical evaluations to examine the performance of GBA and make a ﬁne-grained analysis. The evaluations involve three industrial-scale recommendation tasks: 1) On the Criteo-1TB dataset [13], we implement DeepFM, where the hyper-parameters on Criteo-4GB (AUC 0.8043) are utilized; 2) On the Alimama dataset [ 8], we implement DIEN [32] and use the recommended hyper-parameters in the original design; 3) On the Private dataset, we implement YouTubeDNN, and we tune the best hyper-parameters. The models are implemented in DeepRec [4] with the expandable HashTables. Detailed information on the dataset and the models are listed in Tab. 5.1. We imitate the continual training without changing the hyper-parameters to the models, and ensure a similar cluster status for all evaluations. Inheriting from a pre-trained checkpoint, we repeatedly train on the data of every day and evaluate the data of the subsequent day. The training cluster is equipped with a Tesla-V100S GPU and Skylake CPU. We focus on AUC as the accuracy metric and global/local QPS (QPS of all/single workers) as the efﬁciency metric. We select several state-of-the-art PS-based training modes: Bounded staleness (Hop-BS) restricts the maximal differences of gradient version between the fastest and the slowest workers, controlled by b1; Bulk synchronous parallel (BSP) aggregates a pre-set number b2 of gradients when applying gradients to the parameters, regardless of the gradient version; Backup worker (Hop-BW) ignores the pre-set number b3 of gradients from the slowest workers during each gradient aggregation. We enumerate the specialized hyper-parameters of each training mode and record the statistics when reaching its best AUC. 7Table 5.2: Global QPS of the compared training mode on the three tasks. Sync. Async. Hop-BS BSP Hop-BW GBA Criteo 1,436K(±224K) 3,253K(±84K) 2,227K(±336K) 3,247K(±93K) 2,559K(±294K) 3,240K(±97K) Alimama182K(±52K) 403K( ±33K) 217K(±65K) 403K( ±33K) 288K( ±48K) 399K(±35K) Private 43K(±21K) 90K( ±15K) 29K(±11K) 88K( ±17K) 66K( ±24K) 87K(±19K) Table 5.3: Fine-grained analysis between GBA and the other training modes. Local QPS AUC # of drop Avg. grad. staleness (max) Async. GBA Sync. GBA Hop-BW GBA Hop-BS GBA BSP 78K(±23K) 74K(±25K) 0.7864 0.7864 300K 1,454 0.06 (2) 0.21 (11) 2.61 (12) 90K(±15K) 87K(±19K) 0.7864 0.7866 300K 898 0.04 (2) 0.15 (11) 1.92 (12) 99K(±12K) 98K(±12K) 0.7864 0.7865 300K 786 0.03 (2) 0.12 (9) 1.62 (10) 5.2 Performance of Training Modes We ﬁrst examine the performance of GBA. Figure 6(a-c) records the AUC tendencies after switching from synchronous to the other training modes over the three recommendation tasks. We mainly focus on the AUC at the ﬁrst day and the last day, as well as the average AUC scores throughout the datasets. Although Hop-BW eliminates staleness, the ignorance of a large volume of data makes it perform the worst (also taken as evidence why we tend not to use local all-reduce in training recommender systems). The manipulation of global batch size contributes to the best performance on both sides of switching. Compared to the best baselines (i.e., Hop-BW), GBA improves AUC by at least 0.2% on average over the three datasets, which has the potential to increase by 1% revenue in the real-world business. Meanwhile, after switching from synchronous training, GBA obtains immediate good accuracy (AUC at the ﬁrst day), while there are explicit re-convergence on the other training modes, as depicted in Figure 6(g). Figure 6(d-f,h) illustrates the AUCs of these training modes after switching to synchronous train- ing.We can see that GBA tends to obtain at least equal accuracy to the continuous synchronous training without switching. On the contrary, the models inherited from the other baselines require consuming more data to reach the desired accuracy of the synchronous training. The tendency of the AUC gaps between the synchronous training and the compared training modes reﬂects that the parameters trained by GBA are the most compatible with the synchronous training. It veriﬁes that switching from GBA to synchronous training is also tuning-free. We collect metrics of the training efﬁciency during the above experiments, and report their global QPS in Tab. 5.2. The results reveal that GBA performs similarly to the asynchronous training. Although Hop-BS works better than BSP and Hop-BW in accuracy, it struggles to deal with the slow workers. It indicates that when facing a resource shortage in the shared cluster, GBA can provide similar accuracy with synchronous training mode, while running as fast as the asynchronous training mode. 5.3 Fine-grained Analysis We further probe into the performance of GBA. Here, we take the recommendation task on the Private dataset (the most complex model) as an example, switching from the synchronous mode to GBA. We ﬁrst analyze how the cluster status affects the performance of GBA. The experiments are repeated in the cluster during different periods of a day. We collect AUC, QPS, average gradient staleness on the dense parameters (for fair comparison among the baselines), and the number of excluded batches, as shown in Tab. 5.3. From the results, we can infer that GBA properly ﬁnds the balance between the staleness and the excluded data (as deﬁned in Eqn. (1)), i.e., excluding fewer data compared to Hop-BW and suppressing the staleness to the same level of Hop-BS. GBA also shows strong robustness on the dynamic cluster status, obtaining stable performance on AUC. We then examine the impact of the batch sizes in GBA. Figure 7 depicts the average AUC score and the global QPS when we modify the local batch size (the number of workers is thereby changed) and keep the global batch size unchanged. Considering the hardware limitation on worker and the communication overhead on PS, we vary the number of workers from 100 to 800. We can see a steady state of the AUC score (i.e., absolute difference less than 10−4), while the training achieves 8100 200 400 800 # of workers 0.7865 0.7866AUC 25 86 152 Global QPS / K Figure 7: The average AUC on the 8-day test sets and the training efﬁciency via GBA, varying the number of workers while maintaining the global batch size. 250 500 1K 2K 4K Local batch size 0.78 0.79AUC Figure 8: The range of AUC on the 8- day test sets via GBA of 400 workers, varying the local batch size. a signiﬁcant efﬁciency boost when using more workers. It can thereby be inferred that GBA has a good capability of scaling out. Besides, we ﬁx the number of workers to 400 and change the local batch size for each worker, which means that the global batch size would differ. As shown in Fig. 8, the inconsistent global batch size with the synchronous training makes the training encounter lower AUC scores after switching. Although the larger global batch size may have the potential to achieve better accuracy (i.e., owing to the stable and accurate gradient), the experiment indicates the model would hardly reach its best accuracy without tuning. These results verify that using the same global batch size in GBA as in the synchronous training is necessary to get rid of tuning when switching the training mode. 6 Conclusion A tuning-free switching approach is demanded to take full advantage of the synchronous and asynchronous training, which can improve the training efﬁciency in the shared cluster. We raise insights from the investigation over the production training workloads that the inconsistent batch size and the gradient staleness are two main reasons to fail the switching regarding the model accuracy. Then GBA training mode is proposed for asynchronously training recommendation models via aggregating gradients by the global batch size. GBA enables switching between synchronous training and asynchronous training of the continual learning tasks with the accuracy and efﬁciency guarantees. With GBA, users can freely switch the training modes according to the status of the shared training clusters, without tuning hyper-parameters. GBA is implemented through a token-control mechanism to ensure that the faster worker should contribute more gradients to the aggregation while the gradients from the straggling workers would be decayed. Evaluations of three representative continual training tasks of recommender systems reveal that GBA achieves similar accuracy with the synchronous training, while resembling the efﬁciency of the canonical asynchronous training. Currently, GBA requires the users to select the training mode according to their own judgment on the cluster status. In the future, we will attempt to make GBA be adaptive to the cluster status. The guidelines of automatic switching would be derived from more analyses upon the training trace logs. It could be formulated as an optimization problem under many control factors including but not limited to the overall QPS, training cost, and task scheduling with priority. References [1] B. Acun, M. Murphy, X. Wang, J. Nie, C.-J. Wu, and K. Hazelwood. Understanding training efﬁciency of deep learning recommendation models at scale. In 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA), pages 802–814. IEEE, 2021. [2] Y . Chen, J. Wang, Y . Lu, Y . Han, Z. Lv, X. Min, H. Cai, W. Zhang, H. Fan, C. Li, et al. Fangorn: adaptive execution framework for heterogeneous workloads on shared clusters. Proceedings of the VLDB Endowment, 14(12):2972–2985, 2021. [3] H.-T. Cheng, L. Koc, J. Harmsen, T. Shaked, T. Chandra, H. Aradhye, G. Anderson, G. Corrado, W. Chai, M. Ispir, et al. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems, pages 7–10, 2016. [4] D. community. Deeprec. https://github.com/alibaba/DeepRec 2022.5.11. 9[5] P. Covington, J. Adams, and E. Sargin. Deep neural networks for youtube recommendations. In Proceedings of the 10th ACM conference on recommender systems, pages 191–198, 2016. [6] S. Dutta, G. Joshi, S. Ghosh, P. Dube, and P. Nagpurkar. Slow and stale gradients can win the race: Error-runtime trade-offs in distributed SGD. In International Conference on Artiﬁcial Intelligence and Statistics, pages 803–812. PMLR, 2018. [7] S. Eliad, I. Hakimi, A. De Jagger, M. Silberstein, and A. Schuster. Fine-tuning giant neural networks on commodity hardware with automatic pipeline model parallelism. In 2021 USENIX Annual Technical Conference (USENIX ATC 21), pages 381–396, 2021. [8] A. Group. Ad display/click data on taobao.com. https://tianchi.aliyun.com/dataset/ dataDetail?dataId=56&lang=en-us under CC BY-NC-SA 4.0, visited on 2022.5.11. [9] Y . Guo, M. Liu, T. Yang, and T. Rosing. Improved schemes for episodic memory-based lifelong learning. Advances in Neural Information Processing Systems, 33:1023–1035, 2020. [10] F. He, T. Liu, and D. Tao. Control batch size and learning rate to generalize well: Theoretical and empirical evidence. Advances in Neural Information Processing Systems, 32, 2019. [11] Q. Ho, J. Cipar, H. Cui, S. Lee, J. K. Kim, P. B. Gibbons, G. A. Gibson, G. Ganger, and E. P. Xing. More effective distributed ml via a stale synchronous parallel parameter server. Advances in neural information processing systems, 26, 2013. [12] J. Hron, K. Krauth, M. Jordan, and N. Kilbertus. On component interactions in two-stage recommender systems. Advances in Neural Information Processing Systems, 34, 2021. [13] C. Inc. Criteo 1tb click logs dataset. https://ailab.criteo.com/ download-criteo-1tb-click-logs-dataset/ visited on 2022.5.11, . [14] C. Inc. Kaggle display advertising challenge dataset. http://labs.criteo.com/2014/02/ kaggle-display-advertising-challenge-dataset/ visited on 2020.1.10, . [15] Y . Jiang, Y . Zhu, C. Lan, B. Yi, Y . Cui, and C. Guo. A uniﬁed architecture for accelerating distributed {DNN}training in heterogeneous{GPU/CPU}clusters. In 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20), pages 463–479, 2020. [16] S. Kim, G.-I. Yu, H. Park, S. Cho, E. Jeong, H. Ha, S. Lee, J. S. Jeong, and B.-G. Chun. Parallax: Sparsity-aware data parallel training of deep neural networks. In Proceedings of the Fourteenth EuroSys Conference 2019, pages 1–15, 2019. [17] A. Kumar, A. Beutel, Q. Ho, and E. Xing. Fugue: Slow-worker-agnostic distributed learning for big models on big data. In Artiﬁcial Intelligence and Statistics, pages 531–539. PMLR, 2014. [18] L. Li, K. Jamieson, A. Rostamizadeh, E. Gonina, J. Ben-Tzur, M. Hardt, B. Recht, and A. Talwalkar. A system for massively parallel hyperparameter tuning. Proceedings of Machine Learning and Systems, 2:230–246, 2020. [19] M. Li, D. G. Andersen, J. W. Park, A. J. Smola, A. Ahmed, V . Josifovski, J. Long, E. J. Shekita, and B.-Y . Su. Scaling distributed machine learning with the parameter server. In11th USENIX Symposium on Operating Systems Design and Implementation (OSDI 14), pages 583–598, 2014. [20] S. Li, O. Mangoubi, L. Xu, and T. Guo. Sync-switch: Hybrid parameter synchronization for distributed deep learning. In 2021 IEEE 41st International Conference on Distributed Computing Systems (ICDCS), pages 528–538. IEEE, 2021. [21] J. Lian, X. Zhou, F. Zhang, Z. Chen, X. Xie, and G. Sun. xdeepfm: Combining explicit and implicit feature interactions for recommender systems. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1754–1763, 2018. [22] Q. Luo, J. Lin, Y . Zhuo, and X. Qian. Hop: Heterogeneity-aware decentralized training. In Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, pages 893–907, 2019. 10[23] Q. Luo, J. He, Y . Zhuo, and X. Qian. Prague: High-performance heterogeneity-aware asyn- chronous decentralized training. In Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, pages 401–416, 2020. [24] G. Nadiradze, A. Sabour, P. Davies, S. Li, and D. Alistarh. Asynchronous decentralized sgd with quantized and local updates. Advances in Neural Information Processing Systems , 34, 2021. [25] J. Wu, W. Hu, H. Xiong, J. Huan, V . Braverman, and Z. Zhu. On the noisy gradient descent that generalizes as sgd. In International Conference on Machine Learning, pages 10367–10376. PMLR, 2020. [26] B. Ying, K. Yuan, Y . Chen, H. Hu, P. Pan, and W. Yin. Exponential graph is provably efﬁcient for decentralized deep training. Advances in Neural Information Processing Systems, 34, 2021. [27] Y . You, J. Li, S. Reddi, J. Hseu, S. Kumar, S. Bhojanapalli, X. Song, J. Demmel, K. Keutzer, and C.-J. Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes. In ICLR, 2020. [28] H. Yu and R. Jin. On the computation and communication complexity of parallel sgd with dynamic batch sizes for stochastic non-convex optimization. In International Conference on Machine Learning, pages 7174–7183. PMLR, 2019. [29] Y . Zhang, L. Chen, S. Yang, M. Yuan, H. Yi, J. Zhang, J. Wang, J. Dong, Y . Xu, Y . Song, et al. Picasso: Unleashing the potential of gpu-centric training for wide-and-deep recommender systems. In 2022 IEEE International Conference on Data Engineering (ICDE), 2022. [30] S. Zheng, Q. Meng, T. Wang, W. Chen, N. Yu, Z.-M. Ma, and T.-Y . Liu. Asynchronous stochastic gradient descent with delay compensation. In International Conference on Machine Learning, pages 4120–4129. PMLR, 2017. [31] G. Zhou, X. Zhu, C. Song, Y . Fan, H. Zhu, X. Ma, Y . Yan, J. Jin, H. Li, and K. Gai. Deep interest network for click-through rate prediction. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining, pages 1059–1068, 2018. [32] G. Zhou, N. Mou, Y . Fan, Q. Pi, W. Bian, C. Zhou, X. Zhu, and K. Gai. Deep interest evolution network for click-through rate prediction. In Proceedings of the AAAI conference on artiﬁcial intelligence, volume 33, pages 5941–5948, 2019. [33] Y . Zhou, Y . Yu, W. Dai, Y . Liang, and E. Xing. On convergence of model parallel proximal gradient algorithm for stale synchronous parallel system. In Artiﬁcial Intelligence and Statistics, pages 713–722. PMLR, 2016. 11Appendix A Proof of Equation (4) Assumption. Throughout the paper, we make the following assumptions: 1. F(w) is an L-smooth function, i.e., F(w1) ≤F(w2) + (w1 −w2)T∇F(w2) + L 2 ∥w1 −w2∥2 2; 2. F(w) is strongly convex with parameter c, i.e., 2c(F(w) −F∗) ≤∥∇F(w)∥2 2; 3. The stochastic gradient g(wτ(m,k)) is an unbiased estimate of the true gradient, i.e., E(g(wτ(m,k))) = E(∇F(wτ(m,k))); 4. The variance of the stochastic gradient g(wτ(m,k)) in the asynchronous training is bounded as E ( ∥g(wτ(m,k)) −∇F(wτ(m,k))∥2 2 ) ≤σ2 Ba + Θ Ba E(∥∇F(wτ(m,k))∥2 2), and in the synchronous training is bounded as E ( ∥g(wτ(m,k)) −∇F(wτ(m,k))∥2 2 ) ≤σ2 Bs + Θ Bs E(∥∇F(wτ(m,k))∥2 2). Theorem 1. Based on the above Assumption andη≤ 1 2L( Θ MBa +1) , also suppose that for someγ ≤1, E ( ∥∇F(wk) −∇F(wτ(m,k))∥2 2 ) ≤γE(∥∇F(wk)∥2 2), thus the expectation of error after k+ 1 steps of gradient updates in the asynchronous trainingis deduced by E(F(wk+1) −F∗) ≤ ηLσ2 2cγ′MBa + (1 −ηγ′c)k+1 ( E(F(w0) −F∗) − ηLσ2 2cγ′MBa ) , where γ′= 1 −γ+ p0 2 ,p0 is a lower bound on the conditional probability that the token equals to the global step, i.e., τ(m,k) = k. Corollary 1. To characterize the strong sparsity in the recommendation models, we suppose that for some γ ≤1, ζE(∥∇F(wk) −∇F(wτ(m,k))∥2 2) ≤γE(∥∇F(wk)∥2 2), γ = {γ, ς = 1, ζγ, ς̸= 1, (5) thus the expectation of error after k+ 1 steps of gradient updates in the asynchronous trainingis E(F(wk+1) −F∗) ≤ ηLσ2 2cρMBa + (1 −ηρc)k+1 ( E(F(w0) −F∗) − ηLσ2 2cρMBa ) , where ρ= 1 −p1γ−(1 −p1)ζγ + p0 2 , ς = Idense(parameter), IA(x) is the indicator function of whether xbelongs to A, p1 = P(ς = 1). It is worth noting that we have ρ >0 and ρ > γ′owing to ζγ < γ≤1. Since 2+p0−2ζγ 2(γ−ζγ) > 1, P(ς = 1) < 2+p0−2ζγ 2(γ−ζγ) . The model GBA achieves a better performance in terms of dense and sparse parameters, like the error ﬂoor ηLσ2 2cρMBa is smaller than ηLσ2 2cγ′MBa mentioned in Theorem 1, and the convergence speed is more quickly. Clearly, the value of p1 is differ for various distributions and accordingly the values of ηLσ2 2cρMBa and 1 −ηρcare different. Theorem 2. Based on the above Assumption and η ≤ 1 2L( Θ NsBs +1) , the expectation of error after k+ 1 steps of gradient updates in the synchronous trainingis deduced by E(F(wk+1) −F∗) ≤ ηLσ2 2cNsBs + (1 −ηc)k+1 ( E(F(w0) −F∗) − ηLσ2 2cNsBs ) . To provide the proofs of Theorem 1, Corollary 1, and Theorem 2, we ﬁrst prove the following lemmas. 12Lemma 1. Let g(wτ(i,k)) denote the i-th gradient of k-th global step, and assume its expectation E(g(wτ(i,k))) = E(∇F(wτ(i,k))).Then E ( ∥g(wτ(i,k)) −∇F(wk)∥2 2 ) = E(∥g(wτ(i,k))∥2 2) −E(∥∇F(wτ(i,k))∥2 2) + E ( ∥∇F(wτ(i,k)) −∇F(wk)∥2 2 ) . proof of Lemma 1. E ( ∥g(wτ(i,k)) −∇F(wk)∥2 2 ) = E ( ∥g(wτ(i,k)) −∇F(wτ(i,k)) + ∇F(wτ(i,k)) −∇F(wk)∥2 2 ) = E ( ∥g(wτ(i,k)) −∇F(wτ(i,k))∥2 2 ) + E ( ∥∇F(wτ(i,k)) −∇F(wk)∥2 2 ) + 2E (⣨ g(wτ(i,k)) −∇F(wτ(i,k)),∇F(wτ(i,k)) −∇F(wk) ⟩) . (6) Since E(g(wτ(i,k))) = E(∇F(wτ(i,k))), E (⣨ g(wτ(i,k)) −∇F(wτ(i,k)),∇F(wτ(i,k)) −∇F(wk) ⟩) = 0. Returning to (6), we have E ( ∥g(wτ(i,k)) −∇F(wk)∥2 2 ) = E ( ∥g(wτ(i,k)) −∇F(wτ(i,k))∥2 2 ) + E ( ∥∇F(wτ(i,k)) −∇F(wk)∥2 2 ) = E(∥g(wτ(i,k))∥2 2) + E(∥∇F(wτ(i,k))∥2 2) −2E ( ⟨g(wτ(i,k)),∇F(wτ(i,k))⟩ ) + E ( ∥∇F(wτ(i,k)) −∇F(wk)∥2 2 ) = E(∥g(wτ(i,k))∥2 2) −E(∥∇F(wτ(i,k))∥2 2) + E ( ∥∇F(wτ(i,k)) −∇F(wk)∥2 2 ) . (7) Lemma 2. Let vk = 1 M M∑ i=1 g(wτ(i,k)), and suppose the variance of g(wτ(i,k)) is bounded as E ( ∥g(wτ(i,k)) −∇F(wτ(i,k))∥2 2 ) ≤σ2 Ba + Θ Ba ∥∇F(wτ(i,k))∥2 2. Then the sum of g(wτ(i,k)) is bounded as follows E(∥vk∥2 2) ≤ σ2 MBa + M∑ i=1 Θ M2Ba E(∥∇F(wτ(i,k))∥2 2) + 1 M M∑ i=1 E(∥∇F(wτ(i,k))∥2 2). proof of Lemma 2. E(∥vk∥2 2) = E ( ∥1 M M∑ i=1 g(wτ(i,k))∥2 2 ) = 1 M2 E ( ∥ M∑ i=1 g(wτ(i,k))∥2 2 ) = 1 M2 E ( ∥ M∑ i=1 (g(wτ(i,k)) −∇F(wτ(i,k))) + M∑ i=1 ∇F(wτ(i,k))∥2 2 ) = 1 M2 E ( ∥ M∑ i=1 (g(wτ(i,k)) −∇F(wτ(i,k)))∥2 2 ) + 1 M2 E ( ∥ M∑ i=1 ∇F(wτ(i,k))∥2 2 ) + 2 M2 E (⟨M∑ i=1 (g(wτ(i,k)) −∇F(wτ(i,k))), M∑ i=1 ∇F(wτ(i,k))∥2 2 ⟩) (8) 13Owing to E(g(wτ(i,k))) = E(∇F(wτ(i,k))), E(∥vk∥2 2) = 1 M2 E ( ∥ M∑ i=1 (g(wτ(i,k)) −∇F(wτ(i,k)))∥2 2 ) + 1 M2 E ( ∥ M∑ i=1 ∇F(wτ(i,k))∥2 2 ) = 1 M2 M∑ i=1 E ( ∥g(wτ(i,k)) −∇F(wτ(i,k))∥2 2 ) + 1 M2 E ( ∥ M∑ i=1 ∇F(wτ(i,k))∥2 2 ) + 2 M2 M−1∑ i=1 M∑ j=i+1 E (⣨ g(wτ(i,k)) −∇F(wτ(i,k)),g(wτ(j,k)) −∇F(wτ(j,k)) ⟩) = 1 M2 M∑ i=1 E ( ∥g(wτ(i,k)) −∇F(wτ(i,k))∥2 2 ) + 1 M2 E ( ∥ M∑ i=1 ∇F(wτ(i,k))∥2 2 ) ≤ σ2 MBa + M∑ i=1 Θ M2Ba E(∥∇F(wτ(i,k))∥2 2) + 1 M2 E ( ∥ M∑ i=1 ∇F(wτ(i,k))∥2 2 ) . (9) The second term in (9) could be obtained by E ( ∥ M∑ i=1 ∇F(wτ(i,k))∥2 2 ) = M∑ i=1 E(∥∇F(wτ(i,k))∥2 2) + M−1∑ i=1 M∑ j=i+1 2E(⟨∇F(wτ(i,k)),∇F(wτ(j,k))⟩) (a) ≤ M∑ i=1 E(∥∇F(wτ(i,k))∥2 2) + M−1∑ i=1 M∑ j=i+1 E ( ∥∇F(wτ(i,k))∥2 2 + ∥∇F(wτ(j,k))∥2 2 ) = M∑ i=1 E(∥∇F(wτ(i,k))∥2 2) + M∑ i=1 (M −1)E(∥∇F(wτ(i,k))∥2 2) = M∑ i=1 ME(∥∇F(wτ(i,k))∥2 2). (10) Here step (a) follows from 2⟨x,y⟩≤∥ x∥2 2 + ∥y∥2 2. Based on (9) and (10), we have E(∥vk∥2 2) ≤ σ2 MBa + M∑ i=1 Θ M2Ba E(∥∇F(wτ(i,k))∥2 2) + 1 M M∑ i=1 E(∥∇F(wτ(i,k))∥2 2). Lemma 3. Suppose p0 is a lower bound on the conditional probability that the token equals to the global step, i.e., τ(i,k) = k, thus E(∥∇F(wτ(i,k))∥2 2) ≥p0E(∥∇F(wk)∥2 2). proof of Lemma 3. E(∥∇F(wτ(i,k))∥2 2) = p0E ( ∥∇F(wτ(i,k))∥2 2 |τ(i,k) = k ) + (1 −p0)E ( ∥∇F(wτ(i,k))∥2 2 |τ(i,k) ̸= k ) ≥p0E(∥∇F(wk)∥2 2). (11) 14Next, we will provide the proofs of Theorem 1, Corollary 1, and Theorem 2. proof of Theorem 1. Let wk+1 = wk −ηvk, vk = 1 M M∑ i=1 g(wτ(i,k)), we have F(wk+1) ≤F(wk) + (wk+1 −wk)T∇F(wk) + L 2 ∥wk+1 −wk∥2 2 ≤F(wk) + ⟨−ηvk,∇F(wk)⟩+ L 2 η2∥vk∥2 2 = F(wk) − η M M∑ i=1 ⟨g(wτ(i,k)),∇F(wk)⟩+ L 2 η2∥vk∥2 2. (12) Owing to 2⟨x,y⟩= ∥x∥2 2 + ∥y∥2 2 −∥x−y∥2 2,(12) is shown as follows, F(wk+1) ≤F(wk) − η M M∑ i=1 (1 2∥g(wτ(i,k))∥2 2 + 1 2∥∇F(wk)∥2 2 −1 2∥g(wτ(i,k)) −∇F(wk)∥2 2 ) + L 2 η2∥vk∥2 2 = F(wk) −η 2∥∇F(wk∥2 2 − η 2M M∑ i=1 ∥g(wτ(i,k))∥2 2 + η 2M M∑ i=1 ∥g(wτ(i,k)) −∇F(wk)∥2 2 + L 2 η2∥vk∥2 2. (13) Taking expectation, E(F(wk+1)) ≤E(F(wk)) −η 2E(∥∇F(wk∥2 2) − η 2M M∑ i=1 E(∥g(wτ(i,k))∥2 2) + η 2M M∑ i=1 E ( ∥g(wτ(i,k)) −∇F(wk)∥2 2 ) + L 2 η2E(∥vk∥2 2) (a) = E(F(wk)) −η 2E(∥∇F(wk∥2 2) − η 2M M∑ i=1 E(∥g(wτ(i,k))∥2 2) + η 2M M∑ i=1 E(∥g(wτ(i,k))∥2 2) − η 2M M∑ i=1 E(∥∇F(wτ(i,k))∥2 2) + η 2M M∑ i=1 E ( ∥∇F(wτ(i,k)) −∇F(wk)∥2 2 ) + L 2 η2E(∥vk∥2 2) (b) ≤E(F(wk)) −η 2E(∥∇F(wk∥2 2) − η 2M M∑ i=1 E(∥∇F(wτ(i,k))∥2 2) + η 2γE(∥∇F(wk)∥2 2) + L 2 η2E(∥vk∥2 2) = E(F(wk)) −η 2(1 −γ)E(∥∇F(wk)∥2 2) − η 2M M∑ i=1 E(∥∇F(wτ(i,k))∥2 2) + L 2 η2E(∥vk∥2 2) (c) ≤E(F(wk)) −η 2(1 −γ)E(∥∇F(wk)∥2 2) − η 2M M∑ i=1 E(∥∇F(wτ(i,k))∥2 2) + L 2 η2 ( σ2 MBa + M∑ i=1 Θ M2Ba E(∥∇F(wτ(i,k))∥2 2) + 1 M M∑ i=1 E(∥∇F(wτ(i,k))∥2 2) ) = E(F(wk)) −η 2(1 −γ)E(∥∇F(wk)∥2 2) + Lη2σ2 2MBa − η 2M M∑ i=1 ( 1 − LηΘ MBa −Lη ) E(∥∇F(wτ(i,k))∥2 2). (14) Here step (a) follows from Lemma 1, step (b) follow from E(∥∇F(wk) −∇F(wτ(m,k))∥2 2) ≤ γE(∥∇F(wk)∥2 2), and step (c) follows from Lemma 2. 15Since η≤ 1 2L( Θ MBa +1) , (14) could be obtained by E(F(wk+1)) ≤E(F(wk)) −η 2(1 −γ)E(∥∇F(wk)∥2 2) + Lη2σ2 2MBa − η 4M M∑ i=1 E(∥∇F(wτ(i,k))∥2 2) (d) ≤E(F(wk)) −η 2(1 −γ)E(∥∇F(wk)∥2 2) + Lη2σ2 2MBa −η 4p0E(∥∇F(wk)∥2 2) (e) ≤E(F(wk)) −ηc(1 −γ)E(F(wk) −F∗) −ηcp0 2 E(F(wk) −F∗) + Lη2σ2 2MBa = E(F(wk)) −ηcγ′E(F(wk) −F∗) + Lη2σ2 2MBa , (15) where γ′= 1 −γ+ p0 2 .Here step (d) follows from Lemma 3, step (e) follows from F(w) is strongly convex with parameter c. Therefore, E(F(wk+1) −F∗) ≤ ηLσ2 2cγ′MBa + (1 −ηγ′c)k+1 ( E(F(w0) −F∗) − ηLσ2 2cγ′MBa ) . proof of Corollary 1. Based on ζE(∥∇F(wk)−∇F(wτ(m,k))∥2 2) ≤γE(∥∇F(wk)∥2 2),the step (b) of (14) should be written as follows, E(F(wk+1)) ≤E(F(wk)) −η 2E(∥∇F(wk)∥2 2) − η 2M M∑ i=1 E(∥∇F(wτ(i,k))∥2 2) + Lη2 2 E(∥vk∥2 2) + η 2M M∑ i=1 E ( ∥∇F(wτ(i,k)) −∇F(wk) |ς = 1∥2 2 ) + η 2M M∑ i=1 E ( ∥∇F(wτ(i,k)) −∇F(wk) |ς ̸= 1∥2 2 ) ≤E(F(wk)) −η 2E(∥∇F(wk)∥2 2) − η 2M M∑ i=1 E(∥∇F(wτ(i,k))∥2 2) + Lη2 2 E(∥vk∥2 2) + ηγp1 2M M∑ i=1 E(∥∇F(wk)∥2 2) + ηζγ(1 −p1) 2M M∑ i=1 E(∥∇F(wk)∥2 2) = E(F(wk)) −η 2 ( 1 −p1γ−(1 −p1)ζγ ) E(∥∇F(wk)∥2 2) − η 2M M∑ i=1 E(∥∇F(wτ(i,k))∥2 2) + Lη2 2 E(∥vk∥2 2) ≤E(F(wk)) −η 2 ( 1 −p1γ−(1 −p1)ζγ ) E(∥∇F(wk)∥2 2) + Lη2σ2 2MBa − η 4M M∑ i=1 E(∥∇F(wτ(i,k))∥2 2) ≤E(F(wk)) −η 2 ( 1 −p1γ−(1 −p1)ζγ + p0 2 ) E(∥∇F(wk)∥2 2) + Lη2σ2 2MBa ≤E(F(wk)) −ηcρE(F(wk) −F∗) + Lη2σ2 2MBa (16) where ρ= 1 −p1γ−(1 −p1)ζγ + p0 2 ,p1 = P(ς = 1). Therefore, E(F(wk+1) −F∗) ≤ ηLσ2 2cρMBa + (1 −ηρc)k+1 ( E(F(w0) −F∗) − ηLσ2 2cρMBa ) . 16proof of Theorem 2. Let wk+1 = wk −ηvk, vk = 1 Ns Ns∑ i=1 g(wτ(i,k)), we have F(wk+1) ≤F(wk) + (wk+1 −wk)T∇F(wk) + L 2 ∥wk+1 −wk∥2 2 = F(wk) + ⟨−ηvk,∇F(wk)⟩+ L 2 η2∥vk∥2 2 = F(wk) − η Ns Ns∑ i=1 ⟨g(wτ(i,k)),∇F(wk)⟩+ L 2 η2∥vk∥2 2 = F(wk) − η Ns Ns∑ i=1 (1 2∥g(wτ(i,k))∥2 2 + 1 2∥∇F(wk)∥2 2 −1 2∥g(wτ(i,k)) −∇F(wk)∥2 2 ) + L 2 η2∥vk∥2 2 = F(wk) −η 2∥∇F(wk)∥2 2 − η 2Ns Ns∑ i=1 ∥g(wτ(i,k))∥2 2 + η 2Ns Ns∑ i=1 ∥g(wτ(i,k)) −∇F(wk)∥2 2 + L 2 η2∥vk∥2 2. (17) Taking expectation, E(F(wk+1)) ≤E(F(wk)) −η 2E(∥∇F(wk)∥2 2) − η 2Ns Ns∑ i=1 E(∥g(wτ(i,k))∥2 2) + L 2 η2E(∥vk∥2 2) + η 2Ns Ns∑ i=1 E ( ∥g(wτ(i,k)) −∇F(wk)∥2 2 ) . (18) The last term in (18) could be obtained on the basis of E(g(wτ(i,k))) = E(∇F(wk)), E ( ∥g(wτ(i,k)) −∇F(wk)∥2 2 ) = E(∥g(wτ(i,k))∥2 2) + E(∥∇F(wk)∥2 2) −2E(⟨g(wτ(i,k)),∇F(wk)⟩) = E(∥g(wτ(i,k))∥2 2) −E(∥∇F(wk)∥2 2). (19) Returning to (18), we have E(F(wk+1)) ≤E(F(wk)) −η 2E(∥∇F(wk)∥2 2) − η 2Ns Ns∑ i=1 E(∥g(wτ(i,k))∥2 2) + L 2 η2E(∥vk∥2 2) + η 2Ns Ns∑ i=1 E(∥g(wτ(i,k))∥2 2) − η 2Ns Ns∑ i=1 E(∥∇F(wk)∥2 2). (20) Similar to Lemma 2, E(∥vk∥2 2) = E ( ∥1 Ns Ns∑ i=1 g(wτ(i,k))∥2 2 ) = 1 N2s E ( ∥ Ns∑ i=1 (g(wτ(i,k)) −∇F(wk))∥2 2 ) + 1 N2s E ( ∥ Ns∑ i=1 ∇F(wk)∥2 2 ) = 1 N2s Ns∑ i=1 E ( ∥g(wτ(i,k)) −∇F(wk)∥2 2 ) + E(∥∇F(wk)∥2 2) ≤ σ2 NsBs + ( Θ NsBs + 1 ) E(∥∇F(wk)∥2 2). (21) 17Based on (20) and (21), we obtain E(F(wk+1)) ≤E(F(wk)) −ηE(∥∇F(wk)∥2 2) + Lη2σ2 2NsBs + Lη2 2 ( Θ NsBs + 1 ) E(∥∇F(wk)∥2 2) = E(F(wk)) −η ( 1 − LηΘ 2NsBs −Lη 2 ) E(∥∇F(wk)∥2 2) + Lη2σ2 2NsBs (a) ≤E(F(wk)) −η 2E(∥∇F(wk)∥2 2) + Lη2σ2 2NsBs (b) ≤E(F(wk)) −ηcE(F(wk) −F∗) + Lη2σ2 2NsBs . (22) Here step (a) follows from η≤ 1 2L( Θ NsBs +1) , step (b) follows from 2c(F(w) −F∗) ≤∥∇F(w)∥2 2. Therefore, E(F(wk+1) −F∗) ≤(1 −ηc)E(F(wk) −F∗) + Lη2σ2 2NsBs ≤(1 −ηc)k+1 ( E(F(w0) −F∗) − Lησ2 2cNsBs ) + Lησ2 2cNsBs . (23) B Workﬂows of GBA In this section, we abstract the workﬂows of GBA. Algorithm 1 summarizes the workﬂow on workers in GBA. We implement pipeline between data downloading and data ingestion to accelerate the training. After completing the computation of gradients, the worker would directly send the gradient with the token back to the PS in a non-blocking way. In this way, the fast workers would ingest much more data than the straggling workers. When a worker recovered from a failure, it would drop the previous state (e.g., data in the batch buffer and token) and proceed to deal with the new batch. The disappearance of a speciﬁc token would not change the correctness and efﬁciency of GBA. Algorithm 1 Workﬂow on workers Ensure: Downloading threads: Download data asynchronously to a download buffer; Pack threads: Prepare the batch of data from the download buffer to a batch buffer; Computation threads: Execute the forward and backward pass of the model; Communication threads: Pull and push parameters by GRPC; 1: Downloading threads 2: repeat 3: Get the addresses of a number of batches (data shard) from PS. 4: repeat 5: if The download buffer is not full then 6: Download a batch of data. 7: else 8: Sleep 100ms. 9: end if 10: until All the data from this shard has been downloaded 11: until No more data shards to get 12: 13: Computation threads 14: repeat 15: Get a batch from the batch buffer in a blocking way. 16: Pull the parameters and fetch a token from PS. 17: Compute the forward and backward pass. 18: Send the local gradient and the token back to PS in a non-blocking way. 19: until No more data to ingest Algorithm 2 summarizes the workﬂow on PSs in GBA. The token generation threads, the pull responding threads, and the push responding threads work asynchronously to avoid blocking of the 18process. Different from the dense parameters, the embedding parameters are processed and updated by IDs, instead of by the entire embedding parameters. In practice, we optimize the memory usage as we could execute the weighted sum over some gradients in advance based on the tokens. Algorithm 2 Workﬂow on PSs Ensure: Token generation thread: Generate tokens to the token list; Pull responding threads: Send the parameters and a token to the worker; Push responding threads: Receive the gradients from a worker and apply them if necessary. 1: Token generation thread (Only on PS 0, with lock) 2: if Successfully acquire the lock then 3: if The number of tokens in the token list is less than the number of workers then 4: Insert new tokens to the tail of the token list (a Queue) 5: end if 6: end if 7: 8: Pull responding threads 9: Receive the request from a worker with the embedding IDs. 10: Look up the embedding tables based on the IDs for the embedding parameters. 11: Fetch a token from the token list and trigger the token generation thread (only on PS 0). 12: Send the parameters (and the token) back to the worker. 13: 14: Push responding threads 15: Receive the gradients from a worker. 16: Store the gradients with the token to the gradient buffer. 17: if At least Na gradients are cached in the gradient buffer then 18: Pop Na gradients from the gradient buffer. 19: Update the global step of appearance tagged to each ID. 20: Decay the gradients of the dense parameters based on the current global step and the attached token. 21: Decay the gradients of the embedding parameter based on the tagged global step of each ID and the attached token to the gradient. 22: Calculate the weighted sum of the gradients of the dense parameters, divided by Na. 23: Calculate the weighted sum of the gradients of the embedding parameters, divided by the number of workers that encountered the particular ID. 24: end if C Detailed statistics of Table 6 in the submission Here we want to clarify the statements about Figure 6 in the submission, and present the detailed statistics. Table 6.1-6.3 depict the AUCs after inheriting the checkpoints trained via synchronous training. Table 6.5-6.7 introduce the AUCs after inheriting the checkpoints trained by the compared training modes and being switched to synchronous training. We collect the mean AUCs from the ﬁrst day, the last day, and all days across the three datasets, as shown in Table 6.4 and Table 6.8. We can infer from the two tables that GBA provides the closest AUC scores as synchronous training. GBA appears with the lowest immediate AUC drop after switching, i.e., merely 0.1% decrement after switching from/to synchronous training at the ﬁrst day. Throughout the three datasets, GBA outperforms the other baselines by at least 0.2% (Hop-BW in Table 6.4) when switching from synchronous training and 0.1% (Hop-BS in Table 6.8) when switching to synchronous training. D Proof of sudden drop in performance after switching. Theorem 3. Based on the above Assumption and 1 NL( Θ B +1) ≤η≤ 1 2L( Θ B +1) , also suppose that for some γ ≤1, E ( ∥∇F(wk) −∇F(wτ(i,k))∥2 2 ) ≤γE(∥∇F(wk)∥2 2), in the asynchronous training, the expectation of loss in the k+ 1 step is deduced by E(F(wk+1)) ≤E(F(wk)) −η 2(1 −γ)E(∥∇F(wk)∥2 2) + Lη2σ2 2B −η 4E(∥∇F(wτ(k))∥2 2). (24) 19Table 6.1: Figure 6(a) - Criteo (from Sync.) Date Sync. GBA Hop-BW Hop-BS BSP Aysnc. 13 0.7999 0.7964 0.7954 0.7924 0.7930 0.5000 14 0.7957 0.7932 0.7959 0.7869 0.7886 0.5000 15 0.7967 0.7957 0.7895 0.7891 0.7889 0.5000 16 0.7963 0.7956 0.7932 0.5040 0.7891 0.5000 17 0.7962 0.7955 0.7930 0.5040 0.7883 0.5000 18 0.7957 0.7950 0.7939 0.5030 0.7862 0.5000 19 0.7972 0.7966 0.7968 0.5030 0.7863 0.5000 20 0.7974 0.7973 0.7985 0.5030 0.7868 0.5000 21 0.7965 0.7959 0.7948 0.5050 0.7863 0.5000 22 0.7957 0.7955 0.7939 0.5040 0.7865 0.5000 23 0.7987 0.7986 0.7933 0.5060 0.7871 0.5000 Avg. 0.7969 0.7959 0.7944 0.5819 0.7879 0.5000 Table 6.2: Figure 6(b) - Alimama (from Sync.) Date Sync. GBA Hop-BW Hop-BS BSP Aysnc. 6 0.6490 0.6489 0.6472 0.6488 0.6472 0.5000 7 0.6503 0.6502 0.6478 0.6503 0.6500 0.5000 8 0.6523 0.6523 0.6483 0.6523 0.6512 0.5000 Avg. 0.6505 0.6505 0.6478 0.6505 0.6495 0.5000 Table 6.3: Figure 6(c) - Private (from Sync.) Date Sync. GBA Hop-BW Hop-BS BSP Aysnc. 15 0.7877 0.7880 0.7870 0.7875 0.7880 0.7795 16 0.7874 0.7877 0.7860 0.7878 0.7870 0.7785 17 0.7856 0.7860 0.7840 0.7858 0.7850 0.7774 18 0.7884 0.7888 0.7850 0.7882 0.7877 0.7873 19 0.7894 0.7905 0.7855 0.7890 0.7886 0.7878 20 0.7785 0.7788 0.7750 0.7781 0.7774 0.7598 21 0.7865 0.7868 0.7823 0.7863 0.7850 0.7769 22 0.7862 0.7870 0.7825 0.7858 0.7862 0.7754 Avg. 0.7862 0.7867 0.7834 0.7861 0.7856 0.7778 Table 6.4: Average AUC decrement on three datasets between GBA and the other baselines (from Sync.) Sync. Hop-BW Hop-BS BSP Aysnc. 1st day +0.0011 -0.0012 -0.0015 -0.0017 -0.1513 last day -0.0002 -0.0046 -0.0979 -0.0045 -0.1542 Average +0.0002 -0.0025 -0.0716 -0.0034 -0.1518 20Table 6.5: Figure 6(d) - Criteo (to Sync.) Date Sync. GBA Hop-BW Hop-BS BSP Aysnc. 13 0.7999 0.7963 0.7968 0.7937 0.7913 0.7872 14 0.7957 0.7947 0.7933 0.7917 0.7907 0.7902 15 0.7967 0.7957 0.7952 0.7932 0.7923 0.7922 16 0.7963 0.7954 0.7956 0.7937 0.7944 0.7939 17 0.7962 0.7956 0.7946 0.7943 0.7945 0.7935 18 0.7957 0.7958 0.7949 0.7931 0.7922 0.7929 19 0.7972 0.7966 0.7960 0.7952 0.7944 0.7946 20 0.7974 0.7970 0.7970 0.7962 0.7957 0.7957 21 0.7965 0.7963 0.7956 0.7950 0.7931 0.7952 22 0.7957 0.7955 0.7955 0.7953 0.7940 0.7950 23 0.7987 0.7982 0.7978 0.7973 0.7964 0.7972 Avg. 0.7969 0.7961 0.7957 0.7944 0.7935 0.7934 Table 6.6: Figure 6(e) - Alimama (to Sync.) Date Sync. GBA Hop-BW Hop-BS BSP Aysnc. 6 0.6490 0.6492 0.6401 0.6484 0.6452 0.6352 7 0.6503 0.6504 0.6437 0.6499 0.6487 0.6426 8 0.6523 0.6523 0.6471 0.6520 0.6503 0.6456 Avg. 0.6505 0.6506 0.6436 0.6501 0.6481 0.6411 Table 6.7: Figure 6(f) - Private (to Sync.) Date Sync. GBA Hop-BW Hop-BS BSP Aysnc. 15 0.7877 0.7878 0.7783 0.7859 0.7732 0.7870 16 0.7874 0.7877 0.7844 0.7867 0.7767 0.5000 17 0.7856 0.7855 0.7813 0.7853 0.7782 0.5000 18 0.7884 0.7883 0.7858 0.7880 0.7805 0.5000 19 0.7894 0.7896 0.7870 0.7889 0.7848 0.5000 20 0.7785 0.7786 0.7761 0.7784 0.7746 0.5000 21 0.7865 0.7865 0.7843 0.7864 0.7828 0.5000 22 0.7862 0.7863 0.7855 0.7861 0.7838 0.5000 Avg. 0.7862 0.7863 0.7828 0.7857 0.7793 0.5359 Table 6.8: Average AUC decrement on three datasets between GBA and the other baselines (to Sync.) Sync. Hop-BW Hop-BS BSP Aysnc. 1st day +0.0011 -0.0060 -0.0018 -0.0079 -0.0080 last day +0.0001 -0.0021 -0.0005 -0.0021 -0.0980 Average +0.0002 -0.0036 -0.0009 -0.0040 -0.0875 21If we switch to the synchronous trainingin the k+ 1step, the expectation of loss is shown as follows E(F(wk+1)) ≤E(F(wk)) −η 2(1 −γ)E(∥∇F(wk∥2 2) + Lη2σ2 2B + Lη2σ2 2BN + (Lη2Θ 2B + Lη2 2 − η 2N)E ( ∥∇F(wk)∥2 2 ) − η 4NE ( ∥∇F(wτ(k))∥2 2 ) (25) and switching the asynchronous mode to the synchronous mode may drop in performance. proof of Theorem 3. According to Theorem 1, (24) could be obtained if we apply the asynchronous training mode. Next, we will prove (25). Let wk+1 = wk −ηvk, vk = 1 N N∑ i=1 g(wτ(i,k)), we have F(wk+1) ≤F(wk) + (wk+1 −wk)T∇F(wk) + L 2 ∥wk+1 −wk∥2 2 ≤F(wk) + ⟨−ηvk,∇F(wk)⟩+ L 2 η2∥vk∥2 2 = F(wk) − η N N∑ i=1 ⟨g(wτ(i,k)),∇F(wk)⟩+ L 2 η2∥vk∥2 2. (26) Since we switch the asynchronous training to the synchronous training, the gradient of each worker g(wτ(i,k)), i= 1,2,...,N, in the kstep may be different. Owing to 2⟨x,y⟩= ∥x∥2 2 + ∥y∥2 2 −∥x−y∥2 2,the expectation of (26) is shown as follows, E(F(wk+1)) ≤E(F(wk)) −η 2E(∥∇F(wk∥2 2) − η 2N N∑ i=1 E(∥g(wτ(i,k))∥2 2) + η 2N N∑ i=1 E ( ∥g(wτ(i,k)) −∇F(wk)∥2 2 ) + L 2 η2E(∥vk∥2 2). (27) Since there applied a gradient in thekstep of the asynchronous training, we may assumeg(wτ(1,k)) = g(wk). Hence, η 2N N∑ i=1 E ( ∥g(wτ(i,k)) −∇F(wk)∥2 2 ) = η 2N ( E(∥g(wτ(1,k)) −∇F(wk)∥2 2) + N∑ j=2 E(∥g(wτ(j,k)) −∇F(wk)∥2 2) ) = η 2N ( E(∥g(wk)∥2 2) −E(∥∇F(wk)∥2 2) ) + η 2N N∑ j=2 ( E(∥g(wτ(j,k))∥2 2) −E(∥∇F(wτ(j,k))∥2 2) + E(∥∇F(wτ(j,k)) −∇F(wk)∥2 2) ) (28) Besides, η 2N N∑ i=1 E(∥g(wτ(i,k))∥2 2) = η 2NE(∥g(wk)∥2 2) + η 2N N∑ j=2 E(∥g(wτ(j,k))∥2 2). (29) 22Based on (28) and (29), we have E(F(wk+1)) ≤E(F(wk)) −η 2E(∥∇F(wk∥2 2) − η 2NE(∥∇F(wk)∥2 2) + L 2 η2E(∥vk∥2 2) − η 2N N∑ j=2 E(∥∇F(wτ(j,k))∥2 2) + η 2N N∑ j=2 E(∥∇F(wτ(j,k)) −∇F(wk)∥2 2) (a) ≤E(F(wk)) −η 2E(∥∇F(wk∥2 2) − η 2NE(∥∇F(wk)∥2 2) + L 2 η2E(∥vk∥2 2) − η 2N N∑ j=2 E(∥∇F(wτ(j,k))∥2 2) + γη 2 E(∥∇F(wk∥2 2) = E(F(wk)) −η 2(1 −γ)E(∥∇F(wk∥2 2) − η 2NE(∥∇F(wk)∥2 2) + L 2 η2E(∥vk∥2 2) − η 2N N∑ j=2 E(∥∇F(wτ(j,k))∥2 2). (30) Here, step (a) follows from E ( ∥∇F(wk) −∇F(wτ(j,k))∥2 2 ) ≤γE(∥∇F(wk)∥2 2). E(∥vk∥2 2) = E ( ∥1 N N∑ i=1 g(wτ(i,k))∥2 2 ) = 1 N2 E ( ∥ N∑ i=1 g(wτ(i,k))∥2 2 ) = 1 N2 E ( ∥ N∑ i=1 g(wτ(i,k)) −∇F(wτ(i,k)) + ∇F(wτ(i,k))∥2 2 ) = 1 N2 E ( ∥ N∑ i=1 g(wτ(i,k)) −∇F(wτ(i,k))∥2 2 ) + 1 N2 E ( ∥ N∑ i=1 ∇F(wτ(i,k))∥2 2 ) ≤E ( ∥g(wk) −∇F(wk)∥2 2 ) + 1 N2 N∑ j=2 E ( ∥g(wτ(j,k)) −∇F(wτ(j,k))∥2 2 ) + E ( ∥∇F(wk)∥2 2 ) + 1 N N∑ j=2 E ( ∥∇F(wτ(j,k))∥2 2 ) ≤σ2 B + Θ BE ( ∥∇F(wk)∥2 2 ) + 1 N2 N∑ j=2 (σ2 B + Θ BE(∥∇F(wτ(j,k))∥2 2) ) + E ( ∥∇F(wk)∥2 2 ) + 1 N N∑ j=2 E ( ∥∇F(wτ(j,k))∥2 2 ) (31) Based on (31), we obtain E(F(wk+1)) ≤E(F(wk)) −η 2(1 −γ)E(∥∇F(wk∥2 2) + Lη2σ2 2B + Lη2σ2 2BN + (Lη2Θ 2B + Lη2 2 − η 2N ) E ( ∥∇F(wk)∥2 2 ) + N∑ j=2 (Lη2Θ 2BN2 + Lη2 2N − η 2N ) E ( ∥∇F(wτ(j,k))∥2 2 ) . (32) In (32), N∑ j=2 (Lη2Θ 2BN2 + Lη2 2N − η 2N ) E ( ∥∇F(wτ(j,k))∥2 2 ) = η 2N N∑ j=2 (LηΘ BN + Lη−1 ) E ( ∥∇F(wτ(j,k))∥2 2 ) ≤− η 4NE ( ∥∇F(wτ(k))∥2 2 ) . (33) 23Therefore, E(F(wk+1)) ≤E(F(wk)) −η 2(1 −γ)E(∥∇F(wk∥2 2) + Lη2σ2 2B + Lη2σ2 2BN + (Lη2Θ 2B + Lη2 2 − η 2N ) E ( ∥∇F(wk)∥2 2 ) − η 4NE ( ∥∇F(wτ(k))∥2 2 ) (34) Owing to η≥ 1 NL( Θ B +1) , we have LΘη B + Lη≥ 1 N and (Lη2Θ 2B + Lη2 2 − η 2N)E ( ∥∇F(wk)∥2 2 ) = η 2(LηΘ B + Lη− 1 N)E ( ∥∇F(wk)∥2 2 ) ≥0. (35) Since −η 4NE ( ∥∇F(wτ(k))∥2 2 ) >−η 4 E ( ∥∇F(wτ(k))∥2 2 ) , we have E(F(wk)) −η 2(1 −γ)E(∥∇F(wk)∥2 2) + Lη2σ2 2B −η 4E(∥∇F(wτ(k))∥2 2) ≤E(F(wk)) −η 2(1 −γ)E(∥∇F(wk∥2 2) + Lη2σ2 2B + Lη2σ2 2BN + (Lη2Θ 2B + Lη2 2 − η 2N)E ( ∥∇F(wk)∥2 2 ) − η 4NE ( ∥∇F(wτ(k))∥2 2 ) . (36) Therefore, switching the asynchronous mode to the synchronous mode in the k+ 1 step may drop in performance. Theorem 4. Based on the above Assumption and η ≤ 1 2L( Θ B +1) , in the synchronous training, the expectation of loss in the k+ 1 step is deduced by E(F(wk+1)) ≤E(F(wk)) −η ( 1 −LηΘ 2NB −Lη 2 ) E(∥∇F(wk)∥2 2) + Lη2σ2 2NB . (37) If we switch to the asynchronous training in the k+ 1 step, the expectation of loss becomes as follows E(F(wk+1)) ≤E(F(wk)) −η(1 −LηΘ 2B −Lη 2 )E(∥∇F(wk))∥2 2 + Lη2σ2 2B , (38) and switching the synchronous mode to the asynchronous mode may drop in performance. proof of Theorem 4. According to the Theorem 2, (37) could be obtained. Next, we will prove (38). E(F(wk+1)) ≤E(F(wk)) −η 2E(∥∇F(wk∥2 2) −η 2E(∥g(wτ(1,k))∥2 2) + η 2E ( ∥g(wτ(1,k)) −∇F(wk)∥2 2 ) + L 2 η2E(∥vk∥2 2). (39) Since E ( ∥g(wτ(1,k)) −∇F(wk)∥2 2 ) = E(∥g(wk)∥2 2) −E(∥∇F(wk)∥2 2), we have E(F(wk+1)) ≤E(F(wk)) −ηE(∥∇F(wk)∥2 2) + Lη2 2 E(∥vk∥2 2). Owing to E(∥vk∥2 2) = E(∥g(wτ(1,k))∥2 2) = E(∥g(wτ(1,k)) −∇F(wk) + ∇F(wk)∥2 2) = E(∥g(wτ(1,k)) −∇F(wk))∥2 2) + E(∥∇F(wk))∥2 2) ≤σ2 B + (Θ B + 1)E(∥∇F(wk))∥2 2, (40) we obtain E(F(wk+1)) ≤E(F(wk)) −η(1 −Lη 2 −LηΘ 2B )E(∥∇F(wk))∥2 2 + Lη2σ2 2B . (41) 24Owing to Lη2σ2 2B > Lη2σ2 2NB and LηΘ 2B > LηΘ 2NB, we have E(F(wk)) −η ( 1 −LηΘ 2NB −Lη 2 ) E(∥∇F(wk)∥2 2) + Lη2σ2 2NB ≤E(F(wk)) −η(1 −LηΘ 2B −Lη 2 )E(∥∇F(wk))∥2 2 + Lη2σ2 2B . (42) Hence, switching the synchronous mode to the asynchronous mode may drop in performance. 25",
      "meta_data": {
        "arxiv_id": "2205.11048v2",
        "authors": [
          "Wenbo Su",
          "Yuanxing Zhang",
          "Yufeng Cai",
          "Kaixu Ren",
          "Pengjie Wang",
          "Huimin Yi",
          "Yue Song",
          "Jing Chen",
          "Hongbo Deng",
          "Jian Xu",
          "Lin Qu",
          "Bo zheng"
        ],
        "published_date": "2022-05-23T05:22:42Z",
        "pdf_url": "https://arxiv.org/pdf/2205.11048v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the challenge of switching between synchronous (all-reduce) and asynchronous (parameter server) training modes for large-scale recommendation models without time-consuming hyper-parameter tuning. It proposes Global Batch gradients Aggregation (GBA), a tuning-free approach that ensures consistent global batch sizes and handles gradient staleness, thereby allowing seamless switching. GBA demonstrates comparable convergence properties with synchronous training and strong robustness against gradient staleness. Experiments show GBA improves AUC by up to 0.2% compared to state-of-the-art asynchronous training and achieves at least 2.4x speedup over synchronous training under strained hardware resources.",
        "methodology": "GBA is implemented on the Parameter Server (PS) architecture and operates by keeping the global batch size (actual batch size of gradient aggregation) consistent with synchronous training. It uses a token-control mechanism where each batch processed by a worker is associated with a token indicating the global step when the batch was sent. PSs maintain a gradient buffer of capacity M (calculated to match the global batch size). When the buffer is full (M gradients collected), these gradients are aggregated and applied. A staleness decay strategy, defined by f(τ(m,k),k) = {0, if k - τ(m,k) > ι; 1, if k - τ(m,k) ≤ ι}, is employed to discard severely stale gradients, where ι is a tolerance threshold. This mechanism allows faster workers to proceed without waiting and mitigates negative impacts from staleness. The paper also provides convergence analysis showing GBA's theoretical properties are comparable to synchronous training, particularly benefiting from the sparsity of recommendation models.",
        "experimental_setup": "Evaluations were conducted on three industrial-scale continual learning recommendation tasks: DeepFM on the Criteo-1TB dataset, DIEN on the Alimama dataset, and YouTubeDNN on a Private dataset with trillions of parameters. Models were implemented in DeepRec with expandable HashTables. The training simulated continual learning by inheriting pre-trained checkpoints, training on daily data, and evaluating on subsequent day's data. Hardware included Tesla-V100S GPUs and Skylake CPUs. Performance was measured using AUC (accuracy) and Global/Local QPS (efficiency). GBA was compared against state-of-the-art PS-based training modes: Bounded Staleness (Hop-BS), Bulk Synchronous Parallel (BSP), Backup Worker (Hop-BW), and standard asynchronous (Async) and synchronous (Sync) training. Hyper-parameters for baselines were tuned to achieve their best AUC, while GBA utilized a staleness tolerance threshold (ι).",
        "limitations": "A current limitation of GBA is that it requires users to manually select the training mode (GBA or synchronous HPC training) based on their judgment of the cluster status. The system does not yet automatically adapt to dynamic cluster conditions.",
        "future_research_directions": "Future work will focus on making GBA adaptive to cluster status, removing the need for manual selection. This involves deriving guidelines for automatic switching based on more in-depth analyses of training trace logs. The problem of automatic switching could be formulated as an optimization problem considering multiple control factors such as overall QPS, training cost, and task scheduling with priority."
      }
    },
    {
      "title": "RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations",
      "abstract": "The training of graph neural networks (GNNs) is extremely time consuming\nbecause sparse graph-based operations are hard to be accelerated by hardware.\nPrior art explores trading off the computational precision to reduce the time\ncomplexity via sampling-based approximation. Based on the idea, previous works\nsuccessfully accelerate the dense matrix based operations (e.g., convolution\nand linear) with negligible accuracy drop. However, unlike dense matrices,\nsparse matrices are stored in the irregular data format such that each\nrow/column may have different number of non-zero entries. Thus, compared to the\ndense counterpart, approximating sparse operations has two unique challenges\n(1) we cannot directly control the efficiency of approximated sparse operation\nsince the computation is only executed on non-zero entries; (2) sub-sampling\nsparse matrices is much more inefficient due to the irregular data format. To\naddress the issues, our key idea is to control the accuracy-efficiency trade\noff by optimizing computation resource allocation layer-wisely and\nepoch-wisely. Specifically, for the first challenge, we customize the\ncomputation resource to different sparse operations, while limit the total used\nresource below a certain budget. For the second challenge, we cache previous\nsampled sparse matrices to reduce the epoch-wise sampling overhead. Finally, we\npropose a switching mechanisms to improve the generalization of GNNs trained\nwith approximated operations. To this end, we propose Randomized Sparse\nComputation, which for the first time demonstrate the potential of training\nGNNs with approximated operations. In practice, rsc can achieve up to\n$11.6\\times$ speedup for a single sparse operation and a $1.6\\times$ end-to-end\nwall-clock time speedup with negligible accuracy drop.",
      "full_text": "RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations Zirui Liu 1 Shengyuan Chen 2 Kaixiong Zhou 1 Daochen Zha 1 Xiao Huang 2 Xia Hu 1 Abstract Training graph neural networks (GNNs) is ex- tremely time-consuming because sparse graph- based operations are hard to be accelerated by community hardware. Prior art successfully reduces the computation cost of dense matrix based operations (e.g., convolution and linear) via sampling-based approximation. However, unlike dense matrices, sparse matrices are stored in an irregular data format such that each row/column may have a different number of non-zero entries. Thus, compared to the dense counterpart, approx- imating sparse operations has two unique chal- lenges (1) we cannot directly control the effi- ciency of approximated sparse operation since the computation is only executed on non-zero entries; (2) sampling sparse matrices is much more ineffi- cient due to the irregular data format. To address the issues, our key idea is to control the accuracy- efficiency trade-off by optimizing computation re- source allocation layer-wisely and epoch-wisely. For the first challenge, we customize the com- putation resource to different sparse operations, while limiting the total used resource below a cer- tain budget. For the second challenge, we cache previously sampled sparse matrices to reduce the epoch-wise sampling overhead. To this end, we propose Randomized Sparse Computation. In practice, RSC can achieve up to 11.6× speedup for a single sparse operation and 1.6× end-to- end wall-clock time speedup with almost no ac- curacy drop. Codes are available at https:// github.com/warai-0toko/RSC-ICML. 1Department of Computer Science, Rice University, Houston, TX, USA 2Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong SAR. Correspondence to: Xia Hu <xia.hu@rice.edu>. Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s). 1. Introductions Graph Neural Networks (GNNs) have achieved great suc- cess across different graph-related tasks (Hamilton et al., 2017; Hu et al., 2020; Ying et al., 2018; Jiang et al., 2022; Zhou et al., 2022; 2023). However, despite its effective- ness, the training of GNNs is very time-consuming. Specifi- cally, GNNs are characterized by an interleaved execution that switches between the aggregation and update phases. Namely, in the aggregation phase, every node aggregates messages from its neighborhoods at each layer, which is implemented based on sparse matrix-based operations (Fey & Lenssen, 2019; Wang et al., 2019). In the update phase, each node will update its embedding based on the aggre- gated messages, where the update function is implemented with dense matrix-based operations (Fey & Lenssen, 2019; Wang et al., 2019). In Figure 1, SpMM and MatMul are the sparse and dense operations in the aggregation and update phases, respectively. Through profiling, we found that the aggregation phase may take more than 90% running time for GNN training. This is because the sparse matrix opera- tions in the aggregation phase have many random memory accesses and limited data reuse, which is hard to be acceler- ated by community hardwares (e.g., CPUs and GPUs) (Duan et al., 2022b; Han et al., 2016; Duan et al., 2022a). Thus, training GNNs with large graphs is often time-inefficient. ognb-proteins Reddit ogbn-product0 20 40 60 80 100Percentage of Time Consumption Other MatMul(forward) MatMul(backward) SpMM(forward) SpMM(backward) Figure 1: The time profiling of a two-layer GCNs on dif- ferent datasets. SpMM may take 70% ∼ 90% of the total time. We measure the time on a single NVIDIA RTX3090 (24GB). The detailed software and hardware information can be found in Appendix D. Existing works towards this problem can be roughly divided into three categories. First, some works propose distributed GNNs training systems, which focus on minimizing the 1 arXiv:2210.10737v2  [cs.LG]  2 Jul 2023RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations communication cost among hardware (Zheng et al., 2020; Ramezani et al., 2022; Wan et al., 2022b; Md et al., 2021; Wan et al., 2022a). Second, another research line optimizes the memory access pattern of sparse operations via coalesc- ing the memory access and fusing consecutive operations (Zhang et al., 2022; Huang et al., 2020a; Rahman et al., 2021; Wang et al., 2021). Third, some other works try to accelerate the training process from the optimization aspect, i.e., using fewer iterations to converge (Narayanan et al., 2022; Cong et al., 2020; Xu et al., 2021; Cai et al., 2021). In parallel, an orthogonal direction is to replace the ex- pensive operations with their faster-approximated versions (Adelman et al., 2021; Drineas et al., 2006b). The key idea is to sub-sample tensors onto low dimensional spaces and perform the original operations here. For example, for the linear operation between two matrices A ∈ Rn×m and B ∈ Rm×q, we first obtain A′ ∈ Rn×k and B′ ∈ Rk×q (k < m) by picking k representative columns of A and the corresponding rows of B (Drineas et al., 2006b). Then we approximate AB ≈ A′B′. With this procedure, the number of floating-point operations (FLOPs) and memory access are both reduced. Based on the idea, previous work success- fully accelerates the dense matrix based operations, such as convolution and linear operations (Adelman et al., 2021). The approximated operation can plug-and-play replace the exact operation to improve per-operation efficiency, and thus is compatible with most of the efficient training methods. Despite the potential, this perspective however has not been explored for the sparse operations in GNNs. The approximation method reduces the computational com- plexity at the cost of giving noisy outputs. Thus, there naturally exists an accuracy-efficiency trade-off. Com- pared to approximating dense matrix operations, there are two unique challenges to optimizing the trade-off for ap- proximated sparse operations. First, unlike the previous example of approximating linear operation, k cannot di- rectly control the efficiency (FLOPs) for sparse operations. This is because, for dense matrices, each row/column has the same amount of parameters. Thus the reduction of FLOPs in approximated dense operations is determined by the dimensions of the sub-sampled matrices (i.e., k). How- ever, in sparse operations, each row/column in the sparse adjacency matrix has different numbers of non-zero en- tries, and the computation is only executed on non-zero entries (i.e., irregular data format). Thus, the reduction of FLOPs in the sparse operations is decided by the selection of representative rows/columns. It lacks a mechanism to directly control the efficiency-accuracy trade-off for each sparse operation. Second, compared to the dense counter- part, sub-sampling (i.e., slicing) the sparse matrix is much more time-consuming due to its irregular data format (Han et al., 2016; Fey & Lenssen, 2019), which counteracts the acceleration from the FLOPs reduction. To this end, we propose Randomized Sparse Computation, dubbed RSC , the first approximation framework tailored for efficient GNN training. Our core idea is to control the trade-off by optimizing the computation resource alloca- tion at the “global” level. Specifically, to tackle the first challenge, at the layer-wise level, we propose to customize the FLOPs of each sparse operation while limiting the total FLOPs under a certain budget. The rationale behind this strategy is that each operation may have a different contribu- tion to the model accuracy. Thus, we could to assign more computational resources to “important” operations under a certain budget. More concretely, we frame it as a constraint optimization problem. Then we propose a greedy algorithm to solve it efficiently. To tackle the second challenge, at the epoch-wise level, we found that the selection of represen- tative row/columns tends to remain similar across nearby iterations. Based on this finding, we develop a caching mechanism to reuse the previously sampled sparse matrix across nearby iterations to reduce per-epoch sampling time. Finally, inspired by the recent finding that the final stage of training usually needs smaller noise to help convergence (Li et al., 2019; Dao et al., 2022), we propose to use approxi- mated sparse operation during most of the training process, while switching back to the original sparse operation at the final stage. This switching mechanism significantly reduces the accuracy drop, at the cost of slightly less speedup. We summarize our contributions as follows: • We accelerate the training of GNNs from a new perspec- tive, namely, replacing the expensive sparse operations with their faster-approximated versions. • Instead of focusing on balancing the efficiency-accuracy trade-off at the operation level, we control the trade-off through optimizing resource allocation at the layer-wise and epoch-wise levels. • We propose a caching mechanism to reduce the cost of sampling sparse matrices by reusing previous results. • Extensive experiments have demonstrated the effective- ness of the proposed method. Particularly, RSC can achieve up to 11.6× speedup for a single sparse opera- tion and a 1.6× end-to-end wall-clock time speedup with negligible (≈ 0.3%) accuracy drop. 2. Background and Preliminary 2.1. Graph Neural Networks Let G = ( V, E) be an undirected graph with V = (v1, ··· , v|V|) and E = (e1, ··· , e|E|) being the set of nodes and edges, respectively. Let X ∈ R|V|×d be the node feature matrix. A ∈ R|V|×|V| is the graph adjacency matrix, where Ai,j = 1 if (vi, vj) ∈ Eelse Ai,j = 0. 2RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations ˜A = ˜D−1 2 (A + I) ˜D−1 2 is the normalized adjacency ma- trix, where ˜D is the degree matrix of A + I. GNNs re- cursively update the embedding of a node by aggregating embeddings of its neighbors. For example, the forward pass of the lth Graph Convolutional Network (GCN) layer (Kipf & Welling, 2017) can be defined as: H(l+1) = ReLU( ˜AH(l)Θ(l)), (1) where H(l) is the node embedding matrix at the lth layer and H(0) = X. Θ(l) is the weight matrix of the lth layer. In practice, ˜A is often stored in the sparse matrix format, e.g., compressed sparse row (CSR) (Fey & Lenssen, 2019). From the implementation aspect, the computation of Equa- tion (1) can be described as: H(l+1) = ReLU   SpMM \u0012 ˜A, MatMul(H(l), Θ(l)) \u0013! , where SpMM(·, ·) is the Sparse-Dense Matrix Multiplica- tion and MatMul(·, ·) is the Dense Matrix Multiplication. Sparse operations, such as SpMM , have many random mem- ory accesses and limited data reuse. Thus they are much slower than the dense counterpart (Han et al., 2016; Duan et al., 2022b). To get a sense of the scale, we show in Figure 1 that for GCNs, SpMM may take roughly 70% ∼ 90% of the total training time. 2.2. Fast Approximated MatMul with Sampling Let X ∈ Rn×m, Y ∈ Rm×q. The goal is to efficiently esti- mate the matrix production XY . Truncated Singular Value Decomposition (SVD) outputs provably optimal low-rank estimation of XY (Adelman et al., 2021). However, SVD is almost as expensive as matrix production itself. Instead, the sampling algorithm is proposed to approximate the matrix product XY by sampling k columns of X and correspond- ing rows of Y to form smaller matrices, which are then multiplied as usual (Drineas et al., 2006b). This algorithm reduces the computational complexity from O(mnq) to O(knq). Specifically, XY = mX i=1 X:,iYi,: ≈ kX t=1 1 st X:,itYit,: = approx(XY ), (2) where X:,i ∈ Rn×1 and Yi,: ∈ R1×q are the ith column and row of X and Y , respectively. In this paper, we call (X:,i, Yi,:) the ith column-row pair. k is the number of sam- ples (1 ≤ k ≤ m). {pi}m i=1 is a probability distribution over the column-row pairs. it ∈ {1, ··· m} is the index of the sampled column-row pair at the tth trial. st is the scale fac- tor. Theoretically, (Drineas et al., 2006b) shows that if we set st = 1 kpit , then we have E[approx(XY )] =XY . Fur- ther, the approximation errorE[||XY −approx(XY )||F ] is minimized when the sampling probabilities {pi}m i=1 are proportional to the product of the column-row Euclidean norms (Drineas et al., 2006b): pi = ||X:,i||2 ||Yi,:||2Pm j=1 ||X:,j||2 ||Yj,:||2 . (3) 2.2.1. T OP-k SAMPLING The above sampling-based method is originally developed for accelerating the general application ofMatMul (Drineas et al., 2006b). Directly applying it to neural networks may be sub-optimal since it does not consider the characteristic of neural network weights. Based on the empirical observation that the distribution of weights remains centered around zero during training (Glorot & Bengio, 2010; Han et al., 2015), (Adelman et al., 2021) proposes a top-k sampling algorithm: Picking k column-row pairs with the largest ||X:,i||2 ||Yi,:||2Pm j=1 ||X:,j||2 ||Yj,:||2 deterministically without scaling. Equivalently, it means pi of column-row pairs with the k- largest value in Equation (3) equals 1, otherwise it equals 0. And sit is a constant 1. Albeit without the scaling while sampling column-row pairs deterministically, under on the assumption of zero-centered weight distribution, (Adelman et al., 2021) theoretically show that top-k sampling still yields an unbiased estimation of XY with minimal approx- imation error. Consequently, the top-k sampling algorithm empirically shows a significantly lower accuracy drop when approximating the convolution and linear operations in the neural networks (Adelman et al., 2021). In the next section, we explore how to approximate the expensive sparse operation via the top-k sampling. 3. The Proposed Framework The overview of RSC is shown in Figure 2, where we use the computation graph of GCN as an example. We first explore which SpMM in the computation graph can be re- placed with its approximated version (Section 3.1). Then since GNNs have multiple SpMM and each of them may have different importance to the model performance, we then automatically allocate computation resources to dif- ferent SpMM (Section 3.2). Finally, we explore two simple and effective tricks for improvingRSC , including a caching mechanism to reduce the overhead of sampling sparse ma- trices (Section 3.3.1) and a switching mechanism to reduce the accuracy drop (Section 3.3.2). 3.1. Where to Apply the Approximation 3.1.1. E XPERIMENTAL ANALYSIS Each sparse operation is executed twice at each train- ing step, i.e., one in the forward pass and the other one 3RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations SPMM MatMul Approx SpMM MatMul Forward Pass Backward Pass Down Sampl ing Cache Caching (Sec 3.3.1) Down Sampl ing Constraint  Optimization Eq. 5 𝑘!  Resource Allocation (Sec 3.2) Θ(!) 𝑯(!$%) 𝛁𝑯(!$%) 𝛁Θ(!) 𝛁𝑯(!)𝑯(!) 𝑱(!) 𝛁𝑱(!) 𝑨' Figure 2: Overview of RSC . For convenience, ReLU is ignored. RSC only replace the SpMM in the backward pass with its approximated version using top-k sampling (Section 3.1). kl is the number of samples for top-k sampling at the lth layer, which is automatically allocated (Section 3.2). To reduce the overhead of sampling, we also cache the sampled graph and reuse it across nearby iterations (Section 3.3). in the backward pass. As shown in Figure 2, here we take SpMM in the lth GCN layer as an example, the for- ward one is H(l+1) = ReLU(SpMM( ˜A, J(l))), where J(l) = MatMul(H(l), Θ(l)) is the intermediate node representations. And the backward one is ∇J(l) = SpMM( ˜A⊤, ∇H(l+1)). ∇J(l) and ∇H(l) are the gradient with respect to J(l) and H(l), respectively. Even though the approximation method itself is statisti- cally unbiased, replacing the exact sparse operation with their faster-approximated versions still injects noise to the computation graph. As we analyzed above, each SpMM is executed twice in the training step. Below we first exper- imentally analyze the impact of the injected noise in the forward pass and the backward pass. As shown in Table 1, we apply top-k sampling to approximate the SpMM in the forward pass, backward pass, or both, respectively. Table 1: Preliminary results on approximatingSpMM via top- k sampling. The model is a two-layer GCN, and the dataset is Reddit. Here we set thek as 0.1|V| across different layers. Method Reddit without approximation 95.39±0.04 only forward 16.45±0.39 only backward 95.25±0.03 forward and backward 80.74±1.00 From Table 1, the accuracy drop is negligible if we only replace SpMM in the backward pass. Notably, if we apply ap- proximation in both the forward and backward pass, the re- sult is significantly better than only applying top-k sampling in the forward pass. The reason is that when only apply- ing approximation in the forward pass, some row/columns are not included in the computation graph, so intuitively these row/columns should be excluded in the backward pass. “forward and backward” result in Table 1 is built based on this intuition such that in the backward pass, we use the column-row pairs sampled in the forward pass to compute the gradient (Adelman et al., 2021). However, it is still not comparable to the result of applying approximation only in the backward pass. Below we mathematically analyze the reason behind the results in Table 1. 3.1.2. T HEORETICAL ANALYSIS We first analyze the case of approximating the sparse opera- tions in the forward pass. Namely, replacingSpMM( ˜A, J(l)) with approx( ˜AJ(l)). We note that we have E[f(x)] ̸= f(E[x]) for any non-linear function f(·), e.g., E[x2] ̸= E2[x]. Thus, even when the approximation method gives an unbiased estimation, i.e., E[approx( ˜AJ(l))] = ˜AJ(l), the node embeddings H(l+1) are still biased since the acti- vation function is non-linear. To see this, E[H(l+1)] =E[ReLU(approx( ˜AJ(l))]) ̸= ReLU(E[approx( ˜AJ(l))]) =H(l+1). Thus, if we apply the approximation for the SpMM in the forward pass, the bias will be propagated layer-by-layer and cause significantly worse results. For the case of only approximating the sparse operation in the backward pass, we have the following proposition: Proposition 3.1 (Proof in Appendix A). If the approxima- tion method is itself unbiased, and we only replace theSpMM in the backward pass with its approximated version, while leaving the forward one unchanged, then the calculated gradient is provably unbiased. The high-level idea is that the gradient of the activation function in the backward pass is only related to the pre- activations in the forward pass, and thus is independent of the approximation error introduced in the backward pass. Due to the page limit, we also discuss why sampling-based approximation is suitable for accelerating GNNs in Ap- pendix A. As suggested by our theoretical and empirical analysis, as shown in Figure 2, we only approximate the sparse operations in the backward pass, while leaving all other operations unchanged. 3.2. How to Apply the Approximation As we mentioned, for sparse operations, the acceleration is decided by the selection of sampled column-row pairs. To see this, as shown in Figure 3, suppose we use top- k sampling to approximate SpMM( ˜A⊤, ∇H). Since the computations are only executed on the non-zero entries, so selecting the orange pairs (i.e., pair 1 and 3) will result in 3 7 × less computational cost (FLOPs) compared to selecting 4RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations 1 11 111111132100123 3210∇𝐻!!∇𝐻!\"∇𝐻!#∇𝐻\"! ∇𝐻#!∇𝐻$! ∇𝐻\"\" ∇𝐻#\"∇𝐻$\" ∇𝐻\"# ∇𝐻##∇𝐻$# Nodeembeddinggradients∇𝐻∈ℝ!×#,with𝑑=3Sparseadjacencymatrix𝐴$∈ℝ!×!,with𝑁=4 × Figure 3: For approximated sparse operations, the accelera- tion is decided by the selection of column-row pairs. the blue pair (i.e., pair 0 and 2). For both the orange and blue cases, we have k = 2. Thus, the number of samples k cannot directly constrain the FLOPs for each individual operation. Moreover, a GNN has multiple operations (or layers), and the model accuracy has a different sensitivity to the approximation error at different layers. To optimize the accuracy-efficiency trade-off, our key idea is to customize the computation resources (i.e., FLOPs) for each layer by adjusting the number of samples kl in the l-th layer. In this way, we minimize the impact of approximation, while limiting the overall FLOPs under a certain budget. Based on the idea, we frame the resource allocation problem as the following constrained optimization problem: min {kl} − LX l=1 X i∈Topkl ∥ ˜A⊤ :,i∥2∥∇H(l+1) i,: ∥2 ∥ ˜A∥F ∥∇H(l+1)∥F , (4a) s.t. LX l=1 X i∈Topkl #nnzi ∗ dl ≤ C LX i=1 |E|dl, (4b) where C is the budget (0 < C <1) that controls the overall reduced FLOPs. kl is the number of samples for the top-k sampling at the l-th layer. dl is the hidden dimensions ofl-th layer, and #nnzi is the number of non-zero entries at the i-th column of ˜A⊤. Topkl is the set of indices associated with the kl largest ∥ ˜A⊤ :,i∥2∥∇H(l+1) i,: ∥2. Equation (4a) is equivalent to minimizing the relative ap- proximation error E[|| ˜A⊤∇H(l+1)−approx( ˜A⊤∇H(l+1))||F ∥ ˜A∥F ∥∇H(l+1)||F ] summarized over all layers (Adelman et al., 2021). Also, different sparse operations are weighted summation by the magnitude of gradient ∥∇H(l+1)∥2, which implicitly en- codes the importance of different operations. Equation (4b) is the constraint that controls the overall FLOPs. Specifically, the FLOPs of SpMM between ˜A and the gradient ∇H ∈ RN×d is O(|E|d) and P j∈V #nnzj = |E|. We note that Equation (4b) also bounds the number of memory access of SpMM . 3.2.1. GREEDY SOLUTION The above combination optimization objective is NP-hard, albeit it can be solved by dynamic programming. However, dynamic programming is very slow, which somehow con- tradicts our purpose of being efficient. Thus, we propose to use a greedy algorithm to solve it. Specifically, it starts with the highest kl = |V| for all layers. In each move, it chooses a kl among {kl}L l=1 to reduce by a step size (e.g., 0.02|V|), such that the increment of errors in Equation (4a) is mini- mal. The greedy algorithm will stop when the current total FLOPs fits in the budget in Equation (4b). This algorithm runs super fast, and we found that it has minimal impact on efficiency. We provide the pseudo-code of our greedy algorithm in Algorithm 1 of Appendix B. 3.3. When to Apply the Approximation 3.3.1. CACHE THE SAMPLED SPARSE MATRICES We first give the details about the Compressed Sparse Row (CSR) format for representing the sparse matrix here. CSR stores nonzero values in a matrix and their position in three arrays: index array Rowptr, column array Col, and value array Val. The elements in Rowptr act as the starting indices of the elements in Col and Val that correspond to each row. Specifically, the elements of row i are stored in indices Rowptr[i] to Rowptr[i+ 1] − 1 of Col and Val . The elements in Col and Val are the column index and value in that column, respectively. Figure 5 shows the CSR format of the matrix shown in Figure 3. We ignore the Val array here for illustration convenience. Executing the top- k sampling contains two steps: First, it decides the indices corresponding to the top- k largest column row norms in Equation (3). Second, slicing the matrices according to the indices. In practice, the overhead of the first step can be ignored. However, unlike dense matrices, slicing the adjacency matrix is much slower due to its irregular data format. To see this, suppose the top-k indices of the sparse matrix in Figure 3 correspond to the orange column-row pairs. Figure 5 shows the process of slicing the adjacency matrix in CSR format by reserving only the orange columns. Slicing sparse matrices requires to re-process the graph to build the new Rowptr and Col (Fey & Lenssen, 2019), which introduces significant time overhead, especially for large graphs. For the full graph training, we use the same adjacency matrix across different epochs1. We made a crucial observation that the top-k indices in the adjacency matrix tend to be the same across iterations. In Figure 4, we plot the AUC score of top- k indices between every iteration t and iteration t + 10for 1For sub-graph based training, we can first sample all of the sub-graphs offline. Then during the training, we apply the caching mechanism to each sampled graph. 5RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations 0 50 100 150 200 250 300 350 Epochs 0.0 0.2 0.4 0.6 0.8 1.0Matching score Reddit GCN 1st layer GCN 2nd layer GraphSAGE 1st layer GraphSAGE 2nd layer 0 50 100 150 200 250 300 350 Epochs 0.0 0.2 0.4 0.6 0.8 1.0Matching score Yelp GCN 1st layer GCN 2nd layer GraphSAGE 1st layer GraphSAGE 2nd layer 0 50 100 150 200 250 300 350 Epochs 0.0 0.2 0.4 0.6 0.8 1.0Matching score ogbn-proteins GCN 1st layer GCN 2nd layer GraphSAGE 1st layer GraphSAGE 2nd layer Figure 4: For each layer, the selected column-row pairs tend to be very similar across iterations. Models here are two-layer GCN and GraphSAGE. Here we show the matching scores (AUC) of top-k indices between every 10 steps. Figure 5: The process of slicing the sparse matrix in Figure 3 by only reserving orange columns (in CSR format). each layer throughout the whole training process. Here we note that AUC score is a commonly used ranking measure and a 1.0 AUC score means the ranking of column-row pairs is identical across iterations. The results in Figure 4 indicate that the top-k indices won’t change significantly within a few iterations. Thus, as shown in Figure 2, we propose to reuse the sampled adjacency matrix for each layer across nearby iterations. Discussion. The rationale behind the success of caching is the slow rate of change in the learned embeddings within GNNs (Fey et al., 2021; Wan et al., 2022a). Prior research has leveraged this “staleness” of embeddings to enhance the efficiency of GNN training [1, 2]. The success of caching can also be explained by the staleness: if embeddings (and their gradients) across consecutive steps remain nearly iden- tical, the sampled sparse matrix will also exhibit minimal variation. Later we experimentally show that the caching mechanism does not impact the model performance a lot, but leads to a significant speedup. 3.3.2. SWITCH BACK AT THE END When training neural networks, the common practice is to use a large learning rate for exploration and anneal to a small one for final convergence (Li et al., 2019). The ratio- nale behind this strategy is that, at the end of the training process, we need to fine-tune our model with small noise for convergence. Since our approximation sparse operations will bring extra noise to the gradient, intuitively, we can switch back to the original sparse operations to help con- vergence. More formally, we propose to use approximated sparse operation during most of the training process, while switching back to the original sparse operation at the final stage. We experimentally show that this switching mecha- nism significantly reduces the accuracy drop at the cost of slightly less acceleration effect. We note that the switching mechanism is not proposed in this paper. The switching mechanism takes inspiration from previous work Dao et al. (2022), and both our work and Dao et al. (2022) utilize the switching mechanism to minimize the impact of approximation. 4. Related work and Discussion Due to the page limit, we first discuss the related work on approximated matrix multiplication. Other related topics, i.e., subgraph-based training, randomized GNN training, and non-approximated GNN acceleration, can be found in Appendix C. Approximated Matrix Multiplication.The approximated matrix production can be roughly divided into three cat- egories. However, only a few of them can be used for accelerating GNN training. Specifically, (1) Random walk- based methods (Cohen & Lewis, 1999) performs random walks on a graph representation of the dense matrices, but is only applicable to non-negative matrices; (2) Butterfly- based methods (Chen et al., 2021; Dao et al., 2022) replace dense matrices with butterfly matrices. It is not applicable to SpMM in GNNs because the adjacency matrix often cannot be reduced to a butterfly matrix. (3) Column-row sampling methods(Drineas et al., 2006a; Drineas & Kannan, 2001) sample the input matrices with important rows and columns, then perform the production on the sampled matrix as usual. 5. Limitations First, to guarantee the model accuracy, we only replace the sparse operation in the backward pass. Thus the upper bound of RSC ’s speedup is limited. However, we note that the backward pass usually is more time-consuming than the forward pass, which is also empirically shown in Table 2. Second, some GNNs rely on the scatter-and-gather instead of SpMM (and its variant) to perform the aggregation, such as GAT (Veliˇckovi´c et al., 2017). They are not covered in this paper. However, scatter-and-gather based GNNs can also 6RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations be accelerated by RSC because the column-row sampling is also applicable to scatter and gather operation. Similarly, the caching and switching mechanisms are also applicable to them. However, for the resource allocation Algorithm 1, the scatter and gather operations require tailored error bound and the computation cost modeling in Equation (4). We leave it as future work. 6. Experiments We verify the effectiveness of our proposed framework via answering the following research questions: Q1: How ef- fective is RSC in terms of accuracy with reduced training time? Q2: How effective is our proposed allocation strategy compared to the uniform allocation strategy? Q3: What is the layer-wise ratio assigned by RSC ? Q4: How effec- tive is the caching and switching mechanism in terms of the trade-off between efficiency and accuracy? If without explicitly mentioned, all reported results are averaged over ten random trials 6.1. Experimental Settings Datasets and Baselines. To evaluateRSC , we adopt four common large-scale graph benchmarks from different do- mains, i.e., Reddit (Hamilton et al., 2017), Yelp (Zeng et al., 2020), ogbn-proteins (Hu et al., 2020), and ogbn- products (Hu et al., 2020). We evaluate RSC under both the mini-batch training and full-batch training settings. For the mini-batch training setting, we integrate RSC with one of the state-of-the-art sampling methods, GraphSAINT (Zeng et al., 2020). For the full-batch training setting, we inte- grate RSC with three popular models: two commonly used shallow models, namely, GCN (Kipf & Welling, 2017) and GraphSAGE (Hamilton et al., 2017), and one deep model GCNII (Chen et al., 2020). To avoid creating confusion, GCN, GraphSAGE, and GCNII are all trained with the whole graph at each step. For a fair comparison, we use the MEAN aggregator for GraphSAGE and GraphSAINT throughout the paper. Details about the hyperparameters and datasets are in Appendix D. Hyperparameter settings. RSC contains three parts. First, the allocation strategy. We choose the overall budget C in Equation (4b) from {0.1, 0.3, 0.5}. We run the resource allocation strategy every ten steps. The step size α in Algo- rithm 1 is set as 0.02|V|. Second, the caching mechanism. According to Figure 4, we sample the adjacency matrix every ten steps and reuse the sampled matrices for nearby steps. Third, the switching mechanism, where we apply RSC for 80% of the total epochs, while switching back to the original operations for the rest of the 20% epochs. Due to the page limit, We present a detailed hyperparameter study in Appendix E Figure 11 and Figure 12. Evaluation metrics. To evaluate the practical usage of RSC , we report the wall clock time speedup measured on GPUs. Specifically, the speedup equalsTbaseline/Trsc, where Tbaseline and Trsc are the wall clock training time of baseline and RSC , respectively. We note that the Trsc includes the running time of the greedy algorithm, and the effects of caching and switching. 6.2. Performance Analysis 6.2.1. A CCURACY -EFFICIENCY TRADE -OFF To answer Q1, we summarize the speedup and the test accuracy/F1-micro/AUC of different methods in Table 3. Since RSC accelerates the sparse operation in the backward pass, we also provide the detailed efficiency analysis in Table 2. In summary, we observe: ❶ At the operation level, RSC can accelerate the sparse operation in the backward pass by up to 11.6×. For end- to-end training, the accuracy drop of applying RSC over baselines is negligible (0.3%) across different models and datasets, while achieving up to 1.6× end-to-end wall clock time speedup. The gap between the operation speedup and the end-to-end speedup is due to the following two reasons. First, we focus on accelerating the sparse computations in GNNs, which is the unique bottleneck to GNNs. The other dense computations can certainly be accelerated by approximation methods, but this is beyond the scope of this paper. Second, we only accelerate the sparse computation in the backward pass instead of the forward one to guaran- tee performance. We note that for approximation methods that accelerate the training process at operation level, a 1.2 ≈ 1.3× wall-clock speedup with negligible accuracy drop can be regarded as non-trivial (for details, please see Table 1 in (Adelman et al., 2021)), especially considering that these approximation methods are orthogonal to most of the existing efficient training methods. For GraphSAINT, the speedup of RSC is around 1.1×, which is smaller than the full graph training. This is because for subgraph-based training, the equivalent “batch size” is much smaller than the full graph counterparts. As a result, the GPU utility is low since it does not assign each processor a sufficient amount of work and the bottleneck is the mini-batch transfer time (Kaler et al., 2022). We note that the mini-batch sampling and transfer time can be optimized from the system perspec- tive (Kaler et al., 2022), which is orthogonal to our work. The speedup is expected to be larger when the mini-batch sampling time is optimized. 6.2.2. A BLATION ON RESOURCE ALLOCATION . Due to the page limit, we first show the running time of the greedy algorithm in Appendix E Table 11. We conclude that the overhead of the greedy algorithm is negligible com- pared to the acceleration effect of RSC . To answer Q2, we 7RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations Table 2: Comparison on the efficiency at the operation level. fwd/bwd is the wall-clock time for a single forward/backward pass (ms). SpMM MEAN corresponds to the MEAN aggregator used in GraphSAGE (Appendix A.3). Reddit Yelp ogbn- proteins ogbn- products fwd bwd fwd bwd fwd bwd fwd bwd SpMM Baseline 36.28 44.23 26.88 34.38 31.72 42.99 261.03 316.80 +RSC - 3.81 (11.6 ×) - 9.86 (3.49 ×) - 14.87 (2.89 ×) - 35.28 (8.98 ×) SpMMMEAN (Appendix A.3) Baseline 36.21 44.27 26.78 34.38 31.80 43.11 261.03 316.84 +RSC - 7.47 (5.92 ×) - 19.62 (1.75 ×) - 5.22 (8.26 ×) - 71.59 (4.43 ×) Table 3: Comparison on the test accuracy/F1-micro/AUC and speedup on four datasets. Bold faces indicate the accuracy drop is negligible (≈ 0.3%) or the result is better compared to the baseline.The hardware here is a RTX3090 (24GB). # nodes # edges 230K 11.6M 717K 7.9M 132K 39.5M 2.4M 61.9M Model Methods Reddit Yelp ogbn- proteins ogbn- products Acc. Budget C Speedup F1-microBudget C Speedup AUC Budget C Speedup Acc. Budget C Speedup Graph- SAINT Baseline 96.40±0.03 1 1 × 63.30±0.14 1 1 × — — — 79.01±0.21 1 1 × +RSC 96.24±0.030.1 1.11 × 63.34±0.180.1 1.09 × — — — 78.99±0.32 0.3 1.04 × GCN Baseline 95.33±0.03 1 1 × 44.28±1.04 1 1 × 71.99±0.66 1 1 × 75.74±0.11 1 1 × +RSC 95.13±0.050.1 1.47 × 46.09±0.540.1 1.17 × 71.60±0.450.3 1.51 × 75.44±0.21 0.3 1.35 × GraphSAGE (full batch) Baseline 96.61±0.05 1 1 × 63.06±0.18 1 1 × 76.09±0.77 1 1 × 78.73 ± 0.12 1 1 × +RSC 96.52±0.040.1 1.32 × 62.89±0.190.1 1.13 × 76.30±0.420.3 1.60 × 78.50± 0.090.1 1.53 × GCNII Baseline 96.71±0.07 1 1 × 63.45±0.17 1 1 × 73.79±1.32 1 1 × — — — +RSC 96.50±0.120.3 1.45 × 63.57±0.210.1 1.19 × 75.20±0.540.5 1.41 × — — — /uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000014/uni00000014/uni00000011/uni00000015/uni00000014/uni00000011/uni00000016/uni00000014/uni00000011/uni00000017/uni00000014/uni00000011/uni00000018 /uni00000036/uni00000053/uni00000048/uni00000048/uni00000047/uni00000058/uni00000053 /uni0000001a/uni00000018 /uni0000001b/uni00000013 /uni0000001b/uni00000018 /uni0000001c/uni00000013 /uni0000001c/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c /uni0000002a/uni00000026/uni00000031/uni00000003/uni0000000b/uni00000035/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000057/uni0000000c /uni00000038/uni00000051/uni0000004c/uni00000049/uni00000052/uni00000055/uni00000050/uni00000003/uni00000024/uni0000004f/uni0000004f/uni00000052/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni00000035/uni00000036/uni00000026 /uni00000014/uni00000011/uni00000014/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000015/uni00000013/uni00000013/uni00000014/uni00000011/uni00000015/uni00000015/uni00000018/uni00000014/uni00000011/uni00000015/uni00000018/uni00000013/uni00000014/uni00000011/uni00000015/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000016/uni00000013/uni00000013/uni00000014/uni00000011/uni00000016/uni00000015/uni00000018 /uni00000036/uni00000053/uni00000048/uni00000048/uni00000047/uni00000058/uni00000053 /uni0000001c/uni00000019/uni00000011/uni00000018/uni0000001b /uni0000001c/uni00000019/uni00000011/uni00000019/uni00000013 /uni0000001c/uni00000019/uni00000011/uni00000019/uni00000015 /uni0000001c/uni00000019/uni00000011/uni00000019/uni00000017/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c /uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000036/uni00000024/uni0000002a/uni00000028/uni00000003/uni0000000b/uni00000035/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000057/uni0000000c /uni00000038/uni00000051/uni0000004c/uni00000049/uni00000052/uni00000055/uni00000050/uni00000003/uni00000024/uni0000004f/uni0000004f/uni00000052/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni00000035/uni00000036/uni00000026 /uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000014/uni00000014/uni00000011/uni00000015/uni00000014/uni00000011/uni00000016/uni00000014/uni00000011/uni00000017 /uni00000036/uni00000053/uni00000048/uni00000048/uni00000047/uni00000058/uni00000053 /uni0000001c/uni00000018/uni00000011/uni00000018 /uni0000001c/uni00000019/uni00000011/uni00000013 /uni0000001c/uni00000019/uni00000011/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c /uni0000002a/uni00000026/uni00000031/uni0000002c/uni0000002c/uni00000003/uni0000000b/uni00000035/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000057/uni0000000c /uni00000038/uni00000051/uni0000004c/uni00000049/uni00000052/uni00000055/uni00000050/uni00000003/uni00000024/uni0000004f/uni0000004f/uni00000052/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051 /uni00000035/uni00000036/uni00000026 Figure 6: The Pareto frontier of the accuracy-efficiency trade-off for RSC and the uniform allocation. Here we disabled the caching and switch mechanism for a fair comparison. More results can be found in Appendix E Table 4: Ablation on the caching and switching mechanism. Experiments are conducted on ogbn-proteins. All results are averaged over five random trials. Ablation on Caching Switching AUC Speedup GCN ✗ ✗ 71.60 ± 0.66 1.19 × ✗ ✓ 72.19 ± 0.79 1.14 × ✓ ✗ 69.80 ± 0.60 1.60 × ✓ ✓ 71.60 ± 0.45 1.51 × GraphSAGE ✗ ✗ 75.23 ± 0.79 1.37 × ✗ ✓ 76.39 ± 0.39 1.32 × ✓ ✗ 75.53 ± 0.60 1.78 × ✓ ✓ 76.30 ± 0.42 1.60 × GCNII ✗ ✗ 74.07 ± 0.83 1.10 × ✗ ✓ 74.50 ± 0.52 1.04 × ✓ ✗ 72.47 ± 0.75 1.46 × ✓ ✓ 75.20 ± 0.54 1.41 × compare RSC with the uniform allocation strategy, i.e., set- ting kl = C|V| for all sparse operations in the backward pass. As shown in Figure 6, we plot the Pareto frontier of the accuracy-efficiency trade-off on the Reddit dataset for RSC and the uniform strategy with different C. For a fair comparison, we disabled the caching and switching mechanism. Due to page limit, more results are shown in Appendix E. We observe that: ❷ RSC exhibits a supe- rior trade-off between accuracy and efficiency compared to the uniform allocation, especially under high speedup regime. Namely, compared to the uniform allocation, RSC can achieve higher model accuracy under the same speedup. This can be explained by the fact that each operation has a different importance to the model performance. RSC can au- tomatically allocate more resources to important operations under a given total budget. To answer Q3, due to the page limit, we visualize the al- located kl for each layer across iterations in Appendix E Figure 7, and the degree of picked nodes in Appendix E Figure 8. We observe: ❸ The kl assigned by RSC evolves along with the training. 6.2.3. A BLATION ON CACHING AND SWITCHING . In section 6.2.2, we have shown the superior results of the proposed resource allocation strategy. As we mentioned in Section 3.3, we also introduce two simple tricks to for improving RSC , i.e., the caching and switching mechanism. To verify the effect of each of them (Q4), we conduct incre- mental evaluations on GCN, GraphSAGE and GCNII with 8RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations ogbn-proteins, which are summarized in Table 4. The row without caching and switching in Table 4 corresponds to the results with the proposed resource allocation strategy. We observe: ❹ Switching mechanism significantly improves the model performance, at the cost of slightly less acceleration effect. As we analyzed in Section 3.3.2, the improvement can be explained by the fact that the final training stage requires smaller gradient noise to help convergence. ❺ Caching mechanism significantly improves the wall-clock time speedup, at the cost of worse model performance. Al- though caching mechanism can reduce the overhead of sam- pling, the performance drop is too large (> 1%). Intuitively, the accuracy drop of caching also implies that we could not use a “static” down-sampled graph throughout the training process. ❻ Surprisingly, jointly applying the caching and switching, the performance drop can be minimized. 7. Acknowledgements The authors thank the anonymous reviewers for their helpful comments. The work is in part supported by NSF grants NSF IIS-2224843. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies. 8. Conclusions and Future work We propose RSC , which replaces the sparse computations in GNNs with their fast approximated versions. RSC can be plugged into most of the existing training frameworks to improve their efficiency. Future work includes exploring RSC for GNNs that rely on scatter-and-gather operations. References Adelman, M., Levy, K., Hakimi, I., and Silberstein, M. Faster neural network training with approximate tensor operations. Advances in Neural Information Processing Systems, 34:27877–27889, 2021. Cai, T., Luo, S., Xu, K., He, D., Liu, T.-y., and Wang, L. Graphnorm: A principled approach to accelerating graph neural network training. In International Conference on Machine Learning, pp. 1204–1215. PMLR, 2021. Chen, B., Dao, T., Liang, K., Yang, J., Song, Z., Rudra, A., and Re, C. Pixelated butterfly: Simple and efficient sparse training for neural network models. arXiv preprint arXiv:2112.00029, 2021. Chen, J., Zhu, J., and Song, L. Stochastic training of graph convolutional networks with variance reduction. In Inter- national conference on machine learning. PMLR, 2017. Chen, M., Wei, Z., Huang, Z., Ding, B., and Li, Y . Sim- ple and deep graph convolutional networks. In Interna- tional Conference on Machine Learning, pp. 1725–1735. PMLR, 2020. Chiang, W.-L., Liu, X., Si, S., Li, Y ., Bengio, S., and Hsieh, C.-J. Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks. In Proceed- ings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 257–266, 2019. Cohen, E. and Lewis, D. D. Approximating matrix multi- plication for pattern recognition tasks. Journal of Algo- rithms, 30(2):211–252, 1999. Cong, W., Forsati, R., Kandemir, M., and Mahdavi, M. Minimal variance sampling with provable guarantees for fast training of graph neural networks. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , pp. 1393–1403, 2020. Dao, T., Chen, B., Sohoni, N. S., Desai, A., Poli, M., Gro- gan, J., Liu, A., Rao, A., Rudra, A., and R´e, C. Monarch: Expressive structured matrices for efficient and accurate training. In International Conference on Machine Learn- ing, pp. 4690–4721. PMLR, 2022. Drineas, P. and Kannan, R. Fast monte-carlo algorithms for approximate matrix multiplication. In Proceedings 42nd IEEE Symposium on Foundations of Computer Science, pp. 452–459. IEEE, 2001. Drineas, P., Kannan, R., and Mahoney, M. W. Fast monte carlo algorithms for matrices i: Approximating matrix multiplication. SIAM Journal on Computing, 36(1):132– 157, 2006a. Drineas, P., Kannan, R., and Mahoney, M. W. Fast monte carlo algorithms for matrices i: Approximating matrix multiplication. SIAM Journal on Computing, 36(1):132– 157, 2006b. Duan, K., Liu, Z., Wang, P., Zheng, W., Zhou, K., Chen, T., Hu, X., and Wang, Z. A comprehensive study on large-scale graph training: Benchmarking and rethink- ing. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track , 2022a. URL https://openreview.net/forum? id=2QrFr_U782Z. Duan, K., Liu, Z., Wang, P., Zheng, W., Zhou, K., Chen, T., Hu, X., and Wang, Z. A comprehensive study on large-scale graph training: Benchmarking and rethinking. 2022b. Feng, W., Zhang, J., Dong, Y ., Han, Y ., Luan, H., Xu, Q., Yang, Q., Kharlamov, E., and Tang, J. Graph random neural networks for semi-supervised learning on graphs. 9RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations Advances in neural information processing systems, 33: 22092–22103, 2020. Feng, W., Dong, Y ., Huang, T., Yin, Z., Cheng, X., Khar- lamov, E., and Tang, J. Grand+: Scalable graph random neural networks. In Proceedings of the ACM Web Confer- ence 2022, pp. 3248–3258, 2022. Fey, M. and Lenssen, J. E. Fast graph representation learning with PyTorch Geometric. In ICLR Workshop on Repre- sentation Learning on Graphs and Manifolds, 2019. Fey, M., Lenssen, J. E., Weichert, F., and Leskovec, J. Gn- nautoscale: Scalable and expressive graph neural net- works via historical embeddings. In International confer- ence on machine learning, 2021. Glorot, X. and Bengio, Y . Understanding the difficulty of training deep feedforward neural networks. In Pro- ceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249–256. JMLR Workshop and Conference Proceedings, 2010. Hamilton, W. L., Ying, R., and Leskovec, J. Inductive representation learning on large graphs. InProceedings of the 31st International Conference on Neural Information Processing Systems, pp. 1025–1035, 2017. Han, S., Pool, J., Tran, J., and Dally, W. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015. Han, S., Liu, X., Mao, H., Pu, J., Pedram, A., Horowitz, M. A., and Dally, W. J. Eie: Efficient inference engine on compressed deep neural network. ACM SIGARCH Computer Architecture News, 44(3):243–254, 2016. Han, X., Jiang, Z., Liu, N., and Hu, X. G-mixup: Graph data augmentation for graph classification. arXiv preprint arXiv:2202.07179, 2022. Hu, W., Fey, M., Zitnik, M., Dong, Y ., Ren, H., Liu, B., Catasta, M., and Leskovec, J. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020. Huang, G., Dai, G., Wang, Y ., and Yang, H. Ge-spmm: General-purpose sparse matrix-matrix multiplication on gpus for graph neural networks. In SC20: International Conference for High Performance Computing, Network- ing, Storage and Analysis, pp. 1–12. IEEE, 2020a. Huang, Q., He, H., Singh, A., Lim, S.-N., and Benson, A. R. Combining label propagation and simple models out-performs graph neural networks. In International Conference on Learning Representations, 2020b. Huang, W., Zhang, T., Rong, Y ., and Huang, J. Adap- tive sampling towards fast graph representation learning. In Advances in Neural Information Processing Systems, 2018. Jiang, Z., Han, X., Fan, C., Liu, Z., Zou, N., Mostafavi, A., and Hu, X. Fmp: Toward fair graph message passing against topology bias. arXiv preprint arXiv:2202.04187, 2022. Jin, W., Ma, Y ., Liu, X., Tang, X., Wang, S., and Tang, J. Graph structure learning for robust graph neural networks. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 66–74, 2020. Kaler, T., Stathas, N., Ouyang, A., Iliopoulos, A.-S., Schardl, T., Leiserson, C. E., and Chen, J. Accelerating training and inference of graph neural networks with fast sampling and pipelining. Proceedings of Machine Learning and Systems, 4:172–189, 2022. Kipf, T. N. and Welling, M. Semi-supervised classi- fication with graph convolutional networks. In In- ternational Conference on Learning Representations , 2017. URL https://openreview.net/forum? id=SJU4ayYgl. Klicpera, J., Bojchevski, A., and G ¨unnemann, S. Predict then propagate: Graph neural networks meet personal- ized pagerank. In International Conference on Learning Representations, 2018. Li, Y ., Wei, C., and Ma, T. Towards explaining the regu- larization effect of initial large learning rate in training neural networks. Advances in Neural Information Pro- cessing Systems, 32, 2019. Liu, Z., Jin, H., Wang, T.-H., Zhou, K., and Hu, X. Di- vaug: Plug-in automated data augmentation with explicit diversity maximization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4762– 4770, 2021. Martinsson, P.-G. and Tropp, J. Randomized numerical linear algebra: foundations & algorithms (2020). arXiv preprint arXiv:2002.01387, 2020. Md, V ., Misra, S., Ma, G., Mohanty, R., Georganas, E., Heinecke, A., Kalamkar, D., Ahmed, N. K., and Avancha, S. Distgnn: Scalable distributed training for large-scale graph neural networks. In Proceedings of the Interna- tional Conference for High Performance Computing, Net- working, Storage and Analysis, pp. 1–14, 2021. Narayanan, S. D., Sinha, A., Jain, P., Kar, P., and SEL- LAMANICKAM, S. Iglu: Efficient GCN training via lazy updates. In International Conference on Learning 10RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations Representations, 2022. URL https://openreview. net/forum?id=5kq11Tl1z4. Qiu, J., Dhulipala, L., Tang, J., Peng, R., and Wang, C. Lightne: A lightweight graph processing system for net- work embedding. In Proceedings of the 2021 interna- tional conference on management of data, pp. 2281–2289, 2021. Rahman, M. K., Sujon, M. H., and Azad, A. Fusedmm: A unified sddmm-spmm kernel for graph embedding and graph neural networks. In 2021 IEEE International Par- allel and Distributed Processing Symposium (IPDPS), pp. 256–266. IEEE, 2021. Ramezani, M., Cong, W., Mahdavi, M., Kandemir, M., and Sivasubramaniam, A. Learn locally, correct globally: A distributed algorithm for training graph neural networks. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=FndDxSz3LxQ. Rong, Y ., Huang, W., Xu, T., and Huang, J. Dropedge: Towards deep graph convolutional networks on node clas- sification. arXiv preprint arXiv:1907.10903, 2019. Savas, B. and Dhillon, I. S. Clustered low rank approxi- mation of graphs in information science applications. In Proceedings of the 2011 SIAM International Conference on Data Mining, pp. 164–175. SIAM, 2011. Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., and Bengio, Y . Graph attention networks. In International Conference on Learning Representations, 2017. Wan, C., Li, Y ., Kim, N. S., and Lin, Y . {BDS}- {gcn}: Efficient full-graph training of graph convolu- tional nets with partition-parallelism and boundary sam- pling, 2021. URL https://openreview.net/ forum?id=uFA24r7v4wL. Wan, C., Li, Y ., Li, A., Kim, N. S., and Lin, Y . Bns-gcn: Efficient full-graph training of graph convolutional net- works with partition-parallelism and random boundary node sampling. Proceedings of Machine Learning and Systems, 4:673–693, 2022a. Wan, C., Li, Y ., Wolfe, C. R., Kyrillidis, A., Kim, N. S., and Lin, Y . Pipegcn: Efficient full-graph training of graph convolutional networks with pipelined feature communi- cation. arXiv preprint arXiv:2203.10428, 2022b. Wang, M., Zheng, D., Ye, Z., Gan, Q., Li, M., Song, X., Zhou, J., Ma, C., Yu, L., Gai, Y ., Xiao, T., He, T., Karypis, G., Li, J., and Zhang, Z. Deep graph library: A graph- centric, highly-performant package for graph neural net- works. arXiv preprint arXiv:1909.01315, 2019. Wang, Y ., Feng, B., and Ding, Y . Tc-gnn: Accelerating sparse graph neural network computation via dense tensor core on gpus. arXiv preprint arXiv:2112.02052, 2021. Wang, Z., Wu, X. C., Xu, Z., and Ng, T. E. Cupcake: Acom- pression optimizer for scalable communication-efficient distributed training. Wang, Z., Xu, Z., Wu, X., Shrivastava, A., and Ng, T. E. Dragonn: Distributed randomized approximate gradients of neural networks. In International Conference on Ma- chine Learning, pp. 23274–23291. PMLR, 2022. Wu, F., Souza, A., Zhang, T., Fifty, C., Yu, T., and Wein- berger, K. Simplifying graph convolutional networks. In International conference on machine learning, pp. 6861– 6871. PMLR, 2019. Xu, K., Zhang, M., Jegelka, S., and Kawaguchi, K. Op- timization of graph neural networks: Implicit acceler- ation by skip connections and more depth. In Inter- national Conference on Machine Learning, pp. 11592– 11602. PMLR, 2021. Ying, R., He, R., Chen, K., Eksombatchai, P., Hamilton, W. L., and Leskovec, J. Graph convolutional neural net- works for web-scale recommender systems. In Proceed- ings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 974–983, 2018. Yu, L., Shen, J., Li, J., and Lerer, A. Scalable graph neu- ral networks for heterogeneous graphs. arXiv preprint arXiv:2011.09679, 2020. Yuan, B., Wolfe, C. R., Dun, C., Tang, Y ., Kyril- lidis, A., and Jermaine, C. Distributed learning of fully connected neural networks using independent sub- net training. Proc. VLDB Endow. , 15(8):1581–1590, 2022. URL https://www.vldb.org/pvldb/ vol15/p1581-wolfe.pdf. Zeng, H., Zhou, H., Srivastava, A., Kannan, R., and Prasanna, V . Graphsaint: Graph sampling based in- ductive learning method. In International Conference on Learning Representations, 2020. URL https:// openreview.net/forum?id=BJe8pkHFwS. Zha, D., Feng, L., Tan, Q., Liu, Z., Lai, K.-H., Bhushanam, B., Tian, Y ., Kejariwal, A., and Hu, X. Dreamshard: Gen- eralizable embedding table placement for recommender systems. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Pro- cessing Systems, 2022. URL https://openreview. net/forum?id=_atSgd9Np52. 11RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations Zha, D., Feng, L., Luo, L., Bhushanam, B., Liu, Z., Hu, Y ., Nie, J., Huang, Y ., Tian, Y ., Kejariwal, A., and Hu, X. Pre- train and search: Efficient embedding table sharding with pre-trained neural cost models. CoRR, abs/2305.01868, 2023. doi: 10.48550/arXiv.2305.01868. URL https: //doi.org/10.48550/arXiv.2305.01868. Zhang, H., Yu, Z., Dai, G., Huang, G., Ding, Y ., Xie, Y ., and Wang, Y . Understanding gnn computational graph: A coordinated computation, io, and memory perspective. Proceedings of Machine Learning and Systems, 4:467– 484, 2022. Zheng, D., Ma, C., Wang, M., Zhou, J., Su, Q., Song, X., Gan, Q., Zhang, Z., and Karypis, G. Distdgl: distributed graph neural network training for billion-scale graphs. In 2020 IEEE/ACM 10th Workshop on Irregular Appli- cations: Architectures and Algorithms (IA3), pp. 36–44. IEEE, 2020. Zhong, S., Zhang, G., Huang, N., and Xu, S. Revisit kernel pruning with lottery regulated grouped convolutions. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=LdEhiMG9WLO. Zhou, K., Liu, Z., Chen, R., Li, L., Choi, S., and Hu, X. Table2graph: Transforming tabular data to unified weighted graph. In Raedt, L. D. (ed.), Proceedings of the Thirty-First International Joint Conference on Ar- tificial Intelligence, IJCAI 2022, Vienna, Austria, 23- 29 July 2022 , pp. 2420–2426. ijcai.org, 2022. doi: 10.24963/ijcai.2022/336. URL https://doi.org/ 10.24963/ijcai.2022/336. Zhou, K., Choi, S.-H., Liu, Z., Liu, N., Yang, F., Chen, R., Li, L., and Hu, X. Adaptive label smoothing to regularize large-scale graph training. In Proceedings of the 2023 SIAM International Conference on Data Mining (SDM), pp. 55–63. SIAM, 2023. Zou, D., Hu, Z., Wang, Y ., Jiang, S., Sun, Y ., and Gu, Q. Layer-dependent importance sampling for training deep and large graph convolutional networks. arXiv preprint arXiv:1911.07323, 2019. 12RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations A. Mathematical Analysis A.1. Why Sampling-based Approximation for GNN? In the main text, we mentioned SpMM is the main speed bottleneck for GNNs. Below we illustrate why the column- row sampling is suitable for accelerating SpMM in GNNs, from the approximation error perspective. Here we analyze ˜AJ(l) = SpMM( ˜A, J(l)) for illustration convenience. For the backward pass of SpMM , the analysis is similar, except that we are approximating ∇J(l) = SpMM( ˜A⊤, ∇H(l+1)). Column-row sampling approximates the matrix production by excluding some “unimportant” columns and rows in the original matrix. So intuitively, the approximation error E[|| ˜AJ(l) − approx( ˜AJ(l))||F ] is low if the “unimportant” columns/rows are correlated in the selected one. Namely, ˜A and J(l) are low-rank. Formally, we have the following theorem: Theorem A.1 ((Martinsson & Tropp, 2020)) . Suppose we approximate ˜AJ(l) using column-row sampling, and pi is obtained by Equation (3). Then for any positive number ϵ, if the number of samples k satisfies k ≥ ϵ−2(srank( ˜A) + srank(J(l))) log(|V| + d), we have E[|| ˜AJ(l) − approx( ˜AJ(l))||F ] ≤ 2ϵ, where srank in Theorem A.1 is called the stable rank, which is the continuous surrogate measure for the rank that is largely unaffected by tiny singular values. Formally for any matrix Y , srank(Y ) =||Y ||2 F ||Y ||2 ≤ rank(Y ). Fortunately, most real-world graphs are cluster-structured, which means the adjacency matrix ˜A is low-rank (Qiu et al., 2021; Savas & Dhillon, 2011). The low-rank property of real-world graphs is also wildly reported in previous work (Jin et al., 2020; Qiu et al., 2021). Moreover, the intermediate activations J(l) and the activation gradients are also low-rank, due to the aggregation. Namely, low-rank means “correlation” in the row/column space. The embedding (i.e., rows in the activation matrix) of connected nodes tend to close due to the graph propagation, which resulting in the low-rank property of the activation matrix. Thus for GNNs, the approximation error is low with a relatively small number of sample k. This perspective is also experimentally verified in the experiment section. A.2. Proof of Proposition 1 Proposition A.2 (Proof in Appendix A). If the approximation method is itself unbiased, and we only replace the SpMM in the backward pass with its approximated version, while leaving the forward one unchanged, then the calculated gradient is provably unbiased. Here we note that in the main text, for the notation convenience, we ignore the backward pass of ReLU. However, the proof here will consider the non-linear activation function to prove the unbiasedness. Let H(l+1) pre = SpMM( ˜A, J(l)) be the pre-activation. The backward pass of ReLU is: E[∇H(l+1) pre ] =E[1 H(l+1) pre >0 ⊙ ∇H(l+1)] = 1 H(l+1) pre >0 ⊙ E[∇H(l+1)], (5) where ⊙ is the element-wise product and 1 is the indicator function. The element-wise product is linear operation and 1 H(l+1) pre >0 is only related to the pre-activation in the forward pass, we only apply the approximation during the backward pass so 1 H(l+1) pre >0 can be extracted from the expectation. We know that for the last layer, we have E[∇H(L)] = H(L) since we do not apply ReLU at the output layer. We then can prove by induction that E[∇H(l+1)] = H(l+1) and E[∇J(l)] =E[approx( ˜A⊤∇H(l+1) pre )] =∇J(l) for any layer l. A.3. Analysis of MEAN aggregator For GraphSAGE, one commonly used aggregator is the MEAN aggregator, which can be expressed as follows: H(l+1) = W1H(l) + W2SpMM MEAN(A, H(l)), (6) 13RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations where SpMM MEAN is one variant of the vanilla SpMM , which replace the reducer function from sum(·) to mean(·). We note that in popular GNN packages, the MEAN aggregator usually is implemented based on SpMM MEAN (Fey & Lenssen, 2019; Wang et al., 2019) to reduce the memory usage. Here we give an example of SpMM MEAN to illustrate how it works: SpMM MEAN(   1 0 0 4 5 6  , \u00147 8 9 10 \u0015 ) = \"1 2 (1 × 7 + 0× 9) 1 2 (1 × 8 + 0× 10) 1 2 (0 × 7 + 4× 9) 1 2 (0 × 8 + 4× 10) 1 2 (5 × 7 + 6× 9) 1 2 (5 × 8 + 6× 10) # , Equivalently, the SpMM MEAN can also be expressed as: SpMM MEAN(A, H(l)) =D−1AH(l), where D is the degree matrix ofA. Thus, although we did not normalize the adjacency matrix in GraphSAGE, when applying the top-k sampling to approximate SpMM MEAN, the column norm of A:,ji is actually 1√ Degji due to the normalization. Also, for GraphSAGE, the inputs to the first SpMM MEAN operation are A and X. They do not require gradient since they are not trainable. Thus, the first SAGE layer is not presented in Figure 8 and Figure 7. B. Pseudo code of the greedy algorithm Algorithm 1 The greedy algorithm Inputs: Gradients of node embeddings{∇H(1), ···∇ H(L)}, adjacency matrix A, graph G = (V, E), hidden dimensions {d1, ··· dL}. Parameters: The step size α, the overall budget C. Outputs: The layer-wise {k1, ··· kL} associated with the top-k sampling. B ← PL l=1 |E|dl. ∀i, kl ← |V|, Topkl ← {1, ···|V|} . while B ≥ C PL l=1 |E|dl do m ← arg minl∈{1,···L}(P i∈Topkl ∥A⊤ :,i∥2∥∇H(l+1) i,: ∥2 ∥A∥F ∥∇H(l+1)∥F − P i∈Topkl−α|V| ∥A⊤ :,i∥2∥∇H(l+1) i,: ∥2) ∥A∥F ∥∇H(l+1)∥F /* Choose the layer m to reduce by a step size α|V|, such that the increment of errors is minimal. */ B ← B − dm P i∈Topkm∩i/∈Topkm−α|V| #nnzi /*Since we exclude some column-row pairs for layer m, here we reduce the budget B accordingly. */ km ← km − α|V| /* Update km accordingly. */ Topkm ← the set of indices i associated with km largest ∥A⊤ :,i∥2∥∇H(l+1) i,: ∥2 ∥A∥F ∥∇H(l+1)∥F /* Update Topkm accordingly. */ end while Return {k1, ··· , kL} In algorithm 1, here we provide the pseudo code of our greedy algorithm for solving the constrained optimization problem. In Table 11, we show the run time of the greedy algorithm, which is negligible compared to the acceleration effect. C. Extended Related works Connections to Graph Data Augmentation Data augmentation (Liu et al., 2021; Han et al., 2022) is wildly adopted in the graph learning for improving model generalization, including dropping nodes (Feng et al., 2020), dropping edges (Rong et al., 2019), and graph mixup (Han et al., 2022). As shown in Figure 5, the top- k sampling drops the entire columns in the adjacency matrix, while keeping the number of rows unchanged. That means RSC drops all of the out edges for a set of nodes. This can be viewed as the “structural dropedge” for improving the efficiency. Since we only apply the top-k sampling in the backward pass and top- k indices are different for each operation, RSC essentially forward pass with the whole graph, backward pass with different subgraphs at each layer. This structural dropedge and heterogeneous backward propagation introduce the regularization effect. Thus as shown in the experiment section, RSC may also improve the model accuracy over the baseline. 14RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations Subgraph-based GNN training. The key idea of this line of work is to improve the scalability of GNNs by separating the graph into overlapped small batches, then training models with sampled subgraphs (Hamilton et al., 2017; Huang et al., 2018; Zou et al., 2019; Chiang et al., 2019; Zeng et al., 2020). Based on this idea, various sampling techniques have been proposed, including the node-wise sampling (Hamilton et al., 2017; Chen et al., 2017), layer-wise sampling (Huang et al., 2018; Zou et al., 2019), and subgraph sampling (Chiang et al., 2019; Zeng et al., 2020). However, this approach reduces the memory footprint but results in extra time cost to compute the overlapping nodes between batches. Generally, methods in this category are orthogonal to RSC , and they can be combined. Graph precomputation. The graph precomputation methods decouple the message passing from the model training, either as a preprocessing step (Wu et al., 2019; Klicpera et al., 2018; Yu et al., 2020) or post-processing step (Huang et al., 2020b), where the model is simplified as the Multi-Layer Perceptron (MLP). We did consider this line of work in this paper since the backbone model is not GNN anymore. Distributed GNN training. The distributed training leverages extra hardwares to increase the memory capacity and training efficiency (Zha et al., 2023; 2022; Yuan et al., 2022; Wang et al., 2022; Wang et al.). However, the graph data cannot be trivially divided into independent partitions due to the node connectivity. Thus, the graph distributed training frameworks propose to split graph into related partitions and minimize the communication overhead (Wan et al., 2021; 2022b; Ramezani et al., 2022). Our methods are orthogonal to this line of work. Other randomized GNN training. Dropedge (Rong et al., 2019) randomly drops edges to avoid the over-smoothing problem. Graph Random Neural Networks (Grand) (Feng et al., 2020) randomly drop nodes to generate data augmentation for improving model generalization. Grand+ improves the scalability over Grand by pre-computing a general propagation matrix and employ it to perform data augmentation (Feng et al., 2022). As shown in Section C, the key difference between GRAND(+) and RSC is that RSC does not drop any node. Instead RSC drops all of the out edges for a set of nodes only during backward pass. Moreover, the drop pattern are evolving during the training process. This can be viewed as the “structural dropedge”. However, unlike Dropedge (Rong et al., 2019), RSC drop the column-row pairs according to the euclidean norm instead of uniformly dropping. D. Experimental Settings D.1. Software and Hardware Descriptions All experiments are conducted on a server with four NVIDIA 3090 GPUs, four AMD EPYC 7282 CPUs, and 252GB host memory. We implement all models based on Pytorch and Pytorch Geometric. During our experiments, we found that the version of Pytorch, Pytorch Sparse, and Pytorch Scatter can significantly impact the running speed of the baseline. Here we list the details of our used packages in all experiments in Table 5. Table 5: Package configurations of our experiments. Package Version CUDA 11.1 pytorch sparse 0.6.12 pytorch scatter 2.0.8 pytorch geometric 1.7.2 pytorch 1.9.0 OGB 1.3.2 D.2. Statistics of benchmark datasets The statistics for all used datasets are shown in Table 6. We follow the standard data splits and all datasets are directly downloaded from Pytorch Geometric or the protocol of OGB (Hu et al., 2020). D.3. Hyperparameter Settings Regarding Reddit and Yelp dataset, we follow the hyperparameter reported in the respective papers as closely as possible. Regarding ogbn-proteins and ogbn-products dataset, we follow the hyperparameter configurations and codebases provided 15RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations Table 6: Dataset Statistics. Dataset Task Nodes Edges Classes Label Rates Reddit multi-class 232,965 11,606,919 41 65.86% Yelp multi-label 716,847 6,977,409 100 75.00% ogbn-proteins binary-Class 132,534 39,561,252 2 65.00% ogbn-products multi-class 2,449,029 61,859,076 47 8.03% on the OGB (Hu et al., 2020) leader-board. Please refer to the OGB website for more details. The optimizer is Adam for all these models. All methods terminate after a fixed number of epochs. We report the test accuracy associated with the highest validation score. Table 10 summarize the hyperparameter configuration of GraphSAINT. Table 7, Table 8, and Table 9 summarize the hyperparameter configuration of full-Batch GCN, GraphSAGE, and GCNII, respectively. Table 7: Configuration of Full-Batch GCN. Dataset Training Archtecture Learning Rates Epochs Dropout BatchNorm Layers Hidden Dimension Reddit 0.01 400 0.5 Yes 3 256 Yelp 0.01 500 0.1 Yes 3 512 ogbn- proteins 0.01 1000 0.5 No 3 256 ogbn- products 0.001 500 0.5 No 3 256 Table 8: Configuration of Full-Batch GraphSAGE. Dataset Training Archtecture Learning Rates Epochs Dropout BatchNorm Layers Hidden Dimension Reddit 0.01 400 0.5 Yes 3 256 Yelp 0.01 500 0.1 Yes 3 512 ogbn- proteins 0.01 1000 0.5 No 3 256 ogbn- products 0.001 500 0.5 No 3 256 Table 9: Configuration of Full-Batch GCNII. Dataset Training Archtecture Learning Rates Epochs Dropout BatchNorm Layers Hidden Dimension Reddit 0.01 400 0.5 Yes 4 256 Yelp 0.01 500 0.1 Yes 4 256 ogbn- proteins 0.01 1000 0.5 No 4 256 E. More experiment results The running time of the greedy algorithm is shown in 11. We also visualize the allocated kl for each layer across iterations in Figure 7, and the degree of picked nodes in Figure 8. Here we use Reddit dataset for the case study. We observe that the kl assigned by RSC evolves along with the training. 16RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations Table 10: Training configuration of GraphSAINT. Dataset RandomWalk Sampler Training Archtecture Walk length Roots Learning Rates Epochs Dropout BatchNorm Layers Hidden Dimension Reddit 4 8000 0.01 40 0.1 Yes 3 128 Yelp 2 8000 0.01 75 0.1 Yes 3 512 ogbn- products 3 60000 0.01 20 0.5 No 3 256 Table 11: The running time (second) of the greedy algorithm. Reddit Yelp ogbn- proteins ogbn- products GCN 0.03 0.03 0.03 0.03 GraphSAGE 0.02 0.02 0.03 0.03 GCNII 0.05 0.05 0.06 - /uni00000013/uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013 /uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000056 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000037/uni0000004b/uni00000048/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000010/uni0000005a/uni0000004c/uni00000056/uni00000048/uni00000003/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000003ki | | /uni0000002a/uni00000026/uni00000031/uni00000003/uni0000000b/uni00000035/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000057/uni0000000c /uni0000002a/uni00000026/uni00000031/uni00000003/uni00000014/uni00000056/uni00000057/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni0000002a/uni00000026/uni00000031/uni00000003/uni00000015/uni00000051/uni00000047/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni0000002a/uni00000026/uni00000031/uni00000003/uni00000016/uni00000055/uni00000047/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni00000013/uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013 /uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000056 /uni00000013/uni00000011/uni00000014/uni00000013 /uni00000013/uni00000011/uni00000014/uni00000018 /uni00000013/uni00000011/uni00000015/uni00000013 /uni00000037/uni0000004b/uni00000048/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000010/uni0000005a/uni0000004c/uni00000056/uni00000048/uni00000003/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000003ki | | /uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000036/uni00000024/uni0000002a/uni00000028/uni00000003/uni0000000b/uni00000035/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000057/uni0000000c /uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000036/uni00000024/uni0000002a/uni00000028/uni00000003/uni00000015/uni00000051/uni00000047/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000036/uni00000024/uni0000002a/uni00000028/uni00000003/uni00000016/uni00000055/uni00000047/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni00000013/uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013 /uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000056 /uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni00000015 /uni00000037/uni0000004b/uni00000048/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000010/uni0000005a/uni0000004c/uni00000056/uni00000048/uni00000003/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000003ki | | /uni0000002a/uni00000026/uni00000031/uni0000002c/uni0000002c/uni00000003/uni0000000b/uni00000035/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000057/uni0000000c /uni0000002a/uni00000026/uni00000031/uni0000002c/uni0000002c/uni00000003/uni00000014/uni00000056/uni00000057/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni0000002a/uni00000026/uni00000031/uni0000002c/uni0000002c/uni00000003/uni00000015/uni00000051/uni00000047/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni0000002a/uni00000026/uni00000031/uni0000002c/uni0000002c/uni00000003/uni00000016/uni00000055/uni00000047/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni0000002a/uni00000026/uni00000031/uni0000002c/uni0000002c/uni00000003/uni00000017/uni00000057/uni0000004b/uni00000003/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055 Figure 7: The allocated layer-wise kl for GCN, GraphSAGE and GCNII on Reddit, where budget C is set as 0.1. The input of the SpMM in the first GraphSAGE layer does not require gradient and thus absent in the Figure (Appendix A.3). 0 50 100 150 200 250 300 350 400 Epochs 50 100 150Node degrees GCN 1st layer GCN 2nd layer GCN 3rd layer 0 50 100 150 200 250 300 350 400 Epochs 20 40 60 80 100 120Node degrees GraphSAGE 2nd layer GraphSAGE 3rd layer 0 50 100 150 200 250 300 350 400 Epochs 50 100 150Node degrees GCNII 1st layer GCNII 2nd layer GCNII 3rd layer GCNII 4th layer Figure 8: The averaged degrees of nodes picked by top-k sampling along the whole training process, where the applied dataset is Reddit and overall budget C is set as 0.1. E.1. Additional Ablation Results to the Resource Allocation Algorithm (Figure 6) Due to the page limit, we present more ablation study on the resource allocation algorithm here. Specifically, in Figure 9, we compare RSC to the uniform allocation on ogbn-proteins dataset with GCN, GraphSAGE, and GCNII, respectively. In Figure 10, we compare RSC to the uniform allocation on Yelp dataset with GCN, GraphSAGE, and GCNII, respectively. We conclude that RSC generally outperforms the uniform allocation strategy. E.2. Hyperparameter Sensitivity Analysis Here we analyze the impacts of the main hyperparameters of RSC : (1) the budget C, which controls the efficiency-accuracy trade-off; (2) the step size α in the greedy Algorithm 1; (3) when switching back to the original sparse operations. In Figure 12, we vary only one of them with the others fixed. We conclude (1) larger budget C leads to better accuracy with smaller speedup, since we are using more computational resources to approximate the full operation. (2) larger step size α leads 17RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations 1.01.21.41.61.8 Speedup 67 68 69 70 71 72Accuracy (%) GCN (ogbn-proteins) Uniform Allocation RSC 1.21.41.61.82.02.2 Speedup 70 71 72 73 74 75 76Accuracy (%) GraphSAGE (ogbn-proteins) Uniform Allocation RSC 1.01.21.41.61.8 Speedup 64 66 68 70 72 74Accuracy (%) GCNII (ogbn-proteins) Uniform Allocation RSC Figure 9: The Pareto frontier of the accuracy-efficiency trade-off for RSC and the uniform allocation. The dataset is ogbn-proteins. Here we disabled the caching and switch mechanism for a fair comparison. 0.960.981.001.021.041.061.081.10 Speedup 42 44 46 48Accuracy (%) GCN (Yelp) Uniform Allocation RSC 1.0001.0251.0501.0751.1001.1251.1501.175 Speedup 63.0 63.2 63.4 63.6 63.8 64.0Accuracy (%) GraphSAGE (Yelp) Uniform Allocation RSC 1.101.121.141.161.181.201.221.241.26 Speedup 64.0 64.1 64.2 64.3 64.4Accuracy (%) GCNII (Yelp) Uniform Allocation RSC Figure 10: The Pareto frontier of the accuracy-efficiency trade-off for RSC and the uniform allocation. The dataset is Yelp. Here we disabled the caching and switch mechanism for a fair comparison. to marginally larger speedup since the greedy algorithm will terminate earlier. Also the step size α does not affect the model accuracy a lot. In practice, we set α = 0.02|V|. (3) The later we switch back to the original operation, the larger the accuracy drop and the smaller the speedup, it is equivalent to using less resources to approximate the full operation epoch-wisely. Thus, we apply RSC for 80% of the total epochs to balance the trade-off. 0 100 200 300 400 Epochs 20 40 60 80Validation Accuracy GCN (Reddit) Baseline C=0.1 C=0.2 C=0.3 0 100 200 300 400 Epochs 20 40 60 80 100Validation Accuracy GCNII (Reddit) Baseline C=0.1 C=0.2 C=0.3 Figure 11: Learning curves for validation accuracy under different overall budget C on Reddit dataset. Here we disabled the caching and switching mechanism for ablating the effect of C. 0.1 0.2 0.3 0.4 0.5 (a) Budget C 73 74 75 76Test AUC Baseline AUC RSC AUC RSC Speedup 1.5 1.6 1.7 1.8 Speedup GraphSAGE (ogbn-proteins) 0.01| |  0.02| |  0.05| |  0.1| |  0.2| | (b) step size  75.9 76.0 76.1 76.2 76.3Test AUC Baseline AUC RSC AUC RSC Speedup 1.58 1.60 1.62 1.64 Speedup GraphSAGE (ogbn-proteins) At 60%  total epochs At 70%  total epochs At 80%  total epochs At 90%  total epochs At 95%  total epochs (c) When switching back to the original 75.50 75.75 76.00 76.25 76.50Test AUC Baseline AUC RSC AUC RSC Speedup 1.45 1.50 1.55 1.60 1.65 1.70 Speedup GraphSAGE (ogbn-proteins) Figure 12: Hyperparameter analysis w.r.t. the budget C, the step size α in Algorithm 1, and when switching back to the original operations. The model is GraphSAGE and the applied dataset is ogbn-proteins. 18",
      "meta_data": {
        "arxiv_id": "2210.10737v2",
        "authors": [
          "Zirui Liu",
          "Shengyuan Chen",
          "Kaixiong Zhou",
          "Daochen Zha",
          "Xiao Huang",
          "Xia Hu"
        ],
        "published_date": "2022-10-19T17:25:33Z",
        "pdf_url": "https://arxiv.org/pdf/2210.10737v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper proposes Randomized Sparse Computation (RSC), a novel approximation framework to accelerate Graph Neural Network (GNN) training. The core problem addressed is the time-consuming nature of sparse graph-based operations (SpMM) in GNNs, which are hard to accelerate on hardware and differ from dense matrix operations in terms of efficiency control and sampling overhead. RSC tackles these challenges by optimizing computation resource allocation layer-wise and epoch-wise. Key contributions include: 1) accelerating GNN training by replacing expensive sparse operations with faster approximated versions, specifically in the backward pass; 2) controlling the accuracy-efficiency trade-off through a layer-wise resource allocation strategy based on a constrained optimization problem solved by a greedy algorithm; 3) introducing an epoch-wise caching mechanism to reduce the overhead of sampling sparse matrices by reusing results from nearby iterations; and 4) incorporating a switching mechanism to revert to original sparse operations in the final training stages to minimize accuracy drop. RSC achieves up to 11.6x speedup for a single sparse operation and 1.6x end-to-end wall-clock time speedup with negligible accuracy loss.",
        "methodology": "RSC's methodology involves approximating sparse matrix-based operations (Sparse-Dense Matrix Multiplication, SpMM) primarily in the backward pass of GNN training using top-k sampling. To manage the accuracy-efficiency trade-off, RSC employs a layer-wise resource allocation strategy where the number of samples (kl) for top-k sampling at each layer is customized. This is formulated as a constrained optimization problem (Equation 4a and 4b) that minimizes the relative approximation error across layers while keeping total FLOPs under a specified budget. A greedy algorithm is proposed for efficient solution. To address the inefficiency of slicing sparse matrices due to their irregular format, an epoch-wise caching mechanism is introduced, reusing previously sampled sparse matrices across nearby iterations based on the observation that top-k indices remain similar. Furthermore, RSC integrates a switching mechanism where approximated operations are used for the majority of training, then switched back to exact sparse operations in the final epochs to enhance convergence and reduce accuracy degradation. This approach ensures unbiased gradients due to approximation being applied only in the backward pass.",
        "experimental_setup": "Experiments were conducted on a server equipped with four NVIDIA 3090 GPUs, four AMD EPYC 7282 CPUs, and 252GB host memory. The models were implemented using Pytorch and Pytorch Geometric, with specific package versions (CUDA 11.1, pytorch sparse 0.6.12, pytorch scatter 2.0.8, pytorch geometric 1.7.2, pytorch 1.9.0, OGB 1.3.2). Four large-scale graph datasets were used: Reddit, Yelp, ogbn-proteins, and ogbn-products, covering multi-class, multi-label, and binary-class tasks. RSC was evaluated in both mini-batch training (integrated with GraphSAINT) and full-batch training (integrated with GCN, GraphSAGE, and GCNII). GraphSAGE and GraphSAINT utilized the MEAN aggregator. Hyperparameters included a budget C (0.1, 0.3, 0.5) for resource allocation run every ten steps, a greedy algorithm step size alpha of 0.02|V|, a caching frequency of sampling every ten steps, and a switching mechanism that applied RSC for 80% of total epochs. Evaluation metrics included wall-clock time speedup on GPUs and test accuracy/F1-micro/AUC, with results averaged over ten random trials.",
        "limitations": "First, RSC's maximum speedup is inherently limited because it only replaces sparse operations in the backward pass to maintain model accuracy. While the backward pass is typically more time-consuming, this design choice means the forward pass operations remain exact, constraining overall acceleration. Second, the current framework is primarily designed for GNNs that rely on SpMM (and its variants) for aggregation. It is not directly tailored for GNNs that use scatter-and-gather operations (e.g., GAT). Although column-row sampling, caching, and switching mechanisms could potentially apply, the resource allocation algorithm (specifically the error bound and computation cost modeling in Equation 4) would require custom adaptations for scatter-and-gather operations.",
        "future_research_directions": "A primary future research direction involves extending RSC to GNNs that utilize scatter-and-gather operations instead of SpMM for aggregation. This would require tailoring the error bound and computation cost modeling within the resource allocation algorithm (Equation 4) to accommodate the specific characteristics of scatter-and-gather operations, thereby broadening the applicability of RSC."
      }
    },
    {
      "title": "Two Heads Are Better Than One: Boosting Graph Sparse Training via Semantic and Topological Awareness",
      "abstract": "Graph Neural Networks (GNNs) excel in various graph learning tasks but face\ncomputational challenges when applied to large-scale graphs. A promising\nsolution is to remove non-essential edges to reduce the computational overheads\nin GNN. Previous literature generally falls into two categories:\ntopology-guided and semantic-guided. The former maintains certain graph\ntopological properties yet often underperforms on GNNs due to low integration\nwith neural network training. The latter performs well at lower sparsity on\nGNNs but faces performance collapse at higher sparsity levels. With this in\nmind, we take the first step to propose a new research line and concept termed\nGraph Sparse Training (GST), which dynamically manipulates sparsity at the data\nlevel. Specifically, GST initially constructs a topology & semantic anchor at a\nlow training cost, followed by performing dynamic sparse training to align the\nsparse graph with the anchor. We introduce the Equilibria Sparsification\nPrinciple to guide this process, effectively balancing the preservation of both\ntopological and semantic information. Ultimately, GST produces a sparse graph\nwith maximum topological integrity and no performance degradation. Extensive\nexperiments on 6 datasets and 5 backbones showcase that GST (I) identifies\nsubgraphs at higher graph sparsity levels (1.67%~15.85% $\\uparrow$) than\nstate-of-the-art sparsification methods, (II) preserves more key spectral\nproperties, (III) achieves 1.27-3.42$\\times$ speedup in GNN inference and (IV)\nsuccessfully helps graph adversarial defense and graph lottery tickets.",
      "full_text": "Two Heads Are Better Than One: Boosting Graph Sparse Training via Semantic and Topological Awareness Guibin Zhang 1 † Yanwei Yue1 † Kun Wang2 Junfeng Fang 2 Yongduo Sui2 Kai Wang3 Yuxuan Liang4 Dawei Cheng 1 Shirui Pan 5 Tianlong Chen 6 Abstract Graph Neural Networks (GNNs) excel in vari- ous graph learning tasks but face computational challenges when applied to large-scale graphs. A promising solution is to remove non-essential edges to reduce the computational overheads in GNN. Previous literature generally falls into two categories: topology-guided and semantic-guided. The former maintains certain graph topological properties yet often underperforms on GNNs due to low integration with neural network training. The latter performs well at lower sparsity on GNNs but faces performance collapse at higher sparsity levels. With this in mind, we take thefirst step to propose a new research line and concept termed Graph Sparse Training (GST), which dynamically manipulates sparsity at the data level. Specifically, GST initially constructs a topology & semantic anchor at a low training cost, followed by performing dynamic sparse training to align the sparse graph with the anchor. We introduce the Equilibria Sparsification Principle to guide this process, effectively balancing the preserva- tion of both topological and semantic informa- tion. Ultimately, GST produces a sparse graph with maximum topological integrity and no per- formance degradation. Extensive experiments on 6 datasets and 5 backbones showcase that GST (I) identifies subgraphs at higher graph sparsity levels (1.67% ∼ 15.85%↑) than state-of-the-art sparsifiers, (II) preserves more key spectral prop- erties, (III) achieves 1.27 − 3.42× speedup in GNN inference and (IV) successfully helps graph adversarial defense and graph lottery tickets. *Equal contribution 1Tongji Univerity2University of Science and Technology of China3National University of Singapore4Hong Kong University of Science and Technology (Guangzhou Cam- pus) 5Griffith University 6Massachusetts Institute of Technology. Correspondence to: Kun Wang <wk520529@mail.ustc.edu.cu>, Tianlong Chen <tianlong@mit.edu>. The source code is available athttps://anonymous.4open. science/r/GST-0F15 Accuracy Spectral Preservation Figure 1.Graph sparsifier (UGS & Spectral Radius) comparison on Ogbn-Proteins using 3-layer GraphSAGE at varying graph spar- sity levels {10%, 20%, ·· ·, 60%}. (Left) ROC-AUC score after different levels of sparsification. (Right) The spectral preservation ratio2of the obtained sparse graph. 1. Introduction Graph Neural Networks (GNNs) (Wu et al., 2020; Zhou et al., 2020) have emerged as leading solutions for graph- related learning tasks (Velickovic et al., 2017; Hamilton et al., 2017; Zhang & Chen, 2018; 2019; Ying et al., 2018b; Zhang et al., 2018). The essence of GNNs lies in their iterative neighborhood aggregation and update processes. The former aggregates neighboring node embeddings via sparse matrix-based operations ( e.g., SpMM and SDDMM), while the latter updates central nodes using dense matrix- based operations ( e.g., MatMul) (Fey & Lenssen, 2019; Wang et al., 2019). SpMM often accounts for the largest portion (50%∼70%) of GNN’s computational load (Qiu et al., 2021; Liu et al., 2023), mainly determined by the size of the graph input. However, large-scale graph inputs are commonplace in real-world applications (Wang et al., 2022; Jin et al., 2021; Zhang et al., 2024), presenting significant computational challenges that impede feature aggregation in GNN training and inference. These issues sadly drop a daunting obstacle on the way toward GNNs’ on-device de- ployment, especially in resource-constrained environments. Given thatSpMM dominates the computational load of GNNs, 2Following (Liu et al., 2023), we define the spectral preserva- tion ratio as the relative error of the top-200 eigenvalues, repre- sented as P200 i=1 |λi−λ′ i| λi , where λi and λ′ i denote the i-th eigen- value of the original and sparse graphs, respectively. 1 arXiv:2402.01242v1  [cs.LG]  2 Feb 2024Boosting Graph Sparse Training via Semantic and Topological Awareness and the execution time of such sparse operation is propor- tional to the graph’s edge count (Liu et al., 2023), previous approaches to mitigate this inefficiency primarily concen- trated on graph sparsification by discarding non-essential edges. They generally fall into two categories: ■ The first research line involves calculating topology- guided edge scores for removing the less significant ones, such as resistance effective (Spielman & Srivastava, 2008), eigenvalue (Batson et al., 2013), pairwise distances between- ness (David, 1995), size of all cuts (Bencz ´ur & Karger, 1996) and node degree distributions (Eden et al., 2018). ■ The other primarily relies on semantic-based scores derived from gradients, momentum, magnitude, etc., dur- ing GNN training, utilizing methods such as trainable masks (Chen et al., 2021; Wang et al., 2023c;a), probabilis- tic sampling (Zheng et al., 2020; Luo et al., 2021) or meta- gradients (Wan & Schweitzer, 2021) to score and prune non-essential edges. Scrutinizing the existing graph sparsifiers, they focus on either topology or semantics, thereby suffer from inherent limitations correspondingly: ➠ Limited Performance. Though theoretically capable of preserving specific spectral properties (Batson et al., 2013), topology-guided sparsifiers overlook the rich graph features, GNN training dynamics, and downstream tasks, resulting in suboptimal performance when integrated with GNN (Luo et al., 2021). Semantic-guided sparsifiers dynamically assess edge importance during GNN train- ing, while they often suffer from significant performance collapse when targeting high graph sparsity, due mostly to detrimental impact on the overall connectivity of the graph (Hui et al., 2023; Wang et al., 2023b). ➠ Empirical Observations. We apply a typical semantic- based sparsifier, UGS (Chen et al., 2021) and a topology- based sparsifier, Spectral Radius (SR) (Chan & Akoglu, 2016; Karhadkar et al., 2023) on Ogbn-Proteins (Hu et al., 2020). From Fig. 1, we observe that: 1 UGS maintains GNN performance well at lower sparsity levels (0%∼30%), yet encounters a dramatic performance drop (over 5.5%↓) at higher sparsity (50%∼), accompanied by significant spectral preservation loss; 2 SR consistently fails to match GNN performance on sparse graphs with that on dense graphs, though its performance deterioration is gradual, with a milder spectral preservation loss. The aforementioned observations prompt questions about graph sparsifiers: Can we ideally leverage the strengths and mitigate the shortcomings of both research lines? Going beyond this, considering topology-guided methods are inher- ently ahead of and independent of training, while semantic- guided ones are closely intertwined with GNN optimization, their integration is naturally incongruent. We further ques- tion that: How can we effectively combine topology- and semantic-aware sparsification, embodying the principle that two heads are better than one? To this end, we take the first step to explore dynamic graph-level sparse training in both a semantic- and topology- aware manner. We innovatively propose Graph Sparse Training (GST), which iteratively updates and maintains a sub-counterpart of the original graph during training. To highlight, GST explores the dynamic sparse issue at data- level for the first time, opening a potential pathway for in- tegrating spectral theory and dynamic training algorithms. Additionally, we introduce the Equilibria Sparsification Principle to guide the exploration process, which offers a new paradigm for future sparsifier development. More specifically, considering the inherent properties of the graph itself, GST starts with full graph training to build the dependable anchor, providing a well-aligned semantic and topological benchmark for later dynamic sparse train- ing. Then, GST prunes towards the larger sparsity, resulting in a sparse graph with suboptimal performance (Ma et al., 2021; Frankle et al., 2020; Chen et al., 2021). Finally, the GNN continues to train while iteratively updating the graph, i.e., pruning and growing an equal number of edges, to dis- cover an optimal sparse structure. During each update, we adhere to the Equilibria Sparsification Principle that pri- oritizes both semantic and topological significance, explic- itly minimizing the discrepancy between the current sparse graph and the anchor graph. We methodically explore the graph structure through GST, aiming to retain the maximum amount of semantic and topological information. Briefly put, our contributions can be summarized as: • In this work, we systematically review two graph sparsi- fication research lines: topology-guided and semantic- guided. For the first time, we develop a novel framework that combines their strengths and explicitly preserves graph topology integrity to boost GNN performance main- tenance at extreme graph sparsity levels. • We introduce Graph Sparse Training (GST), an inno- vative pruning framework that manipulates sparsity at the data level, exploring sparse graph structure with both semantic- and topology-awareness. Additionally, GST boasts high versatility, effectively aiding various main- stream tasks, including graph adversarial defense and graph lottery tickets. • Our extensive experiments on 6 datasets and 5 backbones demonstrate that GST (I) identifies subgraphs at higher graph sparsity levels (1.67% ∼ 15.85% ↑) compared to SOTA sparsification methods in node classification tasks, without compromising performance, (II) effectively pre- serves ∼ 15% more spectral properties, (III) achieves tan- gible 1.27−3.42× GNN inference speedup, and (IV) suc- cessfully combat edge perturbation ( 0.35% ∼ 7.23%↑) and enhances graph lottery tickets (0.22% ∼ 1.58%↑). 2Boosting Graph Sparse Training via Semantic and Topological Awareness 2. Related Work Topology-based sparsifiers are early attempts at graph lightweighting. Essentially, they use a theoretically inspired pre-defined metric to score edge importance and prune those with lower scores. SCAN (Xu et al., 2007) assesses global connectivity importance using Jaccard similarity. L- spar (Satuluri et al., 2011) and Local Similarity (Hamann et al., 2016) filter edges locally based on Jaccard similarity. (Spielman & Srivastava, 2008) suggested graph sampling via effective resistance, and (Liu et al., 2023) accelerated its computation with unbiased approximation. However, these methods, typically effective in traditional graph tasks (e.g., graph clustering) (Chen et al., 2023), struggle to maintain performance when applied to GNNs due to their unaware- ness of GNN training (Luo et al., 2021). Semantic-based sparsifiers are more closely integrated with GNNs. They aim to dynamically identify important edges via GNN training semantics. NeuralSparse (Zheng et al., 2020) introduces a learning-based sparsification net- work to select k edges per node. Meta-gradient sparsi- fier (Wan & Kokel, 2021) leverages meta-gradients to peri- odically remove edges. Additionally, Graph Lottery Ticket (GLT) (Chen et al., 2021; Wang et al., 2023c; 2022) offers a new paradigm for graph sparsification, which gradually prunes the graph to target sparsity using iterative magnitude pruning (IMP). However, these methods often struggle at higher graph sparsity levels, due to their disruption of the graph topology (Hui et al., 2023). Sprase Training Target Semantic? Topology? PRC § SNIP (Lee et al., 2018) Sparse NN ✓ ✗ ✓ RigL (Evci et al., 2020) Sparse NN ✓ ✗ ✓ IMDB (Hoang et al., 2023) Sparse NN ✗ ✓ ✓ DSnT (Zhang et al., 2023) Sparse LLM ✓ ✗ ✓ GST (Ours) Sparse Graph ✓ ✓ ✓ § PRC: Prune Rate Control Table 1.Comparison among different sparsifiers. Dynamic Sparse Training (DST) is gaining attention as a method to reduce the computational cost in network train- ing. It entails starting with a randomly sparsified network and periodically updating its connections (Evci et al., 2020). Recent advancements have expanded DST’s scope both theoretically (Liu et al., 2021; Huang et al., 2023) and algo- rithmically (Liu et al., 2020; Zhang et al., 2023). Our GST fundamentally differs from DST in at least two aspects. as shown in Tab. 1: (1) Research target. While current DST focuses on sparsifying network parameters, GST inno- vatively explores the feasibility of the “prune and regrow” paradigm in graph data. (2) Update methodology. DST essentially aligns with semantic-guided methods, adjusting connections based on semantic information (gradient, mo- mentum, etc.) during network training. In contrast, GST considers both semantic and topological information of the graph. We place more discussions and comparisons with related work in Appendix B. 3. Methodology 3.1. Notations and Formulations Notations. Consider a graph G = {V, E}, where V repre- sents the nodes and E the edges. The feature matrix of G is X ∈ RN×D , with N = |V| being the total node count and each node vi ∈ Vhaving an D-dimensional feature vector xi = X[i, ·]. A ∈ {0, 1}N×N is the adjacency ma- trix, where A[i, j] = 1 signifies an edge (vi, vj) ∈ E. Let f(·; Θ) represent a GNN with Θ as its parameters, then Z = f({A, X}; Θ) is the model output. Note that G is alternatively denoted as {A, X} here, and we will inter- changeably use its two notations. Consider semi-supervised node classification, its objective function L is: L(G, Θ) = − 1 |Vlabel| X vi∈Vlabel yi log(zi), (1) where L is the cross-entropy loss calculated over the labelled node set Vlabel, and yi and zi denotes the label and prediction of vi, respectively. Problem Formulation. The target of graph sparsification is to identify the sparsest subgraph Gsub = {A ⊙ Mg, X}, where Mg ∈ {0, 1}N×N is a binary mask indicating edge removal if Mg[i, j] = 0 for (vi, vj) ∈ Eand ⊙ denotes element-wise product. Additionally, the subgraph Gsub should satisfy the condition that training the GNN on Gsub achieves performance comparable to that on the original graph G. The objective is as follows: arg max Mg sg = 1 − ∥Mg∥0 ∥A∥0 s. t. |L({A, X}, Θ) − L({A ⊙ Mg, X}, Θ)| < ϵ, (2) where sg is the graph sparsity, ∥ · ∥0 counts the number of non-zero elements, and ϵ is the threshold for permissible per- formance difference. We define the extreme graph sparsity as the maximal sg without compromising accuracy. 3.2. Framework Overview Fig. 2 illustrates the overall workflow of GST. Starting with an initialized GNN and the adjacency matrix A as input, we first train the GNN together with a graph mask mg ∈ R|A| produced by a graph masker for limited epochs. During this phase, we capture the optimal topological and semantic information as the anchor graph (Sec 3.3). Subsequently, we apply one-shot pruning to mg, reducing it to the de- sired sparsity sg% and creating a sketched sparse graph. From this sparse base, we continue training, dynamically fine-tuning the sparse graph’s structure to align semanti- cally and topologically with the anchor graph (Sec. 3.4). Through iterative refinement and exploration of graph struc- ture, we ultimately achieve a sparse graph with maintained performance, reduced memory footprint, and faster infer- ence speed (Sec. 3.5). 3Boosting Graph Sparse Training via Semantic and Topological Awareness Topology- guided Input Graph(s)  Full Graph Training 𝒇 𝒎𝒈 ⊙𝐀,𝐗 ,𝜣 One-shot  pruning Anchor  𝒇{(𝐀⊙𝒎𝒈𝒜,𝐗);𝚯} ... Model Output 𝐙𝒜 Mask 𝒎𝒈𝒜 target sparsity  𝒔𝒈% : logits of each class Regrow from  ¬𝐌𝒈 (𝝁) ⊙𝐀 Prune in 𝐌𝒈 (𝝁) ⊙𝐀 Iterative Process Grow towards the optimal performance EV: Eigenvalue Variation OD: Output Discrepancy Topology 𝒢𝒜 = {𝐀⊙𝒎𝒈𝒜,𝐙𝒜} 𝒢𝑜𝑠 EV Score OD Magnitude Semantic Topology  anchor Semantic  anchor Output Graph(s) Mask  𝒎𝒈 ∈ ℝ|𝐀| Mask 𝐌𝒈𝒐𝒔 ∈ {𝟎,𝟏}|𝐀| Admirable  accuracy Storage saving Inference Speedup Semantic- guided 𝒢𝑠1 𝒢𝑠3 < Accuracy 𝒢𝑠2 GST 𝐌𝒈 𝟎 ≪ 𝐀 𝟎 Baseline GPU Latency ↓ MACs ↓ Overview of GST NetworkGraph Full Graph  Training One-shot Pruning Prune Regrow Topology Anchor Semantic- & Topology- preserving sparse graph Semantic Anchor Figure 2.(Left) The overview of GST; (Right) The detailed pipeline of GST. GST dynamically adjusts and updates the sparse graph, guided by an anchor graph from full-graph training, to optimize topological and semantic preservation, and finally yields a sparse subgraph at the desired sparsity along with admirable accuracy, storage saving, and inference speedup. 3.3. Pursuing Anchor Graph As outlined above, both topology-guided and semantic- guided sparsifiers typically derive guidance, explicitly or implicitly, from the original dense graph, striving to mini- mize their divergence from it (Tsitsulin & Perozzi, 2023). In line with the principle of efficiency in DST, several works such as Early-Bird (EB) and Graph EB have demonstrated that limited training can also construct high-quality bench- marks (Achille et al., 2018; You et al., 2019; 2022). There- fore, we propose conducting limited training on the original graph, i.e., full graph training, to capture an anchor encom- passing the original graph’s topology and semantics. This approach provides natural “instructions” for subsequent dy- namic adjustment of the target sparse graph. To start with, we derive a (dense) graph mask mg ∈ R|A| with a parame- terized graph masker Ψ: mg[i, j] = ( Ψ([xi||xj]), if(i, j) ∈ E, 0, otherwise, (3) where mg[i, j] denotes the edge score for eij, the graph masker Ψ takes the concatenation of embeddings of vi and vj as input and output the edge score. Practically, we employ a 2-layer MLP for its implementation. We then co-optimize Θ, and Ψ with the following loss function: Lanchor = L({mg ⊙ A, X}, Θ) , (4) After the full graph training of E epochs (practically E ≤ 200), we select the optimal mask mA g and model output ZA = f({mA g ⊙ A, X}, ΘA) from the epoch with the highest validation score, collectively termed as the anchor graph GA = {mA g ⊙ A, ZA}. This process is rooted in an intuitive concept: although mg and Θ can be undertrained at this stage, the early training is capable of discovering vital connections and connectivity patterns (Achille et al., 2018; You et al., 2019; 2022). Consequently,mA g and ZA retain crucial properties for GNN training with the original graph G. The anchor graph will be utilized in Sec. 3.5 to guide the exploration of sparse graph structure. Given the target graph sparsity sg%, we zero the lowest- magnitude elements inmA g w.r.t. sg, and obtain its binarized version Mos g ∈ {0, 1}|mA g |. Such a sparse mask obtained via one-shot pruning is suboptimal (Ma et al., 2021; Frankle et al., 2020), and we will start from this point and continually refine towards a more optimal graph structure. 3.4. Dynamical Sparse Graph Training With Mos g at hand, we proceed to train the GNN model together with the fixed subgraph and the graph masker, de- noted as f({mg ⊙ Mos g ⊙ A, X}, Θ), with the objective function similar to Eq. 4. We aim to gradually evolve the sparse graph structure toward both better topological and semantical preservation. To this end, we periodically reacti- vate the semantically and topologically significant edges in the pruned subgraph and substitute them for less important portions in the current subgraph within D epochs. We set the interval for each update ( i.e., drop ↔ regrow) at ∆T epochs, with the total number of updates being ⌈D/∆T⌉. We aim to develop a comprehensive criterion ϕ that evaluates the importance of an edge from both topo- 4Boosting Graph Sparse Training via Semantic and Topological Awareness logical and semantic perspectives, which will guide the “ex- change of edges” between the current edgesE(µ) = Mg ⊙A and its complement EC (µ) = ¬Mg ⊙A. Consider the update process between interval µ and µ + 1: M(prune) = ArgTopK \u0010 −ϕ(M(µ) g ⊙ A), Υ(µ) \u0011 , M(regrow) = ArgTopK \u0010 ϕ(¬M(µ) g ⊙ A), Υ(µ) \u0011 , (5) where ArgTopK(m, k) returns the indices of top- k ele- ments of matrix m, and Υ(·) is the update scheduler that controls the number of edges to be swapped at each update. The configuration of Υ(·) is detailed in Appendix G.4. We then update the sparse graph as follows: M(µ+1) g = \u0010 M(µ) g \\ M(prune) g \u0011 ∪ M(regrow) g . (6) Then, in (µ + 1)-th interval, we continue training the GNN with the updated sparse graph M(µ+1) g ⊙A for another ∆T epochs. This iterative process of updating and refining the sparse graph structure aims towards optimal performance. The remaining question now is: how do we design an ideal evaluation criterion ϕ? 3.5. Topological & Semantical Criterion Design To answer the question above, we introduce the Equilibria Principle to guide the design of graph pruning criteria, aim- ing to establish a new paradigm for sparsifier development: Principle 3.1 (Equilibria Sparsification Principle). Given a graph G = {A, X} and target sparsity sg%, an ideal sparsifier Mg ∈ {0, 1}|A|(∥Mg∥0 ∥A∥0 = sg%) and its result- ing subgraph Gsub = {Mg ⊙ A, X} should satisfy the following condition: arg min Mg E   X T ′∈{T } T ′(G, Gsub) + X S′∈{S} S′(G, Gsub)  , (7) where E denotes the expectation, {T }denotes all possible metrics measuring the topological information difference between the sparse and original graphs (graph distance, spectral gap, etc.), and {S} represents all possible met- rics measuring semantic information differences (gradients, momentum, etc.). However, it is impractical to traverse and satisfy all pos- sible metrics, so we provide two exemplified approaches for topology/semantics preservation metrics T and S, and utilize them to guide the update process in Sec. 3.4. Topology Criterion. Despite the myriad of edge scoring methods (Tsitsulin & Perozzi, 2023), we opt for eigen- value variation, i.e., the relative error of all eigenvalues, as the metric for edge dropping/regrowing. This is be- cause most topology-guided scores, including effective resis- tance (Spielman & Srivastava, 2008), spectral radius (Costa et al., 2007), and graph curvature (Tsitsulin & Perozzi, 2023; Forman, 2003), rely partially or wholly on graph Laplacian eigenvalues. Ideally, we aim to identify a set of edges E′ from E(µ) that minimally impact the graph Laplacian and a set E′′ from EC (µ) that maximally affect it (|E′| = |E′′| = Υ(µ)), thereby guiding the sparse graph structure towards restoring the topological properties of the anchor graph. The objective is as follows: arg min E′,E′′ E \u0010 T (GA, G(µ+1)) \u0011 , T (G1, G2) = NX i=1 |λ(1) i − λ(2) i | λ(1) i , (8) where G(µ+1) = {V, E(µ+1)} is the updated sparse graph after the µ-th update, and we choose eigenvalue variation as the implementation forT . However, exhaustively evaluating all combinations of (E′, E′′) is impractical. Therefore, we shift to assessing the impact of individual edges on the graph Laplacian as a measure of their importance: ϕ(topo)(eij) = NX k=1 |λk(GA) − λk(GA \\ eij)| λk(GA) , (9) where λk(GA) denotes the k-th eigenvalue of the anchor graph, and λk(GA \\eij) represents that after removing edge eij. For computational feasibility, we provide an approxi- mate version, with detailed derivation in Appendix C: ϕ(topo)(eij) =  KX k=1 + NX k=N−K ! mA g,ija(k) i b(k) j λk(GA)a(k)T b(k) , (10) where a(k) (b(k)) is the left (right) eigenvector of the anchor graph’s k-th eigenvalue. Notably, we only consider the top- K and bottom-K eigenvalues (practically, K=20) as they sufficiently reflect the overall properties of the graph. Semantic Criterion. From the perspective of semantic preservation, we formulate the objective as follows: arg min E′,E′′ E \u0010 S(GA, G(µ+1)) \u0011 , S(GA, G(µ)) = KL \u0010 f(GA, ΘA), f(G(µ), Θ) \u0011 , (11) where ZA = f(GA, ΘA) is the semantic anchor, f(G(µ), Θ) is the current model output, and S(·, ·) employ the KL divergence (Sun et al., 2022) to evaluate the model output discrepancies. Based on this, we propose the seman- tic criterion: ϕ(sema)(eij) = \f\f\f∇eij S(GA, G(µ)) \f\f\f, (12) where edges with greater gradient magnitude after a single backpropagation step on output discrepancies are considered more valuable, otherwise the opposite. Equilibria Combination. After selecting appropriate topo- logical/semantic criteria, we combine these to form the final drop/regrow criterion, aligning with the equilibria principle: ϕ(eij) = βs · ϕ(sema)(eij) + βt · ϕ(topo)(eij), (13) 5Boosting Graph Sparse Training via Semantic and Topological Awareness GCN GIN GAT Figure 3.Performance comparison of graph sparsification methods on Citeseer (First Row) and PubMed (Second Row) under different sparsity levels. The gray dashed line represents the original baseline performance. where βs and βt are corresponding scaling coefficients. At the end of interval µ, we exchange edges with the highest ϕ in EC (µ) and those with the lowest ϕ in EC (µ), resulting in the updated sparse graph G(µ+1), as described in Sec. 3.4. Model Summary. Through such iterative exploration of graph structure, GST eventually achieves a sparse subgraph at the desired sparsity with no performance degradation. The overall algorithm framework is showcased in Algo. 1, and its complexity analysis is presented in Appendix D. Im- portantly, GST, as a versatile concept, exhibits compatibility with various mainstream research lines, thereby providing substantial support for advancements in these areas, such as graph adversarial defense and graph lottery ticket, etc. (discussed in Sec. 4.5). 4. Experiments In this section, we conduct extensive experiments to answer the following research questions (RQ): RQ1: Can GST effectively find resilient sparse subgraphs? RQ2: How effective is GST in terms of preserving spectral information, i.e., eigenvalues? RQ3: Does GST genuinely accelerate the GNN inference? RQ4: Can GST serve as a universal operator? RQ5: How sensitive is GST to its key hyperparameters and components? 4.1. Experiment Setup Dataset. To exhaustively assess GST across various bench- marks and tasks, we select three popular graph datasets, including Cora, Citeseer, and PubMed (Kipf & Welling, 2017a). For larger-scale graphs, we utilize Ogbn-ArXiv, Ogbn-Proteins, and Ogbn-Products (Hu et al., 2020). Backbones. We evaluate GST under both transductive and inductive settings. For transductive settings, we select GCN (Kipf & Welling, 2017b), GIN (Xu et al., 2019) and GAT (Veliˇckovi´c et al., 2018) for full-batch training. For inductive settings, we select GraphSAGE (Hamilton et al., 2017) and ClusterGCN (Chiang et al., 2019). Baselines. We compare GST against two categories of graph sparsification methods: (1) topology-based sparsifi- cation, including SCAN (Xu et al., 2007), Local Similarity (LSim) (Satuluri et al., 2011), and DSpar (Liu et al., 2023); (2) semantic-based sparsification, including UGS (Chen et al., 2021), meta-gradient sparsifier (MGSpar) (Wan & Schweitzer, 2021), and WD-GLT (Hui et al., 2023). More details on experiment setup can be found in Appendix G. 4.2. GST Excels In Combating Sparsity (RQ1) We present the performance of GCN/GIN/GAT on Cora/Citeseer/PubMed in Figs. 3 and 8 and that of Graph- SAGE/ClusterGCN on Ogbn-ArXiv/Proteins/Products in Tabs. 2, 9 and 10. Each point in the figures represents the test accuracy/ROCAUC of the GNNs with the corre- sponding sparsifier at various levels of graph sparsity. Our observations are as follows: Obs. 1 GST is flexible and consistently outperforms other sparsifiers. Figs. 3 and 8 demonstrate that GST (I) maintains GNN performance best at lower sparsity lev- els, such as on GIN+PubMed with 50% graph sparsity, where other sparsifiers experienced a performance drop of 0.36% ∼ 3.17% compared to the baseline, whereas GST even showed 0.3% improvement; (II) most effectively coun- ters the adverse effects of sparsification at extreme sparsity levels, outperforming other sparsifiers by 1.23% ∼ 5.39% at 80% graph sparsity. Obs. 2 GST resiliently scales to large-scale graphs. As demonstrated in Tabs. 2, 9 and 10, GST maintains robust performance when sparsifying large graphs under inductive setting. Specifically, on Ogbn-ArXiv with 40% sparsity, GST+GraphSAGE/ClusterGCN experienced negligible per- 6Boosting Graph Sparse Training via Semantic and Topological Awareness Table 2.Performance comparison of sparsifiers at different sparsity levels (10% → 60%) on GraphSAGE/ClusterGCN with Ogbn- ArXiv. We report the mean accuracy± stdev of 3 runs. We shade the best-performing value in each column. GraphSAGE10% 20% 30% 40% 50% 60% LSim 69.22±0.11 68.40±0.18 66.15±0.22 64.66±0.31 61.07±0.23 58.21±0.09 DSpar 71.23±0.24 71.03±0.28 68.50±0.33 64.57±0.26 62.79±0.66 60.49±0.58 UGS 68.77±0.21 67.92±0.53 66.30±0.27 66.57±0.18 65.72±0.14 63.40±0.36 GST 71.12±0.23 71.14±0.18 71.46±0.37 70.57±0.34 68.02±0.58 66.55±0.53 ClusterGCN10% 20% 30% 40% 50% 60% LSim 67.27±0.54 67.80±0.46 65.49±0.45 64.97±0.52 63.56±0.39 62.18±0.39 DSpar 68.75±0.75 68.40±0.69 66.72±0.56 66.09±0.71 65.45±0.51 63.72±0.50 GST 69.44±0.14 69.21±0.24 68.17±0.63 68.02±0.49 67.33±0.37 65.98±0.50 formance losses of only0.43% and 0.48%, respectively. For Ogbn-Products+ClusterGCN at 60% sparsity, GST outper- forms DSpar and LSim by 3.45% and 7.95%, respectively. Obs. 3 Different GNN backbones and graphs show vary- ing resistance to sparsification. As shown in Fig. 3, GIN/GAT are less affected by graph sparsification com- pared to GCN. At 80% graph sparsity, sparsifiers generally lead to an accuracy drop of 7.08% ∼ 20.7% on GCN+Cora, whereas that on GAT+Cora is only 6.44% ∼ 14.1%. More- over, the resilience to sparsification varies across graphs; specifically, GAT+Citeseer experiences a3.67% ∼ 19.9% decline at 80% sparsity, while GAT+PubMed shows only a 3.16% ∼ 8.46% degradation. 𝒔𝒈 = 𝟐𝟎% 𝒔𝒈 = 𝟐𝟎% 𝒔𝒈 = 𝟓𝟎% 𝒔𝒈 = 𝟓𝟎% Figure 4.The relative error of the top-200 and bottom-200 eigen- values on PubMed+GCN, i.e., λi−λ′ i λi , sparsified by different meth- ods at sparsity level 20% and 50%. 4.3. Spectral Preservation Helps Sparsification (RQ2) To answer RQ2, we compare the eigenvalue variation be- tween sparse and original graphs, following (Liu et al., 2023). In Fig. 4, we showcase the relative error in the top-200 and bottom-200 eigenvalues of sparse graphs pro- duced by sparsifiers compared to the original graph, on Citeseer/PubMed+GIN/GAT. We select only the top/bottom- 200 eigenvalues because small (large) eigenvalues represent the global clustering (local smoothness) structures of the graphs, adequately reflecting the preservation of spectral information (Liu et al., 2023). We observe that: Obs. 4 GST effectively preserves key eigenvalues. As demonstrated in Figs. 4 and 8 to 10, for the top-200 eigen- values, GST achieves a performance similar to or better than LSim, and significantly surpasses UGS at 50% sparsity. On Citeseer+GIN, its relative error fluctuates only within 0% ∼ 8%. Regarding the bottom-200 eigenvalues, GST provides the best approximation. At 20% sparsity, its av- erage relative error is 20% lower than that of UGS. More analyses can be found in Appendix H.2. GraphSAGE ClusterGCN 88.4 120 60 30 69.0 42.2 53.8 28.1 118.4 98.8 51.6 34.6 90 Baseline LSim DSpar UGS GST Figure 5.The inference latency on Ogbn-Proteins with different sparsifiers when their performance loss is negligible (≤ 1%). 4.4. GST Significantly Accelerates Computations (RQ3) To answer RQ3, we exhaustively compare the efficiency of different sparsifiers via three metrics, extreme graph sparsity, inference MACs, and GPU inference latency in Tab. 3 and Figs. 5 and 11. Further explanations on these metrics can be found in Appendix G.6. We give the following observations: Obs. 5 GST significantly accelerates GNN inference. More specifically, GST’s inference speedup is more pro- nounced for large-scale graphs compared to smaller ones. As shown in Tab. 3, GST provides 33.29% ∼ 37.22% MACs savings for GCN/GIN/GAT, with average inference speedups of 1.30×, 1.37×, and 1.45×, respectively. On Ogbn-Proteins, this can reach 3.14 ∼ 3.42× (in Fig. 5). In conclusion, GST significantly aids in accelerating GNN inference at a practically applicable level. 4.5. High Robustness and Versatility of GST (RQ4) To validate the versatility of GST, this section examines (1) its robustness against edge perturbations, and (2) its applicability to graph lottery tickets identification. In the perturbation experiments, we induce edge perturbations by randomly relocating edge endpoints. For the graph lottery ticket experiment, we simply replace the graph mask found by UGS (Chen et al., 2021) with that by GST in the graph lottery ticket and retrain the GNN from scratch. Figs. 6 and 12 illustrate the performance of GST at various spar- 7Boosting Graph Sparse Training via Semantic and Topological Awareness Table 3.Efficiency comparison between GST and other sparsifiers. “Sparsity (%)” indicates the extreme graph sparsity of different sparsifiers; “MACs (m)” represents the inference MACs (= 1 2 FLOPs); “Latency (ms)” refers to GPU reference latency. We shade the best results and underline the second-best results for each dataset. The GPU latency results are averaged over five random trials. Model Cora Citeseer PubMed Avg. Sparsity (%) MACs (m) Latency (ms) Sparsity (%) MACs (m) Latency (ms) Sparsity (%) MACs (m) Latency (ms)RankMAC SavingsSpeedup GCN Baseline – 1996.55 4 .27 – 6318.00 6 .08 – 5077.84 6 .25 – 0% 1 .00×SCAN 11.93% 1758.37 4 .13 6 .52% 5526.99 5 .88 23 .68% 3875.41 5 .52 7 16.04% 1 .03×LSim 11.58% 1765.35 3 .99 21 .83% 4938.78 5 .35 11 .84% 4476.62 5 .67 5 15.06% 1 .10×DSpar 9.78% 1882.44 4 .20 8 .22% 5943.22 5 .97 18 .60% 4266.18 5 .49 6 8.65% 1 .05×UGS 19.10% 1758.37 4 .13 27 .20% 4599.50 5 .22 24 .91% 3905.67 5 .12 3 23.73% 1 .13×WD-GLT19.46% 1702.34 4 .24 19 .46% 4989.16 5 .37 11 .24% 4455.10 5 .67 4 18.20% 1 .11×MGSpar 31.00% 1568.21 3.92(1.08×) 30.00% 4318.75 5.03(1.20×) 27.00% 3758.53 5.11(1.22×) 2 24.67% 1 .13×GST 40.00% 1397.59 3.36(1.27×) 43 .00% 3890.80 4.48(1.35×) 35 .00% 3092.81 4.82(1.29×) 1 34.49% 1.30× GIN Baseline – 2006.26 2 .53 – 6328.22 5 .02 – 5108.12 6 .12 – 0% 1 .00×SCAN 11.93% 1766.91 2 .24 12 .52% 5535.93 3 .84 11 .82% 4504.34 5 .81 5 12.10% 1 .16×LSim 11.58% 1773.93 2 .34 10 .43% 5668.19 4 .11 11 .84% 4503.32 5 .74 6 11.28% 1 .12×DSpar 21.11% 1533.29 2.02(1.25×) 14.77% 5372.09 3 .71 16 .18% 4157.10 5 .16 3 19.09% 1 .26×UGS 19.4% 1617 .05 2 .18 27 .5% 4587.96 3 .60(1.39×) 27.6% 3698 .28 5 .02 4 24.83% 1 .25×WD-GLT19.7% 1590 .33 2 .16 16 .4% 5198 .11 3 .73 32 .4% 3450 .59 4.89(1.25×) 2 23.66% 1 .27×MGSpar 10.00% 1784.52 2 .39 5 .00% 6087.18 4 .90 7 .00% 4885.16 5 .94 7 6.49% 1 .05×GST 31.00% 1304.27 1.85(1.36×) 26 .43% 4633.97 3.52(1.42×) 53 .00% 3109.47 4.57(1.33×) 1 33.29% 1 .37× GAT Baseline – 8029.60 13 .2 – 25309.91 13 .5 – 20675.00 15 .3 – 0% 1 .00×SCAN 24.16% 6089.65 9 .9 24 .87% 19015.33 11 .8 23 .68% 15779.16 12.6(1.21×) 3 22.23% 1 .23×LSim 23.22% 6165.13 10 .2 21 .83% 19784.75 11 .8 11 .84% 18227.08 14 .9 6 16.96% 1 .15×DSpar 17.34% 6788.29 11 .33 22 .85% 19655.05 11 .7 11 .58% 18106.99 14 .8 7 15.25% 1 .12×UGS 18.22% 6584.27 11 .15 25 .54% 18850.75 12 .36 18 .22% 16953.50 13 .92 6 18.66× 1.13×WD-GLT19.71% 6504.33 10.91(1.20×) 28.54% 18223.13 11 .57 13 .54% 17784.61 14 .50 4 18.59% 1 .16×MGSpar 19.00% 6400.25 11 .08 32 .00% 17114.76 11.19(1.20×) 20.00% 16309.54 12 .8 2 27.59% 1 .23×GST 35.00% 5216.28 7.90(1.67×) 41 .00% 14920.45 10.30(1.31×) 39 .00% 13438.75 11.23(1.36×) 1 37.22% 1 .45× Cora Citeseer PubMed Figure 6.The robust performance of GST on edge perturbations with a varying fraction of perturbed edges (0% → 40%). sity levels against edge perturbation, and Tab. 11 presents the performance improvement achieved by combining GST with UGS. Observations include: 1 GST significantly en- hances GNN’s robustness against edge perturbations. For instance, on Cora+GCN, GST at 10% sparsity achieved up to 7.23% performance improvement (in Fig. 12). 2 Across all sparsity levels, GST consistently aids UGS in identify- ing graph lottery tickets (in Tab. 11). Detailed analysis is provided in Appendix H.4. 4.6. Ablation & Parameter Sensitivity Analysis (RQ5) Ablation Study. In order to better verify the effectiveness of each component of GST, We have made corresponding mod- ifications to GST and designed the following three variants: (1) GST w/o tuning: moving the dynamic sparse training part of GST; (2) GST w/o sema: merely utilizing ϕ(sema) when updating the sparse graph; (3) GST w/o topo: merely utilizing ϕ(topo) when updating the sparse graph. As indicated in Tab. 4 and Fig. 13, it is evident that 1 the removal of the dynamic fine-tuning process in GST w/o tuning leads to a significant performance drop; 2 using ei- ther ϕ(topo) or ϕ(sema) alone cannot match the performance of the original GST, with the omission of ϕ(sema) having a more pronounced impact. In summary, removing either component deteriorates the effectiveness of GST. For de- tailed data, as well as parameter sensitivity experiments, refer to Appendix H.5. Table 4.Ablation study on GST with its three variants. We re- port the extreme graph sparsity on Citeseer+GCN/GIN and Ogbn- ArXiv+GraphSAGE/ClusterGCN. Dataset Citeseer Ogbn-ArXiv Backbone GCN GIN GraphSAGE ClusterGCN GST 43.43±1.46 26.43±1.07 35.18±0.72 21.44±0.95 GSTw/o tuning 30.41±0.75 10.39±1.78 18.32±1.14 12.57±0.56 GSTw/o sema 39.82±1.49 25.09±1.32 29.78±1.42 19.24±0.85 GSTw/o topo 42.08±0.56 25.93±1.26 32.21±0.49 19.67±0.79 5. Conclusion & Future Work This paper studies the notorious inefficiency of GNNs. Dif- ferent from previous research literature, we open a novel topic, termed Graph Sparse Training (GST), for the first time. GST aims to fine-tune the sparse graph structure during the training process, utilizing anchors derived from full graph training as supervisory signals. GST also pro- poses the Equilibria Principle to balance both topological and semantic information preservation. Extensive exper- iments demonstrate that integrating the GST concept can enhance both performance and inference speedup. Addi- tionally, GST can serve as a philosophy to benefit a wide array of training algorithms. In the future, we plan to ex- tend GST to more complex scenarios, including heterophilic and heterogeneous graphs, and further explore the feasibil- ity of in-time sparsification & acceleration in real-world applications (recommender systems, fraud detection, etc.). 8Boosting Graph Sparse Training via Semantic and Topological Awareness References Achille, A., Rovere, M., and Soatto, S. Critical learning periods in deep networks. In International Conference on Learning Representations, 2018. Batson, J., Spielman, D. A., Srivastava, N., and Teng, S.-H. Spectral sparsification of graphs: theory and algorithms. Communications of the ACM, 56(8):87–94, 2013. Bencz´ur, A. A. and Karger, D. R. Approximating st mini- mum cuts in ˜o (n 2) time. In Proceedings of the twenty- eighth annual ACM symposium on Theory of computing, pp. 47–55, 1996. Bhattacharjee, R., Dexter, G., Drineas, P., Musco, C., and Ray, A. Sublinear time eigenvalue approximation via ran- dom sampling. arXiv preprint arXiv:2109.07647, 2021. Chan, H. and Akoglu, L. Optimizing network robustness by edge rewiring: a general framework. Data Mining and Knowledge Discovery, 30:1395–1425, 2016. Chen, J., Zhu, J., and Song, L. Stochastic training of graph convolutional networks with variance reduction. arXiv preprint arXiv:1710.10568, 2017. Chen, J., Ma, T., and Xiao, C. Fastgcn: fast learning with graph convolutional networks via importance sampling. In Proceedings of ICLR, 2018. Chen, T., Sui, Y ., Chen, X., Zhang, A., and Wang, Z. A unified lottery ticket hypothesis for graph neural networks. In International Conference on Machine Learning , pp. 1695–1706. PMLR, 2021. Chen, Y ., Ye, H., Vedula, S., Bronstein, A., Dreslinski, R., Mudge, T., and Talati, N. Demystifying graph sparsifica- tion algorithms in graph properties preservation. arXiv preprint arXiv:2311.12314, 2023. Chiang, W.-L., Liu, X., Si, S., Li, Y ., Bengio, S., and Hsieh, C.-J. Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 257–266, 2019. doi: 10.1145/3292500.3330925. Costa, L. d. F., Rodrigues, F. A., Travieso, G., and Vil- las Boas, P. R. Characterization of complex networks: A survey of measurements. Advances in physics, 56(1): 167–242, 2007. Cui, G. and Wei, Z. Mgnn: Graph neural networks inspired by distance geometry problem. InProceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 335–347, 2023. David, J. Algorithms for analysis and design of robust controllers. 1995. Dettmers, T., Zettlemoyer, L., and Zhang. Sparse networks from scratch: Faster training without losing performance. arXiv preprint arXiv:1907.04840, 2019. Eden, T., Jain, S., Pinar, A., Ron, D., and Seshadhri, C. Provable and practical approximations for the degree dis- tribution using sublinear graph samples. In Proceedings of the 2018 World Wide Web Conference, pp. 449–458, 2018. Evci, U., Gale, T., Menick, J., Castro, P. S., and Elsen, E. Rigging the lottery: Making all tickets winners. In International Conference on Machine Learning, pp. 2943– 2952. PMLR, 2020. Fey, M. and Lenssen, J. E. Fast graph representation learning with pytorch geometric.arXiv preprint arXiv:1903.02428, 2019. Forman. Bochner’s method for cell complexes and com- binatorial ricci curvature. Discrete & Computational Geometry, 29:323–374, 2003. Frankle, J., Dziugaite, G. K., Roy, D. M., and Carbin, M. Pruning neural networks at initialization: Why are we missing the mark? arXiv preprint arXiv:2009.08576 , 2020. Hamann, M., Lindner, G., Meyerhenke, H., Staudt, C. L., and Wagner, D. Structure-preserving sparsification meth- ods for social networks. Social Network Analysis and Mining, 6:1–22, 2016. Hamilton, W., Ying, Z., and Leskovec, J. Inductive represen- tation learning on large graphs. In Proceedings of NIPS, 2017. Hoang, D., Liu, S., Marculescu, R., and Wang, Z. Re- visiting pruning at initialization through the lens of ra- manujan graph. In International Conference on Learning Representations. International Conference on Learning Representations (ICLR) 2023, 2023. Hu, W., Fey, M., Zitnik, M., Dong, Y ., Ren, H., Liu, B., Catasta, M., and Leskovec, J. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020. Huang, S., Lei, B., Xu, D., Peng, H., Sun, Y ., Xie, M., and Ding, C. Dynamic sparse training via balancing the exploration-exploitation trade-off. In 2023 60th ACM/IEEE Design Automation Conference (DAC) , pp. 1–6. IEEE, 2023. 9Boosting Graph Sparse Training via Semantic and Topological Awareness Hui, B., Yan, D., Ma, X., and Ku, W.-S. Rethinking graph lottery tickets: Graph sparsity matters. In The Eleventh International Conference on Learning Representations, 2023. Jin, W., Zhao, L., Zhang, S., Liu, Y ., Tang, J., and Shah, N. Graph condensation for graph neural networks. arXiv preprint arXiv:2110.07580, 2021. Karhadkar, K., Banerjee, P. K., and Montufar, G. Fosr: First-order spectral rewiring for addressing oversquashing in gnns. In The Eleventh International Conference on Learning Representations, 2023. Kipf, T. N. and Welling, M. Semi-Supervised Classification with Graph Convolutional Networks. In Proceedings of the 5th International Conference on Learning Represen- tations, 2017a. Kipf, T. N. and Welling, M. Semi-supervised classification with graph convolutional networks. In Proceedings of ICLR, 2017b. Lee, N., Ajanthan, T., and Torr, P. H. Snip: Single-shot network pruning based on connection sensitivity. arXiv preprint arXiv:1810.02340, 2018. Liu, J., Xu, Z., Shi, R., Cheung, R. C., and So, H. K. Dy- namic sparse training: Find efficient sparse network from scratch with trainable masked layers. arXiv preprint arXiv:2005.06870, 2020. Liu, S., Yin, L., Mocanu, D. C., and Pechenizkiy, M. Do we actually need dense over-parameterization? in-time over- parameterization in sparse training. In International Con- ference on Machine Learning , pp. 6989–7000. PMLR, 2021. Liu, Z., Zhou, K., Jiang, Z., Li, L., Chen, R., Choi, S.-H., and Hu, X. Dspar: An embarrassingly simple strategy for efficient gnn training and inference via degree-based spar- sification. Transactions on Machine Learning Research, 2023. Luo, D., Cheng, W., Yu, W., Zong, B., Ni, J., Chen, H., and Zhang, X. Learning to drop: Robust graph neural network via topological denoising. In Proceedings of the 14th ACM international conference on web search and data mining, pp. 779–787, 2021. Ma, X., Yuan, G., Shen, X., Chen, T., Chen, X., Chen, X., Liu, N., Qin, M., Liu, S., Wang, Z., et al. Sanity checks for lottery tickets: Does your winning ticket really win the jackpot? Advances in Neural Information Processing Systems, 34:12749–12760, 2021. Mocanu, D. C., Mocanu, E., Stone, P., Nguyen, P. H., Gibescu, M., and Liotta, A. Scalable training of arti- ficial neural networks with adaptive sparse connectivity inspired by network science. Nature communications, 9 (1):2383, 2018. Qiu, S., You, L., and Wang, Z. Optimizing sparse matrix multiplications for graph neural networks. In Interna- tional Workshop on Languages and Compilers for Paral- lel Computing, pp. 101–117. Springer, 2021. Rong, Y ., Huang, W., Xu, T., and Huang, J. Dropedge: Towards deep graph convolutional networks on node clas- sification. arXiv preprint arXiv:1907.10903, 2019. Rozemberczki, B., Allen, C., and Sarkar, R. Multi-scale at- tributed node embedding. Journal of Complex Networks, 9(2):cnab014, 2021. Satuluri, V ., Parthasarathy, S., and Ruan, Y . Local graph sparsification for scalable clustering. In Proceedings of the 2011 ACM SIGMOD International Conference on Management of data, pp. 721–732, 2011. Spielman, D. A. and Srivastava, N. Graph sparsification by effective resistances. In Proceedings of the fortieth annual ACM symposium on Theory of computing , pp. 563–568, 2008. Sun, Q., Li, J., Peng, H., Wu, J., Fu, X., Ji, C., and Philip, S. Y . Graph structure learning with variational informa- tion bottleneck. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 36, pp. 4165–4174, 2022. Tsitsulin, A. and Perozzi, B. The graph lottery ticket hypoth- esis: Finding sparse, informative graph structure. arXiv preprint arXiv:2312.04762, 2023. Velickovic, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., and Bengio, Y . Graph attention networks.stat, 1050: 20, 2017. Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Li`o, P., and Bengio, Y . Graph attention networks. In International Conference on Learning Representations, 2018. Wan, G. and Kokel, H. Graph sparsification via meta- learning. DLG@ AAAI, 2021. Wan, G. and Schweitzer, H. Edge sparsification for graphs via meta-learning. In 2021 IEEE 37th International Con- ference on Data Engineering (ICDE) , pp. 2733–2738. IEEE, 2021. Wang, K., Liang, Y ., Wang, P., Wang, X., Gu, P., Fang, J., and Wang, Y . Searching lottery tickets in graph neural net- works: A dual perspective. In The Eleventh International Conference on Learning Representations, 2022. 10Boosting Graph Sparse Training via Semantic and Topological Awareness Wang, K., Liang, Y ., Li, X., Li, G., Ghanem, B., Zimmer- mann, R., Yi, H., Zhang, Y ., Wang, Y ., et al. Brave the wind and the waves: Discovering robust and generaliz- able graph lottery tickets. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023a. Wang, K., Liang, Y ., Wang, P., Wang, X., Gu, P., Fang, J., and Wang, Y . Searching lottery tickets in graph neural networks: A dual perspective. In The Eleventh International Conference on Learning Representations, 2023b. URL https://openreview.net/forum? id=Dvs-a3aymPe. Wang, M., Zheng, D., Ye, Z., Gan, Q., Li, M., Song, X., Zhou, J., Ma, C., Yu, L., Gai, Y ., et al. Deep graph library: A graph-centric, highly-performant package for graph neural networks. arXiv preprint arXiv:1909.01315, 2019. Wang, Y ., Liu, S., Chen, K., Zhu, T., Qiao, J., Shi, M., Wan, Y ., and Song, M. Adversarial erasing with pruned elements: Towards better graph lottery ticket. arXiv preprint arXiv:2308.02916, 2023c. Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., and Philip, S. Y . A comprehensive survey on graph neural networks. IEEE transactions on neural networks and learning sys- tems, 32(1):4–24, 2020. Xu, K., Hu, W., Leskovec, J., and Jegelka, S. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. Xu, X., Yuruk, N., Feng, Z., and Schweiger, T. A. Scan: a structural clustering algorithm for networks. In Proceed- ings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 824–833, 2007. Ying, R., He, R., Chen, K., Eksombatchai, P., Hamilton, W. L., and Leskovec, J. Graph convolutional neural net- works for web-scale recommender systems. In Proceed- ings of KDD, 2018a. Ying, Z., You, J., Morris, C., Ren, X., Hamilton, W., and Leskovec, J. Hierarchical graph representation learning with differentiable pooling. Advances in neural informa- tion processing systems, 31, 2018b. You, H., Li, C., Xu, P., Fu, Y ., Wang, Y ., Chen, X., Baraniuk, R. G., Wang, Z., and Lin, Y . Drawing early-bird tickets: Towards more efficient training of deep networks. arXiv preprint arXiv:1909.11957, 2019. You, H., Lu, Z., Zhou, Z., Fu, Y ., and Lin, Y . Early-bird gcns: Graph-network co-optimization towards more ef- ficient gcn training and inference via drawing early-bird lottery tickets. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 8910–8918, 2022. Yuan, G., Ma, X., Niu, W., Li, Z., Kong, Z., Liu, N., Gong, Y ., Zhan, Z., He, C., Jin, Q., et al. Mest: Accurate and fast memory-economic sparse training framework on the edge. Advances in Neural Information Processing Systems, 34: 20838–20850, 2021. Zhang, G., Wang, K., Huang, W., Yue, Y ., Wang, Y ., Zim- mermann, R., Zhou, A., Cheng, D., Zeng*, J., and Liang*, Y . Graph lottery ticket automated. InThe International Conference on Learning Representations, 2024. Zhang, J., Dong, Y ., Wang, Y ., Tang, J., and Ding, M. Prone: Fast and scalable network representation learning. In IJCAI, volume 19, pp. 4278–4284, 2019. Zhang, M. and Chen, Y . Link prediction based on graph neural networks. In Proceedings of NIPS, 2018. Zhang, M. and Chen, Y . Inductive matrix comple- tion based on graph neural networks. arXiv preprint arXiv:1904.12058, 2019. Zhang, M., Cui, Z., Neumann, M., and Chen, Y . An end-to- end deep learning architecture for graph classification. In Proceedings of the AAAI conference on artificial intelli- gence, volume 32, 2018. Zhang, Y ., Zhao, L., Lin, M., Sun, Y ., Yao, Y ., Han, X., Tanner, J., Liu, S., and Ji, R. Dynamic sparse no training: Training-free fine-tuning for sparse llms. arXiv preprint arXiv:2310.08915, 2023. Zheng, C., Zong, B., Cheng, W., Song, D., Ni, J., Yu, W., Chen, H., and Wang, W. Robust graph representation learning via neural sparsification. In International Con- ference on Machine Learning, pp. 11458–11468. PMLR, 2020. Zhou, J., Cui, G., Hu, S., Zhang, Z., Yang, C., Liu, Z., Wang, L., Li, C., and Sun, M. Graph neural networks: A review of methods and applications. AI open, 1:57–81, 2020. Zhu, M. and Gupta, S. To prune, or not to prune: exploring the efficacy of pruning for model compression. arXiv preprint arXiv:1710.01878, 2017. 11Boosting Graph Sparse Training via Semantic and Topological Awareness A. Notations Table 5.The notations that are commonly used in Methodology (Sec. 3). Notation Definition G = {V, E} = {A, X} Input graph A Input adjacency matrix X Node features Θ Weight matrices of GNN λi(G) The i-th eigenvalue of G sg Target graph sparsity Ψ Graph masker mA g Graph mask at the point of the highest validation score during full-graph training ZA = f({mA g ⊙ A, X}, ΘA) GNN output at the point of the highest validation score during full-graph training GA = {mA g ⊙ A, ZA} Anchor graph that contains the topological & semantic information of the original graph MOS g (Binary) one-shot graph mask M(µ) g (Binary) graph mask at µ-th interval E(µ) Remained edges at µ-th interval EC (µ) Pruned edges at µ-th interval B. More Dicussions on Related Work Sprase Training Target Semantic?Topology? PRC § Criterion † Backbone SNIP (Lee et al., 2018) Sparse NN ✓ ✗ ✓ magnitude, gradient AlexNet, VGG, LSTM, etc. SET (Mocanu et al., 2018) Sparse NN ✓ ✗ ✓ magnitude, gradient MLP, CNN SNFS (Dettmers et al., 2019) Sparse NN ✓ ✗ ✓ magnitude, momentum AlexNet, VGG, ResNet, etc. RigL (Evci et al., 2020) Sparse NN ✓ ✗ ✓ magnitude, gradient ResNet, MobileNet ITOP (Liu et al., 2021) Sparse NN ✓ ✗ ✓ magnitude, gradient MLP, VGG, ResNet, etc. DST (Liu et al., 2020) Sparse NN ✓ ✗ ✗ magnitude LeNet, LSTM, VGG, etc. MEST (Yuan et al., 2021) Sparse NN ✓ ✗ ✓ magnitude, gradient ResNet IMDB (Hoang et al., 2023) Sparse NN ✗ ✓ ✓ IMDB property ResNet, VGG DSnT (Zhang et al., 2023) Sparse LLM ✓ ✗ ✓ magnitude, output variation LLaMA, Vicuna, OPT GST (Ours) Sparse Graph ✓ ✓ ✓ magnitude, eigenvalue, gradient GCN, GIN, GAT, etc. § Prune Rate Control. Whether the method has control over the sparsity rate. † Criterion refers to the drop or regrow criterion during connection update. Table 6.Comparison among different sparse training techniques, regarding their sparsification target, semantic and topological awareness, sparsity rate controllability, drop/grow criterion and backbones. Note that our proposed GST is the initial endeavor to apply the concept of dynamic sparse training to graph data, incorporating graph-related domain knowledge. To highlight our contributions, we detail in Tab. 6 how our proposed GST differs from traditional (dynamic) sparse training methods, with the two most crucial distinctions as follows: • Sparsification Target: Previous dynamic sparse training methods primarily focused on pruning parameters, typically within traditional CNN frameworks (e.g., VGG, Resnet) or large language models (e.g., LLaMA, OPT). In contrast, GST is a novel exploration of dynamically sparsifying the input data, specifically graph data. • Drop & Regrow Criterion: Most prior sparse training techniques utilize semantic information from the model training procedure (e.g., magnitude, momentum, gradients) for parameter pruning. GST is not a trivial adaptation of DST to graph data; instead, it represents the first attempt that combines well-developed semantic criteria and domain knowledge about graph topology. Specifically, GST, for the first time, considers both the impact of sparsification on model output (reflecting semantics) and on graph Laplacian eigenvalues (reflecting topology). It optimizes these two objectives in tandem to obtain the optimal sparse subgraph. 12Boosting Graph Sparse Training via Semantic and Topological Awareness C. Eigenvalue Variation Approximation Given the anchor graph GA = {mA ⊙ A, ZA}, we aim to assess how removing a specific edge affects GA’s overall topological properties, as indicated by the eigenvalue variation, as follows: ϕ(topo)(eij) = NX k=1 |∆λk| λk = NX k=1 |λk(GA) − λk(GA \\ eij)| λk(GA) , (14) However, recalculating the N eigenvalues for each GA \\ eij theoretically requires O(E · N3) complexity, which is computationally impractical. Therefore, we consider approximating the eigenvalue variation. We denote the impact of removing eij on the anchor graph as ∆A, where ∆Aij = −1, and the rest are zeros. The anchor graph after removing eij can be expressed as mA g ⊙ (A + ∆A). Similarly, the influence of removing eij on the right eigenvalue λk and the corresponding right eigenvector b(k) of GA is denoted as ∆λk and ∆b, respectively. According to the definitions of eigenvalues and eigenvectors, we have: \u0000 mA g ⊙ (A + ∆A) \u0001 (b(k) + ∆b(k)) = (λk + ∆λk)(b(k) + ∆b(k)). (15) It is noteworthy that for large matrices, it is reasonable to assume that the removal of a link or node has a minor impact on the spectral properties of the graph. Therefore, both ∆b(k) and ∆λ(k) are small. Left-multiplying this equation by the transpose of the left eigenvector aT and neglecting second-order terms a(k)T ∆A∆b(k) and ∆λka(k)T ∆b(k), we have: ∆λk = mA g,ija(k) i b(k) j a(k)T b(k) , (16) Based on this, the eigenvalue variation of eij can be further expressed as: ϕ(topo)(eij) = NX k=1 \f\f\f\f\f mA g,ija(k) i b(k) j λka(k)T b(k) \f\f\f\f\f, (17) However, for large graphs, computing all eigenvalues/eigenvectors still incurs significant computational overhead. Con- sidering that small (large) eigenvalue can effectively indicate the global clustering (local smoothness) structure of the graphs (Zhang et al., 2019), we only select the top-K and bottom-K (K=20) eigenvalues to compute their variation: ϕ(topo)(eij) = ( KX k=1 + NX k=N−K ) \f\f\f\f\f mA g,ija(k) i b(k) j λka(k)T b(k) \f\f\f\f\f, (18) Additionally, we utilize non-trivial approximations of eigenvalues/eigenvectors with sublinear,i.e., o(N2), time complex- ity (Bhattacharjee et al., 2021) to reduce the computational budget further. D. Compelxity Analysis During the full-graph training stage, the inference time complexity of GST is: O  L × E × D| {z } aggregation + graph masker z }| { E × D + L × N × D| {z } update  , (19) where L is the number of GNN layers and D is the feature dimension. Subsequently, we approximate the eigen- value/eigenvector for each edge, with a sublinear time complexity of o(N2). It is worth noting that this computation is performed only once. The inference time complexity of the sparse training procedure is: O  L × ∥Mg ⊙ A∥0 × D| {z } sparsified aggregation + graph masker z }| { ∥Mg ⊙ A∥0 × D + L × N × D| {z } update  , (20) 13Boosting Graph Sparse Training via Semantic and Topological Awareness Algorithm 1 Algorithm workflow of GST Input : G = (A, X), GNN model f(G, Θ0), GNN’s initialization Θ0, target sparsity sg%, update interval ∆T, the number of epochs to obtain anchor E, the number of epochs for sparse graph fine-tuning D, learning rate η. Output :Sparse graph Gsub = {Mg ⊙ A, X} 1 for iteration i ← 1 to E do 2 Compute the edge mask mi g via graph masker Ψ ; ▷ Eq. 3 3 Forward fsub \u0000 {mi g ⊙ A, Θi}, mθ \u0001 to compute Lanchor; ▷ Eq. 4 4 Backpropagate to update Θi+1 ← Θi − η∇ΘLanchor. 5 Update masks mg with graph masker Ψ. 6 end /* Obtain Anchor Graph */ 7 Record the anchor graph GA = {mA g ⊙ A, ZA} with the highest validation score. /* Obtain One-shot Mask */ 8 Set sg% of the lowest magnitude values in mA g to 0 and others to 1, then obtain one-shot mask MOS g . /* Dynamically Update Edge Mask */ 9 Set M(1) g ← MOS g . 10 for iteration d ← 1 to D do 11 Compute interval index µ ← ⌈d/∆T⌉. 12 Forward f \u0010 {mg ⊙ M(µ) g ⊙ A, X}, Θ \u0011 to compute the Lanchor. 13 Update Θ and mg accordingly. /* Update Graph Structure */ 14 if µ = ⌈d/∆T⌉ then 15 Set E(µ) ← edges in M(µ) g ⊙ A, EC (µ) ← edges in ¬M(µ) g ⊙ A. 16 for edge (i, j) in E do 17 Compute semantic criteria ϕ(sema)(eij) ← \f\f∇eij S(GA, G(µ)) \f\f. ; ▷ Eq. 12 18 Compute topological criteria ϕ(topo)(eij) ← \u0010PK k=1 + PN k=N−K \u0011 mA g,ija(k) i b(k) j λk(GA)a(k)T b(k) ; ▷ Eq. 10 19 Combine semantic and topological criteria ϕ(eij) ← βs · ϕ(sema)(eij) + βt · ϕ(topo)(eij); ▷ Eq. 13 20 end 21 Compute the number of edges to be swapped r ← Υ(µ); ▷ Eq. 22 /* Select Drop/Regrow Edges */ 22 Set M(prune) ← ArgTopK \u0000 −ϕ(E(µ)), r \u0001 , M(regrow) ← ArgTopK \u0010 ϕ(EC (µ)), r \u0011 ; ▷ Eq. 5 23 Update edge masks M(µ+1) g ← \u0010 M(µ) g \\ M(prune) g \u0011 ∪ M(regrow) g ; ▷ Eq. 6 24 end The memory complexity of GST is: O  L × N × D| {z } node embeddings + GNN parameter z }| { L × |Θ| ×D2 + |Ψ| ×D| {z } graph masker   (21) E. Algorithm Framework The algorithm framework is presented in Algo. 1. F. Dataset Description We conclude the dataset statistics in Tab. 7 14Boosting Graph Sparse Training via Semantic and Topological Awareness Table 7.Graph datasets statistics. Dataset Nodes Edges Ave. Degree Features Classes Metric Cora 2,708 5,429 3.88 1,433 7 Accuracy Citeseer 3,327 4,732 1.10 3,703 6 Accuracy PubMed 19,717 44,338 8.00 500 3 Accuracy Ogbn-ArXiv 169,343 1,166,243 13.77 128 40 Accuracy Ogbn-Proteins 132,534 39,561,252 597.00 8 2 ROC-AUC Ogbn-Products 2,449,029 61,859,140 50.52 100 47 Accuracy G. Experimental Configurations G.1. Train-val-test Splitting of Datasets. To rigorously verify the effectiveness of our proposed GST, we unify the dataset splitting strategy across all GNN backbones and baselines. As for node classification tasks of small- and medium-size datasets, we utilize 420 (Citeseer) and 460 (PubMed) labeled data for training, 500 nodes for validation and 500 nodes for testing. For Squirrel and Chameleon datasets, we follow the original settings in (Cui & Wei, 2023; Rozemberczki et al., 2021), and set the train/valid/test ratio as 60%/20%/20%. The data splits for Ogbn-ArXiv, Ogbn-Proteins, and Ogbn-Products were provided by the benchmark (Hu et al., 2020). Specifically, for Ogbn-ArXiv, we train on papers published until 2017, validate on papers from 2018 and test on those published since 2019. For Ogbn-Proteins, protein nodes were segregated into training, validation, and test sets based on their species of origin. For Ogbn-Products, we use the sales ranking (popularity) to split nodes into training/validation/test sets. Concretely, we sort the products according to their sales ranking and use the top 8% for training, the next top 2% for validation, and the rest for testing. G.2. Baseline Configurations We detail how we report the results of baseline methods: • Topology-based sparsification – SCAN (Spielman & Srivastava, 2008): SCAN uses structural similarity (called SCAN similarity) measures to detect clusters, hubs, and outliers. We utilize the implementation in (Chen et al., 2023) – Local Similarity (Hamann et al., 2016): Local Similarity ranks edges using the Jaccard score and computes log(rank(eij))/ log(deg(eij)) as the similarity score, and selects edges with the highest similarity scores. We utilize the implementation in (Chen et al., 2023). – DSpar (Liu et al., 2023): DSpar is an extension of effective resistance sparsifier, which aims to reduce the high computational budget of calculating effective resistance through an unbiased approximation. We adopt their official implementation (Liu et al., 2023). • Semantic-based sparsification – UGS (Chen et al., 2021): We utilize the official implementation from the authors. Notably, UGS was originally designed for joint pruning of model parameters and edges. Specifically, it sets separate pruning parameters for parameters and edges, namely the weight pruning ratio pθ and the graph pruning ratio pg. In each iteration, a corresponding proportion of parameters/edges is pruned. For a fairer comparison, we set pθ = 0%, while maintaining pg = 5% (consistent with the original paper). – WD-GLT (Hui et al., 2023): WD-GLT inherits the iterative magnitude pruning paradigm from UGS, so we also set pθ = 0%, pg = 5% across all datasets and backbones. The perturbation ratio α is tuned among {0, 1}. Since no official implementation is provided, we carefully reproduced the results according to the original paper. – Meta-gradient sparsifier (Wan & Schweitzer, 2021): The Meta-gradient sparsifier prunes edges based on their meta-gradient importance scores, assessed over multiple training epochs. Since no official implementation is provided, we carefully replicated the results following the guidelines in the original paper. In addition to our selected baselines, we also enumerate other classic baselines relevant to graph sparsification and explain why they were not chosen for our study: 15Boosting Graph Sparse Training via Semantic and Topological Awareness • Effective Resistance Sparsifier (Spielman & Srivastava, 2008): This method is one of the most renowned spectral sparsifiers. However, due to its high time complexity (′(E log(|V|)3)), we opted for its approximate version, DSpar (Liu et al., 2023). • DropEdge (Rong et al., 2019): Though DropEdge also involves dropping edges during training, the dropping process is random across different training epochs. Thus, it is not capable of outputting a compact yet performant subgraph, and we therefore do not consider it for comparison. • NeuralSparse (Zheng et al., 2020): NeuralSparse is a well-recognized method for graph sparsification. Nevertheless, it cannot regulate the ultimate graph sparsity ratio. Consequently, we do not take it into consideration. • FastGCN (Chen et al., 2018): FastGCN and other graph samplers (Chen et al., 2017; Ying et al., 2018a) samples neighbors for each in-batch node with a certain probability. However, the sampling process is only for training and does not essentially output a sparse graph. G.3. Update Scheduler Υ(µ) is the update scheduler which determines the number of edges to be swapped at each update. We simply adopt the Inverse Power (Zhu & Gupta, 2017): Υ(µ) = τ(1 − µ ⌈D/∆T⌉)κ, (22) where τ denotes the initial ratio and κ is the decay factor controlling how fast the ratio decreases with intervals. G.4. Parameter Configurations The main parameters of GST include: E (number of epochs to acquire the anchor graph), D (number of epochs to dynamically fine-tune the sparse graph), τ (the initial ratio of edges to swap), κ (the decay factor of Υ(µ)), ∆T (the update interval) and the learning rate. We uniformly set τ to 0.3 and κ to 1. Ablation experiments regarding them are available in Appendix H.5. The other hyperparameters are listed in Tab. 8. Table 8.Hyper-parameter configurations. Computing Infrastructures: NVIDIA Tesla V100 (32GB) Software Framework: Pytorch Param Cora Citeseer PubMed ArXiv Proteins Products GCN GIN GAT GCN GIN GAT GCN GIN GAT SAGE ClusterGCN SAGE ClusterGCN SAGE ClusterGCN E 100 100 100 100 200 200 200 200 200 200 100 200 200 200 200 D 400 400 400 500 500 500 600 600 600 300 300 300 300 300 300 lr 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.01 0.01 0.01 0.01 0.01 0.01 ∆ T 20 20 20 20 20 20 20 20 20 3 3 3 3 3 3 G.5. Performance Metrics Accuracy represents the ratio of correctly predicted outcomes to the total predictions made. The ROC-AUC (Receiver Operating Characteristic-Area Under the Curve) value quantifies the probability that a randomly selected positive example will have a higher rank than a randomly selected negative example. Hit@50 denotes the proportion of correctly predicted edges among the top 50 candidate edges. G.6. Efficiency Metrics To assess the efficiency of sparse graphs generated by various sparsifiers, we utilize two metrics: MACs (Multiply- Accumulate Operations) and GPU Inference Latency (ms). MACs theoretically represent the model’s inference speed, based on FLOPs (Floating Point Operations Per Second). Although SpMM is theoretically faster than MatMul based on MACs/FLOPs, this advantage is not always realized in practice due to SpMM’s random memory access pattern. To gauge the real-world applicability of our method, we additionally measure latency on GPUs in milliseconds (ms). 16Boosting Graph Sparse Training via Semantic and Topological Awareness H. Additional Experimental Results H.1. Experiments for RQ1 This section details GST’s performance on Cora, Ogbn-Proteins, and Ogbn-Products datasets. Fig. 7 shows the performance of GST with GCN/GIN/GAT. Notably, GST excels on Cora+GAT, experiencing only a negligible performance loss (≈ 1.8%) at 60% graph sparsity. Tabs. 9 and 10 present GST’s performance on Ogbn-Proteins and Products. LSim, DSpar, and UGS were chosen as baselines due to the limitations of other baselines in inductive settings or scalability to large graphs. Generally, GST maintains superior performance across all sparsity levels. Specifically, it preserves GraphSAGE/ClusterGCN performance with negligible loss (≤ 1%) at 10% ∼ 30% sparsity. At 60% sparsity, GST outperforms LSim/DSpar by up to 6.95%. GCN GIN GAT Figure 7.Performance comparison of graph sparsification methods on Cora under different sparsity levels. The grey dashed line represents the original baseline performance. Table 9.Performance comparison of sparsifiers at different sparsity levels ( 10% → 60%) on GraphSAGE/ClusterGCN with Ogbn- Proteins. We report the mean accuracy ± stdev of 3 runs. We shade the best results and underline the second-best results. GraphSAGE Sparsity 10% 20% 30% 40% 50% 60% LSim 76.92±0.58 75.03±1.17 73.20±0.79 73.69±0.52 72.14±0.48 70.09±0.59 DSpar 76.04±0.25 75.86±0.27 74.46±0.39 73.45±0.26 70.22±0.54 69.23±0.40 UGS 77.47±0.27 76.38±0.63 74.58±0.49 72.12±0.38 72.38±0.33 71.45±0.75 GST 77.59±0.52 77.56±0.79 76.45±0.66 76.02±0.68 75.45±0.77 73.55±0.50 ClusterGCN (GCN aggr) Sparsity 10% 20% 30% 40% 50% 60% LSim 75.44±0.57 74.18±0.60 74.27±0.89 72.15±0.64 69.42±0.91 66.46±0.85 DSpar 76.38±0.79 76.22±1.13 75.28±0.85 74.66±1.05 72.89±0.74 71.39±0.92 GST 76.29±0.56 76.11±0.57 76.16±0.44 75.83±0.69 73.03±0.65 73.80±0.77 H.2. Experiments for RQ2 In Figs. 4 and 8 to 10, we showcase the spectral preservation performance of GST, UGS, and LSim on Citeseer/PubMed with GCN/GIN. Specifically, we illustrate the distribution of relative error for the top-200 and bottom-200 eigenvalues at20% and 50% sparsity levels produced by different methods. Our observations include: (1) GST significantly outperforms UGS/LSim in preserving the bottom-200 eigenvalues, which indicate local smoothness. For instance, on PubMed+GIN (in Fig. 10), at 50% sparsity, GST’s average relative error is about10% lower than UGS and 15% lower than LSim, demonstrating GST’s 17Boosting Graph Sparse Training via Semantic and Topological Awareness Table 10.Performance comparison of sparsifiers at different sparsity levels {10%, 20%, 30%, 40%, 50%, 60%} on Graph- SAGE/ClusterGCN with Ogbn-Products. Due to the immense scale of Ogbn-Products, we report results from a single run only. We shade the best results and underline the second-best results. GraphSAGE Sparsity 10% 20% 30% 40% 50% 60% LSim 77.96 76 .60 74 .98 72 .23 72 .67 72 .49 DSpar 78.25 77.41 75.19 74.20 74.57 74.08 GST 78.79 78.52 77.15 77.03 75.86 75.77 ClusterGCN (GCN aggr) Sparsity 10% 20% 30% 40% 50% 60% LSim 77.04 74 .34 72 .50 70 .24 69 .92 65 .10 DSpar 78.25 76.11 74.39 72.06 69.74 69.56 GST 78.38 78.45 77.83 77.19 75.71 73.05 unique advantage in maintaining the local smoothness of sparse graphs. (2) For the top-200 eigenvalues, GST performs best on larger graphs, notably PubMed, with PubMed+GCN/GIN (in Figs. 4 and 10) showing GST surpassing UGS/LSim for the top-200 eigenvalue preservation. 𝒔𝒈 = 𝟐𝟎% 𝒔𝒈 = 𝟐𝟎% 𝒔𝒈 = 𝟓𝟎% 𝒔𝒈 = 𝟓𝟎% Figure 8.The relative error of the top-200 and bottom-200 eigenvalues on Citeseer+GCN, i.e., λi−λ′ i λi , sparsified by different methods at sparsity level 20% and 50%. H.3. Experiments for RQ3 In Figs. 5 and 11, we demonstrate GST’s inference acceleration for GraphSAGE/ClusterGCN on OGB datasets. Due to UGS’s inapplicability to the inductive ClusterGCN, values for UGS+ClusterGCN are omitted. It is observed that 18Boosting Graph Sparse Training via Semantic and Topological Awareness 𝒔𝒈 = 𝟐𝟎% 𝒔𝒈 = 𝟐𝟎% 𝒔𝒈 = 𝟓𝟎% 𝒔𝒈 = 𝟓𝟎% Figure 9.The relative error of the top-200 and bottom-200 eigenvalues on Citeseer+GIN, i.e., λi−λ′ i λi , sparsified by different methods at sparsity level 20% and 50%. GST’s inference acceleration on large-scale graphs is more pronounced than on small-scale graphs (as shown in Tab. 3). With negligible performance loss (≤ 1%), GST achieves an inference speedup of 2.50 ∼ 2.85× on Ogbn-Products and 3.14 ∼ 3.42× on Ogbn-Proteins. H.4. Experiments for RQ4 As discussed in Sec. 4.5, we validate the versatility and plug-and-play nature of GST through two tasks: graph adversarial defense and graph lottery ticket identification. Fig. 12 demonstrates how GST at sparsity levels of {10%, 20%, 30%} assists GCN/GAT in combating edge perturbations. Observations reveal that GST with proper sparsity significantly enhances GNN’s resilience to edge perturbations. Across various datasets and backbones, the right level of graph sparsity notably improves GNN performance post-disturbance. For instance, on Cora+GCN, GST-10% recovers 7.23% of GCN’s performance under a30% perturbation ratio; on PubMed+GAT, GST-30% helps GAT regain5.09% performance at a 40% perturbation ratio. This suggests that graphs of different sizes require varying degrees of sparsity for effective edge perturbation resistance. Tab. 11 presents the performance when replacing UGS’s iteratively pruned sparse graphs with GST-discovered graphs of the same sparsity. The results show that in most iterations, GST-discovered graphs significantly outperform UGS-located graph lottery tickets, demonstrating GST’s broad applicability. H.5. Experiments for RQ5 In this section, we present detailed data for ablation study and parameter sensitivity analysis. Fig. 13 displays the performance of GST and its three variants on Citeseer+GCN/GIN and Ogbn-ArXiv+GraphSAGE/ClusterGCN. Tab. 13 shows the performance of GST under various settings of α and κ, while Tab. 12 demonstrates how the performance of GST varies with ∆T. Regarding ∆T, we can observe that smaller graphs (Citeseer) benefit from a lower update frequency, whereas larger graphs 19Boosting Graph Sparse Training via Semantic and Topological Awareness 𝒔𝒈 = 𝟐𝟎% 𝒔𝒈 = 𝟐𝟎% 𝒔𝒈 = 𝟓𝟎% 𝒔𝒈 = 𝟓𝟎% Figure 10.The relative error of the top-200 and bottom-200 eigenvalues on PubMed+GIN, i.e., λi−λ′ i λi , sparsified by different methods at sparsity level 20% and 50%. GraphSAGE ClusterGCN 77.880 40 20 64.5 50.8 44.2 38.4 66.261.7 43.5 32.0 60 Baseline LSim DSpar UGS GST GraphSAGE ClusterGCN 672.0 800 400 200 684.4 368.1 235.3 733.5 572.5 327.0 600 Baseline LSim DSpar GST 819.7 Ogbn-ArXiv Ogbn-Products Figure 11.The inference latency on Ogbn-ArXiv/Products with different sparsifiers when their performance loss is negligible (≤ 1%). (Ogbn-ArXiv) perform better with more frequent updates. Specifically, ∆T = 20 shows consistently good performance on Citeseer+GCN/GIN, and ∆T = 3 excels on Ogbn-Arxiv. This observation is intuitive: larger graphs, with their more complex structures and a wider variety of potential sparse graph structures, necessitate more frequent structural updates. Therefore, we standardize the setting of ∆T = 20 for small graphs and ∆T = 3 for large graphs. Regarding parameters τ and κ, it’s clear that GST’s performance is relatively insensitive to these choices, with fluctuations on GCN, GIN, and GAT not exceeding1.69%, 1.88%, and 1.25% respectively. Overall, however, a largerα and a smaller κ, which both correspond to more frequent structural updates, tend to yield better performance. 20Boosting Graph Sparse Training via Semantic and Topological Awareness Cora Citeseer PubMed Figure 12.The robust performance of GST on edge perturbations by perturbing a varying fraction of edges {0%, 5%, 10%, 15%, 20%, 30%, 40%}, tested on GCN (1st row) and GAT (2nd row). Table 11.Performance comparison of original UGS and UGS+GST on GCN backbone. Weight Sparsity 20.0% 48 .80% 67 .23% 79 .03% 86 .58% 96 .50% 98 .95% Graph Sparsity 5.0% 14 .3% 22 .6% 30 .17% 36 .98% 43 .12% 48 .67% Dataset Cora UGS 79.38 78 .08 77 .36 78 .05 76 .22 75 .35 74 .83 UGS+GST 80.25 79 .21 78 .83 77 .66 77 .24 76 .38 75 .62 ∆ 0.87 ↑ 1.13 ↑ 1.47 ↑ 0.39 ↓ 1.02 ↑ 1.03 ↑ 0.79 ↑ Dataset Citeseer UGS 70.30 69 .77 68 .14 69 .02 68 .53 67 .59 66 .84 UGS+GST 70.08 69 .54 69 .48 69 .27 68 .79 68 .16 67 .34 ∆ 0.22 ↓ 0.23 ↓ 1.34 ↑ 0.22 ↑ 0.82 ↑ 0.57 ↑ 0.5 ↑ Dataset PubMed UGS 78.51 77 .21 75 .60 75 .17 74 .85 75 .21 74 .96 UGS+GST 78.60 78 .05 77 .18 77 .03 75 .92 75 .69 75 .30 ∆ 0.09 ↑ 0.84 ↑ 1.58 ↑ 0.86 ↓ 1.07 ↑ 0.48 ↑ 0.34 ↑ Table 12.Ablation study on GST with its different ∆T. We report the extreme graph sparsity on Citeseer+GCN/GIN and Ogbn- ArXiv+GraphSAGE/ClusterGCN. Dataset Citeseer Ogbn-ArXiv Backbone GCN GIN GraphSAGE ClusterGCN Sparsity 20% 50% 20% 50% 20% 50% 20% 50% ∆T = 3 69.16 68.01 69.15 66.02 71.80 68.02 69.21 67.33 ∆T = 5 70.62 68.73 69.52 66.05 71.18 67.95 69.10 67.84 ∆T = 20 70.89 68.46 69.71 66.16 68.04 67.77 69.40 66.25 ∆T = 50 69.75 67.59 68.48 65.43 68.06 66.08 69.23 66.90 21Boosting Graph Sparse Training via Semantic and Topological Awareness Figure 13.Ablation study of GST tested on Citeseer+GCN/GIN and Ogbn-ArXiv+GraphSAGE/ClusterGCN(GCN aggr). Table 13.Parameter sensitivity analysis on initial swapping ratio α and decay factor κ. We report the performance of GST on Citeseer dataset at 20% graph sparsity. 20% GCN GIN GAT α = 0.1 α = 0.2 α = 0.3 α = 0.1 α = 0.2 α = 0.3 α = 0.1 α = 0.2 α = 0.3 κ = 1 69.32 70 .86 70 .89 67 .64 68 .98 69 .35 68 .03 68 .42 68 .55 κ = 2 69.14 70 .84 69 .85 67 .25 70 .86 70 .89 67 .66 68 .12 68 .33 κ = 3 68.26 68 .30 69 .20 67 .48 69 .11 69 .20 67 .50 67 .17 67 .95 22",
      "meta_data": {
        "arxiv_id": "2402.01242v1",
        "authors": [
          "Guibin Zhang",
          "Yanwei Yue",
          "Kun Wang",
          "Junfeng Fang",
          "Yongduo Sui",
          "Kai Wang",
          "Yuxuan Liang",
          "Dawei Cheng",
          "Shirui Pan",
          "Tianlong Chen"
        ],
        "published_date": "2024-02-02T09:10:35Z",
        "pdf_url": "https://arxiv.org/pdf/2402.01242v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the computational challenges of Graph Neural Networks (GNNs) on large-scale graphs by proposing Graph Sparse Training (GST). GST is a novel framework for dynamic graph-level sparse training that for the first time combines semantic and topological awareness to create sparse graphs without performance degradation. It introduces the Equilibria Sparsification Principle to guide this process, balancing both types of information. GST significantly outperforms state-of-the-art sparsifiers, identifying subgraphs at higher sparsity levels (1.67% to 15.85% improvement), preserving more key spectral properties (~15% more), achieving substantial GNN inference speedup (1.27 to 3.42x), and proving versatile in aiding graph adversarial defense (0.35% to 7.23% improvement) and graph lottery tickets (0.22% to 1.58% improvement).",
        "methodology": "GST operates in two main phases: First, it constructs an 'anchor graph' by performing limited full graph training using a graph masker (2-layer MLP) to capture optimal topological and semantic information. This involves co-optimizing GNN parameters and the graph masker using a cross-entropy loss. Second, it initiates dynamic sparse graph training from a one-shot pruned version of the anchor graph. During this phase, it iteratively refines the sparse graph structure by pruning and regrowing an equal number of edges, guided by the Equilibria Sparsification Principle. This principle combines a topological criterion (eigenvalue variation, approximated for computational feasibility by considering only top-K and bottom-K eigenvalues) and a semantic criterion (gradient magnitude of KL divergence between current and anchor model outputs). Edges with the lowest combined score are pruned, and those with the highest scores from the complement set are regrown.",
        "experimental_setup": "Extensive experiments were conducted on 6 datasets: small-to-medium scale (Cora, Citeseer, PubMed) and large-scale (Ogbn-ArXiv, Ogbn-Proteins, Ogbn-Products). Five GNN backbones were used: GCN, GIN, GAT (transductive settings), and GraphSAGE, ClusterGCN (inductive settings). Baselines included topology-based sparsifiers (SCAN, Local Similarity (LSim), DSpar) and semantic-based sparsifiers (UGS, Meta-gradient sparsifier (MGSpar), WD-GLT). Performance was evaluated using node classification accuracy/ROC-AUC, spectral preservation (relative error of eigenvalues), GNN inference speed (MACs and GPU inference latency), robustness against edge perturbations (randomly relocating edge endpoints), and applicability to graph lottery ticket identification. Ablation studies and parameter sensitivity analyses (for epochs, update interval, initial swapping ratio, and decay factor) were also performed.",
        "limitations": "The paper does not explicitly list limitations in a dedicated section. However, implicit limitations can be inferred: the exact calculation of eigenvalue variations is computationally impractical, necessitating an approximation (only top-K and bottom-K eigenvalues are considered). The method also involves several hyperparameters that require tuning for optimal performance across different datasets and backbones, which could add complexity to its application.",
        "future_research_directions": "Future research directions include extending GST to more complex graph scenarios, such as heterophilic and heterogeneous graphs. Additionally, the authors plan to further explore the feasibility of 'in-time' sparsification and acceleration of GNNs in real-world applications, specifically mentioning recommender systems and fraud detection."
      }
    },
    {
      "title": "Joint Edge-Model Sparse Learning is Provably Efficient for Graph Neural Networks",
      "abstract": "Due to the significant computational challenge of training large-scale graph\nneural networks (GNNs), various sparse learning techniques have been exploited\nto reduce memory and storage costs. Examples include \\textit{graph\nsparsification} that samples a subgraph to reduce the amount of data\naggregation and \\textit{model sparsification} that prunes the neural network to\nreduce the number of trainable weights. Despite the empirical successes in\nreducing the training cost while maintaining the test accuracy, the theoretical\ngeneralization analysis of sparse learning for GNNs remains elusive. To the\nbest of our knowledge, this paper provides the first theoretical\ncharacterization of joint edge-model sparse learning from the perspective of\nsample complexity and convergence rate in achieving zero generalization error.\nIt proves analytically that both sampling important nodes and pruning neurons\nwith the lowest-magnitude can reduce the sample complexity and improve\nconvergence without compromising the test accuracy. Although the analysis is\ncentered on two-layer GNNs with structural constraints on data, the insights\nare applicable to more general setups and justified by both synthetic and\npractical citation datasets.",
      "full_text": "JOINT EDGE -MODEL SPARSE LEARNING IS PROVABLY EFFICIENT FOR GRAPH NEURAL NETWORKS Shuai Zhang Rensselaer Polytechnic Institute zhangs29@rpi.edu Meng Wang Rensselaer Polytechnic Institute wangm7@rpi.edu Pin-Yu Chen IBM Research Pin-Yu.Chen@ibm.com Sijia Liu Michigan State University liusiji5@msu.edu Songtao Lu IBM Research songtao@ibm.com Miao Liu IBM Research miao.liu1@ibm.com ABSTRACT Due to the signiﬁcant computational challenge of training large-scale graph neural networks (GNNs), various sparse learning techniques have been exploited to reduce memory and storage costs. Examples include graph sparsiﬁcation that samples a subgraph to reduce the amount of data aggregation and model sparsiﬁcation that prunes the neural network to reduce the number of trainable weights. De- spite the empirical successes in reducing the training cost while maintaining the test accuracy, the theoretical generalization analysis of sparse learning for GNNs remains elusive. To the best of our knowledge, this paper provides the ﬁrst theoreti- cal characterization of joint edge-model sparse learning from the perspective of sample complexity and convergence rate in achieving zero generalization error. It proves analytically that both sampling important nodes and pruning neurons with lowest-magnitude can reduce the sample complexity and improve convergence without compromising the test accuracy. Although the analysis is centered on two-layer GNNs with structural constraints on data, the insights are applicable to more general setups and justiﬁed by both synthetic and practical citation datasets. 1 I NTRODUCTION Graph neural networks (GNNs) can represent graph structured data effectively and ﬁnd applications in objective detection (Shi & Rajkumar, 2020; Yan et al., 2018), recommendation system (Ying et al., 2018; Zheng et al., 2021), rational learning (Schlichtkrull et al., 2018), and machine translation (Wu et al., 2020; 2016). However, training GNNs directly on large-scale graphs such as scientiﬁc citation networks (Hull & King, 1987; Hamilton et al., 2017; Xu et al., 2018), social networks (Kipf & Welling, 2017; Sandryhaila & Moura, 2014; Jackson, 2010), and symbolic networks (Riegel et al., 2020) becomes computationally challenging or even infeasible, resulting from both the exponential aggregation of neighboring features and the excessive model complexity, e.g., training a two-layer GNN on Reddit data (Tailor et al., 2020) containing 232,965 nodes with an average degree of 492 can be twice as costly as ResNet-50 on ImageNet (Canziani et al., 2016) in computation resources. The approaches to accelerate GNN training can be categorized into two paradigms: (i) sparsifying the graph topology (Hamilton et al., 2017; Chen et al., 2018; Perozzi et al., 2014; Zou et al., 2019), and (ii) sparsifying the network model (Chen et al., 2021b; You et al., 2022). Sparsifying the graph topology means selecting a subgraph instead of the original graph to reduce the computation of neighborhood aggregation. One could either use a ﬁxed subgraph (e.g., the graph typology (Hübler et al., 2008), graph shift operator (Adhikari et al., 2017; Chakeri et al., 2016), or the degree distribution (Leskovec & Faloutsos, 2006; V oudigari et al., 2016; Eden et al., 2018) is preserved) or apply sampling algorithms, such as edge sparsiﬁcation (Hamilton et al., 2017), or node sparsiﬁcation (Chen et al., 2018; Zou et al., 2019) to select a different subgraph in each iteration. Sparsifying the network model means reducing the complexity of the neural network model, including removing the non-linear activation (Wu et al., 2019; He et al., 2020), quantizing neuron weights (Tailor et al., 2020; Bahri et al., 2021) and output of the intermediate layer (Liu et al., 2021), pruning network (Frankle & Carbin, 2019), or 1 arXiv:2302.02922v1  [cs.LG]  6 Feb 2023knowledge distillation (Yang et al., 2020; Hinton et al., 2015; Yao et al., 2020; Jaiswal et al., 2021). Both sparsiﬁcation frameworks can be combined, such as joint edge sampling and network model pruning in (Chen et al., 2021b; You et al., 2022). Despite many empirical successes in accelerating GNN training without sacriﬁcing test accuracy, the theoretical evaluation of training GNNs with sparsiﬁcation techniques remains largely unexplored. Most theoretical analyses are centered on the expressive power of sampled graphs (Hamilton et al., 2017; Cong et al., 2021; Chen et al., 2018; Zou et al., 2019; Rong et al., 2019) or pruned networks (Malach et al., 2020; Zhang et al., 2021; da Cunha et al., 2022). However, there is limited gen- eralization analysis, i.e., whether the learned model performs well on testing data. Most existing generalization analyses are limited to two-layer cases, even for the simplest form of feed-forward neural networks (NNs), see, e.g., (Zhang et al., 2020a; Oymak & Soltanolkotabi, 2020; Huang et al., 2021; Shi et al., 2022) as examples. To the best of our knowledge, only Li et al. (2022); Allen-Zhu et al. (2019a) go beyond two layers by considering three-layer GNNs and NNs, respectively. However, Li et al. (2022) requires a strong assumption, which cannot be justiﬁed empirically or theoretically, that the sampled graph indeed presents the mapping from data to labels. Moreover, Li et al. (2022); Allen-Zhu et al. (2019a) focus on a linearized model around the initialization, and the learned weights only stay near the initialization (Allen-Zhu & Li, 2022). The linearized model cannot justify the advantages of using multi-layer (G)NNs and network pruning. As far as we know, there is no ﬁnite-sample generalization analysis for the joint sparsiﬁcation, even for two-layer GNNs. Contributions. This paper provides the ﬁrst theoretical generalization analysis of joint topology- model sparsiﬁcation in training GNNs, including (1) explicit bounds of the required number of known labels, referred to as the sample complexity, and the convergence rate of stochastic gradient descent (SGD) to return a model that predicts the unknown labels accurately; (2) quantitative proof for that joint topology and model sparsiﬁcation is a win-win strategy in improving the learning performance from the sample complexity and convergence rate perspectives. We consider the following problem setup to establish our theoretical analysis: node classiﬁcation on a one-hidden-layer GNN, assuming that some node features are class-relevant (Shi et al., 2022), which determines the labels, while some node features are class-irrelevant, which contains only irrelevant information for labeling, and the labels of nodes are affected by the class-relevant features of their neighbors. The data model with this structural constraint characterizes the phenomenon that some nodes are more inﬂuential than other nodes, such as in social networks (Chen et al., 2018; Veliˇckovi´c et al., 2018), or the case where the graph contains redundancy information (Zheng et al., 2020). Speciﬁcally, the sample complexity is quadratic in (1 −β)/α, where αin (0,1] is the probability of sampling nodes of class-relevant features, and a larger αmeans class-relevant features are sampled more frequently. β in [0,1) is the fraction of pruned neurons in the network model using the magnitude-based pruning method such as (Frankle & Carbin, 2019). The number of SGD iterations to reach a desirable model is linear in (1 −β)/α. Therefore, our results formally prove that graph sampling reduces both the sample complexity and number of iterations more signiﬁcantly provided that nodes with class-relevant features are sampled more frequently. The intuition is that importance sampling helps the algorithm learns the class-relevant features more efﬁciently and thus reduces the sample requirement and convergence time. The same learning improvement is also observed when the pruning rate increases as long as βdoes not exceed a threshold close to 1. 2 G RAPH NEURAL NETWORKS : F ORMULATION AND ALGORITHM 2.1 P ROBLEM FORMULATION 𝑦𝑦𝑣𝑣 𝑾𝑾 Aggregation 𝒙𝒙𝑣𝑣 𝑿𝑿𝒩𝒩(𝑣𝑣) ReLU Positive label Negative label Unknown label 𝒃𝒃 Σ Output  layer Figure 1: Illustration of node classiﬁcation in the GNN Given an undirected graph G(V,E), where Vis the set of nodes, Eis the set of edges. Let R denote the maximum node degree. For any node v ∈V , let xv ∈Rd and yv ∈{+1,−1}denote its input feature and corresponding label1, respectively. Given all node features {xv}v∈Vand partially known labels {yv}v∈Dfor nodes in D⊂V , the semi-supervised node classiﬁcation problem aims to predict all unknown labels yv for v∈V/D. 1The analysis can be extended to multi-class classiﬁcation, see Appendix I. 2This paper considers a graph neural network with non-linear aggregator functions, as shown in Figure 1. The weights of the Kneurons in the hidden layer are denoted as {wk ∈Rd}K k=1, and the weights in the linear layer are denoted as {bk ∈R}K k=1. Let W ∈Rd×K and b ∈RK be the concatenation of {wk}K k=1 and {bk}K k=1, respectively. For any node v, let N(v) denote the set of its (1-hop) neighbors (with self-connection), and XN(v) ∈Rd×|N(v)|contains features in N(v). Therefore, the output of the GNN for node vcan be written as: g(W,b; XN(v)) = 1 K K∑ k=1 bk ·AGG(XN(v),wk), (1) where AGG(XN(v),w) denotes a general aggregator using features XN(v) and weight w, e.g., weighted sum of neighbor nodes (Veliˇckovi´c et al., 2018), max-pooling (Hamilton et al., 2017), or min-pooling (Corso et al., 2020) with some non-linear activation function. We consider ReLU as φ(·) = max{·,0}. Given the GNN, the label yv at node vis predicted by sign(g(W,b; XN(v))). We only updateW due to the homogeneity of ReLU function, which is a common practice to simplify the analysis (Allen-Zhu et al., 2019a; Arora et al., 2019; Oymak & Soltanolkotabi, 2020; Huang et al., 2021). The training problem minimizes the following empirical risk function (ERF): min W : ˆfD(W,b(0)) := − 1 |D| ∑ v∈D yv ·g ( W,b(0); XN(v) ) . (2) The test error is evaluated by the following generalization error function: I(g(W,b)) = 1 |V| ∑ v∈V max { 1 −yv ·g ( W,b; XN(v) ) ,0 } . (3) If I(g(W,b)) = 0, yv = sign(g(W,b; XN(v))) for all v, indicating zero test error. Albeit different from practical GNNs, the model considered in this paper can be viewed as a one- hidden-layer GNN, which is the state-of-the-art practice in generalization and convergence analyses with structural data (Brutzkus & Globerson, 2021; Damian et al., 2022; Shi et al., 2022; Allen-Zhu & Li, 2022). Moreover, the optimization problem of (2) is already highly non-convex due to the non-linearity of ReLU functions. For example, as indicated in (Liang et al., 2018; Safran & Shamir, 2018), one-hidden-layer (G)NNs contains intractably many spurious local minima. In addition, the VC dimension of the GNN model and data distribution considered in this paper is proved to be at least an exponential function of the data dimension (see Appendix G for the proof). This model is highly expressive, and it is extremely nontrivial to obtain a polynomial sample complexity, which is one contribution of this paper. 2.2 GNN L EARNING ALGORITHM VIA JOINT EDGE AND MODEL SPARSIFICATION The GNN learning problem (2) is solved via a mini-batch SGD algorithm, as summarized in Algorithm 1. The coefﬁcients bk’s are randomly selected from+1 or −1 and remain unchanged during training. The weights wk in the hidden layer are initialized from a multi-variate Gaussian N(0,δ2Id) with a small constant δ, e.g. δ= 0.1. The training data are divided into disjoint subsets, and one subset is used in each iteration to update W through SGD. Algorithm 1 contains two training stages: pre-training on W (lines 1-4) with few iterations, and re-training on the pruned model M ⊙W (lines 5-8), where ⊙stands for entry-wise multiplication. Here neuron-wise magnitude pruning is used to obtain a weight mask M and graph topology sparsiﬁcation is achieved by node sampling (line 8). During each iteration, only part of the neighbor nodes is fed into the aggregator function in (1) at each iteration, where N(s)(t) denotes the sampled subset of neighbors of node vat iteration t. Edge sparsiﬁcation samples a subset of neighbors, rather than the entire N(v), for every node v in computing (1) to reduce the per-iteration computational complexity. This paper follows the GraphSAGE framework (Hamilton et al., 2017), where r(r≪R) neighbors are sampled (all these neighbors are sampled if |N(v)|≤ r) for each node at each iteration. At iteration t, the gradient is ∇ˆf(t) D (W,b(0)) = − 1 |D| ∑ v∈D yv ·∇W g ( W,b(0); XN(t) s (v) ) . (4) 3Algorithm 1 Training GNN via Joint Edge and Model Sparsiﬁcation Input: Node features X, known node labels{yv}v∈Dwith D⊆V , step size cη = 10δwith constant δ, the number of sampled edges r, the pruning rate β, the pre-training iterations T′= ∥X∥∞/cη, the number of iterations T. Initialization: W(0),b(0) as w(0) k ∼N(0,δ2Id) and b(0) k ∼Uniform({−1,+1}) for k∈[K]; Pre-training: update model weights W through mini-batch SGD with edge sampling 1: Divide Dinto disjoint subsets {D(t′)}T′ t′=1 2: for t′= 0,1,2,··· ,T′−1 do 3: Sample N(t′) s (v) for every node vin D(t′); 4: W(t′+1) = W(t′) −cη ·∇W ˆf(t) D (W(t′),b(0)); 5: end for Pruning: set βfraction of neurons in W(T′) with the lowest magnitude weights to 0, and obtain the corre- sponding binary mask M; Re-training: rewind the weights to the original initialization asM ⊙W(0), and update model weights through SGD with edge sampling; 6: Divide Dinto disjoint subsets {D(t)}T t=1 7: for t= 0,1,2,··· ,T do 8: Sample N(t) s (v) for every node vin D(t); 9: W(t+1) = W(t) −cη ·∇W ˆf(t) D(t) (M ⊙W(t),b(0)); 10: end for Return: W(T) and b(0). where D⊆V is the subset of training nodes with labels. The aggregator function used in this paper is the max-pooling function, i.e., AGG(XN(t) s (v),w) = max n∈N(t) s (v) φ(⟨w , xn ⟩), (5) which has been widely used in GraphSAGE (Hamilton et al., 2017) and its variants (Guo et al., 2021; Oh et al., 2019; Zhang et al., 2022b; Lo et al., 2022). This paper considers an importance sampling strategy with the idea that some nodes are sampled with a higher probability than other nodes, like the sampling strategy in (Chen et al., 2018; Zou et al., 2019; Chen et al., 2021b). Model sparsiﬁcation ﬁrst pre-trains the neural network (often by only a few iterations) and then prunes the network by setting some neuron weights to zero. It then re-trains the pruned model with fewer parameters and is less computationally expensive to train. Existing pruning methods include neuron pruning and weight pruning. The former sets all entries of a neuron wk to zeros simultaneously, while the latter sets entries of wk to zeros independently. This paper considers neuron pruning. Similar to (Chen et al., 2021b), we ﬁrst train the original GNN until the algorithm converges. Then, magnitude pruning is applied to neurons via removing a β(β ∈[0,1)) ratio of neurons with the smallest norm. Let M ∈{0,1}d×K be the binary mask matrix with all zeros in column kif neuron kis removed. Then, we rewind the remaining GNN to the original initialization (i.e., M ⊙W(0)) and re-train on the model M ⊙W. 3 T HEORETICAL ANALYSIS 3.1 T AKEAWAYS OF THE THEORETICAL FINDINGS Before formally presenting our data model and theoretical results, we ﬁrst brieﬂy introduce the key takeaways of our results. We consider the general setup that the node features as a union of the noisy realizations of some class-relevant features and class-irrelevant ones, and δdenotes the upper bound of the additive noise. The label yv of node vis determined by class-relevant features in N(v). We assume for simplicity that N(v) contains the class-relevant features for exactly one class. Some major parameters are summarized in Table 1. The highlights include: (T1) Sample complexity and convergence analysis for zero generalization error. We prove that the learned model (with or without any sparsiﬁcation) can achieve zero generalization error with high probability over the randomness in the initialization and the SGD steps. The sample complexity is linear in σ2 and K−1. The number of iterations is linear in σ and K−1/2. Thus, the learning 4Table 1: Some Important Notations K Number of neurons in the hidden layer; σ Upper bound of additive noise in input features; r Number of sampled edges for each node; R The maximum degree of original graph G; L the number of class-relevant and class-irrelevant patterns; α the probability of containing class-relevant nodes in the sampled neighbors of one node; β Pruning rate of model weights; β ∈[0,1 −1/L); β = 0means no pruning; performance is enhanced in terms of smaller sample complexity and faster convergence if the neural network is slightly over-parameterized. (T2) Edge sparsiﬁcation and importance sampling improve the learning performance. The sample complexity is a quadratic function of r, indicating that edge sparsiﬁcation reduces the sample complexity. The intuition is that edge sparsiﬁcation reduces the level of aggregation of class-relevant with class-irrelevant features, making it easier to learn class-relevant patterns for the considered data model, which improves the learning performance. The sample complexity and the number of iterations are quadratic and linear inα−1, respectively. As a largerαmeans the class-relevant features are sampled with a higher probability, this result is consistent with the intuition that a successful importance sampling strategy helps to learn the class-relevant features faster with fewer samples. (T3) Magnitude-based model pruning improves the learning performance. Both the sample complexity and the computational time are linear in (1 −β)2, indicating that if more neurons with small magnitude are pruned, the sample complexity and the computational time are reduced. The intuition is that neurons that accurately learn class-relevant features tend to have a larger magnitude than other neurons’, and removing other neurons makes learning more efﬁcient. (T4) Edge and model sparsiﬁcation is a win-win strategy in GNN learning.Our theorem provides a theoretical validation for the success of joint edge-model sparsiﬁcation. The sample complexity and the number of iterations are quadratic and linear in 1−β α , respectively, indicating that both techniques can be applied together to effectively enhance learning performance. 3.2 F ORMAL THEORETICAL RESULTS Data model. Let P= {pi}L i=1 (∀L≤d) denote an arbitrary set of orthogonal vectors2 in Rd. Let p+ := p1 and p−:= p2 be the positive-class and negative-class pattern, respectively, which bear the causal relation with the labels. The remaining vectors in Pare class irrelevant patterns. The node features xv of every nodevis a noisy version of one of these patterns, i.e.,xv = pv+zv,where pv ∈ P, and zv is an arbitrary noise at node vwith ∥z∥2 ≤σfor some σ. yv is +1 (or −1) if node vor any of its neighbors contains p+ (or p−). Speciﬁcally, divide Vinto four disjoint sets V+, V−, VN+ and VN−based on whether the node feature is relevant or not (N in the subscript) and the label, i.e., pv = p+, ∀v∈V+; pv = p−, ∀v∈V−; and pv ∈{p3,..., pL}, ∀v∈ VN+ ∪VN−. Then, we have yv = 1,∀v∈V+ ∪VN+, and yv = −1,∀v∈V−∪VN−. 1 42 3 𝒱𝒱+ 𝒱𝒱𝑁𝑁+ 𝒱𝒱−𝒱𝒱𝑁𝑁− Figure 2: Toy example of the data model. Node 1 and 2 have label +1. Nodes 3 and 4 are labeled as −1. Nodes 1 and 4 have class- relevant features. Nodes 2 and 3 have class- irrelevant features. V+ = {1}, VN+ = {2}, VN−= {3}, V−= {4} We assume (A1) Every v in VN+ (or VN−) is connected to at least one node in V+ (or V−). There is no edge between V+ and VN−. There is no edge between V−and VN+. (A2) The positive and negative labels in Dare balanced, i.e., ⏐⏐|D∩(V+ ∪VN+)|−|D∩(V−∪VN−)| ⏐⏐= O( √ |D|). (A1) indicates that connected nodes in the graph tend to have the same labels and eliminates the case that node v is connected to both p+ and p−to simplify the analysis. A numerical justiﬁcation of such an assumption in Cora dataset can be found in Appendix F.2. (A2) can be relaxed to the case that the observed labels are unbalanced. One only needs to up-weight the minority class in the ERF in (2) accordingly, which is a common trick in imbalance GNN learning (Chen et al., 2021a), and our analysis holds with minor 2The orthogonality constraint simpliﬁes the analysis and has been employed in (Brutzkus & Globerson, 2021). We relaxed this constraint in the experiments on the synthetic data in Section 4. 5modiﬁcation. The data model of orthogonal patterns is introduced in (Brutzkus & Globerson, 2021) to analyze the advantage of CNNs over fully connected neural networks. It simpliﬁes the analysis by eliminating the interaction of class-relevant and class-irrelevant patterns. Here we generalize to the case that the node features contain additional noise and are no longer orthogonal. To analyze the impact of importance sampling quantitatively, let α denote a lower bound of the probability that the sampled neighbors of vcontain at least one node in V+ or V−for any node v(see Table 1). Clearly, α= r/Ris a lower bound for uniform sampling 3. A larger αindicates that the sampling strategy indeed selects nodes with class-relevant features more frequently. Theorem 1 concludes the sample complexity ( C1) and convergence rate ( C2) of Algorithm 1 in learning graph-structured data via graph sparsiﬁcation. Speciﬁcally, the returned model achieves zero generalization error (from (8)) with enough samples (C1) after enough number of iterations (C2). Theorem 1. Let the step size cη be some positive constant and the pruning rate β ∈[0,1 −1/L). Given the bounded noise such that σ <1/Land sufﬁcient large model such that K >L2 ·log qfor some constant q >0. Then, with probability at least 1 −q−10, when (C1) the number of labeled nodes satisﬁes |D|= Ω ( (1 + L2σ2 + K−1) ·α−2 ·(1 + r2) ·(1 −β)2 ·L2 ·log q ) , (6) (C2) the number of iterations in the re-training stage satisﬁes T = Ω ( c−1 η (1 + |D|−1/2) ·(1 + Lσ+ K−1/2) ·(1 −β) ·α−1 ·L ) , (7) the model returned by Algorithm 1 achieves zero generalization error, i.e., I ( g(W(T),U(T)) ) = 0. (8) 3.3 T ECHNICAL CONTRIBUTIONS Although our data model is inspired by the feature learning framework of analyzing CNNs (Brutzkus & Globerson, 2021), the technical framework in analyzing the learning dynamics differs from existing ones in the following aspects. First, our work provides the ﬁrst polynomial-order sample complexity bound in (6) that quantitatively characterizes the parameters’ dependence with zero generalization error. In (Brutzkus & Globerson, 2021), the generalization bound is obtained by updating the weights in the second layer (linear layer) while the weights in the non-linear layer are ﬁxed. However, the high expressivity of neural networks mainly comes from the weights in non-linear layers. Updating weights in the hidden layer can achieve a smaller generalization error than updating the output layer. Therefore, this paper obtains a polynomial-order sample complexity bound by characterizing the weights update in the non-linear layer (hidden layer), which cannot be derived from (Brutzkus & Globerson, 2021). In addition, updating weights in the hidden layer is a non-convex problem, which is more challenging than the case of updating weights in the output layer as a convex problem. Second, the theoretical framework in this paper can characterize the magnitude-based pruning method while the approach in (Brutzkus & Globerson, 2021) cannot. Speciﬁcally, our analysis provides a tighter bound such that the lower bound of “lucky neurons” can be much larger than the upper bound of “unlucky neurons” (see Lemmas 2-5), which is the theoretical foundation in characterizing the beneﬁts of model pruning but not available in (Brutzkus & Globerson, 2021) (see Lemmas 5.3 & 5.5). On the one hand, (Brutzkus & Globerson, 2021) only provides a uniform bound for “unlucky neurons” in all directions, but Lemma 4 in this paper provides speciﬁc bounds in different directions. On the other hand, this paper considers the inﬂuence of the sample amount, and we need to characterize the gradient offsets between positive and negative classes. The problem is challenging due to the existence of class-irrelevant patterns and edge sampling in breaking the dependence between the labels and pattern distributions, which leads to unexpected distribution shifts. We characterize groups of special data as the reference such that they maintain a ﬁxed dependence on labels and have a controllable distribution shift to the sampled data. Third, the theoretical framework in this paper can characterize the edge sampling while the approach in (Brutzkus & Globerson, 2021) cannot. (Brutzkus & Globerson, 2021) requires the data samples 3The lower bounds of αfor some sampling strategy are provided in Appendix G.1. 6containing class-relevant patterns in training samples via margin generalization bound (Shalev- Shwartz & Ben-David, 2014; Bartlett & Mendelson, 2002). However, with data sampling, the sampled data may no longer contain class-relevant patterns. Therefore, updating on the second layer is not robust, but our theoretical results show that updating in the hidden layer is robust to outliers caused by egde sampling. 3.4 T HE PROOF SKETCH Before presenting the formal roadmap of the proof, we provide a high-level illustration by borrowing the concept of “lucky neuron”, where such a node has good initial weights, from (Brutzkus & Globerson, 2021). We emphasize that only the concept is borrowed, and all the properties of the “lucky neuron”, e.g., (10) to (13), are developed independently with excluded theoretical ﬁndings from other papers. In this paper, we justify that the magnitude of the \"lucky neurons\" grows at a rate of sampling ratio of class-relevant features, while the magnitude of the \"unlucky neurons\" is upper bounded by the inverse of the size of training data (see proposition 2). With large enough training data, the \"lucky neurons\" have large magnitudes and dominate the output value. By pruning neurons with small magnitudes, we can reserve the \"lucky neurons\" and potentially remove \"unlucky neurons\" (see proposition 3). In addition, we prove that the primary direction of \"lucky neurons\" is consistence with the class-relevant patterns, and the ratio of \"lucky neurons\" is sufﬁciently large (see proposition 1). Therefore, the output is determined by the primary direction of the “lucky neuron”, which is the corresponding class-relevant pattern Speciﬁcally, we will prove that, for every nodevwith yv = 1, the prediction by the learned weights W(T) is accurate, i.e., g(M ⊙W(T),b(0); XN(v)) >1. The arguments for nodes with negative labels are the same. Then, the zero test error is achieved from the deﬁned generalization error in (3). Divide the neurons into two subsets B+ = {k |b(0) k = +1}and B−= {k |b(0) k = −1}. We ﬁrst show that there exist some neurons iin B+ with weights w(t) i that are close to p+ for all iterations t≥0. These neurons, referred to as “lucky neurons,” play a dominating role in classifying v, and the fraction of these neurons is at least close to 1/L. Formally, Proposition 1. Let K+ ⊆B+ denote the set of “lucky neurons” that for any iin K+, min ∥z∥2≤σ ⟨w(t) i , p+ + z ⟩≥ max p∈P/p+,∥z∥2≤σ ⟨w(t) i , p + z ⟩ for all t. (9) Then it holds that |K+|/K ≥(1 −K−1/2 −Lσ)/L. (10) We next show in Proposition 2 that when|D|is large enough, the projection of the weight w(t) i of a lucky neuron ion p+ grows at a rate of cηα. Then importance sampling with a large αcorresponds to a high rate. In contrast, the neurons in B−increase much slower in all directions except for p−. Proposition 2. ⟨w(t) i , p+ ⟩≥ cη ( α−σ √ (1 + r2)/|D| ) t, ∀i∈K+,∀t |⟨w(t) j , p ⟩|≤ cη(1 + σ) √ (1 + r2)/|D|·t, ∀j ∈B−,∀p ∈P/p−,∀t. (11) Proposition 3 shows that the weights magnitude of a “lucky neuron” inK+ is larger than that of a neuron in B+/K+. Combined with (10), “lucky neurons” will not be pruned by magnitude pruning, as long as β <1 −1/L. Let Kβ denote the set of neurons after pruning with |Kβ|= (1 −β)K. Proposition 3. There exists a small positive integer Csuch that ∥w(t) i ∥2 >∥w(t) j ∥2, ∀i∈K+,∀j ∈B+/K+,∀t≥C. (12) Moreover,Kβ ∩K+ = K+ for all β ≤1 −1/L. Therefore, with a sufﬁciently large number of samples, the magnitudes of lucky neurons increase much faster than those of other neurons (from proposition 2). Given a sufﬁciently large fraction of lucky neurons (from proposition 1), the outputs of the learned model will be strictly positive. Moreover, with a proper pruning rate, the fraction of lucky neurons can be further improved (from proposition 3), which leads to a reduced sample complexity and faster convergence rate. In the end, we consider the case of no feature noise to illustrate the main computation 7g(M ⊙W(T),M ⊙b(0); XN(v)) = 1 K (∑ i∈B+∩Kβ maxu∈N(v) φ(⟨w(T) i , xu ⟩) −∑ j∈B−∩Kβ maxu∈N(v) φ(⟨w(T) j , xu ⟩) ) ≥ 1 K ∑ i∈K+ max u∈N(v) φ(⟨w(T) i , xu ⟩) − 1 K ∑ j∈B−∩Kβ max p∈P/p− |⟨w(T) j , p ⟩| ≥ [ α|K+|/K−(1 −β)(1 +σ) √ (1 +r2)/|D| ] cηT > 1, (13) where the ﬁrst inequality follows from the fact that Kβ ∩K+ = K+, φis the nonnegative ReLU function, and N(v) does not contain p−for a node vwith yv = +1. The second inequality follows from Proposition 2. The last inequality follows from (10), and conclusions (C1) & (C2). That completes the proof. Please see the supplementary material for details. 4 N UMERICAL EXPERIMENTS 4.1 S YNTHETIC DATA EXPERIMENTS We generate a graph with 10000 nodes, and the node degree is 30. The one-hot vectors e1 and e2 are selected as p+ and p−, respectively. The class-irrelevant patterns are randomly selected from the null space of p+ and p−. That is relaxed from the orthogonality constraint in the data model. ∥p∥2 is normalized to 1 for all patterns. The noise zv belongs to Gaussian N(0,σ2). The node features and labels satisfy (A1) and (A2), and details of the construction can be found in Appendix F. The test error is the percentage of incorrect predictions of unknown labels. The learning process is considered as a success if the returned model achieves zero test error. Sample Complexity. We ﬁrst verify our sample complexity bound in (6). Every result is averaged over 100 independent trials. A white block indicates that all the trials are successful, while a black block means all failures. In these experiments, we vary one parameter and ﬁx all others. In Figure 3, r= 15 and we vary the importance sampling probability α. The sample complexity is linear in α−2. Figure 4 indicates that the sample complexity is almost linear in (1 −β)2 up to a certain upper bound, where βis the pruning rate. All these are consistent with our theoretical predictions in (6). 4 3.1 2.2 1 70 130 190 250 Figure 3: |D|against the impor- tance sampling probability α 1.0 0.8 0.6 0.4 0.2 15 45 75 105 135 165 Figure 4: |D|against the pruning rate β 1 1.2 1.4-1 40 60 80Number of iterations  = 0  = 0.05  = 0.10 Figure 5: The number of iterations against α Training Convergence Rate. Next, we evaluate how sampling and pruning reduce the required number of iterations to reach zero generalization error. Figure 5 shows the required number of iterations for different αunder different noise level σ. Each point is averaged over 1000 independent realizations, and the regions in low transparency denote the error bars with one standard derivation. We can see that the number of iterations is linear in 1/α, which veriﬁes our theoretical ﬁndings in (6). Thus, importance sampling reduces the number of iterations for convergence. Figure 6 illustrates the required number of iterations for convergence with various pruning rates. The baseline is the average iterations of training the dense networks. The required number of iterations by magnitude pruning is almost linear in β, which veriﬁes our theoretical ﬁndings in (7). In comparison, random pruning degrades the performance by requiring more iterations than the baseline to converge. Magnitude pruning removes neurons with irrelevant information. Figure 7 shows the distribu- tion of neuron weights after the algorithm converges. There are 104 points by collecting the neurons in 100 independent trials. The y-axis is the norm of the neuron weights wk, and the y-axis stands for the angle of the neuron weights between p+ (bottom) or p−(top). The blue points in cross represent wk’s withbk = 1, and the red ones in circle represent wk’s withbk = −1. In both cases, wk with a small norm indeed has a large angle with p+ (or p−) and thus, contains class-irrelevant information for classifying class +1 (or −1). Figure 7 veriﬁes Proposition 3 showing that magnitude pruning removes neurons with class-irrelavant information. 8Performance enhancement with joint edge-model sparsiﬁcation. Figure 8 illustrates the learning success rate when the importance sampling probability αand the pruning ratio βchange. For each pair of αand β, the result is averaged over 100 independent trials. We can observe when either αor βincreases, it becomes more likely to learn a desirable model. 0 0.2 0.4 0.6 0.80 50 100 150Number of iterations Magnitude pruning Random pruning Baseline Figure 6: Number of iterations against the pruning rate β 0 /20 50 100 0/2 Figure 7: Distribution of the neu- ron weights 0 0.15 0.30 0.45 0.67 0.77 0.87 0.97 Figure 8: Learning success rate of join edge-model sparsiﬁcation 4.2 J OINT -SPARSIFICATION ON REAL CITATION DATASETS We evaluate the joint edge-model sparsiﬁcation algorithms in real citation datasets (Cora, Citeseer, and Pubmed) (Sen et al., 2008) on the standard GCN (a two-message passing GNN) (Kipf & Welling, 2017). The Uniﬁed GNN Sparsiﬁcation (UGS) in (Chen et al., 2021b) is implemented here as the edge sampling method, and the model pruning approach is magnitude-based pruning. Figure 9 shows the performance of node classiﬁcation on Cora dataset. As we can see, the joint sparsiﬁcation helps reduce the sample complexity required to meet the same test error of the original model. For example, P2, with the joint rates of sampled edges and pruned neurons as (0.90,0.49), and P3, with the joint rates of sampled edges and pruned neurons as (0.81,0.60), return models that have better testing performance than the original model (P1) trained on a larger data set. By varying the training sample size, we ﬁnd the characteristic behavior of our proposed theory: the sample complexity reduces with the joint sparsiﬁcation. Figure 10 shows the test errors on the Citeseer dataset under different sparsiﬁcation rates, and darker colors denote lower errors. In both ﬁgures, we observe that the joint edge sampling and pruning can reduce the test error even when more than 90% of neurons are pruned and 25% of edges are removed, which justiﬁes the efﬁciency of joint edge-model sparsiﬁcation. In addition, joint model- edge sparsiﬁcation with a smaller number of training samples can achieve similar or even better performance than that without sparsiﬁcation. For instance, when we have 120 training samples, the test error is 30.4% without any speciﬁcation. However, the joint sparsiﬁcation can improve the test error to 28.7% with only 96 training samples. We only include partial results due to the space limit. Please see the supplementary materials for more experiments on synthetic and real datasets. (1.00, 0.00) (0.85, 0.49) (0.72, 0.74) (0.62, 0.87) Rate of sampled edges   Rate of pruned neurons 0.19 0.2 0.21 0.22Test error Node classification on Cora Figure 9: Test error on Cora. 1 0.95 0.9 0.86 0.81 0.77 0.73 Rate of sampled edges 0 0.36 0.6 0.74 0.84 0.89 0.93Rate of pruned neurons (log scale) Citeseer (|D| = 120) 0.304 0.287 0.289 0.297 0.296 0.299 0.299 0.281 0.276 0.293 0.304 0.308 0.298 0.301 0.288 0.285 0.289 0.296 0.295 0.302 0.297 0.283 0.279 0.279 0.286 0.288 0.291 0.302 0.285 0.28 0.276 0.281 0.284 0.291 0.287 0.291 0.279 0.277 0.284 0.291 0.289 0.289 0.292 0.279 0.284 0.286 0.296 0.292 0.293 1 0.95 0.9 0.86 0.81 0.77 0.73 Rate of sampled edges 0 0.36 0.6 0.74 0.84 0.89 0.93 Citeseer (|D|=96) 0.309 0.292 0.294 0.302 0.301 0.304 0.304 0.298 0.303 0.298 0.309 0.313 0.303 0.306 0.3 0.296 0.294 0.301 0.3 0.307 0.302 0.288 0.293 0.287 0.299 0.293 0.296 0.307 0.29 0.295 0.297 0.296 0.289 0.296 0.292 0.296 0.304 0.293 0.293 0.296 0.294 0.294 0.297 0.299 0.303 0.299 0.301 0.297 0.298 Figure 10: Heatmaps depicting the test error on Citeseer with 96 and 120 training nodes. 5 C ONCLUSIONS Encouraged by the empirical success of sparse learners in accelerating GNN training, this paper characterizes the impact of graph sampling and neuron pruning on the sample complexity and convergence rate for a desirable test accuracy quantitatively. To the best of our knowledge, this is the ﬁrst theoretical generalization analysis of joint edge-model sparsiﬁcation in training GNNs. Future directions include generalizing the analysis to multi-layer cases, other graph sampling strategies, e.g., FastGCN (Chen et al., 2018), or link-based classiﬁcation problems. 9ACKNOWLEDGEMENT This work was supported by AFOSR FA9550-20-1-0122, ARO W911NF-21-1-0255, NSF 1932196 and the Rensselaer-IBM AI Research Collaboration (http://airc.rpi.edu), part of the IBM AI Horizons Network (http://ibm.biz/AIHorizons). We thank Kevin Li and Sissi Jian at Rensselaer Polytechnic Institute for the help in formulating numerical experiments. We thank all anonymous reviewers for their constructive comments. REPRODUCIBILITY STATEMENT For the theoretical results in Section 3.2, we provide the necessary lemmas in Appendix D and a complete proof of the major theorems based on the lemmas in Appendix E. The proof of all the lemmas are included in Appendix H. For experiments in Section 4, the implementation details in generating the data and ﬁgures are summarized in the Appendix F, and the source code can be found in the supplementary material. REFERENCES Bijaya Adhikari, Yao Zhang, Sorour E Amiri, Aditya Bharadwaj, and B Aditya Prakash. Propagation- based temporal network summarization. IEEE Transactions on Knowledge and Data Engineering, 30(4):729–742, 2017. Zeyuan Allen-Zhu and Yuanzhi Li. Feature puriﬁcation: How adversarial training performs robust deep learning. In 2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS), pp. 977–988. IEEE, 2022. Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized neural networks, going beyond two layers. In Advances in Neural Information Processing Systems, pp. 6158–6169, 2019a. Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over- parameterization. In International Conference on Machine Learning, pp. 242–252. PMLR, 2019b. Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In International Conference on Machine Learning, pp. 322–332. PMLR, 2019. Mehdi Bahri, Gaétan Bahl, and Stefanos Zafeiriou. Binary graph neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9492–9501, 2021. Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov):463–482, 2002. Alon Brutzkus and Amir Globerson. An optimization and generalization analysis for max-pooling networks. In Uncertainty in Artiﬁcial Intelligence, pp. 1650–1660. PMLR, 2021. Alfredo Canziani, Adam Paszke, and Eugenio Culurciello. An analysis of deep neural network models for practical applications. arXiv preprint arXiv:1605.07678, 2016. Alireza Chakeri, Hamidreza Farhidzadeh, and Lawrence O Hall. Spectral sparsiﬁcation in spectral clustering. In 2016 23rd International Conference on Pattern Recognition (ICPR), pp. 2301–2306. IEEE, 2016. Deli Chen, Yankai Lin, Guangxiang Zhao, Xuancheng Ren, Peng Li, Jie Zhou, and Xu Sun. Topology- imbalance learning for semi-supervised node classiﬁcation. Advances in Neural Information Processing Systems, 34:29885–29897, 2021a. Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: Fast learning with graph convolutional networks via importance sampling. In International Conference on Learning Representations, 2018. 10Tianlong Chen, Yongduo Sui, Xuxi Chen, Aston Zhang, and Zhangyang Wang. A uniﬁed lottery ticket hypothesis for graph neural networks. In International Conference on Machine Learning, pp. 1695–1706. PMLR, 2021b. Weilin Cong, Morteza Ramezani, and Mehrdad Mahdavi. On provable beneﬁts of depth in train- ing graph convolutional networks. In A. Beygelzimer, Y . Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems , 2021. URL https: //openreview.net/forum?id=r-oRRT-ElX. Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Liò, and Petar Veli ˇckovi´c. Principal neighbourhood aggregation for graph nets. Advances in Neural Information Processing Systems, 33:13260–13271, 2020. Arthur da Cunha, Emanuele Natale, and Laurent Viennot. Proving the strong lottery ticket hypothesis for convolutional neural networks. In International Conference on Learning Representations , 2022. Alexandru Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent. In Conference on Learning Theory, pp. 5413–5452. PMLR, 2022. Amit Daniely and Eran Malach. Learning parities with neural networks. Advances in Neural Information Processing Systems, 33:20356–20365, 2020. Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. In International Conference on Learning Representations, 2018. Simon S Du, Kangcheng Hou, Russ R Salakhutdinov, Barnabas Poczos, Ruosong Wang, and Keyulu Xu. Graph neural tangent kernel: Fusing graph neural networks with graph kernels. In Advances in Neural Information Processing Systems, pp. 5724–5734, 2019. Talya Eden, Shweta Jain, Ali Pinar, Dana Ron, and C Seshadhri. Provable and practical approxima- tions for the degree distribution using sublinear graph samples. In Proceedings of the 2018 World Wide Web Conference, pp. 449–458, 2018. Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In International Conference on Learning Representations, 2019. URL https: //openreview.net/forum?id=rJl-b3RcF7. Vikas Garg, Stefanie Jegelka, and Tommi Jaakkola. Generalization and representational limits of graph neural networks. In International Conference on Machine Learning, pp. 3419–3430. PMLR, 2020. Fangtai Guo, Zaixing He, Shuyou Zhang, Xinyue Zhao, Jinhui Fang, and Jianrong Tan. Normalized edge convolutional networks for skeleton-based hand gesture recognition. Pattern Recognition, 118:108044, 2021. Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, pp. 1024–1034, 2017. Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. Lightgcn: Simplifying and powering graph convolution network for recommendation. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 639–648, 2020. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. stat, 1050:9, 2015. Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:22118–22133, 2020. 11Baihe Huang, Xiaoxiao Li, Zhao Song, and Xin Yang. Fl-ntk: A neural tangent kernel-based framework for federated learning analysis. In International Conference on Machine Learning, pp. 4423–4434. PMLR, 2021. Christian Hübler, Hans-Peter Kriegel, Karsten Borgwardt, and Zoubin Ghahramani. Metropolis algorithms for representative subgraph sampling. In 2008 Eighth IEEE International Conference on Data Mining, pp. 283–292. IEEE, 2008. Richard Hull and Roger King. Semantic database modeling: Survey, applications, and research issues. ACM Computing Surveys, 19(3):201–260, 1987. Matthew O Jackson. Social and economic networks. Princeton university press, 2010. Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, 2018. Ajay Kumar Jaiswal, Haoyu Ma, Tianlong Chen, Ying Ding, and Zhangyang Wang. Spending your winning lottery better after drawing it. arXiv preprint arXiv:2101.03255, 2021. Svante Janson. Large deviations for sums of partly dependent random variables. Random Structures & Algorithms, 24(3):234–248, 2004. Stefani Karp, Ezra Winston, Yuanzhi Li, and Aarti Singh. Local signal adaptivity: Provable feature learning in neural networks beyond kernels. Advances in Neural Information Processing Systems, 34:24883–24897, 2021. Thomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. In International Conference on Learning Representations, 2017. Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha Sohl-Dickstein. Deep neural networks as gaussian processes. In International Conference on Learning Representations, 2018. Jure Leskovec and Christos Faloutsos. Sampling from large graphs. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 631–636, 2006. Hongkang Li, Meng Wang, Sijia Liu, Pin-Yu Chen, and Jinjun Xiong. Generalization guarantee of training graph convolutional networks with graph topology sampling. In International Conference on Machine Learning, pp. 13014–13051. PMLR, 2022. Hongkang Li, Meng Wang, Sijia Liu, and Pin-Yu Chen. A theoretical understanding of vision trans- formers: Learning, generalization, and sample complexity. In International Conference on Learn- ing Representations, 2023. URL https://openreview.net/forum?id=jClGv3Qjhb. Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. In Advances in Neural Information Processing Systems, pp. 8157–8166, 2018. Shiyu Liang, Ruoyu Sun, Yixuan Li, and Rayadurgam Srikant. Understanding the loss surface of neural networks for binary classiﬁcation. In International Conference on Machine Learning, pp. 2835–2843. PMLR, 2018. Zirui Liu, Kaixiong Zhou, Fan Yang, Li Li, Rui Chen, and Xia Hu. Exact: Scalable graph neural networks training via extreme activation compression. In International Conference on Learning Representations, 2021. Wai Weng Lo, Siamak Layeghy, Mohanad Sarhan, Marcus Gallagher, and Marius Portmann. E- graphsage: A graph neural network based intrusion detection system for iot. In NOMS 2022-2022 IEEE/IFIP Network Operations and Management Symposium, pp. 1–9. IEEE, 2022. Eran Malach, Gilad Yehudai, Shai Shalev-Schwartz, and Ohad Shamir. Proving the lottery ticket hypothesis: Pruning is all you need. In International Conference on Machine Learning , pp. 6682–6691. PMLR, 2020. 12Jihun Oh, Kyunghyun Cho, and Joan Bruna. Advancing graphsage with a data-driven node sampling. arXiv preprint arXiv:1904.12935, 2019. Samet Oymak and Mahdi Soltanolkotabi. Toward moderate overparameterization: Global con- vergence guarantees for training shallow neural networks. IEEE Journal on Selected Areas in Information Theory, 1(1):84–105, 2020. Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, Lina Yao, and Chengqi Zhang. Adversarially regularized graph autoencoder for graph embedding. arXiv preprint arXiv:1802.04407, 2018. Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representa- tions. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 701–710, 2014. Ryan Riegel, Alexander Gray, Francois Luus, Naweed Khan, Ndivhuwo Makondo, Ismail Yunus Akhalwaya, Haifeng Qian, Ronald Fagin, Francisco Barahona, Udit Sharma, et al. Logical neural networks. arXiv preprint arXiv:2006.13155, 2020. Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convo- lutional networks on node classiﬁcation. In International Conference on Learning Representations, 2019. Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks. In International Conference on Machine Learning, pp. 4430–4438, 2018. Aliaksei Sandryhaila and Jose MF Moura. Big data analysis with signal processing on graphs: Representation and processing of massive data sets with irregular structure.IEEE Signal Processing Magazine, 31(5):80–90, 2014. Franco Scarselli, Ah Chung Tsoi, and Markus Hagenbuchner. The vapnik–chervonenkis dimension of graph and recursive neural networks. Neural Networks, 108:248–259, 2018. Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. Modeling relational data with graph convolutional networks. In Proceedings of European Semantic Web Conference, pp. 593–607. Springer, 2018. Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective classiﬁcation in network data. AI magazine, 29(3):93–93, 2008. Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014. Weijing Shi and Raj Rajkumar. Point-gnn: Graph neural network for 3d object detection in a point cloud. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1711–1719, 2020. Zhenmei Shi, Junyi Wei, and Yingyu Liang. A theoretical analysis on feature learning in neural networks: Emergence from inputs and advantage over ﬁxed features. In International Conference on Learning Representations, 2022. Shyam Anil Tailor, Javier Fernandez-Marques, and Nicholas Donald Lane. Degree-quant: Quantization-aware training for graph neural networks. In International Conference on Learning Representations, 2020. Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. International Conference on Learning Representations, 2018. Saurabh Verma and Zhi-Li Zhang. Stability and generalization of graph convolutional neural networks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1539–1548, 2019. Elli V oudigari, Nikos Salamanos, Theodore Papageorgiou, and Emmanuel J Yannakoudakis. Rank degree: An efﬁcient algorithm for graph sampling. In 2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, pp. 120–129. IEEE, 2016. 13Kuansan Wang, Zhihong Shen, Chiyuan Huang, Chieh-Han Wu, Yuxiao Dong, and Anshul Kanakia. Microsoft academic graph: When experts are not enough. Quantitative Science Studies, 1(1): 396–413, 2020. Zixin Wen and Yuanzhi Li. Toward understanding the feature learning process of self-supervised contrastive learning. In International Conference on Machine Learning, pp. 11112–11122. PMLR, 2021. Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Sim- plifying graph convolutional networks. In International Conference on Machine Learning, pp. 6861–6871, 2019. Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation sys- tem: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016. Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A comprehensive survey on graph neural networks. IEEE Transactions on Neural Networks and Learning Systems, 32(1):4–24, 2020. Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In International Conference on Machine Learning, pp. 5453–5462. PMLR, 2018. Keyulu Xu, Mozhi Zhang, Stefanie Jegelka, and Kenji Kawaguchi. Optimization of graph neural networks: Implicit acceleration by skip connections and more depth. In International Conference on Machine Learning, pp. 11592–11602. PMLR, 2021. Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial temporal graph convolutional networks for skeleton-based action recognition. In Proceedings of Association for the Advancement of Artiﬁcial Intelligence, 2018. Yiding Yang, Jiayan Qiu, Mingli Song, Dacheng Tao, and Xinchao Wang. Distilling knowledge from graph convolutional networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7074–7083, 2020. Huaxiu Yao, Chuxu Zhang, Ying Wei, Meng Jiang, Suhang Wang, Junzhou Huang, Nitesh Chawla, and Zhenhui Li. Graph few-shot learning via knowledge transfer. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pp. 6656–6663, 2020. Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec. Graph convolutional neural networks for web-scale recommender systems. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 974–983, 2018. Haoran You, Zhihan Lu, Zijian Zhou, Yonggan Fu, and Yingyan Lin. Early-bird gcns: Graph-network co-optimization towards more efﬁcient gcn training and inference via drawing early-bird lottery tickets. AAAI Conference on Artiﬁcial Intelligence, 2022. Shuai Zhang, Meng Wang, Sijia Liu, Pin-Yu Chen, and Jinjun Xiong. Fast learning of graph neural networks with guaranteed generalizability:one-hidden-layer case. In International Conference on Machine Learning, 2020a. Shuai Zhang, Meng Wang, Jinjun Xiong, Sijia Liu, and Pin-Yu Chen. Improved linear convergence of training cnns with generalizability guarantees: A one-hidden-layer case. IEEE Transactions on Neural Networks and Learning Systems, 32(6):2622–2635, 2020b. Shuai Zhang, Meng Wang, Sijia Liu, Pin-Yu Chen, and Jinjun Xiong. How unlabeled data improve generalization in self-training? a one-hidden-layer theoretical analysis. In International Conference on Learning Representations, 2022a. Tao Zhang, Hao-Ran Shan, and Max A Little. Causal graphsage: A robust graph method for classiﬁcation based on causal sampling. Pattern Recognition, 128:108696, 2022b. 14Zeru Zhang, Jiayin Jin, Zijie Zhang, Yang Zhou, Xin Zhao, Jiaxiang Ren, Ji Liu, Lingfei Wu, Ruoming Jin, and Dejing Dou. Validating the lottery ticket hypothesis with inertial manifold theory. Advances in Neural Information Processing Systems, 34, 2021. Cheng Zheng, Bo Zong, Wei Cheng, Dongjin Song, Jingchao Ni, Wenchao Yu, Haifeng Chen, and Wei Wang. Robust graph representation learning via neural sparsiﬁcation. In International Conference on Machine Learning, pp. 11458–11468. PMLR, 2020. Liyuan Zheng, Zhen Zuo, Wenbo Wang, Chaosheng Dong, Michinari Momma, and Yi Sun. Het- erogeneous graph neural networks with neighbor-sim attention mechanism for substitute product recommendation. AAAI Conference on Artiﬁcial Intelligence, 2021. Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees for one-hidden-layer neural networks. In International Conference on Machine Learning , pp. 4140–4149, 2017. Xianchen Zhou and Hongxia Wang. The generalization error of graph convolutional networks may enlarge with more layers. Neurocomputing, 424:97–106, 2021. Difan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, and Quanquan Gu. Layer-dependent importance sampling for training deep and large graph convolutional networks. Proceedings of Advances in Neural Information Processing Systems, 32, 2019. 15Supplementary Materials for: Joint Edge-Model Sparse Learning is Provably Efﬁcient for Graph Neural Networks In the following contexts, the related works are included in Appendix A. Appendix B provides a high-level idea for the proof techniques. Appendix C summarizes the notations for the proofs, and the useful lemmas are included in Appendix D. Appendix E provides the detailed proof of Theorem 2, which is the formal version of Theorem 1. Appendix F describes the details of synthetic data experiments in Section 4, and several other experimental results are included because of the limited space in the main contexts. The lower bound of the VC-dimension is proved in Appendix G. Appendix G.1 provides the bound of αfor some edge sampling strategies. Additional proofs for the useful lemmas are summarized in Appendix H. In addition, we provide a high-level idea in extending the framework in this paper to a multi-class classiﬁcation problem in Appendix I. A R ELATED WORKS Generalization analysis of GNNs. Two recent papers (Du et al., 2019; Xu et al., 2021) exploit the neural tangent kernel (NTK) framework (Malach et al., 2020; Allen-Zhu et al., 2019b; Jacot et al., 2018; Du et al., 2018; Lee et al., 2018) for the generalization analysis of GNNs. It is shown in (Du et al., 2019) that the graph neural tangent kernel (GNTK) achieves a bounded generalization error only if the labels are generated from some special function, e.g., the function needs to be linear or even. (Xu et al., 2021) analyzes the generalization of deep linear GNNs with skip connections. The NTK approach considers the regime that the model is sufﬁciently over-parameterized, i.e., the number of neurons is a polynomial function of the sample amount, such that the landscape of the risk function becomes almost convex near any initialization. The required model complexity is much more signiﬁcant than the practical case, and the results are irrelevant of the data distribution. As the neural network learning process is strongly correlated with the input structure (Shi et al., 2022), distribution-free analysis, such as NTK, might not accurately explain the learning performance on data with special structures. Following the model recovery frameworks (Zhong et al., 2017; Zhang et al., 2022a; 2020b), Zhang et al. (2020a) analyzes the generalization of one-hidden-layer GNNs assuming the features belong to Gaussian distribution, but the analysis requires a special tensor initialization method and does not explain the practical success of SGD with random initialization. Besides these, the generalization gap between the training and test errors is characterized through the classical Rademacher complexity in (Scarselli et al., 2018; Garg et al., 2020) and uniform stability framework in (Verma & Zhang, 2019; Zhou & Wang, 2021). Generalization analysis with structural constraints on data. Assuming the data come from mixtures of well-separated distributions, (Li & Liang, 2018) analyzes the generalization of one- hidden-layer fully-connected neural networks. Recent works (Shi et al., 2022; Brutzkus & Globerson, 2021; Allen-Zhu & Li, 2022; Karp et al., 2021; Wen & Li, 2021; Li et al., 2023) analyze one-hidden- layer neural networks assuming the data can be divided into discriminative and background patterns. Neural networks with non-linear activation functions memorize the discriminative features and have guaranteed generalization in the unseen data with same structural constraints, while no linear classiﬁer with random initialization can learn the data mapping in polynomial sizes and time (Shi et al., 2022; Daniely & Malach, 2020). Nevertheless, none of them has considered GNNs or sparsiﬁcation. B O VERVIEW OF THE TECHNIQUES Before presenting the proof details, we will provide a high-level overview of the proof techniques in this section. To warm up, we ﬁrst summarize the proof sketch without edge and model sparsiﬁcation 16methods. Then, we illustrate the major challenges in deriving the results for edge and model sparsiﬁcation approaches. B.1 G RAPH NEURAL NETWORK LEARNING ON DATA WITH STRUCTURAL CONSTRAINTS For the convenience of presentation, we use D+ and D−to denote the set of nodes with positive and negative labels in D, respectively, where D+ = (V+ ∪VN+) ∩D and D−= (V−∪VN−) ∩D. Recall that p+ only exists in the neighbors of node v∈D+, and p−only exists in the neighbors of node v∈D−. In contrast, class irrelevant patterns are distributed identically for data in D+ and D−. In addition, for some neuron, the gradient direction will always be near p+ for v ∈D+, while the gradient derived from v∈D−is always almost orthogonal to p+. Such neuron is the lucky neuron deﬁned in Proposition 1 in Section 3.4, and we will formally deﬁne the lucky neuron in Appendix C from another point of view. Take the neurons in B+ for instance, where B+ = {k |bk = +1}denotes the set of neurons with positive coefﬁcients in the linear layer. For a lucky neuron k∈K+, the projection of the weights on p+ strictly increases (see Lemma 2 in Appendix D). For other neurons, which are named as unlucky neurons, class irrelevant patterns are identically distributed and independent of yv, the gradient generated from D+ and D−are similar. Speciﬁcally, because of the offsets between D+ and D−, the overall gradient is in the order of √ (1 + R2)/|D|, where Ris the degree of graph. With a sufﬁciently large amount of training samples, the projection of the weights on class irrelevant patterns grows much slower than that on class relevant patterns. One can refer to Figure 11 for an illustration of neuron weights update. 𝒘𝒘(0) Gradient from 𝒟𝒟+ Gradient from 𝒟𝒟− 𝐶𝐶 + 𝒪𝒪 1 + 𝑅𝑅2 |𝒟𝒟| 𝐶𝐶 + 𝒪𝒪 1 + 𝑅𝑅2 |𝒟𝒟| 𝒑𝒑+ 𝑜𝑜𝑜𝑜𝒑𝒑− 𝒘𝒘(0) 𝒘𝒘(1) 𝒘𝒘(2) 𝒘𝒘(3) 𝒘𝒘(4) 𝒑𝒑+ 𝑜𝑜𝑜𝑜𝒑𝒑− 𝒑𝒑 ∈ 𝒫𝒫𝑁𝑁 𝒑𝒑 ∈ 𝒫𝒫𝑁𝑁 (a) 𝒘𝒘(0) Gradient from 𝒟𝒟+ Gradient from 𝒟𝒟− 𝐶𝐶 + 𝒪𝒪 1 + 𝑅𝑅2 |𝒟𝒟| 𝐶𝐶 + 𝒪𝒪 1 + 𝑅𝑅2 |𝒟𝒟| 𝒑𝒑+ 𝑜𝑜𝑜𝑜𝒑𝒑− 𝒘𝒘(0) 𝒘𝒘(1) 𝒘𝒘(2) 𝒘𝒘(3) 𝒘𝒘(4) 𝒑𝒑+ 𝑜𝑜𝑜𝑜𝒑𝒑− 𝒑𝒑 ∈ 𝒫𝒫𝑁𝑁 𝒑𝒑 ∈ 𝒫𝒫𝑁𝑁 (b) Figure 11: Illustration of iterations {w(t)}T t=1: (a) lucky neuron, and (b) unlucky neuron. Similar to the derivation of w(t) k for k∈B+, we can show that neurons in K−, which are the lucky neurons with respect to k ∈B−, have their weights updated mainly in the direction of p−. Recall that the output of GNN model is written as (20), the corresponding coefﬁcients in the linear layer for k∈B+ are all positive. With these in hand, we know that the neurons in B+ have a relatively large magnitude in the direction of p+ compared with other patterns, and the corresponding coefﬁcients bk are positive. Then, for the node v∈V+ ∪VN+, the calculated label will be strictly positive. Similar to the derivation above, the calculated label for the node v∈V−∪VN−will be strictly negative. Edge sparsiﬁcation. Figure 11(b) shows that the gradient in the direction of any class irrelevant patterns is a linear function of √ 1 + R2 without sampling. Sampling on edges can signiﬁcantly reduce the degree of the graph, i.e., the degree of the graph is reduced to rwhen only sampling r neighbor nodes. Therefore, the projection of neuron weights on any p ∈PN becomes smaller, and the neurons are less likely to learn class irrelevant features. In addition, the computational complexity per iteration is reduced since we only need to traverse a subset of the edges. Nevertheless, sampling 17on graph edges may lead to missed class-relevant features in some training nodes (a smallerα), which will degrade the convergence rate and need a larger number of iterations. Model sparsiﬁcation. Comparing Figure 11(a) and 11(b), we can see that the magnitudes of a lucky neuron grow much faster than these of an unlucky neuron. In addition, from Lemma 6, we know that the lucky neuron at initialization will always be the lucky neuron in the following iterations. Therefore, the magnitude-based pruning method on the original dense model removes unlucky neurons but preserves the lucky neurons. When the fraction of lucky neurons is improved, the neurons learn the class-relevant features faster. Also, the algorithm can tolerate a larger gradient noise derived from the class irrelevant patterns in the inputs, which is in the order of 1/ √ |D|from Figure 11(b). Therefore, the required samples for convergence can be signiﬁcantly reduced. Noise factor z. The noise factor z degrades the generalization mainly in the following aspects. First, in Brutzkus & Globerson (2021), the sample complexity depends on the size of {xv}v∈D, which, however, can be as large as |D|when there is noise. Second, the fraction of lucky neurons is reduced as a function of the noise level. With a smaller fraction of lucky neurons, we require a larger number of training samples and iterations for convergence. C N OTATIONS In this section, we implement the details of data model and problem formulation described in Section 3.2, and some important notations are deﬁned to simplify the presentation of the proof. In addition, all the notations used in the following proofs are summarized in Tables 2 and 3. C.1 D ATA MODEL WITH STRUCTURAL CONSTRAINTS Recall the deﬁnitions in Section 3.2, the node feature for node vis written as xv = pv + zv, (14) where pv ∈P, and zv is bounded noise with ∥zv∥2 ≤σ. In addition, there are Lorthogonal patterns in P, denoted as {pℓ}L ℓ=1. p+ := p1 is the positive class relevant pattern, p−:= p2 is the negative class relevant pattern, and the rest of the patterns, denoted as PN, are the class irrelevant patterns. For node v, its label yv is positive or negative if its neighbors contain p+ or p−. By saying a node v contains class relevant feature, we indicate that pv = p+ or p−. Depending on xv and yv, we divide the nodes in Vinto four disjoint partitions, i.e., V= V+ ∪V−∪ VN+ ∪VN−, where V+ := {v|pv = p+}; V−:= {v|pv = p−}; VN+ := {v|pv ∈PN,yv = +1}; VN−:= {v|pv ∈PN,yv = −1}. (15) Then, we consider the model such that (i) the distribution of p+ and p−are identical, namely, Prob(pv = p+) = Prob(pv = p−), (16) and (ii) p ∈PN are identically distributed in VN+ and VN−, namely, Prob(p |v∈VN+) = Prob(p |v∈VN−) for any p ∈PN. (17) It is easy to verify that, when (16) and (17) hold, the number of positive and negative labels in Dare balanced, such that Prob(yv = +1) = Prob(yv = −1) = 1 2. (18) If D+ and D−are highly unbalanced, namely, ⏐⏐|D+|−|D−| ⏐⏐≫ √ |D|, the objective function in (2) can be modiﬁed as ˆfD:= − 1 2|D+| ∑ v∈D+ yv ·g ( W; XN(v) ) − 1 2|D−| ∑ v∈D− yv ·g ( W; XN(v) ) , (19) and the required number of samples |D|in (6) is replaced with min{|D+|,|D−|}. 18C.2 G RAPH NEURAL NETWORK MODEL It is easy to verify that (1) is equivalent to the model g(W,U; x) = 1 K ∑ k AGG(XN(v),wk) − 1 K ∑ k AGG(XN(v),uk), (20) where the neuron weights {wk}K k=1 in (20) are with respect to the neuron weights with bk = +1 in (1), and the neuron weights {uk}K k=1 in (20) are respect to the neuron weights with bk = −1 in (1). Here, we abuse Kto represent the number of neurons in {k|bk = +1}or {k|bk = −1}, which differs from the Kin (1) by a factor of 2. Since this paper aims at providing order-wise analysis, the bounds for Kin (1) and (20) are the same. Corresponding to the model in (20), we denote M+ as the mask matrix after pruning with respect to W and M−as the mask matrix after pruning with respect to U. For the convenience of analysis, we consider balanced pruning in W and U, i.e., ∥M+∥0 = ∥M−∥0 = (1 −β)Kd. C.3 A DDITIONAL NOTATIONS FOR THE PROOF Pattern function M(v). Now, recall that at iteration t, the aggregator function for node vis written as AGG(XN(t)(v),w(t) k ) = max n∈N(t)(v) φ(⟨w(t) k , xn ⟩). (21) Then, at iteration t, we deﬁne the pattern function M(t) : V→P ⋃{0}at iteration tas M(t)(v; w) =    0, if max n∈N(t)(v) φ(⟨w , xn ⟩) ≤0 argmax {xn|n∈N(t)(v)} φ(⟨w , xn ⟩), otherwise . (22) Similar to the deﬁnition of pv in (14), we deﬁne M(t) p and M(t) z such that M(t) p is the noiseless pattern with respect to M(t) while M(t) z is noise with respect to M(t). In addition, we deﬁne M: V→P ⋃{0}for the case without edge sampling such that M(v; w) =    0, if max n∈N(v) φ(⟨w , xn ⟩) ≤0 argmax {xn|n∈N(v)} φ(⟨w , xn ⟩), otherwise . (23) Deﬁnition of lucky neuron. We call a neuron is the lucky neuron at iteration tif and only if its weights vector in {w(t) k }K k=1 satisﬁes Mp(v; w(t) k ) ≡p+ for any v∈V (24) or its weights vector in {u(t) k }K k=1 satisﬁes Mp(v; u(t) k ) ≡p− for any v∈V. (25) Let W(t), U(t) be the set of the lucky neuron at t-th iteration such that W(t) = {k|M(t) p (v; w(t) k ) = p+ for any v∈V}, U(t) = {k|M(t) p (v; u(t) k ) = p− for any v∈V} (26) All the other other neurons, denoted as Wc(t) and Uc(t), are the unlucky neurons at iteration t. Compared with the deﬁnition of “lucky neuron” in Proposition 1 in Section 3.4, we have∩T t=1W(t) = K+. From the contexts below (see Lemma 6), one can verify that K+ = ∩T t=1W(t) = W(0). 19Gradient of the lucky neuronand unlucky neuron. We can rewrite the gradient descent in (4) as ∂ˆf(t) D ∂w(t) k = − 1 |D| ∑ v∈D ∂g(W(t),U(t); XN(t)(v)) ∂w(t) k = − 1 2|D+| ∑ v∈D+ ∂g(W(t),U(t); XN(t)(v)) ∂w(t) k + 1 2|D−| ∑ v∈D− ∂g(W(t),U(t); XN(t)(v)) ∂w(t) k , (27) where D+ and D−stand for the set of nodes inDwith positive labels and negative labels, respectively. According to the deﬁnition of M(t) in (22), it is easy to verify that ∂g(W(t),U(t); XN(v)) ∂w(t) k = M(t)(v; w(t) k ), (28) and the update of wk is w(t+1) k = w(t) k + cη ·Ev∈Dyv ·M(t)(v; w(t) k ), or w(t+1) k = w(t) k + cη ·Ev∈D+ M(t)(v; w(t) k ) −cη ·Ev∈D−M(t)(v; w(t) k ), (29) where we abuse the notation Ev∈Sto denote Ev∈Sf(v) = 1 |S| ∑ v∈S f(v) (30) for any set Sand some function f. Additionally, without loss of generality, the neuron that satisﬁes maxp∈P⟨w(0) k , p ⟩< 0 is not considered because (1) such neuron is not updated at all; (2) the probability of such neuron is negligible as 2−L. Finally, as the focus of this paper is order-wise analysis, some constant numbers may be ignored in part of the proofs. In particular, we use h1(L) ≳ h2(L) to denote there exists some positive constant Csuch that h1(L) ≥C·h2(L) when L∈R is sufﬁciently large. Similar deﬁnitions can be derived for h1(L) ≂ h2(L) and h1(L) ≲ h2(L). D U SEFUL LEMMAS Lemma 1 indicates the relations of the number of neurons and the fraction of lucky neurons. When the number of neurons in the hidden layer is sufﬁciently large as (31), the fraction of lucky neurons is at least (1 −εK −Lσ/π)/Lfrom (32), where σis the noise level, and Lis the number of patterns. Lemma 1. Suppose the initialization {w(0) k }K k=1 and {u(0) k }K k=1 are generated through i.i.d. Gaus- sian. Then, if the number of neurons Kis large enough as K ≥ε−2 K L2 log q, (31) the fraction of lucky neuron, which is deﬁned in (24) and (25), satisﬁes ρ≥(1 −εK −Lσ π ) ·1 L (32) Lemmas 2 and 3 illustrate the projections of the weights for a lucky neuron in the direction of class relevant patterns and class irrelevant patterns. Lemma 2. For lucky neuron k ∈W(t), let w(t+1) k be the next iteration returned by Algorithm 1. Then, the neuron weights satisfy the following inequality: 1. In the direction of p+, we have ⟨w(t+1) k , p+ ⟩≥⟨ w(t) k , p+ ⟩+ cη ( α−σ √ (1 + r2) logq |D| ) ; 20Table 2: Important notations of sets [Z],Z ∈N+ The set of {1,2,3,··· ,Z} V The set of nodes in graph G E The set of edges in graph G P The set of class relevant and class irrelevant patterns K+ The set of lucky neurons with respect to W(0) K− The set of lucky neurons with respect to U(0) Kβ+(t) The set of unpruned neurons with respect to W(t) Kβ−(t) The set of unpruned neurons with respect to U(t) PN The set of class irrelevant patterns D The set of training data D+ The set of training data with positive labels D− The set of training data with negative labels N(v),v ∈V The neighbor nodes of node v(including vitself) in graph G N(t) s (v),v ∈V The sampled nodes of node vat iteration t W(t) The set of lucky neurons with respect to weights W(t) at iteration t U(t) The set of lucky neurons with respect to weights U(t) at iteration t Wc(t) The set of unlucky neurons with respect to weights W(t) at iteration t Uc(t) The set of unlucky neurons with respect to weights U(t) at iteration t 2. In the direction of p−or class irrelevant patterns such that for any p ∈P/p+, we have ⟨w(t+1) k , p ⟩−⟨ w(t) k , p ⟩≥− cη −σ−cη ·σ √ (1 + r2) logq |D| , and ⟨w(t+1) k , p ⟩−⟨ w(t) k , p ⟩≤ cη ·(1 + σ) · √ (1 + r2) logq |D| . Lemma 3. For lucky neuronk∈U(t), let u(t+1) k be the next iteration returned by Algorithm 1. Then, the neuron weights satisfy the following inequality: 1. In the direction of p−, we have ⟨u(t+1) k , p−⟩≥⟨ u(t) k , p−⟩+ cη ( α−σ √ (1 + r2) logq |D| ) ; 2. In the direction of p+ or class irrelevant patterns such that for any p ∈P/p−, we have ⟨u(t+1) k , p ⟩−⟨ u(t) k , p ⟩≥− cη −σ−cη ·σ √ (1 + r2) logq |D| , and ⟨u(t+1) k , p ⟩−⟨ u(t) k , p ⟩≤ cη ·(1 + σ) · √ (1 + r2) logq |D| . Lemmas 4 and 5 show the update of weights in an unlucky neuron in the direction of class relevant patterns and class irrelevant patterns. 21Table 3: Important notations of scalars and matrices π Mathematical constant that is the ratio of a circle’s circumference to its diameter d The dimension of input feature K The number of neurons in the hidden layer xv,v ∈V The input feature for node vin Rd pv,v ∈V The noiseless input feature for node vin Rd zv,v ∈V The noise factor for node vin Rd yv,v ∈V The label of node vin {−1,+1} R The degree of the original graph G W,U The neuron weights in hidden layer XN The collection of {xn}n∈Nin Rd×|N| r The size of sampled neighbor nodes L The size of class relevant and class irrelevant features p+ The class relevant pattern with respect to the positive label p− The class relevant pattern with respect to the negative label z The additive noise in the input features σ The upper bound of ∥z∥2 for the noise factor z M+ The mask matrix for W of the pruned model M− The mask matrix for U of the pruned model Lemma 4. For an unlucky neuronk∈Wc(t), let w(t+1) k be the next iteration returned by Algorithm 1. Then, the neuron weights satisfy the following inequality. 1. In the direction of p+, we have ⟨w(t+1) k , p+ ⟩≥⟨ w(t) k , p+ ⟩−cη ·σ √ (1 + r2) logq |D| ; 2. In the direction of p−, we have ⟨w(t+1) k , p−⟩−⟨ w(t) k , p−⟩≥− cη −σ−cη ·σ √ (1 + r2) logq |D| , and ⟨w(t+1) k , p−⟩−⟨ w(t) k , p−⟩≤ cη ·σ· √ (1 + r2) logq |D| ; 3. In the direction of class irrelevant patterns such that any p ∈PN, we have ⏐⏐⟨w(t+1) k , p ⟩−⟨ w(t) k , p ⟩ ⏐⏐≤cη ·(1 + σ) √ (1 + r2) logq |D| . Lemma 5. For an unlucky neuronk∈Uc(t), let u(t+1) k be the next iteration returned by Algorithm 1. Then, the neuron weights satisfy the following inequality. 1. In the direction of p−, we have ⟨u(t+1) k , p−⟩≥⟨ u(t) k , p−⟩−cη ·σ √ (1 + r2) logq |D| ; 222. In the direction of p+, we have ⟨u(t+1) k , p+ ⟩−⟨ u(t) k , p+ ⟩≥− cη −σ−cη ·σ √ (1 + r2) logq |D| , and ⟨u(t+1) k , p+ ⟩−⟨ u(t) k , p+ ⟩≤ cη ·σ· √ (1 + r2) logq |D| ; 3. In the direction of class irrelevant patterns such that any p ∈PN, we have ⏐⏐⟨u(t+1) k , p ⟩−⟨ u(t) k , p ⟩ ⏐⏐≤cη ·(1 + σ) · √ (1 + r2) logq |D| . Lemma 6 indicates the lucky neurons at initialization are still the lucky neuron during iterations, and the number of lucky neurons is at least the same as the the one at initialization. Lemma 6. Let W(t), U(t) be the set of the lucky neuron at t-th iteration in (26). Then, we have W(t) ⊆W(t+ 1), U(t) ⊆U(t+ 1) (33) if the number of samples |D|≳ α−2(1 + r2) logq. Lemma 7 shows that the magnitudes of some lucky neurons are always larger than those of all the unlucky neurons. Lemma 7. Let {W(t′)}T′ t′=1 be iterations returned by Algorithm 1 before pruning. Then, let Wc(t′) be the set of unlucky neuron at t′-th iteration, then we have ⟨w(t′) k1 , w(t′) k1 ⟩<⟨w(t′) k2 , w(t′) k2 ⟩ (34) for any k1 ∈W(t′) and k2 ∈Wc(0). Lemma 8 shows the moment generation bound for partly dependent random variables, which can be used to characterize the Chernoff bound of the graph structured data. Lemma 8 (Lemma 7, Zhang et al. (2020a)). Given a set of X= {xn}N n=1 that contains N partly dependent but identical distributed random variables. For eachn∈[N], suppose xn is dependent with at most dX random variables in X(including xn itself), and the moment generate function of xn satisﬁes Exnesxn ≤eCs2 for some constant Cthat may depend on the distribution of xn. Then, the moment generation function of ∑N n=1 xn is bounded as EXes∑N n=1 xn ≤eCdXNs2 . (35) E P ROOF OF MAIN THEOREM In what follows, we present the formal version of the main theorem (Theorem 2) and its proof. Theorem 2. Let εN ∈(0,1) and εK ∈(0,1 −σL π ) be some positive constant. Then, suppose the number of training samples satisﬁes |D|≳ ε−2 N ·α−2 ·(1 −β)2 ·(1 + σ)2 · ( 1 −εK −σL π )−2 ·(1 + r2) ·L2 ·log q, (36) and the number of neurons satisﬁes K ≳ ε−2 K L2 log q, (37) for some positive constant q. Then, after T number of iterations such that T ≳ cη ·(1 −β) ·L α·(1 −εN −σ) ·(1 −εK −σL/π), (38) the generalization error function in (3) satisﬁes I ( g(W(T),U(T)) ) = 0 (39) with probability at least 1 −q−C for some constant C >0. 23Proof of Theorem 2. Let Kβ+ and Kβ−be the indices of neurons with respect to M+ ⊙W and M−⊙U, respectively. Then, for any node v∈V with label yv = 1, we have g(M+ ⊙W(t),M−⊙U(t); v) = 1 |Kβ+| ∑ k∈Kβ+ max n∈N(v) φ ( ⟨w(t) k , xn ⟩ ) − 1 |Kβ−| ∑ k∈Kβ− max n∈N(v) φ ( ⟨u(t) k , xn ⟩ ) ≥ 1 |Kβ+| ∑ k∈Kβ+ max n∈N(v) φ ( ⟨w(t) k , xn ⟩ ) − 1 |Kβ−| ∑ k∈Kβ− max n∈N(v) |⟨u(t) k , pn ⟩|− 1 |Kβ−| ∑ k∈Kβ− max n∈N(v) |⟨u(t) k , zn ⟩| = 1 |Kβ+| ∑ k∈W(t) ⟨w(t) k , p+ + zn ⟩+ 1 |Kβ+| ∑ k∈Kβ+/W(t) max n∈N(v) φ ( ⟨w(t) k , xn ⟩ ) − 1 |Kβ−| ∑ k∈Kβ− max n∈N(v) |⟨u(t) k , pn ⟩|− 1 |Kβ−| ∑ k∈Kβ− max n∈N(v) |⟨u(t) k , zn ⟩| ≥ 1 |Kβ+| ∑ k∈W(t) ⟨w(t) k , p+ + zn ⟩ − 1 |Kβ−| ∑ k∈Kβ− max n∈N(v) |⟨u(t) k , pn ⟩|− 1 |Kβ−| ∑ k∈Kβ− max n∈N(v) |⟨u(t) k , zn ⟩|, (40) where W(t) is the set of lucky neurons at iteration t. Then, we have 1 |Kβ+| ∑ k∈W(t) ⟨w(t) k , p+ + zn ⟩≥ 1 −σ |Kβ+| ∑ k∈W(0) ⟨w(t) k , p+ ⟩ ≥1 −σ |Kβ+|·|W(0)|·cη · ( α− √ (1 + r2) logq |D| ) ·t ≳ 1 |Kβ+|·|W(0)|·cη ·α·t (41) where the ﬁrst inequality comes from Lemma 6, the second inequality comes from Lemma 2. On the one hand, from Lemma 2, we know that at least (1 −εK −σL π )Kneurons out of {w(0) k }K k=1 are lucky neurons. On the other hand, we know that the magnitude of neurons in W(0) before pruning is always larger than all the other neurons. Therefore, such neurons will not be pruned after magnitude based pruning. Therefore, we have |W(0)|= (1 −εK −σL π )K = (1 −εK −σL π )|K+|/(1 −β) (42) Hence, we have 1 |Kβ+| ∑ k∈W(t) ⟨w(t) k , p+ ⟩≳(1 −εK −σL π ) · 1 (1 −β)L ·α·t. (43) In addition, XN(v) does not contain p−when yv = 1. Then, from Lemma 3 and Lemma 5, we have 1 |Kβ−| ∑ k∈Kβ− max n∈N(v) |⟨u(t) k , pn ⟩|≲ cη(1 + σ) √ (1 + r2) logq |D| t ≲ εN ·(1 −εK −σL π ) ·α· 1 (1 −β)Lt, (44) where the last inequality comes from (36). 24Moreover, we have 1 |Kβ−| ∑ k∈Kβ− max n∈N(v) |⟨u(t) k , zn ⟩|≤ Ek∈Kβ−∥u(t) k ∥2 ·∥zn∥2 ≤σ·Ek∈Kβ− ∑ p∈P/p− ⟨u(t) k , p ⟩ ≲ σ·cη ·L(1 + σ) √ (1 + r2) logq |D| ·t. (45) Combining (43), (44), and (45), we have g(W(T),U(T); v) ≥(1 −εN)(1 −εK −σL/π)(1 −σL) α (1 −β)L ·cη ·T. (46) Therefore, when the number of iterations satisﬁes T > cη ·(1 −β) ·L α·(1 −εN −σ) ·(1 −εK −σL/π) ·(1 −σL), (47) we have g(W(t),U(t); v) >1. Similar to the proof of (46), for any v∈V with label yv = −1, we have g(M+ ⊙W(T),M−⊙U(T); v) = 1 |Kβ+| ∑ k∈Kβ+ max n∈N(v) φ ( ⟨w(T) k , xn ⟩ ) − 1 |Kβ−| ∑ k∈Kβ− max n∈N(v) φ ( ⟨u(T) k , xn ⟩ ) ≤− 1 |Kβ−| ∑ k∈Kβ− max n∈N(v) φ ( ⟨w(T) k , xn ⟩ ) + 1 |Kβ+| ∑ k∈Kβ+ max n∈N(v) |⟨u(T) k , pn ⟩|+ 1 |Kβ+| ∑ k∈Kβ− max n∈N(v) |⟨u(T) k , zn ⟩| = − 1 |Kβ−| ∑ k∈W(T) ⟨w(T) k , p−+ zn ⟩− 1 |Kβ−| ∑ k∈Kβ−/W(t) max n∈N(v) φ ( ⟨w(T) k , xn ⟩ ) + 1 |Kβ+| ∑ k∈Kβ+ max n∈N(v) |⟨u(T) k , pn ⟩|+ 1 |Kβ+| ∑ k∈Kβ+ max n∈N(v) |⟨u(T) k , zn ⟩| ≥− 1 |Kβ−| ∑ k∈W(T) ⟨w(t) k , p−+ zn ⟩ + 1 |Kβ+| ∑ k∈Kβ+ max n∈N(v) |⟨u(t) k , pn ⟩|+ 1 |Kβ+| ∑ k∈Kβ+ max n∈N(v) |⟨u(t) k , zn ⟩|, ≤− 1 |Kβ−||U(T)|·cη ·αT +cη ·(1 + σ) ·T · √ (1 + r2) logq |D| + cη ·σ·(1 + σ) · √ (1 + r2) logq |D| ·T ≲ −(1 −εN)(1 −εK −σL/π) ·(1 −σL) · α (1 −β)L ≤−1. (48) Hence, we have g(W(t),U(t); v) <−1 for any vwith label yv = −1. In conclusion, the generalization function in (3) achieves zero when conditions (36), (37), and (38) hold. 25F N UMERICAL EXPERIMENTS F.1 I MPLEMENTATION OF THE EXPERIMENTS Generation of the synthetic graph structured data. The synthetic data used in Section 4 are generated in the following way. First, we randomly generate three groups of nodes, denoted as V+, V−, and VN. The nodes in V+ are assigned with noisy p+, and the nodes in V−are assigned with noisy p−. The patterns of nodes in VN are class-irrelevant patterns. For any node vin VN, vwill contact to some nodes in either V+ or V−uniformly. If the node connects to nodes in V+, then its label is +1. Otherwise, its label is −1. Finally, we will add random connections among the nodes within V+, V−or VN. To verify our theorems, each node in VN will connect to exactly one node in V+ or V−, and the degree of the nodes in VN is exactly M by randomly selecting M −1 other nodes in VN. We use full batch gradient descent for synthetic data. Algorithm 1 terminates if the training error becomes zero or the maximum of iteration 500 is reached. If not otherwise speciﬁed, δ = 0.1, cη = 1, r = 20, α= r/R, β = 0.2, d= 50, L= 200, σ = 0.2, and |D|= 100 with the rest of nodes being test data. Implementation of importance sampling on edges. We are given the sampling rate of importance edges αand the number of sampled neighbor nodes r. First, we sample the importance edge for each node with the rate of α. Then, we randomly sample the remaining edges without replacement until the number of sampled nodes reaches r. Implementation of magnitude pruning on model weights. The pruning algorithm follows exactly the same as the pseudo-code in Algorithm 1. The number of iterations T′is selected as 5. Illustration of the error bars. In the ﬁgures, the region in low transparency indicates the error bars. The upper envelope of the error bars is based on the value of mean plus one standard derivation, and the lower envelope of the error bars is based on the value of mean minus one standard derivation. F.2 E MPIRICAL JUSTIFICATION OF DATA MODEL ASSUMPTIONS In this part, we will use Cora dataset as an example to demonstrate that our data assumptions can model some real application scenarios. Cora dataset is a citation network containing 2708 nodes, and nodes belong to seven classes, namely, “Neural Networks”, “Rule Learning”, “Reinforcement Learning”, “Probabilistic Methods”, “Theory”, “Genetic Algorithms”, and “Case Based”. For the convenience of presentation, we denote the labels above as “class 1” to “class 7”, respectively. For each node, we calculate the aggregated features vector by aggregating its 1-hop neighbor nodes, and each feature vector is in a dimension of 1433. Then, we construct matrices by collecting the aggregated feature vectors for nodes with the same label, and the largest singular values of the matrix with respect to each class can be found in Table 4. Then, the cosine similarity, i.e., arccos <z1,z2> ∥z1∥2·∥z2∥2 , among the ﬁrst principal components, which is the right singular vector with the largest singular value, for different classes is provided, and results are summarized in Table 5. Table 4: The top 5 largest singular value (SV) of the collecting feature matrix for the classes Labels First SV Second SV Third SV Fourth SV Fifth SV class 1 8089.3 101.6 48.1 46.7 42.5 class 2 4451.2 48.7 28.0 26.7 22.4 class 3 4634.5 53.1 29.4 28.1 24.1 class 4 6002.5 72.9 37.0 35.6 31.4 class 5 5597.8 67.4 33.3 33.1 29.1 class 6 5947.9 72.1 36.7 35.5 31.3 class 7 5084.6 62.0 32.3 30.7 27.2 26Table 5: The cosine similarity among the classes class 1 class 2 class 3 class 4 class 5 class 6 class 7 class 1 0 89.3 88.4 89.6 87.4 89.4 89.5 class 2 89.3 0 89.1 89.5 88.7 89.7 88.7 class 3 88.4 89.1 0 89.7 89.9 89.0 89.1 class 4 89.6 89.5 89.7 0 88.0 88.9 88.7 class 5 87.4 88.7 89.9 88.0 0 89.8 88.2 class 6 89.4 89.7 89.0 88.9 89.8 0 89.5 class 7 89.5 88.7 89.1 88.7 88.3 89.5 0 From Table 4, we can see that the collected feature matrices are all approximately rank-one. In addition, from Table 5, we can see that the ﬁrst principal components of different classes are almost orthogonal. For instance, the pair-wise angles of feature matrices that correspond to “class 1”, “class 2”, and “class 3” are 89.3, 88.4, and 89.2, respectively. Table 4 indicates that the features of the nodes are highly concentrated in one direction, and Table 5 indicates that the principal components for different classes are almost orthogonal and independent. Therefore, we can view the primary direction of the feature matrix as the class-relevant features. If the node connects to class-relevant features of two classes frequently, the feature matrix will have at least two primary directions, which leads to a matrix with a rank of two or higher. If the nodes in one class connect to class-relevant features for two classes frequently, the ﬁrst principal component for this class will be a mixture of at least two class-relevant features, and the angles among the principal components for different classes cannot be almost orthogonal. Therefore, we can conclude that most nodes in different classes connect to different class-relevant features, and the class-relevant are almost orthogonal to each other. In addition, following the same experiment setup above, we implement another numerical experiment on a large-scale dataset Ogbn-Arxiv to further justify our data model. The cosine similarity between the estimated class-relevant features for the ﬁrst 10 classes are summarized in Table 6. As we can see, most of the angles are between 65 to 90, which suggests a sufﬁciently large distance between the class-relevant patterns for different classes. Moreover, to justify the existence of node features in V+(V−) and VN+(VN−), we have included a comparison of the node features and the estimated class-relevant features from the same class. Figure 12 illustrates the cosine similarity between the node features and the estimated class-relevant features from the same class. As we can see, the node with an angle smaller than 40 can be viewed as V+(V−), and the other nodes can be viewed as VN+(VN−), which veriﬁes the existence of class-relevant and class-irrelevant patterns. Similar results to ours that the node embeddings are distributed in a small space are also observed in (Pan et al., 2018) via clustering the node features. F.3 A DDITIONAL EXPERIMENTS ON SYNTHETIC DATA Figure 13 shows the phase transition of the sample complexity when K changes. The sample complexity is almost a linear function of 1/K. Figure 14 shows that the sample complexity increases as a linear function of σ2, which is the noise level in the features. In Figure 15, αis ﬁxed at 0.8 while increasing the number of sampled neighbors r. The sample complexity is linear in r2. Figure 16 illustrates the required number of iterations for different number of sampled edges, and R = 30. The ﬁtted curve, which is denoted as a black dash line, is a linear function of α−1 for α= r/R. We can see that the ﬁtted curve matches the empirical results for α= r/R, which veriﬁes the bound in (7). Also, applying importance sampling, the number of iterations is signiﬁcantly reduced with a large α. Figure 17 illustrates the required number of iterations for convergence with different pruning rates β. All the results are averaged over 100 independent trials. The black dash line stands for the baseline, the average number of iterations of training original dense networks. The blue line with circle marks is the performance of magnitude pruning of neuron weights. The number of iterations is almost a linear function of the pruning rate, which veriﬁes our theoretical ﬁndings in (7). The red line with star 27Table 6: The cosine similarity among the classes class 1 class 2 class 3 class 4 class 5 class 6 class 7 class 8 class 9 class 10 class 1 0 84.72 77.39 88.61 86.81 89.88 87.38 86.06 83.42 72.54 class 2 84.72 0 59.15 69.91 54.85 48.86 58.39 72.35 67.11 57.93 class 3 77.38 59.15 0 67.62 63.26 46.55 54.32 66.43 57.17 45.48 class 4 88.61 69.91 67.62 0 75.27 73.23 33.58 68.16 38.80 84.71 class 5 86.81 54.85 63.26 75.27 0 35.00 66.68 52.44 72.37 65.64 class 6 71.22 70.83 65.44 66.64 61.99 0.00 67.58 71.60 64.31 67.88 class 7 71.90 68.03 68.43 69.68 67.64 67.58 0.00 68.24 70.38 60.50 class 8 61.19 70.86 70.43 62.79 63.24 71.60 68.24 0.00 64.58 70.65 class 9 62.25 70.73 64.69 71.03 71.70 64.31 70.38 64.58 0.00 71.88 class 10 69.69 67.38 68.49 64.92 67.17 67.88 60.50 70.65 71.88 0.00 20 40 60 80 100 120 Angle between the node feature and the  estimated class-relevant feature from the same class 0 2 4 6 10 4 Figure 12: Distribution of the cosine similarity between the node feature and the estimated class- relevant feature from the same class marks shows the convergence without rewinding to the original initialization, whose results are worse than the baseline because random initialization after pruning leads to a less overparameterized model. Figure 18 indicates the test errors with different numbers of samples by averaging over1000 inde- pendent trials. The red line with circle marks shows the performance of training the original dense model. The blue line with star marks concerns the model after magnitude pruning, and the test error is reduced compared with training the original model. Additional experiment by using random pruning is summarized as the black line with the diamond mark, and the test error is consistently larger than those by training on the original model. Further, we justify our theoretical characterization in Cora data. Compared with synthetic data, αis not the sampling rate of edges but the sampling rate of class-relevant features, which is unknown for real datasets. Also, there is no standard method for us to deﬁne the success of the experiments for training practical data. Therefore, we make some modiﬁcations in the experiments to ﬁt into our theoretical framework. We utilize the estimated class-relevant feature from Appendix F.2 to determine whether the node is class-relevant or not, i.e., if we call the node feature as a class-relevant feature if the angle between the node feature and class-relevant feature is smaller than 30. For synthetic data, we deﬁne the success of a trial if it achieves zero generalization error. Instead, for practical data, we call the trial is success if the test accuracy is larger than 80%. Figure 19 illustrates the sample complexity against α−2, and the curve of the phrase transition is almost a line, which justify our 280.1 0.08 0.06 0.04 0.02 80 140 200 260 Figure 13: |D|against the number of neurons K 0.04 0.12 0.20 0.28 0.36 2 20 80 140 200 260 320 380 Figure 14: |D|against the noise level σ2. 100 420 740 10601380 40 80 120 160 200 240 Figure 15: |D|against the number of sampled neighbors r 30 24 18 12 r 40 60 80Number of iterations  = r/R  = 1.1 r/R  = 1.2 r/R Fitted curve of (7) Figure 16: Number of iterations against the edge sampling rate with random & important sam- pling. 0 0.2 0.4 0.6 0.8 60 100 140 160Number of iterations Rewinding initialization Random initialization Baseline Figure 17: Number of iterations against pruning rate of the model weights. 40 120 200 280 Number of samples 10 -2 10 -1 Test error Magnitude pruning Original model Random pruning Figure 18: The test error against the number of samples. theoretical results in (6). Figures 20 and 21 illustrate the convergence rate of the ﬁrst 50 epochs when training on the Cora test. We can see that the convergence rates are linear functions of the pruning rate and α−1 as indicated by our theoretical results in (6). 1.8 1.6 1.4 1.2 1 140 133 126 119 112 Figure 19: D against the es- timated importance sampling probability α. 0 0.2 0.4 0.6 0.80.954 0.956 0.958 0.96 0.962Convergence rate Convergence rate of training Cora Figure 20: Dagainst the pruning rate β. 1 1.1 1.2 1.3 1.40.958 0.959 0.96 0.961 0.962 0.963Convergence rate Figure 21: The convergence rate against the estimated importance sampling probability α. F.4 A DDITIONAL EXPERIMENTS ON RELAXED DATA ASSUMPTIONS In the following numerical experiments, we relax the assumption (A1) by adding extra edges between (1) V+ and VN−, (2) V−and VN+ , and (3) V+ and V−. We randomly select γfraction of the nodes that it will connect to a node in V−(or V+) if its label is positive (or negative). We call the selected nodes “outlier nodes”, and the other nodes are denoted as “clean nodes”. Please note that two identical nodes from the set of “outlier nodes” can have different labels, which suggests that one cannot ﬁnd any mapping from the “outlier nodes” to its label. Therefore, we evaluate the generalization only on these “clean nodes” but train the GNN on the mixture of “clean nodes” and “outlier nodes”. Figure 23 illustrates the phase transition of the sample complexity when γchanges. The number of sampled edges r= 20, pruning rate β = 0.2, the data dimension d= 50, and the number of patterns L= 200. We can see that the sample complexity remains almost the same when γis smaller than 0.3, which indicates that our theoretical insights still hold with a relaxed assumption (A1). 29𝒱𝒱+ 𝒱𝒱𝑁𝑁+ 𝒱𝒱−𝒱𝒱𝑁𝑁− Figure 22: Toy example of the data model. Node 1 and 2 have label +1. Nodes 3 and 4 are labeled as −1. Nodes 1 and 4 have class-relevant features. Nodes 2 and 3 have class- irrelevant features. V+ = {1}, VN+ = {2}, VN− = {3}, V−= {4}. 0 0.15 0.30 0.45 0.60 15 45 75 105 135 165 Figure 23: The number of training nodes |D|against γ.    1 0.86 0.73 0.63 Rate of sampled edges    0  0.6 0.84 0.93 0.97 0.99 Rate of pruned       neurons  (log scale) 0.2 0.25 0.3    1 0.86 0.73 0.63 Rate of sampled edges    0  0.6 0.84 0.93 0.97 0.99 Rate of pruned       neurons  (log scale) 0.2 0.25 0.3    1 0.86 0.73 0.63 Rate of sampled edges    0  0.6 0.84 0.93 0.97 0.99 Rate of pruned       neurons  (log scale)  0.28 0.3 0.32 0.34    1 0.86 0.73 0.63 Rate of sampled edges    0  0.6 0.84 0.93 0.97 0.99 Rate of pruned       neurons  (log scale)  0.28 0.3 0.32 0.34    1 0.86 0.73 0.63 Rate of sampled edges    0  0.6 0.84 0.93 0.97 0.99 Rate of pruned neurons  (log scale)  0.2 0.22 0.24 0.26    1 0.86 0.73 0.63 Rate of sampled edges    0  0.6 0.84 0.93 0.97 0.99 Rate of pruned neurons  (log scale)  0.2 0.22 0.24 0.26 Figure 24: Heatmaps depicting the test errors on Cora (sub-ﬁgures in the ﬁrst row), Citeseer (sub- ﬁgures in the second row), Pubmed (sub-ﬁgures in the third row) with different number of labeled nodes. F.5 A DDITIONAL EXPERIMENTS ON SMALL -SCALE REAL DATA In Figures 26-25, we implement the edge sampling methods together with magnitude-based neuron pruning methods. In Figures 26 and 24, the Uniﬁed GNN Sparsiﬁcation (UGS) in Chen et al. (2021b) is implemented as the edge sampling method 4, and the GNN model used here is the standard graph 4The experiments are implemented using the codes from https://github.com/VITA-Group/ Unified-LTH-GNN 30convolutional neural network (GCN) Kipf & Welling (2017) (two message passing). In Figure 25, we implement GraphSAGE with pooling aggregation as the sampling method 5. Except in Figure 24, we use 140 (Cora), 120 (Citeseer) and 60 (PubMed) labeled data for training, 500 nodes for validation and 1000 nodes for testing. In Figure 24, the number of training data is denoted in the title of each sub-ﬁgure.    1 0.86 0.73 0.63 Rate of sampled edges    0  0.6 0.84 0.93 0.97 0.99 Rate of pruned neurons  (log scale) 0.2 0.22 0.24 0.26 0.28    1 0.86 0.73 0.63 Rate of sampled edges    0  0.6 0.84 0.93 0.97 0.99 Rate of pruned neurons  (log scale) 0.28 0.3 0.32    1 0.86 0.73 0.63 Rate of sampled edges    0  0.6 0.84 0.93 0.97 0.99 Rate of pruned neurons  (log scale)  0.2 0.21 0.22 0.23 0.24 Figure 25: Node classiﬁcation performance of joint GraghSAGE-pooling and model pruning. Figure 26 show that both edge sampling using UGS and magnitude-based neuron pruning can reduce the test error on Cora, Citeseer, and Pubmed datasets, which justify our theoretical ﬁndings that joint sparsiﬁcation improves the generalization. In comparison, random pruning degrades the performance with large test errors than the baseline. Figures 24 and 25 show the test errors on Cora, Citeseer, and Pubmed datasets under different sampling and pruning rates, and darker colors denote lower errors. In all the sub-ﬁgures, we can observe that the joint edge sampling and pruning can reduce the test error, which justiﬁes the efﬁciency of joint edge-model sparsiﬁcation. For Figures 24, by comparing the performance of joint sparsiﬁcation on the same dataset but with different training samples, one can conclude that joint model-edge sparsiﬁcation with a smaller number of training samples can achieve similar or even better performance than that without sparsiﬁcation. 0.20.40.60.81 Rate of sampled edges 0.2 0.25 0.3 0.35 0.4Test error Cora UGS Random sampling Baseline 0.20.40.60.81 Rate of sampled edges 0.3 0.35 0.4Test error Citeseer UGS Random sampling Baseline 1 0.8 0.6 0.4 0.2 Rate of sampled edges 0.2 0.25 0.3Test error Pubmed UGS Random sampling Baseline 0.990.90 Model sparsity (log scale) 0.2 0.25 0.3 0.35 0.4Test error Magnitude-based model pruning Random pruning Baseline 0.990.90 Model sparsity (log scale) 0.3 0.35 0.4Test error Magnitude-based model pruning Random pruning Baseline 0.9990.990.90 Model sparsity (log scale) 0.2 0.25 0.3Test error Magnitude-based model pruning Random pruning Baseline Figure 26: Node classiﬁcation performance of GCN on Cora (sub-ﬁgures in the ﬁrst column), Citeseer (sub-ﬁgures in the second column), Pubmed (sub-ﬁgures in the third column). F.6 A DDITIONAL EXPERIMENTS ON LARGE -SCALE REAL DATA In this part, we have implemented the experiment on a large-scale protein dataset named Ogbn- Proteins Hu et al. (2020) and Citation dataset named Obgn-Arxiv Wang et al. (2020) via a 28- layer ResGCN. The experiments follow the same setup as Chen et al. (2021b) by implementing Uniﬁed GNN sparsiﬁcation (UGS) and magnitude-based neuron pruning method as the training 5The experiments are implemented using the codes from https://github.com/williamleif/ GraphSAGE 31algorithms. The Ogbn-proteins dataset is an undirected graph with 132,534 nodes and 39,561,252 edges. Nodes represent proteins, and edges indicate different types of biologically meaningful associations between proteins. All edges come with 8-dimensional features, where each dimension represents the approximate conﬁdence of a single association type and takes values between 0 and 1. The Ogbn-Arxiv dataset is a citation network with 169,343 nodes and 1,166,243 edges. Each node is an arXiv paper and each edge indicates that one paper cites another one. Each node comes with a 128-dimensional feature vector obtained by averaging the embeddings of words in its title and abstract. The test errors of training the ResGCN on the datasets above using joint edge-model sparsitiﬁcation are summarized in Figure 27. We can see that the joint model sparsiﬁcation can improve the generalization error, which have justiﬁed our theoretical insights of joint sparsiﬁcation in reducing the sample complexity. (1.00, 0.00) (0.85,0.49) (0.72,0.74) Rate of sampled edges   Rate of pruned neurons 0.15 0.152 0.154 0.156 0.158 0.16Test error Test error on Ogbn-Proteins (a) Ogbn-Proteins. (1.00, 0.00) (0.85,0.49) (0.72,0.74) Rate of sampled edges   Rate of pruned neurons 0.278 0.28 0.282 0.284 0.286 0.288Test error Test error on Ogbn-Arxiv (b) Ogbn-Arxiv. Figure 27: Test error on (a) Ogbn-Proteins and (b) Ogbn-Arxiv. G T HE VC DIMENSION OF THE GRAPH STRUCTURED DATA AND TWO -LAYER GNN Following the framework in Scarselli et al. (2018), we deﬁne the input data as (Gv,v), where Gv is the sub-graph with respect to node v. The input data distribution considered in this paper is denoted as G×V . To simplify the analysis, for any (Gv,v) ∈V, N(v) includes exactly one feature from {p+ ,p−}, and all the other node features comes from PN. Speciﬁcally, we summarize the VC dimension of the GNN model as follows. Deﬁnition 1 (Section 4.1, Scarselli et al. (2018)). Let gbe the GNN model and data set Dbe from G×V. gis said to shatter Dif for any set of binary assignmentsy ∈{0,1}|D|, there exist parameters Θ such that g(Θ; (Gv,v)) >0 if yv = 1 and g(Θ; (Gv,v)) <0 if yv = 0. Then, the VC-dim of the GNN is deﬁned as the size of the maximum set that can be shattered, namely, VC-dim(g) = max Dis shattered by g |D|. (49) Based on the deﬁnition above, we derive the VC-dimension of the GNN model over the data distribution considered in this paper, which is summarized in Theorem 3. Theorem 3 shows that the VC-dimension of the GNN model over the data distribution is at least2L/2 −1 , which is an exponential function of L. Inspired by Brutzkus & Globerson (2021) for CNNs, the major idea is to construct a special dataset that the neural network can be shattered. Here, we extend the proof by constructing a set of graph structured data as in (51). Then, for any combination of labels, from (54), one can construct a solvable linear system that can ﬁt such labels. Theorem 3 (VC-dim). Suppose the number of orthogonal features is L, then the VC dimension of the GNN model over the data G×V satisﬁes VC-dim(HGNN(G×V )) ≥2 L 2 −1 . (50) 32Proof. Without loss of generality, we denote the class irrelevant feature as {pi}L−2 i=1 . Therefore, P= {pi}L−2 i=1 ⋃p+ ⋃p−. Let us deﬁne a set Jsuch that J= {J ∈{0,1}L/2−1 |Ji = 0 or 1,1 ≤i≤L/2 −1}. Then, we deﬁne a subset D∈G×V with the size of 2L/2−1 based on the set J. For any (Gv,v), we consider the graph structured data Gv that connects to exactly L/2 −1 neighbors. Recall that for each node v, the feature matrix XN(v) must contain either p+ or p−, we randomly pick one node u∈N(v), and let xu be p+ or p−. Next, we sort all the other nodes in N(v) and label them as {vi}L/2−1 i=1 . Then, given an element J ∈J, we let the feature of node vi as xvi = Jip2i−1 + (1 −Ji)p2i, (51) where Ji is the i-th entry of vector J. we denote such constructed data (Gv,v) as DJ. Therefore, the data set Dis deﬁned as D= {DJ |J ∈J}. (52) Next, we consider the GNN model in the form of (20) with K = 2L/2−1. Given the label yJ for each data DJ ∈D, we obtain a group of αJ ∈R by solving the following equations ∑ J′∈J/J† αJ′ = yJ,∀J ∈J, (53) where J†= 1 −J. We can see that (53) can be re-written as   0 1 ··· 1 1 0 ··· 1 ... ... ... ... 1 1 ··· 0   ·   α1 α2 ... αL/2−1   =   y1† y2† ... y(L/2−1)†   (54) It is easy to verify that the matrix in (54) is invertible. Therefore, we have a unique solution of {αJ}J∈J given any ﬁxed {yJ}J∈J. As Kis selected as 2L/2−1, we denote the neuron weights as {wJ}J∈J and {uJ}J∈J by deﬁning wJ = max{αJ,0}· ∑ v′∈N(v),v∈DJ xv′, uJ = max{−αJ,0}· ∑ v′∈N(v),v∈DJ xv′. (55) Then, for each DJ ∈D, we substitute (55) into (20): g= ∑ J′∈J ( max v′∈N(v) σ ( ⟨wJ′ , xv′ ⟩ ) − max v′∈N(v) σ ( ⟨uJ′ , xv′ ⟩ )) . (56) When αJ′ ≥0, we have uJ′ = 0, and max v′∈N(v) σ ( ⟨wJ′ , xv′ ⟩ ) =αJ′·max ∑ u′∈N(u),u∈DJ xu′ ∑ v′∈N(v),v∈DJ xv′ =αJ′·max i (Jip2i−1 + (1 −Ji)p2i) ·(J′ ip2i−1 + (1 −J′ i)p2i) = {0, if J′= 1 −J αJ′, otherwise , (57) 33where the second equality comes from (51), and the last equality comes from the orthogonality of p ∈P. Similar to (57), when αJ′ <0, we have − max v′∈N(v) σ ( ⟨uJ′ , xv′ ⟩ ) = −(−αJ′) ·max ∑ u′∈N(u),u∈DJ xu′ ∑ v′∈N(v),v∈DJ xv′ =αJ′·max i (Jip2i−1 + (1 −Ji)p2i) ·(J′ ip2i−1 + (1 −J′ i)p2i) = {0, if J′= 1 −J αJ′,otherwise . (58) Therefore, (55) can be calculated as g= ∑ J′∈J ( max v′∈N(v) σ ( ⟨wJ′ , xv′ ⟩ ) − max v′∈N(v) σ ( ⟨uJ′ , xv′ ⟩ )) = ∑ J′∈J/J† αJ′ = yI, (59) which completes the proof. G.1 T HE IMPORTANCE SAMPLING PROBABILITY FOR VARIOUS SAMPLING STRATEGY In this part, we provide the bound of αfor different sampling strategies. For simpliﬁcation, we consider a graph such that every node degree is R(R>r ), and we sample ﬁxed rneighbor nodes for any node v∈V. Lemma 9 provides the value of αfor uniform sampling strategy, and αis in the order of r/Rand depends on the average degree of nodes containing class-relevant patterns. Formally, we have Lemma 9. For uniform sampling strategy such that all neighbor nodes are sampled with the same probability, we have 1 − ( 1 − ¯c R )r ≤Eα≤1 − ( 1 − ¯c R−¯c+ 1 )r , (60) where ¯cis the degree of the nodes in D∩(V+ ∪V−). Corollary 9.1. For uniform sampling strategy, when the node inVN connects to exactly one node in V+ ∩V−, then Eα= r R. Corollary 9.2. For uniform sampling strategy,Eα≥¯cr R when r≪R. Remark 9.1: Given α= r R, (6) yields the same sample complexity bound for all r, which leads to no improvement by using graph sampling. In addition, from (7), we can see that the required number of iterations increase by a factor of R/r, while the sampled edges during each iteration is reduced by a factor of r/R. Considering the computational resources used in other steps, we should expect an increased total computational time. Remark 9.2: From Corollary 9.2, we can see that uniform sampling can save the sample complexity by a factor of 1 ¯c2 when r≪Ron average. However, one realization of αmay have large variance with too small r. In practice, a medium range of ris desired for both saving sample complexity and algorithm stability. For FastGCN Chen et al. (2018), each node is assigned with a sampling probability. We consider a simpliﬁed version of such importance sampling approach by dividing training dataDinto two subsets DI and DU, and the sampling probability of the nodes in the same subset is identical. Let γbe the ratio of sampling probability of two groups, i.e., γ = Prob(v∈DIis sampled) Prob(v∈DUis sampled) (γ ≥1). Lemma 10 describes the importance sampling strategy when sampling class-relevant nodes is higher than sampling class-irrelevant nodes by a factor of γ. From Corollary 10.1, we know that αcan be improved over uniform sampling by a factor of γ. Formally, we have 34Lemma 10. Suppose DI = V+ ∪V−, and DU = VN, we have Eα≥ γr R−r+ γ. (61) Corollary 10.1. For the importance sampling strategy in Lemma 10,Eα≥γ·r R when R≫r. Remark 10.1: From Corollary 10.1, we can see that the applied importance sampling in Lemma 10 can save the sample complexity by a factor of 1 γ2 when r≪Ron average. Lemma 10 describes the importance sampling strategy when a fraction of nodes with class-irrelevant features are assigned with a high sampling probability. From Corollary 10.1, we know that αcan be improved over uniform sampling by a factor of γ 1+(γ−1)λ, where λis the fraction |VN ∩DI|/|VN|. Formally, we have Lemma 11. Suppose DI = V+ ∪V−∪Vλ, where Vλ ⊆VN with |Vλ|= λ|VN|. We have Eα≥ γr( 1 + (γ−1)λ ) R−r+ γ. (62) Corollary 11.1. For the importance sampling strategy in Lemma 11, Eα ≥ γ 1+(γ−1)λ ·r R when R≫r. Remark 11.1: From Corollary 11.1, we can see that the applied importance sampling in Lemma 11 can save the sample complexity by a factor of ( 1+(γ−1)λ γ )2 when r≪Ron average. H P ROOF OF USEFUL LEMMAS H.1 P ROOF OF LEMMA 1 The major idea is to obtain the probability of being a lucky neuron as shown in (66). Compared with the noiseless case, which can be easily solved by applying symmetric properties in Brutzkus & Globerson (2021), this paper takes extra steps to characterize the boundary shift caused by the noise in feature vectors, which are shown in (65). Proof of Lemma 1. For a random generated weights wk that belongs to Gaussian distribution, let us deﬁne the random variable ik such that ik = {1, if wk is the lucky neuron 0, otherwise . (63) Next, we provide the derivation of ik’s distribution. Letθ1 be the angle between p+ and the initial weights w. Let {θℓ}L ℓ=2 be the angles between p−and p ∈PN. Then, it is easy to verify that {θℓ}L ℓ=1 belongs to the uniform distribution on the interval [0,2π]. Because p ∈P are orthogonal to each other. Then, ⟨w , p ⟩with p ∈P are independent with each other given w belongs to Gaussian distribution. It is equivalent to saying that {θℓ}L ℓ=1 are independent with each other. Consider the noise level σ (in the order of 1/L) is signiﬁcantly smaller than 1, then the probability of a lucky neuron can be bounded as Prob ( θ1 + ∆θ≤θℓ −∆θ≤2π, 2 ≤ℓ≤L ) , (64) where ∆θ≂ σ. Then, we have Prob ( θ1 + ∆θ≤θℓ −∆θ≤2π, 2 ≤ℓ≤L ) = L∏ ℓ=1 Prob ( θ1 + ∆θ≤θℓ −∆θ≤2π ) = (2π−θ1 −∆θ 2π )L−1 . (65) 35Next, we can bound the probability as Prob(w is the lucky neuron) = ∫ 2π 0 1 2π · (2π−θ1 −∆θ 2π )L−1 dθ1 ≃1 L · (2π−2∆θ 2π )L ≃1 L · ( 1 −Lσ π ) . (66) We know ik belongs to Bernoulli distribution with probability 1 L(1 −Lσ π ). By Hoeffding’s inequality, we know that 1 L ( 1 −Lσ π ) − √ Clog q K ≤ 1 K K∑ k=1 ik ≤1 L ( 1 −Lσ π ) + √ Clog q K (67) with probability at least q−C. Let K = D(ε−2 K L2 log q), we have 1 K K∑ k=1 ik ≥ ( 1 −εK −Lσ π ) ·1 L. (68) To guarantee the probability in (68) is positive, the noise level needs to satisfy σ <π L. (69) H.2 P ROOF OF LEMMA 2 The bound of the gradient is divided into two parts in (74), where I1 and I2 concern the noiseless features and the noise, respectively. The term I2 can be bounded using standard Chernoff bound, see (80) for the ﬁnal result. While for I1, the gradient derived from D+ is always in the direction of p+ with a lucky neuron. Therefore, the neuron weights keep increasing in the direction of p+ from (83). Also, if ⟨wk , xv ⟩>0 for some yv = −1, the gradient derived from vwill force wk moving in the opposite directions of xv. Therefore, the gradient derived from D−only has a negative contribution in updating wk. For any pattern existing in {pv}v∈D−, which is equivalent to say for any p ∈P/p+, the value of ⟨wk , p ⟩has a upper bound, see (90). Proof of Lemma 2. For any v∈D+ and a lucky neuron with weights w(t) k , we have M(v; w(t) k ) = p+ + Mz(v; w(t) k ). (70) If the neighbor node with class relevant pattern is sampled in N(t)(v), the gradient is calculated as ∂g(W(t),U(t); XN(t)(v)) ∂wk ⏐⏐⏐ wk=w(t) k = p+ + Mz(v; w(t) k ). (71) For v∈D−and a lucky neuron w(t) k , we have M(t)(v; w(t) k ) ∈{xn}n∈N(v) ⋃ {0}, (72) and the gradient can be represented as ∂g(W(t),U(t); XN(t)(v)) ∂wk ⏐⏐⏐ wk=w(t) k =M(t)(v; w(t) k ) =M(t) p (v; w(t) k ) + M(t) z (v; w(t) k ). (73) 36Deﬁnition of I1 and I2. From the analysis above, we have ⟨w(t+1) k , p ⟩−⟨ w(t) k , p ⟩ = cη ·Ev∈D⟨yv ·M(t)(v; w(t) k ) , p ⟩ = cη · ( Ev∈D⟨yv ·M(t) p (v; w(t) k ) , p ⟩+ Ev∈D⟨yv ·M(t) z (v; w(t) k ) , p ⟩ ) := cη(I1 + I2). (74) Bound of I2. Recall that the noise factor zv are identical and independent with v, it is easy to verify that yv and Mz(v; w(t) k ) are independent. Then, I2 in (74) can be bounded as |I2|= |Ev∈Dyv ·Ev∈D⟨Mz(v; w(t) k ) , p ⟩| ≤|Ev∈Dyv|·|Ev∈D⟨Mz(v; w(t) k ) , p ⟩| ≤σ·|Ev∈Dyv|. (75) Recall that the number of sampled neighbor nodes is ﬁxed r. Hence, for any ﬁxed node v, there are at most (1 + r2) (including vitself) elements in { u ⏐⏐u∈D } are dependent with yv. Also, from assumption (A2), we have Prob(y = 1) = 1 2 and Prob(y = −1) = 1 2 . From Lemma 8, the moment generation function of ∑ v∈D(yv −Eyv) satisﬁes e ∑ v∈Ds(yv−Eyv) ≤eC(1+r2)|D|s2 , (76) where Eyv = 0 and Cis some positive constant. By Chernoff inequality, we have Prob {⏐⏐⏐⏐ ∑ v∈D (yv −E yv) ⏐⏐⏐⏐>th } ≤eC(1+r2)|D|s2 e|D|·th·s (77) for any s> 0. Let s= th/ ( C(1 + r2) ) and th= √ (1 + r2)|D|log q, we have ⏐⏐⏐ ∑ v∈D (yv −E yv) ⏐⏐⏐≤C √ (1 + r2)|D|log q (78) with probability at least 1 −q−c. From (A2) in Section 3.2, we know that |Eyv|≲ √ |D|. (79) Therefore, I2 is bounded as |I2|≤ ⏐⏐⏐ ∑ v∈D 1 |D|yv ⏐⏐⏐ ≤ ⏐⏐⏐ ∑ v∈D 1 |D|(yv −E yv) ⏐⏐⏐+ |E yv| ≲σ √ (1 + r2) logq |D| . (80) Bound of I1 when p = p+. Because the neighbors of node vwith negative label do not contain p+, Mp(v; w(t) k ) in (73) cannot be p+. In addition, p+ is orthogonal to {pn}n∈N(v) ⋃{0}. Then, we have ⣨ p+,∂g(W,U; XN(v)) ∂wk ⏐⏐⏐ wk=w(t) k ⟩ = ⟨p+ , Mz(v; w(t) k ) ⟩. (81) Let us use D(t) s to denote the set of nodes that the class relevant pattern is included in the sampled neighbor nodes. Recall that α, deﬁned in Section 3, is the sampling rate of nodes with important edges, we have |D(t) s |= α|D+|. (82) 37Combining (71), (73) and (81), we have I1 = ⟨Ev∈D+ Mp(v; w(t) k ) −Ev∈D−Mp(v; w(t) k ) , p+ ⟩ = ⟨Ev∈D+ Mp(v; w(t) k ) , p+ ⟩ ≥⟨ Ev∈DsMp(v; w(t) k ) , p+ ⟩ ≥α. (83) Bound of I1 when p ̸= p+. Next, for any p ∈P/P+, we will prove the following equation: I1 ≤ √ (1 + r2) logq |D| . (84) When ⟨w(t+1) k , p ⟩≤− σ, we have M(t) p (v; w(t) k ) ̸= p, and ⟨M(t) p (v; w(t) k ) , p ⟩= 0 for any v∈D. (85) Therefore, we have I1 = ⟨Ev∈D+ M(t) p (v; w(t) k ) −Ev∈D−M(t) p (v; w(t) k ) , p ⟩= 0 . (86) When ⟨w(t+1) k , p ⟩>−σ, we have I1 = ⟨Ev∈D+ M(t) p (v; w(t) k ) −Ev∈D−M(t) p (v; w(t) k ) , p ⟩ = ⟨Ev∈D+/DsM(t) p (v; w(t) k ) −Ev∈D−M(t) p (v; w(t) k ) , p ⟩. (87) Let us deﬁne a mapping H: Rd −→Rd such that H(pv) = {p− if pv = p+ pv otherwise , (88) Then, we have I1 =⟨Ev∈D+ M(t) p (v; w(t) k ) −Ev∈D−M(t) p (v; w(t) k ) , p ⟩ =⟨Ev∈D+/DsM(t) p (v; w(t) k ) −Ev∈D−M(t) p (v; w(t) k ) , p ⟩ =⟨Ev∈D+/DsM(t) p (H(v); w(t) k ) −Ev∈D−M(t) p (v; w(t) k ) , p ⟩ ≤⟨Ev∈D+ M(t) p (H(v); w(t) k ) −Ev∈D−M(t) p (v; w(t) k ) , p ⟩ =⟨Ev∈D+ M(t) p (H(v); w(t) k ) −Ev∈D−M(t) p (H(v); w(t) k ) , p ⟩ (89) where the third equality holds because N(t)(v) does not contain p+ for v ∈ D+/Ds, and H(XN(t)(v)) = XN(t)(v). Also, the last equality holds because xv does not contain p+ for node v∈D−. Therefore, we have ⟨Ev∈D+ M(t) p (H(v); w(t) k ) −Ev∈D−M(t) p (H(v); w(t) k ) , p ⟩ =Ev∈D⟨yvM(t) p (H(v); w(t) k ) , p ⟩ ≤ ⏐⏐Ev∈Dyv ⏐⏐· ⏐⏐Ev∈D⟨M(t) p (H(v); w(t) k ) , p ⟩ ⏐⏐ ≤ √ (1 + r2) logq |D| (90) with probability at least 1 −q−C for some positive constant. Proof of statement 1. From (80) and (83), the update of wk in the direction of p+ is bounded as ⟨w(t+1) k , p+ ⟩−⟨ w(t) k , p+ ⟩≥ cη(I1 −|I2|) ≥cη ·(α−σ √ (1 + r2) logq |D| ). (91) 38Then, we have ⟨w(t) k , p+ ⟩−⟨ w(0) k , p+ ⟩≥ cη ·(α−σ √ (1 + r2) logq |D| ) ·t. (92) Proof of statement 2. From (80) and (90), the update of wk in the direction of p ∈P/p+ is upper bounded as ⟨w(t+1) k , p ⟩−⟨ w(t) k , p ⟩≤ cη(I1 + |I2|) ≤cη ·(1 + σ) · √ (1 + r2) logq |D| . (93) Therefore, we have ⟨w(t) k , p ⟩−⟨ w(0) k , p ⟩≤ cη ·(1 + σ) · √ (1 + r2) logq |D| ·t. (94) The update of wk in the direction of p ∈P/p+ is lower bounded as ⟨w(t+1) k , p ⟩−⟨ w(t) k , p ⟩≥ cη(I1 −|I2|) ≥− cη · ( 1 + σ· √ (1 + r2) logq |D| ) . (95) To derive the lower bound, we prove the following equation via mathematical induction: ⟨w(t) , p ⟩≥− cη ( 1 + σ+ σt· √ (1 + r2) logq |D| ) . (96) It is clear that (96) holds when t= 0. Suppose (96) holds for t. When ⟨w(t) k , p ⟩≤− σ, we have M(t) p (v; w(t) k ) ̸= p, and ⟨M(t) p (v; w(t) k ) , p ⟩= 0 for any v∈D. (97) Therefore, we have I1 = ⟨Ev∈D+ M(t) p (v; w(t) k ) −Ev∈D−M(t) p (v; w(t) k ) , p ⟩= 0 , (98) and ⟨w(t+1) k , p ⟩= ⟨w(t) k , p ⟩+ cη ·(I1 + I2) = ⟨w(t) k , p ⟩+ cη ·I2 ≥−cη · ( 1 + σ cη + σt· √ (1 + r2) logq |D| ) −cη √ (1 + r2) logq |D| = −cη · ( 1 + σ cη + σ(t+ 1) · √ (1 + r2) logq |D| ) . (99) When ⟨w(t) k , p ⟩>−σ, we have ⟨w(t+1) k , p ⟩ = ⟨w(t) k , p ⟩+ cη(I1 + I2) = ⟨w(t) k , p ⟩+ cη ·⟨Ev∈D+ M(t) p (v; w(t) k ) −Ev∈D−M(t) p (v; w(t) k ) , p ⟩+ cηI2 = ⟨w(t) k , p ⟩−cη ·⟨Ev∈D+ M(t) p (v; w(t) k ) , p ⟩+ cηI2 ≥− σ−cη −cησ √ (1 + r2) logq |D| . (100) Therefore, from (99) and (100), we know that (96) holds for t+ 1. 39H.3 P ROOF OF LEMMA 4 The bound of the gradient is divided into two parts in (101), where I3 and I4 are respect to the noiseless features and the noise, respectively. The bound for I4 is similar to that for I2 in proving Lemma 2. However, for a unlucky neuron, the gradient derived from D+ is not always in the direction of p+. We need extra techniques to characterize the offsets between D+ and D−. One critical issue is to guarantee the independence of M(t)(v) and yv, which is solved by constructing a matched data, see (108) for the deﬁnition. We show that the magnitude of w(t) k scale in the order of √ (1 + r2)/|D| from (113). Proof of Lemma 4. Deﬁnition of I1 and I2. Similar to (74), we deﬁne the items I3 and I4 as ⟨w(t+1) k , p ⟩−⟨ w(t) k , p ⟩ = cη ·Ev∈D⟨yv ·M(t) p (v; w(t) k ) , p ⟩+ Ev∈D⟨yv ·M(t) z (v; w(t) k ) , p ⟩ := cη ·(I3 + I4). (101) Bound of I4. Following the similar derivation of (80), we can obtain |I4|≲ σ √ (1 + r2) logq |D| . (102) Bound of I3 when p = p+. From (101), we have I3 = ⟨Ev∈D+ M(t) p (v; w(t) k ) −Ev∈D−M(t) p (v; w(t) k ) , p+ ⟩ = ⟨Ev∈D+ M(t) p (v; w(t) k ) , p+ ⟩ ≥0. (103) Bound of I3 when p ∈PN. To boundI4 for any ﬁxed p ∈PN, we maintain the subsetsSk(t) ⊆D− such that Sk(t) = {v∈D−|M(t) p (v; w(t) k ) = p−}. First, when Sk(t) = ∅, it is easy to verify that I3 = 0. (104) Therefore, Sk(t+ 1) = ∅and Sk(t′) = ∅for all t′≥t. Second, when Sk(t) ̸= ∅, then we have I3 = −|Sk(t)| |D−| ∥p−∥2. (105) Note that if ⟨w(t) k , p− ⟩ < −σ, then Sk(t) must be an empty set. Therefore, after at most t0 = ∥w(0) k ∥2+σ cη number of iterations, Sk(t0) = ∅, and Sk(t) = ∅with t≥t0 from (104). Next, for some large t0 and any v∈V, we deﬁne a mapping F: V−→Rrd such that F(v) = [ o⊤ v1 o⊤ v2 ··· o⊤ vr ]⊤ . (106) From (101), we have I4 = Ev∈V⟨yvM(t) p (v; w(t) k ) , p ⟩, (107) where {v1,··· ,vr}= N(t)(v) and {ovi = xvi, if xvi ̸= p+ or p− ovi = 0, if xvi = p+ or p− . (108) When y= 1, we have ⟨M(t) p (x) , p ⟩= { ⟨M(t) p (F(v)) , p ⟩, if M(t) p (F(v)) ̸= p+ 0 ≤⟨M (t) p (F(v)) , p ⟩, if M(t) p (F(v)) = p+ . (109) 40When y= −1 and t≥t0, recall that Sk(t) = D−, then we have ⟨M(t) p (x) , p ⟩= ⟨M(t) p (F(x)) , p ⟩. (110) Combining (109) and (110), we have I3 ≤Ev∈V [ y·⟨M(t) p (F(v)) , p ⟩ ] . (111) From assumptions (A2) and (17), we know that the distribution of p+ and p−are identical, and the distributions of any class irrelevant patterns p ∈PN are independent with y. Therefore, it is easy to verify that yand F(v) are independent with each other, then we have I3 ≤Ev∈Dyv ·Ev⟨M(F(v)) , p ⟩. (112) From (112) and (78), we have I3 ≤ √ (1 + r2) logq |D| (113) for any p ∈PN. Proof of statement 1. From (102) and (103), we have ⟨w(t+1) k , p ⟩−⟨ w(t) k , p ⟩= cη ·(I3 + I4) ≥− cη|I4| ≥− cησ √ (1 + r2) logq |D| . (114) Therefore, we have ⟨w(t) k , p ⟩−⟨ w(0) k , p ⟩≥− cησ √ (1 + r2) logq |D| ·t. (115) Proof of statement 2. When p = p−, from the deﬁnition of I3 in (101), we know I3 = ⟨Ev∈D+ M(t) p (v; w(t) k ) −Ev∈D−M(t) p (v; w(t) k ) , p−⟩ = −⟨Ev∈D−M(t) p (v; D−) , p−⟩ ≤0, (116) The update of wk in the direction of p−is upper bounded as ⟨w(t+1) k , p−⟩−⟨ w(t) k , p−⟩≤ cη(I3 + I4) ≤cηI4 ≤cη ·σ· √ (1 + r2) logq |D| . (117) Therefore, we have ⟨w(t) k , p−⟩−⟨ w(0) k , p ⟩≤ cη ·σ· √ (1 + r2) logq |D| ·t. (118) To derive the lower bound, we prove the following equation via mathematical induction: ⟨w(t) , p−⟩≥− cη ( 1 + σ+ σt· √ (1 + r2) logq |D| ) . (119) It is clear that (119) holds when t= 0. Suppose (119) holds for t. When ⟨w(t) k , p−⟩≤− σ, we have M(t) p (v; w(t) k ) ̸= p−, and ⟨M(t) p (v; w(t) k ) , p−⟩= 0 for any v∈D. (120) 41Therefore, we have I3 = ⟨Ev∈D+ M(t) p (v; w(t) k ) −Ev∈D−M(t) p (v; w(t) k ) , p−⟩= 0 , (121) and ⟨w(t+1) k , p−⟩= ⟨w(t) k , p−⟩+ cη ·(I3 + I4) = ⟨w(t) k , p−⟩+ cη ·I4 ≥−cη · ( 1 + σ cη + σt· √ (1 + r2) logq |D| ) −cη √ (1 + r2) logq |D| = −cη · ( 1 + σ cη + σ(t+ 1) · √ (1 + r2) logq |D| ) . (122) When ⟨w(t) k , p−⟩>−σ, we have ⟨w(t+1) k , p−⟩ = ⟨w(t) k , p−⟩+ cη(I3 + I4) = ⟨w(t) k , p−⟩+ cη ·⟨Ev∈D+ M(t) p (v; w(t) k ) −Ev∈D−M(t) p (v; w(t) k ) , p−⟩+ cηI4 = ⟨w(t) k , p−⟩−cη ·⟨Ev∈D+ M(t) p (v; w(t) k ) , p−⟩+ cηI4 ≥− σ−cη −cησ √ (1 + r2) logq |D| . (123) Therefore, from (122) and (123), we know that (119) holds for t+ 1. Proof of statement 3. From (102) and (113), we have |⟨w(t+1) k , p ⟩−⟨ w(t) k , p ⟩|= cη ·|I3 + I4| ≤cη ·(1 + σ) · √ (1 + r2) logq |D| . (124) Therefore, we have |⟨w(t) k , p ⟩−⟨ w(0) k , p ⟩|≤ cη ·(1 + σ) · √ (1 + r2) logq |D| ·t. (125) H.4 P ROOF OF LEMMA 6 The major idea is to show that the weights of lucky neurons will keep increasing in the direction of class-relevant patterns. Therefore, the lucky neurons consistently select the class-relevant patterns in the aggregation function. Proof of Lemma 6. For any k∈U(0), we have ⟨w(0) k , (1 −σ) ·p+ ⟩≥⟨ w(0) k , (1 + σ) ·p ⟩ (126) for any p ∈P/p+ by the deﬁnition of lucky neuron in (24). Next, from Lemma 2, we have ⟨w(t) k , p+ ⟩≥ cη ( α−σ √ (1 + r2) logq |D| ) ·t, and ⟨w(t) k , p ⟩≤ cη(1 + σ) · √ (1 + r2) logq |D| ·t. (127) 42Combining (126) and (127), we have ⟨w(t) k , p+ ⟩− 1 + σ 1 −σ⟨w(t) k , p ⟩≳ cη · ( α−(1 + 4σ) · √ (1 + r2) logq |D| ) ·t. (128) When |D|≳ α−2 ·(1 + r2) logq, we have ⟨w(t) k , p+ ⟩≥ 1+σ 1−σ⟨w(t) k , p ⟩. Therefore, we have k∈W(t) and W(0) ⊆W(t). (129) One can derive the proof for U(t) following similar steps above. H.5 P ROOF OF LEMMA 7 The proof is built upon the statements of the lucky neurons and unlucky neurons in Lemmas 2 and 4. The magnitude of the lucky neurons in the direction ofp+ is in the order of α, while the magnitude of unlucky neurons is at most 1/ √ |D|. Given a sufﬁciently large |D|, the magnitude of a lucky neuron is always larger than that of an unlucky neuron. Proof of Lemma 7. From Lemma 6, we know that (1) if w(0) k1 is lucky neuron, w(0) k1 is still lucky neuron for any t′≥0; (2) if w(t′) k1 is unlucky neuron, w(t′′) k1 is still unlucky neuron for any t′′≤t′ For any k1 ∈W(0), from Lemma 2, we have ⟨w(t′) k1 , w(t′) k1 ⟩ 1 2 ≥⟨w(t′) k1 , p+ ⟩ ≥cη · ( α−σ √ (1 + r2) logq |D| ) t′. (130) For any k2 ∈Wc(t′), from Lemma 2, we have ⟨w(t′) k2 , w(t′) k2 ⟩ 1 2 = ∑ p∈P ⟨w(t′) k1 , p ⟩ = ∑ p∈P/p+ ⟨w(t′) k1 , p ⟩+ ⟨w(t′) k1 , p+ ⟩ ≤ ∑ p∈P/p+ ⟨w(t′) k1 , p ⟩+ max p∈P/p+ ⟨w(t′) k1 , p ⟩ ≤L·cη ·(1 + σ) · √ (1 + r2) logq |D| ·t′. (131) Therefore, ⟨w(t′) k2 , w(t′) k2 ⟩ 1 2 <⟨w(t′) k1 , w(t′) k1 ⟩ 1 2 if |D|is greater than α−2(1 + r2)L2 log q. H.6 P ROOF OF LEMMA 8 Proof of Lemma 8 . According to the Deﬁnitions in Janson (2004), there exists a family of {(Xj,wj)}j, where Xj ⊆X and wj ∈[0,1], such that ∑ jwj ∑ xnj∈Xj xnj = ∑N n=1 xn, and∑ jwj ≤dX by equations (2.1) and (2.2) in Janson (2004). Then, let pj be any positive numbers with ∑ jpj = 1. By Jensen’s inequality, for anys∈R, we have es∑N n=1 xn = e ∑ jpj swj pj Xj ≤ ∑ j pje swj pj Xj , (132) where Xj = ∑ xnj∈Xj xnj. 43Then, we have EXes∑N n=1 xn ≤EX ∑ j pje swj pj Xj = ∑ j pj ∏ Xj EXe swj pj xnj ≤ ∑ j pj ∏ Xj e Cw2 j p2 j s2 ≤ ∑ j pje C|Xj|w2 j p2 j s2 . (133) Let pj = wj|Xj|1/2 ∑ jwj|Xj|1/2 , then we have EXes∑N n=1 xn ≤ ∑ j pjeC (∑ jwj|Xj|1/2 )2 s2 = eC (∑ jwj|Xj|1/2 )2 s2 . (134) By Cauchy-Schwarz inequality, we have (∑ j wj|Xj|1/2)2 ≤ ∑ j wj ∑ j wj|Xj|≤ dXN. (135) Hence, we have EXes∑N n=1 xn ≤eCdXNs2 . (136) I E XTENSION TO MULTI -CLASS CLASSIFICATION Consider the classiﬁcation problem with four classes, we use the label y ∈{+1,−1}2 to denote the corresponding class. Similarly to the setup in Section 2, there are four orthogonal class relevant patterns, namely p1,p2,p3,p4. In the ﬁrst layer (hidden layer), we have K neurons with weights wk ∈Rd. In the second layer (linear layer), the weights are denoted as bk ∈R2 for the k-th neuron. Let W ∈Rd×K and B ∈R2×K be the collections of wk’s andbk’s, respectively. Then, the output of the graph neural network, denoted as g ∈R2, is calculated as g(W,B; XN(v)) = 1 K K∑ k=1 bk ·AGG(XN(v),wk). (137) Then, the label generated by the graph neural network is written as yest = sign(g). (138) Given the set of data D, we divide them into four groups as Hidden Layer Output  Layer 𝑦𝑦𝑣𝑣 = 𝑦𝑦𝑣𝑣,1 𝑦𝑦𝑣𝑣,2 𝑾𝑾 𝒃𝒃1 Aggregation 𝒙𝒙𝑣𝑣 𝑿𝑿𝒩𝒩(𝑣𝑣) ReLU Positive label Negative label Unknown label 𝒃𝒃2 Figure 28: Illustration of graph neural network learning for multi-class classiﬁcation D1 = {(x,y) |y = (1,1)}, D2 = {(x,y) |y = (1,−1)}, D3 = {(x,y) |y = (−1,1)}, and D4 = {(x,y) |y = (−1,−1)}. (139) 44The corresponding loss function in (2) can be revised as ˆfD(W,U) = − 1 |D| ∑ v∈D y⊤ v g ( W,b; XN(v) ) (140) Then, we initialize the weights of bk as (1,1),(1,−1),(−1,1),and (−1,−1) equally. Next, we divide the weights wk into four groups based on the value of bk, such that W1 = {k|bk = (1,1)}, W2 = {k|bk = (1,−1)}, W3 = {k|bk = (−1,1)}, and W4 = {k|bk = (−1,−1)}. (141) Hence, for any kin W1, we have ∂f ∂w = − 1 |D| ∑ v∈D ( yv,1 ·∂g1 ∂w −yv,2 ·∂g2 ∂w ) . (142) When x∈D1, we have yv,1 ·∂g1 ∂w −yv,2 ·∂g2 ∂w = −2φ′(w⊤x) ·x. (143) When x∈D2, we have yv,1 ·∂g1 ∂w −yv,2 ·∂g2 ∂w = 0. (144) When x∈D3, we have yv,1 ·∂g1 ∂w −yv,2 ·∂g2 ∂w = 0. (145) When x∈D4, we have yv,1 ·∂g1 ∂w −yv,2 ·∂g2 ∂w = 2φ′(w⊤x) ·x. (146) Then, for any k∈W1, we have w(t+1) k = w(t) k + cη ·Ev∈D1 M(t)(v; w(t) k ) −cη ·Ev∈D4 M(t)(v; w(t) k ), (147) Comparing (147) with (29), one can derived that the weights w(t) k will update mainly along the direction of p1 but have bounded magnitudes in other directions. By following the steps in (142) to (147), similar results can be derived for for any k∈Wi with 1 ≤i≤4. 45",
      "meta_data": {
        "arxiv_id": "2302.02922v1",
        "authors": [
          "Shuai Zhang",
          "Meng Wang",
          "Pin-Yu Chen",
          "Sijia Liu",
          "Songtao Lu",
          "Miao Liu"
        ],
        "published_date": "2023-02-06T16:54:20Z",
        "venue": "The Eleventh International Conference on Learning Representations,\n  2023",
        "pdf_url": "https://arxiv.org/pdf/2302.02922v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper provides the first theoretical generalization analysis of joint edge-model sparse learning for Graph Neural Networks (GNNs), focusing on sample complexity and convergence rate in achieving zero generalization error. It analytically proves that both sampling important nodes (edge sparsification) and pruning lowest-magnitude neurons (model sparsification) can reduce the sample complexity and improve convergence speed without compromising test accuracy. The work offers explicit bounds for the required number of known labels and the convergence rate of SGD, demonstrating that joint sparsification is a 'win-win' strategy.",
        "methodology": "The theoretical analysis is centered on a one-hidden-layer GNN with ReLU activation and a max-pooling aggregator function, addressing a semi-supervised node classification problem. The learning algorithm employs mini-batch Stochastic Gradient Descent (SGD) in two stages: a short pre-training phase, followed by magnitude-based neuron pruning (setting a β fraction of lowest-magnitude neurons' weights to zero) and then re-training of the pruned model. Edge sparsification is achieved by node sampling (e.g., GraphSAGE framework, sampling 'r' neighbors) and an importance sampling strategy where class-relevant features are sampled more frequently. The data model assumes node features are noisy realizations of orthogonal class-relevant and class-irrelevant patterns, with labels determined by class-relevant features of neighbors.",
        "experimental_setup": "The research validates its findings using both synthetic and real-world datasets. For synthetic data, a graph with 10,000 nodes and degree 30 was generated, using one-hot vectors for class-relevant patterns and None space vectors for class-irrelevant ones, with Gaussian noise. Real-world experiments were conducted on small-scale citation datasets (Cora, Citeseer, Pubmed) using a standard GCN (two-message passing) and on large-scale datasets (Ogbn-Arxiv, Ogbn-Proteins) using a 28-layer ResGCN. Edge sampling methods like Uniﬁed GNN Sparsiﬁcation (UGS) and GraphSAGE-pooling were employed. Validation was performed by measuring test error, learning success rate, sample complexity phase transitions, and convergence rates across varying sampling probabilities (α) and pruning rates (β).",
        "limitations": "The theoretical analysis is primarily centered on two-layer GNNs (specifically, one-hidden-layer GNNs) with structural constraints on data, such as assumptions (A1) about specific graph connectivity patterns (e.g., no edges between V+ and VN-) and (A2) regarding balanced positive and negative labels in the training set (though relaxations are mentioned). The data model initially assumes orthogonal patterns for simplicity, although this constraint is relaxed in synthetic experiments. The derived bounds, particularly for convergence rate, hold as long as the pruning rate β does not exceed a threshold close to 1.",
        "future_research_directions": "Future research directions include generalizing the current theoretical analysis to multi-layer GNN architectures, exploring the impact and provable efficiency of other graph sampling strategies (such as FastGCN), and extending the analysis to different graph learning tasks like link-based classification problems."
      }
    },
    {
      "title": "A Unified Lottery Ticket Hypothesis for Graph Neural Networks",
      "abstract": "With graphs rapidly growing in size and deeper graph neural networks (GNNs)\nemerging, the training and inference of GNNs become increasingly expensive.\nExisting network weight pruning algorithms cannot address the main space and\ncomputational bottleneck in GNNs, caused by the size and connectivity of the\ngraph. To this end, this paper first presents a unified GNN sparsification\n(UGS) framework that simultaneously prunes the graph adjacency matrix and the\nmodel weights, for effectively accelerating GNN inference on large-scale\ngraphs. Leveraging this new tool, we further generalize the recently popular\nlottery ticket hypothesis to GNNs for the first time, by defining a graph\nlottery ticket (GLT) as a pair of core sub-dataset and sparse sub-network,\nwhich can be jointly identified from the original GNN and the full dense graph\nby iteratively applying UGS. Like its counterpart in convolutional neural\nnetworks, GLT can be trained in isolation to match the performance of training\nwith the full model and graph, and can be drawn from both randomly initialized\nand self-supervised pre-trained GNNs. Our proposal has been experimentally\nverified across various GNN architectures and diverse tasks, on both\nsmall-scale graph datasets (Cora, Citeseer and PubMed), and large-scale\ndatasets from the challenging Open Graph Benchmark (OGB). Specifically, for\nnode classification, our found GLTs achieve the same accuracies with 20%~98%\nMACs saving on small graphs and 25%~85% MACs saving on large ones. For link\nprediction, GLTs lead to 48%~97% and 70% MACs saving on small and large graph\ndatasets, respectively, without compromising predictive performance. Codes\navailable at https://github.com/VITA-Group/Unified-LTH-GNN.",
      "full_text": "A Uniﬁed Lottery Ticket Hypothesis for Graph Neural Networks Tianlong Chen * 1 Yongduo Sui* 2 Xuxi Chen 1 Aston Zhang 3 Zhangyang Wang1 Abstract With graphs rapidly growing in size and deeper graph neural networks (GNNs) emerging, the training and inference of GNNs become increas- ingly expensive. Existing network weight prun- ing algorithms cannot address the main space and computational bottleneck in GNNs, caused by the size and connectivity of the graph. To this end, this paper ﬁrst presents a uniﬁed GNN spar- siﬁcation (UGS) framework that simultaneously prunes the graph adjacency matrix and the model weights, for effectively accelerating GNN infer- ence on large-scale graphs. Leveraging this new tool, we further generalize the recently popular lottery ticket hypothesisto GNNs for the ﬁrst time, by deﬁning a graph lottery ticket (GLT) as a pair of core sub-dataset and sparse sub-network, which can be jointly identiﬁed from the original GNN and the full dense graph by iteratively applying UGS. Like its counterpart in convolutional neu- ral networks, GLT can be trained in isolation to match the performance of training with the full model and graph, and can be drawn from both ran- domly initialized and self-supervised pre-trained GNNs. Our proposal has been experimentally veriﬁed across various GNN architectures and di- verse tasks, on both small-scale graph datasets (Cora, Citeseer and PubMed), and large-scale datasets from the challenging Open Graph Bench- mark (OGB). Speciﬁcally, for node classiﬁcation, our found GLTs achieve the same accuracies with 20% ∼98% MACs saving on small graphs and 25% ∼85% MACs saving on large ones. For link prediction, GLTs lead to 48% ∼97% and 70% MACs saving on small and large graph datasets, respectively, without compromising predictive performance. Codes are at https://github. com/VITA-Group/Unified-LTH-GNN . *Equal contribution 1Department of Electrical and Computer Engineering, University of Texas at Austin 2University of Science and Technology of China 3AWS Deep Learning. Correspondence to: Zhangyang Wang <atlaswang@utexas.edu>. Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s). GNN Sparsity (%) GNN Sparsity (%)0.00 0.00 83.22 94.5097.7559.04 86.58 Figure 1.Summary of our achieved performance (y-axis) at differ- ent graph and GNN sparsity levels (x-axis) on Cora and Citeceer node classiﬁcation. The size of markers represent the inference MACs (= 1 2 FLOPs) of each sparse GCN on the corresponding sparsiﬁed graphs. Black circles (•) indicate the baseline, i.e., un- pruned dense GNNs on the full graph. Blue circles (•) are random pruning results. Orange circles (•) represent the performance of a previous graph sparsiﬁcation approach, i.e., ADMM (Li et al., 2020b). Red stars (#) are established by our method (UGS). 1. Introduction Graph Neural Networks (GNNs) (Zhou et al., 2018; Kipf & Welling, 2016; Chen et al., 2019; Veliˇckovi´c et al., 2017) have established state-of-the-art results on various graph- based learning tasks, such as node or link classiﬁcation (Kipf & Welling, 2016; Veliˇckovi´c et al., 2017; Qu et al., 2019; Verma et al., 2019; Karimi et al., 2019; You et al., 2020d;c), link prediction (Zhang & Chen, 2018), and graph classiﬁca- tion (Ying et al., 2018; Xu et al., 2018; You et al., 2020b). GNNs’ superior performance results from the structure- aware exploitation of graphs. To update the feature of each node, GNNs ﬁrst aggregate features from neighbor connected nodes, and then transform the aggregated embed- dings via (hierarchical) feed-forward propagation. However, the training and inference of GNNs suffer from the notorious inefﬁciency, and pose hurdle to GNNs from being scaled up to real-world large-scale graph applications. This hurdle arises from both algorithm and hardware lev- els. On the algorithm level, GNN models can be thought of a composition of traditional graphs equipped with deep neural network (DNN) algorithms on vertex features. The execution of GNN inference falls into three distinct cat- arXiv:2102.06790v2  [cs.LG]  7 Jun 2021A Uniﬁed Lottery Ticket Hypothesis for Graph Neural Networks egories with unique computational characteristics: graph traversal, DNN computation, and aggregation. Especially, GNNs broadly follow a recursive neighborhood aggrega- tion (or message passing) scheme, where each node ag- gregates feature vectors of its multi-hop neighbors to com- pute its new feature vector. The aggregation phase costs massive computation when the graphs are large and with dense/complicated neighbor connections (Xu et al., 2018). On the hardware level, GNN’s computational structure de- pends on the often sparse and irregular structure of the graph adjacency matrices. This results in many random memory accesses and limited data reuse, but also requires relatively little computation. As a result, GNNs have much higher inference latency than other neural networks, limiting them to applications where inference can be pre-computed ofﬂine (Geng et al., 2020; Yan et al., 2020). This paper aims at aggressively trimming down the explo- sive GNN complexity, from the algorithm level. There are two streams of works: simplifying the graph, or simplifying the model. For the ﬁrst stream, many have explored vari- ous sampling-based strategies (H¨ubler et al., 2008; Chakeri et al., 2016; Calandriello et al., 2018; Adhikari et al., 2017; Leskovec & Faloutsos, 2006; V oudigari et al., 2016; Eden et al., 2018; Zhao, 2015; Chen et al., 2018a), often combined with mini-batch training algorithms for locally aggregating and updating features. Zheng et al. (2020) investigated graph sparsiﬁcation, i.e., pruning input graph edges, and learned an extra DNN surrogate. Li et al. (2020b) also addressed graph sparsiﬁcation by formulating an optimiza- tion objective, solved by alternating direction method of multipliers (ADMM) (Bertsekas & Rheinboldt, 1982). The second stream of efforts were traditionally scarce, since the DNN parts of most GNNs are (comparably) lightly pa- rameterized, despite the recent emergence of increasingly deep GNNs (Li et al., 2019). Although model compression is well studied for other types of DNNs (Cheng et al., 2017), it has not been discussed much for GNNs. One latest work (Tailor et al., 2021) explored the viability of training quan- tized GNNs, enabling the usage of low precision integer arithmetic during inference. Other forms of well-versed DNN compression techniques, such as model pruning (Han et al., 2016), have not been exploited for GNNs up to our best knowledge. More importantly, no prior discussion was placed on jointly simplifying the input graphs and the mod- els for GNN inference. In view of such, this paper asks: to what extent could we co-simplify the input graph and the model, for ultra-efﬁcient GNN inference? 1.1. Summary of Our Contributions This paper makes multi-fold contributions to answer the above questions. Unlike pruning convolutional DNNs which are heavily overparameterized, directly pruning the much less parameterized GNN model would have only limited room to gain. Our ﬁrst technical innovation is to for the ﬁrst time present an end-to-end optimization framework called uniﬁed GNN sparsiﬁcation (UGS) that simultane- ously prunes the graph adjacency matrix and the model weights. UGS makes no assumption to any GNN architec- ture or graph structure, and can be ﬂexibly applied across various graph-based learning scenarios at scale. Considering UGS as the generalized pruning for GNNs, our second technical innovation is to generalize the pop- ular lottery ticket hypothesis (LTH) to GNNs for the ﬁrst time. LTH (Frankle & Carbin, 2018) demonstrates that one can identify highly sparse and independently trainable sub- networks from dense models, by iterative pruning. It was initially observed in convolutional DNNs, and later broadly found in natural language processing (NLP) (Chen et al., 2020b), generative models (Kalibhat et al., 2020), reinforce- ment learning (Yu et al., 2020) and lifelong learning (Chen et al., 2020b). To meaningfully generalize LTH to GNNs, we deﬁne a graph lottery ticket (GLT) as a pair of core sub-dataset and sparse sub-network which can be jointly identiﬁed from the full graph and the original GNN model, by iteratively applying UGS. Like its counterpart in convolu- tional DNNs, a GLT could be trained from its initialization to match the performance of training with the full model and graph, and its inference cost is drastically smaller. Our proposal has been experimentally veriﬁed, across var- ious GNN architectures and diverse tasks, on both small- scale graph datasets (Cora, Citeseer and PubMed), and large- scale datasets from the challenging Open Graph Benchmark (OGB). Our main observations are outlined below: • UGS is widely applicable to simplifying a GNN dur- ing training and reducing its inference MACs (multi- ply–accumulate operations). Moreover, by iteratively applying UGS, GLTs can be broadly located from for both shallow and deep GNN models, on both small- and large-scale graph datasets, with substantially re- duced inference costs and unimpaired generalization. • For node classiﬁcation, our found GLTs achieve 20% ∼98% MACs saving, with up to 5% ∼58.19% sparsity on graphs and 20% ∼97.75% sparsity on GNN models, at little to no performance degradation. For example in Figure 1, on Cora and Citeseer node classiﬁcation, our GLTs ( #) achieve comparable or sometimes even slightly better performance than the baselines of full models and graphs ( •), with only 41.16% and 5.57% MACs, respectively. • For link prediction, GLTs lead to48% ∼97% and 70% MACs saving, coming from up to 22.62% ∼55.99% sparsity on graphs and and 67.23% ∼97.19% sparsity on GNN models, again without performance loss. • Our proposed framework can scale up to deep GNNA Uniﬁed Lottery Ticket Hypothesis for Graph Neural Networks models (up to 28 layers) on large graphs (e.g., Ogbn- ArXiv and Ogbn-Proteins), without bells and whistles. • Besides from random initializations, GLTs can also be drawn from the initialization via self-supervised pre-training – an intriguing phenomenon recently just reported for NLP (Chen et al., 2020b) and computer vision models (Chen et al., 2020a). Using a latest GNN pre-training algorithm (You et al., 2020b) for initialization, GLTs can be found to achieve robust performance with even sparser graphs and GNNs. 2. Related Work Graph Neural Networks. There are mainly three cate- gories of GNNs (Dwivedi et al., 2020): i) extending original convolutional neural networks to the graph regime (Scarselli et al., 2008; Bruna et al., 2013; Kipf & Welling, 2016; Hamil- ton et al., 2017); ii) introducing anisotropic operations on graphs such as gating and attention (Battaglia et al., 2016; Monti et al., 2017; Veliˇckovi´c et al., 2018), and iii) improv- ing upon limitations of existing models (Xu et al., 2019; Morris et al., 2019; Chen et al., 2019; Murphy et al., 2019). Among this huge family, Graph Convolutional Networks (GCNs) are widely adopted, which can be categorized as spectral domain based methods (Defferrard et al., 2016; Kipf & Welling, 2016) and spatial domain bases methods (Simonovsky & Komodakis, 2017; Hamilton et al., 2017). The computational cost and memory usage of GNNs will expeditiously increase with the graph size. The aim of graph sampling or sparsiﬁcation is to extract a small sub- graph from the original large one, which can remain ef- fective for learning tasks (Zheng et al., 2020; Hamilton et al., 2017) while reducing the cost. Previous works on sampling focus on preserving certain pre-deﬁned graph met- rics (H ¨ubler et al., 2008), graph spectrum (Chakeri et al., 2016; Adhikari et al., 2017), or node distribution (Leskovec & Faloutsos, 2006; V oudigari et al., 2016; Eden et al., 2018). FastGCN (Chen et al., 2018a) introduced a global impor- tance sampling method instead of locally neighbor sampling. VRGCN (Chen et al., 2018b) proposed a control variate based algorithm, but requires all intermediate vertex embed- dings to be saved during training. Cluster-GCN (Chiang et al., 2019) used clustering to partition subgraphs for train- ing, but often suffers in stability. Zheng et al. (2020); Li et al. (2020b) cast graph sparsiﬁcation as optimization problems, solved by learning surrogates and ADMM, respectively. Lottery Ticket Hypothesis (LTH). Since the original LTH (Frankle & Carbin, 2018), a lot of works have ex- plored the prospect of trainable sparse subnetworks in place of the full models without sacriﬁcing performance. Frankle et al. (2019); Renda et al. (2020) introduced the rewinding techniques to scale up LTH. LTH was also adopted in differ- ent ﬁelds (Evci et al., 2019; Savarese et al., 2020; Liu et al., 2019; You et al., 2020a; Gale et al., 2019; Yu et al., 2020; Kalibhat et al., 2020; Chen et al., 2021b; 2020b;c; 2021a; Ma et al., 2021; Gan et al., 2021). However, GNN is NOT “yet another” ﬁeld that can be easily cracked by LTH. That is again due to GNNs having much smaller models, while all the aforementioned LTH works focus on simplifying their redundant models. To our best knowledge, this work is not only the ﬁrst to generalize LTH to GNNs, but also the ﬁrst to extend LTH from simplifying models to a new data-model co-simpliﬁcation prospect. 3. Methodology 3.1. Notations and Formulations Let G = {V,E}represent an undirected graph with |V| nodes and |E|edges. For V = {v1,...,v |V|}, let X ∈ R|V|×F denote the node feature matrix of the whole graph, where xi = X[i,:] is the F-dimensional attribute vector of node vi ∈V. As for E= {e1,...,e |E|}, en = (vi,vj) ∈E means that there exists a connection between nodevi and vj. An adjacency matrix A ∈R|V|×|V| is deﬁned to describe the overall graph topology, whereA[i,j] = 1if (vi,vj) ∈E else A[i,j] = 0. For example, the two-layer GNN (Kipf & Welling, 2016) can be deﬁned as follows: Z = S( ˆAσ( ˆAXΘ(0))Θ(1)), (1) where Z is the prediction of GNN f(G,Θ). The graph G can be alternatively denoted as {A,X}, Θ = (Θ(0),Θ(1)) is the weights of the two-layer GNN, S(·) represents the softmax function, σ(·) denotes the activation function (e.g., ReLU), ˆA = ˜D−1 2 (A+ I) ˜D 1 2 is normalized by the degree matrix ˜D of A + I. Considering the transductive semi- supervised classiﬁcation task, the objective function Lis: L(G,Θ) =− 1 |Vlabel| ∑ vi∈Vlabel yilog(zi), (2) where Lis the cross-entropy loss over all labeled samples Vlabel ⊂V, and yi is the annotated label vector of node vi for its corresponding prediction zi = Z[i,:]. 3.2. Uniﬁed GNN Sparsiﬁcation We present out end-to-end framework, Uniﬁed GNN Sparsiﬁcation (UGS), to simultaneously reduce edges in Gand the parameters in GNNs. Speciﬁcally, we introduce two differentiable masks mg and mθ for indicating the in- signiﬁcant connections and weights in the graph and GNNs, respectively. The shapes of mg and mθ are identical to those the adjacency matrix A and the weights Θ, respec- tively. Given A, Θ, mg and mθ are co-optimized from end to end, under the following objective: LUGS := L({mg ⊙A,X},mθ ⊙Θ) +γ1∥mg∥1 + γ2∥mθ∥1, (3)A Uniﬁed Lottery Ticket Hypothesis for Graph Neural Networks Graph GNNs Sparse Graph Sparse GNNs Iterative Unified Graph Sparsification Figure 2.An illustration of uniﬁed GNN sparsiﬁcation (UGS). Dash/solid lines denote the removed/remaining edges and weights in the graph and GNNs, respectively. Note that graphs and GNNs are co-optimized to ﬁnd optimal solutions for the uniﬁed sparsiﬁcation. Algorithm 1 Uniﬁed GNN Sparsiﬁcation (UGS) Input: Graph G= {A,X}, GNN f( G,Θ0) , GNN’s ini- tialization Θ0, initial masks m0 g = A, m0 θ = 1 ∈ R∥Θ0∥0 , Step size η, λg, and λθ. Output: Sparsiﬁed masks mg and mθ 1: for iteration i= 0 ,1,2 ,..., N −1do 2: Forward f( ·,mi θ ⊙Θi) with G= {mi g ⊙A,X}to compute the loss LUGS in Equation 3. 3: Backpropagate to update Θi+1 ←Θi−η∇ΘiLUGS. 4: Update mi+1 g ←mi g −λg∇migLUGS. 5: Update mi+1 θ ←mi θ −λθ∇mi θ LUGS. 6: end for 7: Set pg = 5% of the lowest magnitude values in mN g to 0 and others to 1, then obtain mg. 8: Set pθ = 20% of the lowest magnitude values in mN θ to 0 and others to 1, then obtain mθ. where ⊙is the element-wise product, γ1 and γ2 are the hyparameters to control ℓ1 sparsity regularizers of mg and mθ respectively. After the training is done, we set the lowest-magnitude elements in mg and mθ to zero, w.r.t. pre-deﬁned ratios pg and pθ. Then, the two sparse masks are applied to prune A and Θ, leading to the ﬁnal sparse graph and model. Alg. 1 outlines the procedure of UGS, and it can be considered as the generalized pruning for GNNs. 3.3. Graph Lottery Tickets Graph lottery tickets (GLT). Given a GNN f( ·,Θ) and a graph G= {A,X}, the associated subnetworks of GNN and sub-graph can be deﬁned as f( ·,mθ ⊙Θ) and Gs = {mg⊙A,X}, where mg and mθ are binary masks deﬁned in Section 3.2. If a subnetwork f( ·,mθ ⊙Θ) trained on a sparse graph Gs has performance matching or surpassing the original GNN trained on the full graph Gin terms of achieved standard testing accuracy, then we deﬁnef( {mg⊙ A,X},mθ⊙Θ0) as a uniﬁed graph lottery tickets (GLTs), where Θ0 is the original initialization for GNNs which the found lottery ticket subnetwork is usually trained from. Unlike previous LTH literature (Frankle & Carbin, 2018), our identiﬁed GLT will consist of three elements: i) a sparse Algorithm 2 Iterative UGS to ﬁnd Graph Lottery Tickets Input: Graph G = {A,X}, GNN f( G,Θ0) , GNN’s initialization Θ0, pre-deﬁned sparsity levels sg for graphs and sθ for GNNs, Initial masks mg = A, mθ = 1 ∈R∥Θ0∥0 . Output: GLT f( {mg ⊙A,X},mθ ⊙Θ0) 1: while 1−∥mg∥0 ∥A∥0 <sg and 1−∥mθ∥0 ∥Θ∥0 <sθ do 2: Sparsify GNN f( ·,mθ ⊙Θ0) with G = {mg ⊙ A,X}using UGS, as presented in Algorithm 1. 3: Update mg and mθ accordingly. 4: Rewinding GNN’s weights toΘ0 5: Rewinding masks, mg = mg ⊙A 6: end while graph Gs = {mg ⊙A,X}; ii) the sparse mask mθ for the model weight; and iii) the model weight’s initializationΘ0. Finding GLT. Classical LTH leverages iterative magnitude-based pruning (IMP) to identify lottery tickets. In a similar fashion, we apply our UGS algorithm to prune both the model and the graph during training, as outlined Algorithm 2, obtaining the graph mask mg and model weight mask mθ of GLT. Then, the GNN weights are rewound to the original initialization Θ. We repeat the above two steps iteratively, until reaching the desired sparsity sg and sθ for the graph and GNN, respectively. Complexity analysis of GLTs. The inference time com- plexity of GLTs isO(L ×∥mg⊙A∥0 ×F + L ×∥mθ∥0 × |V|× F 2) , where L is the number of layers, ∥mg ⊙A∥0 is the number of remaining edges in the sparse graph, F is the dimension of node features, |V|is the number of nodes. The memory complexity isO(L ×|V|×F+L ×∥mθ∥0 ×F 2) . In our implementation, pruned edges will be removed from E, and would not participate in the next round’s computation. 4. Experiments In this section, extensive experiments are reported to val- idate the effectiveness of UGS and the existence of GLTs across diverse graphs and GNN models. Our subjects in- clude small- and medium-scale graphs with two-layer GraphA Uniﬁed Lottery Ticket Hypothesis for Graph Neural Networks 0.00 5.00 9.7514.2618.5522.6226.4930.1733.6636.9840.1343.1245.9648.6751.2353.6755.9958.1960.2862.2664.15 Graph Sparsity (%) 30 40 50 60 70 80Accuracy (Cora) GCN UGS (Ours) Random Pruning ADMM Baseline GLT 0.00 5.00 9.7514.2618.5522.6226.4930.1733.6636.9840.1343.1245.9648.6751.2353.6755.9958.1960.2862.2664.15 Graph Sparsity (%) 50 55 60 65 70 75 80 GIN UGS (Ours) Random Pruning Baseline GLT 0.00 5.00 9.7514.2618.5522.6226.4930.1733.6636.9840.1343.1245.9648.6751.2353.6755.9958.1960.2862.2664.15 Graph Sparsity (%) 70 72 74 76 78 80 82 GAT UGS (Ours) Random Pruning Baseline GLT 010020030040050060070080090010001100120013001400150016001700180019002000 Inference MACs (M) 30 40 50 60 70 80Accuracy (Cora) UGS (Ours) Random Pruning ADMM Baseline (2007M) GLT (826M) 010020030040050060070080090010001100120013001400150016001700180019002000 Inference MACs (M) 50 55 60 65 70 75 80 UGS (Ours) Random Pruning Baseline (2015M) GLT (1615M) 0 100020003000400050006000700080009000100001100012000130001400015000 Inference MACs (M) 70 72 74 76 78 80 82 UGS (Ours) Random Pruning Baseline (15959M) GLT (1122M) 0.00 5.00 9.7514.2618.5522.6226.4930.1733.6636.9840.1343.1245.9648.6751.2353.6755.9958.1960.2862.2664.15 Graph Sparsity (%) 20 30 40 50 60 70Accuracy (Citeseer) UGS (Ours) Random Pruning ADMM Baseline GLT 0.00 5.00 9.7514.2618.5522.6226.4930.1733.6636.9840.1343.1245.9648.6751.2353.6755.9958.1960.2862.2664.15 Graph Sparsity (%) 50 55 60 65 70 UGS (Ours) Random Pruning Baseline GLT 0.00 5.00 9.7514.2618.5522.6226.4930.1733.6636.9840.1343.1245.9648.6751.2353.6755.9958.1960.2862.2664.15 Graph Sparsity (%) 58 60 62 64 66 68 70 72 UGS (Ours) Random Pruning Baseline GLT 04008001200160020002400280032003600400044004800520056006000 Inference MACs (M) 20 30 40 50 60 70Accuracy (Citeseer) UGS (Ours) Random Pruning ADMM Baseline (6299M) GLT (350M) 04008001200160020002400280032003600400044004800520056006000 Inference MACs (M) 50 55 60 65 70 UGS (Ours) Random Pruning Baseline (6323M) GLT (141M) 0 2000400060008000100001200014000160001800020000220002400026000280003000032000340003600038000400004200044000460004800050000 Inference MACs (M) 58 60 62 64 66 68 70 72 UGS (Ours) Random Pruning Baseline (50293M) GLT (2233M) 0.00 5.00 9.7514.2618.5522.6226.4930.1733.6636.9840.1343.1245.9648.6751.2353.6755.9958.1960.2862.2664.15 Graph Sparsity (%) 40 50 60 70 80Accuracy (PubMed) UGS (Ours) Random Pruning ADMM Baseline GLT 0.00 5.00 9.7514.2618.5522.6226.4930.1733.6636.9840.1343.1245.9648.6751.2353.6755.9958.1960.2862.2664.15 Graph Sparsity 70 72 74 76 78 80 UGS (Ours) Random Pruning Baseline GLT 0.00 5.00 9.7514.2618.5522.6226.4930.1733.6636.9840.1343.1245.9648.6751.2353.6755.9958.1960.2862.2664.15 Graph Sparsity (%) 74 75 76 77 78 79 80 UGS (Ours) Random Pruning Baseline GLT 025050075010001250150017502000225025002750300032503500375040004250450047505000 Inference MACs (M) 40 50 60 70 80Accuracy (PubMed) UGS (Ours) Random Pruning ADMM Baseline (5168M) GLT (152M) 025050075010001250150017502000225025002750300032503500375040004250450047505000 Inference MACs (M) 70 72 74 76 78 80 UGS (Ours) Random Pruning Baseline (5188M) GLT (345M) 0 200040006000800010000120001400016000180002000022000240002600028000300003200034000360003800040000 Inference MACs (M) 74 75 76 77 78 79 80 UGS (Ours) Random Pruning Baseline (40824M) GLT (1082M) Figure 3.Node classiﬁcation performance over achieved graph sparsity levels or inference MACs of GCN, GIN, and GAT on Cora, Citeseer, and PubMed datasets, respectively. Red stars (#) indicate the located GLTs, which reach comparable performance with high sparsity and low inference MACs. Dash lines represent the baseline performance of full GNNs on full graphs. More results over GNN sparsity are provided in Appendix A2.1. Convolutional Network (GCN) (Kipf & Welling, 2016), Graph Isomorphism Network (GIN) (Xu et al., 2018), and Graph Attention Network (GAT) (Veliˇckovi´c et al., 2017) in Section 4.2; as well as large-scale graphs with 28-layer deep ResGCNs (Li et al., 2020a) in Section 4.2. Besides, in Section 4.3, we investigate GLTs under the self-supervised pre-training (You et al., 2020b). Ablation studies and visu- alizations are provided in Section 4.4 and 4.5.A Uniﬁed Lottery Ticket Hypothesis for Graph Neural Networks 010020030040050060070080090010001100120013001400150016001700180019002000 Inference MACs (M) 70 75 80 85 90ROC-AUC (Cora) GCN UGS (Ours) Random Pruning Baseline (2007M) GLT (531M) 010020030040050060070080090010001100120013001400150016001700180019002000 Inference MACs (M) 65 70 75 80 85 GIN UGS (Ours) Random Pruning Baseline (2015M) GLT (670M) 0 100020003000400050006000700080009000100001100012000130001400015000 Inference MACs (M) 85 86 87 88 89 90 91 GAT UGS (Ours) Random Pruning Baseline (15959M) GLT (1397M) 04008001200160020002400280032003600400044004800520056006000 Inference MACs (M) 80 85 90 95ROC-AUC (Citeseer) UGS (Ours) Random Pruning Baseline (6299M) GLT (1061M) 04008001200160020002400280032003600400044004800520056006000 Inference MACs (M) 75 80 85 90 95 UGS (Ours) Random Pruning Baseline (6323M) GLT (1680M) 0 2000400060008000100001200014000160001800020000220002400026000280003000032000340003600038000400004200044000460004800050000 Inference MACs (M) 91 92 93 94 95 96 UGS (Ours) Random Pruning Baseline (50293M) GLT (4343M) 025050075010001250150017502000225025002750300032503500375040004250450047505000 Inference MACs (M) 86 87 88 89 90 91 92 93ROC-AUC (PubMed) UGS (Ours) Random Pruning Baseline (5168M) GLT (599M) 025050075010001250150017502000225025002750300032503500375040004250450047505000 Inference MACs (M) 80 82 84 86 88 90 UGS (Ours) Random Pruning Baseline (5188M) GLT (2697M) 0 200040006000800010000120001400016000180002000022000240002600028000300003200034000360003800040000 Inference MACs (M) 88 89 90 91 92 93 94 UGS (Ours) Random Pruning Baseline (40824M) GLT (1319M) Figure 4.Link predictionperformance over inference MACs of GCN, GIN, and GAT on Cora, Citeseer, and PubMed datasets, respectively. Red stars (#) indicate the located GLTs, which reach comparable performance with the least inference MACs. Dash lines represent the baseline performance of unpruned GNNs on full graphs. More results over graph sparsity and GNN sparsity are referred to Appendix A2.2. Table 1.Graph datasets statistics. Dataset Task TypeNodesEdgesAve. DegreeFeaturesClassesMetric Cora Node Classiﬁcation2,708 5,429 3.88 1,433 7 AccuracyLink Prediction ROC-AUC CiteseerNode Classiﬁcation3,327 4,732 2.84 3,703 6 AccuracyLink Prediction ROC-AUC PubMedNode Classiﬁcation19,71744,338 4.50 500 3 AccuracyLink Prediction ROC-AUC Ogbn-ArXivNode Classiﬁcation169,3431,166,24313.77 128 40 AccuracyOgbn-ProteinsNode Classiﬁcation132,53439,561,252597.00 8 2 ROC-AUC Ogbl-CollabLink Prediction235,8681,285,46510.90 128 2 Hits@50 Datasets We use popular semi-supervised graph datasets: Cora, Citeseer and PubMed (Kipf & Welling, 2016), for both node classiﬁcation and link prediction tasks. For exper- iments on large-scale graphs, we use the Open Graph Bench- mark (OGB) (Hu et al., 2020), such as Ogbn-ArXiv, Ogbn- Proteins, and Ogbl-Collab. More datasets statistics are sum- marized in Table 1. Other details such as the datasets’ train- val-test splits are included in Appendix A1. Training and Inference Details Our evaluation metrics are shown in Table 1, following Kipf & Welling (2016); Hu et al. (2020); Mavromatis & Karypis (2020). More detailed conﬁgurations such as learning rate, training iterations, and hyperparameters in UGS, are referred to Appendix A1. 4.1. The Existence of Graph Lottery Ticket We ﬁrst examine whether uniﬁed graph lottery tickets exist and can be located by UGS. Results of GCN/GIN/GAT on Cora/Citesser/PubMed for node classiﬁcation and link prediction are collected in Figures 3 and 4, respectively. Note that each point in the ﬁgures denotes the achieved performance with respect to a certain graph sparsity, GNN sparsity, and inference MACs. However, due to the limited space, we only include one or two of these three sparsity indicators in the main text, and the rest can be found in Appendix A2. We list the following Observations. Obs.1. GLTs broadly exist with substantial MACs sav- ing. Graph lottery tickets at a range of graph sparsity from 5% to 58.19% without performance deterioration, can be identiﬁed across GCN, GIN and GAT on Cora, Citeseer, and PubMed datasets for both node classiﬁcation and link pre- diction tasks. Such GLTs signiﬁcantly reduce 59% ∼97%, 20% ∼98%, 91% ∼97% inference MACs for GCN, GIN and GAT across all datasets. Obs.2. UGS is ﬂexible and consistently shows superior performance. UGS consistently surpasses random prun- ing by substantial performance margins across all datasets and GNNs, which validates the effectiveness of our proposal. The previous state-of-the-art method, i.e., ADMM (Li et al., 2020b), achieves a competitive performance to UGS at mod- erate graph sparsity levels, and performs 3 ∼4% worse than UGS when graphs are heavily pruned. Note that the ADMM approach by Li et al. (2020b) isA Uniﬁed Lottery Ticket Hypothesis for Graph Neural Networks only applicable when two conditions are met: i) graphs are stored via adjacency matrices—however, that is not practical for large graphs (Hu et al., 2020); ii) aggregat- ing features with respect to adjacency matrices—however, recent designs of GNNs (e.g., GIN and GAT) commonly use the much more computation efﬁcient approach of syn- chronous/asynchronous message passing (Gilmer et al., 2017; Busch et al., 2020). On the contrary, our proposed UGS is ﬂexible enough and free of these limitations. Obs.3. GNN-speciﬁc and Graph-speciﬁc analyses: GAT is more amenable to sparsiﬁed graphs; Cora is more sensitive to pruning. As demonstrated in Figures 3 and 4, compared to GCN and GIN, GLTs in GAT can be found at higher sparsity levels; meanwhile randomly pruned graphs and GAT can still reach satisﬁed performance and maintain higher accuracies on severely sparsiﬁed graphs. One possible explanation is that attention-based aggregation is capable of re-identifying important connections in pruned graphs which makes GAT be more amenable to sparsiﬁ- cation. Compared the sparsity of located GLTs (i.e., the position of red stars (#)) across three graph datasets, we ﬁnd that Cora is the most sensitive graph to pruning and PubMed is more robust to be sparsiﬁed. 4.2. Scale Up Graph Lottery Tickets To scale up graph lottery tickets, we further conduct experi- ments on 28-layer deep ResGCNs (Li et al., 2020a) on large- scale datasets that have more than millions of connections, like Ogbn-ArXiv and Ogbn-Proteins for node classiﬁcation, Ogbl-Collab for link prediction in Table 1. We summarize our observations and derive insights below. 0 20000400006000080000 Inference MACs (M) 56 58 60 62 64 66 68 70 72 74Accuracy Ogbn-ArXiv UGS (Ours) Random Pruning Baseline (89687M) GLT (13497M) 35000450005500065000750008500095000 Inference MACs (M) 78 79 80 81 82 83 84 85 86ROC-AUC Ogbn-Proteins UGS (Ours) Random Pruning Baseline (97110M) GLT (73024M) 0 20000400006000080000100000120000 Inference MACs (M) 15 20 25 30 35 40 45 50 55Hits@50 Ogbl-Collab UGS (Ours) Random Pruning Baseline (117419M) GLT (35138M) Figure 5.Node classiﬁcation and link prediction performance of 28-layer deep ResGCNs on large-scale graph datasets. Obs.4. UGS is scaleable and GLT exists in deep GCNs on large-scale datasets. Figure 5 demonstrates that UGS can be scaled up to deep GCNs on large-scale graphs. Found GLTs obtain matched performance with 85%, 25%, 70% MACs saving on Ogbn-ArXiv, Ogbn-Proteins, and Ogbl- Collab, respectively. Obs.5. Denser graphs (e.g., Ogbn-Proteins) are more resilient to sparsiﬁcation. As shown in Figure 5, com- paring the node classiﬁcation results on Ogbn-ArXiv (Ave. degree: 13.77) and Ogbn-Proteins (Ave. degree: 597.00), Ogbn-Proteins has a negligible performance gap between UGS and random pruning, even on heavily pruned graphs. Since nodes with high degrees in denser graphs have less chance to be totally isolated during pruning, it may con- tribute to more robustness to sparsiﬁcation. Similar observa- tions can be drawn from the comparison between PubMed and other two small graphs in Figure 3 and 4. 4.3. Graph Lottery Ticket from Pre-training 010020030040050060070080090010001100120013001400150016001700180019002000 Inference MACs (M) 72 74 76 78 80 82Accuracy (Cora) Node Classification UGS GraphCL+UGS 010020030040050060070080090010001100120013001400150016001700180019002000 Inference MACs (M) 82 84 86 88 90 92 94ROC-AUC (Cora) Link Predication UGS GraphCL+UGS 04008001200160020002400280032003600400044004800520056006000 Inference MACs (M) 68 69 70 71 72 73 74Accuracy (Citeseer) UGS GraphCL+UGS 04008001200160020002400280032003600400044004800520056006000 Inference MACs (M) 88 90 92 94 96 98ROC-AUC (Citeseer) UGS GraphCL+UGS Figure 6.Drawing graph lottery tickets from randomly initialized and self-supervised pre-trained (GraphCL (You et al., 2020b)) GCNs on node classiﬁcation (low label rate: only 5.0% and 3.6% nodes in Cora and Citesser are labeled) and link prediction. High-quality lottery tickets can be drawn from self- supervised pre-trained models, as recently found in both NLP and computer vision ﬁelds (Chen et al., 2020b;a). In the GLT case, we also assess the impact of replacing ran- dom initialization with self-supervised graph pre-training, i.e., GraphCL (You et al., 2020b), on transductive semi- supervised node classiﬁcation and link prediction. From Figure 6 and A12, we gain a few interesting observa- tions. First, UGS with the GraphCL pre-trained initializa- tion consistently presents superior performance at moderate sparsity levels (≤40% graph sparsity ≃≤ 85% MACs sav- ing). While the two settings perform similar at extreme spar- sity, it indicates that for excessively pruned graphs, the ini- tialization is no longer the performance bottleneck; Second, GraphCL beneﬁts GLT on multiple downstream tasks includ- ing node classiﬁcation and link prediction; Third, especially on the transductive semi-supervised setup, GLTs with ap- propriate sparsity levels can even enlarge the performance gain from pre-training, for example, see GLT on Citeseer with 22.62% graph sparsity and 2068M inference MACs.A Uniﬁed Lottery Ticket Hypothesis for Graph Neural Networks 4.4. Ablation Study Pruning ratio pg and pθ. We extensively investigate the pruning ratios pg, pθ in UGS for graph and GNN sparsi- ﬁcation. As shown in Figure 7, with a ﬁxed pθ = 20%, only the setting of pg = 5%can identify the GLT, and it performs close to pg = 10%at higher sparsity levels (e.g., ≥25%). Aggressively pruning the graph’s connections in each round of iterative UGS, e.g., pg = 20%, leads to substantially degraded accuracies, especially for large spar- sities. On the other hand, with a ﬁxed pg = 20%, all there settings of pθ = 10%,20%,40% show similar performance, and even higher pruning ratios produce slight better results. It again veriﬁes that the key bottleneck in pruning GNNs mainly lies in the sparsiﬁcation of graphs. In summary, we adopt pg = 5%and pg = 20%(follow previous LTH works (Frankle & Carbin, 2018)) for all the experiments. 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 Graph Sparsity (%) 65 70 75 80Accuracy pg = 5%, pθ = 20% pg = 10%, pθ = 20% pg = 20%, pθ = 20% Baseline 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 Weight Sparsity (%) 50 60 70 80Accuracy pg = 5%, pθ = 10% pg = 5%, pθ = 20% pg = 5%, pθ = 40% Baseline 0 5 10 15 20 25 30 35 40 45 50 55 60 78 79 80 81 82 Figure 7.Ablation studies of pruning ratios for the graph and GNN sparsiﬁcation by UGS, i.e., pg, pθ. Each curve records the achieved performance of 20 rounds iterative UGS. GCN on Cora is adopted here. The embedded sub-graph is a zoom-in of the red box region. Table 2.Performance comparisons of Random GLT versus GLT from GCN on Cora, at several sparsity levels. Settings (Graph Sparsity, GNN Sparsity)=(sg%,sθ%) (18.55,59.04) (22.62,67.23) (36.98,86.58) (55.99,97.19) Random GLT 79.70 78.50 75.70 63.70 GLT 80.80 80.30 79.30 75.30 Random graph lottery tickets. Randomly re-initializing located sparse models, i.e., random lottery tickets, usually serves as a necessary baseline for validating the effective- ness of rewinding processes (Frankle & Carbin, 2018). In Table 2, we compare GLT to Random GLT, the latter by ran- domly re-initializing GNN’s weights and learnable masks, and GLT shows aapparently superior performance, consis- tent with previous observations (Frankle & Carbin, 2018). 4.5. Visualization and Analysis In this section, we visualize the sparsiﬁed graphs in GLTs from UGS in Figure 8, and further measure the graph properties1 shown in Table A5, including clustering co- efﬁcient (Luce & Perry, 1949), as well as node and edge betweenness centrality (Freeman, 1977). Speciﬁcally, clus- tering coefﬁcient measures the proportion of edges between the nodes within a given node’s neighborhood; node and edge betweenness centrality show the degree of central a vertex or an edge is in the graph (Narayanan, 2005). Re- ported numbers in Table A5 are averaged over all the nodes. Cora  Citeseer  Original Sparsified Original Sparsified  PubMed  Figure 8.Visualization of sub-graphs (Original/Sparsiﬁed) from Cora, Citeseer, and PubMed. Original sub-graphs in the ﬁrst and third columns are randomly sampled from full graphs. The corresponding uniﬁed sparsiﬁed sub-graphs of GLTs at 48.67% sparsity, are provided in the second and forth columns. Both Figure 8 and Table A5 show that sparse graphs ob- tained from UGS seem to maintain more “critical” vertices which used to have much denser connections. It may pro- vide possible insights on what GLTs prefer to preserve. 5. Conclusion and Discussion In this paper, we ﬁrst propose uniﬁed GNN sparsiﬁcation to generalize the notion or pruning in GNNs. We further establish the LTH for GNNs, by leveraging UGS and con- sidering a novel joint data-model lottery ticekt. The new uniﬁed LTH for GNNs generalizes across various GNN ar- chitectures, learning tasks, datasets, and even initialization ways. In general, we ﬁnd GLT to tremendously trim down the inference MACs, without sacriﬁcing task performance. It remains open how much we could translate GLT’s high sparsity into practical acceleration and energy-saving ben- eﬁts. Most DNN accelerators are optimized for dense and regular computation, making edge-based operations hard to implement efﬁciently. To our best knowledge, the hardware acceleration research on GNNs just starts to gain interests (Auten et al., 2020; Abadal et al., 2020; Geng et al., 2020; Wang et al., 2020; Kiningham et al., 2020). We expect GLT to be implemented using sparse-dense matrix multi- plication (SpMM) operations from highly optimized sparse matrix libraries, such as Intel MKL (Wang et al., 2014) or cuSPARSE (Naumov et al., 2010). 1NetworkX ( https://networkx.org) is used for our analyses.A Uniﬁed Lottery Ticket Hypothesis for Graph Neural Networks References Abadal, S., Jain, A., Guirado, R., L ´opez-Alonso, J., and Alarc´on, E. Computing graph neural networks: A sur- vey from algorithms to accelerators. arXiv preprint arXiv:2010.00130, 2020. Adhikari, B., Zhang, Y ., Amiri, S. E., Bharadwaj, A., and Prakash, B. A. Propagation-based temporal network sum- marization. IEEE Transactions on Knowledge and Data Engineering, 30(4):729–742, 2017. Auten, A., Tomei, M., and Kumar, R. Hardware accelera- tion of graph neural networks. In 2020 57th ACM/IEEE Design Automation Conference (DAC), pp. 1–6. IEEE, 2020. Battaglia, P., Pascanu, R., Lai, M., Rezende, D. J., and kavukcuoglu, K. Interaction networks for learning about objects, relations and physics. In Proceedings of the 30th International Conference on Neural Information Processing Systems, 2016. Bertsekas, D. P. and Rheinboldt, W. Constrained optimiza- tion and lagrange multiplier methods. 1982. Bruna, J., Zaremba, W., Szlam, A., and LeCun, Y . Spec- tral networks and locally connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013. Busch, J., Pi, J., and Seidl, T. Pushnet: Efﬁcient and adaptive neural message passing. arXiv preprint arXiv:2003.02228, 2020. Calandriello, D., Lazaric, A., Koutis, I., and Valko, M. Im- proved large-scale graph learning through ridge spectral sparsiﬁcation. In International Conference on Machine Learning, pp. 688–697. PMLR, 2018. Chakeri, A., Farhidzadeh, H., and Hall, L. O. Spectral sparsi- ﬁcation in spectral clustering. In 2016 23rd international conference on pattern recognition (icpr), pp. 2301–2306. IEEE, 2016. Chen, J., Ma, T., and Xiao, C. Fastgcn: Fast learning with graph convolutional networks via importance sampling. In International Conference on Learning Representations, 2018a. Chen, J., Zhu, J., and Song, L. Stochastic training of graph convolutional networks with variance reduction. In Inter- national Conference on Machine Learning, pp. 942–950. PMLR, 2018b. Chen, T., Frankle, J., Chang, S., Liu, S., Zhang, Y ., Carbin, M., and Wang, Z. The lottery tickets hypothesis for supervised and self-supervised pre-training in computer vision models. arXiv preprint arXiv:2012.06908, 2020a. Chen, T., Frankle, J., Chang, S., Liu, S., Zhang, Y ., Wang, Z., and Carbin, M. The lottery ticket hypothesis for pre- trained bert networks. arXiv preprint arXiv:2007.12223, 2020b. Chen, T., Cheng, Y ., Gan, Z., Liu, J., and Wang, Z. Ultra- data-efﬁcient gan training: Drawing a lottery ticket ﬁrst, then training it toughly. arXiv preprint arXiv:2103.00397, 2021a. Chen, T., Zhang, Z., Liu, S., Chang, S., and Wang, Z. Long live the lottery: The existence of winning tick- ets in lifelong learning. In International Conference on Learning Representations , 2021b. URL https: //openreview.net/forum?id=LXMSvPmsm0g. Chen, X., Cheng, Y ., Wang, S., Gan, Z., Wang, Z., and Liu, J. Earlybert: Efﬁcient bert training via early-bird lottery tickets. arXiv preprint arXiv:2101.00063, 2020c. Chen, Z., Villar, S., Chen, L., and Bruna, J. On the equiv- alence between graph isomorphism testing and function approximation with gnns. In Advances in Neural Infor- mation Processing Systems, 2019. Cheng, Y ., Wang, D., Zhou, P., and Zhang, T. A survey of model compression and acceleration for deep neural networks. arXiv preprint arXiv:1710.09282, 2017. Chiang, W.-L., Liu, X., Si, S., Li, Y ., Bengio, S., and Hsieh, C.-J. Cluster-gcn: An efﬁcient algorithm for training deep and large graph convolutional networks. In Proceed- ings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 257–266, 2019. Defferrard, M., Bresson, X., and Vandergheynst, P. Con- volutional neural networks on graphs with fast localized spectral ﬁltering. In Proceedings of the 30th International Conference on Neural Information Processing Systems, pp. 3844–3852, 2016. Dwivedi, V . P., Joshi, C. K., Laurent, T., Bengio, Y ., and Bresson, X. Benchmarking graph neural networks. arXiv preprint arXiv:2003.00982, 2020. Eden, T., Jain, S., Pinar, A., Ron, D., and Seshadhri, C. Provable and practical approximations for the degree dis- tribution using sublinear graph samples. In Proceedings of the 2018 World Wide Web Conference, pp. 449–458, 2018. Evci, U., Pedregosa, F., Gomez, A., and Elsen, E. The difﬁculty of training sparse neural networks. arXiv, abs/1906.10732, 2019. Frankle, J. and Carbin, M. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In Interna- tional Conference on Learning Representations, 2018.A Uniﬁed Lottery Ticket Hypothesis for Graph Neural Networks Frankle, J., Dziugaite, G. K., Roy, D. M., and Carbin, M. Linear mode connectivity and the lottery ticket hypothesis. arXiv, abs/1912.05671, 2019. Freeman, L. C. A set of measures of centrality based on betweenness. Sociometry, pp. 35–41, 1977. Gale, T., Elsen, E., and Hooker, S. The state of sparsity in deep neural networks. arXiv, abs/1902.09574, 2019. Gan, Z., Chen, Y .-C., Li, L., Chen, T., Cheng, Y ., Wang, S., and Liu, J. Playing lottery tickets with vision and language. arXiv preprint arXiv:2104.11832, 2021. Geng, T., Li, A., Shi, R., Wu, C., Wang, T., Li, Y ., Haghi, P., Tumeo, A., Che, S., Reinhardt, S., et al. Awb-gcn: A graph convolutional network accelerator with runtime workload rebalancing. In 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), pp. 922–936. IEEE, 2020. Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and Dahl, G. E. Neural message passing for quantum chem- istry. In International Conference on Machine Learning, pp. 1263–1272. PMLR, 2017. Hamilton, W., Ying, Z., and Leskovec, J. Inductive repre- sentation learning on large graphs. In Advances in neural information processing systems, pp. 1024–1034, 2017. Han, S., Mao, H., and Dally, W. J. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In 4th International Conference on Learning Representations, 2016. Hu, W., Fey, M., Zitnik, M., Dong, Y ., Ren, H., Liu, B., Catasta, M., and Leskovec, J. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687, 2020. H¨ubler, C., Kriegel, H.-P., Borgwardt, K., and Ghahramani, Z. Metropolis algorithms for representative subgraph sampling. In 2008 Eighth IEEE International Conference on Data Mining, pp. 283–292. IEEE, 2008. Kalibhat, N. M., Balaji, Y ., and Feizi, S. Winning lottery tickets in deep generative models, 2020. Karimi, M., Wu, D., Wang, Z., and Shen, Y . Explainable deep relational networks for predicting compound-protein afﬁnities and contacts. arXiv preprint arXiv:1912.12553, 2019. Kiningham, K., Re, C., and Levis, P. Grip: A graph neural network acceleratorarchitecture. arXiv preprint arXiv:2007.13828, 2020. Kipf, T. N. and Welling, M. Semi-supervised classiﬁca- tion with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. Leskovec, J. and Faloutsos, C. Sampling from large graphs. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 631–636, 2006. Li, G., Muller, M., Thabet, A., and Ghanem, B. Deepgcns: Can gcns go as deep as cnns? In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9267–9276, 2019. Li, G., Xiong, C., Thabet, A., and Ghanem, B. Deep- ergcn: All you need to train deeper gcns. arXiv preprint arXiv:2006.07739, 2020a. Li, J., Zhang, T., Tian, H., Jin, S., Fardad, M., and Zafarani, R. Sgcn: A graph sparsiﬁer based on graph convolutional networks. In Paciﬁc-Asia Conference on Knowledge Dis- covery and Data Mining, pp. 275–287. Springer, 2020b. Liu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T. Re- thinking the value of network pruning. In 7th Interna- tional Conference on Learning Representations, 2019. Luce, R. and Perry, A. D. A method of matrix analysis of group structure. Psychometrika, 14:95–116, 1949. Ma, H., Chen, T., Hu, T.-K., You, C., Xie, X., and Wang, Z. Good students play big lottery better. arXiv preprint arXiv:2101.03255, 2021. Mavromatis, C. and Karypis, G. Graph infoclust: Leverag- ing cluster-level node information for unsupervised graph representation learning. arXiv preprint arXiv:2009.06946, 2020. Monti, F., Boscaini, D., Masci, J., Rodola, E., Svoboda, J., and Bronstein, M. M. Geometric deep learning on graphs and manifolds using mixture model cnns. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5115–5124, 2017. Morris, C., Ritzert, M., Fey, M., Hamilton, W. L., Lenssen, J. E., Rattan, G., and Grohe, M. Weisfeiler and leman go neural: Higher-order graph neural networks. In Proceed- ings of the AAAI Conference on Artiﬁcial Intelligence , 2019. Murphy, R., Srinivasan, B., Rao, V ., and Ribeiro, B. Rela- tional pooling for graph representations. In International Conference on Machine Learning, pp. 4663–4673, 2019. Narayanan, S. The betweenness centrality of biological networks. PhD thesis, Virginia Tech, 2005.A Uniﬁed Lottery Ticket Hypothesis for Graph Neural Networks Naumov, M., Chien, L., Vandermersch, P., and Kapasi, U. Cusparse library. In GPU Technology Conference, 2010. Qu, M., Bengio, Y ., and Tang, J. Gmnn: Graph markov neural networks. arXiv preprint arXiv:1905.06214, 2019. Renda, A., Frankle, J., and Carbin, M. Comparing rewind- ing and ﬁne-tuning in neural network pruning. In 8th International Conference on Learning Representations, 2020. Savarese, P., Silva, H., and Maire, M. Winning the lottery with continuous sparsiﬁcation. In Advances in Neural In- formation Processing Systems 33 pre-proceedings, 2020. Scarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M., and Monfardini, G. The graph neural network model. IEEE Transactions on Neural Networks, 2008. Simonovsky, M. and Komodakis, N. Dynamic edge- conditioned ﬁlters in convolutional neural networks on graphs. In Proceedings of the IEEE conference on com- puter vision and pattern recognition , pp. 3693–3702, 2017. Tailor, S. A., Fernandez-Marques, J., and Lane, N. D. Degree-quant: Quantization-aware training for graph neu- ral networks. In International Conference on Learning Representations, 2021. URL https://openreview. net/forum?id=NSBrFgJAHg. Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., and Bengio, Y . Graph attention networks.arXiv preprint arXiv:1710.10903, 2017. Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Li`o, P., and Bengio, Y . Graph attention networks. In International Conference on Learning Representations, 2018. Verma, V ., Qu, M., Lamb, A., Bengio, Y ., Kannala, J., and Tang, J. Graphmix: Regularized training of graph neural networks for semi-supervised learning. arXiv preprint arXiv:1909.11715, 2019. V oudigari, E., Salamanos, N., Papageorgiou, T., and Yan- nakoudakis, E. J. Rank degree: An efﬁcient algorithm for graph sampling. In 2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM), pp. 120–129. IEEE, 2016. Wang, E., Zhang, Q., Shen, B., Zhang, G., Lu, X., Wu, Q., and Wang, Y . Intel math kernel library. In High- Performance Computing on the Intel® Xeon Phi™ , pp. 167–188. Springer, 2014. Wang, Z., Guan, Y ., Sun, G., Niu, D., Wang, Y ., Zheng, H., and Han, Y . Gnn-pim: A processing-in-memory architecture for graph neural networks. In Conference on Advanced Computer Architecture, pp. 73–86. Springer, 2020. Xu, K., Hu, W., Leskovec, J., and Jegelka, S. How powerful are graph neural networks? arXiv preprint arXiv:1810.00826, 2018. Xu, K., Hu, W., Leskovec, J., and Jegelka, S. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. URL https:// openreview.net/forum?id=ryGs6iA5Km. Yan, M., Deng, L., Hu, X., Liang, L., Feng, Y ., Ye, X., Zhang, Z., Fan, D., and Xie, Y . Hygcn: A gcn accelerator with hybrid architecture. In 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA), pp. 15–29. IEEE, 2020. Ying, Z., You, J., Morris, C., Ren, X., Hamilton, W., and Leskovec, J. Hierarchical graph representation learning with differentiable pooling. In Advances in Neural Infor- mation Processing Systems, pp. 4800–4810, 2018. You, H., Li, C., Xu, P., Fu, Y ., Wang, Y ., Chen, X., Baraniuk, R. G., Wang, Z., and Lin, Y . Drawing early-bird tickets: Toward more efﬁcient training of deep networks. In 8th International Conference on Learning Representations, 2020a. You, Y ., Chen, T., Sui, Y ., Chen, T., Wang, Z., and Shen, Y . Graph contrastive learning with augmentations.Advances in Neural Information Processing Systems, 33, 2020b. You, Y ., Chen, T., Wang, Z., and Shen, Y . When does self-supervision help graph convolutional networks? In III, H. D. and Singh, A. (eds.), Proceedings of the 37th International Conference on Machine Learning , volume 119 of Proceedings of Machine Learning Re- search, pp. 10871–10880. PMLR, 13–18 Jul 2020c. URL http://proceedings.mlr.press/v119/ you20a.html. You, Y ., Chen, T., Wang, Z., and Shen, Y . L2-gcn: Layer- wise and learned efﬁcient training of graph convolutional networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2127– 2135, 2020d. Yu, H., Edunov, S., Tian, Y ., and Morcos, A. S. Playing the lottery with rewards and multiple languages: lottery tickets in rl and nlp. In 8th International Conference on Learning Representations, 2020. Zhang, M. and Chen, Y . Link prediction based on graph neural networks. In Advances in Neural Information Processing Systems, pp. 5165–5175, 2018.A Uniﬁed Lottery Ticket Hypothesis for Graph Neural Networks Zhao, P. gsparsify: Graph motif based sparsiﬁcation for graph clustering. In Proceedings of the 24th ACM Inter- national on Conference on Information and Knowledge Management, pp. 373–382, 2015. Zheng, C., Zong, B., Cheng, W., Song, D., Ni, J., Yu, W., Chen, H., and Wang, W. Robust graph representation learning via neural sparsiﬁcation. In International Con- ference on Machine Learning, 2020. Zhou, J., Cui, G., Zhang, Z., Yang, C., Liu, Z., Wang, L., Li, C., and Sun, M. Graph neural networks: A review of methods and applications. arXiv preprint arXiv:1812.08434, 2018.A Uniﬁed Lottery Ticket Hypothesis for Graph Neural Networks A1. More Implementation Details Datasets Download Links. As for small-scale datasets, we take the commonly used semi-supervised node classiﬁ- cation graphs: Cora, Citeseer and pubMed. For larger-scale datasets, we use three Open Graph Benchmark (OGB) (Hu et al., 2020) datasets: Ogbn-ArXiv, Ogbn-Proteins and Ogbl- Collab. All the download links of adopted graph datasets are included in Table A3. Table A3.Download links of graph datasets. Dataset Download links and introduction websites Cora https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz Citeseer https://linqs-data.soe.ucsc.edu/public/lbc/citeseer.tgz PubMed https://linqs-data.soe.ucsc.edu/public/Pubmed-Diabetes.tgz Ogbn-ArXiv https://ogb.stanford.edu/docs/nodeprop/#ogbn-arxiv Ogbn-Proteinshttps://ogb.stanford.edu/docs/nodeprop/#ogbn-proteins Ogbl-Collab https://ogb.stanford.edu/docs/linkprop/#ogbl-collab Train-val-test Splitting of Datasets. As for node classi- ﬁcation of small- and medium-scale datasets, we use 140 (Cora), 120 (Citeseer) and 60 (PubMed) labeled data for training, 500 nodes for validation and 1000 nodes for test- ing. As for link prediction task of small- and medium-scale datasets Cora, Citeseer and PubMed, we random sample 10% edges as our testing set, 5% for validation, and the rest 85% edges are training set. The training/validation/test splits for Ogbn-ArXiv, Ogbn-Proteins and Ogbl-Collab are given by the benchmark (Hu et al., 2020). Speciﬁcally, as for Ogbn-ArXiv, we train on the papers published until 2017, validation on those published in 2018 and test on those published since 2019. As for Ogbn-Proteins, we split the proteins nodes into training/validation/test sets accord- ing to the species which the proteins come from. As for Ogbl-Collab, we use the collaborations until 2017 as train- ing edges, those in 2018 as validation edges, and those in 2019 as test edges. More Details about GNNs. As for small- and medium- scale datasets Cora, Citeseer and PubMed, we choose the two-layer GCN/GIN/GAT networks with 512 hidden units to conduct all our experiments. As for large-scale datasets Ogbn-ArXiv, Ogbn-Proteins and Ogbl-Collab, we use the ResGCN (Li et al., 2020a) with 28 GCN layers to conduct all our experiments. As for Ogbn-Proteins dataset, we also use the edge encoder module, which is a linear transform function in each GCN layer to encode the edge features. And we found if we also prune the weight of this module together with other weight, it will seriously hurt the performance, so we do not prune them in all of our experiments. Training Details and Hyper-parameter Conﬁguration. We conduct numerous experiments with different hyper- parameter, such as iterations, learning rate, γ1, γ2, and we choose the best hyper-parameter conﬁguration to report the ﬁnal results. All the training details and hyper-parameters are summarzed in Table A4. As for Ogbn-Proteins dataset, due to its too large scale, we use the commonly used ran- dom sample method (Li et al., 2020a) to train the whole graph. Speciﬁcally, we random sample ten subgraphs from the whole graph and we only feed one subgraph to the GCN at each iteration. For each subgraph, we train 10 iterations to ensure better convergence. And we train 100 epochs for the whole graph (100 iterations for each subgraph). Evaluation Details We report the test accuracy/ROC- AUC/Hits@50 according to the best validation results dur- ing the training process to avoid overﬁtting. All training and evaluation are conducted for one run. As for the previous state-of-the-art method ADMM (Li et al., 2020b), we use the same training and evaluation setting as the original pa- per description, and we can reproduce the similar results compared with original paper. Computing Infrastructures We use the NVIDIA Tesla V100 (32GB GPU) to conduct all our experiments. A2. More Experiment Results A2.1. Node Classiﬁcation on Small- and Medium-scale Graphs with shallow GNNs As shown in Figure A9, we also provide extensive re- sults over GNN sparsity of GCN/GIN/GAT on three small datasets, Cora/Citeseer/PubMed. We observe that UGS ﬁnds the graph wining tickets at a range of GNNs sparsity from 20% ∼90% without performance deterioration, which sig- niﬁcantly reduces MACs and the storage memory during both training and inference processes. A2.2. Link Prediction on Small- and Medium-scale Graphs with shallow GNNs More results of link prediction with GCN/GIN/GAT on Cora/Citeseer/PubMed datasets are shown in Figure A10. We observe the similar phenomenon as the node classiﬁca- tion task: using our proposed UGS can ﬁnd the graph wining tickets at a range of graph sparsity from 5% ∼50% and GNN sparsity from 20% ∼90% without performance dete- rioration, which greatly reduce the computational cost and storage space during both training and inference processes. A2.3. Large-scale Graphs with Deep ResGCNs More results of larger-scale graphs with deep ResGCNs are shown in Figure A11. Results show that our proposed UGS can found GLTs, which can reach the non-trivial spar- sity levels of graph 30% ∼50% and weight 20% ∼80% without performance deterioration.A Uniﬁed Lottery Ticket Hypothesis for Graph Neural Networks Table A4.Implementation details of node classiﬁcation and link prediction. Task Node Classiﬁcation Link Prediction Dataset Cora Citeseer PubMed Ogbn-ArXiv Ogbn-Proteins Cora Citeseer PubMed Ogbn-Collab Iteration 200 200 200 500 100 200 200 200 500 Learning Rate 8e-3 1e-2 1e-2 1e-2 1e-2 1e-3 1e-3 1e-3 1e-2 Optimizer Admm Admm Admm Admm Admm Admm Admm Admm Admm Weight Decay 8e-5 5e-4 5e-4 0 0 0 0 0 0 γ1 1e-2 1e-2 1e-6 1e-6 1e-1 1e-4 1e-4 1e-4 1e-6 γ2 1e-2 1e-2 1e-3 1e-6 1e-3 1e-4 1e-4 1e-4 1e-5 0.00 20.00 36.00 48.80 59.04 67.2373.7979.0383.2286.5891.4195.6098.85 GNN Sparsity (%) 30 40 50 60 70 80Accuracy (Cora) GCN UGS (Ours) Random Pruning ADMM Baseline GLT (59.04%) 0.00 20.00 36.00 48.80 59.04 67.2373.7979.0383.2286.5891.4195.6098.85 GNN Sparsity (%) 50 55 60 65 70 75 80 GIN UGS (Ours) Random Pruning Baseline GLT (20.00%) 0.00 20.00 36.00 48.80 59.04 67.2373.7979.0383.2286.5891.4195.6098.85 GNN Sparsity (%) 70 72 74 76 78 80 82 GAT UGS (Ours) Random Pruning Baseline GLT (93.13%) 0.00 20.00 36.00 48.80 59.04 67.2373.7979.0383.2286.5891.4195.6098.85 GNN Sparsity (%) 20 30 40 50 60 70Accuracy (Citeseer) UGS (Ours) Random Pruning ADMM Baseline GLT (94.50%) 0.00 20.00 36.00 48.80 59.04 67.2373.7979.0383.2286.5891.4195.6098.85 GNN Sparsity (%) 50 55 60 65 70 UGS (Ours) Random Pruning Baseline GLT (98.20%) 0.00 20.00 36.00 48.80 59.04 67.2373.7979.0383.2286.5891.4195.6098.85 GNN Sparsity (%) 58 60 62 64 66 68 70 72 UGS (Ours) Random Pruning Baseline GLT (95.60%) 0.00 20.00 36.00 48.80 59.04 67.2373.7979.0383.2286.5891.4195.6098.85 GNN Sparsity (%) 40 50 60 70 80Accuracy (PubMed) UGS (Ours) Random Pruning ADMM Baseline GLT (97.75%) 0.00 20.00 36.00 48.80 59.04 67.2373.7979.0383.2286.5891.4195.6098.85 GNN Sparsity 70 72 74 76 78 80 UGS (Ours) Random Pruning Baseline GLT (94.50%) 0.00 20.00 36.00 48.80 59.04 67.2373.7979.0383.2286.5891.4195.6098.85 GNN Sparsity (%) 74 75 76 77 78 79 80 UGS (Ours) Random Pruning Baseline GLT (97.75%) Figure A9.Node classiﬁcation performance over achieved GNNs sparsity of GCN, GIN, and GAT on Cora, Citeseer, and PubMed datasets, respectively. Red stars (#) indicate the located GLTs, which reach comparable performance with extreme GNN sparsity. Dash lines represent the baseline performance of unpruned GNNs on full graphs. A2.4. Graph Lottery Ticket with Pre-training More results of node classiﬁcation and link prediction on Cora and Citeseer dataset of GraphCL (You et al., 2020b) pre-training are shown in Figure A12. Results demonstrate that when using self-supervised pre-training, UGS can iden- tify graph lottery tickets with higher qualities. A2.5. More Analyses of Sparsiﬁed Graphs As shown in Table A5, graph measurements are reported, including clustering coefﬁcient, node and egde betweeness centrality. Results indicate that UGS seems to produce sparse graphs with more “critical” vertices which used to have more connections.A Uniﬁed Lottery Ticket Hypothesis for Graph Neural Networks 0.00 5.00 9.7514.2618.5522.6226.4930.1733.6636.9840.1343.1245.9648.6751.2353.6755.9958.1960.2862.2664.15 Graph Sparsity (%) 70 75 80 85 90ROC-AUC (Cora) GCN UGS (Ours) Random Pruning Baseline GLT (26.49%) 0.00 5.00 9.7514.2618.5522.6226.4930.1733.6636.9840.1343.1245.9648.6751.2353.6755.9958.1960.2862.2664.15 Graph Sparsity (%) 65 70 75 80 85 GIN UGS (Ours) Random Pruning Baseline GLT (22.62%) 0.00 5.00 9.7514.2618.5522.6226.4930.1733.6636.9840.1343.1245.9648.6751.2353.6755.9958.1960.2862.2664.15 Graph Sparsity (%) 85 86 87 88 89 90 91 GAT UGS (Ours) Random Pruning Baseline GLT (43.12%) 0.00 20.00 36.00 48.80 59.04 67.2373.7979.0383.2286.5891.4195.6098.85 GNN Sparsity (%) 70 75 80 85 90ROC-AUC (Cora) UGS (Ours) Random Pruning Baseline GLT (73.79%) 0.00 20.00 36.00 48.80 59.04 67.2373.7979.0383.2286.5891.4195.6098.85 GNN Sparsity (%) 65 70 75 80 85 UGS (Ours) Random Pruning Baseline GLT (67.23%) 0.00 20.00 36.00 48.80 59.04 67.2373.7979.0383.2286.5891.4195.6098.85 GNN Sparsity (%) 85 86 87 88 89 90 91 UGS (Ours) Random Pruning Baseline GLT (91.41%) 0.00 5.00 9.7514.2618.5522.6226.4930.1733.6636.9840.1343.1245.9648.6751.2353.6755.9958.1960.2862.2664.15 Graph Sparsity (%) 80 85 90 95ROC-AUC (Citeseer) UGS (Ours) Random Pruning Baseline GLT (33.66%) 0.00 5.00 9.7514.2618.5522.6226.4930.1733.6636.9840.1343.1245.9648.6751.2353.6755.9958.1960.2862.2664.15 Graph Sparsity (%) 75 80 85 90 95 UGS (Ours) Random Pruning Baseline GLT (26.49%) 0.00 5.00 9.7514.2618.5522.6226.4930.1733.6636.9840.1343.1245.9648.6751.2353.6755.9958.1960.2862.2664.15 Graph Sparsity (%) 91 92 93 94 95 96 UGS (Ours) Random Pruning Baseline GLT (43.12%) 0.00 20.00 36.00 48.80 59.04 67.2373.7979.0383.2286.5891.4195.6098.85 GNN Sparsity (%) 80 85 90 95ROC-AUC (Citeseer) UGS (Ours) Random Pruning Baseline GLT (83.22%) 0.00 20.00 36.00 48.80 59.04 67.2373.7979.0383.2286.5891.4195.6098.85 GNN Sparsity (%) 75 80 85 90 95 UGS (Ours) Random Pruning Baseline GLT (73.79%) 0.00 20.00 36.00 48.80 59.04 67.2373.7979.0383.2286.5891.4195.6098.85 GNN Sparsity (%) 91 92 93 94 95 96 UGS (Ours) Random Pruning Baseline GLT (91.41%) 0.00 5.00 9.7514.2618.5522.6226.4930.1733.6636.9840.1343.1245.9648.6751.2353.6755.9958.1960.2862.2664.15 Graph Sparsity (%) 86 87 88 89 90 91 92 93ROC-AUC (PubMed) UGS (Ours) Random Pruning Baseline GLT (40.13%) 0.00 5.00 9.7514.2618.5522.6226.4930.1733.6636.9840.1343.1245.9648.6751.2353.6755.9958.1960.2862.2664.15 Graph Sparsity (%) 80 82 84 86 88 90 UGS (Ours) Random Pruning Baseline GLT (14.26%) 0.00 5.00 9.7514.2618.5522.6226.4930.1733.6636.9840.1343.1245.9648.6751.2353.6755.9958.1960.2862.2664.15 Graph Sparsity (%) 88 89 90 91 92 93 94 UGS (Ours) Random Pruning Baseline GLT (55.99%) 0.00 20.00 36.00 48.80 59.04 67.2373.7979.0383.2286.5891.4195.6098.85 GNN Sparsity (%) 86 87 88 89 90 91 92 93ROC-AUC (PubMed) UGS (Ours) Random Pruning Baseline GLT (89.26%) 0.00 20.00 36.00 48.80 59.04 67.2373.7979.0383.2286.5891.4195.6098.85 GNN Sparsity (%) 80 82 84 86 88 90 UGS (Ours) Random Pruning Baseline GLT (48.80%) 0.00 20.00 36.00 48.80 59.04 67.2373.7979.0383.2286.5891.4195.6098.85 GNN Sparsity (%) 88 89 90 91 92 93 94 UGS (Ours) Random Pruning Baseline GLT (97.19%) Figure A10.Link prediction performance over achieved GNNs sparsity of GCN, GIN, and GAT on Cora, Citeseer, and PubMed datasets, respectively. Red stars (#) indicate the located GLTs, which reach comparable performance with the extreme graph sparsity and GNN sparsity. Dash lines represent the baseline performance of unpruned GNNs on full graphs.A Uniﬁed Lottery Ticket Hypothesis for Graph Neural Networks 0.00 5.00 9.7514.2618.5522.6226.4930.1733.6636.9840.1343.1245.9648.6751.2353.6755.9958.1960.2862.2664.15 Graph Sparsity (%) 56 58 60 62 64 66 68 70 72 74Accuracy Ogbn-ArXiv UGS (Ours) Random Pruning Baseline GLT (45.96%) 0.00 5.00 9.7514.2618.5522.6226.4930.1733.6636.9840.1343.1245.9648.6751.2353.6755.9958.1960.2862.2664.15 Graph Sparsity (%) 78 79 80 81 82 83 84 85 86ROC-AUC Ogbn-Proteins UGS (Ours) Random Pruning Baseline GLT (26.49%) 0.00 5.00 9.7514.2618.5522.6226.4930.1733.6636.9840.1343.1245.9648.6751.2353.6755.9958.1960.2862.2664.15 Graph Sparsity (%) 15 20 25 30 35 40 45 50 55Hits@50 Ogbl-Collab UGS (Ours) Random Pruning Baseline GLT (26.49%) 0.00 20.00 36.00 48.80 59.04 67.2373.7979.0383.2286.5891.4195.6098.85 GNN Sparsity (%) 56 58 60 62 64 66 68 70 72 74Accuracy UGS (Ours) Random Pruning Baseline GLT (93.13%) 0.00 20.00 36.00 48.80 59.04 67.2373.7979.0383.2286.5891.4195.6098.85 GNN Sparsity (%) 78 79 80 81 82 83 84 85 86ROC-AUC UGS (Ours) Random Pruning Baseline GLT (73.79%) 0.00 20.00 36.00 48.80 59.04 67.2373.7979.0383.2286.5891.4195.6098.85 GNN Sparsity (%) 15 20 25 30 35 40 45 50 55Hits@50 UGS (Ours) Random Pruning Baseline GLT (73.79%) Figure A11.Node classiﬁcation and link prediction performance over achieved graph sparsity and GNN sparsity of 28-layer deep ResGCNs on large-scale graph datasets. 0.005.009.7514.2618.5522.6226.4930.1733.6636.9840.1343.1245.9651.2355.9960.2864.15 Graph Sparsity (%) 72 74 76 78 80 82Accuracy (Cora) Node Classification UGS GraphCL+UGS 0.005.009.7514.2618.5522.6226.4930.1733.6636.9840.1343.1245.9651.2355.9960.2864.15 Graph Sparsity (%) 82 84 86 88 90 92 94ROC-AUC (Cora) Link Predication UGS GraphCL+UGS 0.00 20.00 36.00 48.80 59.0467.2373.7979.0386.5891.4198.85 GNN Sparsity (%) 72 74 76 78 80 82Accuracy (Cora) Node Classification UGS GraphCL+UGS 0.00 20.00 36.00 48.80 59.0467.2373.7979.0386.5891.4198.85 GNN Sparsity (%) 82 84 86 88 90 92 94ROC-AUC (Cora) Link Predication UGS GraphCL+UGS 0.005.009.7514.2618.5522.6226.4930.1733.6636.9840.1343.1245.9651.2355.9960.2864.15 Graph Sparsity (%) 68 69 70 71 72 73 74Accuracy (Citeseer) UGS GraphCL+UGS 0.005.009.7514.2618.5522.6226.4930.1733.6636.9840.1343.1245.9651.2355.9960.2864.15 Graph Sparsity (%) 88 90 92 94 96 98ROC-AUC (Citeseer) UGS GraphCL+UGS 0.00 20.00 36.00 48.80 59.0467.2373.7979.0386.5891.4198.85 GNN Sparsity (%) 68 69 70 71 72 73 74Accuracy (Citeseer) UGS GraphCL+UGS 0.00 20.00 36.00 48.80 59.0467.2373.7979.0386.5891.4198.85 GNN Sparsity (%) 88 90 92 94 96 98ROC-AUC (Citeseer) UGS GraphCL+UGS Figure A12.Drawing graph lottery tickets from randomly initialized and self-supervised pre-trained (GraphCL (You et al., 2020b)) GCNs on node classiﬁcation (low label rate) and link prediction. Corresponding results over achieved graph and GNN sparsity are presented. Table A5.Graph measurements of original graphs, sparse graphs from UGS, random pruning, and ADMM sparsiﬁcation. Measurements Cora Citeseer PubMed Original UGS RP ADMM Original UGS RP ADMM Original UGS RP ADMM Clustering Coefﬁcient0.141470.076110.089290.045500.240670.158550.15407 0.086090.060180.036580.037490.02470 Node Betweenness0.001020.000860.00066 0.000210.001650.001900.00158 0.001220.000270.000240.00022 0.00018 Edge Betweenness0.000810.000870.00068 0.000320.001010.001440.00123 0.001370.000140.000190.00015 0.00018",
      "meta_data": {
        "arxiv_id": "2102.06790v2",
        "authors": [
          "Tianlong Chen",
          "Yongduo Sui",
          "Xuxi Chen",
          "Aston Zhang",
          "Zhangyang Wang"
        ],
        "published_date": "2021-02-12T21:52:43Z",
        "pdf_url": "https://arxiv.org/pdf/2102.06790v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces a Unified GNN Sparsification (UGS) framework that simultaneously prunes the graph adjacency matrix and the GNN model weights to address the computational and memory bottlenecks in GNNs on large-scale graphs. It further generalizes the Lottery Ticket Hypothesis (LTH) to Graph Neural Networks (GNNs) for the first time, defining a Graph Lottery Ticket (GLT) as a pair of a core sub-dataset and a sparse sub-network. These GLTs can be jointly identified from the original GNN and the full dense graph by iteratively applying UGS. GLTs can be trained in isolation from random initialization or self-supervised pre-training to match the performance of the full model and graph, achieving substantial MACs savings (20-98% for node classification and 48-97% for link prediction) without compromising predictive performance across various GNN architectures and diverse tasks on both small and large-scale datasets.",
        "methodology": "The core methodology is the Unified GNN Sparsification (UGS) framework. UGS introduces two differentiable masks: 'mg' for the graph adjacency matrix (A) and 'mθ' for the GNN model weights (Θ). These masks are co-optimized end-to-end using an objective function 'LUGS' that combines the standard GNN loss with L1 sparsity regularizers for both 'mg' and 'mθ'. The UGS algorithm iteratively updates model weights and masks through backpropagation. After each training round, the lowest-magnitude elements in 'mg' and 'mθ' are set to zero based on predefined ratios ('pg' and 'pθ'). To find Graph Lottery Tickets (GLTs), the UGS algorithm is iteratively applied, and after each pruning step, the GNN weights are rewound to their original initialization. This iterative process continues until desired graph and GNN sparsity levels are reached. A GLT is successfully identified if the sparse sub-network trained on the sparse graph matches or surpasses the performance of the original GNN on the full graph.",
        "experimental_setup": "Experiments were conducted using various GNN architectures: two-layer GCN, GIN, and GAT for small- and medium-scale datasets, and 28-layer deep ResGCNs for large-scale datasets. The datasets included popular semi-supervised graphs (Cora, Citeseer, PubMed) for small/medium scale, and Open Graph Benchmark (OGB) datasets (Ogbn-ArXiv, Ogbn-Proteins, Ogbl-Collab) for large scale. Evaluation tasks covered node classification and link prediction. Metrics used were accuracy, ROC-AUC, and Hits@50, depending on the task and dataset. The performance of UGS and GLTs was compared against random pruning and a previous state-of-the-art graph sparsification method (ADMM). Ablation studies were performed on pruning ratios ('pg' and 'pθ'). The viability of drawing GLTs from self-supervised pre-trained GNNs (using GraphCL) was also explored. Training involved specific splits for train/validation/test sets, optimized hyperparameters (learning rate, weight decay, γ1, γ2), and a 32GB NVIDIA Tesla V100 GPU infrastructure. For large datasets like Ogbn-Proteins, a random subgraph sampling method was used during training.",
        "limitations": "The paper acknowledges that translating the high sparsity achieved by GLTs into practical acceleration and energy-saving benefits remains an open challenge. Current DNN accelerators are typically optimized for dense and regular computations, making it difficult to efficiently implement sparse, edge-based operations prevalent in GNNs. The authors note that hardware acceleration research for GNNs is still nascent. While the expectation is that GLT could be implemented using highly optimized sparse-dense matrix multiplication (SpMM) operations from existing libraries, this is a future consideration rather than a current, directly realized benefit of their method.",
        "future_research_directions": "Future research directions primarily involve translating the high sparsity of Graph Lottery Tickets (GLTs) into tangible practical acceleration and energy-saving benefits. This necessitates further exploration into hardware acceleration for GNNs, specifically focusing on designing accelerators capable of efficiently handling sparse and irregular graph structures and edge-based operations. Additionally, research could focus on optimizing the implementation of GLTs using existing highly optimized sparse matrix libraries, such as Intel MKL or cuSPARSE, for sparse-dense matrix multiplication (SpMM) operations."
      }
    },
    {
      "title": "Graph Lottery Ticket Automated"
    },
    {
      "title": "Degree-Quant: Quantization-Aware Training for Graph Neural Networks",
      "abstract": "Graph neural networks (GNNs) have demonstrated strong performance on a wide\nvariety of tasks due to their ability to model non-uniform structured data.\nDespite their promise, there exists little research exploring methods to make\nthem more efficient at inference time. In this work, we explore the viability\nof training quantized GNNs, enabling the usage of low precision integer\narithmetic during inference. We identify the sources of error that uniquely\narise when attempting to quantize GNNs, and propose an architecturally-agnostic\nmethod, Degree-Quant, to improve performance over existing quantization-aware\ntraining baselines commonly used on other architectures, such as CNNs. We\nvalidate our method on six datasets and show, unlike previous attempts, that\nmodels generalize to unseen graphs. Models trained with Degree-Quant for INT8\nquantization perform as well as FP32 models in most cases; for INT4 models, we\nobtain up to 26% gains over the baselines. Our work enables up to 4.7x speedups\non CPU when using INT8 arithmetic.",
      "full_text": "Published as a conference paper at ICLR 2021 DEGREE -QUANT : Q UANTIZATION -AWARE TRAINING FOR GRAPH NEURAL NETWORKS Shyam A. Tailor∗ Department of Computer Science & Technology University of Cambridge Javier Fernandez-Marques* Department of Computer Science University of Oxford Nicholas D. Lane Department of Computer Science and Technology University of Cambridge & Samsung AI Center ABSTRACT Graph neural networks (GNNs) have demonstrated strong performance on a wide variety of tasks due to their ability to model non-uniform structured data. Despite their promise, there exists little research exploring methods to make them more efﬁ- cient at inference time. In this work, we explore the viability of training quantized GNNs, enabling the usage of low precision integer arithmetic during inference. For GNNs seemingly unimportant choices in quantization implementation cause dra- matic changes in performance. We identify the sources of error that uniquely arise when attempting to quantize GNNs, and propose an architecturally-agnostic and stable method, Degree-Quant, to improve performance over existing quantization- aware training baselines commonly used on other architectures, such as CNNs. We validate our method on six datasets and show, unlike previous attempts, that models generalize to unseen graphs. Models trained with Degree-Quant for INT8 quantization perform as well as FP32 models in most cases; for INT4 models, we obtain up to 26% gains over the baselines. Our work enables up to 4.7×speedups on CPU when using INT8 arithmetic. 1 I NTRODUCTION GNNs have received substantial attention in recent years due to their ability to model irregularly structured data. As a result, they are extensively used for applications as diverse as molecular interactions (Duvenaud et al., 2015; Wu et al., 2017), social networks (Hamilton et al., 2017), recommendation systems (van den Berg et al., 2017) or program understanding (Allamanis et al., 2018). Recent advancements have centered around building more sophisticated models including new types of layers (Kipf & Welling, 2017; Velickovic et al., 2018; Xu et al., 2019) and better aggregation functions (Corso et al., 2020). However, despite GNNs having few model parameters, the compute required for each application remains tightly coupled to the input graph size. A 2-layer Graph Convolutional Network (GCN) model with 32 hidden units would result in a model size of just 81KB but requires 19 GigaOPs to process the entire Reddit graph. We illustrate this growth in ﬁg. 1. One major challenge with graph architectures is therefore performing inference efﬁciently, which limits the applications they can be deployed for. For example, GNNs have been combined with CNNs for SLAM feature matching (Sarlin et al., 2019), however it is not trivial to deploy this technique on smartphones, or even smaller devices, whose neural network accelerators often do not implement ﬂoating point arithmetic, and instead favour more efﬁcient integer arithmetic. Integer quantization is one way to lower the compute, memory and energy budget required to perform inference, without requiring modiﬁcations to the model architecture; this is also useful for model serving in data centers. Although quantization has been well studied for CNNs and language models (Jacob et al., 2017; Wang et al., 2018; Zafrir et al., 2019; Prato et al., 2019), there remains relatively little work addressing ∗Equal contribution. Correspondence to: Shyam Tailor <sat62@cam.ac.uk> 1 arXiv:2008.05000v3  [cs.LG]  15 Mar 2021Published as a conference paper at ICLR 2021 Cora Citeseer Pubmed Reddit Amazon 108 109 1010 1011 1012 1013 OPs ResNet18* (44MB) WideResNet101* (230MB) MobileNetV2* (13MB) * single 3x224x224 image 2-GAT 2-GCN 4-GAT 4-GCN 0.13MB 1.14MB 0.03MB 0.23MB Figure 1: Despite GNN model sizes rarely exceeding 1MB, the OPs needed for inference grows at least linearly with the size of the dataset and node features. GNNs with models sizes 100×smaller than popular CNNs require many more OPs to process large graphs. hl v hl+1 v CNN GNN hl+1 = hl ∗K hl+1 v = γ(hl v,⋀ u∈N(v)[φ(hl u,hl v,euv)]) Layer l Layer l+ 1 Figure 2: While CNNs operate on regular grids, GNNs operate on graphs with varying topology. A node’s neighborhood size and ordering varies for GNNs. Both architectures use weight sharing. GNN efﬁciency (Mukkara et al., 2018; Jia et al., 2020; Zeng & Prasanna, 2020; Yan et al., 2020). To the best of our knowledge, there is no work explicitly characterising the issues that arise when quantizing GNNs or showing latency beneﬁts of using low-precision arithmetic. The recent work of Wang et al. (2020) explores only binarized embeddings of a single graph type (citation networks). In Feng et al. (2020) a heterogeneous quantization framework assigns different bits to embedding and attention coefﬁcients in each layer while maintaining the weights at full precision (FP32). Due to the mismatch in operands’ bit-width the majority of the operations are performed at FP32 after data casting, making it impractical to use in general purpose hardware such as CPUs or GPUs. In addition they do not demonstrate how to train networks which generalize to unseen input graphs. Our framework relies upon uniform quantization applied to all elements in the network and uses bit-widths (8-bit and 4-bit) that are supported by off-the-shelf hardware such as CPUs and GPU for which efﬁcient low-level operators for common operations found in GNNs exists. This work considers the motivations and problems associated with quantization of graph architectures, and provides the following contributions: • The explanation of the sources of degradation in GNNs when using lower precision arith- metic. We show how the choice of straight-through estimator (STE) implementation, node degree, and method for tracking quantization statistics signiﬁcantly impacts performance. • An architecture-agnostic method for quantization-aware training on graphs, Degree-Quant (DQ), which results in INT8 models often performing as well as their FP32 counterparts. At INT4, models trained with DQ typically outperform quantized baselines by over 20%. We show, unlike previous work, that models trained with DQ generalize tounseen graphs. We provide code at this URL: https://github.com/camlsys/degree-quant. • We show that quantized networks achieve up to 4.7×speedups on CPU with INT8 arithmetic, relative to full precision ﬂoating point, with 4-8×reductions in runtime memory usage. 2 B ACKGROUND 2.1 M ESSAGE PASSING NEURAL NETWORKS (MPNN S) Many popular GNN architectures may be viewed as generalizations of CNN architectures to an irregular domain: at a high level, graph architectures attempt to build representations based on a node’s neighborhood (see ﬁg. 2). Unlike CNNs, however, this neighborhood does not have a ﬁxed ordering or size. This work considers GNN architectures conforming to the MPNN paradigm (Gilmer et al., 2017). A graph G= (V,E) has node features X ∈RN×F , an incidence matrix I ∈N2×E, and optionally D- dimensional edge features E ∈RE×D. The forward pass through an MPNN layer consists of message passing, aggregation and update phases:h(i) l+1 = γ(h(i) l ,⋀ j∈N(i)[φ(h(j) l ,h(i) l ,eij)]). Messages from 2Published as a conference paper at ICLR 2021 node uto node vare calculated using function φ, and are aggregated using a permutation-invariant function ⋀. The features at vare subsequently updated using γ. We focus on three architectures with corresponding update rules: 1. Graph Convolution Network (GCN):h(i) l+1 = ∑ j∈N(i)∪{i}( 1√ didj Wh(j) l ) (Kipf & Welling, 2017), where di refers to the degree of node i. 2. Graph Attention Network (GAT): h(i) l+1 = αi,iWh(i) l + ∑ j∈N(i)(αi,jWh(j) l ), where α represent attention coefﬁcients (Velickovic et al., 2018). 3. Graph Isomorphism Network (GIN): h(i) l+1 = fΘ[(1 +ϵ)h(i) l + ∑ j∈N(i) h(j) l ], where f is a learnable function (e.g. a MLP) and ϵis a learnable constant (Xu et al., 2019). 2.2 Q UANTIZATION FOR NON-GRAPH NEURAL NETWORKS Quantization allows for model size reduction and inference speedup without changing the model architecture. While there exists extensive studies of the impact of quantization at different bit- widths (Courbariaux et al., 2015; Han et al., 2015; Louizos et al., 2017) and data formats (Micikevicius et al., 2017; Carmichael et al., 2018; Kalamkar et al., 2019), it is 8-bit integer (INT8) quantization that has attracted the most attention. This is due to INT8 models reaching comparable accuracy levels to FP32 models (Krishnamoorthi, 2018; Jacob et al., 2017), offer a 4×model compression, and result in inference speedups on off-the-shelf hardware as 8-bit arithmetic is widely supported. Quantization-aware training (QAT) has become the de facto approach towards designing robust quantized models with low error (Wang et al., 2018; Zafrir et al., 2019; Wang et al., 2018). In their simplest forms, QAT schemes involve exposing the numerical errors introduced by quantization by simulating it on the forward pass Jacob et al. (2017) and make use of STE (Bengio et al., 2013) to compute the gradients—as if no quantization had been applied. For integer QAT, the quantization of a tensor xduring the forward pass is often implemented as: xq = min(qmax,max(qmin,⌊x/s+ z⌋)), where qmin and qmax are the minimum and maximum representable values at a given bit-width and signedness, sis the scaling factor makingxspan the [qmin,qmax] range and, zis the zero-point, which allows for the real value 0 to be representable in xq. Both sand zare scalars obtained at training time. hen, the tensor is dequantized as: ˆx= (xq −z)s, where the resulting tensor ˆx∼xfor a high enough bit-width. This similarity degrades at lower bit-widths. Other variants of integer QAT are presented in Jacob et al. (2017) and Krishnamoorthi (2018). To reach performance comparable to FP32 models, QAT schemes often rely on other techniques such as gradient clipping, to mask gradient updates based on the largest representable value at a given bit-width; stochastic, or noisy, QAT, which stochastically applies QAT to a portion of the weights at each training step (Fan et al., 2020; Dong et al., 2017); or the re-ordering of layers (Sheng et al., 2018; Alizadeh et al., 2019). 3 Q UANTIZATION FOR GNN S In this section, we build an intuition for why GNNs would fail with low precision arithmetic by identifying the sources of error that will disproportionately affect the accuracy of a low precision model. Using this insight, we propose our technique for QAT with GNNs, Degree-Quant. Our analysis focuses on three models: GCN, GAT and GIN. This choice was made as we believe that these are among the most popular graph architectures, with strong performance on a variety of tasks (Dwivedi et al., 2020), while also being representative of different trends in the literature. 3.1 S OURCES OF ERROR QAT relies upon the STE to make an estimate of the gradient despite the non-differentiable rounding operation in the forward pass. If this approximation is inaccurate, however, then poor performance will be obtained. In GNN layers, we identify the aggregation phase, where nodes combine messages from a varying number of neighbors in a permutation-invariant fashion, as a source of substantial numerical error, especially at nodes with high in-degree. Outputs from aggregation have magnitudes 3Published as a conference paper at ICLR 2021 Figure 3: Analysis of values collected immediately after aggregation at the ﬁnal layer of FP32 GNNs trained on Cora. Generated using channel data collected from 100 runs for each architecture. As in-degree grows, so does the mean and variance of channel values after aggregation. that vary signiﬁcantly depending on a node’s in-degree: as it increases, the variance of aggregation values will increase.1 Over the course of training qmin and qmax, the quantization range statistics, become severely distorted by infrequent outliers, reducing the resolution for the vast majority of values observed. This reults in increased rounding error for nodes with smaller in-degrees. Controlling qmin and qmax hence becomes a trade-off balancing truncation error and rounding error. We can derive how the mean and variance of the aggregation output values vary as node in-degree,n, increases for each of the three GNN layers. Suppose we model incoming message values for a single output dimension with random variables Xi, without making assumptions on their exact distribution or independence. Further, we use Yn as the random variable representing the value of node output after the aggregation step. With GIN layers, we have Yn = (1 +ϵ)X0 + ∑n i=1 Xi. It is trivial to prove that E(Yn) = O(n). The variance of the aggregation output is also O(n) in the case that that ∑ i̸=j Cov(Xi,Xj) ≪∑ i Var(Xi). We note that if ∑ i̸=j Cov(Xi,Xj) is large then it implies that the network has learned highly redundant features, and may be a sign of over-ﬁtting. Similar arguments can be made for GCN and GAT layers; we would expect GCN aggregation values to grow like O(√n), and GAT aggregation values to remain constant (O(1)) due to the attention coefﬁcients. We empirically validate these predictions on GNNs trained on Cora; results are plotted in ﬁg. 3. We see that the aggregation values do follow the trends predicted, and that for the values of in-degree in the plot (up to 168) the covariance terms can be neglected. As expected, the variance and mean of the aggregated output grow fastest for GIN, and are roughly constant for GAT as in-degree increases. From this empirical evidence, it would be expected that GIN layers are most affected by quantization. By using GIN and GCN as examples, we can see how aggregation error causes error in weight updates. Suppose we consider a GIN layer incorporating one weight matrix in the update function i.e. h(i) l+1 = f(Wy(i) GIN), where f is an activation function, y(i) GIN = (1 +ϵ)h(i) l + ∑ j∈N(i) h(j) l , and N(i) denotes the in-neighbors of node i. Writing y(i) GCN = ∑ k∈N(i)( 1√didk Wh(j) l ), we see that the derivatives of the loss with respect to the weights for GCN and GIN are: GIN ∂L ∂W = |V |∑ i=1 ( ∂L ∂h(i) l+1 ◦f′(Wy(i) GIN) ) y(i)⊤ GIN GCN ∂L ∂W = |V |∑ i=1 ∑ j∈N(i) 1√ didj ( ∂L ∂h(i) l+1 ◦f′(y(i) GCN) ) h(j)⊤ l The larger the error in y(i) GIN—caused by aggregation error—the greater the error in the weight gradients for GIN, which results in poorly performing models being obtained. The same argument applies to GCN, with the h(j)⊤ l and y(i) GCN terms introducing aggregation error into the weight updates. 3.2 O UR METHOD : D EGREE -QUANT To address these sources of error we propose Degree-Quant (DQ), a method for QAT with GNNs. We consider both inaccurate weight updates and unrepresentative quantization ranges. 1The reader should note that we are not referring to the concept of estimator variance, which is the subject of sampling based approaches—we are exclusively discussing the variance of values immediately after aggregation. 4Published as a conference paper at ICLR 2021 Protect & Quantize . . . Aggregate & Update. . . Figure 4: High-level view of the stochastic element of Degree-Quant. Protected (high in-degree) nodes, in blue, operate at full precision, while unprotected nodes (red) operate at reduced precision. High in-degree nodes contribute most to poor gradient estimates, hence they are stochastically protected from quantization more often. Algorithm 1 Degree-Quant (DQ). Functions accepting a protective mask m perform only the masked computa- tions at full precision: intermediate tensors are not quantized. At test time protective masking is disabled. In ﬁg. 11 (in the Appendix) we show with a diagram how a GCN layers makes use of DQ. 1: procedure TRAIN FORWARD PASS (G,p) 2: ⊿Calculate mask and quantized weights, Θ′, which all operations share 3: m ←BERNOULLI (p) 4: Θ′←QUANTIZE (Θ) 5: ⊿Messages with masked sources are at full precision (excluding weights) 6: M←MESSAGE CALCULATE (G,Θ′,m) 7: X ←QUANTIZE (AGGREGATE (M,Θ′,m), m) ⊿No quantization for masked nodes 8: return UPDATE (X,Θ′,m) ⊿Quantized weights always used 9: end procedure Stochastic Protection from Quantization to Improve Weight Update Accuracy . DQ aims to encourage more accurate weight updates by stochastically protecting nodes in the network from quantization. At each layer a protective node mask is generated; all masked nodes have the phases of the message passing, aggregation and update performed at full precision. This includes messages sent by protected nodes to other nodes, as shown in ﬁg. 4 (a detailed diagram is shown in ﬁg. 11). It is also important to note that the weights used at all nodes are the same quantized weights; this is motivated by the fact that our method is used to encourage more accurate gradients to ﬂow back to the weights through high in-degree nodes. At test time protection is disabled: all nodes operate at low precision. To generate the mask, we pre-process each graph before training and create a vector of probabilities p with length equal to the number of nodes. At training time, mask m is generated by sampling using the Bernoulli distribution: m ∼Bernoulli(p). In our scheme pi is higher if the in-degree of node i is large, as we ﬁnd empirically that high in-degree nodes contribute most towards error in weight updates. We use a scheme with two hyperparameters, pmin and pmax; nodes with the maximum in-degree are assigned pmax as their masking probability, with all other nodes assigned a probability calculated by interpolating between pmin and pmax based on their in-degree ranking in the graph. Percentile Tracking of Quantization Ranges . Figure 3 demonstrates large ﬂuctuations in the variance of the aggregation output as in-degree increases. Since these can disproportionately affect the ranges found by using min-max or momentum-based quantization, we propose using percentiles. While percentiles have been used for post-training quantization (Wu et al., 2020), we are the ﬁrst (to the best of our knowledge) to propose making it a core part of QAT; we ﬁnd it to be a key contributor to achieving consistent results with graphs. Using percentiles involves ordering the values in the tensor and clipping a fraction of the values at both ends of the distribution. The fraction to clip is a hyperparameter. We are more aggressive than existing literature on the quantity we discard: we clip the top and bottom 0.1%, rather than 0.01%, as we observe the ﬂuctuations to be a larger issue with GNNs than with CNNs or DNNs. Quantization ranges are more representative of the vast majority of values in this scheme, resulting in less rounding error. We emphasize that a core contribution of DQ is that it is architecture-agnostic. Our method enables a wide variety of architectures to use low precision arithmetic at inference time. Our method is also or- thogonal—and complementary—to other techniques for decreasing GNN computation requirements, such as sampling based methods which are used to reduce memory consumption (Zeng et al., 2020), or weight pruning (Blalock et al., 2020) approaches to achieve further model compression. 5Published as a conference paper at ICLR 2021 vanillaSTE STE with Gradient ClippingDataset Model min/max momentum min/max momentumArch. W8A8 W4A4 W8A8 W4A4 W8A8 W4A4 W8A8 W4A4 Cora(Acc. %)↑ GCN 81.0±0.7 65.3±4.9 42.3±11.1 49 .4±8.8 80.8±0.8 62 .3±5.2 66.9±18.2 77.2±2.5GAT 76.0±2.2 16 .8±8.5 81.7±1.3 51.7±5.8 76.4±2.6 15 .4±8.1 81.9±0.7 47.4±5.0GIN 69.9±1.9 25 .9±2.6 49.2±10.2 42.8±4.0 69.2±2.3 29 .5±3.5 75.1±1.1 40.5±5.0 MNIST(Acc. %)↑ GCN 90.4±0.2 51.3±7.5 90.1±0.5 70.6±2.4 90.4±0.3 54 .8±1.5 90.2±0.4 10 .3±0.0GAT 95.8±0.1 20.1±3.3 95.7±0.3 67 .4±3.2 95.7±0.1 30 .2±7.4 95.7±0.3 76.3±1.2GIN 96.5±0.3 62.4±21.8 96.7±0.2 91 .0±0.6 96.4±0.4 19 .5±2.1 75.3±18.1 10 .8±0.9 ZINC(Loss)↓ GCN 0.486±0.01 0.747±0.02 0.509±0.01 0.710±0.05 0.495±0.01 0.766±0.02 0.483±0.01 0.692±0.01GAT 0.471±0.01 0.740±0.02 0.571±0.03 0.692±0.06 0.466±0.01 0.759±0.04 0.463±0.01 0.717±0.03GIN 0.393±0.02 1.206±0.27 0.386±0.03 0.572±0.02 0.390±0.02 1.669±0.10 0.388±0.02 0.973±0.24 Table 1: Impact on performance of four typical quantization implementations for INT8 and INT4. The conﬁguration that resulted in best performing models for each dataset-model pair is bolded. Hyperparameters for each experiment were ﬁne-tuned independently. As expected, adding clipping does not change performance with min/max but does with momentum. A major contribution of this work is identifying that seemingly unimportant choices in quantization implementation cause dramatic changes in performance. 4 E XPERIMENTS In this section we ﬁrst analyse how the choice of quantization implementation affects performance of GNNs. We subsequently evaluate Degree-Quant against the strong baselines of: FP32, INT8-QAT and, INT8-QAT with stochastic masking of weights (Fan et al., 2020). We refer to this last approach as noisy QAT or nQAT. To make explicit that we are quantizing both weights and activations, we use the notation W8A8. We repeat the experiments at INT4. Our study evaluates performance on six datasets and includes both node-level and graph-level tasks. The datasets used were Cora, CiteSeer, ZINC, MNIST and CIFAR10 superpixels, and REDDIT-BINARY . Across all datasets INT8 models trained with Degree-Quant manage to recover most of the accuracy lost as a result of quantization. In some instances, DQ-INT8 outperform the extensively tuned FP32 baselines. For INT4, DQ outperforms all QAT baselines and results in double digits improvements over QAT-INT4 in some settings. Details about each dataset and our experimental setup can be found in appendix A.1. 4.1 I MPACT OF QUANTIZATION GRADIENT ESTIMATOR ON CONVERGENCE The STE is a workaround for when the forward pass contains non-differentiable operations (e.g. round- ing in QAT) that has been widely adopted in practice. While the choice of STE implementation generally results in marginal differences for CNNs—even for binary networks (Alizadeh et al., 2019)—it is unclear whether only marginal differences will also be observed for GNNs. Motivated by this, we study the impact of four off-the-shelve quantization procedures on the three architectures evaluated for each type of dataset; the implementation details of each one is described in appendix A.3. We perform this experiment to ensure that we have the strongest possible QAT baselines. Results are shown in table 1. We found the choice quantization implementation to be highly dependent on the model architecture and type of problem to be solved: we see a much larger variance than is observed with CNNs; this is an important discovery for future work building on our study. We observe a general trend in all INT4 experiments beneﬁting from momentum as it helps smoothing out the quantization statistics for the inherently noisy training stage at low bitwidths. This trend applies as well for the majority of INT8 experiments, while exhibiting little impact on MNIST. For INT8 Cora-GCN, large gradient norm values in the early stages of training (see ﬁg. 5) mean that these models not beneﬁt from momentum as quantization ranges fail to keep up with the rate of changes in tensor values; higher momentum can help but also leads to instability. In contrast, GAT has stable initial training dynamics, and hence obtains better results with momentum. For the molecules dataset ZINC, we consistently obtained lower regression loss when using momentum. We note that GIN models often suffer from higher performance degradation (as was ﬁrst noted in ﬁg. 3), specially at W4A4. This is not the case however for image datasets using superpixels. We believe that datasets with Gaussian-like node degree distributions (see ﬁg. 9) are more tolerant of the imprecision introduced by quantization, compared to datasets with tailed distributions. We leave more in-depth analysis of how graph topology affects quantization as future work. 6Published as a conference paper at ICLR 2021 Quant. Model Node Classiﬁcation (Accuracy %) Graph Classiﬁcation (Accuracy %) Graph Regression (Loss) Scheme Arch. Cora ↑ Citeseer↑ MNIST↑ CIFAR-10↑ ZINC ↓ Ref. (FP32) GCN 81.4 ±0.7 71 .1 ±0.7 90 .0 ±0.2 54 .5 ±0.1 0 .469 ±0.002 GAT 83.1 ±0.4 72 .5 ±0.7 95 .6 ±0.1 65 .4 ±0.4 0 .463 ±0.002 GIN 77.6 ±1.1 66 .1 ±0.9 93 .9 ±0.6 53 .3 ±3.7 0 .414 ±0.009 Ours (FP32) GCN 81.2 ±0.6 71 .4 ±0.9 90 .9 ±0.4 58 .4 ±0.5 0 .450 ±0.008 GAT 83.2 ±0.3 72 .4 ±0.8 95 .8 ±0.4 65 .1 ±0.8 0 .455 ±0.006 GIN 77.9 ±1.1 65 .8 ±1.5 96 .4 ±0.4 57 .4 ±0.7 0 .334 ±0.024 QAT (W8A8) GCN 81.0 ±0.7 71 .3 ±1.0 90 .9 ±0.2 56 .4 ±0.5 0 .481 ±0.029 GAT 81.9 ±0.7 71 .2 ±1.0 95 .8 ±0.3 66 .3 ±0.4 0 .460 ±0.005 GIN 75.6 ±1.2 63 .0 ±2.6 96 .7 ±0.2 52 .4 ±1.2 0 .386 ±0.025 nQAT (W8A8) GCN 81.0 ±0.8 70 .7 ±0.8 91 .1 ±0.1 56 .2 ±0.5 0 .472 ±0.015 GAT 82.5 ±0.5 71 .2 ±0.7 96 .0 ±0.1 66 .7 ±0.2 0 .459 ±0.007 GIN 77.4 ±1.3 65 .1 ±1.4 96 .4 ±0.3 52 .7 ±1.4 0 .405 ±0.016 GCN 81.7 ±0.7 (+0.7) 71.0 ±0.9 (-0.3) 90.9 ±0.2 (-0.2) 56.3 ±0.1 (-0.1) 0.434 ±0.009 (+9.8) GAT 82.7 ±0.7 (+0.2) 71.6 ±1.0 (+0.4) 95.8 ±0.4 (-0.2) 67.7 ±0.5 (+1.0) 0.456 ±0.005 (+0.9)DQ (W8A8) GIN 78.7 ±1.4 (+1.3) 67.5 ±1.4 (+2.4) 96.6 ±0.1 (-0.1) 55.5 ±0.6 (+2.8) 0.357 ±0.014 (+7.5) QAT (W4A4) GCN 77.2 ±2.5 64 .1 ±4.1 70 .6 ±2.4 38 .1 ±1.6 0 .692 ±0.013 GAT 55.6 ±5.4 65 .3 ±1.9 76 .3 ±1.2 41 .0 ±1.1 0 .655 ±0.032 GIN 42.5 ±4.5 18 .6 ±2.9 91 .0 ±0.6 45 .6 ±3.6 0 .572 ±0.02 nQAT (W4A4) GCN 78.1 ±1.5 65 .8 ±2.6 70 .9 ±1.5 40 .1 ±0.7 0 .669 ±0.128 GAT 54.9 ±5.6 65 .5 ±1.7 78 .4 ±1.5 41 .0 ±0.6 0 .637 ±0.012 GIN 45.0 ±5.0 34 .6 ±3.8 91 .3 ±0.5 48 .7 ±1.7 0 .561 ±0.068 GCN 78.3 ±1.7 (+0.2) 66.9 ±2.4 (+1.1) 84.4 ±1.3 (+13.5) 51.1 ±0.7 (+11.0) 0.536 ±0.011 (+26.2) GAT 71.2 ±2.9 (+16.3) 67.6 ±1.5 (+2.1) 93.1 ±0.3 (+14.7) 56.5 ±0.6 (+15.5) 0.520 ±0.021 (+20.6)DQ (W4A4) GIN 69.9 ±3.4 (+24.9) 60.8 ±2.1 (+26.2) 95.5 ±0.4 (+4.2) 50.7 ±1.6 (+2.0) 0.431 ±0.012 (+23.2) Table 2: This table is divided into three sets of rows with FP32 baselines at the top. We provide two baselines for INT8 and INT4: standard QAT and stochastic QAT (nQAT). Both are informed by the analysis in 4.1, with nQAT achieving better performance in some cases. Models trained with Degree-Quant (DQ) are always comparable to baselines, and usually substantially better, especially for INT4. DQ is a stable method which requires little tuning to obtain excellent results across a variety of architectures and datasets. 4.2 O BTAINING QUANTIZATION BASELINES Our FP32 results, which we obtain after extensive hyperparameter tuning, and those from the baselines are shown at the top of table 2. We observed large gains on MNIST, CIFAR10 and, ZINC. For our QAT-INT8 and QAT-INT4 baselines, we use the quantization conﬁgurations informed by our analysis in section 4.1. For Citeseer we use the best resulting setup analysed for Cora, and for CIFAR- 10 that from MNIST. Then, the hyperparameters for each experiment were ﬁne tuned individually, including noise rate n ∈[0.5,0.95] for nQAT experiments. QAT-INT8 and QAT-INT4 results in table 2 and QAT-INT4, with the exception of MNIST (an easy to classify dataset), corroborate our hypothesis that GIN layers are less resilient to quantization. This was ﬁrst observed in ﬁg. 3. In the case of ZINC, while all models results in noticeable degradation, GIN sees a more severe 16% increase of regression loss compared to our FP32 baseline. For QAT W4A4 an accuracy drop of over 35% and 47% is observed for Cora and Citeseer respectively. The stochasticity induced by nQAT helped in recovering some of the accuracy lost as a result of quantization for citation networks (both INT8 and INT4) but had little impact on other datasets and harmed performance in some cases. 4.3 C OMPARISONS OF DEGREE -QUANT WITH EXISTING QUANTIZATION APPROACHES Degree-Quant provides superior quantization for all GNN datasets and architectures. Our results with DQ are highlighted in gray in table 2 and table 3. Citation networks trained with DQ for W8A8 manage to recover most of the accuracy lost as a result of QAT and outperform most of nQAT baselines. In some instances DQ-W8A8 models outperform the reference FP32 baselines. At 4-bits, DQ results in even larger gains compared to W4A4 baselines. We see DQ being more effective for GIN layers, outperforming INT4 baselines for Cora (+24.9%), Citeseer (+26.2%) and REDDIT- BINARY (+23.0%) by large margins. Models trained with DQ at W4A4 for graph classiﬁcation and graph regression also exhibit large performance gains (of over 10%) in most cases. For ZINC, all 7Published as a conference paper at ICLR 2021 Quantization Model REDDIT-BIN (Acc. %) ↑ Ref. (FP32) GIN 92.2 ±2.3 Ours (FP32) GIN 92.0 ±1.5 QAT-W8A8 GIN 76.1 ±7.5 nQAT-W8A8 GIN 77.5 ±3.4 DQ-W8A8 GIN 91.8 ±2.3 (+14.3) QAT-W4A4 GIN 54.4 ±6.6 nQAT-W4A4 GIN 58.0 ±6.3 DQ-W4A4 GIN 81.3 ±4.4 (+23.0) Table 3: Results for DQ-INT8 GIN models perform nearly as well as at FP32. For INT4, DQ offers a signiﬁcant increase in accuracy. Device Arch. Zinc (Batch=10K) Reddit FP32 W8A8 Speedup FP32 W8A8 Speedup CPU GCN 181ms 42ms 4.3 × 13.1s 3.1s 4.2 × GAT 190ms 50ms 3.8 × 13.1s 2.8s 4.7 × GIN 182ms 43ms 4.2 × 13.1s 3.1s 4.2 × GPU GCN 39ms 31ms 1.3 × 191ms 176ms 1.1 × GAT 17ms 15ms 1.1 × OOM OOM - GIN 39ms 31ms 1.3 × 191ms 176ms 1.1 × Table 4: INT8 latency results run on a 22 core 2.1GHz Intel Xeon Gold 6152 and, on a GTX 1080Ti GPU. Quantization provides large speedups on a variety of graphs for CPU and non-negligible speedups with unoptimized INT8 GPU kernels. models achieve over 20% lower regression loss. Among the top performing models using DQ, ratios of pmin and pmax in [0.0,0.2] were the most common. Figure 10 in the appendix shows validation loss curves for GIN models trained using different DQ probabilities on the REDDIT-BINARY dataset. 5 D ISCUSSION Latency and Memory Implications . In addition to offering signiﬁcantly lower memory usage (4×with INT8), quantization can reduce latency—especially on CPUs. We found that with INT8 arithmetic we could accelerate inference by up to 4.7×. We note that the latency beneﬁt depends on the graph topology and feature dimension, therefore we ran benchmarks on a variety of graph datasets, including Reddit2, Zinc, Cora, Citeseer, and CIFAR-10; Zinc and Reddit results are shown in table 4, with further results given in the appendix. For a GCN layer with in- and out-dimension of 128, we get speed-ups of: 4.3×on Reddit, 2.5×on Zinc, 1.3×on Cora, 1.3×on Citeseer and, 2.1×on CIFAR-10. It is also worth emphasizing that quantized networks are necessary to efﬁciently use accelerators deployed in smartphones and smaller devices as they primarily accelerate integer arithmetic, and that CPUs remain a common choice for model serving on servers. The decrease in latency on CPUs is due to improved cache performance for the sparse operations; GPUs, however, see less beneﬁt due to their massively-parallel nature which relies on mechanisms other than caching to hide slow random memory accesses, which are unavoidable in this application. Figure 5: qmax with absolute min/max and percentile ranges, applied to INT8 GCN training on Cora. We ob- serve that the percentile max is half that of the absolute, doubling resolution for the majority of values. Figure 6: Analysis of how INT8 GAT performance degrades on Cora as individual elements are reduced to 4-bit precision without DQ. For GAT the message elements are crucial to classiﬁcation performance. Ablation Study: Beneﬁts of Percentile Ranges . Figure 5 shows the value of percentiles during training. We see that when using absolute min/max the upper range grows to over double the range required for 99.9% of values, effectively halving the resolution of the quantized values. DQ is more stable, and we obtained strong results with an order of magnitude less tuning relative to the baselines. Ablation Study: Source of Degradation at INT4. Figure 6 assesses how INT8 GAT (without DQ) degrades as single elements are converted to INT4, in order to understand the precipitous drop in 2The largest graph commonly benchmarked on in the GNN literature 8Published as a conference paper at ICLR 2021 accuracy in the INT4 baselines; further plots for GCN and GIN are included in the appendix. We observe that most elements cause only modest performance losses relative to a full INT8 model. DQ is most important to apply to elements which are constrained by numerical precision, such as the aggregation and message elements in GAT. Weight elements, however, are consistently unaffected. Ablation Study: Effect of Stochastic Element in Degree-Quant . We observe that the stochastic protective masking in DQ alone often achieves most of the performance gain over the QAT baseline; results are given in table 9 in the appendix. The beneﬁt of the percentile-based quantization ranges is stability, although it can yield some performance gains. The full DQ method provides consistently good results on all architectures and datasets, without requiring an extensive analysis as in 4.1. 6 C ONCLUSION This work has presented Degree-Quant, an architecture-agnostic and stable method for training quantized GNN models that can be accelerated using off-the-shelf hardware. With 4-bit weights and activations we achieve 8×compression while surpassing strong baselines by margins regularly exceeding 20%. At 8-bits, models trained with DQ perform on par or better than the baselines while achieving up to 4.7×lower latency than FP32 models. Our work offers a comprehensive foundation for future work in this area and is a ﬁrst step towards enabling GNNs to be deployed more widely, including to resource constrained devices such as smartphones. ACKNOWLEDGEMENTS This work was supported by Samsung AI and by the UK’s Engineering and Physical Sciences Research Council (EPSRC) with grants EP/M50659X/1 and EP/S001530/1 (the MOA project) and the European Research Council via the REDIAL project. REFERENCES R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk. Slic superpixels compared to state-of-the-art superpixel methods. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(11):2274–2282, 2012. Milad Alizadeh, Javier Fernández-Marqués, Nicholas D. Lane, and Yarin Gal. A empirical study of binary neural networks’ optimisation. In International Conference on Learning Representations , 2019. URL https://openreview.net/forum?id=rJfUCoR5KX. Milad Alizadeh, Arash Behboodi, Mart van Baalen, Christos Louizos, Tijmen Blankevoort, and Max Welling. Gradient ℓ1 regularization for quantization robustness. arXiv preprint arXiv:2002.07520, 2020. Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning to represent programs with graphs. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview. net/forum?id=BJOFETxR-. Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation, 2013. Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John Guttag. What is the state of neural network pruning?, 2020. Zachariah Carmichael, Hamed F. Langroudi, Char Khazanov, Jeffrey Lillie, John L. Gustafson, and Dhireesha Kudithipudi. Deep positron: A deep neural network using the posit number system, 2018. Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Liò, and Petar Veliˇckovi´c. Principal neighbourhood aggregation for graph nets, 2020. Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during propagations, 2015. Yinpeng Dong, Renkun Ni, Jianguo Li, Yurong Chen, Jun Zhu, and Hang Su. Learning accurate low-bit deep neural networks with stochastic quantization, 2017. 9Published as a conference paper at ICLR 2021 David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alán Aspuru- Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular ﬁngerprints. In Advances in neural information processing systems, pp. 2224–2232, 2015. Vijay Prakash Dwivedi, Chaitanya K. Joshi, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Benchmark- ing graph neural networks, 2020. Steven K. Esser, Jeffrey L. McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra S. Modha. Learned step size quantization. In International Conference on Learning Representations , 2020. URL https://openreview.net/forum?id=rkgO66VKDS. Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, Remi Gribonval, Herve Jegou, and Armand Joulin. Training with quantization noise for extreme model compression, 2020. Boyuan Feng, Yuke Wang, Xu Li, Shu Yang, Xueqiao Peng, and Yufei Ding. Sgquant: Squeezing the last bit on graph neural networks with specialized quantization, 2020. Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric, 2019. Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry. CoRR, abs/1704.01212, 2017. URL http://arxiv.org/abs/1704. 01212. William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs, 2017. Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding, 2015. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift, 2015. Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efﬁcient integer-arithmetic-only inference, 2017. Zhihao Jia, Sina Lin, Mingyu Gao, Matei Zaharia, and Alex Aiken. Improving the accuracy, scalability, and performance of graph neural networks with roc. In Proceedings of Machine Learning and Systems 2020, pp. 187–198. 2020. Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for molecular graph generation, 2018. Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja V ooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen, Jiyan Yang, Jongsoo Park, Alexander Heinecke, Evangelos Georganas, Sudarshan Srinivasan, Abhisek Kundu, Misha Smelyanskiy, Bharat Kaul, and Pradeep Dubey. A study of bﬂoat16 for deep learning training, 2019. Thomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum? id=SJU4ayYgl. Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efﬁcient inference: A whitepaper, 2018. Liam Li, Kevin Jamieson, Afshin Rostamizadeh, Ekaterina Gonina, Jonathan Ben-tzur, Moritz Hardt, Benjamin Recht, and Ameet Talwalkar. A system for massively parallel hyperparameter tuning. In Proceedings of Machine Learning and Systems 2020, pp. 230–246. 2020. Christos Louizos, Karen Ullrich, and Max Welling. Bayesian compression for deep learning, 2017. Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training, 2017. Anurag Mukkara, Nathan Beckmann, Maleen Abeydeera, Xiaosong Ma, and Daniel Sanchez. Exploiting locality in graph analytics through hardware-accelerated traversal scheduling. In Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture, MICRO-51, pp. 1–14. IEEE Press, 2018. ISBN 9781538662403. doi: 10.1109/MICRO.2018.00010. URL https://doi.org/10.1109/MICRO. 2018.00010. 10Published as a conference paper at ICLR 2021 Gabriele Prato, Ella Charlaix, and Mehdi Rezagholizadeh. Fully quantized transformer for machine translation, 2019. Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional networks on node classiﬁcation. In International Conference on Learning Representations , 2020. URL https://openreview.net/forum?id=Hkx1qkrKPr. Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superglue: Learning feature matching with graph neural networks. arXiv preprint arXiv:1911.11763, 2019. Tao Sheng, Chen Feng, Shaojie Zhuo, Xiaopeng Zhang, Liang Shen, and Mickey Aleksic. A quantization- friendly separable convolution for mobilenets. 2018 1st Workshop on Energy Efﬁcient Machine Learning and Cognitive Computing for Embedded Applications (EMC2), Mar 2018. doi: 10.1109/emc2.2018.00011. URL http://dx.doi.org/10.1109/emc2.2018.00011. Moran Shkolnik, Brian Chmiel, Ron Banner, Gil Shomron, Yuri Nahshan, Alex Bronstein, and Uri Weiser. Robust quantization: One model to rule them all. arXiv preprint arXiv:2002.07686, 2020. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overﬁtting. The journal of machine learning research, 15(1): 1929–1958, 2014. Rianne van den Berg, Thomas N. Kipf, and Max Welling. Graph convolutional matrix completion, 2017. Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. Graph attention networks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id=rJXMpikCZ. Hanchen Wang, Defu Lian, Ying Zhang, Lu Qin, Xiangjian He, Yiguang Lin, and Xuemin Lin. Binarized graph neural network, 2020. Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han. Haq: Hardware-aware automated quantization with mixed precision, 2018. Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev, and Paulius Micikevicius. Integer quantization for deep learning inference: Principles and empirical evaluation, 2020. Zhenqin Wu, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S. Pappu, Karl Leswing, and Vijay Pande. Moleculenet: A benchmark for molecular machine learning, 2017. Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=ryGs6iA5Km. Bencheng Yan, Chaokun Wang, Gaoyang Guo, and Yunkai Lou. Tinygnn: Learning efﬁcient graph neural networks. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD ’20, pp. 1848–1856, New York, NY , USA, 2020. Association for Computing Machinery. ISBN 9781450379984. doi: 10.1145/3394486.3403236. URL https://doi.org/10. 1145/3394486.3403236. Oﬁr Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. Q8bert: Quantized 8bit bert, 2019. Hanqing Zeng and Viktor Prasanna. Graphact. The 2020 ACM/SIGDA International Symposium on Field- Programmable Gate Arrays, Feb 2020. doi: 10.1145/3373087.3375312. URL http://dx.doi.org/10. 1145/3373087.3375312. Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. Graphsaint: Graph sampling based inductive learning method, 2020. 11Published as a conference paper at ICLR 2021 A A PPENDIX Readers seeking advice on implementation will ﬁnd appendix A.5 especially useful. We provide signiﬁcant advice surrounding best practices on quantization for GNNs, along with techniques which we believe can boost our methods beyond the performance described in this paper, but for which we did not have time to fully evaluate. A.1 E XPERIMENTAL SETUP As baselines we use the architectures and results reported by Fey & Lenssen (2019) for citation networks, Dwivedi et al. (2020) for MNIST, CIFAR-10 and ZINC and, Xu et al. (2019) for REDDIT- BINARY . We re-implemented the architectures and datasets used in these publications and replicated the results reported at FP32. Models using GIN layers learn parameter ϵ. These models are often referred to as GIN- ϵ. The high-level description of these architectures is shown in table 5. The number of parameters for each architecture-dataset in this work are shown in table 6. Our infrastructure was implemented using PyTorch Geometric (PyG) (Fey & Lenssen, 2019). We generate candidate hyperparameters using random search, and prune trials using the asynchronous hyperband algorithm (Li et al., 2020). Hyperparameters searched over were learning rate, weight decay, and dropout (Srivastava et al., 2014) and drop-edge (Rong et al., 2020) probabilities. The search ranges were initialized centered at the values used in the reference implementations of the baselines. Degree-Quant requires searching for two additional hyperparameters, pmin and pmax, these were tuned in a grid-search fashion. We report our results using the hyperparameters which achieved the best validation loss over 100 runs on the Cora and Citeseer datasets, 10 runs for MNIST, CIFAR-10 and ZINC, and 10-fold cross-validation for REDDIT-BINARY . We generally used fewer hyperparameter runs for our DQ runs than we did for baselines—even ignoring the searches over the various STE conﬁgs. As our method is more stable, ﬁnding a reasonable set of parameters was easier than before. As is usual with quantization experiments, we found it useful to decrease the learning rate relative to the FP32 baseline. Our experiments ran on several machines in our SLURM cluster using Intel CPUs and NVIDIA GPUs. Each machine was running Ubuntu 18.04. The GPU models in our cluster were: V100, RTX 2080Ti and GTX 1080Ti. Model # Layers # Hidden Units Residual Output MLP Arch. Cit M C Z R Cit M C Z R Cit M C Z R Cit M C Z R GCN 2 4 4 4 - 16 146 146 145 - × ✓ ✓ ✓ - × ✓ ✓ ✓ - GAT 2 4 4 4 - 8 19 19 18 - × ✓ ✓ ✓ - × ✓ ✓ ✓ - GIN 2 4 4 4 5 16 110 110 110 64 × ✓ ✓ ✓ × × ✓ ✓ ✓ ✓ Table 5: High level description of the architectures evaluated for citation networks (Cit), MNIST (M), CIFAR-10 (C), ZINC (Z) and REDDIT-BINARY (R). We relied on Adam optimizer for all experiments. For all batched experiments, we used 128 batch-sizes. All GAT models used 8 attention heads. All GIN architectures used 2-layer MLPs, except those for citation networks which used a single linear layer. Model Node Classiﬁcation Graph Classiﬁcation Graph Regression Arch. Cora Citeseer MNIST CIFAR-10 REDDIT-BIN ZINC GCN 23063 59366 103889 104181 - 105454 GAT 92373 237586 113706 114010 - 105044 GIN 23216 59536 104554 104774 42503 102088 Table 6: Number of parameters for each of the evaluated architectures For QAT experiments, all elements of each network are quantized: inputs to each layer, the weights, the messages sent between nodes, the inputs to aggregation stage and its outputs and, the outputs of the update stage (which are the outputs of the GNN layer before activation). In this way, all intermediate tensors in GNNs are quantized with the exception of the attention mechanism in GAT; we do not quantize after the softmax calculation, due to the numerical precision required at this 12Published as a conference paper at ICLR 2021 stage. With the exception of Cora and Citeseer, the models evaluated in this work make use of Batch Normalization (Ioffe & Szegedy, 2015). For deployments of quantized models, Batch Normalization layers are often folded with the weights (Krishnamoorthi, 2018). This is to ensure the input to the next layer is within the expected [qmin,qmax] ranges. In this work, for both QAT baselines and QAT+DQ, we left BN layers unfolded but ensure the inputs and outputs were quantized to the appropriate number of bits (i.e. INT8 or INT4) before getting multiplied with the layer weights. We leave as future work proposing a BN folding mechanism applicable for GNNs and studying its impact for deployments of quantized GNNs. The GIN models evaluated on REDDIT-BINARY used QAT for all layers with the exception of the input layer of the MLP in the ﬁrst GIN layer. This compromise was needed to overcome the severe degradation introduced by quantization when operating on nodes with a single scalar as feature. A.2 D ATASETS We show in Table 7 the statistics for each dataset either used or referred to in this work. For Cora and Citeseer datasets, nodes correspond to documents and edges to citations between these. Node features are a bag-of-words representation of the document. The task is to classify each node in the graph (i.e. each document) correctly. The MNIST and CIFAR-10 datasets (commonly used for image classiﬁcation) are transformed using SLIC (Achanta et al., 2012) into graphs where each node represents a cluster of perceptually similar pixels or superpixels. The task is to classify each image using their superpixels graph representation. The ZINC dataset contains graphs representing molecules, were each node is an atom. The task is to regress a molecular property (constrained solubility (Jin et al., 2018)) given the graph representation of the molecule. Nodes in graphs of the REDDIT-BINARY dataset represent users of a Reddit thread with edges drawn between a pair of nodes if these interacted. This dataset contains graphs of two types of communities: question-answer threads and discussion threads. The task is to determine if a given graph is from a question-answer thread or a discussion thread. We use standard splits for MNIST, CIFAR-10 and ZINC. For citation datasets (Cora and Citeseer), we use the splits used by Kipf & Welling (2017). For REDDIT-BINARY we use 10-fold cross validation. Dataset Graphs Nodes Edges Features Labels Cora 1 2,708 5,278 1,433 7 Citeseer 1 3,327 4,552 3,703 6 Pubmed 1 19,717 44,324 500 3 MNIST 70K 40-75 564.53 (avg) 3 10 CIFAR10 60K 85-150 941.07 (avg) 5 10 ZINC 12K 9-37 49.83 (avg) 28 1 REDDIT-BINARY 2K 429.63 (avg) 497.75 (avg) 1 2 Reddit 1 232,965 114,848,857 602 41 Amazon 1 9,430,088 231,594,310 300 24 Table 7: Statistics for each dataset used in the paper. Some datasets are only referred to in ﬁg. 1 A.3 Q UANTIZATION IMPLEMENTATIONS In section 4.1 we analyse different readily available quantization implementations and how they impact in QAT results. First, vanilla STE, which is the reference STE (Bengio et al., 2013) that lets the gradients pass unchanged; and gradient clipping (GC), which clips the gradients based on the maximum representable value for a given quantization level. Or in other words, GC limits gradients if the tensor’s magnitudes are outside the [qmin, qmax] range. xmin = {min(X) if step = 0 min(xmin,X) otherwise (1) xmin = {min(X) if step = 0 (1 −c)xmin + cmin(X) otherwise (2) 13Published as a conference paper at ICLR 2021 The quantization modules keep track of the input tensor’s min and max values,xmin and xmax, which are then used to compute qmin, qmax, zero-point and scale parameters. For both vanilla STE and GC, we study two popular ways of keeping track of these statistics: min/max, which tracks the min/max tensor values observed over the course of training; and momentum, which computes the moving averages of those statistic during training. The update rules for xmin for STE min/max and STE momentum are presented in eq. (1) and eq. (2) respectively, whereXis the tensor to be quantized and cis the momentum hyperparameter, which in all our experiments is set to its default 0.01. Equivalent rules apply when updating xmax (omitted). For stochastic QAT we followed the implementation described in Fan et al. (2020), where at each training step a binary mask sampled from a Bernoulli distribution is used to specify which elements of the weight tensor will be quantized and which will be left at full precision. We experimented with block sizes larger than one (i.e. a single scalar) but often resulted in a sever drop in performance. All the reported results use block size of one. A.4 D EGREE -QUANT AND GRAPH LEVEL SUMMARIZATION The percentile operation in our quantization scheme remains important for summarizing the graph when doing graph-level tasks, such as graph regression (Zinc) or graph classiﬁcation (MNIST, CIFAR- 10 and REDDIT-BINARY). Since the number of nodes in each input graph is not constant, this can cause the summarized representation produced from the ﬁnal graph layer to have a more tailed distribution than would be seen with other types of architectures (e.g. CNN). Adding the percentile operation reduces the impact of these extreme tails in the fully connected graph-summarization layers, thereby increasing overall performance. The arguments regarding weight update accuracy also still apply, as the ∂L ∂h(i) l+1 term in the equations for the GCN and GIN should be more accurate compared to when the activations are always quantized before the summarization. This phenomenon is also noted by Fan et al. (2020). A.5 I MPLEMENTATION ADVICE We provide details that will be useful for others working in the area, including suggestions that should boost the performance of our results and accelerate training. We release code on GitHub; this code is a clean implementation of the paper, suitable for users in downstream works. A.5.1 Q UANTIZATION SETUP As our work studies the pitfalls of quantization for GNNs, we were more aggressive in our imple- mentation than is absolutely necessary: everything (where reasonably possible) in our networks is quantized. In practice, this leaves low-hanging fruit for improvements in accuracy: • Not quantizing the ﬁnal layer (as is common practice for CNNs and Transformers) helps with accuracy, especially at INT4. A similar practice at the ﬁrst layer will also be useful. • Using higher precision for the “summarization” stages of the model, which contributes little towards the runtime in most cases. • Taking advantage of mixed precision: since the beneﬁts of quantization are primarily in the message passing phase (discussed below), one technique to boost accuracy is to only make the messages low precision. We advise choosing a more realistic (less aggressive) convention than used in this work. The ﬁrst two items would be appropriate. A.5.2 R ELATIVE VALUE OF PERCENTILES COMPARED TO PROTECTIVE MASKING There are two components to our proposed technique: stochastic, topology-aware, masking and percentile-based range observers for quantizers. We believe that percentiles provide more immediate value, especially at INT4. We ﬁnd that they are useful purely from the perspective of stabilizing the optimization and reducing the sensitivity to hyperparameters. 14Published as a conference paper at ICLR 2021 However, adding the masking does improve performance further. This is evident from table 9. In fact, performance may be degraded slightly when percentiles are also applied: this can be observed by comparing table 9 to the main results in the paper, table 2. A.5.3 P ERCENTILES The key downside with applying percentiles for range observers is that the operation can take signiﬁcant time. Training with DQ is slower than before—however, since there is less sensitivity to hyperparameters, fewer runs end up being needed. We are conﬁdent that an effective way to speed up this operation is to use sampling. We expect 10% of the data should be adequate, however we believe that even 1% of the data may be sufﬁcient (dataset and model dependent). However, we have not evaluated this setup in the paper; it is provided in the code release for experimentation. A.5.4 I MPROVING ON PERCENTILES We believe that it is possible to signiﬁcantly boost the performance of GNN quantization by employing a learned step size approach. Although we used percentiles in this paper to illustrate the range- precision trade-off for GNNs, we expect that learning the ranges will lead to better results. This approach, pioneered by works such as Esser et al. (2020), has been highly effective on CNNs even down to 2 bit quantization. Another approach would be to use robust quantization: the ideas in these works are to reduce the impact of changing quantization ranges i.e. making the architecture more robust to quantization. Works in this area include Alizadeh et al. (2020) and Shkolnik et al. (2020). A.5.5 I MPROVING LATENCY The slowest step of GNN inference is typically the sparse operations. It is therefore best to minimize the sizes of the messages between nodes i.e. quantize the message phase most aggressively. This makes the biggest impact on CPUs which are dependent on caches to obtain good performance. We evaluated our code on CPU using Numpy and Scipy routines. For the GPU, we used implemen- tations from PyTorch and PyTorch Geometric and lightly modiﬁed them to support INT8 where necessary. These results, while useful for illustrating the beneﬁts of quantization, are by no means optimal: we did not devote signiﬁcant time to improving latency. We believe better results can be obtained by taking advantage of techniques such as cache blocking or kernel fusion. A.5.6 P ITFALLS Training these models can be highly unstable: some experiments in the paper had standard deviations as large as 18%. We observed this to affect citation network experiments to the extent that they would not converge on GPUs: all these experiments had to be run on CPUs. A.6 D EGRADATION STUDIES Figures 7 and 8 show the results of the ablation study conducted in section 5 for GCN and GIN. We observe that GCN is more tolerant to INT4 quantization than other architectures. GIN, however, requires accurate representations after the update stage, and heavily suffers from further quantization like GAT. The idea of performing different stages of inference at different precisions has been proposed, although it is uncommon (Wang et al., 2018). 15Published as a conference paper at ICLR 2021 Figure 7: Degradation of INT8 GCN on Cora as indi- vidual elements are converted to INT4without Degree- Quant. Figure 8: Degradation of INT8 GIN on Cora as indi- vidual elements are converted to INT4without Degree- Quant. Figure 9: In-degree distribution for each of the six datasets assessed. Note that a log y-axis is used for all datasets except for MNIST and CIFAR-10. 16Published as a conference paper at ICLR 2021 0 25 50 75 100 125 150 175 200 Epoch 0.2 0.4 0.6 0.8 1.0 1.2Val Loss FP32 DQ-INT8 (0.0,0.1) DQ-INT8 (0.1,0.2) DQ-INT8 (0.2,0.2) DQ-INT8 (0.2,0.3) Figure 10: Validation loss curves for GIN models evaluated on REDDIT-BINARY . Results averaged across 10-fold cross- validation. We show four DQ-INT8 experiments each with a differ- ent values for (pmin,pmax) and our FP32 baseline. Quantization Model REDDIT-BIN ↑ Ref. (FP32) GIN 92.2 ±2.3 Ours (FP32) GIN 92.0 ±1.5 DQ-INT8 (0.0, 0.1) GIN 91.8 ±2.3 DQ-INT8 (0.1, 0.2) GIN 90.1 ±2.5 DQ-INT8 (0.2, 0.2) GIN 89.0 ±3.0 DQ-INT8 (0.2, 0.3) GIN 88.1 ±3.0 Table 8: Final test accuracies for FP32 and DQ-INT8 models whose validation loss curves are shown in ﬁg. 10 Quantization Model Node Classiﬁcation Graph Regression Scheme Arch. Cora ↑ Citeseer ↑ ZINC ↓ QAT-INT8 + DQ Masking GCN 81.1 ±0.6 71 .0 ±0.7 0 .468 ±0.014 GAT 82.1 ±0.1 71 .4 ±0.8 0 .462 ±0.005 GIN 78.9 ±1.2 67 .1 ±1.7 0 .347 ±0.028 QAT-INT4 + DQ Masking GCN 78.5 ±1.4 62 .8 ±8.5 0 .599 ±0.015 GAT 64.4 ±9.3 68 .9 ±1.2 0 .529 ±0.008 GIN 71.2 ±2.9 56 .7 ±3.8 0 .427 ±0.010 nQAT-INT4 + Percentile GCN 75.6 ±2.5 64 .8 ±3.8 0 .633 ±0.012 GAT 70.1 ±2.8 51 .4 ±3.4 0 .596 ±0.008 GIN 63.5 ±2.0 46 .3 ±4.1 0 .771 ±0.058 Table 9: Ablation study against the two elements of Degree-Quant (DQ). The ﬁrst two rows of results are obtained with only the stochastic element of Degree-Quant enabled for INT8 and INT4. Percentile-based quantization ranges are disabled in these experiments. The bottom row of results were obtained with noisy quantization (nQAT) at INT4 with the use of percentiles. DQ masking alone is often sufﬁcient to achieve excellent results, but the addition of percentile-based range tracking can be beneﬁcial to increase stability. We can see that using nQAT with percentiles is not sufﬁcient to achieve results of the quality DQ provides. Device Arch. CIFAR-10 Cora Citeseer FP32 W8A8 Speedup FP32 W8A8 Speedup FP32 W8A8 Speedup CPU GCN 182ms 88ms 2.1× 0.94ms 0.74ms 1.3× 0.97ms 0.76ms 1.3× GAT 500ms 496ms 1.0× 0.86ms 0.78ms 1.1× 0.99ms 0.88ms 1.1× GIN 144ms 44ms 3.3× 0.85ms 0.68ms 1.3× 0.95ms 0.55ms 1.7× GPU GCN 2.1ms 1.6ms 1.3× 0.08ms 0.09ms 0.9× 0.09ms 0.09ms 1.0× GAT 30.0ms 27.1ms 1.1× 0.57ms 0.64ms 0.9× 0.56ms 0.64ms 0.9× GIN 20.9ms 16.2ms 1.2 × 0.09ms 0.07ms 1.3× 0.09ms 0.07ms 1.3× Table 10: INT8 latency results run on a 22 core 2.1GHz Intel Xeon Gold 6152 and, on a GTX 1080Ti GPU. All layers have 128 in/out features. For CIFAR-10 we used batch size of 1K graphs. 17Published as a conference paper at ICLR 2021 NRde AcWLYaWLRQV ¬N¬ F MaWPXO F F' AOO WeLghWV aUeXQLfRUPO\\ TXaQWL]edWR INT-N QXaQWL]e QXaQWL]e DQ MaVN F'NRde MeVVaJeV FP32 AJJUeJaWed MeVVaJeV Each QRde gaWheUV Whe PeVVageV fURP QeLghbRXU QRdeV(WheVe caQ be FP32 RU/aQd INT-N). The LQWeUPedLaWeaggUegaWLRQ LV aOZa\\V FP32. TheQ, Whe UeVXOW Rf WheaggUegaWLRQ LV TXaQWL]ed WR INT-N RU OefW aW FP32deSeQdLQg RQ Whe DQ PaVN aW Whe cXUUeQW WUaLQLQg VWeS. INT-N F' La\\eU OXWSXW F' ReWaLQ fRUbacNZaUd SaVV F' USdaWe SWage DQ PaVN fRU cXUUeQW WUaLQLQg VWeS. ¬NRdeVWR be UeWaLQed aW FP32 aUe VhRZQ LQ ZhLWe ¬N¬ DQ MaVN IQWeUPedLaWeaggUegaWLRQRXWSXW QXaQWL]e DQ MaVN (qXanti]ed in the preYioXs la\\er) INT-N FP32 Figure 11: Diagram representing how DQ makes use of a topology-aware quantization strategy that is better suited for GNNs. The diagram illustrates this for a GCN layer. At every training stage, a degree-based mask is generated. This mask is used in all quantization layers located after each of the stages in the message-passing pipeline. By retaining at FP32 nodes with higher-degree more often, the noisy updates during training have a lesser impact and therefore models perform better, even at INT4. 18Published as a conference paper at ICLR 2021 NRde AcWLYaWLRQV ¬N¬ F MaWPXO Quanti]e F'NRde MeVVaJeV AJJUeJaWed MeVVaJeV F' La\\eU OXWSXW F' Update Stage F F' Quanti]eZith noise Retain forbackZard pass F' Aggregate Quanti]e Unlike Zith DegreeQuant, allstages make use of uniformquanti]ation Zithout introducingtopolog\\-aZare masking¬ Quanti]e (qXanti]ed in the preYioXs la\\er) INT-N FP32 Figure 12: Diagram representing how nQAT is implemented for GNNs. The diagram illustrates this for a GCN layer. The stochastic stage only takes place when quantizing the weights, the remaining of the quantization modules happen following a standard QAT strategy. A QAT diagram would be similar to this one but fully quantizing the weights. 19Published as a conference paper at ICLR 2021 Final Layer Output Fout MLP Quantize QAT + percentileswithout making useof DQ mask  Graph-level Task Output  N  Summarization Fout Final Layer Output Fout MLP Quantize Graph-level Task Output  N  Summarization Fout Graph summarization and output stage for graph-level tasks using DQ Graph summarization and output stage for graph-level tasks using nQAT Weights arestochasticallymasked Fout' Fout' Figure 13: Diagrams representing how the output graph-summarization stages for graph-level tasks (e.g. graph classiﬁcation, graph regression) are implemented when making use of DQ (left) and nQAT (right). GNNs making use of DQ during the node-aggregation stages (see ﬁg. 11), do not use the stochastic element of DQ in the output MLP layers but still make use of percentiles. For models making use of nQAT, the ﬁnal MLP still makes use of stochastic quantization of weights. 20Published as a conference paper at ICLR 2021 B C ODE LISTINGS Our code depends on PyTorch Geometric (Fey & Lenssen, 2019). These snippets should be compatible with Python 3.7 and PyTorch Geometric version 1.4.3. You can see the full code on GitHub: https://github.com/camlsys/degree-quant. B.1 M ASK GENERATION class ProbabilisticHighDegreeMask: def __init__(self, low_prob, high_prob, per_graph=True): self.low_prob = low_prob self.high_prob = high_prob self.per_graph = per_graph def _process_graph(self, graph): # Note that: # 1. The probability of being masked increases as the indegree increases # 2. All nodes with the same indegree have the same bernoulli p # 3. you can set this such that all nodes have some probability of being masked n = graph.num_nodes indegree = degree(graph.edge_index[1], n, dtype=torch.long) counts = torch.bincount(indegree) step_size = (self.high_prob - self.low_prob) / n indegree_ps = counts * step_size indegree_ps = torch.cumsum(indegree_ps, dim=0) indegree_ps += self.low_prob graph.prob_mask = indegree_ps[indegree] return graph def __call__(self, data): if self.per_graph and isinstance(data, Batch): graphs = data.to_data_list() processed = [] for g in graphs: g = self._process_graph(g) processed.append(g) return Batch.from_data_list(processed) else: return self._process_graph(data) def evaluate_prob_mask(data): return torch.bernoulli(data.prob_mask).to(torch.bool) B.2 M ESSAGE PASSING WITH DEGREE -QUANT Here we provide code to implement the layers as used by our proposal. These are heavily based off of the classes provided by PyTorch Geometric, with only minor modiﬁcations to insert the quantization steps where necessary. The normal quantized versions are similar, except without any concept of high/low masking. class MessagePassingMultiQuant(nn.Module): \"\"\"This class is a lightweight modification of the default PyTorch Geometric MessagePassing class\"\"\" # irrelevant methods removed def propagate(self, edge_index, mask, size=None, **kwargs): # some lines skipped ... msg = self.message(**msg_kwargs) if self.training: # This is for the masking of messages: edge_mask = torch.index_select(mask, 0, edge_index[0]) out = torch.empty_like(msg) out[edge_mask] = self.mp_quantizers[\"message_high\"](msg[edge_mask]) out[~edge_mask] = self.mp_quantizers[\"message_low\"]( msg[~edge_mask] ) else: out = self.mp_quantizers[\"message_low\"](msg) aggr_kwargs = self.__distribute__(self.__aggr_params__, kwargs) 21Published as a conference paper at ICLR 2021 aggrs = self.aggregate(out, **aggr_kwargs) if self.training: out = torch.empty_like(aggrs) out[mask] = self.mp_quantizers[\"aggregate_high\"](aggrs[mask]) out[~mask] = self.mp_quantizers[\"aggregate_low\"](aggrs[~mask]) else: out = self.mp_quantizers[\"aggregate_low\"](aggrs) update_kwargs = self.__distribute__(self.__update_params__, kwargs) updates = self.update(out, **update_kwargs) if self.training: out = torch.empty_like(updates) out[mask] = self.mp_quantizers[\"update_high\"](updates[mask]) out[~mask] = self.mp_quantizers[\"update_low\"](updates[~mask]) else: out = self.mp_quantizers[\"update_low\"](updates) return out B.2.1 GCN class GCNConvMultiQuant(MessagePassingMultiQuant): # Some methods missed... def forward(self, x, edge_index, mask, edge_weight=None): # quantizing input if self.training: x_q = torch.empty_like(x) x_q[mask] = self.layer_quantizers[\"inputs_high\"](x[mask]) x_q[~mask] = self.layer_quantizers[\"inputs_low\"](x[~mask]) else: x_q = self.layer_quantizers[\"inputs_low\"](x) # quantizing layer weights w_q = self.layer_quantizers[\"weights_low\"](self.weight) if self.training: x = torch.empty((x_q.shape[0], w_q.shape[1])).to(x_q.device) x_tmp = torch.matmul(x_q, w_q) x[mask] = self.layer_quantizers[\"features_high\"](x_tmp[mask]) x[~mask] = self.layer_quantizers[\"features_low\"](x_tmp[~mask]) else: x = self.layer_quantizers[\"features_low\"](torch.matmul(x_q, w_q)) if self.normalize: edge_index, norm = self.norm( edge_index, x.size(self.node_dim), edge_weight, self.improved, x.dtype, ) else: norm = edge_weight norm = self.layer_quantizers[\"norm\"](norm) return self.propagate(edge_index, x=x, norm=norm, mask=mask) 22",
      "meta_data": {
        "arxiv_id": "2008.05000v3",
        "authors": [
          "Shyam A. Tailor",
          "Javier Fernandez-Marques",
          "Nicholas D. Lane"
        ],
        "published_date": "2020-08-11T20:53:50Z",
        "pdf_url": "https://arxiv.org/pdf/2008.05000v3.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the challenge of making Graph Neural Networks (GNNs) more efficient at inference time by exploring quantization-aware training. It identifies unique sources of error in GNN quantization, such as the choice of straight-through estimator (STE) implementation, node degree, and method for tracking quantization statistics, which dramatically impact performance. The main contribution is Degree-Quant (DQ), an architecture-agnostic and stable quantization-aware training (QAT) method for GNNs. DQ-trained INT8 models often perform as well as their FP32 counterparts, while INT4 models achieve up to 26% gains over baselines and generalize to unseen graphs. The work also demonstrates up to 4.7x speedups on CPU with INT8 arithmetic and 4-8x reductions in runtime memory usage.",
        "methodology": "The proposed method, Degree-Quant (DQ), consists of two key components: 1) Stochastic Protection from Quantization: During training, a topology-aware mask is generated for each layer using a Bernoulli distribution, where nodes with higher in-degree are assigned higher probabilities of being protected. Masked nodes have their message passing, aggregation, and update phases performed at full precision (FP32), while all weights remain quantized. This aims to improve weight update accuracy by reducing aggregation error. 2) Percentile Tracking of Quantization Ranges: Instead of traditional min-max or momentum-based methods, DQ uses percentile-based observers (clipping the top and bottom 0.1% of values) to determine quantization ranges (qmin, qmax). This makes the ranges more representative of the majority of values, reducing rounding error, especially given the large fluctuations in aggregation output variance in GNNs. The method is architecture-agnostic and applied to GCN, GAT, and GIN models.",
        "experimental_setup": "The study evaluates GNN performance on six datasets: Cora, CiteSeer (node classification), MNIST and CIFAR-10 superpixels, REDDIT-BINARY (graph classification), and ZINC (graph regression). Models include GCN, GAT, and GIN architectures with varying layers and hidden units. Baselines comprise FP32 models (extensively tuned), standard QAT, and noisy QAT (nQAT) which uses stochastic masking of weights. All elements of the networks (inputs, weights, messages, aggregation stages, update stages) are uniformly quantized to 8-bit (INT8) and 4-bit (INT4) precision, except for the GAT softmax calculation and the input layer MLP for GIN on REDDIT-BINARY. Hyperparameter tuning involved random search with asynchronous hyperband for general parameters (learning rate, weight decay, dropout, drop-edge) and grid search for DQ's pmin and pmax. Validation was performed over 100 runs for citation networks, 10 runs for image and molecular datasets, and 10-fold cross-validation for REDDIT-BINARY. Latency benchmarks were conducted on a 22-core 2.1GHz Intel Xeon Gold 6152 CPU and a GTX 1080Ti GPU.",
        "limitations": "Training quantized GNN models can be highly unstable, with some experiments exhibiting large standard deviations (up to 18%) and convergence issues on GPUs, particularly for citation networks, necessitating CPU runs. The percentile operation, while beneficial for stability, can be computationally intensive and slow down training. Certain GIN models, especially when dealing with nodes having single scalar features, experienced severe degradation from quantization and required a compromise of not quantizing the input layer of the MLP in the first GIN layer. Additionally, for deployments, Batch Normalization layers were left unfolded, which is less optimized compared to common BN folding practices for quantized models.",
        "future_research_directions": "Future work could involve a more in-depth analysis of how graph topology specifically affects quantization. Developing and studying a Batch Normalization folding mechanism tailored for GNNs is also suggested. Less aggressive quantization strategies, such as not quantizing the first or final layers, using higher precision for graph summarization stages, or adopting mixed-precision approaches (e.g., only quantizing message passing aggressively), could be explored. To speed up training, investigating sampling methods for percentile calculation is recommended. Furthermore, employing learned step size quantization approaches or robust quantization techniques could potentially yield better results, especially at very low bit-widths. Finally, optimizing latency through techniques like cache blocking or kernel fusion for hardware-specific implementations could further enhance efficiency."
      }
    },
    {
      "title": "$\\rm A^2Q$: Aggregation-Aware Quantization for Graph Neural Networks",
      "abstract": "As graph data size increases, the vast latency and memory consumption during\ninference pose a significant challenge to the real-world deployment of Graph\nNeural Networks (GNNs). While quantization is a powerful approach to reducing\nGNNs complexity, most previous works on GNNs quantization fail to exploit the\nunique characteristics of GNNs, suffering from severe accuracy degradation.\nThrough an in-depth analysis of the topology of GNNs, we observe that the\ntopology of the graph leads to significant differences between nodes, and most\nof the nodes in a graph appear to have a small aggregation value. Motivated by\nthis, in this paper, we propose the Aggregation-Aware mixed-precision\nQuantization ($\\rm A^2Q$) for GNNs, where an appropriate bitwidth is\nautomatically learned and assigned to each node in the graph. To mitigate the\nvanishing gradient problem caused by sparse connections between nodes, we\npropose a Local Gradient method to serve the quantization error of the node\nfeatures as the supervision during training. We also develop a Nearest Neighbor\nStrategy to deal with the generalization on unseen graphs. Extensive\nexperiments on eight public node-level and graph-level datasets demonstrate the\ngenerality and robustness of our proposed method. Compared to the FP32 models,\nour method can achieve up to a 18.6x (i.e., 1.70bit) compression ratio with\nnegligible accuracy degradation. Morever, compared to the state-of-the-art\nquantization method, our method can achieve up to 11.4\\% and 9.5\\% accuracy\nimprovements on the node-level and graph-level tasks, respectively, and up to\n2x speedup on a dedicated hardware accelerator.",
      "full_text": "arXiv:2302.00193v1  [cs.LG]  1 Feb 2023 Published as a conference paper at ICLR 2023 A2Q: A G G R E G AT I O N-AW A R E QUA N T I Z AT IO N F O R GR A P H NE U R A L NE T WO R K S Zeyu Zhu1, 2 Fanrong Li2 Zitao Mo2 Qinghao Hu2 Gang Li3 Zejian Liu2 Xiaoyao Liang3 Jian Cheng2∗ 1School of Future T echnology, University of Chinese Academy of Sciences 2Institute of Automation, Chinese Academy of Sciences 3Shanghai Jiao T ong University {zhuzeyu2021, lifanrong2017, mozitao2017}@ia.ac.cn, {huqinghao2014, liuzejian2018}@ia.ac.cn, {gliaca}@sjtu.edu.cn {liang-xy}@cs.sjtu.edu.cn {jcheng}@nlpr.ia.ac.cn ABSTRACT As graph data size increases, the vast latency and memory con sumption during in- ference pose a signiﬁcant challenge to the real-world deplo yment of Graph Neural Networks (GNNs). While quantization is a powerful approach to reducing GNNs complexity, most previous works on GNNs quantization fail t o exploit the unique characteristics of GNNs, suffering from severe accuracy de gradation. Through an in-depth analysis of the topology of GNNs, we observe that the topology of the graph leads to signiﬁcant differences between nodes, an d most of the nodes in a graph appear to have a small aggregation value. Motivate d by this, in this paper, we propose the Aggregation-A ware mixed-precision Q uantization ( A2Q) for GNNs, where an appropriate bitwidth is automatically le arned and assigned to each node in the graph. T o mitigate the vanishing gradient problem caused by sparse connections between nodes, we propose a Local Gradie nt method to serve the quantization error of the node features as the supervisi on during training. W e also develop a Nearest Neighbor Strategy to deal with the gen eralization on unseen graphs. Extensive experiments on eight public node-level a nd graph-level datasets demonstrate the generality and robustness of our proposed m ethod. Compared to the FP32 models, our method can achieve up to a 18.6x (i.e., 1. 70bit) compression ratio with negligible accuracy degradation. Morever, comp ared to the state-of-the- art quantization method, our method can achieve up to 11.4% a nd 9.5% accuracy improvements on the node-level and graph-level tasks, resp ectively, and up to 2x speedup on a dedicated hardware accelerator. 1 I NTRODUC TI ON Recently, Graph Neural Networks (GNNs) have attracted much attention due to their superior learn- ing and representing ability for non-Euclidean geometric d ata. A number of GNNs have been widely used in real-world applications, such as recommendation sy stem (Jin et al., 2020), and social net- work analysis (Lerer et al., 2019), etc. Many of these tasks p ut forward high requirements for low- latency inference. However, the real-world graphs are ofte n extremely large and irregular, such as Reddit with 232,965 nodes, which needs 19G ﬂoating-point op erations (FLOPs) to be processed by a 2-layer Graph Convolutional Network (GCN) with only 81KB pa rameters (T ailor et al., 2020), while ResNet-50, a 50-layer DNN, only takes 8G FLOPs to process an i mage (Canziani et al., 2016). What is worse, it requires a huge amount of memory access for GNNs i nference, e.g., the nodes features size of Reddit is up to 534MB, leading to high latency. Theref ore, the aforementioned problems pose a challenge to realize efﬁcient inference of GNNs. Neural network quantization can reduce the model size and accelerate inference without modify- ing the model architecture, which has become a promising met hod to solve this problem in re- ∗ Corresponding author 1Published as a conference paper at ICLR 2023 /uni0000003e/uni00000014/uni0000000f/uni00000014/uni00000013/uni00000040/uni0000003e/uni00000014/uni00000014/uni0000000f/uni00000015/uni00000013/uni00000040/uni0000003e/uni00000015/uni00000014/uni0000000f/uni00000016/uni00000013/uni00000040/uni0000003e/uni00000016/uni00000014/uni0000000f/uni00000017/uni00000013/uni00000040/uni0000003e/uni00000017/uni00000014/uni0000000f/uni00000014/uni00000019/uni0000001b/uni00000040 /uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000029/uni00000048/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni0000000b/uni00000024/uni00000049/uni00000057/uni00000048/uni00000055/uni00000003/uni00000024/uni0000004a/uni0000004a/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni0000000c /uni0000002a/uni00000026/uni00000031 /uni0000002a/uni0000002c/uni00000031 /uni0000002a/uni00000024/uni00000037 (a) /uni0000003e/uni00000014/uni0000000f/uni00000016/uni00000013/uni00000040/uni0000003e/uni00000016/uni00000014/uni0000000f/uni0000001a/uni00000013/uni00000040/uni0000003e/uni0000001a/uni00000014/uni0000000f/uni00000014/uni00000014/uni00000013/uni00000040/uni0000003e/uni00000014/uni00000014/uni00000014/uni0000000f/uni00000014/uni00000018/uni00000013/uni00000040/uni0000003e/uni00000014/uni00000018/uni00000013/uni0000000f/uni00000015/uni00000013/uni00000013/uni00000040 /uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000029/uni00000048/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni0000000b/uni00000024/uni00000049/uni00000057/uni00000048/uni00000055/uni00000003/uni00000024/uni0000004a/uni0000004a/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni0000000c /uni0000002a/uni0000002c/uni00000031/uni00000042/uni0000002f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000015 /uni0000002a/uni0000002c/uni00000031/uni00000042/uni0000002f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000016 /uni0000002a/uni0000002c/uni00000031/uni00000042/uni0000002f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000017 /uni0000002a/uni0000002c/uni00000031/uni00000042/uni0000002f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000018 (b) Figure 1: The analysis of the average aggerated node feature s in different in-degrees node groups on various tasks. (a) The values at the ﬁnal layer for GNNs train ed on Cora. (b) The values at the 2-5 layer of GIN trained on REDDIT -BINAR Y . The average values ar e all generated from 10 runs. cent years. Unfortunately, there remain some issues in the e xisting works on GNNs quantization. Feng et al. (2020) only quantizes the node feature and keeps ﬂ oating point calculations during infer- ence. T ailor et al. (2020) proposes a degree-quant training strategy to quantize GNNs to the low-bit ﬁxed point but causes a large accuracy drop, e.g., 11.1% accu racy drops when quantizing to 4bits. Moreover, some works (W ang et al., 2021b; Bahri et al., 2021; W ang et al., 2021a; Jing et al., 2021) quantize GNNs into 1-bit and compute with XNOR and bit count o perations. However, these 1-bit quantization methods are either restricted to the node-lev el tasks or can not generalize well to other GNNs. Most of the above methods do not make full use of the property o f GNNs and graph data, re- sulting in severe accuracy degradation or poor generalizat ion. As presented in MPNN framework (Gilmer et al., 2017), GNNs processing is divided into two ph ase: First, in the aggregation phase, a node collects information from neighboring nodes and uses the aggregation function to generate hidden features; second, in the update phase, the hidden fea tures are transformed into new features by an update function. W e analyze the nodes features after ag gregation in Figure 1 and ﬁnd that the higher the in-degree is, the larger the node features ten d to be after aggregation. And the fea- tures vary signiﬁcantly between nodes with different in-de grees, which represent the topology of a graph. Moreover, according to Xie et al. (2014); Aiello et al . (2001), the degrees of nodes in most real-world graph data often follow the power-law distribut ion, i.e., nodes with a low degree account for the majority of graph data. Therefore, specially quanti zing the nodes features according to the topology of the graphs will be beneﬁcial to reduce the quanti zation error while achieving a higher compression ratio. In this paper, we propose theAggregation-A ware Quantization(A2Q) method, which quantizes different nodes features with different learnable quantiz ation parameters, including bitwidth and step size. These parameters can be adaptively learned durin g training and are constrained by a penalty on memory size to improve the compression ratio. How ever, when quantizing the model in semi-supervised tasks, the gradients for most quantizat ion parameters are zero due to the sparse connections between nodes, which makes the training non-tr ivial. W e propose the Local Gradient method to solve this problem by introducing quantization er ror as supervised information. Finally, to generalize our method to unseen graphs in which the number of the nodes varies, we develop the Nearest Neighbor Strategy which assigns the learned quantization parameters to the un seen graph nodes. T o the best of our knowledge, we are the ﬁrst to introdu ce the mixed-precision quantization to the GNNs. Compared with the previous works, our proposed m ethods can signiﬁcantly compress GNNs with negligible accuracy drop. In summary, the key contributions of this paper are as follows: 1) W e propose the Aggregation-A ware mixed-precision Quant ization ( A2Q) method to enable an adaptive learning of quantization parameters. Our learn ing method is powerful by fully 2Published as a conference paper at ICLR 2023 utilizing the characteristic of GNNs, and the learned bitwi dth is strongly related to the topology of the graph. 2) A Local Gradient method is proposed to train the quantization parameters in s emi- supervised learning tasks. Furthermore, to generalize our method to the unseen graphs in which the number of input nodes is variable, we develop the Nearest Neighbor Strategy to select quantization parameters for the nodes of the unsee n graphs. 3) Experiments demonstrate that we can achieve a compressio n ratio up to 18.6x with negli- gible accuracy degradation compared to the full-precision (FP32) models. Moreover, the model trained with our A2Q method outperforms the state-of-the-art (SOT A) method up to 11.4% with a speedup up to 2.00x in semi-supervised tasks, and obtains up to 9.5% gains with a 1.16x speedup in graph-level tasks. W e provide o ur code at this URL: https://github.com/weihai-98/A2Q. 2 R ELATED WORK Graph Neural Networks: The concept of the graph neural network was ﬁrst proposed in Scarselli et al. (2008), which attempted to generalize neur al networks to model non-Euclidean data. In the following years, various GNN models were proposed. Fo r example, Graph Convolution Net- work (GCN) (Kipf & W elling, 2016) uses a layer-wise propagat ion rule that is based on a ﬁrst-order approximation of spectral convolutions on graphs, Graph Is omorphism Network (GIN) (Xu et al., 2018) designed a provably maximally powerful GNN under the M PNN framework, and Graph At- tention Network (GA T) (V eliˇ ckovi´ c et al., 2017) introduc es the attention mechanism to graph pro- cessing. Although GNNs have encouraging performance in a wi de range of domains (Jin et al., 2020; Y ang, 2019), the huge amount of ﬂoat-point operations and memory access in process pose a challenge to efﬁcient inference, which hinder the applicat ions of GNNs. Quantized GNNs: As a promising method to reduce the model size and accelerate the inference process, quantization is also applied to GNNs. Some works qu antize features and weights in GNNs to low bitwidths (Feng et al., 2020; T ailor et al., 2020) or ev en 1-bit (W ang et al., 2021b; Bahri et al., 2021; W ang et al., 2021a; Jing et al., 2021), i.e., use ﬁxed-p oint numbers instead of ﬂoating-point numbers for computation. But when the compression ratio is h igh (e.g., <4bit), the performance degradation of these works is signiﬁcant, and the generaliz ation of 1-bit method is limited. There are also some works on vector quantization (VQ), which use th e vectors in a codebook obtained during the training process instead of the original feature s (Ding et al., 2021; Huang et al., 2022). However, searching for vectors in the codebook is computati onally complex. Mixed-Precision Quantization: Based on the idea that different layers have different sensi tiv- ities to quantization, mixed-precision quantization is pr oposed in CNNs to quantize different layers to different bitwidths for better model compression. Early works (W ang et al., 2019; Lou et al., 2019) proposed reinforcement learning (RL) based methods t o search bitwidth for different lay- ers, but they often require large computational resources, which limits the exploration of the search space. Another important class of mixed-precision method i s the criteria-based method, they use the speciﬁc criteria to represent the quantization sensitivit y, e.g., (Dong et al., 2019; 2020; Chen et al., 2021)quantize different layers with different bitwidths b ased on the trace of the Hessian. Recently, there are some other methods to learn the bitwidth during tra ining (Uhlich et al., 2019; Esser et al., 2019; Jain et al., 2020). However, due to the huge difference between GNNs and CNNs, it is dif- ﬁcult to use these methods on GNNs directly, and our A2Q is the ﬁrst method to introduce the mixed-precision quantization to GNNs, further improving t he inference efﬁciency of GNNs. 3 M ETHOD In this section, we describe our proposed Aggregation-A war e Quantization in detail. Firstly, we present the formulation of the mixed-precision quantizati on for GNNs, which fully utilizes the prop- erty of GNNs and graph data. Secondly, we introduce the Local Gradient method to address the gradient vanishing problem during training. Finally, we de tail the Nearest Neighbor Strategy, which is used for generalizing our approach to the unseen graphs. 3Published as a conference paper at ICLR 2023  !\"\"  !\"#  !\"$   !#\"  !##  !#$   !$\"  !$#  !$$   !%\"  !%#  !%$   !&\"  !&#  !&$  !'\"  !'#  !'$ (\" (# ($ (% (& (' ) *\"\" ) *\"# ) *#\" ) *## ) *$\" ) *$# +\" ´ = ,\"\" ,\"# ,#\" ,## ,$\" ,$# ,%\" ,%# ,&\" ,&# ,'\" ,'# (\"-\" (\"-# (#-\" (#-# ($-\" ($-# (%-\" (%-# (&-\" (&-# ('-\" ('-# Nodes Features Weights   ! N F2 F Figure 2: Perform matrix multiplication by the integer represented.¯x and ¯w are both integers. Figure 3: The gradients to xq in GCN trained on Cora by sampling 400 nodes. 3.1 A G G RE G AT IO N -AWA RE QUA N T IZ AT IO N W e assume a graph data with N nodes and the node features are F -dimensional, i.e., the feature map is X ∈ RN×F and xi is the features of node i. W e use the learnable parameters step size α i ∈ R+ and bitwidth bi ∈ R+ to quantize the features of the i-th node as: ¯xi = sign(xi)      ⌊|xi| α i + 0. 5⌋, |x| < α i(2[bi]−1 − 1) 2[bi]−1 − 1, |xi| ≥ α i(2[bi]−1 − 1) , (1) where ⌊·⌋ is the ﬂoor function, and [·] is the round function to ensure the bitwidth used to quantize is an integer. The learnable parameters are sX = ( α 1, α 2, ..., α N ), and bX = ( b1, b 2, ..., b N ). Then we can obtain the ﬁxed-point feature map ¯X, and the original feature can be represented as Xq = SX · ¯X, where SX = diag(α 1, α 2, ..., α N ). Note that we use [b] + 1 as the quantization bitwidth for the features after ReLU because the values are a ll non-negative. In the update phase, the node features are often transformed with a linear mapping or an MLP in which matrix multiplication XW is the main computation, and the transformed node features a re the input to the next layer in GNNs. In order to accelerate the update phase, we also quantize W . Due to the fact that W in a certain layer is shared by all nodes, we quantize W to the same bitwidth of 4bits for all GNNs in this paper. However, each column of W has its learnable quantization step size, i.e., sW = ( β1, β 2, .., β F2 ), where F2 is the output-dimension of the node features in current layer and βi is the quantization step size for the i-th column of W , and we also use Eq. 1 to quantize W . W e can obtain the integer representation ¯W and the quantized representation Wq = ¯W · SW , where SW = diag(β1, β 2, ..., β F2 ). The ﬂoat-point matrix multiplication in the update phase c an be reformulated as follow: X · W ≈ Xq · Wq = ( SX · ¯X) · ( ¯W · SW ) = ( ¯X · ¯W ) ⊙ (sX ⊗ sW ) , (2) where ⊙ denotes an element-wise multiplication, and ⊗ denotes the outer product. After training, we can obtain sX and sW so that the outer product can be pre-processed before infere nce. An example is illustrated in Figure 2. For the aggregation phas e, i.e., AX, A is the adjacency matrix and A ∈ { 0, 1}N×N , we quantize the X as the quantization way of W because the nodes features involved in the aggregation process come from the update pha se, in which the features lose the topology information of graphs. Then the aggregation phase can be performed by integer operations to reduce the computational overhead. The quantization parameters(s, b ) are trained by the backpropagation algorithm. Since the ﬂoo r and round functions used in the quantization process are not differentiable, we use the straight- through estimator (Bengio et al., 2013) to approximate the g radient through these functions, and the gradients of the quantization parameters can be calculated by: ∂L ∂s = d∑ i=1 ∂L ∂xi q · ∂xi q ∂s , (3) ∂L ∂b = d∑ i=1 ∂L ∂xi q · ∂xi q ∂b , (4) where d is the dimension of the vector x, (s, b ) are the quantization parameters for x, and xi q is the value of i-th dimension in xq. Detailed information about quantization process and the backpropagation are shown in Appendix A.1 and A.3 Proof 2 and 3. 4Published as a conference paper at ICLR 2023 In order to improve the compression ratio of the node feature s, we introduce a penalty term on the memory size: Lmemory = ( 1 η · L∑ l=1 N∑ i=1 diml ·bl i− Mtarget )2 , (5) where L is the number of layers in the GNNs, N is the total number of nodes, diml is the length of the node features in l-th layer, bl iis the quantization bitwidth for node i in l-th layer, Mtarget is the target memory size on the total node features memory size, an d η = 8 ∗ 1024, which is a constant to convert the unit of memory size to KB. Then the model and quantization parameters can be trained by the loss function: Ltotal = Ltask + λ · Lmemory , (6) where Ltask is the task-related loss function and λ is a penalty factor on Lmemory . 3.2 L O CA L GRA D IE N T Although the above end-to-end learning method is concise an d straightforward, the gradients for the quantization parameters of nodes features, i.e., ∂L task ∂s and ∂L task ∂b , are almost zero during the training process of semi-supervised tasks, which poses a si gniﬁcant challenge to train the quantiza- tion parameters for nodes features. W e analyze the property of GNNs and graph data, and ﬁnd that two reasons lead to this phenomenon: 1. The extreme sparsity of the connections between nodes in graph data. 2. Only a tiny fraction of nodes with labels are used for training in semi-supervised tasks (e.g., 0. 30% in PubMed dataset). Therefore, ∂L task ∂x q for most node features are zero (detailed proof in Appendix A.3.2), which results in that the gradient s for quantization parameters of these nodes vanish according to Eq. 3 and Eq. 4. T o clarify, we visua lize the ∂L task ∂x q in the second layer of GCN trained on Cora. As shown in Figure 3, most gradients fo r the nodes features are zero. The gradients of the Ltask w .r.t. quantized nodes features can be viewed as the supervi sed infor- mation from the labeled nodes which enable the training of th e quantization parameters for nodes features. However, this supervised information is missing due to zero gradients. Considering the quantization error is related to the Ltask, we introduce the quantization error E = 1 d |xq − x|1 as the supervised information for the quantization parameter s of nodes features, where x is the features before quantization, xq is the features after quantization and | · | 1 denotes the L1 norm. W e refer to this method as Local Gradient because the gradients are computed by the local quantizatio n er- rors instead of back-propagated task-related gradients. T hen the quantization parameters for node features can be trained by gradients from E: ∂E ∂s = 1 d d∑ i=1 sign(xi q− xi) · ∂xi q ∂s , (7) ∂E ∂b = 1 d d∑ i=1 sign(xi q− xi) · ∂xi q ∂b . (8) Note that the quantization parameters of W are still trained by utilizing the gradients in Eq. 3. 3.3 N E A RE S T NE IG H BO R ST RAT E G Y In graph-level tasks, the quantized GNNs are required to gen eralize to unseen graphs. In such a scenario, the number of input nodes may vary during training or inference. However, the learnable method can only train a ﬁxed number of (s, b ) pairs which are the same as the number of input nodes, so it is challenging to learn the s and b for every node in graph-level tasks. T o solve this problem, we propose the Nearest Neighbor Strategy , which allows learning of a ﬁxed number of quantization parameters and select quantization paramete rs for the unseen graphs. The proposed strategy is shown in Algorithm 1. T o ensure the n umerical range of xq is as close as to x at FP32, a simple way is to keep the maximum quantization valu e equal to the maximum absolute value of x. Based on this idea, we ﬁrst initialize m groups of quantization parameters, then we calculate the maximum quantization value for every group , i.e., qmax = s(2[b]−1 − 1). When quantizing the features of node i, the feature with the largest absolute value fi in the node features xi is ﬁrst selected, and then we ﬁnd the nearest qmax and quantize the node features with the (s, b ) corresponding to this qmax. When performing backpropagation, we ﬁrst calculate the gr adients of the loss function w .r.t. quantization parameters accordin g to Eq. 3 and Eq. 4. For a speciﬁc set of quantization parameters (sj , b j), we collect the gradients from the nodes that have used them 5Published as a conference paper at ICLR 2023 Algorithm 1 Nearest Neighbor Strategy 1: ForwardPass (X = ( x1, x2, ..., xN )T ): 2: Initialize(s, b), s ∈ Rm×1 + , b ∈ Rm×1 + before training 3: Calculate qmax = s ⊙ (2b−1 − 1) 4: Calculate the maximum absolute value in the features of each node: fi = max j abs(x(j) i ) 5: Search the index of quantization parameters for each node: indexi = arg min k |fi − qk max| 6: Quantize the i-th node features using (sindexi , b indexi ) 7: return Xq 8: end T able 1: The results comparison on node-level tasks. The ave rage bits are counted for each task when the best results are achieved. Dataset Model Accuracy A verage bits Compression Ratio Spee dup Cora GCN(FP32) 81.5±0.7% 32 1x — GCN(DQ ) 78.3±1.7% 4 8x 1x GCN(ours)80.9±0.6% 1.70 18.6x 2.00x GA T(FP32) 83.1±0.4% 32 1x — GA T(DQ ) 71.2±2.9% 4 8x 1x GA T(ours)82.6±0.6% 2.03 15.4x 1.49x CiteSeer GCN(FP32) 71.1±0.7% 32 1x — GCN(DQ ) 66.9±2.4% 4 8x 1x GCN(ours)70.6±1.1% 1.87 17.0x 1.91x GIN(FP32) 66.1±0.9% 32 1x — GIN(DQ ) 60.8±2.1% 4 8x 1x GIN(ours)65.1±1.7% 2.54 12.6x 1.37x PubMed GA T(FP32) 79.0±0.3% 32 1x — GA T(DQ) 70.6±12.5% 4 8x 1x GA T(ours)78.8±0.4% 2.12 15.1x 1.38x ogbn-arxiv GCN(FP32) 71.7±0.3% 32 1x — GCN(DQ) 65.4±3.9% 4 8x 1x GCN(ours)71.1±0.3% 2.65 12.1x 1.28x and add these gradients together. After the model has been tr ained, we obtain the quantization parameters (s, b). Since qmax can be calculated and sorted in advance, searching the neare st qmax can be implemented by binary searching. Usually, we set m = 1000 for all graph-level tasks in our paper and the overhead introduced to inference time is negli gible. 4 E XPERIME NT S 4.1 E X P E RIM E N TA L SE T T IN G S In this section, we evaluate our method on three typical GNN m odels, i.e., GCN, GIN, and GA T . And we compare our method with the FP32 GNN model and DQ-INT4 ( T ailor et al., 2020) on eight datasets, including four node-level semi-learning t asks (Cora, CiteSeer, PubMed, ogbn-arxiv) (Hu et al., 2020; Y ang et al., 2016) and four graph-level task s (REDDIT -BINAR Y , MNIST , CI- F AR10, ZINC) (Y anardag & V ishwanathan, 2015; Dwivedi et al. , 2020), to demonstrate the gen- erality and robustness of our method. Among these datasets, ZINC is a dataset for regression tasks, which uses regression loss as the metric of the model perform ance, while others are all for classiﬁ- cation tasks. For a fair comparison, we set the quantization bitwidth ofW for all GNNs to 4bits as DQ-INT4. W e count the average bitwidths for nodes features in all layers of the overall model and list them in our 6Published as a conference paper at ICLR 2023 T able 2: The results comparison on graph-level tasks. Dataset Model Accuracy (Loss ↓ ) A verage bits Compression ratio Speedup MNIST GCN(FP32) 90.1±0.2% 32 1x — GCN(DQ) 84.4±1.3% 4 8x 1x GCN(ours)89.9±0.8% 3.50 9.12x 1.17x GIN(FP32) 96.4±0.4% 32 1x — GIN(DQ) 95.5±0.4% 4 8x 1x GIN(ours)95.7±0.2% 3.75 8.52x 1.07x CIF AR10 GCN(FP32) 55.9±0.4% 32 1x — GCN(DQ) 51.1±0.7% 4 8x 1x GCN(ours)52.5±0.8% 3.32 9.62x 1.25x GA T(FP32) 65.4±0.4% 32 1x — GA T(DQ) 56.5±0.6% 4 8x 1x GA T(ours)64.7±2.8% 3.73 8.57x 1.12x ZINC GCN(FP32) 0.450±0.008 32 1x — GCN(DQ) 0.536±0.011 4 8x 1x GCN(ours)0.492±0.056 3.68 8.68x 1.08x REDDIT - BINARY GIN(FP32) 92.2±2.3% 32 1x — GIN(DQ) 81.3±4.4% 4 8x 1x GIN(ours)90.8±1.8% 3.50 9.14x 1.16x results, denoted by “ A verage bits”. Since today’s CPUs and G PUs can not support mixed-precision operations well, we implement a precision-scalable hardwa re accelerator to perform the overall in- ference process for GNN. The accelerator employs massive bi t-serial multipliers Judd et al. (2016), therefore, the latency of the integer multiplications is de termined by the bitwidth of the node fea- tures. T o evaluate the performance gains of our method over D Q-INT4, we develop a cycle-accurate simulator for our accelerator. More details about accelera tor architecture are shown in Appendix A.7.5. Moreover, we show the compression ratio of quantized GNNs compared to the FP32 models in terms of overall memory size. For simplicity, we use GNN(D Q) to represent the GNNs quantized by DQ-INT4 and GNN-dataset to represent the task in which we r un the experiment, e.g., GCN- Cora represents the GCN model trained on Cora. Detailed info rmation about datasets and settings is in Appendix A.5 and Appendix A.6. 4.2 N O D E -L E V E L TA S K S T able 1 shows the experimental results on three GNN architec tures trained on four node-level datasets. Compared with DQ-INT4, our method can achieve sig niﬁcantly better accuracy on each task, even with a higher compression ratio, improving the in ference performance with 1.28x to 2.00x speedups. On almost all node-level tasks, our proposed A2Q has negligible accuracy drop compared to the FP32 baselines while achieving 12.1x-18.6x compress ion ratio. Since both GIN and GA T in- volve more complex computations, such as the calculation of attention coefﬁcients in GA T , it is more challenging to quantize those models, and DQ performs p oorly on these two models. How- ever, our method can overcome this problem and maintain comp arable accuracy compared with the FP32 models. Our method can outperform the DQ-INT4 by 11.4% o n the GA T -Cora task with a smaller bitwidth (2.03 v.s. 4). Even on ogbn-arxiv, which ha s a large number of nodes, A2Q can achieve a 12.1x compression ratio compared with FP32 baseli ne with comparable accuracy, which demonstrates the robustness of our method. Moreover, to dem onstrate the generality of our method, we also evaluate our method on heterogeneous graphs and the i nductive learning tasks and compare with more related works in Appendix A.7.1. 4.3 G RA P H -L E V E L TA S K S T able 2 presents the comparison results on the graph-level t asks. Our method can obtain better results on all tasks than DQ-INT4 with higher compression and a consi derable speedup. Especially on the GIN-REDDIT -BINAR Y task, our method outperforms DQ-INT4 by 9.5% while achieving a 1.16x 7Published as a conference paper at ICLR 2023 /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000014/uni0000004e /uni00000015/uni0000004e /uni00000016/uni0000004e /uni00000019/uni0000001b/uni00000016 /uni00000015/uni00000016/uni0000001c/uni00000018 /uni00000015/uni00000017/uni00000019 /uni00000016/uni00000013 /uni00000013 (a) GCN-CiteSeer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000013 /uni00000014/uni0000004e /uni00000015/uni0000004e /uni00000016/uni0000004e /uni00000014/uni00000018 /uni00000016/uni00000018/uni00000018 /uni00000015/uni0000001a /uni00000014/uni0000001c/uni00000016/uni0000001b /uni0000001c/uni00000019/uni0000001b /uni00000015/uni00000016/uni00000014 (b) GIN-CiteSeer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000018 /uni00000014/uni00000011/uni00000013 /uni00000014/uni00000011/uni00000018 /uni00000015/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013 /uni00000014/uni0000004e /uni00000015/uni0000004e /uni00000016/uni0000004e /uni00000017/uni00000013/uni0000001c /uni00000015/uni0000001a/uni00000013/uni00000013 /uni00000015/uni00000014/uni0000001b /uni00000013 /uni00000013 /uni00000013 (c) GA T -CiteSeer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b /uni00000014/uni00000013 /uni00000013 /uni00000014/uni00000013/uni0000004e /uni00000015/uni00000013/uni0000004e /uni00000016/uni00000013/uni0000004e /uni00000017/uni00000013/uni0000004e /uni00000018/uni00000013/uni0000004e /uni00000013 /uni00000015/uni00000015/uni00000016/uni00000018/uni0000001b /uni00000015/uni0000001c/uni00000017/uni0000001a/uni00000013 /uni00000018/uni00000018/uni0000001c/uni00000017 /uni00000013 /uni00000013 (d) The ﬁrst layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000018/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000013 /uni00000014/uni00000013/uni0000004e /uni00000015/uni00000013/uni0000004e /uni00000016/uni00000013/uni0000004e /uni00000017/uni00000013/uni0000004e /uni00000018/uni00000013/uni0000004e /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013/uni00000016/uni00000013 /uni00000018/uni0000001a/uni00000015/uni00000018/uni00000016 /uni00000014/uni00000016/uni0000001c/uni00000013 (e) The second layer Figure 4: The relationship between quantized bitwidth and a verage in-degrees of nodes. (a), (b) and (c) represent the results of three GNN models trained on C iteSeer. (d) and (e) are results about the ﬁrst and the second layer of an MLP , which is the update fun ction of GIN trained on REDDIT - BINAR Y . The green bars represent the average in-degrees for the certain bitwidth used by nodes and the orange polylines represent the number of the nodes that u se this certain bitwidth. speedup. Even for graph datasets with similar in-degrees, s uch as MNIST and CIF AR10, our method also learns the appropriate bitwidths for higher compressi on ratio and better accuracy. Although on GIN-MINST task, the improvement of our method is relatively small due to the similarity of the in- degrees between different nodes, our method can achieve com parable accuracy with smaller bitwidth (3.75 v.s. 4). 4.4 A NA LY S IS T o understand why our approach works, we analyze the relatio nship between the learned bitwidths and the topology of the graph. Figure 4(a) and 4(b) reveal tha t the bitwidth learned by A2Q is strongly related to the topology of graph data in the node-le vel tasks. As the bitwidth increases, the average in-degrees of nodes become larger. In other word s, A2Q method tends to learn higher bitwidth for nodes with higher in-degrees. However, in GA T , as shown in Figure 4(c), the learned bits are irregular. This is because the features aggregated in GA T are topology-free. However, our method can still learn appropriate quantization bitwid ths for different nodes, which improves accuracy while reducing memory usage. In addition, Figure 4 also shows the node distribution for different bitwidths and the result is consistent with power -law distribution. Since nodes in graph data mainly have low in-degrees, most of the nodes are quantized t o low bitwidth ( ≤ 4), compressing the GNNs as much as possible. And there are also some high in-d egree nodes quantized to high bitwidth, which can help to maintain the accuracy of the GNN m odels. As a result, the average bitwidth of the entire graph features is low , and the accurac y degradation is negligible. For the graph-level tasks in which the number of nodes varies , our method is also aggregation- aware. W e select a layer of GIN trained on REDDIT -BINAR Y and a nalyze the relationship between bitwidth and average in-degrees of nodes using the correspo nding bitwidth to quantize in Figure 4(d) and 4(e). It can be seen that the bitwidth learned for nod es features input to the second layer of MLP , which is the update function in GIN for graph-level task s, does not present a correlation with the topology of graph. W e analyze the reason and ﬁnd that the n ode features before the second layer is the result mapped by the ﬁrst layer of MLP and is activated b y the activation function, e.g., ReLU, which results in the node features losing the topology infor mation. W e present more experiment results in Appendix A.7. to demonstrate that our method is ge nerally applicable. 5 A BLATION STUDY The advantage of learning-based mixed-precision quantiza tion: In Figure 5, we compare our A2Q with the manual mixed-precision method, which manually ass igns high-bit to those nodes with high in-degrees and low-bit to those nodes with low in-degre es. In the ﬁgure, the postﬁx “learn” denotes that using A2Q method, “manual” denotes that we assign bits to nodes and the model only learns the stepsize, and “mixed-precision” denotes that th e model uses the same quantization method as DQ-INT4 but assigning different bitwidths to nodes. For t he “mixed-precision”, we assign 5bits to those nodes with 50% top in-degrees and assign 3bits to oth ers. The implications are two-fold. First, compared with the DQ-INT4, which uses the same quanti zation bitwidth, the mixed-precision 8Published as a conference paper at ICLR 2023 T able 3: Ablation Study. Model Conﬁg Accuracy A verage bits GIN-Cora no-lr 33.7±4.1% 4 no-lr-b 75.6±0.2% 4 no-lr-s 56.1±4.9% 3.85 lr-all 77.8±1.6% 2.37 GCN- CiteSeer FP32 71.1±0.7% 32 Global 56.8±6.7% 3 Local 70.6±1.1% 1.87 /uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000018/uni00000015/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000016/uni00000011/uni00000013/uni00000016/uni00000011/uni00000018/uni00000017/uni00000011/uni00000013 /uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000025/uni0000004c/uni00000057/uni00000056 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013 /uni0000001a/uni00000013 /uni0000001b/uni00000013/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni0000000b/uni00000008/uni0000000c /uni00000019/uni00000014/uni00000011/uni00000019/uni00000008/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000008 /uni0000001a/uni0000001c/uni00000011/uni0000001c/uni00000008/uni0000001a/uni0000001b/uni00000011/uni0000001b/uni00000008 /uni00000015/uni00000011/uni00000016/uni00000008 /uni00000015/uni00000014/uni00000011/uni00000018/uni00000008 /uni0000002a/uni00000026/uni00000031/uni00000010/uni00000026/uni00000052/uni00000055/uni00000044/uni00000010/uni0000004f/uni00000048/uni00000044/uni00000055/uni00000051 /uni0000002a/uni00000026/uni00000031/uni00000010/uni00000026/uni00000052/uni00000055/uni00000044/uni00000010/uni00000050/uni00000044/uni00000051/uni00000058/uni00000044/uni0000004f /uni0000002a/uni0000002c/uni00000031/uni00000010/uni00000026/uni0000004c/uni00000057/uni00000048/uni00000036/uni00000048/uni00000048/uni00000055/uni00000010/uni0000004f/uni00000048/uni00000044/uni00000055/uni00000051 /uni0000002a/uni0000002c/uni00000031/uni00000010/uni00000026/uni0000004c/uni00000057/uni00000048/uni00000036/uni00000048/uni00000048/uni00000055/uni00000010/uni00000050/uni00000044/uni00000051/uni00000058/uni00000044/uni0000004f /uni0000002a/uni0000002c/uni00000031/uni00000010/uni00000026/uni0000004c/uni00000057/uni00000048/uni00000036/uni00000048/uni00000048/uni00000055/uni00000010/uni00000027/uni00000034/uni00000010/uni0000002c/uni00000031/uni00000037/uni00000017 /uni0000002a/uni0000002c/uni00000031/uni00000010/uni00000026/uni0000004c/uni00000057/uni00000048/uni00000036/uni00000048/uni00000048/uni00000055/uni00000010/uni00000050/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000010/uni00000053/uni00000055/uni00000048/uni00000046/uni0000004c/uni00000056/uni0000004c/uni00000052/uni00000051 /uni0000002a/uni00000026/uni00000031/uni00000010/uni00000026/uni00000052/uni00000055/uni00000044/uni00000010/uni00000027/uni00000034/uni00000010/uni0000002c/uni00000031/uni00000037/uni00000017 /uni0000002a/uni00000026/uni00000031/uni00000010/uni00000026/uni00000052/uni00000055/uni00000044/uni00000010/uni00000050/uni0000004c/uni0000005b/uni00000048/uni00000047/uni00000010/uni00000053/uni00000055/uni00000048/uni00000046/uni0000004c/uni00000056/uni0000004c/uni00000052/uni00000051 Figure 5: The comparison between learning bitwidth and assign manually. method obtains 1.1% gains on GCN-Cora tasks demonstrating t hat the mixed-precision method is more effective. Second, the results of the learning metho d outperform the manual method on all tasks. Especially for the models with a high compression ratio, on GIN-CiteSeer task, learning method can achieve 21.5% higher accuracy. This demonstrate s that our learning method can perform better than the assignment method according to prior knowle dge for mixed-precision quantization of GNNs. The power of learning the quantization parameters:Ablations of two quantization parameters (s, b) on the GIN-Cora task are reported in the ﬁrst row of T able 3. Th e “no-lr” denotes that do not use learning method, “no-lr-b” denotes that only learn the s tep size s , “no-lr-s” denotes that only learn the bitwidths b, and “lr-all” denotes that learn the bitwidth and step size s imultaneously. W e can see that learning the step size can signiﬁcantly increas e the accuracy and even the “no-lr-bit” model can outperform the DQ-INT4 at the same compression rat io. When learning the bitwidth and step size simultaneously, the model can achieve higher accu racy with a higher compression ratio. This is because our method learns lower bitwidths for most no des with low in-degrees and higher bitwidths for a tiny fraction of nodes with high in-degrees, which can improve the compression ratio while achieving higher accuracy. Local Gradient v .s. Global Gradient:T o demonstrate the effectiveness of our Local Gradient method, we compare the models trained with and without it on t he GCN-CiteSeer task in the last row of T able 3. The “Global” denotes that the model is trained with Eq. 3 and Eq. 4. The model trained with the local method outperforms the global method by 13.8% with a higher compression ratio. This is because the Local Gradient method can learn qu antization parameters for all nodes, while only quantization parameters for a part of nodes can be updated with the Global Gradient method due to the extreme sparse connection in the graph on th e node-level semi-supervised tasks. The overhead of Nearest Neighbor Strategy:W e evaluate the real inference time of the GIN model on the 2080ti GPU. On REDDIT -BINAR Y task, the model without t he selection process requires 121.45ms, while it takes 122.60ms for the model with our Near est Neighbor Strategy, which only introduces 0.95% overhead. But with the help of the Nearest N eighbor Strategy, our model can obtain 19.3% accuracy gains for quantized GIN on REDDIT -BIN AR Y . 6 C ONCLUSIO N This paper proposes A2Q, an aggregation-aware mixed-precision quantization meth od for GNNs, and introduces the Local Gradient and Nearest Neighbor Stra tegy to generalize A2Q to the node- level and graph-level tasks, respectively. Our method can l earn the quantization parameters for different nodes by fully utilizing the property of GNNs and g raph data. The model quantized by our A2Q can achieve up to a 18.6x compression ratio, and the accuracy degradation is negligible compared with the FP32 baseline. Compared with the prior SOT A, DQ-INT4, our method can signiﬁcantly improve 11.4% accuracy with up to a 2.00x speed up on different tasks. Our work provides a general, robust and feasible solution to speed up the inference of GNNs. 9Published as a conference paper at ICLR 2023 REFERENC ES Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lu cchi, Pascal Fua, and Sabine S ¨ usstrunk. Slic superpixels compared to state-of-the-ar t superpixel methods. IEEE transactions on pattern analysis and machine intelligence , 34(11):2274–2282, 2012. William Aiello, Fan Chung, and Linyuan Lu. A random graph mod el for power law graphs. Exper- imental mathematics , 10(1):53–66, 2001. Mehdi Bahri, Ga ´ etan Bahl, and Stefanos Zafeiriou. Binary g raph neural networks. In Proceedings of the IEEE/CVF Conference on Computer V ision and P attern Re cognition, pp. 9492–9501, 2021. Rajeev Balasubramonian, Andrew B Kahng, Naveen Muralimano har, Ali Shaﬁee, and V aishnav Srinivas. Cacti 7: New tools for interconnect exploration i n innovative off-chip memories. ACM T ransactions on Architecture and Code Optimization (TACO) , 14(2):1–25, 2017. Y oshua Bengio, Nicholas L ´ eonard, and Aaron Courville. Est imating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432 , 2013. John Brennan, Stephen Bonner, Amir Atapour-Abarghouei, Ph ilip T Jackson, Boguslaw Obara, and Andrew Stephen McGough. Not half bad: Exploring half-preci sion in graph convolutional neural networks. In 2020 IEEE International Conference on Big Data (Big Data) , pp. 2725–2734. IEEE, 2020. Alfredo Canziani, Adam Paszke, and Eugenio Culurciello. An analysis of deep neural network models for practical applications. arXiv preprint arXiv:1605.07678 , 2016. W eihan Chen, Peisong W ang, and Jian Cheng. T owards mixed-pr ecision quantization of neural networks via constrained optimization. In Proceedings of the IEEE/CVF International Conference on Computer V ision , pp. 5350–5359, 2021. Mucong Ding, Kezhi Kong, Jingling Li, Chen Zhu, John Dickers on, Furong Huang, and T om Gold- stein. Vq-gnn: A universal framework to scale up graph neura l networks using vector quantiza- tion. Advances in Neural Information Processing Systems , 34:6733–6746, 2021. Zhen Dong, Zhewei Y ao, Amir Gholami, Michael W Mahoney, and K urt Keutzer. Hawq: Hessian aware quantization of neural networks with mixed-precisio n. In Proceedings of the IEEE/CVF International Conference on Computer V ision , pp. 293–302, 2019. Zhen Dong, Zhewei Y ao, Daiyaan Arfeen, Amir Gholami, Michae l W Mahoney, and Kurt Keutzer. Hawq-v2: Hessian aware trace-weighted quantization of neu ral networks. Advances in neural information processing systems , 33:18518–18529, 2020. V ijay Prakash Dwivedi, Chaitanya K Joshi, Thomas Laurent, Y oshua Bengio, and Xavier Bresson. Benchmarking graph neural networks. arXiv preprint arXiv:2003.00982 , 2020. Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathi nakumar Appuswamy, and Dharmen- dra S Modha. Learned step size quantization. arXiv preprint arXiv:1902.08153 , 2019. Boyuan Feng, Y uke W ang, Xu Li, Shu Y ang, Xueqiao Peng, and Y uf ei Ding. Sgquant: Squeezing the last bit on graph neural networks with specialized quantiza tion. In 2020 IEEE 32nd International Conference on T ools with Artiﬁcial Intelligence (ICTAI) , pp. 1044–1052. IEEE, 2020. Matthias Fey and Jan Eric Lenssen. Fast graph representatio n learning with pytorch geometric. arXiv preprint arXiv:1903.02428 , 2019. Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol V inyals, and George E Dahl. Neural message passing for quantum chemistry. In International conference on machine learning , pp. 1263–1272. PMLR, 2017. Rafael G ´ omez-Bombarelli, Jennifer N W ei, David Duvenaud, Jos´ e Miguel Hern ´ andez-Lobato, Benjam´ ın S´ anchez-Lengeling, Dennis Sheberla, Jorge Agu ilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Al´ an Aspuru-Guzik. Automatic chemical de sign using a data-driven contin- uous representation of molecules. ACS central science , 4(2):268–276, 2018. 10Published as a conference paper at ICLR 2023 Will Hamilton, Zhitao Y ing, and Jure Leskovec. Inductive re presentation learning on large graphs. Advances in neural information processing systems , 30, 2017. Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mar k A Horowitz, and William J Dally. Eie: Efﬁcient inference engine on compressed deep ne ural network. ACM SIGARCH Computer Architecture News , 44(3):243–254, 2016. W eihua Hu, Matthias Fey, Marinka Zitnik, Y uxiao Dong, Hongy u Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machi ne learning on graphs. Advances in neural information processing systems , 33:22118–22133, 2020. Linyong Huang, Zhe Zhang, Zhaoyang Du, Shuangchen Li, Hongz hong Zheng, Y uan Xie, and Nianxiong T an. Epquant: A graph neural network compression approach based on product quan- tization. Neurocomputing, 503:49–61, 2022. Sambhav Jain, Albert Gural, Michael Wu, and Chris Dick. Trai ned quantization thresholds for accurate and efﬁcient ﬁxed-point inference of deep neural n etworks. Proceedings of Machine Learning and Systems , 2:112–128, 2020. Bowen Jin, Chen Gao, Xiangnan He, Depeng Jin, and Y ong Li. Mul ti-behavior recommendation with graph convolutional networks. In Proceedings of the 43rd International ACM SIGIR Con- ference on Research and Development in Information Retriev al, pp. 659–668, 2020. Y ongcheng Jing, Y iding Y ang, Xinchao W ang, Mingli Song, and Dacheng T ao. Meta-aggregator: Learning to aggregate for 1-bit graph neural networks. In Proceedings of the IEEE/CVF Interna- tional Conference on Computer V ision , pp. 5301–5310, 2021. Patrick Judd, Jorge Albericio, T ayler Hetherington, T or M A amodt, and Andreas Moshovos. Stripes: Bit-serial deep neural network computing. In 2016 49th Annual IEEE/ACM International Sym- posium on Microarchitecture (MICRO) , pp. 1–12. IEEE, 2016. Thomas N Kipf and Max W elling. Semi-supervised classiﬁcati on with graph convolutional net- works. arXiv preprint arXiv:1609.02907 , 2016. Adam Lerer, Ledell Wu, Jiajun Shen, Timothee Lacroix, Luca W ehrstedt, Abhijit Bose, and Alex Peysakhovich. Pytorch-biggraph: A large scale graph embed ding system. Proceedings of Ma- chine Learning and Systems , 1:120–131, 2019. Qian Lou, Feng Guo, Lantao Liu, Minje Kim, and Lei Jiang. Auto q: Automated kernel-wise neural network quantization. arXiv preprint arXiv:1902.05690 , 2019. Mike O’Connor. Highlights of the high-bandwidth memory (hb m) standard. In Memory forum workshop, volume 3, 2014. Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbu chner, and Gabriele Monfardini. The graph neural network model. IEEE transactions on neural networks , 20(1):61–80, 2008. V ivienne Sze, Y u-Hsin Chen, Tien-Ju Y ang, and Joel S Emer. Ef ﬁcient processing of deep neural networks. Synthesis Lectures on Computer Architecture , 15(2):1–341, 2020. Shyam A T ailor, Javier Fernandez-Marques, and Nicholas D La ne. Degree-quant: Quantization- aware training for graph neural networks. arXiv preprint arXiv:2008.05000 , 2020. Stefan Uhlich, Lukas Mauch, Fabien Cardinaux, Kazuki Y oshi yama, Javier Alonso Garcia, Stephen Tiedemann, Thomas Kemp, and Akira Nakamura. Mixed precisio n dnns: All you need is a good parametrization. arXiv preprint arXiv:1905.11452 , 2019. Petar V eliˇ ckovi´ c, Guillem Cucurull, Arantxa Casanova, A driana Romero, Pietro Lio, and Y oshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903 , 2017. Hanchen W ang, Defu Lian, Y ing Zhang, Lu Qin, Xiangjian He, Y i guang Lin, and Xuemin Lin. Binarized graph neural network. W orld W ide W eb, 24(3):825–848, 2021a. 11Published as a conference paper at ICLR 2023 Junfu W ang, Y unhong W ang, Zhen Y ang, Liang Y ang, and Y uanfan g Guo. Bi-gcn: Binary graph convolutional network. In Proceedings of the IEEE/CVF Conference on Computer V ision a nd P attern Recognition , pp. 1561–1570, 2021b. Kuan W ang, Zhijian Liu, Y ujun Lin, Ji Lin, and Song Han. Haq: H ardware-aware automated quan- tization with mixed precision. In Proceedings of the IEEE/CVF Conference on Computer V ision and P attern Recognition , pp. 8612–8620, 2019. Cong Xie, Ling Y an, Wu-Jun Li, and Zhihua Zhang. Distributed power-law graph computing: Theoretical and empirical analysis. Advances in neural information processing systems , 27, 2014. Keyulu Xu, W eihua Hu, Jure Leskovec, and Stefanie Jegelka. H ow powerful are graph neural networks? arXiv preprint arXiv:1810.00826 , 2018. Pinar Y anardag and SVN V ishwanathan. Deep graph kernels. In Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery an d data mining , pp. 1365–1374, 2015. Hongxia Y ang. Aligraph: A comprehensive graph neural netwo rk platform. In Proceedings of the 25th ACM SIGKDD international conference on knowledge disc overy & data mining , pp. 3165– 3166, 2019. Zhilin Y ang, William Cohen, and Ruslan Salakhudinov. Revis iting semi-supervised learning with graph embeddings. In International conference on machine learning , pp. 40–48. PMLR, 2016. Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and V iktor Prasanna. Graph- saint: Graph sampling based inductive learning method. arXiv preprint arXiv:1907.04931 , 2019. Y iren Zhao, Duo W ang, Daniel Bates, Robert Mullins, Mateja J amnik, and Pietro Lio. Learned low precision graph neural networks. arXiv preprint arXiv:2009.09232 , 2020. 12Published as a conference paper at ICLR 2023 A A PPENDIX A.1 U N IF O RM QUA N T IZ AT IO N In this section, we will give a detailed introduction to the c ontent related to quantiﬁcation. A.1.1 Q UA N T IZ AT IO N PRO CE S S For a vector x, the xq is a quantized representation. Given the quantization step size s, s ∈ R+, and the quantization bitwidth b, b ∈ N+, then the uniform quantization is implemented as: ¯x = sign(x)      ⌊|x| s + 0. 5⌋, |x| < s (2b−1 − 1) 2b−1 − 1, |x| ≥ s(2b−1 − 1) . (9) The x at 32bits is mapped to the integer number set {−2b−1 + 1 , ..., 0, ..., 2b−1 − 1} where the bitwidth is #b bits, and the quantized representation can be calculated as xq = s · ¯x. For inference, ¯x can be used to compute matrix multiplication in the update ph ase or perform other computations in GNNs layers and the output of these computations then are r escaled by the corresponding s using a relatively lower cost scalar-vector multiplication. As a n illustrative example, for vectors x ∈ R3×1 and y ∈ R3×1, the quantization parameters are both s = 0 . 1, b = 5 , the process of inner product between these two vectors by integers is shown in Figure 6. Wh en the values in a vector are all non-negative, we do not need to represent the sign bit in the ﬁ xed-point representation. Therefore, the value can use #b bits to quantize instead of using the ﬁrst bit to represent th e sign bit. Then the quantization range of uniform quantization is [−s(2b − 1), s (2b − 1)]. 0.11 - 0.21 1.29 0.31  0.58 -0.27  -0.436 1 - 2 13  3 6 -3 -48   ! = 0.1  \" = 0.1Preform by float-point representation  Perform by integers representation  -48  Rescale  -48  ! \" -0.48  × = × = × = 0.11 (0.11) 0.5 1 0.1 sign ê ú ´ + = ê ú  ë û  0.21( 0.21) 0.5 2 0.1sign ê - ú - ´ + = - ê ú  ë û  5 1 0.1 (2 1) 1.5 -´ - =  Quantization process  Figure 6: An example of performing inner product by integers representation. A.1.2 G RA D IE N T IN BACK P RO PAG AT IO N Due to the ﬂoor function used in the quantization process is n ot differentiable, the gradient of xq with respect to x vanishes almost everywhere, which makes it impossible to tr ain the model by the backpropagation algorithm. Therefore, we use the straight -through estimator (Bengio et al., 2013) to approximate the gradient through the ﬂoor function, i.e., ∂L ∂x = ∂L ∂x q I|x|≤s(2b−1), where I|x|≤s(2b−1) is a indicator function, whose value is 1 when |x| ≤ s(2b − 1), and vice versa. In our paper, the quantiﬁcation parameters (s, b ) are learnable, the gradients of xq w .r.t. (s, b ) used in Eq. 3 and Eq. 4 are:   ∂x q ∂s ∂x q ∂b  =        [ 1 s (xq − x) 0 ] , |x| < s (2b−1 − 1) sign(x) [ ( 2b−1 − 1 ) 2b−1 ln (2) s ] , |x| ≥ s(2b−1 − 1) . (10) 13Published as a conference paper at ICLR 2023 T able 4: The aggregation functions and update functions for GNNs used in this paper, di denotes the degree of node i, the ε denotes a learnable constant, and α represent attention coefﬁcients. Model Aggregation function Update function GCN h(l) i = ∑ j∈N (i)∪{i} 1√di √ dj x(l−1) j x(l) i = ReLU (W (l)h(l) i + b(l)) GIN h(l) i = (1 + ε(l))x(l−1) i + ∑ j∈N (i) x(l−1) j x(l) i = MLP (l)(h(l) i , W (l), b(l)) GA T h(l) i = ∑ j∈N (i)∪{i} α (l) i,j x(l−1) j x(l) i = W (l)hl i+ b(l) T able 5: The statistics for density of adjacency matrix and t he labeled nodes in four node-level datasets. Cora CiteSeer PubMed ogbn-arxiv Density of A 0.144% 0.112% 0.028% 0.008% Labled nodes 5.17% 3.61% 0.30% 53.70% A.2 M O RE A BO U T GRA P H NE U RA L NE T WO RK S In this section, we ﬁrst give detailed information about the MPNN framework (Gilmer et al., 2017), and then provide a detailed examination of the three GNNs use d in our papers. A graph G = ( V, E) consist of nodes V = {1, ..., N } and edges E ⊆ V × V has node features X ∈ RN×F and optionally H-dimensional edge features E ∈ RE×H . The MPNN framework can be formulated by x(l) i = γ(l)(x(l−1) i , □ j∈N (i) φ(l)(x(l−1) i , x(l−1) j , e(l−1) ij )), where φ is a differentiable kernel function, □ is the aggregation function which is permutation-invarian t, and the γ is a learnable update function, xi is the features of node i and eij is the features of edge between node i and j, N (i) = {j : ( i, j ) ∈ E} , and l represents the l-th layer of the GNNs. In this paper, we focus on three typical GNN models whose forw ardpass all can be represented by the MPNN framework, Graph Convolution Network (GCN) (Kip f & W elling, 2016), Graph Iso- morphism Network (GIN) (Xu et al., 2018), and Graph Attentio n Network (GA T) (V eliˇ ckovi´ c et al., 2017). the detailed information is shown in T able 4. A.3 P RO O F S O F TH E O RE T ICA L RE S U LT S This section provides formal proof of the theoretical resul ts of our paper. A.3.1 N OTAT IO N S Here, we deﬁne the notations utilized in our proof. A = {0, 1}N×N is the adjacency matrix that indicates whether there is an edge between each pair of nodes , e.g., if there is an edge between node i and node j, then aij = 1 , otherwise, aij = 0 . Then, ˜A = A + I is the adjacency matrix for a graph 14Published as a conference paper at ICLR 2023 that is added to the self-loops. The degree matrix D = diag(d1, d 2, ..., d n), where di = ∑ j aij and the degree matrix for the graph having self-loops is ˜D = ( ˜d1, ˜d2, ..., ˜dn), where ˜di = ∑ j ˜aij. A.3.2 P RO O F S Proof 1. The gradients of the loss function with respect to the node fe atures in semi-supervised tasks are most zero. Without loss of generality, we use the GCN model as an example. From the T able 4, the graph convolution operation can be described as X(l+1) = σ( ˆAX(l)W (l)), (11) where ˆA = ˜D− 1 2 ˜A ˜D− 1 2 , is the normalized adjacency matrix, W (l) ∈ RFin×Fout is a learnable weight matrix in the l-th layer of GCN. X(l) is the input of the l-th layer and the output of the (l − 1)-th layer in GCN. σ is the non-linear activation function, e.g., ReLU. Note tha t the ˆA is an extreme sparse matrix for node-level datasets in our paper. In our training process of the model, we usenll loss as our task loss function L. Only the nodes in the train set T have labels. For the last layer of GCN, we get the node feature s to be classiﬁed by H(l+1) = softmax(X(l+1)). Then the gradient of L with respect to X(l+1) is G1 = ∇X(l+1) L = ∂L ∂H(l+1) · ∂H(l+1) ∂X(l+1) = [ lij ] ∈ RN×Fout , (12) where only the G1 i,:, i ∈ T is not zero, otherwise, G1 i,: = 0 . Then, the gradient of the loss function with respect to X(l) is G2 = ∇X(l) L = ˆAT (∇X(l+1) L ⊙ σ′( ˆAX(l)W (l)))(W (l))T . (13) For node j do not have an edge with the node i, i ∈ T , G2 j,: = 0 . T able 5 lists the density of the adjacency matrix A and the percentage of the labeled nodes in four node-level da tasets. Because the sparsity property of adjacency matrix and the nodes with trained labels only account for a tiny fraction of the graph, the gradients from the loss function f or most node features are zero. Proof 2. The normalized adjacency matrix ˆA is not needed to be quantized for the GCN model. W e take the process of XW → A(XW ) as an illustrative example, which represents ﬁrst calculat e the B = XW and then calculate AB. For the l-th layer of FP32 models, the ﬁrst stage is Bl = XlWl, and then calculate the Xl+1 = ˆABl, where Xl ∈ RN×F1 , Wl ∈ RF1×F2 and A ∈ RN×N . The step-size for Bl, Xl and Wl is SBl , SXl and SWl , respectively. And they are all diagonal matrices. The integer representations are calculated as Bl = Bl qSBl , Xl = SXl Xl q and Wl = Wl qSWl . Note that for the node-level tasks, we can obtain the SBl , SXl and SWl in advance. And for the graph-level tasks, we can obtain them through one mor e element-wise multiplication whose overhead is negligible, as the comparison in T able 6. Then th e ﬁrst stage is: Bl = Xl · Wl = ( SXl · Xl q) · (Wl q · SWl ) , (14) and there exists Bl = Bl qSBl . Therefore, the integers representation for the next stage can be calculated as: Bl q = BlS−1 Bl = ( SXl · Xl q) · (Wl q · SWl )S−1 Bl = ( SXl · Xl q) · (Wl q · (SWl S−1 Bl )) = ( SXl ⊗ (SWl S−1 Bl )) ⊙ (Xl q · Wl q) , (15) where the (SXl ⊗ (SWl S−1 Bl )) can be calculated ofﬂine. Then we obtain the ﬁxed-point repr esenta- tion Bl q for the next stage and do not introduce overhead. The process of node degree normalization after the aggregat ion process can be represented as Xl+1 = σ( ˆABl), where ˆA = D− 1 2 ˜AD− 1 2 is the normalized adjacency matrix, and σ is the 15Published as a conference paper at ICLR 2023 Figure 7: The pipeline of the quantization process on our acc elerator. non-linear activation function. D− 1 2 at the right side of ˜A can be fused into the SXl and then calculate Bl q as Eq. 15. Then the features of the (l + 1) -th layer Xl+1 can be obtained as Xl+1 = σ(D− 1 2 ˜ABl q). And there exits Xl+1 = SXl+1 X(l+1) q. Therefore, the X(l+1) q can be obtained as: X(l+1) q = S−1 Xl+1 Xl+1 = S−1 Xl+1 σ(D− 1 2 ˜ABl q) . (16) Note that the elements in diagonal matrix SXl+1 are all positive because this matrix is made up of step-size, which is always positive. Then we can obtain X(l+1) q = σ(S−1 Xl+1 D− 1 2 ˜ABl q) , where S−1 Xl+1 D− 1 2 can be obtained before inference and ˜A ∈ { 0, 1}N×N . The computation of ˜ABl q only has addition operations and the S−1 Xl+1 D− 1 2 can be obtained before inference for node-level tasks or introduce only once more element-wise multiplication to ca lculate for the graph-level tasks. The D− 1 2 at the left side is fused into the element-wise multiplicati on performed by the next layer and the D− 1 2 at the right side is fused into the element-wise multiplicat ion performed by the current layer and the element-wise multiplication is a necessary st age in the quantized model. Therefore, we can perform the node degree normalization using ﬁxed point a ddition operation instead of quantizing the normalized adjacency matrix which may introduce more qu antization error. Proof 3. The quantization process can be fused with Batch Normalizat ion operations. When GNNs have Batch Normalization (BN) Layers, the calcula tion process is as follows (Note that we have fused the mean and standard-deviation with the l earned parameters in BN): Xl+1 = BN (σ( ˆABl q)) = σ( ˆABl q)Y + Z , (17) where Y = diag(y1, y 2, ..., y F2 ) ∈ RF2×F2 , Z = ( z1, z2, ..., zF2 ) ∈ RN×F2 and zi = (θi, θ i, ..., θ i)T ∈ RN among which yi and θi are the BN parameters for the i-th dimension fea- ture of the nodes features. And there exits that Xl+1 = SXl+1 Xl+1 q. Therefore, Xl+1 q = S−1 Xl+1 Xl+1 = S−1 Xl+1 (σ( ˆABl q)Y + Z) = ( S−1 Xl+1 ⊗ Y ) ⊙ (σ( ˆABl q)) + S−1 Xl+1 Z . (18) Through Eq. 18, we can fuse the quantization of the next layer into the BN operation of the cur- rent layer, which will not introduce overhead because the BN layer itself requires ﬂoating point operations. Note that the ﬂoat point operations are also ele ment-wise. 16Published as a conference paper at ICLR 2023 T able 6: The comparison between ﬁxed-point operations and ﬂ oat-point operations for some tasks using the Nearest Neighbor Strategy. T ask GIN-RE-IB GCN-MNIST GA T -CIF AR10 GCN-ZINC Fixed-point(M) 936.96 455.69 1387.98 504.62 Float-point(M) 7.35 2.06 13.71 1.74 Ratio 0.78% 0.45% 0.98% 0.34% A.4 T H E OV E RH E A D ANA LY S IS O F NE A RE S T NE IG H BO R ST RAT E G Y Through our dedicated hardware and the optimized pipeline, we reduce the overhead introduced by the Nearest Neighbor Strategy (NNS) as much as possible. As t he pipeline is shown in Figure 7, we fuse the (NNS) with the following operations. The ﬁxed-po int results produced by the previous stage are used to ﬁrst multiply the corresponding step-size from the previous stage (an element- wise ﬂoat point multiplication) and then execute the NNS pro cess. After getting the step-size, these features are quantized immediately (an element-wise ﬂoat p oint multiplication). Therefore, through this fusion way, we do not need the extra memory to store a copy of FP32 features. In addition, the overhead of the NNS is from one more element- wise ﬂoat point multiplication and the search process. W e provide a comparison of the number of ﬂ oat-point operations and ﬁxed-point operations for different graph-level tasks in T able 6, wher e ‘Fixed-point’ denotes the ﬁxed-point op- eration, ‘Float-point’ denotes the ﬂoat-point operation a nd the ‘Ratio’ denotes the percentage of the ﬂoat-point operations in the overall process. The extra ﬂoa t-point operations introduced by NNS is only a tiny fraction of the ﬁxed-point operations. On the oth er hand, through our optimized pipeline and the comparator array used in our accelerator the latency introduced by the search process of the NNS can be overlapped. Therefore, the overhead introduced b y NNS is negligible. A.5 D ATA S E T S W e show the statistics for each dataset used in our work in T ab le 7. For datasets in node-level tasks, nodes correspond to documents and edges to citations between them. Node features are a bag-of-words representation of the document. The target is to classify each node in the graph cor- rectly. The Cora, CiteSeer and PubMed are from Y ang et al. (2016). The ogbn-arxiv, ogbl-mag and ogbn-collab are from Hu et al. (2020). The Flickr is from Zeng et al. (2019). The Reddit is from Hamilton et al. (2017). In graph-level tasks, REDDIT -BINARY(Y anardag & V ishwanathan, 2015) is a balanced dataset where each graph corresponds to a n online discussion thread and the nodes correspond to users. There would be an edge between two nodes if at least one of them responded to another’s comment. The task is then to identify whether a given graph belongs to a question/answer-based community or a discussion-based co mmunity The MNIST and CIF AR-10 datasets (Dwivedi et al., 2020) which are often used for imag e classiﬁcation tasks are transformed into graphs in which every node is represented by their super pixel and location, and the edges are constructed by Achanta et al. (2012). The task is to classify the image using its graph representation. The ZINC G ´ omez-Bombarelli et al. (2018) dataset contains graphs re presenting molecules, where each node is an atom. The task is to regress the penalized logP (also called constrained solubility in some works) of a given graph. In Figure 8, we show the in-deg ree distribution for all the datasets we use in our paper. A.6 E X P E RIM E N TA L SE T U P T o make a fair comparison, we adopt the same GNN architecture s as T ailor et al. (2020) on every task, and the FP32 baseline is also the same. For those tasks t hat T ailor et al. (2020) does not do, we adopt the same architecture as their FP32 version. For ogbn-arxiv and PubMed, we use the 17Published as a conference paper at ICLR 2023 T able 7: The statistics for each dataset used in this work. T ask Name Graphs Nodes Edges Features Classes Node-level Cora 1 2708 10556 1433 7 CiteSeer 1 3327 9104 3703 6 PubMed 1 19717 88648 500 3 ogbn-arxiv 1 169343 1166243 128 23 ogbn-mag 1 1939743 25582108 128 349 ogbl-collab 1 235868 1285465 128 – Reddit 1 232965 11606919 602 41 Flickr 1 89250 899756 500 7 Graph-level REDDIT -BINAR Y 2000 ∼429.6 ∼995.5 0 2 MNIST 70000 ∼71 ∼565 3 10 CIF AR10 60000 ∼117.6 ∼941.2 5 10 ZINC 12000 ∼23 ∼49.8 28 — architectures and FP32 results reported by Hu et al. (2020) a nd Kipf & W elling (2016) respectively. W e use standard splits for MNIST , CIF AR-10, and ZINC (Dwived i et al., 2020). For Cora, CiteSeer and PubMed, we use the splits used by Y ang et al. (2016). For REDDIT -BINA R Y , we use 10-fold cross-validation. Our data split way is also the same as DQ-I NT4. Figure 9 shows the architectures of the models used in our eva luations, including the layers, the number of hidden units, and whether to use a skip connection. Our method is implemented using PyT orch Geometric (Fey & Lenssen, 2019). W e quantize the same parts as the DQ-INT4 in all models except for the normali zed adjacency matrix in the GCN model, which we have proven that the quantization of this mat rix is not necessary in Appendix A.3.2, proof 2.. The values in the Cora and CiteSeer are all 0 or 1, therefore, we do not quantize the input features for the ﬁrst layer of the GNNs trained on the two data sets as DQ. For all quantized GNNs, we train them by Adam optimizer. The learning rate and the lea rning rate schedule are consistent with their FP32 version. In our method, the quantization par ameters (s, b ) are also learnable, so we set the learning rate for them, including the b for features, s for features, and s for weights. When initializing, the parameters of the models are initial ized as their FP32 version, the quantization bits for all nodes and weight matrixes are initialized by 4bi ts, and the step sizes for node features and weights are initialized by s ∈ N (0. 01, 0. 01) except for the graph-level tasks on GA T , where we initialize the step size by s ∈ U (0, 1). The N is normal distribution and the U is uniform distribution. And for GA T model trained on graph-level data sets, we just learn the quantization bits of the node features, while in the attention coefﬁcients com putation part, we use the exact 4bit to quantize. The batch size is 128 in all graph-level tasks. The results reported in our work for GNNs on Cora, CiteSeer and PubMed are averaged over 100 runs with different seeds, and the resu lts for ogbn-arxiv, MNIST, CIF AR-10and ZINC are averaged over ten runs. The results on REDDIT - BINARY are obtained by 10-fold cross-validation and the split seed is 12345, which is the same as DQ-INT4. All experiments in our paper ran on R TX 2080Ti GPU driven by Ubuntu 18.04. The version of the CUDA and Pytorch are 10.2 and 1.8.0, respectiv ely. A.6.1 E X P E RIM E N TA L S E T U P S F O R T H E A BL AT IO N S T U DY . The advantage of learning-based mixed-precision quantization: During the experiment of com- paring the learning bitwidth and bit assignment, we ensure t he average bits of node features of these two methods are comparable to verify the effectiveness of ou r A2Q method. As an example, if the average bit is 2.2bit when assigning the bit to nodes with dif ferent in-degrees, we will ﬁrst sort the 18Published as a conference paper at ICLR 2023 /uni00000013/uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013 /uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000014/uni00000013 /uni00000013 /uni00000014/uni00000013 /uni00000014 /uni00000014/uni00000013 /uni00000015 /uni00000014/uni00000013 /uni00000016 /uni00000031/uni00000052/uni00000047/uni00000048/uni00000003/uni00000031/uni00000058/uni00000050 (a) Cora /uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013 /uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000014/uni00000013 /uni00000013 /uni00000014/uni00000013 /uni00000014 /uni00000014/uni00000013 /uni00000015 /uni00000014/uni00000013 /uni00000016 /uni00000031/uni00000052/uni00000047/uni00000048/uni00000003/uni00000031/uni00000058/uni00000050 (b) CiteSeer /uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013/uni00000014/uni00000017/uni00000013/uni00000014/uni00000019/uni00000013 /uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000014/uni00000013 /uni00000013 /uni00000014/uni00000013 /uni00000014 /uni00000014/uni00000013 /uni00000015 /uni00000014/uni00000013 /uni00000016 /uni00000014/uni00000013 /uni00000017 /uni00000031/uni00000052/uni00000047/uni00000048/uni00000003/uni00000031/uni00000058/uni00000050 (c) PubMed /uni00000014/uni00000013 /uni00000014 /uni00000014/uni00000013 /uni00000015 /uni00000014/uni00000013 /uni00000016 /uni00000014/uni00000013 /uni00000017 /uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000014/uni00000013 /uni00000013 /uni00000014/uni00000013 /uni00000014 /uni00000014/uni00000013 /uni00000015 /uni00000014/uni00000013 /uni00000016 /uni00000014/uni00000013 /uni00000017 /uni00000014/uni00000013 /uni00000018 /uni00000031/uni00000052/uni00000047/uni00000048/uni00000003/uni00000031/uni00000058/uni00000050 (d) ogbn-arxiv /uni00000013/uni00000018/uni00000014/uni00000013/uni00000014/uni00000018 /uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000014/uni00000013 /uni00000016 /uni00000014/uni00000013 /uni00000017 /uni00000014/uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000019 /uni00000031/uni00000052/uni00000047/uni00000048/uni00000003/uni00000031/uni00000058/uni00000050 (e) MNIST /uni00000013/uni00000018/uni00000014/uni00000013/uni00000014/uni00000018 /uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000014/uni00000013 /uni00000014 /uni00000014/uni00000013 /uni00000015 /uni00000014/uni00000013 /uni00000016 /uni00000014/uni00000013 /uni00000017 /uni00000014/uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000019 /uni00000031/uni00000052/uni00000047/uni00000048/uni00000003/uni00000031/uni00000058/uni00000050 (f) CIF AR10 /uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013 /uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000014/uni00000013 /uni00000013 /uni00000014/uni00000013 /uni00000014 /uni00000014/uni00000013 /uni00000015 /uni00000014/uni00000013 /uni00000016 /uni00000014/uni00000013 /uni00000017 /uni00000014/uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000019 /uni00000031/uni00000052/uni00000047/uni00000048/uni00000003/uni00000031/uni00000058/uni00000050 (g) REDDIT -BINAR Y /uni00000014/uni00000015/uni00000016/uni00000017 /uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000014/uni00000013 /uni00000017 /uni00000014/uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000019 /uni00000031/uni00000052/uni00000047/uni00000048/uni00000003/uni00000031/uni00000058/uni00000050 (h) ZINC Figure 8: The in-degree distribution for each dataset used i n this work. nodes by their in-degrees and then select the nodes with the t op 20% in-degrees, and quantize those by 3bit, and for the remaining nodes, we use 2bit to quantize. In the model trained by the bit assign- ment method, the bit is not learnable, and other hyperparame ters are all consistent with the model using the A2Q method. For the “GCN-Cora-mixed-precision” and “GIN-Cite Seer-mixed-precision” tasks, we use 3bit and 5bit to quantize the GNNs while keeping the average bitwidth at 4bits. In particular, we assign 5bits to those nodes with 50% top in-de grees and assign 3bits to others. 19Published as a conference paper at ICLR 2023 Figure 9: The model architectures used in our evaluations, t he head number for all GA T models on different tasks are 8. T able 8: The results comparison on GCN-PubMed and GIN-ogbn- arxiv. Accuracy A verage bits Compression Ratio Speedup PubMed GCN(FP32) 78.9±0.7% 32 1x — GCN(DQ) 62.5±2.4% 4 8x 1x GCN(ours)77.5±0.1% 1.90 16.8x 1.45x ogbn-arxiv GIN(FP32) 68.8±0.2% 32 1x — GIN(DQ) 57.6±2.2% 4 8x 1x GIN(ours)65.2±0.4% 3.82 8.4x 1.02x The power of learning the quantization parameters:For the “no-lr-bit”, we initialize the bitwidth as 4bits for all nodes features and just train the step size. F or the “no-lr-step”, we initialize the step size as previously mentioned but do not train them. For the “n o-lr”, we just initialize the bitwidth and the step size, but do not train them. Local Gradient v .s. Global Gradient:All settings of the model trained by global gradient is consistent with the model trained by local gradient method. The overhead of Nearest Neighbor Strategy:The model, without using the Nearest Neighbor Strategy, selects the quantization parameters according t o their in-degrees. Every in-degree has a corresponding group of quantization parameters. Those nod es whose in-degrees are larger than 1000 will share the same group quantization parameters. In t his way, The quantization parameters used by the nodes features can be determined as soon as the gra ph data is available, without the need for selection during the inference process, and then we can c ompare the overhead introduced by the selection process. A.7 M O RE EX P E RIM E N T S RE S U LT S This section is a complementary part about experiments resu lts to demonstrate that our A2Q quan- tization method is general and robust. 20Published as a conference paper at ICLR 2023 T able 9: The results comparison on inductive learning tasks and more graphs. T ask Acc(%) A verage bits Compression Ratio GCN-mag 30.8±0.1(FP32) 32 1x 32.7±0.4(Ours) 2.7 11.7x GCN-collab 44.8±1.1(FP32) 32 1x 44.9±1.5(Ours) 2.5 12.7x GraphSage- REDDIT 95.2±0.1(FP32) 32 1x 95.3±0.1(Ours) 3.9 8.1x GraphSage- Flickr 50.9±1.0(FP32) 32 1x 50.0±0.5%(Ours) 3.8 8.4x T able 10: Comparison with more quantization method. T ask Acc(%) A verage Bits Compression Ratio GCN-Cora 80.9±0.0(Half-pre) 16 1x 80.9±0.6(Ours) 1.7 9.40x GA T -CiteSeer 68.0±0.1(LPGNAS) 8 1x 71.9±0.7(Ours) 1.9 4.21x GraphSage-Cora 74.3±0.1(LPGNAS) 12 1x 74.5±0.2(Ours) 2.7 4.44x GraphSage- Flickr 49.7±0.3(LPGNAS) 8 1x 50.0±0.5(Ours) 3.8 2.11x /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000018/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000014/uni00000013/uni0000001a/uni0000001a /uni00000014/uni00000017/uni00000013/uni00000017 /uni00000014/uni00000017/uni0000001b/uni00000018/uni00000015/uni00000013 (a) GCN-Cora /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000018/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000015/uni00000014/uni0000001c/uni00000019 /uni00000017/uni00000017/uni00000013 /uni00000019/uni00000017/uni0000001b/uni00000013 /uni00000013 (b) GIN-Cora /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018 /uni00000019/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000018/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni0000001a/uni00000016 /uni00000014/uni0000001b/uni00000019/uni0000001a /uni0000001a/uni00000015/uni0000001a /uni00000017/uni00000013/uni00000014/uni00000013 (c) GA T -Cora /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000015/uni00000013/uni00000017/uni0000001b /uni00000014/uni0000001a/uni00000019/uni00000019/uni00000015 /uni0000001a/uni00000013 /uni00000013 /uni00000013 (d) GCN-PubMed /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000014/uni00000013/uni00000017/uni00000016/uni00000018 /uni0000001c/uni00000013/uni00000017/uni0000001c /uni00000015/uni00000016/uni00000015/uni00000014/uni00000013 /uni00000013 (e) GA T -PubMed /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000015/uni00000018 /uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013/uni00000013 /uni0000001a/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni0000001a/uni0000001b/uni00000014/uni00000019/uni00000018 /uni0000001b/uni00000019/uni00000014/uni00000013/uni0000001b /uni00000017/uni00000017/uni00000016/uni0000001b/uni00000018/uni00000016/uni00000013/uni0000001c/uni00000014/uni00000014/uni00000014 (f) GCN-ogbn-arxiv /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013/uni00000013 /uni0000001a/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013 /uni00000015/uni00000013/uni0000001c/uni0000001b/uni0000001b /uni00000014/uni00000017/uni0000001b/uni00000016/uni00000018/uni00000018 /uni00000013 /uni00000013 (g) GIN-ogbn-arxiv Figure 10: The relationship between bit and average in-degr ees of nodes using the corresponding bitwidth to quantize. (a), (b) and (c) Three GNN models train ed on Cora. (d) and (e) GCN and GA T trained on PubMed, respectively. (f) and (g) GCN and GIN trai ned on ogbn-arxiv, respectively. 21Published as a conference paper at ICLR 2023 T able 11: The effect of #m on the accuracy of quantized model, using GIN trained on REDDIT - BINAR Y as an example. The average bitwidth is 4bits. GIN(FP32): 92.2± 2.3% GIN(DQ): 81.3± 4.4% m 100 400 800 1000 1500 Accuracy 88.7±3.5% 90.6±3.8% 92.0±2.2% 92.5±1.8% 92.6±1.9% A.7.1 N O D E -L E V E L TA S K S In T able 8, we show more task results on PubMed and ogbn-arxiv. On the GA T -ogbn-arxiv task, our GPU raised the Out Of Memory error, so we do not report the resu lts on the GA T -ogbn-arxiv task. The model quantized by our A2Q method is also signiﬁcantly better than DQ-INT4, which show s that our A2Q is general. W e do not compare with DQ-INT8 because our result s are comparable with the FP32 baseline with a much larger compression ratio than D Q-INT8. W e also show the relationship between bit and average in-deg rees of nodes using the corresponding bitwidth to quantize on more tasks in Figure 10. W e present th e results of the ﬁnal layer of GNNs. The results show that the bitwidth learned by our A2Q method is also aggregation-aware, which means that our method is robust. W e also evaluate the inductive model, GraphSage, on some other node-level tasks to demonstrate the generality of our method on inductive learning tasks. Du e to the sampling operation in the GraphSage model, the subgraph input to the model varies, we a pply our nearest neighbor strategy to these tasks, i.e., GraphSage-Flickr and GraphSage-Reddit . In addition, we evaluate our method on more datasets, such as the ogbn-mag and ogbl-collab. ogbn-m ag is a heterogeneous graph and the ogbl-collab is used for the link prediction tasks. The results of our experiments are presented in T able 9, where we can see that our approach still works well and even brings some generalization performance improvement while signiﬁcantly com- pressing the model size. This also demonstrates that our Nei ghbor Nearest Strategy generalizes well on inductive models for node-level tasks. W e also compare with more quantization methods on GNNs. Zhaoet al. (2020) uses the Network Architecture Search (NAS) to search for the best quantizati on strategy for different components in the GNNs. Brennan et al. (2020) explore the use of half-preci sion (i.e., FP16) in the forward and backward passes of GNNs. T able 10 presents the comparison re sults on various tasks with these two methods. ‘Half-pre’ denotes the method in Brennan et al. (2020), and ‘LPGNAS’ denotes the method in Zhao et al. (2020). The results demonstrate that ou r method achieves better accuracy with a smaller quantization bitwidth on all tasks. A.7.2 G RA P H -L E V E L TA S K S W e propose the Nearest Neighbor Strategy to quantize the nod e features in graph-level tasks, in which the number of nodes input to models is various. In our Ne arest Neighbor Strategy, #m groups quantization parameters (s, b ) should be initialized, and we explore the effect of the value of m on the performance of the quantized model in T able 11 using the G IN trained on REDDIT -BINAR Y dataset. W e can observe that when the value of m is smaller than 800, the accuracy increases as the value of m increases. When the value of m is higher than 800, the performances of the models with different m are similar. However, the models with a larger m are more stable. Moreover, the selection of m may be related to the number of nodes input to the model. Accor ding to our experiments, we ﬁnally select m as 1000 for all graph-level tasks. T able 12 lists the comparison results on GIN-ZINC and GA T -ZI NC. On the regression tasks, our method is also signiﬁcantly better than DQ-INT4. Notably, w e do not learn different bitwidths for the nodes in ZINC datasets due to the similar topology struct ure between nodes. 22Published as a conference paper at ICLR 2023 T able 12: The results comparison on GIN-ZINC and GA T -ZINC. Modle Dataset Loss ↓ A verage bits Compression Ratio ZINC GA T(FP32) 0.455±0.006 32 1x GA T(DQ) 0.520±0.021 4 8x GA T(ours)0.495±0.006 4 8x GIN(FP32) 0.334±0.024 32 1x GIN(DQ) 0.431±0.012 4 8x GIN(ours)0.380±0.022 4 8x /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013 /uni00000013 /uni00000016/uni00000013/uni00000015/uni00000015/uni00000015 /uni00000013 /uni00000013 (a) 1-st layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b /uni00000014/uni00000013 /uni00000014/uni00000015/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013/uni00000015/uni00000014/uni00000013 /uni00000016/uni00000013/uni00000015/uni00000013/uni00000014 /uni00000013 /uni00000013 (b) 2-nd layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013/uni00000014 /uni00000016/uni00000013/uni00000015/uni00000015/uni00000014 /uni00000013 /uni00000013 /uni00000013 (c) 3-rd layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000016/uni00000016/uni00000018/uni00000018 /uni00000015/uni00000019/uni0000001b/uni00000019/uni0000001a /uni00000013 /uni00000013 /uni00000013 (d) 4-th layer Figure 11: The relationship between bit and average in-degr ees of nodes using the corresponding bitwidth to quantize on different layers of GCN trained on CI F AR10. /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013 /uni00000013 /uni00000016/uni00000013/uni00000013/uni00000016/uni00000015 /uni00000013 /uni00000013 (a) 1-st layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013 /uni00000018/uni00000015/uni0000001c/uni0000001a /uni00000015/uni00000017/uni0000001a/uni00000016/uni00000018 /uni00000013 /uni00000013 (b) 2-nd layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013/uni00000016/uni00000013 /uni0000001c/uni00000013/uni00000019/uni00000017 /uni00000015/uni00000013/uni0000001c/uni00000016/uni0000001b /uni00000013 /uni00000013 (c) 3-rd layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000019 /uni00000015/uni00000017/uni00000018/uni00000015 /uni00000015/uni00000019/uni00000016/uni00000019/uni00000015 /uni00000014/uni00000015/uni00000014/uni00000015/uni00000013 /uni00000013 (d) 4-th layer Figure 12: The relationship between bit and average in-degr ees of nodes using the corresponding bitwidth to quantize on different layers of GIN trained on CI F AR10. W e also show the relationship between bit and average in-deg ree of nodes using the correspond- ing bit to quantize for more graph-level tasks in different l ayers immediately after the aggregation phase in Figure 11-Figure 16. The quantization bitwidths le arned for graph-level tasks are also aggregation-aware. Because the difference of the in-degre es between different nodes is little in the MNIST and CIF AR10 dataset resulting in the aggregated fe atures are similar between different nodes, the relationship between learned bitwidths and the i n-degrees is irregular in some layers, e.g., the 2-nd layer in GCN trained on MNIST . 23Published as a conference paper at ICLR 2023 /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013 /uni0000001a/uni00000018/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000018/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013 /uni00000013 /uni00000014/uni00000018/uni00000013/uni00000016/uni0000001c /uni00000013 /uni00000013 (a) 1-st layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013 /uni0000001a/uni00000018/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000018/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013 /uni00000014/uni00000017/uni00000013/uni0000001b/uni00000016 /uni0000001c/uni00000018/uni00000019 /uni00000013 /uni00000013 (b) 2-nd layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013 /uni0000001a/uni00000018/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000018/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013 /uni00000014/uni00000017/uni00000018/uni0000001c/uni00000015 /uni00000017/uni00000017/uni0000001a/uni00000013 /uni00000013 (c) 3-rd layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015/uni00000018/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013 /uni0000001a/uni00000018/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000018/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013 /uni00000014/uni00000017/uni0000001a/uni00000018/uni0000001c /uni00000015/uni0000001b/uni00000013/uni00000013 /uni00000013 (d) 4-th layer Figure 13: The relationship between bit and average in-degr ees of nodes using the corresponding bitwidth to quantize on different layers of GA T trained on CI F AR10. /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013 /uni00000017/uni0000001c/uni0000001a/uni00000016 /uni00000015/uni00000014/uni00000017/uni0000001c /uni00000013 /uni00000013 (a) 1-st layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013 /uni0000001a/uni00000014/uni00000015/uni00000015 /uni00000013 /uni00000013 /uni00000013 (b) 2-nd layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013 /uni0000001b/uni00000014/uni00000014 /uni00000019/uni00000016/uni00000014/uni00000014 /uni00000013 /uni00000013 (c) 3-rd layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000015/uni00000019/uni00000015/uni00000015 /uni00000017/uni00000018/uni00000013/uni00000013 /uni00000013 /uni00000013 /uni00000013 (d) 4-th layer Figure 14: The relationship between bit and average in-degr ees of nodes using the corresponding bitwidth to quantize on different layers of GCN trained on MN IST . /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013 /uni00000013 /uni0000001c/uni00000013/uni00000014/uni0000001a /uni00000013 /uni00000013 (a) 1-st layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013 /uni00000013 /uni0000001c/uni00000013/uni00000014/uni0000001a /uni00000013 /uni00000013 (b) 2-nd layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013/uni00000014/uni00000013/uni0000001a /uni0000001b/uni0000001c/uni00000014/uni00000013 /uni00000013 /uni00000013 (c) 3-rd layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013 /uni00000014/uni0000001c/uni00000019/uni0000001b /uni0000001a/uni00000013/uni00000017/uni0000001c /uni00000013 /uni00000013 (d) 4-th layer Figure 15: The relationship between bit and average in-degr ees of nodes using the corresponding bitwidth to quantize on different layers of GIN trained on MN IST . /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013 /uni00000013 /uni0000001c/uni00000013/uni0000001c/uni00000016 /uni00000013 /uni00000013 (a) 1-st layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000014/uni00000019/uni00000015/uni0000001b /uni0000001a/uni00000017/uni00000017/uni0000001a /uni00000014/uni0000001b/uni00000013 /uni00000013 (b) 2-nd layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013 /uni0000001b/uni0000001b/uni00000014/uni00000017 /uni00000015/uni0000001a/uni0000001c/uni00000013 /uni00000013 (c) 3-rd layer /uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019 /uni00000025/uni0000004c/uni00000057 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000051/uni00000052/uni00000047/uni00000048/uni00000056 /uni00000013 /uni00000013/uni00000015/uni00000014 /uni0000001c/uni00000013/uni00000019/uni0000001c /uni00000016/uni00000013 (d) 4-th layer Figure 16: The relationship between bit and average in-degr ees of nodes using the corresponding bitwidth to quantize on different layers of GA T trained on MN IST . 24Published as a conference paper at ICLR 2023 T able 13: The impact of the depth of GNNs on quantization perf ormance. Layers 3 4 5 T ask Accu(%) A varage Bits Accu(%) A varage Bits Accu(%) A varage Bits GCN-Cora FP32 80.5±0.6 32 79.3±0.1 32 75.8±3.2 32 Ours 80.2±0.6 2.94 78.2±0.9 3.54 75.0±1.2 3.61 GIN-Cora FP32 49.4±15.8 32 37.1±13.1 32 — — Ours 54.5±12.6 3.3 36.4±11.1 3.1 — — T able 14: The comparison between the model with and without s kip connection on GCN-Cora task. Layers GCN-Cora Without skip connection With skip connection FP32 Ours FP32 Ours 3 Accu(%) 80.5±0.6 80.2±0.6 82.5±0.5 82.2±0.7 Bits 32 2.94 32 2.37 4 Accu(%) 79.3±0.1 78.2±0.9 81.9±0.7 81.5±0.3 Bits 32 3.54 32 2.63 5 Accu(%) 75.8±3.2 75.0±1.2 81.1±1.1 80.6±0.6 Bits 32 3.61 32 2.72 6 Accu(%) 73.8±1.6 73.1±1.9 80.1±0.8 80.4±0.7 Bits 32 4.62 32 2.98 /uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000024/uni00000055/uni0000004c/uni00000003/uni00000050/uni00000048/uni00000044/uni00000051 /uni0000002f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni00000013 /uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000025/uni0000004c/uni00000057/uni0000005a/uni0000004c/uni00000047/uni00000057/uni0000004b /uni00000015/uni00000011/uni00000018/uni0000001a /uni00000016/uni00000011/uni00000013/uni00000017 /uni00000016/uni00000011/uni00000019/uni00000013 /uni00000017/uni00000011/uni00000016/uni00000013 /uni00000016/uni00000011/uni00000016/uni0000001b Figure 17: The average bitwidth for 2nd-5th layer in ﬁve layers GCN. /uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018 /uni00000024/uni00000055/uni0000004c/uni00000003/uni00000050/uni00000048/uni00000044/uni00000051 /uni0000002f/uni00000044/uni0000005c/uni00000048/uni00000055 /uni00000013 /uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018 /uni00000019 /uni00000025/uni0000004c/uni00000057/uni0000005a/uni0000004c/uni00000047/uni00000057/uni0000004b /uni00000015/uni00000011/uni0000001c/uni00000014 /uni00000017/uni00000011/uni00000013/uni00000013 /uni00000017/uni00000011/uni0000001a/uni0000001c /uni00000018/uni00000011/uni00000016/uni00000017 /uni00000019/uni00000011/uni00000013/uni00000017 /uni00000017/uni00000011/uni00000019/uni00000015 /uni00000014/uni00000011/uni00000015/uni00000016 /uni00000014/uni00000011/uni0000001c/uni00000018 /uni00000015/uni00000011/uni0000001b/uni00000015 /uni00000016/uni00000011/uni0000001b/uni00000014 /uni00000018/uni00000011/uni00000013/uni0000001b /uni00000015/uni00000011/uni0000001c/uni0000001b /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000016 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 /uni00000034/uni00000058/uni00000044/uni00000051/uni00000057/uni0000004c/uni0000005d/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000048/uni00000055/uni00000055/uni00000052/uni00000055 /uni00000024 /uni00000059/uni0000004a/uni00000003/uni00000045/uni0000004c/uni00000057/uni00000056/uni00000003/uni0000005a/uni0000004c/uni00000057/uni0000004b/uni00000003/uni00000056/uni0000004e/uni0000004c/uni00000053 /uni00000024 /uni00000059/uni0000004a/uni00000003/uni00000045/uni0000004c/uni00000057/uni00000056/uni00000003/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000056/uni0000004e/uni0000004c/uni00000053 /uni00000034/uni00000028/uni00000055/uni00000055/uni00000052/uni00000055/uni00000003/uni0000005a/uni0000004c/uni00000057/uni0000004b/uni00000003/uni00000056/uni0000004e/uni0000004c/uni00000053 /uni00000034/uni00000028/uni00000055/uni00000055/uni00000052/uni00000055/uni00000003/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000056/uni0000004e/uni0000004c/uni00000053 Figure 18: The average bitwidth and quan- tization error for 2nd-6th layer in six layers GCN. A.7.3 M O RE ABL AT IO N ST U DY The impact of the depth of GNNs on quantization performance: W e explore how a different number of GNN layers impacts the quantization performance o f GCN-Cora and GIN-CiteSeer. W e explore the quantization performance on 3,4,5,6 layers GCN model and 3,4 layers GIN model (the GCN and GIN used in T able 1 are 2 layers). W e did not explore the deeper GNN models because 25Published as a conference paper at ICLR 2023 T able 15: The comparison results on other aggregation funct ions. Baseline(FP32) Ours Bit Compression Ratio GIN sum 77.6±1.1% 77.8±1.6% 2.37 13.5x GIN mean 78.8±0.1% 78.5±0.6% 2.37 13.5x GIN max 78.6±1.6% 78.6±0.5% 1.97 16.2x /uni0000003e/uni00000014/uni0000000f/uni00000014/uni00000013/uni00000040 /uni0000003e/uni00000014 /uni00000014/uni0000000f/uni00000015/uni00000013/uni00000040/uni0000003e/uni00000015/uni00000014/uni0000000f/uni00000016/uni00000013/uni00000040/uni0000003e/uni00000016/uni00000014/uni0000000f/uni00000017/uni00000013/uni00000040/uni0000003e/uni00000017/uni00000014/uni0000000f/uni00000014/uni00000019/uni0000001b/uni00000040 /uni0000002c/uni00000051/uni00000010/uni00000047/uni00000048/uni0000004a/uni00000055/uni00000048/uni00000048 /uni000000ed/uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000018/uni00000011/uni00000013 /uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000015/uni00000011/uni00000018 /uni00000024 /uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000029/uni00000048/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni0000000b/uni00000024/uni00000049/uni00000057/uni00000048/uni00000055/uni00000003 /uni00000024/uni0000004a/uni0000004a/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni0000000c /uni00000036/uni00000058/uni00000050 /uni00000030/uni00000048/uni00000044/uni00000051 /uni00000030/uni00000044/uni0000005b Figure 19: The average aggregated nodes features in differe nt in-degree groups for models with different aggregation functions. the accuracy of the model decreases drastically as the number of model layers increases due to the over-smooth phenomenon in GNNs. As shown in T able 13, our method can also maintain the performance with a high compression ratio for the model with different layers compared with the FP32 model. In addition, we observe that the learned quantization bitwidth increases with the number of layers. W e analysis the average bitwidth used by 2nd to 5th layer for t he ﬁve layers GCN model in Figure 17. Our method learns a higher bitwidth for the deeper layer. Due to the over-smooth phenomenon that exists in the deep layer, the embedding features of diff erent nodes are similar in the deep layer. Therefore, we consider the deeper layer may need a higher qua ntization bitwidth to distinguish the embedding features of different nodes. The impact of skip connection on quantization performance:The ﬁrst column denoted by ‘With- out skip connection’ and the second column denoted by ‘With s kip connection’ of 18 present the comparison results for different layers GCN on Cora dataset s without skip connection and with skip connection, respectively. For the model with skip connecti on, our method is also effective. Our method learns a higher bitwidth for the deeper layer. Due to t he over-smooth phenomenon that ex- ists in the deep layer, we consider that the deeper layer may n eed a higher quantization bitwidth to distinguish the embedding features of different nodes. and the higher learned quantization bitwidth for deeper layers also alleviate quantization error. And co mpared to the quantized model with a skip connection, the learned quantization bitwidths are hi gher for the quantized model without skip connection. Figure 18 presents that the quantization error s of the model with skip connection are always higher than the model without skip connection in ever y layer which means that the model without skip connection is more sensitive to the quantizati on error. Therefore, a higher quantization bitwidth is necessary for the model without skip connection to maintain the performance. W e will add these analyses to the appendix in the revision. Scalability for models that use other aggregation functions: T o demonstrate that our method is also helpful to the GNNs using other aggregation functions r ather than the sum function, we replace the aggregation function of the GIN model, which is based on t he MPNN framework with mean and max functions, and we conduct the comparison experiment on t he Cora dataset. As shown in T able 26Published as a conference paper at ICLR 2023 T able 16: The comparison reults with the binary quantizatio n method on Cora and CiteSeer datasets. Accuracy A verage bits Compression ratio Cora GCN(FP32) 81.5±0.7% 32 1x Bi-GCN 81.2±0.8% 1 32x GCN(ours)81.4±0.7% 1.61 19.9x GIN(FP32) 77.6±1.1% 32 1x Bi-GIN 33.7±6.6% 1 32x GIN(ours)77.4±0.8% 1.92 16.7x GA T(FP32) 83.1±0.4% 32 1x Bi-GA T 31.9±0% 1 32x GA T(ours)82.6±0.5% 2.03 15.8x CiteSeer GCN(FP32) 71.1±0.7% 32 1x Bi-GCN 70.7±2.4% 1 32x GCN(ours) 70.7±0.7% 1.98 16.2x GIN(FP32) 66.1±0.9% 32 1x Bi-GIN 29.1±1.7% 1 32x GIN(ours)65.6±1.5% 2.39 13.4x GA T(FP32) 72.5±0.7% 32 1x Bi-GA T 20.6±2.6% 1 32x GA T(ours)71.0±0.7% 2.15 14.9x 19, the accuracy degradation is negligible and the compress ion ratio is high , indicating that our quantization scheme also applies to the GNNs with mean or max aggregation function. W e analyze the average features for different aggregation functions i n different in-degrees group in Figure 19. The average features of the sum and max functions are highly d ependent on in-degrees. The other insight is that the variance of the features is also highly de pendent on in-degrees. The analysis demonstrates the generality of our approach, w hich can capture differences between nodes introduced by topology information of graphs and comp ress the model size as much as possi- ble while maintaining the performance. A.7.4 C O M PA RIS O N WIT H BINA RY QUA N T IZ AT IO N ME T H O D In this section, we show the advantages of our method over the binary quantization method for GNNs. W e select the binary quantization method in W ang et al. (2021b) as our baseline. W e just ran the experiments on the node-level because the binary quanti zation method only supports node-level tasks, which is one of the drawbacks of the binary quantizati on method in GNNs. W e quantize the same part as W ang et al. (2021b) does for a fair comparison. The comparison results are shown in T able 16. The binary quantization method performs well on GCN, where the aggregation and update phases are simple. How ever, on both models, GA T and GIN, the accuracy drops signiﬁcantly compared with the FP32 baseline, which makes the deploy- ment unrealistic. However, our method is immune to this prob lem, although it has to use a higher average bit for node features which we believe is necessary f or GA T and GIN. In summary, our method outperforms the binary quantization method in two wa ys: 1. Our method can quantize more complex GNN models and ensure th e accuracy degradation is negligible compared with the FP32 baseline while achieving a high compression ratio of 13.4x- 19.9x. 2.Our method can be applied to graph-level tasks. However, the binary quantization method can not handle them. 27Published as a conference paper at ICLR 2023 Edge Buffer  Weight Buffer  Output Buffer  Input Buffer  Decode Control Unit  DRAM  Data Control signal  MAC …MAC MAC  MAC …MAC MAC  MAC …MAC MAC  MAC …MAC MAC  PE (a) 1 0 1 1 0 1 0 1Features  Weights  = 1 0 1 1023 hh h 1 0 1 1122 hh 1 0 1 1021 hh 1 0 1 1120 hh + + + = 55  0 1 0 1 1 0 1 1 + reg  <<  & (b) Figure 20: (a) The overview of our accelerator architecture . (b) An example of the bit-serial calcu- lation and the architecture of the MAC. A.7.5 A CCE L E RATO R ARCH IT E CT U RE In this section, we introduce the architecture of our hardwa re accelerator designed for GNN infer- ence. As presented in Section 3.1, we quantize each node feat ure to an appropriate precision and ﬁx the weights to 4bits. T o support mixed-precision computa tion, we adopt bit-serial multipliers at the core. Speciﬁcally, we follow the methodology in Judd et a l. (2016) to only serialize the node features. This way, it takes m cycles to complete the multiplication between an m-bit node feature with a 4bit weight, as shown in Figure 20(b). The product invo lving 2n is implemented by left-shift, i.e., for 2n × a, we can shift a left by n bits to implement the product. T o increase the computationa l throughput, we use 256 × 16 MACs which can process 256 16-dimensional features in paral lel. As shown in Figure 20(a), the compute unit is composed of 256 P rocessing Engines (PEs), each containing a row of 16 MACs. The architecture of the MAC is sho wn in Figure 20(b). The on-chip memory consists of an Edge Buffer, which stores t he adjacency matrix of graphs, a W eight Buffer, which stores the weight of the GNNs, an Input B uffer, and an Output Buffer to store the input features and the output result, and the regis ter of each MAC to store the partial sum. T o reduce data movement in the memory hierarchy, the input bu ffer and output buffer work in a swapped fashion, as the output of the current layer is the inp ut to the next layer. W e set the memory size of Input Buffer, Output Buffer, Edge Buffer, and the W ei ght Buffer to 2MB, 2MB, 256KB, and 256KB, respectively. The overview of our architecture is sh own in Figure 20(a). T o calculate Bl = XlW l, 256 consecutive rows in Xl and a column of W l are mapped onto the MAC array to compute 256 inner products in each phase. T o a chieve this, a column of W l is broadcast and shared among PEs. The results of the inner prod ucts are written to the output buffer, which can be reused to reduce the off-chip DRAM access. The ca lculation of Xl+1 = ABl is also in a inner-product manner. In this scenario, A is a sparse matrix. W e therefore represent A in the Compressed Sparse Row (CSR) format, where full zero rows or elements of A are eliminated. During inference, consecutive compressed rows of A and a column of Bl are mapped onto the MAC array in each phase. W e also sort the nodes in descending o rder according to their in-degrees, and the nodes with similar in-degrees are processed in paral lel simultaneously to alleviate the load imbalance problem when performing the aggregation operati ons. A.7.6 E N E RG Y EFFICIE N CY ANA LY S IS Our method can save energy cost signiﬁcantly from the follow ing two aspects: 1. By compressing the model size as much as possible, e.g., 18 .6x compression ratio on GCN-Cora as shown in T able 1, our method can signiﬁcantly reduce the me mory footprints. Figure 21 presents the energy table for the 45nm technology node. It shows that m emory access consumes further more energy than arithmetic operations. Therefore, the memory f ootprints domains the energy cost, and then compressing the model can save much energy cost. 2. Through our quantization method and the accelerator, themodel can perform inference using the ﬁxed-point operations instead of ﬂoat-point operations, w hich are much more energy-consuming 28Published as a conference paper at ICLR 2023 Figure 21: The energy table for 45nm technology node(Han et al., 2016; Sze et al., 2020). /uni0000002a/uni00000026/uni00000031/uni00000010/uni00000026/uni00000052/uni00000055/uni00000044/uni0000002a/uni0000002c/uni00000031/uni00000010/uni00000026/uni0000004c/uni00000057/uni00000048/uni00000036/uni00000048/uni00000048/uni00000055/uni0000002a/uni00000024 /uni00000037 /uni00000010/uni00000033/uni00000058/uni00000045/uni00000030/uni00000048/uni00000047/uni0000002a/uni00000026/uni00000031/uni00000010/uni00000030/uni00000031/uni0000002c/uni00000036/uni00000037/uni0000002a/uni00000024 /uni00000037 /uni00000010/uni00000026/uni0000002c/uni00000029 /uni00000024/uni00000035/uni00000014/uni00000013/uni0000002a/uni0000002c/uni00000031/uni00000010/uni00000035/uni00000028/uni00000010/uni00000025/uni0000002c/uni0000002a/uni00000048/uni00000052/uni00000003/uni00000030/uni00000048/uni00000044/uni00000051 /uni00000037 /uni00000044/uni00000056/uni0000004e/uni00000056 /uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018 /uni00000015/uni00000013 /uni00000015/uni00000018 /uni00000016/uni00000013 /uni00000016/uni00000018 /uni00000017/uni00000013 /uni00000028/uni00000051/uni00000048/uni00000055/uni0000004a/uni0000005c/uni00000003/uni00000028/uni00000049/uni00000049/uni0000004c/uni00000046/uni0000004c/uni00000048/uni00000051/uni00000046/uni0000005c/uni0000000b/uni00000029/uni0000002f/uni00000032/uni00000033/uni00000056/uni00000012/uni0000002d/uni0000000c /uni00000015/uni00000015/uni00000011/uni00000018× /uni00000016/uni00000016/uni00000011/uni00000014× /uni00000017/uni00000011/uni00000018× /uni00000018/uni00000011/uni00000018× /uni0000001c/uni00000011/uni00000019× /uni0000001b/uni00000011/uni00000017× /uni00000014/uni00000013/uni00000011/uni0000001a× /uni00000014× /uni00000014× /uni00000014× /uni00000014× /uni00000014× /uni00000014× /uni00000014× /uni0000002a/uni00000033/uni00000038 /uni00000024 /uni00000015 /uni00000034 Figure 22: The energy efﬁciency compared with 2080Ti GPU on various tasks. than ﬁxed-point operations. As shown in Figure 21, the 32bit ﬂoat MUL T consumes 18.5x energy compared to the 8bit int MUL T . Therefore, our method’s energ y consumption is much lower than the FP32 model. T o illustrate the advantage of our approach in terms of energy efﬁciency, we compare our accelerator with the 2080Ti GPU on various tasks. T o estimate the energy e fﬁciency of GPU, we use the nvidia- smi to obtain the power of GPU when performing the inference and m easure the inference time by time function provided by Python. Then we can get the energy cost o f GPU. W e also model the energy cost of our method on the accelerator. W e use High Band width Memory (HBM) as our off- chip storage. Then we count the number of integer operations , and ﬂoating point operations, and the number of accesses to SRAM and HBM when performing the inf erence process of the quantized models on our accelerator. Based on the data in T able 21, we es timate the energy consumed by ﬁxed- point operations and ﬂoating-point operations. The static and dynamic power of SRAM is estimated using CACTI 7.0(Balasubramonian et al., 2017). The energy o f HBM 1.0 is estimated with 7 pJ/bit as in (O’Connor, 2014). Figure 22 presents these results, wh ich shows that the the energy efﬁciency of our method is signiﬁcantly better than GPU. A.8 C O M P L E X IT Y ANA LY S IS In this section, we provide the analysis of the complexity of our proposed A2Q method, including the computational complexity and space complexity. Space Complexity:When analyzing the space complexity, we use the data size of t he node features as an approximation of the entire loaded data, including wei ghts and features, because the node features account for more than 90% of the overall memory cons umption for a GNN model. For a GNN has L layers, we assume that the input data to the ﬁrst laye r is X ∈ RN×F0 , and the dimension of the hidden features is F1. Then the dimension of the input to the 2-(L-1) layer is N × F1. After quantizing the model, the average bits of the feature maps ar e bm. The memory size includes two parts: 1. the nodes features bm[NF0 + ( L − 1)NF1]. 2. the quantization step size (a step size is a ﬂoat-point number which is 32bit) for each node 32NL. Therefore, the space complexity of the overall GNN model is as follows: M = bm[NF0 + (L − 1)NF1] + 32 NL. (19) W e can also obtain the ratio of the memory consumption of the s tep size in overall memory size: r = 32NL bm[NF0 + (L − 1)NF1]. (20) In the node-level tasks, the F0 is usually much larger than 32, e.g., 3703 in the CiteSeer dat aset. Moreover, in the graph-level tasks, we usually set m = 1000 , which is much smaller than the number of the input nodes to models, i.e., N. Therefore, alth ough our method learns the quantization step size for each node the memory overhead introduced by the quantization step size is negligible. Computational Complexity: The forward pass is divided into the aggregation and update p hases according to the MPNN framework. The aggregation phase can b e represented as Hl = ˆAXl, 29Published as a conference paper at ICLR 2023 and then the update phase calculates Xl+1 = HlW l. For ˆA ∈ RN×N , Xl ∈ RN×F1 , and W l ∈ RF1×F2 , the computational complexity of the FP32 models is O(N2F1 + NF1F2), which are all the ﬂoat-point operations. After quantizing the mod el, the ﬂoat-point matrix multiplication can be replaced by integer multiplication, and the element- wise operation, which calculates the mul- tiplication between integers and ﬂoat-point numbers accor ding to the Eq. 2. Then the computational complexity is C = OI (N2F1 + NF1F2) + OE (NF2), (21) where OI represents the complexity of the integers multiplication, whose cost is much lower than the ﬂoat-point operations, and the OE represents the complexity of the element-wise operations. Note that although we quantize each node features by different st ep size, the complexity of element-wise operation involving ﬂoat-point is the same as the DQ-INT4 be cause the number of element-wise operations is equal to the number of the elements in a feature map, i.e., N × F2. 30",
      "meta_data": {
        "arxiv_id": "2302.00193v1",
        "authors": [
          "Zeyu Zhu",
          "Fanrong Li",
          "Zitao Mo",
          "Qinghao Hu",
          "Gang Li",
          "Zejian Liu",
          "Xiaoyao Liang",
          "Jian Cheng"
        ],
        "published_date": "2023-02-01T02:54:35Z",
        "pdf_url": "https://arxiv.org/pdf/2302.00193v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the challenge of high latency and memory consumption in Graph Neural Networks (GNNs) inference, especially with increasing graph data size. It proposes Aggregation-Aware mixed-precision Quantization (A2Q) for GNNs, which automatically learns and assigns an appropriate bitwidth to each node based on graph topology. Key contributions include the A2Q method for adaptive quantization parameter learning, a Local Gradient method to mitigate vanishing gradients in semi-supervised tasks, and a Nearest Neighbor Strategy for generalization to unseen graphs. Experiments show up to 18.6x compression ratio with negligible accuracy degradation compared to FP32 models, and up to 11.4% / 9.5% accuracy improvements and up to 2x speedup compared to state-of-the-art quantization methods on node-level and graph-level tasks, respectively.",
        "methodology": "The A2Q method quantizes node features using learnable per-node bitwidths (b_i) and step sizes (alpha_i), constrained by a memory size penalty. Weights (W) are quantized to a fixed 4-bit precision with learnable per-column step sizes. For training quantization parameters in semi-supervised tasks where task-related gradients for most nodes are zero, a Local Gradient method is introduced. This method uses the local quantization error (L1 norm difference between original and quantized features) as supervision. To generalize to unseen graphs with varying node counts in graph-level tasks, a Nearest Neighbor Strategy is developed. This strategy initializes 'm' groups of quantization parameters, calculates the maximum quantization value (q_max) for each group, and then assigns parameters to a node based on the nearest q_max to its largest absolute feature value. Backpropagation aggregates gradients from nodes using the same parameter group. Straight-through estimator is used for non-differentiable quantization functions.",
        "experimental_setup": "The method is evaluated on three GNN models: GCN, GIN, and GAT. Eight public datasets are used, comprising four node-level semi-supervised tasks (Cora, CiteSeer, PubMed, ogbn-arxiv) and four graph-level tasks (REDDIT-BINARY, MNIST, CIFAR10, ZINC). ZINC is a regression task, others are classification. For fair comparison, weight quantization bitwidth is fixed to 4 bits as in DQ-INT4. Performance is measured by accuracy (or loss for ZINC), average bitwidths, compression ratio, and speedup on a precision-scalable hardware accelerator. Node-level tasks are averaged over 100 runs (Cora, CiteSeer, PubMed) or 10 runs (ogbn-arxiv), and graph-level tasks use 10-fold cross-validation (REDDIT-BINARY) or 10 runs (MNIST, CIFAR10, ZINC). Ablation studies are performed to demonstrate the advantage of learning-based mixed-precision, the power of learning quantization parameters, and the effectiveness of Local Gradient and Nearest Neighbor Strategy. Comparisons are made with FP32 baselines, DQ-INT4, Half-precision, and LPGNAS.",
        "limitations": "The paper implies limitations of existing GNN quantization methods, stating that most fail to exploit unique GNN characteristics, leading to severe accuracy degradation or poor generalization (e.g., 1-bit methods restricted to node-level tasks or poor generalization). The necessity of a dedicated hardware accelerator is mentioned because current CPUs and GPUs do not support mixed-precision operations well. The effectiveness of the Nearest Neighbor Strategy is somewhat dependent on the number of initialized parameter groups ('m'), where a too-small 'm' can reduce accuracy. The learned bitwidths in GAT models show irregular correlation with in-degrees (Figure 4c), and for specific internal layers in GIN models trained on graph-level tasks, the learned bitwidth may lose correlation with graph topology after activation functions (Figure 4d, 4e).",
        "future_research_directions": "Not explicitly stated as a separate section, but the paper implies future work in its scope and demonstrated robustness. The generality and robustness demonstrated on various tasks (node-level, graph-level, inductive learning, heterogeneous graphs, link prediction) suggest the method can be broadly applied and potentially extended to even more complex graph learning scenarios or different types of GNN architectures. Further optimization of the hardware accelerator for specific GNN mixed-precision operations is also an implicit direction, given its current implementation."
      }
    },
    {
      "title": "Not All Bits have Equal Value: Heterogeneous Precisions via Trainable Noise"
    },
    {
      "title": "Are GATs Out of Balance?",
      "abstract": "While the expressive power and computational capabilities of graph neural\nnetworks (GNNs) have been theoretically studied, their optimization and\nlearning dynamics, in general, remain largely unexplored. Our study undertakes\nthe Graph Attention Network (GAT), a popular GNN architecture in which a node's\nneighborhood aggregation is weighted by parameterized attention coefficients.\nWe derive a conservation law of GAT gradient flow dynamics, which explains why\na high portion of parameters in GATs with standard initialization struggle to\nchange during training. This effect is amplified in deeper GATs, which perform\nsignificantly worse than their shallow counterparts. To alleviate this problem,\nwe devise an initialization scheme that balances the GAT network. Our approach\ni) allows more effective propagation of gradients and in turn enables\ntrainability of deeper networks, and ii) attains a considerable speedup in\ntraining and convergence time in comparison to the standard initialization. Our\nmain theorem serves as a stepping stone to studying the learning dynamics of\npositive homogeneous models with attention mechanisms.",
      "full_text": "Are GATs Out of Balance? Nimrah Mustafa∗ nimrah.mustafa@cispa.de Aleksandar Bojchevski† a.bojchevski@uni-koeln.de Rebekka Burkholz∗ burkholz@cispa.de ∗CISPA Helmholtz Center for Information Security, 66123 Saarbrücken, Germany †University of Cologne, 50923 Köln, Germany Abstract While the expressive power and computational capabilities of graph neural net- works (GNNs) have been theoretically studied, their optimization and learning dynamics, in general, remain largely unexplored. Our study undertakes the Graph Attention Network (GAT), a popular GNN architecture in which a node’s neighbor- hood aggregation is weighted by parameterized attention coefficients. We derive a conservation law of GAT gradient flow dynamics, which explains why a high portion of parameters in GATs with standard initialization struggle to change dur- ing training. This effect is amplified in deeper GATs, which perform significantly worse than their shallow counterparts. To alleviate this problem, we devise an initialization scheme that balances the GAT network. Our approach i) allows more effective propagation of gradients and in turn enables trainability of deeper networks, and ii) attains a considerable speedup in training and convergence time in comparison to the standard initialization. Our main theorem serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms. 1 Introduction A rapidly growing class of model architectures for graph representation learning are Graph Neural Networks (GNNs) [20] which have achieved strong empirical performance across various applications such as social network analysis [8], drug discovery [27], recommendation systems [61], and traffic forecasting [50]. This has driven research focused on constructing and assessing specific architectural designs [59] tailored to various tasks. On the theoretical front, mostly the expressive power [51, 26] and computational capabilities [45] of GNNs have been studied. However, there is limited understanding of the underlying learning mechanics. We undertake a classical variant of GNNs, the Graph Attention Network (GAT) [48, 9]. It overcomes the limitation of standard neighborhood representation averaging in GCNs [29, 22] by employing a self-attention mechanism [47] on nodes. Attention performs a weighted aggregation over a node’s neighbors to learn their relative importance. This increases the expressiveness of the neural network. GAT is a popular model that continues to serve as strong baseline. However, a major limitation of message-passing GNNs in general, and therefore GATs as well, is severe performance degradation when the depth of the network is even slightly increased. SOTA results are reportedly achieved by models of 2 or 3 layers. This problem has largely been attributed to over-smoothing [35], a phenomenon in which node representations become indistinguishable when multiple layers are stacked. To relieve GNNs of over-smoothing, practical techniques inspired by classical deep neural networks tweak the training process, e.g. by normalization [11, 64, 65, 66] or regularization [38, 42, 55, 67], and impose architectural changes such as skip connections (dense 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2310.07235v2  [cs.LG]  25 Oct 2023and residual) [34, 53] or offer other engineering solutions [ 33] or their combinations [ 12]. Other approaches to overcome over-smoothing include learning CNN-like spatial operators from random paths in the graph instead of the point-wise graph Laplacian operator [ 17] and learning adaptive receptive fields (different ‘effective’ neighborhoods for different nodes) [63, 36, 52]. Another problem associated with loss of performance in deeper networks is over-squashing [1], but we do not discuss it since non-attentive models are more susceptible to it, which is not our focus. Alternative causes for the challenged trainability of deeper GNNs beyond over-smoothing is hampered signal propagation during training (i.e. backpropagation) [25, 62, 13]. To alleviate this problem for GCNs, a dynamic addition of skip connections to vanilla-GCNs, which is guided by gradient flow, and a topology-aware isometric initialization are proposed in [ 25]. A combination of orthogonal initialization and regularization facilitates gradient flow in [21]. To the best of our knowledge, these are the only works that discuss the trainability issue of deep GNNs from the perspective of signal propagation, which studies a randomly initialized network at initialization. Here, we offer an insight into a mechanism that allows these approaches to improve the trainability of deeper networks that is related to the entire gradient flow dynamics. We focus in particular on GAT architectures and the specific challenges that are introduced by attention. Our work translates the concept of neuronwise balancedness from traditional deep neural networks to GNNs, where a deeper understanding of gradient dynamics has been developed and norm balanced- ness is a critical assumption that induces successful training conditions. One reason is that the degree of initial balancedness is conserved throughout the training dynamics as described by gradient flow (i.e. a model of gradient descent with infinitesimally small learning rate or step size). Concretely, for fully connected feed-forward networks and deep convolutional neural networks with continuous homogenous activation functions such as ReLUs, the difference between the squared l2-norms of incoming and outgoing parameters to a neuron stays approximately constant during training [ 15, 32]. Realistic optimization elements such as finite learning rates, batch stochasticity, momentum, and weight decay break the symmetries induced by these laws. Yet, gradient flow equations can be extended to take these practicalities into account [31]. Inspired by these advances, we derive a conservation law for GATs with positive homogeneous activation functions such as ReLUs. As GATs are a generalization of GCNs, most aspects of our insights can also be transferred to other architectures. Our consideration of the attention mechanism would also be novel in the context of classic feed-forward architectures and could have intriguing implications for transformers that are of independent interest. In this work, however, we focus on GATs and derive a relationship between model parameters and their gradients, which induces a conservation law under gradient flow. Based on this insight, we identify a reason for the lack of trainability in deeper GATs and GCNs, which can be alleviated with a balanced parameter initialization. Experiments on multiple benchmark datasets demonstrate that our proposal is effective in mitigating the highlighted trainability issues, as it leads to considerable training speed-ups and enables significant parameter changes across all layers. This establishes a causal relationship between trainability and parameter balancedness also empirically, as our theory predicts. The main theorem presented in this work serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms such as transformers [47] and in particular, vision transformers which require depth more than NLP transformers[49]. Our contributions are as follows. • We derive a conservation law of gradient flow dynamics for GATs, including its variations such as shared feature weights and multiple attention heads. • This law offers an explanation for the lack of trainability of deeper GATs with standard initialization. • To demonstrate empirically that our theory has established a causal link between balancedness and trainability, we propose a balanced initialization scheme that improves the trainability of deeper networks and attains considerable speedup in training and convergence time, as our theory predicts. 2 Theory: Conservation laws in GATs Preliminaries For a graph G = (V, E) with node set V and edge set E ⊆ V × V , where the neighborhood of a node v is given as N(v) = {u|(u, v) ∈ E}, a GNN layer computes each node’s representation by aggregating over its neighbors’ representation. In GATs, this aggregation is weighted by parameterized attention coefficients αuv, which indicate the importance of node u for v. 2Table 1: Attributes of init. schemes Init. Feat. Attn. Bal.? Xav Xavier Xavier No XavZ Xavier Zero No BalX Xavier Zero Yes BalO LLortho Zero Yes Figure 1: Params. to balance for neuron i ∈ [nl] Given input representations hl−1 v for all nodes v ∈ V , a GAT 1 layer l transforms those to: hl v = ϕ( X u∈N(v) αl uv · Wl shl−1 u ), where (1) αl uv = exp((al)⊤ · LeakyReLU(Wl shl−1 u + Wl t hl−1 v ))P u′∈N(v) exp((al)⊤ · LeakyReLU(Wlshl−1 u′ + Wl t hl−1v ))) . (2) We consider ϕ to be a positively homogeneous activation functions (i.e ϕ(x) =xϕ′(x) and conse- quently, ϕ(ax) =aϕ(x) for positive scalars a), such as a ReLU ϕ(x) = max{x, 0} or LeakyReLU ϕ(x) = max{x, 0} + −α max{−x, 0}. The feature transformation weights Ws and Wt for source and target nodes, respectively, may also be shared such that W = Ws = Wt. Definition 2.1. Given training data {(xm, ym)}M m=1 ⊂ Rd × Rp for M ≤ V , let f : Rd → Rp be the function represented by a network constructed by stacking L GAT layers as defined in Eq. (1) and (2) with W = Ws = Wt and h0 m = g(xm). Each layer l ∈ [L] of size nl has associated parameters: a feature weight matrix Wl ∈ Rnl×nl−1 and an attention vector al ∈ Rnl, where n0 = d and nL = p. Given a differentiable loss function ℓ : Rd × Rp → R, the loss L = 1/M PM i=1 ℓ(f(xm), ym) is used to update model parameters w ∈ {Wl, al}L l=1 with learning rate γ by gradient descent, i.e., wt+1 = wt − γ∇wL, where ∇wL = [∂L/∂w1, . . . ,∂L/∂w|w|] and w0 is set by the initialization scheme. For an infinitesimal γ → 0, the dynamics of gradient descent behave similarly to gradient flow defined by dw/dt = −∇wL, where t is the continuous time index. We useW[i, :], W[:, i], and a[i] to denote the ith row of W, column of W, and entry ofa, respectively. Note that Wl[i, :] is the vector of weights incoming to neuron i ∈ [nl] and Wl+1[:, i] is the vector of weights outgoing from the same neuron. For the purpose of discussion, let i ∈ [dl], j∈ [dl−1], and k ∈ [dl+1], as depicted in Fig. 1. ⟨·, ·⟩ denotes the scalar product. Conservation Laws We focus the following exposition on weight-sharing GATs, as this variant has the practical benefit that it requires fewer parameters. Yet, similar laws also hold for the non-weight- sharing case, as we state and discuss in the supplement. For brevity, we also defer the discussion of laws for the multi-headed attention mechanism to the supplement. Theorem 2.2 (Structure of gradients). Given the feature weight and attention parameters Wl and al of a layer l in a GAT network as described in Def. 2.1, the structure of the gradients for layer l ∈ [L − 1] in the network is conserved according to the following law: ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. (3) Corollary 2.3 (Norm conservation of weights incoming and outgoing at every neuron). Given the gradient structure of Theorem 2.2 and positive homogeneity of the activation ϕ in Eq. (1), gradient flow in the network adheres to the following invariance for l ∈ [L − 1]: d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 = d dt \u0010\r\rWl+1[: i] \r\r2\u0011 , (4) 1This definition of GAT is termed GATv2 in [9]. Hereafter, we refer to GATv2 as GAT. 3(a) δ with GD  (b) δ with Adam  (c) c with Xavier  (d) c with BalO Figure 2: Visualizing conservation laws: (a),(b) show δ ≈ 0 (can not empirically be exactly zero due to finite γ = 0.1) from Eq. (7) for hidden layers when L = 3. (c),(d) show the value of c from Eq. (5), which is determined by the initialization and should be 0 when the network is balanced, for L = 10 trained with GD. With Xav., the network is unbalanced and hence c ̸= 0for all neurons. With BalO, c = 0exactly at initialization for all neurons by design but during training, it varies slightly andc ≈ 0 as the network becomes slightly unbalanced due to the finite γ = 0.1. As both δ and c approximately meet their required value, we conclude that the derived law holds for practical purposes as well. It then directly follows by integration over time that the l2-norms of feature and attention weights incoming to and outgoing from a neuron are preserved such that \r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2 − \r\rWl+1[: i] \r\r2 = c, (5) where c = d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 at initialization (i.e. t = 0). We denote the ‘degree of balancedness’ by c and call an initialization balanced if c = 0. Corollary 2.4 (Norm conservation across layers). Given Eq. (4), the invariance of gradient flow at the layer level for l ∈ [L − 1] by summing over i ∈ [nl] is: d dt \u0010\r\rWl\r\r2 F − \r\ral\r\r2\u0011 = d dt \u0010\r\rWl+1\r\r2 F \u0011 . (6) Remark 2.5. Similar conservation laws also hold for the original less expressive GAT version [48] as well as for the vanilla GCN [29] yielded by fixing al = 0. Insights We first verify our theory and then discuss its implications in the context of four different initializations for GATs, which are summarized in Table 1. Xavier (Xav.) initialization [19] is the standard default for GATs. We describe the Looks-Linear-Orthogonal (LLortho) initialization later. In order to empirically validate Theorem 2.2, we rewrite Equation 3 to define δ as: δ = ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al, ∇alL⟩ − ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ = 0. (7) We observe how the value of δ varies during training in Fig. 2 for both GD and Adam optimizers with γ = 0.1. Although the theory assumes infinitesimal learning rates due to gradient flow, it still holds sufficiently (δ ≈ 0) in practice for normally used learning rates as we validate in Fig. 2. Furthermore, although the theory assumes vanilla gradient descent optimization, it still holds for the Adam optimizer (Fig. 2b). This is so because the derived law on the level of scalar products between gradients and parameters (see Theorem 2.2) still holds, as it states a relationship between gradients and parameters that is independent of the learning rate or precise gradient updates. Having verified Theorem 2.2, we use it to deduce an explanation for a major trainability issue of GATs with Xavier initialization that is amplified with increased network depth. The main reason is that the last layer has a comparatively small average weight norm, as E \r\rWL[:, i] \r\r2 = 2nL/(nL + nL−1) << 1, where the number of outputs is smaller than the layer width nL << nL−1. In contrast, E \r\rWL−1[i, :] \r\r2 = 2nL−1/(nL−1 + nL−2) = 1 and EaL−1[i]2 = 2/(1 +nL−1). In consequence, the right-hand side of our derived conservation law nL−2X j=1 (WL−1 ij )2 ∇WL−1 ij L WL−1 ij − (aL−1 i )2 ∇aL−1 i L aL−1 i = nLX k=1 (WL ki)2 ∇WL ki L WL ki (8) 4(a) Frac. of sig. params with relative change > 0.05  (b) Rel. grad. norms of feat. wghts. & training curve Figure 3: For L = 5, a high fraction of parameters change, and gradients propagate to earlier layers for both the unbalanced and balanced initialization Xav. and BalO initialization achieve 75.5% and 79.9% test accuracy, respectively. Note that the balanced initialization in (b) is able to train faster. (a) Frac. of sig. params with relative change > 0.5  (b) Rel. grad. norms of feat. wghts. & training curve Figure 4: For L = 10, unbalanced initialization is unable to train the model as relative gradients are extremely small, and the parameters (and loss) do not change. However, the balanced initialization is able to propagate gradients, change parameters, and thus attain training loss → 0. Xav. and Bal O achieve 39.3% and 80.2% test accuracy, respectively. Note that the balanced initialization in (b) is also able to train faster than the 5 layer network with Xavier initialization in Fig. 3b. is relatively small, as PnL k=1(WL ki)2 << 1, on average, while the weights on the left-hand side are orders of magnitude larger. This implies that the relative gradients on the left-hand side are likely relatively small to meet the equality unless all the signs of change are set in such a way that they satisfy the equality, which is highly unlikely during training. Therefore, relative gradients in layer L −1 and accordingly also the previous layers of the same dimension can only change in minor ways. The amplification of this trainability issue with depth can also be explained by the recursive substitu- tion of Theorem 2.2 in Eq. (8) that results in a telescoping series yielding: n1X j=1 n0X m=1 W(1) jm 2 ∇W(1) jm L W(1) jm − L−1X l=1 nlX o=1 a(l) o 2 ∇a(l) o L a(l) o = nL−1X i=1 nLX k=1 W(L) ki 2 ∇W(L) ki L W(L) ki (9) Generally, 2n1 < n0 and thus E \r\rW1[j :] \r\r2 = 2n1/(n1 + n0) < 1. Since the weights in the first layer and the gradients propagated to the first layer are both small, gradients of attention parameters of the intermediate hidden layers must also be very small in order to satisfy Eq. (9). Evidently, the problem aggravates with depth where the same value must now be distributed over the parameters and gradients of more layers. Note that the norms of the first and last layers are usually not arbitrary but rather determined by the input and output scale of the network. Analogously, we can see see how a balanced initialization would mitigate the problem. Equal weight norms \r\rWL−1[i, :] \r\r2 and \r\rWL[:, i] \r\r2 in Eq. (8) (as the attention parameters are set to 0 during balancing, see Procedure 2.6) would allow larger relative gradients in layerL − 1, as compared to the imbalanced case, that can enhance gradient flow in the network to earlier layers. In other words, gradients on both sides of the equation have equal room to drive parameter change. 5Figure 5: Relative gradient norms of feature (left axis, solid) and of attention (right axis, stylized) parameters for l ∈ [1, 5, 10] and L = 10, sampled every 25 epochs. Test accuracy is at the top. Both attention and feature gradients at the first, middle, and last layer of the network with both balanced initializations are much larger than with unbalanced initialization (note axis scales). To visualize the trainability issue, we study the relative change |(w∗−w0)/w∗|. of trained network parameters (w∗) w.r.t. their initial value. In order to observe meaningful relative change, we only con- sider parameters with a significant contribution to the model output (w∗ ≥ 10−4). We also plot the rel- ative gradient norms of the feature weights across all layers to visualize their propagation. We display these values forL = 5and L = 10in Fig. 3 and 4, respectively, and observe a stark contrast, similar to the trends in relative gradient norms of selected layers of a 10 layer GAT in Fig. 5. In all experiments, GAT was trained with the specified initialization on Cora using GD with γ = 0.1 for 5000 epochs. Interestingly, the attention parameters change most in the first layer and increasingly more towards the last layer, as seen in Fig. 3a and 4a. This is consistently observed in both the 5 and 10 layer networks if the initialization is balanced, but with unbalanced initialization, the 10 layer network is unable to produce the same effect. As our theory defines a coarse-level conservation law, such fine-grained training dynamics cannot be completely explained by it. Since the attention parameters are set to 0 in the balanced initializations (see Procedure 2.6), as ablation, we also observe the effect of initializing attention and feature parameters with zero and Xavier, respectively. From Eq.(2), al = 0leads to αuv = 1/|N(v)|, implying that all neighbors u of the node v are equally important at initialization. Intuitively, this allows the network to learn the importance of neighbors without any initially introduced bias and thus avoids the ‘chicken and egg’ problem that arises in the initialization of attention over nodes[30]. Although this helps generalization in some cases (see Fig. 6), it alone does not help the trainability issue as seen in Fig. 4a. It may in fact worsen it (see Fig. 5), which is explained by Eq. 8. These observations underline the further need for parameter balancing. Procedure 2.6 (Balancing). Based on Eq. (5) from the norm preservation law 2.3, we note that in order to achieve balancedness, (i.e. set c = 0in Eq.(5)), the randomly initialized parameters Wl and al must satisfy the following equality for l ∈ [L]: \r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2 = \r\rWl+1[: i] \r\r2 This can be achieved by scaling the randomly initialized weights as follows: 1. Set al = 0 for l ∈ [L]. 2. Set W1[i, :] = W1[i,:] ∥W1[i,:]∥ √βi, for i ∈ [n1] where βi is a hyperparameter 3. Set Wl+1[:, i] = Wl+1[:,i] ∥Wl+1[:,i]∥ \r\rWl[i, :] \r\r for i ∈ [nl] and l ∈ [L − 1] In step 2, βi determines the initial squared norm of the incoming weights to neuron i in the first layer. It can be any constant thus also randomly sampled from a normal or uniform distribution. We explain why we set βi = 2for i ∈ [n1] in the context of an orthogonal initialization. Remark 2.7. This procedure balances the network starting from l = 1towards l = L. Yet, it could also be carried out in reverse order by first defining \r\rWL[:, i] \r\r2 = βi for i ∈ [nL−1]. Balanced Orthogonal Initialization The feature weights Wl for l ∈ [L] need to be initialized randomly before balancing, which can simply be done by using Xavier initialization. However, existing works have pointed out benefits of orthogonal initialization to achieve initial dynamical 6(a) Test accuracy (higher is better).  (b) Epochs to best model (lower is better). Figure 6: GAT trained on Cora using SGD isometry for DNNs and CNNs [37, 44, 23], as it enables training very deep architectures. In line with these findings, we initialize Wl, before balancing, to be an orthogonal matrix with a looks-linear (LL) mirrored block structure [46, 6], that achieves perfect dynamical isometry in perceptrons with ReLU activation functions. Due to the peculiarity of neighborhood aggregation, the same technique does not induce perfect dynamical isometry in GATs (or GNNs). Exploring how dynamical isometry can be achieved or approximated in general GNNs could be an interesting direction for future work. Nevertheless, using a (balanced) LL-orthogonal initialization enhances trainaibility, particularly of deeper models. We defer further discussion on the effects of orthogonal initialization, such as with identity matrices, to the appendix. We outline the initialization procedure as follows: Let ∥ and |= denote row-wise and column- wise matrix concatenation, respectively. Let us draw a random orthonormal submatrix Ul and define W1 = [U1 ∥ −U1] where U1 ∈ R n1 2 ×n0 , Wl = [[Ul ∥ −Ul] |= [−Ul ∥ Ul]] where Ul ∈ R nl 2 × nl−1 2 for l = {2, . . . , L− 1}, and WL = [UL |= −UL] where UL ∈ RnL× nl−1 2 . Since UL is an orthonormal matrix, \r\rWL[:, i] \r\r2 = 2by definition, and by recursive application of Eq. (2.3 (with al = 0), balancedness requires that \r\rW1[i :, ] \r\r2 = 2. Therefore, we set βi = 2for i ∈ n1. 3 Experiments The main purpose of our experiments is to verify the validity of our theoretical insights and deduce an explanation for a major trainability issue of GATs that is amplified with increased network depth. Based on our theory, we understand the striking observation in Figure 4a that the parameters of default Xavier initialized GATs struggle to change during training. We demonstrate the validity of our theory with the nature of our solution. As we balance the parameter initialization (see Procedure 2.6), we allow the gradient signal to pass through the network layers, which enables parameter changes during training in all GAT layers. In consequence, balanced initialization schemes achieve higher generalization performance and significant training speed ups in comparison with the default Xavier initialization. It also facilitates training deeper GAT models. To analyze this effect systematically, we study the different GAT initializations listed in Table 1 on generalization (in % accuracy) and training speedup (in epochs) using nine common benchmark datasets for semi-supervised node classification tasks. We defer dataset details to the supplement. We use the standard provided train/validation/test splits and have removed the isolated nodes from Citeseer. We use the Pytorch Geometric framework and run our experiments on either Nvidia T4 Tensor Core GPU with 15 GB RAM or Nvidia GeForce RTX 3060 Laptop GPU with 6 GB RAM. We allow each network to train, both with SGD and Adam, for 5000 epochs (unless it converges earlier, i.e. achieves training loss ≤ 10−4) and select the model state with the highest validation accuracy. For each experiment, the mean ±95% confidence interval over five runs is reported for accuracy and epochs to the best model. All reported results use ReLU activation, weight sharing and no biases, unless stated otherwise. No weight sharing leads to similar trends, which we therefore omit. SGD We first evaluate the performance of the different initialization schemes of GAT models using vanilla gradient descent. Since no results have been previously reported using SGD on these datasets, to the best of our knowledge, we find that setting a learning rate of 0.1, 0.05 and 0.005 for L = [2, 5], L = [10, 20], and L = 40, respectively, allows for reasonably stable training on Cora, Citeseer, and Pubmed. For the remaining datasets, we set the learning rate to 0.05, 0.01, 0.005 and 0.0005 for 7Table 2: Mean accuracy(%) ± 95% CI over five runs of GAT with width= 64trained using SGD. Init. L = 2 L = 5 L = 10 L = 20 L = 40 Citeseer Xav 71.82 ± 2.73 58 .40 ± 2.25 24 .70 ± 8.90 19 .23 ± 1.54 18 .72 ± 1.15 BalX 71.62 ± 0.80 68 .83 ± 1.62 64 .13 ± 1.57 54 .88 ± 7.95 42 .63 ± 17.47 BalO 72.02 ± 0.63 70 .63 ± 0.60 68 .83 ± 1.68 68 .70 ± 1.18 63 .40 ± 1.43 Pubmed Xav 77.26 ± 1.39 70 .68 ± 2.16 67 .32 ± 10.70 36 .52 ± 11.50 27 .20 ± 13.99 BalX 78.02 ± 0.73 75.66 ± 1.81 77.60 ± 1.56 76.44 ± 1.70 75 .74 ± 2.94 BalO 77.68 ± 0.45 76.62 ± 1.59 77.04 ± 2.14 78.20 ± 0.61 77 .80 ± 1.41 Actor Xav 27.32 ± 0.59 24 .60 ± 0.93 24 .08 ± 0.80 22 .29 ± 3.26 19 .46 ± 5.75 BalX 26.00 ± 0.59 23 .93 ± 1.42 24.21 ± 0.78 24 .74 ± 1.14 23.88 ± 0.97 BalO 26.59 ± 1.03 24 .61 ± 0.77 24.17 ± 0.62 24 .24 ± 1.05 23.93 ± 1.53 Chameleon Xav 52.81 ± 1.37 54.21 ± 1.05 30 .31 ± 5.96 22 .19 ± 2.04 22 .28 ± 3.15 BalX 51.18 ± 1.94 54.21 ± 0.82 52 .11 ± 3.72 51.89 ± 1.89 38 .64 ± 10.31 BalO 50.00 ± 3.07 53 .95 ± 1.81 51 .84 ± 3.21 52.72 ± 0.13 44 .30 ± 1.61 Cornell Xav 42.70 ± 2.51 41.08 ± 2.51 42.70 ± 1.34 25.41 ± 14.64 22 .70 ± 13.69 BalX 41.08 ± 6.84 35 .14 ± 11.82 41 .08 ± 2.51 40 .00 ± 4.93 37.84 ± 5.62 BalO 42.16 ± 1.64 43.24 ± 2.12 36.76 ± 5.02 35.68 ± 3.29 36.22 ± 3.42 Squirrel Xav 35.20 ± 0.44 40 .96 ± 0.92 21 .65 ± 1.52 20 .23 ± 1.69 19 .67 ± 0.29 BalX 35.95 ± 1.69 40.98 ± 0.87 38.98 ± 1.49 38.35 ± 1.07 25 .38 ± 4.62 BalO 35.83 ± 0.92 42.52 ± 1.19 38.85 ± 1.36 39.15 ± 0.44 26 .57 ± 1.83 Texas Xav 60.00 ± 1.34 60 .54 ± 3.42 58 .92 ± 1.34 49 .73 ± 20.97 17 .84 ± 26.98 BalX 60.54 ± 1.64 61 .62 ± 2.51 61 .62 ± 2.51 58 .92 ± 2.51 56.22 ± 1.34 BalO 60.00 ± 1.34 57 .30 ± 1.34 56 .76 ± 0.00 58 .38 ± 4.55 57.30 ± 3.91 Wisconsin Xav 51.37 ± 6.04 51.37 ± 8.90 51 .76 ± 3.64 43 .14 ± 25.07 31 .76 ± 31.50 BalX 49.80 ± 8.79 54.51 ± 4.19 47.84 ± 7.16 50.59 ± 10.49 41.18 ± 1.54 BalO 49.80 ± 4.24 55 .69 ± 3.64 51.76 ± 5.68 49.02 ± 4.36 48.24 ± 4.52 L = [2, 5], L= 10, L = 20, and L = 40, respectively. Note that we do not perform fine-tuning of the learning rate or other hyperparameters with respect to performance and use the same settings for all initializations to allow fair comparison. Figure 6 highlights that models initialized with balanced parameters, Bal O, consistently achieve the best accuracy and significantly speed up training, even on shallow models of 2 layers. In line with our theory, the default Xavier (Xav) initialization hampers model training (see also Figure 4a). Interestingly, the default Xavier (Xav) initialized deeper models tend to improve in performance with width but cannot compete with balanced initialization schemes. For example, the accuracy achieved by Xav for L = 10increases from 24.7% to 68.7% when the width is increased from 64 to 512. We hypothesize that the reason why width aids generalization performance is that a higher number of (almost random) features supports better models. This would also be in line with studies of overparameterization in vanilla feed-forward architectures, where higher width also aids random feature models [2, 4, 57, 43]. The observation that width overparameterized models enable training deeper models may be of independent interest in the context of how overparameterization in GNNs may be helpful. However, training wider and deeper (hence overall larger) models is computationally inefficient, even for datasets with the magnitude of Cora. In contrast, the BalO initialized model for L = 10is already able to attain 79.7% even with width= 64and improves to 80.9% with width= 512. Primarily the 8(a) Test accuracy.  (b) Epochs to convergence. Figure 7: GAT with 64 hidden dimensions trained using Adam. parameter balancing must be responsible for the improved performance, as it is not sufficient to simply initialize the attention weights to zero, as shown in Figure 4a and 5. For the remaining datasets, we report only the performance of networks with 64 hidden dimension in Table 2 for brevity. Since we have considered the XavZ initialization only as an ablation study and find it to be ineffective (see Figure 6a), we do not discuss it any further. Note that for some datasets (e.g.Pubmed, Wisconsin), deeper models (e.g. L = 40) are unable to train at all with Xavier initialization, which explains the high variation in test accuracy across multiple runs as, in each run, the model essentially remains a random network. The reduced performance even with balanced initialization at higher depth may be due to a lack of convergence within5000 epochs. Nevertheless, the fact that the drop in performance is not drastic, but rather gradual as depth increases, indicates that the network is being trained, i.e. trainability is not an issue. As a stress test, we validate that GAT networks with balanced initialization maintain their trainability at even larger depths of 64 and 80 layers (see Fig. 9 in appendix). In fact, the improved performance and training speed-up of models with balanced initialization as opposed to models with standard initialization is upheld even more so for very deep networks. Apart from Cora, Citeseer and Pubmed, the other datasets are considered heterophilic in nature and standard GNN models do not perform very well on them. State-of-the-art performance on these datasets is achieved by specialized models. We do not compare with them for two reasons: i) they do not employ an attention mechanism, which is the focus of our investigations, but comprise various special architectural elements and ii) our aim is not to outperform the SOTA, but rather highlight a mechanism that underlies the learning dynamics and can be exploited to improve trainability. We also demonstrate the effectiveness of a balanced initialization in training deep GATs in comparison to LipschitzNorm [14], a normalization technique proposed specifically for training deep GNNs with self-attention layers such as GAT (see Table 6 in appendix). Adam Adam, the most commonly used optimizer for GNNs, stabilizes the training dynamics and can compensate partially for problematic initializations. However, the drop in performance with depth, though smaller, is inevitable with unbalanced initialization. As evident from Figure 7, BalO initialized models achieve higher accuracy than or at par with Xavier initialization and converge in fewer epochs. As we observe with SGD, this difference becomes more prominent with depth, despite the fact that Adam itself significantly improves the trainability of deeper networks over SGD. Our argument of small relative gradients (see Eq.(8)) also applies to Adam. We have used the initial learning rates reported in the literature [9]: 0.005 for Cora and Citeseer, and 0.01 for Pubmed for the 2 and 5 layer networks. To allow stable training of deeper networks, we reduce the initial learning rate by a factor 0.1 for the 10 and 20 layer networks on all three datasets. Architectural variations We also consider other architectural variations such as employing ELUs instead of ReLUs, using multiple attention heads, turning off weight sharing, and addition of standard elements such as weight decay and dropout. In all cases, the results follow a similar tendency as already reported. Due to a lack of space, we defer the exact results to the appendix. These variations further increase the performance of networks with our initialization proposals, which can therefore be regarded as complementary. Note that residual skip connections between layers are also supported 9in a balanced initialization provided their parameters are initialized with zero. However, to isolate the contribution of the initialization scheme and validate our theoretical insights, we have focused our exposition on the vanilla GAT version. Limitations The derived conservation law only applies to the self-attention defined in the original GAT and GATv2 models, and their architectural variations such as ωGAT[18] (see Fig. 10 in appendix). Note that the law also holds for the non-attentive GCNs (see Table 7 in appendix) which are a special case of GATs (where the attention parameters are simply zero). Modeling different kinds of self-attention such as the dot-product self-attention in [28] entails modification of the conservation law, which has been left for future work. 4 Discussion GATs [9] are powerful graph neural network models that form a cornerstone of learning from graph- based data. The dynamic attention mechanism provides them with high functional expressiveness, as they can flexibly assign different importance to neighbors based on their features. However, as we slightly increase the network depth, the attention and feature weights face difficulty changing during training, which prevents us from learning deeper and more complex models. We have derived an explanation of this issue in the form of a structural relationship between the gradients and parameters that are associated with a feature. This relationship implies a conservation law that preserves a sum of the respective squared weight and attention norms during gradient flow. Accordingly, if weight and attention norms are highly unbalanced as is the case in standard GAT initialization schemes, relative gradients for larger parameters do not have sufficient room to increase. This phenomenon is similar in nature to the neural tangent kernel (NTK) regime [ 24, 54], where only the last linear layer of a classic feed-forward neural network architecture can adapt to a task. Conservation laws for basic feed-forward architectures [15, 3, 32] do not require an infinite width assumption like NTK-related theory and highlight more nuanced issues for trainability. Furthermore, they are intrinsically linked to implicit regularization effects during gradient descent [41]. Our results are of independent interest also in this context, as we incorporate the attention mechanism into the analysis, which has implications for sequence models and transformers as well. One of these implications is that an unbalanced initialization hampers the trainability of the involved parameters. Yet, the identification of the cause already contains an outline for its solution. Balancing the initial norms of feature and attention weights leads to more effective parameter changes and significantly faster convergence during training, even in shallow architectures. To further increase the trainability of feature weights, we endow them with a balanced orthogonal looks-linear structure, which induces perfect dynamical isometry in perceptrons [10] and thus enables signal to pass even through very deep architectures. Experiments on multiple benchmark datasets have verified the validity of our theoretical insights and isolated the effect of different modifications of the initial parameters on the trainability of GATs. 5 Acknowledgements We gratefully acknowledge funding from the European Research Council (ERC) under the Horizon Europe Framework Programme (HORIZON) for proposal number 101116395 SPARSE-ML. 10References [1] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. In International Conference on Learning Representations, 2021. [2] Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient descent for deep linear neural networks. In International Conference on Learning Representations, 2019. [3] Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach, 2018. [4] Jimmy Ba, Murat A. Erdogdu, Taiji Suzuki, Denny Wu, and Tianzong Zhang. Generalization of two-layer neural networks: An asymptotic viewpoint. In International Conference on Learning Representations, 2020. [5] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016. [6] David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams. The shattered gradients problem: If resnets are the answer, then what is the question? In International Conference on Machine Learning, 2018. [7] Peter L. Bartlett, David P. Helmbold, and Philip M. Long. Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks. In International Conference on Machine Learning, 2018. [8] T. Bian, X. Xiao, T. Xu, P. Zhao, W. Huang, Y . Rong, and J. Huang. Rumor detection on social media with bi-directional graph convolutional networks. In AAAI Conference on Artificial Intelligence, 2020. [9] Shaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? In International Conference on Learning Representations, 2022. [10] Rebekka Burkholz and Alina Dubatovka. Initialization of ReLUs for dynamical isometry. In Advances in Neural Information Processing Systems, volume 32, 2019. [11] Tianle Cai, Shengjie Luo, Keyulu Xu, Di He, Tie-Yan Liu, and Liwei Wang. Graphnorm: A principled approach to accelerating graph neural network training. In International Conference on Machine Learning, 2021. [12] Tianlong Chen, Kaixiong Zhou, Keyu Duan, Wenqing Zheng, Peihao Wang, Xia Hu, and Zhangyang Wang. Bag of tricks for training deeper graph neural networks: A comprehensive benchmark study. In IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [13] Weilin Cong, Morteza Ramezani, and Mehrdad Mahdavi. On provable benefits of depth in training graph convolutional networks. In Advances in Neural Information Processing Systems, 2021. [14] George Dasoulas, Kevin Scaman, and Aladin Virmaux. Lipschitz normalization for self-attention layers with application to graph neural networks. In International Conference on Machine Learning, 2021. [15] S. S. Du, W. Hu, and J. D. Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. In Advances in Neural Information Processing Systems, 2018. [16] Moshe Eliasof, Eldad Haber, and Eran Treister. Pde-gcn: Novel architectures for graph neural networks motivated by partial differential equations. In Advances in Neural Information Processing Systems, 2021. [17] Moshe Eliasof, Eldad Haber, and Eran Treister. pathgcn: Learning general graph spatial operators from paths. In International Conference on Machine Learning, 2022. [18] Moshe Eliasof, Lars Ruthotto, and Eran Treister. Improving graph neural networks with learnable propagation operators. In International Conference on Machine Learning, 2023. 11[19] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In International Conference on Artificial Intelligence and Statistics, volume 9, pages 249–256, May 2010. [20] M. Gori, G. Monfardini, and F. Scarselli. A new model for learnig in graph domains. In IEEE International Joint Conference on Neural Networks, 2005. [21] Kai Guo, Kaixiong Zhou, Xia Hu, Yu Li, Yi Chang, and Xin Wang. Orthogonal graph neural networks. In AAAI Conference on Artificial Intelligence, 2022. [22] William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, 2018. [23] Wei Hu, Lechao Xiao, and Jeffrey Pennington. Provable benefit of orthogonal initialization in optimizing deep linear networks. In International Conference on Machine Learning, 2020. [24] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems , 2018. [25] Ajay Jaiswal, Peihao Wang, Tianlong Chen, Justin F. Rousseau, Ying Ding, and Zhangyang Wang. Old can be gold: Better gradient flow can make vanilla-gcns great again. In Advances in Neural Information Processing Systems, 2022. [26] Stefanie Jegelka. Theory of graph neural networks: Representation and learning. In The International Congress of Mathematicians, 2022. [27] S. Kearnes, K. McCloskey, M. Berndl, V . Pande, and P. Riley. Molecular graph convolutions: Moving beyond fingerprints. In Journal of Computer-Aided Molecular Design, 2016. [28] Dongkwan Kim and Alice Oh. How to find your friendly neighborhood: Graph attention design with self-supervision. In International Conference on Learning Representations, 2021. [29] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017. [30] Boris Knyazev, Graham W. Taylor, and Mohamed R. Amer. Understanding attention and generalization in graph neural networks. In Advances in Neural Information Processing Systems, 2019. [31] D. Kunin, J. Sagastuy-Brena, S. Ganguli, D.L.K. Yamins, and H. Tanaka. Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics. InInternational Conference on Learning Representations, 2021. [32] Thien Le and Stefanie Jegelka. Training invariances and the low-rank phenomenon: beyond linear networks. In International Conference on Learning Representations, 2022. [33] Guohao Li, Matthias Müller, Bernard Ghanem, and Vladlen Koltun. Training graph neural networks with 1000 layers. In International Conference on Machine Learning, 2022. [34] Guohao Li, Matthias Müller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as cnns? In International Conference on Computer Vision, 2019. [35] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In AAAI Conference on Artificial Intelligence, 2018. [36] Meng Liu, Hongyang Gao, and Shuiwang Ji. Towards deeper graph neural networks. In SIGKDD International Conference on Knowledge Discovery and Data Mining, 2020. [37] Dmytro Mishkin and Jiri Matas. All you need is a good init. In International Conference on Learning Representations, 2016. [38] Pál András Papp, Karolis Martinkus, Lukas Faber, and Roger Wattenhofer. Dropgnn: Ran- dom dropouts increase the expressiveness of graph neural networks. In Advances in Neural Information Processing Systems, 2021. 12[39] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geo- metric graph convolutional networks. In International Conference on Learning Representations, 2020. [40] Oleg Platonov, Denis Kuznedelev, Michael Diskin, Artem Babenko, and Liudmila Prokhorenkova. A critical look at the evaluation of gnns under heterophily: are we really making progress? In International Conference on Learning Representations, 2023. [41] Noam Razin and Nadav Cohen. Implicit regularization in deep learning may not be explainable by norms. In Advances in Neural Information Processing Systems, 2020. [42] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional networks on node classification. In International Conference on Learning Representations, 2020. [43] Karthik Abinav Sankararaman, Soham De, Zheng Xu, W. Ronny Huang, and Tom Goldstein. The impact of neural network overparameterization on gradient confusion and stochastic gradient descent. In International Conference on Machine Learning, 2020. [44] Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In International Conference on Learning Representations, 2014. [45] F. Scarselli, M. Gori, A.C Tsoi, M. Hagenbuchner, and G. Monfardini. Computational capabili- ties of graph neural networks. In IEEE Transactions on Neural Networks, 2009. [46] Wenling Shang, Kihyuk Sohn, Diogo Almeida, and Honglak Lee. Understanding and im- proving convolutional neural networks via concatenated rectified linear units. In International Conference on Machine Learning, 2016. [47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa- tion Processing Systems, 2017. [48] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018. [49] Peihao Wang, Wenqing Zheng, Tianlong Chen, and Zhangyang Wang. Anti-oversmoothing in deep vision transformers via the fourier domain analysis: From theory to practice. In International Conference on Learning Representations, 2022. [50] Z. Wu, S. Pan, G. Long, J. Jiang, and C. Zhang. Graph wavenet for deep spatial-temporal graph modeling. In International Joint Conference on Artificial Intelligence, 2019. [51] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2019. [52] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In International Conference on Machine Learning, 2018. [53] Keyulu Xu, Mozhi Zhang, Stefanie Jegelka, and Kenji Kawaguchi. Optimization of graph neural networks: Implicit acceleration by skip connections and more depth. In International Conference on Machine Learning, 2021. [54] Greg Yang and Edward J. Hu. Tensor programs iv: Feature learning in infinite-width neural networks. In Marina Meila and Tong Zhang, editors, International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 11727–11737, 2021. [55] Han Yang, Kaili Ma, and James Cheng. Rethinking graph regularization for graph neural networks. In Advances in Neural Information Processing Systems, 2021. 13[56] Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. In International Conference on Machine Learning, 2016. [57] Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for under- standing neural networks. In Advances in Neural Information Processing Systems, 2019. [58] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform bad for graph representation? In Advances in Neural Information Processing Systems, 2021. [59] Jiaxuan You, Rex Ying, and Jure Leskovec. Design space for graph neural networks. In Advances in Neural Information Processing Systems, 2021. [60] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J. Kim. Graph transformer networks. In Advances in Neural Information Processing Systems, 2020. [61] M. Zhang and Y . Chen. Inductive matrix completion based on graph neural networks. In International Conference on Learning Representations, 2020. [62] Wentao Zhang, Zeang Sheng, Ziqi Yin, Yuezihan Jiang, Yikuan Xia, Jun Gao, Zhi Yang, and Bin Cui. Model degradation hinders deep graph neural networks. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022. [63] Wentao Zhang, Mingyu Yang, Zeang Sheng, Yang Li, Wen Ouyang, Yangyu Tao, Zhi Yang, and Bin Cui. Node dependent local smoothing for scalable graph learning. In Advances in Neural Information Processing Systems, 2021. [64] Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. In International Conference on Learning Representations, 2020. [65] Kaixiong Zhou, Xiao Huang, Yuening Li, Daochen Zha, Rui Chen, and Xia Hu. Towards deeper graph neural networks with differentiable group normalization. In Advances in Neural Information Processing Systems, 2020. [66] Kuangqi Zhou, Yanfei Dong, Kaixin Wang, Wee Sun Lee, Bryan Hooi, Huan Xu, and Jiashi Feng. Understanding and resolving performance degradation in graph convolutional networks. In Conference on Information and Knowledge Management, 2021. [67] Difan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, and Quanquan Gu. Layer- dependent importance sampling for training deep and large graph convolutional networks. In Advances in Neural Information Processing Systems, 2019. 14Appendices 6 Proofs of Theorems Notation. Scalar and element-wise products are denoted ⟨, ⟩ and ⊙, respectively. Boldface lowercase and uppercase symbols represent vectors and matrices, respectively. Our proof of Theorem 2.2 utilizes a rescale invariance that follows from Noether’s theorem, as stated by [31]. Note that we could also derive the gradient structure directly from the derivatives, but the rescale invariance is easier to follow. Definition 6.1 (Rescale invariance). The loss L(θ) is rescale invariant with respect to disjoint subsets of the parameters θ1 and θ2 if for every λ >0 we have L(θ) =L((λθ1, λ−1θ2, θd)), where θ = (θ1, θ2, θd). We frequently utilize the following relationship. Lemma 6.2 (Gradient structure due to rescale invariance [31]). The rescale invariance of L enforces the following geometric constraint on the gradients of the loss with respect to its parameters: ⟨θ1, ∇θ1 L⟩ − ⟨θ2, ∇θ2 L⟩ = 0. (10) We first use this rescale invariance to prove our main theorem for a GAT with shared feature transformation weights and a single attention head, as described in Def. 2.1. The underlying principle generalizes to other GAT versions as well, as we exemplify with two other variants. Firstly, we study the case of unshared weights for feature transformation of source and target nodes. Secondly, we discuss multiple attention heads. Proof of Theorem 2.2 Our main proof strategy relies on identifying a multiplicative rescale invariance in GATs that allows us to apply Lemma 6.2. We identify rescale invariances for every neuron i at layer l that induce the stated gradient structure. Specifically, we define the components of in-coming weights to the neuron as θ1 = {w | w ∈ Wl[i, : ]} and the union of all out-going edges (regarding features and attention) as θ2 = {w | w ∈ Wl+1[: , i]} ∪ {al[i]}. It is left to show that these parameters are invariant under rescaling. Let us, therefore, evaluate the GAT loss at λθ1 and λ−1θ2 and show that it remains invariant for any choice of λ >0. Note that the only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and αl uv. We denote the scaled network components with a tilde resulting in ˜hl u[i], ˜hl+1 v [k], and ˜αl uv As we show, parameters of upper layers remain unaffected, as ˜hl+1 v [k] coincides with its original non-scaled variant ˜hl+1 v [k] =hl+1 v [k]. Let us start with the attention coefficients. Note that ˜al[i] =λ−1al[i] and ˜Wl[i, j] =λWl[i, j]. This implies that ˜αl uv = exp(el uv)P u′∈N(v) exp(eluv) = αl uv , because (11) ˜el uv = (al)⊤ · ϕ2(Wl(hl−1 u + hl−1 v )) =el uv, (12) which follows from the positive homogeneity of ϕ2 that allows ˜el uv = λ−1al[i]ϕ2( nl−1X j λWl[i, j](hl−1 u [j] +hl−1 v [j]) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) (13) = λ−1λal[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl[i, j](hl−1 u [j] +hl−1 v [j]) (14) = el uv. (15) 15Since ˜αl uv = αl uv, it follows that ˜hu l [i] =ϕ1   X z∈N(u) αl zu nl−1X j λWl[i, j]hl−1 z [j]   = λϕ1   X z∈N(u) αl zu nl−1X j Wl[i, j]hl−1 z [j]   = λhl u[i] In the next layer, we therefore have ˜hl+1 v [k] =ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]˜hl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1[k, i]λhl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i Wl+1[k, i]hl u[i]   = hl+1 v [k] Thus, the output node representations of the network remain unchanged, and according to Def. 6.1, the loss L is rescale-invariant. Consequently, as per Lemma 6.2, the constraint in Eq.(10) can be written as: ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ − ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩ = 0. which can be rearranged to Eq.((2.2): ⟨Wl[i, :], ∇Wl[i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. thus proving Theorem 2.2. Proof of Corollary 2.3 Given that gradient flow applied on loss L is captured by the differential equation dw dt = −∇wL (16) which implies: d dt \r\rWl[i, :] \r\r2 = 2⟨Wl[i, :], d dtWl[i, :]⟩ = −2⟨Wl[i, :], ∇Wl[i,:]L⟩ (17) substituting in Eq.(4) similarly for gradient flow of Wl+1[:, i] and al[i], as done in Eq.(17) yields Theorem 2.2: d dt \u0010\r\rWl[i, :] \r\r2 − \r\ral[i] \r\r2\u0011 = d dt \u0010\r\rWl+1[: i] \r\r2\u0011 . −2⟨Wl[i, :], ∇Wl[i,:]L⟩ −(−2)⟨al[i], ∇al[i]L⟩ = −2⟨Wl+1[:, i], ∇Wl+1[:,i]L⟩. Therefore, Eq.(4) and consequently Eq.(5) in Corollary 2.3 hold. Summing over i ∈ [nl] on both sides of Eq.(4) yields Corollary 2.4. 16Theorem 6.3 (Structure of gradients for GAT without weight-sharing). Let a GAT network as defined by Def. 2.1 consisting of L layers be given. The feature transformation parameters Wl s and Wl t of the source and target nodes of an edge and attention parametersal of a GAT layer l are defined according to Eq.(1) and (2). Then the gradients for layer l ∈ [L − 1] in the network are governed by the following law: ⟨Wl s[i, :], ∇Wls[i,:]L⟩ + ⟨Wl t [i, :], ∇Wl t [i,:]L⟩ − ⟨al[i], ∇al[i]L⟩ = ⟨Wl+1 s [:, i], ∇Wl+1 s [:,i]L⟩ (18) Proof. The proof is analogous to the derivation of Theorem 2.2. We follow the same principles and define the disjoint subsets θ1 and θ2 of the parameter set θ, associated with a neuron i in layer l accordingly, as follows: θ1 = {w|w ∈ Wl s[i, :]} ∪ {w|w ∈ Wl t [i, :]} θ2 = {w|w ∈ Wl+1 s [:, i]} ∪ {al[i]} Then, the invariance of node representations follows similarly to the proof of Theorem 2.2. The only components of the network that potentially change under rescaling are hl u[i], hl+1 v [j], and αl uv. We denote the scaled network components with a tilde resulting in ˜hl u[i], ˜hl+1 v [k], and ˜αl uv As we show, parameters of upper layers remain unaffected, as ˜hl+1 v [k] coincides with its original non-scaled variant ˜hl+1 v [k] =hl+1 v [k]. Let us start with the attention coefficients. Note that ˜al[i] =λ−1al[i], ˜Ws l [i, j] = λWl s[i, j] and ˜Wl t [i, j] =λWl t [i, j]. This implies that ˜αl uv = exp(el uv)P u′∈N(v) exp(eluv) = αl uv , because (19) ˜el uv = (al)⊤ · ϕ2(Wl shl−1 u + Wl t hl−1 v ) =el uv, (20) which follows from the positive homogeneity of ϕ2 that allows ˜el uv = λ−1al[i]ϕ2( nl−1X j λWl s[i, j]hl−1 u [j] +λWl t [i, j]hl−1 v [j] (21) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j]) (22) = λ−1λal[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j] (23) + nlX i′̸=i al[i]ϕ2( nl−1X j Wl s[i, j]hl−1 u [j] +Wl t [i, j]hl−1 v [j]) (24) = el uv. (25) Since ˜αl uv = αl uv, it follows that ˜hu l [i] =ϕ1   X z∈N(u) αl zu nl−1X j λWl s[i, j]hl−1 z [j]   = λϕ1   X z∈N(u) αl zu nl−1X j Wl s[i, j]hl−1 z [j]   = λhl u[i] 17In the next layer, we therefore have ˜hv l+1 [k] =ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1 s [k, i]˜hl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i λ−1Wl+1 s [k, i]λhl u[i]   = ϕ1   X u∈N(v) αl+1 uv nlX i Wl+1 s [k, i]hl u[i]   = hl+1 v [k] Thus, the application of Lemma 6.2 derives Theorem 6.3. Theorem 6.4 (Structure of gradients for GAT with multi-headed attention) . Given the feature transformation parameters Wl k and attention parameters al k of an attention head k ∈ [K] in a GAT layer of a L layer network. Then the gradients of layer l ∈ [L − 1] respect the law: KX k ⟨Wl k[i, :], ∇Wl k[i,:]L⟩ − KX k ⟨al k[i], ∇al k[i]L⟩ = KX k ⟨Wl+1 k [:, i], ∇Wl+1 k [:,i]L⟩. (26) Proof. Each attention head is independently defined by Eq.1 and Eq.2, and thus Theorem 2.2 holds for each head, separately. The aggregation of multiple heads in a layer is done over node representations of each head in either one of two ways: Concatenation: hl v =∥K k ϕ(P u∈N(v) αkuv l · Wl khl−1 u ) , or Average: hl v = 1 K PK k ϕ(P u∈N(v) αkuv l · Wl khl−1 u ) Thus, the rescale invariance and consequent structure of gradients of the parameters in each head are not altered and Eq.(6.2) holds by summation of the conservation law over all heads. For the most general case of a GAT layer without weight-sharing and with multi-headed attention, each term in Theorem 6.3 is summed over all heads. Further implications analogous to Corollary 2.3, and 2.4 can be derived for these variants by similar principles. 7 Training Dynamics In the main paper, we have shared observations regarding the learning dynamics of GAT networks of varying depths with standard initialization and or balanced initialization schemes. Here, we report additional figures for more varied depths to complete the picture. All figures in this section correspond to a L layer GAT network trained on Cora. For SGD, the learning rate is set to γ = 0.1, γ = 0.05 and γ = 0.01 for L = 5, L = 10and L = 20, respectively. For Adam, γ = 0.005 and γ = 0.0001 for L = 5and L = 10, respectively. Relative change of a feature transformation parameter w is defined as |(w∗−w0)/w∗| where w0 is the initialized value and w∗ is the value for the model with maximum validation accuracy during training. Absolute change |a∗ − a0| is used for attention parameters a since attention parameters are initialized with 0 in the balanced initialization. We view the fraction of parameters that remain unchanged during training separately in Figure 8, and examine the layer-wise distribution of change in parameters in Figure 11 without considering the unchanged parameters. To analyze how gradients vary in the network during training, we define the relative gradient norm of feature transformation and attention parameters for a layer l as ∥∇WlWl∥F/∥Wl∥F and ∥∇alal∥/∥al∥, respectively. Figures 3b and 4b, and Figure 12 depict relative gradient norms for training under SGD and Adam respectively. 18(a) L = 5  (b) L = 10  (c) L = 20 Figure 8: Layer-wise fraction of feature transformation parameters Wl and attention parameters al with zero relative and absolute change, respectively, trained with SGD. For L = 2, no parameters with zero change existed. A small number of feature weights do not change in a 5 layer unbalanced network initialized with Xavier, but this fraction becomes significantly large when depth is increased to L = 10. Note that W1 contains a much larger number of parameters compared to the intermediate layers (specifically in this case, though it is generally common). At L = 20, nearly all w ∈ Wl with an unbalanced initialization (Xav. and Xav+ZeroAtt) struggle to change during training, whereas the balanced Xavier and LL-orthogonal initialization are able to drive change in most w ∈ Wl and all a ∈ al parameters, allowing the network to train. 8 Additional Results The results of the main paper have focused on the vanilla GAT having ReLU activation between consecutive layers, a single attention head, and shared weights for feature transformation of the source and target nodes, optimized with vanilla gradient descent (or Adam) without any regularization. Here, we present additional results for training with architectural variations, comparing a balanced initialization with a normalization scheme focused on GATs, and discussing the impact of an orthogonal initialization and the applicability of the derived conservation law to other message- passing GNNs (MPGNNs). Training variations To understand the impact of different architectural variations, common regular- ization strategies such as dropout and weight decay, and different activation functions, we conducted additional experiments. For a 10-layer network with width 64, Table 3 and 4 report our results for SGD and Adam, respectively. The values of hyperparameters were taken from [48]. Table 3: Mean test accuracy(%) ± 95% CI over five runs of GAT trained on Cora using SGD Variation L Xav Bal X BalO None (Vanilla GAT) 5 73 .00 ± 3.02 76 .96 ± 2.21 79.48 ± 0.43 10 25 .48 ± 18.13 77 .72 ± 1.49 79.46 ± 1.34 attention heads = 8 5 73 .56 ± 2.71 77 .44 ± 1.54 79.58 ± 0.53 10 25 .50 ± 18.18 77 .02 ± 2.76 79.06 ± 0.73 activation = ELU 5 75 .68 ± 1.80 79 .20 ± 1.07 79.64 ± 0.36 10 73 .02 ± 2.27 78.64 ± 1.72 47.76 ± 7.39 dropout = 0.6 5 42 .14 ± 15.97 79 .18 ± 1.17 81.00 ± 0.62 10 24 .90 ± 9.50 30 .94 ± 1.04 44.40 ± 1.84 weight decay = 0.0005 5 67 .26 ± 6.30 77 .36 ± 1.74 79.56 ± 0.48 10 18 .78 ± 11.96 76 .56 ± 2.91 79.40 ± 1.15 weight sharing = False 5 70 .80 ± 7.00 77 .28 ± 1.45 79.82 ± 0.63 10 19 .54 ± 14.03 76 .04 ± 1.77 79.06 ± 0.32 We find that the balanced initialization performs similar in most variations, outperforming the unbalanced standard initialization by a substantial margin. Dropout generally does not seem to be helpful to the deeper network ( L = 10), regardless of the initialization. From Table 3 and 4, it is evident that although techniques like dropout and weight decay may aid optimization, they are 19Table 4: Mean test accuracy(%) ± 95% CI over five runs of GAT trained on Cora using Adam Variation L Xav Bal X BalO None (Vanilla GAT) 5 76 .18 ± 1.61 79 .38 ± 2.24 80.20 ± 0.57 10 70 .86 ± 3.99 75 .72 ± 2.35 79.62 ± 1.27 attention heads = 8 5 75 .62 ± 1.74 78 .54 ± 1.06 79.56 ± 0.85 10 70 .94 ± 2.76 75 .48 ± 2.48 79.74 ± 1.10 activation = ELU 5 76 .56 ± 1.72 78 .72 ± 1.02 79.56 ± 0.64 10 75 .30 ± 1.42 78.48 ± 1.79 76.52 ± 0.97 dropout = 0.6 5 76 .74 ± 1.44 78 .42 ± 1.28 79.92 ± 0.48 10 32 .48 ± 6.99 31 .76 ± 1.33 76.34 ± 0.95 weight decay = 0.0005 5 75 .10 ± 2.05 78 .52 ± 1.41 79.80 ± 0.63 10 28 .74 ± 12.04 74 .68 ± 3.06 79.70 ± 1.14 weight sharing = False 5 76 .56 ± 1.72 78 .72 ± 1.02 79.56 ± 0.64 10 71 .12 ± 2.23 73 .32 ± 1.23 79.46 ± 1.16 alone not enough to enable the trainability of deeper network GATs and thus are complementary to balancedness. Note that ELU is not a positively homogeneous activation function, which is an assumption in out theory. In practice, however, it does not impact the Xavier-balanced initialization (Bal X). However, the Looks-Linear orthogonal structure is specific to ReLUs. Therefore, the orthogonality and balancedness of the Bal O initialization are negatively impacted by ELU, although the Adam optimizer seems to compensate for it to some extent. In addition to increasing the trainability of deeper networks, the balanced initialization matches the state-of-the-art performance of Xavier initialized 2 layer GAT, given the architecture and optimization hyperparameters as reported in [48]. We used an existing GAT implementation and training script2 of Cora that follows the original GAT(v1)[48] paper and added our balanced initialization to the code. As evident from Table 5, the balanced initialization matches SOTA performance of GAT(v1) on Cora (83-84%) (on a version of the dataset with duplicate edges). GAT(v2) achieved 78.5% accuracy on Pubmed but the corresponding hyperparameter values were not reported. Hence, we transferred them from [48]. This way, we are able to match the state-of-the-art performance of GAT(v2) on Pubmed, as shown in Table 5. Table 5: GAT performance on Cora and Pubmed: mean test accuracy(%) ± 95% CI over five runs. . Xav Bal X BalO Cora 84.50 ± 0.52 84.58 ± 0.65 84.55 ± 0.47 Pubmed 78.38 ± 0.77 78 .52 ± 0.54 78.56 ± 0.20 Comparison with Lipshitz Normalization A feasible comparison can be carried out with [14] that proposes a Lipschitz normalization technique aimed at improving the performance of deeper GATs in particular. We use their provided code to reproduce their experiments on Cora, Citeseer, and Pubmed for 2,5,10,20 and 40-layer GATs with Lipschitz normalization and compare them with LLortho+Bal initialization in Table 6. Note that Lipschitz normalization has been shown to outperform other previous normalization techniques for GNNs such as pair-norm [64] and layer-norm [5]. Impact of orthogonal initialization In the main paper, we have advocated and presented results for using a balanced LL-orthogonal initialization. Here, we discuss two special cases of orthogonal initialization (and their balanced versions): identity matrices that have also been used for GNNs in 2https://github.com/Diego999/pyGAT 20Table 6: Comparing a balanced LL-orthogonal initialization to Lipschitz normalization applied with a standard (imbalanced) Xavier initialization: the balanced initialization results in a much higher accuracy as the depth of the network increases. Dataset Cora Citeseer Pubmed Layers Lip. Norm. Bal O Init. Lip. Norm. Bal O Init. Lip. Norm. Bal O Init. 2 82.1 79.5 65 .4 67.7 74.8 76.0 5 77 .1 80.2 63.0 67.7 73.7 75.7 10 78 .0 79.6 43.6 67.4 52.8 76.9 20 72 .2 77.3 18.2 66.3 23.3 77.3 40 12 .9 75.9 18.1 63.2 36.6 77.5 [18, 16], and matrices with looks-linear structure using an identity submatrix (LLidentity) since a looks-linear structure would be more effective for ReLUs [10]. In line with [18], identity and LLidentity are used to initialize the hidden layers while Xavier is used to initialize the first and last layers. A network initialized with identity is balanced by adjusting the weights of the first and last layer to have norm1 (as identity matrices have row-wise and column-wise norm). A network with LLidentity initialization is balanced to have norm 2 in the first and last layer, similar to LLortho initialization. We compare the performance using these four base initializations (one standard Xavier, and three orthogonal cases) and their balanced counterparts in Fig. 9. We observe that balancing an (LL-)orthogonal initialization results in an improvement in the general- ization ability of the network in most cases and speeds up training, particularly for deeper networks. However, note that an (LL-)orthogonal initialization itself also has a positive effect on trainability in particular of deep networks. Contributing to this fact is the mostly-balanced nature of an (LL- )orthogonal initialization i.e. given hidden layers of equal dimensions the network is balanced in all layers except the first and last layers (assuming zero attention parameters), which allows the model to train, as opposed to the more severely imbalanced standard Xavier initialization. This further enforces the key takeaway of our work that norm imbalance at initialization hampers the trainability of GATs. In addition, the LLortho+Bal initialization also speeds up training over the LLortho initialization even in cases in which the generalization performance of the model is at par for both initializations. Figure 9: GAT network with varying initialization trained using SGD. Note that identity initializations have also been explored in the context of standard feed-forward neural networks. While they tend to work in practice, the lack of induced feature diversity can be problematic from a theoretical point of view (see e.g. for a counter-example [7]). However, potentially due to the reasons regarding feature diversity and ReLU activation discussed above, the balanced looks-linear random orthogonal initialization (LLortho+Bal.) outperforms initialization with identity matrices (see Fig. 9). In most cases, the balanced versions outperform the imbalanced version of the base initialization and the performance gap becomes exceedingly large as network depth increases. 21Applicability to other MPGNNs As GAT is a generalization of the GCN, all theorems are also applicable to GCNs (where the attention parameters are simply0). We provide additional experiments in Table 7, comparing the performance of GCN models of varying depths with imbalanced and balanced initializations trained on Cora using the SGD optimizer. We use the same hyperparameters as reported for GAT. As expected, the trainability benefits of a balanced initialization are also evident in deeper GCN networks. Table 7: GCN with varying initialization trained using SGD. Dataset Depth Xavier Xavier+Bal LLortho LLortho+Bal Cora 2 77 .8 ± 0.9 80 .5 ± 0.5 78 .0 ± 0.3 80.9 ± 0.4 5 73 .2 ± 3.4 78 .3 ± 0.8 80.3 ± 0.8 79.6 ± 1.0 10 24 .1 ± 4.5 77 .6 ± 2.0 80 .0 ± 1.1 80.0 ± 0.9 20 14 .4 ± 11.2 62 .8 ± 3.6 78 .7 ± 0.5 78.8 ± 1.5 40 13 .4 ± 0.9 65 .9 ± 7.3 28 .3 ± 16.2 77.1 ± 0.9 64 9 .8 ± 5.4 33 .0 ± 13.4 27 .3 ± 12.7 76.7 ± 1.3 80 12 .4 ± 19.3 33 .8 ± 12.9 38 .9 ± 21.3 77.1 ± 1.3 Citeseer 2 66 .6 ± 20.0 71 .3 ± 1.8 66 .0 ± 3.2 72.3 ± 0.9 5 60 .9 ± 12.3 66 .9 ± 15.0 69 .0 ± 6.4 70.1 ± 1.8 10 23 .8 ± 36.8 66 .0 ± 5.9 70.6 ± 0.9 69.8 ± 10.9 20 16 .4 ± 18.2 47 .9 ± 10.0 67 .0 ± 8.6 69.7 ± 4.5 40 13 .9 ± 56.8 37 .3 ± 92.8 44 .8 ± 6.8 64.7 ± 13.6 64 13 .8 ± 41.4 29 .5 ± 15.0 37 .3 ± 79.6 66.3 ± 0.5 80 12 .4 ± 42.7 25 .8 ± 3.6 30 .1 ± 21.8 64.1 ± 3.2 The underlying principle of a balanced network initialization holds in general. However, adapting the balanced initialization to different methods entails modification of the conservation law derived for GATs to correspond to the specific architecture of the other method. For example, the proposed balanced initialization can be applied directly to a more recent variant of GAT, ωGAT [18], for which the same conservation law holds and also benefits from a balanced initialization. We conduct additional experiments to verify this. The results in Fig. 10 follow a similar pattern as for GATs: a balanced orthogonal initialization with looks linear structure (LLortho+Bal) of ωGAT performs the best, particularly by a wide margin at much higher depths (64 and 80 layers). Figure 10: ωGAT network with varying trained on Cora. In this work, we focus our exposition on GATs and take the first step in modeling the training dynamics of attention-based models for graph learning. An intriguing direction for future work is to derive modifications in the conservation law for other attention-based models utilizing the dot-product self-attention mechanism such as SuperGAT [28] and other architectures based on Transformers[47], 22that are widely used in Large Language Models (LLMs) and have recently also been adapted for graph learning [58, 60]. 9 Datasets Summary We used nine benchmark datasets for the transductive node classification task in our experiments, as summarized in Table 8. For the Platenoid datasets (Cora, Citeseer, and Pubmed) [56], we use the graphs provided by Pytorch Geometric (PyG), in which each link is saved as an undirected edge in both directions. However, the number of edges is not exactly double (for example, 5429 edges are reported in the raw Cora dataset, but the PyG dataset has 10556 < (2 × 5429)) edges as duplicate edges have been removed in preprocessing. We also remove isolated nodes in the Citeseer dataset. The WebKB (Cornell, Texas, and Wisconsin), Wikipedia (Squirrel and Chameleon) and Actor datasets [39], are used from the replication package provided by [40], where duplicate edges are removed. All results in this paper are according to the dataset statistics reported in Table 8. Table 8: Summary of datasets used in experiments. Dataset Cora Cite. Pubmed Cham. Squi. Actor Corn. Texas Wisc. # Nodes 2708 3327 19717 2277 5201 7600 183 183 251 # Edges 10556 9104 88648 31371 198353 26659 277 279 450 # Features 1433 3703 500 2325 2089 932 1703 1703 1703 # Classes 7 6 3 5 5 5 5 5 5 # Train 140 120 60 1092 2496 3648 87 87 120 # Validation 500 500 500 729 1664 2432 59 59 80 # Test 1000 1000 1000 456 1041 1520 37 37 51 23(a) L = 10: The balanced initialization allows larger changes in a higher number of parameters in Wl across all layers l with the highest margin in l = 1. The change distribution for the parameters in al is missing for l = 5and l ∈ [7, 10] because these parameters remain unchanged (see Fig.8). (b) L = 20: The same pattern as in L = 10is seen. However, the difference between the trainability of models with unbalanced and balanced initialization becomes even more prominent. No attention parameters change at all with standard Xavier initialization (and hence their change distribution is not visible in the plot). Figure 11: Distributions of the relative and absolute change in the values of feature transformation parameters Wl and attention parameters al, respectively, when trained using SGD with unbalanced (Xav. and Xav+ZeroAtt) and balanced (Xav+Bal and LLortho+Bal) initialization. The black markers in each standard box-whisker plot denote the mean. In general, larger changes occur in attention parameters at later layers closer to the output of the network, whereas feature parameters change more in the earlier layers at the input of the network. We observe this also from the perspective of relative gradients, as higher gradients cause higher parameter changes. 24(a) L = 5: test accuracy (left to right) is 71.6%, 76.2%, 77.3%, and 80.5%. (b) L = 10: test accuracy (left to right) is 63.2%, 68.8%, 71.8%, and 78.0%. Figure 12: Layer-wise relative gradient norms of GAT parameters trained with Adam, which in itself improves trainability over SGD, especially for L = 10(see Fig.4b) even with standard initialization. However, with a balanced initialization, the training is faster and the model also generalizes better, as evident from the significant margin in test accuracy. A general noticeable trend, particularly with balanced initialization, is that the attention parameters have higher gradients at the end of the network (and thus change more in the later layers (see Fig. 11)) whereas the feature parameters have high gradients at the start of the network. Also, it can be seen that the feature parameters begin to change a few epochs before the attention parameters change noticeably, while the attention parameters continue to change for a few more epochs after the gradients of feature parameters have dropped. We speculate a representational benefit could drive this behaviour, i.e. with depth, the learned representation of neighbors becomes increasingly informative for a node and thus leads to a higher activity of the attention mechanism. 25",
      "meta_data": {
        "arxiv_id": "2310.07235v2",
        "authors": [
          "Nimrah Mustafa",
          "Aleksandar Bojchevski",
          "Rebekka Burkholz"
        ],
        "published_date": "2023-10-11T06:53:05Z",
        "pdf_url": "https://arxiv.org/pdf/2310.07235v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The research identifies that Graph Attention Networks (GATs) with standard initialization suffer from poor trainability and performance degradation when depth increases, mainly due to a high portion of parameters struggling to change during training. It derives a conservation law of GAT gradient flow dynamics, explaining this issue, particularly amplified in deeper networks. To address this, the study proposes a balanced initialization scheme that enables more effective gradient propagation, improves the trainability of deeper GATs, and achieves considerable speedups in training and convergence time.",
        "methodology": "The core methodology involves deriving a conservation law for GAT gradient flow dynamics, applicable to variations like shared feature weights and multiple attention heads, assuming positive homogeneous activation functions (e.g., ReLU). This law quantifies the relationship between parameter gradients and norms, revealing that unbalanced initial norms hinder gradient propagation. Based on this insight, a balanced initialization scheme is proposed (Procedure 2.6): setting attention parameters to zero and carefully scaling feature weights layer-by-layer to ensure a specific norm equality (Wl[i, :]^2 - al[i]^2 = Wl+1[: i]^2). This scheme can be combined with a Balanced Orthogonal Initialization using a looks-linear mirrored block structure to further enhance trainability and promote dynamical isometry.",
        "experimental_setup": "Experiments were conducted on nine benchmark datasets for semi-supervised node classification: Cora, Citeseer, Pubmed, Actor, Chameleon, Cornell, Squirrel, Texas, and Wisconsin, using standard train/validation/test splits (isolated nodes removed from Citeseer). The GAT models were implemented using the Pytorch Geometric framework and trained on Nvidia T4 or RTX 3060 GPUs. Optimization was performed using SGD and Adam for up to 5000 epochs, with the best model selected based on validation accuracy. Evaluation metrics included mean accuracy (%) and epochs to the best model, with 95% confidence intervals over five runs. The study compared standard Xavier initialization against various balanced initializations (Balanced Xavier, Balanced Orthogonal) and also included ablation studies (Xavier with Zero attention) and comparisons with Lipschitz Normalization, evaluating performance under architectural variations like ELU activation, multiple attention heads, no weight sharing, dropout, and weight decay.",
        "limitations": "The derived conservation law specifically applies to the self-attention mechanisms in the original GAT and GATv2 models, as well as architectural variations like ωGAT. While it extends to non-attentive GCNs (as a special case where attention parameters are zero), it requires modifications for other types of self-attention, such as dot-product self-attention found in models like SuperGAT, which is left for future work. Additionally, ELU activation, not being positively homogeneous, negatively impacts the orthogonality and balancedness of the Looks-Linear orthogonal structure, though Adam optimization partially compensates.",
        "future_research_directions": "Future research could focus on deriving modifications to the conservation law for other attention-based models, particularly those utilizing dot-product self-attention mechanisms like SuperGAT and Transformer architectures used in Large Language Models (LLMs) and graph learning. Another promising direction is exploring how dynamical isometry can be achieved or approximated in general Graph Neural Networks (GNNs), building upon the current work's findings regarding balanced orthogonal initialization."
      }
    },
    {
      "title": "Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing",
      "abstract": "Transformer models have been widely adopted in various domains over the last\nyears, and especially large language models have advanced the field of AI\nsignificantly. Due to their size, the capability of these networks has\nincreased tremendously, but this has come at the cost of a significant increase\nin necessary compute. Quantization is one of the most effective ways to reduce\nthe computational time and memory consumption of neural networks. Many studies\nhave shown, however, that modern transformer models tend to learn strong\noutliers in their activations, making them difficult to quantize. To retain\nacceptable performance, the existence of these outliers requires activations to\nbe in higher bitwidth or the use of different numeric formats, extra\nfine-tuning, or other workarounds. We show that strong outliers are related to\nvery specific behavior of attention heads that try to learn a \"no-op\" or just a\npartial update of the residual. To achieve the exact zeros needed in the\nattention matrix for a no-update, the input to the softmax is pushed to be\nlarger and larger during training, causing outliers in other parts of the\nnetwork. Based on these observations, we propose two simple (independent)\nmodifications to the attention mechanism - clipped softmax and gated attention.\nWe empirically show that models pre-trained using our methods learn\nsignificantly smaller outliers while maintaining and sometimes even improving\nthe floating-point task performance. This enables us to quantize transformers\nto full INT8 quantization of the activations without any additional effort. We\ndemonstrate the effectiveness of our methods on both language models (BERT,\nOPT) and vision transformers.",
      "full_text": "Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing Yelysei Bondarenko, Markus Nagel, Tijmen Blankevoort Qualcomm AI Research∗ Amsterdam, The Netherlands {ybond, markusn, tijmen}@qti.qualcomm.com Abstract Transformer models have been widely adopted in various domains over the last years, and especially large language models have advanced the field of AI signif- icantly. Due to their size, the capability of these networks has increased tremen- dously, but this has come at the cost of a significant increase in necessary com- pute. Quantization is one of the most effective ways to reduce the computational time and memory consumption of neural networks. Many studies have shown, however, that modern transformer models tend to learn strong outliers in their activations, making them difficult to quantize. To retain acceptable performance, the existence of these outliers requires activations to be in higher bitwidth or the use of different numeric formats, extra fine-tuning, or other workarounds. We show that strong outliers are related to very specific behavior of attention heads that try to learn a “no-op” or just a partial update of the residual. To achieve the exact zeros needed in the attention matrix for a no-update, the input to the softmax is pushed to be larger and larger during training, causing outliers in other parts of the network. Based on these observations, we propose two sim- ple (independent) modifications to the attention mechanism - clipped softmax and gated attention . We empirically show that models pre-trained using our methods learn significantly smaller outliers while maintaining and sometimes even improving the floating-point task performance. This enables us to quantize transformers to full INT8 quantization of the activations without any additional effort. We demonstrate the effectiveness of our methods on both language models (BERT, OPT) and vision transformers. Our source code is available at https: //github.com/qualcomm-ai-research/outlier-free-transformers . 1 Introduction Quantization has been one of the most impactful ways to reduce the computational complexity of transformer networks. Previous work has shown that quantizing networks to 4-bit weights is possible without losing too much accuracy [66, 69]. Some research even shows 4-bit weights might be optimal when trading off model size and bit-width [12]. However, quantizing transformers is not always trivial. When quantizing the activations of a trans- former, significant problems arise with outliers in specific layers. This has been noted by several researchers that suggest fixes to transformers after training to ameliorate their effect [13, 67]. These methods are frequently tedious and either require retraining the network, require implementing specific hardware for input-channel quantization [13] or require parts of the activations to still be in higher bit-widths, reducing the effectiveness of the activation quantization [67]. In this paper, we set out to solve the transformer outlier problem entirely by changing the architecture of the network itself. We hope to make transformers easy to quantize from the get-go without needing ∗Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc. 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2306.12929v2  [cs.LG]  9 Nov 2023any post-processing. To do so, we thoroughly analyze why these outliers appear. Previous work has found the existence of these outliers [4, 13], but in our work, we come to a fuller understanding of these outlying values. We find that the outliers occur because attention heads are trying not to update the hidden state, and in the process, strong outliers appear due to the softmax function. This happens for language and vision transformers and different specific transformer architectures. This understanding is the foundation for two new tweaks we suggest to transformer architectures that can remove the problem of the outliers entirely. 2 Background and related work In this section, we briefly cover the basics of neural network quantization and discuss why modern transformers are difficult to quantize. Quantization One of the most powerful ways to decrease the computational time and memory consumption of neural networks is quantization, which uses low-bit representations for the weights and activation tensors. On top of that, using low-bit fixed-point representations, such as INT8, one can further reduce energy consumption since the fixed-point operations are more efficient than their floating-point counterparts [23, 59]. We simulate the quantization process in floating-point according to Jacob et al. [26]. We use the following definition of the quantization function: bx := q (x; s, z, b) =s · \u0010 clip \u0010jx s m + z; 0, 2b − 1 \u0011 − z \u0011 , (1) where x denotes the quantizer input (i.e., network weights or activations), s ∈ R+ the scale factor or the step-size, z ∈ Z the zero point, and b ∈ N the bitwidth. ⌊·⌉ denotes the round-to-nearest-integer operator. This quantization scheme is called uniform affine or asymmetric quantization [24, 32, 76] and it is one of the most commonly used quantization schemes because it allows for efficient implementation of fixed-point arithmetic. In the case of symmetric quantization, we restrict the quantization grid to be symmetric around z = 0. In this work, we focus on post-training quantization (PTQ) methods, which take a pre-trained FP32 network and convert it directly into a fixed-point network without the need for the original training pipeline [2, 5, 7, 25, 32, 35, 41, 43, 44, 75]. These methods require either no data or only a small calibration dataset and are easier to use compared to quantization-aware training (QAT, Bhalgat et al. 3, Esser et al. 16, Gupta et al. 21, Jacob et al. 26, Krishnamoorthi 32) methods that have you train the entire network for more epochs. For more details on neural network quantization, we refer the reader to [19, 46]. Outliers in Transformers Multiple studies have shown that modern transformer-based language models tend to learn outliers in weights and activations [4, 13, 31]. These outliers are present only in a small fixed set of embedding dimensions, but they appear regularly and consistently across multiple layers and data sequences. It was also shown that those outliers play a crucial role in the model predictions and clipping them or by setting to zero the corresponding parameters significantly degrades the model task performance [31, 49]. The strongest in magnitude outliers typically appear at the output of the feed-forward network, FFN, although Dettmers et al.[13] showed that for big enough transformer-based language models they start appearing after every linear layer, including query, key, and value projection layers. This phenomenon holds for many tasks, training objectives and models (both encoder and decoder transformers), including BERT [14], RoBERTa [37], DistilBERT [53], MobileBERT [55], ELECTRA [9], BART [33], XLNet [68], GPT-2 [50], and OPT [74]. Because of these strong outliers, applying per-tensor PTQ for the FFN’s output and the residual sum will likely cause a notable error because of the following trade-off between the range and the precision. On the one hand, using a large quantization range for small-ranged values leads to a loss in representation (high rounding error). On the other hand, a small quantization range for large values leads to a very high clipping error. For the case of significant transformer outliers, frequently, no good trade-off can be found between the rounding and clipping error, resulting in an overall high error. There have been numerous attempts to fix the issue of transformer quantization [4, 12, 13, 17, 27, 28, 51, 54, 62, 63, 69, 71]. Most of these approaches resort to finer quantization granularity (row-wise, 2(a) FFN output in layer #10  (b) FFN output in layer #11 Figure 1: Histograms of outlier counts vs. token positions (blue) and hidden dimensions (green), recorded from the MNLI-m validation set on BERT-base. We use zero-based indexing for dimensions. channel-wise, group-wise weight and activation quantization), use higher bitwidth and/or different numeric format to represent those outliers better or require extra fine-tuning (in the form of QAT and/or knowledge distillation). In other words, they adapt quantization to work with outliers, which often comes at the expense of general applicability or extra inference overhead. In contrast, in this work, we want to address the root cause of the problem and understand why outliers are learned in the first place and suggest a new pre-training protocol that significantly reduces the magnitude of outliers yielding way more quantization-friendly models that can be effortlessly quantized using PTQ without strong degradation of performance. 3 Outlier analysis Outliers in BERT models In Section 2 we discussed that outliers are present only in a few designated embedding dimensions but they appear regularly and consistently across multiple layers and data sequences. We also discussed that the strongest magnitude outliers in BERT typically appear at the output of FFN in the last encoder layers. We start by taking the pre-trained BERT-base-uncased checkpoint from HuggingFace [65] and fine- tune it on MNLI dataset from the well-known GLUE benchmark [ 61] (see experimental details in C.1). To identify the outlier dimensions, we pass the MNLI-m validation set through the network and record all outliers1 at the FFN output in layers #10 and #112. As we can see in Figure 1, there are indeed only a few hidden dimensions where outliers ever occur. We also notice that the majority of outliers (> 97%) correlate with the position of delimiter tokens – [SEP], “.”, and “,”. To better understand the role of those outliers, we analyze the attention patterns of the corresponding attention heads. BERT-base uses multi-head attention with nheads = 12and each head operating on a consecutive subset of dhead = 64features. Therefore, the hidden dimension #180, which happens to have the highest outlier count in both layers #10 and #11, corresponds to attention head #3. In Figure 2 (and more examples in Appendix A.1) we show examples of the attention matrices, values and their product for that head. A common pattern we found is that the attention head assigns almost all of its probability mass to [SEP] tokens, and other less informative tokens like dots/commas, while these tokens also have small values in V associated with those tokens. This results in a small magnitude product between the two (see Figure 2a). This effectively corresponds to a (soft) no-update of the hidden representation, where only small noise is added after the residual. In other cases (Figure 2b and 2c), we observe that a significant portion of attention probability is still spent on delimiter tokens. However, by allocating some of the probability mass on other tokens (together with the small values for the delimiter tokens), this results in a (soft) selective update of the hidden representation. These patterns in self-attention seem to be a learned “workaround” for the limitations of having the softmax and the residual connections in cases where the attention head does not want to update the representation of some or all of the tokens. These observations are in line with Clark et al. [8], Kovaleva et al. [30] that also argued that attending exclusively or almost exclusively to delimiter tokens such as [SEP], periods/commas acts as a “no-op” when the attention head’s function is not applicable. 1We follow Bondarenko et al. [4] and consider outliers as values that exceed 6 standard deviations from the mean of the corresponding activation tensor. 2We use 1-based indexing for encoder layers and attention heads throughout the paper. 3(a) Attention layer #11, data sequence #1 (b) Attention layer #11, data sequence #5 (c) Attention layer #10, data sequence #5 Figure 2: Visualization of the patterns in the self-attention, specifically the attention probabilities, values, and their product (left, middle and right columns, respectively), in attention head #3 for BERT-base, computed on several data sequences from MNLI-m validation set. (a)  (b)  (c)  (d)  (e) Figure 3: A summary of our outlier analysis for ViT demonstrated on a random image from ImageNet validation set. (a) An input image. (b) Outliers in the output of layer #11. (c) Cumulative attention weight spent on every patch (matrix of attention probabilities summed over rows) in the attention head #1, layer #12. (d) A corresponding matrix of attention probabilities. (e) An average magnitude of values for outlier and non-outlier patches. Outliers in ViT We conduct a similar analysis for Vision transformer [15] trained on ImageNet [52]. For this study, we use a pre-trained checkpoint following our experimental setup from Section 5. We highlight our findings in Figure 3 and provide more examples in Appendix A.2. Our analysis shows many similarities to the BERT case. Instead of delimiter tokens, the majority of outliers seem to correlate with some random uninformative patches (e.g., in the background). We also see that the corresponding attention head in the next layer allocates the majority of attention probabilities to the same patches. Finally, those outlier patches on average have a distinctly smaller magnitude of values compared to non-outlier ones, leading to similar no-update behavior. The fact that those values are not as close to zero as it was in the BERT case might be related to the smaller model capacity3, or a relatively shorter training procedure. Hypothesis Based on these observations, we pose the following hypothesis on how this behavior of attention heads is related to outliers: 1. In order for an attention block to not update a representation of a token on the residual, some attention heads want to allocate most of their attention probability mass to some fixed and common set of tokens that have a low information content (e.g., delimiter tokens or background patches) that can be learned to have a small value function output. 3We use ViT/S-16 configuration that has only 22M parameters. 42. From the definition of the softmax function4, it is easy to see that this would require an input of the softmax to have a relatively big dynamic range (Figure 4, 1 ). In fact, in the limit case where softmax is exactly zero, this would require an infinite dynamic range: softmax (x)i = 0 ⇔ ∃ j ̸= i, xj − xi = +∞ (2) 3. Since Layer Normalization ([1], 2 ) normalizes the outliers, the magnitude of the FFN output in the previous layer ( 3 ) has to be very high to still produce a sufficiently big dynamic range after the LayerNorm. Note, that this is also applicable for the transformer models with LayerNorm applied prior to the self-attention or linear transformations instead, a variant adopted by GPT, OPT, and many vision transformers [15, 38, 57, 58]. 4. Finally, as softmax will never output exact zeros, it will always back-propagate a gradient signal to grow bigger outliers5. The outliers will thus tend to become stronger in magnitude, the longer the network is trained. 4 Method Figure 4: A schematic illus- tration of the attention layer in BERT. Hidden activation tensor is denoted by x. ⊕ is an element-wise addition. A problematic output of the FFN that generates largest in magni- tude outliers is highlighted in red. Notice how those outliers in the previous layer influence the behavior in the attention mechanism in the next layer. In this section, we introduce our proposed modifications for the softmax attention mechanism. Based on our insights from Section 3, the core idea of these modifications is to grant the model the ability to produce very small the magnitude (or even exact zeros) output of attention function, without producing outliers. Recall that the self-attention [60] is defined as follows: Attention(x) := softmax \u0012Q(x)K(x)T √dhead \u0013 V (x) (3) where Q, K and V are learnable linear projections of the input x. Most modern transformer models employ a multi-headed variant of self-attention, where dmodel features are partitioned into nheads groups of dhead features, and the final output is the concatenation of the outputs of (3) applied to each group. 4.1 Clipped softmax First, we propose to replace softmax function in (3) with the follow- ing clipped softmax: clipped_softmax(x; ζ, γ) := clip ((ζ − γ) · softmax(x) +γ, 0, 1) . (4) Here x is the input and ζ ≥ 1, γ ≤ 0 are the stretch factors which are hyper-parameters of the method. Similar formulation was used before for sigmoid function [40, 45]. We can view (4) as stretching the output of the softmax from(0, 1) to (γ, ζ) and then clipping back to (0, 1) so that we can represent exact zeros if γ <0 and exact ones if ζ > 1. Specifically, the values of the softmax larger than 1−γ ζ−γ are rounded to one whereas values smaller than −γ ζ−γ are rounded to zero. With this drop-in replacement, we can achieve exact zeros (and ones) with a finite range for the softmax input. In addition to that, whenever values are clipped they will not give a gradient, preventing the outliers to grow further. 4softmax (x)i = exp (xi) / Pd j=1 exp (xj) 5Let y = softmax (x). Then ∂yi ∂xj ̸= 0∀i, j. 54.2 Gated attention An alternative way of architecting the model to have a small attention output without outliers is to equip it with an explicit conditional gating mechanism, as shown in Figure 5. The idea is that the model can use the gating to either keep or nullify the update to the representation of certain tokens and not rely on the attention probabilities and values to achieve the same outcome. Specifically, we propose the following modification to the attention function: Gated_attention(x) := sigmoid (G(x)) ⊙ softmax \u0012Q(x)K(x)T √dhead \u0013 V (x). (5) Here G is the gating function,⊙ is an element-wise multiplication across the token axis and everything else remains the same as in (3). The gating function G is parameterized by a small neural network that is learned jointly with the rest of the model. We replace the attention formulation with the proposed variant in every layer on the transformer network. Figure 5: A schematic il- lustration of our proposed gated attention. Gating module design Recall that the input to the attention layer x has shape (T, dmodel) that is reshaped into (nheads, T, dhead) for the multi-headed self-attention, where T is the sequence length. We chose to define the gating function on a per-head basis. For each head i ∈ {1, . . . , nheads}, we specify Gi : Rdhead → R and the output of the gating module is πi ∈ RT that is computed as follows: bπi,t = Gi(xi,t,:) ∀t ∈ {1, . . . , T} (6) πi,: = sigmoid(bπi,:), (7) note that gating modules are shared between different token positions but not shared across attention heads. We want our gating module to be as lightweight as possible. To start with, we experiment with Gi’s parameterized by a single linear layer. This gives us a gating module that is computationally inexpensive and has a memory overhead of just nheads · (dhead + 1)∼ dmodel extra parameters (which is equivalent to 1 extra token) per attention layer6. We also investigate the effect of using several other gating functions in Appendix B.1. 5 Experiments In this section, we evaluate the proposed modifications to self-attention on several language models (BERT, OPT) and the vision transformers (ViT). We first test the different hyperparameters for the methods and provide insight into how they work. Then we set out to test our method in terms of accuracy, and the difference in quantization improvement after training. All detailed hyperparameters of our experiments are in Appendix C. BERT We experiment with BERT-base-uncased (109M parameters) pre-training using the masked language modeling (MLM) objective. Following [ 14], we use the concatenation of the training sets of BookCorpus [77] and English Wikipedia7. We implement our methods in PyTorch [48] and use training and evaluation pipelines from HuggingFace libraries [ 20, 34, 65]. We follow closely the pre-training procedure from [ 14]. To speed up training and experimentation, we train with a maximum sequence length of 128 for the whole duration of the training. We evaluate on Wikipedia validation set and report the MLM perplexity. OPT We experiment with a 125M sized variant of OPT [74] pre-training using the causal language modeling (CLM) objective. Due to compute constraints, we train the model on the same dataset that was used for BERT pre-training (BookCorpus + Wikipedia) with a maximum sequence length of 512 6For instance, in case of BERT-base, this amounts to less than 0.009% of the total model size. 7Specifically, we use the English subset of Wiki-40b,https://huggingface.co/datasets/wiki40b, that contains cleaned-up text of English Wikipedia and training/validation splits. 6γ ζ FP16 ppl.↓ Max inf. norm Avg. kurtosis W8A8 ppl. ↓ 0 1 4.49±0.01 735±55 3076±262 1294±1046 (= Vanilla) 0 1 .003 4.48±0.01 715±335 2159±238 451±57 0 1 .03 4.49±0.00 741±66 1707±1249 1469±646 −0.003 1 4.46±0.00 688±64 2149±110 636±566 −0.03 1 4.41±0.01 20±1 80±6 4.55±0.01 −0.003 1 .003 4.47±0.00 683±23 2494±1205 268±120 −0.03 1 .03 4.43±0.03 22±3 73±8 4.56±0.05 Table 1: The impact of clipped softmax hyperparameters on BERT-base. and batch size of 192. Similar to our BERT experiments, we use training and evaluation pipelines from HuggingFace libraries. We evaluate on Wikipedia validation set and report the CLM perplexity. ViT Finally, we explore the effectiveness of proposed techniques on vision transformer [15] (ViT- S/16 configuration, 22M parameters) trained on ImageNet-1K [11, 52]. For these experiments, we adopt the training and validation pipelines from PyTorch Image models library [64]. We report top-1 accuracy on the validation set of ImageNet. Quantization setup In all experiments, after the model is trained, we apply 8-bit PTQ. We use uniform affine quantization – symmetric weights, asymmetric activations – with the static activation range setting, as discussed in Section 2. We quantize all weights and activations (both input and output), except the final linear layer for BERT and OPT models. We explore several choices of range estimation (see Appendix C.4) and report the best configuration for each experiment, based on the model performance. We repeat each PTQ experiment 3 times with different random seeds8 and report mean and standard deviation for accuracy/perplexity. We train each network two times with different random seeds and report mean and standard deviation. To assess the amount of outliers in the trained model, we use two metrics: the maximum ∥x∥∞ averaged across the validation set, and kurtosis of x averaged across all layers, where x is the output of an attention layer. These metrics have been shown to correlate well with the model quantizability [4, 6]. 5.1 The impact of clipped softmax hyperparameters (γ and ζ) We investigate the effect of different values of the clipped softmax stretch parameters and present the results in Table 1. We can see that most of the improvement happens when we use γ < 0 (clipping at zero). For instance, using the value of γ = −0.03 leads to a significantly smaller infinity norm, kurtosis, and quantized model perplexity, compared to the baseline. It is also clear that in the limit |γ| →0 we approach the vanilla softmax attention. Using ζ >1 (clipping at one) yields similar results to the vanilla softmax. Finally, when we combine both γ <0 and ζ >1, for which the results seem similar to just clipping at 0. We, therefore, conclude that for dampening outliers, only the lower-range clipping allows exact zeros matter. Going forward we use only γ <0 and in Appendix B.5 we confirm that ζ >1 is not required for ViT. These observations are in line with our hypothesis that by giving the model the mechanism for representing exact zeros in the attention, we don’t need to learn the strong outliers. 5.2 Clipped softmax γ vs. sequence length As having an extra hyper-parameter that needs to be tuned per model or setup is generally not desirable, we study the sensitivity of the stretch factor γ and its relation with the sequence length T. Recall that the matrix of attention probabilities P has dimensions T × T and each row sums up to one. Because of that, the average value in P is 1/T. It is reasonable to assume that if we define 8Different random subsets of training data are used for quantizer range estimation. 7(a) Relative FP16 log-perplexity  (b) Maximum infinity norm Figure 6: The performance of clipped softmax using γ = −α/T parameterization on BERT-6L. (a) Relative (compared to vanilla softmax pre-training) FP16 log-perplexity ↑ on Wikitext validation set. (b) Maximum infinity norm of the attention layer output (note the logarithmic y-axis). (a) BERT-6L  (b) ViT Figure 7: The performance of Linear gated attention using different bias initialization settings. γ := −α T , where α >0 is a new hyperparameter, there might be a set or a range of values of α that works well across different sequence lengths. To study this, we train a 6-layer variant of BERT-base (BERT-6L) for 500000 steps on WikiText- 103 [ 42] with a batch size of 128 with several values of maximum sequence lengths T ∈ {32, 64, 128, 192, 256} and values of α ∈ {1/4, 1/2, 1, 2, 4, 8}. As we can see from Figure 6, using a clipped softmax with α ∈ [2, 4] significantly dampens the magnitude of outliers while maintaining good FP16 perplexity across all explored sequence lengths. 5.3 The impact of bias initialization in gated attention In all our gated attention experiments, we randomly initialize the weights of G, following [22]. By initializing the bias to a specific value, however, we can set gates to be more open or more closed initially. More open at the start means we initialize closer to the original network, but given the exponential nature of the gate it might take many iterations for the gate to learn to close. Similarly, if the gates are all closed at the start, we deviate too far from the original model training, causing a potential decrease in performance. Assuming Linear Gi’s with small initial weights, if we set the bias to the value of binit, then Gi(·) ≈ binit and πi(·) = sigmoid(Gi(·)) ≈ sigmoid(binit) =:πinit, at the start of training. We study the effect of different values of binit for Linear gated attention on BERT-6L and ViT. We set the bias for all Gi’s to the same value of binit. For BERT-6L, we use the same setup as in Section 5.2, with a fixed sequence length of 128. For ViT, we use the main setup, except we train it for 150 epochs instead of 300. In Figure 7 we see in both BERT and ViT cases that using bias with very highπinit generally performs similarly to the vanilla attention (comparable floating-point performance but strong outliers and poor quantized performance) while setting bias to have very low πinit dampens outliers quite well but leads to strong degradation in the floating-point and quantized performance. The reasonable ranges of πinit seems to be around [0.25, 0.9] for BERT and [0.1, 0.5] for ViT. The wide range indicates the relative robustness of our method to this hyperparameter. 8Model Method FP16/32 Max inf. norm Avg. kurtosis W8A8 BERT (ppl.↓) Vanilla 4.49 ±0.01 735±55 3076±262 1294±1046 Clipped softmax 4.39±0.00 21.5±1.5 80±6 4.52±0.01 Gated attention 4.45 ±0.03 39.2±26.0 201±181 4.65±0.04 OPT (ppl.↓) Vanilla 15.84 ±0.05 340±47 1778±444 21.18±1.89 Clipped softmax 16.29 ±0.07 63.2±8.8 19728±7480 37.20±2.40 Gated attention 15.55±0.05 8.7±0.6 18.9±0.9 16.02±0.07 ViT (acc.↑) Vanilla 80.75 ±0.10 359±81 1018±471 69.24±6.93 Clipped softmax 80.89 ±0.13 73.7±14.9 22.9±1.6 79.77±0.25 Gated attention 81.01±0.06 79.8±0.5 19.9±0.3 79.82±0.11 Table 2: A summary of results for our proposed methods applied on BERT, OPT-125m, and ViT. Model Method FP16 Max inf. norm Avg. kurtosis W8A8 OPT-350m (ppl.↓) Vanilla 13.19 253 2689 37.52 ±3.84 Gated attention 13.01 65.4 261 14.42±0.06 OPT-1.3B (ppl.↓) Vanilla 12.13 428 2756 989.6 ±175 Gated attention 12.21 67.2 444 29.95±0.42 Table 3: The performance of gated attention applied on bigger variants of OPT model. 5.4 Main results We summarize our main set of results in Table 2. As we can see, in almost all cases, both of our proposed techniques dampen the outliers’ magnitude to a great extent, reduce the kurtosis, and yield models with significantly higher quantized performance, which is close to the original FP16/32 performance. In addition to that, for each model, at least one of our methods also improves the floating-point task performance. We hypothesize this is because the network is helped with learning the “no-op” updates more easily. However, we are cautious about the improved performance as this is not consistent across all hyper-parameters and it is unclear if it generalizes to more architectures and larger models. The only case where our method failed to perform well was the clipped softmax applied to OPT. At the moment, we do not have an explanation of why this is the case and leave it for future work. We list selected hyper-parameters and show extended results in Appendix B. We also show the results of our proposed methods quantized to lower bitwidths in Appendix B.7. Results for bigger modelsWe study the question of scalability of our methods to larger models. In Table 3 we show the gated attention results for 350m and 1.3B variants of OPT. Due to compute constraints, we trained networks for 105 steps with batch size of 256 and the rest is the same as in our main pre-training setup. As we can see, our proposed gated attention is also very effective at dampening the outliers and significantly improving the quantized model performance when applied to bigger models. We further study in Appendix B.6 how gated attention can decrease outliers when fine-tuning bigger pre-trained models with outliers. 5.5 Qualitative results In Figure 8 we compare the learned attention patterns using vanilla softmax and our proposed methods (more examples in Appendix A.1). As we can see, both methods can represent a partial/soft no-op behavior, but in case of our methods this does not require strong outliers elsewhere in the network. Note that we found similar patterns in multiple attention heads, but the exact head indices where we observed such patterns depend on random initialization. In the case of clipped softmax, smaller attention weights are generally more diffused while higher weights are more saturated (which comes from the stretching and clipping). In the case of gated attention, the output of the softmax is significantly different since the update of the hidden representation is now further modulated by gating probabilities. 9(a) Vanilla softmax (Attention layer #11, head #3) (b) Clipped softmax (Attention layer #11, head #8) (c) Gated attention (Attention layer #11, head #5) Figure 8: Visualization of the self-attention patterns for BERT-base trained using vanilla and our proposed techniques, computed on data sequence #5 from MNLI-m validation set. (a), (b): attention probabilities, values, and their product. (c): gating probabilities π = sigmoid (G(x)), attention probabilities (output of softmax), values, and their combined product. 6 Discussion “No-op” behavior It is interesting to note that the identified “no-op” behavior is likely not limited to transformers and that convolutional architectures likely learn something similar. We also see that despite the network trying to learn a full “no-op”, still a small amount of noise is added to each residual, which may constitute a form of network regularization. Investigating this further might give us a clue as to why neural networks generalize despite being significantly overparametrized if many parameters are rendered unused by not updating the representation in later layers [72]. Limitations While we studied the scalability of our method for models up to 1.3B size, we haven’t explored the case of very large transformers that are trained for way longer. Given the fundamental understanding of the issue underlying our solutions, we expect the same effect on larger-scale models. We show a very small improvement in FP16/FP32 performance due to our methods, but we do not deem our results exhaustive enough to claim that this will hold in general. Lastly, our methods do have a hyperparameter each, although we show that both methods are relatively robust to its hyperparameter, having one is never optimal. Impact As our methods help transformers to be more efficient, we expect only positive outcomes of our work. Making neural networks more efficient will help with their high power consumption at inference. It further helps to move inference from the cloud to edge devices which can overcome potential privacy concerns. We cannot fathom any negative impact from our work that is not severely construed. 7 Conclusions We have thoroughly analyzed the activation outlier problem that makes transformers difficult to quantize. We showed that transformer networks try to learn not to update residuals and that by doing so, through the combination of the softmax, residual connections and LayerNorm, significant outliers appear in transformers. Based on this insight, we proposed two methods to address this at the core – clipped softmax and gated attention. These structural changes to transformers give similar, if not better, floating-point performance after training but significantly improve the post-training quantization results. We hope that with these two architectural changes to transformers, anyone can train high-performance transformers that are easy to quantize and can benefit from efficient integer inference. 10References [1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [2] Ron Banner, Yury Nahshan, Elad Hoffer, and Daniel Soudry. Post-training 4-bit quantization of convolution networks for rapid-deployment. arXiv preprint arXiv:1810.05723, 2018. [3] Yash Bhalgat, Jinwon Lee, Markus Nagel, Tijmen Blankevoort, and Nojun Kwak. Lsq+: Improving low-bit quantization through learnable offsets and better initialization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2020. [4] Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding and overcoming the chal- lenges of efficient transformer quantization. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7947–7969, Online and Punta Cana, Dominican Republic, Novem- ber 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.627. URL https://aclanthology.org/2021.emnlp-main.627. [5] Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Zeroq: A novel zero shot quantization framework. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13169–13178, 2020. [6] Brian Chmiel, Ron Banner, Gil Shomron, Yury Nahshan, Alex Bronstein, Uri Weiser, et al. Robust quantization: One model to rule them all. Advances in neural information processing systems , 33: 5308–5317, 2020. [7] Yoni Choukroun, Eli Kravchik, Fan Yang, and Pavel Kisilev. Low-bit quantization of neural networks for efficient inference. In ICCV Workshops, pages 3009–3018, 2019. [8] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. What does BERT look at? an analysis of BERT’s attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 276–286, Florence, Italy, August 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-4828. URL https://aclanthology.org/ W19-4828. [9] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020. [10] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 702–703, 2020. [11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009. [12] Tim Dettmers and Luke Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. arXiv preprint arXiv:2212.09720, 2022. [13] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplica- tion for transformers at scale. In Advances in Neural Information Processing Systems, 2022. [14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/ N19-1423. [15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [16] Steven K. Esser, Jeffrey L. McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra S. Modha. Learned step size quantization. In International Conference on Learning Representations (ICLR), 2020. 11[17] Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, Rémi Gribonval, Hervé Jégou, and Armand Joulin. Training with quantization noise for extreme model compression. arXiv preprint arXiv:2004.07320, 2020. [18] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. [19] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. A survey of quantization methods for efficient neural network inference. arXiv preprint arXiv:2103.13630, 2021. [20] Sylvain Gugger, Lysandre Debu, Thomas Wolf, Philipp Schmid, Zachary Mueller, and Sourab Mangrulkar. Accelerate: Training and inference at scale made simple, efficient and adaptable. https://github.com/ huggingface/accelerate, 2022. [21] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited numerical precision. In International conference on machine learning, pages 1737–1746. PMLR, 2015. [22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026–1034, 2015. [23] M. Horowitz. 1.1 computing’s energy problem (and what we can do about it). In 2014 IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC), pages 10–14, 2014. doi: 10.1109/ ISSCC.2014.6757323. [24] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized neural networks: Training neural networks with low precision weights and activations. The Journal of Machine Learning Research, 18(1):6869–6898, 2017. [25] Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. Improving post training neural quantization: Layer-wise calibration and integer programming. arXiv preprint arXiv:2006.10518, 2020. [26] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer- arithmetic-only inference. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2704–2713, 2018. [27] Minsoo Kim, Sihwa Lee, Sukjin Hong, Du-Seong Chang, and Jungwook Choi. Understanding and improving knowledge distillation for quantization-aware training of large transformer encoders. arXiv preprint arXiv:2211.11014, 2022. [28] Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. I-bert: Integer-only bert quantization. arXiv preprint arXiv:2101.01321, 2021. [29] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [30] Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. Revealing the dark secrets of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4365–4374, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10. 18653/v1/D19-1445. URL https://aclanthology.org/D19-1445. [31] Olga Kovaleva, Saurabh Kulshreshtha, Anna Rogers, and Anna Rumshisky. Bert busters: Outlier dimen- sions that disrupt transformers. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3392–3405, 2021. [32] Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efficient inference: A whitepaper. arXiv preprint arXiv:1806.08342, 2018. [33] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 7871–7880, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.703. URL https://aclanthology.org/ 2020.acl-main.703. 12[34] Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Sasko, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clément Delangue, Théo Matussière, Lysan- dre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, François Lagunas, Alexander Rush, and Thomas Wolf. Datasets: A community library for natural language processing. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstra- tions, pages 175–184, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. URL https://aclanthology.org/2021.emnlp-demo.21. [35] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. arXiv preprint arXiv:2102.05426, 2021. [36] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023. [37] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. [38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012–10022, 2021. [39] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [40] Christos Louizos, Max Welling, and Diederik P Kingma. Learning sparse neural networks through l_0 regularization. arXiv preprint arXiv:1712.01312, 2017. [41] Eldad Meller, Alexander Finkelstein, Uri Almog, and Mark Grobman. Same, same but different: Re- covering neural network quantization error through weight factorization. In International Conference on Machine Learning, pages 4486–4495. PMLR, 2019. [42] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. [43] Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free quantization through weight equalization and bias correction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1325–1334, 2019. [44] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? Adaptive rounding for post-training quantization. In International Conference on Machine Learning (ICML), 2020. [45] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training quantization. In International Conference on Machine Learning, pages 7197–7206. PMLR, 2020. [46] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart van Baalen, and Blankevoort Tijmen. A white paper on neural network quantization. arXiv preprint arXiv:2106.08295, 2021. [47] Vinod Nair and Geoffrey E. Hinton. Rectified Linear Units Improve Restricted Boltzmann Machines. In Proceedings of the 27th International Conference on Machine Learning, pages 807–814. Omnipress, 2010. [48] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Neural Information Processing Systems (NeuRIPS). 2019. [49] Giovanni Puccetti, Alessio Miaschi, and Felice Dell’Orletta. How do BERT embeddings organize linguistic knowledge? In Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 48–57, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.deelio-1.6. URL https://aclanthology.org/ 2021.deelio-1.6. 13[50] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. [51] Bita Rouhani, Daniel Lo, Ritchie Zhao, Ming Liu, Jeremy Fowers, Kalin Ovtcharov, Anna Vinogradsky, Sarah Massengill, Lita Yang, Ray Bittner, Alessandro Forin, Haishan Zhu, Taesik Na, Prerak Patel, Shuai Che, Lok Chand Koppaka, Xia Song, Subhojit Som, Kaustav Das, Saurabh Tiwary, Steve Reinhardt, Sitaram Lanka, Eric Chung, and Doug Burger. Pushing the limits of narrow precision inferencing at cloud scale with microsoft floating point. In Neural Information Processing Systems (NeurIPS 2020). ACM, November 2020. [52] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 2015. [53] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019. [54] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Q-bert: Hessian based ultra low precision quantization of bert. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8815–8821, 2020. [55] Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. MobileBERT: a compact task-agnostic BERT for resource-limited devices. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2158–2170, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.195. URL https://aclanthology.org/ 2020.acl-main.195. [56] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1–9, 2015. [57] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Hervé Jégou. Going deeper with image transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 32–42, 2021. [58] Hugo Touvron, Matthieu Cord, and Hervé Jégou. Deit iii: Revenge of the vit. In Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXIV, pages 516–533. Springer, 2022. [59] Mart van Baalen, Andrey Kuzmin, Suparna S Nair, Yuwei Ren, Eric Mahurin, Chirag Patel, Sundar Subramanian, Sanghyuk Lee, Markus Nagel, Joseph Soriaga, and Tijmen Blankevoort. Fp8 versus int8 for efficient deep learning inference. 2023. [60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 6000–6010, 2017. [61] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , pages 353–355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/anthology/W18-5446. [62] Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier suppression: Pushing the limit of low-bit transformer language models. arXiv preprint arXiv:2209.13325, 2022. [63] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, and Xianglong Liu. Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling. arXiv preprint arXiv:2304.09145, 2023. [64] Ross Wightman. Pytorch image models. https://github.com/rwightman/pytorch-image-models, 2019. [65] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, pages 38–45, 2020. 14[66] Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, and Yuxiong He. Understanding int4 quantization for transformer models: Latency speedup, composability, and failure cases. 2023. [67] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In CVPR, 2022. [68] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xl- net: Generalized autoregressive pretraining for language understanding. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Pro- cessing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/ paper_files/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf. [69] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization for large-scale transform- ers. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Ad- vances in Neural Information Processing Systems , volume 35, pages 27168–27183. Curran Asso- ciates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ adf7fa39d65e2983d724ff7da57f00ac-Paper-Conference.pdf. [70] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6023–6032, 2019. [71] Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. Q8bert: Quantized 8bit bert. arXiv preprint arXiv:1910.06188, 2019. [72] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. 2017. [73] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017. [74] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. [75] Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, and Zhiru Zhang. Improving neural network quantization without retraining using outlier channel splitting. In International conference on machine learning, pages 7543–7552. PMLR, 2019. [76] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016. [77] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In The IEEE International Conference on Computer Vision (ICCV), December 2015. 15Supplementary materials A Additional graphs from outlier analysis In this section, we present additional graphs from our outlier investigation in Section 3 for BERT and vision transformer. (a)  (b)  (c) Figure 9: A summary of several outlier statistics recorded from ImageNet validation set on ViT. (a) Average infinity norm of the output of each attention layer. (b) A histogram of outlier counts in attention layer #10 vs. hidden dimensions. We use zero-based indexing for dimensions. (c) A heatmap of outlier counts in attention layer #10 vs. patch positions. A.1 BERT Recall from Figure 1 that all the outliers are only present in hidden dimensions #123, #180, #225, #308, #381, #526, #720 (with the majority of them in #180, #720). These hidden dimensions correspond to attention heads #2, #3, #4, #5, #6, #9, and #12. In Figures 10 and 11 we show more examples of the discovered self-attention patterns for attention heads #3 and #12 (↔ hidden dim #180 and #720, respectively). We also show self-attention patterns in attention heads and layers which are not associated with the outliers in Figures 12 and 13, respectively. Finally, in Figures 14 and 15 we show more examples of the attention patterns learned in the network trained with clipped softmax and gated attention. A.2 ViT Figure 9 further shows that there are a lot of similarities in the outlier behavior in the vision transformer, compared to BERT. The strongest magnitude outliers generally happen in the later layers, peaking at layers #10 and #11. The majority of outliers (> 99%) are only ever happening in only 10 hidden dimensions, primarily in dimensions #48 and #43, which corresponds to the attention head #1. Finally, averaged across the entire ImageNet validation set, the outliers seem to be concentrated at the boundaries of the image, which suggest a strong correlation with the background (and a negative correlation with the object, which is usually in the center of the image in the ImageNet dataset). In Figures 16 and 17, we show more examples of outlier and self-attention patterns in the attention head #1 (↔ hidden dimensions #48, #43) for a random subset of images from the ImageNet validation set (in layers #10 and #11, respecively). B Detailed results In this section, we provide extended results for each model, including the used hyperparameters and other design choices. We also present some additional ablation studies. 16Configuration G Memory overhead (per attention layer) # extra parameters # extra tokens Linear nheads × Linear(dhead → 1) nheads(dhead + 1) ∼ 1 MLP nheads × MLP(dhead → nhid → 1) nheads(nhid(dhead + 2) + 1) ∼ nhid All-heads-linear Linear(dmodel → nheads) nheads(dmodel + 1) ∼ nheads Table 4: An overview of the gating function parameterizations explored in this paper and their memory overhead. B.1 Gating architectures We investigate the choice of several gating functions, summarized in Table 4. The configuration “MLP” parameterizes each Gi with a feed-forward net with one hidden layer of size nhid and a ReLU non-linearity [47]. We also explore what happens if we allow the mixing of the representation from different attention heads in the “All-heads-linear” setting, where we use a single linear layer to produce the gating probabilities for all attention heads at once. All three options are tested below. Unless explicitly stated otherwise, we initialize the bias of the gating function to zero (i.e., binit = 0 ↔ πinit = 0.5). B.2 BERT Method FP16 ppl. ↓ Max inf norm Avg. Kurtosis W8A8 ppl. ↓ Vanilla 4.49 ±0.01 735.0±54.9 3076±262 1294±1046 CS (γ = −0.005) 4.44 ±0.02 406.6±35.2 1963±753 75.27±39.57 CS (γ = −0.01) 4.35 ±0.01 198.3±78.7 1581±839 7.06±2.37 CS (γ = −0.015) 4.37 ±0.01 38.9±7.9 165±34 4.54±0.01 CS (γ = −0.02) 4.39 ±0.02 31.7±6.3 90±20 4.56±0.02 CS (γ = −0.025) 4.39±0.00 21.5±1.5 80±6 4.52±0.01 CS (γ = −0.03) 4.41 ±0.01 20.4±0.2 79±6 4.55±0.01 CS (γ = −0.04) 4.51 ±0.05 19.8±9.0 85±7 4.65±0.06 GA, Linear (πinit = 0.25) 4.49 ±0.00 139.8±62.3 739±412 5.05±0.27 GA, Linear (πinit = 0.5) 4.48 ±0.00 177.3±33.2 652±81 5.13±0.15 GA, Linear (πinit = 0.75) 4.49 ±0.00 71.4±49.9 262±147 4.88±0.22 GA, Linear (πinit = 0.9) 4.49 ±0.00 171.5±8.8 559±141 5.15±0.03 GA, MLP (nhid = 4) 4.45±0.03 39.2±26.0 201±181 4.65±0.04 GA, MLP (nhid = 64) 4.49 ±0.01 117.0±48.3 507±167 4.77±0.01 GA, All-heads-linear 4.49 ±0.01 58.3±41.2 334±321 4.67±0.03 Table 5: Main results for our proposed clipped softmax (CS) and gated attention (GA) applied to BERT-base. We report the masked language modeling perplexity (ppl. for short) on the English Wikipedia validation set for both the floating-point baseline and W8A8 quantized model. We also report the maximum ∥x∥∞ averaged across the validation set, and kurtosis of x averaged across all layers, where x is the output of an attention layer. Detailed results for BERT-base are summarized in Table 5. As we can see, across most of the settings, both of our methods significantly dampen the outliers’ magnitude, reduce the kurtosis, drastically improve the quantized performance, while maintaining and sometimes improving the FP16 perplexity. B.3 OPT Detailed results for OPT-125m are summarized in Table 6. In our early experiments on a smaller OPT model, we found that applying the weight decay on LayerNorm weights γ (which isn’t the case, by default) has a strong effect on reducing the outliers’ magnitude while yielding the comparable FP16 performance. Therefore, we present the results of applying our gated attention approach in both cases, with and without applying weight decay on LNγ. As we can see in Table 6, in both cases gated attention (further) dampens the outliers’ magnitude to a 17Method LN γ wd FP16 ppl. ↓ Max inf norm Avg. Kurtosis W8A8 ppl. ↓ Vanilla ✕ 15.84±0.05 339.6±47.2 1777±444. 21.18±1.89 GA, Linear (πinit = 0.1) ✕ 15.61±0.05 35.6±4.5 42.4±22.9 16.41±0.18 GA, Linear (πinit = 0.25) ✕ 15.50±0.04 35.8±0.5 59.0±48.3 16.25±0.08 GA, Linear (πinit = 0.5) ✕ 15.54±0.01 46.5±5.0 40.6±8.9 16.30±0.01 GA, All-heads-linear ✕ 15.43±0.01 32.8±1.7 24.2±3 16.30±0.12 Vanilla ✓ 15.96±0.03 87.7±31.9 2080±1460 39.46±16.59 CS (γ = −1/512) ✓ 15.99±0.02 106.4±7.0 5764±2150 185.23±220.00 CS (γ = −2/512) ✓ 15.90±0.02 102.0±27.0 11290±4372 60.90±52.70 CS (γ = −4/512) ✓ 15.86±0.01 83.1±20.6 17174±7791 84.64±10.55 CS (γ = −8/512) ✓ 16.13±0.09 61.5±9.9 19204±4284 42.62±3.64 CS (γ = −12/512) ✓ 16.29±0.07 63.2±8.8 19727±7479 37.22±2.39 GA, Linear (πinit = 0.1) ✓ 15.69±0.05 7.3±0.4 25.4±10 16.23±0.08 GA, Linear (πinit = 0.25) ✓ 15.55±0.05 8.7±0.6 18.9±1 16.02±0.07 GA, Linear (πinit = 0.5) ✓ 15.63±0.00 10.8±0.7 42.0±19 16.20±0.01 GA, All-heads-linear ✓ 15.53±0.01 7.9±0.3 13.8±1 16.09±0.08 Table 6: Main results for our proposed clipped softmax (CS) and gated attention (GA) applied to OPT-125m. We report the causal language modeling perplexity (ppl. for short) on the English Wikipedia validation set for both the floating-point baseline and W8A8 quantized model. We also report the maximum ∥x∥∞ averaged across the validation set, and kurtosis of x averaged across all layers, where x is the output of an attention layer. great extent, reduces the kurtosis, and yields models with significantly higher quantized performance, which is close to the original FP16 performance. B.4 ViT Method Patch. Embd. LN FP32 acc. Max inf norm Avg. Kurtosis W8A8 acc. Vanilla ✕ 80.75±0.10 358.5±81.2 1018.3±471.5 69.24±6.93 CS (γ = −0.003) ✕ 80.24±0.05 69.3±20.7 25.6±8.6 78.71±0.33 CS (γ = −0.004) ✕ 80.38±0.01 74.9±10.6 30.6±4.9 78.66±0.49 GA, Linear (πinit = 0.25) ✕ 80.62±0.01 86.0±8.0 23.4±2.7 79.16±0.05 GA, Linear (πinit = 0.5) ✕ 80.32±0.02 88.4±17.9 27.9±14.0 78.90±0.25 GA, MLP (nhid = 4) ✕ 80.62±0.05 118.2±40.5 47.8±29.8 78.79±0.29 Vanilla ✓ 80.98±0.08 81.1±2.5 24.5±1.8 79.62±0.06 CS (γ = −0.0001) ✓ 80.89±0.13 73.7±14.9 22.9±1.6 79.77±0.25 CS (γ = −0.0003) ✓ 80.92±0.07 78.9±5.5 23.8±0.5 79.63±0.05 CS (γ = −0.0005) ✓ 80.95±0.08 72.9±11.8 24.4±0.7 79.73±0.08 CS (γ = −0.001) ✓ 80.95±0.16 80.8±2.1 24.1±0.7 79.69±0.03 CS (γ = −0.002) ✓ 80.80±0.07 78.0±0.5 25.8±0.7 79.32±0.07 CS (γ = −0.003) ✓ 80.79±0.02 75.6±7.9 28.1±4.0 79.00±0.10 GA, Linear (πinit = 0.5) ✓ 81.01±0.06 79.8±0.5 19.9±0.3 79.82±0.11 GA, Linear (πinit = 0.75) ✓ 81.01±0.05 77.8±0.3 21.8±1.9 79.80±0.08 GA, Linear (πinit = 0.9) ✓ 80.92±0.11 70.6±8.0 23.2±3.7 79.64±0.09 Table 7: Main results for our proposed clipped softmax (CS) and gated attention (GA) applied to ViT-S/16. We report the top-1 accuracy on ImageNet-1K validation set for floating-point baseline and W8A8 quantized model. We also report the maximum ∥x∥∞ averaged across the validation set, and kurtosis of x averaged across all layers, where x is the output of the attention layer. Detailed results for ViT-S/16 are summarized in Table 7. After our preliminary experiments on ViT, we noticed that distinct outliers already originate after the patch embeddings. Therefore, we experimented with adding the LayerNorm after the patch 18embeddings (which was absent in the model definition, by default). As we can see in Table 6, together with this change, both of our proposed methods greatly dampens the outliers’ magnitude, reduces the kurtosis, and yields models with significantly higher quantized performance, which is within 1% of the original FP32 accuracy. B.5 The impact of clipped softmax hyperparameters (γ and ζ) on ViT γ ζ FP32 acc. Max inf norm W8A8 acc. 0 1 78.80±0.42 426±69 71.27±0.88 (= Vanilla) 0 1 .001 78.78±0.29 411±88 71.24±0.59 0 1 .002 78.90±0.17 420±47 70.74±0.34 0 1 .004 78.80±0.45 377±67 72.31±0.06 0 1 .01 78.81±0.30 419±77 71.35±0.26 −0.00001 1 78.81±0.21 432±76 69.02±0.19 −0.0001 1 78.81±0.36 380±64 64.04±10.8 −0.001 1 78.42±0.63 282±105 68.43±6.50 −0.003 1 78.26±0.06 99±36 76.49±0.48 −0.01 1 78.10±0.14 391±21 75.83±1.12 −0.03 1 70.26±1.46 197±2 65.80±1.41 −0.001 1 .001 78.45±0.53 283±82 65.03±8.54 −0.003 1 .003 78.25±0.14 119±17 76.37±0.45 Table 8: The impact of clipped softmax hyperparameters on ViT-S/16. We investigate the effect of different values of the clipped softmax stretch parameters applied to the vision transformer and present the results in Table 8. To speed up training, for this experiment we trained ViT for 150 epochs instead of the usual 300 epochs. For this experiment, we did not apply LayerNorm after the patch embeddings. We found similar observations compared to BERT. Specifically, most of the improvement happens when we use γ <0 (clipping at zero) whereas using ζ >1 (clipping at one) yields similar results to the vanilla softmax and combining both γ <0 and ζ >1 yields similar results compared to just clipping at zero. B.6 Fine-tuning experiment Method FP16 ppl. ↓ Max inf norm Avg. Kurtosis Vanilla fine-tuning 29.46 79.3 2086 Fine-tuning w/ Gated attention 29.18 50.9 665 Table 9: OPT-1.3B fine-tuning results with vanilla softmax and gated attention. We report the causal language modeling perplexity (ppl. for short) on the English Wikipedia validation set. We also report the maximum ∥x∥∞ averaged across the validation set, and kurtosis of x averaged across all layers, where x is the output of an attention layer. One of the drawbacks of our proposed framework is that it requires training from scratch, which could be expensive when applied to very large models. To address this, we explored whetherfine-tuning using gated attention can still lead to improved performance and decreased outliers for larger models. We used OPT-1.3B pre-trained checkpoint from HuggingFace and fine-tuned it on Bookcorpus + Wikipedia for 4000 steps with batch size 256, maximum sequence length 512, maximum learning rate 10−5, and linear LR schedule with 400 warmup steps. We use the same LR for both model parameters and gating module parameters. The rest of hyper-parameters are the same as for our pre-training setup. We adapted our gating approach as follows. We initialized bias as binit = 0, which corresponds to the expected initial gating probability output of πinit = 0.5. We multiply the gating probability by 2 so that the expected gate output is 1 and we approximate the attention output of 19the vanilla softmax at the start of fine-tuning. We add a small activation regularization term at the output of each FFN to further encourage the reduction in the magnitude of activations, as unlike when training from scratch outliers are already present in the pre-trained model and need to be suppressed. As we can see from Table 9, fine-tuning with our proposed gated attention results in a better perplexity and also reduced maximum infinity norm and the average kurtosis compared to fine-tuning with vanilla softmax. B.7 Low-bit quantization results Bitwidths Weight range estimation Vanilla Clipped softmax Gated attention FP16 − 4.49±0.01 4.39±0.00 4.45±0.03 W8A8 min-max 1294 ±1046 4.52±0.01 4.65±0.04 W6A8 min-max 598 ±254 4.64±0.01 4.79±0.03 W6A8 MSE 6.49 ±0.38 4.56±0.01 4.71±0.03 W4A8 MSE 6.52 ±0.02 4.90±0.02 5.02±0.03 W6A6 MSE 42.8 ±11.7 6.64±0.14 5.90±0.11 Table 10: A summary of results for our proposed methods applied to BERT-base and quantized to different bitwidthds for weights and activations (using the same PTQ setup as in all previous experi- ments). We report the masked language modeling perplexity on the English Wikipedia validation set. Note that our proposed methods are not limited to 8-bit quantization only and in general can be combined with other more advanced quantization and weight compression methods, including [18, 35, 36, 45, 63, 67]. In Table 10, we show the results of our proposed methods applied to BERT-base and quantized to different bitwidths using our simple post-training quantization setup. Unless stated otherwise, for low-bit (<8-bit) weights and activations we use MSE range estimator as recommended by [2, 7] since it gives better results. As we can see, in all cases both of our methods significantly improve the perplexity compared to the vanilla softmax pre-training. We also notice that generally the performance progressively degrades as we decrease the bitwidths, which is to be expected. Achieving good results with low-bit activation quantization in general is a challenging problem. Further, we notice that the perplexity of the vanilla model significantly improves whenever we consider a low-bit weight quantization with MSE ranges compared to the INT8 case. This can be explained by the fact that using MSE range estimation for weights leads to an implicit clipping of activations (in the same and all subsequent layers in the network), which happen to be of the right amount so that it doesn’t hurt the perplexity. We found that by going from W8A8 to W6A8 the average kurtosis is reduced from 3406±547 to 631±94 and the maximum infinity norm is reduced from 577±80 to 158±40. However, in all cases the resulting model still has significantly larger outliers and a worse performance than both of our proposed methods. Finally, as said before, if achieving good low-bit quantization performance is the goal, it is recommended to combine our methods with more advanced quantization techniques. C Experimental details C.1 BERT Fine-tuning on MNLI dataset We use pre-trained checkpoint BERT-base-uncased (109M param- eters) from HuggingFace repository. We follow standard fine-tuning practices from [14] and [ 65] Each data sequence is tokenized and truncated to the maximum sequence length of 128. Shorter sequences are padded to the same length of 128 using a special [PAD] token. We fine-tune for 3 epochs using Adam [29] with a batch size of 16 and no weight decay. The learning rate is initially set to its maximum value of of 2 · 10−5 and is linearly decayed to zero by the end of fine-tuning. Pre-training from scratch We follow closely the pre-training procedure from [14]. We concate- nate, tokenize, and split the training set into sequences of length 128 (to speed up training and experimentation, we do not fine-tune on longer sequences of 512). We use the masked language modeling objective with the probability of masking p = 0.15. We train with a batch size of 256 20sequences for 106 steps, using AdamW optimizer [ 39] with the maximum learning rate of 10−4, learning rate warm up over the first 104 steps, following by a linear decay to zero by the end of training. We use L2 weight decay of 0.01, L2 gradient norm clipping of 1.0, and dropout probability of 0.1 on all layers. We also use FP16 mixed-precision from HuggingFace Accelerate library [20]. C.2 OPT pre-training To speed up experimentation, we train OPT-125m sized model on the concatenation of Wikipedia and BookCorpus (same as BERT pre-training). We train with a batch size of 48 and 4 gradient accumulation steps (which results in the effective batch size of 192), so that we can perform pre- training on a single A100 80GB GPU. We concatenate, tokenize, and split the training set into sequences of length 512 and train for 125000 steps (500000 forward passes). We use the rest of the hyper-parameters and follow pre-training practices from [74] and [65]. We initialize weights using a normal distribution with zero mean and a standard deviation of 0.006. All bias terms are initialized to zero. We use AdamW optimizer with (β1, β2) = (0.9, 0.95). We use the linear learning rate schedule, warming up from 0 to the maximum value† of 4 · 10−4 over the first 2000 steps, following by a linear decay to zero by the end of training. We use L2 weight decay of 0.1, L2 gradient norm clipping of 1.0, and dropout probability of 0.1 on all layers. We also use FP16 mixed-precision from HuggingFace Accelerate library [20]. Note that in our experiments for all model sizes we use the consistent LayerNorm placement before the attention block, unlike OPT-350m checkpoint from HuggingFace that places LayerNorm after the attention block. C.3 ViT pre-training We use the model definition for ViT-S/16 and the training pipeline from PyTorch Image models library [64]. All training is done on resolution 224 ×224 and 16 ×16 patches. For data augmentation, we use RandAugment [10], Mixup [73], CutMix [70], random image cropping [56], horizontal flip, label smoothing ε = 0.1, color jitter 0.4, and random (between bilinear and bicubic) interpolation during training. We train with a batch size of 512 for 300 epochs, using AdamW optimizer and the L2 weight decay of 0.03. We use the cosine learning rate schedule, warming up from 10−6 to the maximum value of 10−3 over the first 20 epochs, followed by a LR decay by a factor of 10 every 30 epochs, until it reaches the minimum value of 10−5. C.4 Quantization settings Weights In all cases, we use symmetric uniform quantization of weights. We use min-max weight quantization for all models except the OPT model, for which we found the MSE estimator to perform better in all cases. Activations We adopt static range estimation approach, which determines quantization parameters for the network by passing a few batches of calibration data through the model before inference. Specifically, we use a running min-max estimator [32], which uses an exponential moving average of the min and max over multiple batches. In all cases, we use running min-max with 0.9 momentum over 16 batches randomly sampled from respective training sets. For OPT model, we also experiment with using 99.99% and 99.999% percentiles instead of actual min and max. We select the best configuration for each experiment (including baseline), based on the model performance. In almost all cases, we found that setting activation quantization ranges using 99.999% percentiles gives the lowest W8A8 perplexity. D Compute cost We compare the runtime of our proposed methods in Table 11. As we can see, the clipped softmax is only marginally more expensive compared to using the vanilla softmax attention. The gated attention †In our experiments, we found this value to perform better compared to the value of 6 · 10−4 listed in the paper. 21Model Vanilla Clipped softmax Gated attention (Linear / MLP) BERT 92.8 ±1.2 93.6±0.8 97.7 / 119.1 OPT 53.6 ±0.4 54.4±0.4 55.7 / 64.7 ViT 101.8 ±0.3 104.0±0.7 110.8 / 122.9 Table 11: An overview of the runtime of the proposed methods, compared to the vanilla pre-training, measured in hours on Nvidia-A100 GPUs. using the linear G adds the compute overhead between 3% and 8%, depending on the model. We found that adding weight decay on LayerNorm γ for OPT and adding the LayerNorm after the patch embeddings for ViT had a negligible effect on the runtime. We estimated that the compute cost of producing the main results in the paper is about 320 GPU days (on A100) and the total cost of the project (including preliminary experiments and ablation studies) to be about 1400 GPU days. 22(a) Attention layer #10, data sequence #16 (b) Attention layer #11, data sequence #16 (c) Attention layer #10, data sequence #21 (d) Attention layer #11, data sequence #21 (e) Attention layer #10, data sequence #61 (f) Attention layer #11, data sequence #61 (g) Attention layer #10, data sequence #88 (h) Attention layer #11, data sequence #88 Figure 10: Visualization of the self-attention patterns (attention probabilities, values, and their product in left, middle and right columns, respectively) in attention head #3 (↔ channel dim #180) for BERT-base trained with vanilla softmax, computed on several random data sequences from MNLI-m validation set. 23(a) Attention layer #10, data sequence #16 (b) Attention layer #11, data sequence #16 (c) Attention layer #10, data sequence #21 (d) Attention layer #11, data sequence #21 (e) Attention layer #10, data sequence #61 (f) Attention layer #11, data sequence #61 (g) Attention layer #10, data sequence #88 (h) Attention layer #11, data sequence #88 Figure 11: Visualization of the self-attention patterns (attention probabilities, values, and their product in left, middle and right columns, respectively) in attention head #12 ( ↔ channel dim #720) for BERT-base trained with vanilla softmax, computed on several random data sequences from MNLI-m validation set. 24(a) Attention layer #10, attention head #1 (b) Attention layer #11, attention head #1 (c) Attention layer #10, attention head #7 (d) Attention layer #11, attention head #7 (e) Attention layer #10, attention head #8 (f) Attention layer #11, attention head #8 (g) Attention layer #10, attention head #10 (h) Attention layer #11, attention head #10 Figure 12: Visualization of the self-attention patterns (attention probabilities, values, and their product in left, middle and right columns, respectively) in attention heads that are not associated with the strong outliers for BERT-base trained with vanilla softmax, computed on data sequences #16 from MNLI-m validation set. 25(a) Attention layer #1 (b) Attention layer #2 (c) Attention layer #3 (d) Attention layer #4 (e) Attention layer #5 (f) Attention layer #6 (g) Attention layer #7 (h) Attention layer #8 Figure 13: Visualization of the self-attention patterns (attention probabilities, values, and their product in left, middle and right columns, respectively) in attention head #3 (↔ channel dim #180) and the first eight layers of BERT-base trained with vanilla softmax, computed on data sequences #16 from MNLI-m validation set. 26(a) Attention layer #10, Attention head #3, data sequence #1 (b) Attention layer #11, Attention head #3, data sequence #1 (c) Attention layer #10, Attention head #3, data sequence #5 (d) Attention layer #11, Attention head #3, data sequence #5 (e) Attention layer #10, Attention head #3, data sequence #7 (f) Attention layer #11, Attention head #3, data sequence #7 (g) Attention layer #10, Attention head #12, data sequence #1 (h) Attention layer #11, Attention head #12, data sequence #1 Figure 14: Visualization of the self-attention patterns (attention probabilities, values, and their product in left, middle and right columns, respectively) for BERT-base trained with Clipped softmax, computed on several random data sequences from MNLI-m validation set. 27(a) Attention layer #10, Attention head #3, data sequence #1 (b) Attention layer #11, Attention head #3, data sequence #1 (c) Attention layer #10, Attention head #3, data sequence #5 (d) Attention layer #11, Attention head #3, data sequence #5 (e) Attention layer #10, Attention head #3, data sequence #7 (f) Attention layer #11, Attention head #3, data sequence #7 (g) Attention layer #10, Attention head #12, data sequence #1 (h) Attention layer #11, Attention head #12, data sequence #1 Figure 15: Visualization of the self-attention patterns (from left to right: gating probabilities π = sigmoid (G(x)), output of softmax, values, and their combined product) for BERT-base trained with gated attention, computed on several random data sequences from MNLI-m validation set. 28(a)  (b)  (c)  (d)  (e) Figure 16: A summary of our outlier analysis for ViT demonstrated on a random subset from ImageNet validation set. (a) An input image. (b) Outliers in the output of layer #10. (c) Cumulative attention weight spent on every patch (matrix of attention probabilities summed over rows) in the attention head #1, in the next layer #11. (d) A corresponding matrix of attention probabilities. (e) An average magnitude of values (V ) for outlier and non-outlier patches. 29(a)  (b)  (c)  (d)  (e) Figure 17: A summary of our outlier analysis for ViT demonstrated on a random subset from ImageNet validation set. (a) An input image. (b) Outliers in the output of layer #11. (c) Cumulative attention weight spent on every patch (matrix of attention probabilities summed over rows) in the attention head #1, in the next layer #12. (d) A corresponding matrix of attention probabilities. (e) An average magnitude of values (V ) for outlier and non-outlier patches. 30",
      "meta_data": {
        "arxiv_id": "2306.12929v2",
        "authors": [
          "Yelysei Bondarenko",
          "Markus Nagel",
          "Tijmen Blankevoort"
        ],
        "published_date": "2023-06-22T14:39:04Z",
        "pdf_url": "https://arxiv.org/pdf/2306.12929v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper identifies the root cause of strong activation outliers in Transformer models, which significantly hinder efficient quantization. It shows that outliers arise when attention heads attempt 'no-op' or partial updates of the residual connection, pushing softmax inputs to extreme values. To address this, the authors propose two novel and independent modifications to the attention mechanism: 'clipped softmax' and 'gated attention'. These methods enable models to learn significantly smaller outliers, maintaining or even improving floating-point performance, and facilitating straightforward full INT8 quantization of activations without additional effort. The effectiveness is demonstrated across language models (BERT, OPT) and vision transformers.",
        "methodology": "The study analyzes outlier behavior in BERT and Vision Transformers, identifying outliers as values exceeding 6 standard deviations from the mean. It hypothesizes that outliers are a learned workaround for attention heads to achieve no-update behavior by concentrating attention probability on uninformative tokens with small value function outputs, requiring large softmax input dynamic range. The proposed 'clipped softmax' replaces the standard softmax function with a stretched and clipped version, `clip ((ζ − γ) · softmax(x) + γ, 0, 1)`, allowing exact zeros/ones with finite input range and preventing gradient flow for clipped values. 'Gated attention' introduces an explicit conditional gating mechanism `sigmoid (G(x)) ⊙ softmax (Q(x)K(x)T/√dhead) V(x)`, where G is a lightweight learnable neural network (e.g., linear layer or MLP) per attention head, modulating the attention output to explicitly control updates. The work focuses on post-training quantization (PTQ) using uniform affine quantization (symmetric weights, asymmetric activations) with static activation range estimation.",
        "experimental_setup": "The methods were evaluated on BERT-base-uncased (109M parameters) for Masked Language Modeling, OPT-125M, OPT-350M, and OPT-1.3B variants for Causal Language Modeling, and ViT-S/16 (22M parameters) for ImageNet-1K. Pre-training for BERT and OPT used BookCorpus and English Wikipedia, while ViT was trained on ImageNet-1K. BERT was fine-tuned on the MNLI dataset. Quantization involved 8-bit PTQ, uniform affine (symmetric weights, asymmetric activations), and static range estimation using running min-max over 16 calibration batches (with 99.999% percentiles for OPT). Metrics included MLM/CLM perplexity and Top-1 accuracy for floating-point and W8A8 quantized models, along with maximum L-infinity norm and average kurtosis to quantify outliers. Hyperparameter sensitivity was studied for clipped softmax (γ, ζ, and γ=-α/T) and gated attention (bias initialization πinit). A fine-tuning experiment was conducted on OPT-1.3B. Lower bitwidth quantization (W6A8, W4A8, W6A6) with MSE range estimation was also explored.",
        "limitations": "The scalability of the proposed methods to very large transformers trained for significantly longer durations was not explicitly explored, although the authors anticipate similar effects based on the fundamental understanding of the issue. The observed small improvements in floating-point (FP16/FP32) performance are not definitively generalized across all architectures or larger models. Each proposed method introduces an additional hyperparameter, despite showing relative robustness. The clipped softmax method notably failed to perform well when applied to OPT models, a phenomenon for which the authors currently lack an explanation.",
        "future_research_directions": "Future work could involve further investigation into the identified 'no-op' behavior to understand its implications for network regularization and generalization, particularly how it relates to over-parameterization in neural networks. Explaining the failure of clipped softmax on OPT models is another open research question. Additionally, the authors suggest combining their proposed methods with other advanced quantization and weight compression techniques to further enhance performance, especially for achieving good low-bit quantization results."
      }
    }
  ]
}