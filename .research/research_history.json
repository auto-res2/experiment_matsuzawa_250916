{
  "research_topic": "Continuous Learningのメモリ効率に関して改善したい",
  "queries": [
    "memory efficient continual learning",
    "continual learning model compression",
    "rehearsal buffer compression",
    "sparse experience replay",
    "continual learning quantization"
  ],
  "research_study_list": [
    {
      "title": "Continual Learning in Low-rank Orthogonal Subspaces",
      "abstract": "In continual learning (CL), a learner is faced with a sequence of tasks,\narriving one after the other, and the goal is to remember all the tasks once\nthe continual learning experience is finished. The prior art in CL uses\nepisodic memory, parameter regularization or extensible network structures to\nreduce interference among tasks, but in the end, all the approaches learn\ndifferent tasks in a joint vector space. We believe this invariably leads to\ninterference among different tasks. We propose to learn tasks in different\n(low-rank) vector subspaces that are kept orthogonal to each other in order to\nminimize interference. Further, to keep the gradients of different tasks coming\nfrom these subspaces orthogonal to each other, we learn isometric mappings by\nposing network training as an optimization problem over the Stiefel manifold.\nTo the best of our understanding, we report, for the first time, strong results\nover experience-replay baseline with and without memory on standard\nclassification benchmarks in continual learning. The code is made publicly\navailable.",
      "full_text": "Continual Learning in Low-rank Orthogonal Subspaces Arslan Chaudhry1, Naeemullah Khan1, Puneet K. Dokania1,2, Philip H. S. Torr1 University of Oxford1 & Five AI Ltd., UK2 arslan.chaudhry@eng.ox.ac.uk Abstract In continual learning (CL), a learner is faced with a sequence of tasks, arriving one after the other, and the goal is to remember all the tasks once the continual learning experience is ﬁnished. The prior art in CL uses episodic memory, parameter regularization or extensible network structures to reduce interference among tasks, but in the end, all the approaches learn different tasks in a joint vector space. We believe this invariably leads to interference among different tasks. We propose to learn tasks in different (low-rank) vector subspaces that are kept orthogonal to each other in order to minimize interference. Further, to keep the gradients of different tasks coming from these subspaces orthogonal to each other, we learn isometric mappings by posing network training as an optimization problem over the Stiefel manifold. To the best of our understanding, we report, for the ﬁrst time, strong results over experience-replay baseline with and without memory on standard classiﬁcation benchmarks in continual learning.1 1 Introduction In continual learning, a learner experiences a sequence of tasks with the objective to remember all or most of the observed tasks to speed up transfer of knowledge to future tasks. Learning from a diverse sequence of tasks is useful as it allows for the deployment of machine learning models that can quickly adapt to changes in the environment by leveraging past experiences. Contrary to the standard supervised learning setting, where only a single task is available, and where the learner can make several passes over the dataset of the task, the sequential arrival of multiple tasks poses unique challenges for continual learning. The chief one among which is catastrophic forgetting [McCloskey and Cohen, 1989], whereby the global update of model parameters on the present task interfere with the learned representations of past tasks. This results in the model forgetting the previously acquired knowledge. In neural networks, to reduce the deterioration of accumulated knowledge, existing approaches modify the network training broadly in three different ways. First, regularization-based approaches [Kirk- patrick et al., 2016, Zenke et al., 2017, Aljundi et al., 2018, Chaudhry et al., 2018, Nguyen et al., 2018] reduce the drift in network parameters that were important for solving previous tasks. Second, modular approaches [Rusu et al., 2016, Lee et al., 2017] add network components as new tasks arrive. These approaches rely on the knowledge of correct module selection at test time. Third, and perhaps the strongest, memory-based approaches [Lopez-Paz and Ranzato, 2017, Hayes et al., 2018, Isele and Cosgun, 2018, Riemer et al., 2019], maintain a small replay buffer, called episodic memory, and mitigate catastrophic forgetting by replaying the data in the buffer along with the new task data. One common feature among all the three categories is that, in the end, all the tasks are learned in the same 1Code: https://github.com/arslan-chaudhry/orthog_subspace 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:2010.11635v2  [cs.LG]  8 Dec 2020Stiefel Euclidean proj Figure 1: ORTHOG -SUBSPACE . Each blob, with the three ellipses, represents a vector space and its subspaces at a certain layer. The projection operator in the layer Lkeeps the subspaces orthogonal (no overlap). The overlap in the intermediate layers is minimized when the weight matrices are learned on the Stiefel manifold. vector space where a vector space is associated with the output of a hidden layer of the network. We believe this restriction invariably leads to forgetting of past tasks. In this work, we propose to learn different tasks in different vector subspaces. We require these subspaces to be orthogonal to each other in order to prevent the learning of a task from interfering catastrophically with previous tasks. More speciﬁcally, for a point in the vector space inRm, typically the second last layer of the network, we project each task to a low-dimensional subspace by a task-speciﬁc projection matrix P ∈Rm×m, whose rank is r, where r≪m. The projection matrices are generated ofﬂine such that for different tasks they are mutually orthogonal. This simple projection in the second last layer reduces the forgetting considerably in the shallower networks – the average accuracy increases by up to 13% and forgetting drops by up to 66% compared to the strongest experience replay baseline [Chaudhry et al., 2019b] in a three-layer network. However, in deeper networks, the backpropagation of gradients from the different projections of the second last layer do not remain orthogonal to each other in the earlier layers resulting in interference in those layers. To reduce the interference, we use the fact that a gradient on an earlier layer is a transformed version of the gradient received at the projected layer – where the transformation is linear and consists of the product of the weight matrix and the diagonal Jacobian matrix of the non-linearity of the layers in between. Reducing interference then requires this transformation to be an inner-product preserving transformation, such that, if two vectors are orthogonal at the projected layer, they remain close to orthogonal after the transformation. This is equivalent to learning orthonormal weight matrices – a well-studied problem of learning on Stiefel manifolds [Absil et al., 2009, Bonnabel, 2013]. Our approach, dubbed ORTHOG -SUBSPACE , generates two projected orthogonal vectors (gradients) – one for the current task and another for one of the previous tasks whose data is stored in a tiny replay buffer – and updates the network weights such that the weights remain on a Stiefel manifold. We visually describe our approach in Fig. 1. For the same amount of episodic memory, ORTHOG -SUBSPACE , improves upon the strong experience replay baseline by8% in average accuracy and50% in forgetting on deeper networks. 2 Background In this section, we describe the continual learning setup followed by necessary preliminaries for our approach. 2.1 Continual Learning Setup We assume a continual learner experiencing a stream of data triplets(xi,yi,ti) containing an input xi, a target yi, and a task identiﬁer ti ∈T = {1,...,T }. Each input-target pair (xi,yi) ∈X×Y ti is an identical and independently distributed example drawn from some unknown distribution Pti(X,Y ), representing the ti-th learning task. We assume that the tasks are experienced in order ti ≤tj for all i ≤j, and the learner cannot store any but a few samples from Pti in a tiny replay buffer Mi. Under this setup, our goal is to estimate a predictor f = (w◦Φ) : X×T →Y , composed of a 2feature extractor ΦΘ : X→H , which is an L-layer feed-forward neural network parameterized by Θ = {Wl}L l=1, and a classiﬁer wθ : H→Y , that minimizes the multi-task error 1 T T∑ t=1 E(x,y)∼Pt [ ℓ(f(x,t),y) ], (1) where H∈ Rm is an inner product space, Y= ∪t∈TYt, and ℓ: Y×Y→ R≥0 is a loss function. To further comply with the strict sequential setting, similar to prior work [Lopez-Paz and Ranzato, 2017, Riemer et al., 2019], we consider streams of data that areexperienced only once. We only focus on classiﬁcation tasks where either the input or output distribution changes over time. We assume that a task descriptor, identifying the correct classiﬁcation head, is given at both train and test times. Metrics Once the continual learning experience is ﬁnished, we measure two statistics to evaluate the quality of the algorithm: average accuracy, and average maximum forgetting. First, the average accuracy is deﬁned as Accuracy = 1 T T∑ j=1 aT,j, (2) where ai,j denotes the test accuracy on taskjafter the model has ﬁnished experiencing taski. Second, the average maximum forgetting is deﬁned as Forgetting = 1 T −1 T−1∑ j=1 max l∈{1,...,T−1} (al,j −aT,j), (3) that is, the decrease in performance for each of the tasks between their peak accuracy and their accuracy after the continual learning experience is ﬁnished. 2.2 Preliminaries Let the inner product in Hbe denoted by ⟨·,·⟩, and v be an element of H. A matrix O ∈Rm×r, where r ≪m, parameterizes an m×m dimensional orthogonal projection matrix P, given by P = O(O⊤O)−1O⊤, where rank(P) = r. A vector u = Pv, will be the projection of v in a subspace U⊂H with dim(U) = r. Furthermore, if the columns of Oare assumed to be orthonormal, then the projection matrix is simpliﬁed to P = OO⊤. Deﬁnition 2.1 (Orthogonal Subspace). Subspaces Uand Wof a vector space Hare orthogonal if ⟨u,w⟩= 0, ∀u∈U,w ∈W. Deﬁnition 2.2 (Isometry). A linear transformation T : V→V is called an isometry if it is distance preserving i.e. ∥T(v) −T(w)∥= ∥v−w∥, ∀v,w ∈V. A linear transformation that preserves distance must preserve angles and vice versa. We record this in the following theorem. Theorem 2.1. T is an isometry iff it preserves inner products. The proof is fairly standard and given in Appendix Appendix A. Corollary 2.1.1. If T1 and T2 are two isometries then their composition T1 ◦T2 is also an isometry. An orthogonal matrix preserves inner products and therefore acts as an isometry of Euclidean space. Enforcing orthogonality1 during network training corresponds to solving the following constrained 1Note, an orthogonal matrix is always square. However, the matrices we consider can be nonsquare. In this work, the orthogonal matrix is used in the sense of W⊤W = I. 3Stiefel Manifold Tangent plane at Wt Wt Wt+1 Gradient of loss function at Wt Projection of the gradient onto the tangent plane Retraction to the manifold  Figure 2: Gradient computed at a given point (Wt) in the manifold is ﬁrst projected to the tangent plane. There exists a closed form for this step. This projected gradient is then retracted to a point in the manifold giving the ﬁnal update Wt+1. optimization problem: min θ,Θ={Wl,bl}L l=1 ℓ(f(x,t),y), s.t. W⊤ l Wl = I, ∀l∈{1,··· ,L}, (4) where I is an identity matrix of appropriate dimensions. The solution set of the above problem is a valid Riemannian manifold when the inner product is deﬁned. It is called the Stiefel manifold, deﬁned as ¯Ml = {Wl ∈Rnl×nl−1 |W⊤ l Wl = I}, where nl is the number of neurons in layer l, and it is assumed that nl ≥nl−1. For most of the neural network architectures, this assumption holds. For a convolutional layer Wl ∈Rcout×cin×h×w, we reshape it to Wl ∈Rcout×(cin·h·w). The optimization of a differentiable cost function over a Stiefel manifold has been extensively studied in literature [Absil et al., 2009, Bonnabel, 2013]. Here, we brieﬂy summarize the two main steps of the optimization process and refer the reader to Absil et al. [2009] for further details. For a given point Wl on the Stiefel manifold, let TWl represent the tangent space at that point. Further, let gl, a matrix, be the gradient of the loss function with respect to Wl. The ﬁrst step of optimization projects gl to TWl using a closed form ProjTWl (gl) = AWl, where ‘A’ is a skew-symmetric matrix given by (see Appendix B for the derivation): A= glW⊤ l −Wlg⊤ l . (5) Once the gradient projection in the tangent space is found, the second step is to generate a descent curve of the loss function in the manifold. The Cayley transform deﬁnes one such curve using a parameter τ ≥0, specifying the length of the curve, and a skew-symmetric matrix U [Nishimori and Akaho, 2005]: Y(τ) = ( I+ τ 2 U )−1( I−τ 2 U ) Wl, (6) It can be seen that the curve stays on the Stiefel manifold i.e. Y(τ)⊤Y(τ) = I and Y(0) = Wl, and that its tangent vector at τ = 0 is Y′(0) = −UWl. By setting U = A= glW⊤ l −Wlg⊤ l , the curve will be a descent curve for the loss function. Li et al. [2020] showed that one can bypass the expensive matrix inversion in (6) by following the ﬁxed-point iteration of the Cayley transform, Y(τ) = Wl −τ 2 A(Wl + Y(τ)). (7) Li et al. [2020] further showed that under some mild continuity assumptions (7) converges to the closed form (6) faster than other approximation algorithms. The overall optimization on Stiefel manifold is shown in 2. 3 Continual Learning in Orthogonal Subspaces We now describe our continual learning approach. Consider a feature extractor in the form of a feed-forward neural network consisting of Lhidden layers, that takes an input x∈Rd and passes it through the following recursions: hl = σ(Wlhl−1 + bl), where σ(·) is a non-linearity, h0 = x, and 4hL = φ∈Rm. The network is followed by an application-speciﬁc head ( e.g.) a classiﬁer in case of a classiﬁcation task. The network can be thought of as a mapping, Φ : X→H , from one vector space (X∈ Rd) to another (H∈ Rm). When the network is trained for more than one tasks, a shared vector space Hcan be learned if the model has a simultaneous access to all the tasks. In continual learning, on the other hand, when tasks arrive in a sequence, learning a new task can interfere in the space where a previous task was learned. This can result in the catastrophic forgetting of the previous task if the new task is different from the previous one(s). In this work, we propose to learn tasks in orthogonal subspaces such that learning of a new task has minimal interference with already learned tasks. We assume that the network is sufﬁciently parameterized, which often is the case with deep networks, so that all the tasks can be learned in independent subspaces. We deﬁne a family of sets Vthat partitions H, such that, a) Vdoes not contain the empty set ( ∅ /∈ V), b) the union of sets in Vis equal to H(∪Vt∈VVt = H), and c) the intersection of any two distinct sets in Vis empty ((∀Vi,Vj ∈V) i̸= j =⇒ Vi ∩Vj = ∅)). A set Vt ∈V deﬁnes a subspace for task t. We obtain such a subspace by projecting the feature map φ= hL ∈Rm into an r-dimensional space, where r ≪m, via a projection matrix Pt ∈Rm×m of rank r, i.e. we obtain the features for task tby φt = PthL, while ensuring: P⊤ t Pt = I, P⊤ t Pk = 0, ∀k̸= t. (8) The said projection matrix can be easily constructed by ﬁrst generating a set ofmrandom orthonormal basis1 in Rm, then picking r = ⌊m/T⌋of those basis (matrix columns) to form a matrix Ot, and, ﬁnally, obtaining the projections as Pt = OtO⊤ t . For different tasks these basis form a disjoint set P= {P1,··· ,PT}. If the total number of tasks exceeds T, then one can potentially dynamically resize the m×morthogonal matrix while maintaining the required properties. For example, to make space for 2T tasks one can resize the original matrix to 2m×2mwith zero padding, and backup the previous matrix. This would entail dynamically resizing the second last layer of the network. The set Pcan be computed ofﬂine and stored in a hash table that can be readily fetched given a task identiﬁer. The projection only adds a single matrix multiplication in the forward pass of the network making it as efﬁcient as standard training. Next, lets examine the effect of the projection step on the backward pass of the backpropagation algorithm. For a task t, the gradient of the objective ℓ(·,·) on any intermediate layer hl can be decomposed using the chain rule as, gt l = ∂ℓ ∂hl = ( ∂ℓ ∂hL )∂hL ∂hl = (∂φt ∂hL ∂ℓ ∂φt )∂hL ∂hl , = ( Pt ∂ℓ ∂φt )L−1∏ k=l ∂hk+1 ∂hk = gt L L−1∏ k=l Dk+1Wk+1, (9) where Dk+1 is a diagonal matrix representing the Jacobian of the pointwise nonlinearity σk+1(·). For a ReLU nonlinearity and assuming that the non-linearity remains in the linear region during the training [Serra et al., 2017, Arora et al., 2019], we assume the Jacobian matrix to be an identity. It can be seen that for the projected layer L(the second last layer), the gradients of different tasks are orthogonal by construction i.e. gt L ⊥gk̸=t L (c.f . (8)). Hence the gradient interference will be zero at the layer L. However, according to (9), as the gradients are backpropogated to the previous layers they start to become less and less orthogonal (c.f . Fig. 4). This results in interference among different tasks in earlier layers, especially when the network is relatively deep. Let us rewrite the gradients at the intermediate layer l during the training of task t as a linear transformation of the gradient at the layer Li.e. gt l = T(gt L). According to (9), and assuming the Jacobian matrix of the non-linearity to be identity (Dk = I), this transformation is given by T(u) = u L−1∏ k=l Wk+1. (10) 1We generate a random matrix and apply the Gram–Schmidt process ofﬂine, before the continual learning experience begins. 5Algorithm 1 Training of ORTHOG -SUBSPACE on sequential data D= {D1,··· ,DT}, with Θ = {Wl}L l=1 initialized as orthonormalized matrices, P= {P1,··· ,PT}orthogonal projections, learning rate ‘α’,s= 2, q= 0.5, ϵ= 10−8. 1: procedure ORTHOG -SUBSPACE (D,P,α,s,q,ϵ ) 2: M←{}∗ T 3: for t∈{1,··· ,T}do 4: for (xt,yt) ∼Dt do 5: k∼{1,··· ,t −1} ⊿Sample a past task from the replay buffer 6: (xk,yk) ∼Mk ⊿Sample data from the episodic memory 7: gt ←∇Θ,θ(ℓ(f(xt,yt),Pt)) ⊿Compute gradient on the current task 8: gk ←∇Θ,θ(ℓ(f(xk,yk),Pk)) ⊿Compute gradient on the past task 9: g←gt + gk 10: for l= {1,··· ,L}do ⊿Layer-wise update on Stiefel manifold 11: A←glW⊤ l −Wlg⊤ l 12: U ←AWl ⊿Project the gradient onto the tangent space 13: τ ←min(α,2q/(||Wl||+ ϵ)) ⊿Select adaptive learning rate Li et al. [2020] 14: Y0 ←Wl −τU ⊿ Iterative estimation of the Cayley Transform 15: for i= {1,··· ,s}do 16: Yi ←Wl −τ 2 A(Wl + Yi−1) 17: end for 18: Wl ←Ys 19: end for 20: θ←θ−α·gL+1 ⊿Update the classiﬁer head 21: Mt ←(xt,yt) ⊿Add the sample to a ring buffer 22: end for 23: end for 24: return Θ,θ 25: end procedure As noted earlier, gt L ⊥gk̸=t L by construction, then to reduce the interference between any gt l and gk̸=t l , the transformation T(·) in (10) needs to be such that it preserves the inner-product between T(gt L) and T(gk̸=t L ). In other words, T(·) needs to be an isometry 2.2. As discussed in Sec. 2.2, this is equivalent to ensuring that weight matrices {Wl}L l=1 are orthonormal matrices. We learn orthonormal weights of a neural network by posing the network training as an optimization problem over a Stiefel manifold [Absil et al., 2009, Bonnabel, 2013]. More speciﬁcally, the network is initialized from random orthonormal matrices. A tiny replay buffer, storing the examples of past tasks (k < t), is maintained to compute the gradients {gk l }L l=1. The gradients on the current task t, {gt l}L l=1, are computed and weights in each layer lare updated as follows: a) ﬁrst the effective gradient gl = gt l + gk l is projected onto the tangent space at the current estimate of the weight matrix Wl, b) the iterative Cayley Transform (7) is used to retract the update to the Stiefel manifold. The projection onto the tangent space is carried out using the closed-form described in (5). The resulting algorithm keeps the network weights orthonormal throughout the continual learning experience while reducing the loss using the projected gradients. Fig. 3 shows this orthonormality reduces the inner product between the gradients of different tasks. We denote our approach as ORTHOG -SUBSPACE and provide the pseudo-code in Alg. 1. 4 Experiments We now report experiments on continual learning benchmarks in classiﬁcation tasks. 4.1 Continual Learning Benchmarks We evaluateaverage accuracy (2) and forgetting (3) on four supervised classiﬁcation benchmarks. Permuted MNIST is a variant of the MNIST dataset of handwritten digits [LeCun, 1998] where each task applies a ﬁxed random pixel permutation to the original dataset. Rotated MNIST is another variant of MNIST, where each task applies a ﬁxed random image rotation (between0 and 180 degrees) to the original dataset. Both of the MNIST benchmark contain 23 tasks, each with 10000 samples from 10 different classes. Split CIFAR is a variant of the CIFAR-100 dataset [Krizhevsky 6and Hinton, 2009, Zenke et al., 2017], where each task contains the data pertaining 5 random classes (without replacement) out of the total 100 classes. Split miniImageNet is a variant of the ImageNet dataset [Russakovsky et al., 2015, Vinyals et al., 2016], containing a subset of images and classes from the original dataset. Similar to Split CIFAR, in Split miniImageNet each task contains the data from 5 random classes (without replacement) out of the total 100 classes. Both CIFAR-100 and miniImageNet contain 20 tasks, each with 250 samples from each of the 5 classes. Similar to Chaudhry et al. [2019a], for each benchmark, the ﬁrst 3 tasks are used for hyper-parameter tuning (grids are available in Appendix D). The learner can perform multiple passes over the datasets of these three initial tasks. We assume that the continual learning experience begins after these initial tasks and ignore them in the ﬁnal evaluation. 4.2 Baselines We compare ORTHOG -SUBSPACE against several baselines which we describe next. Finetune is a vanilla model trained on a data stream, without any regularization or episodic memory. ICARL [Rebufﬁ et al., 2017] is a memory-based method that uses knowledge-distillation [Hinton et al., 2014] and episodic memory to reduce forgetting. EWC [Kirkpatrick et al., 2016] is a regularization-based method that uses the Fisher Information matrix to record the parameter importance. VCL [Nguyen et al., 2018] is another regularization-based method that uses variational inference to approximate the posterior distribution of the parameters which is regularized during the continual learning experience. AGEM [Chaudhry et al., 2019a] is a memory-based method similar to [Lopez-Paz and Ranzato, 2017] that uses episodic memory as an optimization constraint to reduce forgetting on previous tasks. MER [Riemer et al., 2019] is another memory-based method that uses ﬁrst-order meta-learning formulation [Nichol and Schulman, 2018] to reduce forgetting on previous tasks.ER-Ring [Chaudhry et al., 2019b] is the strongest memory-based method that jointly trains new task data with that of the previous tasks. Finally, Multitask is an oracle baseline that has access to all data to optimize (1). It is useful to estimate an upper bound on the obtainable Accuracy (2). 4.3 Code, Architecture and Training Details Except for VCL, all baselines use the same uniﬁed code base which is made publicly available. For VCL [Nguyen et al., 2018], the ofﬁcial implementation is used which only works on fully-connected networks. All baselines use the same neural network architectures: a fully-connected network with two hidden layers of 256 ReLU neurons in the MNIST experiments, and a standard ResNet18 [He et al., 2016] in CIFAR and ImageNet experiments. All baselines do a single-pass over the dataset of a task, except for episodic memory that can be replayed multiple times. The task identiﬁers are used to select the correct output head in the CIFAR and ImageNet experiments. Batch size is set to 10 across experiments and models. A tiny ring memory of 1 example per class per task is stored for the memory-based methods. For ORTHOG -SUBSPACE , episodic memory is not used for MNIST experiments, and the same amount of memory as other baselines is used for CIFAR100 and miniImageNet experiments. All experiments run for ﬁve different random seeds, each corresponding to a different dataset ordering among tasks, that are ﬁxed across baselines. Averages and standard deviations are reported across these runs. 4.4 Results Tab. 1 shows the overall results on all benchmarks. First, we observe that on relatively shallower networks (MNIST benchmarks), even without memory and preservation of orthogonality during the network training, ORTHOG -SUBSPACE outperform the strong memory-based baselines by a large margin: +7.1% and +9.2% absolute gain in average accuracy, 66% and 42% reduction in forgetting compared to the strongest baseline (ER-Ring), on Permuted and Rotated MNIST, respectively. This shows that learning in orthogonal subspaces is an effective strategy in reducing interference among different tasks. Second, for deeper networks, when memory is used and orthogonality is preserved, ORTHOG -SUBSPACE improves upon ER-Ring considerably: 4.7% and 1.6% absolute gain in average accuracy, 50% and 16.6% reduction in forgetting, on CIFAR100 and miniImageNet, respectively. While we focus on tiny episodic memory, in Tab. 3 of Appendix C, we provide results for larger episodic memory sizes. Our conclusions on tiny memory hold, however, the gap between the performance of ER-Ring and ORTHOG -SUBSPACE gets reduced as the episodic memory size is increased. The network can sufﬁciently mitigate forgetting by relearning on a large episodic memory. 7Table 1: Accuracy (2) and Forgetting (3) results of continual learning experiments. When used, episodic memories contain up to one example per class per task. Last row is a multi-task oracle baseline. METHOD PERMUTEDMNIST R OTATEDMNIST MEMORY ACCURACY FORGETTING ACCURACY FORGETTING FINETUNE \u0017 50.6 (±2.57) 0.44 (±0.02) 43.1 (±1.20) 0.55 (±0.01) EWC [KIRKPATRICK ET AL., 2016] \u0017 68.4 (±0.76) 0.25 (±0.01) 43.6 (±0.81) 0.53 (±0.01) VCL [NGUYEN ET AL., 2018] \u0017 51.8 (±1.54) 0.44 (±0.01) 48.2 (±0.99) 0.50 (±0.01) VCL-RANDOM[NGUYEN ET AL., 2018] \u0013 52.3 (±0.66) 0.43 (±0.01) 54.4 (±1.44) 0.44 (±0.01) AGEM [CHAUDHRY ET AL., 2019A] \u0013 78.3 (±0.42) 0.15 (±0.01) 60.5 (±1.77) 0.36 (±0.01) MER [RIEMER ET AL., 2019] \u0013 78.6 (±0.84) 0.15 (±0.01) 68.7 (±0.38) 0.28 (±0.01) ER-RING [CHAUDHRY ET AL., 2019B] \u0013 79.5 (±0.31) 0.12 (±0.01) 70.9 (±0.38) 0.24 (±0.01) ORTHOG-SUBSPACE(OURS) \u0017 86.6 (±0.91) 0.04 (±0.01) 80.1 (±0.95) 0.14 (±0.01) MULTITASK 91.3 0.0 94.3 0.0 METHOD SPLIT CIFAR S PLIT MINIIMAGENET MEMORY ACCURACY FORGETTING ACCURACY FORGETTING FINETUNE \u0017 42.6 (±2.72) 0.27 (±0.02) 36.1 (±1.31) 0.24 (±0.03) EWC [KIRKPATRICK ET AL., 2016] \u0017 43.2 (±2.77) 0.26 (±0.02) 34.8 (±2.34) 0.24 (±0.04) ICARL [REBUFFI ET AL., 2017] \u0013 46.4 (±1.21) 0.16 (±0.01) - - AGEM [CHAUDHRY ET AL., 2019A] \u0013 51.3 (±3.49) 0.18 (±0.03) 42.3 (±1.42) 0.17 (±0.01) MER [RIEMER ET AL., 2019] \u0013 49.7 (±2.97) 0.19 (±0.03) 45.5 (±1.49) 0.15 (±0.01) ER-RING [CHAUDHRY ET AL., 2019B] \u0013 59.6 (±1.19) 0.14 (±0.01) 49.8 (±2.92) 0.12 (±0.01) ORTHOG-SUBSPACE(OURS) \u0013 64.3 (±0.59) 0.07 (±0.01) 51.4 (±1.44) 0.10 (±0.01) MULTITASK 71.0 0.0 65.1 0.0 Table 2: Systematic evaluation of Projection, Memory and Orthogonalization in ORTHOG -SUBSPACE . METHOD SPLIT CIFAR S PLIT MINI IMAGE NET PROJECTION ER S TIEFEL ACCURACY FORGETTING ACCURACY FORGETTING \u0013 \u0017 \u0017 50.3 (±2.21) 0.21 (±0.02) 40.1 (±2.16) 0.20 (±0.02) \u0017 \u0013 \u0017 59.6 (±1.19) 0.14 (±0.01) 49.8 (±2.92) 0.12 (±0.01) \u0013 \u0013 \u0017 61.2 (±1.84) 0.10 (±0.01) 49.5 (±2.21) 0.11 (±0.01) \u0013 \u0013 \u0013 64.3 (±0.59) 0.07 (±0.01) 51.4 (±1.44) 0.10 (±0.01) Tab. 2 shows a systematic evaluation of projection(8), episodic memory and orthogonalization (7) in ORTHOG -SUBSPACE . First, without memory and orthogonalization, while a simple projection yields competitive results compared to various baselines (c.f . Tab. 1), the performance is still a far cry from ER-Ring. However, when the memory is used along with subspace projection one can already see a better performance compared to ER-Ring. Lastly, when the orthogonality is ensured by learning on a Stiefel manifold, the model achieves the best performance both in terms of accuracy and forgetting. Finally, Fig. 3 shows the distribution of the inner product of gradients between the current and previous tasks, stored in the episodic memory. Everything is kept the same except in one case the weight matrices are learned on the Stiefel manifold while in the other no such constraint is placed on the weights. We observe that when the weights remain on the Stiefel manifold, the distribution is more peaky around zero. This empirically validates our hypothesis that by keeping the transformation (10) isometric, the gradients of different tasks remain near orthogonal to one another in all the layers. 5 Related work In continual learning [Ring, 1997], also called lifelong learning [Thrun, 1998], a learner faces a sequence of tasks without storing the complete datasets of these tasks. This is in contrast to multitask learning [Caruana, 1997], where the learner can simultaneously access data from all tasks. The objective in continual learning is to avoid catastrophic forgetting The main challenge in continual learning is to avoid catastrophic forgetting [McCloskey and Cohen, 1989, McClelland et al., 1995, Goodfellow et al., 2013] on already seen tasks so that the learner is able to learn new tasks quickly. Existing literature in continual learning can be broadly categorized into three categories. 80.00 0.05 0.10 0.15 0.20 Inner product b\\w Task and Memory gradients 0 100 200 300Count No Orthogonality Stiefel (Orthogonality) Layer 15 0.0 0.1 0.2 0.3 0.4 Inner product b\\w Task and Memory gradients 0 50 100 150 200Count No Orthogonality Stiefel (Orthogonality) Layer 10 Figure 3: Histogram of inner product of current task and memory gradients in different layers in Split CIFAR. The more left the distribution is the more orthogonal the gradients are and the less the interference is between the current and previous tasks. First, regularization approachesreduce the drift in parameters important for past tasks [Kirkpatrick et al., 2016, Aljundi et al., 2018, Nguyen et al., 2018, Zenke et al., 2017]. For the large number of tasks, the parameter importance measures suffer from brittleness as the locality assumption embedded in the regularization-based approaches is violated [Titsias et al., 2019]. Furthermore, Chaudhry et al. [2019a] showed that these approaches can only be effective when the learner can perform multiple passes over the datasets of each task – a scenario not assumed in this work. Second,modular approaches use different network modules that can be extended for each new task [Fernando et al., 2017, Aljundi et al., 2017, Rosenbaum et al., 2018, Chang et al., 2018, Xu and Zhu, 2018, Ferran Alet, 2018]. By construction, modular approaches have zero forgetting, but their memory requirements increase with the number of tasks [Rusu et al., 2016, Lee et al., 2017]. Third, memory approaches maintain and replay a small episodic memory of data from past tasks. In some of these methods [Li and Hoiem, 2016, Rebufﬁ et al., 2017], examples in the episodic memory are replayed and predictions are kept invariant by means of distillation [Hinton et al., 2014]. In other approaches [Lopez-Paz and Ranzato, 2017, Chaudhry et al., 2019a, Aljundi et al., 2019] the episodic memory is used as an optimization constraint that discourages increases in loss at past tasks. Some works [Hayes et al., 2018, Riemer et al., 2019, Rolnick et al., 2018, Chaudhry et al., 2019b, 2020] have shown that directly optimizing the loss on the episodic memory, also known as experience replay, is cheaper than constraint-based approaches and improves prediction performance. Recently, Prabhu et al. [2020] showed that training at test time, using a greedily balanced collection of episodic memory, improved performance on a variety of benchmarks. Similarly, Javed and White [2019], Beaulieu et al. [2020] showed that learning transferable representations via meta-learning reduces forgetting when the model is trained on sequential tasks. Similar in spirit to our work is OGD Farajtabar et al. [2019] where the gradients of each task are learned in the orthogonal space of all the previous tasks’ gradients. However, OGD differs signiﬁcantly from our work in terms of memory and compute requirements. Unlike OGD, where the memory of previous task gradients is maintained, which is equivalent to storing nt ×Sdimensional matrix for each task, where nt are the number of examples in each task and Sis the network size, we only store m×rdimensional matrix Ot, where mis the dimension of the feature vector (m≪S) and ris the rank of the subspace, and a tiny replay buffer for each task. For large network sizes, OGD is impractical to use. Furthermore, at each training step OGD subtracts the gradient projections from the space spanned by the gradients in memory, whereas we only project the feature extraction layer to a subspace and maintain orthogonality via learning on Stiefel manifolds. Finally, learning orthonormal weight matrices has been extensively studied in literature. Orthogonal matrices are used in RNNs for avoiding exploding/ vanishing gradient problem [Arjovsky et al., 2016, Wisdom et al., 2016, Jing et al., 2017]. While the weight matrices are assumed to be square in the earlier works, works including [Huang et al., 2018] considered learning non-square orthogonal matrices (called orthonormal matrices in this work) by optimizing over the Stiefel manifolds. More recently, Li et al. [2020] proposed an iterative version of Cayley transform [Nishimori and Akaho, 2005], a key component in optimizing over Stiefel manifolds. Whereas optimizing over the Stiefel manifold ensure strict orthogonality in the weights, [Jia et al., 2019] proposed an algorithm, Singular Value Bounding (SVB), for soft orthogonality constraints. We use strict orthogonality in this work and leave the exploration of soft orthogonality for future research. 96 Conclusion We presented ORTHOG -SUBSPACE , a continual learning method that learns different tasks in orthogo- nal subspaces. The gradients in the projected layer are kept orthogonal in earlier layers by learning isometric transformations. The isometric transformations are learned by posing the network training as an optimization problem over the Stiefel manifold. The proposed approach improved considerably over strong memory replay-based baselines in standard continual learning benchmarks of image classiﬁcation. 7 Broader Impact Continual learning methods like the one we propose allow machine learning models to efﬁciently learn on new data without requiring constant retraining on previous data. This type of learning can be useful when the model is expected to perform in multiple environments and a simultaneous retraining on all the environments is not feasible. However, one of the core assumptions in continual learning is that model should have zero forgetting on previous data. In some scenarios, partially forgetting old data may be acceptable or even preferable, for example if older data was more biased (in any sense) than more recent data. A machine learning practitioner should be aware of this fact and use continual learning approaches only when suitable. Acknowledgment This work was supported by EPSRC/MURI grant EP/N019474/1, Facebook (DeepFakes grant), Five AI UK, and the Royal Academy of Engineering under the Research Chair and Senior Research Fellowships scheme. AC is funded by the Amazon Research Award (ARA) program. References P.-A. Absil, R. Mahony, and R. Sepulchre. Optimization algorithms on matrix manifolds. Princeton University Press, 2009. R. Aljundi, P. Chakravarty, and T. Tuytelaars. Expert gate: Lifelong learning with a network of experts. In CVPR, pages 7120–7129, 2017. R. Aljundi, F. Babiloni, M. Elhoseiny, M. Rohrbach, and T. Tuytelaars. Memory aware synapses: Learning what (not) to forget. In ECCV, 2018. R. Aljundi, M. Lin, B. Goujaud, and Y . Bengio. Online continual learning with no task boundaries. arXiv preprint arXiv:1903.08671, 2019. M. Arjovsky, A. Shah, and Y . Bengio. Unitary evolution recurrent neural networks. InInternational Conference on Machine Learning, pages 1120–1128, 2016. S. Arora, S. S. Du, W. Hu, Z. Li, and R. Wang. Fine-grained analysis of optimization and gen- eralization for overparameterized two-layer neural networks. arXiv preprint arXiv:1901.08584, 2019. S. Beaulieu, L. Frati, T. Miconi, J. Lehman, K. O. Stanley, J. Clune, and N. Cheney. Learning to continually learn. arXiv preprint arXiv:2002.09571, 2020. S. Bonnabel. Stochastic gradient descent on riemannian manifolds. IEEE Transactions on Automatic Control, 58(9):2217–2229, 2013. R. Caruana. Multitask learning. Machine learning, 28(1):41–75, 1997. M. Chang, A. Gupta, S. Levine, and T. L. Grifﬁths. Automatically composing representation transformations as a means for generalization. In ICML workshop Neural Abstract Machines and Program Induction v2, 2018. A. Chaudhry, P. K. Dokania, T. Ajanthan, and P. H. Torr. Riemannian walk for incremental learning: Understanding forgetting and intransigence. In ECCV, 2018. 10A. Chaudhry, M. Ranzato, M. Rohrbach, and M. Elhoseiny. Efﬁcient lifelong learning with a-gem. In ICLR, 2019a. A. Chaudhry, M. Rohrbach, M. Elhoseiny, T. Ajanthan, P. K. Dokania, P. H. Torr, and M. Ranzato. Continual learning with tiny episodic memories. arXiv preprint arXiv:1902.10486, 2019b. A. Chaudhry, A. Gordo, P. K. Dokania, P. Torr, and D. Lopez-Paz. Using hindsight to anchor past knowledge in continual learning. arXiv preprint arXiv:2002.08165, 2020. M. Farajtabar, N. Azizan, A. Mott, and A. Li. Orthogonal gradient descent for continual learning. arXiv preprint arXiv:1910.07104, 2019. C. Fernando, D. Banarse, C. Blundell, Y . Zwols, D. Ha, A. A. Rusu, A. Pritzel, and D. Wier- stra. Pathnet: Evolution channels gradient descent in super neural networks. arXiv preprint arXiv:1701.08734, 2017. L. P. K. Ferran Alet, Tomas Lozano-Perez. Modular meta-learning. arXiv preprint arXiv:1806.10166v1, 2018. I. J. Goodfellow, M. Mirza, D. Xiao, A. Courville, and Y . Bengio. An empirical investigation of catastrophic forgetting in gradient-based neural networks. arXiv preprint arXiv:1312.6211, 2013. T. L. Hayes, N. D. Cahill, and C. Kanan. Memory efﬁcient experience replay for streaming learning. arXiv preprint arXiv:1809.05922, 2018. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, pages 770–778, 2016. G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. In NIPS, 2014. L. Huang, X. Liu, B. Lang, A. W. Yu, Y . Wang, and B. Li. Orthogonal weight normalization: Solution to optimization over multiple dependent stiefel manifolds in deep neural networks. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018. D. Isele and A. Cosgun. Selective experience replay for lifelong learning. arXiv preprint arXiv:1802.10269, 2018. K. Javed and M. White. Meta-learning representations for continual learning. In Advances in Neural Information Processing Systems, pages 1820–1830, 2019. K. Jia, S. Li, Y . Wen, T. Liu, and D. Tao. Orthogonal deep neural networks.IEEE transactions on pattern analysis and machine intelligence, 2019. L. Jing, Y . Shen, T. Dubcek, J. Peurifoy, S. Skirlo, Y . LeCun, M. Tegmark, and M. Soljaˇci´c. Tunable efﬁcient unitary neural networks (eunn) and their application to rnns. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1733–1741. JMLR. org, 2017. J. Kirkpatrick, R. Pascanu, N. C. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, and R. Hadsell. Overcoming catastrophic forgetting in neural networks. PNAS, 2016. A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. https://www.cs.toronto.edu/ kriz/cifar.html, 2009. Y . LeCun. The mnist database of handwritten digits.http://yann.lecun.com/exdb/mnist/, 1998. J. Lee, J. Yun, S. Hwang, and E. Yang. Lifelong learning with dynamically expandable networks. arXiv preprint arXiv:1708.01547, 2017. J. Li, F. Li, and S. Todorovic. Efﬁcient riemannian optimization on the stiefel manifold via the cayley transform. In International Conference on Learning Representations, 2020. Z. Li and D. Hoiem. Learning without forgetting. In ECCV, pages 614–629, 2016. D. Lopez-Paz and M. Ranzato. Gradient episodic memory for continuum learning. In NIPS, 2017. 11J. L. McClelland, B. L. McNaughton, and R. C. O’Reilly. Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory. Psychological review, 102(3):419, 1995. M. McCloskey and N. J. Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. Psychology of learning and motivation, 24:109–165, 1989. C. V . Nguyen, Y . Li, T. D. Bui, and R. E. Turner. Variational continual learning.ICLR, 2018. A. Nichol and J. Schulman. Reptile: a scalable metalearning algorithm. arXiv preprint arXiv:1803.02999, 2018, 2018. Y . Nishimori and S. Akaho. Learning algorithms utilizing quasi-geodesic ﬂows on the stiefel manifold. Neurocomputing, 67:106–135, 2005. A. Prabhu, P. H. S. Torr, and P. K. Dokania. GDumb: A simple approach that questions our progress in continual learning. In ECCV, 2020. S.-V . Rebufﬁ, A. Kolesnikov, and C. H. Lampert. iCaRL: Incremental classiﬁer and representation learning. In CVPR, 2017. M. Riemer, I. Cases, R. Ajemian, M. Liu, I. Rish, Y . Tu, and G. Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interference. In ICLR, 2019. M. B. Ring. Child: A ﬁrst step towards continual learning. Machine Learning, 28(1):77–104, 1997. D. Rolnick, A. Ahuja, J. Schwarz, T. P. Lillicrap, and G. Wayne. Experience replay for continual learning. CoRR, abs/1811.11682, 2018. URL http://arxiv.org/abs/1811.11682. C. Rosenbaum, T. Klinger, and M. Riemer. Routing networks: Adaptive selection of non-linear functions for multi-task learning. In ICLR, 2018. O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. IJCV, 115(3):211–252, 2015. A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, and R. Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016. T. Serra, C. Tjandraatmadja, and S. Ramalingam. Bounding and counting linear regions of deep neural networks. arXiv preprint arXiv:1711.02114, 2017. H. D. Tagare. Notes on optimization on stiefel manifolds. In Technical report, Technical report. Yale University, 2011. S. Thrun. Lifelong learning algorithms. In Learning to learn, pages 181–209. Springer, 1998. M. K. Titsias, J. Schwarz, A. G. d. G. Matthews, R. Pascanu, and Y . W. Teh. Functional regularisation for continual learning using gaussian processes. arXiv preprint arXiv:1901.11356, 2019. O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra, et al. Matching networks for one shot learning. In NIPS, pages 3630–3638, 2016. S. Wisdom, T. Powers, J. Hershey, J. Le Roux, and L. Atlas. Full-capacity unitary recurrent neural networks. In Advances in neural information processing systems, pages 4880–4888, 2016. J. Xu and Z. Zhu. Reinforced continual learning. In arXiv preprint arXiv:1805.12369v1, 2018. F. Zenke, B. Poole, and S. Ganguli. Continual learning through synaptic intelligence. In ICML, 2017. 12Appendix Section A provides a proof that isometry preserves angles. Section B derives the closed-form of the gradient projection on the tangent space at a point in the Stiefel manifold. Section C gives further experimental results. Section D lists the grid considered for hyper-parameters. A Isometry Preserves Angles Theorem A.1. T is an isometry iff it preserves inner products. Proof. Suppose T is an isometry. Then for any v,w ∈V, ∥T(v) −T(w)∥2 = ∥v−w∥2 ⟨T(v) −T(w),T(v) −T(w)⟩= ⟨v−w,v −w⟩ ∥T(v)∥2 + ∥T(w)∥2 −2⟨T(v),T(w)⟩= ∥v∥2 + ∥w∥2 −2⟨v,w⟩. Since ∥T(u)∥= ∥u∥for any uin V, all the length squared terms in the last expression above cancel out and we get ⟨T(v),T(w)⟩= ⟨v,w⟩. Conversely, if T preserves inner products, then ⟨T(v−w),T(v−w)⟩= ⟨v−w,v −w⟩, which implies ∥T(v−w)∥= ∥v−w∥, and since T is linear, ∥T(v) −T(w)∥= ∥v−w∥. This shows that T preserves distance. B Closed-form of Projection in Tangent Space This section closely follows the arguments of Tagare [2011]. Let {X ∈Rn×p|X⊤X = I}deﬁnes a manifold in Euclidean space Rn×p, where n > p. This manifold is called the Stiefel manifold. Let TX denotes a tangent space at X. Lemma B.1. Any Z ∈TX satisﬁes: Z⊤X+ X⊤Z = 0 i.e. Z⊤X is a skew-symmetric p×pmatrix. Note, that Xconsists of porthonormal vectors in Rn. Let X⊥be a matrix consisting of the additional n−porthonormal vectors in Rn i.e. X⊥lies in the orthogonal compliment of X, X⊤X⊥= 0. The concatenation of X and X⊥, [XX⊥] is n×northonormal matrix. Then, any matrix U ∈Rn×p can be represented as: U = XA+ X⊥B, where Ais a p×pmatrix, and Bis a (n−p) ×pmatrix. Lemma B.2. A matrix Z = XA+ X⊥Bbelongs to the tangent space at a point on Stiefel manifold TX iff A is skew-symmetric. Let G∈Rn×p be the gradient computed at X. Let the projection of the gradient on the tangent space is denoted by πTX (G). Lemma B.3. Under the canonical inner product, the projection of the gradient on the tangent space is given by πTX (G) = AX, where A= GX⊤−XG⊤. 13Proof. Express G = XGA + X⊥GB. Let Z be any vector in the tangent space, expressed as Z = XZA + X⊥ZB, where ZA is a skew-symmetric matrix according to B.2. Therefore, πTX (G) = tr(G⊤Z), = tr((XGA + X⊥GB)⊤(XZA + X⊥ZB)), = tr(G⊤ AZA + G⊤ BZB). (11) Writing GA as GA = sym(GA) + skew(GA), and plugging in (11) gives, πTX (G) = tr(skew(GA)⊤ZA + G⊤ BZB). (12) Let U = XA+ X⊥B is the vector that represents the projection of Gon the tangent space at X. Then, ⟨U,Z⟩c = tr(U⊤(I−1 2XX⊤)Z), = tr((XA+ X⊥B)⊤(I−1 2XX⊤)(XZA + X⊥ZB)), = tr(1 2A⊤ZA + B⊤ZB) (13) By comparing (12) and (13), we get A= 2skew(GA) and B = GB. Thus, U = 2Xskew(GA) + X⊥GB, = X(GA −G⊤ A) + X⊥GB, ∵ skew(GA) = 1 2(GA −G⊤ A) = XGA −XG⊤ A + G−XGA, ∵ G= XGA + X⊥GB = G−XG⊤ A, = G−XG⊤X, ∵ GA = X⊤G, = GX⊤X−XG⊤X, = (GX⊤−XG⊤)X C More Results Table 3: Accuracy (2) and Forgetting(3) results of continual learning experiments for larger episodic memory sizes. 2, 3 and 5 samples per class per task are stored, respectively. Top table is for Split CIFAR. Bottom table is for Split miniImageNet. METHOD ACCURACY FORGETTING 2 3 5 2 3 5 AGEM 52.2 (±2.59) 56.1 (±1.52) 60.9 (±2.50) 0.16 (±0.01) 0.13 (±0.01) 0.11 (±0.01) ER-RING 61.9 (±1.92) 64.8 (±0.77) 67.2 (±1.72) 0.11 (±0.02) 0.08 (±0.01) 0.06 (±0.01) ORTHOG-SUBSPACE 64.7 (±0.53) 66.8 (±0.83) 67.3 (±0.98) 0.07 (±0.01) 0.05 (±0.01) 0.05 (±0.01) METHOD ACCURACY FORGETTING 2 3 5 2 3 5 AGEM 45.2 (±2.35) 47.5 (±2.59) 49.2 (±3.35) 0.14 (±0.01) 0.13 (±0.01) 0.10 (±0.01) ER-RING 51.2 (±1.99) 53.9 (±2.04) 56.8 (±2.31) 0.10 (±0.01) 0.09 (±0.02) 0.06 (±0.01) ORTHOG-SUBSPACE 53.4 (±1.23) 55.6 (±0.55) 58.2 (±1.08) 0.07 (±0.01) 0.06 (±0.01) 0.05 (±0.01) D Hyper-parameter Selection In this section, we report the hyper-parameters grid considered for experiments. The best values for different benchmarks are given in parenthesis. 14• Multitask – learning rate: [0.003, 0.01, 0.03 (CIFAR, miniImageNet), 0.1 (MNIST perm, rot), 0.3, 1.0] • Finetune – learning rate: [0.003, 0.01, 0.03 (CIFAR, miniImageNet), 0.1 (MNIST perm, rot), 0.3, 1.0] • EWC – learning rate: [0.003, 0.01, 0.03 (CIFAR, miniImageNet), 0.1 (MNIST perm, rot), 0.3, 1.0] – regularization: [0.1, 1, 10 (MNIST perm, rot, CIFAR, miniImageNet), 100, 1000] • AGEM – learning rate: [0.003, 0.01, 0.03 (CIFAR, miniImageNet), 0.1 (MNIST perm, rot), 0.3, 1.0] • MER – learning rate: [0.003, 0.01, 0.03 (MNIST, CIFAR, miniImageNet), 0.1, 0.3, 1.0] – within batch meta-learning rate: [0.01, 0.03, 0.1 (MNIST, CIFAR, miniImageNet), 0.3, 1.0] – current batch learning rate multiplier: [1, 2, 5 (CIFAR, miniImageNet), 10 (MNIST)] • ER-Ring – learning rate: [0.003, 0.01, 0.03 (CIFAR, miniImageNet), 0.1 (MNIST perm, rot), 0.3, 1.0] • ORTHOG -SUBSPACE – learning rate: [0.003, 0.01, 0.03, 0.1 (MNIST perm, rot), 0.2 (miniImageNET), 0.4 (CIFAR), 1.0] 150.0 0.2 0.4 0.6 0.8 Inner product b\\w Task and Memory gradients 0 20 40 60 80Count No Orthogonality Stiefel (Orthogonality) (a) L1 0.0 0.2 0.4 0.6 0.8 Inner product b\\w Task and Memory gradients 0 25 50 75 100Count No Orthogonality Stiefel (Orthogonality) (b) L2 0.0 0.2 0.4 0.6 Inner product b\\w Task and Memory gradients 0 25 50 75 100Count No Orthogonality Stiefel (Orthogonality) (c) L3 0.0 0.2 0.4 0.6 0.8 Inner product b\\w Task and Memory gradients 0 25 50 75 100 125Count No Orthogonality Stiefel (Orthogonality) (d) L4 0.0 0.2 0.4 0.6 0.8 Inner product b\\w Task and Memory gradients 0 50 100 150Count No Orthogonality Stiefel (Orthogonality) (e) L5 0.0 0.2 0.4 0.6 0.8 Inner product b\\w Task and Memory gradients 0 50 100 150Count No Orthogonality Stiefel (Orthogonality) (f) L6 0.0 0.2 0.4 0.6 Inner product b\\w Task and Memory gradients 0 50 100 150Count No Orthogonality Stiefel (Orthogonality) (g) L7 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Inner product b\\w Task and Memory gradients 0 50 100 150Count No Orthogonality Stiefel (Orthogonality) (h) L8 0.0 0.1 0.2 0.3 0.4 0.5 Inner product b\\w Task and Memory gradients 0 50 100 150Count No Orthogonality Stiefel (Orthogonality) (i) L9 0.0 0.1 0.2 0.3 0.4 0.5 Inner product b\\w Task and Memory gradients 0 50 100 150Count No Orthogonality Stiefel (Orthogonality) (j) L10 0.0 0.1 0.2 0.3 0.4 Inner product b\\w Task and Memory gradients 0 50 100 150 200Count No Orthogonality Stiefel (Orthogonality) (k) L11 0.0 0.1 0.2 0.3 0.4 Inner product b\\w Task and Memory gradients 0 100 200Count No Orthogonality Stiefel (Orthogonality) (l) L12 0.0 0.1 0.2 0.3 Inner product b\\w Task and Memory gradients 0 100 200 300Count No Orthogonality Stiefel (Orthogonality) (m) L13 0.00 0.05 0.10 0.15 0.20 Inner product b\\w Task and Memory gradients 0 50 100 150 200Count No Orthogonality Stiefel (Orthogonality) (n) L14 0.000 0.025 0.050 0.075 0.100 0.125 0.150 Inner product b\\w Task and Memory gradients 0 100 200Count No Orthogonality Stiefel (Orthogonality) (o) L15 0.00 0.05 0.10 0.15 0.20 Inner product b\\w Task and Memory gradients 0 100 200 300Count No Orthogonality Stiefel (Orthogonality) (p) L16 Figure 4: Histogram of inner product of current task and memory gradients in all layers in Split CIFAR. 16",
      "meta_data": {
        "arxiv_id": "2010.11635v2",
        "authors": [
          "Arslan Chaudhry",
          "Naeemullah Khan",
          "Puneet K. Dokania",
          "Philip H. S. Torr"
        ],
        "published_date": "2020-10-22T12:07:43Z",
        "venue": "NeurIPS, 2020",
        "pdf_url": "https://arxiv.org/pdf/2010.11635v2.pdf"
      }
    },
    {
      "title": "Contextual Transformation Networks for Online Continual Learning"
    },
    {
      "title": "SparCL: Sparse Continual Learning on the Edge",
      "abstract": "Existing work in continual learning (CL) focuses on mitigating catastrophic\nforgetting, i.e., model performance deterioration on past tasks when learning a\nnew task. However, the training efficiency of a CL system is\nunder-investigated, which limits the real-world application of CL systems under\nresource-limited scenarios. In this work, we propose a novel framework called\nSparse Continual Learning(SparCL), which is the first study that leverages\nsparsity to enable cost-effective continual learning on edge devices. SparCL\nachieves both training acceleration and accuracy preservation through the\nsynergy of three aspects: weight sparsity, data efficiency, and gradient\nsparsity. Specifically, we propose task-aware dynamic masking (TDM) to learn a\nsparse network throughout the entire CL process, dynamic data removal (DDR) to\nremove less informative training data, and dynamic gradient masking (DGM) to\nsparsify the gradient updates. Each of them not only improves efficiency, but\nalso further mitigates catastrophic forgetting. SparCL consistently improves\nthe training efficiency of existing state-of-the-art (SOTA) CL methods by at\nmost 23X less training FLOPs, and, surprisingly, further improves the SOTA\naccuracy by at most 1.7%. SparCL also outperforms competitive baselines\nobtained from adapting SOTA sparse training methods to the CL setting in both\nefficiency and accuracy. We also evaluate the effectiveness of SparCL on a real\nmobile phone, further indicating the practical potential of our method.",
      "full_text": "SparCL: Sparse Continual Learning on the Edge Zifeng Wang1,†, Zheng Zhan1,†, Yifan Gong1, Geng Yuan1, Wei Niu2, Tong Jian1, Bin Ren2, Stratis Ioannidis1, Yanzhi Wang1, Jennifer Dy1 1 Northeastern University, 2 College of William and Mary {zhan.zhe, gong.yifa, geng.yuan, yanz.wang}@northeastern.edu, {zifengwang, jian, ioannidis, jdy}@ece.neu.edu, wniu@email.wm.edu, bren@cs.wm.edu Abstract Existing work in continual learning (CL) focuses on mitigating catastrophic for- getting, i.e., model performance deterioration on past tasks when learning a new task. However, the training efﬁciency of a CL system is under-investigated, which limits the real-world application of CL systems under resource-limited scenarios. In this work, we propose a novel framework called Sparse Continual Learning (SparCL), which is the ﬁrst study that leverages sparsity to enable cost-effective continual learning on edge devices. SparCL achieves both training acceleration and accuracy preservation through the synergy of three aspects: weight sparsity, data efﬁciency, and gradient sparsity. Speciﬁcally, we propose task-aware dynamic masking (TDM) to learn a sparse network throughout the entire CL process, dy- namic data removal (DDR) to remove less informative training data, and dynamic gradient masking (DGM) to sparsify the gradient updates. Each of them not only improves efﬁciency, but also further mitigates catastrophic forgetting. SparCL consistently improves the training efﬁciency of existing state-of-the-art (SOTA) CL methods by at most 23×less training FLOPs, and, surprisingly, further improves the SOTA accuracy by at most1.7%. SparCL also outperforms competitive base- lines obtained from adapting SOTA sparse training methods to the CL setting in both efﬁciency and accuracy. We also evaluate the effectiveness of SparCL on a real mobile phone, further indicating the practical potential of our method. Source code will be released. 1 Introduction The objective of Continual Learning (CL) is to enable an intelligent system to accumulate knowledge from a sequence of tasks, such that it exhibits satisfying performance on both old and new tasks (31). Recent methods mostly focus on addressing the catastrophic forgetting (42) problem – learning model tends to suffer performance deterioration on previously seen tasks. However, in the real world, when the CL applications are deployed in resource-limited platforms ( 47) such as edge devices, the learning efﬁciency, w.r.t. both training speed and memory footprint, are also crucial metrics of interest, yet they are rarely explored in prior CL works. Existing CL methods can be categorized into regularization-based (2; 31; 36; 67), rehearsal-based (8; 11; 49; 60), and architecture-based (30; 41; 51; 57; 58; 69). Both regularization- and rehearsal-based methods directly train a dense model, which might even be over-parametrized for the union of all tasks (18; 38); Though several architecture-based methods (50; 56; 63) start with a sparse sub-network from the dense model, they still grow the model size progressively to learn emerging tasks. The aforementioned methods, although striving for greater performance with less forgetting, still introduce signiﬁcant memory and computation overhead during the whole CL process. †Both authors contributed equally to this work 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2209.09476v1  [cs.LG]  20 Sep 2022Figure 1: Left: Overview of SparCL. SparCL consists of three complementary components: task-aware dynamic masking (TDM) for weight sparsity, dynamic data removal (DDR) for data efﬁciency, and dynamic gradient masking (DGM) for gradient sparsity. Right: SparCL successfully preserves the accuracy and signiﬁcantly improves efﬁciency over DER++ (8), one of the SOTA CL methods, with different sparsity ratios on the Split Tiny-ImageNet (15) dataset. Recently, another stream of work, sparse training (4; 19; 34) has emerged as a new training trend to achieve training acceleration, which embraces the promising training-on-the-edge paradigm. With sparse training, each iteration takes less time with the reduction in computation achieved by sparsity, under the traditional i.i.d. learning setting. Inspired by these sparse training methods, we naturally think about introducing sparse training to the ﬁeld of CL. A straightforward idea is to directly combine existing sparse training methods, such as SNIP ( 34), RigL (19), with a rehearsal buffer under the CL setting. However, these methods fail to consider key challenges in CL to mitigate catastrophic forgetting, for example, properly handling transition between tasks. As a result, these sparse training methods, though enhancing training efﬁciency, cause signiﬁcant accuracy drop (see Section 5.2). Thus, we would like to explore a general strategy, which is orthogonal to existing CL methods, that not only leverages the idea of sparse training for efﬁciency, but also addresses key challenges in CL to preserve (or even improve) accuracy. In this work, we propose Sparse Continual Learning (SparCL), a general framework for cost-effective continual learning, aiming at enabling practical CL on edge devices. As shown in Figure 1 (left), SparCL achieves both learning acceleration and accuracy preservation through the synergy of three aspects: weight sparsity, data efﬁciency, and gradient sparsity. Speciﬁcally, to maintain a small dynamic sparse network during the whole CL process, we develop a novel task-aware dynamic masking (TDM) strategy to keep only important weights for both the current and past tasks, with special consideration during task transitions. Moreover, we propose a dynamic data removal (DDR) scheme, which progressively removes “easy-to-learn” examples from training iterations, which further accelerates the training process and also improves accuracy of CL by balancing current and past data and keeping more informative samples in the buffer. Finally, we provide an additional dynamic gradient masking (DGM) strategy to leverage gradient sparsity for even better efﬁciency and knowledge preservation of learned tasks, such that only a subset of sparse weights are updated. Figure 1 (right) demonstrates that SparCL successfully preserves the accuracy and signiﬁcantly improves efﬁciency over DER++ (8), one of the SOTA CL methods, under different sparsity ratios. SparCL is simple in concept, compatible with various existing rehearsal-based CL methods, and efﬁcient under practical scenarios. We conduct comprehensive experiments on multiple CL bench- marks to evaluate the effectiveness of our method. We show that SparCL works collaboratively with existing CL methods, greatly accelerates the learning process under different sparsity ratios, and even sometimes improves upon the state-of-the-art accuracy. We also establish competing baselines by combining representative sparse training methods with advanced rehearsal-based CL methods. SparCL again outperforms these baselines in terms of both efﬁciency and accuracy. Most importantly, we evaluate our SparCL framework on real edge devices to demonstrate the practical potential of our method. We are not aware of any prior CL works that explored this area and considered the constraints of limited resources during training. In summary, our work makes the following contributions: • We propose Sparse Continual Learning (SparCL), a general framework for cost-effective continual learning, which achieves learning acceleration through the synergy of weight sparsity, data efﬁ- ciency, and gradient sparsity. To the best of our knowledge, our work is the ﬁrst to introduce the idea of sparse training to enable efﬁcient CL on edge devices. 2• SparCL shows superior performance compared to both conventional CL methods and CL-adapted sparse training methods on all benchmark datasets, leading to at most 23×less training FLOPs and, surprisingly, 1.7% improvement over SOTA accuracy. • We evaluate SparCL on a real mobile edge device, demonstrating the practical potential of our method and also encouraging future research on CL on-the-edge. The results indicate that our framework can achieve at most 3.1×training acceleration. 2 Related work 2.1 Continual Learning The main focus in continual learning (CL) has been mitigating catastrophic forgetting. Existing methods can be classiﬁed into three major categories. Regularization-based methods (2; 31; 36; 67) limit updates of important parameters for the prior tasks by adding corresponding regularization terms. While these methods reduce catastrophic forgetting to some extent, their performance deteriorates under challenging settings ( 39), and on more complex benchmarks ( 49; 60). Rehearsal-based methods (12; 13; 24) save examples from previous tasks into a small-sized buffer to train the model jointly with the current task. Though simple in concept, the idea of rehearsal is very effective in practice and has been adopted by many state-of-the-art methods ( 8; 10; 48). Architecture-based methods (41; 50; 56; 58; 62) isolate existing model parameters or assign additional parameters for each task to reduce interference among tasks. As mentioned in Section 1, most of these methods use a dense model without consideration of efﬁciency and memory footprint, thus are not applicable to resource-limited settings. Our work, orthogonal to these methods, serves as a general framework for making these existing methods efﬁcient and enabling a broader deployment, e.g., CL on edge devices. A limited number of works explore sparsity in CL, however, for different purposes. Several methods (40; 41; 52; 56) incorporate the idea of weight pruning ( 23) to allocate a sparse sub-network for each task to reduce inter-task interference. Nevertheless, these methods still reduce the full model sparsity progressively for every task and ﬁnally end up with a much denser model. On the contrary, SparCL maintains a sparse network throughout the whole CL process, introducing great efﬁciency and memory beneﬁts both during training and at the output model. A recent work ( 14) aims at discovering lottery tickets (20) under CL, but still does not address efﬁciency. However, the existence of lottery tickets in CL serves as a strong justiﬁcation for the outstanding performance of our SparCL. 2.2 Sparse Training There are two main approaches for sparse training: ﬁxed-mask sparse training and dynamic sparse training. Fixed-mask sparse training methods ( 34; 53; 55; 59) ﬁrst apply pruning, then execute traditional training on the sparse model with the obtained ﬁxed mask. The pre-ﬁxed structure limits the accuracy performance, and the ﬁrst stage still causes huge computation and memory consumption. To overcome these drawbacks, dynamic mask methods (4; 16; 19; 44; 45) adjust the sparsity topology during training while maintaining low memory footprint. These methods start with a sparse model structure from an untrained dense model, then combine sparse topology exploration at the given sparsity ratio with the sparse model training. Recent work (66) further considers to incorporate data efﬁciency into sparse training for better training accelerations. However, all prior sparse training works are focused on the traditional training setting, while CL is a more complicated and difﬁcult scenario with inherent characteristics not explored by these works. In contrast to prior sparse training methods, our work explores a new learning paradigm that introduces sparse training into CL for efﬁciency and also addresses key challenges in CL, mitigating catastrophic forgetting. 3 Continual Learning Problem Setup In supervised CL, a model fθ learns from a sequence of tasks D= {D1,..., DT}, where each task Dt = {(xt,i,yt,i)}nt i=1 consists of input-label pairs, and each task has a disjoint set of classes. Tasks arrive sequentially, and the model must adapt to them. At the t-th step, the model gains access to data from the t-th task. However, a small ﬁx-sized rehearsal buffer Mis allowed to save data from prior tasks. At test time, the easiest setting is to assume task identity is known for each coming test example, named task-incremental learning (Task-IL). If this assumption does not hold, we have the 3...... T ask-aware Dynamic Masking  (TDM)  (a) -and- Expand Shrink Dynamic Data Removal  (DDR) Dynamic Gradient Masking  (DGM) Remove  “easier” samples Update  important gradients  (b) -and- Shrink Expand T ask 1 T ask t (a) TDM DDR DGM (b) T ask T    Epochs Figure 2: Illustration of the SparCL workﬂow. Three components work synergistically to improve training efﬁciency and further mitigate catastrophic forgetting for preserving accuracy. more difﬁcult class-incremental learning (Class-IL) setting. In this work, we mainly focus on the more challenging Class-IL setting, and only report Task-IL performance for reference. The goal of conventional CL is to train a model sequentially that performs well on all tasks at test time. The main evaluation metric is average test accuracy on all tasks. In real-world resource- limited scenarios, we should further consider training efﬁciency of the model. Thus, we measure the performance of the model more comprehensively by including training FLOPs and memory footprint. 4 Sparse Continual Learning (SparCL) Our method, Sparse Continual Learning, is a uniﬁed framework composed of three complementary components: task-aware dynamic masking for weight sparsity, dynamic data removal for data efﬁciency, and dynamic gradient masking for gradient sparsity. The entire framework is shown in Figure 2. We will illustrate each component in detail in this section. 4.1 Task-aware Dynamic Masking To enable cost-effective CL in resource limited scenarios, SparCL is designed to maintain a dynamic structure when learning a sequence of tasks, such that it not only achieves high efﬁciency, but also intelligently adapts to the data stream for better performance. Speciﬁcally, we propose a strategy named task-aware dynamic masking (TDM), which dynamically removes less important weights and grows back unused weights for stronger representation power periodically by maintaining a single binary weight mask throughout the CL process. Different from typical sparse training work, which only leverages the weight magnitude ( 44) or the gradient w.r.t. data from a single training task (19; 66), TDM considers also the importance of weights w.r.t. data saved in the rehearsal buffer, as well as the switch between CL tasks. Speciﬁcally, TDM strategy starts from a randomly initialized binary mask Mθ = M0, with a given sparsity constraint ∥Mθ∥0/∥θ∥0 = 1−s, where s∈[0,1] is the sparsity ratio. Moreover, it makes different intra- and inter-task adjustments to keep a dynamic sparse set of weights based on their continual weight importance (CWI). We summarize the process of task-aware dynamic masking in Algorithm 1 and elaborate its key components in detail below. Continual weight importance (CWI).For a model fθ parameterized by θ, the CWI of weight w⊂θis deﬁned as follows: CWI(w) =∥w∥1 + α∥∂˜L(Dt; θ) ∂w ∥1 + β∥∂L(M; θ) ∂w ∥1, (1) where Dt denotes the training data from the t-th task, Mis the current rehearsal buffer, and α, βare coefﬁcients to control the inﬂuence of current and buffered data, respectively. Moreover,Lrepresents the cross-entropy loss for classiﬁcation, while ˜Lis the single-head (1) version of the cross-entropy loss, which only considers classes from the current task by masking out the logits of other classes. Intuitively, CWI ensures we keep (1) weights of larger magnitude for output stability, (2) weights important for the current task for learning capacity, and (3) weights important for past data to mitigate catastrophic forgetting. Moreover, inspired by the classiﬁcation bias in CL (1), we use the single-head cross-entropy loss when calculating importance score w.r.t. the current task to make the importance estimation more accurate. 4Algorithm 1:Task-aware Dynamic Masking (TDM) Input: Model weight θ, number of tasks T, training epochs of the t-th task Kt, binary sparse mask Mθ, sparsity ratio s, intra-task adjustment ratio pintra, inter-task adjustment ratio pinter, update interval δk Initialize: θ, Mθ, s.t. ∥Mθ∥0/∥θ∥0 = 1−s for t= 1,...,T do for e= 1,...,K T do if t> 1 then /* Inter-task adjustment */ Expand Mθ by randomly adding unused weights, s.t. ∥Mθ∥0/∥θ∥0 = 1−(s−pinter) if e= δkthen Shrink Mθ by removing the least important weights according to equation 1, s.t. ∥Mθ∥0/∥θ∥0 = 1−s end end if e mod δk = 0then /* Intra-task adjustment */ Shrink Mθ by removing the least important weights according to equation 1, s.t. ∥Mθ∥0/∥θ∥0 = 1−(s+ pintra) Expand Mθ by randomly adding unused weights, s.t. ∥Mθ∥0/∥θ∥0 = 1−s end Update θ⊙Mθ via backpropagation end end Intra-task adjustment.When training the t-th task, a natural assumption is that the data distribution is consistent inside the task, thus we would like to update the sparse model in a relatively stable way while keeping its ﬂexibility. Thus, in Algorithm 1, we choose to update the sparsity mask Mθ in a shrink-and-expand way every δkepochs. We ﬁrst remove pintra of the weights of least CWI to retain learned knowledge so far. Then we randomly select unused weights to recover the learning capacity for the model and keep the sparsity ratio sunchanged. Inter-task adjustment.When tasks switches, on the contrary, we assume data distribution shifts immediately. Ideally, we would like the model to keep the knowledge learned from old tasks as much as possible, and to have enough learning capacity to accommodate the new task. Thus, instead of the shrink-and-expand strategy for intra-task adjustment, we follow an expand-and-shrink scheme. Speciﬁcally, at the beginning of the (t+ 1)-th task, we expand the sparse model by randomly adding a proportion of pinter unused weights. Intuitively, the additional learning capacity facilitates fast adoption of new knowledge and reduces interference with learned knowledge. We allow our model to have smaller sparsity (i.e., larger learning capacity) temporarily for the ﬁrst δk epochs as a warm- up period, and then remove the pinter weights with least CWI, following the same process in the intra-task case, to satisfy the sparsity constraint. 4.2 Dynamic Data Removal In addition to weight sparsity, decreasing the amount of training data can be directly translated into the saving of training time without any requirements for hardware support. Thus, we would also like to explore data efﬁciency to reduce the training workload. Some prior CL works select informative examples to construct the rehearsal buffer ( 3; 6; 64). However, the main purpose of them is not training acceleration, thus they either introduce excessive computational cost or consider different problem settings. By considering the features of CL, we present a simple yet effective strategy, dynamic data removal (DDR), to reduce training data for further acceleration. We measure the importance of each training example by the occurrence of misclassiﬁcation (54; 66) during CL. In TDM, the sparse structure of our model updates periodically every δkepochs, so we align our data removal process with the update of weight mask for further efﬁciency and training 5stability. In Section 4.1, we have partitioned the training process for the t-th task into Nt = Kt/δk stages based on the dynamic mask update. Therefore, we gradually remove training data at the end of i-th stage, based on the following policy: 1) Calculate the total number of misclassiﬁcations fi(xj) for each training example during the i-th stage. 2) Remove a proportion of ρi training samples with the least number of misclassiﬁcations. Although our main purpose is to keep the “harder” examples to learn to consolidate the sparse model, we can get further beneﬁts for better CL result. First, the removal of “easier” examples increases the probability that “harder” examples to be saved to the rehearsal buffer, given the common strategy,e.g. reservoir sampling (13), to buffer examples. Thus, we construct a more informative buffer in a implicit way without heavy computation. Moreover, since the buffer size is much smaller than the training set size of each task, the data from the buffer and the new task is highly imbalanced, dynamic data removal also relieves the data imbalance issue. Formally, we set the data removal proportion for each task as ρ∈[0,1], and a cutoff stage, such that: cutoff∑ i=1 ρi = ρ, Nk∑ i=cutoff+1 ρi = 0 (2) The cutoff stage controls the trade-off between efﬁciency and accuracy: When we set the cutoff stage earlier, we reduce the training time for all the following stages; however, when the cutoff stage is set too early, the model might underﬁt the removed training data. Note that when we set ρi = 0for all i= 1,2,...,N t and cutoff = Nt, we simply recover the vanilla setting without any data efﬁciency considerations. In our experiments, we assume ρi = ρ/cutoff, i.e., removing equal proportion of data at the end of every stage, for simplicity. We also conduct comprehensive exploration study for ρ and the selection of the cutoff stage in Section 5.3 and Appendix D.3. 4.3 Dynamic Gradient Masking With TDM and DDR, we can already achieve bi-level efﬁciency during training. To further boost training efﬁciency, we explore sparsity in gradient and propose dynamic gradient masking (DGM) for CL. Our method focuses on reducing computational cost by only applying the most important gradients onto the corresponding unpruned model parameters via a gradient mask. The gradient mask is also dynamically updated along with the weight mask deﬁned in Section 4.1. Intuitively, while targeting for better training efﬁciency, DGM also promotes the preservation of past knowledge by preventing a fraction of weights from update. Formally, our goal here is to ﬁnd a subset of unpruned parameters (or, equivalently, a gradient mask MG) to update over multiple training iterations. For a model fθ parameterized by θ, we have the corresponding gradient matrix Gcalculated during each iteration. To prevent the pruned weights from updating, the weight mask Mθ will be applied onto the gradient matrix Gas G⊙Mθ during backpropagation. Besides the gradients of pruned weights, we in addition consider to remove less important gradients for faster training. To achieve this, we introduce the continual gradient importance (CGI) based on the CWI to measure the importance of weight gradients. CGI(w) =α∥∂˜L(Dt; θ) ∂w ∥1 + β∥∂L(M; θ) ∂w ∥1. (3) We remove a proportion q of non-zero gradients from Gwith less importance measured by CGI and we have ∥MG∥0/∥θ∥0 = 1−(s+ q). The gradient mask MG is then applied onto the gradient matrix G. During the entire training process, the gradient mask MG is updated with a ﬁxed interval. 5 Experiment 5.1 Experiment Setting Datasets. We evaluate our SparCL on two representative CL benchmarks, Split CIFAR-10 ( 32) and Split Tiny-ImageNet (15) to verify the efﬁcacy of SparCL. In particular, we follow (8; 67) by splitting CIFAR-10 and Tiny-ImageNet into 5 and 10 tasks, each of which consists of 2 and 20 classes respectively. Dataset licensing information can be found in Appendix C. Comparing methods. In particular, we select CL methods of all kinds including regularization- based (EWC ( 31), LwF ( 36)), architecture-based (PackNet ( 41), LPS ( 56)), and rehearsal-based 6Table 1: Comparison with CL methods. SparCL consistently improves training efﬁciency of the corresponding CL methods while preserves (or even improves) accuracy on both class- and task-incremental settings. Method Sparsity Buffer size Split CIFAR-10 Split Tiny-ImageNet Class-IL (↑) Task-IL (↑) FLOPs Train×1015(↓) Class-IL (↑) Task-IL (↑) FLOPs Train×1016(↓) EWC (31) 0.00 – 19.49±0.12 68.29±3.92 8.3 7.58±0.10 19.20±0.31 13.3LwF (36) 19.61±0.05 63.29±2.35 8.3 8.46±0.22 15.85±0.58 13.3 PackNet (41) 0.50† – - 93.73 ±0.55 5.0 – 61.88 ±1.01 7.3LPS (56) - 94.50 ±0.47 5.0 – 63.37 ±0.83 7.3 A-GEM (12) 0.00 200 20.04±0.34 83.88±1.49 11.1 8.07±0.08 22.77±0.03 17.8iCaRL (49) 49.02±3.20 88.99±2.13 11.1 7.53±0.79 28.19±1.47 17.8FDR (5) 30.91±2.74 91.01±0.68 13.9 8.70±0.19 40.36±0.68 22.2ER (13) 44.79±1.86 91.19±0.94 11.1 8.49±0.16 38.17±2.00 17.8DER++ (8) 64.88±1.17 91.92±0.60 13.9 10.96±1.17 40.87±1.16 22.2 SparCL-ER75 46.89±0.68 92.02±0.72 2.0 8.98±0.38 39.14±0.85 3.2SparCL-DER++75 0.75 66.30±0.98 94.06±0.45 2.5 12.73±0.40 42.06±0.73 4.0SparCL-ER90 45.81±1.05 91.49±0.47 0.9 8.67±0.41 38.79±0.39 1.4SparCL-DER++90 0.90 200 65.79±1.33 93.73±0.24 1.1 12.27±1.06 41.17±1.31 1.8SparCL-ER95 44.59±0.23 91.07±0.64 0.5 8.43±0.09 38.20±0.46 0.8SparCL-DER++95 0.95 65.18±1.25 92.97±0.37 0.6 10.76±0.62 40.54±0.98 1.0 A-GEM (12) 0.00 500 22.67±0.57 89.48±1.45 11.1 8.06±0.04 25.33±0.49 17.8iCaRL (49) 47.55±3.95 88.22±2.62 11.1 9.38±1.53 31.55±3.27 17.8FDR (5) 28.71±3.23 93.29±0.59 13.9 10.54±0.21 49.88±0.71 22.2ER (13) 57.74±0.27 93.61±0.27 11.1 9.99±0.29 48.64±0.46 17.8DER++ (8) 72.70±1.36 93.88±0.50 13.9 19.38±1.41 51.91±0.68 22.2 SparCL-ER75 60.80±0.22 93.82±0.32 2.0 10.48±0.29 50.83±0.69 3.2SparCL-DER++75 0.75 74.09±0.84 95.19±0.34 2.5 20.75±0.88 52.19±0.43 4.0SparCL-ER90 59.34±0.97 93.33±0.10 0.9 10.12±0.53 49.46±1.22 1.4SparCL-DER++90 0.90 500 73.42±0.95 94.82±0.23 1.1 19.62±0.67 51.93±0.36 1.8SparCL-ER95 57.75±0.45 92.73±0.34 0.5 9.91±0.17 48.57±0.50 0.8SparCL-DER++95 0.95 72.14±0.78 94.39±0.15 0.6 19.01±1.32 51.26±0.78 1.0 †PackNet and LPS actually have a decreased sparsity after learning every task, we use 0.50 to roughly represent the average sparsity. (A-GEM (12), iCaRL (43), FDR (5), ER (13), DER++ (8)) methods. Note that PackNet and LPS are only compatible with task-incremental learning. We also adapt representative sparse training methods (SNIP (34), RigL (19)) to the CL setting by combining them with DER++ (SNIP-DER++, RigL-DER++). Variants of our method.To show the generality of SparCL, we combine it with DER++ (one of the SOTA CL methods), and ER (simple and widely-used) as SparCL-DER++ and SparCL-ER, respectively. We also vary the weight sparsity ratio (0.75,0.90,0.95) of SparCL for a comprehensive evaluation. Evaluation metrics.We use the average accuracy on all tasks to evaluate the performance of the ﬁnal model. Moreover, we evaluate the training FLOPs (19), and memory footprint (66) (including feature map pixels and model parameters during training) to demonstrate the efﬁciency of each method. Please see Appendix D.1 for detailed deﬁnitions of these metrics. Experiment details.For fair comparison, we strictly follow the settings in prior CL work (8; 28). We sets the per task training epochs to 50 and 100 for Split CIFAR-10 and Tiny-ImageNet, respectively, with a batch size of 32. For the model architecture, We follow (8; 49) and adopt the ResNet-18 (25) without any pre-training. We also use the best hyperparameter setting reported in ( 8; 56) for CL methods, and in (19; 34) for CL-adapted sparse training methods. For SparCL and its competing CL-adapted sparse training methods, we adopt a uniform sparsity ratio for all convolutional layers. Please see Appendix D for other details. 5.2 Main Results Comparison with CL methods.Table 1 summarizes the results on Split CIFAR-10 and Tiny- ImageNet, under both class-incremental (Class-IL) and task-incremental (Task-IL) settings. From Table 1, we can clearly tell that SparCL signiﬁcantly improves ER and DER++, while also outperforms other CL baselines, in terms of training efﬁciency (measured in FLOPs). With higher sparsity ratio, SparCL leads to less training FLOPs. Notably, SparCL achieves23×training efﬁciency improvement upon DER++ with a sparsity ratio of 0.95. On the other hand, our framework also improves the average accuracy of ER and DER++ consistently under all cases with a sparsity ratio of 0.75 and 0.90, and only slight performance drop when sparsity gets larger as 0.95. In particular, SparCL-DER++ 7Table 2: Comparison with CL-adapted sparse training methods. All methods are combined with DER++ with a 500 buffer size. SparCL outperforms all methods in both accuracy and training efﬁciency, under all sparsity ratios. All three methods here can save 20% ∼ 51% memory footprint, please see Appendix D.2 for details. Method Spasity Split CIFAR-10 Split Tiny-ImageNet Class-IL (↑) FLOPs Train ×1015(↓) Class-IL (↑) FLOPs Train ×1016(↓) DER++ (8) 0.00 72.70±1.36 13.9 19.38±1.41 22.2 SNIP-DER++ (34) 69.82±0.72 1.6 16.13±0.61 2.5RigL-DER++ (19)0.90 69.86±0.59 1.6 18.36±0.49 2.5SparCL-DER++90 73.42±0.95 1.1 19.62±0.67 1.8 SNIP-DER++ (34) 66.07±0.91 0.9 14.76±0.52 1.5RigL-DER++ (19)0.95 66.53±1.13 0.9 15.88±0.63 1.5SparCL-DER++95 72.14±0.78 0.6 19.01±1.32 1.0 Table 3: Ablation study on Split-CIFAR10 with 0.75 sparsity ratio. All components contributes to the overall performance, in terms of both accuracy and efﬁciency (training FLOPs and memory footprint). TDM DDR DGMClass-IL (↑) FLOPs Train ×1015(↓) MemoryFootprint (↓) \u0017 \u0017 \u0017 72.70 13.9 247MB\u0013 \u0017 \u0017 73.37 3.6 180MB\u0013 \u0013 \u0017 73.80 2.8 180MB\u0013 \u0017 \u0013 73.97 3.3 177MB\u0013 \u0013 \u0013 74.09 2.5 177MB with 0.75 sparsity ratio sets new SOTA accuracy, with all buffer sizes under both benchmarks. The outstanding performance of SparCL indicates that our proposed strategies successfully preserve accuracy by further mitigating catastrophic forgetting with a much sparser model. Moreover, the improvement that SparCL brings to two different existing CL methods shows the generalizability of SparCL as a uniﬁed framework, i.e., it has the potential to be combined with a wide array of existing methods. We would also like to take a closer look at PackNet and LPS, which also leverage the idea of sparsity to split the model by different tasks, a different motivation from training efﬁciency. Firstly, they are only compatible with the Task-IL setting, since they leverage task identity at both training and test time. Moreover, the model sparsity of these methods reduces with the increasing number of tasks, which still leads to much larger overall training FLOPs than that of SparCL. This further demonstrates the importance of keeping a sparse model without permanent expansion throughout the CL process. Comparison with CL-adapted sparse training methods.Table 2 shows the result under the more difﬁcult Class-IL setting. SparCL outperforms all CL-adapted sparse training methods in both accuracy and training FLOPs. The performance gap between SparCL-DER++ and other methods gets larger with a higher sparsity. SNIP- and RigL-DER++ achieve training acceleration at the cost of compromised accuracy, which suggests that keeping accuracy is a non-trivial challenge for existing sparse training methods under the CL setting. SNIP generates the static initial mask after network initialization which does not consider the structure suitability among tasks. Though RigL adopts a dynamic mask, the lack of task-aware strategy prevents it from generalizing well to the CL setting. 5.3 Effectiveness of Key Components Ablation study.We provide a comprehensive ablation study in Table 3 using SparCL-DER++ with 0.75 sparsity on Split CIFAR10. Table 3 demonstrates that all components of our method contribute to both efﬁciency and accuracy improvements. Comparing row 1 and 2, we can see that the majority of FLOPs decrease results from TDM. Interestingly, TDM leads to an increase in accuracy, indicating TDM generates a sparse model that is even more suitable for learning all tasks than then full dense model. Comparing row 2 and 3, we can see that DDR indeed further accelerates training by removing less informative examples. As discussed in Section 4.2, when we remove a certain number of samples (30% here), we achieve a point where we keep as much informative samples as we need, and also balance the current and buffered data. Comparing row 2 and 4, DGM reduce both training FLOPs and memory footprint while improve the performance of the network. Finally, the last row demonstrates the collaborative performance of all components. We also show the same ablation study with 0.90 sparsity in Appendix D.4 for reference. Detail can be found in Appendix D.1. 8Figure 3: Comparison between DDR and One- shot (66) data removal strategy w.r.t. different data removal proportion ρ. DDR outperforms One-shot and also achieves better accuracy when ρ ≤ 30%. Figure 4: Comparison with CL-adapted sparse training methods in training acceleration rate and accuracy results. The radius of circles are measured by memory footprint. Exploration on DDR.To understand the inﬂuence of the data removal proportionρ, and the cutoff stage for each task, we show corresponding experiment results in Figure 3 and Appendix D.3, respectively. In Figure 3, we ﬁx cutoff = 4, i.e., gradually removing equal number of examples every 5 epochs until epoch 20, and vary ρfrom 10% to 90%. We also compare DDR with One-shot removal strategy (66), which removes all examples at once at cutoff. DDR outperforms One-shot consistently with different ρin average accuracy. Also note that since DDR removes the examples gradually before the cutoff stage, DDR is more efﬁcient than One-shot. When ρ≤30%, we also observe increased accuracy of DDR compared with the baseline without removing any data. When ρ≥40%, the accuracy gets increasingly lower for both strategies. The intuition is that when DDR removes a proper amount of data, it removes redundant information while keeps the most informative examples. Moreover, as discussed in Section 4.2, it balances the current and buffered data, while also leave informative samples in the buffer. When DDR removes too much data, it will also lose informative examples, thus the model has not learned these examples well before removal. Exploration on DGM.We test the efﬁcacy of DGM at different sparsity levels. Detailed exploratory experiments are shown in Appendix D.5 for reference. The results indicate that by setting the proportion qwithin an appropriate range, DGM can consistently improve the accuracy performance regardless of the change of weight sparsity. 5.4 Mobile Device Results The training acceleration results are measured on the CPU of an off-the-shelf Samsung Galaxy S20 smartphone, which has the Qualcomm Snapdragon 865 mobile platform with a Qualcomm Kryo 585 Octa-core CPU. We run each test on a batch of 32 images to denote the training speed. The detail of on-mobile compiler-level optimizations for training acceleration can be found in Appendix E.1. The acceleration results are shown in Figure 4. SparCL can achieve approximately 3.1×and 2.3× training acceleration with 0.95 sparsity and 0.90 sparsity, respectively. Besides, our framework can also save 51% and 48% memory footprint when the sparsity is 0.95 and 0.90. Furthermore, the obtained sparse models save the storage consumption by using compressed sparse row (CSR) storage and can be accelerated to speed up the inference on-the-edge. We provide on-mobile inference acceleration results in Appendix E.2. 6 Conclusion This paper presents a uniﬁed framework named SparCL for efﬁcient CL that achieves both learning acceleration and accuracy preservation. It comprises three complementary strategies: task-aware dynamic masking for weight sparsity, dynamic data removal for data efﬁciency, and dynamic gradient masking for gradient sparsity. Extensive experiments on standard CL benchmarks and real-world edge device evaluations demonstrate that our method signiﬁcantly improves upon existing CL methods and outperforms CL-adapted sparse training methods. We discuss the limitations and potential negative social impacts of our method in Appendix A and B, respectively. 9References [1] Hongjoon Ahn, Jihwan Kwak, Subin Lim, Hyeonsu Bang, Hyojun Kim, and Taesup Moon. Ss-il: Separated softmax for incremental learning. In CVPR, 2021. 4 [2] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuyte- laars. Memory aware synapses: Learning what (not) to forget. In ECCV, 2018. 1, 3 [3] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample selection for online continual learning. NeurIPS, 2019. 5 [4] Guillaume Bellec, David Kappel, Wolfgang Maass, and Robert Legenstein. Deep rewiring: Training very sparse deep networks. In ICLR, 2018. 2, 3 [5] Ari S Benjamin, David Rolnick, and Konrad Kording. Measuring and regularizing networks in function space. arXiv preprint arXiv:1805.08289, 2018. 7 [6] Zalán Borsos, Mojmir Mutny, and Andreas Krause. Coresets via bilevel optimization for continual learning and streaming. NeurIPS, 2020. 5 [7] Miles Brundage, Shahar Avin, Jack Clark, Helen Toner, Peter Eckersley, Ben Garﬁnkel, Allan Dafoe, Paul Scharre, Thomas Zeitzoff, Bobby Filar, et al. The malicious use of artiﬁcial intelligence: Forecasting, prevention, and mitigation. arXiv preprint arXiv:1802.07228, 2018. 15 [8] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. Dark experience for general continual learning: a strong, simple baseline. In NeurIPS, 2020. 1, 2, 3, 6, 7, 8 [9] Alfredo Canziani, Adam Paszke, and Eugenio Culurciello. An analysis of deep neural network models for practical applications. arXiv preprint arXiv:1605.07678, 2016. 16 [10] Hyuntak Cha, Jaeho Lee, and Jinwoo Shin. Co2l: Contrastive continual learning. In ICCV, 2021. 3 [11] Arslan Chaudhry, Albert Gordo, Puneet Kumar Dokania, Philip Torr, and David Lopez-Paz. Us- ing hindsight to anchor past knowledge in continual learning. arXiv preprint arXiv:2002.08165, 2(7), 2020. 1 [12] Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efﬁcient lifelong learning with a-gem. arXiv preprint arXiv:1812.00420, 2018. 3, 7 [13] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS Torr, and Marc’Aurelio Ranzato. On tiny episodic memories in continual learning. arXiv preprint arXiv:1902.10486, 2019. 3, 6, 7 [14] Tianlong Chen, Zhenyu Zhang, Sijia Liu, Shiyu Chang, and Zhangyang Wang. Long live the lottery: The existence of winning tickets in lifelong learning. In ICLR, 2020. 3 [15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR. Ieee, 2009. 2, 6 [16] Tim Dettmers and Luke Zettlemoyer. Sparse networks from scratch: Faster training without losing performance. arXiv preprint arXiv:1907.04840, 2019. 3 [17] Peiyan Dong, Siyue Wang, Wei Niu, Chengming Zhang, Sheng Lin, Zhengang Li, Yifan Gong, Bin Ren, Xue Lin, and Dingwen Tao. Rtmobile: Beyond real-time mobile acceleration of rnns for speech recognition. In DAC, pages 1–6. IEEE, 2020. 17 [18] Xuanyi Dong and Yi Yang. Network pruning via transformable architecture search. In NeurIPS, pages 759–770, 2019. 1 [19] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery: Making all tickets winners. In ICML, pages 2943–2952. PMLR, 2020. 2, 3, 4, 7, 8, 16 10[20] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. ICLR, 2019. 3 [21] Yifan Gong, Geng Yuan, Zheng Zhan, Wei Niu, Zhengang Li, Pu Zhao, Yuxuan Cai, Sijia Liu, Bin Ren, Xue Lin, et al. Automatic mapping of the best-suited dnn pruning schemes for real-time mobile acceleration. ACM Transactions on Design Automation of Electronic Systems (TODAES), 27(5):1–26, 2022. 18 [22] Yifan Gong, Zheng Zhan, Zhengang Li, Wei Niu, Xiaolong Ma, Wenhao Wang, Bin Ren, Caiwen Ding, Xue Lin, Xiaolin Xu, et al. A privacy-preserving-oriented dnn pruning and mobile acceleration framework. In GLSVLSI, pages 119–124, 2020. 17 [23] Song Han, Jeff Pool, et al. Learning both weights and connections for efﬁcient neural network. In NeurIPS, pages 1135–1143, 2015. 3 [24] Tyler L Hayes, Nathan D Cahill, and Christopher Kanan. Memory efﬁcient experience replay for streaming learning. In ICRA, 2019. 3 [25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 7 [26] Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi Yang. Soft ﬁlter pruning for accelerating deep convolutional neural networks. arXiv preprint arXiv:1808.06866, 2018. 17 [27] Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi Yang. Filter pruning via geometric median for deep convolutional neural networks acceleration. In CVPR, pages 4340–4349, 2019. 17 [28] Yen-Chang Hsu, Yen-Cheng Liu, Anita Ramasamy, and Zsolt Kira. Re-evaluating con- tinual learning scenarios: A categorization and case for strong baselines. arXiv preprint arXiv:1810.12488, 2018. 7 [29] Tong Jian, Yifan Gong, Zheng Zhan, Runbin Shi, Nasim Soltani, Zifeng Wang, Jennifer G Dy, Kaushik Roy Chowdhury, Yanzhi Wang, and Stratis Ioannidis. Radio frequency ﬁngerprinting on the edge. IEEE Transactions on Mobile Computing, 2021. 17 [30] Zixuan Ke, Bing Liu, and Xingchang Huang. Continual learning of a mixed sequence of similar and dissimilar tasks. NeurIPS, 2020. 1 [31] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. PNAS, 114(13):3521–3526, 2017. 1, 3, 6, 7 [32] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. https://www.cs.toronto.edu/ kriz/learning-features-2009-TR.pdf, 2009. 6, 15 [33] Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998. 15 [34] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network pruning based on connection sensitivity. ICLR, 2019. 2, 3, 7, 8 [35] Tuanhui Li, Baoyuan Wu, Yujiu Yang, Yanbo Fan, Yong Zhang, and Wei Liu. Compressing convolutional neural networks via factorized convolutional ﬁlters. In CVPR, pages 3977–3986, 2019. 17 [36] Zhizhong Li and Derek Hoiem. Learning without forgetting. TPAMI, 40(12):2935–2947, 2017. 1, 3, 6, 7 [37] Xiaolong Ma, Fu-Ming Guo, Wei Niu, Xue Lin, Jian Tang, Kaisheng Ma, Bin Ren, and Yanzhi Wang. Pconv: The missing but desirable sparsity in dnn weight pruning for real-time execution on mobile devices. In AAAI, pages 5117–5124, 2020. 17 11[38] Xiaolong Ma, Wei Niu, Tianyun Zhang, Sijia Liu, Sheng Lin, Hongjia Li, Wujie Wen, Xiang Chen, Jian Tang, Kaisheng Ma, et al. An image enhancing pattern-based sparsity for real-time inference on mobile devices. In ECCV, pages 629–645. Springer, 2020. 1 [39] Zheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe, Hyunwoo Kim, and Scott Sanner. Online continual learning in image classiﬁcation: An empirical survey. arXiv preprint arXiv:2101.10423, 2021. 3 [40] Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggyback: Adapting a single network to multiple tasks by learning to mask weights. In ECCV, pages 67–82, 2018. 3 [41] Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative pruning. In CVPR, 2018. 1, 3, 6, 7 [42] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of learning and motivation, volume 24, pages 109–165. Elsevier, 1989. 1 [43] Sanket Vaibhav Mehta, Darshan Patil, Sarath Chandar, and Emma Strubell. An empirical investigation of the role of pre-training in lifelong learning. ICML Workshop, 2021. 7 [44] Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine Gibescu, and Antonio Liotta. Scalable training of artiﬁcial neural networks with adaptive sparse connectivity inspired by network science. Nature communications, 9(1):1–12, 2018. 3, 4 [45] Hesham Mostafa and Xin Wang. Parameter efﬁcient training of deep convolutional neural networks by dynamic sparse reparameterization. In ICML, pages 4646–4655. PMLR, 2019. 3 [46] Wei Niu, Xiaolong Ma, Sheng Lin, Shihao Wang, Xuehai Qian, Xue Lin, Yanzhi Wang, and Bin Ren. Patdnn: Achieving real-time dnn execution on mobile devices with pattern-based weight pruning. arXiv preprint arXiv:2001.00138, 2020. 18 [47] Lorenzo Pellegrini, Vincenzo Lomonaco, Gabriele Grafﬁeti, and Davide Maltoni. Con- tinual learning at the edge: Real-time training on smartphone devices. arXiv preprint arXiv:2105.13127, 2021. 1 [48] Quang Pham, Chenghao Liu, and Steven Hoi. Dualnet: Continual learning, fast and slow. NeurIPS, 2021. 3 [49] Sylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classiﬁer and representation learning. In CVPR, pages 2001–2010, 2017. 1, 3, 7 [50] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016. 1, 3 [51] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. In ICML, 2018. 1 [52] Ghada Sokar, Decebal Constantin Mocanu, and Mykola Pechenizkiy. Spacenet: Make free space for continual learning. Neurocomputing, 439:1–11, 2021. 3 [53] Hidenori Tanaka, Daniel Kunin, Daniel L Yamins, and Surya Ganguli. Pruning neural networks without any data by iteratively conserving synaptic ﬂow. NeurIPS, 33:6377–6389, 2020. 3 [54] Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J Gordon. An empirical study of example forgetting during deep neural network learning. arXiv preprint arXiv:1812.05159, 2018. 5 [55] Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by preserving gradient ﬂow. In ICLR, 2019. 3 [56] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis Ioannidis. Learn-prune-share for lifelong learning. In ICDM, 2020. 1, 3, 6, 7 12[57] Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, et al. Dualprompt: Complementary prompting for rehearsal-free continual learning. ECCV, 2022. 1 [58] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pﬁster. Learning to prompt for continual learning.CVPR, 2022. 1, 3 [59] Paul Wimmer, Jens Mehnert, and Alexandru Condurache. Freezenet: Full performance by reduced storage costs. In ACCV, 2020. 3 [60] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, and Yun Fu. Large scale incremental learning. In CVPR, pages 374–382, 2019. 1, 3 [61] Yushu Wu, Yifan Gong, Pu Zhao, Yanyu Li, Zheng Zhan, Wei Niu, Hao Tang, Minghai Qin, Bin Ren, and Yanzhi Wang. Compiler-aware neural architecture search for on-mobile real-time super-resolution. ECCV, 2022. 18 [62] Shipeng Yan, Jiangwei Xie, and Xuming He. Der: Dynamically expandable representation for class incremental learning. In CVPR, pages 3014–3023, 2021. 3 [63] Li Yang, Sen Lin, Junshan Zhang, and Deliang Fan. Grown: Grow only when necessary for continual learning. arXiv preprint arXiv:2110.00908, 2021. 1 [64] Jaehong Yoon, Divyam Madaan, Eunho Yang, and Sung Ju Hwang. Online coreset selection for rehearsal-based continual learning. arXiv preprint arXiv:2106.01085, 2021. 5 [65] Kun-Hsing Yu, Andrew L Beam, and Isaac S Kohane. Artiﬁcial intelligence in healthcare. Nature biomedical engineering, 2(10):719–731, 2018. 15 [66] Geng Yuan, Xiaolong Ma, Wei Niu, Zhengang Li, Zhenglun Kong, Ning Liu, Yifan Gong, Zheng Zhan, Chaoyang He, Qing Jin, et al. Mest: Accurate and fast memory-economic sparse training framework on the edge. NeurIPS, 34, 2021. 3, 4, 5, 7, 9, 16, 17 [67] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In ICML, 2017. 1, 3, 6 [68] Zheng Zhan, Yifan Gong, Pu Zhao, Geng Yuan, Wei Niu, Yushu Wu, Tianyun Zhang, Malith Jayaweera, David Kaeli, Bin Ren, et al. Achieving on-mobile real-time super-resolution with neural architecture and pruning search. In ICCV, pages 4821–4831, 2021. 17 [69] Tingting Zhao, Zifeng Wang, Aria Masoomi, and Jennifer Dy. Deep bayesian unsupervised lifelong learning. Neural Networks, 149:95–106, 2022. 1 Checklist 1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s contributions and scope? [Yes] The claims match the experimental results and it is expected to generalize according to the diverse experiments stated in our paper. We include all of our code, data, and models in the supplementary materials, which can reproduce our experimental results. (b) Did you describe the limitations of your work? [Yes] See Section 6 and Appendix A. (c) Did you discuss any potential negative societal impacts of your work? [Yes] See Section 6 and Appendix B. (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] We have read the ethics review guidelines and ensured that our paper conforms to them. 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [N/A] Our paper is based on the experimental results and we do not have any theoretical results. 13(b) Did you include complete proofs of all theoretical results? [N/A] Our paper is based on the experimental results and we do not have any theoretical results. 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experi- mental results (either in the supplemental material or as a URL)? [Yes] See Section 5.1, Section 5.4 and we provide code to reproduce the main experimental results. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Section 5.1 and Section 5.4. (c) Did you report error bars (e.g., with respect to the random seed after running experi- ments multiple times)? [Yes] See Table 1, Table 2, ﬁg 1, ﬁg 3. (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Section 5.1, Section 5.4. 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] We mentioned and cited the datasets (Split CIFAR-10 and Tiny-ImageNet), and all comparing methods with their paper and github in it. (b) Did you mention the license of the assets? [Yes] The licences of used datasets/models are provided in the cited references and we state them explicitly in Appendix C. (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] We provide code for our proposed method in the supplement. (d) Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [N/A] (e) Did you discuss whether the data you are using/curating contains personally identiﬁable information or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] 14A Limitations One limitation of our method is that we assume a rehearsal buffer is available throughout the CL process. Although the assumption is widely-accepted, there are still situations that a rehearsal buffer is not allowed. However, as a framework targeting for efﬁciency, our work has the potential to accelerate all types of CL methods. For example, simply removing the terms related to rehearsal buffer in equation 1 and equation 3 could serve as a naive variation of our method that is compatible with other non-rehearsal methods. It is interesting to further improve SparCL to be more generic for all kinds of CL methods. Moreover, the benchmarks we use are limited to vision domain. Although using vision-based benchmarks has been a common practice in the CL community, we believe evaluating our method, as well as other CL methods, on datasets from other domains such as NLP will lead to a more comprehensive and reliable conclusion. We will keep track of newer CL benchmarks from different domains and further improve our work correspondingly. B Potential Negative Societal Impact Although SparCL is a general framework to enhance efﬁciency for various CL methods, we still need to be aware of its potential negative societal impact. For example, we need to be very careful about the trade-off between accuracy and efﬁciency when using SparCL. If one would like to pursue efﬁciency by setting the sparsity ratio too high, then even SparCL will result in signiﬁcant accuracy drop, since the over-sparsiﬁed model does not have enough representation power. Thus, we should pay much attention when applying SparCL on accuracy-sensitive applications such as healthcare (65). Another example is that, SparCL as a powerful tool to make CL methods efﬁcient, can also strengthen models for malicious applications ( 7). Therefore, we encourage the community to come up with more strategies and regulations to prevent malicious use of artiﬁcial intelligence. C Dataset Licensing Information • CIFAR-10 (32) is licensed under the MIT license. • The licensing information of Tiny-ImageNet ( 33) is not available. However, the data is available for free to researchers for non-commercial use. D Additional Experiment Details and Results We set α = 0.5,β = 1 in equation 1 and equation 3. We also set δk = 5, pinter = 0.01, pintra = 0.005. We also match different weight sparsity with gradient sparsity for best performance. We sample 20% data from Split CIFAR-10 training set for validation, and we use grid-search on this validation set to help us select the mentioned best hyperparameters. We use the same set of hyperparameters for both datasets. For accurate evaluation, we repeat each experiments 3 times using different random seeds and report the average performance. During our experiments, we adopt unstructured sparsity type and uniform sparsity ratio (0.75,0.90,0.95) for all convolutional layers in the models. D.1 Evaluation Metrics Explanation Training FLOPsThe FLOPs of a single forward pass is calculated by taking the sum of the number of multiplications and additions in each layer lfor a given layer sparsity sl. Each iteration in the training process is composed of two phases, i.e., the forward propagation and backward propagation. The goal of the forward pass is to calculate the loss of the current set of parameters on a given batch of data. It can be formulated as al = σ(zl) =σ(wl ∗al−1 + bl) for each layer lin the model. Here, w, b, and zrepresent the weights, biases, and output before activation, respectively; σ(.) denotes the activation function; ais the activations; ∗means convolution operation. The formulation indicates that the layer activations are calculated in sequence using the previous activations and the parameters of the layer. Activation of layers are stored in memory for the backward pass. 15As for the backward propogation, the objective is to back-propagate the error signal while calculating the gradients of the parameters. The two main calculation steps can be represented as: δl = δl+1 ∗rotate180°(wl) ⊙σ′(zl), (4) Gl = al−1 ∗δl, (5) where δl is the error associated with the layer l, Gl denotes the gradients, ⊙represents Hadamard product, σ′(.) denotes the derivative of activation, androtate180°(.) means rotating the matrix by180° is the matrix transpose operation. During the backward pass, each layer lcalculates two quantities, i.e., the gradient of the activations of the previous layer and the gradient of its parameters. Thus, the backward passes are counted astwice the computation expenses of the forward pass (19). We omit the FLOPs needed for batch normalization and cross entropy. In our work, the total FLOPs introduced by TDM, DDR, and DGM on split CIFAR-10 is approximately 4.5 ×109 which is less than 0.0001% of total training FLOPs. For split Tiny-ImageNet, the total FLOPs of them is approximately 1.8 ×1010, which is also less than 0.0001% of total training FLOPs. Therefore, the computation introduced by TDM, DDR, and DGM is negligible. Memory FootprintsFollowing works ( 9; 66), the deﬁnition of memory footprints contain two parts: 1) activations (feature map pixels) during training phase, and 2) model parameters during training phase. For experiments, activations, model weights, and gradients are stored in 32-bit ﬂoating-point format for training. The memory footprint results are calculated with an approximate summation of them. D.2 Details of Memory Footprint The memory footprint is composed of three parts: activations, model weights, and gradients. They are all represented as bw-bit numbers for training. The number of activations in the model is the sum of the activations in each layer. Suppose that the output feature of the l-th layer with a batch size of Bis represented as al ∈RB×Ol×Hl×Wl , where Ol is the number of channels and Hl ×Wl is the feature size. The total number of activations of the model is thus B∑ lOlHlWl. As for the model weights, our SparCL training a sparse model with a sparsity ratio s∈[0,1] from scratch. The sparse model is obtained from a dense model with a total of N weights. A higher value of sindicates fewer non-zero weights in the sparse model. Compressed sparse row (CSR) format is commonly used for sparse storage, which greatly reduces the number of indices need to be stored for sparse matrices. As our SparCL adopt only one sparsity type and we use a low-bit format to store the indices, we omit the indices storage here. Therefore, the memory footprint for model representation is (1 −s)Nbw. Similar calculations can be applied for the gradient matrix. Besides the sparsity ratio s, additional q gradients are masked out from the gradient matrix, resulting a sparsity ratio s+ q. Therefore, the storage of gradients can be approximated as (1 −(s+ q))Nbw. Combining the activations, model representation, and gradients, the total memory footprint in SparCL can be represented as (2B∑ lOlHlWl + (1−s)N + (1−(s+ q))N)bw. DDR requires store indices for the easier examples during the training process. The number of training examples for Split CIFAR-10 and Split Tiny-ImageNet on each task is 10000. In our work, we only need about 3KB (remove 30% training data) for indices storage (in the int8 format) and the memory cost is negligible compared with the total memory footprint. D.3 Effect of Cutoff Stage Table A1: Effect of cutoff. cutoff 1 2 3 4 5 6 7 8 9 Class-IL (↑) 71.54 72.38 72.74 73.20 73.10 73.32 73.27 73.08 73.23 To evaluate the effect of the cutoff stage, we use the same setting as in Figure 3 by setting the sparsity ratio to 0.90. We keep the data removal proportion ρ = 30%, and only change cutoff. 16Table A1 shows the relationship between cutoff and the Class-IL average accuracy. Note that from the perspective of efﬁciency, we would like thecutoff stage as early as possible, so that the remaining epochs will have less examples. However, from Table A1, we can see that if we set it too early, i.e., cutoff ≤3, the accuracy drop is signiﬁcant. This indicate that even for the “easy-to-learn” examples, removing them too early results in underﬁtting. As a balance point between accuracy and efﬁciency, we choose cutoff = 4in our ﬁnal version. D.4 Supplementary Ablation Study Table A2: Ablation study on Split-CIFAR10 with 0.90 sparsity. TDM DDR DGM Class-IL (↑) FLOPs Train ×1015(↓) Memory Footprint (↓) \u0017 \u0017 \u0017 72.70 13.9 247MB \u0013 \u0017 \u0017 72.98 1.6 166MB \u0013 \u0013 \u0017 73.20 1.2 166MB \u0013 \u0017 \u0013 73.30 1.5 165MB \u0013 \u0013 \u0013 73.42 1.1 165MB Similar to Table 3, we show ablation study with 0.90 sparsity ratio in Table A2. Under a larger sparsity ratio, the conclusion that all components contribute to the ﬁnal performance still holds. However, we can observe that the accuracy increase that comes from DDR and DGM is less than what we show in Table 3. We assume that larger sparsity ratio makes it more difﬁcult for the model to retain good accuracy in CL. Similar results has also been observed in (66) under the usual i.i.d. learning setting. D.5 Exploration on DGM Table A3: Ablation study of the gradient sparsity ratio on Split-CIFAR10. weight sparsity gradient sparsityClass-IL (↑) FLOPs Train ×1015(↓) Memory Footprint (↓) 0.75 0.78 74.08 3.4 178MB 0.75 0.80 73.97 3.3 177MB 0.75 0.82 73.79 3.3 177MB 0.75 0.84 73.26 3.2 176MB 0.90 0.91 73.33 1.6 166MB 0.90 0.92 73.30 1.5 165MB 0.90 0.93 72.64 1.5 165MB We conduct further experiments to demonstrate the inﬂuence of gradient sparsity, and the results are shown in Table A3. There are two sets of the experiments with different weight sparsity settings: 0.75 and 0.90. Within each set of the experiments (the weight sparsity is ﬁxed), we vary the gradient sparsity values. From the results we can see that increasing the gradient sparsity can decrease the FLOPs and memory footprint. However, the accuracy performance degrades more obvious when the gradient sparsity is too much for the weight sparsity. The results indicate that suitable gradient sparsity setting can bring further efﬁciency to the training process while boosting the accuracy performance. In the main results, the gradient sparsity is set as 0.80 for 0.75 weight sparsity, and set as 0.92 for 0.90 weight sparsity. E On-Mobile Compiler Optimizations and Inference Results E.1 Compiler Optimizations Each iteration in the training process is composed of two phases, i.e., the forward propagation and backward propagation. Prior works (17; 22; 26; 27; 29; 35; 37; 68) have proved that sparse weight matrices (tensors) can provide inference acceleration via reducing the number of multiplications in convolution operation. Therefore, the forward propagation phase, which is the same as inference, 17can be accelerated by the sparsity inherently. As for backward pass, both of the calculation steps are based on convolution, i.e., matrix multiplication. Equation 4 uses sparse weight matrix (tensor) as the operand, thus can be accelerated in the same way as the forward propagation. Equation 5 allows a sparse output result since the gradient matrix is also sparse. Thus, both two steps have reduced computations, which are roughly proportional to the sparsity ratio, providing the acceleration for the backward propagation phase. Compiler optimizations are used to accelerate the inference in prior works (21; 46; 61). In this work, we extend the compiler optimization techniques for accelerating the forward and backward pass during training on the edge devices. Our compiler optimizations are general, support both sparse model training and inference accelerations on mobile platforms. The optimizations include 1) the supports for sparse models; 2) an auto-tuning process to determine the best-suited conﬁgurations of parameters for different mobile CPUs. The details of our compiler optimizations are presented as follows. E.1.1 Supports for Sparse Models Our framework supports sparse model training and inference accelerations with unstructured pruning. For the sparse (pruned) model, the framework ﬁrst compacts the model storage with a compression format called Compressed Sparse Row (CSR) format, and then performs computation reordering to reduce the branches within each thread and eliminates the load imbalance among threads. A row reordering optimization is also included to further improve the regularity of the weight matrix. After this reordering, the continuous rows with identical or similar numbers of non-zero weights are processed by multi-threads simultaneously, thus eliminating thread divergence and achieving load balance. Each thread processes more than one rows, thus eliminating branches and improving instruction-level parallelism. Moreover, a similar optimization ﬂow (i.e., model compaction and computation reorder and other optimizations) is employed to support all compiler optimizations for sparsity as PatDNN (46). E.1.2 Auto-Tuning for Different Mobile CPUs During DNN sparse training and inference execution, there are many tuning parameters, e.g., matrix tiling sizes, loop unrolling factors, and data placement on memory, that inﬂuence the performance. It is hard to determine the best-suited conﬁguration of these parameters manually. To alleviate this problem, our compiler incorporates an auto-tuning approach for sparse (pruned) models. The Genetic Algorithm is leveraged to explore the best-suited conﬁgurations automatically. It starts the parameter search process with an arbitrary number of chromosomes and explores the parallelism better. Acceleration codes for different DNN models and different mobile CPUs can be generated efﬁciently and quickly through this auto-tuning process. E.2 Inference Acceleration Results On Mobile 60 12 18 2 4 A ccelation R esult s of R esNet -18 on  Split CIF AR-10 0 . 00 0 . 7 5 0 . 90 0 . 95 100 20 30 40 A ccelation R esult s of R esNet -18 on Split Tin y-ImageNet 0 . 00 0 . 7 5 0 . 90 0 . 95 Figure 5: Inference results of sparse models obtained from SparCL under different sparsity ratio compared with dense models obtained from traditional CL methods (sparsity ratio 0.00). Besides accelerating the training process, SparCL also possesses the advantages of providing a sparse model as the output for faster inference. To demonstrate this, we show the inference acceleration results of SparCL with different sparsity ratio settings on mobile in Figure 5. The inference time is measured on the CPU of an off-the-shelf Samsung Galaxy S20 smartphone. Each test takes 50 runs on different inputs with 8 threads on CPU. As different runs do not vary greatly, only the average time is reported. From the results we can see that the obtained sparse model from SparCL can signiﬁcantly accelerate the inference on both Split-CIFAR-10 and Tiny-ImageNet dataset compared to the model obtained by traditional CL training. For ResNet-18 on Split-CIFAR-10, the model obtained by traditional CL training, which is a dense model, takes 18.53ms for inference. The model provided by SparCL can achieve an inference time of 14.01ms, 8.30ms, and 5.85ms with sparsity 18ratio of 0.75, 0.90, and 0.95, respectively. The inference latency of the dense ResNet-18 obtained by traditional CL training on Tiny-ImageNet is 39.64 ms. While the sparse models provided by SparCL with sparsity ratio settings as 0.75, 0.90, and 0.95 reach inference speed of 33.06ms, 20.37ms, and 15.49ms, respectively, on Tiny-ImageNet. 19",
      "meta_data": {
        "arxiv_id": "2209.09476v1",
        "authors": [
          "Zifeng Wang",
          "Zheng Zhan",
          "Yifan Gong",
          "Geng Yuan",
          "Wei Niu",
          "Tong Jian",
          "Bin Ren",
          "Stratis Ioannidis",
          "Yanzhi Wang",
          "Jennifer Dy"
        ],
        "published_date": "2022-09-20T05:24:48Z",
        "pdf_url": "https://arxiv.org/pdf/2209.09476v1.pdf"
      }
    },
    {
      "title": "MECTA: Memory-Economic Continual Test-Time Model Adaptation"
    },
    {
      "title": "Improving Task-free Continual Learning by Distributionally Robust Memory Evolution",
      "abstract": "Task-free continual learning (CL) aims to learn a non-stationary data stream\nwithout explicit task definitions and not forget previous knowledge. The widely\nadopted memory replay approach could gradually become less effective for long\ndata streams, as the model may memorize the stored examples and overfit the\nmemory buffer. Second, existing methods overlook the high uncertainty in the\nmemory data distribution since there is a big gap between the memory data\ndistribution and the distribution of all the previous data examples. To address\nthese problems, for the first time, we propose a principled memory evolution\nframework to dynamically evolve the memory data distribution by making the\nmemory buffer gradually harder to be memorized with distributionally robust\noptimization (DRO). We then derive a family of methods to evolve the memory\nbuffer data in the continuous probability measure space with Wasserstein\ngradient flow (WGF). The proposed DRO is w.r.t the worst-case evolved memory\ndata distribution, thus guarantees the model performance and learns\nsignificantly more robust features than existing memory-replay-based methods.\nExtensive experiments on existing benchmarks demonstrate the effectiveness of\nthe proposed methods for alleviating forgetting. As a by-product of the\nproposed framework, our method is more robust to adversarial examples than\nexisting task-free CL methods. Code is available on GitHub\n\\url{https://github.com/joey-wang123/DRO-Task-free}",
      "full_text": "Improving Task-free Continual Learning by Distributionally Robust Memory Evolution Zhenyi Wang1 Li Shen 2 Le Fang 1 Qiuling Suo 1 Tiehang Duan 3 Mingchen Gao 1 Abstract Task-free continual learning (CL) aims to learn a non-stationary data stream without explicit task deﬁnitions and not forget previous knowledge. The widely adopted memory replay approach could gradually become less effective for long data streams, as the model may memorize the stored examples and overﬁt the memory buffer. Second, existing methods overlook the high un- certainty in the memory data distribution since there is a big gap between the memory data dis- tribution and the distribution of all the previous data examples. To address these problems, for the ﬁrst time, we propose a principled memory evolution framework to dynamically evolve the memory data distribution by making the mem- ory buffer gradually harder to be memorized with distributionally robust optimization (DRO). We then derive a family of methods to evolve the memory buffer data in the continuous probabil- ity measure space with Wasserstein gradient ﬂow (WGF). The proposed DRO is w.r.t the worst-case evolved memory data distribution, thus guarantees the model performance and learns signiﬁcantly more robust features than existing memory-replay- based methods. Extensive experiments on ex- isting benchmarks demonstrate the effectiveness of the proposed methods for alleviating forget- ting. As a by-product of the proposed framework, our method is more robust to adversarial exam- ples than existing task-free CL methods. Code is available on GitHub https://github.com/ joey-wang123/DRO-Task-free 1Department of Computer Science and Engineering, Univer- sity at Buffalo, NY , USA2JD Explore Academy, Beijing, China 3Meta, Seattle, WA, USA. Correspondence to: Zhenyi Wang <zhenyiwa@buffalo.edu>, Li Shen <mathshenli@gmail.com>, Mingchen Gao <mgao8@buffalo.edu>. Proceedings of the39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s). 1. Introduction Continual learning (CL) is to learn on a sequence of tasks without forgetting previous ones. Most CL methods assume knowing task identities and boundaries during training. In- stead, this work focuses on a more general and challeng- ing setup, i.e., task-free continual learning (Aljundi et al., 2019b). This learning scenario does not assume explicit task deﬁnition, and data distribution gradually evolves with- out clear task boundaries, making it applicable to a broader range of real-world problems. The current widely adopted memory replay approach stores a small portion of previous tasks in memory and replays them with the new mini-batch data. The CL model would overﬁt the memory buffer, and this approach could be grad- ually less effective for mitigating forgetting as the model repeatedly learns the memory buffer (Jin et al., 2021), il- lustrated in Figure 1 (a). The model could memorize the memory buffer, and previous knowledge will be quickly lost. Furthermore, there is a big gap between memory data distribution and the distribution of all the previous data ex- amples. These methods ignore the high uncertainty of the memory data distribution since a limited memory buffer cannot accurately reﬂect the stationary distribution of all examples seen so far in the data stream, illustrated in Figure 2 (a). Most existing CL works ignore this issue. To address the above issues, we propose a distributionally robust optimization framework (DRO) to evolve the memory data distribution dynamically. This learning objective makes the memory data increasingly harder to be memorized and helps the model alleviate memory overﬁtting and improve generalization, illustrated in Figure 1 (b) and (c). In addition, the proposed DRO framework considers the underlying high uncertainty of memory data distribution. It evolves memory data distribution to ﬁll the gap between the memory data distribution and the ideal stationary distribution of all the previous data, illustrated in Figure 2 (b). We optimize the model performance under the worst-case evolved memory data distribution since we cannot know the exact distribu- tion of all the previous examples. The model can thus learn substantially more robust features with performance guaran- tees than previous work. For the image domain, adversarial examples can have the same appearance as natural images arXiv:2207.07256v2  [cs.LG]  20 Aug 2022Improving Task-free Continual Learning by Distributionally Robust Memory Evolution but have different classiﬁcation results. Our proposed DRO memory evolution framework is robust to such adversarial examples due to the worst-case performance optimization on the evolved memory data distribution. (a) t= t0 (easy to memorize and overﬁt) (b) t= t1 (diverse and hard) (c) t= t2 (diverse and harder to classify) Figure 1.T-SNE visualization of evolved memory embedded by ResNet18 on CIFAR10 at different CL timestamps. Each dot repre- sents a data point’s feature extracted by the last layer of ResNet18. Each color denotes one class of memory data. Initially, the classes are very easy to memorize and overﬁt. Memory evolution makes the memory more diverse and harder to classify and memorize. (a) Experience replay (raw memory data) (b) Our method (evolved memory data) Figure 2.The blue line denotes the stationary distribution of all the previous data, and the red line denotes the memory buffer data distribution. (a): Standard experience replay (ER) (left) replays on the raw memory data. There is a big gap between the raw memory data distribution µ0 and the stationary distribution of all the data in the data stream ν. (b): Our method (right) ﬁlls this gap by evolving the memory data distribution to narrow the gap between the evolved memory data distribution µT and ν. Formally, we ﬁrst design energy functional that measures the degree of forgetting on a speciﬁc memory data distribu- tion and the distributional distance between the evolved and raw memory distribution. The goal is to minimize the energy functional F(µ) for the worse-case performance probability measure (density) πthat is within the neighbor probability measures of the raw memory data probability measure µ0. We name this optimization as task-free DRO. However, it is extremely challenging to solve this optimization problem in an inﬁnite-dimensional probability measure space (function space) which contains an inﬁnite number of probability mea- sures. To efﬁciently solve the task-free DRO, we formulate it from a new continuous dynamics perspective and convert it into a gradient ﬂow system. Speciﬁcally, the memory data distribution evolves in probability measure space as Wasserstein gradient ﬂow (WGF), and model parameters evolve in Euclidean space. We name it as Dynamic DRO, which makes it convenient to use function gradient-descent in probability measure space to solve the task-free DRO. Also, it facilitates the derivation of different methods for memory data distribution evolution. We then introduce three speciﬁc memory evolution methods to solve the Dynamic DRO, including Langevin Dynamics (Welling & Teh, 2011), Stein Variational Gradient Descent (Liu & Wang, 2016), and Hamiltonian ﬂow (Ma et al., 2015; Liu et al., 2019). The proposed memory evolution framework is general, ﬂexible, and easily extendable, with many potential extensions for fu- ture research. We evaluate the effectiveness of the proposed framework by performing comprehensive experiments on several datasets and comparing them to various strong base- lines. We summarize our contributions as follows: • We propose the ﬁrst principled, general, and ﬂexible memory evolution framework for task-free CL from the perspective of distributionally robust optimization, named task-free DRO. Our proposed method is sub- stantially more effective for mitigating forgetting. As a by-product of the proposed DRO framework, our method is more robust to adversarial examples than previous works. • We formulate the task-free DRO from a new contin- uous dynamics perspective and cast it as a gradient ﬂow system, named Dynamic DRO. We propose a fam- ily of memory evolution methods with different ways to efﬁciently solve the Dynamic DRO, opening up a new research direction for presenting new strategies to evolve the memory data. • Extensive experiments on several datasets demonstrate the effectiveness of the proposed method for reducing forgetting and increasing the robustness to adversarial examples. Our framework is versatile and can be seam- lessly integrated with existing memory-replay-based methods to improve their performance. 2. Related Work Continual learning (CL) aims to maintain previous knowledge when learning on sequential tasks with data distribution shift. Most existing CL methods (Lopez-Paz & Ranzato, 2017; Nguyen et al., 2018; Kirkpatrick et al., 2017; Zenke et al., 2017; von Oswald et al., 2019; Wang et al., 2021; Saha et al., 2021; Pham et al., 2021; Wang et al., 2022) are only applicable to the task-aware setting, where there are clear task deﬁnitions and boundaries among the sequentially learned task sequences. Task-free continual learning (He et al., 2019; Zeno et al., 2019; Aljundi et al., 2019b; Chrysakis & Moens, 2020) instead focuses on the more general case where the data distribution could changeImproving Task-free Continual Learning by Distributionally Robust Memory Evolution arbitrarily without explicit task splits. This learning sce- nario has become increasingly important due to the broader application scenarios, and more challenging problem nature (Aljundi et al., 2019b; Lee et al., 2020). Task-free CL Existing approaches for task-free CL can be categorized into two classes. The ﬁrst (majority) one is memory-based methods (Aljundi et al., 2019c;a), which store a small number of data from the previous data stream and replay them with the new mini-batch data later. The sec- ond type is the expansion-based method, such as CN-DPM (Lee et al., 2020). However, CN-DPM needs to increase the memory and computation overhead with the network structure’s expansion and the increase of the network param- eters. Hence, this work focuses on memory-replay-based methods without expanding the network structure since it is simple and effective. Most existing works in this category (Chaudhry et al., 2019b;a) directly perform replay on the raw data without any adaptation. MIR (Aljundi et al., 2019a) pro- poses to replay the samples with which are most interfered. GEN-MIR (Aljundi et al., 2019a) further uses generative models to synthesize the memory examples. The heuristic method, GMED (Jin et al., 2021), proposes to edit memory examples so that the examples are more likely forgotten and harder to be memorized. Our methods share similar moti- vations with GMED, which individually edits the memory data without considering memory data distribution uncer- tainty and population-level statistics. In contrast, our DRO framework focuses on population-level and distribution- level evolution. Orthogonal to our work, Gradient-based Sample Selection (GSS) (Aljundi et al., 2019c) focuses on storing diverse examples. These methods lack theoretical guarantees. In contrast, our framework is principled and focuses on evolving memory data distributions. Distributionally robust optimization (DRO) is an effec- tive optimization framework to handle decision-making un- der uncertainty (Rahimian & Mehrotra, 2019). The basic idea of DRO is ﬁrst to construct a set of probability distri- butions as an ambiguity set and minimize the worst-case performance in this ambiguity set, thus guaranteeing the model performance. There are various applications of DRO in machine learning problems, including tackling the group- shift (Sagawa et al., 2020), subpopulation shift (Zhai et al., 2021), and class imbalances (Xu et al., 2020). To our best knowledge, our work is the ﬁrst principled method with DRO for memory evolution in task-free CL, named task-free DRO. It dynamically evolves memory data distributions to avoid forgetting and learn robust features. In addition, we formulate the proposed task-free DRO from a new continuous dynamics perspective, making it convenient to handle the evolving memory data distribution with differ- ent evolution dynamics. Besides, we propose three ways to evolve the memory data, opening up a new research direc- tion to explore more effective memory evolution methods. 3. Method In this section, we ﬁrst present the problem setup in Section 3.1 for the task-free CL setting. Then, we propose our task- free DRO framework in Section 3.2. Next, we view the task-free DRO from a new continuous dynamics perspective and formulate the task-DRO in an equivalent gradient ﬂow system, named Dynamic DRO, in Section 3.3. In Section 3.4, we present three speciﬁc memory evolution methods to solve the Dynamic DRO efﬁciently. 3.1. Problem Setup A sequence of mini-batch labeled data (xk,yk,hk) sequen- tially arrives at each timestampkand forms a non-stationary data stream. Each data point is associated with a latent task identity hk, where xk denotes the mini-batch data received at timestamp k, yk is the data label associated with xk. Ac- cording to (Aljundi et al., 2019b), a more general deﬁnition of task-free CL is that data distribution shift could happen at any time without explicit task splits. Our method can be directly applied to those more general cases as well. Dur- ing both the training and testing time, the task identity hk is not available to the learner. At the same time, a small memory buffer Mis maintained, and replay the data in M when learning the new task to avoid forgetting the previ- ously learned knowledge. The memory bufferMis updated by reservoir sampling (RS), similar to (Riemer et al., 2019). The goal is to learn a model f(x,θ) to perform well on all the tasks seen until timestamp k. Standard memory replay for CL (Chaudhry et al., 2019b) is to optimize an objective under a known probability distribution µ0. Formally speak- ing, CL with standard memory replay can be expressed as: min ∀θ∈Θ [L(θ,xk,yk) + E x∼µ0 L(θ,x,y)], (1) where θare model parameters and xis the raw memory buffer data with probability measure (density)µ0, i.e., ∀x∈ M,x∼µ0. L(θ,x,y) is the loss function associated with the data (x,y). In the following, we temporally omit the term L(θ,xk,yk) due to the fact that (xk,yk) is the mini- batch data arrived at timestamp kand does not depend on the memory data distribution. 3.2. DRO for Task-free CL Distributionally Robust Optimization (DRO) (Rahimian & Mehrotra, 2019) is a systematic and elegant framework to model the decision-making with ambiguity in the under- lying probability distribution. For task-free CL, different from the standard memory-replay methods in Section 3.1, implicitly assuming µ0 is known. In contrast, our proposedImproving Task-free Continual Learning by Distributionally Robust Memory Evolution DRO framework takes that the underlying actual probability distribution of memory data µis unknown and lies in an am- biguity set of probability distributions. Modeling the mem- ory data uncertainty is particularly useful when the memory buffer is small compared to the whole dataset since the memory has limited coverage to approximate the stationary distribution of all examples seen so far, illustrated in Figure 2 (a). Thus, there is signiﬁcant uncertainty in modeling the multi-task learning scenarios (the performance upper bound) with only a small memory buffer. The proposed DRO frame- work optimizes the worst-case performance in the ambiguity set of probability distributions since we cannot access the distribution of all the previous data. Therefore, it helps the model generalize to previous tasks since it can potentially narrow the gap between the memory data distribution and the distribution of all the previous data, illustrated in Figure 2 (b). As a by-product of this optimization framework, it also helps learn features robust to data distribution pertur- bations. On the other hand, the memory buffer is updated slowly during CL (e.g., by reservoir sampling), and standard memory-replay repeatedly trains the memory buffer, and the CL model can easily overﬁt the memory buffer, as illustrated in Figure 1 (a). Thus, the memory buffer could become less effective for mitigating forgetting. By optimizing the worst- case evolved memory data distribution at each iteration, our proposed DRO can also alleviate the memory overﬁtting problem by transforming the memory data to make them more difﬁcult to memorize. This is illustrated in Figure 1 (b) and (c). Mathematically speaking, the proposed DRO for task-free CL can be expressed as: min ∀θ∈Θ sup µ∈P EµL(θ,x,y) (2) s.t. P= {µ: D(µ||π) ≤D(µ0||π) ≤ϵ}, (3) E x∼µ,x′∼µ0 ∇θL(θ,x,y) ·∇θL(θ,x′,y) ≥λ, (4) where the inner sup optimization is to gradually make the memory data distribution increasingly harder to be memo- rized. Pin Eq. (3) denotes the ambiguity set of probability measures (distributions or densities) for the memory data distribution to characterize its uncertainty. One common choice to deﬁne Pis through Kullback-Leibler (KL) di- vergence. D(µ0||π) denotes the KL divergence between probability measure µ0 and π, where πis the target worst- case evolved memory data distribution, i.e., the probability distribution π that achieves supµ∈PEµL(θ,x,y). ϵ is a constant threshold to characterize the closeness between µ0 and π to ensure the worst-case evolved memory data distribution πdoes not deviate from the raw memory data distribution µ0 too much. Eq. (4) constrains the gradi- ent dot product between the worst-case evolved memory data distribution and raw memory data distribution, i.e., ∇θL(θ,x,y)·∇θL(θ,x′,y), to avoid the evolved memory data deviate from the raw memory data too much. Intu- itively, if the gradient dot product is positive, it means the evolved memory data has a similar update direction com- pared to the raw data. λ is a threshold to determine the constraint magnitude. To solve the optimization, i.e., Eq. (2-4), the worst-case optimization that involves the sup optimization within the KL-divergence ball is generally computationally intractable since it involves the optimization over inﬁnitely many prob- ability distributions. To address this problem, by Lagrange duality (Boyd & Vandenberghe, 2004), we convert Eq. (2- 4) into the following unconstrained optimization problem (detailed derivations are put in Appendix B): min ∀θ∈Θ sup µ [EµL(θ,x,y) −γD(µ||π)+ β E x∼µ,x′∼µ0 ∇θL(θ,x,y) ·∇θL(θ,x′,y)], (5) Where γ and β control the magnitude of regularization. Since the KL-divergence term D(µ||π) is implicitly han- dled by gradient ﬂow (discussed in the following sections), we set γ = 1 for simplicity throughout this paper. The gradient of the third term (gradient dot product term) can be efﬁciently approximated by ﬁnite difference in practice. The optimization Eq. (5) is still computationally hard to solve because the inner sup optimization is over probability measure space, which is an inﬁnite-dimensional function space. We name Eq. (5) as task-free DRO. 3.3. Task-free DRO: A Continuous Dynamics View To make it tractable to solve the task-free DRO Eq. (5), we formulate it from a new continuous dynamics perspective. This new perspective brings signiﬁcant advantages over di- rectly solving Eq. (5): 1) the optimization of Eq. (5) over probability measure space can be converted into a continu- ous probability distribution evolution, equivalent as a WGF, enabling gradient-based solutions in probability measure space (function space); 2) we can efﬁciently solve the WGF by various methods, which provide potential derivations of many different memory evolution methods. We then pro- pose a novel solution by decomposing the task-free DRO Eq. (5) into a gradient ﬂow system. Speciﬁcally, we solve the inner sup optimization problem for memory evolution with WGF in Wasserstein space of probability measures and solve the outer optimization problem for the model parame- ters with gradient ﬂow in Euclidean space. We convert the task-free DRO into a gradient ﬂow system that alternately updates the memory evolution and model parameters. Given a memory buffer at timestamp k, the raw memory data is M= {(x1 0,y1),(x2 0,y2),··· ,(xN 0 ,yN)}, where N is the memory buffer size. We perform a similar memory evolution procedure at each CL timestamp and omit the CL timestamp kfor notation clarity. We denotexi tas the ithdat- apoint in the evolved memory buffer after tevolution steps.Improving Task-free Continual Learning by Distributionally Robust Memory Evolution The raw memory data is assumed to be i.i.d. sampled from the random variable X0, i.e., {x1 0,x2 0,··· ,xN 0 }∼ X0. X0 follows the probability distribution µ0, i.e., X0 ∼µ0. At evolution time t, the memory data Mis evolved as random variable Xt, i.e., {x1 t,x2 t,··· ,xN t } ∼Xt and probabil- ity distribution of random variable Xt follows the prob- ability measure µt, i.e., Xt ∼µt. The empirical mea- sure on the evolved memory buffer at time tis deﬁned as ˆµt = 1 N ∑i=N i=1 δ(xi t) and δis the Dirac measure. We model the memory evolution process as continuous WGF in proba- bility measure space, i.e., use (µt)t≥0 to model the proba- bility distribution evolution of memory data. The evolving (µt)t≥0 will in turn determine the memory evolution process (Xt)t≥0 in Euclidean space. Below, we deﬁne and present the gradient ﬂow in Wasser- stein space. Let P2(Rd) denote the space of probability measures on Rd with ﬁnite second-order moments. Each el- ement µ∈P2(Rd) is a probability measure represented by its density function µ: Rd →R with respect to Lebesgue measure dx. The Wasserstein distance between two proba- bility measures µ1,µ2 ∈P2(Rd) is deﬁned as: W2(µ1,µ2) = ( min ω∈∏(µ1,µ2) ∫ ||x−x′||2dω(x,x′)) )1/2 , where ∏(µ1,µ2) = {ω|ω(A ×Rd) = µ1(A),ω(Rd × B) = µ2(B)}. ω is the joint probability measure with marginal measure of µ1 and µ2 respectively. Thus, W2 = (P2(Rd),W2) forms a metric space. Deﬁnition 3.1 (Wasserstein Gradient Flow) . (Ambrosio et al., 2008). Suppose we have a Wasserstein space W2 = (P2(Rd),W2). A curve of (µt)t≥0 of probability measures is a Wasserstein gradient ﬂow for functional F if it satisﬁes ∂tµt = ∇W2 F(µt) :=div ( µt∇δF δµ(µt) ) (6) where := means deﬁned as, and div(r) :=∑d i=1 ∂zi ri(z) is the divergence operator of a vector-valued function r : Rd →Rd, where zi and ri are the ith element of zand r; ∇is the gradient of a scalar-valued function.∇W2 F(µt) := div(µt∇δF δµ(µt)) is the Wasserstein gradient of functional F at µt, where δF δµ(µt) is the ﬁrst variation of F at µt. The ﬁrst variation of a functional in function space is analogous to the ﬁrst-order gradient of a function in Euclidean distance. We put more detailed deﬁnition in Appendix D. Intuitively, the WGF describes that the probability measure µt follows the steepest curve of the functional F(µ) in Wasserstein space of probability measures (function space) to gradually move towards the target probability measure. We deﬁne the energy functionalF(µ) for memory evolution as the following: F(µ) =V(µ) +D(µ||π) (7) V(µ) =−EµL(θ,x,y)−βEµ∇θL(θ,x,y)·∇θL(θ,x′,y). (8) By deﬁning such energy functional F(µ), the Eq. (5) can be equivalently solved by the following gradient ﬂow system Eq. (9, 10), we name it as Dynamic DRO.   ∂tµt = div ( µt∇δF δµ(µt) ) ; (9) dθ dt = −∇θEµt L(θ,x,y), (10) where Eq. (9) solves the inner sup problem in Eq. (5) with WGF in Wasserstein space and Eq. (10) solves the outer minimization problem in Eq. (5) for parameter update with gradient ﬂow in Euclidean space. In the following, we focus on solving Eq. (9) and Eq. (10). 3.4. Training Algorithm for Dynamic DRO We propose three different methods for efﬁciently solving the Dynamic DRO in Eq. (9)-Eq. (10). The ﬁrst solution is Langevin dynamics with a diffusion process to evolve the memory data distribution; we name this method WGF-LD. Next, different from WGF-LD, which relies on randomness to transform the memory data, we kernelize the WGF in Eq. (9) by solving the WGF in reproducing kernel Hilbert space (RKHS). It deterministically transforms the memory data; we name this method WGF-SVGD. Furthermore, we generalize the above WGF and improve their ﬂexibility to incorporate prior knowledge or geometry information. One instantiation of this general WGF uses Hamiltonian dynamics; we name it WGF-HMC. More novel memory evolution methods are worth exploring in future work. Before introducing the proposed solutions, we ﬁrst present some preliminaries for solving the WGF. By calculus of variation (Miersemann, 2012), the ﬁrst variation for the KL divergence and V(µ) are (Ambrosio et al., 2008): δD(µ||π) δµ = logµ π + 1; δV(u) δµ =U(x,θ)= −L(θ,x,y)−β∇θL(θ,x,y)·∇θL(θ,x′,y). Langevin Dynamics for Dynamic DRO. If we directly use the energy functional F(µ) (Eq. (7)) in Eq. (9), solving gradient ﬂow Eq. (9) corresponds to the Langevin dynamics with the following stochastic differential equation (Welling & Teh, 2011): dX = −∇XU(X,θ)dt+ √ 2dWt, (11)Improving Task-free Continual Learning by Distributionally Robust Memory Evolution (a) t= t0  (b) t= t1  (c) t= t2 Figure 3.Illustration of WGF-LD for memory evolution at differ- ent time t0 <t1 <t2. Initially (t= t0), the raw memory data is easy to overﬁt. From t= t1 to t= t2, the memory data becomes harder to classify and overﬁt. The black arrow (corresponds to the ﬁrst term (∇xU(xi t,θ))), drives the memory data to become harder to classify. The white circle (corresponds to the second term (Brownian motion)) serves as a random force such that the memory data becomes more diverse. where X = (Xt)t≥0 is the memory evolution process as previously deﬁned. W = (Wt)t≥0 is the standard Brownian motion in Rn (Bass, 2011). If Xt ∼µt evolves according to the Langevin dynamics Eq. (11) in Euclidean space, then µ(x,t) =µt(x) evolves according to gradient ﬂow Eq. (9) in the space of probability measures (Jordan et al., 1998). If we discretize the above equation and view each datapoint in memory as one particle, the memory buffer data evolves with Langevin dynamics to obtain diverse memory data by the following updates: xi t+1 −xi t = −α(∇xU(xi t,θ)) + √ 2αξt. (12) As illustrated in Figure 3, the ﬁrst term in Eq. (12) drives the particles (memory data) towards the worst-case mem- ory data distribution πto make the memory data gradually harder to be memorized by dynamically increasing the en- ergy functional. For the second term, it adds noise to en- courage diversity in the transformed memory data, where ξt is standard Gaussian noise, and αis step size, i.e., a proper amount of added Gaussian noise tailored to the used step size. We name this memory evolution method as WGF-LD. Kernelized Method for Dynamic DRO. We replace the Wasserstein gradient ∇W2 F(µt) by the transforma- tion Kµ∇W2 F(µt) under the integral operator Kµf(x) =∫ K(x,x′)f(x′)dµ(x′); where the RKHS space induced by the kernel Kis denoted by H. The probability measure in the kernelized Wasserstein space actually follows the kernelized WGF (Liu, 2017): ∂tµt = div(µtKµt ∇δF δµ(µt)). (13) Eq. (13) can be viewed as the WGF Eq. (9) in RKHS. It indicates that the random variable Xt which describes the evolved memory data at time t evolves as the following (a) t= t0  (b) t= t1  (c) t= t2 Figure 4.Illustration of WGF-SVGD for memory evolution at dif- ferent time t0 <t1 <t2. Initially (t= t0), the raw memory data is easy to overﬁt. From t= t1 to t= t2, the memory data becomes harder to classify and overﬁt. The black arrow (corresponds to the ﬁrst term ( k(xi t,xj t)∇xj t U(xj t,θ))) drives the memory data to become harder to classify. The orange arrow (corresponds to the second term (∇xj t k(xi t,xj t))) serves as repulsive force such that the memory data becomes more diverse. differential equation (Liu, 2017): dX dt = −[Kµ∇δF δµ(µt)](X). (14) This kernelized version is the deterministic approximation of the WGF in Eq. (9) (Liu & Wang, 2016). If we discretize the above equation and view each datapoint in memory as one particle, we can obtain the following memory evolution update equation: xi t+1−xi t = −α N j=N∑ j=1 [k(xi t,xj t)∇xj t U(xj t,θ)    smoothed gradient + ∇xj t k(xi t,xj t)    repulsive term ], (15) As illustrated in Figure 4, the ﬁrst term drives the memory data towards the worst-case memory data distribution by increasing the energy functional. The update is driven by the kernel weighted sum of the gradients from the memory data points, thus smoothing the memory data gradients. The second term serves as a repulsive force that prevents the memory data points from collapsing into a single mode, thus diversifying the memory data population. In this pa- per, we use Gaussian kernel k(xi,xj) =exp(−(xi−xj)2 2σ2 ). We name this memory evolution method as WGF-SVGD. We put detailed derivations of this evolution equation in Appendix E. General Memory Evolution for Dynamic DRO. (Ma et al., 2015) found that any continuous Markov process that provides samples from the target distribution can be written in a very general sampler form. The corresponding general WGF for memory evolution can be written as: ∂tµt = div(µt(D+ Q)∇δF δµ(µt)), (16)Improving Task-free Continual Learning by Distributionally Robust Memory Evolution Where Dis a positive semideﬁnite diffusion matrix, Qis a skew-symmetric curl matrix representing the deterministic traversing effect (Ma et al., 2015). One particular case is: D= ( 0 0 0 C ) ,Q= ( 0 −I I 0 ) , Where Cis the friction term, and Iis the identity matrix. This WGF corresponds to Hamiltonian dynamics (Chen et al., 2014) and can be solved by the following evolution: { xt+1 −xt = vt, vt+1 −vt = −α∇U(x,θ) −τv+ √ 2ταξt, (17) Where vis the momentum variable, and τ is the momentum weight. We name this method as WGF-HMC. The most attractive property of this WGF for memory distri- bution evolution is that we can freely specify the matrix Q and Dtailored to practical requirements. We can consider prior knowledge or geometry information by designing tai- lored Qand D, or develop the kernelized version of this general WGF. These research directions specialized for CL are left as future work to explore. Our framework is quite ﬂexible and general due to the energy functional design and WGF speciﬁcation. The proposed memory evolution algorithm is shown in Algo- rithm 1, with the ﬂexibility to use various evolution methods. Line 3-4 describes that a mini-batch data arrives at time k and samples a mini-batch data from the memory buffer. Line 5-7 is to evolve the mini-batch memory data with T steps depending on using which evolution methods. Line 8 updates the model parameters with the evolved memory data and mini-batch data received at time k. Line 9 updates the memory buffer with the mini-batch data received at time kusing reservoir sampling. Note that we do not replace the raw memory data with the evolved memory data for imple- mentation simplicity in the current version. If we replace the raw memory data with the evolved one, we can reduce the number of evolution steps needed at each CL timestamp to improve efﬁciency since an example can be adequately evolved after accumulating many timestamps. Earlier ex- amples can be evolved less frequently than the later ones. But replacing the raw memory data with the evolved one could slightly reduce the performance in our experiment. Our method needs to store a mini-batch of evolved memory data. This memory cost is negligible compared to the entire data stream memory buffer. We can view memory evolution from two perspectives. First, local evolutionevolves the cur- rent raw memory data distribution with several adaptation steps. Second, global evolutionevolves the memory data at different CL timestamps due to different model parameters. Discrete input. For the application of our methods in the discrete input domain (such as text), text can be embedded in continuous space (e.g., word embedding) and then evolve on embedding with our methods. Algorithm 1 Distributionally Robust Memory Evolution. 1: REQUIRE: model parameters θ, learning rate η, evolution rate (step size)α, number of evolution stepsT at each iteration, memory buffer M; Kis the number of mini-batch data in the data stream. 2: for k= 1to Kdo 3: a new mini-batch data (xk,yk) arrives. 4: sample mini-batch from memory buffer, i.e., (x,y) ∼M 5: for t= 1to T do 6: (x,y) = Evolve((x,y)) by WGF-LD (Eq. (12)) or WGF-SVGD (Eq. (15))) or WGF-HMC (Eq. (17)). 7: end for 8: θk+1 = θk −η∇θ[L(θk,x,y) +L(θk,xk,yk)] 9: update memory buffer by reservoir sampling, M = reservoirsampling(M,(xk,yk)) 10: end for 4. Experiments To evaluate the effectiveness of our memory evolution meth- ods, we compared a variety of state-of-the-art baseline ap- proaches. We ﬁrst describe the benchmark datasets and baselines in Section 4.1. Then, we compare various base- lines on several datasets in Section 4.2, and evaluate the methods in terms of robustness to adversarial examples in Section 4.3. We perform ablation study in Section 4.4. 4.1. Experiment Setup CIFAR10, following (Aljundi et al., 2019a), we split the CIFAR-10 dataset into 5 disjoint tasks with the same train- ing, validation, and test sets. MiniImagenet, following (Aljundi et al., 2019a), we split MiniImagenet (Vinyals et al., 2016) dataset with 100 image classes into 20 disjoint tasks. Each task has 5 classes. CIFAR-100 (Krizhevsky, 2009), contains 100 image classes. We also split it into 20 disjoint tasks. Baselines. We performed comprehensive experiments by comparing to the following strong baselines, including Ex- perience Replay (ER) (Chaudhry et al., 2019b), Maximally Interfering Retrieval (MIR) (Aljundi et al., 2019a), AGEM (Chaudhry et al., 2019a), Gradient-Based Sample Selection (GSS-Greedy) (Aljundi et al., 2019c) and GMED (Jin et al., 2021). Furthermore, following (Jin et al., 2021), we also compare data augmentation, such as random rotations, scal- ing, and horizontal ﬂipping applied to memory buffer data in ER and name this baseline as ERaug. Following (Aljundi et al., 2019a), we also compare (1) ﬁne-tuning, which trains on each latent task sequentially when new batches of each task arrive without any forgetting mitigation mechanism; (2) iid online: which trains the model with a single-pass through the iid sampled data on the same set of samples;Improving Task-free Continual Learning by Distributionally Robust Memory Evolution (3) iid ofﬂine: (upper-bound) which trains the model with multiple passes through the iid sampled data. We train the model with 5 epochs for this baseline. Detailed descriptions of the compared baselines are placed in Appendix A. In addition, our proposed methods are orthogonal to existing memory-replay-based CL methods. Thus, they are versatile, and we can seamlessly combine the proposed methods with them. For illustration, we combine our proposed methods with ER, MIR, and GMED to show the effectiveness. We name the combination methods ER+WGF-LD, ER+WGF- SVGD, ER+WGF-HMC, MIR+WGF-LD, GMED+WGF- LD, etc. Combining with other memory-replay-based meth- ods is straightforward. Implementation Details. We use the Resnet-18 as (Aljundi et al., 2019a). The number of evolution steps T is set to be 5, the evolution rate αis set to be 0.01 for CIFAR10, 0.05 for CIFAR100 and 0.001 for MiniImagenet, β = 0.003 and momentum τ = 0.1. We set the memory buffer size to be 500 for CIFAR-10 dataset, 5K for CIFAR-100 and 10Kfor MiniImagenet. All other hyperparameters are the same as (Aljundi et al., 2019a). All reported results in our experiments are the average accuracy of 10 runs with standard deviation. 4.2. Comparison to Continual Learning We compare the proposed methods to various task-free CL baselines. Table 1 shows the effectiveness of combining the proposed method with existing memory-replay methods, e.g., ER, MIR and GMED. We can observe that our method outperforms these strong baselines. In particular, for ER and ER + WGF methods, our method outperforms baselines by 4.5%, 1.4%, and 2.8% on CIFAR10, CIFAR-100, and Mini- ImageNet, respectively. For MIR and MIR + WGF methods, our method outperforms baselines by3.8%, 1.6%, and 2.1% on CIFAR10, CIFAR-100, and MiniImageNet, respectively. For GMED and GMED + WGF methods, our method out- performs baselines by 3.6%, 0.9%, and 1.4% on CIFAR10, CIFAR-100, and MiniImageNet, respectively. Our methods outperform baselines because they dynamically evolve the memory data distribution and sufﬁciently explore the input space in a principled way. The proposed methods generate more diverse memory data, and the evolved memory be- comes more difﬁcult for the CL model to memorize than baseline methods. In addition, WGF-SVGD generally per- forms better than WGF-SGLD and WGF-HMC. We believe this is because, in RKHS, the evolved memory data is en- couraged to be far apart by the kernel repulsive forces; the evolved memory data may better represent the distribution of all the previous data. Table 1.Comparison to continual learning baselines on CIFAR10, CIFAR-100 and MiniImagenet by combing our proposed method with existing CL methods Algorithm CIFAR10 CIFAR-100 MiniImagenet ﬁne-tuning 18.9±0.1 3 .1±0.2 2 .9±0.5 A-GEM 19.0±0.3 2 .4±0.2 3 .0±0.4 GSS-Greedy 29.9±1.5 19 .5±1.3 17 .4±0.9 ER 33.3±2.8 20 .1±1.2 24 .8±1.0 ER + WGF-LD 37.6±1.5 21.5±1.3 27.3±1.0 ER + WGF-SVGD 36.5±1.4 21 .3±1.5 27.6±1.3 ER + WGF-HMC 37.8±1.3 21.2±1.4 27 .2±1.1 MIR 34.4±2.5 20 .0±1.7 25 .3±1.7 MIR + WGF-LD 38.2±1.2 21.6 ±1.2 26.9±1.0 MIR + WGF-SVGD37.0±1.4 21 .2±1.5 27.4±1.2 MIR + WGF-HMC 37.9±1.5 21 .3±1.4 27 .1±1.3 GMED (ER) 34.8±2.2 20 .9±1.6 27 .3±1.8 GMED + WGF-LD 38.4±1.6 21.7±1.7 28 .3±1.9 GMED + WGF-SVGD37.6±1.7 21.8±1.5 28.7 ±1.5 GMED + WGF-HMC37.8±1.2 21 .5±1.9 28 .4±1.3 ERaug+ ER 46.3±2.7 18 .3±1.9 30 .8±2.2 ERaug+ WGF-LD 47.6±2.4 19 .8±2.2 31 .9±1.8 ERaug+ WGF-SVGD47.9±2.5 19.9±2.3 32.2±1.5 ERaug+ WGF-HMC 47.8±2.6 20.3±2.1 31.7±2.0 iid online 60.3±1.4 18 .7±1.2 17 .7±1.5 iid ofﬂine 78.7±1.1 44 .9±1.5 39 .8±1.4 Table 2.Carlini & Wagner attack model performance for the CIFAR-100 and Mini-Imagenet dataset Algorithm CIFAR-10 CIFAR-100 Mini-Imagenet ER 2.0±0.1 0 .0 0 .0 GMED 2.1±0.1 0 .0 0 .0 ER + WGF-LD 8.0±0.2 3.0±0.2 3.1 ±0.1 ER + WGF-SVGD 4.2±0.1 0 .0 2 .2±0.2 ER + WGF-HMC 8.2±0.3 2.5±0.2 3 .0±0.1 4.3. Robustness to Adversarial Perturbations In this section, we evaluate the robustness of the CL model to adversarial perturbed examples. Given a clas- siﬁer f(x,θ), for an image x, the goal is to ﬁnd another example x′that is close enough to xmeasured by some distance function D(x.x′) ≤ϵsuch that the classiﬁer clas- siﬁes it into another different class, i.e. f(x,θ) ̸= f(x′,θ). In this paper, we focus on the most commonly used ℓ∞and ℓ2 norm as the distance function. Our memory evolution methods optimize on the worst-case evolved memory data distribution during CL; the model would be thus naturally robust to adversarial perturbations. For ℓ∞norm attack, we evaluate the robustness under the strong PGD ℓ∞ attack (Madry et al., 2018), which con- structs adversarial examples with projected gradient descent and ℓ∞ norm constraints. The adversarial perturbation magnitude ranges from [1/255,2/255,··· ,10/255] with 20 steps attack and stepsize of 2 255 on CIFAR-100 and Mini- ImageNet. For ℓ2 norm attack, we adopt the strong Carlini & Wagner attack (Carlini & Wagner, 2017). For illustra-Improving Task-free Continual Learning by Distributionally Robust Memory Evolution tion, we evaluate the model robustness to adversarial exam- ples after training with ER, GMED, ER + WGF-LD, ER + WGF-SVGD, and ER + WGF-HMC, respectively. Fig- ure 5 shows the PGD ℓ∞attack results on CIFAR-100 and Mini-ImageNet. Our WGF-HMC and WGF-LD memory evolution signiﬁcantly outperforms naive ER baseline by 4% −12% depending on the perturbation magnitude. In addition, WGF-HMC and WGF-LD perform more robust than WGF-SVGD. We believe this is because the random- ness introduced in WGF-HMC and WGF-LD can better ex- plore the input space and thus can generate harder evolved memory data. WGF-SVGD smooths the function gradient by the kernel function, thus may generate less hard exam- ples. Table 2 shows the Carlini & Wagner attack results. We can see that under the strong Carlini & Wagner attack, ER baseline accuracy becomes zero, and our methods still outperform baselines ranging from 6.1%,3.0%,3.1% on CIFAR-10, CIFAR-100, Mini-ImageNet, respectively. Both results demonstrate the robustness of our proposed methods to adversarial examples. Figure 5.PGD ℓ∞ attack results on two datasets: CIFAR-100 (left) and Mini-ImageNet (right). 4.4. Ablation Study Effects of different memory size. To investigate the effect of different smaller memory sizes on the model performance, we evaluate the effects on Mini-ImageNet with memory sizes of 3000, 5000, and 10000. We evaluate the effects on CIFAR-100 with the memory sizes of 2000, 3000, and 5000. We show the results in Table 3. In most cases, our WGF memory evolution substantially outperforms the baselines with different memory buffer sizes. Effect of number of evolution steps . To investigate the effect of different evolution steps, we compare 3, 5, and 7 evolution steps, respectively. Table 4 shows the performance variation of different number of evolution steps. We can ﬁnd that the performance improves slightly with an increasing number of evolution steps. For efﬁciency and sufﬁciently ex- ploring the input space to evolve harder memory examples, we choose the evolution step of 5. Hyperparameter sensitivity. Due to space limitation, we put hyperparameter sensitivity analysis, including the regu- larizer weight βand evolution rate α, in Appendix C. Computation cost. We compare the proposed ER + WGF to ER to evaluate its running efﬁciency. Table 5 shows the Table 3.Effect of different memory size on the model performance for the CIFAR-100 and Mini-Imagenet datasets. CIFAR-100 Memory Size 2000 3000 5000 ER 11.2±1.0 15 .0±0.9 20 .1±1.2 ER + WGF-LD 12.9±1.2 17.0±1.1 21.5±1.3 ER + WGF-SVGD 12.3±1.1 16 .0±1.2 21 .3±1.5 ER + WGF-HMC 12.7±1.0 17.2±1.0 21.2±1.4 MIR 11.6±0.8 15 .6±1.0 20 .0±1.7 MIR + WGF-LD 13.1±0.9 17 .3±1.2 21.6±1.2 MIR + WGF-SVGD 12.7±1.0 16 .5±1.3 21 .2±1.5 MIR + WGF-HMC 13.2±1.2 17.5 ±1.1 21.3±1.4 Mini-Imagenet Memory Size 3000 5000 10000 ER 13.4±1.4 17 .9±1.6 24 .8±0.9 ER + WGF-LD 16.2±1.2 20.8±1.2 27 .3±1.0 ER + WGF-SVGD 15.7±1.2 21.3±1.0 27.6 ±1.3 ER + WGF-HMC 15.9±1.5 20 .6±1.4 27 .2±1.1 MIR 12.6±1.5 17 .4±1.2 25 .3±1.7 MIR + WGF-LD 15.5±1.4 20 .5±1.1 26 .9±1.0 MIR + WGF-SVGD 15.3±1.2 20.7±1.6 27.4 ±1.2 MIR + WGF-HMC 15.8±1.7 20.3±1.5 27 .1±1.3 Table 4.Effect of number of evolution steps on Mini-ImageNet. Evolution Steps 3 5 7 ER + WGF-LD 27.0±0.9 27 .3±1.0 27 .5±1.4 ER + WGF-SVGD27.2±1.2 27.6±1.3 27.2±1.2 ER + WGF-HMC 27.1±1.3 27 .2±1.1 27 .6±1.0 efﬁciency comparison results. We set the simple baseline ER with a running time unit of 1. Our method has 3-4 times the computational cost compared to a simple ER baseline. In future work, we will improve its computational efﬁciency. Table 5.Computation efﬁciency (relative training time) of the pro- posed method compared to baseline. Algorithm running time ER 1.0 ER + WGF-LD 3.4 ER + WGF-SVGD 4.1 ER + WGF-HMC 3.5 5. Conclusion This paper proposes a novel concept of DRO memory evolu- tion for task-free continual learning to dynamically evolve the memory data distribution to mitigate the memory over- ﬁtting issue and ﬁll the gap between the memory data dis- tribution and the distribution of all the previous data. We propose a family of memory evolution methods with WGF. The proposed principled framework is general, ﬂexible, and easily expandable. Future work includes designing more informative functional and novel gradient ﬂow dynamics to incorporate physical intuitions and geometry constraints.Improving Task-free Continual Learning by Distributionally Robust Memory Evolution Extensive experiments compared to various state-of-the- art methods demonstrate the effectiveness of the proposed method. Interestingly, our methods are more robust to ad- versarial examples than compared baselines. 6. Acknowledgement We thank all the anonymous reviewers for their insightful and thoughtful comments. This research was supported in part by NSF through grant IIS-1910492.Improving Task-free Continual Learning by Distributionally Robust Memory Evolution References Aljundi, R., Belilovsky, E., Tuytelaars, T., Charlin, L., Cac- cia, M., Lin, M., and Page-Caccia, L. Online continual learning with maximal interfered retrieval. Advances in Neural Information Processing Systems 32, pp. 11849– 11860, 2019a. Aljundi, R., Kelchtermans, K., and Tuytelaars, T. Task-free continual learning. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019b. Aljundi, R., Lin, M., Goujaud, B., and Bengio, Y . Gradient based sample selection for online continual learning. In Advances in Neural Information Processing Systems 30, 2019c. Ambrosio, L., Gigli, N., and Savare, G. Gradient ﬂows: In metric spaces and in the space of probability measures. (Lectures in Mathematics. ETH), 2008. Bass, R. F. Stochastic Processes. Cambridge Series in Sta- tistical and Probabilistic Mathematics. Cambridge Uni- versity Press, 2011. doi: 10.1017/CBO9780511997044. Boyd, S. and Vandenberghe, L. Convex optimization. Cam- bridge university press, 2004. Carlini, N. and Wagner, D. Towards evaluating the ro- bustness of neural networks. 2017 IEEE Symposium on Security and Privacy (SP), 2017. Chaudhry, A., Ranzato, M., Rohrbach, M., and Elhoseiny, M. Efﬁcient lifelong learning with a-gem. Proceedings of the International Conference on Learning Representa- tions, 2019a. Chaudhry, A., Rohrbach, M., Elhoseiny, M., Ajanthan, T., Dokania, P. K., Torr, P. H. S., and Ranzato, M. Continual learning with tiny episodic memories. https://arxiv.org/abs/1902.10486, 2019b. Chen, T., Fox, E., and Guestrin, C. Stochastic gradient hamiltonian monte carlo. In Proceedings of the 31st International Conference on Machine Learning, 2014. Chewi, S., Le Gouic, T., Lu, C., Maunu, T., and Rigollet, P. Svgd as a kernelized wasserstein gradient ﬂow of the chi- squared divergence. In Advances in Neural Information Processing Systems, 2020. Chrysakis, A. and Moens, M.-F. Online continual learning from imbalanced data. Proceedings of the 37th Interna- tional Conference on Machine Learning, 119:1952–1961, 2020. He, X., Sygnowski, J., Galashov, A., Rusu, A. A., Teh, Y . W., and Pascanu, R. Task agnostic continual learning via meta learning. https://arxiv.org/abs/1906.05201, 2019. Jin, X., Sadhu, A., Du, J., and Ren, X. Gradient-based edit- ing of memory examples for online task-free continual learning. Advances in Neural Information Processing Systems, 2021. Jordan, R., Kinderlehrer, D., , and Otto., F. The variational formulation of the fokker–planck equation.SIAM Journal on Mathematical Analysis, 1998. Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Des- jardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., Hassabis, D., Clopath, C., Kumaran, D., and Hadsell, R. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 2017. Krizhevsky, A. Learning multiple layers of features from tiny images. Technical report, 2009. Lee, S., Ha, J., Zhang, D., and Kim, G. A neural dirichlet process mixture model for task-free continual learning. In Proceedings of the 17th International Conference on Machine Learning, 2020. Liu, C., Zhuo, J., and Zhu, J. Understanding mcmc dynam- ics as ﬂows on the wasserstein spac. Proceedings of the International Conference on Machine Learning, 2019. Liu, Q. Stein variational gradient descent as gradient ﬂow. Advances in Neural Information Processing Sys- tems, 2017. Liu, Q. and Wang, D. Stein variational gradient descent: A general purpose bayesian inference algorithm. Advances in Neural Information Processing Systems, 2016. Lopez-Paz, D. and Ranzato, M. Gradient episodic memory for continual learning. Advances in Neural Information Processing Systems, 2017. Ma, Y .-A., Chen, T., and Fox, E. B. A complete recipe for stochastic gradient mcmc. Advances in Neural Informa- tion Processing Systems, 2015. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards deep learning models resistant to adversarial attacks. Proceedings of the International Con- ference on Learning Representations, 2018. Miersemann, E. Calculus of Variations, Lecture Notes. Leipzig University, 2012. Nguyen, C. V ., Li, Y ., Bui, T. D., and Turner, R. E. Vari- ational continual learning. Proceedings of the Interna- tional Conference on Learning Representations, 2018. Pham, Q., Liu, C., Sahoo, D., and HOI, S. Contextual transformation networks for online continual learning. Proceedings of the International Conference on Learning Representations, 2021.Improving Task-free Continual Learning by Distributionally Robust Memory Evolution Rahimian, H. and Mehrotra, S. Distributionally robust opti- mization: A review. 2019. Riemer, M., Cases, I., Ajemian, R., Liu, M., Rish, I., Tu, Y ., and Tesauro, G. Learning to learn without forget- ting by maximizing transfer and minimizing interference. International Conference on Learning Representations, 2019. Sagawa, S., Koh, P. W., Hashimoto, T. B., and Liang, P. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case gener- alization. International Conference on Learning Repre- sentations, 2020. Saha, G., Garg, I., and Roy, K. Gradient projection memory for continual learning. Proceedings of the International Conference on Learning Representations, 2021. Vinyals, O., Blundell, C., Lillicrap, T., kavukcuoglu, k., and Wierstra, D. Matching networks for one shot learning. 29, 2016. von Oswald, J., Henning, C., Sacramento, J., and Grewe, B. F. Continual learning with hypernetworks. https://arxiv.org/abs/1906.00695, 2019. Wang, Z., Duan, T., Fang, L., Suo, Q., and Gao, M. Meta learning on a sequence of imbalanced domains with difﬁ- culty awareness. In Proceedings of the IEEE/CVF Inter- national Conference on Computer Vision, pp. 8947–8957, 2021. Wang, Z., Shen, L., Duan, T., Zhan, D., Fang, L., and Gao, M. Learning to learn and remember super long multi- domain task sequence. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7982–7992, 2022. Welling, M. and Teh, Y . W. Bayesian learning via stochastic gradient langevin dynamics. Proceedings of the Interna- tional Conference on Machine Learning, 2011. Xu, Z., Dan, C., Khim, J., and Ravikumar, P. Class-weighted classiﬁcation: Trade-offs and robust approaches. Pro- ceedings of the International Conference on Machine Learning, 2020. Zenke, F., Poole, B., and Ganguli, S. Con- tinual learning through synaptic intelligence. https://arxiv.org/abs/1703.04200, 2017. Zeno, C., Golan, I., Hoffer, E., and Soudry, D. Task ag- nostic continual learning using online variational bayes. https://arxiv.org/abs/1803.10123, 2019. Zhai, R., Dan, C., Kolter, J. Z., and Ravikumar, P. Doro: Dis- tributional and outlier robust optimization. Proceedings of the International Conference on Machine Learning, 2021.Improving Task-free Continual Learning by Distributionally Robust Memory Evolution A. Baselines The detailed descriptions of baselines are the following: Experience Replay (ER) (Chaudhry et al., 2019b), which stores a small set of examples from previous tasks with reservoir sampling (Chaudhry et al., 2019b). At later time, we randomly sample a set of examples from the memory buffer to train with the received mini-batch data together to avoid forgetting. Maximally Interfering Retrieval (MIR) (Aljundi et al., 2019a), the goal of MIR is to select the examples that are easily forgettable for replay. The idea of MIR is not using randomly selected data from the memory buffer, but instead replaying the samples that would be (maximally) interfered by the new received data. Following (Aljundi et al., 2019a), we evaluate the model forgetting on 25 examples for Mini-ImageNet dataset, and 50 examples for other datasets. AGEM (Chaudhry et al., 2019a), AGEM tries to ensure that at every training step the average episodic memory loss over the previous tasks does not increase. They project the gradient update direction such that they are less interfered with current data by projecting the gradient to the closest in L2 norm gradient that keeps the angle within 90 degree. Gradient-Based Sample Selection (GSS-Greedy) (Aljundi et al., 2019c) is to encourage diverse examples in the memory buffer. We use GSS-Greedy, which is efﬁcient and performs the best. GMED (Jin et al., 2021). GMED is the recent memory-replay methods, which edits the memory data so that they are harder to be memorized, shares the similar hypothesis as (Aljundi et al., 2019a). B. Lagrangian Duality Derivation The basic idea in Lagrangian duality (Boyd & Vandenberghe, 2004) is to consider the constraints by augmenting the objective function with a weighted sum of the constraint functions. We ﬁrst convert the optimization Eq. (2), Eq. (3) and Eq. (4) into the following optimization. min ∀θ∈Θ sup µ∈P EµL(θ,x,y) (18) s.t. P= {µ: D(µ||π) ≤D(µ0||π) ≤ϵ} (19) − E x∼µ,x′∼µ0 ∇θL(θ,x,y) ·∇θL(θ,x′,y) ≤−σ. (20) Then, by Lagrangian duality (Boyd & Vandenberghe, 2004), we can get the equivalent constrained optimization: min ∀θ∈Θ sup µ [EµL(θ,x,y) −γD(µ||π)+ β E x∼µ,x′∼µ0 ∇θL(θ,x,y) ·∇θL(θ,x′,y)]. (21) where γand βare the Lagrange multiplier associated with the corresponding inequality constraints. C. More Experiments Results Hyperparameter Sensitivity Analysis We use ER+WGF-LD as ablation study, similar results for other memory evolution methods. We perform sensitivity analysis for the regularization weight βof gradient dot product between perturbed memory data and raw data. The results are shown in Table 6. In particular, we show the results thatβ = 0.0, i.e., without gradient dot product regularization. Also, we perform sensitivity analysis on memory evolution rate αin Table 7, 8 and 9.Improving Task-free Continual Learning by Distributionally Robust Memory Evolution Table 6.Sensitivity analysis of regularization weight β. Hyperparameter CIFAR-10 CIFAR-100 Mini-Imagenet β= 0.0 37 .2±1.7 21 .2±1.5 27 .1±1.4 β= 0.001 37 .3±1.8 21 .6±1.2 27 .5±1.8 β= 0.003 37 .6±1.5 21 .5±1.3 27 .3±1.0 β= 0.005 37 .5±1.1 21 .3±1.1 27 .6±1.4 Table 7.Sensitivity analysis of evolution rate αon CIFAR10. α= 0.005 α= 0.01 α= 0.03 37.5±1.2 37 .6±1.5 37 .9±1.6 Table 8.Sensitivity analysis of evolution rate αon CIFAR100. α= 0.01 α= 0.05 α= 0.1 21.6±1.2 21 .5±1.3 21 .3±1.2 Table 9.Sensitivity analysis of evolution rate αon miniImageNet. α= 0.0001 α= 0.001 α= 0.005 27.5±1.2 27 .3±1.0 26 .9±1.5 D. Foundations of Calculus of Variations In calculus of variations, the ﬁrst variation is deﬁned as following: Deﬁnition D.1 (First Variations). The ﬁrst variation of a functional F(µ) is the functional at µ δF δµ(µ) = lim ϵ→0 F(µ+ ϵψ) −F(µ) ϵ , (22) where ψis arbitrary function. For more detailed background knowledge of calculus of variations, we recommend the reader refer to (Miersemann, 2012). E. Derivations of the memory evolution methods Speciﬁcally, we derive the WGF-SVGD as an example: δD(µ||π) δµ = logµ π + 1; δV(u) δµ =U(x,θ)= −L(θ,x,y)−β∇θL(θ,x,y)·∇θL(θ,x′,y). We deﬁne the target worst-case evolved memory data distribution asπ∝ e−U by the energy function U(x,θ) deﬁned above. We replace the Wasserstein gradient∇W2 F(µt) by the transformation Kµ∇W2 F(µt) under the integral operator Kµf(x) =∫ K(x,x′)f(x′)dµ(x′);Improving Task-free Continual Learning by Distributionally Robust Memory Evolution The probability measure in the kernelized Wasserstein space actually follows the kernelized WGF (Liu, 2017): ∂tµt = div(µtKµt ∇δF δµ(µt)). (23) Eq. (23) can be viewed as the WGF Eq. (9) in RKHS. It indicates that the random variable Xt which describes the evolved memory data at time tevolves as the following differential equation (Liu, 2017; Chewi et al., 2020): dX dt = −[Kµ∇δF δµ(µt)](X). (24) First, we derive the kernelized Wasserstein gradient of Kµt ∇log µt π (x) := ∫ K(x,·)∇log µt π = ∫ K(x,·)∇Udµt − ∫ ∇2K(x,·)dµt (25) where, we apply integration by parts in the second identity. ∇2 means the gradient of the kernel w.r.t. the second argument. It indicates that the random variable Xt which describes the evolved memory data at time t evolves as the following differential equation (Liu, 2017): We plug Eq. 25 into Eq. 24, we can obtain the following equations. dX dt = − ∫ K(x,·)∇Udµt + ∫ ∇2K(x,·)dµt (26) If we discretize the above Eq. 26 and view each datapoint in memory as one particle, we can obtain the following memory evolution update equation: xi t+1 −xi t = −α N j=N∑ j=1 [k(xi t,xj t)∇xj t U(xj t,θ)    smoothed gradient + ∇xj t k(xi t,xj t)    repulsive term ], (27)",
      "meta_data": {
        "arxiv_id": "2207.07256v2",
        "authors": [
          "Zhenyi Wang",
          "Li Shen",
          "Le Fang",
          "Qiuling Suo",
          "Tiehang Duan",
          "Mingchen Gao"
        ],
        "published_date": "2022-07-15T02:16:09Z",
        "pdf_url": "https://arxiv.org/pdf/2207.07256v2.pdf"
      }
    },
    {
      "title": "SparCL: Sparse Continual Learning on the Edge",
      "abstract": "Existing work in continual learning (CL) focuses on mitigating catastrophic\nforgetting, i.e., model performance deterioration on past tasks when learning a\nnew task. However, the training efficiency of a CL system is\nunder-investigated, which limits the real-world application of CL systems under\nresource-limited scenarios. In this work, we propose a novel framework called\nSparse Continual Learning(SparCL), which is the first study that leverages\nsparsity to enable cost-effective continual learning on edge devices. SparCL\nachieves both training acceleration and accuracy preservation through the\nsynergy of three aspects: weight sparsity, data efficiency, and gradient\nsparsity. Specifically, we propose task-aware dynamic masking (TDM) to learn a\nsparse network throughout the entire CL process, dynamic data removal (DDR) to\nremove less informative training data, and dynamic gradient masking (DGM) to\nsparsify the gradient updates. Each of them not only improves efficiency, but\nalso further mitigates catastrophic forgetting. SparCL consistently improves\nthe training efficiency of existing state-of-the-art (SOTA) CL methods by at\nmost 23X less training FLOPs, and, surprisingly, further improves the SOTA\naccuracy by at most 1.7%. SparCL also outperforms competitive baselines\nobtained from adapting SOTA sparse training methods to the CL setting in both\nefficiency and accuracy. We also evaluate the effectiveness of SparCL on a real\nmobile phone, further indicating the practical potential of our method.",
      "full_text": "SparCL: Sparse Continual Learning on the Edge Zifeng Wang1,†, Zheng Zhan1,†, Yifan Gong1, Geng Yuan1, Wei Niu2, Tong Jian1, Bin Ren2, Stratis Ioannidis1, Yanzhi Wang1, Jennifer Dy1 1 Northeastern University, 2 College of William and Mary {zhan.zhe, gong.yifa, geng.yuan, yanz.wang}@northeastern.edu, {zifengwang, jian, ioannidis, jdy}@ece.neu.edu, wniu@email.wm.edu, bren@cs.wm.edu Abstract Existing work in continual learning (CL) focuses on mitigating catastrophic for- getting, i.e., model performance deterioration on past tasks when learning a new task. However, the training efﬁciency of a CL system is under-investigated, which limits the real-world application of CL systems under resource-limited scenarios. In this work, we propose a novel framework called Sparse Continual Learning (SparCL), which is the ﬁrst study that leverages sparsity to enable cost-effective continual learning on edge devices. SparCL achieves both training acceleration and accuracy preservation through the synergy of three aspects: weight sparsity, data efﬁciency, and gradient sparsity. Speciﬁcally, we propose task-aware dynamic masking (TDM) to learn a sparse network throughout the entire CL process, dy- namic data removal (DDR) to remove less informative training data, and dynamic gradient masking (DGM) to sparsify the gradient updates. Each of them not only improves efﬁciency, but also further mitigates catastrophic forgetting. SparCL consistently improves the training efﬁciency of existing state-of-the-art (SOTA) CL methods by at most 23×less training FLOPs, and, surprisingly, further improves the SOTA accuracy by at most1.7%. SparCL also outperforms competitive base- lines obtained from adapting SOTA sparse training methods to the CL setting in both efﬁciency and accuracy. We also evaluate the effectiveness of SparCL on a real mobile phone, further indicating the practical potential of our method. Source code will be released. 1 Introduction The objective of Continual Learning (CL) is to enable an intelligent system to accumulate knowledge from a sequence of tasks, such that it exhibits satisfying performance on both old and new tasks (31). Recent methods mostly focus on addressing the catastrophic forgetting (42) problem – learning model tends to suffer performance deterioration on previously seen tasks. However, in the real world, when the CL applications are deployed in resource-limited platforms ( 47) such as edge devices, the learning efﬁciency, w.r.t. both training speed and memory footprint, are also crucial metrics of interest, yet they are rarely explored in prior CL works. Existing CL methods can be categorized into regularization-based (2; 31; 36; 67), rehearsal-based (8; 11; 49; 60), and architecture-based (30; 41; 51; 57; 58; 69). Both regularization- and rehearsal-based methods directly train a dense model, which might even be over-parametrized for the union of all tasks (18; 38); Though several architecture-based methods (50; 56; 63) start with a sparse sub-network from the dense model, they still grow the model size progressively to learn emerging tasks. The aforementioned methods, although striving for greater performance with less forgetting, still introduce signiﬁcant memory and computation overhead during the whole CL process. †Both authors contributed equally to this work 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2209.09476v1  [cs.LG]  20 Sep 2022Figure 1: Left: Overview of SparCL. SparCL consists of three complementary components: task-aware dynamic masking (TDM) for weight sparsity, dynamic data removal (DDR) for data efﬁciency, and dynamic gradient masking (DGM) for gradient sparsity. Right: SparCL successfully preserves the accuracy and signiﬁcantly improves efﬁciency over DER++ (8), one of the SOTA CL methods, with different sparsity ratios on the Split Tiny-ImageNet (15) dataset. Recently, another stream of work, sparse training (4; 19; 34) has emerged as a new training trend to achieve training acceleration, which embraces the promising training-on-the-edge paradigm. With sparse training, each iteration takes less time with the reduction in computation achieved by sparsity, under the traditional i.i.d. learning setting. Inspired by these sparse training methods, we naturally think about introducing sparse training to the ﬁeld of CL. A straightforward idea is to directly combine existing sparse training methods, such as SNIP ( 34), RigL (19), with a rehearsal buffer under the CL setting. However, these methods fail to consider key challenges in CL to mitigate catastrophic forgetting, for example, properly handling transition between tasks. As a result, these sparse training methods, though enhancing training efﬁciency, cause signiﬁcant accuracy drop (see Section 5.2). Thus, we would like to explore a general strategy, which is orthogonal to existing CL methods, that not only leverages the idea of sparse training for efﬁciency, but also addresses key challenges in CL to preserve (or even improve) accuracy. In this work, we propose Sparse Continual Learning (SparCL), a general framework for cost-effective continual learning, aiming at enabling practical CL on edge devices. As shown in Figure 1 (left), SparCL achieves both learning acceleration and accuracy preservation through the synergy of three aspects: weight sparsity, data efﬁciency, and gradient sparsity. Speciﬁcally, to maintain a small dynamic sparse network during the whole CL process, we develop a novel task-aware dynamic masking (TDM) strategy to keep only important weights for both the current and past tasks, with special consideration during task transitions. Moreover, we propose a dynamic data removal (DDR) scheme, which progressively removes “easy-to-learn” examples from training iterations, which further accelerates the training process and also improves accuracy of CL by balancing current and past data and keeping more informative samples in the buffer. Finally, we provide an additional dynamic gradient masking (DGM) strategy to leverage gradient sparsity for even better efﬁciency and knowledge preservation of learned tasks, such that only a subset of sparse weights are updated. Figure 1 (right) demonstrates that SparCL successfully preserves the accuracy and signiﬁcantly improves efﬁciency over DER++ (8), one of the SOTA CL methods, under different sparsity ratios. SparCL is simple in concept, compatible with various existing rehearsal-based CL methods, and efﬁcient under practical scenarios. We conduct comprehensive experiments on multiple CL bench- marks to evaluate the effectiveness of our method. We show that SparCL works collaboratively with existing CL methods, greatly accelerates the learning process under different sparsity ratios, and even sometimes improves upon the state-of-the-art accuracy. We also establish competing baselines by combining representative sparse training methods with advanced rehearsal-based CL methods. SparCL again outperforms these baselines in terms of both efﬁciency and accuracy. Most importantly, we evaluate our SparCL framework on real edge devices to demonstrate the practical potential of our method. We are not aware of any prior CL works that explored this area and considered the constraints of limited resources during training. In summary, our work makes the following contributions: • We propose Sparse Continual Learning (SparCL), a general framework for cost-effective continual learning, which achieves learning acceleration through the synergy of weight sparsity, data efﬁ- ciency, and gradient sparsity. To the best of our knowledge, our work is the ﬁrst to introduce the idea of sparse training to enable efﬁcient CL on edge devices. 2• SparCL shows superior performance compared to both conventional CL methods and CL-adapted sparse training methods on all benchmark datasets, leading to at most 23×less training FLOPs and, surprisingly, 1.7% improvement over SOTA accuracy. • We evaluate SparCL on a real mobile edge device, demonstrating the practical potential of our method and also encouraging future research on CL on-the-edge. The results indicate that our framework can achieve at most 3.1×training acceleration. 2 Related work 2.1 Continual Learning The main focus in continual learning (CL) has been mitigating catastrophic forgetting. Existing methods can be classiﬁed into three major categories. Regularization-based methods (2; 31; 36; 67) limit updates of important parameters for the prior tasks by adding corresponding regularization terms. While these methods reduce catastrophic forgetting to some extent, their performance deteriorates under challenging settings ( 39), and on more complex benchmarks ( 49; 60). Rehearsal-based methods (12; 13; 24) save examples from previous tasks into a small-sized buffer to train the model jointly with the current task. Though simple in concept, the idea of rehearsal is very effective in practice and has been adopted by many state-of-the-art methods ( 8; 10; 48). Architecture-based methods (41; 50; 56; 58; 62) isolate existing model parameters or assign additional parameters for each task to reduce interference among tasks. As mentioned in Section 1, most of these methods use a dense model without consideration of efﬁciency and memory footprint, thus are not applicable to resource-limited settings. Our work, orthogonal to these methods, serves as a general framework for making these existing methods efﬁcient and enabling a broader deployment, e.g., CL on edge devices. A limited number of works explore sparsity in CL, however, for different purposes. Several methods (40; 41; 52; 56) incorporate the idea of weight pruning ( 23) to allocate a sparse sub-network for each task to reduce inter-task interference. Nevertheless, these methods still reduce the full model sparsity progressively for every task and ﬁnally end up with a much denser model. On the contrary, SparCL maintains a sparse network throughout the whole CL process, introducing great efﬁciency and memory beneﬁts both during training and at the output model. A recent work ( 14) aims at discovering lottery tickets (20) under CL, but still does not address efﬁciency. However, the existence of lottery tickets in CL serves as a strong justiﬁcation for the outstanding performance of our SparCL. 2.2 Sparse Training There are two main approaches for sparse training: ﬁxed-mask sparse training and dynamic sparse training. Fixed-mask sparse training methods ( 34; 53; 55; 59) ﬁrst apply pruning, then execute traditional training on the sparse model with the obtained ﬁxed mask. The pre-ﬁxed structure limits the accuracy performance, and the ﬁrst stage still causes huge computation and memory consumption. To overcome these drawbacks, dynamic mask methods (4; 16; 19; 44; 45) adjust the sparsity topology during training while maintaining low memory footprint. These methods start with a sparse model structure from an untrained dense model, then combine sparse topology exploration at the given sparsity ratio with the sparse model training. Recent work (66) further considers to incorporate data efﬁciency into sparse training for better training accelerations. However, all prior sparse training works are focused on the traditional training setting, while CL is a more complicated and difﬁcult scenario with inherent characteristics not explored by these works. In contrast to prior sparse training methods, our work explores a new learning paradigm that introduces sparse training into CL for efﬁciency and also addresses key challenges in CL, mitigating catastrophic forgetting. 3 Continual Learning Problem Setup In supervised CL, a model fθ learns from a sequence of tasks D= {D1,..., DT}, where each task Dt = {(xt,i,yt,i)}nt i=1 consists of input-label pairs, and each task has a disjoint set of classes. Tasks arrive sequentially, and the model must adapt to them. At the t-th step, the model gains access to data from the t-th task. However, a small ﬁx-sized rehearsal buffer Mis allowed to save data from prior tasks. At test time, the easiest setting is to assume task identity is known for each coming test example, named task-incremental learning (Task-IL). If this assumption does not hold, we have the 3...... T ask-aware Dynamic Masking  (TDM)  (a) -and- Expand Shrink Dynamic Data Removal  (DDR) Dynamic Gradient Masking  (DGM) Remove  “easier” samples Update  important gradients  (b) -and- Shrink Expand T ask 1 T ask t (a) TDM DDR DGM (b) T ask T    Epochs Figure 2: Illustration of the SparCL workﬂow. Three components work synergistically to improve training efﬁciency and further mitigate catastrophic forgetting for preserving accuracy. more difﬁcult class-incremental learning (Class-IL) setting. In this work, we mainly focus on the more challenging Class-IL setting, and only report Task-IL performance for reference. The goal of conventional CL is to train a model sequentially that performs well on all tasks at test time. The main evaluation metric is average test accuracy on all tasks. In real-world resource- limited scenarios, we should further consider training efﬁciency of the model. Thus, we measure the performance of the model more comprehensively by including training FLOPs and memory footprint. 4 Sparse Continual Learning (SparCL) Our method, Sparse Continual Learning, is a uniﬁed framework composed of three complementary components: task-aware dynamic masking for weight sparsity, dynamic data removal for data efﬁciency, and dynamic gradient masking for gradient sparsity. The entire framework is shown in Figure 2. We will illustrate each component in detail in this section. 4.1 Task-aware Dynamic Masking To enable cost-effective CL in resource limited scenarios, SparCL is designed to maintain a dynamic structure when learning a sequence of tasks, such that it not only achieves high efﬁciency, but also intelligently adapts to the data stream for better performance. Speciﬁcally, we propose a strategy named task-aware dynamic masking (TDM), which dynamically removes less important weights and grows back unused weights for stronger representation power periodically by maintaining a single binary weight mask throughout the CL process. Different from typical sparse training work, which only leverages the weight magnitude ( 44) or the gradient w.r.t. data from a single training task (19; 66), TDM considers also the importance of weights w.r.t. data saved in the rehearsal buffer, as well as the switch between CL tasks. Speciﬁcally, TDM strategy starts from a randomly initialized binary mask Mθ = M0, with a given sparsity constraint ∥Mθ∥0/∥θ∥0 = 1−s, where s∈[0,1] is the sparsity ratio. Moreover, it makes different intra- and inter-task adjustments to keep a dynamic sparse set of weights based on their continual weight importance (CWI). We summarize the process of task-aware dynamic masking in Algorithm 1 and elaborate its key components in detail below. Continual weight importance (CWI).For a model fθ parameterized by θ, the CWI of weight w⊂θis deﬁned as follows: CWI(w) =∥w∥1 + α∥∂˜L(Dt; θ) ∂w ∥1 + β∥∂L(M; θ) ∂w ∥1, (1) where Dt denotes the training data from the t-th task, Mis the current rehearsal buffer, and α, βare coefﬁcients to control the inﬂuence of current and buffered data, respectively. Moreover,Lrepresents the cross-entropy loss for classiﬁcation, while ˜Lis the single-head (1) version of the cross-entropy loss, which only considers classes from the current task by masking out the logits of other classes. Intuitively, CWI ensures we keep (1) weights of larger magnitude for output stability, (2) weights important for the current task for learning capacity, and (3) weights important for past data to mitigate catastrophic forgetting. Moreover, inspired by the classiﬁcation bias in CL (1), we use the single-head cross-entropy loss when calculating importance score w.r.t. the current task to make the importance estimation more accurate. 4Algorithm 1:Task-aware Dynamic Masking (TDM) Input: Model weight θ, number of tasks T, training epochs of the t-th task Kt, binary sparse mask Mθ, sparsity ratio s, intra-task adjustment ratio pintra, inter-task adjustment ratio pinter, update interval δk Initialize: θ, Mθ, s.t. ∥Mθ∥0/∥θ∥0 = 1−s for t= 1,...,T do for e= 1,...,K T do if t> 1 then /* Inter-task adjustment */ Expand Mθ by randomly adding unused weights, s.t. ∥Mθ∥0/∥θ∥0 = 1−(s−pinter) if e= δkthen Shrink Mθ by removing the least important weights according to equation 1, s.t. ∥Mθ∥0/∥θ∥0 = 1−s end end if e mod δk = 0then /* Intra-task adjustment */ Shrink Mθ by removing the least important weights according to equation 1, s.t. ∥Mθ∥0/∥θ∥0 = 1−(s+ pintra) Expand Mθ by randomly adding unused weights, s.t. ∥Mθ∥0/∥θ∥0 = 1−s end Update θ⊙Mθ via backpropagation end end Intra-task adjustment.When training the t-th task, a natural assumption is that the data distribution is consistent inside the task, thus we would like to update the sparse model in a relatively stable way while keeping its ﬂexibility. Thus, in Algorithm 1, we choose to update the sparsity mask Mθ in a shrink-and-expand way every δkepochs. We ﬁrst remove pintra of the weights of least CWI to retain learned knowledge so far. Then we randomly select unused weights to recover the learning capacity for the model and keep the sparsity ratio sunchanged. Inter-task adjustment.When tasks switches, on the contrary, we assume data distribution shifts immediately. Ideally, we would like the model to keep the knowledge learned from old tasks as much as possible, and to have enough learning capacity to accommodate the new task. Thus, instead of the shrink-and-expand strategy for intra-task adjustment, we follow an expand-and-shrink scheme. Speciﬁcally, at the beginning of the (t+ 1)-th task, we expand the sparse model by randomly adding a proportion of pinter unused weights. Intuitively, the additional learning capacity facilitates fast adoption of new knowledge and reduces interference with learned knowledge. We allow our model to have smaller sparsity (i.e., larger learning capacity) temporarily for the ﬁrst δk epochs as a warm- up period, and then remove the pinter weights with least CWI, following the same process in the intra-task case, to satisfy the sparsity constraint. 4.2 Dynamic Data Removal In addition to weight sparsity, decreasing the amount of training data can be directly translated into the saving of training time without any requirements for hardware support. Thus, we would also like to explore data efﬁciency to reduce the training workload. Some prior CL works select informative examples to construct the rehearsal buffer ( 3; 6; 64). However, the main purpose of them is not training acceleration, thus they either introduce excessive computational cost or consider different problem settings. By considering the features of CL, we present a simple yet effective strategy, dynamic data removal (DDR), to reduce training data for further acceleration. We measure the importance of each training example by the occurrence of misclassiﬁcation (54; 66) during CL. In TDM, the sparse structure of our model updates periodically every δkepochs, so we align our data removal process with the update of weight mask for further efﬁciency and training 5stability. In Section 4.1, we have partitioned the training process for the t-th task into Nt = Kt/δk stages based on the dynamic mask update. Therefore, we gradually remove training data at the end of i-th stage, based on the following policy: 1) Calculate the total number of misclassiﬁcations fi(xj) for each training example during the i-th stage. 2) Remove a proportion of ρi training samples with the least number of misclassiﬁcations. Although our main purpose is to keep the “harder” examples to learn to consolidate the sparse model, we can get further beneﬁts for better CL result. First, the removal of “easier” examples increases the probability that “harder” examples to be saved to the rehearsal buffer, given the common strategy,e.g. reservoir sampling (13), to buffer examples. Thus, we construct a more informative buffer in a implicit way without heavy computation. Moreover, since the buffer size is much smaller than the training set size of each task, the data from the buffer and the new task is highly imbalanced, dynamic data removal also relieves the data imbalance issue. Formally, we set the data removal proportion for each task as ρ∈[0,1], and a cutoff stage, such that: cutoff∑ i=1 ρi = ρ, Nk∑ i=cutoff+1 ρi = 0 (2) The cutoff stage controls the trade-off between efﬁciency and accuracy: When we set the cutoff stage earlier, we reduce the training time for all the following stages; however, when the cutoff stage is set too early, the model might underﬁt the removed training data. Note that when we set ρi = 0for all i= 1,2,...,N t and cutoff = Nt, we simply recover the vanilla setting without any data efﬁciency considerations. In our experiments, we assume ρi = ρ/cutoff, i.e., removing equal proportion of data at the end of every stage, for simplicity. We also conduct comprehensive exploration study for ρ and the selection of the cutoff stage in Section 5.3 and Appendix D.3. 4.3 Dynamic Gradient Masking With TDM and DDR, we can already achieve bi-level efﬁciency during training. To further boost training efﬁciency, we explore sparsity in gradient and propose dynamic gradient masking (DGM) for CL. Our method focuses on reducing computational cost by only applying the most important gradients onto the corresponding unpruned model parameters via a gradient mask. The gradient mask is also dynamically updated along with the weight mask deﬁned in Section 4.1. Intuitively, while targeting for better training efﬁciency, DGM also promotes the preservation of past knowledge by preventing a fraction of weights from update. Formally, our goal here is to ﬁnd a subset of unpruned parameters (or, equivalently, a gradient mask MG) to update over multiple training iterations. For a model fθ parameterized by θ, we have the corresponding gradient matrix Gcalculated during each iteration. To prevent the pruned weights from updating, the weight mask Mθ will be applied onto the gradient matrix Gas G⊙Mθ during backpropagation. Besides the gradients of pruned weights, we in addition consider to remove less important gradients for faster training. To achieve this, we introduce the continual gradient importance (CGI) based on the CWI to measure the importance of weight gradients. CGI(w) =α∥∂˜L(Dt; θ) ∂w ∥1 + β∥∂L(M; θ) ∂w ∥1. (3) We remove a proportion q of non-zero gradients from Gwith less importance measured by CGI and we have ∥MG∥0/∥θ∥0 = 1−(s+ q). The gradient mask MG is then applied onto the gradient matrix G. During the entire training process, the gradient mask MG is updated with a ﬁxed interval. 5 Experiment 5.1 Experiment Setting Datasets. We evaluate our SparCL on two representative CL benchmarks, Split CIFAR-10 ( 32) and Split Tiny-ImageNet (15) to verify the efﬁcacy of SparCL. In particular, we follow (8; 67) by splitting CIFAR-10 and Tiny-ImageNet into 5 and 10 tasks, each of which consists of 2 and 20 classes respectively. Dataset licensing information can be found in Appendix C. Comparing methods. In particular, we select CL methods of all kinds including regularization- based (EWC ( 31), LwF ( 36)), architecture-based (PackNet ( 41), LPS ( 56)), and rehearsal-based 6Table 1: Comparison with CL methods. SparCL consistently improves training efﬁciency of the corresponding CL methods while preserves (or even improves) accuracy on both class- and task-incremental settings. Method Sparsity Buffer size Split CIFAR-10 Split Tiny-ImageNet Class-IL (↑) Task-IL (↑) FLOPs Train×1015(↓) Class-IL (↑) Task-IL (↑) FLOPs Train×1016(↓) EWC (31) 0.00 – 19.49±0.12 68.29±3.92 8.3 7.58±0.10 19.20±0.31 13.3LwF (36) 19.61±0.05 63.29±2.35 8.3 8.46±0.22 15.85±0.58 13.3 PackNet (41) 0.50† – - 93.73 ±0.55 5.0 – 61.88 ±1.01 7.3LPS (56) - 94.50 ±0.47 5.0 – 63.37 ±0.83 7.3 A-GEM (12) 0.00 200 20.04±0.34 83.88±1.49 11.1 8.07±0.08 22.77±0.03 17.8iCaRL (49) 49.02±3.20 88.99±2.13 11.1 7.53±0.79 28.19±1.47 17.8FDR (5) 30.91±2.74 91.01±0.68 13.9 8.70±0.19 40.36±0.68 22.2ER (13) 44.79±1.86 91.19±0.94 11.1 8.49±0.16 38.17±2.00 17.8DER++ (8) 64.88±1.17 91.92±0.60 13.9 10.96±1.17 40.87±1.16 22.2 SparCL-ER75 46.89±0.68 92.02±0.72 2.0 8.98±0.38 39.14±0.85 3.2SparCL-DER++75 0.75 66.30±0.98 94.06±0.45 2.5 12.73±0.40 42.06±0.73 4.0SparCL-ER90 45.81±1.05 91.49±0.47 0.9 8.67±0.41 38.79±0.39 1.4SparCL-DER++90 0.90 200 65.79±1.33 93.73±0.24 1.1 12.27±1.06 41.17±1.31 1.8SparCL-ER95 44.59±0.23 91.07±0.64 0.5 8.43±0.09 38.20±0.46 0.8SparCL-DER++95 0.95 65.18±1.25 92.97±0.37 0.6 10.76±0.62 40.54±0.98 1.0 A-GEM (12) 0.00 500 22.67±0.57 89.48±1.45 11.1 8.06±0.04 25.33±0.49 17.8iCaRL (49) 47.55±3.95 88.22±2.62 11.1 9.38±1.53 31.55±3.27 17.8FDR (5) 28.71±3.23 93.29±0.59 13.9 10.54±0.21 49.88±0.71 22.2ER (13) 57.74±0.27 93.61±0.27 11.1 9.99±0.29 48.64±0.46 17.8DER++ (8) 72.70±1.36 93.88±0.50 13.9 19.38±1.41 51.91±0.68 22.2 SparCL-ER75 60.80±0.22 93.82±0.32 2.0 10.48±0.29 50.83±0.69 3.2SparCL-DER++75 0.75 74.09±0.84 95.19±0.34 2.5 20.75±0.88 52.19±0.43 4.0SparCL-ER90 59.34±0.97 93.33±0.10 0.9 10.12±0.53 49.46±1.22 1.4SparCL-DER++90 0.90 500 73.42±0.95 94.82±0.23 1.1 19.62±0.67 51.93±0.36 1.8SparCL-ER95 57.75±0.45 92.73±0.34 0.5 9.91±0.17 48.57±0.50 0.8SparCL-DER++95 0.95 72.14±0.78 94.39±0.15 0.6 19.01±1.32 51.26±0.78 1.0 †PackNet and LPS actually have a decreased sparsity after learning every task, we use 0.50 to roughly represent the average sparsity. (A-GEM (12), iCaRL (43), FDR (5), ER (13), DER++ (8)) methods. Note that PackNet and LPS are only compatible with task-incremental learning. We also adapt representative sparse training methods (SNIP (34), RigL (19)) to the CL setting by combining them with DER++ (SNIP-DER++, RigL-DER++). Variants of our method.To show the generality of SparCL, we combine it with DER++ (one of the SOTA CL methods), and ER (simple and widely-used) as SparCL-DER++ and SparCL-ER, respectively. We also vary the weight sparsity ratio (0.75,0.90,0.95) of SparCL for a comprehensive evaluation. Evaluation metrics.We use the average accuracy on all tasks to evaluate the performance of the ﬁnal model. Moreover, we evaluate the training FLOPs (19), and memory footprint (66) (including feature map pixels and model parameters during training) to demonstrate the efﬁciency of each method. Please see Appendix D.1 for detailed deﬁnitions of these metrics. Experiment details.For fair comparison, we strictly follow the settings in prior CL work (8; 28). We sets the per task training epochs to 50 and 100 for Split CIFAR-10 and Tiny-ImageNet, respectively, with a batch size of 32. For the model architecture, We follow (8; 49) and adopt the ResNet-18 (25) without any pre-training. We also use the best hyperparameter setting reported in ( 8; 56) for CL methods, and in (19; 34) for CL-adapted sparse training methods. For SparCL and its competing CL-adapted sparse training methods, we adopt a uniform sparsity ratio for all convolutional layers. Please see Appendix D for other details. 5.2 Main Results Comparison with CL methods.Table 1 summarizes the results on Split CIFAR-10 and Tiny- ImageNet, under both class-incremental (Class-IL) and task-incremental (Task-IL) settings. From Table 1, we can clearly tell that SparCL signiﬁcantly improves ER and DER++, while also outperforms other CL baselines, in terms of training efﬁciency (measured in FLOPs). With higher sparsity ratio, SparCL leads to less training FLOPs. Notably, SparCL achieves23×training efﬁciency improvement upon DER++ with a sparsity ratio of 0.95. On the other hand, our framework also improves the average accuracy of ER and DER++ consistently under all cases with a sparsity ratio of 0.75 and 0.90, and only slight performance drop when sparsity gets larger as 0.95. In particular, SparCL-DER++ 7Table 2: Comparison with CL-adapted sparse training methods. All methods are combined with DER++ with a 500 buffer size. SparCL outperforms all methods in both accuracy and training efﬁciency, under all sparsity ratios. All three methods here can save 20% ∼ 51% memory footprint, please see Appendix D.2 for details. Method Spasity Split CIFAR-10 Split Tiny-ImageNet Class-IL (↑) FLOPs Train ×1015(↓) Class-IL (↑) FLOPs Train ×1016(↓) DER++ (8) 0.00 72.70±1.36 13.9 19.38±1.41 22.2 SNIP-DER++ (34) 69.82±0.72 1.6 16.13±0.61 2.5RigL-DER++ (19)0.90 69.86±0.59 1.6 18.36±0.49 2.5SparCL-DER++90 73.42±0.95 1.1 19.62±0.67 1.8 SNIP-DER++ (34) 66.07±0.91 0.9 14.76±0.52 1.5RigL-DER++ (19)0.95 66.53±1.13 0.9 15.88±0.63 1.5SparCL-DER++95 72.14±0.78 0.6 19.01±1.32 1.0 Table 3: Ablation study on Split-CIFAR10 with 0.75 sparsity ratio. All components contributes to the overall performance, in terms of both accuracy and efﬁciency (training FLOPs and memory footprint). TDM DDR DGMClass-IL (↑) FLOPs Train ×1015(↓) MemoryFootprint (↓) \u0017 \u0017 \u0017 72.70 13.9 247MB\u0013 \u0017 \u0017 73.37 3.6 180MB\u0013 \u0013 \u0017 73.80 2.8 180MB\u0013 \u0017 \u0013 73.97 3.3 177MB\u0013 \u0013 \u0013 74.09 2.5 177MB with 0.75 sparsity ratio sets new SOTA accuracy, with all buffer sizes under both benchmarks. The outstanding performance of SparCL indicates that our proposed strategies successfully preserve accuracy by further mitigating catastrophic forgetting with a much sparser model. Moreover, the improvement that SparCL brings to two different existing CL methods shows the generalizability of SparCL as a uniﬁed framework, i.e., it has the potential to be combined with a wide array of existing methods. We would also like to take a closer look at PackNet and LPS, which also leverage the idea of sparsity to split the model by different tasks, a different motivation from training efﬁciency. Firstly, they are only compatible with the Task-IL setting, since they leverage task identity at both training and test time. Moreover, the model sparsity of these methods reduces with the increasing number of tasks, which still leads to much larger overall training FLOPs than that of SparCL. This further demonstrates the importance of keeping a sparse model without permanent expansion throughout the CL process. Comparison with CL-adapted sparse training methods.Table 2 shows the result under the more difﬁcult Class-IL setting. SparCL outperforms all CL-adapted sparse training methods in both accuracy and training FLOPs. The performance gap between SparCL-DER++ and other methods gets larger with a higher sparsity. SNIP- and RigL-DER++ achieve training acceleration at the cost of compromised accuracy, which suggests that keeping accuracy is a non-trivial challenge for existing sparse training methods under the CL setting. SNIP generates the static initial mask after network initialization which does not consider the structure suitability among tasks. Though RigL adopts a dynamic mask, the lack of task-aware strategy prevents it from generalizing well to the CL setting. 5.3 Effectiveness of Key Components Ablation study.We provide a comprehensive ablation study in Table 3 using SparCL-DER++ with 0.75 sparsity on Split CIFAR10. Table 3 demonstrates that all components of our method contribute to both efﬁciency and accuracy improvements. Comparing row 1 and 2, we can see that the majority of FLOPs decrease results from TDM. Interestingly, TDM leads to an increase in accuracy, indicating TDM generates a sparse model that is even more suitable for learning all tasks than then full dense model. Comparing row 2 and 3, we can see that DDR indeed further accelerates training by removing less informative examples. As discussed in Section 4.2, when we remove a certain number of samples (30% here), we achieve a point where we keep as much informative samples as we need, and also balance the current and buffered data. Comparing row 2 and 4, DGM reduce both training FLOPs and memory footprint while improve the performance of the network. Finally, the last row demonstrates the collaborative performance of all components. We also show the same ablation study with 0.90 sparsity in Appendix D.4 for reference. Detail can be found in Appendix D.1. 8Figure 3: Comparison between DDR and One- shot (66) data removal strategy w.r.t. different data removal proportion ρ. DDR outperforms One-shot and also achieves better accuracy when ρ ≤ 30%. Figure 4: Comparison with CL-adapted sparse training methods in training acceleration rate and accuracy results. The radius of circles are measured by memory footprint. Exploration on DDR.To understand the inﬂuence of the data removal proportionρ, and the cutoff stage for each task, we show corresponding experiment results in Figure 3 and Appendix D.3, respectively. In Figure 3, we ﬁx cutoff = 4, i.e., gradually removing equal number of examples every 5 epochs until epoch 20, and vary ρfrom 10% to 90%. We also compare DDR with One-shot removal strategy (66), which removes all examples at once at cutoff. DDR outperforms One-shot consistently with different ρin average accuracy. Also note that since DDR removes the examples gradually before the cutoff stage, DDR is more efﬁcient than One-shot. When ρ≤30%, we also observe increased accuracy of DDR compared with the baseline without removing any data. When ρ≥40%, the accuracy gets increasingly lower for both strategies. The intuition is that when DDR removes a proper amount of data, it removes redundant information while keeps the most informative examples. Moreover, as discussed in Section 4.2, it balances the current and buffered data, while also leave informative samples in the buffer. When DDR removes too much data, it will also lose informative examples, thus the model has not learned these examples well before removal. Exploration on DGM.We test the efﬁcacy of DGM at different sparsity levels. Detailed exploratory experiments are shown in Appendix D.5 for reference. The results indicate that by setting the proportion qwithin an appropriate range, DGM can consistently improve the accuracy performance regardless of the change of weight sparsity. 5.4 Mobile Device Results The training acceleration results are measured on the CPU of an off-the-shelf Samsung Galaxy S20 smartphone, which has the Qualcomm Snapdragon 865 mobile platform with a Qualcomm Kryo 585 Octa-core CPU. We run each test on a batch of 32 images to denote the training speed. The detail of on-mobile compiler-level optimizations for training acceleration can be found in Appendix E.1. The acceleration results are shown in Figure 4. SparCL can achieve approximately 3.1×and 2.3× training acceleration with 0.95 sparsity and 0.90 sparsity, respectively. Besides, our framework can also save 51% and 48% memory footprint when the sparsity is 0.95 and 0.90. Furthermore, the obtained sparse models save the storage consumption by using compressed sparse row (CSR) storage and can be accelerated to speed up the inference on-the-edge. We provide on-mobile inference acceleration results in Appendix E.2. 6 Conclusion This paper presents a uniﬁed framework named SparCL for efﬁcient CL that achieves both learning acceleration and accuracy preservation. It comprises three complementary strategies: task-aware dynamic masking for weight sparsity, dynamic data removal for data efﬁciency, and dynamic gradient masking for gradient sparsity. Extensive experiments on standard CL benchmarks and real-world edge device evaluations demonstrate that our method signiﬁcantly improves upon existing CL methods and outperforms CL-adapted sparse training methods. We discuss the limitations and potential negative social impacts of our method in Appendix A and B, respectively. 9References [1] Hongjoon Ahn, Jihwan Kwak, Subin Lim, Hyeonsu Bang, Hyojun Kim, and Taesup Moon. Ss-il: Separated softmax for incremental learning. In CVPR, 2021. 4 [2] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuyte- laars. Memory aware synapses: Learning what (not) to forget. In ECCV, 2018. 1, 3 [3] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample selection for online continual learning. NeurIPS, 2019. 5 [4] Guillaume Bellec, David Kappel, Wolfgang Maass, and Robert Legenstein. Deep rewiring: Training very sparse deep networks. In ICLR, 2018. 2, 3 [5] Ari S Benjamin, David Rolnick, and Konrad Kording. Measuring and regularizing networks in function space. arXiv preprint arXiv:1805.08289, 2018. 7 [6] Zalán Borsos, Mojmir Mutny, and Andreas Krause. Coresets via bilevel optimization for continual learning and streaming. NeurIPS, 2020. 5 [7] Miles Brundage, Shahar Avin, Jack Clark, Helen Toner, Peter Eckersley, Ben Garﬁnkel, Allan Dafoe, Paul Scharre, Thomas Zeitzoff, Bobby Filar, et al. The malicious use of artiﬁcial intelligence: Forecasting, prevention, and mitigation. arXiv preprint arXiv:1802.07228, 2018. 15 [8] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. Dark experience for general continual learning: a strong, simple baseline. In NeurIPS, 2020. 1, 2, 3, 6, 7, 8 [9] Alfredo Canziani, Adam Paszke, and Eugenio Culurciello. An analysis of deep neural network models for practical applications. arXiv preprint arXiv:1605.07678, 2016. 16 [10] Hyuntak Cha, Jaeho Lee, and Jinwoo Shin. Co2l: Contrastive continual learning. In ICCV, 2021. 3 [11] Arslan Chaudhry, Albert Gordo, Puneet Kumar Dokania, Philip Torr, and David Lopez-Paz. Us- ing hindsight to anchor past knowledge in continual learning. arXiv preprint arXiv:2002.08165, 2(7), 2020. 1 [12] Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efﬁcient lifelong learning with a-gem. arXiv preprint arXiv:1812.00420, 2018. 3, 7 [13] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS Torr, and Marc’Aurelio Ranzato. On tiny episodic memories in continual learning. arXiv preprint arXiv:1902.10486, 2019. 3, 6, 7 [14] Tianlong Chen, Zhenyu Zhang, Sijia Liu, Shiyu Chang, and Zhangyang Wang. Long live the lottery: The existence of winning tickets in lifelong learning. In ICLR, 2020. 3 [15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR. Ieee, 2009. 2, 6 [16] Tim Dettmers and Luke Zettlemoyer. Sparse networks from scratch: Faster training without losing performance. arXiv preprint arXiv:1907.04840, 2019. 3 [17] Peiyan Dong, Siyue Wang, Wei Niu, Chengming Zhang, Sheng Lin, Zhengang Li, Yifan Gong, Bin Ren, Xue Lin, and Dingwen Tao. Rtmobile: Beyond real-time mobile acceleration of rnns for speech recognition. In DAC, pages 1–6. IEEE, 2020. 17 [18] Xuanyi Dong and Yi Yang. Network pruning via transformable architecture search. In NeurIPS, pages 759–770, 2019. 1 [19] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery: Making all tickets winners. In ICML, pages 2943–2952. PMLR, 2020. 2, 3, 4, 7, 8, 16 10[20] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. ICLR, 2019. 3 [21] Yifan Gong, Geng Yuan, Zheng Zhan, Wei Niu, Zhengang Li, Pu Zhao, Yuxuan Cai, Sijia Liu, Bin Ren, Xue Lin, et al. Automatic mapping of the best-suited dnn pruning schemes for real-time mobile acceleration. ACM Transactions on Design Automation of Electronic Systems (TODAES), 27(5):1–26, 2022. 18 [22] Yifan Gong, Zheng Zhan, Zhengang Li, Wei Niu, Xiaolong Ma, Wenhao Wang, Bin Ren, Caiwen Ding, Xue Lin, Xiaolin Xu, et al. A privacy-preserving-oriented dnn pruning and mobile acceleration framework. In GLSVLSI, pages 119–124, 2020. 17 [23] Song Han, Jeff Pool, et al. Learning both weights and connections for efﬁcient neural network. In NeurIPS, pages 1135–1143, 2015. 3 [24] Tyler L Hayes, Nathan D Cahill, and Christopher Kanan. Memory efﬁcient experience replay for streaming learning. In ICRA, 2019. 3 [25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 7 [26] Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi Yang. Soft ﬁlter pruning for accelerating deep convolutional neural networks. arXiv preprint arXiv:1808.06866, 2018. 17 [27] Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi Yang. Filter pruning via geometric median for deep convolutional neural networks acceleration. In CVPR, pages 4340–4349, 2019. 17 [28] Yen-Chang Hsu, Yen-Cheng Liu, Anita Ramasamy, and Zsolt Kira. Re-evaluating con- tinual learning scenarios: A categorization and case for strong baselines. arXiv preprint arXiv:1810.12488, 2018. 7 [29] Tong Jian, Yifan Gong, Zheng Zhan, Runbin Shi, Nasim Soltani, Zifeng Wang, Jennifer G Dy, Kaushik Roy Chowdhury, Yanzhi Wang, and Stratis Ioannidis. Radio frequency ﬁngerprinting on the edge. IEEE Transactions on Mobile Computing, 2021. 17 [30] Zixuan Ke, Bing Liu, and Xingchang Huang. Continual learning of a mixed sequence of similar and dissimilar tasks. NeurIPS, 2020. 1 [31] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. PNAS, 114(13):3521–3526, 2017. 1, 3, 6, 7 [32] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. https://www.cs.toronto.edu/ kriz/learning-features-2009-TR.pdf, 2009. 6, 15 [33] Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998. 15 [34] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network pruning based on connection sensitivity. ICLR, 2019. 2, 3, 7, 8 [35] Tuanhui Li, Baoyuan Wu, Yujiu Yang, Yanbo Fan, Yong Zhang, and Wei Liu. Compressing convolutional neural networks via factorized convolutional ﬁlters. In CVPR, pages 3977–3986, 2019. 17 [36] Zhizhong Li and Derek Hoiem. Learning without forgetting. TPAMI, 40(12):2935–2947, 2017. 1, 3, 6, 7 [37] Xiaolong Ma, Fu-Ming Guo, Wei Niu, Xue Lin, Jian Tang, Kaisheng Ma, Bin Ren, and Yanzhi Wang. Pconv: The missing but desirable sparsity in dnn weight pruning for real-time execution on mobile devices. In AAAI, pages 5117–5124, 2020. 17 11[38] Xiaolong Ma, Wei Niu, Tianyun Zhang, Sijia Liu, Sheng Lin, Hongjia Li, Wujie Wen, Xiang Chen, Jian Tang, Kaisheng Ma, et al. An image enhancing pattern-based sparsity for real-time inference on mobile devices. In ECCV, pages 629–645. Springer, 2020. 1 [39] Zheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe, Hyunwoo Kim, and Scott Sanner. Online continual learning in image classiﬁcation: An empirical survey. arXiv preprint arXiv:2101.10423, 2021. 3 [40] Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggyback: Adapting a single network to multiple tasks by learning to mask weights. In ECCV, pages 67–82, 2018. 3 [41] Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative pruning. In CVPR, 2018. 1, 3, 6, 7 [42] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of learning and motivation, volume 24, pages 109–165. Elsevier, 1989. 1 [43] Sanket Vaibhav Mehta, Darshan Patil, Sarath Chandar, and Emma Strubell. An empirical investigation of the role of pre-training in lifelong learning. ICML Workshop, 2021. 7 [44] Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine Gibescu, and Antonio Liotta. Scalable training of artiﬁcial neural networks with adaptive sparse connectivity inspired by network science. Nature communications, 9(1):1–12, 2018. 3, 4 [45] Hesham Mostafa and Xin Wang. Parameter efﬁcient training of deep convolutional neural networks by dynamic sparse reparameterization. In ICML, pages 4646–4655. PMLR, 2019. 3 [46] Wei Niu, Xiaolong Ma, Sheng Lin, Shihao Wang, Xuehai Qian, Xue Lin, Yanzhi Wang, and Bin Ren. Patdnn: Achieving real-time dnn execution on mobile devices with pattern-based weight pruning. arXiv preprint arXiv:2001.00138, 2020. 18 [47] Lorenzo Pellegrini, Vincenzo Lomonaco, Gabriele Grafﬁeti, and Davide Maltoni. Con- tinual learning at the edge: Real-time training on smartphone devices. arXiv preprint arXiv:2105.13127, 2021. 1 [48] Quang Pham, Chenghao Liu, and Steven Hoi. Dualnet: Continual learning, fast and slow. NeurIPS, 2021. 3 [49] Sylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classiﬁer and representation learning. In CVPR, pages 2001–2010, 2017. 1, 3, 7 [50] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016. 1, 3 [51] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. In ICML, 2018. 1 [52] Ghada Sokar, Decebal Constantin Mocanu, and Mykola Pechenizkiy. Spacenet: Make free space for continual learning. Neurocomputing, 439:1–11, 2021. 3 [53] Hidenori Tanaka, Daniel Kunin, Daniel L Yamins, and Surya Ganguli. Pruning neural networks without any data by iteratively conserving synaptic ﬂow. NeurIPS, 33:6377–6389, 2020. 3 [54] Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J Gordon. An empirical study of example forgetting during deep neural network learning. arXiv preprint arXiv:1812.05159, 2018. 5 [55] Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by preserving gradient ﬂow. In ICLR, 2019. 3 [56] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis Ioannidis. Learn-prune-share for lifelong learning. In ICDM, 2020. 1, 3, 6, 7 12[57] Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, et al. Dualprompt: Complementary prompting for rehearsal-free continual learning. ECCV, 2022. 1 [58] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pﬁster. Learning to prompt for continual learning.CVPR, 2022. 1, 3 [59] Paul Wimmer, Jens Mehnert, and Alexandru Condurache. Freezenet: Full performance by reduced storage costs. In ACCV, 2020. 3 [60] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, and Yun Fu. Large scale incremental learning. In CVPR, pages 374–382, 2019. 1, 3 [61] Yushu Wu, Yifan Gong, Pu Zhao, Yanyu Li, Zheng Zhan, Wei Niu, Hao Tang, Minghai Qin, Bin Ren, and Yanzhi Wang. Compiler-aware neural architecture search for on-mobile real-time super-resolution. ECCV, 2022. 18 [62] Shipeng Yan, Jiangwei Xie, and Xuming He. Der: Dynamically expandable representation for class incremental learning. In CVPR, pages 3014–3023, 2021. 3 [63] Li Yang, Sen Lin, Junshan Zhang, and Deliang Fan. Grown: Grow only when necessary for continual learning. arXiv preprint arXiv:2110.00908, 2021. 1 [64] Jaehong Yoon, Divyam Madaan, Eunho Yang, and Sung Ju Hwang. Online coreset selection for rehearsal-based continual learning. arXiv preprint arXiv:2106.01085, 2021. 5 [65] Kun-Hsing Yu, Andrew L Beam, and Isaac S Kohane. Artiﬁcial intelligence in healthcare. Nature biomedical engineering, 2(10):719–731, 2018. 15 [66] Geng Yuan, Xiaolong Ma, Wei Niu, Zhengang Li, Zhenglun Kong, Ning Liu, Yifan Gong, Zheng Zhan, Chaoyang He, Qing Jin, et al. Mest: Accurate and fast memory-economic sparse training framework on the edge. NeurIPS, 34, 2021. 3, 4, 5, 7, 9, 16, 17 [67] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In ICML, 2017. 1, 3, 6 [68] Zheng Zhan, Yifan Gong, Pu Zhao, Geng Yuan, Wei Niu, Yushu Wu, Tianyun Zhang, Malith Jayaweera, David Kaeli, Bin Ren, et al. Achieving on-mobile real-time super-resolution with neural architecture and pruning search. In ICCV, pages 4821–4831, 2021. 17 [69] Tingting Zhao, Zifeng Wang, Aria Masoomi, and Jennifer Dy. Deep bayesian unsupervised lifelong learning. Neural Networks, 149:95–106, 2022. 1 Checklist 1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s contributions and scope? [Yes] The claims match the experimental results and it is expected to generalize according to the diverse experiments stated in our paper. We include all of our code, data, and models in the supplementary materials, which can reproduce our experimental results. (b) Did you describe the limitations of your work? [Yes] See Section 6 and Appendix A. (c) Did you discuss any potential negative societal impacts of your work? [Yes] See Section 6 and Appendix B. (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] We have read the ethics review guidelines and ensured that our paper conforms to them. 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [N/A] Our paper is based on the experimental results and we do not have any theoretical results. 13(b) Did you include complete proofs of all theoretical results? [N/A] Our paper is based on the experimental results and we do not have any theoretical results. 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experi- mental results (either in the supplemental material or as a URL)? [Yes] See Section 5.1, Section 5.4 and we provide code to reproduce the main experimental results. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Section 5.1 and Section 5.4. (c) Did you report error bars (e.g., with respect to the random seed after running experi- ments multiple times)? [Yes] See Table 1, Table 2, ﬁg 1, ﬁg 3. (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Section 5.1, Section 5.4. 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] We mentioned and cited the datasets (Split CIFAR-10 and Tiny-ImageNet), and all comparing methods with their paper and github in it. (b) Did you mention the license of the assets? [Yes] The licences of used datasets/models are provided in the cited references and we state them explicitly in Appendix C. (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] We provide code for our proposed method in the supplement. (d) Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [N/A] (e) Did you discuss whether the data you are using/curating contains personally identiﬁable information or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] 14A Limitations One limitation of our method is that we assume a rehearsal buffer is available throughout the CL process. Although the assumption is widely-accepted, there are still situations that a rehearsal buffer is not allowed. However, as a framework targeting for efﬁciency, our work has the potential to accelerate all types of CL methods. For example, simply removing the terms related to rehearsal buffer in equation 1 and equation 3 could serve as a naive variation of our method that is compatible with other non-rehearsal methods. It is interesting to further improve SparCL to be more generic for all kinds of CL methods. Moreover, the benchmarks we use are limited to vision domain. Although using vision-based benchmarks has been a common practice in the CL community, we believe evaluating our method, as well as other CL methods, on datasets from other domains such as NLP will lead to a more comprehensive and reliable conclusion. We will keep track of newer CL benchmarks from different domains and further improve our work correspondingly. B Potential Negative Societal Impact Although SparCL is a general framework to enhance efﬁciency for various CL methods, we still need to be aware of its potential negative societal impact. For example, we need to be very careful about the trade-off between accuracy and efﬁciency when using SparCL. If one would like to pursue efﬁciency by setting the sparsity ratio too high, then even SparCL will result in signiﬁcant accuracy drop, since the over-sparsiﬁed model does not have enough representation power. Thus, we should pay much attention when applying SparCL on accuracy-sensitive applications such as healthcare (65). Another example is that, SparCL as a powerful tool to make CL methods efﬁcient, can also strengthen models for malicious applications ( 7). Therefore, we encourage the community to come up with more strategies and regulations to prevent malicious use of artiﬁcial intelligence. C Dataset Licensing Information • CIFAR-10 (32) is licensed under the MIT license. • The licensing information of Tiny-ImageNet ( 33) is not available. However, the data is available for free to researchers for non-commercial use. D Additional Experiment Details and Results We set α = 0.5,β = 1 in equation 1 and equation 3. We also set δk = 5, pinter = 0.01, pintra = 0.005. We also match different weight sparsity with gradient sparsity for best performance. We sample 20% data from Split CIFAR-10 training set for validation, and we use grid-search on this validation set to help us select the mentioned best hyperparameters. We use the same set of hyperparameters for both datasets. For accurate evaluation, we repeat each experiments 3 times using different random seeds and report the average performance. During our experiments, we adopt unstructured sparsity type and uniform sparsity ratio (0.75,0.90,0.95) for all convolutional layers in the models. D.1 Evaluation Metrics Explanation Training FLOPsThe FLOPs of a single forward pass is calculated by taking the sum of the number of multiplications and additions in each layer lfor a given layer sparsity sl. Each iteration in the training process is composed of two phases, i.e., the forward propagation and backward propagation. The goal of the forward pass is to calculate the loss of the current set of parameters on a given batch of data. It can be formulated as al = σ(zl) =σ(wl ∗al−1 + bl) for each layer lin the model. Here, w, b, and zrepresent the weights, biases, and output before activation, respectively; σ(.) denotes the activation function; ais the activations; ∗means convolution operation. The formulation indicates that the layer activations are calculated in sequence using the previous activations and the parameters of the layer. Activation of layers are stored in memory for the backward pass. 15As for the backward propogation, the objective is to back-propagate the error signal while calculating the gradients of the parameters. The two main calculation steps can be represented as: δl = δl+1 ∗rotate180°(wl) ⊙σ′(zl), (4) Gl = al−1 ∗δl, (5) where δl is the error associated with the layer l, Gl denotes the gradients, ⊙represents Hadamard product, σ′(.) denotes the derivative of activation, androtate180°(.) means rotating the matrix by180° is the matrix transpose operation. During the backward pass, each layer lcalculates two quantities, i.e., the gradient of the activations of the previous layer and the gradient of its parameters. Thus, the backward passes are counted astwice the computation expenses of the forward pass (19). We omit the FLOPs needed for batch normalization and cross entropy. In our work, the total FLOPs introduced by TDM, DDR, and DGM on split CIFAR-10 is approximately 4.5 ×109 which is less than 0.0001% of total training FLOPs. For split Tiny-ImageNet, the total FLOPs of them is approximately 1.8 ×1010, which is also less than 0.0001% of total training FLOPs. Therefore, the computation introduced by TDM, DDR, and DGM is negligible. Memory FootprintsFollowing works ( 9; 66), the deﬁnition of memory footprints contain two parts: 1) activations (feature map pixels) during training phase, and 2) model parameters during training phase. For experiments, activations, model weights, and gradients are stored in 32-bit ﬂoating-point format for training. The memory footprint results are calculated with an approximate summation of them. D.2 Details of Memory Footprint The memory footprint is composed of three parts: activations, model weights, and gradients. They are all represented as bw-bit numbers for training. The number of activations in the model is the sum of the activations in each layer. Suppose that the output feature of the l-th layer with a batch size of Bis represented as al ∈RB×Ol×Hl×Wl , where Ol is the number of channels and Hl ×Wl is the feature size. The total number of activations of the model is thus B∑ lOlHlWl. As for the model weights, our SparCL training a sparse model with a sparsity ratio s∈[0,1] from scratch. The sparse model is obtained from a dense model with a total of N weights. A higher value of sindicates fewer non-zero weights in the sparse model. Compressed sparse row (CSR) format is commonly used for sparse storage, which greatly reduces the number of indices need to be stored for sparse matrices. As our SparCL adopt only one sparsity type and we use a low-bit format to store the indices, we omit the indices storage here. Therefore, the memory footprint for model representation is (1 −s)Nbw. Similar calculations can be applied for the gradient matrix. Besides the sparsity ratio s, additional q gradients are masked out from the gradient matrix, resulting a sparsity ratio s+ q. Therefore, the storage of gradients can be approximated as (1 −(s+ q))Nbw. Combining the activations, model representation, and gradients, the total memory footprint in SparCL can be represented as (2B∑ lOlHlWl + (1−s)N + (1−(s+ q))N)bw. DDR requires store indices for the easier examples during the training process. The number of training examples for Split CIFAR-10 and Split Tiny-ImageNet on each task is 10000. In our work, we only need about 3KB (remove 30% training data) for indices storage (in the int8 format) and the memory cost is negligible compared with the total memory footprint. D.3 Effect of Cutoff Stage Table A1: Effect of cutoff. cutoff 1 2 3 4 5 6 7 8 9 Class-IL (↑) 71.54 72.38 72.74 73.20 73.10 73.32 73.27 73.08 73.23 To evaluate the effect of the cutoff stage, we use the same setting as in Figure 3 by setting the sparsity ratio to 0.90. We keep the data removal proportion ρ = 30%, and only change cutoff. 16Table A1 shows the relationship between cutoff and the Class-IL average accuracy. Note that from the perspective of efﬁciency, we would like thecutoff stage as early as possible, so that the remaining epochs will have less examples. However, from Table A1, we can see that if we set it too early, i.e., cutoff ≤3, the accuracy drop is signiﬁcant. This indicate that even for the “easy-to-learn” examples, removing them too early results in underﬁtting. As a balance point between accuracy and efﬁciency, we choose cutoff = 4in our ﬁnal version. D.4 Supplementary Ablation Study Table A2: Ablation study on Split-CIFAR10 with 0.90 sparsity. TDM DDR DGM Class-IL (↑) FLOPs Train ×1015(↓) Memory Footprint (↓) \u0017 \u0017 \u0017 72.70 13.9 247MB \u0013 \u0017 \u0017 72.98 1.6 166MB \u0013 \u0013 \u0017 73.20 1.2 166MB \u0013 \u0017 \u0013 73.30 1.5 165MB \u0013 \u0013 \u0013 73.42 1.1 165MB Similar to Table 3, we show ablation study with 0.90 sparsity ratio in Table A2. Under a larger sparsity ratio, the conclusion that all components contribute to the ﬁnal performance still holds. However, we can observe that the accuracy increase that comes from DDR and DGM is less than what we show in Table 3. We assume that larger sparsity ratio makes it more difﬁcult for the model to retain good accuracy in CL. Similar results has also been observed in (66) under the usual i.i.d. learning setting. D.5 Exploration on DGM Table A3: Ablation study of the gradient sparsity ratio on Split-CIFAR10. weight sparsity gradient sparsityClass-IL (↑) FLOPs Train ×1015(↓) Memory Footprint (↓) 0.75 0.78 74.08 3.4 178MB 0.75 0.80 73.97 3.3 177MB 0.75 0.82 73.79 3.3 177MB 0.75 0.84 73.26 3.2 176MB 0.90 0.91 73.33 1.6 166MB 0.90 0.92 73.30 1.5 165MB 0.90 0.93 72.64 1.5 165MB We conduct further experiments to demonstrate the inﬂuence of gradient sparsity, and the results are shown in Table A3. There are two sets of the experiments with different weight sparsity settings: 0.75 and 0.90. Within each set of the experiments (the weight sparsity is ﬁxed), we vary the gradient sparsity values. From the results we can see that increasing the gradient sparsity can decrease the FLOPs and memory footprint. However, the accuracy performance degrades more obvious when the gradient sparsity is too much for the weight sparsity. The results indicate that suitable gradient sparsity setting can bring further efﬁciency to the training process while boosting the accuracy performance. In the main results, the gradient sparsity is set as 0.80 for 0.75 weight sparsity, and set as 0.92 for 0.90 weight sparsity. E On-Mobile Compiler Optimizations and Inference Results E.1 Compiler Optimizations Each iteration in the training process is composed of two phases, i.e., the forward propagation and backward propagation. Prior works (17; 22; 26; 27; 29; 35; 37; 68) have proved that sparse weight matrices (tensors) can provide inference acceleration via reducing the number of multiplications in convolution operation. Therefore, the forward propagation phase, which is the same as inference, 17can be accelerated by the sparsity inherently. As for backward pass, both of the calculation steps are based on convolution, i.e., matrix multiplication. Equation 4 uses sparse weight matrix (tensor) as the operand, thus can be accelerated in the same way as the forward propagation. Equation 5 allows a sparse output result since the gradient matrix is also sparse. Thus, both two steps have reduced computations, which are roughly proportional to the sparsity ratio, providing the acceleration for the backward propagation phase. Compiler optimizations are used to accelerate the inference in prior works (21; 46; 61). In this work, we extend the compiler optimization techniques for accelerating the forward and backward pass during training on the edge devices. Our compiler optimizations are general, support both sparse model training and inference accelerations on mobile platforms. The optimizations include 1) the supports for sparse models; 2) an auto-tuning process to determine the best-suited conﬁgurations of parameters for different mobile CPUs. The details of our compiler optimizations are presented as follows. E.1.1 Supports for Sparse Models Our framework supports sparse model training and inference accelerations with unstructured pruning. For the sparse (pruned) model, the framework ﬁrst compacts the model storage with a compression format called Compressed Sparse Row (CSR) format, and then performs computation reordering to reduce the branches within each thread and eliminates the load imbalance among threads. A row reordering optimization is also included to further improve the regularity of the weight matrix. After this reordering, the continuous rows with identical or similar numbers of non-zero weights are processed by multi-threads simultaneously, thus eliminating thread divergence and achieving load balance. Each thread processes more than one rows, thus eliminating branches and improving instruction-level parallelism. Moreover, a similar optimization ﬂow (i.e., model compaction and computation reorder and other optimizations) is employed to support all compiler optimizations for sparsity as PatDNN (46). E.1.2 Auto-Tuning for Different Mobile CPUs During DNN sparse training and inference execution, there are many tuning parameters, e.g., matrix tiling sizes, loop unrolling factors, and data placement on memory, that inﬂuence the performance. It is hard to determine the best-suited conﬁguration of these parameters manually. To alleviate this problem, our compiler incorporates an auto-tuning approach for sparse (pruned) models. The Genetic Algorithm is leveraged to explore the best-suited conﬁgurations automatically. It starts the parameter search process with an arbitrary number of chromosomes and explores the parallelism better. Acceleration codes for different DNN models and different mobile CPUs can be generated efﬁciently and quickly through this auto-tuning process. E.2 Inference Acceleration Results On Mobile 60 12 18 2 4 A ccelation R esult s of R esNet -18 on  Split CIF AR-10 0 . 00 0 . 7 5 0 . 90 0 . 95 100 20 30 40 A ccelation R esult s of R esNet -18 on Split Tin y-ImageNet 0 . 00 0 . 7 5 0 . 90 0 . 95 Figure 5: Inference results of sparse models obtained from SparCL under different sparsity ratio compared with dense models obtained from traditional CL methods (sparsity ratio 0.00). Besides accelerating the training process, SparCL also possesses the advantages of providing a sparse model as the output for faster inference. To demonstrate this, we show the inference acceleration results of SparCL with different sparsity ratio settings on mobile in Figure 5. The inference time is measured on the CPU of an off-the-shelf Samsung Galaxy S20 smartphone. Each test takes 50 runs on different inputs with 8 threads on CPU. As different runs do not vary greatly, only the average time is reported. From the results we can see that the obtained sparse model from SparCL can signiﬁcantly accelerate the inference on both Split-CIFAR-10 and Tiny-ImageNet dataset compared to the model obtained by traditional CL training. For ResNet-18 on Split-CIFAR-10, the model obtained by traditional CL training, which is a dense model, takes 18.53ms for inference. The model provided by SparCL can achieve an inference time of 14.01ms, 8.30ms, and 5.85ms with sparsity 18ratio of 0.75, 0.90, and 0.95, respectively. The inference latency of the dense ResNet-18 obtained by traditional CL training on Tiny-ImageNet is 39.64 ms. While the sparse models provided by SparCL with sparsity ratio settings as 0.75, 0.90, and 0.95 reach inference speed of 33.06ms, 20.37ms, and 15.49ms, respectively, on Tiny-ImageNet. 19",
      "meta_data": {
        "arxiv_id": "2209.09476v1",
        "authors": [
          "Zifeng Wang",
          "Zheng Zhan",
          "Yifan Gong",
          "Geng Yuan",
          "Wei Niu",
          "Tong Jian",
          "Bin Ren",
          "Stratis Ioannidis",
          "Yanzhi Wang",
          "Jennifer Dy"
        ],
        "published_date": "2022-09-20T05:24:48Z",
        "pdf_url": "https://arxiv.org/pdf/2209.09476v1.pdf"
      }
    },
    {
      "title": "Online Learned Continual Compression with Adaptive Quantization Modules",
      "abstract": "We introduce and study the problem of Online Continual Compression, where one\nattempts to simultaneously learn to compress and store a representative dataset\nfrom a non i.i.d data stream, while only observing each sample once. A naive\napplication of auto-encoders in this setting encounters a major challenge:\nrepresentations derived from earlier encoder states must be usable by later\ndecoder states. We show how to use discrete auto-encoders to effectively\naddress this challenge and introduce Adaptive Quantization Modules (AQM) to\ncontrol variation in the compression ability of the module at any given stage\nof learning. This enables selecting an appropriate compression for incoming\nsamples, while taking into account overall memory constraints and current\nprogress of the learned compression. Unlike previous methods, our approach does\nnot require any pretraining, even on challenging datasets. We show that using\nAQM to replace standard episodic memory in continual learning settings leads to\nsignificant gains on continual learning benchmarks. Furthermore we demonstrate\nthis approach with larger images, LiDAR, and reinforcement learning\nenvironments.",
      "full_text": "Online Learned Continual Compression with Adaptive Quantization Modules Lucas Caccia 1 2 3 Eugene Belilovsky 2 4 Massimo Caccia 2 4 5 Joelle Pineau 1 2 3 Abstract We introduce and study the problem of Online Continual Compression, where one attempts to simultaneously learn to compress and store a rep- resentative dataset from a non i.i.d data stream, while only observing each sample once. A naive application of auto-encoders in this setting en- counters a major challenge: representations de- rived from earlier encoder states must be usable by later decoder states. We show how to use discrete auto-encoders to effectively address this challenge and introduce Adaptive Quantization Modules (AQM) to control variation in the com- pression ability of the module at any given stage of learning. This enables selecting an appropriate compression for incoming samples, while taking into account overall memory constraints and cur- rent progress of the learned compression. Unlike previous methods, our approach does not require any pretraining, even on challenging datasets. We show that using AQM to replace standard episodic memory in continual learning settings leads to sig- niﬁcant gains on continual learning benchmarks. Furthermore we demonstrate this approach with larger images, LiDAR, and reinforcement learn- ing agents. 1. Introduction Interest in machine learning in recent years has been fueled by the plethora of data being generated on a regular basis. Effectively storing and using this data is critical for many applications, especially those involving continual learning. In general, compression techniques can greatly improve data storage capacity, and, if done well, reduce the memory and compute usage in downstream machine learning tasks (Gueguen et al., 2018; Oyallon et al., 2018). Thus, learned compression has become a topic of great interest (Theis 1McGill 2Mila 3Facebook AI Research 4University of Montreal 5ElementAI. Correspondence to: Lucas Caccia <lucas.page-caccia@mail.mcgill.ca>, Eugene Belilovsky <eu- gene.belilovsky@umontreal.ca>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). et al., 2017; Ball´e et al., 2016; Johnston et al., 2018). Yet its application in reducing the size of datasets bound for machine learning applications has been limited. This work focuses on the following familiar setting: new training data arrives continuously for a learning algorithm to exploit, however this data might not be iid, and further- more there is insufﬁcient storage capacity to preserve all the data uncompressed. We may want to train classiﬁers, reinforcement learning policies, or other models continu- ously from this data as it’s being collected, or use samples randomly drawn from it at a later point for a downstream task. For example, an autonomous vehicle (with bounded memory) collects large amounts of high-dimensional train- ing data (video, 3D lidar) in a non-stationary environment (e.g. changing weather conditions), and overtime applies an ML algorithm to improve its behavior using this data. This data might be transferred at a later point for use in down- stream learning. Current learned compression algorithms, e.g. (Torfason et al., 2018), are not well designed to deal with this case, as their convergence speed is too slow to be usable in an online setting. In the ﬁeld of continual/lifelong learning (Thrun & Mitchell, 1995), which has for now largely focused on classiﬁcation, approaches based on storing memories for later use have emerged as some of the most effective in online settings (Lopez-Paz et al., 2017; Aljundi et al., 2018; Chaudhry et al.; 2019; Aljundi et al., 2019). These memories can be stored as is, or via a generative model (Shin et al., 2017). Then, they can either be used for rehearsal (Chaudhry et al., 2019; Aljundi et al., 2019) or for constrained optimization (Lopez-Paz et al., 2017; Chaudhry et al.; Aljundi et al., 2018). Indeed many continual learning applications would be greatly improved with replay approaches if one could afford to store all samples. These approaches are however inherently limited by the amount of data that can be stored Learning a generative model to compress the previous data stream thus seems like an appealing idea. However, learn- ing generative models, particularly in the online and non- stationary setting, continues to be challenging, and can greatly increase the complexity of the continual learning task. Furthermore, such models are susceptible to catas- trophic forgetting (Aljundi et al., 2019). An alternate ap- proach is to learn a compressed representation of the data, arXiv:1911.08019v3  [cs.LG]  20 Aug 2020Online Learned Continual Compression with Adaptive Quantization Modules Online Continual Compression z_1 z_1 z_2 z_3z_1 z_2 X ~ non-iid t=1 t=2 t=3 query  samples query  samples Learner Learner Encoder Encoder Encoder Decoder     t=3 Decoder     t=1 Decoder     t=2 storage storage storage  Decoder     t=3 z_1 Decoder     t=1 Representation Drift z_1 X_3 X_2 X_1 Figure 1.Illustration of the challenges in the Online Continual Compression problem. A model must be able to decode representations encoded by previous versions of the autoencoder, permitting anytime access to data for the learner. This must be accomplished while dealing with a time-varying data distribution and ﬁxed memory constraints which can be more stable than learning generative models. Learned compression in the online and non-stationary set- ting itself introduces several challenges illustrated in Fig 1. Firstly the learned compression module must be able to de- code representations encoded by earlier versions of itself, introducing a problem we refer to as representation drift. Secondly, the learned compressor is itself susceptible to catastrophic forgetting. Finally, the learned compression needs to be adaptive to maintain a prescribed level of re- construction quality even if it has not fully adapted to the current distribution. In this work we demonstrate that the VQ-V AE framework (van den Oord et al., 2017; Razavi et al., 2019), originally introduced in the context of generative modeling and density estimation, can be used online to effectively address repre- sentation drift while achieving high compression. Further- more, when augmented with an internal replay mechanism it can overcome forgetting. Finally we use propose to use multiple gradient-isolated compression levels to allow the compressor to adaptively store samples at different compres- sion scales, based on the amount of data, storage capacity, and effectiveness of the model in compressing samples. The main contributions of this work are: (a) we intro- duce and highlight the online learned continual compres- sion (OCC) problem and its challenges. (b) We show how representation drift, one of the key challenges, can be tackled by effective use of codebooks in the VQ-V AE framework. (c) We propose an architecture using multi- ple VQ-V AE’s, adaptive compression scheme, stream sam- pling scheme, and self-replay mechanism that work together to effectively tackle the OCC problem. (d) We demon- strate this can yield state-of-the-art performance in stan- dard online continual image classiﬁcation benchmarks and demonstrate the applications of our OCC solution in a va- riety of other contexts. The code to reproduce our results is available at https://github.com/pclucas14/ adaptive-quantization-modules 2. Related Work Learned compression has been recently studied for the case of image compression. Works by Theis et al. (2017); Ball´e et al. (2016); Johnston et al. (2018) have shown learned compressions can outperform standard algorithms like JPEG. These methods however are difﬁcult to adapt for online settings as they do not directly address the challenges of the OCC problem (e.g. representation drift). Continual Learning research currently focuses on over- coming catastrophic forgetting (CF) in the supervised learn- ing setting, with some limited work in the generative mod- eling and reinforcement learning settings. Most continual learning methods can be grouped into three major families. Some algorithms dynamically change the model’s architec- ture to incorporate learning from each task separately (Rusu et al., 2016; Li & Hoiem, 2018; Fernando et al., 2017). Al- though these methods can perform well in practice, their introduction of task-speciﬁc weights requires growing com- pute and memory costs which are problematic for the online setting. Another set of techniques employ regularization to constrain weights updates in the hope of maintaining knowl- edge from previous tasks. Notable methods in this class include (Kirkpatrick et al., 2017; Husz´ar, 2017; Zenke et al., 2017; Nguyen et al., 2017; Chaudhry et al., 2018). However, this set of approaches has been shown to be inefﬁcient in the online setting (Chaudhry et al., 2019). The last family of methods encapsulates all that have a mech- anism to store information about the previous data distribu- tions. This memory then serves as a tool for the continual learner to rehearse previous tasks. The simplest instantia- tion of this method is to keep and sample from a buffer of old data to retrain the model after every update (Chaudhry et al., 2019). This approach is widely used in RL where it is known as Experience Replay (ER) (Lin, 1993; Mnih et al., 2013; Andrychowicz et al., 2016). Another method, known as Generative Replay (GR) (Shin et al., 2017), uses generative modeling to store past task distributions. The con- tinual learner then trains on generated samples to alleviateOnline Learned Continual Compression with Adaptive Quantization Modules CF. Other notable examples are Gradient Episodic Memory (GEM) (Lopez-Paz et al., 2017), iCarl (Rebufﬁ et al., 2017), and Maximally Interfered Retrieval (MIR) (Aljundi et al., 2019), as well as (Aljundi et al., 2018; Hu et al., 2018). Most closely related to our work, Riemer et al. (2017) consider compressing memories for use in the continual classiﬁcation task. They also employ a discrete latent variable model but with the Gumbel approximation, which shows to be less effective than our approach. Furthermore a separate ofﬂine iid pre-training step for the learned compression is required in order to surpass the ER baseline, distinctly different from the online continual compression we consider. Lidar compression is considered in (Tu et al., 2019) and (Caccia et al., 2018). Both approaches use a similar projec- tion from 3D (x,y,z ) coordinates to 2D cylindrical coordi- nates, and leverage deep generative models to compress the data. However, neither accounts for potential distribution shift, nor for online learning. In this work we show that us- ing this 2D projection in conjunction with our model allows us to mitigate the two issues above for lidar data. 3. Methodology In this section we outline our approach to the online con- tinual compression problem. First we will review the VQ- V AE and highlight the properties making it effective for representational drift. Then we will describe our adaptive architecture, storage, and sampling scheme. 3.1. Problem Setting: Online Continual Compression We consider the problem setting where a stream of samples x ∼Dt arrives from different distributions Dt changing over time t= 1...T . We have a ﬁxed storage capacity ofC bytes where we would like to store the most representative information from all data distributions D1,...DT . There is notably a trade-off in quality of information versus the amount of samples stored. We propose to use a learned compression model, and most crucially, this model must also be stored within the Cbytes, to encode and decode the data samples. Another critical requirement is that at anytime tthe content of the storage (data and/or compression model) be usable for downstream applications. An important chal- lenge, illustrated in Figure 1, is that the learned compression module will change over time, while we still need to be able to decode the memories in storage. 3.2. Vector Quantized V AE for Online Compression The VQ-V AE is a discrete auto-encoder which relies on a vector quantization step to obtain discrete latent repre- sentations. An embedding table, E ∈RK×D consisting of K vectors of size D, is used to quantize encoder out- puts. Given an input (e.g. an RGB image), the encoder ﬁrst encodes it as a Hh ×Wh ×D tensor, where Hh Encoder     t=1      Encoder     t=3      same after VQ Vector Quantization different codebook Figure 2.Illustration of reduced representation drift from Vector Quantization and Wh denote the height and width of the latent repre- sentation. Then, every Ddimensional vector is quantized using a nearest-neighbor lookup on the embedding table. Speciﬁcally, zij = arg mine∈E ||enc(x)ij −e||2, where i,j refers to a spatial location. The output of the quanti- zation step is then fed through the decoder. The gradient of this non-differentiable step is approximated using the straight-through estimator. An important property to notice is that to reconstruct the input, only the Hh ×Wh indices are required, thus yielding high compression (van den Oord et al., 2017). Critically, the embedding tables are updated independently from the encoder and decoder, namely by minimizing mine ||sg[enc(x)ij] −e||, where sg is the stop gradient operator. Observe in the case of online compression, if the embedding table is ﬁxed, then a change in the encoder parameters and therefore a change in the encoder output for a given input will not change the ﬁnal quantized representation z, unless it is sufﬁciently large, thus we can observe that if the em- beddings change slowly or are ﬁxed we can greatly improve our control of the representational drift. This effect is illus- trated in Figure 2. On the other hand we do need to adapt the embedding table, since randomly selected embeddings would not cover well the space of encoder outputs. 3.3. Adaptive Quantization Modules To address issues of how to optimize storage and sampling in the context of Online Continual Compression we intro- duce Adaptive Quantization Modules (AQM). We use AQM to collectively describe the architecture, adaptive multi-level storage mechanism, and data sampling method used. To- gether they allow effectively constraining individual sample quality and memory usage while keeping in storage a faith- ful representation of the overall distribution.Online Learned Continual Compression with Adaptive Quantization Modules Algorithm 1: AQM L EARNING WITH SELF -REPLAY Input: Learning rate α, EXTERNAL LEARNER 1 Initialize: AQM Memory M; AQM Parameters θaqm 2 for t∈1..T do 3 % Fetch data from current task 4 for Binc ∼Dt do 5 for n∈1..N do 6 B←Binc 7 if t> 1 then 8 % Fetch data from buffer 9 Bre ∼SAMPLE (M,θaqm) 10 B ←(Binc,Bre) 11 end 12 % Update AQM 13 θaqm ←ADAM(θaqm,B,α ) 14 % Send data to external learner 15 EXTERNAL LEARNER (B) 16 if t> 1 then UPDATE BUFFER REP(M,θaqm) 17 %Save current indices 18 ADDTOMEMORY (M,Binc,θaqm) 19 end 20 end 21 end AQM uses an architecture consisting of a sequence of VQ- V AEs, each with a buffer. The AQM approach to online continual compression is overall summarized in Algorithm 1 (note ADDTOMEMORY is described in Appendix A). In- coming data is added to storage using an adaptive compres- sion scheme described in Algorithm 2. Incoming data is also used along with randomly sampled data from storage (self- replay) to update the current AQM model. The randomly sampled data also updates its representation in storage as per Algorithm 3. As illustrated by the optional lines in blue, Algorithm 1 can run concurrently with a downstream learn- ing task (e.g. online continual classiﬁcation) which would use the same batch order. It can also be run independently as part of a data collection. In the sequel we give further details on all these elements 3.3.1. A RCHITECTURE AND TRAINING Each AQM module contains a VQ-V AE and a corresponding buffer of adaptive capacity. A diagram of the architecture is given in Figure 3. We will denote the output after quan- tization of each module i as zi q and the set of codebook indexes used to obtain zi q as ai. Note that ai are the discrete representations we actually store. Each subsequent module produces and stores an ai requiring fewer bits to represent. For RGB images, the compression rate at a given level is given by H×W×3×log2(256) Nc×Hhi×Whi×⌈log2 (Ki)⌉. Here Ki is the number of embeddings in the codebooks, ( Hhi, Whi) the spatial dimension of the latent representation and Nci the number of codebooks. Algorithm 2: ADAPTIVE COMPRESS Input: datapoint x, AQM with Lmodules, threshold dth 1 % Forward all modules, store encodings 2 {zi q,ai}i=1..L= ENCODE (x) 3 for i∈L...1 do 4 % Decode from level i to output space 5 ˆx= DECODE (zi q) 6 % Check reconstruction error 7 if MSE(ˆx,x) <dth then return ai, i 8 end 9 % Otherwise, return original input 10 return x,0 Algorithm 3: UpdateBufferRep Input: Memory M, AQM with Llevels, data D, distortion threshold dth 1 for x∈Ddo 2 hidx, blockid = ADAPTIVE COMPRESS (x, AQM, dth) 3 % Delete Old Repr. 4 DELETE (M[x]) 5 % Add new one 6 ADD(M, hidx) 7 end VQV AE-2 (Razavi et al., 2019) also uses a multi-scale hier- archical organization, where unlike our AQM, the top level models global information such as shape, while the bottom level, conditioned on the top one, models local information. While this architecture is tailored for generative modeling, it is less attractive for compression, as both the bottom and top quantized representations must be stored for high qual- ity reconstructions. Furthermore in AQM each module is learned in a greedy manner using the current estimate of z(i−1) q without passing gradients between modules similar to (Belilovsky et al., 2019; Nøkland & Eidnes, 2019). A subsequent module is not required to build representations which account for all levels of compression, thus minimiz- ing interference across resolutions. This allows the modules to each converge as quickly as possible with minimal drift at their respective resolution, particularly important in the online continual learning case. 3.3.2. M ULTI -LEVEL STORAGE Our aim is to store the maximum number of samples in an allotted Cbytes of storage, while assuring their quality, and our ability to reconstruct them. Samples are thus stored at different levels based on the compressors’ current ability. The process is summarized in Algorithm 2. Such an approach is particularly helpful in the online non- stationary setting, allowing knowledge retention before the compressor network has learned well the current distribution. Note in Alg. 2 samples can be completely uncompressed until the ﬁrst module is able to effectively encode them. This can be crucial in some cases, if the compressor has not yet converged, to avoid storing poorly compressed rep-Online Learned Continual Compression with Adaptive Quantization Modules Figure 3.Architecture of Adaptive Quantization Modules. Each level uses its own loss and maintains its own replay buffer. Yello dotted lines indicate gradient isolation between modules resentations. Further taking into account that compression difﬁculty is not the same for all datapoints, this allows use of more capacity for harder data, and fewer for easier. We also note, since we maintain stored samples at each mod- ule and the modules are decoupled, that such an approach allows to easily distribute training in an asynchronous man- ner as per Belilovsky et al. (2019). 3.3.3. S ELF -REPLAY AND STREAM SAMPLING As shown in Alg. 1 our AQM is equipped with an internal experience replay mechanism (Mnih et al., 2013), which reconstructs a random sample from storage and uses it to perform an update to the AQM modules, while simultane- ously freeing up overall memory if the sample can now be compressed at a later AQM module. This has the effect of both reducing forgetting and freeing memory. In practice we replay at the same rate as incoming samples arrive and thus replay will not increase asymptotic complexity of the online learning. Finally, for efﬁciency the replay can be coupled to an external online learner querying for random samples from the overall memory. Since we would like the AQM to work in cases of a ﬁxed memory capacity it must also be equipped with a mecha- nism for selecting which samples from the stream to store and which to delete from memory. Reservoir Sampling (RS) is a simple yet powerful approach to this problem, used successful in continual learning (Chaudhry et al., 2019). It adds a sample from the stream with prob. p= buffer capacity points seen so far while remove a random sample. However, RS is not directly compatible with AQM primarily because the amount of sam- ples that can be stored varies over time. This is because samples at different levels have different memory usage and memory can be freed by replay. We thus propose an alternative scheme, which maximally ﬁlls available memory and selects non-uniformly samples for deletion. Speciﬁ- cally when a larger amount of samples are added at one point in the stream, they become more likely to be removed. The details of this stream sampling method are provided in Appendix A. 3.3.4. DRIFT CONTROL VIA CODEBOOK STABILIZATION As mentioned previously, a good online compressor must control its representational drift, which occurs when up- dates in the auto-encoder parameters creates a mismatch with the static representations in the buffer. Throughout the paper we measure representational drift by compar- ing the following time varying quantity: DRIFT t(z) = RECON ERR ( Decode(θt; z),x ) where θt the model pa- rameters at timetand (z,x) is a stored compressed represen- tation and its original uncompressed datapoint respectively. For all experiments on images, RECON ERR is simply the mean squared error. As illustrated in Sec 3.2 a slow changing codebook can allow to control drifting representations. This can be in part accomplished by updating the codebook with an exponential moving average as described in (van den Oord et al., 2017, Appendix A), where it was used to reduce the variance of codebook updates. This alone is insufﬁcient to fully control drift, thus once a given module yields satisfactory compressions on the data stream, we freeze the module’s embedding matrix but leave encoder and decoder parameters free to change and adapt to new data. Moreover, we note that ﬁxing the codebook for a given module does not affect the reconstruction performance of subsequent modules, as they only need access to the current module’s decoder which can still freely change. 4. Experiments We evaluate the efﬁcacy of the proposed methods on a suite of canonical and new experiments. In Section 4.1 we present results on standard supervised continual learning benchmarks on CIFAR-10. In Section 4.2 we evaluate other downstream tasks such as standard iid training applied on the storage at the end of online continual compression. For this evaluation we consider larger images from Imagenet, as well as on lidar data. Finally we apply AQM on observations of an agent in an RL environment.Online Learned Continual Compression with Adaptive Quantization Modules Accuracy (↑) M= 20 M= 50 iid online 60.8 ±1.0 60 .8 ±1.0 iid ofﬂine 79.2 ±0.4 79 .2 ±0.4 GEM (Lopez-Paz et al., 2017) 16.8 ±1.1 17 .1 ±1.0 iCarl (5 iter) (Rebufﬁ et al., 2017) 28.6 ±1.2 33 .7 ±1.6 ﬁne-tuning 18.4 ±0.3 18 .4 ±0.3 ER 27.5 ±1.2 33 .1 ±1.7 ER-MIR (Aljundi et al., 2019) 29.8 ±1.1 40 .0 ±1.1 ER-JPEG 33.9 ±1.0 43 .1 ±0.6 Gumbel AE (Riemer et al., 2018) 25.5 ±2.0 28 .8 ±2.9 AQM (ours) 43.5 ±0.7 47 .0 ±0.8 Forgetting (↓) M= 20 M= 50 N/A N/A N/A N/A 73.5 ±1.7 70 .7 ±4.5 49 ±2.4 40 .6 ±1.1 85.4 ±0.7 85 .4 ±0.7 50.5 ±2.4 35 .4 ±2.0 50.2 ±2.0 30 .2 ±2.3 54.8 ±1.2 44 .3 ±0.9 71.5 ±2.8 67 .2 ±3.9 23.0 ±1.0 19 .0 ±1.4 Table 1.Shared head results on disjoint CIFAR-10. Total memory per class M measured in sample memory size. We report (a) Accuracy, (b) Forgetting (lower is better). 4.1. Online Continual Classiﬁcation Although CL has been studied in generative modeling (Ramapuram et al., 2017; Lesort et al., 2018; Zhai et al., 2019; Lesort et al., 2019) and reinforcement learning (Kirk- patrick et al., 2017; Fernando et al., 2017; Riemer et al., 2018), supervised learning is still the standard for evalua- tion of new methods. Thus, we focus on the online continual classiﬁcation of images for which our approach can provide a complement to experience replay. In this setting, a new task consists of new image classes that the classiﬁer must learn, while not forgetting the previous ones. The model is only allowed one pass through the data (Lopez-Paz et al., 2017; Chaudhry et al.; Aljundi et al., 2019; Chaudhry et al., 2019). The online compression here takes the role of replay buffer in replay based methods such as (Chaudhry et al., 2019; Aljundi et al., 2019). We thus run Algorithm 1, with an additional online classiﬁer being updated performed at line 15. Here we consider the more challenging continual classiﬁca- tion setting often referred to as using ashared-head (Aljundi et al., 2019; Farquhar & Gal, 2018; Aljundi et al., 2018). Here the model is not informed of the task (and thereby the subset of classes within it) at test time. This is in contrast to other (less realistic) CL classiﬁcation scenarios where the task, and therefore subset of classes, is provided explicitly to the learner (Farquhar & Gal, 2018; Aljundi et al., 2019). For this set of experiments, we report accuracy, i.e. 1 T ∑T i=1 RT,i, and forgetting, i.e. 1 T−1 ∑T−1 i=1 max(R:,i) − RT,i with R ∈ RT×T representing the accuracy matrix where Ri,j is the test classiﬁcation accuracy on task jwhen task iis completed. Baselines A basic baseline for continual supervised learn- ing is Experience Replay ( ER). It consists of storing old data in a buffer to replay old memories. Although relatively simple recent research has shown it is a critical baseline to consider, and in some settings is actually state-of-the- art (Chaudhry et al., 2019; Aljundi et al., 2019; Rolnick et al., 2018). AQM can be used as an add-on to ER that in- corporates online continual compression. We also compare against ER with standard JPEG compression. In addition we consider the following baselines. iid online (upper-bound) trains the model with a single-pass through the data on the same set of samples, but sampled iid. iid ofﬂine (upper- bound) evaluates the model using multiple passes through the data, sampled iid. We use 5 epochs in all the experiments for this baseline. ﬁne-tuning trains continuously upon ar- rival of new tasks without any forgetting avoidance strategy. iCarl (Rebufﬁ et al., 2017) incrementally classiﬁes using a nearest neighbor algorithm, and prevents catastrophic for- getting by using stored samples. GEM (Lopez-Paz et al., 2017) uses stored samples to avoid increasing the loss on previous task through constrained optimization. It has been shown to be a strong baseline in the online setting. It gives similar results to the recent A-GEM (Chaudhry et al.). ER- MIR (Aljundi et al., 2019) controls the sampling of the replays to bias sampling towards samples that will be for- gotten. We note that the ER-MIR critera is orthogonal to AQM, and both can be applied jointly.Gumbel AE (Riemer et al., 2018) learns an autoencoder for ER using the Gumbel softmax to obtain discrete representations. We evaluate with the standard CIFAR-10 split (Aljundi et al., 2018), where 5 tasks are presented sequentially, each adding two new classes. Evaluations are shown in Table 1. Due to our improved storage of previous data, we observe sig- niﬁcant improvement over other baselines at various mem- ory sizes. We can contrast AQM’s performance with ER’s to understand the net impact of our compression scheme. Speciﬁcally, AQM improves over ER by 16.0% and 13.9% in the M=20 and M=50 case, highlighting the effectiveness of online compression. Our approach is also superior in forgetting by a signiﬁcant margin in both memory settings. To compare directly to reporting in (Riemer et al., 2018) we also benchmarked our implementation on the Incremental CIFAR-100 multi-head experiment (Lopez-Paz et al., 2017) with the same settings as in (Riemer et al., 2018). By using AQM we were able to get 65.3 vs the reported 43.7 using a buffer of size 200. To speciﬁcally isolate the advantageOnline Learned Continual Compression with Adaptive Quantization Modules Accuracy RS 5.2 ±0.2 2 Module AQM (ours) 23.2 ± 1.1 Ablate 2nd Module 20.5 ±1.3 Ablate Fixing Codebook 19.2 ±0.6 Ablate Decoupled Training 16.5 ±0.7 Ablate Adaptive Compression 13.1 ±3.2 Table 2.Imagenet ofﬂine training evaluation from online continual compression. We see a clear gain over a standard Reservoir sam- pling approach. We then ablate each component of our proposal showing each component is important. Note storage used in each experiment is identical (including accounting for model sizes). of gumbel softmax versus the vector quantization for drift, we replaced the vector quantization approach with gumbel softmax in an AQM. We observed signﬁcantly less drift in the case where vector quantization is used. Full details of this experiment are described in the supplementary materials along with visualizations. The CIFAR-10 dataset has a low resolution (3 ×32 ×32) and uses a lot of data per task (10K samples). These two characteristics might leave the online compression problem easier than in a real-life scenario. Speciﬁcally, if the ﬁrst tasks are long enough and the compression rate is not too large, the model can quickly converge and thus not incur too much representation drift. Indeed, we found that us- ing a single module is already sufﬁcient for this task. For these reasons, we now study the AQM in more challenging settings presented in the next section. 4.2. Ofﬂine Evaluation on Larger Images Besides the standard continual classiﬁcation setup, we pro- pose several other evaluations to determine the effectiveness of the stored data and compression module after learning online compression. We also perform a detailed ablation to study the efﬁcacy of each component in AQM. Ofﬂine training on Imagenet We compare the effective- ness of the stored memories of AQM after a certain amount of online continual compression. We do this by training in a standard iid way an ofﬂine classiﬁcation model using only reconstructions obtained from the storage sampled after on- line continual compression has progressed for a period of time. In each case we would have the same sized storage available. We note that simply having more stored memo- ries does not amount to better performance as their quality may be severely degraded and affected by drift. Using this evaluation we ﬁrst compare a standard reservoir sampling approach on uncompressed data to a 2 module AQM using the same size storage. We observe that perfor- mance is drastically increased using the compressed sam- ples. We then use this to perform a series of ablations to Figure 4.Impact of codebook freezing. Vertical black line indi- cates freezing point. We see that AQM is still able to adapt and reduce its reconstruction loss, while having stable compressed representations. Results averaged over 5 runs demonstrate each component of our proposal is important. Speciﬁcally (a) we restrict AQM to have only one module, (b) instead of decoupled training we train modules end-to- end, (c) we remove adaptive compression, thus all samples are stored in the most compressed block, regardless of qual- ity, and (d) we do not stabilize the codebook, the embedding matrices of every block are never ﬁxed. We observe that all these elements contribute to successfully storing a represen- tative set of data for the distribution online. Drift Ablation We have seen the importance of codebook freezing when dealing with high dimensional datasets. How- ever, judging solely from the ﬁnal downstream task perfor- mance it’s difﬁcult to see if the model continues adapting after freezing. As alluded in Sec 3.2 there is a tradeoff be- tween keeping recoverable representations and a model’s ability to continue to adapt. To shed some light on this, we run the following experiment: we run a vanilla VQ-V AE on the same 20 task mini-imagenet stream, without storing any samples. When it reaches a pre-speciﬁed performance threshold, we ﬁx the codebook, and store compressed held- out data from the ﬁrst task. We then continue to update the VQ-V AE parameters, and the memory is kept ﬁxed for the rest of the stream. We apply self-replay but no other AQM mechanisms (e.g. no sampling from the input stream and no adaptive compression). We monitor how well the VQ-V AE can adapt by looking at the streaming reconstruction cost, measured on the in- coming data before an update. We also monitor the drift of samples stored in the buffer. Results are presented in Figure 4. They demonstrate that drift is controlled by stabilizing the codebook, while the model can still improve at nearly the same rate. Further analysis, along with an additional ex- periment showcasing the robustness of vector quantization to small perturbations is included in the Appendix.Online Learned Continual Compression with Adaptive Quantization Modules Figure 5.Top: Sample decoded from the buffer at the end of train- ing from scratch (32x compression rate). Bottom: Original lidar LiDAR Range data enables autonomous vehicles to scan the topography of their surrounding, giving precise mea- surements of an obstacle’s relative location. In its raw form, range data can be very large, making it costly to transmit in real time, or for long term storage. Equipping self-driving cars with a good lidar compressor can enable fast vehicle- to-vehicle (V2V) communication, leading to safer driving (Eckelmann et al., 2017). Moreover, since data collected by autonomous vehicles can be highly non-stationary (new objects on the road, changing weather or trafﬁc conditions), having a compressor which can quickly adapt to this distri- bution change will reduce the required memory for storage (or bandwidth for real time transmission). We proceed to train AQM on the Kitti Dataset (Geiger et al., 2013), which contains 61 LiDAR scan recordings, each belonging to either the “residential”, “road”, “city” envi- ronments. The data is processed as in (Caccia et al., 2018), where points from the same elevation angle are sorted in increasing order of azimuth angle along the same row. This yield a 2D grid, making it compatible with the same architec- ture used in the previous experiments. As in (Caccia et al., 2018; Tu et al., 2019), we report the reconstruction cost in Symmetric Nearest Neighbor Root Mean Squared Error (SNNRMSE) which allows to compare two point clouds. Note AQM can also be adapted to use task relevant criteria besides MSE. We consider two settings. In the ﬁrst, we train AQM from scratch on a data stream consisting of recordings from all three environments. We present (once) all the recordings of an environment before moving on to another, in order to maximise the distribution shift. We show qualitative results in Figure 5 and in the supplementary materials. Ob- serve that we are able to effectively reconstruct the LiDAR samples and can easily tradeoff quality with compression. Overall we obtain 18.8 cm SNNRMSE with 32×compres- Size in Mb Raw 1326.8 Gzip 823.0 AQM 35.5 ±.06 AQM + ﬁnetune 33.0 ±.07 AQM + ﬁnetune + PNG 27.9 ±.01 Table 3.Compression results for the data transmission of the city lidar recordings. We require that each compressed scan has an SNNRMSE under 15 cm. sion, which lies in a range that has been shown in (Tu et al., 2019) to be sufﬁcient to enable SLAM localization with very minimal error. In the second setting, we wish to simulate a scenario where some data is available a priori for the model to leverage. However, this data is limited and does not cover all the possible modalities to which an autonomous vehicle could be exposed. To this end, we pretrain AQM in a fully ofﬂine iid manner on the road and residential recordings. We then simulate the deployment of the compressor on a vehicle, where it must compress and transmit in real time the lidar data feed from a new distribution. We therefore stream the held-out city recordings and show that AQM can be ﬁne-tuned on the ﬂy to reduce the required bandwidth for data transmission. Quantitative results are presented in table 3. We ensure that the reconstructed lidar scans have a SNNRMSE smaller than 15.0 cm. Moreover, since the stored representations in AQM are 2D and discrete, we can apply lossless compression schemes such as Portable Network Graphics (PNG). 4.3. Atari RL Environments Another application of online continual compression is for preserving the states of an reinforcement learning agent op- erating online. These agents may often learn new tasks or enter new rooms thus the observations will often be highly non-iid. Furthermore many existing reinforcement learning algorithms already rely on potentially large replay buffers which can be prohibitive (Mnih et al., 2014; Rolnick et al., 2018) to run and may greatly beneﬁt from an approach such as the AQM to run concurrently with reinforcement learning algorithms. We thus perform a proof of concept for the AQM for storing the state sequence encountered by an RL learner in the atari environment(Bellemare et al., 2013). We use the dataset and tasks introduced in (Anand et al., 2019), which runs a random or learned policy in the atari environments and provides a set of classiﬁcation tasks to evaluate whether key information about the state is preserved. Results are shown Table 4. We run the online learning with the AQM on the data stream observed by the random agent. We use the same observations and optimiza- tion as in (Anand et al., 2019) and report the F1 resultsOnline Learned Continual Compression with Adaptive Quantization Modules Figure 6.Top: original. Bottom: reconstructed from AQM Game Cls Input F1 Pong Orig. State 86.7 AQM Recon 86.8 Ms Pacman Orig. State 89.4 AQM Recon 88.3 Pitfall Orig. State 68.2 AQM Recon 66.7 Table 4.Results on RL probing tasks from (Anand et al., 2019) with linear probe applied to original observation and to reconstruc- tions from AQM after online compression. Acc is averaged for each game over game speciﬁc prediction. of a linear probe directly on states for our reconstructions after online compression and the originals. Results for 3 environments are shown in Table 4 and examples in in Fig 6 and the Appendix. We ﬁnd that AQM can well preserve the critical information while compressing the state by 16x. The reference accuracies achieved by our classiﬁer are sim- ilar to those in (Anand et al., 2019). However, we do not control for the representation size unlike those evaluations of various unsupervised models. 5. Conclusion We have introduced online continual compression. We demonstrated vector quantization can be used to control drift and how to create mechanisms that allow maintain- ing quality and maximizing memory usage. These allowed learning compression while compressing. We have shown effectiveness of this online compression approach on stan- dard continual classiﬁcation benchmarks, as well as for compressing larger images, lidar, and atari data. We believe future work can consider dealing with temporal correlations for video and reinforcement learning tasks, as well as im- proved prioritization of samples for storage. References Aljundi, R., Lin, M., Goujaud, B., and Bengio, Y . Online continual learning with no task boundaries. In arXiv, 2018. Aljundi, R., Caccia, L., Belilovsky, E., Caccia, M., Charlin, L., and Tuytelaars, T. Online continual learning with maximally interfered retrieval. In Advances in Neural Information Processing (NeurIPS), 2019. Anand, A., Racah, E., Ozair, S., Bengio, Y ., C ˆot´e, M.- A., and Hjelm, R. D. Unsupervised state representation learning in atari. arXiv preprint arXiv:1906.08226, 2019. Andrychowicz, M., Denil, M., G´omez, S., Hoffman, M. W., Pfau, D., Schaul, T., and de Freitas, N. Learning to learn by gradient descent by gradient descent. In Lee, D. D., Sugiyama, M., Luxburg, U. V ., Guyon, I., and Garnett, R. (eds.), Advances in Neural Information Processing Sys- tems 29, pp. 3981–3989. Curran Associates, Inc., 2016. Ball´e, J., Laparra, V ., and Simoncelli, E. P. End-to-end optimization of nonlinear transform codes for perceptual quality. In 2016 Picture Coding Symposium (PCS), pp. 1–5. IEEE, 2016. Belilovsky, E., Eickenberg, M., and Oyallon, E. De- coupled greedy learning of cnns. arXiv preprint arXiv:1901.08164, 2019. Bellemare, M. G., Naddaf, Y ., Veness, J., and Bowling, M. The arcade learning environment: An evaluation plat- form for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279, 2013. Caccia, L., van Hoof, H., Courville, A., and Pineau, J. Deep generative modeling of lidar data. arXiv preprint arXiv:1812.01180, 2018. Chaudhry, A., Ranzato, M., Rohrbach, M., and Elhoseiny, M. Efﬁcient lifelong learning with a-gem. In ICLR 2019. Chaudhry, A., Dokania, P. K., Ajanthan, T., and Torr, P. H. Riemannian walk for incremental learning: Un- derstanding forgetting and intransigence. arXiv preprint arXiv:1801.10112, 2018. Chaudhry, A., Rohrbach, M., Elhoseiny, M., Ajanthan, T., Dokania, P. K., Torr, P. H., and Ranzato, M. Continual learning with tiny episodic memories. arXiv preprint arXiv:1902.10486, 2019. Eckelmann, S., Trautmann, T., Ußler, H., Reichelt, B., and Michler, O. V2v-communication, lidar system and posi- tioning sensors for future fusion algorithms in connected vehicles. Transportation research procedia, 27:69–76, 2017. Farquhar, S. and Gal, Y . Towards robust evaluations of continual learning. arXiv preprint arXiv:1805.09733 , 2018.Online Learned Continual Compression with Adaptive Quantization Modules Fernando, C., Banarse, D., Blundell, C., Zwols, Y ., Ha, D., Rusu, A. A., Pritzel, A., and Wierstra, D. Pathnet: Evolu- tion channels gradient descent in super neural networks. arXiv preprint arXiv:1701.08734, 2017. Geiger, A., Lenz, P., Stiller, C., and Urtasun, R. Vision meets robotics: The kitti dataset. The International Journal of Robotics Research, pp. 0278364913491297, 2013. Gueguen, L., Sergeev, A., Kadlec, B., Liu, R., and Yosinski, J. Faster neural networks straight from jpeg. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 31 , pp. 3933–3944. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/ 7649-faster-neural-networks-straight-from-jpeg. pdf. Hu, W., Lin, Z., Liu, B., Tao, C., Tao, Z., Ma, J., Zhao, D., and Yan, R. Overcoming catastrophic forgetting for continual learning via model adaptation. 2018. Husz´ar, F. On quadratic penalties in elastic weight consoli- dation. arXiv preprint arXiv:1712.03847, 2017. Johnston, N., Vincent, D., Minnen, D., Covell, M., Singh, S., Chinen, T., Jin Hwang, S., Shor, J., and Toderici, G. Improved lossy image compression with priming and spatially adaptive bit rates for recurrent networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4385–4393, 2018. Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Des- jardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, pp. 201611835, 2017. Lesort, T., Caselles-Dupr´e, H., Garcia-Ortiz, M., Stoian, A., and Filliat, D. Generative models from the perspective of continual learning. arXiv preprint arXiv:1812.09111, 2018. Lesort, T., Gepperth, A., Stoian, A., and Filliat, D. Marginal replay vs conditional replay for continual learning. In International Conference on Artiﬁcial Neural Networks, pp. 466–480. Springer, 2019. Li, Z. and Hoiem, D. Learning without forgetting. IEEE Transactions on Pattern Analysis and Machine Intelli- gence, 40(12):2935–2947, 2018. Lin, L.-J. Reinforcement learning for robots using neu- ral networks. Technical report, Carnegie-Mellon Univ Pittsburgh PA School of Computer Science, 1993. Lopez-Paz, D. et al. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems, pp. 6467–6476, 2017. Mnih, V ., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. Mnih, V ., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje- land, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., and Hassabis, D. Human-level control through deep reinforcement learning. Nature, 2014. Nguyen, C. V ., Li, Y ., Bui, T. D., and Turner, R. E. Variational continual learning. arXiv preprint arXiv:1710.10628, 2017. Nøkland, A. and Eidnes, L. H. Training neural networks with local error signals. arXiv preprint arXiv:1901.06656, 2019. Oyallon, E., Belilovsky, E., Zagoruyko, S., and Valko, M. Compressing the input for cnns with the ﬁrst-order scatter- ing transform. In The European Conference on Computer Vision (ECCV), September 2018. Ramapuram, J., Gregorova, M., and Kalousis, A. Lifelong generative modeling. arXiv preprint arXiv:1705.09847, 2017. Razavi, A., Oord, A. v. d., and Vinyals, O. Generating diverse high-ﬁdelity images with vq-vae-2.arXiv preprint arXiv:1906.00446, 2019. Rebufﬁ, S.-A., Kolesnikov, A., Sperl, G., and Lampert, C. H. icarl: Incremental classiﬁer and representation learning. In Proc. CVPR, 2017. Riemer, M., Franceschini, M., and Klinger, T. Genera- tion and consolidation of recollections for efﬁcient deep lifelong learning. CoRR, abs/1711.06761, 2017. URL http://arxiv.org/abs/1711.06761. Riemer, M., Cases, I., Ajemian, R., Liu, M., Rish, I., Tu, Y ., and Tesauro, G. Learning to learn without forgetting by maximizing transfer and minimizing interference. arXiv preprint arXiv:1810.11910, 2018. Rolnick, D., Ahuja, A., Schwarz, J., Lillicrap, T. P., and Wayne, G. Experience replay for continual learning, 2018. Rusu, A. A., Rabinowitz, N. C., Desjardins, G., Soyer, H., Kirkpatrick, J., Kavukcuoglu, K., Pascanu, R., and Had- sell, R. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.Online Learned Continual Compression with Adaptive Quantization Modules Shin, H., Lee, J. K., Kim, J., and Kim, J. Continual learn- ing with deep generative replay. In Advances in Neural Information Processing Systems, pp. 2990–2999, 2017. Theis, L., Shi, W., Cunningham, A., and Husz ´ar, F. Lossy image compression with compressive autoencoders. arXiv preprint arXiv:1703.00395, 2017. Thrun, S. and Mitchell, T. M. Lifelong robot learning. Robotics and autonomous systems, 15(1-2):25–46, 1995. Torfason, R., Mentzer, F., ´Ag´ustsson, E., Tschannen, M., Timofte, R., and Gool, L. V . Towards image under- standing from deep compression without decoding. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum? id=HkXWCMbRW. Tu, C., Takeuchi, E., Carballo, A., and Takeda, K. Point cloud compression for 3d lidar sensor using recurrent neural network with residual blocks. In 2019 Interna- tional Conference on Robotics and Automation (ICRA), pp. 3274–3280. IEEE, 2019. van den Oord, A., Vinyals, O., et al. Neural discrete repre- sentation learning. In Advances in Neural Information Processing Systems, pp. 6306–6315, 2017. Zenke, F., Poole, B., and Ganguli, S. Continual learn- ing through synaptic intelligence. arXiv preprint arXiv:1703.04200, 2017. Zhai, M., Chen, L., Tung, F., He, J., Nawhal, M., and Mori, G. Lifelong gan: Continual learning for conditional im- age generation. ArXiv, abs/1907.10107, 2019.Online Learned Continual Compression with Adaptive Quantization Modules A. Buffer Memory Management We consider two settings to manage the samples inside the memory. In the ﬁrst setting, we do not perform any codebook freezing. Therefore, as the model trains, the amount of compressed representations AQM can store increases smoothly. In this setting, the current amount of samples stored by AQM is a good approximation of the model’s capacity. Therefore, we simply use this estimate instead of the total buffer size in the regular reservoir sampling scheme. The algorithm is presented in Alg 4. Algorithm 4: AddToMemory Input: Memory Mwith capacity C(bytes), sample x 1 Nreg = C BY TES(x) 2 capacity = max( Nreg, NUM SAMPLES (M) ) 3 %Probability of adding x 4 add ∼B( capacity SAMPLE AMT SEEN SO FAR ) %Bernoulli 5 if add then 6 hidx, blockid = ADAPTIVE COMPRESS (x, AE, dth) 7 while BITS(hidx) - FREE SPACE(M) >0 do 8 DELETE RANDOM (M) 9 end 10 end This is not the case when we perform codebook freezing. In the latter setting, consider the moment when the ﬁrst codebook is ﬁxed; suddenly, the amount of samples the model can store has increased by a factor equal to the compression rate of the ﬁrst block. Therefore, at this moment, the amount of samples currently stored by AQM is not a good approximation for the model’s capacity. Moreover, when performing codebook freezing, since the capacity suddenly spikes, we must decide between a) having an imbalance in the buffer, where certain temporal regions of the streams are not equally represented, or not utilising all available memory and storing less incoming samples so they are in similar quantities as previous samples. We opt for the former approach, and propose a procedure that allows the buffer to rebalance itself as new training data becomes available. We illustrate the procedure with an example. Consider an AQM where the distribution of samples in the buffer is the one plotted in Fig 10. Speciﬁcally, we show the number of samples stored for each minibatch processed by the model. In this example, very few (<20) samples are stored from the earliest part of the stream, while a much larger number comes from the more recent part of the stream. Assuming that the model is over its memory capacity, we need to remove samples until the memory requirement is met. Ideally, we would like to remove more samples from parts of the stream where samples are abundant. In order to do so, we use Kernel Density Estimation on the histogram in Fig 10. Doing so gives us the line labelled iter0 in Fig 10. We then sample points according to the distribution given by iter0, remove them, and ﬁt a new KDE with the remaining points (labelled iter1). In this example we repeat this procedure 10 times, until iter9. As we can see, the distribution of stored samples becomes closer to the uniform distribution, i.e. the setting where all parts of the streeam are equally represented. Therefore, in the setting where codebook freezing is performed, we ﬁrst add all incoming points to the buffer. Then, points are removed according to the procedure described above. This allows for maximal memory usage while ensuring that the buffer self balances over time. Note that we need to store the timestamp alongside each sample, which has a negligible cost when dealing with high-dimensional inputs. B. Further Details of Experiments We include here further details regarding the models used in our experiments. For all reported results, almost all hyperpa- rameters are kept the same. we set D the size of the embedding table equal to 100, we use a decay value of 0.6 for the Embedding EMA update, and the same architectural blocks. Across problems, we mainly vary the reconstruction threshold, as well how the blocks are stacked and their compression rates (by e.g. changing the number of codebooks per block).Online Learned Continual Compression with Adaptive Quantization Modules Figure 7.(left) histogram of samples in AQM where no buffer balancing was performed. (right) iterative buffer balancing procedure B.1. Cifar For CIFAR-10, we use a 1 block AQM, latent size (16 x 16 x 100) is quantized with (16 x 16 x 1) indices where the last index represents the number of codebooks. The codebook here contains 128 embeddings, giving a compression factor of 13.7×. Due to the simplistic nature of the task and low resolution of the images, AQM already yields good compression before the end of the ﬁrst task, hence adaptive compression and codebook freezing are not required. For the (Riemer et al., 2018) baseline, we ran a hyperparameter search to vary the compression size. Speciﬁcally, we ran a grid search for the number of categories per latent variable, as well as for the number of latent variables. We found the gumbel softmax much less stable during training and harder to cross-validate than vector quantization. Below we show an example of the image quality of our approach compared to (Riemer et al., 2018). We ran both AQM and (Riemer et al., 2018) on the split CIFAR-10 task, then extracted images which happened to be in the buffer of both methods. Figure 8.Bottom row: random buffer reconstructions using (Riemer et al., 2018). Middle row: random buffer reconstructions using SQM. Top row: corresponding original image. Columns are ordered w.r.t their entry time in the buffer, from oldest to newest. All samples above were obtained from the disjoint CIFAR-10 task, and are 12×smaller than their original image. B.2. Imagenet For the Ofﬂine 128 x 128 Imagenet experiment, we use the following three blocks to build AQM, with the following latent sizes : 1. (64 x 64 x 100) quantized using (64 x 64 x 1) indices, with a codebook of 16 vectors, giving a 24 ×compression. 2. (32 x 32 x 100) quantized using (32 x 32 x 1) indices, with a codebook of 256 vectors, giving a 48 ×compression. 3. (32 x 32 x 100) quantized using (32 x 32 x 1) indices, with a codebook of 32 vectors, giving a 76.8 ×compression. For the 2 block AQM, we searched over using blocks (1-2) (2-3) and (1-3). For 1 block AQM we simply tried all three blocks independently. When stacking two blocks with the same latent sizes (e.g. block 2 and 3) the encoder and decoder functions for the second block are the identity. In other words, the second block simply learns another embedding matrix.Online Learned Continual Compression with Adaptive Quantization Modules C. Drift Ablation Here we provide additional results for the drift ablation studied in 4. We repeat the same experiment for different values of the reconstruction threshold parameter, which controls when the codebook freezing occurs. The table below shows that the same conclusion holds across multiple values for this parameter: codebook freezing yields little to no drift, while only negligibly hindering the model’s ability to adapt to a distribution shift. It is worth noting that in cases of severe drift (e.g. with recon th = 7.5) the model diverges because it is rehearsing on samples of poor quality. In this setting, codebook freezing performs better on both the streaming MSE and the drift MSE. recon th Freezing Streaming MSE Drift MSE Streaming + Drift MSE 1 No 0.59 ±0.03 1.01 ±0.14 1.60 ±0.17 1 Yes 0.60 ±0.03 0.49 ±0.02 1.09±0.05 2.5 No 0.63 ±0.06 13.24 ±5.36 13.87 ±5.42 2.5 Yes 0.81 ±0.02 1.07 ±0.05 1.88±1.12 5 No 0.65 ±0.04 55.31 ±36.82 55.96 ±36.86 5 Yes 0.92 ±0.03 1.69 ±0.18 2.61±0.21 7.5 nan nan nan nan 7.5 Yes 0.98 ±0.11 2.10 ±0.28 3.08±0.39 Table 5.Here we provide additional results for the drift ablation discussion in section shown in Fig 4. For clarity all results are multiplied by 100. (e.g. recon th of 2.5 corresponds to 0.025). Results averaged over 5 runs. Figure 9.Visualization of results reported in Table C. We kept the scale of the y axis consistent across the four graphs for easy comparison.Online Learned Continual Compression with Adaptive Quantization Modules D. Atari Below are sample reconstructions used in the Atari experiments. For each game, reconstructions are in the ﬁrst row and the original uncompressed samples in the second row. Reconstructions are 16×smaller than the original RGB images. Figure 10.Pong Figure 11.Pitfall Figure 12.Ms PacmanOnline Learned Continual Compression with Adaptive Quantization Modules E. Lidar Samples Here we show reveral lidar compressions (left) and their original counterpart (right). The compression rate is 32 ×. We note that unlike RGB images, raw lidar scans are stored using ﬂoating points. We calculate the compression rate from the (smaller) polar projection instead of the 3 channel cartesian representation. Figure 13.Lidar reconstruction (left) vs original (right)Online Learned Continual Compression with Adaptive Quantization Modules Figure 14.Lidar reconstruction (left) vs original (right)",
      "meta_data": {
        "arxiv_id": "1911.08019v3",
        "authors": [
          "Lucas Caccia",
          "Eugene Belilovsky",
          "Massimo Caccia",
          "Joelle Pineau"
        ],
        "published_date": "2019-11-19T00:43:16Z",
        "pdf_url": "https://arxiv.org/pdf/1911.08019v3.pdf"
      }
    },
    {
      "title": "Towards Backward-Compatible Continual Learning of Image Compression",
      "abstract": "This paper explores the possibility of extending the capability of\npre-trained neural image compressors (e.g., adapting to new data or target\nbitrates) without breaking backward compatibility, the ability to decode\nbitstreams encoded by the original model. We refer to this problem as continual\nlearning of image compression. Our initial findings show that baseline\nsolutions, such as end-to-end fine-tuning, do not preserve the desired backward\ncompatibility. To tackle this, we propose a knowledge replay training strategy\nthat effectively addresses this issue. We also design a new model architecture\nthat enables more effective continual learning than existing baselines.\nExperiments are conducted for two scenarios: data-incremental learning and\nrate-incremental learning. The main conclusion of this paper is that neural\nimage compressors can be fine-tuned to achieve better performance (compared to\ntheir pre-trained version) on new data and rates without compromising backward\ncompatibility. Our code is available at\nhttps://gitlab.com/viper-purdue/continual-compression",
      "full_text": "Towards Backward-Compatible Continual Learning of Image Compression Zhihao Duan1 Ming Lu2 Justin Yang1 Jiangpeng He1† Zhan Ma2 Fengqing Zhu1 1 Purdue University, West Lafayette, Indiana, U.S.A. 2 Nanjing University, Nanjing, Jiangsu, China {duan90, yang1834, he416, zhu0}@purdue.edu, {minglu, mazhan}@nju.edu.cn Abstract This paper explores the possibility of extending the capa- bility of pre-trained neural image compressors (e.g., adapt- ing to new data or target bitrates) without breaking back- ward compatibility, the ability to decode bitstreams encoded by the original model. We refer to this problem as continual learning of image compression. Our initial findings show that baseline solutions, such as end-to-end fine-tuning, do not preserve the desired backward compatibility. To tackle this, we propose a knowledge replay training strategy that effectively addresses this issue. We also design a new model architecture that enables more effective continual learning than existing baselines. Experiments are conducted for two scenarios: data-incremental learning and rate-incremental learning. The main conclusion of this paper is that neural image compressors can be fine-tuned to achieve better per- formance (compared to their pre-trained version) on new data and rates without compromising backward compati- bility. Our code is available at this link. 1. Introduction Recent years have witnessed the rapid development of deep learning-based image compression. Most existing research in this field considers theoffline learning setting, i.e., once a neural network model is trained, its parameters are fixed and kept unchanged when deployed. However, real-world appli- cations are often complex and dynamic, and an ideal com- pressor should be capable of being incrementally learned and adapted to various scenarios. For example, consider an image storage application with a compressor pre-trained on natural images for certain predefined target bitrates. As new image sources ( e.g., microscopy, remote sensing, and human face images) are encountered, one may want to up- date the compressor to improve its performance on the new data and to support different target rates. This raises an in- teresting question: can neural network-based image com- †Corresponding author & Project lead Enc. Dec. New  Dec. Pre-trained model Fine-tuning New  Enc. New  Dec. Bits Backward compatibility (a) Continual learning of image compression Original  image Reconstruction,  before fine - tuning Reconstruction, after fine - tuning Vanilla fine -tuning With our method (b) The backward compatibility problem. Once a neural compressor (in this experiment, [41]) is fine-tuned, it can no longer decode the bitstream produced by its original version. Our method addresses this issue. Figure 1. The goal of this work is to fine-tune pre-trained learned image compressors with new data or new rates while preserving backward compatibility (Fig. 1a). Baseline methods are backward incompatible, while ours is effective (Fig. 1b). pressors be learned continually, and if so, would it bring performance benefits compared to the pre-trained model? One might assume that simply fine-tuning a pre-trained model would be sufficient. Yet, doing so disrupts the model’s backward compatibility (Fig. 1b), i.e., the ability to decode bitstreams produced by the original model, due to a mismatch between the encoder (pre-trained model) and decoder (fine-tuned model). Maintaining backward com- patibility is crucial, as failing to do so renders existing bit- streams in the storage (or sent from other devices) inac- cessible. It is worth noting that this backward compatibil- ity problem is different from the well-known problem of catastrophic forgetting in neural networks [28]. The unique properties of compression, including the sender-receiver re- lationship and the existence of entropy coding, set it apart from other image processing/vision tasks. Therefore, ex- isting continual learning methods for vision tasks [13, 51] cannot be applied as is, and new strategies must be devel- oped for compression to maintain the decoder’s backward arXiv:2402.18862v1  [eess.IV]  29 Feb 2024compatibility when adapting to new data and rates. To achieve backward compatibility, the most straight- forward way is to modify only the encoding process of a pre-trained model when adapting to new data or new rates [7, 19, 56]. This strategy resembles the common prac- tice in traditional codecs, where there is often a standardized decoder, but various encoders can be designed to accommo- date different applications. Despite its simplicity, keeping the decoder unchanged hinders the model’s ability to adapt to new data and rates, leading to suboptimal performance. This work shows that it is possible to continually train both the encoder and decoder of neural compressors while maintaining backward compatibility. We begin by noticing that as long as the entropy model of a compressor is kept unchanged, it can decode the old bitstreams and obtain the latent representations. Based on this observation, we pro- pose a knowledge replay training scheme that can be used to train the encoder and decoder networks without break- ing backward compatibility. We also design a model ar- chitecture where the entropy model consumes only a small amount of parameters, allowing most model parameters to be learnable in fine-tuning. We formulate two experimental scenarios: data-incremental learning and rate-incremental learning. Experimental results demonstrate that our pro- posed methods enable neural image compressors to obtain improved performance on new data and new rates without breaking backward compatibility. To summarize, our contributions are as follows: • We propose a knowledge replay-based training strategy that can be used to train neural image compressors incre- mentally without breaking backward compatibility; • We design a neural network architecture targeting effec- tive continual learning of image compression; • We formulate two continual learning scenarios for im- age compression: data-incremental learning and rate- incremental learning. Experimental results show that our method outperforms baseline approaches in both cases. 2. Background and Related Work 2.1. Learned lossy image compression (LIC) Most learning-based methods for lossy image compression can be interpreted using the entropy-constrained non-linear transform coding framework [4]. Let X ∼ pdata denote data samples with an underlying data distribution. In this frame- work, a neural network encoderfenc maps X to a latent vari- able Z ≜ fenc(X), and a neural network decoder fdec maps Z back to a reconstruction ˆX ≜ fdec(Z). A learned prob- ability distribution pZ, also known as the entropy model, is used to model the marginal distribution of Z. The learning objective is to minimize the rate-distortion (R-D) loss: min EX∼pdata h −log2 pZ(Z) + λ · d(X, ˆX) i , (1) where d is a distortion metric (e.g., mean squared error),λ is the Lagrange multiplier trading off between rate and distor- tion, and the minimization is over the network parameters of fenc, fdec, and pZ. This framework has also been extended to variable-rate compression [12, 14], where the encoder, decoder, and entropy model are conditioned on λ. During variable-rate training, the model parameters are optimized for various λ sampled from a distribution pΛ: min EX∼pdata,Λ∼pΛ h −log2 pZ(Z|Λ) + Λ· d(X, ˆX) i (2) where Z ≜ fenc(X; Λ), ˆX ≜ fdec(Z; Λ). (3) Existing research in LIC can be categorized into several groups. A major group focuses on designing expressive architectures for fenc and fdec, such as convolutional and transformer-based ones [10, 11, 14, 20, 23, 27, 33, 37–39, 43, 52, 54, 59]. Another line of research lies in designing the entropy model pZ, such as autoregressive [22, 40, 41, 44] and hierarchical models [3, 14, 15, 25]. Other research includes, e.g., quantization methods [17, 18, 21, 55, 57] and variable-rate compression methods [6, 9, 12, 31, 48, 53]. To our knowledge, none of these existing methods are designed to work with continual learning as in our work. The most related line of research to this paper iscontent- adaptive image compression, where the goal is to adapt a compressor to new images or new target rates in a per-image fashion. Solutions to this problem include encoder-side op- timization and decoder-side adaptation. Encoder-side opti- mization methods [7, 19, 56] directly optimize the R-D ob- jective w.r.t. Z during encoding. Decoder-side adaptation methods [42, 47, 50], on the other hand, include parameter- efficient neural network modules in the bitstream, which are executed on the decoder side to improve decoding. Among them, many methods require computationally expensive, it- erative optimization during encoding. The scope of this paper is distinct from content-adaptive image compression in several ways: (a) our goal is to in- crementally train the compressor parameters in-place with- out introducing additional parameters, and (b) as opposed to per-image optimization during testing, our method employs a one-time training procedure and induces no additional computational cost at test time. Our research is comple- mentary to content-adaptive image compression, and they can be combined to further improve the performance. 2.2. Knowledge replay in continual learning Continual learning has been widely studied for image clas- sification [13] and semantic segmentation [8], which aim to learn a sequence of new tasks incrementally without forgetting the previously learned knowledge. Among ex- isting methods for continual learning, replay-based meth- ods [34, 36, 45] have emerged as particularly effective,𝑓enc (0) 𝑓dec (0) Pre-trained model 𝑋test 0 𝑏test 0𝜆 ∈ [𝜆low 0 ,𝜆high (0) ] Fine-tuning 𝑓enc (1) 𝑓dec (1) 𝑋train 1 𝜆 ∈ [𝜆low 1 ,𝜆high (1) ] (a) Continual learning Goal 1: Backward compatibility 𝑏test 0 Goal 2: New data & rate performance 𝑓dec (1) 𝑓enc (1) 𝑓dec (1) 𝑋test 1 𝜆 ∈ [𝜆low 1 ,𝜆high (1) ] 𝑋test 0෠𝑋test 0 Distortion Rate Rate Distortion (b) Evaluation Figure 2. Problem definition. Fig. 2a shows the fine-tuning pro- cess of a pre-trained model, and Fig. 2b shows the two evaluation criteria: backward compatibility and new data/rate performance. Entropy models are kept frozen and omitted in the figures. which applies an additional memory buffer to store exem- plar data from learned tasks and then integrate with new task data to perform knowledge rehearsal during continual learn- ing. Our proposed knowledge replay method adopts a simi- lar principle. However, image compression contains unique challenges distinct from typical computer vision tasks, mak- ing existing continual learning strategies inapplicable to de- ploy in image compression. Thus, a tailored knowledge re- play strategy is required for our problem scenario. 3. Problem Description In this section, we formulate the problem (Sec. 3.1), discuss the backward compatibility issue (Sec. 3.2), and motivate the design of our method (Sec. 3.3). 3.1. Continual learning for compression Assume that we have a pre-trained variable-rate model with supported λ ∈ [λ(0) low, λ(0) high]. We use f(0) enc, f(0) dec, p(0) Z to de- note the pre-trained model’s encoder, decoder, and entropy model, respectively. Also, assume that we have used the pre-trained model to compress a test set of images X(0) test and obtained the corresponding bitstreams b(0) test, as illustrated in Fig. 2a (top). This situation well-silumates a typical image storage application with a learned image compressor. We now aim at fine-tuning the model with new dataX(1) train and a new rate range determined by [λ(1) low, λ(1) high]. Note that the new data and rate range may or may not be the same as the old ones. Similarly, let f(1) enc, f(1) dec, p(1) Z denote the new model components, as illustrated in Fig. 2a (bottom). We expect the new model to achieve two goals: 1. Backward compatibility: The new model should be ca- pable of decoding the bitstreams produced by the old model (Fig. 2b, top). 2. New-data (or new-rate) performance: The new model should perform better than the old one on new data and new rates (Fig. 2b, bottom). Layers𝑥 Layers Layers Layers 𝑧1𝑧2 𝑝𝑍1𝑝𝑍2|𝑍1 ො𝑥 𝑓enc 𝑓dec 𝑝𝑍 Figure 3. The Hyperprior model architecture [3], where the layers are categorized into three groups: fenc, fdec, and pZ. Number of parameters Total fenc fdec pZ M-S Hyp. [41] 17.6M 5.9M 2.5M 8.2M (47%) GMA [11] 26.6M 5.8M 11.7M 9.0M (34%) ELIC [23] 33.8M 9.7M 7.3M 16.7M (49%) STF [59] 99.9M 11.3M 7.1M 81.4M (81%) TCM [33] 45.2M 3.3M 6.8M 35.2M (78%) MLIC++ [26] 116.7M 6.3M 12.0M 98.4M (84%) Our model 35.5M 18.7M 11.9M 4.9M (14%) Table 1. Many existing methods employ a parameter-expensive entropy model. We propose an architecture with a lightweight en- tropy model, which makes continual learning more effective (since more parameters are learnable in the fine-tuning phase). 3.2. Backward compatibility of entropy decoding As mentioned in Fig. 1, fine-tuning the model end-to-end breaks the backward compatibility of the decoder. The pri- mary reason lies in entropy coding: range-based entropy coding algorithms [16, 46], which are commonly used in modern neural compressors, are known to be sensitive to the probability distribution of the encoded symbols. As the new entropy modelp(1) Z is different from the old onep(0) Z , the new model is not able to perform entropy coding correctly using the old bitstreams b(0) test, and thus the correct (quan- tized) latent variables Z(0) test cannot be obtained. In fact, re- cent research [2, 29, 49] has shown that even a small change (e.g., a floating point round-off error) in the entropy model can lead to failure in entropy decoding. 3.3. Freezing the entropy model during fine-tuning To avoid the aforementioned issue, the entropy model pZ needs to be kept unchanged throughout fine-tuning. Then, the latent variablesZ(0) test are guaranteed to be recovered loss- lessly from the old bitstreams, and the problem reduces to continually learning the decoder fdec without forgetting the old knowledge ( i.e., decoding Z(0) test). With our pro- posed training strategy (Sec. 4.1), we show that this can be achieved for many existing neural compressors. We also notice that the entropy model pZ is often the largest component in many existing compressors, most of which are based on the Hyperprior model [3, 41] (Fig. 3). We found that their entropy model takes up a significant portion (e.g., 84% for MLIC++[26]) of the model parame- ters, as shown in Tab. 1. Consequently, a large proportion of model parameters need to be fixed during continual learn-ing, which may impair the new-data (or new-rate) perfor- mance. Motivated by this, we design a model architecture (Sec. 4.2) that employs a lightweight entropy model (Tab. 1, last row), which makes continual learning more effective. 4. Method As just mentioned, our methods include two independent components: the knowledge replay-based training strategy (Sec. 4.1) and a neural network architecture specifically de- signed for continual learning (Sec. 4.2). We now present them sequentially in detail. 4.1. Continual learning with knowledge replay Following the notation introduced in Sec. 3.1, we denote a pre-trained model as f(0) enc , f(0) dec , pZ, which are trained on data X(0) train with λ ∈ [λ(0) low, λ(0) high]. We drop the superscript for pZ since it is kept frozen throughout fine-tuning. In- spired by the idea of knowledge rehearsal with exemplars in class-incremental learning methods [45], we use the old training data X(0) train and the old encoder f(0) enc to perform “knowledge replay” in the fine-tuning process. To this end, we store f(0) enc along with X(0) train within a dedicated memory buffer before the fine-tuning stage. Note that we do not pose restrictive assumptions on training resources and allow ac- cess to the entire training set X(0) train during fine-tuning. In the fine-tuning process, our knowledge replay-based training objective contains two terms. The first term, ℓnew, is the standard R-D loss for the new training dataX(1) train with the new λ value range [λ(1) low, λ(1) high]: ℓnew ≜ E h R(1) + Λ(1) · D(1) i , where (4) R(1) ≜ −log2 pZ(Z(1)|Λ(1)) (5) D(1) ≜ d(X(1) train, ˆX(1) train). (6) In (4), the expectation is w.r.t. X(1) train and Λ(1), where Λ(1) is a random variable with the support being [λ(1) low, λ(1) high]. Its probability density, p(1) Λ , controls how λ is sampled during training. Intuitively, minimizing ℓnew adapts the model pa- rameters to new data and new rates, but it is not sufficient to maintain backward compatibility. The other term in our loss function encourages back- ward compatibility of the model parameters through knowl- edge replay of the old data X(0) train and the old encoder f(0) enc . Specifically, we use f(0) enc to encode X(0) train, which gives the corresponding (quantized) latent variables Z(0) train. Then, the current decoder f(1) dec decodes Z(0) train, and the reconstruction ˆX(0) train is compared with X(0) train to compute the knowledge re- 𝑓enc (0) 𝜆 ∈ [𝜆low 0 ,𝜆high (0) ] 𝐷(0) ෠𝑋train 0 𝑓enc (1) 𝑓dec (1) 𝑋train 1 𝜆 ∈ [𝜆low 1 ,𝜆high (1) ] 𝑅(1) 𝐷(1) ෠𝑋train 1 𝑋train 0 Figure 4. Our knowledge replay-based training strategy contains two components: a distortion loss that encourages backward com- patibility (top, dashed lines), and the standard R-D loss for new data and new rates (bottom, solid lines). The entropy model pZ is kept frozen and is omitted in the figure. play loss function, ℓKR, defined as: ℓKR ≜ E h Λ(0) · D(0) i , where (7) D(0) ≜ d \u0010 X(0) train, f(1) dec \u0010 f(0) enc (X(0) train) \u0011\u0011 . (8) The expectation is w.r.t. X(0) train and Λ(0), where Λ(0) ∼ p(0) Λ controls how λ is sampled during knowledge replay. In our experiments, we choose p(0) Λ to be the same as the one used in pre-training. Note that there is no rate term in ℓKR since the replayed encoder f(0) enc is fixed, and thus the rate term is constant w.r.t. the model parameters being trained. By replaying the old data and the old encoder, the decoder net- work effectively retains backward compatibility, as shown in our experiments (Sec. 5.3). Fig. 4 illustrates our knowledge replay strategy for one training iteration. Our training objective is to minimize a weighted summation of the two terms, ℓnew and ℓKR, with a scalar hyperparameter α ∈ [0, 1]: min (1− α) · ℓnew + α · ℓKR. (9) The hyperparameter α controls the weight of replayed data in the training objective and can be tuned to achieve a de- sired trade-off between backward compatibility and new data/rate performance (shown in Sec. 5.4). Our knowl- edge replay strategy is general and can be applied to various model architectures, enabling backward-compatible contin- ual learning of those models (shown in Sec. 5.3). 4.2. Model architecture Since freezing the entropy model is necessary for backward compatibility, we propose a model architecture that em- ploys a lightweight entropy model by design so that most parameters in the model can stay learnable. An overview of the architecture is shown in Fig. 5. Our model is inspired by the hierarchical residual coding architecture [14, 18, 58], but we decouple the entropy model ( pZ) and the decoder(fdec) into two separate branches, resulting in a reduced en- tropy model size. We now describe each component of the model in detail. Encoding: The encoding process involves a bottom-up pass through the encoder fenc and a top-down pass through the entropy model pZ. Given an input image x, encod- ing begins with fenc, a neural network consisting of a se- quence of downsampling and residual layers that produces a hierarchy of image features (denoted as hi in the figure). Specifically, for an input image with 256 × 256 pixels, fenc produces N = 4 features with spatial dimentions 32 × 32, 16 × 16, 8 × 8, and 4 × 4. All layers are convolutional, so the spatial dimensions scale accordingly for images of dif- ferent sizes. Then, the entropy model pZ starts with a con- stant e0 and iteratively updates it using the features hi from fenc. In each stage, ei−1 is upsampled to the same spatial di- mension as hi and concatenated with hi. The concatenated features are then fed into a sequence of layers to produce zi, the latent variable (which will be entropy coded) for the i-th stage. zi is then aggregated with the upsampled ei−1 through a linear layer and addition operation, and the result is denoted as ei and passed to the next stage. Note that in each stage, the entropy model also estimates the probabil- ity distribution of zi given z<i (with z<i ≜ {z1, ..., zi−1}), which is used for entropy coding. Probabilistic model, quantization, and entropy cod- ing are performed in the same way as for the discretized Gaussian model in Hyperprior-based methods [3, 41]. As a brief recap, the entropy model predicts a mean µi and a scale σi for each latent variable zi, and the probability model for zi is a discretized Gaussian distribution: pi(zi) ≜ pZi|Z<i(zi | z<i) (10) = Z zi+0.5 zi−0.5 N(t; µi, σ2 i ) dt, (11) where the dependence on z<i is through µi and σi. Dur- ing testing, the residual between zi and µi is quantized to the nearest integer, and during training, quantization is sim- ulated by additive uniform noise. Each stage i produces a separate bitstream corresponding to zi (using the asym- metric numeral systems [16] algorithm), and all stages are executed sequentially for i = 1, ..., N. Decoding: Given encoded bitstreams, the decoding pro- cess mirrors the encoding process in reverse. Firstly, the entropy model is executed top-down to iteratively predict pZi|Z<i, based on which the bitstreams are entropy-decoded to obtain zi. Then, the decoder fdec is executed top-down to reconstruct the image. Starting with a constant r0, the de- coder iteratively updates ri using zi and ei in each stage, as shown in the right pane of Fig. 5. In each stage and the final layer, residual layers and upsampling layers are applied to restore the image to its original resolution. Down ↓ 𝑥 ො𝑥 Encoder 𝑓enc Decoder 𝑓decEntropy model 𝑝𝑍 Layers Down ↓ Layers Concat. Layers +𝑧𝑖 𝑝𝑍𝑖|𝑍<𝑖 Layers Up ↑ 𝑒𝑖 Up ↑ Layers Layers Concat. 𝑟𝑖 𝑟𝑖−1 𝑒𝑖 𝑧𝑖 𝑒𝑖−1 Proj. Repeat for 𝑖 = 1,2,…,𝑁 Layers Up ↑ ℎ𝑖 𝑒0 𝑟0 Figure 5. Our model adopts the hierarchical residual coding archi- tecture [14, 18, 58] but decouples the entropy model (pZ) and the decoder (fdec) into two branches. Up ↑ denotes upsampling, Down ↓ denotes down-sampling, and Proj. denotes a linear projection layer. Detailed layer configurations are in Appendix, Sec. 7 DW Conv Feature LayerNorm MLP (𝐻,𝑊,𝐶) Feature 𝜆 Embedding (𝐶,) (𝐶,) MLP +× + Figure 6. Each layer in our model is a ConvNeXt block [35] conditioned on λ through an affine transformation. In the figure, (H, W, C) denotes height, width, and channel dimensions. Rate-conditional network layers: Fig. 6 shows the de- tails of each layer ( i.e., residual block) in our model. Our model employs the ConvNeXt module [35] as the basic building blocks. To achieve variable-rate compression, we adopt the conditional convolution technique [12, 14], which applies an adaptive affine transformation to the convolu- tional layer output (after layer normalization) to control the rate based on the input λ. Variable-rate training: The (pre-)training objective is to minimize the standard R-D loss for variable rate com- pression (Eq. (2)), except that the rate term consists of the sum of the rates for all latent variables: min EX,Λ \" NX i=1 −log2 pi(Zi|Λ) + Λ· d(X, ˆX) # , (12) where X follows the training data distribution, and Λ fol- lows pΛ, a continuous probability distribution that con- trols the sampling strategy of λ during training. After the pre-training phase, we freeze pZ and apply our knowledge replay training strategy presented in Sec. 4.1 to achieve backward-compatible fine-tuning.5. Experiments Our experiments compare various fine-tuning strategies as well as various model architectures for continual learning of image compression. We begin by describing the setup (Sec. 5.1) and baseline methods (Sec. 5.2). Then, Sec. 5.3 presents the main experimental results for our proposed methods. Finally, we provide additional experiments to an- alyze the effectiveness of knowledge replay (Sec. 5.4) and our model architecture (Sec. 5.5). 5.1. Experiment setup We consider two continual learning scenarios in image com- pression: data-incremental learning and rate-incremental learning. In the former, pre-trained models are fine-tuned on a new dataset, and in the latter, models are fine-tuned with a larger rate range (either going higher or lower). De- tailed configurations are shown in Tab. 2, and the datasets and metrics used are listed below. Datasets: we use the COCO [32] dataset train2017 split for pre-training all models. The dataset contains 118,287 images with around 640 × 420 pixels. We randomly crop the images to 256 × 256 patches during training. For data- incremental learning, we adopt the CelebA-HQ dataset [30] at 256 × 256 pixels, a commonly-used human face image dataset for generative image modeling [24]. The dataset consists of 30,000 images, where 24,000 are for training, 3,000 for validation, and the remaining 3,000 for testing. Metrics: We use bits per pixel (bpp), peak signal-to- noise ratio (PSNR, computed for the RGB space), and BD- Rate [5] to measure compression performance, all of which are standard metrics for image compression. As described in Sec. 3, we evaluate each method for two objectives: • Backward compatibility: We use the fine-tuned model to decode b(0) test , the bitstreams encoded by the pre-trained model, to obtain reconstructions ˆX(0) test . The bpp is com- puted for b(0) test (which is a constant independent of fine- tuning strategies), and the PSNR is computed between the reconstructions ˆX(0) test and the original images X(0) test . • New data & rate performance: We compress the new data X(1) test with the new rates, determined by the λ value range [λ(1) low, λ(1) high], to obtain bpp and PSNR metrics. We report all results in terms of BD-Rate in the main paper (due to space constraints), and we provide the correspond- ing PSNR-bpp curves in the Appendix. 5.2. Methods in comparison In addition to our model, we choose Mean & Scale Hyper- prior [41] (MSH) and Gaussian Mixture & Attention [11] (GMA) as two base models for our experiments. These two are commonly used and representative models for learned image compression, and we believe the experimental con- clusions based on them can be generalized to other existing [λ(0) low , λ(0) high] X(0) train X(0) test Pre-training [32, 1024] COCO Kodak [λ(1) low , λ(1) high] X(1) train X(1) test Data-incremental [32, 1024] CelebA CelebA Rate-incremental (low → high) [32, 4096] COCO Kodak Rate-incremental (high → low) [4, 1024] COCO Kodak Table 2. Experiment configurations. We start with a pre-trained model (pre-training) and fine-tune it either with new data ( data- incremental) or new rates (rate-incremental). BD-Rate (%) w.r.t. VTM 22.0 ↓ Old bitstreams (Kodak) New data (CelebA) Avg. MSH-VR, pre-trained 26.7 17.1 21.9 MSH-VR w/ FT Enc. 26.7 14.8 20.8 MSH-VR w/ FT Enc. & Dec. 419.1 8.95 214.0 MSH-VR w/ KR (ours) 20.1 9.57 14.8 GMA-VR, pre-trained 4.43 -9.23 -2.40 GMA-VR w/ FT Enc. 4.43 -10.5 -3.04 GMA-VR w/ FT Enc. & Dec. 354.6 -17.3 168.7 GMA-VR w/ KR (ours) 4.33 -14.4 -5.04 Our model, pre-trained 1.87 -13.2 -5.67 Our model w/ FT Enc. 1.87 -14.6 -6.37 Our model w/ FT Enc. & Dec. 262.9 -19.0 122.0 Our model w/ KR 0.87 -16.6 -7.87 Table 3. Data-incremental learning (COCO → CelebA) results. PSNR-bpp curves are provided in Appendix, Fig. 8. models. Since variable-rate compression is required to per- form rate-incremental learning, we construct a variable-rate version for each of them and train it in the same way as for our model. The resulting models are referred to asMSH-VR and GMA-VR, respectively. Sec. 8 in the Appendix provides pre-training and fine-tuning hyperparameters, and Sec. 9 in the Appendix provides details on the variable-rate baselines compared to their fixed-rate models. We apply the following fine-tuning strategies for each model and compare their performance: • Pre-traiend model: Using the pre-trained model without fine-tuning with new data or rates is the simplest baseline. • Fine-tuning the encoder only (FT Enc.) : We fine-tune the encoder with new data while keeping other parameters frozen. Since the entropy model and the decoder are never changed, backward compatibility is guaranteed. • Fine-tuning both the encoder and decoder (FT Enc. & Dec.): We fine-tune all model parameters except for the entropy model parameters. Since the decoder changes, backward compatibility may be lost. • Our approach: knowledge replay (KR). We fine-tune the model’s encoder and decoder with the proposed knowledge replay (KR) strategy applied. 5.3. Experimental results Data-incremental learning. Tab. 3 show the results (pre- trained on COCO, fine-tuned on CelebA). We begin byKodak BD-Rate (%) w.r.t. VTM 22.0 ↓ Old bitstreams: bpp ≈ (0.1,0.9) New rate: bpp ≈ (0.1,1.6) Avg. MSH-VR, pre-trained 26.7 - - MSH-VR w/ FT Enc. & Dec. 282.2 23.4 152.80 MSH-VR w/ KR (ours) 19.6 17.3 18.45 GMA-VR, pre-trained 4.43 - - GMA-VR w/ FT Enc. & Dec. 64.1 4.56 34.33 GMA-VR w/ KR (ours) 3.39 2.34 2.87 Our model, pre-trained 1.87 - - Our model w/ FT Enc. & Dec. 17.7 1.45 9.58 Our model w/ KR 0.96 0.70 0.83 Table 4. Rate-incremental learning (low → high) results. PSNR- bpp curves are provided in Appendix, Fig. 9. Kodak BD-Rate (%) w.r.t. VTM 22.0 ↓ Old bitstreams: bpp ≈ (0.1,0.9) New rate: bpp ≈ (0.03,0.9) Avg. MSH-VR, pre-trained 26.7 - - MSH-VR w/ FT Enc. & Dec. 35.1 37.6 36.4 MSH-VR w/ KR (ours) 18.9 29.9 24.4 GMA-VR, pre-trained 4.43 - - GMA-VR w/ FT Enc. & Dec. 10.0 9.11 9.56 GMA-VR w/ KR (ours) 1.75 6.92 4.34 Our model, pre-trained 1.87 - - Our model w/ FT Enc. & Dec. 14.33 5.18 9.76 Our model w/ KR 0.86 4.26 2.56 Table 5. Rate-incremental learning (high → low) results. PSNR- bpp curves are provided in Appendix, Fig. 10. comparing the fine-tuning strategy for the MSH-VR model. Firstly, fine-tuning the encoder (FT Enc.) does not provide a significant improvement for new data BD-Rate (17.1% → 14.8%), which is expected since fine-tuning the encoder only reduces the amortization gap [56] and does not im- prove the model’s capacity. When fine-tuning both the en- coder and decoder (FT Enc. & Dec.), the new data BD-Rate is improved by a much larger margin (17.1%→ 8.95%), in- dicating the importance of updating the decoder. However, this came at the cost of backward compatibility, as shown by the significant increase in BD-Rate for old bitstreams (26.7% → 419.1%). This indicates that, while the decoder fits the new data well, it becomes incompatible with the old bitstreams. With our knowledge replay strategy ( MSH-VR w/ KR), we are able to achieve a competitive new data per- formance (9.57% BD-Rate) without sacrificing backward compatibility. Notably, fine-tuning with our strategy also improves the performance on old bitstreams, which is not the case for the other strategies. On average, the knowledge replay strategy clearly outperforms the other ones. These observations are consistent with the results forGMA-VR and our model, demonstrating the effectiveness of knowledge replay in data-incremental learning. Also, when comparing different model architectures, our model achieves the best performance overall in terms of all metrics. Rate-incremental learning . Tab. 4 presents the re- Fine-tuning BD-Rate w.r.t. VTM 22.0 Config. Data KR loss Old bits. New data Avg. 0 - - 1.87 -13.2 -5.67 1 CA 262.9 -19.0 121.9 2 CA + COCO 23 -16.9 3.05 3 CA ✓ 3.67 -16.3 -6.32 4 (ours) CA + COCO ✓ 0.87 -16.6 -7.87 Table 6. Ablative analysis of our knowledge replay-based training strategy for data-incremental learning (COCO→ CelebA). For the “data” column, CA denotes the CelebA dataset. sults for rate-incremental learning (from low rates to higher rates). In rate-incremental experiments, we omit the FT Enc. baseline, since fine-tuning the encoder alone cannot effectively extend the rate range of any considered mod- els (see Appendix, Sec. 10.2 for details). Starting with the MSH-VR model, we observe that fine-tuning both the en- coder and decoder ( FT Enc. & Dec. ) is able to extend the operational rate range of the model with a similar BD-Rate w.r.t. VTM for the new rates. However, the performance on old bitstreams is significantly degraded, similar to the obser- vation in data-incremental learning experiments. By apply- ing our knowledge replay strategy (MSH-VR w/ KR), in con- trast, the model is able to achieve a competitive BD-Rate for the new rates (17.3%) while maintaining backward compat- ibility. Again, the results for GMA-VR and our model show a similar pattern. Overall, our model with KR outperforms the baselines, validating its effectiveness in rate-incremental learning. For rate-incremental learning from high rates to low rates (Tab. 5), the above observations stay the same. 5.4. Experimental analysis: knowledge replay The effectiveness of the proposed knowledge replay strat- egy has already been verified in the previous experiments. We now provide additional experiments to answer the fol- lowing questions that aim to analyze the individual compo- nents of our knowledge replay strategy. What contributes to the backward compatibility? In data-incremental learning, there are two components in our knowledge replay strategy that may help backward compat- ibility: the replayed training data and the knowledge replay loss. To analyze the contribution of each component, we freeze the entropy model parameters of our model and fine- tune it with the two components separately. To analyze the contribution of each component, we start from “Our model w/ FT Enc. & Dec. ” and apply the two components one by one. Tab. 6 shows the results. Config. 0 is the pre- trained model, and Config. 1 is the “FT Enc. & Dec. ”base- line that does not retain backward compatibility. With re- played data ( Config. 2 ), backward compatibility is largely improved (262.9% → 23% BD-Rate) but is still worse than the pre-trained model. When the knowledge replay loss is applied (Config. 3 and Config. 4 ), the performance on old bitstreams becomes comparable to the pre-trained model.α 0.0 0.25 0.5 0.75 1.0 BD-rate: old bitstreams 262.9 1.40 0.87 0.72 0.63 BD-rate: new data -19.0 -16.8 -16.6 -16.3 -14.4 Avg. BD-rate 122.0 -7.7 -7.9 -7.8 -6.9 Table 7. Data-incremental learning (COCO→ CelebA) results for our model with varying α. The scalar α ∈ [0, 1] controls the ratio of replayed data in fine-tuning, where α = 0.0 means no replay, and α = 1.0 means no new data. BD-rate is w.r.t. VTM 22.0. Latency (in seconds) Params. Entropy Coding Network (CPU) Network (GPU) Enc. Dec. Enc. Dec. Enc. Dec. MSH + VR 19.2M 0.026 0.068 0.318 0.325 0.004 0.010 GMA + VR 33.4M 3.072 6.302 0.941 1.221 0.006 0.022 Ours 35.5M 0.046 0.052 0.474 0.393 0.026 0.011 *Hardware: Intel 10700K CPU (using four threads) and Nvidia Quadro 6000 GPU. *Latency is the average time to encode/decode a Kodak image (768 ×512 pixels), averaged over all 24 images. Time includes entropy coding. Table 8. Computational complexity of the model architectures used in our experiments. We conclude that both the replayed data and the loss func- tion contribute to backward compatibility, and the knowl- edge replay loss is more important among the two. What is the impact of α in the knowledge replay loss function (Eq. (9))? We train our model for data- incremental learning with varyingα, the scalar that controls the ratio of replayed data in each training iteration. Results are shown in Tab. 7. Firstly, it is clear that as α grows from 0 to 1, the performance on old bitstreams monotonically improves (i.e., the BD-Rate decreases). Recall that when α = 0 , no replay is performed, and the model is trained with only the new data; when α = 1, the model is trained with only the replayed data. The results are thus consis- tent with the intuition that more replay leads to better back- ward compatibility. When it comes to the new data perfor- mance, the trend is reversed: as α grows, the performance on new data monotonically degrades, which again matches the intuition. On average, the performance is comparable for α ∈ [0.25, 0.75]. We conclude that our approach is in- sensitive to the choice of α and can achieve a good trade- off between backward compatibility and new data perfor- mance. Our experiments choose α = 0.5 by default, but in practice, the choice of α can be treated as a hyperparameter and determined by the application requirements. 5.5. Experimental analysis: model architecture Computational complexity. Tab. 8 shows the computa- tional complexity of our model and the two baselines. The metrics include the number of parameters, entropy coding latency, and neural network forward pass latency. Except for a few exceptions (the parameter count of MSH-VR, the entropy coding latency of GMA-VR, and the GPU encoding latency of our model), all methods are mostly comparable in terms of computational complexity. Thus, we conclude that the performance improvement of our model is not due BD-Rate (%) w.r.t. VTM 22.0 ↓ Old bitstreams (Kodak) New data (CelebA) Avg. Sequential pZ and fdec 4.42 -12.8 -4.19 Sequential pZ and fdec w/ KR 3.18 -15.8 -6.31 Parallel pZ and fdec 1.87 -13.2 -5.67 Parallel pZ and fdec w/ KR 0.87 -16.6 -7.87 Table 9. Comparing architecture variants of our model in terms of data-incremental learning (COCO → CelebA). to the increase in computational complexity. Impact of decoupling the entropy model and the de- coder. Recall that in order to reduce the parameters of pZ, our architecture decouples pZ and fdec into two par- allel branches. To analyze the impact of doing so, we con- struct a variant of our model wherepZ and fdec are executed in sequential (like in Hyperprior-based models; see Fig. 3). For a fair comparison, the sequential version has the same number of latent feature channels and the same number of parameters as the parallel model, and training is performed in the same way as for all previous experiments. We show the data-incremental learning results in Tab. 9. We observe that our model (Parallel pZ and fdec w/ KR) achieves a bet- ter performance than the sequential version on both old bit- streams and new data, which verifies our design. 5.6. Discussion Our experiments show that neural image compressors can adapt to new data and rates in a backward-compatible man- ner by using the proposed training strategy. In addition to continual learning applications, this observation offers in- sights into related research such as the standardization of learned image compression. Despite recent attempts toward this goal [1, 39], it remains an open question about which part of a selected neural compressor needs to be standard- ized. Our findings suggest a possible direction: only the entropy model (instead of the entire model architecture and parameters) needs to be standardized, and other components (e.g., the decoder network) could be fine-tuned over time with backward-compatible training strategies. 6. Conclusion This paper presents two approaches: a knowledge replay- based training strategy and a neural network architecture, for continual learning of image compression. Our knowl- edge replay strategy enables existing compressors to adapt to new data and target rates while ensuring that previ- ously compressed bitstreams remain decodable. Through extensive experiments, we conclusively answer the ques- tion raised at the beginning of the paper: neural network- based image compressors can be learned continually in a backward-compatible manner, achieving improved perfor- mance on new data, new rates, and old bitstreams.Limitations and future work. Our work serves as a preliminary study on continual learning for image compres- sion. Despite its effectiveness, our knowledge replay strat- egy assumes unconstrained training resources, which may not be true in practice. Also, we focus on the decoder’s backward compatibility, while the same problem can be studied for the encoder. For future work, a possible direc- tion is to extend our two-step method (pre-training and fine- tuning) to multi-step continual learning scenarios. References [1] Jo ˜ao Ascenso, Elena Alshina, and Touradj Ebrahimi. The jpeg ai standard: Providing efficient human and machine visual data con- sumption. IEEE MultiMedia, 30(1):100–111, 2023. 8 [2] Johannes Ball ´e, Nick Johnston, and David Minnen. Integer networks for data compression with latent-variable models.International Con- ference on Learning Representations, 2018. 3 [3] J. Ball ´e, D. Minnen, S. Singh, S. Hwang, and N. Johnston. Vari- ational image compression with a scale hyperprior. International Conference on Learning Representations, 2018. 2, 3, 5 [4] J. Ball ´e, P. A. Chou, D. Minnen, S. Singh, N. Johnston, E. Agustsson, S. Hwang, and Toderici G. Nonlinear transform coding. IEEE Jour- nal of Selected Topics in Signal Processing , 15(2):339–353, 2021. 2 [5] Gisle Bjontegaard. Calculation of average psnr differences between rd-curves. Video Coding Experts Group - M33, 2001. 6 [6] Shilv Cai, Zhijun Zhang, Liqun Chen, Luxin Yan, Sheng Zhong, and Xu Zou. High-fidelity variable-rate image compression via invert- ible activation transformation. Proceedings of the ACM International Conference on Multimedia, pages 2021–2031, 2022. 2 [7] Joaquim Campos, Simon Meierhans, Abdelaziz Djelouah, and Christopher Schroers. Content adaptive optimization for neural im- age compression. Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition Workshops, 2019. 2 [8] Fabio Cermelli, Massimiliano Mancini, Samuel Rota Bulo, Elisa Ricci, and Barbara Caputo. Modeling the background for incremen- tal learning in semantic segmentation. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9233–9242, 2020. 2 [9] Tong Chen and Zhan Ma. Variable bitrate image compression with quality scaling factors. IEEE International Conference on Acoustics, Speech and Signal Processing, pages 2163–2167, 2020. 2 [10] T. Chen, H. Liu, Z. Ma, Q. Shen, X. Cao, and Y . Wang. End-to-end learnt image compression via non-local attention optimization and improved context modeling. IEEE Transactions on Image Process- ing, 30:3179–3191, 2021. 2 [11] Z. Cheng, H. Sun, M. Takeuchi, and J. Katto. Learned image com- pression with discretized gaussian mixture likelihoods and attention modules. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7936–7945, 2020. 2, 3, 6, 4 [12] Yoojin Choi, Mostafa El-Khamy, and Jungwon Lee. Variable rate deep image compression with a conditional autoencoder. Proceed- ings of the IEEE/CVF International Conference on Computer Vision, pages 3146–3154, 2019. 2, 5 [13] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ale ˇs Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44 (7):3366–3385, 2022. 1, 2 [14] Zhihao Duan, Ming Lu, Jack Ma, Yuning Huang, Zhan Ma, and Fengqing Zhu. Qarv: Quantization-aware resnet vae for lossy image compression. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 1–15, 2023. 2, 4, 5 [15] Zhihao Duan, Ming Lu, Zhan Ma, and Fengqing Zhu. Lossy image compression with quantized hierarchical vaes. Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision , pages 198–207, 2023. 2 [16] Jarek Duda, Khalid Tahboub, Neeraj J. Gadgil, and Edward J. Delp. The use of asymmetric numeral systems as an accurate replacement for huffman coding. Picture Coding Symposium, pages 65–69, 2015. 3, 5 [17] Alaaeldin El-Nouby, Matthew J. Muckley, Karen Ullrich, Ivan Laptev, Jakob Verbeek, and Herve Jegou. Image compression with product quantized masked image modeling. Transactions on Ma- chine Learning Research, 2023. 2 [18] Runsen Feng, Zongyu Guo, Weiping Li, and Zhibo Chen. Nvtc: Nonlinear vector transform coding. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 6101–6110, 2023. 2, 4, 5 [19] Chenjian Gao, Tongda Xu, Dailan He, Yan Wang, and Hongwei Qin. Flexible neural image compression via code editing. Advances in Neural Information Processing Systems, 35:12184–12196, 2022. 2 [20] G. Gao, P. You, R. Pan, S. Han, Y . Zhang, Y . Dai, and H. Lee. Neural image compression via attentional multi-scale back projection and frequency decomposition. Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision , pages 14677–14686, 2021. 2 [21] Zongyu Guo, Zhizheng Zhang, Runsen Feng, and Zhibo Chen. Soft then hard: Rethinking the quantization in neural image compression. Proceedings of the International Conference on Machine Learning , 139:3920–3929, 2021. 2 [22] Dailan He, Yaoyan Zheng, Baocheng Sun, Yan Wang, and Hongwei Qin. Checkerboard context model for efficient learned image com- pression. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14766–14775, 2021. 2 [23] Dailan He, Ziming Yang, Weikun Peng, Rui Ma, Hongwei Qin, and Yan Wang. Elic: Efficient learned image compression with unevenly grouped space-channel contextual adaptive coding. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog- nition, pages 5708–5717, 2022. 2, 3 [24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840–6851, 2020. 6 [25] Yueyu Hu, Wenhan Yang, Zhan Ma, and Jiaying Liu. Learning end-to-end lossy image compression: A benchmark. IEEE Trans- actions on Pattern Analysis and Machine Intelligence , 44(8):4194– 4211, 2022. 2 [26] Wei Jiang and Ronggang Wang. MLIC$ˆ {++}$: Linear complex- ity multi-reference entropy modeling for learned image compression. ICML Workshop Neural Compression: From Information Theory to Applications, 2023. 3 [27] Wei Jiang, Jiayu Yang, Yongqi Zhai, Peirong Ning, Feng Gao, and Ronggang Wang. Mlic: Multi-reference entropy model for learned image compression. Proceedings of the ACM International Confer- ence on Multimedia, pages 7618–7627, 2023. 2 [28] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcom- ing catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521–3526, 2017. 1 [29] Esin Koyuncu, Timofey Solovyev, Elena Alshina, and Andr ´e Kaup. Device interoperability for learned image compression with weights and activations quantization.Picture Coding Symposium, pages 151– 155, 2022. 3 [30] Cheng-Han Lee, Ziwei Liu, Lingyun Wu, and Ping Luo. Maskgan: Towards diverse and interactive facial image manipulation.Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5548–5557, 2020. 6, 1[31] Jooyoung Lee, Seyoon Jeong, and Munchurl Kim. Selective com- pression learning of latent representations for variable-rate image compression. Advances in Neural Information Processing Systems , 35:13146–13157, 2022. 2 [32] T. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll´ar, and C. L. Zitnick. Microsoft coco: Common objects in con- text. Proceedings of the European Conference on Computer Vision, pages 740–755, 2014. 6, 1 [33] Jinming Liu, Heming Sun, and Jiro Katto. Learned image compres- sion with mixed transformer-cnn architectures. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14388–14397, 2023. 2, 3 [34] Yaoyao Liu, Yuting Su, An-An Liu, Bernt Schiele, and Qianru Sun. Mnemonics training: Multi-class incremental learning without for- getting. Proceedings of the IEEE/CVF conference on Computer Vi- sion and Pattern Recognition, pages 12245–12254, 2020. 2 [35] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11966–11976, 2022. 5, 2 [36] David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. Advances in neural information pro- cessing systems, 30, 2017. 2 [37] Ming Lu and Zhan Ma. High-efficiency lossy image coding through adaptive neighborhood information aggregation. arXiv preprint arXiv:2204.11448, 2022. 2 [38] Ming Lu, Peiyao Guo, Huiqing Shi, Chuntong Cao, and Zhan Ma. Transformer-based image compression. Data Compression Confer- ence, pages 469–469, 2022. [39] Haichuan Ma, Dong Liu, Ning Yan, Houqiang Li, and Feng Wu. End-to-end optimized versatile image compression with wavelet-like transform. IEEE Transactions on Pattern Analysis and Machine In- telligence, 44(3):1247–1263, 2022. 2, 8 [40] D. Minnen and S. Singh. Channel-wise autoregressive entropy mod- els for learned image compression. Proceedings of the IEEE Inter- national Conference on Image Processing, pages 3339–3343, 2020. 2 [41] D. Minnen, J. Ball ´e, and G. Toderici. Joint autoregressive and hier- archical priors for learned image compression. Advances in Neural Information Processing Systems, 31:10794–10803, 2018. 1, 2, 3, 5, 6, 4 [42] Guanbo Pan, Guo Lu, Zhihao Hu, and Dong Xu. Content adaptive latents and decoder for neural image compression.Proceedings of the European Conference on Computer Vision, pages 556–573, 2022. 2 [43] Yichen Qian, Zhiyu Tan, Xiuyu Sun, Ming Lin, Dongyang Li, Zhen- hong Sun, Li Hao, and Rong Jin. Learning accurate entropy model with global reference for image compression. International Confer- ence on Learning Representations, 2021. 2 [44] Yichen Qian, Xiuyu Sun, Ming Lin, Zhiyu Tan, and Rong Jin. Entro- former: A transformer-based entropy model for learned image com- pression. International Conference on Learning Representations , 2022. 2 [45] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H. Lampert. icarl: Incremental classifier and represen- tation learning. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5533–5542, 2017. 2, 4 [46] J. Rissanen and G. G. Langdon. Arithmetic coding. IBM Journal of Research and Development, 23(2):149–162, 1979. 3 [47] Sheng Shen, Huanjing Yue, and Jingyu Yang. Dec-adapter: Explor- ing efficient decoder-side adapter for bridging screen content and nat- ural image compression. Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12887–12896, 2023. 2 [48] Myungseo Song, Jinyoung Choi, and Bohyung Han. Variable-rate deep image compression through spatially-adaptive feature trans- form. Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2360–2369, 2021. 2 [49] Kuan Tian, Yonghang Guan, Jinxi Xiang, Jun Zhang, Xiao Han, and Wei Yang. Effortless cross-platform video codec: A codebook-based method. arXiv preprint arXiv:2310.10292, 2023. 3 [50] Koki Tsubota, Hiroaki Akutsu, and Kiyoharu Aizawa. Univer- sal deep image compression via content-adaptive optimization with adapters. IEEE/CVF Winter Conference on Applications of Com- puter Vision, pages 2528–2537, 2023. 2 [51] Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. A compre- hensive survey of continual learning: Theory, method and applica- tion. arXiv preprint arXiv:2302.00487, 2023. 1 [52] Yueqi Xie, Ka Leong Cheng, and Qifeng Chen. Enhanced invertible encoding for learned image compression. Proceedings of the ACM International Conference on Multimedia, pages 162–170, 2021. 2 [53] Fei Yang, Luis Herranz, Yongmei Cheng, and Mikhail G. Moze- rov. Slimmable compressive autoencoders for practical neural image compression. Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition, pages 4996–5005, 2021. 2 [54] Yibo Yang and Stephan Mandt. Computationally-efficient neu- ral image compression with shallow decoders. Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 530–540, 2023. 2 [55] Yibo Yang, Robert Bamler, and Stephan Mandt. Variational Bayesian quantization. Proceedings of the International Conference on Ma- chine Learning, 119:10670–10680, 2020. 2 [56] Yibo Yang, Robert Bamler, and Stephan Mandt. Improving infer- ence for neural image compression. Advances in Neural Information Processing Systems, 33:573–584, 2020. 2, 7 [57] Xi Zhang and Xiaolin Wu. Lvqac: Lattice vector quantization cou- pled with spatially adaptive companding for efficient learned image compression. Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition, pages 10239–10248, 2023. 2 [58] Xiaosu Zhu, Jingkuan Song, Lianli Gao, Feng Zheng, and Heng Tao Shen. Unified multivariate gaussian mixture for efficient neural im- age compression. Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition, pages 17591–17600, 2022. 4, 5 [59] Renjie Zou, Chunfeng Song, and Zhaoxiang Zhang. The devil is in the details: Window-based attention for image compression. Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition, pages 17471–17480, 2022. 2, 3Towards Backward-Compatible Continual Learning of Image Compression Supplementary Material 7. Appendix: Model Architecture Details In the main paper, Sec. 4.2 provides a high-level overview of the proposed model architecture. This section provides more details about the model architecture, such as the num- ber of channels and stride sizes for each layer. The detailed model architecture is shown in Fig. 7, where the model components are marked in the same way as in Sec. 4.2. Our model contains four phases, all of which have the same structure, while only different in (1) the number of feature channels, (2) the number of ConvNeXt blocks, and (3) the first phase starts from bias e0 and r0 instead of the feature maps from the previous phase. The spatial dimensions (height and width) in the figure are for an input image with 256 × 256 pixels. Since the model is fully convolutional, the spatial dimensions of in- termediate layer outputs scales accordingly with the input image size. Both initial bias features e0 and r0 have a shape of 1 × 1 × 128, and they are repeated spatially to match the spatial dimensions of z1. 8. Appendix: Training and Fine-tuning Details Tab. 10 lists the pre-training and fine-tuning hyperparame- ters used in our experiments. For a fair comparison, we use the same hyperparameters for training all models, including our proposed model and the baseline models ( i.e., MSH- VR and GMA-VR). Note that the fine-tuning dataset varies for different sets of experiments. For data-incremental learning, we use CelebA-HQ [30], and for rate-incremental learning, we use COCO [32], which is the same as the pre- training dataset. Pre-training Fine-tuning Data augmentation Crop, h-flip Crop, h-flip Input size 256x256 256x256 Optimizer Adam Adam Learning rate 2 × 10−4 1 × 10−4 LR schedule Constant + cosine Cosine Weight decay 0.0 0.0 Batch size 32 32 # iterations 500K 100K # images seen 16M 3.2M Gradient clip 2.0 2.0 EMA 0.9999 - GPU 1 × RTX 3090 1 × A40 Time ≈ 51 hours ≈ 11 hours Table 10. Training Hyperparameters. The GPU time is for training our proposed model, and all other hyperparameters are the same for all models. 9. Appendix: Variable-Rate Baseline Models In the main paper (Sec. 5.2), we mentioned that we con- struct variable-rate versions of the two baseline models (i.e., MSH-VR and GMA-VR) in order to use them in the rate- incremental learning experiment. Fig. 11 shows the rate- distortion performance of the variable-rate versions com- pared to the original ones. As shown in the figure, the variable-rate versions achieve similar performance as the original ones, which validates the our experimental setting. 10. Appendix: Experimental Results 10.1. PSNR-Bpp curves for the main experiments Due to the space constraint, we show only BD-rate results without PSNR-bpp curves in the main paper. This section provides the PSNR-bpp curves for the main experiments (Sec. 5.3). Fig. 8 shows the PSNR-bpp curves for data-incremental learning experiments, which includes the backward compat- ibility experiment (Fig. 8a) and the new-data performance experiment (Fig. 8b). For backward compatibility, it is clear that models with fine-tuned encoder and decoder suffer a significant performance drop on the old bitstreams, while other fine-tuned models obtain comparable performance as the pre-trained models. Among them, our proposed knowl- edge replay strategy achieves even better performance than using the pre-trained model directly. For new-data perfor- mance, our method achieves comparable performance as the models with fine-tuned encoder and decoder (which are not backward compatible), and outperforms the pre-trained models by a clear margin. These observations are consis- tent with what we have observed in the BD-rate results in the main paper. We show the PSNR-bpp curves for rate-incremental learning experiments, including the low-to-high experiment (Fig. 9) and the high-to-low experiment (Fig. 10). The re- sults are consistent with previous observations: (1) Fine- tuning the encoder and decoder does not preserve back- ward compatibility, while our approach does; and (2) Our approach even outperforms all other methods in terms of new-rate performance. 10.2. Fine-tuning the encoder does not generalize the model to new rates We mentioned in Sec. 5.3 that fine-tuning the encoder alone cannot effectively extend the rate range of the pre-trained models. We provide an example for showing this in Fig. 12, where we show the rate-incremental learning (low → high)Down 4x ↓ 256 × 256 3 CNX Down 2x ↓ CNX Concat. CNX + CNX CNX 𝑒4 Up 4x ↑ CNX Linear ×6 64 × 64 128 32 × 32 256 ×6 CNXCNX 32 × 32 32 Down 2x ↓ CNX Concat. CNX + 𝑒3 Linear 𝑧3 16 × 16 384 ×6 CNXCNX 16 × 16 256 Down 2x ↓ CNX Concat. CNX + 𝑒2 Linear 𝑧2 8 × 8 256 ×4 CNXCNX 8 × 8 128 𝑒1 Down 2x ↓ CNX Concat. CNX + Linear 𝑧1 4 × 4 128 ×4 CNXCNX 4 × 4 128 4 × 4 128 𝑝𝑍4|𝑍<4 𝑧4 𝑝𝑍3|𝑍<3 𝑝𝑍2|𝑍1 𝑝𝑍1 Up 2x ↑ CNX CNX Up 2x ↑ CNX CNX Up 2x ↑ 4 × 4 128 8 × 8 256 16 × 16 384 32 × 32 256 CNX Concat. 𝑟2 CNX Up 2x ↑ Linear ×2 𝑒2 𝑧2 ×2 CNX Concat. 𝑟1 Linear ×2 𝑒1 𝑧1 4 × 4 128 𝑒0  𝑟0  4 × 4 128 8 × 8 256 8 × 8 256 CNX Concat. 𝑟3 CNX Up 2x ↑ Linear ×3 𝑒3 𝑧3 ×3 16 × 16 384 16 × 16 384 CNX Concat. 𝑟4 CNX Up 2x ↑ Linear ×3 𝑒4 𝑧4 ×3 32 × 32 256 32 × 32 256 Up 2x ↑ ×6 64 × 64 128 256 × 256 3 Phase 4 Phase 3 Phase 2 Phase 1 Encoder 𝑓enc Decoder 𝑓dec Entropy model 𝑝𝑍 Figure 7. Detailed architecture of the proposed model. In the figure, CNX denotes a ConvNeXt block [35] conditioned on lagrange multiplier λ, as described in Fig. 6. Dimensionality of the layer outputs are shown in the format ofheight × width and channels, where the spatial dimensions (height and width) are for a 256 × 256 input image, and they scales linearly with the input image size. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Bits per pixel (bpp) 24 26 28 30 32 34 36 38RGB PSNR Kodak MSH-VR, pre-trained MSH-VR w/ FT Enc. MSH-VR w/ FT Enc. & Dec. MSH-VR w/ KR (ours) 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Bits per pixel (bpp) 24 26 28 30 32 34 36 38RGB PSNR Kodak GMA-VR, pre-trained GMA-VR w/ FT Enc. GMA-VR w/ FT Enc. & Dec. GMA-VR w/ KR (ours) 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Bits per pixel (bpp) 24 26 28 30 32 34 36 38RGB PSNR Kodak Our model, pre-trained Our model w/ FT Enc. Our model w/ FT Enc. & Dec. Our model w/ KR (a) Performance on the old bitsreams of Kodak (backward compatibility). Each subfigure shows the performance of a different model. 0.1 0.2 0.3 0.4 0.5 0.6 Bits per pixel (bpp) 30 32 34 36 38RGB PSNR CelebA-HQ, 256x256 MSH-VR, pre-trained MSH-VR w/ FT Enc. MSH-VR w/ FT Enc. & Dec. MSH-VR w/ KR (ours) 0.1 0.2 0.3 0.4 0.5 0.6 Bits per pixel (bpp) 30 32 34 36 38RGB PSNR CelebA-HQ, 256x256 GMA-VR, pre-trained GMA-VR w/ FT Enc. GMA-VR w/ FT Enc. & Dec. GMA-VR w/ KR (ours) 0.1 0.2 0.3 0.4 0.5 0.6 Bits per pixel (bpp) 30 32 34 36 38RGB PSNR CelebA-HQ, 256x256 Our model, pre-trained Our model w/ FT Enc. Our model w/ FT Enc. & Dec. Our model w/ KR (b) Performance on CelebA-HQ (new-data performance). Each subfigure shows the performance of a different model. Figure 8. PSNR-Bpp curves for data-incremental learning experiments. In figure (a), the “models, pre-trained” curves overlap with the “models w/ FT Enc. ”curves because their decoder are the same.0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Bits per pixel (bpp) 26 28 30 32 34 36 38RGB PSNR Kodak MSH-VR, pre-trained MSH-VR w/ FT Enc. & Dec. MSH-VR w/ KR (ours) 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Bits per pixel (bpp) 28 30 32 34 36 38RGB PSNR Kodak GMA-VR, pre-trained GMA-VR w/ FT Enc. & Dec. GMA-VR w/ KR (ours) 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Bits per pixel (bpp) 28 30 32 34 36 38RGB PSNR Kodak Our model, pre-trained Our model w/ FT Enc. & Dec. Our model w/ KR (ours) (a) Backward compatibility (bpp range is around [0.1, 0.9]). Each subfigure shows the performance of a different model. 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 Bits per pixel (bpp) 28 30 32 34 36 38 40 42RGB PSNR Kodak MSH-VR, pre-trained MSH-VR w/ FT Enc. & Dec. MSH-VR w/ KR (ours) 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 Bits per pixel (bpp) 28 30 32 34 36 38 40 42RGB PSNR Kodak GMA-VR, pre-trained GMA-VR w/ FT Enc. & Dec. GMA-VR w/ KR (ours) 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 Bits per pixel (bpp) 28 30 32 34 36 38 40 42RGB PSNR Kodak Our model, pre-trained Our model w/ FT Enc. & Dec. Our model w/ KR (ours) (b) New-rate performance (bpp range is around [0.1, 1.6]). Each subfigure shows the performance of a different model. Figure 9. PSNR-Bpp curves for rate-incremental learning (low → high) experiments. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Bits per pixel (bpp) 28 30 32 34 36 38RGB PSNR Kodak MSH-VR, pre-trained MSH-VR w/ FT Enc. & Dec. MSH-VR w/ KR (ours) 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Bits per pixel (bpp) 28 30 32 34 36 38RGB PSNR Kodak GMA-VR, pre-trained GMA-VR w/ FT Enc. & Dec. GMA-VR w/ KR (ours) 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Bits per pixel (bpp) 28 30 32 34 36 38RGB PSNR Kodak Our model, pre-trained Our model w/ FT Enc. & Dec. Our model w/ KR (a) Backward compatibility (bpp range is around [0.1, 0.9]). Each subfigure shows the performance of a different model. 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Bits per pixel (bpp) 24 26 28 30 32 34 36 38RGB PSNR Kodak MSH-VR, pre-trained MSH-VR w/ FT Enc. & Dec. MSH-VR w/ KR (ours) 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Bits per pixel (bpp) 24 26 28 30 32 34 36 38RGB PSNR Kodak GMA-VR, pre-trained GMA-VR w/ FT Enc. & Dec. GMA-VR w/ KR (ours) 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Bits per pixel (bpp) 24 26 28 30 32 34 36 38RGB PSNR Kodak Our model, pre-trained Our model w/ FT Enc. & Dec. Our model w/ KR (b) New-rate performance (bpp range is around [0.03, 0.9]). Each subfigure shows the performance of a different model. Figure 10. PSNR-Bpp curves for rate-incremental learning (high → low) experiments.0.2 0.4 0.6 0.8 1.0 1.2 Bits per pixel (bpp) 28 30 32 34 36 38RGB PSNR Kodak GMA-VR GMA (Cheng et al., 2020) MSH-VR MSH (Ballé et al., 2018) Figure 11. The variable-rate version of the baseline models that we constructed (MSH-VR and GMA-VR) are comparable to the original ones (MSH [41] and GMA [11]) in terms of PNSR-bpp performance on Kodak. 0.10.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.81.9 Bits per pixel (bpp) 28 30 32 34 36 38 40 42RGB PSNR Kodak MSH-VR, pre-trained MSH-VR w/ FT Enc. MSH-VR w/ FT Enc. & Dec. Figure 12. Fine-tuning the encoder does not effectively generalize the pre-train model (MSG-VR, for example) to new rates. performance of the pre-trained MSH-VR, the one with fine- tuned encoder (MSH-VR w/ FT Enc.), and the one with fine- tuned encoder and decoder (MSH-VR w/ FT Enc. & Dec. ). As shown in the figure, fine-tuning the encoder marginally extends the rate range of the pre-trained model, and the PSNR drops visibly when the rate is higher than maximum rate of the pre-trained model. Thus, we do not use this strat- egy in our rate-incremental learning experiments.",
      "meta_data": {
        "arxiv_id": "2402.18862v1",
        "authors": [
          "Zhihao Duan",
          "Ming Lu",
          "Justin Yang",
          "Jiangpeng He",
          "Zhan Ma",
          "Fengqing Zhu"
        ],
        "published_date": "2024-02-29T05:25:04Z",
        "pdf_url": "https://arxiv.org/pdf/2402.18862v1.pdf"
      }
    },
    {
      "title": "Forget-free Continual Learning with Winning Subnetworks"
    },
    {
      "title": "Forget-free Continual Learning with Winning Subnetworks"
    },
    {
      "title": "Impossibility Results for Grammar-Compressed Linear Algebra",
      "abstract": "To handle vast amounts of data, it is natural and popular to compress vectors\nand matrices. When we compress a vector from size $N$ down to size $n \\ll N$,\nit certainly makes it easier to store and transmit efficiently, but does it\nalso make it easier to process?\n  In this paper we consider lossless compression schemes, and ask if we can run\nour computations on the compressed data as efficiently as if the original data\nwas that small. That is, if an operation has time complexity\n$T(\\rm{inputsize})$, can we perform it on the compressed representation in time\n$T(n)$ rather than $T(N)$? We consider the most basic linear algebra\noperations: inner product, matrix-vector multiplication, and matrix\nmultiplication. In particular, given two compressed vectors, can we compute\ntheir inner product in time $O(n)$? Or perhaps we must decompress first and\nthen multiply, spending $\\Omega(N)$ time?\n  The answer depends on the compression scheme. While for simple ones such as\nRun-Length-Encoding (RLE) the inner product can be done in $O(n)$ time, we\nprove that this is impossible for compressions from a richer class: essentially\n$n^2$ or even larger runtimes are needed in the worst case (under complexity\nassumptions). This is the class of grammar-compressions containing most popular\nmethods such as the Lempel-Ziv family. These schemes are more compressing than\nthe simple RLE, but alas, we prove that performing computations on them is much\nharder.",
      "full_text": "arXiv:2010.14181v1  [cs.CC]  27 Oct 2020 Impossibility Results for Grammar-Compressed Linear Algebra Amir Abboud∗ Arturs Backurs† Karl Bringmann‡ Marvin K¨ unnemann§ Abstract To handle vast amounts of data, it is natural and popular to co mpress vectors and matrices. When we compress a vector from size N down to size n ! N, it certainly makes it easier to store and transmit eﬃciently, but does it also make it easier to process? In this paper we consider lossless compression schemes, and ask if we can run our computations on the compressed data as eﬃciently as if the original data wa s that small. That is, if an operation has time complexity T pinput-sizeq, can we perform it on the compressed representation in time T pnq rather than T pNq? We consider the most basic linear algebra operations: inne r product, matrix-vector multiplication, and matrix multiplication. In particular , given two compressed vectors, can we compute their inner product in time Opnq? Or perhaps we must decompress ﬁrst and then multiply, spend ing Ω pNq time? The answer depends on the compression scheme. While for simp le ones such as Run-Length-Encoding (RLE) the inner product can be done in Opnq time, we prove that this is impossible for compressions from a richer class: essentially n2 or even larger runtimes are needed in the worst case (under co mplexity assumptions). This is the class of grammar-compressions containing most popular methods such as the Lempel-Ziv family. These schemes are more compressing than the simple RLE, but alas, we prove that performing computations on them is much harder. 1 Introduction The idea of using compression to speed up computations can be found in any domain that deals with large- scale data, and ML is no exception. By exploiting redundancies and va rious forms of structure in a piece of data, compression algorithms such as zip can reduce its size from N down to n, where n ! N. The data becomes cheaper to store, access, transmit, and perhaps also to analyze. Can we run our ML tools on the compressed data, without decompressing it ﬁrst, and make the computation times proportional to n rather than N? Since most ML algorithms boil down to large amounts of basic algebra ic operations such as multiplications of vectors and matrices, with inner product as the atomic operation, the most basic question in this context is: Main Question.Given two N-dimensional vectors, each in a compressed form of size n ! N, can we compute their inner product in ˜Opnq time1 rather than OpNq? The answer, of course, depends on the compression scheme that we use. There seems to be an inherent tension: more complex schemes have higher compression rates but are harder to analyze without decompres- sion. First, let us clarify that our interest is in exact computations and lossless compressions, even though lossy techniques such as dimensionality reduction [16] are widely used by the ML community. In many cases, e.g. when performing a basic algebraic operation within a larger pipeline , even a small amount of error could add up to make the ﬁnal result unintelligible. Recent years has s een a growing interest in exploring the potential of lossless compression for speeding up ML [35, 83, 59 , 65]. An inspiring result was honorably ∗ IBM Almaden Research Center, amir.abboud@gmail.com † Toyota Technological Institute at Chicago, backurs@ttic.edu. Supported by an NSF Grant CCF-2006806. ‡ Saarland University and Max Planck Institute for Informati cs, Saarland Informatics Campus, bringmann @cs.uni-saarland.de. This work is part of the project TIPEA that has received fund ing from the European Research Council (ERC) under the European Unions Horizon 2020 research and in novation programme (grant agreement No. 850979). §Max Planck Institute for Informatics, Saarland Informatic s Campus, marvin@mpi-inf.mpg.de 1We use the notation ˜Opnq “ n ¨ Nop1q for near-linear time, hiding small terms such as log factors . 1Table 1: The potential savings from grammar-compressed linear algebra: Compression rates on real datasets. We compare zip, a standard grammar-compression, with Run Length Encoding (RLE), a simple method that works well on repetitive or sparse data. For more such results, se e [35, Table 1]. Dataset Size RLE (compression rate) zip (compression rate) ISOLET [30] 30.94 MB 29.83 MB (0.96) 7.94 MB (0.26) US Census 1990 [30] 342.26 MB 341.97 MB (0.99) 51.91 MB (0.15) mentioned as an outstanding paper at NeurIPS last year [65]: any N ˆ d matrix A can be compressed down to a matrix of size d ˆ d such that the optimal solutions of Least-Mean-Squares (LMS) ins tances are exactly the same on A and A1 . This is an example where for a speciﬁc task (LMS solvers) a speciﬁc c ompression scheme (designed by the authors) leads to a solution in time T pnq rather than T pNq, giving a 100x speedup on benchmark data; it makes one wonder if this approach can work in a more general setting. For rather simple compression methods, the answer to our question is positive. A recent Communications of the ACM article [35] exhibits Compressed Linear Algebra [32, 33, 34] a compression scheme for vectors and matrices that uses simple techniques such as Run Length Encoding ( RLE) and allows for fast computations on the compressed data with impressive experimental results when integrated into ML systems. The RLE encoding of a vector simply replaces runs of values by tuples indicatin g the value and the length of the run; e.g. the binary vector 00011111000 gets encoded as 0 31503. Given two vectors encoded in this way with size nRLE , a simple one-pass algorithm can compute their inner product in OpnRLE q time. Before that, there were many algorithms for exploiting succinct encodings of sparse vectors [78, 56, 52]; e.g. by simply listing the nonzero locations the binary vector 0100001000 gets encoded as p2, 7q. These encodings allow for a linear time inner product computation as well. However, these simple methods are often not very compressing. At the other end of the spectrum, we have the heavy-duty and time-tested family of Grammar-Compressions [54] that includes the Lempel-Ziv-family (LZ77, LZ78, LZW, etc.) [58, 91, 86], Byte-Pair Encoding [82], diction ary methods, and others [69, 63]. These compressions are used in ubiquitous applications such as zip, S nappy, GIF, PNG, the built-in Unix utility compress, and even in PDF. Their compression rates are often on a whole diﬀer ent level compared to RLE; e.g. the current draft of this paper reduces from 10KB to 4KB with zip but RLE has no eﬀect. See Table 1 and [35, Table 1] for empirical data showing the quantitat ive potential of these methods for some standard ML datasets. What all these more elaborate compr ession techniques have in common is that they essentially (up to low order terms [76]) encode a string (or vect or) by a Straight-Line Program (SLP): a restricted kind of a context-free grammar that can only produc e one string. In more detail, an SLP is deﬁned over some alphabet Σ, say t0, 1u, and it is a set of replacement rules (or productions) of a very simple form: a rule is either a symbol in Σ or it is the concatenation of tw o previous rules (under some ﬁxed ordering of the rules). The last replacement rule is the sequence de ﬁned by the SLP. For example, we can compress the sequence 01010101 with the rules S1 Ñ 0; S2 Ñ 1; S3 Ñ S1 S2; S4 Ñ S3 S3; S5 Ñ S4 S4 and S5 corresponds to the sequence 01010101. See Figure 1. For some s trings this can give an exponential compression, e.g. the sequence p01qN requires only Oplog Nq rules; note that its RLE has size N. While ﬁnding the smallest SLP for a given string is NP-Hard, it can be approx imated either by the above practical methods or provably up to logarithmic factors [76, 20, 79, 48, 50]. Thus, the holy grail in this context is to perform algebraic operation s in T pcompression-sizeq time even when the vectors are compressed with zip or one of the other heavy-du ty grammar compressions; that is, without unzipping them ﬁrst. Ideally, we would implement a “zip-inner-product” function that takes two zip ﬁles encoding vectors and computes the inner product in near-linea r time (which may not even be enough time to unzip them). A recent paper titled “When LZW meets ML” [59] m akes partial progress towards this goal: the inner product can be computed eﬃciently on their tuple oriented coding where each coordinate is grammar-compressed separately, but not the vector as a whole. This makes their method less compressing since, unlike with zip, the size of the encoding is always at least the dime nsionality of the vectors. Main Question (Restated). Given twoN-dimensional vectors, each grammar-compressed down to sizen ! N, can we compute their inner product in ˜Opnq time rather than OpNq? While eﬃciently analyzing these grammars may seem like a daunting task, a large body of works over the 2S1 Ñ 0 S2 Ñ 1 S3 Ñ S1 S2 S4 Ñ S3 S3 S5 Ñ S4 S4 (a) S5 S4 S3 S1 0 S2 1 S3 S1 0 S2 1 S4 S3 S1 0 S2 1 S3 S1 0 S2 1 (b) 0 1 S1 S2 S3 S4 S5 (c) Figure 1: (a) An SLP generating the sequence 01010101. (b) The c orresponding parse tree. (c) The acyclic graph corresponding to the SLP. last three decades has equipped us with an ingenious toolbox exactlyfor this purpose. It turns out that many important problems can indeed be solved surprisingly faster than th e decompress-then-solve bound, e.g. in pattern matching [71, 53, 11, 36, 18, 61, 40, 45, 49]. This gives hope for a positive answer to our question and that many ML computations could be sped up by operating on gramma r-compressions. These algorithms typically look at the parse trees that haveN leaves but onlyn distinctly labelled internal nodes (see Figure 1), and traverse them starting from the root down, while attempting t o only spend time proportional to the depth of the tree per distinct label. Using tricks that restructure the grammar to make the tree balanced, the depth can be upper bounded by Oplog Nq, making the total time Opn log Nq. To learn more about this subﬁeld of Algorithm Design, we refer the reader to the surveys [9 0, 57, 39, 81, 41, 73, 77, 64, 80]. 1.1 Our Results Alas, our main result is a negative resolution to the main question above. We apply the tools of theoretical computer science, and the recently blossoming ﬁeld of ﬁne-grained complexity, in order to shed light into the mathematical foundations of Compressed Linear Algebra. We prov e new hardness reductions showing cases where the time to compute the inner product must be large (under p opular complexity assumptions) even when the vectors have very small grammar compressions. For example, there are N-dimensional vectors with grammar-compressions of size n “ OpN1{3q where the inner product must take ˜Ωpn2q time2 to compute. The consequences to other settings such as matrix-vector multip lication are further explained below. This creates a strong separation between grammar-compressions, w here we prove an ˜Ωpn2q lower bound, and RLE, where an Opnq algorithm exists. This formally justiﬁes the use of simpler methods in M L systems and guides researchers away from searching for an ultra-eﬃcient “zip -inner-product” function. Fine-Grained Complexity Negative results are paramount to the success of any scientiﬁc dis cipline. The most prominent framework for proving such results in compute r science is the theory of NP-Hardness, where one proves that a problem cannot be solved in polynomial time u nless P “ NP which would imply breakthrough algorithms for famously-hard problems such as SAT and Subset Sum. Without this theory, countless hours would have been wasted by algorithm designers trying to come up with provable, worst-case, polynomial time algorithms for NP-Hard problems. Due to the increas e in data sizes of recent years, the ethos of this theory that “eﬃcient = polynomial” has become obsolet e, and a more demanding attitude where “eﬃcient = linear” has arisen. By replacing the polynomial redu ctions of NP-Hardness with more eﬃcient ones (often linear), ﬁne-grained complexity can prove har dness results even for problems that have polynomial time algorithms. Exemplary results show that linear or sub quadratic algorithms for certain problems, which admit quadratic-time algorithms, would refute popu lar assumptions (conjectures that are similar to but stronger than P ‰ NP ) and have breakthrough consequences for famously hard proble ms. 2The more standard notation is n2´ op1q which indicates an Ω pn1.9999q lower bound, no matter how close to 2 we go. That is, only mildly subquadratic algorithms are possible, e.g. by shaving log factors. 3One of the central assumptions in this theory and in this paper is the 3SUM Conjecture: “ No algorithm can decide, in subquadratic Opn2´ εq time, if there are three numbers that sum to zero among a given set of n numbers”. A recent survey on ﬁne-grained complexity [89] cites dozens of papers, mainly in computational geometry [38] but also in other ﬁelds [72, 85, 7, 8, 21, 55, 10, 43], th at prove 3SUM-Hardness results showing that their algorithms are optimal up to a refutation of this conject ure. In this paper, we prove the ﬁrst 3SUM-Hardness results in ML 3 as far as we are aware. The 3SUM assumption and its generalizations that we use in the theorems below are formally deﬁned and discussed in Sec tion 2. Vector Inner Product Our ﬁrst and main result is a reduction from 3SUM to compressed inne r product of two vectors, negatively resolving our main question. Theorem 1.1.Assuming the 3SUM conjecture, the inner product of two N-dimensional vectors that are grammar-compressed to size n “ ΘpN 1 4 q cannot be computed in Opn2´ εq time where ε ą 0. Moreover, we strengthen and generalize this result in several way s. First, we address the dependence between n and N: could it be that for more or less compressed vectors the picture is diﬀerent? Using a stronger variant of the 3SUM conjecture, the same lower bound of n2 holds even when n “ N1{3, and therefore our result can be stated as an ˜ΩpN 2 3 q lower bound which is quite close to the trivial upper bound of OpNq. Moreover, by a (highly nontrivial) boosting of our reduction, in Sec tion 3 we establish an ˜ΩpN 1 3 q lower bound with n “ Nε for any ε ď 1{3. That is, when the vectors are highly compressed even n10 time is not suﬃcient4; this is in stark contrast to the case of RLE-compressed vectors where Opnq is always possible. Matrix-Vector Multiplication Next, we consider the problem of computing the M ¨ v product of an N-dimensional vector v that is compressed to size n with an N ˆ N matrix M where each row is compressed to size Opnq. Perhaps computing these N inner products as a batch can be done faster than computing each separately. Alas, by another signiﬁcant boosting of our redu ction we prove that this is also impossible. While if the encoding is with RLE the product can be computed in OpNnq time, which is linear in the representation size of the matrix and thus optimal, it turns out tha t for grammar compressions ˜ΩpNn2q is required. The proof is in Section 4. Theorem 1.2.Assuming the 3SUM conjecture, the product of an N ˆ N-dimensional matrix, where each row is grammar-compressed to size n “ ΘpN 1 5 q, with an N-dimensional vector that is grammar-compressed to size n cannot be computed in OpNn2´ εq time where ε ą 0. Matrix Multiplication Finally, we consider matrix multiplication of compressed matrices C “ A ¨ B. There are multiple ways to compress an N ˆ N matrix: we might compress each row or each column, so that the compression size is N ¨ n, or treat the whole matrix as an N2-dimensional vector and compress it to size n. Each way may lead to a diﬀerent time complexity, but no matter which way we choose, the ﬁrst question to ask, and that will determine the time we can hop e for, is: what is the output size? The na¨ ıve answer is that the matrix C has size N ˆ N, but since A and B are compressed, shouldn’t we expect C to also be representable with a small grammar of size n ! N2? Unlike the above questions that deal with computation time, this is an information-theoretic questio n, and in Section 5 we give strong and unconditional negative answers: the matrix C cannot be grammar-compressed to size opN2{ log2 Nq even when A and B are strongly compressible. Moreover, some of our results hold eve n when A and B have very small RLE encodings. Therefore, our results should be of interest to the compressed linear algebra project beyond grammar-compressions. 3We remark that some complexity assumption is necessary for proving the kind of r esults we are interested, since uncon- ditionally proving even very weak lower bounds on the time co mplexity such as Ω pn1` εq and even for NP-Hard problems like SAT (not to mention inner product) is far beyond current tech niques [12]. 4Strictly speaking, such a conditional lower bound of Ω pn10q for highly compressible inputs can already be proven by com- bining a known #P-hardness reduction from SubsetSum [60] wi th a ﬁne-grained hardness of SubsetSum under the Exponentia l Time Hypothesis (see, e.g. [47]). However, such an approach yields only a weak lower bound in terms of the uncompressed si ze N, namely a bound of Ω pNǫq for some non-explicit, possibly tiny ǫ. Our lower bounds always give an explicit, reasonably large value for ǫ. 4Technical Remarks While the tools for proving NP-Hardness results for grammar-comp ressed data are old [64], they only apply in the unrealistic setting where n “ log N, and we are interested in more ﬁne- grained results. Only recently, a FOCS paper by the authors [2] int roduced the techniques for proving such lower bounds. This previous work focused on combinatorial patter n matching problems and the current work extends it to the setting of linear algebra. Our results establis h the hardness even of the simplest setting of binary vectors and matrices over t0, 1u. This setting is particularly studied due to its connection to graphs, where grammar compressions have also received a lot of attention [66, 67]. Moreover, we show that even deciding if the inner product is 0 or ě 1 is hard, and so our lower bounds hold against any bounded approximation algorithms. Extending the lower bounds to other functions such as computing the ℓ2 distance between two vectors is also easy. Like almost all results in ﬁne-grain ed complexity [89], our lower bounds are against both deterministic and randomized algorithms. Finally, we remark that our lower bounds are for the most basic setting of worst-case instances. Extending them to average-case results, showing that instances that come from certain natural d istributions are also hard, is an open question. However, notice that even if the original vectors come from a natural distribution, the distribution of the grammar representations will be completely diﬀerent (and probably far from natural). Therefore, exploiting the structure of non-worst-case instances seems far beyond current reach in this context. 1.2 Other Related Works There have been a few recent works showing ﬁne-grained complexity results for machine learning problems. In particular, [14] showed that the classic algorithm of Viterbi that computes the most likely path in a Hidden Markov Model which results in a given sequence of observatio ns is essentially optimal assuming certain complexity theoretical hypotheses. Another work [13] showed conditional hardness results for multiple empirical risk minimization problems such as kernel support vector m achines, kernel ridge regression, and training the ﬁnal layer of a neural network. Furthermore, there are many works that show hardness for problems that are used in machine learning literature. This includes co nditional lower bounds for kernel low- rank approximation [68], closest pair and its variants [9, 75, 88, 24,29, 28], maximum inner product [6, 22, 23], earth mover’s distance (a.k.a. Wasserstein metric) [74], dynamic time warping distance [3, 17]. Further contexts in which lossless compressions are used for ML ap plications, where the primary focus is on other aspects than increasing algorithmic performance, includ e compressing and accelerating models for deployment on resource-constrained devices (see [44, 26]; e.g ., lossless compressions are used to compress weights after a quantization step) or implementing the principle of min imum description length for feature learning (see [70]). Outside of ML, the idea of improving eﬃciency by operating on (lossles sly) compressed data is well- established in databases [1, 25, 87, 46], and is gaining traction also in b ioinformatics [84]. 2 Preliminaries As described in Section 1, a grammar compression of a sequence (ora vector) is an SLP that produces the sequence. In our proofs we will use the following simple observation a bout SLPs. Proposition 2.1. Let G be an SLP with start symbol S that generates a sequence s. For any α P N, we can compute an SLP G1 that generates the α-fold repetition of s, i.e., sα “ s s ¨ ¨ ¨ slooomooon α times , and has size |G| ` Oplog αq in time Op|G1 |q. Proof sketch. Using Oplog αq repeated squaring rules Si Ñ Si´ 1Si´ 1 and S0 Ñ S, we obtain non-terminals S0, . . . , S tlog2 αu generating s2i for i P t 0, . . . , tlog2 αuu. It is straightforward to combine these non-terminals, according to the binary representation of α, to generate sα using only Oplog αq additional non-terminals. Using this property, we can often compress sequences much more eﬃciently than run-length encoding alone could: E.g., repetitive patterns like p010011qn can be encoded using only Θplog nq bits instead of Θpnq. Indeed, our constructions crucially exploit a repeated application of this property to compress hard instances to very small sizes. 5The Complexity Assumptions As discussed in Section 1, the impossibility results in ﬁne-grained com- plexity are based on certain popular conjectures. One of the cent ral ones concerns the 3SUM problem, which has a few equivalent formulations (up to linear time transformations [31]); we will mostly use the following5. Deﬁnition 2.2 (The 3SUM Problem). Given three sets A, B, C of m integers in t1, . . . , U u, decide if there is a triple a P A, b P B, c P C such that a ` b “ c. It is a simple exercise (that is often given in interviews) to come up with an Opm2q time algorithm, and despite decades of eﬀorts, only mildly subquadratic Opm2{ logc mq bounds for a small 0 ă c ă 3 are known [15, 51, 42, 37, 19]. The 3SUM Conjecture.No algorithm can solve the 3SUM problem in Opm2´ εq time, where ε ą 0. A few remarks about this conjecture. First, a folklore trick of tak ing all numbers modulo a random large prime shows that the problem for arbitrary universe U is equivalent to the case where U “ Opm3 log2 mq (see Lemma B.1 in [5] for a proof). Therefore, we will assume this bou nd on U. When U becomes too small, the problem becomes easy due to an Opm ` U log Uq algorithm using Fast Fourier Transform [27]. However, the problem is conjectured to be hard even when U “ Θpm2q and this is referred to as the Strong 3SUM Conjecture [10, 2]. This stronger assumption allows us to strengthen our lower bounds by reducing N. Second, the hardness of the more general kSUM problem is also used as a complexity assumption [4, 2]. In the formulation that we will use, we are given k sets A1, . . . , A k of m integers in t1, . . . , U u where U “ Θpmrk{2s q and are asked to decide if there are k numbers, one from each set, such that a1 ` ¨ ¨ ¨ ` ak´ 1 “ ak. The Strong kSUM conjecture states that cannot be done in Opmrk{2s´ εq time, for any ε ą 0. We will use this assumption to prove lower bounds even whenn is much smaller thanN. Third, 3SUM and the other hardness assumptions in ﬁne-grained complexity are conjectured to be true even against randomized algorithms that succeed with high probability. This is important since some of our redu ctions are randomized. 3 Vector Inner Product In this section we present the proof of Theorem 1.1 by giving a reduction from 3SUM to the inner product of compressed vectors. A slightly weaker conditional lower bound o f ˜ΩpN1{2q for vectors compressible to n “ N1{4 can be extracted from the proof of Theorem 5.11 in [2]. We use similar tricks, but a diﬀerent and more optimized construction to obtain a stronger conditional lo wer bound of ˜ΩpN2{3q already on less compressible vectors with n “ N1{3. Technically, the novelty is that we manage to encode two sets ( A and B) into one vector of length mU rather than m2U. This new construction is crucial for the extensions we show – we do not see how to prove any lower bound for matrix-vecto r inner product without building on this new construction. Proof.Given an instance of 3SUM, that is, three sets A, B, C of m integers in t1, . . . , U u, we show how to construct vectors v1 A` B, v1 C P t 0, 1uN with N “ 2mU log2 m such that: (1) v1 A` B ¨ v1 C ě 1 if and only there a P A, b P B, c P C with a ` b “ c, (2) both vectors have a compression of size Opm log Uq, and (3) the construction time is Opm log Uq. This reduction suﬃces for proving Theorem 1.1 due to the following ca lculations. Since (as discussed in Section 2) we can assume that U “ Θpm3 log2 mq, the reduction produces two vectors of dimension N “ Θppm log mq4q and compressed size n “ ΘpN1{4q “ Θpm log mq, such that the inner product reveals the answer to the 3SUM instance. Therefore, an Opn2´ εq-time algorithm would solve the 3SUM instance in time Opm2´ εpolylogmq, refuting the 3SUM conjecture. Note that the Opm log Uq time for the reduction itself is negligible. Moreover, if we assume the Strong 3SUM conjectu re, we can start with 3SUM instances where U “ Opm2q and get vectors of dimension N “ Oppm log mq3q, ruling out inner product algorithms with time OpN 2 3 ´ εq. We now present the construction of the vectors. As a ﬁrst step,we observe that for any setX Ď t 1, ..., U u, we can compress its characteristic vector vX P t 0, 1uU , i.e., vX ris “ 1 iﬀ i P X, to size Op|X| log Uq as follows. We write X “ t x1, . . . , x |X| u with x1 ă x2 ă ¨ ¨ ¨ ă x|X| and observe that vX :“ 0x1´ 1 1 0x2´ x1´ 1 1 . . . 1 0x|X| ´ x|X|´ 1´ 1 1 0U´ x|X| , 5For example, instead of a ` b “ c or a ` b ` c “ 0 we may be given a target t and ask for a ` b ` c “ t. 6where each 0-block has length at most U and can thus be encoded using Oplog U) symbols using Proposi- tion 2.1. In total, we obtain a compression of size Op|X| log Uq, which can be computed in time Op|X| log Uq as well. Let A “ t a1, . . . , a nu. The central idea is to let v1 A` B, v1 C consist of m blocks of size 2 U, where the i-th block in v1 A` B gives the characteristic vector of the set ai ` B “ t ai ` b | b P Bu Ď t 1, . . . , 2Uu and the i-th block in v1 C gives the characteristic vector of C Ď t 1, . . . , 2Uu. Formally, we deﬁne v1 A` B :“ 0a1 vB 0U´ a1 lo oooo omo oooo on v1 a1` B 0a2 vB 0U´ a2 lo oooo omo oooo on v1 a2` B . . . 0am vB0U´ am lo ooooo omo ooooo on v1 am` B 0N´ 2mU , v1 C :“ vC 0U vC 0U . . . v C 0U 0N´ 2mU . (Here, the last block of 0s only serves to get the desired dimension o f N for technical reasons.) We observe that v1 A` B and v1 C have an inner product of at least 1 if and only if the characteristic ve ctors of some block i have a common 1-entry. Thus, consider any block i: We have v1 ai` Brks “ p vC 0U qrks “ 1 if and only if k ´ ai P B and k P C, i.e., ai P A, k ´ ai P B, k P C is a solution of the given 3SUM instance. Thus, v1 A` B ¨ v1 C ě 1 if and only if there is some a P A, b P B, c P C such that a ` b “ c, as desired. It remains to show that a Opm log Uq-sized compression of v1 A` B and v1 C can be computed in time Opm log Uq: Clearly, since vC 0U can be compressed to size Opm log Uq eﬃciently, we can also compress its m-fold repetition using Oplog mq additional symbols using Proposition 2.1, as well 0 N´ 2mU which takes Oplog Nq “ Oplog mUq additional symbols; thus, v1 C can be compressed to size Opm log mUq in time Opm log Uq. Furthermore, recall that we can compress vB to size Opm log Uq eﬃciently, and let G be an SLP with starting symbol SB generating vB. Thus, to compress vai` B, we only need to compress the surrounding blocks 0 ai , 0 U´ ai and can reuse SB to generate vB. Since we can encode the 0-blocks using Oplog Uq additional non-terminals, this yields a compression size of Oplog Uq per block i. Together with a Oplog mUq encoding of the trailing block 0 N´ 2mU , this yields again a compression of size Opm log Uq. Note that reusing a non-terminal generating vB was instrumental in giving a compression of sizeOpm log mq rather than Opm2 log mq and that this compression can indeed be computed in time Opm log Uq and concludes the claim. With more work, the above arguments can be generalized to reduce a kSUM instance with k sets of m integers in t1, . . . , U u to vectors of dimension N “ Θpmk´ 2Uq and compressed size Opm log Uq in time Opm log Uq. The main idea is to encode a shift of Ak´ 1 for each tuple of A1, . . . , A k´ 2 in one vector, and encode mk´ 2 repetitions of the remaining set Ak in the other vector. Under the Strong kSUM conjecture, this yields a conditional lower bound for inner product of ˜ΩpN1{3q where n “ OppN{Uq1{pk´ 2q log Nq. Thus, for any ﬁxed ε ą 0, let k be a suﬃciently large constant integer such that 1 {pk ´ 2q ă ε, then the Strong kSUM conjecture implies that N-dimensional vectors with compressed size n “ OpNεq cannot have an OpN1{3´ δ q algorithm for any constant δ ą 0. We formally prove the result in the appendix. 4 Matrix-Vector Multiplication In this section we sketch how to prove Theorem 1.2 by giving a reduction from 3SUM to Matrix-Vector multiplication on compressed data. We give a complete formal proof in the appendix. A helpful tool for this task is the following self-reduction for 3SUM,which follows from combining a known self-reduction [62] with a standard universe-size reduction techn ique on each produced instance [15, 72, 5]. Lemma 4.1 (Self-Reduction for 3SUM) . Let 1 ď s “ spmq ď m and ǫ ą 0 be arbitrary. If there is an algorithm that, given a target t and L “ Oppm{sq2q sets Aℓ, Bℓ, Cℓ of s integers in t1, . . . , O ps3 log2 squ, determines for all 1 ď ℓ ď L whether there are a P Aℓ, b P Bℓ, c P Cℓ with a ` b ` c “ t in total time Opm2´ ǫq, then the 3SUM conjecture is false. Given the above self-reduction, the basic idea is as follows. We const ruct a matrix M whose rows are indexed by the instance 1 ď ℓ ď L and the aim is to construct the row Mℓ and the vector v such that Mℓ ¨ v ě 1 if and only if the instance Aℓ,Bℓ, Cℓ contains a solution, i.e., a P Aℓ, b P Bℓ, c P Cℓ with a` b` c “ t. Unfortunately, we cannot apply our Vector Inner Product const ruction directly: this would encode the set Aℓ ` Bℓ “ t a ` b | a P Aℓ, b P Bℓu into the row Mℓ and the set Cℓ into the vector v – however, in the matrix 7product Mv , each row Mℓ is multiplied with a ﬁxed vector v, while the Cℓ’s diﬀer for each ℓ. We overcome this issue by adapting our construction to encode the set Aℓ ` Bℓ ` Cℓ “ t a ` b ` c | a P Aℓ, b P Bℓ, c P Cℓu into the row Mℓ, and only the common target t into v. As all instances use the same target t, this is indeed possible. Speciﬁcally, using the ideas of Theorem 1.1, which produces a 2 sU-dimensional vectors encoding the sets A ` B and C, both having compressed size Ops log Uq, we show how to produce 3 s2U-dimensional vectors Mℓ and v encoding the sets Aℓ ` Bℓ ` Cℓ and ttu, both having compressed size Ops log Uq. This yields a pL ˆ 3s2Uq-dimensional matrix M and 3s2U-dimensional vector v. There is a choice s “ Θpm2{7q that leads to a quadratic matrix M with dimension N “ Θpm10{7q (as it has Oppm{sq2q “ Opm10{7q rows and Ops2Uq “ Ops5q “ Opm10{7q columns), with row compressions of size n “ Θps log sq “ Θpm2{7 log mq « N1{5. Thus, any OpNn2´ εq algorithm computing M ¨ v would solve 3SUM instances in time ˜Opm2´ 2ε{7q, refuting the 3SUM conjecture. 5 Matrix-Matrix Multiplication In this section, we consider the problem of computing the matrix product C of two N ˆ N matrices A, B. We consider the following representations of the input matrices: • Convenient compression: A is compressed row-wise, B is compressed column-wise. This repre- sentation allows us to compute any single entry Ci,j by running an inner product algorithm on the compressed row Ai and the compressed column Bj . The size of the input is OpN ¯ninq, where ¯nin is the maximum compressed size of the rows Ai and columns Bj . • Strong compression: For any matrix M, we deﬁne strong compression as a grammar compression of M or MT when viewed as n2-dimensional vector, whichever is shortest. When both A, B are given as strong compression, the resulting representation can have a m uch smaller size (it can be opNq), but to compute a single entry Ci,j , we ﬁrst might need to obtain a representation of the row Ai and the column Bj . Similarly, we have several options for representing C: • Row-wise compression of C. This compression is particularly useful if we aim to compute re- peated matrix products A1pA2p¨ ¨ ¨ p AkBqqq. The output size is OpN ¯noutq, where ¯nout is the maximum compressed size over all rows of C. • Column-wise compression of C. This compression is particularly useful if we aim to compute repeated matrix products pppAB1qB2q ¨ ¨ ¨ q Bk. The output size is OpN ¯noutq, where ¯nout is the maximum compressed size over all columns of C. • Strong compression of C. This compression has the smallest output size, which can be even opNq. We show the following result: Theorem 5.1.For inﬁnitely many N, there are N ˆ N matrices A, B with 1. convenient compression of size OpN log Nq (already under RLE), and 2. strong compression of size Oplog2 Nq, such that 3. the matrix product C “ AB has size ΩpN2{ log2 Nq in any grammar-compression (row-wise, column- wise, or strong). As a consequence, there can be no opN2{ log2 Nq algorithm for matrix-matrix multiplication (for any of our discussed representations), since already writing the output requires time ΩpN2{ log2 Nq. The rough proof strategy is to construct an instance C “ AB such that C and CT , when viewed as N2-dimensional vectors, contain all substrings of length 2 log 2 n. By the following standard lemma, such a string has no grammar compression of size opN2{ log Nq. 8Lemma 5.2 (see, e.g., [20, Lemma 3]) . Let ℓ P N. If a string x is generated by a grammar of size n, then x contains at most nℓ distinct substrings of length ℓ. Proof of Theorem 5.1. Let ℓ P N. We ﬁrst deﬁne the matrices A1 , B1 where A1 is a p2ℓ ˆ 2ℓq matrix with rows indexed by strings x P t 0, 1uℓ in lexicographic order, and B1 is a p2ℓ ˆ 2ℓp2ℓqq matrix with columns indexed by py, kq P t 0, 1uℓ ˆ t 1, . . . , 2ℓu in lexicographic order. For arbitrary z P t 0, 1uℓ, let diag pzq denote the ℓ ˆ ℓ diagonal matrix with z on the diagonal. We deﬁne A1 x :“ p x | 1ℓq, B 1 py,1q,...,py,2ℓq :“ ˆ diagp1ℓq 0 0 diagpyq ˙ . Let C1 “ A1 B1 be the p2ℓ ˆ 2ℓp2ℓqq product matrix of A1 and B1 , with rows and columns indexed by t0, 1uℓ and t0, 1uℓ ˆ t 1, . . . , 2ℓu, respectively. Observe that by deﬁnition, pCx,py,1q , . . . , C x,py,2ℓqq “ p x | yq for any x, y P t 0, 1uℓ. In particular, when we view C1 as a 22ℓp2ℓq-length string, it contains all strings in t0, 1u2ℓ as substrings, thus by Lemma 5.2, any row-wise compression is of siz e at least 2 2ℓ{p2ℓq. It is straightforward to make these matrices quadratic with dimens ion N “ Θpℓ2ℓq (by introducing all-0 columns) and to ensure that also column-wise compression has s ize Ωp22ℓ{ℓq “ ΩpN2{ log2 Nq (using transposed constructions to A1 and B1 ). Finally, we can compress each row of A1 and column of B1 trivially to length Opℓq “ Oplog Nq (already using RLE). In the appendix, we also argue how to grammar -compress the concatenation of the columns of A1 and the rows of B1 to size Opℓ2q “ Oplog2 Nq, which concludes the desired bound on the strong compression. Broader Impact The broader impact of our work is to inform algorithm design for compressed linear algebra, which can lead to faster algorithms for a variety of tasks on large data sets. The ethical consequences depend on the speciﬁc application. We do not see any inherently new concerns raised by our results, beyond those that follow generally from faster algorithms and an increased ability to process data. References [1] Daniel Abadi, Samuel Madden, and Miguel Ferreira. Integrating compression and execution in column- oriented database systems. In Proceedings of the 2006 ACM SIGMOD international conferenc e on Management of data , pages 671–682, 2006. [2] Amir Abboud, Arturs Backurs, Karl Bringmann, and Marvin K¨ un nemann. Fine-grained complexity of analyzing compressed data: Quantifying improvements over dec ompress-and-solve. In 58th IEEE Annual Symposium on Foundations of Computer Science, FOCS 2 017, Berkeley, CA, USA, October 15-17, 2017 , pages 192–203, 2017. [3] Amir Abboud, Arturs Backurs, and Virginia Vassilevska Williams. Tigh t hardness results for lcs and other sequence similarity measures. In 2015 IEEE 56th Annual Symposium on Foundations of Computer Science, pages 59–78. IEEE, 2015. [4] Amir Abboud and Kevin Lewi. Exact weight subgraphs and the k-su m conjecture. In International Colloquium on Automata, Languages, and Programming , pages 1–12. Springer, 2013. [5] Amir Abboud, Kevin Lewi, and Ryan Williams. Losing weight by gaining edges. In European Symposium on Algorithms , pages 1–12. Springer, 2014. [6] Amir Abboud, Aviad Rubinstein, and Ryan Williams. Distributed pcp th eorems for hardness of ap- proximation in p. In 2017 IEEE 58th Annual Symposium on Foundations of Computer S cience (FOCS), pages 25–36. IEEE, 2017. [7] Amir Abboud and Virginia Vassilevska Williams. Popular conjectures im ply strong lower bounds for dynamic problems. In 2014 IEEE 55th Annual Symposium on Foundations of Computer S cience, pages 434–443. IEEE, 2014. 9[8] Amir Abboud, Virginia Vassilevska Williams, and Oren Weimann. Conseq uences of faster alignment of sequences. In International Colloquium on Automata, Languages, and Prog ramming, pages 39–51. Springer, 2014. [9] Josh Alman and Ryan Williams. Probabilistic polynomials and hamming nea rest neighbors. In 2015 IEEE 56th Annual Symposium on Foundations of Computer Scien ce, pages 136–150. IEEE, 2015. [10] A. Amir, T. M. Chan, M. Lewenstein, and N. Lewenstein. On hard ness of jumbled indexing. In Proc. ICALP, volume 8572, pages 114–125, 2014. [11] Amihood Amir, Gary Benson, and Martin Farach. Let sleeping ﬁles lie: Pattern matching in z- compressed ﬁles. Journal of Computer and System Sciences , 52(2):299–307, 1996. [12] Sanjeev Arora and Boaz Barak. Computational complexity: a modern approach . Cambridge University Press, 2009. [13] Arturs Backurs, Piotr Indyk, and Ludwig Schmidt. On the ﬁne- grained complexity of empirical risk minimization: Kernel methods and neural networks. In Advances in Neural Information Processing Systems, pages 4308–4318, 2017. [14] Arturs Backurs and Christos Tzamos. Improving viterbi is har d: Better runtimes imply faster clique algorithms. In Proceedings of the 34th International Conference on Machin e Learning-Volume 70 , pages 311–321. JMLR. org, 2017. [15] Ilya Baran, Erik D Demaine, and Mihai Patra¸ scu. Subquadratic algorithms for 3sum. In Workshop on Algorithms and Data Structures , pages 409–421. Springer, 2005. [16] Ella Bingham and Heikki Mannila. Random projection in dimensionality reduction: applications to im- age and text data. In Proceedings of the seventh ACM SIGKDD international confer ence on Knowledge discovery and data mining , pages 245–250, 2001. [17] Karl Bringmann and Marvin K¨ unnemann. Quadratic conditionallower bounds for string problems and dynamic time warping. In 2015 IEEE 56th Annual Symposium on Foundations of Computer S cience, pages 79–97. IEEE, 2015. [18] Patrick C´ egielski, Irene Guessarian, Yury Lifshits, and Yuri Matiyasevich. Window subsequence prob- lems for compressed texts. In Proc. 1st International Computer Science Symposium in Russ ia (CSR’06), pages 127–136. Springer, 2006. [19] Timothy M Chan. More logarithmic-factor speedups for 3sum,(m edian,+)-convolution, and some geo- metric 3sum-hard problems. ACM Transactions on Algorithms (TALG) , 16(1):1–23, 2019. [20] Moses Charikar, Eric Lehman, Ding Liu, Rina Panigrahy, Manoj Pr abhakaran, Amit Sahai, and Abhi Shelat. The smallest grammar problem. STOC’02 and IEEE Transactions on Information Theory , 51(7):2554–2576, 2005. [21] Kuan-Yu Chen, Ping-Hui Hsu, and Kun-Mao Chao. Approximate matching for run-length encoded strings is 3sum-hard. In Annual Symposium on Combinatorial Pattern Matching , pages 168–179. Springer, 2009. [22] Lijie Chen. On the hardness of approximate and exact (bichrom atic) maximum inner product. arXiv preprint arXiv:1802.02325, 2018. [23] Lijie Chen, Shaﬁ Goldwasser, Kaifeng Lyu, Guy N Rothblum, and A viad Rubinstein. Fine-grained com- plexity meets ip= pspace. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on D iscrete Algorithms, pages 1–20. SIAM, 2019. [24] Lijie Chen and Ryan Williams. An equivalence class for orthogonal v ectors. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithm s, pages 21–40. SIAM, 2019. 10[25] Zhiyuan Chen, Johannes Gehrke, and Flip Korn. Query optimizat ion in compressed database systems. In Proceedings of the 2001 ACM SIGMOD international conferenc e on Management of data , pages 271–282, 2001. [26] Tejalal Choudhary, Vipul Mishra, Anurag Goswami, and Jagann athan Sarangapani. A comprehensive survey on model compression and acceleration. Artif. Intell. Rev. , 53(7):5113–5155, 2020. [27] Thomas H Cormen, Charles E Leiserson, Ronald L Rivest, and Cliﬀord Stein. Introduction to algorithms . MIT press, 2009. [28] Karthik CS and Pasin Manurangsi. On closest pair in euclidean metr ic: Monochromatic is as hard as bichromatic. In 10th Innovations in Theoretical Computer Science Conferen ce (ITCS 2019) . Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2018. [29] Roee David and Bundit Laekhanukit. On the complexity of closest pair via polar-pair of point-sets. SIAM Journal on Discrete Mathematics , 33(1):509–527, 2019. [30] Dheeru Dua and Casey Graﬀ. UCI machine learning repository, 2 017. [31] Bartlomiej Dudek, Pawel Gawrychowski, and Tatiana Starikovs kaya. All non-trivial variants of 3-ldt are equivalent. CoRR, abs/2001.01289, 2020. [32] Ahmed Elgohary, Matthias Boehm, Peter J. Haas, Frederick R. Reiss, and Berthold Reinwald. Com- pressed linear algebra for large-scale machine learning. Proc. VLDB Endow. , 9(12):960–971, 2016. [33] Ahmed Elgohary, Matthias Boehm, Peter J. Haas, Frederick R. Reiss, and Berthold Reinwald. Scaling machine learning via compressed linear algebra. SIGMOD Rec. , 46(1):42–49, 2017. [34] Ahmed Elgohary, Matthias Boehm, Peter J. Haas, Frederick R. Reiss, and Berthold Reinwald. Com- pressed linear algebra for large-scale machine learning. VLDB J. , 27(5):719–744, 2018. [35] Ahmed Elgohary, Matthias Boehm, Peter J. Haas, Frederick R. Reiss, and Berthold Reinwald. Com- pressed linear algebra for declarative large-scale machine learning. Commun. ACM , 62(5):83–91, 2019. [36] Martin Farach and Mikkel Thorup. String matching in Lempel-Ziv c ompressed strings. In Proc. 27th Annual ACM Symposium on Theory of Computing (STOC’95) , pages 703–712. ACM, 1995. [37] Ari Freund. Improved subquadratic 3sum. Algorithmica, 77(2):440–458, 2017. [38] Anka Gajentaan and Mark H. Overmars. On a class of opn2q problems in computational geometry. Computational Geometry , 5(3):165–185, 1995. [39] Leszek Gasieniec, Marek Karpinski, Wojciech Plandowski, and Wo jciech Rytter. Eﬃcient algorithms for Lempel-Ziv encoding. Proc. 5th Scandinavian Workshop on Algorithm Theory (SWAT’ 96), pages 392–403, 1996. [40] Pawe/suppress l Gawrychowski. Pattern matching in Lempel-Ziv compressed strings: fast, simple, and determin- istic. In Proc. 19th Annual European Symposium on Algorithms (ESA’11 ), pages 421–432. Springer, 2011. [41] Raﬀaele Giancarlo, Davide Scaturro, and Filippo Utro. Textual d ata compression in computational biology: a synopsis. Bioinformatics, 25(13):1575–1586, 2009. [42] Omer Gold and Micha Sharir. Improved bounds for 3SUM, K-SUM, and linear degeneracy. CoRR, abs/1512.05279, 2015. [43] Isaac Goldstein, Tsvi Kopelowitz, Moshe Lewenstein, and Ely Po rat. How hard is it to ﬁnd (honest) witnesses? arXiv preprint arXiv:1706.05815 , 2017. 11[44] Song Han, Huizi Mao, and William J. Dally. Deep compression: Compr essing deep neural network with pruning, trained quantization and huﬀman coding. In Yoshua Bengio and Yann LeCun, editors, Proc. 4th International Conference on Learning Representations , ICLR 2016 , 2016. [45] Danny Hermelin, Gad M Landau, Shir Landau, and Oren Weimann. U niﬁed compression-based accel- eration of edit-distance computation. Algorithmica, 65(2):339–353, 2013. [46] Balakrishna R Iyer and David Wilhite. Data compression support in databases. In VLDB, volume 94, pages 695–704, 1994. [47] Klaus Jansen, Felix Land, and Kati Land. Bounding the running t ime of algorithms for scheduling and packing problems. SIAM J. Discret. Math. , 30(1):343–366, 2016. [48] Artur Je˙ z. Approximation of grammar-based compression via recompression. Theoretical Computer Science, 592:115–134, 2015. [49] Artur Je˙ z. Faster fully compressed pattern matching by rec ompression. ACM Transactions on Algo- rithms (TALG) , 11(3):20, 2015. [50] Artur Je˙ z. A really simple approximation of smallest grammar.Theoretical Computer Science , 616:141– 150, 2016. [51] Allan Grønlund Jørgensen and Seth Pettie. Threesomes, degen erates, and love triangles. In Proc. of the 55th Annual IEEE Symposium on Foundations of Computer Sc ience (FOCS) , pages 621–630, 2014. [52] Vasileios Karakasis, Theodoros Gkountouvas, Kornilios Kourtis , Georgios Goumas, and Nectarios Koziris. An extended compression format for the optimization of sp arse matrix-vector multiplication. IEEE Transactions on Parallel and Distributed Systems , 24(10):1930–1940, 2012. [53] Marek Karpinski, Wojciech Rytter, and Ayumi Shinohara. Patt ern-matching for strings with short descriptions. In Proc. Annual Symposium on Combinatorial Pattern Matching ( CPM’95), pages 205– 214. Springer, 1995. [54] John C. Kieﬀer and En-Hui Yang. Grammar-based codes: A new class of universal lossless source codes. IEEE Trans. Inf. Theory , 46(3):737–754, 2000. [55] Tsvi Kopelowitz, Seth Pettie, and Ely Porat. Higher lower bound s from the 3sum conjecture. In Proceedings of the twenty-seventh annual ACM-SIAM symposi um on Discrete algorithms , pages 1272– 1287. SIAM, 2016. [56] Kornilios Kourtis, Georgios Goumas, and Nectarios Koziris. Optim izing sparse matrix-vector multipli- cation using index and value compression. In Proceedings of the 5th conference on Computing frontiers , pages 87–96, 2008. [57] N Jesper Larsson. Structures of string matching and data compression . Department of Computer Science, Lund University, 1999. [58] Abraham Lempel and Jacob Ziv. On the complexity of ﬁnite seque nces. IEEE Transactions on Infor- mation Theory , 22(1):75–81, 1976. [59] Fengan Li, Lingjiao Chen, Arun Kumar, Jeﬀrey F Naughton, Jign esh M Patel, and Xi Wu. When lempel-ziv-welch meets machine learning: A case study of acceleratin g machine learning using coding. arXiv preprint arXiv:1702.06943 , 2017. [60] Yury Lifshits. Processing compressed texts: A tractability bo rder. In Bin Ma and Kaizhong Zhang, editors, Proc. 18th Annual Symposium on Combinatorial Pattern Match ing (CPM 2007) , volume 4580 of Lecture Notes in Computer Science , pages 228–240. Springer, 2007. [61] Yury Lifshits, Shay Mozes, Oren Weimann, and Michal Ziv-Ukelso n. Speeding up hmm decoding and training by exploiting sequence repetitions. Algorithmica, 54(3):379–399, 2009. 12[62] Andrea Lincoln, Virginia Vassilevska Williams, Joshua R. Wang, and R . Ryan Williams. Deterministic time-space trade-oﬀs for k-sum. In International Colloquium on Automata, Languages, and Prog ram- ming, pages 58:1–58:14, 2016. [63] Qi Liu, Yu Yang, Chun Chen, Jiajun Bu, Yin Zhang, and Xiuzi Ye. R NACompress: Grammar-based compression and informational complexity measurement of RNA sec ondary structure. BMC bioinfor- matics, 9(1):176, 2008. [64] Markus Lohrey. Algorithmics on slp-compressed strings: A sur vey. Groups Complexity Cryptology , 4(2):241–299, 2012. [65] Alaa Maalouf, Ibrahim Jubran, and Dan Feldman. Fast and accur ate least-mean-squares solvers. In Advances in Neural Information Processing Systems 32: Annu al Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada , pages 8305– 8316, 2019. [66] Sebastian Maneth and Fabian Peternek. A survey on methods a nd systems for graph compression. arXiv preprint arXiv:1504.00616 , 2015. [67] Sebastian Maneth and Fabian Peternek. Grammar-based grap h compression. Information Systems , 76:19–45, 2018. [68] Cameron Musco and David Woodruﬀ. Is input sparsity time possible for kernel low-rank approximation? In Advances in Neural Information Processing Systems , pages 4435–4445, 2017. [69] Craig G Nevill-Manning and Ian H Witten. Compression and explanat ion using hierarchical grammars. The Computer Journal , 40(2 and 3):103–116, 1997. [70] Hristo S. Paskov, Robert West, John C. Mitchell, and Trevor J. Hastie. Compressive feature learning. In Christopher J. C. Burges, L´ eon Bottou, Zoubin Ghahramani, a nd Kilian Q. Weinberger, editors, Advances in Neural Information Processing Systems , pages 2931–2939, 2013. [71] Wojciech Plandowski. Testing equivalence of morphisms on conte xt-free languages. Proc. 2nd Annual European Symposium on Algorithms (ESA’94) , pages 460–470, 1994. [72] Mihai Pˇ atra¸ scu. Towards polynomial lower bounds for dynamic problems. In Proc. of the 42nd Annual ACM Symposium on Theory Of Computing (STOC) , pages 603–610, 2010. [73] Roberto Radicioni and Alberto Bertoni. Grammatical compress ion: compressed equivalence and other problems. Discrete Mathematics and Theoretical Computer Science , 12(4):109, 2010. [74] Dhruv Rohatgi. Conditional hardness of earth mover distance . arXiv preprint arXiv:1909.11068 , 2019. [75] Aviad Rubinstein. Hardness of approximate nearest neighbor s earch. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing , pages 1260–1268, 2018. [76] Wojciech Rytter. Application of Lempel–Ziv factorization to the approximation of grammar-based compression. Theoretical Computer Science , 302(1-3):211–222, 2003. [77] Wojciech Rytter. Grammar compression, lz-encodings, and st ring algorithms with implicit input. In Proc. 31st International Colloquium on Automata, Language s, and Programming (ICALP’04) , pages 15–27. Springer, 2004. [78] Yousef Saad. Iterative methods for sparse linear systems , volume 82. siam, 2003. [79] Hiroshi Sakamoto. A fully linear-time approximation algorithm for grammar-based compression. Journal of Discrete Algorithms , 3(2):416–430, 2005. [80] Hiroshi Sakamoto. Grammar compression: Grammatical infere nce by compression and its application to real data. In ICGI, pages 3–20, 2014. 13[81] D Sculley and Carla E Brodley. Compression and machine learning: A new perspective on feature space vectors. In Proc. Data Compression Conference (DCC’06) , pages 332–341, 2006. [82] Yusuxke Shibata, Takuya Kida, Shuichi Fukamachi, Masayuki Takeda, Ayumi Shinohara, Takeshi Shi- nohara, and Setsuo Arikawa. Byte pair encoding: A text compress ion scheme that accelerates pattern matching. Technical report, Technical Report DOI-TR-161, Dep artment of Informatics, Kyushu Uni- versity, 1999. [83] Yasuo Tabei, Hiroto Saigo, Yoshihiro Yamanishi, and Simon J Puglisi. Scalable partial least squares re- gression on grammar-compressed data matrices. InProceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 1875–1884, 2016. [84] Kedar Tatwawadi, Mikel Hernaez, Idoia Ochoa, and Tsachy Weis sman. Gtrac: fast retrieval from compressed collections of genomic variants. Bioinformatics, 32(17):i479–i486, 2016. [85] Virginia Vassilevska and Ryan Williams. Finding, minimizing, and countin g weighted subgraphs. In Proceedings of the forty-ﬁrst annual ACM symposium on Theor y of computing , pages 455–464, 2009. [86] Terry A. Welch. A technique for high-performance data compr ession. Computer, 6(17):8–19, 1984. [87] Till Westmann, Donald Kossmann, Sven Helmer, and Guido Moerko tte. The implementation and performance of compressed databases. ACM Sigmod Record , 29(3):55–67, 2000. [88] Ryan Williams. On the diﬀerence between closest, furthest, and orthogonal pairs: Nearly-linear vs barely-subquadratic complexity. In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms , pages 1207–1215. SIAM, 2018. [89] Virginia Vassilevska Williams. On some ﬁne-grained questions in algor ithms and complexity. In Pro- ceedings of the ICM , volume 3, pages 3431–3472. World Scientiﬁc, 2018. [90] Ian H Witten, Alistair Moﬀat, and Timothy C Bell. Managing gigabytes: compressing and indexing documents and images . Morgan Kaufmann, 1999. [91] Jacob Ziv and Abraham Lempel. A universal algorithm for sequen tial data compression. IEEE Trans- actions on Information Theory , 23(3):337–343, 1977. A Further Preliminaries For a sequence of vectorsv1, . . . , v ℓ, we let v1 v2 . . . v ℓ “ v1 ˝ v2 ˝ ¨ ¨ ¨ ˝ vℓ “ ⃝ ℓ i“ 1vi denote their concatenation. By the following observation, when proving a lower bound for a compr ession of size Θ pNγ q, the main task is to prove the upper bound n “ OpNγ q; the lower bound n “ ΩpNγ q can be ensured mechanically. Observation A.1. Let 0 ď γ ď 1. Given two N-dimensional vectors u, v of compressed size OpNγ q, we can compute two OpNq-dimensional vectors u1 , v1 of compressed size ΘpNγ q with the same inner product. Proof. Append 0Nγ using ΘpNγq additional rules to the encodings of u and v. The Strong kSUM Assumption To generalize the lower bound of Theorem 1.1 so that it works for an arbitrary relationship between compressed and uncompressed sizes, we will use an assumption about a generalized version of 3SUM. Deﬁnition A.2(The kSUM Problem). Given k sets A1, . . . , A k of m integers in t1, . . . , U u, decide if there are k numbers a1 P A1, . . . , a k P Ak such that a1 ` ¨ ¨ ¨ ` ak´ 1 “ ak. For all constant k ě 3 a simple meet-in-the-middle algorithm with hashing solves kSUM in Opmrk{2s q time, and no faster algorithm by mε factors, for any ε ą 0, is known to date, unless the universe size U is smaller than Opmrk{2s´ εq. This is because Fast Fourier Transform gives an Opm ` kU log Uq time algorithm [27]. It is conjectured that substantially faster algorithms do not e xist (e.g. in [4, 2]). 14The Strong kSUM Conjecture. For all constant k ě 3 it holds that: no algorithm can solve the kSUM problem with U “ Opmrk{2s q in Opmrk{2s´ εq time, where ε ą 0. Observe that this assumption is about all k ě 3 and therefore implies the Strong 3SUM conjecture as a special case. Intuitively, the reason this problem helps us give redu ctions where the vectors are much more compressible is that, compared to 3SUM, as k grows the ratio between the time complexity mk{2 and the input size m grows. B Vector Inner Product In this section, we prove the generalization of the lower bound of Theorem 1.1 to arbitrary relationships between compressed and uncompressed sizes of the vectors. Theorem B.1.Let 0 ă ε ă 1{3. Assuming the Strong kSUM conjecture for all constant k, the inner product of two N-dimensional vectors that are grammar-compressed to size n “ ΘpNεq cannot be computed in OpN1{3´ δq time, where δ ą 0. This result follows from the following stronger statement. Theorem B.2. Let k ě 3. Assuming the Strong kSUM conjecture, the inner product of two N-dimensional vectors that are grammar-compressed to size n “ ΘpN1{r 3k´ 4 2 s q cannot be computed in OpNp1{3` γk q´ δq time, where δ ą 0 and γk :“ # 2 3pk´ 1q , if k is odd, 4 9k´ 12 , if k is even. Observe that the above statement implies Theorem B.1: For any 0 ă ε ă 1{3, we choose k suﬃciently large such that 1 {r 3k´ 4 2 s ă ε. Then using Observation A.1, we obtain that any OpN1{3´ δ q-time algorithm for Vector Inner Product with compressed size n “ ΘpNεq would give an OpN1{3` γk ´ δ1 q-time algorithm for Vector Inner Product with compressed size OpN1{r 3k´ 4 2 s q “ OpNεq, where δ1 “ γk ` δ – this would refute the Strong kSUM conjecture by Theorem B.2. Furthermore, observe that if we set k “ 3, we obtain a ˜ΩpN2{3q lower bound for compressed size n “ ΘpN1{3q under the Strong 3SUM conjecture. In the remainder of this section, we give the proof of Theorem B.2. T he central construction is captured by the following lemma. Lemma B.3.Given sets A1, . . . , A k of integers in t1, . . . , U u, we deﬁne v1 A1`¨¨¨` Ak´ 1 :“ ⃝ pa1,...,ak´ 2qPA1ˆ¨¨¨ˆ Ak´ 2 in lexicographic order 0a1`¨¨¨` ak´ 2 vAk´ 1 0pk´ 2qU´ a1´¨¨¨´ ak´ 2 , v1 Ak :“ p vAk 0pk´ 2qU qmk´ 2 , where vAk´ 1 , vAk P t 0, 1uU denote the characteristic vectors of the sets Ak´ 1, Ak. We have the following properties: 1. The inner product of the mk´ 2pk ´ 1qU-dimensional vectors v1 A1`¨¨¨` Ak´ 1 and v1 Ak is nonzero if and only if there is a tuple pa1, . . . , a kq P A1 ˆ ¨ ¨ ¨ ˆ Ak with a1 ` ¨ ¨ ¨ ` ak´ 1 “ ak. 2. We can compute compressions of v1 A1`¨¨¨` Ak´ 1 , v1 Ak of size Opkm log Uq “ Opm log Uq in time Opm log Uq. Proof. For 1., observe that by construction,v1 A1`¨¨¨` Ak1 and v1 Ak consist of mk´ 2 blocks, indexed bypa1, . . . , a k´ 2q P A1 ˆ ¨ ¨ ¨ ˆ Ak´ 2 and consisting of the sequence 0a1`¨¨¨` ak´ 2 vAk´ 1 0pk´ 2qU´ a1´¨¨¨´ ak´ 2 and vAk 0pk´ 2qU of length pk ´ 1qU, respectively. In particular, in block pa1, . . . , a k´ 2q there is a common 1-entry t if and only if t “ p a1 ` a2 ` ¨ ¨ ¨ ` ak´ 2q ` a for some a P Ak´ 1 and t “ a1 for some a1 P Ak. Thus, there exists a common 1-entry in v1 A1`¨¨¨` Ak´ 2 and v1 Ak if and only if there are pa1, . . . , a kq P A1 ˆ ¨ ¨ ¨ ˆ Ak with a1 ` ¨ ¨ ¨ ` ak´ 1 “ ak. For 2., we ﬁrst recall that as shown in the proof of Theorem 1.1, we c an compute a compression of the characteristic vectors vAk´ 1 and vAk of size Opm log Uq in time Opm log Uq. Thus, using Proposition 2.1, we 15can compute a compression of v1 Ak “ p vAk 0pk´ 2qU qmk´ 2 of size Opm log Uq` Oplogppk´2qUqq` Oplog mk´ 2q “ Opm log Uq in time Opm log Uq. To show the claim for v1 A1`¨¨¨` Ak´ 1 , we proceed inductively and construct the strings v1 Ak´ 1 :“ vAk´ 1 and v1 Ai`¨¨¨` Ak´ 1 :“ ⃝ pai,...,ak´ 2qPAiˆ¨¨¨ˆ Ak´ 2 in lexicographic order 0ai`¨¨¨` ak´ 2 vAk´ 1 0pk´ 1´ iqU´ ai ´¨¨¨´ ak´ 2 , for i “ k ´ 2, . . . , 1. The central observation is that we can write Ai “ t apiq 1 , . . . , a piq m u with apiq 1 ă apiq 2 ă ¨ ¨ ¨ ă apiq m and obtain v1 Ai`¨¨¨` Ak´ 1 “ ⃝ m j“ 10apiq j v1 Ai` 1`¨¨¨` Ak´ 1 0U´ apiq j . Thus, given an SLP Gi` 1 for v1 Ai` 1`¨¨¨` Ak´ 1 with starting symbol Si` 1, we can give an SLP Gi for v1 Ai`¨¨¨` Ak´ 1 of size |Gi` 1| ` Opm log Uq as follows: For each j “ 1, . . . , m , we encode 0 apiq j using Oplog apiq j q “ Oplog Uq additional symbols, re-use Si` 1 to generate v1 Ai` 1`¨¨¨` Ak´ 1 , and encode 0 U´ apiq j using OplogpU ´ apiq j qq “ Oplog Uq additional symbols. Observe that we can obtain this compression in t ime Opm log Uq. Thus, starting from an SLP for v1 Ak´ 1 , after k ´ 2 steps we obtain an SLP G1 for v1 A1`¨¨¨` Ak´ 1 of size Opkm log Uq “ Opm log Uq. The running time of this construction is Opkm log Uq “ Opm log Uq, concluding the proof. Let A1, . . . , A k Ď t 1, . . . , U u be a Strong kSUM instance, i.e., U “ Opmrk{2s q. The reduction given in Lemma B.3 gives two vectors v, v1 of dimension mk´ 2 ¨ p k ´ 1qU such that their inner product allows us to decide the kSUM instance. Furthermore, the vectors have a compressed size of Opm log Uq. We slightly adaptv, v1 by appending 0’s to increase the dimension slightly toN “ mk´ 2¨pk´1qU logrp3k´ 4q{2s U (this does not change their inner product). We verify the following f acts: (1) an OpN1{3` γk ´ δq-time Vector Inner Product algorithm for some δ ą 0 refutes the Strong kSUM conjecture and (2) n “ OpN1{r 3k´ 4 2 s q. Using Observation A.1, this concludes the proof of Theorem B.2. For (1), consider ﬁrst the case that k is odd. Then U “ Opmpk` 1q{2q and N “ Opmk´ 2UpolylogUq “ Opm3pk´ 1q{2polylogmq. Observe that N1{3` γk´ δ “ Opm 3pk´ 1q 2 ¨p 1 3 ` 2 3pk´ 1q ´ δq polylogmq “ Opm k´ 1 2 ` 1´ 3pk´ 1q 2 δq “ Opmr k 2 s´ δ1 q, for any 0 ă δ1 ă 3pk ´ 1qδ{2. Similarly, for even k, we have U “ Opmk{2q and N “ Opmk´ 2UpolylogUq “ Opmp3k´ 4q{2polylogmq. Using 1{3 ` γk “ 1{3 ` 4{p9k ´ 12q “ k{p3k ´ 4q, we obtain that N1{3` γk ´ δ “ Opm 3k´ 4 2 ¨p k 3k´ 4 ´ δq polylogmq “ Opm k 2 ´ δ1 q, for any 0 ă δ1 ă p 3k ´ 4qδ{2. Thus, in both cases, an OpN1{3` γk ´ δq-time Vector Inner Product algorithm refutes the Strong kSUM conjecture by solving the given kSUM instance in time Opmrk{2s´ δ1 q with δ1 ą 0. Finally, for (2), note that N “ Opmk´ 2U logrp3k´ 4q{2s Uq “ Opmrp3k´ 4q{2s logrp3k´ 4q{2s mq. Thus n “ Opm log mq “ OpN1{rp3k´ 4q{2s q, as desired. C Matrix-Vector Product In this section we provide the full proof of Theorem 1.2. We ﬁrst prove a self-reduction for 3SUM as a central tool (using standard techniques), and then proceed to give the ﬁ nal reduction. C.1 Proof of the Self-Reduction Let us restate Lemma 4.1. 16Lemma C.1 (Self-Reduction for 3SUM) . Let 1 ď s “ spmq ď m and ε ą 0 be arbitrary. If there is an algorithm that, given a target t and L “ Oppm{sq2q sets Aℓ, Bℓ, Cℓ of s integers in t1, . . . , O ps3 log2 squ, determines for all 1 ď ℓ ď L whether there are a P Aℓ, b P Bℓ, c P Cℓ with a ` b ` c “ t in total time Opm2´ ǫq, then the 3SUM conjecture is false. In the remainder of this section, we give the proof. LetA, B, C be sets of m integers in t1, . . . , U u. We use a couple of results from earlier work that are stated for the following 3SUM formulation: given three sets A1 , B1 , C1 of m integers in t´U, . . . , U u with U “ Opm3 log2 mq, we are asked to determine whether there are a P A1 , b P B1 , c P C1 such that a ` b ` c “ 0. We ﬁrst reduce our formulation to this formulation by settingA1 :“ A, B1 :“ B, and C1 :“ ´ C “ t´ c | c P Cu. We can now use the following known self-reduction for 3SUM. Lemma C.2(Reformulated from [62, Theorem 13]) . Let s :“ spmq with 1 ď s ď m. Given three sets A1 , B1 , C1 of m integers in t´U, . . . , U u, we can compute, in time Opm2{sq, a list of L “ Oppm{sq2q 3SUM instances, i.e., sets A1 ℓ, B1 ℓ, C1 ℓ with 1 ď ℓ ď L, such that there is an a P A1 , b P B1 , c P C1 with a ` b ` c “ 0 if and only if there is an instance 1 ď ℓ ď L and a triple a P A1 ℓ, b P B1 ℓ, c P C1 ℓ with a ` b ` c “ 0. Furthermore, each A1 ℓ, B1 ℓ, C1 ℓ is a subset of s integers of A1 , B1 , C1 , respectively. Proof sketch. We give the high-level arguments (for details, see the proof of The orem 13 in [62]). For a set S, let min S and max S denote the smallest and largest element in S, respectively. We sort A1 , B1 , C1 and split each array into rm{ss consecutive parts A1 1, . . . , A 1 rm{ss , B1 1, . . . , B 1 rm{ss , C1 1, . . . , C 1 rm{ss , each of at most s elements, such that max A1 i ă min A1 i` 1,max B1 i ă min B1 i` 1 and max C1 i ă min C1 i` 1 for all i. Instead of searching for a 3SUM triple a P A1 i, b P B1 j , c P C1 k for each 1 ď i, j, k ď rm{ss (i.e., Θppm{sq3q subproblems with s elements each), one observes that most subproblems can be trivially solved: We say that a subproblem pi, j, k q is trivial, if min Ai ` min Bj ` min Ck ą 0 or max Ai ` max Bj ` max Ck ă 0; these subproblems cannot contain a solution. The key insight is that there are at mostOppm{sq2q non-trivial subproblems (which follows since the domination partial ordering on t1, . . . , u u3 has at most Opu2q incomparable elements); these can be determined in time Oppm{sq2q. Thus, it suﬃces to list all Oppm{sq2q non-trivial subproblems with s integers in each set in time Opm2{sq. The resulting instances A1 ℓ, B1 ℓ, C1 ℓ consist of integers in t´U, . . . , U u with large universe size U “ Opm3 log2 mq. We reduce the universe size to Ops3 log2 sq using a folklore technique (a slightly stronger result with U “ Ops3q can be achieved using the techniques of [15]). To prepare notation, for any set S, we let S mod p :“ t s mod p | s P Su. Lemma C.3 (Adaptation of [5, Lemma B.1]) . There is some α such that U1 :“ αs3 log s log U satisﬁes the following property: Let A, B, C be sets of s integers in t´U, . . . , U u such that no a P A, b P B, c P C satisﬁes a ` b ` c “ 0. Let p be a prime chosen uniformly at random from t2, . . . , U 1 u. Then the probability that there are ap P A mod p, bp P B mod p, cp P C mod p with ap ` bp ` cp ” 0 pmod pq is at most 1{2. Proof. Let a P A, b P B, c P C be arbitrary. Since a`b`c ‰ 0, note that pa mod pq`pb mod pq`pc mod pq ” 0 pmod pq if and only if p divides a ` b ` c. Since a ` b ` c P t´ 3U, . . . , 3Uu, a ` b ` c has at most log 2p3Uq prime factors. Let P denote the number of prime numbers in t2, . . . , U 1 u; by the prime number theorem we can choose α large enough such that P ě 2s3 log2p3Uq. Thus, the probability that p was chosen among these at most log 2p3Uq prime factors is at most log 2p3Uq{P ď 1{p2s3q. Thus, by a union bound over all s3 triples a P A, b P B, c P C, the probability that there are ap P A mod p, bp P B mod p, cp P C mod p with a ` b ` c ” 0 pmod pq is at most 1 {2. Note that if A, B, C contain a triple a, b, c with a ` b ` c “ 0, then also A mod p, B mod p, C mod p contain a triple ap, bp, cp with ap ` bp ` cp ” 0 pmod pq for any p. We can ﬁnally prove Lemma C.1: Assume that there is an algorithm A that given a target t and L “ Oppm{sq2q instances Aℓ, Bℓ, Cℓ, 1 ď ℓ ď L of s integers in t1, . . . , U 1 u, determines for all 1 ď ℓ ď L whether there are a P Aℓ, b P Bℓ, c P Cℓ with a ` b ` c “ t in total time Opm2´ εq with ε ą 0. Observe that since A runs in time Opm2´ εq, we must have s “ Ωpmεq, since otherwise already the size of the input to A of Θpm2{sq would be ωpm2´ εq. Thus, we have U1 “ Ops3 log2 sq. For r “ 1, . . . , γ log m many repetitions, we do the following: We choose a random prime pr P r 2, U 1s and obtain ℓ instances in t0, . . . , p r ´ 1u Ď t 0, . . . , U u by taking the sets modulo pr, i.e., Aprq ℓ :“ A1 ℓ mod pr, 17Bprq ℓ :“ B1 ℓ mod pr, and Cprq ℓ “ C1 ℓ mod pr. Observe that we may determine whether there is some a P Aprq ℓ , b P Bprq ℓ , c P Cprq ℓ with a ` b ` c ” 0 pmod prq by testing for each t P t 0, pr, 2pru, whether there a P Aprq ℓ , b P Bprq ℓ , c P Cprq ℓ with a ` b ` c “ t. Thus, to do this, and additionally ensure that each integer is in t1, . . . , U 1 u, we add 1 to each integer in Aprq ℓ , Bprq ℓ , Cprq ℓ and for each λ P t 0, 1, 2u, call A on the sets Aprq ℓ , Bprq ℓ , Cprq ℓ , 1 ď ℓ ď L with common target tλ :“ 3 ` λpr. Observe that after these 3 γ log m calls to A, we know for each 1 ď ℓ ď L and 1 ď r ď γ log m whether there are a P A1 ℓ, b P B1 ℓ, c P C1 ℓ with a ` b ` c ” 0 pmod prq. We declare our original 3SUM instance A, B, C to be a YES instance if and only if there is some ℓ such that for all r we have found a witness a P A1 ℓ, b P B1 ℓ, c P C1 ℓ with a ` b ` c ” 0 pmod prq. Note that if A, B, C is a YES instance, we always return YES by Lemma C.2. Otherwise, if A, B, C is a NO instance, consider a ﬁxed ℓ. By Lemmas C.2 and C.3, the probability that for all r, we ﬁnd a P A1 ℓ, b P B1 ℓ, c P C1 ℓ with a ` b ` c ” 0 pmod prq is bounded by 2´ γ log m “ m´ γ. Thus, by a union bound over all ℓ, the probability that we incorrectly return YES in this case is at most Lm´ γ “ Oppm{sq2m´ γq “ Opm2´ γ q. We can make this error probability polynomially small by choosing γ ą 2. Observe that the running time of the above process is Oplog mq times the running time of A (note that the running time used for Lemma C.2 is linear in its output size, which is th e input size of A and thus dominated by the running time of A). Thus, we can solve any 3SUM instance in time Opm2´ ε log mq, which would refute the 3SUM conjecture. This concludes the proof of Le mma C.1. C.2 Main Reduction for Matrix-Vector Multiplication We now turn to the proof of Theorem 1.2. Proof.Let s be a parameter to be chosen later. By Lemma 4.1, it suﬃces to solve L “ Oppm{sq2q 3SUM instances Aℓ, Bℓ, Cℓ consisting of s integers in t1, . . . , U u, U “ Ops3 log2 sq with common target 1 ď t ď 3U in time Opm2´ ǫq for some ǫ ą 0 to contradict the 3SUM conjecture. We construct an pL ˆ 3s2Uq matrix M and v P t 0, 1u3s2U as follows. Intuitively, each row Mℓ and the vector v are partitioned into s2 blocks of size 3 U. Each block is indexed by pi, jq with i, j P t 1, . . . , s u in lexicographic order and the block of Mℓ corresponding to pi, jq encodes the characteristic vector of the set ai ` bj ` Cℓ “ t ai ` bj ` c | c P Cℓu Ď t 1, . . . , 3Uu, where ai is the i-th integer in Aℓ and bj is the j-th integer in Bℓ. Correspondingly, every block pi, jq in v encodes the characteristic vector of the singleton set ttu Ď t 1, . . . , 3Uu. Thus, there is a position in block pi, jq in which both Mℓ and v have a 1 if and only if there is a c P Cℓ such that ai ` bj ` c “ t. Formally, for any 1 ď ℓ ď L, we write Aℓ “ t aℓ 1, . . . , a ℓ su, Bℓ “ t bℓ 1, . . . , b ℓ su and deﬁne Mℓ :“ 0a1` b1 vCℓ 03U´ a1´ a2 looooooooooomooooooooooon vaℓ 1` bℓ 1` Cℓ . . . 0ai` bj vCℓ 03U´ ai´ bj looooooooooomooooooooooon vaℓ i` bℓ j` Cℓ . . . 0as` bs vCℓ 03U´ as´ bs looooooooooomooooooooooon vaℓ s` bℓ s` Cℓ , v :“ 0t´ 1103U´ t . . . 0t´ 1103U´ t . . . 0t´ 1103U´ t, where vCℓ P t 0, 1uU denotes the characteristic vector of Cℓ. By this structure, it is clear that Mℓv ě 1 if and only if there are a P Aℓ, b P Bℓ, c P Cℓ with a ` b ` c “ t. We will show that each row Mℓ can be compressed to size Θ ps log sq (as opposed to its RLE of length Θps3 log sq). We thus will set N “ r3s2U log3 ss “ Θps5 log5 sq, and append 0 N´ 3s2U to each row Mℓ and v, so that we obtain an Lˆ N matrix M1 and N-dimensional vector v1 whose product M1 v1 can be used to solve all instances Aℓ, Bℓ, Cℓ in linear time. Observe that each row has a compression of size Θ pN1{5q “ Θps log sq, as desired. Since L “ Oppm{sq2q and N ě s5, we can set s “ Θpm2{7q such that L ď N (we can indeed make L “ N by introducing zero rows, if necessary). Thus, an OpNn2´ ǫq-time algorithm for multiplying M1 and v1 would solve all L 3SUM instances in time OpNn2´ ǫq “ Oppm{sq2ps log sq2´ ǫq “ Oppm2{sǫqpolylogsq “ Opm2´ 2 7 ǫpolylogmq, which would refute the 3SUM conjecture. Analogous to the proof of Theorems 1.1 and B.2, we can compute a co mpression of size Θ ps log sq in time Ops log sq. Indeed, for each Mℓ, this already follows from Lemma B.3 when setting A1 :“ Aℓ, A2 :“ 18Bℓ, A3 :“ Cℓ, which shows how to compress the string v1 A1` A2` A3 “ Mℓ to size Ops log Uq “ Ops log sq in time Ops log Uq “ Ops log sq. For v, we simply apply Proposition 2.1 to the straightforward compression of 0t´ 1103U´ t to size Oplog Uq, which leads to a compression of v of size Oplog U ` log sq “ Oplog sq. Using Observation A.1, we can make all encodings have size Θ ps log sq, which concludes the proof. D Matrix-Matrix Product In this section, we give the full proof of Theorem 5.1. Proof of Theorem 5.1.Let ℓ P N. We ﬁrst deﬁne the matrices A1 , B1 where A1 is a p2ℓ ˆ 2ℓq matrix with rows indexed by strings x P t 0, 1uℓ in lexicographic order, and B1 is a p2ℓ ˆ 2ℓp2ℓqq matrix with columns indexed by py, kq P t 0, 1uℓ ˆ t 1, . . . , 2ℓu in lexicographic order. For arbitrary z P t 0, 1uℓ, let diag pzq denote the ℓ ˆ ℓ diagonal matrix with z on the diagonal. We deﬁne A1 x :“ p x | 1ℓq, B 1 py,1q,...,py,2ℓq :“ ˆ diagp1ℓq 0 0 diagpyq ˙ . Let C1 “ A1 B1 be the p2ℓ ˆ 2ℓp2ℓqq product matrix of A1 and B1 , with rows and columns indexed by t0, 1uℓ and t0, 1uℓ ˆ t 1, . . . , 2ℓu, respectively. Observe that by deﬁnition, pCx,py,1q , . . . , C x,py,2ℓqq “ p x | yq for any x, y P t 0, 1uℓ. In particular, when we view C1 as a 22ℓp2ℓq-length string, it contains all strings in t0, 1u2ℓ as substrings, thus by Lemma 5.2, any row-wise compression is of siz e at least 2 2ℓ{p2ℓq. To also ensure column-wise incompressibility, we slightly extend the construction by analogous transposed constructions: We let N :“ 2ℓp2ℓ ` 1q and deﬁne the ﬁnal pN ˆ Nq matrices A, B as follows: A :“ ˆ A1 0 0 0 B1T 0 ˙ , B :“ ¨ ˝ B1 0 0 A1T 0 0 ˛ ‚ . Since C :“ AB “ ˆ A1 B1 0 0 pA1 B1 qT ˙ contains all length-p2ℓq strings as substrings of the rows (in the A1 B1 part) and as substrings of the columns (in the pA1 B1 qT part), any strong compression of C is of size at least 22ℓ{p2ℓq “ ΩpN{ log2 Nq, proving the third part of the claim. For the ﬁrst two parts, it remains to show that A and B can be well compressed: For the convenient compression, we observe that any row in A is either of the form px1ℓ | 02ℓ | 0N´ 4ℓq, which has a RLE of length at most |x1ℓ| ` Oplog Nq “ Oplog Nq, or it is of the form p02ℓ | 0i´ 1α02ℓ´ i | 0N´ 4ℓq for some α P t 0, 1u, i P t 1, ..., 2ℓu, which also has a RLE of length at most Oplog Nq. Thus, each of the N rows of A can be compressed to size Oplog Nq, as desired. By a symmetric statement, also each column of B has a RLE of size Oplog Nq. Finally, for the strong compression, we show that we compress AT when viewed as a string, i.e., we compress the concatenation of the columns of A. The main insight is the following: Imagine a binary ℓ- bit counter. Using grammar compression, we can compress the seq uence of values of any ﬁxed bit while the counter counts from 0 to 2 ℓ ´ 1 in size Opℓq. Formally, let G0, G1 be grammar compressions of strings s0,s1. For any 1 ď i ď ℓ, we can encode ps2ℓ´ i 0 s2ℓ´ i 1 q2i´ 1 using only Opℓq additional non-terminals in the canonical way. Speciﬁcally, using Opℓ ´ iq new symbols, we may encode s2ℓ´ i 0 s2ℓ´ i 1 ; let ˜S denote the corresponding non-terminal. We then encode ˜S2i´ 1 using Opiq additional new symbols. In total, we only need Oppℓ ´ iq ` iq “ Opℓq additional symbols, as desired. We apply the above idea to encode the concatenation all columns of A as follows: Consider column i. • For 1 ď i ď ℓ, then by the chosen lexicographic order of the row indices x P t 0, 1uℓ of A1 , note that the i-th column of A is of the form p02ℓ´ i 12ℓ´ i q2i´ 1 | 0N´ 2ℓ . Using the above analysis, we can compress it to size Opℓq ` Oplog Nq “ Oplog Nq. • If ℓ ` 1 ď i ď 2ℓ, the i-th column is of the form 1 2ℓ | 0N´ 2ℓ , which we can compress to size Oplog ℓ ` log Nq “ Oplog Nq. 19• If 2 ℓ ` 1 ď i ď 3ℓ, write i “ 2ℓ ` i1 and observe that the i-th column of A is of the form 0 2ℓ | p0i1 ´ 110ℓ´ i1 q2ℓ . Using Opℓq non-terminals to encode 0 i1 ´ 110ℓ´ i1 , it is immediate that we can compress the complete column using Opℓq additional non-terminals, i.e., yielding a total of Opℓq “ Oplog Nq. • If 3ℓ ` 1 ď i ď 4ℓ, write i “ 3ℓ ` i1 and observe that by the chosen lexicographic order of the column indices py, kq P t 0, 1uℓ ˆ t 1, . . . , 2ℓu of B1 , the i-th column of A is of the form 0 2ℓ | p s2ℓ´ i1 0 s2ℓ´ i1 1 q2i1 ´ 1 where sα :“ 0i1 ´ 1α1ℓ´ i1 . We can give trivial grammars of size Opℓq for s0, s1. Then, by the above analysis, we only need Opℓq additional non-terminals for the counter-like part. In total, we on ly need Opℓq “ Oplog Nq non-terminals to encode the i-th column. • Finally, observe that the remaining columns i “ 4ℓ ` 1, . . . , N consist of pN ´ 4ℓqN zeroes, which we can encode together using only Oplog Nq non-terminals. In summary, we can encode the ﬁrst 4 ℓ columns using Oplog Nq non-terminals each, and only Oplog Nq non-terminals for the remaining columns, so we can fully compress th e concatenation of A’s columns to size Oplog2 Nq, as claimed. 20",
      "meta_data": {
        "arxiv_id": "2010.14181v1",
        "authors": [
          "Amir Abboud",
          "Arturs Backurs",
          "Karl Bringmann",
          "Marvin Künnemann"
        ],
        "published_date": "2020-10-27T10:33:01Z",
        "pdf_url": "https://arxiv.org/pdf/2010.14181v1.pdf"
      }
    },
    {
      "title": "Online Learned Continual Compression with Adaptive Quantization Modules",
      "abstract": "We introduce and study the problem of Online Continual Compression, where one\nattempts to simultaneously learn to compress and store a representative dataset\nfrom a non i.i.d data stream, while only observing each sample once. A naive\napplication of auto-encoders in this setting encounters a major challenge:\nrepresentations derived from earlier encoder states must be usable by later\ndecoder states. We show how to use discrete auto-encoders to effectively\naddress this challenge and introduce Adaptive Quantization Modules (AQM) to\ncontrol variation in the compression ability of the module at any given stage\nof learning. This enables selecting an appropriate compression for incoming\nsamples, while taking into account overall memory constraints and current\nprogress of the learned compression. Unlike previous methods, our approach does\nnot require any pretraining, even on challenging datasets. We show that using\nAQM to replace standard episodic memory in continual learning settings leads to\nsignificant gains on continual learning benchmarks. Furthermore we demonstrate\nthis approach with larger images, LiDAR, and reinforcement learning\nenvironments.",
      "full_text": "Online Learned Continual Compression with Adaptive Quantization Modules Lucas Caccia 1 2 3 Eugene Belilovsky 2 4 Massimo Caccia 2 4 5 Joelle Pineau 1 2 3 Abstract We introduce and study the problem of Online Continual Compression, where one attempts to simultaneously learn to compress and store a rep- resentative dataset from a non i.i.d data stream, while only observing each sample once. A naive application of auto-encoders in this setting en- counters a major challenge: representations de- rived from earlier encoder states must be usable by later decoder states. We show how to use discrete auto-encoders to effectively address this challenge and introduce Adaptive Quantization Modules (AQM) to control variation in the com- pression ability of the module at any given stage of learning. This enables selecting an appropriate compression for incoming samples, while taking into account overall memory constraints and cur- rent progress of the learned compression. Unlike previous methods, our approach does not require any pretraining, even on challenging datasets. We show that using AQM to replace standard episodic memory in continual learning settings leads to sig- niﬁcant gains on continual learning benchmarks. Furthermore we demonstrate this approach with larger images, LiDAR, and reinforcement learn- ing agents. 1. Introduction Interest in machine learning in recent years has been fueled by the plethora of data being generated on a regular basis. Effectively storing and using this data is critical for many applications, especially those involving continual learning. In general, compression techniques can greatly improve data storage capacity, and, if done well, reduce the memory and compute usage in downstream machine learning tasks (Gueguen et al., 2018; Oyallon et al., 2018). Thus, learned compression has become a topic of great interest (Theis 1McGill 2Mila 3Facebook AI Research 4University of Montreal 5ElementAI. Correspondence to: Lucas Caccia <lucas.page-caccia@mail.mcgill.ca>, Eugene Belilovsky <eu- gene.belilovsky@umontreal.ca>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). et al., 2017; Ball´e et al., 2016; Johnston et al., 2018). Yet its application in reducing the size of datasets bound for machine learning applications has been limited. This work focuses on the following familiar setting: new training data arrives continuously for a learning algorithm to exploit, however this data might not be iid, and further- more there is insufﬁcient storage capacity to preserve all the data uncompressed. We may want to train classiﬁers, reinforcement learning policies, or other models continu- ously from this data as it’s being collected, or use samples randomly drawn from it at a later point for a downstream task. For example, an autonomous vehicle (with bounded memory) collects large amounts of high-dimensional train- ing data (video, 3D lidar) in a non-stationary environment (e.g. changing weather conditions), and overtime applies an ML algorithm to improve its behavior using this data. This data might be transferred at a later point for use in down- stream learning. Current learned compression algorithms, e.g. (Torfason et al., 2018), are not well designed to deal with this case, as their convergence speed is too slow to be usable in an online setting. In the ﬁeld of continual/lifelong learning (Thrun & Mitchell, 1995), which has for now largely focused on classiﬁcation, approaches based on storing memories for later use have emerged as some of the most effective in online settings (Lopez-Paz et al., 2017; Aljundi et al., 2018; Chaudhry et al.; 2019; Aljundi et al., 2019). These memories can be stored as is, or via a generative model (Shin et al., 2017). Then, they can either be used for rehearsal (Chaudhry et al., 2019; Aljundi et al., 2019) or for constrained optimization (Lopez-Paz et al., 2017; Chaudhry et al.; Aljundi et al., 2018). Indeed many continual learning applications would be greatly improved with replay approaches if one could afford to store all samples. These approaches are however inherently limited by the amount of data that can be stored Learning a generative model to compress the previous data stream thus seems like an appealing idea. However, learn- ing generative models, particularly in the online and non- stationary setting, continues to be challenging, and can greatly increase the complexity of the continual learning task. Furthermore, such models are susceptible to catas- trophic forgetting (Aljundi et al., 2019). An alternate ap- proach is to learn a compressed representation of the data, arXiv:1911.08019v3  [cs.LG]  20 Aug 2020Online Learned Continual Compression with Adaptive Quantization Modules Online Continual Compression z_1 z_1 z_2 z_3z_1 z_2 X ~ non-iid t=1 t=2 t=3 query  samples query  samples Learner Learner Encoder Encoder Encoder Decoder     t=3 Decoder     t=1 Decoder     t=2 storage storage storage  Decoder     t=3 z_1 Decoder     t=1 Representation Drift z_1 X_3 X_2 X_1 Figure 1.Illustration of the challenges in the Online Continual Compression problem. A model must be able to decode representations encoded by previous versions of the autoencoder, permitting anytime access to data for the learner. This must be accomplished while dealing with a time-varying data distribution and ﬁxed memory constraints which can be more stable than learning generative models. Learned compression in the online and non-stationary set- ting itself introduces several challenges illustrated in Fig 1. Firstly the learned compression module must be able to de- code representations encoded by earlier versions of itself, introducing a problem we refer to as representation drift. Secondly, the learned compressor is itself susceptible to catastrophic forgetting. Finally, the learned compression needs to be adaptive to maintain a prescribed level of re- construction quality even if it has not fully adapted to the current distribution. In this work we demonstrate that the VQ-V AE framework (van den Oord et al., 2017; Razavi et al., 2019), originally introduced in the context of generative modeling and density estimation, can be used online to effectively address repre- sentation drift while achieving high compression. Further- more, when augmented with an internal replay mechanism it can overcome forgetting. Finally we use propose to use multiple gradient-isolated compression levels to allow the compressor to adaptively store samples at different compres- sion scales, based on the amount of data, storage capacity, and effectiveness of the model in compressing samples. The main contributions of this work are: (a) we intro- duce and highlight the online learned continual compres- sion (OCC) problem and its challenges. (b) We show how representation drift, one of the key challenges, can be tackled by effective use of codebooks in the VQ-V AE framework. (c) We propose an architecture using multi- ple VQ-V AE’s, adaptive compression scheme, stream sam- pling scheme, and self-replay mechanism that work together to effectively tackle the OCC problem. (d) We demon- strate this can yield state-of-the-art performance in stan- dard online continual image classiﬁcation benchmarks and demonstrate the applications of our OCC solution in a va- riety of other contexts. The code to reproduce our results is available at https://github.com/pclucas14/ adaptive-quantization-modules 2. Related Work Learned compression has been recently studied for the case of image compression. Works by Theis et al. (2017); Ball´e et al. (2016); Johnston et al. (2018) have shown learned compressions can outperform standard algorithms like JPEG. These methods however are difﬁcult to adapt for online settings as they do not directly address the challenges of the OCC problem (e.g. representation drift). Continual Learning research currently focuses on over- coming catastrophic forgetting (CF) in the supervised learn- ing setting, with some limited work in the generative mod- eling and reinforcement learning settings. Most continual learning methods can be grouped into three major families. Some algorithms dynamically change the model’s architec- ture to incorporate learning from each task separately (Rusu et al., 2016; Li & Hoiem, 2018; Fernando et al., 2017). Al- though these methods can perform well in practice, their introduction of task-speciﬁc weights requires growing com- pute and memory costs which are problematic for the online setting. Another set of techniques employ regularization to constrain weights updates in the hope of maintaining knowl- edge from previous tasks. Notable methods in this class include (Kirkpatrick et al., 2017; Husz´ar, 2017; Zenke et al., 2017; Nguyen et al., 2017; Chaudhry et al., 2018). However, this set of approaches has been shown to be inefﬁcient in the online setting (Chaudhry et al., 2019). The last family of methods encapsulates all that have a mech- anism to store information about the previous data distribu- tions. This memory then serves as a tool for the continual learner to rehearse previous tasks. The simplest instantia- tion of this method is to keep and sample from a buffer of old data to retrain the model after every update (Chaudhry et al., 2019). This approach is widely used in RL where it is known as Experience Replay (ER) (Lin, 1993; Mnih et al., 2013; Andrychowicz et al., 2016). Another method, known as Generative Replay (GR) (Shin et al., 2017), uses generative modeling to store past task distributions. The con- tinual learner then trains on generated samples to alleviateOnline Learned Continual Compression with Adaptive Quantization Modules CF. Other notable examples are Gradient Episodic Memory (GEM) (Lopez-Paz et al., 2017), iCarl (Rebufﬁ et al., 2017), and Maximally Interfered Retrieval (MIR) (Aljundi et al., 2019), as well as (Aljundi et al., 2018; Hu et al., 2018). Most closely related to our work, Riemer et al. (2017) consider compressing memories for use in the continual classiﬁcation task. They also employ a discrete latent variable model but with the Gumbel approximation, which shows to be less effective than our approach. Furthermore a separate ofﬂine iid pre-training step for the learned compression is required in order to surpass the ER baseline, distinctly different from the online continual compression we consider. Lidar compression is considered in (Tu et al., 2019) and (Caccia et al., 2018). Both approaches use a similar projec- tion from 3D (x,y,z ) coordinates to 2D cylindrical coordi- nates, and leverage deep generative models to compress the data. However, neither accounts for potential distribution shift, nor for online learning. In this work we show that us- ing this 2D projection in conjunction with our model allows us to mitigate the two issues above for lidar data. 3. Methodology In this section we outline our approach to the online con- tinual compression problem. First we will review the VQ- V AE and highlight the properties making it effective for representational drift. Then we will describe our adaptive architecture, storage, and sampling scheme. 3.1. Problem Setting: Online Continual Compression We consider the problem setting where a stream of samples x ∼Dt arrives from different distributions Dt changing over time t= 1...T . We have a ﬁxed storage capacity ofC bytes where we would like to store the most representative information from all data distributions D1,...DT . There is notably a trade-off in quality of information versus the amount of samples stored. We propose to use a learned compression model, and most crucially, this model must also be stored within the Cbytes, to encode and decode the data samples. Another critical requirement is that at anytime tthe content of the storage (data and/or compression model) be usable for downstream applications. An important chal- lenge, illustrated in Figure 1, is that the learned compression module will change over time, while we still need to be able to decode the memories in storage. 3.2. Vector Quantized V AE for Online Compression The VQ-V AE is a discrete auto-encoder which relies on a vector quantization step to obtain discrete latent repre- sentations. An embedding table, E ∈RK×D consisting of K vectors of size D, is used to quantize encoder out- puts. Given an input (e.g. an RGB image), the encoder ﬁrst encodes it as a Hh ×Wh ×D tensor, where Hh Encoder     t=1      Encoder     t=3      same after VQ Vector Quantization different codebook Figure 2.Illustration of reduced representation drift from Vector Quantization and Wh denote the height and width of the latent repre- sentation. Then, every Ddimensional vector is quantized using a nearest-neighbor lookup on the embedding table. Speciﬁcally, zij = arg mine∈E ||enc(x)ij −e||2, where i,j refers to a spatial location. The output of the quanti- zation step is then fed through the decoder. The gradient of this non-differentiable step is approximated using the straight-through estimator. An important property to notice is that to reconstruct the input, only the Hh ×Wh indices are required, thus yielding high compression (van den Oord et al., 2017). Critically, the embedding tables are updated independently from the encoder and decoder, namely by minimizing mine ||sg[enc(x)ij] −e||, where sg is the stop gradient operator. Observe in the case of online compression, if the embedding table is ﬁxed, then a change in the encoder parameters and therefore a change in the encoder output for a given input will not change the ﬁnal quantized representation z, unless it is sufﬁciently large, thus we can observe that if the em- beddings change slowly or are ﬁxed we can greatly improve our control of the representational drift. This effect is illus- trated in Figure 2. On the other hand we do need to adapt the embedding table, since randomly selected embeddings would not cover well the space of encoder outputs. 3.3. Adaptive Quantization Modules To address issues of how to optimize storage and sampling in the context of Online Continual Compression we intro- duce Adaptive Quantization Modules (AQM). We use AQM to collectively describe the architecture, adaptive multi-level storage mechanism, and data sampling method used. To- gether they allow effectively constraining individual sample quality and memory usage while keeping in storage a faith- ful representation of the overall distribution.Online Learned Continual Compression with Adaptive Quantization Modules Algorithm 1: AQM L EARNING WITH SELF -REPLAY Input: Learning rate α, EXTERNAL LEARNER 1 Initialize: AQM Memory M; AQM Parameters θaqm 2 for t∈1..T do 3 % Fetch data from current task 4 for Binc ∼Dt do 5 for n∈1..N do 6 B←Binc 7 if t> 1 then 8 % Fetch data from buffer 9 Bre ∼SAMPLE (M,θaqm) 10 B ←(Binc,Bre) 11 end 12 % Update AQM 13 θaqm ←ADAM(θaqm,B,α ) 14 % Send data to external learner 15 EXTERNAL LEARNER (B) 16 if t> 1 then UPDATE BUFFER REP(M,θaqm) 17 %Save current indices 18 ADDTOMEMORY (M,Binc,θaqm) 19 end 20 end 21 end AQM uses an architecture consisting of a sequence of VQ- V AEs, each with a buffer. The AQM approach to online continual compression is overall summarized in Algorithm 1 (note ADDTOMEMORY is described in Appendix A). In- coming data is added to storage using an adaptive compres- sion scheme described in Algorithm 2. Incoming data is also used along with randomly sampled data from storage (self- replay) to update the current AQM model. The randomly sampled data also updates its representation in storage as per Algorithm 3. As illustrated by the optional lines in blue, Algorithm 1 can run concurrently with a downstream learn- ing task (e.g. online continual classiﬁcation) which would use the same batch order. It can also be run independently as part of a data collection. In the sequel we give further details on all these elements 3.3.1. A RCHITECTURE AND TRAINING Each AQM module contains a VQ-V AE and a corresponding buffer of adaptive capacity. A diagram of the architecture is given in Figure 3. We will denote the output after quan- tization of each module i as zi q and the set of codebook indexes used to obtain zi q as ai. Note that ai are the discrete representations we actually store. Each subsequent module produces and stores an ai requiring fewer bits to represent. For RGB images, the compression rate at a given level is given by H×W×3×log2(256) Nc×Hhi×Whi×⌈log2 (Ki)⌉. Here Ki is the number of embeddings in the codebooks, ( Hhi, Whi) the spatial dimension of the latent representation and Nci the number of codebooks. Algorithm 2: ADAPTIVE COMPRESS Input: datapoint x, AQM with Lmodules, threshold dth 1 % Forward all modules, store encodings 2 {zi q,ai}i=1..L= ENCODE (x) 3 for i∈L...1 do 4 % Decode from level i to output space 5 ˆx= DECODE (zi q) 6 % Check reconstruction error 7 if MSE(ˆx,x) <dth then return ai, i 8 end 9 % Otherwise, return original input 10 return x,0 Algorithm 3: UpdateBufferRep Input: Memory M, AQM with Llevels, data D, distortion threshold dth 1 for x∈Ddo 2 hidx, blockid = ADAPTIVE COMPRESS (x, AQM, dth) 3 % Delete Old Repr. 4 DELETE (M[x]) 5 % Add new one 6 ADD(M, hidx) 7 end VQV AE-2 (Razavi et al., 2019) also uses a multi-scale hier- archical organization, where unlike our AQM, the top level models global information such as shape, while the bottom level, conditioned on the top one, models local information. While this architecture is tailored for generative modeling, it is less attractive for compression, as both the bottom and top quantized representations must be stored for high qual- ity reconstructions. Furthermore in AQM each module is learned in a greedy manner using the current estimate of z(i−1) q without passing gradients between modules similar to (Belilovsky et al., 2019; Nøkland & Eidnes, 2019). A subsequent module is not required to build representations which account for all levels of compression, thus minimiz- ing interference across resolutions. This allows the modules to each converge as quickly as possible with minimal drift at their respective resolution, particularly important in the online continual learning case. 3.3.2. M ULTI -LEVEL STORAGE Our aim is to store the maximum number of samples in an allotted Cbytes of storage, while assuring their quality, and our ability to reconstruct them. Samples are thus stored at different levels based on the compressors’ current ability. The process is summarized in Algorithm 2. Such an approach is particularly helpful in the online non- stationary setting, allowing knowledge retention before the compressor network has learned well the current distribution. Note in Alg. 2 samples can be completely uncompressed until the ﬁrst module is able to effectively encode them. This can be crucial in some cases, if the compressor has not yet converged, to avoid storing poorly compressed rep-Online Learned Continual Compression with Adaptive Quantization Modules Figure 3.Architecture of Adaptive Quantization Modules. Each level uses its own loss and maintains its own replay buffer. Yello dotted lines indicate gradient isolation between modules resentations. Further taking into account that compression difﬁculty is not the same for all datapoints, this allows use of more capacity for harder data, and fewer for easier. We also note, since we maintain stored samples at each mod- ule and the modules are decoupled, that such an approach allows to easily distribute training in an asynchronous man- ner as per Belilovsky et al. (2019). 3.3.3. S ELF -REPLAY AND STREAM SAMPLING As shown in Alg. 1 our AQM is equipped with an internal experience replay mechanism (Mnih et al., 2013), which reconstructs a random sample from storage and uses it to perform an update to the AQM modules, while simultane- ously freeing up overall memory if the sample can now be compressed at a later AQM module. This has the effect of both reducing forgetting and freeing memory. In practice we replay at the same rate as incoming samples arrive and thus replay will not increase asymptotic complexity of the online learning. Finally, for efﬁciency the replay can be coupled to an external online learner querying for random samples from the overall memory. Since we would like the AQM to work in cases of a ﬁxed memory capacity it must also be equipped with a mecha- nism for selecting which samples from the stream to store and which to delete from memory. Reservoir Sampling (RS) is a simple yet powerful approach to this problem, used successful in continual learning (Chaudhry et al., 2019). It adds a sample from the stream with prob. p= buffer capacity points seen so far while remove a random sample. However, RS is not directly compatible with AQM primarily because the amount of sam- ples that can be stored varies over time. This is because samples at different levels have different memory usage and memory can be freed by replay. We thus propose an alternative scheme, which maximally ﬁlls available memory and selects non-uniformly samples for deletion. Speciﬁ- cally when a larger amount of samples are added at one point in the stream, they become more likely to be removed. The details of this stream sampling method are provided in Appendix A. 3.3.4. DRIFT CONTROL VIA CODEBOOK STABILIZATION As mentioned previously, a good online compressor must control its representational drift, which occurs when up- dates in the auto-encoder parameters creates a mismatch with the static representations in the buffer. Throughout the paper we measure representational drift by compar- ing the following time varying quantity: DRIFT t(z) = RECON ERR ( Decode(θt; z),x ) where θt the model pa- rameters at timetand (z,x) is a stored compressed represen- tation and its original uncompressed datapoint respectively. For all experiments on images, RECON ERR is simply the mean squared error. As illustrated in Sec 3.2 a slow changing codebook can allow to control drifting representations. This can be in part accomplished by updating the codebook with an exponential moving average as described in (van den Oord et al., 2017, Appendix A), where it was used to reduce the variance of codebook updates. This alone is insufﬁcient to fully control drift, thus once a given module yields satisfactory compressions on the data stream, we freeze the module’s embedding matrix but leave encoder and decoder parameters free to change and adapt to new data. Moreover, we note that ﬁxing the codebook for a given module does not affect the reconstruction performance of subsequent modules, as they only need access to the current module’s decoder which can still freely change. 4. Experiments We evaluate the efﬁcacy of the proposed methods on a suite of canonical and new experiments. In Section 4.1 we present results on standard supervised continual learning benchmarks on CIFAR-10. In Section 4.2 we evaluate other downstream tasks such as standard iid training applied on the storage at the end of online continual compression. For this evaluation we consider larger images from Imagenet, as well as on lidar data. Finally we apply AQM on observations of an agent in an RL environment.Online Learned Continual Compression with Adaptive Quantization Modules Accuracy (↑) M= 20 M= 50 iid online 60.8 ±1.0 60 .8 ±1.0 iid ofﬂine 79.2 ±0.4 79 .2 ±0.4 GEM (Lopez-Paz et al., 2017) 16.8 ±1.1 17 .1 ±1.0 iCarl (5 iter) (Rebufﬁ et al., 2017) 28.6 ±1.2 33 .7 ±1.6 ﬁne-tuning 18.4 ±0.3 18 .4 ±0.3 ER 27.5 ±1.2 33 .1 ±1.7 ER-MIR (Aljundi et al., 2019) 29.8 ±1.1 40 .0 ±1.1 ER-JPEG 33.9 ±1.0 43 .1 ±0.6 Gumbel AE (Riemer et al., 2018) 25.5 ±2.0 28 .8 ±2.9 AQM (ours) 43.5 ±0.7 47 .0 ±0.8 Forgetting (↓) M= 20 M= 50 N/A N/A N/A N/A 73.5 ±1.7 70 .7 ±4.5 49 ±2.4 40 .6 ±1.1 85.4 ±0.7 85 .4 ±0.7 50.5 ±2.4 35 .4 ±2.0 50.2 ±2.0 30 .2 ±2.3 54.8 ±1.2 44 .3 ±0.9 71.5 ±2.8 67 .2 ±3.9 23.0 ±1.0 19 .0 ±1.4 Table 1.Shared head results on disjoint CIFAR-10. Total memory per class M measured in sample memory size. We report (a) Accuracy, (b) Forgetting (lower is better). 4.1. Online Continual Classiﬁcation Although CL has been studied in generative modeling (Ramapuram et al., 2017; Lesort et al., 2018; Zhai et al., 2019; Lesort et al., 2019) and reinforcement learning (Kirk- patrick et al., 2017; Fernando et al., 2017; Riemer et al., 2018), supervised learning is still the standard for evalua- tion of new methods. Thus, we focus on the online continual classiﬁcation of images for which our approach can provide a complement to experience replay. In this setting, a new task consists of new image classes that the classiﬁer must learn, while not forgetting the previous ones. The model is only allowed one pass through the data (Lopez-Paz et al., 2017; Chaudhry et al.; Aljundi et al., 2019; Chaudhry et al., 2019). The online compression here takes the role of replay buffer in replay based methods such as (Chaudhry et al., 2019; Aljundi et al., 2019). We thus run Algorithm 1, with an additional online classiﬁer being updated performed at line 15. Here we consider the more challenging continual classiﬁca- tion setting often referred to as using ashared-head (Aljundi et al., 2019; Farquhar & Gal, 2018; Aljundi et al., 2018). Here the model is not informed of the task (and thereby the subset of classes within it) at test time. This is in contrast to other (less realistic) CL classiﬁcation scenarios where the task, and therefore subset of classes, is provided explicitly to the learner (Farquhar & Gal, 2018; Aljundi et al., 2019). For this set of experiments, we report accuracy, i.e. 1 T ∑T i=1 RT,i, and forgetting, i.e. 1 T−1 ∑T−1 i=1 max(R:,i) − RT,i with R ∈ RT×T representing the accuracy matrix where Ri,j is the test classiﬁcation accuracy on task jwhen task iis completed. Baselines A basic baseline for continual supervised learn- ing is Experience Replay ( ER). It consists of storing old data in a buffer to replay old memories. Although relatively simple recent research has shown it is a critical baseline to consider, and in some settings is actually state-of-the- art (Chaudhry et al., 2019; Aljundi et al., 2019; Rolnick et al., 2018). AQM can be used as an add-on to ER that in- corporates online continual compression. We also compare against ER with standard JPEG compression. In addition we consider the following baselines. iid online (upper-bound) trains the model with a single-pass through the data on the same set of samples, but sampled iid. iid ofﬂine (upper- bound) evaluates the model using multiple passes through the data, sampled iid. We use 5 epochs in all the experiments for this baseline. ﬁne-tuning trains continuously upon ar- rival of new tasks without any forgetting avoidance strategy. iCarl (Rebufﬁ et al., 2017) incrementally classiﬁes using a nearest neighbor algorithm, and prevents catastrophic for- getting by using stored samples. GEM (Lopez-Paz et al., 2017) uses stored samples to avoid increasing the loss on previous task through constrained optimization. It has been shown to be a strong baseline in the online setting. It gives similar results to the recent A-GEM (Chaudhry et al.). ER- MIR (Aljundi et al., 2019) controls the sampling of the replays to bias sampling towards samples that will be for- gotten. We note that the ER-MIR critera is orthogonal to AQM, and both can be applied jointly.Gumbel AE (Riemer et al., 2018) learns an autoencoder for ER using the Gumbel softmax to obtain discrete representations. We evaluate with the standard CIFAR-10 split (Aljundi et al., 2018), where 5 tasks are presented sequentially, each adding two new classes. Evaluations are shown in Table 1. Due to our improved storage of previous data, we observe sig- niﬁcant improvement over other baselines at various mem- ory sizes. We can contrast AQM’s performance with ER’s to understand the net impact of our compression scheme. Speciﬁcally, AQM improves over ER by 16.0% and 13.9% in the M=20 and M=50 case, highlighting the effectiveness of online compression. Our approach is also superior in forgetting by a signiﬁcant margin in both memory settings. To compare directly to reporting in (Riemer et al., 2018) we also benchmarked our implementation on the Incremental CIFAR-100 multi-head experiment (Lopez-Paz et al., 2017) with the same settings as in (Riemer et al., 2018). By using AQM we were able to get 65.3 vs the reported 43.7 using a buffer of size 200. To speciﬁcally isolate the advantageOnline Learned Continual Compression with Adaptive Quantization Modules Accuracy RS 5.2 ±0.2 2 Module AQM (ours) 23.2 ± 1.1 Ablate 2nd Module 20.5 ±1.3 Ablate Fixing Codebook 19.2 ±0.6 Ablate Decoupled Training 16.5 ±0.7 Ablate Adaptive Compression 13.1 ±3.2 Table 2.Imagenet ofﬂine training evaluation from online continual compression. We see a clear gain over a standard Reservoir sam- pling approach. We then ablate each component of our proposal showing each component is important. Note storage used in each experiment is identical (including accounting for model sizes). of gumbel softmax versus the vector quantization for drift, we replaced the vector quantization approach with gumbel softmax in an AQM. We observed signﬁcantly less drift in the case where vector quantization is used. Full details of this experiment are described in the supplementary materials along with visualizations. The CIFAR-10 dataset has a low resolution (3 ×32 ×32) and uses a lot of data per task (10K samples). These two characteristics might leave the online compression problem easier than in a real-life scenario. Speciﬁcally, if the ﬁrst tasks are long enough and the compression rate is not too large, the model can quickly converge and thus not incur too much representation drift. Indeed, we found that us- ing a single module is already sufﬁcient for this task. For these reasons, we now study the AQM in more challenging settings presented in the next section. 4.2. Ofﬂine Evaluation on Larger Images Besides the standard continual classiﬁcation setup, we pro- pose several other evaluations to determine the effectiveness of the stored data and compression module after learning online compression. We also perform a detailed ablation to study the efﬁcacy of each component in AQM. Ofﬂine training on Imagenet We compare the effective- ness of the stored memories of AQM after a certain amount of online continual compression. We do this by training in a standard iid way an ofﬂine classiﬁcation model using only reconstructions obtained from the storage sampled after on- line continual compression has progressed for a period of time. In each case we would have the same sized storage available. We note that simply having more stored memo- ries does not amount to better performance as their quality may be severely degraded and affected by drift. Using this evaluation we ﬁrst compare a standard reservoir sampling approach on uncompressed data to a 2 module AQM using the same size storage. We observe that perfor- mance is drastically increased using the compressed sam- ples. We then use this to perform a series of ablations to Figure 4.Impact of codebook freezing. Vertical black line indi- cates freezing point. We see that AQM is still able to adapt and reduce its reconstruction loss, while having stable compressed representations. Results averaged over 5 runs demonstrate each component of our proposal is important. Speciﬁcally (a) we restrict AQM to have only one module, (b) instead of decoupled training we train modules end-to- end, (c) we remove adaptive compression, thus all samples are stored in the most compressed block, regardless of qual- ity, and (d) we do not stabilize the codebook, the embedding matrices of every block are never ﬁxed. We observe that all these elements contribute to successfully storing a represen- tative set of data for the distribution online. Drift Ablation We have seen the importance of codebook freezing when dealing with high dimensional datasets. How- ever, judging solely from the ﬁnal downstream task perfor- mance it’s difﬁcult to see if the model continues adapting after freezing. As alluded in Sec 3.2 there is a tradeoff be- tween keeping recoverable representations and a model’s ability to continue to adapt. To shed some light on this, we run the following experiment: we run a vanilla VQ-V AE on the same 20 task mini-imagenet stream, without storing any samples. When it reaches a pre-speciﬁed performance threshold, we ﬁx the codebook, and store compressed held- out data from the ﬁrst task. We then continue to update the VQ-V AE parameters, and the memory is kept ﬁxed for the rest of the stream. We apply self-replay but no other AQM mechanisms (e.g. no sampling from the input stream and no adaptive compression). We monitor how well the VQ-V AE can adapt by looking at the streaming reconstruction cost, measured on the in- coming data before an update. We also monitor the drift of samples stored in the buffer. Results are presented in Figure 4. They demonstrate that drift is controlled by stabilizing the codebook, while the model can still improve at nearly the same rate. Further analysis, along with an additional ex- periment showcasing the robustness of vector quantization to small perturbations is included in the Appendix.Online Learned Continual Compression with Adaptive Quantization Modules Figure 5.Top: Sample decoded from the buffer at the end of train- ing from scratch (32x compression rate). Bottom: Original lidar LiDAR Range data enables autonomous vehicles to scan the topography of their surrounding, giving precise mea- surements of an obstacle’s relative location. In its raw form, range data can be very large, making it costly to transmit in real time, or for long term storage. Equipping self-driving cars with a good lidar compressor can enable fast vehicle- to-vehicle (V2V) communication, leading to safer driving (Eckelmann et al., 2017). Moreover, since data collected by autonomous vehicles can be highly non-stationary (new objects on the road, changing weather or trafﬁc conditions), having a compressor which can quickly adapt to this distri- bution change will reduce the required memory for storage (or bandwidth for real time transmission). We proceed to train AQM on the Kitti Dataset (Geiger et al., 2013), which contains 61 LiDAR scan recordings, each belonging to either the “residential”, “road”, “city” envi- ronments. The data is processed as in (Caccia et al., 2018), where points from the same elevation angle are sorted in increasing order of azimuth angle along the same row. This yield a 2D grid, making it compatible with the same architec- ture used in the previous experiments. As in (Caccia et al., 2018; Tu et al., 2019), we report the reconstruction cost in Symmetric Nearest Neighbor Root Mean Squared Error (SNNRMSE) which allows to compare two point clouds. Note AQM can also be adapted to use task relevant criteria besides MSE. We consider two settings. In the ﬁrst, we train AQM from scratch on a data stream consisting of recordings from all three environments. We present (once) all the recordings of an environment before moving on to another, in order to maximise the distribution shift. We show qualitative results in Figure 5 and in the supplementary materials. Ob- serve that we are able to effectively reconstruct the LiDAR samples and can easily tradeoff quality with compression. Overall we obtain 18.8 cm SNNRMSE with 32×compres- Size in Mb Raw 1326.8 Gzip 823.0 AQM 35.5 ±.06 AQM + ﬁnetune 33.0 ±.07 AQM + ﬁnetune + PNG 27.9 ±.01 Table 3.Compression results for the data transmission of the city lidar recordings. We require that each compressed scan has an SNNRMSE under 15 cm. sion, which lies in a range that has been shown in (Tu et al., 2019) to be sufﬁcient to enable SLAM localization with very minimal error. In the second setting, we wish to simulate a scenario where some data is available a priori for the model to leverage. However, this data is limited and does not cover all the possible modalities to which an autonomous vehicle could be exposed. To this end, we pretrain AQM in a fully ofﬂine iid manner on the road and residential recordings. We then simulate the deployment of the compressor on a vehicle, where it must compress and transmit in real time the lidar data feed from a new distribution. We therefore stream the held-out city recordings and show that AQM can be ﬁne-tuned on the ﬂy to reduce the required bandwidth for data transmission. Quantitative results are presented in table 3. We ensure that the reconstructed lidar scans have a SNNRMSE smaller than 15.0 cm. Moreover, since the stored representations in AQM are 2D and discrete, we can apply lossless compression schemes such as Portable Network Graphics (PNG). 4.3. Atari RL Environments Another application of online continual compression is for preserving the states of an reinforcement learning agent op- erating online. These agents may often learn new tasks or enter new rooms thus the observations will often be highly non-iid. Furthermore many existing reinforcement learning algorithms already rely on potentially large replay buffers which can be prohibitive (Mnih et al., 2014; Rolnick et al., 2018) to run and may greatly beneﬁt from an approach such as the AQM to run concurrently with reinforcement learning algorithms. We thus perform a proof of concept for the AQM for storing the state sequence encountered by an RL learner in the atari environment(Bellemare et al., 2013). We use the dataset and tasks introduced in (Anand et al., 2019), which runs a random or learned policy in the atari environments and provides a set of classiﬁcation tasks to evaluate whether key information about the state is preserved. Results are shown Table 4. We run the online learning with the AQM on the data stream observed by the random agent. We use the same observations and optimiza- tion as in (Anand et al., 2019) and report the F1 resultsOnline Learned Continual Compression with Adaptive Quantization Modules Figure 6.Top: original. Bottom: reconstructed from AQM Game Cls Input F1 Pong Orig. State 86.7 AQM Recon 86.8 Ms Pacman Orig. State 89.4 AQM Recon 88.3 Pitfall Orig. State 68.2 AQM Recon 66.7 Table 4.Results on RL probing tasks from (Anand et al., 2019) with linear probe applied to original observation and to reconstruc- tions from AQM after online compression. Acc is averaged for each game over game speciﬁc prediction. of a linear probe directly on states for our reconstructions after online compression and the originals. Results for 3 environments are shown in Table 4 and examples in in Fig 6 and the Appendix. We ﬁnd that AQM can well preserve the critical information while compressing the state by 16x. The reference accuracies achieved by our classiﬁer are sim- ilar to those in (Anand et al., 2019). However, we do not control for the representation size unlike those evaluations of various unsupervised models. 5. Conclusion We have introduced online continual compression. We demonstrated vector quantization can be used to control drift and how to create mechanisms that allow maintain- ing quality and maximizing memory usage. These allowed learning compression while compressing. We have shown effectiveness of this online compression approach on stan- dard continual classiﬁcation benchmarks, as well as for compressing larger images, lidar, and atari data. We believe future work can consider dealing with temporal correlations for video and reinforcement learning tasks, as well as im- proved prioritization of samples for storage. References Aljundi, R., Lin, M., Goujaud, B., and Bengio, Y . Online continual learning with no task boundaries. In arXiv, 2018. Aljundi, R., Caccia, L., Belilovsky, E., Caccia, M., Charlin, L., and Tuytelaars, T. Online continual learning with maximally interfered retrieval. In Advances in Neural Information Processing (NeurIPS), 2019. Anand, A., Racah, E., Ozair, S., Bengio, Y ., C ˆot´e, M.- A., and Hjelm, R. D. Unsupervised state representation learning in atari. arXiv preprint arXiv:1906.08226, 2019. Andrychowicz, M., Denil, M., G´omez, S., Hoffman, M. W., Pfau, D., Schaul, T., and de Freitas, N. Learning to learn by gradient descent by gradient descent. In Lee, D. D., Sugiyama, M., Luxburg, U. V ., Guyon, I., and Garnett, R. (eds.), Advances in Neural Information Processing Sys- tems 29, pp. 3981–3989. Curran Associates, Inc., 2016. Ball´e, J., Laparra, V ., and Simoncelli, E. P. End-to-end optimization of nonlinear transform codes for perceptual quality. In 2016 Picture Coding Symposium (PCS), pp. 1–5. IEEE, 2016. Belilovsky, E., Eickenberg, M., and Oyallon, E. De- coupled greedy learning of cnns. arXiv preprint arXiv:1901.08164, 2019. Bellemare, M. G., Naddaf, Y ., Veness, J., and Bowling, M. The arcade learning environment: An evaluation plat- form for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279, 2013. Caccia, L., van Hoof, H., Courville, A., and Pineau, J. Deep generative modeling of lidar data. arXiv preprint arXiv:1812.01180, 2018. Chaudhry, A., Ranzato, M., Rohrbach, M., and Elhoseiny, M. Efﬁcient lifelong learning with a-gem. In ICLR 2019. Chaudhry, A., Dokania, P. K., Ajanthan, T., and Torr, P. H. Riemannian walk for incremental learning: Un- derstanding forgetting and intransigence. arXiv preprint arXiv:1801.10112, 2018. Chaudhry, A., Rohrbach, M., Elhoseiny, M., Ajanthan, T., Dokania, P. K., Torr, P. H., and Ranzato, M. Continual learning with tiny episodic memories. arXiv preprint arXiv:1902.10486, 2019. Eckelmann, S., Trautmann, T., Ußler, H., Reichelt, B., and Michler, O. V2v-communication, lidar system and posi- tioning sensors for future fusion algorithms in connected vehicles. Transportation research procedia, 27:69–76, 2017. Farquhar, S. and Gal, Y . Towards robust evaluations of continual learning. arXiv preprint arXiv:1805.09733 , 2018.Online Learned Continual Compression with Adaptive Quantization Modules Fernando, C., Banarse, D., Blundell, C., Zwols, Y ., Ha, D., Rusu, A. A., Pritzel, A., and Wierstra, D. Pathnet: Evolu- tion channels gradient descent in super neural networks. arXiv preprint arXiv:1701.08734, 2017. Geiger, A., Lenz, P., Stiller, C., and Urtasun, R. Vision meets robotics: The kitti dataset. The International Journal of Robotics Research, pp. 0278364913491297, 2013. Gueguen, L., Sergeev, A., Kadlec, B., Liu, R., and Yosinski, J. Faster neural networks straight from jpeg. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 31 , pp. 3933–3944. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/ 7649-faster-neural-networks-straight-from-jpeg. pdf. Hu, W., Lin, Z., Liu, B., Tao, C., Tao, Z., Ma, J., Zhao, D., and Yan, R. Overcoming catastrophic forgetting for continual learning via model adaptation. 2018. Husz´ar, F. On quadratic penalties in elastic weight consoli- dation. arXiv preprint arXiv:1712.03847, 2017. Johnston, N., Vincent, D., Minnen, D., Covell, M., Singh, S., Chinen, T., Jin Hwang, S., Shor, J., and Toderici, G. Improved lossy image compression with priming and spatially adaptive bit rates for recurrent networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4385–4393, 2018. Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Des- jardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, pp. 201611835, 2017. Lesort, T., Caselles-Dupr´e, H., Garcia-Ortiz, M., Stoian, A., and Filliat, D. Generative models from the perspective of continual learning. arXiv preprint arXiv:1812.09111, 2018. Lesort, T., Gepperth, A., Stoian, A., and Filliat, D. Marginal replay vs conditional replay for continual learning. In International Conference on Artiﬁcial Neural Networks, pp. 466–480. Springer, 2019. Li, Z. and Hoiem, D. Learning without forgetting. IEEE Transactions on Pattern Analysis and Machine Intelli- gence, 40(12):2935–2947, 2018. Lin, L.-J. Reinforcement learning for robots using neu- ral networks. Technical report, Carnegie-Mellon Univ Pittsburgh PA School of Computer Science, 1993. Lopez-Paz, D. et al. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems, pp. 6467–6476, 2017. Mnih, V ., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. Mnih, V ., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje- land, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., and Hassabis, D. Human-level control through deep reinforcement learning. Nature, 2014. Nguyen, C. V ., Li, Y ., Bui, T. D., and Turner, R. E. Variational continual learning. arXiv preprint arXiv:1710.10628, 2017. Nøkland, A. and Eidnes, L. H. Training neural networks with local error signals. arXiv preprint arXiv:1901.06656, 2019. Oyallon, E., Belilovsky, E., Zagoruyko, S., and Valko, M. Compressing the input for cnns with the ﬁrst-order scatter- ing transform. In The European Conference on Computer Vision (ECCV), September 2018. Ramapuram, J., Gregorova, M., and Kalousis, A. Lifelong generative modeling. arXiv preprint arXiv:1705.09847, 2017. Razavi, A., Oord, A. v. d., and Vinyals, O. Generating diverse high-ﬁdelity images with vq-vae-2.arXiv preprint arXiv:1906.00446, 2019. Rebufﬁ, S.-A., Kolesnikov, A., Sperl, G., and Lampert, C. H. icarl: Incremental classiﬁer and representation learning. In Proc. CVPR, 2017. Riemer, M., Franceschini, M., and Klinger, T. Genera- tion and consolidation of recollections for efﬁcient deep lifelong learning. CoRR, abs/1711.06761, 2017. URL http://arxiv.org/abs/1711.06761. Riemer, M., Cases, I., Ajemian, R., Liu, M., Rish, I., Tu, Y ., and Tesauro, G. Learning to learn without forgetting by maximizing transfer and minimizing interference. arXiv preprint arXiv:1810.11910, 2018. Rolnick, D., Ahuja, A., Schwarz, J., Lillicrap, T. P., and Wayne, G. Experience replay for continual learning, 2018. Rusu, A. A., Rabinowitz, N. C., Desjardins, G., Soyer, H., Kirkpatrick, J., Kavukcuoglu, K., Pascanu, R., and Had- sell, R. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.Online Learned Continual Compression with Adaptive Quantization Modules Shin, H., Lee, J. K., Kim, J., and Kim, J. Continual learn- ing with deep generative replay. In Advances in Neural Information Processing Systems, pp. 2990–2999, 2017. Theis, L., Shi, W., Cunningham, A., and Husz ´ar, F. Lossy image compression with compressive autoencoders. arXiv preprint arXiv:1703.00395, 2017. Thrun, S. and Mitchell, T. M. Lifelong robot learning. Robotics and autonomous systems, 15(1-2):25–46, 1995. Torfason, R., Mentzer, F., ´Ag´ustsson, E., Tschannen, M., Timofte, R., and Gool, L. V . Towards image under- standing from deep compression without decoding. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum? id=HkXWCMbRW. Tu, C., Takeuchi, E., Carballo, A., and Takeda, K. Point cloud compression for 3d lidar sensor using recurrent neural network with residual blocks. In 2019 Interna- tional Conference on Robotics and Automation (ICRA), pp. 3274–3280. IEEE, 2019. van den Oord, A., Vinyals, O., et al. Neural discrete repre- sentation learning. In Advances in Neural Information Processing Systems, pp. 6306–6315, 2017. Zenke, F., Poole, B., and Ganguli, S. Continual learn- ing through synaptic intelligence. arXiv preprint arXiv:1703.04200, 2017. Zhai, M., Chen, L., Tung, F., He, J., Nawhal, M., and Mori, G. Lifelong gan: Continual learning for conditional im- age generation. ArXiv, abs/1907.10107, 2019.Online Learned Continual Compression with Adaptive Quantization Modules A. Buffer Memory Management We consider two settings to manage the samples inside the memory. In the ﬁrst setting, we do not perform any codebook freezing. Therefore, as the model trains, the amount of compressed representations AQM can store increases smoothly. In this setting, the current amount of samples stored by AQM is a good approximation of the model’s capacity. Therefore, we simply use this estimate instead of the total buffer size in the regular reservoir sampling scheme. The algorithm is presented in Alg 4. Algorithm 4: AddToMemory Input: Memory Mwith capacity C(bytes), sample x 1 Nreg = C BY TES(x) 2 capacity = max( Nreg, NUM SAMPLES (M) ) 3 %Probability of adding x 4 add ∼B( capacity SAMPLE AMT SEEN SO FAR ) %Bernoulli 5 if add then 6 hidx, blockid = ADAPTIVE COMPRESS (x, AE, dth) 7 while BITS(hidx) - FREE SPACE(M) >0 do 8 DELETE RANDOM (M) 9 end 10 end This is not the case when we perform codebook freezing. In the latter setting, consider the moment when the ﬁrst codebook is ﬁxed; suddenly, the amount of samples the model can store has increased by a factor equal to the compression rate of the ﬁrst block. Therefore, at this moment, the amount of samples currently stored by AQM is not a good approximation for the model’s capacity. Moreover, when performing codebook freezing, since the capacity suddenly spikes, we must decide between a) having an imbalance in the buffer, where certain temporal regions of the streams are not equally represented, or not utilising all available memory and storing less incoming samples so they are in similar quantities as previous samples. We opt for the former approach, and propose a procedure that allows the buffer to rebalance itself as new training data becomes available. We illustrate the procedure with an example. Consider an AQM where the distribution of samples in the buffer is the one plotted in Fig 10. Speciﬁcally, we show the number of samples stored for each minibatch processed by the model. In this example, very few (<20) samples are stored from the earliest part of the stream, while a much larger number comes from the more recent part of the stream. Assuming that the model is over its memory capacity, we need to remove samples until the memory requirement is met. Ideally, we would like to remove more samples from parts of the stream where samples are abundant. In order to do so, we use Kernel Density Estimation on the histogram in Fig 10. Doing so gives us the line labelled iter0 in Fig 10. We then sample points according to the distribution given by iter0, remove them, and ﬁt a new KDE with the remaining points (labelled iter1). In this example we repeat this procedure 10 times, until iter9. As we can see, the distribution of stored samples becomes closer to the uniform distribution, i.e. the setting where all parts of the streeam are equally represented. Therefore, in the setting where codebook freezing is performed, we ﬁrst add all incoming points to the buffer. Then, points are removed according to the procedure described above. This allows for maximal memory usage while ensuring that the buffer self balances over time. Note that we need to store the timestamp alongside each sample, which has a negligible cost when dealing with high-dimensional inputs. B. Further Details of Experiments We include here further details regarding the models used in our experiments. For all reported results, almost all hyperpa- rameters are kept the same. we set D the size of the embedding table equal to 100, we use a decay value of 0.6 for the Embedding EMA update, and the same architectural blocks. Across problems, we mainly vary the reconstruction threshold, as well how the blocks are stacked and their compression rates (by e.g. changing the number of codebooks per block).Online Learned Continual Compression with Adaptive Quantization Modules Figure 7.(left) histogram of samples in AQM where no buffer balancing was performed. (right) iterative buffer balancing procedure B.1. Cifar For CIFAR-10, we use a 1 block AQM, latent size (16 x 16 x 100) is quantized with (16 x 16 x 1) indices where the last index represents the number of codebooks. The codebook here contains 128 embeddings, giving a compression factor of 13.7×. Due to the simplistic nature of the task and low resolution of the images, AQM already yields good compression before the end of the ﬁrst task, hence adaptive compression and codebook freezing are not required. For the (Riemer et al., 2018) baseline, we ran a hyperparameter search to vary the compression size. Speciﬁcally, we ran a grid search for the number of categories per latent variable, as well as for the number of latent variables. We found the gumbel softmax much less stable during training and harder to cross-validate than vector quantization. Below we show an example of the image quality of our approach compared to (Riemer et al., 2018). We ran both AQM and (Riemer et al., 2018) on the split CIFAR-10 task, then extracted images which happened to be in the buffer of both methods. Figure 8.Bottom row: random buffer reconstructions using (Riemer et al., 2018). Middle row: random buffer reconstructions using SQM. Top row: corresponding original image. Columns are ordered w.r.t their entry time in the buffer, from oldest to newest. All samples above were obtained from the disjoint CIFAR-10 task, and are 12×smaller than their original image. B.2. Imagenet For the Ofﬂine 128 x 128 Imagenet experiment, we use the following three blocks to build AQM, with the following latent sizes : 1. (64 x 64 x 100) quantized using (64 x 64 x 1) indices, with a codebook of 16 vectors, giving a 24 ×compression. 2. (32 x 32 x 100) quantized using (32 x 32 x 1) indices, with a codebook of 256 vectors, giving a 48 ×compression. 3. (32 x 32 x 100) quantized using (32 x 32 x 1) indices, with a codebook of 32 vectors, giving a 76.8 ×compression. For the 2 block AQM, we searched over using blocks (1-2) (2-3) and (1-3). For 1 block AQM we simply tried all three blocks independently. When stacking two blocks with the same latent sizes (e.g. block 2 and 3) the encoder and decoder functions for the second block are the identity. In other words, the second block simply learns another embedding matrix.Online Learned Continual Compression with Adaptive Quantization Modules C. Drift Ablation Here we provide additional results for the drift ablation studied in 4. We repeat the same experiment for different values of the reconstruction threshold parameter, which controls when the codebook freezing occurs. The table below shows that the same conclusion holds across multiple values for this parameter: codebook freezing yields little to no drift, while only negligibly hindering the model’s ability to adapt to a distribution shift. It is worth noting that in cases of severe drift (e.g. with recon th = 7.5) the model diverges because it is rehearsing on samples of poor quality. In this setting, codebook freezing performs better on both the streaming MSE and the drift MSE. recon th Freezing Streaming MSE Drift MSE Streaming + Drift MSE 1 No 0.59 ±0.03 1.01 ±0.14 1.60 ±0.17 1 Yes 0.60 ±0.03 0.49 ±0.02 1.09±0.05 2.5 No 0.63 ±0.06 13.24 ±5.36 13.87 ±5.42 2.5 Yes 0.81 ±0.02 1.07 ±0.05 1.88±1.12 5 No 0.65 ±0.04 55.31 ±36.82 55.96 ±36.86 5 Yes 0.92 ±0.03 1.69 ±0.18 2.61±0.21 7.5 nan nan nan nan 7.5 Yes 0.98 ±0.11 2.10 ±0.28 3.08±0.39 Table 5.Here we provide additional results for the drift ablation discussion in section shown in Fig 4. For clarity all results are multiplied by 100. (e.g. recon th of 2.5 corresponds to 0.025). Results averaged over 5 runs. Figure 9.Visualization of results reported in Table C. We kept the scale of the y axis consistent across the four graphs for easy comparison.Online Learned Continual Compression with Adaptive Quantization Modules D. Atari Below are sample reconstructions used in the Atari experiments. For each game, reconstructions are in the ﬁrst row and the original uncompressed samples in the second row. Reconstructions are 16×smaller than the original RGB images. Figure 10.Pong Figure 11.Pitfall Figure 12.Ms PacmanOnline Learned Continual Compression with Adaptive Quantization Modules E. Lidar Samples Here we show reveral lidar compressions (left) and their original counterpart (right). The compression rate is 32 ×. We note that unlike RGB images, raw lidar scans are stored using ﬂoating points. We calculate the compression rate from the (smaller) polar projection instead of the 3 channel cartesian representation. Figure 13.Lidar reconstruction (left) vs original (right)Online Learned Continual Compression with Adaptive Quantization Modules Figure 14.Lidar reconstruction (left) vs original (right)",
      "meta_data": {
        "arxiv_id": "1911.08019v3",
        "authors": [
          "Lucas Caccia",
          "Eugene Belilovsky",
          "Massimo Caccia",
          "Joelle Pineau"
        ],
        "published_date": "2019-11-19T00:43:16Z",
        "pdf_url": "https://arxiv.org/pdf/1911.08019v3.pdf"
      }
    },
    {
      "title": "Analysis of Stochastic Processes through Replay Buffers",
      "abstract": "Replay buffers are a key component in many reinforcement learning schemes.\nYet, their theoretical properties are not fully understood. In this paper we\nanalyze a system where a stochastic process X is pushed into a replay buffer\nand then randomly sampled to generate a stochastic process Y from the replay\nbuffer. We provide an analysis of the properties of the sampled process such as\nstationarity, Markovity and autocorrelation in terms of the properties of the\noriginal process. Our theoretical analysis sheds light on why replay buffer may\nbe a good de-correlator. Our analysis provides theoretical tools for proving\nthe convergence of replay buffer based algorithms which are prevalent in\nreinforcement learning schemes.",
      "full_text": "arXiv:2206.12848v1  [cs.LG]  26 Jun 2022 Analysis of Stochastic Processes through Replay Buffers Shirli Di Castro Shashua 1 Shie Mannor 1 2 Dotan Di Castro 3 Abstract Replay buffers are a key component in many re- inforcement learning schemes. Y et, their theo- retical properties are not fully understood. In this paper we analyze a system where a stochas- tic processX is pushed into a replay buffer and then randomly sampled to generate a stochastic processY from the replay buffer. W e provide an analysis of the properties of the sampled pro- cess such as stationarity, Markovity and autocor- relation in terms of the properties of the original process. Our theoretical analysis sheds light on why replay buffer may be a good de-correlator. Our analysis provides theoretical tools for prov- ing the convergence of replay buffer based al- gorithms which are prevalent in reinforcement learning schemes. 1. Introduction A Replay buffer (RB) is a mechanism for saving past gen- erated data samples and for sampling data for off-policy reinforcement learning (RL) algorithms ( Lin, 1993). The RB serves a First-In-First-Out (FIFO) buffer with a ﬁxed capacity and it enables sampling mini-batches from previ- ously saved data points. Its structure and sampling mecha- nism provide a unique characteristic: the RB serves asde- correlator of data samples. T ypically, the agent in RL al- gorithms encounters sequences of highly correlated states and learning from these correlated data points may be prob- lematic since many deep learning algorithms suffer from high estimation variance when data samples are dependent. Thus, a mechanism that decorrelates the input such as the RB can improve data efﬁciency and reduce sample com- plexity. Since its usage in the DQN algorithm ( Mnih et al. , 2013), RB mechanism have become popular in many off- 1 T echnion Institute of T echnology , Haifa, Israel 2 NVIDIA Re- search, Israel 3 Bosch Center of AI, Haifa, Israel. Correspondence to: Shirli Di Castro Shashua <sdicastro@gmail.com>. policy RL algorithms ( Lillicrap et al. , 2015; Haarnoja et al. , 2018). Previous work has been done on the empirical beneﬁts of RB usage ( Fedus et al. , 2020; Zhang & Sutton , 2017), but still there is a lack in theoretical understanding of how the RB mechanism works. Understanding the prop- erties of RBs is crucial for convergence and ﬁnite sample analysis of algorithms that use a RB in training. For the best of our knowledge, this is the ﬁrst work to tackle these theoretical aspects. In this work we focus on the following setup. W e de- ﬁne a random processX that is pushed into a N samples size RB and analyze the characteristics of the stochastic process ofK samples that is sampled from the RB. W e analyze if properties of the original random process such as Markovity and stationarity are maintained and quantify the auto-correlation and covariance in the new RB process (later denoted by Y ) when possible. Our motivation comes from RL algorithms that use RB. Speciﬁcally, we focus on the induced Markov chain given a policy but we note that the analysis in this paper is also rel- evant to general random processes that are kept in a FIFO queue. This is relevant for domains such as First Come First Served domains ( Laguna & Marklund , 2013). Our goal is to provide analytical tools for analyzing algorithm s that use RBs. Our results can provide theoretical under- standing of phenomena seen in experiments using RBs that have never been analyzed theoretically before. Our the- ory for RBs provides tools for proving convergence of RB- based RL algorithms. Our main contributions are: 1. Formulating RBs as random processes and analyze their properties such as stationarity, Markovity, ergod- icity, auto-correlation and covariance. 2. Comparing between properties of the original random process that was pushed into the RB and the sampled process at the output of the RB. Particularly we prove that when sampling uniformly from the RB, the RB forms as a de-correlator between the sampled batches. 3. Connecting our RB theory to RL by demonstrating this connection through a RB-based actor critic RL al- gorithm that samplesK transitions from RB with size N for updating its parameters. W e prove, for the ﬁrst time, the asymptotic convergence of such RB-basedAnalysis of Stochastic Processes through Replay Buffers Figure 1. Replay buffer ﬂow diagram: Process X enters the RB which stores {Xt, . . . X t−N+1} in positions (1, . . . , N ), respectively . As time proceeds and t > N , old transitions are thrown away from the RB. At each time ste p t, a random subset Jt of K time steps is sampled from the RB. W is simply [RB, J ]. Y is the process of averaging a function over X at times from the subset J. Lastly , the process Z is simply a function applied on the variable X. Comp aring Y to Z, we can see that Z can serve as an online update whil e Y can serve as a RB-based update. actor critic algorithm. The paper is structured as follows. W e begin with present- ing the setup in Section 2. W e then state our main results regarding RB properties in Section 3. In Section 4 we con- nect between our RB theory and its use in RL and provide a convergence proof for an RB-based actor critic algorithm. Afterward, in Section 5 we position our work in existing literature and conclude in Section 6. 2. Setup for Replay Buffer Analysis 2.1. Replay Buffer Structure LetX ≜ (Xt)∞ t=0 be a stochastic process where the subscript t indicates time. The samples are dynamically pushed into a Replay Buffer (RB; Lin, 1993; Mnih et al. , 2013) of capacity N, i.e., it is a First-In-First-Out (FIFO) buffer that can hold the N latest samples. W e deﬁne the state of the RB at time t with RBt = {Xt−N+1, . . . , X t}. Suppose that the buffer cells are numbered from 1 to N. The latest observation of X is pushed into cell 1, the ob- servation before into cell 2, etc. When a new observation arrives, the observation in cell n is pushed into cell n + 1 for 1 ≤ n < N , while the observation in cell N is thrown away. The random process RB = ( RBt)∞ t=0 contains the last N samples of X. The random process Y is deﬁned as the av- erage of random K samples (without replacement) out of the N samples and applying a function f(·) : · → RD 1 1 W e note that f(·) may also depend on t but we leave that for the sake of simplicity . where D is the dimension of the algorithm 2. The function f(·) may correspond to a typical RL function that one usu- ally ﬁnd in RL algorithms such as linear function approxi- mation, T emporal Difference, etc. ( Bertsekas & Tsitsiklis , 1996). W e elaborate on possible RL functions in Section 4.2. 2.2. Replay Buffer Sampling Method W e analyze the ”unordered sampling without replacement” strategy from the RB. W e note that other sampling meth- ods may be analyzed, but we chose this speciﬁc sampling due to its popularity in many deep reinforcement learning algorithms 3. Let N be a set of indices: N = {1, . . . , N } and let ¯J be a subset of K indices from N. Given N and K, let CN,K be the set of all possible subsets ¯J for speciﬁc N and K. Then, the probability of sampling subset ¯J is pN,K binom( ¯J) = 1 (N K) ∀ ¯J ∈ CN,K , where (N K ) ≜ N! (N−K)!K! is the binomial coefﬁcient. 2.3. Replay Buffer Related Processes W e denote the set of K temporal indices of the sam- ples from RB by the random process J (corresponds to a ”Batch” in Deep Learning) where Jt = {jk}K k=1 ⊂ {t− N + 1, . . . , t } 4. Similarly, the corresponding K RB in- dices process is ¯J where ¯Jt ⊂ { 1, . . . , N }. W e remark that 2 For example, in linear function approximation of Actor-Cri tic algorithms, D is the dimension of the linear basis used to approx- imate the value function by the critic. 3 In Section 6 we discuss shortly future directions for other sampling schemes. 4 W e note that in the ﬁrst K steps the batch is of size smaller than K and in the ﬁrst N steps the RB is not full.Analysis of Stochastic Processes through Replay Buffers both Jt and ¯Jt contain identical information but one refers to the absolute time, and one to the indices of the RB. W e deﬁne the random processWt ≜ [RBt, Jt] which holds both the information on the RB as well on the sampling from it. For later usage, we deﬁne the processXt going through a function f(·) with Zt ≜ f(Xt). The resulting Yt has the structure of Yt = 1 K ∑ j∈Jt Zj = 1 K ∑ j∈Jt f(Xj). The stochastic processes relations that are described abov e are visualized in Figure 1. 3. Replay Buffer Properties In this section we analyze the properties of a random pro- cessY that is sampled from the RB and used in some RL algorithm. Speciﬁcally, we analyze stationarity, Markovi ty, ergodicity, auto-correlation, and covariance. 3.1. Stationarity, Markovity and Ergodicity The following Lemmas characterize the connection be- tween different properties of X that enter the RB and the properties of the processes RB and Y . Stationarity is not a typical desired RL property since we constantly thrive to improve the policy (and thus the in- duced policy) but we bring it here for the sake of complete- ness. Lemma 1(Stationarity). Let Xt and ¯Jt be stationary pro- cesses. Then, RBt and Yt are stationary. The proof for Lemma 1 is deferred to Section A.1 in the supplementary material. In the next Lemma we analyze when the processRB is Markovian. This property is important in RL analysis. Note thatYt is not necessarily Markovian, however, Wt is Markovian. For this, with some abuse of notation, we de- ﬁneXn2 n1 (t) ≜ {Xt−n2+1, . . . , X t−n1+1|RBt} as the set of random variables realizaions from process X stored in the RB at time t from position n1 to n2. Lemma 2 (Markovity). Let Xt be a Markov process. Then: (1) RBt and Wt are Markovian. (2) The transition proba- bilities of RBt for t ≥ N are: P (RBt+1|RBt) =      P (Xt+1|Xt) if Xt ∈ X1 1 (t) and RBt+1 = {Xt+1} ∪ XN−1 1 (t), 0 otherwise. If Jt is sampled according to ”unordered sampling without replacement”, then the transition probabilities of Wt for t ≥ N are: P (Wt+1|Wt) =        1 ( N K)P (Xt+1|Xt) ∀Jt+1 ∈ CN,K , if Xt ∈ X1 1 (t) and RBt+1 = {Xt+1} ∪ XN−1 1 (t) 0 otherwise. The proof for Lemma 2 is deferred to Section A.2 in the supplementary material. In RL, the properties of aperiodicity and irreducible (that together form ergodicity; Norris, 1998) are crucial in many convergence proofs. The following states that these proper - ties are preserved when using RB. W e denote supp (Yt) as the set of all the values Yt can take. Lemma 3 (Ergodicity). Let Xt be a Markov process that is aperiodic and irreducible. Then, RBt and Wt are aperi- odic and irreducible. Moreover , every point y ∈ supp(Yt) is visited inﬁnitely often. The proof for Lemma 3 is deferred to Section A.3 in the supplementary material. 3.2. Auto-Correlation and Covariance In this section we analyze the auto-correlation and covari- ance of the processY expressed by process X properties. When X is stationary, the auto-correlation and covariance functions for X are: RX (τ) = E[XtXt+τ ] CX (τ) = E [(Xt − E [Xt]) (Xt+τ − E [Xt+τ ])] In the same way, the deﬁnition of the auto-correlation and covariance functions for the processY are RY (τ) and CY (τ), respectively. In the following theorem we prove the relationship between the auto-correlation and covari- ance functions of processesY and X. For that, we need to deﬁne the distribution of all time differences between two batches of samples as follows. Deﬁnition 1(Time difference distribution between two batches of samples) . Consider a RB of size N. Consider taking two different random permutations (batches), de- noted by¯Ja and ¯Jb, both of length K in two possibly dif- ferent time points, ta and tb = ta + τ. Let τ′ be a random variable where its distribution Fτ ′ (·) is the probability of each difference between each sample of ¯Ja and ¯Jb. The support of τ′ is τ − N + 1 ≤ τ′ ≤ τ + N − 1. Theorem 1 (Auto-Correlation and Covariance) . Let τ be the difference between two time steps of the processes Y . Let Eτ ′ be an expectation according to the distribution Fτ ′ (·). Then: RY (τ) = Eτ ′ [RZ (τ′)] , CY (τ) = Eτ ′ [CZ (τ′)] . (1)Analysis of Stochastic Processes through Replay Buffers The proof for Theorem 1 is in Section B.1 in the supple- mentary material. W e note two things. First, we note that we did not specify how the sampling is done from the RB and it is expressed by the random variable τ′ from Deﬁni- tion 1, i.e., Eq. ( 1) is a general expression. Second, we note that we express the correlation using process Z and not process X directly, but process Z auto-correlation and covariance can be computed directly in any practical case using the relationZt = f(Xt). For the speciﬁc case of ”unordered sampling without re- placement”, we express the relation between the second moments ofZ and Y explicitly through the distribution of τ′. Lemma 4 (Time difference Distribution for uniform batches). The random variable τ′ distribution for ”un- ordered sampling without replacement” is P (τ′) =      N−|d| N2 τ′ = τ + d, d ∈ {− N + 1, . . . 0, . . . N − 1} 0 τ′ < τ − N + 1 or τ′ > τ + N − 1 . The proof for Lemma 4 is in Section B.2 in the supplemen- tary material. In the following corollary we state the exact dependence in the case of random sampling ofK samples from a RB with size N. Corollary 1. Consider process Z where sampling is ac- cording to ”unordered sampling without replacement”. Then, the auto-correlation and covarinace of the process Y are: RY (τ) = 1 N2 N−1∑ d=−N+1 (N − |d|)RZ (d + τ) CY (τ) = 1 N2 N−1∑ d=−N+1 (N − |d|)CZ (d + τ) The proof for Corollary 1 is in Section B.3 in the supple- mentary material. W e see that using a RB reduces the auto- correlation and covaraince of processZ by factor of N. In- terestingly, this reduction is independent of K. This result proves the de-correlation effect of using RBs and provides an explicit expression for that. 4. Replay Buffers in Reinforcement Learning In the previous section we analyzed properties of stochastic processes that go through a RB. In this section we analyze RBs in RL. Stabilizing learning in modern off-policy deep RL algorithms, such as Deep Q-Networks ( Mnih et al. , 2013) or DDPG ( Lillicrap et al. , 2015), is based on saving past observed transitions in a RB. Even though its use is ex- tensive, the theoretical understanding of sampling batches mechanism from a RB is still quite limited. This is our focus in this section. W e begin with describing the setup that will serve us in this section. Then we connect between the random processes as deﬁned in Section 3 and common stochastic updates used in RL. W e then describe an RB-based actor-critic algorithm that uses a batch ofK samples from the RB in each param- eters update step. This type of algorithm serves as a basic example for popular usages of RBs in RL. W e note that other versions of RB-based RL algorithms (such as deep RL algorithms, value-based algorithms, discounted settings of the value function) can be analyzed with the stochastic processes tools we provide in this work. Finally, we present a full convergence proof for the RB-based actor critic algo- rithm. Despite its popularity, to the best of our knowledge, there is only handful of proofs that consider RB in RL al- gorithm analysis (e.g., Di-Castro Shashua et al. , 2021 or Lazic et al. , 2021). Most of the convergence proofs for off- policy algorithms assume that a single sample is sampled from the RB. Di-Castro Shashua et al. (2021) proved for the ﬁrst time the convergence of an RB-based algorithm. However, their algorithm and technical tools were focused on the sim-to-real challenge with multiple MDP environ- ments, and they focused only on a single sample batch from the RB instead ofK samples (which complicates the proof). Therefore, for completeness and focusing on the RB prop- erties, we provide a proof for RB-based algorithms, with a single MDP environment and a batch ofK samples. Simi- larly to previous works, we consider here a setup with lin- ear function approximation ( Bertsekas & Tsitsiklis , 1996). 4.1. Setup for Markov Decision Process An environment in RL is modeled as a Markov Deci- sion Process (MDP; Puterman, 1994), where S and A are the state space and action space, respectively. W e let P(S′|S, A) denote the probability of transitioning from state S ∈ S to state S′ ∈ S when applying action A ∈ A . W e consider a probabilistic policy πθ(A|S), parameterized by θ ∈ Θ ⊂ Rd which expresses the probability of the agent to choose an action A given that it is in state S. The MDP measure P (S′|S, A) and the policy measure πθ(A|S) induce together a Markov Chain (MC) measure Pθ(S′|S) (Pθ in matrix form). W e let µθ denote the stationary dis- tribution induced by the policy πθ. The reward function is denoted by r(S, A). In this work we focus on the average reward setting 5 . The goal of the agent is to ﬁnd a policy that maximizes the av- erage reward that the agent receives during its interaction 5 The discount factor settings can be obtained in similar way t o current setup.Analysis of Stochastic Processes through Replay Buffers with the environment. Under an ergodicity assumption, the average reward over time eventually converges to the ex- pected reward under the stationary distribution ( Bertsekas, 2005): ηθ ≜ lim T →∞ ∑ T t=0 r(St, At) T = ES∼µθ ,A∼πθ [r(S, A)]. (2) The state-value function evaluates the overall expected ac - cumulated rewards given a starting state S and a policy πθ V πθ (S) ≜ E [ ∞∑ t=0 (r(St, At) − ηθ) ⏐ ⏐ ⏐ ⏐ ⏐S0 = S, πθ ] , (3) where the actions follow the policy At ∼ πθ(·|St) and the next state follows the transition probability St+1 ∼ P (·|St, At). Let O = {S, A, S ′} be a transition from the environment. Let Ot be a transition at time t. The temporal difference error δ(O) (TD; Bertsekas & Tsitsiklis , 1996) is a random variable based on a single sampled transition from the RB, δ(O) = r(S, A) − η + φ(S′)⊤w − φ(S)⊤w, (4) where ˆV πθ w (S) = φ(S)⊤w is a linear approximation for V πθ (S), φ(S) ∈ Rd is a feature vector for state S and w ∈ Rd is the critic parameter vector. W e denote by Φ ∈ R|S|×d the matrix of all state feature vectors. 4.2. Replay Buffer as a Random Process in RL In Section 3 we compared between properties of general random process X going through a RB and yielding a pro- cess Y . In the RL context we have Xt ≜ Ot, meaning our basic component is a single transition of state-action- next-state observed at timet. In addition, we deﬁned Zt ≜ f(Xt) process where f(·) is a general function. In RL, f(·) is commonly deﬁned as the value function, the Q- function, the TD-error, the empirical average reward, the critic or actor gradients or any other function that com- putes a desirable update, based on an observed transition O. Common RL algorithms that use a single f(Ot) compu- tation in the parameters update step are commonly referred ason-policy algorithms where they update their parameters based only on the last observed transition in the Markov chain. See Figure 1 for a comparison between on-line up- dates and RB-based updates. Using the formulation of ran- dom processes we presented in Section 3, we can character- ize the on-line updates, based on a single last transition as follows: Zreward t = freward(Ot) = r(St, At) − ηt Zcritic t = fcritic(Ot) = δ(Ot)φ(St) Zactor t = factor (Ot) = δ(Ot)∇ log πθ(At|St) Algorithm 1 Linear Actor Critic with RB samples 1: Initialize Replay Buffer RB with size N. 2: Initialize actor parameters θ0, critic parameters w0 and average reward estimator η0. 3: Learning steps {αη t}, {αw t}, {αθ t}. 4: for t = 0 , . . . do 5: Interact with MDP M according to policy πθt and add the transition {St, At, r(St, At), St+1} to RB t. 6: Sample Jt - K random time indices form RBt. De- note the corresponding transitions as {Oj }j∈Jt . 7: δ(Oj ) = r(Sj , Aj ) − ηt + φ(S′ j )⊤wt − φ(Sj )⊤wt 8: Update average reward ηt+1 = ηt + αη t( 1 K ∑ j∈Jt r(Sj , Aj ) − ηt) 9: Update critic wt+1 = wt +αw t 1 K ∑ j∈Jt δ(Oj )φ(Sj ) 10: Update actor θt+1 = Γ ( θt − αθ t 1 K ∑ j∈Jt δ(Oj )∇θ log πθ(Aj |Sj ) ) 11: end for When using RB-based off-policy algorithms, the param- eters updates are computed over an average of K func- tions which are based on K transitions that were sam- pled randomly from the last stored N transitions. This exactly matches our deﬁnition of the process Y : Yt = 1 K ∑ j∈Jt f(Xj ) = 1 K ∑ j∈Jt Zj . The following updates are typical in RB-based off-policy algorithms: Y reward t = 1 K ∑ j∈Jt Zreward j = 1 K ∑ j∈Jt r(Sj , Aj ) − ηt Y critic t = 1 K ∑ j∈Jt Zcritic j = 1 K ∑ j∈Jt δ(Oj )φ(Sj ) Y actor t = 1 K ∑ j∈Jt Zactor j = 1 K ∑ j∈Jt δ(Oj )∇ log πθ(Aj |Sj ) (5) In Algorithm 1, we present a linear actor critic algorithm based on RB samples where the algorithm updates the ac- tor and critic using a random batch of transitions from the RB. In Section 4.5 we show how the results from Section 3 regarding a random process that is pushed into the RB, and the deﬁnitions ofX and Y processes are helpful in proving the asymptotic convergence of this algorithm. 4.3. Linear Actor Critic with RB Samples Algorithm The basic RB-based algorithm we analyze in this work is presented in Algorithm 1. W e propose a two time scale linear actor critic optimization scheme (similarly t o Konda & Tsitsiklis , 2000), which is an RB-based version of Bhatnagar et al. (2008) algorithm. Our algorithm is fully described by the random process Wt = [ RBt, Jt] and by the algorithm updates Y reward t , Y critic t and Y actor t describedAnalysis of Stochastic Processes through Replay Buffers in equation ( 5). See Figure 2 for a visualized ﬂow diagram of Algorithm 1. In Algorithm 1 we consider an environment, modeled as an MDP M, and we maintain a replay buffer RB with capacity N. The agent collects transitions {S, A, r(S, A), S′} from the environment and stores them in the RB. W e train the agent in an off-policy manner. At each time stept, the agent samples Jt – a subset of K random time indices from RBt which deﬁnes the random transitions batch for optimizing the average reward, critic and actor parameters. Note that for the actor updates, we use a projectionΓ(·) that projects any θ ∈ Rd to a compact set Θ whenever θ /∈ Θ . 4.4. Expectations of Critic and Actor Updates in Algorithm 1 W e divide the convergence analysis of Algorithm 1 into two parts. The ﬁrst part, presented in this section, is unique to our paper - we describe in Lemmas 5 and 6 a closed form of the expectations of the actor and critic updates, based on a random batch ofK transitions from the RB. In the second part, presented in Section 4.5, we use Stochas- tic Approximation tools for proving the algorithm updates convergence, based on the results from Lemmas 5 and 6. W e note that Section 4.5 follows the steps of the conver- gence proofs presented by Di-Castro Shashua et al. (2021) and Bhatnagar et al. (2009). For time t − n + 1 where 1 ≤ n ≤ N, we deﬁne the in- duced MC with a corresponding policy parameter θt−n+1. For this parameter, we denote the corresponding state dis- tribution vectorρt−n+1 and a transition matrix Pt−n+1 (both induced by the policy πθt−n+1 ). Finally, we deﬁne the following diagonal matrix Dt−n+1 ≜ diag(ρt−n+1) and the reward vector rt−n+1 with elements rt−n+1(S) =∑ A πθt−n+1 (A|S)r(S, A). Based on these deﬁnitions we deﬁne Ct ≜ 1 N N∑ n=1 Dt−n+1 (Pt−n+1 − I) bt ≜ 1 N N∑ n=1 Dt−n+1 (rt−n+1 − ηθe) . (6) where I is the identity matrix and e is a vector of ones. Let Dθ ≜ diag(µθ) and deﬁne Cθ ≜ Dθ (Pθ − I) , b θ ≜ Dθ (rθ − ηθe) . (7) In our RB setting, since we have at time t a RB with the last N samples, Ct and bt in Equation ( 6) represent a superpo- sition of all related elements of these samples. For proving the convergence of the critic, we assume the policy is ﬁxed. Then, when t → ∞ , ρt−n+1 → µθ for all index n. This means that the induced MC is one for all the samples in the RB, so the sum overN disappears for Cθ and bθ. The following two lemmas compute the expectation of the critic and actor updates when using a random batch of K samples. The expectations are over all possible random batches sampled from the RB. Recall that¯Jt ⊂ { 1, . . . , N } and CN,K is the set of all possible subsets ¯J for speciﬁc N and K. These lemmas are the main results for proving con- vergence of RB-based RL algorithms. Lemma 5. Assume we have a RB with N transitions and we sample random K transitions from the RB. Then: E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δ(Ot−n+1)φ(St−n+1)  = Φ ⊤CθΦ w + Φ ⊤bθ, where Cθ and bθ are deﬁned in ( 7). Lemma 6. Assume we have a RB with N transitions and we sample random K transitions from the RB. Then: E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δπθ (Ot−n+1)∇θ log πθ(At−n+1|St−n+1)   = ∇θηθ − ∑ S µθ(S) ( φ(S)⊤∇θwπθ − ∇θ ¯V πθ (S) ) , where ¯V πθ (S) = ∑ A∈A πθ(A|S) ( r(S, A) − ηθ + ∑ S′∈S P (S′|S, A)φ(S′)⊤wπθ ) . The proofs for Lemmas 5 and 6 are in sections C.1 and D.1, respectively, in the supplementary material. 4.5. Asymptotic Convergence of Algorithm 1 W e are now ready to present the convergence theo- rems for the critic and actor in Algorithm 1. In the proof of our theorems we use tools from Stochastic Ap- proximation (SA) ( Kushner & Y in , 2003; Borkar, 2009; Bertsekas & Tsitsiklis , 1996), a standard tool in the liter- ature for analyzing iterations of processes such as two time scale Actor-Critic in the context of RL. W e showed in Lemma 2 that the process Wt = [ RBt, Jt] of sampling K random transitions from the RB is a Markov process. In addition, we showed in Lemma 3 that if the orig- inal Markov chain is irreducible and aperiodic, then also the RB Markov process is irreducible and aperiodic. This property is required for the existence of unique stationary distribution and for proving the convergence of the itera- tions in Algorithm 1 using SA tools. W e note that proving convergence for a general function approximation is hard. W e choose to demonstrate the convergence for a linear func- tion approximation (LF A; Bertsekas & Tsitsiklis , 1996), in order to keep the convergence proof as simple as possibleAnalysis of Stochastic Processes through Replay Buffers Figure 2. Replay buffer in reinforcement learning ﬂow diagram: The ra ndom processes described in Figure 1 are reﬂected in Algorithm 1. Here the random process that enters the RB is O which is a tuple of (S, A, S ′). The RB stores the last N transitions {Ot, . . . O t−N−1} in positions (1, . . . , N ), respectively . As time proceeds and t > N , old transition are thrown away from the RB. At each time step t, a random subset of K time steps is sampled from the RB and is denoted as Jt. W is simply [RB, J ]. In Algorithm 1 we have three different updates, Y reward t , Y critic t and Y actor t , all are averages over functions of transitions sampled fro m the RB. Then the parameters are updated accordingly . Finally , the policy parameter θt+1 is used to sample the action in transition Ot+1 that later enters to the RB. while focusing in the proof on the RB and random batches aspects of the algorithm. W e present several assumptions that are necessary for prov- ing the convergence of Algorithm 1. Assumption 4 is needed for the uniqueness of the convergence point of the critic. In addition, we choose a stateS∗ to be of value 0, i.e., V πθ (S∗) = 0 (due to Assumption 2, S∗ can be any of S ∈ S ). Assumption 5 is required in order to get a with probability 1 using the SA convergence. In our actor-critic setup we need two time-scales convergence, thus, in this assumption the critic is a ‘faster’ recursion than the actor. Assumption 1. 1. The set Θ is compact. 2. The reward |r(·, ·)| ≤ 1 for all S ∈ S , A ∈ A . Assumption 2. F or any policy πθ, the induced Markov chain of the MDP process {St}t≥0 is irreducible and ape- riodic. Assumption 3. F or any state–action pair (S, A), πθ(A|S) is continuously differentiable in the parameter θ. Assumption 4. 1. The matrix Φ has full rank. 2. The functions φ(S) are Liphschitz in S and bounded. 3. F or every w ∈ Rd, Φ w ̸= e where e is a vector of ones. Assumption 5. The step-sizes {αη t}, {αw t}, {αθ t}, t ≥ 0 satisfy ∑ ∞ tαη t = ∑ ∞ tαw t = ∑ ∞ tαθ t = ∞,∑ ∞ t(αη t)2, ∑ ∞ t(αw t)2, ∑ ∞ t(αθ t)2 < ∞ and αθ t = o(αw t). Now we are ready to prove the following theorems, regard- ing Algorithm 1. W e note that Theorem 2 and 3 state the critic and actor convergence. Theorem 2. (Convergence of the Critic to a ﬁxed point) Under Assumptions 1-5, for any given π and {ηt}, {wt} as in the updates in Algorithm 1, we have ηt → ηθ and wt → wπ with probability 1, where wπ is obtained as a unique solution to Φ ⊤CθΦ w + Φ ⊤bθ = 0 . The proof for Theorem 2 is in Section C in the supple- mentary material. It follows the proof for Lemma 5 in Bhatnagar et al. (2009). For establishing the convergence of the actor updates, we deﬁne additional terms. Let Z de- note the set of asymptotically stable equilibria of the ODE ˙θ = ˆΓ(−∇θηθ) and let Zǫ be the ǫ-neighborhood of Z. W e deﬁne ξπθ = ∑ S µθ(S) ( φ(S)⊤∇θwπθ − ∇θ ¯V πθ (S) ) . Theorem 3. (Convergence of the actor) Under Assumptions 1-5, given ǫ > 0, ∃δ > 0 such that for θt, t ≥ 0 obtained using Algorithm 1, if supθt ∥ξπθ t ∥ < δ , then θt → Z ǫ as t → ∞ with probability one. The proof for Theorem 3 is in Section D in the supple- mentary material. It follows the proof for Theorem 2 in Bhatnagar et al. (2009).Analysis of Stochastic Processes through Replay Buffers 5. Related W ork Actor critic algorithms analysis: The convergence analysis of our proposed RB-based actor critic algo- rithm is based on the Stochastic Approximation method ( Kushner & Clark , 2012). Konda & Tsitsiklis (2000) pro- posed the actor-critic algorithm, and established the asymptotic convergence for the two time-scale actor-critic, with TD( λ) learning-based critic. Bhatnagar et al. (2009) proved the convergence result for the original actor-criti c and natural actor-critic methods. Di Castro & Meir (2010) proposed a single time-scale actor-critic algorithm and proved its convergence. W orks on ﬁnite sample analysis for actor critic algorithms ( Wu et al. , 2020; Zou et al. , 2019; Dalal et al. , 2018) analyze the case of last transition update and do not analyze the RB aspects in these algorithms. Recently, Di-Castro Shashua et al. (2021) proved for the ﬁrst time the convergence of an RB-based actor critic al- gorithm. However, their algorithm and technical tools were focused on the sim-to-real challenge with multiple MDP en- vironments, and they focused only on a single sample batch from the RB instead of K samples. W e provide a proof for RB-based algorithms, with a single MDP environment and a batch ofK samples. Replay Buffer analysis: Experience replay ( Lin, 1993) is a central concept for achieving good performance in deep reinforcement learning. Deep RB-based algorithms such as deep Q-learning (DQN, Mnih et al. , 2013), deep de- terministic policy gradient (DDPG; Lillicrap et al. , 2015), actor critic with experience replay (ACER; W ang et al. , 2016), T win Delayed Deep Deterministic policy gradi- ent (TD3, Fujimoto et al. , 2018), Soft Actor Critic (SAC, Haarnoja et al. , 2018) and many others use RBs to improve performance and data efﬁciency. W e focus mainly on works that provide some RB properties analysis. Zhang & Sutton (2017) and Liu & Zou (2018) study the effect of replay buffer size on the agent per- formance . Fedus et al. (2020) investigated through sim- ulated experiments how the learning process is affected by the ratio of learning updates to experience collected. Other works focus on methods to prioritize samples in the RB and provide experimental results to emphasis perfor- mance improvement when using prioritized sampling from RB ( Schaul et al. , 2015; Pan et al. , 2018; Zha et al. , 2019; Horgan et al. , 2018; Lahire et al. , 2021). W e, on the other hand, focus on the theoretical aspects of RB properties and provide convergence results for RB-based algorithms. Lazic et al. (2021) proposed a RB version for a regularized policy iteration algorithm. They provide an additional mo- tivation for using RBs, in addition to the advantage of re- duced temporal correlations: They claim that using RB in online learning in MDPs can approximate well the average of past value functions. Their analysis also suggests a new objective for sub-sampling or priority-sampling transiti ons in the RB, which differs priority-sampling objectives of pr e- vious work ( Schaul et al. , 2015). Regarding RB analysis in Deep RL algorithms, Fan et al. (2020) performed a ﬁnite sample analysis on DQN algo- rithm ( Mnih et al. , 2013). In their analysis, they simpli- ﬁed the technique of RB with an independence assump- tion and they replaced the distribution over random batches with a ﬁxed distribution. These assumptions essentially re - duce DQN to the neural ﬁtted Q-iteration (FQI) algorithm ( Riedmiller, 2005). In our work we focus on asymptotic convergence and analyze explicitly the distribution of ran - dom batches from the RB. 6. Conclusions In this work we analyzed RB and showed some basic ran- dom processes properties of it as ergodicity, stationarity, Markovity, correlation, and covariance. The latter two are of most interest since they can explain the success of mod- ern RL algorithm based on RB. In addition, we developed theoretical tools of stochastic process analysis for replay buffers. W e provided an example of how to use these tools to analyze the convergence of an RB-based actor critic al- gorithm. Similarly, other common RB-based algorithms in reinforcement learning such as DQN ( Mnih et al. , 2013), DDPG ( Lillicrap et al. , 2015), TD3 ( Fujimoto et al. , 2018), SAC ( Haarnoja et al. , 2018) and many others can be ana- lyzed now , using the tools we developed in this work. As a future research, we propose two directions that are of great interest and complete the analysis we provided in this work: 1. Spectrum analysis of the learning processes. Since we adopted an approach of ”Signals and Systems” with random signals ( Oppenheim et al. , 1997; Porat, 2008), one can use spectrum analysis in order to dis- cover instabilities or cycles in the learning process. 2. More complex RBs. There is a large experimental body of work that tries to propose different schemes of RBs. Some of them apply different independent on RL quantities sampling techniques while other apply dependent on RL quantities schemes (e.g., prioritized RB depends on the TD signal; Schaul et al. , 2015). In this work we paved the ﬁrst steps to apply analysis on such schemes (both dependent and independent). 7. Acknowledgements This work was partially supported by the Israel Science Foundation under contract 2199/20.Analysis of Stochastic Processes through Replay Buffers References Bertsekas, D.Dynamic programming and optimal control . Athena scientiﬁc Belmont, MA, 2005. Bertsekas, D. P . and Tsitsiklis, J. N. Neuro-dynamic pro- gramming. Athena Scientiﬁc, 1996. Bhatnagar, S. and Kumar, S. A simultaneous pertur- bation stochastic approximation-based actor-critic algo - rithm for markov decision processes. IEEE T ransactions on Automatic Control , 49(4):592–598, 2004. Bhatnagar, S., Ghavamzadeh, M., Lee, M., and Sutton, R. S. Incremental natural actor-critic algorithms. In Advances in neural information processing systems , pp. 105–112, 2008. Bhatnagar, S., Sutton, R. S., Ghavamzadeh, M., and Lee, M. Natural actor–critic algorithms. Automatica, 45(11): 2471–2482, 2009. Borkar, V . S. Stochastic approximation: a dynamical sys- tems viewpoint , volume 48. Springer, 2009. Borkar, V . S. and Meyn, S. P . The ode method for con- vergence of stochastic approximation and reinforcement learning. SIAM Journal on Control and Optimization , 38 (2):447–469, 2000. Dalal, G., Sz ¨ or ´ enyi, B., Thoppe, G., and Mannor, S. Finite sample analyses for td (0) with function approximation. InProceedings of the AAAI Conference on Artiﬁcial In- telligence, 2018. Di Castro, D. and Meir, R. A convergent online single time scale actor critic algorithm. The Journal of Machine Learning Research , 11:367–410, 2010. Di-Castro Shashua, S., Di Castro, D., and Mannor, S. Sim and real: Better together. Advances in Neural Informa- tion Processing Systems , 34, 2021. Fan, J., W ang, Z., Xie, Y ., and Y ang, Z. A theoretical anal- ysis of deep q-learning. In Learning for Dynamics and Control, pp. 486–489. PMLR, 2020. Fedus, W ., Ramachandran, P ., Agarwal, R., Bengio, Y ., Larochelle, H., Rowland, M., and Dabney, W . Revisiting fundamentals of experience replay. InInternational Con- ference on Machine Learning , pp. 3061–3071. PMLR, 2020. Fujimoto, S., Hoof, H., and Meger, D. Addressing function approximation error in actor-critic methods. In Interna- tional Conference on Machine Learning , pp. 1587–1596. PMLR, 2018. Haarnoja, T ., Zhou, A., Abbeel, P ., and Levine, S. Soft actor-critic: Off-policy maximum entropy deep rein- forcement learning with a stochastic actor. InInterna- tional conference on machine learning , pp. 1861–1870. PMLR, 2018. Horgan, D., Quan, J., Budden, D., Barth-Maron, G., Hessel, M., V an Hasselt, H., and Silver, D. Dis- tributed prioritized experience replay.arXiv preprint arXiv:1803.00933 , 2018. Konda, V . R. and Tsitsiklis, J. N. Actor-critic algorithms. In Advances in neural information processing systems , pp. 1008–1014. Citeseer, 2000. Kushner, H. and Y in, G. G. Stochastic approximation and recursive algorithms and applications , volume 35. Springer Science & Business Media, 2003. Kushner, H. J. and Clark, D. S. Stochastic approximation methods for constrained and unconstrained systems , vol- ume 26. Springer Science & Business Media, 2012. Laguna, M. and Marklund, J. Business process modeling, simulation, and design . T aylor & Francis, 2013. Lahire, T ., Geist, M., and Rachelson, E. Large batch expe- rience replay. arXiv preprint arXiv:2110.01528 , 2021. Lazic, N., Y in, D., Abbasi-Y adkori, Y ., and Szepesvari, C. Improved regret bound and experience replay in regular- ized policy iteration.arXiv preprint arXiv:2102.12611 , 2021. Lillicrap, T . P ., Hunt, J. J., Pritzel, A., Heess, N., Erez, T ., T assa, Y ., Silver, D., and Wierstra, D. Continuous con- trol with deep reinforcement learning. arXiv preprint arXiv:1509.02971 , 2015. Lin, L.-J. Reinforcement learning for robots using neural networks. T echnical report, Carnegie-Mellon Univ Pitts- burgh P A School of Computer Science, 1993. Liu, R. and Zou, J. The effects of memory replay in re- inforcement learning. In 2018 56th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pp. 478–485. IEEE, 2018. Mnih, V ., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. Norris, J. R. Markov chains . Number 2 in 1. Cambridge university press, 1998. Oppenheim, A. V ., Willsky, A. S., Nawab, S. H., Hern ´ andez, G. M., et al. Signals & systems . Pearson Educaci ´ on, 1997.Analysis of Stochastic Processes through Replay Buffers Pan, Y ., Zaheer, M., White, A., Patterson, A., and White, M. Organizing experience: a deeper look at replay mech- anisms for sample-based planning in continuous state do- mains.arXiv preprint arXiv:1806.04624 , 2018. Porat, B. Digital processing of random signals: theory and methods. Courier Dover Publications, 2008. Puterman, M. L. Markov Decision Processes . Wiley and Sons, 1994. Riedmiller, M. Neural ﬁtted q iteration–ﬁrst experi- ences with a data efﬁcient neural reinforcement learning method. InEuropean conference on machine learning , pp. 317–328. Springer, 2005. Schaul, T ., Quan, J., Antonoglou, I., and Silver, D. Prioritized experience replay. arXiv preprint arXiv:1511.05952 , 2015. W ang, Z., Bapst, V ., Heess, N., Mnih, V ., Munos, R., Kavukcuoglu, K., and de Freitas, N. Sample efﬁ- cient actor-critic with experience replay.arXiv preprint arXiv:1611.01224 , 2016. Wu, Y ., Zhang, W ., Xu, P ., and Gu, Q. A ﬁnite time analysis of two time-scale actor critic methods. arXiv preprint arXiv:2005.01350 , 2020. Zha, D., Lai, K.-H., Zhou, K., and Hu, X. Experience replay optimization. arXiv preprint arXiv:1906.08387 , 2019. Zhang, S. and Sutton, R. S. A deeper look at experience replay. arXiv preprint arXiv:1712.01275 , 2017. Zou, S., Xu, T ., and Liang, Y . Finite-sample analysis for sarsa with linear function approximation. arXiv preprint arXiv:1902.02234 , 2019.Analysis of Stochastic Processes through Replay Buffers A. Proofs for Lemmas in Section 3 A.1. Proof of Lemma 1 Proof. Stationarity of RBt: Recall that stationarity (in the strong sense) means that fo r m = 1 , 2, . . . , there are times (t1, t2, . . . , t m) such that for all τ ∈ Z FX (Xt1+τ , . . . , X tm+τ ) = FX (Xt1 , . . . , X tm ), where FX (Xt1 , . . . , X tm ) is the cumulative distribution. Then, FRB (RBt1+τ , . . . , RB tm+τ ) (1) = FX (Xt1+τ −N+1, . . . , X t1+τ , Xt2+τ −N+1, . . . , X t2+τ , . . . , Xtm+τ −N+1, . . . , X tm+τ ), (2) = FX (Xt1−N+1, . . . , X t1 , Xt2−N+1, . . . , X t2 , . . . , Xtm−N+1, . . . , X tm ) (3) = FRB (RBt1 , . . . , RB tm ), where we use the RB deﬁnition in (1), stationarity of X in (2), and expressing RB based on X in (3). Stationarity of Yt: Similarly, for m = 1 , 2, . . . , there are times (t1, t2, . . . , t m) such that for all τ ∈ Z FY (Yt1+τ , . . . , Y tm+τ ) (1) = FX  1 K ∑ j∈ ¯Jt1+τ f(Xj), . . . , 1 K ∑ j∈ ¯Jtm+τ f(Xj )   (2) = ∑ ¯Jt1+τ ,..., ¯Jtm+τ F ¯Jt1+τ ,..., ¯Jtm+τ (j1, . . . , j m)× FX  1 K ∑ j∈ ¯Jt1+τ f(Xj), . . . , 1 K ∑ j∈ ¯Jtm+τ f(Xj ) ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ j1, . . . , j m   (3) = ∑ ¯Jt1 ,..., ¯Jtm F ¯Jt1 ,..., ¯Jtm (j1, . . . , j m)× FX  1 K ∑ j∈ ¯Jt1 f(Xj ), . . . , 1 K ∑ j∈ ¯Jtm f(Xj ) ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ j1, . . . , j m   (4) = FX  1 K ∑ j∈ ¯Jt1 f(Xj ), . . . , 1 K ∑ j∈ ¯Jtm f(Xj )   (5) = FY (Yt1 , . . . , Y tm ), where in (1) we use the process Y deﬁnition, in (2) we use iterated expectation, in (3) we use b oth the stationarity of X and ¯J, in (4) we use again iterated expectation, and in (5) we use Y deﬁnition. A.2. Proof of Lemma 2 Proof. W e ﬁrst note that we use some abuse of notation when referring sometimes to random variables from processes X, RB, W and J the same as their realizations. However, to avoid an overhea d of the proof, we keep the notations simple and short.Analysis of Stochastic Processes through Replay Buffers Proving Markovity requires that P (RBt+1|RBt, RBt−1, . . . , RB 0) = P (RBt+1|RBt). (A.1) P (Wt+1|Wt, Wt−1, . . . , W 0) = P (Wt+1|Wt). (A.2) W e start with proving the Markovity of RBt. Let us denote Xn2 n1 (t) ≜ {Xt−n2+1, . . . , X t−n1+1|RBt} as the set of realizations of the random variables from process X stored in the RB at time t in positions n1 to n2. Note that when a new transition is pushed to the RB into position n = 1 , the oldest transition in position n = N is thrown away, and all the transitions in the RB move one index forward. W e present h ere some observations regarding the RB that will help us through the proof: RBt = XN 1 (t) = {Xt−N+1, . . . , X t−n+1 . . . , X t} (RB deﬁnition). (A.3) XN 1 (t + 1) = {Xt+1} ∪ XN−1 1 (t) (A.4) XN−1 1 (t) ⊂ XN 1 (t) (A.5) Xt ∈ XN 1 (t) (A.6) P (Xt+1|Xt, . . . X 0) = P (Xt+1|Xt) (Since Xt is assumed to be Markovian). (A.7) P (a, b|c1, c2, . . . ) = P (a|b, c1, c2, . . . ) · P (b|c1, c2, . . . ) (Expressing joint probability (A.8) as a conditional probabilities product). P (a|b) = p(a) (If a and b are independent) . (A.9) Computing the l.h.s. of equation ( A.1) yields P (RBt+1|RBt, . . . , RB 0) (A.3) = P ( XN 1 (t + 1) ⏐ ⏐XN 1 (t), . . . , X N 1 (0) ) ( A.4) = P ( Xt+1, X N−1 1 (t) ⏐ ⏐XN 1 (t), . . . , X N 1 (0) ) ( A.8) = P ( Xt+1 ⏐ ⏐XN−1 1 (t), X N 1 (t), . . . , X N 1 (0) ) · P ( XN−1 1 (t) ⏐ ⏐XN 1 (t), . . . , X N 1 (0) ) ( A.5),(A.6),(A.7) = P (Xt+1|Xt) Similarly, computing the r.h.s of ( A.1) yields P (RBt+1|RBt) (A.3) = P ( XN 1 (t + 1) ⏐ ⏐XN 1 (t) ) ( A.4) = P ( Xt+1, X N−1 1 (t) ⏐ ⏐XN 1 (t) ) ( A.8) = P ( Xt+1 ⏐ ⏐XN−1 1 (t), X N 1 (t) ) · P ( XN−1 1 (t) ⏐ ⏐XN 1 (t) ) ( A.5),(A.6),(A.7) = P (Xt+1|Xt) Both sides of ( A.1) are equal and therefore RBt is Markovian. In addition we have that for t ≥ N: P (RBt+1|RBt) = { P (Xt+1|Xt) if Xt ∈ X1 1 (t) and RBt+1 = {Xt+1} ∪ XN−1 1 (t) 0 otherwise . Next, we prove the Markovity of Wt. Recall that Wt is deﬁned as: Wt = [ RBt, Jt] (A.10) where Jt ⊂ { t − N + 1, . . . , t } is a random subset of K time indices. By their deﬁnition, RBt and Jt are independent for all t. Computing the l.h.s. of equation ( A.2) yields P (Wt+1|Wt, . . . , W 0) (A.10) = P (RBt+1, Jt+1|RBt, Jt, . . . , RB 0, J0) (A.8) = P (RBt+1|Jt+1, RBt, Jt, . . . , RB 0, J0) · P (Jt+1|RBt, Jt, . . . , RB 0, J0) (A.1),(A.9) = P (RBt+1|RBt) · P (Jt+1) (A.9) = P (RBt+1, Jt+1|RBt, Jt) (A.10) = P (Wt+1|Wt)Analysis of Stochastic Processes through Replay Buffers W e have the required result in ( A.2), therefore Wt is Markovian. In addition, If Jt+1 is sampled according to ”unordered sampling without replacement” (deﬁned in Section 2.2), then for t ≥ N: P (Wt+1|Wt) = P (RBt+1|RBt)·P (Jt+1) =        1 (N K) P (Xt+1|Xt) if Xt ∈ X1 1 (t) and RBt+1 = {Xt+1} ∪ XN−1 1 (t) ∀Jt+1 ∈ CN,K , 0 otherwise. . A.3. Proof of Lemma 3 Proof. W e prove by contradiction. Let us assume that the process RB is neither aperiodic nor irreducible. If it is periodic, then one of the indices in the RB is periodic. Without loss of g enerality, let us assume that this is the l delayed time-steps index. But since in this index we have a periodic process, i.e ., it is the process X delayed in l steps, it contradicts the assumption that X is aperiodic. W e prove irreducibility in a similar way. Since the process Y is a deterministic function of the process RB, it must be aperiodic and irreducible as well, otherwise it will contradict the aperiodicity and irreducibility of t he process RB. Finally, since f(·) is a deterministic function, and since for each t, Yt is an image of an ergodic process Xt, i.e., each x ∈ Xt is visited inﬁnitely often, and as a results each point y ∈ supp(Yt) of the image of f(·) is visited inﬁnitely often, otherwise, it contradicts the d eterministic nature of f(·) or the ergodicity of X.Analysis of Stochastic Processes through Replay Buffers B. Auto Correlation and Covariance proofs B.1. Proof of Theorem 1 Proof. Let ¯Jt ⊂ { 1, . . . N } and ¯Jt+τ ⊂ { 1, . . . N } be subsets of K indices each. W e begin with calculating the auto- correlation of process Zt. RY (τ) = E[YtYt+τ ] (1) = E  1 K ∑ n∈ ¯Jt Zt−n+1 · 1 K ∑ m∈ ¯Jt+τ Zt+τ −m+1   (2) = E  1 K ∑ n∈ ¯Jt f(Xt−n+1) · 1 K ∑ m∈ ¯Jt+τ f(Xt+τ −m+1)   (3) = E ¯Jt, ¯Jt+τ ∼CN,K ,{Xt−n+1}n∈ ¯Jt ∼RBt,{Xt+τ −m+1}n∈ ¯Jt+τ ∼RBt+τ  1 K ∑ n∈ ¯Jt f(Xt−n+1) · 1 K ∑ m∈ ¯Jt+τ f(Xt+τ −m+1)   (4) = E ¯Jt, ¯Jt+τ ∼CN,K [ E{Xt−n+1}n∈ ¯Jt ∼RBt,{Xt+τ −m+1}n∈ ¯Jt+τ ∼RBt+τ [ 1 K ∑ n∈ ¯Jt f(Xt−n+1) · 1 K ∑ m∈ ¯Jt+τ f(Xt+τ −m+1) ⏐ ⏐ ⏐ ⏐ ⏐ ¯Jt, ¯Jt+τ ] ] (5) = E ¯Jt, ¯Jt+τ ∼CN,K [ E{Xt−n+1}n∈ ¯Jt ∼RBt,{Xt+τ −m+1}n∈ ¯Jt+τ ∼RBt+τ [ En∼ ¯Jt,m∼ ¯Jt+τ [f(Xt−n+1) · f(Xt+τ −m+1)] ⏐ ⏐ ⏐ ⏐ ⏐ ¯Jt, ¯Jt+τ ] ] (6) = E ¯Jt, ¯Jt+τ ∼CN,K [ En∼ ¯Jt,m∼ ¯Jt+τ [ EXt−n+1,Xt+τ −m+1 [f(Xt−n+1) · f(Xt+τ −m+1)] ⏐ ⏐¯Jt, ¯Jt+τ ] ] (7) = E ¯Jt, ¯Jt+τ ∼CN,K [ En∼ ¯Jt,m∼ ¯Jt+τ [ E [Zt−n+1Zt+τ −m+1] ⏐ ⏐¯Jt, ¯Jt+τ ] ] (8) = E ¯Jt, ¯Jt+τ ∼CN,K [ En∼ ¯Jt,m∼ ¯Jt+τ [ RZ (τ + n − m) ⏐ ⏐¯Jt, ¯Jt+τ ] ] (9) = Eτ ′∼ ˜Jτ [RZ (τ′)] where in (1) we used the deﬁnition of Y using the indices subse ts ¯Jt and ¯Jt+τ . In (2) used the deﬁnition of Z and in (3) we wrote the expectation explicitly. In (4) we used the condi tional expectation and in (5) we wrote 1 K ∑ n∈ ¯Jt f(·) and 1 K ∑ m∈ ¯Jt+τ f(·) as an expectations since given the subsets ¯Jt and ¯Jt+τ , the probability of sampling index n or m from the RB is uniform and equals 1 K . In (6) we are left with the marginal expectations for every p ossible couple of indices. In (7) we used again the deﬁnition of Z and in (8) we used the deﬁ nition of the auto-correlation function of Z. In (9) we deﬁned τ′ = τ + n − m to be the time difference between each couple of indices from ¯Jt and ¯Jt+τ . Note that τ′ ∈ ˜Jτ where ˜Jτ = {τ − N + 1, . . . , τ + N − 1}. The calculation for the covariance CY (τ) follows the same steps as we did for RY (τ). B.2. Proof of Lemma 4 Proof. Let ¯Jt = {¯jk}K k=1 ⊂ { N, . . . , 1} and ¯Jt+τ = {¯lk}K k=1 ⊂ { N, . . . , 1} be subsets of K indices each. Let Jt = {jk}K k=1 ⊂ { t − N + 1, . . . , t } and Jt+τ = {lk}K k=1 ⊂ { t + τ − N + 1, . . . , t + τ} be subsets of K time-steps each. The relations between these two subsets are: ¯jk = n → jk = t − n + 1 and ¯lk = m → lk = t + τ − m + 1. W e saw in Section B.1 that we can move from these two subsets into the set of all poss ible differences ˜Jτ . Recall that we deﬁned τ′ = τ + n − m to be the time difference between each couple of indices from ¯Jt and ¯Jt+τ . Note that τ′ ∈ ˜Jτ where ˜Jτ = {τ − N + 1, . . . , τ + N − 1}.Analysis of Stochastic Processes through Replay Buffers W e now deﬁne some sets and multisets that will help us in the ca lculation of P (τ′). Deﬁnition 2. Let CN,K be the set of all possible permutations of choosing K samples out of N samples without replacement. Observe that: |CN,K | = (N K ) Deﬁnition 3. Let LN,K be the set of all possible tuples of two batches of K samples ch osen from a set of N samples. Observe that: |LN,K | = ((N K )) 2 Deﬁnition 4. Let MN,K be the multiset of all possible two-sample tuple generated f or all possible couple of batches in the set LN,K . Let ¯MN,K be the unique set of MN,K : Observe that: |MN,K | = K2 · ((N K )) 2 | ¯MN,K | = N2 Deﬁnition 5. Let M(n, m) be the number of times the unique tuple (n, m) appears in the multiset MN,K . Observe that: M(n, m) = ((N − 1 K − 1 )) 2 for all n ∈ { 1, . . . , N }, m ∈ { τ, . . . , N + τ} Deﬁnition 6. Let DN,K,τ ′ be the number of unique sample tuples which have the time diff erence such that: n−m = τ′ −τ Observe that: DN,K,τ ′ = N − |τ′ − τ| Here we consider the ”unordered sampling without replaceme nt” (described in Section 2) for sampling ¯Jt and ¯Jt+τ and we would like to calculate the probability distribution for ea ch time difference τ′, that is P (τ′). W e have total of K2 · ((N K ) ) 2 such differences since we have (N K ) possible permutations for each batch and in each permutatio n we have K time elements. W e saw in the deﬁnitions above that from all K2 · ((N K ) ) 2 possible two-sample couples we have N2 unique sample couples, each of which has ((N−1 K−1 ) ) 2 repetitions. From these N2 couples, we have only N − |τ′ − τ| unique sample tuples (n, m) that holds n − m = τ′ − τ. W e deﬁne d = τ′ − τ, therefore −N + 1 ≤ d ≤ N − 1. W e now can calculate P (τ′): P (τ′) = ((N−1 K−1 ) ) 2 · (N − |τ′ − τ|) K2 · ((N K ) ) 2 (1) = ((N−1 K−1 ) ) 2 · (N − |d|) K2 · ((N K ) ) 2 (2) = (N − 1)! · (N − 1)! · K! · K! · (N − K)! · (N − K)! · (N − |d|) K2 · (K − 1)! · (K − 1)! · (N − K)! · (N − K)! · N! · N! (3) = N − |d| N2 where in (1) we substitute τ′ − τ = d. In (2) we wrote explicitly the binomial terms. In (3) we canc eled similar elements in the denominator and numerator. Notice that this probabil ity formula is relevant only for τ − N + 1 ≤ τ′ ≤ τ + N − 1Analysis of Stochastic Processes through Replay Buffers and other values of τ′ can not be reached form combining these two batches. Therefo re, P (τ′) = 0 for τ − N + 1 > τ ′ and τ′ > τ + N − 1. Interestingly, this proof shows how parameter K is canceled out, meaning this time difference distribution is independent on K. In addition, we can observe that the resulting distributio n can be considered as a convolution of two rectangles, which represents the time limits of each batch and the unifor m sampling, and the resulting convolution, a triangle which represents P (τ′). B.3. Proof of Corollary 1 Proof. Combining Theorem 1 and Lemma 4 we get: RY (τ) = Eτ ′ [RZ (τ′)] = ∑ τ ′ P (τ′)RZ (τ′) 1 = N−1∑ d=−N+1 N − |d| N2 RZ (d + τ) where in (1) we used P (τ′) from Lemma 4 and changed the variables: d = τ′ − τ for τ − N + 1 ≤ τ′ ≤ τ + N − 1. Similar development can be done to CY (τ). C. Proof of Theorem 2: A verage reward and critic convergence Proof. Recall that our TD-error update Algorithm 1 is deﬁned as δ(Oj ) = r(Sj , Aj ) − η + φ(S′ j )⊤w − φ(Sj )⊤w, where Oj = {Sj , Aj , r(Sj , Aj ), S′ j }. In the critic update in Algorithm 1 we use an empirical mean of TD-errors of several sampled observations, denoted as {Oj }j∈J . Then, the critic update is deﬁned as w′ = w + αw 1 K ∑ j∈J δ(Oj )φ(Sj ). where J is a random subset of K samples from RB with size N. Using the deﬁnition of the sampled random K indices ¯J, instead of J, we can write the update as: w′ = w + αw 1 K ∑ n∈ ¯J δ(Ot−n+1)φ(St−n+1). In this proof we follow the proof of Lemma 5 in Bhatnagar et al. (2009). Observe that the average reward and critic updates from Algorithm 1 can be written as ηt+1 = ηt + αη t ( F η t + Mη t+1 ) (C.1) wt+1 = vt + αw t ( F w t + Mw t+1 ) , (C.2) where F η t ≜ E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt r(St−n+1, At−n+1) − η ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ Ft   Mη t+1 ≜  1 K ∑ n∈ ¯Jt r(St−n+1, At−n+1) − ηt  − F η t F w t ≜ E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δ(Ot−n+1)φ(St−n+1) ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ Ft   Mw t+1 ≜ 1 K ∑ n∈ ¯Jt δ(Ot−n+1)φ(St−n+1) − F w tAnalysis of Stochastic Processes through Replay Buffers and Ft is a σ-algebra deﬁned as Ft ≜ {ητ , wτ , M η τ , M w τ : τ ≤ t}. W e use Theorem 2.2 of Borkar & Meyn (2000) to prove convergence of these iterates. Brieﬂy, this theor em states that given an iteration as in ( C.1) and ( C.2), these iterations are bounded w .p.1 if Assumption 6. 1. F η t and F w t are Lipschitz, the functions F∞(η) = lim σ→∞ F η(ση)/σ and F∞(w) = limσ→∞ F w(σw)/σ are Lipschitz, and F∞(η) and F∞(w) are asymptotically stable in the origin. 2. The sequences Mη t+1 and Mw t+1 are martingale difference noises and for some Cη 0 , Cw 0 E [ (Mη t+1)2⏐ ⏐Ft ] ≤ Cη 0 (1 + ∥ηt∥2) E [ (Mw t+1)2⏐ ⏐Ft ] ≤ Cw 0 (1 + ∥wt∥2). W e begin with the average reward update in ( C.1). The ODE describing its asymptotic behavior corresponds t o ˙η = E ¯J∼CN,K ,{Ot−n+1}n∈ ¯J ∼RB  1 K ∑ n∈ ¯Jt r(St−n+1, At−n+1) − η  ≜ F η. (C.3) F η is Lipschitz continuous in η. The function F∞(η) exists and satisﬁes F∞(η) = −η. The origin is an asymptotically stable equilibrium for the ODE ˙η = F∞(η) and the related Lyapunov function is given by η2/2. For the critic update, consider the ODE ˙w = E ¯J∼CN,K ,{Ot−n+1}n∈ ¯J ∼RB  1 K ∑ n∈ ¯J δ(Ot−n+1)φ(St−n+1)  ≜ F w In Lemma 5 we show that this ODE can be written as ˙w = Φ ⊤CθΦ w + Φ ⊤bθ, (C.4) where Cθ and bθ are deﬁned in ( 7). F w is Lipschitz continuous in w and F∞(w) exists and satisﬁes F∞(w) = Φ ⊤CθΦ w. Consider the system ˙w = F∞(w) (C.5) In assumption 4 we assume that Φ w ̸= e for every w ∈ Rd. Therefore, the only asymptotically stable equilibrium fo r ( C.5) is the origin (see the explanation in the proof of Lemma 5 in Bhatnagar et al. (2009)). Therefore, for all t ≥ 0 E [ (Mη t+1)2⏐ ⏐Ft ] ≤ Cη 0 (1 + ∥ηt∥2 + ∥wt∥2) E [ (Mw t+1)2⏐ ⏐Ft ] ≤ Cw 0 (1 + ∥ηt∥2 + ∥wt∥2) for some Cη 0 , Cw 0 < ∞. Mη t can be directly seen to be uniformly bounded almost surely. T hus, Assumptions (A1) and (A2) of Borkar & Meyn (2000) are satisﬁed for the average reward, TD-error, and critic u pdates. From Theorem 2.1 of Borkar & Meyn (2000), the average reward, TD-error, and critic iterates are uni formly bounded with probability one. Note that when t → ∞ , ( C.3) has ηθ deﬁned as in ( 2) as its unique globally asymptotically stable equilibrium with V2(η) = ( η − ηθ)2 serving as the associated Lyapunov function. Next, suppose that w = wπ is a solution to the system Φ ⊤CθΦ w = 0 . Under Assumption 4, using the same arguments as in the proof of Lemma 5 in Bhatnagar et al. (2009), wπ is the unique globally asymptotically stable equilibrium o f the ODE ( C.4). Assumption 6 is now veriﬁed and under Assumption 5, the claim follows from Theorem 2.2, pp. 450 of (Borkar & Meyn , 2000).Analysis of Stochastic Processes through Replay Buffers C.1. Proof of Lemma 5 Proof. W e compute the expectation of the critic update with linear f unction approximation according to Algorithm 1. In this proof, we focus on the ”Unordered sampling without repl acement” strategy for sampling batch of K transitions from the replay buffer (see Section 2.2 for this strategy probability distribution). Recall that n is a position in the RB and it corresponds to transition Ot−n+1 = ( St−n+1, At−n+1, S′ t−n+1). W e will use the notation of ¯J ⊂ { 1, . . . , n, . . . , N } to refer the K indices sampled batches. In addition we will use the followi ng observations: P (n| ¯J, n ∈ ¯J) = 1 K , P (n| ¯J, n /∈ ¯J) = 0 (C.6) P (n ∈ ¯J) = K N , P (n /∈ ¯J) = 1 − K N P (n| ¯J) = P (n ∈ ¯J) · P (n| ¯J, n ∈ ¯J) + P (n /∈ ¯J) · P (n| ¯J, n /∈ ¯J) = K N 1 K + 0 = 1 N (C.7) P ( ¯J) = 1(N K ) (C.8) |CN,K | = (N K ) (C.9) Now we can compute the desired expectation: E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δ(Ot−n+1)φ(St−n+1)   = E ¯Jt∼CN,K  E{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δ(Ot−n+1)φ(St−n+1) ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ ¯Jt     ( C.6) = E ¯Jt∼CN,K [ E{Ot−n+1}n∈ ¯Jt ∼RBt [ En∼ ¯Jt [δ(Ot−n+1)φ(St−n+1)] ⏐ ⏐¯Jt ] ] 1 = E ¯Jt∼CN,K [ En∼ ¯Jt [ EOt−n+1 [δ(Ot−n+1)φ(St−n+1)] ] ⏐ ⏐¯Jt ] 2 = ∑ ¯Jt∈CN,K P ( ¯Jt) N∑ n=1 P (n| ¯Jt)EOt−n+1 [δ(Ot−n+1)φ(St−n+1)] ( C.7),(C.8) = ∑ ¯Jt∈CN,K 1(N K ) N∑ n=1 1 N EOt−n+1 [δ(Ot−n+1)φ(St−n+1)] (C.9) = 1 N N∑ n=1 EOt−n+1 [δ(Ot−n+1)φ(St−n+1)] 3 = 1 N N∑ n=1 ESt−n+1,At−n+1,S′ t−n+1 [( r(St−n+1, At−n+1) − η + φ(S′ t−n+1)⊤w − φ(St−n+1)⊤w ) φ(St−n+1) ] (C.10) where in (1) we are left with the marginal expectations for ea ch observation, in (2) we wrote expectations explicitly and in (3) we used the deﬁnition of the TD-error in ( 4). Next, for time t − n + 1 where 1 ≤ n ≤ N, we deﬁne the induced MC with a corresponding policy paramet er θt−n+1. For this parameter, we denote the corresponding state distr ibution vector ρt−n+1 and a transition matrix Pt−n+1 (both induced by the policy πθt−n+1 . In addition, we deﬁne the following diagonal matrix Dt−n+1 ≜ diag(ρt−n+1). Similarly to ( Bertsekas & Tsitsiklis , 1996) Lemma 6.5, pp.298, we can substitute the inner expectation ESt−n+1,At−n+1,S′ t−n+1 [( r(St−n+1, At−n+1) − η + φ(S′ t−n+1)⊤w − φ(St−n+1)⊤w ) φ(St−n+1) ] = Φ ⊤Dt−n+1 (Pt−n+1 − I) Φ w + Φ ⊤Dt−n+1(rt−n+1 − ηθe), (C.11)Analysis of Stochastic Processes through Replay Buffers where I is the |S| × |S| identity matrix, e in |S| × 1 vector of ones and rt−n+1 is a |S| × 1 vector deﬁned as rt−n+1(s) =∑ a πθt−n+1 (A|S)r(S, A). Combining equations ( 6), ( C.10) and ( C.11) yields 1 N N∑ n=1 ( Φ ⊤Dt−n+1 (Pt−n+1 − I) Φ w + Φ ⊤Dt−n+1(rt−n+1 − ηθe) ) = Φ ⊤CtΦ w + Φ ⊤bt, (C.12) In the limit, t → ∞ and ρt−n+1 → µθ for all index n. Using Cθ and bθ deﬁned in ( 7), ( C.10) can be expressed as E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δ(Ot−n+1)φ(St−n+1)  = Φ ⊤CθΦ w + Φ ⊤bθ. (C.13)Analysis of Stochastic Processes through Replay Buffers D. Proof of Theorem 3: Actor convergence Proof. Recall that our TD-error update in Algorithm 1 is deﬁned as δ(Oj ) = r(Sj , Aj ) − η + φ(S′ j )⊤w − φ(Sj )⊤w, where Oj = {Sj, Aj , r(Sj , Aj ), S′ j }. In the actor update in Algorithm 1 we use an empirical mean of TD-errors of several sampled observations, denoted as {Oj }j∈J . Then, the actor update is deﬁned as θ′ = Γ  θ − αθ 1 K ∑ j∈J δ(Oj )∇ log πθ(Aj |Sj )  . where J is a random subset of K samples from RB with size N. Using the deﬁnition of the sampled random K indices ¯J, instead of J, we can write the update as: θ′ = Γ  θ − αθ 1 K ∑ n∈ ¯J δ(Ot−n+1)∇ log πθ(At−n+1|St−n+1)  . In this proof we follow the proof of Theorem 2 in Bhatnagar et al. (2009). Let O = {S, A, S ′} and let δπ(O) = r(S, A) − η + φ(S′)⊤wπ − φ(S)⊤wπ, where wπ is the convergent parameter of the critic recursion with pro bability one (see its deﬁnition in the proof for Theorem 2). Observe that the actor parameter update from Algorithm 1 can be written as θt+1 = Γ ( θt − αθ t ( δ(O)∇θ log πθ(A|S) + F θ t − F θ t + Nθt t − Nθt t ) ) = Γ ( θt − αθ t ( Mθ t+1 + (F θ t − Nθt t ) + Nθt t ) ) where F θ t ≜ E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δ(Ot−n+1)∇θ log πθ(At−n+1|St−n+1) ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ Ft   Mθ t+1 ≜ 1 K ∑ n∈ ¯Jt δ(Ot−n+1)∇θ log πθ(At−n+1|St−n+1) − F θ t Nθ t ≜ E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δπθ (Ot−n+1)∇θ log πθ(At−n+1|St−n+1) ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ Ft   and Ft is a σ-algebra deﬁned as Ft ≜ {ητ , wτ , θτ , M η τ , M w τ , M θ τ : τ ≤ t}. Since the critic converges along the faster timescale, from Theorem 2 it follows that F θ t − Nθt t = o(1). Now , let M2(t) = t−1∑ r=0 αθ rMθ r+1, t ≥ 1. The quantities δ(O) can be seen to be uniformly bounded since from the proof in The orem 2, {ηt} and {wt} are bounded sequences. Therefore, using Assumption 5, {M2(t)} is a convergent martingale sequence ( Bhatnagar & Kumar , 2004). Consider the actor update along the slower timescale corres ponding to αθ tin Algorithm 1. Let w(·) be a vector ﬁeld on a set Θ . Deﬁne another vector ﬁeld: ˆΓ ( w(y) ) = lim 0<η→0 (Γ ( y+ηw(y) ) −y η ) . In case this limit is not unique, we let ˆΓ ( w(y) ) be the set of all possible limit points (see pp. 191 of ( Kushner & Clark , 2012)). Consider now the ODE ˙θ = ˆΓ  −E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δπθ (Ot−n+1)∇θ log πθ(At−n+1|St−n+1)     (D.1)Analysis of Stochastic Processes through Replay Buffers Substituting the result from Lemma 6, the above ODE is analogous to ˙θ = ˆΓ(−∇θηθ + ξπθ ) = ˆΓ ( − Nθ t ) (D.2) where ξπθ = ∑ S µθ(S) ( φ(S)⊤∇θwπθ − ∇θ ¯V πθ (S) ) . Consider also an associated ODE: ˙θ = ˆΓ ( − ∇θηθ ) (D.3) W e now show that h1(θt) ≜ −Nθt t is Lipschitz continuous. Here wπθ t corresponds to the weight vector to which the critic update converges along the faster timescale when the corres ponding policy is πθt (see Theorem 2). Note that µθ(S), S ∈ S is continuously differentiable in θ and have bounded derivatives. Also, ¯ηθt is continuously differentiable as well and has bounded derivative as can also be seen from ( 2). Further, wπθ t can be seen to be continuously differentiable with bounded derivatives. Finally, ∇2πθt (A|S) exists and is bounded. Thus h1(θt) is a Lipschitz continuous function and the ODE ( D.1) is well posed. LetZ denote the set of asymptotically stable equilibria of ( D.3) i.e., the local minima of ηθ, and let Zǫ be the ǫ- neighborhood of Z. T o complete the proof, we are left to show that as supθ ∥ξπθ ∥ → 0 (viz. δ → 0), the trajectories of ( D.2) converge to those of ( D.3) uniformly on compacts for the same initial condition in bot h. This claim follows the same arguments as in the proof of Theorem 2 in Bhatnagar et al. (2009). D.1. Proof of Lemma 6 Proof. W e compute the required expectation with linear function ap proximation according to Algorithm 1. Following the same steps when proving the expectation for the critic in Sec tion C.1, we have: E ¯Jt∼CN,K , {Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δπ θ (Ot−n+1)∇θ log πθ (At−n+1|St−n+1)   = 1 N N∑ n=1 ESt−n+1,A t−n+1,S ′ t−n+1 [( r(St−n+1, A t−n+1) − η + φ(S′ t−n+1)⊤w − φ(St−n+1)⊤w ) ∇θ log πθ (At−n+1|St−n+1) ] Recall the deﬁnition of the state distribution vector ρt−n+1 in Section 4.4. In the limit, t → ∞ and ρt−n+1 → µθ for all index n, then: E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δπθ (Ot−n+1)∇θ log πθ(At−n+1|St−n+1)   = ∑ S∈S µθ(S) ∑ A∈A πθ(A|S) ( r(S, A) − ηθ + ∑ S′∈S P (S′|S, A)φ(S′)⊤wπθ − φ(S)⊤wπθ ) ∇θ log πθ(A|S) W e deﬁne now the following term: ¯V πθ (S) = ∑ A∈A πθ(A|S) ¯Qπθ (S, A) = ∑ A∈A πθ(A|S) ( r(S, A) − ηθ + ∑ S′∈S P (S′|S, A)φ(S′)⊤wπθ ) , (D.4) where ¯V πθ (S) and ¯Qπθ (S, A) correspond to policy πθ. Note that here, the convergent critic parameter wπθ is used. Let’s look at the gradient of ( D.4):Analysis of Stochastic Processes through Replay Buffers ∇θ ¯V πθ (S) = ∇θ (∑ A∈A πθ(A|S) ¯Qπθ (S, A) ) = ∑ A∈A ∇θπθ(A|S) ( r(S, A) − ηθ + ∑ S′∈S P (S′|S, A)φ(S′)⊤wπθ ) + ∑ A∈A πθ(A|S) ( −∇θηθ + ∑ S′∈S P (S′|S, A)φ(S′)⊤∇θwπθ ) = ∑ A∈A ∇θπθ(A|S) ( r(S, A) − ηθ + ∑ S′∈S P (S′|S, A)φ(S′)⊤wπθ ) − ∇θηθ + ∑ A∈A πθ(A|S) ∑ S′∈S P (S′|S, A)φ(S′)⊤∇θwπθ Summing both sides over the stationary distribution µθ ∑ S µθ(S)∇θ ¯V πθ (S) = ∑ S µθ(S) ∑ A∈A ∇θπθ(A|S) ( r(S, A) − ηθ + ∑ S′∈S P (S′|S, A)φ(S′)⊤wπθ ) + ∑ S µθ(S) ( −∇θηθ + ∑ A∈A πθ(A|S) ∑ S′∈S P (S′|S, A)φ(S′)⊤∇θwπθ ) = E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δπθ (Ot−n+1)∇θ log πθ(At−n+1|St−n+1)   − ∇θηθ + ∑ S µθ(S) ∑ A∈A πθ(A|S) ∑ S′∈S P (S′|S, A)φ(S′)⊤∇θwπθ Then: ∇θηθ = E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δπθ (Ot−n+1)∇θ log πθ(At−n+1|St−n+1)   + ∑ S µθ(S) (∑ A∈A πθ(A|S) ∑ S′∈S P (S′|S, A)φ(S′)⊤∇θvπθ − ∇θ ¯V πθ (S) ) . Since µθ is a stationary distribution, ∑ S µθ(S) ∑ A∈A πθ(A|S) ∑ S′∈S P (S′|S, A)φ(S′)⊤∇θwπθ = ∑ S µθ(S) ∑ S′∈S Pθ(S′|S)φ(S′)⊤∇θwπθ = ∑ S′ ∑ S µθ(S)Pθ(S′|S)φ(S′)⊤∇θwπθ = ∑ S′ µθ(S′)φ(S′)⊤∇θwπθ , Then, ∇θηθ = E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δπθ (Ot−n+1)∇θ log πθ(At−n+1|St−n+1)   + ∑ S µθ(S) ( φ(S)⊤∇θwπθ − ∇θ ¯V πθ (S) ) The result follows immediately.",
      "meta_data": {
        "arxiv_id": "2206.12848v1",
        "authors": [
          "Shirli Di Castro Shashua",
          "Shie Mannor",
          "Dotan Di-Castro"
        ],
        "published_date": "2022-06-26T11:20:44Z",
        "pdf_url": "https://arxiv.org/pdf/2206.12848v1.pdf"
      }
    },
    {
      "title": "Analysis of Stochastic Processes through Replay Buffers",
      "abstract": "Replay buffers are a key component in many reinforcement learning schemes.\nYet, their theoretical properties are not fully understood. In this paper we\nanalyze a system where a stochastic process X is pushed into a replay buffer\nand then randomly sampled to generate a stochastic process Y from the replay\nbuffer. We provide an analysis of the properties of the sampled process such as\nstationarity, Markovity and autocorrelation in terms of the properties of the\noriginal process. Our theoretical analysis sheds light on why replay buffer may\nbe a good de-correlator. Our analysis provides theoretical tools for proving\nthe convergence of replay buffer based algorithms which are prevalent in\nreinforcement learning schemes.",
      "full_text": "arXiv:2206.12848v1  [cs.LG]  26 Jun 2022 Analysis of Stochastic Processes through Replay Buffers Shirli Di Castro Shashua 1 Shie Mannor 1 2 Dotan Di Castro 3 Abstract Replay buffers are a key component in many re- inforcement learning schemes. Y et, their theo- retical properties are not fully understood. In this paper we analyze a system where a stochas- tic processX is pushed into a replay buffer and then randomly sampled to generate a stochastic processY from the replay buffer. W e provide an analysis of the properties of the sampled pro- cess such as stationarity, Markovity and autocor- relation in terms of the properties of the original process. Our theoretical analysis sheds light on why replay buffer may be a good de-correlator. Our analysis provides theoretical tools for prov- ing the convergence of replay buffer based al- gorithms which are prevalent in reinforcement learning schemes. 1. Introduction A Replay buffer (RB) is a mechanism for saving past gen- erated data samples and for sampling data for off-policy reinforcement learning (RL) algorithms ( Lin, 1993). The RB serves a First-In-First-Out (FIFO) buffer with a ﬁxed capacity and it enables sampling mini-batches from previ- ously saved data points. Its structure and sampling mecha- nism provide a unique characteristic: the RB serves asde- correlator of data samples. T ypically, the agent in RL al- gorithms encounters sequences of highly correlated states and learning from these correlated data points may be prob- lematic since many deep learning algorithms suffer from high estimation variance when data samples are dependent. Thus, a mechanism that decorrelates the input such as the RB can improve data efﬁciency and reduce sample com- plexity. Since its usage in the DQN algorithm ( Mnih et al. , 2013), RB mechanism have become popular in many off- 1 T echnion Institute of T echnology , Haifa, Israel 2 NVIDIA Re- search, Israel 3 Bosch Center of AI, Haifa, Israel. Correspondence to: Shirli Di Castro Shashua <sdicastro@gmail.com>. policy RL algorithms ( Lillicrap et al. , 2015; Haarnoja et al. , 2018). Previous work has been done on the empirical beneﬁts of RB usage ( Fedus et al. , 2020; Zhang & Sutton , 2017), but still there is a lack in theoretical understanding of how the RB mechanism works. Understanding the prop- erties of RBs is crucial for convergence and ﬁnite sample analysis of algorithms that use a RB in training. For the best of our knowledge, this is the ﬁrst work to tackle these theoretical aspects. In this work we focus on the following setup. W e de- ﬁne a random processX that is pushed into a N samples size RB and analyze the characteristics of the stochastic process ofK samples that is sampled from the RB. W e analyze if properties of the original random process such as Markovity and stationarity are maintained and quantify the auto-correlation and covariance in the new RB process (later denoted by Y ) when possible. Our motivation comes from RL algorithms that use RB. Speciﬁcally, we focus on the induced Markov chain given a policy but we note that the analysis in this paper is also rel- evant to general random processes that are kept in a FIFO queue. This is relevant for domains such as First Come First Served domains ( Laguna & Marklund , 2013). Our goal is to provide analytical tools for analyzing algorithm s that use RBs. Our results can provide theoretical under- standing of phenomena seen in experiments using RBs that have never been analyzed theoretically before. Our the- ory for RBs provides tools for proving convergence of RB- based RL algorithms. Our main contributions are: 1. Formulating RBs as random processes and analyze their properties such as stationarity, Markovity, ergod- icity, auto-correlation and covariance. 2. Comparing between properties of the original random process that was pushed into the RB and the sampled process at the output of the RB. Particularly we prove that when sampling uniformly from the RB, the RB forms as a de-correlator between the sampled batches. 3. Connecting our RB theory to RL by demonstrating this connection through a RB-based actor critic RL al- gorithm that samplesK transitions from RB with size N for updating its parameters. W e prove, for the ﬁrst time, the asymptotic convergence of such RB-basedAnalysis of Stochastic Processes through Replay Buffers Figure 1. Replay buffer ﬂow diagram: Process X enters the RB which stores {Xt, . . . X t−N+1} in positions (1, . . . , N ), respectively . As time proceeds and t > N , old transitions are thrown away from the RB. At each time ste p t, a random subset Jt of K time steps is sampled from the RB. W is simply [RB, J ]. Y is the process of averaging a function over X at times from the subset J. Lastly , the process Z is simply a function applied on the variable X. Comp aring Y to Z, we can see that Z can serve as an online update whil e Y can serve as a RB-based update. actor critic algorithm. The paper is structured as follows. W e begin with present- ing the setup in Section 2. W e then state our main results regarding RB properties in Section 3. In Section 4 we con- nect between our RB theory and its use in RL and provide a convergence proof for an RB-based actor critic algorithm. Afterward, in Section 5 we position our work in existing literature and conclude in Section 6. 2. Setup for Replay Buffer Analysis 2.1. Replay Buffer Structure LetX ≜ (Xt)∞ t=0 be a stochastic process where the subscript t indicates time. The samples are dynamically pushed into a Replay Buffer (RB; Lin, 1993; Mnih et al. , 2013) of capacity N, i.e., it is a First-In-First-Out (FIFO) buffer that can hold the N latest samples. W e deﬁne the state of the RB at time t with RBt = {Xt−N+1, . . . , X t}. Suppose that the buffer cells are numbered from 1 to N. The latest observation of X is pushed into cell 1, the ob- servation before into cell 2, etc. When a new observation arrives, the observation in cell n is pushed into cell n + 1 for 1 ≤ n < N , while the observation in cell N is thrown away. The random process RB = ( RBt)∞ t=0 contains the last N samples of X. The random process Y is deﬁned as the av- erage of random K samples (without replacement) out of the N samples and applying a function f(·) : · → RD 1 1 W e note that f(·) may also depend on t but we leave that for the sake of simplicity . where D is the dimension of the algorithm 2. The function f(·) may correspond to a typical RL function that one usu- ally ﬁnd in RL algorithms such as linear function approxi- mation, T emporal Difference, etc. ( Bertsekas & Tsitsiklis , 1996). W e elaborate on possible RL functions in Section 4.2. 2.2. Replay Buffer Sampling Method W e analyze the ”unordered sampling without replacement” strategy from the RB. W e note that other sampling meth- ods may be analyzed, but we chose this speciﬁc sampling due to its popularity in many deep reinforcement learning algorithms 3. Let N be a set of indices: N = {1, . . . , N } and let ¯J be a subset of K indices from N. Given N and K, let CN,K be the set of all possible subsets ¯J for speciﬁc N and K. Then, the probability of sampling subset ¯J is pN,K binom( ¯J) = 1 (N K) ∀ ¯J ∈ CN,K , where (N K ) ≜ N! (N−K)!K! is the binomial coefﬁcient. 2.3. Replay Buffer Related Processes W e denote the set of K temporal indices of the sam- ples from RB by the random process J (corresponds to a ”Batch” in Deep Learning) where Jt = {jk}K k=1 ⊂ {t− N + 1, . . . , t } 4. Similarly, the corresponding K RB in- dices process is ¯J where ¯Jt ⊂ { 1, . . . , N }. W e remark that 2 For example, in linear function approximation of Actor-Cri tic algorithms, D is the dimension of the linear basis used to approx- imate the value function by the critic. 3 In Section 6 we discuss shortly future directions for other sampling schemes. 4 W e note that in the ﬁrst K steps the batch is of size smaller than K and in the ﬁrst N steps the RB is not full.Analysis of Stochastic Processes through Replay Buffers both Jt and ¯Jt contain identical information but one refers to the absolute time, and one to the indices of the RB. W e deﬁne the random processWt ≜ [RBt, Jt] which holds both the information on the RB as well on the sampling from it. For later usage, we deﬁne the processXt going through a function f(·) with Zt ≜ f(Xt). The resulting Yt has the structure of Yt = 1 K ∑ j∈Jt Zj = 1 K ∑ j∈Jt f(Xj). The stochastic processes relations that are described abov e are visualized in Figure 1. 3. Replay Buffer Properties In this section we analyze the properties of a random pro- cessY that is sampled from the RB and used in some RL algorithm. Speciﬁcally, we analyze stationarity, Markovi ty, ergodicity, auto-correlation, and covariance. 3.1. Stationarity, Markovity and Ergodicity The following Lemmas characterize the connection be- tween different properties of X that enter the RB and the properties of the processes RB and Y . Stationarity is not a typical desired RL property since we constantly thrive to improve the policy (and thus the in- duced policy) but we bring it here for the sake of complete- ness. Lemma 1(Stationarity). Let Xt and ¯Jt be stationary pro- cesses. Then, RBt and Yt are stationary. The proof for Lemma 1 is deferred to Section A.1 in the supplementary material. In the next Lemma we analyze when the processRB is Markovian. This property is important in RL analysis. Note thatYt is not necessarily Markovian, however, Wt is Markovian. For this, with some abuse of notation, we de- ﬁneXn2 n1 (t) ≜ {Xt−n2+1, . . . , X t−n1+1|RBt} as the set of random variables realizaions from process X stored in the RB at time t from position n1 to n2. Lemma 2 (Markovity). Let Xt be a Markov process. Then: (1) RBt and Wt are Markovian. (2) The transition proba- bilities of RBt for t ≥ N are: P (RBt+1|RBt) =      P (Xt+1|Xt) if Xt ∈ X1 1 (t) and RBt+1 = {Xt+1} ∪ XN−1 1 (t), 0 otherwise. If Jt is sampled according to ”unordered sampling without replacement”, then the transition probabilities of Wt for t ≥ N are: P (Wt+1|Wt) =        1 ( N K)P (Xt+1|Xt) ∀Jt+1 ∈ CN,K , if Xt ∈ X1 1 (t) and RBt+1 = {Xt+1} ∪ XN−1 1 (t) 0 otherwise. The proof for Lemma 2 is deferred to Section A.2 in the supplementary material. In RL, the properties of aperiodicity and irreducible (that together form ergodicity; Norris, 1998) are crucial in many convergence proofs. The following states that these proper - ties are preserved when using RB. W e denote supp (Yt) as the set of all the values Yt can take. Lemma 3 (Ergodicity). Let Xt be a Markov process that is aperiodic and irreducible. Then, RBt and Wt are aperi- odic and irreducible. Moreover , every point y ∈ supp(Yt) is visited inﬁnitely often. The proof for Lemma 3 is deferred to Section A.3 in the supplementary material. 3.2. Auto-Correlation and Covariance In this section we analyze the auto-correlation and covari- ance of the processY expressed by process X properties. When X is stationary, the auto-correlation and covariance functions for X are: RX (τ) = E[XtXt+τ ] CX (τ) = E [(Xt − E [Xt]) (Xt+τ − E [Xt+τ ])] In the same way, the deﬁnition of the auto-correlation and covariance functions for the processY are RY (τ) and CY (τ), respectively. In the following theorem we prove the relationship between the auto-correlation and covari- ance functions of processesY and X. For that, we need to deﬁne the distribution of all time differences between two batches of samples as follows. Deﬁnition 1(Time difference distribution between two batches of samples) . Consider a RB of size N. Consider taking two different random permutations (batches), de- noted by¯Ja and ¯Jb, both of length K in two possibly dif- ferent time points, ta and tb = ta + τ. Let τ′ be a random variable where its distribution Fτ ′ (·) is the probability of each difference between each sample of ¯Ja and ¯Jb. The support of τ′ is τ − N + 1 ≤ τ′ ≤ τ + N − 1. Theorem 1 (Auto-Correlation and Covariance) . Let τ be the difference between two time steps of the processes Y . Let Eτ ′ be an expectation according to the distribution Fτ ′ (·). Then: RY (τ) = Eτ ′ [RZ (τ′)] , CY (τ) = Eτ ′ [CZ (τ′)] . (1)Analysis of Stochastic Processes through Replay Buffers The proof for Theorem 1 is in Section B.1 in the supple- mentary material. W e note two things. First, we note that we did not specify how the sampling is done from the RB and it is expressed by the random variable τ′ from Deﬁni- tion 1, i.e., Eq. ( 1) is a general expression. Second, we note that we express the correlation using process Z and not process X directly, but process Z auto-correlation and covariance can be computed directly in any practical case using the relationZt = f(Xt). For the speciﬁc case of ”unordered sampling without re- placement”, we express the relation between the second moments ofZ and Y explicitly through the distribution of τ′. Lemma 4 (Time difference Distribution for uniform batches). The random variable τ′ distribution for ”un- ordered sampling without replacement” is P (τ′) =      N−|d| N2 τ′ = τ + d, d ∈ {− N + 1, . . . 0, . . . N − 1} 0 τ′ < τ − N + 1 or τ′ > τ + N − 1 . The proof for Lemma 4 is in Section B.2 in the supplemen- tary material. In the following corollary we state the exact dependence in the case of random sampling ofK samples from a RB with size N. Corollary 1. Consider process Z where sampling is ac- cording to ”unordered sampling without replacement”. Then, the auto-correlation and covarinace of the process Y are: RY (τ) = 1 N2 N−1∑ d=−N+1 (N − |d|)RZ (d + τ) CY (τ) = 1 N2 N−1∑ d=−N+1 (N − |d|)CZ (d + τ) The proof for Corollary 1 is in Section B.3 in the supple- mentary material. W e see that using a RB reduces the auto- correlation and covaraince of processZ by factor of N. In- terestingly, this reduction is independent of K. This result proves the de-correlation effect of using RBs and provides an explicit expression for that. 4. Replay Buffers in Reinforcement Learning In the previous section we analyzed properties of stochastic processes that go through a RB. In this section we analyze RBs in RL. Stabilizing learning in modern off-policy deep RL algorithms, such as Deep Q-Networks ( Mnih et al. , 2013) or DDPG ( Lillicrap et al. , 2015), is based on saving past observed transitions in a RB. Even though its use is ex- tensive, the theoretical understanding of sampling batches mechanism from a RB is still quite limited. This is our focus in this section. W e begin with describing the setup that will serve us in this section. Then we connect between the random processes as deﬁned in Section 3 and common stochastic updates used in RL. W e then describe an RB-based actor-critic algorithm that uses a batch ofK samples from the RB in each param- eters update step. This type of algorithm serves as a basic example for popular usages of RBs in RL. W e note that other versions of RB-based RL algorithms (such as deep RL algorithms, value-based algorithms, discounted settings of the value function) can be analyzed with the stochastic processes tools we provide in this work. Finally, we present a full convergence proof for the RB-based actor critic algo- rithm. Despite its popularity, to the best of our knowledge, there is only handful of proofs that consider RB in RL al- gorithm analysis (e.g., Di-Castro Shashua et al. , 2021 or Lazic et al. , 2021). Most of the convergence proofs for off- policy algorithms assume that a single sample is sampled from the RB. Di-Castro Shashua et al. (2021) proved for the ﬁrst time the convergence of an RB-based algorithm. However, their algorithm and technical tools were focused on the sim-to-real challenge with multiple MDP environ- ments, and they focused only on a single sample batch from the RB instead ofK samples (which complicates the proof). Therefore, for completeness and focusing on the RB prop- erties, we provide a proof for RB-based algorithms, with a single MDP environment and a batch ofK samples. Simi- larly to previous works, we consider here a setup with lin- ear function approximation ( Bertsekas & Tsitsiklis , 1996). 4.1. Setup for Markov Decision Process An environment in RL is modeled as a Markov Deci- sion Process (MDP; Puterman, 1994), where S and A are the state space and action space, respectively. W e let P(S′|S, A) denote the probability of transitioning from state S ∈ S to state S′ ∈ S when applying action A ∈ A . W e consider a probabilistic policy πθ(A|S), parameterized by θ ∈ Θ ⊂ Rd which expresses the probability of the agent to choose an action A given that it is in state S. The MDP measure P (S′|S, A) and the policy measure πθ(A|S) induce together a Markov Chain (MC) measure Pθ(S′|S) (Pθ in matrix form). W e let µθ denote the stationary dis- tribution induced by the policy πθ. The reward function is denoted by r(S, A). In this work we focus on the average reward setting 5 . The goal of the agent is to ﬁnd a policy that maximizes the av- erage reward that the agent receives during its interaction 5 The discount factor settings can be obtained in similar way t o current setup.Analysis of Stochastic Processes through Replay Buffers with the environment. Under an ergodicity assumption, the average reward over time eventually converges to the ex- pected reward under the stationary distribution ( Bertsekas, 2005): ηθ ≜ lim T →∞ ∑ T t=0 r(St, At) T = ES∼µθ ,A∼πθ [r(S, A)]. (2) The state-value function evaluates the overall expected ac - cumulated rewards given a starting state S and a policy πθ V πθ (S) ≜ E [ ∞∑ t=0 (r(St, At) − ηθ) ⏐ ⏐ ⏐ ⏐ ⏐S0 = S, πθ ] , (3) where the actions follow the policy At ∼ πθ(·|St) and the next state follows the transition probability St+1 ∼ P (·|St, At). Let O = {S, A, S ′} be a transition from the environment. Let Ot be a transition at time t. The temporal difference error δ(O) (TD; Bertsekas & Tsitsiklis , 1996) is a random variable based on a single sampled transition from the RB, δ(O) = r(S, A) − η + φ(S′)⊤w − φ(S)⊤w, (4) where ˆV πθ w (S) = φ(S)⊤w is a linear approximation for V πθ (S), φ(S) ∈ Rd is a feature vector for state S and w ∈ Rd is the critic parameter vector. W e denote by Φ ∈ R|S|×d the matrix of all state feature vectors. 4.2. Replay Buffer as a Random Process in RL In Section 3 we compared between properties of general random process X going through a RB and yielding a pro- cess Y . In the RL context we have Xt ≜ Ot, meaning our basic component is a single transition of state-action- next-state observed at timet. In addition, we deﬁned Zt ≜ f(Xt) process where f(·) is a general function. In RL, f(·) is commonly deﬁned as the value function, the Q- function, the TD-error, the empirical average reward, the critic or actor gradients or any other function that com- putes a desirable update, based on an observed transition O. Common RL algorithms that use a single f(Ot) compu- tation in the parameters update step are commonly referred ason-policy algorithms where they update their parameters based only on the last observed transition in the Markov chain. See Figure 1 for a comparison between on-line up- dates and RB-based updates. Using the formulation of ran- dom processes we presented in Section 3, we can character- ize the on-line updates, based on a single last transition as follows: Zreward t = freward(Ot) = r(St, At) − ηt Zcritic t = fcritic(Ot) = δ(Ot)φ(St) Zactor t = factor (Ot) = δ(Ot)∇ log πθ(At|St) Algorithm 1 Linear Actor Critic with RB samples 1: Initialize Replay Buffer RB with size N. 2: Initialize actor parameters θ0, critic parameters w0 and average reward estimator η0. 3: Learning steps {αη t}, {αw t}, {αθ t}. 4: for t = 0 , . . . do 5: Interact with MDP M according to policy πθt and add the transition {St, At, r(St, At), St+1} to RB t. 6: Sample Jt - K random time indices form RBt. De- note the corresponding transitions as {Oj }j∈Jt . 7: δ(Oj ) = r(Sj , Aj ) − ηt + φ(S′ j )⊤wt − φ(Sj )⊤wt 8: Update average reward ηt+1 = ηt + αη t( 1 K ∑ j∈Jt r(Sj , Aj ) − ηt) 9: Update critic wt+1 = wt +αw t 1 K ∑ j∈Jt δ(Oj )φ(Sj ) 10: Update actor θt+1 = Γ ( θt − αθ t 1 K ∑ j∈Jt δ(Oj )∇θ log πθ(Aj |Sj ) ) 11: end for When using RB-based off-policy algorithms, the param- eters updates are computed over an average of K func- tions which are based on K transitions that were sam- pled randomly from the last stored N transitions. This exactly matches our deﬁnition of the process Y : Yt = 1 K ∑ j∈Jt f(Xj ) = 1 K ∑ j∈Jt Zj . The following updates are typical in RB-based off-policy algorithms: Y reward t = 1 K ∑ j∈Jt Zreward j = 1 K ∑ j∈Jt r(Sj , Aj ) − ηt Y critic t = 1 K ∑ j∈Jt Zcritic j = 1 K ∑ j∈Jt δ(Oj )φ(Sj ) Y actor t = 1 K ∑ j∈Jt Zactor j = 1 K ∑ j∈Jt δ(Oj )∇ log πθ(Aj |Sj ) (5) In Algorithm 1, we present a linear actor critic algorithm based on RB samples where the algorithm updates the ac- tor and critic using a random batch of transitions from the RB. In Section 4.5 we show how the results from Section 3 regarding a random process that is pushed into the RB, and the deﬁnitions ofX and Y processes are helpful in proving the asymptotic convergence of this algorithm. 4.3. Linear Actor Critic with RB Samples Algorithm The basic RB-based algorithm we analyze in this work is presented in Algorithm 1. W e propose a two time scale linear actor critic optimization scheme (similarly t o Konda & Tsitsiklis , 2000), which is an RB-based version of Bhatnagar et al. (2008) algorithm. Our algorithm is fully described by the random process Wt = [ RBt, Jt] and by the algorithm updates Y reward t , Y critic t and Y actor t describedAnalysis of Stochastic Processes through Replay Buffers in equation ( 5). See Figure 2 for a visualized ﬂow diagram of Algorithm 1. In Algorithm 1 we consider an environment, modeled as an MDP M, and we maintain a replay buffer RB with capacity N. The agent collects transitions {S, A, r(S, A), S′} from the environment and stores them in the RB. W e train the agent in an off-policy manner. At each time stept, the agent samples Jt – a subset of K random time indices from RBt which deﬁnes the random transitions batch for optimizing the average reward, critic and actor parameters. Note that for the actor updates, we use a projectionΓ(·) that projects any θ ∈ Rd to a compact set Θ whenever θ /∈ Θ . 4.4. Expectations of Critic and Actor Updates in Algorithm 1 W e divide the convergence analysis of Algorithm 1 into two parts. The ﬁrst part, presented in this section, is unique to our paper - we describe in Lemmas 5 and 6 a closed form of the expectations of the actor and critic updates, based on a random batch ofK transitions from the RB. In the second part, presented in Section 4.5, we use Stochas- tic Approximation tools for proving the algorithm updates convergence, based on the results from Lemmas 5 and 6. W e note that Section 4.5 follows the steps of the conver- gence proofs presented by Di-Castro Shashua et al. (2021) and Bhatnagar et al. (2009). For time t − n + 1 where 1 ≤ n ≤ N, we deﬁne the in- duced MC with a corresponding policy parameter θt−n+1. For this parameter, we denote the corresponding state dis- tribution vectorρt−n+1 and a transition matrix Pt−n+1 (both induced by the policy πθt−n+1 ). Finally, we deﬁne the following diagonal matrix Dt−n+1 ≜ diag(ρt−n+1) and the reward vector rt−n+1 with elements rt−n+1(S) =∑ A πθt−n+1 (A|S)r(S, A). Based on these deﬁnitions we deﬁne Ct ≜ 1 N N∑ n=1 Dt−n+1 (Pt−n+1 − I) bt ≜ 1 N N∑ n=1 Dt−n+1 (rt−n+1 − ηθe) . (6) where I is the identity matrix and e is a vector of ones. Let Dθ ≜ diag(µθ) and deﬁne Cθ ≜ Dθ (Pθ − I) , b θ ≜ Dθ (rθ − ηθe) . (7) In our RB setting, since we have at time t a RB with the last N samples, Ct and bt in Equation ( 6) represent a superpo- sition of all related elements of these samples. For proving the convergence of the critic, we assume the policy is ﬁxed. Then, when t → ∞ , ρt−n+1 → µθ for all index n. This means that the induced MC is one for all the samples in the RB, so the sum overN disappears for Cθ and bθ. The following two lemmas compute the expectation of the critic and actor updates when using a random batch of K samples. The expectations are over all possible random batches sampled from the RB. Recall that¯Jt ⊂ { 1, . . . , N } and CN,K is the set of all possible subsets ¯J for speciﬁc N and K. These lemmas are the main results for proving con- vergence of RB-based RL algorithms. Lemma 5. Assume we have a RB with N transitions and we sample random K transitions from the RB. Then: E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δ(Ot−n+1)φ(St−n+1)  = Φ ⊤CθΦ w + Φ ⊤bθ, where Cθ and bθ are deﬁned in ( 7). Lemma 6. Assume we have a RB with N transitions and we sample random K transitions from the RB. Then: E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δπθ (Ot−n+1)∇θ log πθ(At−n+1|St−n+1)   = ∇θηθ − ∑ S µθ(S) ( φ(S)⊤∇θwπθ − ∇θ ¯V πθ (S) ) , where ¯V πθ (S) = ∑ A∈A πθ(A|S) ( r(S, A) − ηθ + ∑ S′∈S P (S′|S, A)φ(S′)⊤wπθ ) . The proofs for Lemmas 5 and 6 are in sections C.1 and D.1, respectively, in the supplementary material. 4.5. Asymptotic Convergence of Algorithm 1 W e are now ready to present the convergence theo- rems for the critic and actor in Algorithm 1. In the proof of our theorems we use tools from Stochastic Ap- proximation (SA) ( Kushner & Y in , 2003; Borkar, 2009; Bertsekas & Tsitsiklis , 1996), a standard tool in the liter- ature for analyzing iterations of processes such as two time scale Actor-Critic in the context of RL. W e showed in Lemma 2 that the process Wt = [ RBt, Jt] of sampling K random transitions from the RB is a Markov process. In addition, we showed in Lemma 3 that if the orig- inal Markov chain is irreducible and aperiodic, then also the RB Markov process is irreducible and aperiodic. This property is required for the existence of unique stationary distribution and for proving the convergence of the itera- tions in Algorithm 1 using SA tools. W e note that proving convergence for a general function approximation is hard. W e choose to demonstrate the convergence for a linear func- tion approximation (LF A; Bertsekas & Tsitsiklis , 1996), in order to keep the convergence proof as simple as possibleAnalysis of Stochastic Processes through Replay Buffers Figure 2. Replay buffer in reinforcement learning ﬂow diagram: The ra ndom processes described in Figure 1 are reﬂected in Algorithm 1. Here the random process that enters the RB is O which is a tuple of (S, A, S ′). The RB stores the last N transitions {Ot, . . . O t−N−1} in positions (1, . . . , N ), respectively . As time proceeds and t > N , old transition are thrown away from the RB. At each time step t, a random subset of K time steps is sampled from the RB and is denoted as Jt. W is simply [RB, J ]. In Algorithm 1 we have three different updates, Y reward t , Y critic t and Y actor t , all are averages over functions of transitions sampled fro m the RB. Then the parameters are updated accordingly . Finally , the policy parameter θt+1 is used to sample the action in transition Ot+1 that later enters to the RB. while focusing in the proof on the RB and random batches aspects of the algorithm. W e present several assumptions that are necessary for prov- ing the convergence of Algorithm 1. Assumption 4 is needed for the uniqueness of the convergence point of the critic. In addition, we choose a stateS∗ to be of value 0, i.e., V πθ (S∗) = 0 (due to Assumption 2, S∗ can be any of S ∈ S ). Assumption 5 is required in order to get a with probability 1 using the SA convergence. In our actor-critic setup we need two time-scales convergence, thus, in this assumption the critic is a ‘faster’ recursion than the actor. Assumption 1. 1. The set Θ is compact. 2. The reward |r(·, ·)| ≤ 1 for all S ∈ S , A ∈ A . Assumption 2. F or any policy πθ, the induced Markov chain of the MDP process {St}t≥0 is irreducible and ape- riodic. Assumption 3. F or any state–action pair (S, A), πθ(A|S) is continuously differentiable in the parameter θ. Assumption 4. 1. The matrix Φ has full rank. 2. The functions φ(S) are Liphschitz in S and bounded. 3. F or every w ∈ Rd, Φ w ̸= e where e is a vector of ones. Assumption 5. The step-sizes {αη t}, {αw t}, {αθ t}, t ≥ 0 satisfy ∑ ∞ tαη t = ∑ ∞ tαw t = ∑ ∞ tαθ t = ∞,∑ ∞ t(αη t)2, ∑ ∞ t(αw t)2, ∑ ∞ t(αθ t)2 < ∞ and αθ t = o(αw t). Now we are ready to prove the following theorems, regard- ing Algorithm 1. W e note that Theorem 2 and 3 state the critic and actor convergence. Theorem 2. (Convergence of the Critic to a ﬁxed point) Under Assumptions 1-5, for any given π and {ηt}, {wt} as in the updates in Algorithm 1, we have ηt → ηθ and wt → wπ with probability 1, where wπ is obtained as a unique solution to Φ ⊤CθΦ w + Φ ⊤bθ = 0 . The proof for Theorem 2 is in Section C in the supple- mentary material. It follows the proof for Lemma 5 in Bhatnagar et al. (2009). For establishing the convergence of the actor updates, we deﬁne additional terms. Let Z de- note the set of asymptotically stable equilibria of the ODE ˙θ = ˆΓ(−∇θηθ) and let Zǫ be the ǫ-neighborhood of Z. W e deﬁne ξπθ = ∑ S µθ(S) ( φ(S)⊤∇θwπθ − ∇θ ¯V πθ (S) ) . Theorem 3. (Convergence of the actor) Under Assumptions 1-5, given ǫ > 0, ∃δ > 0 such that for θt, t ≥ 0 obtained using Algorithm 1, if supθt ∥ξπθ t ∥ < δ , then θt → Z ǫ as t → ∞ with probability one. The proof for Theorem 3 is in Section D in the supple- mentary material. It follows the proof for Theorem 2 in Bhatnagar et al. (2009).Analysis of Stochastic Processes through Replay Buffers 5. Related W ork Actor critic algorithms analysis: The convergence analysis of our proposed RB-based actor critic algo- rithm is based on the Stochastic Approximation method ( Kushner & Clark , 2012). Konda & Tsitsiklis (2000) pro- posed the actor-critic algorithm, and established the asymptotic convergence for the two time-scale actor-critic, with TD( λ) learning-based critic. Bhatnagar et al. (2009) proved the convergence result for the original actor-criti c and natural actor-critic methods. Di Castro & Meir (2010) proposed a single time-scale actor-critic algorithm and proved its convergence. W orks on ﬁnite sample analysis for actor critic algorithms ( Wu et al. , 2020; Zou et al. , 2019; Dalal et al. , 2018) analyze the case of last transition update and do not analyze the RB aspects in these algorithms. Recently, Di-Castro Shashua et al. (2021) proved for the ﬁrst time the convergence of an RB-based actor critic al- gorithm. However, their algorithm and technical tools were focused on the sim-to-real challenge with multiple MDP en- vironments, and they focused only on a single sample batch from the RB instead of K samples. W e provide a proof for RB-based algorithms, with a single MDP environment and a batch ofK samples. Replay Buffer analysis: Experience replay ( Lin, 1993) is a central concept for achieving good performance in deep reinforcement learning. Deep RB-based algorithms such as deep Q-learning (DQN, Mnih et al. , 2013), deep de- terministic policy gradient (DDPG; Lillicrap et al. , 2015), actor critic with experience replay (ACER; W ang et al. , 2016), T win Delayed Deep Deterministic policy gradi- ent (TD3, Fujimoto et al. , 2018), Soft Actor Critic (SAC, Haarnoja et al. , 2018) and many others use RBs to improve performance and data efﬁciency. W e focus mainly on works that provide some RB properties analysis. Zhang & Sutton (2017) and Liu & Zou (2018) study the effect of replay buffer size on the agent per- formance . Fedus et al. (2020) investigated through sim- ulated experiments how the learning process is affected by the ratio of learning updates to experience collected. Other works focus on methods to prioritize samples in the RB and provide experimental results to emphasis perfor- mance improvement when using prioritized sampling from RB ( Schaul et al. , 2015; Pan et al. , 2018; Zha et al. , 2019; Horgan et al. , 2018; Lahire et al. , 2021). W e, on the other hand, focus on the theoretical aspects of RB properties and provide convergence results for RB-based algorithms. Lazic et al. (2021) proposed a RB version for a regularized policy iteration algorithm. They provide an additional mo- tivation for using RBs, in addition to the advantage of re- duced temporal correlations: They claim that using RB in online learning in MDPs can approximate well the average of past value functions. Their analysis also suggests a new objective for sub-sampling or priority-sampling transiti ons in the RB, which differs priority-sampling objectives of pr e- vious work ( Schaul et al. , 2015). Regarding RB analysis in Deep RL algorithms, Fan et al. (2020) performed a ﬁnite sample analysis on DQN algo- rithm ( Mnih et al. , 2013). In their analysis, they simpli- ﬁed the technique of RB with an independence assump- tion and they replaced the distribution over random batches with a ﬁxed distribution. These assumptions essentially re - duce DQN to the neural ﬁtted Q-iteration (FQI) algorithm ( Riedmiller, 2005). In our work we focus on asymptotic convergence and analyze explicitly the distribution of ran - dom batches from the RB. 6. Conclusions In this work we analyzed RB and showed some basic ran- dom processes properties of it as ergodicity, stationarity, Markovity, correlation, and covariance. The latter two are of most interest since they can explain the success of mod- ern RL algorithm based on RB. In addition, we developed theoretical tools of stochastic process analysis for replay buffers. W e provided an example of how to use these tools to analyze the convergence of an RB-based actor critic al- gorithm. Similarly, other common RB-based algorithms in reinforcement learning such as DQN ( Mnih et al. , 2013), DDPG ( Lillicrap et al. , 2015), TD3 ( Fujimoto et al. , 2018), SAC ( Haarnoja et al. , 2018) and many others can be ana- lyzed now , using the tools we developed in this work. As a future research, we propose two directions that are of great interest and complete the analysis we provided in this work: 1. Spectrum analysis of the learning processes. Since we adopted an approach of ”Signals and Systems” with random signals ( Oppenheim et al. , 1997; Porat, 2008), one can use spectrum analysis in order to dis- cover instabilities or cycles in the learning process. 2. More complex RBs. There is a large experimental body of work that tries to propose different schemes of RBs. Some of them apply different independent on RL quantities sampling techniques while other apply dependent on RL quantities schemes (e.g., prioritized RB depends on the TD signal; Schaul et al. , 2015). In this work we paved the ﬁrst steps to apply analysis on such schemes (both dependent and independent). 7. Acknowledgements This work was partially supported by the Israel Science Foundation under contract 2199/20.Analysis of Stochastic Processes through Replay Buffers References Bertsekas, D.Dynamic programming and optimal control . Athena scientiﬁc Belmont, MA, 2005. Bertsekas, D. P . and Tsitsiklis, J. N. Neuro-dynamic pro- gramming. Athena Scientiﬁc, 1996. Bhatnagar, S. and Kumar, S. A simultaneous pertur- bation stochastic approximation-based actor-critic algo - rithm for markov decision processes. IEEE T ransactions on Automatic Control , 49(4):592–598, 2004. Bhatnagar, S., Ghavamzadeh, M., Lee, M., and Sutton, R. S. Incremental natural actor-critic algorithms. In Advances in neural information processing systems , pp. 105–112, 2008. Bhatnagar, S., Sutton, R. S., Ghavamzadeh, M., and Lee, M. Natural actor–critic algorithms. Automatica, 45(11): 2471–2482, 2009. Borkar, V . S. Stochastic approximation: a dynamical sys- tems viewpoint , volume 48. Springer, 2009. Borkar, V . S. and Meyn, S. P . The ode method for con- vergence of stochastic approximation and reinforcement learning. SIAM Journal on Control and Optimization , 38 (2):447–469, 2000. Dalal, G., Sz ¨ or ´ enyi, B., Thoppe, G., and Mannor, S. Finite sample analyses for td (0) with function approximation. InProceedings of the AAAI Conference on Artiﬁcial In- telligence, 2018. Di Castro, D. and Meir, R. A convergent online single time scale actor critic algorithm. The Journal of Machine Learning Research , 11:367–410, 2010. Di-Castro Shashua, S., Di Castro, D., and Mannor, S. Sim and real: Better together. Advances in Neural Informa- tion Processing Systems , 34, 2021. Fan, J., W ang, Z., Xie, Y ., and Y ang, Z. A theoretical anal- ysis of deep q-learning. In Learning for Dynamics and Control, pp. 486–489. PMLR, 2020. Fedus, W ., Ramachandran, P ., Agarwal, R., Bengio, Y ., Larochelle, H., Rowland, M., and Dabney, W . Revisiting fundamentals of experience replay. InInternational Con- ference on Machine Learning , pp. 3061–3071. PMLR, 2020. Fujimoto, S., Hoof, H., and Meger, D. Addressing function approximation error in actor-critic methods. In Interna- tional Conference on Machine Learning , pp. 1587–1596. PMLR, 2018. Haarnoja, T ., Zhou, A., Abbeel, P ., and Levine, S. Soft actor-critic: Off-policy maximum entropy deep rein- forcement learning with a stochastic actor. InInterna- tional conference on machine learning , pp. 1861–1870. PMLR, 2018. Horgan, D., Quan, J., Budden, D., Barth-Maron, G., Hessel, M., V an Hasselt, H., and Silver, D. Dis- tributed prioritized experience replay.arXiv preprint arXiv:1803.00933 , 2018. Konda, V . R. and Tsitsiklis, J. N. Actor-critic algorithms. In Advances in neural information processing systems , pp. 1008–1014. Citeseer, 2000. Kushner, H. and Y in, G. G. Stochastic approximation and recursive algorithms and applications , volume 35. Springer Science & Business Media, 2003. Kushner, H. J. and Clark, D. S. Stochastic approximation methods for constrained and unconstrained systems , vol- ume 26. Springer Science & Business Media, 2012. Laguna, M. and Marklund, J. Business process modeling, simulation, and design . T aylor & Francis, 2013. Lahire, T ., Geist, M., and Rachelson, E. Large batch expe- rience replay. arXiv preprint arXiv:2110.01528 , 2021. Lazic, N., Y in, D., Abbasi-Y adkori, Y ., and Szepesvari, C. Improved regret bound and experience replay in regular- ized policy iteration.arXiv preprint arXiv:2102.12611 , 2021. Lillicrap, T . P ., Hunt, J. J., Pritzel, A., Heess, N., Erez, T ., T assa, Y ., Silver, D., and Wierstra, D. Continuous con- trol with deep reinforcement learning. arXiv preprint arXiv:1509.02971 , 2015. Lin, L.-J. Reinforcement learning for robots using neural networks. T echnical report, Carnegie-Mellon Univ Pitts- burgh P A School of Computer Science, 1993. Liu, R. and Zou, J. The effects of memory replay in re- inforcement learning. In 2018 56th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pp. 478–485. IEEE, 2018. Mnih, V ., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. Norris, J. R. Markov chains . Number 2 in 1. Cambridge university press, 1998. Oppenheim, A. V ., Willsky, A. S., Nawab, S. H., Hern ´ andez, G. M., et al. Signals & systems . Pearson Educaci ´ on, 1997.Analysis of Stochastic Processes through Replay Buffers Pan, Y ., Zaheer, M., White, A., Patterson, A., and White, M. Organizing experience: a deeper look at replay mech- anisms for sample-based planning in continuous state do- mains.arXiv preprint arXiv:1806.04624 , 2018. Porat, B. Digital processing of random signals: theory and methods. Courier Dover Publications, 2008. Puterman, M. L. Markov Decision Processes . Wiley and Sons, 1994. Riedmiller, M. Neural ﬁtted q iteration–ﬁrst experi- ences with a data efﬁcient neural reinforcement learning method. InEuropean conference on machine learning , pp. 317–328. Springer, 2005. Schaul, T ., Quan, J., Antonoglou, I., and Silver, D. Prioritized experience replay. arXiv preprint arXiv:1511.05952 , 2015. W ang, Z., Bapst, V ., Heess, N., Mnih, V ., Munos, R., Kavukcuoglu, K., and de Freitas, N. Sample efﬁ- cient actor-critic with experience replay.arXiv preprint arXiv:1611.01224 , 2016. Wu, Y ., Zhang, W ., Xu, P ., and Gu, Q. A ﬁnite time analysis of two time-scale actor critic methods. arXiv preprint arXiv:2005.01350 , 2020. Zha, D., Lai, K.-H., Zhou, K., and Hu, X. Experience replay optimization. arXiv preprint arXiv:1906.08387 , 2019. Zhang, S. and Sutton, R. S. A deeper look at experience replay. arXiv preprint arXiv:1712.01275 , 2017. Zou, S., Xu, T ., and Liang, Y . Finite-sample analysis for sarsa with linear function approximation. arXiv preprint arXiv:1902.02234 , 2019.Analysis of Stochastic Processes through Replay Buffers A. Proofs for Lemmas in Section 3 A.1. Proof of Lemma 1 Proof. Stationarity of RBt: Recall that stationarity (in the strong sense) means that fo r m = 1 , 2, . . . , there are times (t1, t2, . . . , t m) such that for all τ ∈ Z FX (Xt1+τ , . . . , X tm+τ ) = FX (Xt1 , . . . , X tm ), where FX (Xt1 , . . . , X tm ) is the cumulative distribution. Then, FRB (RBt1+τ , . . . , RB tm+τ ) (1) = FX (Xt1+τ −N+1, . . . , X t1+τ , Xt2+τ −N+1, . . . , X t2+τ , . . . , Xtm+τ −N+1, . . . , X tm+τ ), (2) = FX (Xt1−N+1, . . . , X t1 , Xt2−N+1, . . . , X t2 , . . . , Xtm−N+1, . . . , X tm ) (3) = FRB (RBt1 , . . . , RB tm ), where we use the RB deﬁnition in (1), stationarity of X in (2), and expressing RB based on X in (3). Stationarity of Yt: Similarly, for m = 1 , 2, . . . , there are times (t1, t2, . . . , t m) such that for all τ ∈ Z FY (Yt1+τ , . . . , Y tm+τ ) (1) = FX  1 K ∑ j∈ ¯Jt1+τ f(Xj), . . . , 1 K ∑ j∈ ¯Jtm+τ f(Xj )   (2) = ∑ ¯Jt1+τ ,..., ¯Jtm+τ F ¯Jt1+τ ,..., ¯Jtm+τ (j1, . . . , j m)× FX  1 K ∑ j∈ ¯Jt1+τ f(Xj), . . . , 1 K ∑ j∈ ¯Jtm+τ f(Xj ) ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ j1, . . . , j m   (3) = ∑ ¯Jt1 ,..., ¯Jtm F ¯Jt1 ,..., ¯Jtm (j1, . . . , j m)× FX  1 K ∑ j∈ ¯Jt1 f(Xj ), . . . , 1 K ∑ j∈ ¯Jtm f(Xj ) ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ j1, . . . , j m   (4) = FX  1 K ∑ j∈ ¯Jt1 f(Xj ), . . . , 1 K ∑ j∈ ¯Jtm f(Xj )   (5) = FY (Yt1 , . . . , Y tm ), where in (1) we use the process Y deﬁnition, in (2) we use iterated expectation, in (3) we use b oth the stationarity of X and ¯J, in (4) we use again iterated expectation, and in (5) we use Y deﬁnition. A.2. Proof of Lemma 2 Proof. W e ﬁrst note that we use some abuse of notation when referring sometimes to random variables from processes X, RB, W and J the same as their realizations. However, to avoid an overhea d of the proof, we keep the notations simple and short.Analysis of Stochastic Processes through Replay Buffers Proving Markovity requires that P (RBt+1|RBt, RBt−1, . . . , RB 0) = P (RBt+1|RBt). (A.1) P (Wt+1|Wt, Wt−1, . . . , W 0) = P (Wt+1|Wt). (A.2) W e start with proving the Markovity of RBt. Let us denote Xn2 n1 (t) ≜ {Xt−n2+1, . . . , X t−n1+1|RBt} as the set of realizations of the random variables from process X stored in the RB at time t in positions n1 to n2. Note that when a new transition is pushed to the RB into position n = 1 , the oldest transition in position n = N is thrown away, and all the transitions in the RB move one index forward. W e present h ere some observations regarding the RB that will help us through the proof: RBt = XN 1 (t) = {Xt−N+1, . . . , X t−n+1 . . . , X t} (RB deﬁnition). (A.3) XN 1 (t + 1) = {Xt+1} ∪ XN−1 1 (t) (A.4) XN−1 1 (t) ⊂ XN 1 (t) (A.5) Xt ∈ XN 1 (t) (A.6) P (Xt+1|Xt, . . . X 0) = P (Xt+1|Xt) (Since Xt is assumed to be Markovian). (A.7) P (a, b|c1, c2, . . . ) = P (a|b, c1, c2, . . . ) · P (b|c1, c2, . . . ) (Expressing joint probability (A.8) as a conditional probabilities product). P (a|b) = p(a) (If a and b are independent) . (A.9) Computing the l.h.s. of equation ( A.1) yields P (RBt+1|RBt, . . . , RB 0) (A.3) = P ( XN 1 (t + 1) ⏐ ⏐XN 1 (t), . . . , X N 1 (0) ) ( A.4) = P ( Xt+1, X N−1 1 (t) ⏐ ⏐XN 1 (t), . . . , X N 1 (0) ) ( A.8) = P ( Xt+1 ⏐ ⏐XN−1 1 (t), X N 1 (t), . . . , X N 1 (0) ) · P ( XN−1 1 (t) ⏐ ⏐XN 1 (t), . . . , X N 1 (0) ) ( A.5),(A.6),(A.7) = P (Xt+1|Xt) Similarly, computing the r.h.s of ( A.1) yields P (RBt+1|RBt) (A.3) = P ( XN 1 (t + 1) ⏐ ⏐XN 1 (t) ) ( A.4) = P ( Xt+1, X N−1 1 (t) ⏐ ⏐XN 1 (t) ) ( A.8) = P ( Xt+1 ⏐ ⏐XN−1 1 (t), X N 1 (t) ) · P ( XN−1 1 (t) ⏐ ⏐XN 1 (t) ) ( A.5),(A.6),(A.7) = P (Xt+1|Xt) Both sides of ( A.1) are equal and therefore RBt is Markovian. In addition we have that for t ≥ N: P (RBt+1|RBt) = { P (Xt+1|Xt) if Xt ∈ X1 1 (t) and RBt+1 = {Xt+1} ∪ XN−1 1 (t) 0 otherwise . Next, we prove the Markovity of Wt. Recall that Wt is deﬁned as: Wt = [ RBt, Jt] (A.10) where Jt ⊂ { t − N + 1, . . . , t } is a random subset of K time indices. By their deﬁnition, RBt and Jt are independent for all t. Computing the l.h.s. of equation ( A.2) yields P (Wt+1|Wt, . . . , W 0) (A.10) = P (RBt+1, Jt+1|RBt, Jt, . . . , RB 0, J0) (A.8) = P (RBt+1|Jt+1, RBt, Jt, . . . , RB 0, J0) · P (Jt+1|RBt, Jt, . . . , RB 0, J0) (A.1),(A.9) = P (RBt+1|RBt) · P (Jt+1) (A.9) = P (RBt+1, Jt+1|RBt, Jt) (A.10) = P (Wt+1|Wt)Analysis of Stochastic Processes through Replay Buffers W e have the required result in ( A.2), therefore Wt is Markovian. In addition, If Jt+1 is sampled according to ”unordered sampling without replacement” (deﬁned in Section 2.2), then for t ≥ N: P (Wt+1|Wt) = P (RBt+1|RBt)·P (Jt+1) =        1 (N K) P (Xt+1|Xt) if Xt ∈ X1 1 (t) and RBt+1 = {Xt+1} ∪ XN−1 1 (t) ∀Jt+1 ∈ CN,K , 0 otherwise. . A.3. Proof of Lemma 3 Proof. W e prove by contradiction. Let us assume that the process RB is neither aperiodic nor irreducible. If it is periodic, then one of the indices in the RB is periodic. Without loss of g enerality, let us assume that this is the l delayed time-steps index. But since in this index we have a periodic process, i.e ., it is the process X delayed in l steps, it contradicts the assumption that X is aperiodic. W e prove irreducibility in a similar way. Since the process Y is a deterministic function of the process RB, it must be aperiodic and irreducible as well, otherwise it will contradict the aperiodicity and irreducibility of t he process RB. Finally, since f(·) is a deterministic function, and since for each t, Yt is an image of an ergodic process Xt, i.e., each x ∈ Xt is visited inﬁnitely often, and as a results each point y ∈ supp(Yt) of the image of f(·) is visited inﬁnitely often, otherwise, it contradicts the d eterministic nature of f(·) or the ergodicity of X.Analysis of Stochastic Processes through Replay Buffers B. Auto Correlation and Covariance proofs B.1. Proof of Theorem 1 Proof. Let ¯Jt ⊂ { 1, . . . N } and ¯Jt+τ ⊂ { 1, . . . N } be subsets of K indices each. W e begin with calculating the auto- correlation of process Zt. RY (τ) = E[YtYt+τ ] (1) = E  1 K ∑ n∈ ¯Jt Zt−n+1 · 1 K ∑ m∈ ¯Jt+τ Zt+τ −m+1   (2) = E  1 K ∑ n∈ ¯Jt f(Xt−n+1) · 1 K ∑ m∈ ¯Jt+τ f(Xt+τ −m+1)   (3) = E ¯Jt, ¯Jt+τ ∼CN,K ,{Xt−n+1}n∈ ¯Jt ∼RBt,{Xt+τ −m+1}n∈ ¯Jt+τ ∼RBt+τ  1 K ∑ n∈ ¯Jt f(Xt−n+1) · 1 K ∑ m∈ ¯Jt+τ f(Xt+τ −m+1)   (4) = E ¯Jt, ¯Jt+τ ∼CN,K [ E{Xt−n+1}n∈ ¯Jt ∼RBt,{Xt+τ −m+1}n∈ ¯Jt+τ ∼RBt+τ [ 1 K ∑ n∈ ¯Jt f(Xt−n+1) · 1 K ∑ m∈ ¯Jt+τ f(Xt+τ −m+1) ⏐ ⏐ ⏐ ⏐ ⏐ ¯Jt, ¯Jt+τ ] ] (5) = E ¯Jt, ¯Jt+τ ∼CN,K [ E{Xt−n+1}n∈ ¯Jt ∼RBt,{Xt+τ −m+1}n∈ ¯Jt+τ ∼RBt+τ [ En∼ ¯Jt,m∼ ¯Jt+τ [f(Xt−n+1) · f(Xt+τ −m+1)] ⏐ ⏐ ⏐ ⏐ ⏐ ¯Jt, ¯Jt+τ ] ] (6) = E ¯Jt, ¯Jt+τ ∼CN,K [ En∼ ¯Jt,m∼ ¯Jt+τ [ EXt−n+1,Xt+τ −m+1 [f(Xt−n+1) · f(Xt+τ −m+1)] ⏐ ⏐¯Jt, ¯Jt+τ ] ] (7) = E ¯Jt, ¯Jt+τ ∼CN,K [ En∼ ¯Jt,m∼ ¯Jt+τ [ E [Zt−n+1Zt+τ −m+1] ⏐ ⏐¯Jt, ¯Jt+τ ] ] (8) = E ¯Jt, ¯Jt+τ ∼CN,K [ En∼ ¯Jt,m∼ ¯Jt+τ [ RZ (τ + n − m) ⏐ ⏐¯Jt, ¯Jt+τ ] ] (9) = Eτ ′∼ ˜Jτ [RZ (τ′)] where in (1) we used the deﬁnition of Y using the indices subse ts ¯Jt and ¯Jt+τ . In (2) used the deﬁnition of Z and in (3) we wrote the expectation explicitly. In (4) we used the condi tional expectation and in (5) we wrote 1 K ∑ n∈ ¯Jt f(·) and 1 K ∑ m∈ ¯Jt+τ f(·) as an expectations since given the subsets ¯Jt and ¯Jt+τ , the probability of sampling index n or m from the RB is uniform and equals 1 K . In (6) we are left with the marginal expectations for every p ossible couple of indices. In (7) we used again the deﬁnition of Z and in (8) we used the deﬁ nition of the auto-correlation function of Z. In (9) we deﬁned τ′ = τ + n − m to be the time difference between each couple of indices from ¯Jt and ¯Jt+τ . Note that τ′ ∈ ˜Jτ where ˜Jτ = {τ − N + 1, . . . , τ + N − 1}. The calculation for the covariance CY (τ) follows the same steps as we did for RY (τ). B.2. Proof of Lemma 4 Proof. Let ¯Jt = {¯jk}K k=1 ⊂ { N, . . . , 1} and ¯Jt+τ = {¯lk}K k=1 ⊂ { N, . . . , 1} be subsets of K indices each. Let Jt = {jk}K k=1 ⊂ { t − N + 1, . . . , t } and Jt+τ = {lk}K k=1 ⊂ { t + τ − N + 1, . . . , t + τ} be subsets of K time-steps each. The relations between these two subsets are: ¯jk = n → jk = t − n + 1 and ¯lk = m → lk = t + τ − m + 1. W e saw in Section B.1 that we can move from these two subsets into the set of all poss ible differences ˜Jτ . Recall that we deﬁned τ′ = τ + n − m to be the time difference between each couple of indices from ¯Jt and ¯Jt+τ . Note that τ′ ∈ ˜Jτ where ˜Jτ = {τ − N + 1, . . . , τ + N − 1}.Analysis of Stochastic Processes through Replay Buffers W e now deﬁne some sets and multisets that will help us in the ca lculation of P (τ′). Deﬁnition 2. Let CN,K be the set of all possible permutations of choosing K samples out of N samples without replacement. Observe that: |CN,K | = (N K ) Deﬁnition 3. Let LN,K be the set of all possible tuples of two batches of K samples ch osen from a set of N samples. Observe that: |LN,K | = ((N K )) 2 Deﬁnition 4. Let MN,K be the multiset of all possible two-sample tuple generated f or all possible couple of batches in the set LN,K . Let ¯MN,K be the unique set of MN,K : Observe that: |MN,K | = K2 · ((N K )) 2 | ¯MN,K | = N2 Deﬁnition 5. Let M(n, m) be the number of times the unique tuple (n, m) appears in the multiset MN,K . Observe that: M(n, m) = ((N − 1 K − 1 )) 2 for all n ∈ { 1, . . . , N }, m ∈ { τ, . . . , N + τ} Deﬁnition 6. Let DN,K,τ ′ be the number of unique sample tuples which have the time diff erence such that: n−m = τ′ −τ Observe that: DN,K,τ ′ = N − |τ′ − τ| Here we consider the ”unordered sampling without replaceme nt” (described in Section 2) for sampling ¯Jt and ¯Jt+τ and we would like to calculate the probability distribution for ea ch time difference τ′, that is P (τ′). W e have total of K2 · ((N K ) ) 2 such differences since we have (N K ) possible permutations for each batch and in each permutatio n we have K time elements. W e saw in the deﬁnitions above that from all K2 · ((N K ) ) 2 possible two-sample couples we have N2 unique sample couples, each of which has ((N−1 K−1 ) ) 2 repetitions. From these N2 couples, we have only N − |τ′ − τ| unique sample tuples (n, m) that holds n − m = τ′ − τ. W e deﬁne d = τ′ − τ, therefore −N + 1 ≤ d ≤ N − 1. W e now can calculate P (τ′): P (τ′) = ((N−1 K−1 ) ) 2 · (N − |τ′ − τ|) K2 · ((N K ) ) 2 (1) = ((N−1 K−1 ) ) 2 · (N − |d|) K2 · ((N K ) ) 2 (2) = (N − 1)! · (N − 1)! · K! · K! · (N − K)! · (N − K)! · (N − |d|) K2 · (K − 1)! · (K − 1)! · (N − K)! · (N − K)! · N! · N! (3) = N − |d| N2 where in (1) we substitute τ′ − τ = d. In (2) we wrote explicitly the binomial terms. In (3) we canc eled similar elements in the denominator and numerator. Notice that this probabil ity formula is relevant only for τ − N + 1 ≤ τ′ ≤ τ + N − 1Analysis of Stochastic Processes through Replay Buffers and other values of τ′ can not be reached form combining these two batches. Therefo re, P (τ′) = 0 for τ − N + 1 > τ ′ and τ′ > τ + N − 1. Interestingly, this proof shows how parameter K is canceled out, meaning this time difference distribution is independent on K. In addition, we can observe that the resulting distributio n can be considered as a convolution of two rectangles, which represents the time limits of each batch and the unifor m sampling, and the resulting convolution, a triangle which represents P (τ′). B.3. Proof of Corollary 1 Proof. Combining Theorem 1 and Lemma 4 we get: RY (τ) = Eτ ′ [RZ (τ′)] = ∑ τ ′ P (τ′)RZ (τ′) 1 = N−1∑ d=−N+1 N − |d| N2 RZ (d + τ) where in (1) we used P (τ′) from Lemma 4 and changed the variables: d = τ′ − τ for τ − N + 1 ≤ τ′ ≤ τ + N − 1. Similar development can be done to CY (τ). C. Proof of Theorem 2: A verage reward and critic convergence Proof. Recall that our TD-error update Algorithm 1 is deﬁned as δ(Oj ) = r(Sj , Aj ) − η + φ(S′ j )⊤w − φ(Sj )⊤w, where Oj = {Sj , Aj , r(Sj , Aj ), S′ j }. In the critic update in Algorithm 1 we use an empirical mean of TD-errors of several sampled observations, denoted as {Oj }j∈J . Then, the critic update is deﬁned as w′ = w + αw 1 K ∑ j∈J δ(Oj )φ(Sj ). where J is a random subset of K samples from RB with size N. Using the deﬁnition of the sampled random K indices ¯J, instead of J, we can write the update as: w′ = w + αw 1 K ∑ n∈ ¯J δ(Ot−n+1)φ(St−n+1). In this proof we follow the proof of Lemma 5 in Bhatnagar et al. (2009). Observe that the average reward and critic updates from Algorithm 1 can be written as ηt+1 = ηt + αη t ( F η t + Mη t+1 ) (C.1) wt+1 = vt + αw t ( F w t + Mw t+1 ) , (C.2) where F η t ≜ E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt r(St−n+1, At−n+1) − η ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ Ft   Mη t+1 ≜  1 K ∑ n∈ ¯Jt r(St−n+1, At−n+1) − ηt  − F η t F w t ≜ E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δ(Ot−n+1)φ(St−n+1) ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ Ft   Mw t+1 ≜ 1 K ∑ n∈ ¯Jt δ(Ot−n+1)φ(St−n+1) − F w tAnalysis of Stochastic Processes through Replay Buffers and Ft is a σ-algebra deﬁned as Ft ≜ {ητ , wτ , M η τ , M w τ : τ ≤ t}. W e use Theorem 2.2 of Borkar & Meyn (2000) to prove convergence of these iterates. Brieﬂy, this theor em states that given an iteration as in ( C.1) and ( C.2), these iterations are bounded w .p.1 if Assumption 6. 1. F η t and F w t are Lipschitz, the functions F∞(η) = lim σ→∞ F η(ση)/σ and F∞(w) = limσ→∞ F w(σw)/σ are Lipschitz, and F∞(η) and F∞(w) are asymptotically stable in the origin. 2. The sequences Mη t+1 and Mw t+1 are martingale difference noises and for some Cη 0 , Cw 0 E [ (Mη t+1)2⏐ ⏐Ft ] ≤ Cη 0 (1 + ∥ηt∥2) E [ (Mw t+1)2⏐ ⏐Ft ] ≤ Cw 0 (1 + ∥wt∥2). W e begin with the average reward update in ( C.1). The ODE describing its asymptotic behavior corresponds t o ˙η = E ¯J∼CN,K ,{Ot−n+1}n∈ ¯J ∼RB  1 K ∑ n∈ ¯Jt r(St−n+1, At−n+1) − η  ≜ F η. (C.3) F η is Lipschitz continuous in η. The function F∞(η) exists and satisﬁes F∞(η) = −η. The origin is an asymptotically stable equilibrium for the ODE ˙η = F∞(η) and the related Lyapunov function is given by η2/2. For the critic update, consider the ODE ˙w = E ¯J∼CN,K ,{Ot−n+1}n∈ ¯J ∼RB  1 K ∑ n∈ ¯J δ(Ot−n+1)φ(St−n+1)  ≜ F w In Lemma 5 we show that this ODE can be written as ˙w = Φ ⊤CθΦ w + Φ ⊤bθ, (C.4) where Cθ and bθ are deﬁned in ( 7). F w is Lipschitz continuous in w and F∞(w) exists and satisﬁes F∞(w) = Φ ⊤CθΦ w. Consider the system ˙w = F∞(w) (C.5) In assumption 4 we assume that Φ w ̸= e for every w ∈ Rd. Therefore, the only asymptotically stable equilibrium fo r ( C.5) is the origin (see the explanation in the proof of Lemma 5 in Bhatnagar et al. (2009)). Therefore, for all t ≥ 0 E [ (Mη t+1)2⏐ ⏐Ft ] ≤ Cη 0 (1 + ∥ηt∥2 + ∥wt∥2) E [ (Mw t+1)2⏐ ⏐Ft ] ≤ Cw 0 (1 + ∥ηt∥2 + ∥wt∥2) for some Cη 0 , Cw 0 < ∞. Mη t can be directly seen to be uniformly bounded almost surely. T hus, Assumptions (A1) and (A2) of Borkar & Meyn (2000) are satisﬁed for the average reward, TD-error, and critic u pdates. From Theorem 2.1 of Borkar & Meyn (2000), the average reward, TD-error, and critic iterates are uni formly bounded with probability one. Note that when t → ∞ , ( C.3) has ηθ deﬁned as in ( 2) as its unique globally asymptotically stable equilibrium with V2(η) = ( η − ηθ)2 serving as the associated Lyapunov function. Next, suppose that w = wπ is a solution to the system Φ ⊤CθΦ w = 0 . Under Assumption 4, using the same arguments as in the proof of Lemma 5 in Bhatnagar et al. (2009), wπ is the unique globally asymptotically stable equilibrium o f the ODE ( C.4). Assumption 6 is now veriﬁed and under Assumption 5, the claim follows from Theorem 2.2, pp. 450 of (Borkar & Meyn , 2000).Analysis of Stochastic Processes through Replay Buffers C.1. Proof of Lemma 5 Proof. W e compute the expectation of the critic update with linear f unction approximation according to Algorithm 1. In this proof, we focus on the ”Unordered sampling without repl acement” strategy for sampling batch of K transitions from the replay buffer (see Section 2.2 for this strategy probability distribution). Recall that n is a position in the RB and it corresponds to transition Ot−n+1 = ( St−n+1, At−n+1, S′ t−n+1). W e will use the notation of ¯J ⊂ { 1, . . . , n, . . . , N } to refer the K indices sampled batches. In addition we will use the followi ng observations: P (n| ¯J, n ∈ ¯J) = 1 K , P (n| ¯J, n /∈ ¯J) = 0 (C.6) P (n ∈ ¯J) = K N , P (n /∈ ¯J) = 1 − K N P (n| ¯J) = P (n ∈ ¯J) · P (n| ¯J, n ∈ ¯J) + P (n /∈ ¯J) · P (n| ¯J, n /∈ ¯J) = K N 1 K + 0 = 1 N (C.7) P ( ¯J) = 1(N K ) (C.8) |CN,K | = (N K ) (C.9) Now we can compute the desired expectation: E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δ(Ot−n+1)φ(St−n+1)   = E ¯Jt∼CN,K  E{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δ(Ot−n+1)φ(St−n+1) ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ ¯Jt     ( C.6) = E ¯Jt∼CN,K [ E{Ot−n+1}n∈ ¯Jt ∼RBt [ En∼ ¯Jt [δ(Ot−n+1)φ(St−n+1)] ⏐ ⏐¯Jt ] ] 1 = E ¯Jt∼CN,K [ En∼ ¯Jt [ EOt−n+1 [δ(Ot−n+1)φ(St−n+1)] ] ⏐ ⏐¯Jt ] 2 = ∑ ¯Jt∈CN,K P ( ¯Jt) N∑ n=1 P (n| ¯Jt)EOt−n+1 [δ(Ot−n+1)φ(St−n+1)] ( C.7),(C.8) = ∑ ¯Jt∈CN,K 1(N K ) N∑ n=1 1 N EOt−n+1 [δ(Ot−n+1)φ(St−n+1)] (C.9) = 1 N N∑ n=1 EOt−n+1 [δ(Ot−n+1)φ(St−n+1)] 3 = 1 N N∑ n=1 ESt−n+1,At−n+1,S′ t−n+1 [( r(St−n+1, At−n+1) − η + φ(S′ t−n+1)⊤w − φ(St−n+1)⊤w ) φ(St−n+1) ] (C.10) where in (1) we are left with the marginal expectations for ea ch observation, in (2) we wrote expectations explicitly and in (3) we used the deﬁnition of the TD-error in ( 4). Next, for time t − n + 1 where 1 ≤ n ≤ N, we deﬁne the induced MC with a corresponding policy paramet er θt−n+1. For this parameter, we denote the corresponding state distr ibution vector ρt−n+1 and a transition matrix Pt−n+1 (both induced by the policy πθt−n+1 . In addition, we deﬁne the following diagonal matrix Dt−n+1 ≜ diag(ρt−n+1). Similarly to ( Bertsekas & Tsitsiklis , 1996) Lemma 6.5, pp.298, we can substitute the inner expectation ESt−n+1,At−n+1,S′ t−n+1 [( r(St−n+1, At−n+1) − η + φ(S′ t−n+1)⊤w − φ(St−n+1)⊤w ) φ(St−n+1) ] = Φ ⊤Dt−n+1 (Pt−n+1 − I) Φ w + Φ ⊤Dt−n+1(rt−n+1 − ηθe), (C.11)Analysis of Stochastic Processes through Replay Buffers where I is the |S| × |S| identity matrix, e in |S| × 1 vector of ones and rt−n+1 is a |S| × 1 vector deﬁned as rt−n+1(s) =∑ a πθt−n+1 (A|S)r(S, A). Combining equations ( 6), ( C.10) and ( C.11) yields 1 N N∑ n=1 ( Φ ⊤Dt−n+1 (Pt−n+1 − I) Φ w + Φ ⊤Dt−n+1(rt−n+1 − ηθe) ) = Φ ⊤CtΦ w + Φ ⊤bt, (C.12) In the limit, t → ∞ and ρt−n+1 → µθ for all index n. Using Cθ and bθ deﬁned in ( 7), ( C.10) can be expressed as E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δ(Ot−n+1)φ(St−n+1)  = Φ ⊤CθΦ w + Φ ⊤bθ. (C.13)Analysis of Stochastic Processes through Replay Buffers D. Proof of Theorem 3: Actor convergence Proof. Recall that our TD-error update in Algorithm 1 is deﬁned as δ(Oj ) = r(Sj , Aj ) − η + φ(S′ j )⊤w − φ(Sj )⊤w, where Oj = {Sj, Aj , r(Sj , Aj ), S′ j }. In the actor update in Algorithm 1 we use an empirical mean of TD-errors of several sampled observations, denoted as {Oj }j∈J . Then, the actor update is deﬁned as θ′ = Γ  θ − αθ 1 K ∑ j∈J δ(Oj )∇ log πθ(Aj |Sj )  . where J is a random subset of K samples from RB with size N. Using the deﬁnition of the sampled random K indices ¯J, instead of J, we can write the update as: θ′ = Γ  θ − αθ 1 K ∑ n∈ ¯J δ(Ot−n+1)∇ log πθ(At−n+1|St−n+1)  . In this proof we follow the proof of Theorem 2 in Bhatnagar et al. (2009). Let O = {S, A, S ′} and let δπ(O) = r(S, A) − η + φ(S′)⊤wπ − φ(S)⊤wπ, where wπ is the convergent parameter of the critic recursion with pro bability one (see its deﬁnition in the proof for Theorem 2). Observe that the actor parameter update from Algorithm 1 can be written as θt+1 = Γ ( θt − αθ t ( δ(O)∇θ log πθ(A|S) + F θ t − F θ t + Nθt t − Nθt t ) ) = Γ ( θt − αθ t ( Mθ t+1 + (F θ t − Nθt t ) + Nθt t ) ) where F θ t ≜ E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δ(Ot−n+1)∇θ log πθ(At−n+1|St−n+1) ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ Ft   Mθ t+1 ≜ 1 K ∑ n∈ ¯Jt δ(Ot−n+1)∇θ log πθ(At−n+1|St−n+1) − F θ t Nθ t ≜ E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δπθ (Ot−n+1)∇θ log πθ(At−n+1|St−n+1) ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ Ft   and Ft is a σ-algebra deﬁned as Ft ≜ {ητ , wτ , θτ , M η τ , M w τ , M θ τ : τ ≤ t}. Since the critic converges along the faster timescale, from Theorem 2 it follows that F θ t − Nθt t = o(1). Now , let M2(t) = t−1∑ r=0 αθ rMθ r+1, t ≥ 1. The quantities δ(O) can be seen to be uniformly bounded since from the proof in The orem 2, {ηt} and {wt} are bounded sequences. Therefore, using Assumption 5, {M2(t)} is a convergent martingale sequence ( Bhatnagar & Kumar , 2004). Consider the actor update along the slower timescale corres ponding to αθ tin Algorithm 1. Let w(·) be a vector ﬁeld on a set Θ . Deﬁne another vector ﬁeld: ˆΓ ( w(y) ) = lim 0<η→0 (Γ ( y+ηw(y) ) −y η ) . In case this limit is not unique, we let ˆΓ ( w(y) ) be the set of all possible limit points (see pp. 191 of ( Kushner & Clark , 2012)). Consider now the ODE ˙θ = ˆΓ  −E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δπθ (Ot−n+1)∇θ log πθ(At−n+1|St−n+1)     (D.1)Analysis of Stochastic Processes through Replay Buffers Substituting the result from Lemma 6, the above ODE is analogous to ˙θ = ˆΓ(−∇θηθ + ξπθ ) = ˆΓ ( − Nθ t ) (D.2) where ξπθ = ∑ S µθ(S) ( φ(S)⊤∇θwπθ − ∇θ ¯V πθ (S) ) . Consider also an associated ODE: ˙θ = ˆΓ ( − ∇θηθ ) (D.3) W e now show that h1(θt) ≜ −Nθt t is Lipschitz continuous. Here wπθ t corresponds to the weight vector to which the critic update converges along the faster timescale when the corres ponding policy is πθt (see Theorem 2). Note that µθ(S), S ∈ S is continuously differentiable in θ and have bounded derivatives. Also, ¯ηθt is continuously differentiable as well and has bounded derivative as can also be seen from ( 2). Further, wπθ t can be seen to be continuously differentiable with bounded derivatives. Finally, ∇2πθt (A|S) exists and is bounded. Thus h1(θt) is a Lipschitz continuous function and the ODE ( D.1) is well posed. LetZ denote the set of asymptotically stable equilibria of ( D.3) i.e., the local minima of ηθ, and let Zǫ be the ǫ- neighborhood of Z. T o complete the proof, we are left to show that as supθ ∥ξπθ ∥ → 0 (viz. δ → 0), the trajectories of ( D.2) converge to those of ( D.3) uniformly on compacts for the same initial condition in bot h. This claim follows the same arguments as in the proof of Theorem 2 in Bhatnagar et al. (2009). D.1. Proof of Lemma 6 Proof. W e compute the required expectation with linear function ap proximation according to Algorithm 1. Following the same steps when proving the expectation for the critic in Sec tion C.1, we have: E ¯Jt∼CN,K , {Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δπ θ (Ot−n+1)∇θ log πθ (At−n+1|St−n+1)   = 1 N N∑ n=1 ESt−n+1,A t−n+1,S ′ t−n+1 [( r(St−n+1, A t−n+1) − η + φ(S′ t−n+1)⊤w − φ(St−n+1)⊤w ) ∇θ log πθ (At−n+1|St−n+1) ] Recall the deﬁnition of the state distribution vector ρt−n+1 in Section 4.4. In the limit, t → ∞ and ρt−n+1 → µθ for all index n, then: E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δπθ (Ot−n+1)∇θ log πθ(At−n+1|St−n+1)   = ∑ S∈S µθ(S) ∑ A∈A πθ(A|S) ( r(S, A) − ηθ + ∑ S′∈S P (S′|S, A)φ(S′)⊤wπθ − φ(S)⊤wπθ ) ∇θ log πθ(A|S) W e deﬁne now the following term: ¯V πθ (S) = ∑ A∈A πθ(A|S) ¯Qπθ (S, A) = ∑ A∈A πθ(A|S) ( r(S, A) − ηθ + ∑ S′∈S P (S′|S, A)φ(S′)⊤wπθ ) , (D.4) where ¯V πθ (S) and ¯Qπθ (S, A) correspond to policy πθ. Note that here, the convergent critic parameter wπθ is used. Let’s look at the gradient of ( D.4):Analysis of Stochastic Processes through Replay Buffers ∇θ ¯V πθ (S) = ∇θ (∑ A∈A πθ(A|S) ¯Qπθ (S, A) ) = ∑ A∈A ∇θπθ(A|S) ( r(S, A) − ηθ + ∑ S′∈S P (S′|S, A)φ(S′)⊤wπθ ) + ∑ A∈A πθ(A|S) ( −∇θηθ + ∑ S′∈S P (S′|S, A)φ(S′)⊤∇θwπθ ) = ∑ A∈A ∇θπθ(A|S) ( r(S, A) − ηθ + ∑ S′∈S P (S′|S, A)φ(S′)⊤wπθ ) − ∇θηθ + ∑ A∈A πθ(A|S) ∑ S′∈S P (S′|S, A)φ(S′)⊤∇θwπθ Summing both sides over the stationary distribution µθ ∑ S µθ(S)∇θ ¯V πθ (S) = ∑ S µθ(S) ∑ A∈A ∇θπθ(A|S) ( r(S, A) − ηθ + ∑ S′∈S P (S′|S, A)φ(S′)⊤wπθ ) + ∑ S µθ(S) ( −∇θηθ + ∑ A∈A πθ(A|S) ∑ S′∈S P (S′|S, A)φ(S′)⊤∇θwπθ ) = E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δπθ (Ot−n+1)∇θ log πθ(At−n+1|St−n+1)   − ∇θηθ + ∑ S µθ(S) ∑ A∈A πθ(A|S) ∑ S′∈S P (S′|S, A)φ(S′)⊤∇θwπθ Then: ∇θηθ = E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δπθ (Ot−n+1)∇θ log πθ(At−n+1|St−n+1)   + ∑ S µθ(S) (∑ A∈A πθ(A|S) ∑ S′∈S P (S′|S, A)φ(S′)⊤∇θvπθ − ∇θ ¯V πθ (S) ) . Since µθ is a stationary distribution, ∑ S µθ(S) ∑ A∈A πθ(A|S) ∑ S′∈S P (S′|S, A)φ(S′)⊤∇θwπθ = ∑ S µθ(S) ∑ S′∈S Pθ(S′|S)φ(S′)⊤∇θwπθ = ∑ S′ ∑ S µθ(S)Pθ(S′|S)φ(S′)⊤∇θwπθ = ∑ S′ µθ(S′)φ(S′)⊤∇θwπθ , Then, ∇θηθ = E ¯Jt∼CN,K ,{Ot−n+1}n∈ ¯Jt ∼RBt  1 K ∑ n∈ ¯Jt δπθ (Ot−n+1)∇θ log πθ(At−n+1|St−n+1)   + ∑ S µθ(S) ( φ(S)⊤∇θwπθ − ∇θ ¯V πθ (S) ) The result follows immediately.",
      "meta_data": {
        "arxiv_id": "2206.12848v1",
        "authors": [
          "Shirli Di Castro Shashua",
          "Shie Mannor",
          "Dotan Di-Castro"
        ],
        "published_date": "2022-06-26T11:20:44Z",
        "pdf_url": "https://arxiv.org/pdf/2206.12848v1.pdf"
      }
    },
    {
      "title": "On Efficient Constructions of Checkpoints",
      "abstract": "Efficient construction of checkpoints/snapshots is a critical tool for\ntraining and diagnosing deep learning models. In this paper, we propose a lossy\ncompression scheme for checkpoint constructions (called LC-Checkpoint).\nLC-Checkpoint simultaneously maximizes the compression rate and optimizes the\nrecovery speed, under the assumption that SGD is used to train the model.\nLC-Checkpointuses quantization and priority promotion to store the most crucial\ninformation for SGD to recover, and then uses a Huffman coding to leverage the\nnon-uniform distribution of the gradient scales. Our extensive experiments show\nthat LC-Checkpoint achieves a compression rate up to $28\\times$ and recovery\nspeedup up to $5.77\\times$ over a state-of-the-art algorithm (SCAR).",
      "full_text": "On Efﬁcient Constructions of Checkpoints Yu Chen1 Zhenming Liu 1 Bin Ren 1 Xin Jin 2 Abstract Efﬁcient construction of checkpoints/snapshots is a critical tool for training and diagnosing deep learning models. In this paper, we propose a lossy compression scheme for checkpoint constructions (called LC-Checkpoint). LC-Checkpoint simul- taneously maximizes the compression rate and optimizes the recovery speed, under the assump- tion that SGD is used to train the model. LC- Checkpoint uses quantization and priority promo- tion to store the most crucial information for SGD to recover, and then uses a Huffman coding to leverage the non-uniform distribution of the gra- dient scales. Our extensive experiments show that LC-Checkpoint achieves a compression rate up to 28×and recovery speedup up to 5.77×over a state-of-the-art algorithm (SCAR). 1. Introduction Efﬁcient construction of checkpoints (snapshots) has been increasingly important to deep learning research. In the arms race of developing more accurate models, researchers utilize heavier computing infrastructure and develop deeper and larger models. Without proper infrastructure support, the research process inevitably becomes fragile. For exam- ple, distributed computation fails from time to time, leading to the excessive need to re-train models (Qiao et al., 2018b). Diagnosing deep learning models also evolves to a complex procedure partly because that the community has a better understanding of deep learning models and produces more rules for “debugging” them. Some common errors include gradient explosion (Goodfellow et al., 2016), “divide by zero” (Ioffe & Szegedy, 2015), and dead activation. This calls for the need to construct “breakpoints,” resembling those used in debugging computer programs, so that re- searchers can conveniently jump to the state right before the model “crashes” in the training. 1William & Mary, Williamsburg, Virginia, USA2Johns Hop- kins University, Baltimore, Maryland, USA. Correspondence to: Yu Chen <ychen39@email.wm.edu>. Proceedings of the 37 th International Conference on Machine Learning, Online, PMLR 119, 2020. Copyright 2020 by the au- thor(s). Producing checkpoints frequently enables failed training process to restart with minimum wasted time, and serves as breakpoints for debugging models. So far the standard practice of constructing checkpoints is primitive. The most common practice is to save the model state directly, counting on that the backend system is sufﬁciently robust so that this operation does not become a bottleneck (Baylor et al., 2017). Attempts of partially storing model states are also examined (Qiao et al., 2018b) but these works usually focus on recovery speed, instead of directly tackling system issues. The most pronounced technical challenge here is that deep models are usually large, so producing frequent checkpoints creates unmanageable burdens to both I/O and storage, even under modern distributed platforms (Abadi et al., 2016; Li et al., 2014; Low et al., 2012). Therefore, this leads to our question: Research Q: How can we compress model checkpoints? We speciﬁcally aim to design a lossy compressing scheme, addressing two criteria simultaneously. First, like standard compression problems, we need to maximize the compres- sion rate. Second, the scheme needs to be optimized for the downstream application of training. When a model restarts from our lossy checkpoints, it needs to efﬁciently resume to the most recent state (e.g., restart from a failed process or reach the state preceding the crash). Compression of model states is a new technical problem that requires addressing cross-cutting constraints from informa- tion theory, learning algorithm, and system design. We need to leverage statistical patterns encoded in the model state and factor in how the model states interact with a learning algo- rithm (more speciﬁcally, stochastic gradient type algorithms in the deep learning setting). This means neither standard lossy compression algorithms nor recently developed model compression algorithms (Han et al., 2015a; Courbariaux et al., 2015; Hong et al., 2016; Leng et al., 2018; Lin et al., 2016) directly work in our setting. Standard lossy compres- sion algorithms aim to minimize reconstruction error but our end goal is to enable a learning algorithm to “quickly recover.” Model compression techniques aim to transform a (static) model into a simpler one while ensuring the fore- casts are not perturbed much whereas in our setting we need a reliable coding scheme that functions well throughout the arXiv:2009.13003v1  [cs.LG]  28 Sep 2020On Efﬁcient Constructions of Checkpoints entire dynamic process of learning, which is an orthogo- nal and perhaps more challenging goal. In addition, our algorithm must be efﬁcient and scalable so that it can be executed frequently. Our solution. To achieve our aims, we focus on a delta- encoding scheme (Mogul et al., 1997), tracking only the information on the difference between two checkpoints. Un- der this scheme, we examine whether we can cut the least useful information (with respect to training) from the model state, and ensure that the remaining information is amenable for compression. A perhaps surprising message here is that ℓ2-norm reconstruction error for the “delta” appears to be an ineffective metric for minimizing the recovery time. In- stead, our algorithm ﬁrst removes all the parameters with inconsequential updates, and then quantizes the remain- der information. These strategies resemble those used in distributed training with the goal of minimizing communi- cation cost (Alistarh et al., 2017). After we obtain the most signiﬁcant information for portion of parameter updates, we represent them in suitable format and apply a Huffman cod- ing to further compress these bits, so that the compression rate can be at the information theoretic limit. This strategy resembles recent techniques for model compression (Han et al., 2015a; Wu et al., 2016; Park et al., 2017; Zhou et al., 2017; Rastegari et al., 2016). The contribution of this paper includes: • Proposal of a fundamental research question on com- pressing model states for training recovery. • Characterization of a family of compression schemes that can efﬁciently track the learning process, based on a stylized model we develop. • Design of a lossy coding scheme with high- compression rate that integrates both classical com- pression techniques and recent ones developed for dis- tributed learning and model compression. • Optimization of training systems that minimizes the overhead of producing checkpoints on the ﬂy. Our extensive evaluation demonstrates that by simultane- ously leveraging techniques from distributed training and model compression, our algorithm delivers a solution (called LC-Checkpoint, LC refers to Lossy Compression) with a compression rate of up to 28x and superior recovering time—achieving up to 5.77×recovery speedup over a state- of-the-art algorithm (SCAR). 2. Our approach We now describe our compression framework. We introduce a stylized model for the learning process to facilitate the analysis of the system design trade-off. Then we explain our design principles, determined by both the stylized model and our extensive experiments. Our model. A “high-dimensional” vector u ∈Rn repre- sents the model state. An iterative algorithm (e.g., stochastic gradient descent) is used to gradually move the model state vector u toward a local optimal point u∗. Let ut be the model state at the t-th round. In our stylized model, we assume ut performs a (drifted) random walk that converges to u∗. Speciﬁcally, we use the following process to model ui’s trajectory. LetL= ∥u0 −u∗∥. ut+1 = u∗ + η(ut −u∗) +ϵt, (1) where η and Ljointly model the convergence rate of the algorithm, and ϵt is a random noise component to reﬂect the stochastic nature of SGD. When ηis set to be a small constant, the model characterizes those algorithms that have linear convergence rate. When η= (1−1/L), this model characterizes those algorithms whose convergence rates are 1 −1/t(Boyd & Vandenberghe, 2004). While our model does not captures the detail of many SGD algorithms, be- cause different SGD algorithms have different convergence rate, designing a unifying model that highlights design trade- offs requires us to make simplifying assumptions. Our design principles. We next describe our design prin- ciples. P1. Minimize irritation to SGD. When we design lossy compression scheme, a portion of information is inevitably lost, causing performance degradation to a learning algo- rithm. We ﬁnd that we should not simply use ℓ2 recon- struction error to measure degradation of SGD. This can be best illustrated by the stylized model. For simplicity, let u∗ = 0, so ut+1 = ut −((1 −η)ut + ϵt). The delta term we want to compress is ((1 −η)ut + ϵt). When we use a lossy compression, it corresponds to adding an ad- ditional noise term that is a function of ut and ϵt. So with the compression scheme, the new learning process becomes ut+1 = ut −((1 −η)ut + ϵt + f(ut,ϵt)). Ob- serving that as long as I E[f(ut,ϵt) | ut,ϵt] = 0, and Var(f(ut,ϵt) | ut,ϵt) is dominated (smaller than) by Var(ϵt), then the convergence quality remains unchanged, by standard results from stochastic approximation (Lai, 2009; Kushner & Yin, 2003). There are many constructs that satisfy the expectation and variance constraints. Let us consider an example of keeping the most signiﬁcant bit of ((1 −η)ut + ϵt) by using stan- dard randomized rounding (Alistarh et al., 2017). Because of the nature of the rounding algorithm, the expectation is 0. In addition, because the most signiﬁcant bit is kept, the information loss in rounding will not be greater than ∥((1 −η)ut + ϵt) ∥2 = O(std(ϵt)) under a mild assump- tion that ϵt’s standard deviation also scales proportionally to ∥ut∥over time. Therefore, this rounding scheme does notOn Efﬁcient Constructions of Checkpoints affect the performance of the training algorithm. In general, the 1-bit encoding is a special case of quantization. A wide family of quantization schemes will satisfy the expectation and variance constraint. Our algorithm will explore this trade-off. Note also when we minimize ℓ2 reconstruction error, this corresponds to keeping top-kheaviest entries in ut+1 −ut. P2. Maximize redundancies in residual information. Our compression scheme also needs to ensure the information we keep exhibits large redundancy, as measured by entropy. This will enable us to use traditional coding schemes such as Huffman code to compress the data at the information theoretic limit. The interplay between P1 and P2 highlights the unique struc- ture of our compression problem. This can be best illustrated by a compression scheme called TOPN. This compression scheme keeps the largest elements in δt. We observe (i) while this scheme minimizes ℓ2 reconstruction error, it does not have superior recovery time. Many other compression schemes that possess the aforementioned properties recover equally fast, as suggested by our stylized model. (ii) It is dif- ﬁcult to perform compression for the TOPN scheme. TOPN scheme usually needs to track 10% of all the entries in δt to be effective. The overhead of tracking the locations of these elements is surprisingly high. This is because in part that the vector is not sufﬁciently sparse so sparse matrix representation does not help. Our solution, on the other hand, carefully complies P1 and circumvents the need to track the locations of the entries we keep and thus achieves signiﬁcantly higher compression rate. P3. Do not use random projections and/or sketches. No- tably, we discover that sketch-based randomized projection techniques (e.g., Woodruff et al. (2014)) harm the compres- sion. Roughly speaking, sketches compress information by projecting multiple numbers into one cell. While this could speed up query time, it only irritates the gradient de- scent algorithm in our setting. Consider a toy example in which ut ∈R2 and the optimal point u∗ = (0,10). Let ut = (5,5) be the current state so the gradient is along the direction (−1,1). When we apply sketches (say CountMin sketches), it collapses the direction (−1,1) into a single point 0. When we make a query, the gradients for both coordinates are incorrect. Sketches are more useful when the entries in the gradient vector are heterogeneous and queries need to be answered at “line rate” (e.g., do not slow down the training Ivkin et al. (2019)). Here, when a model needs to be recovered from a checkpoint, the job is less time-sensitive. Therefore, even we face heterogeneous pa- rameters, it is more effective to carefully disentangle crucial information from inconsequential ones than using arbitrary Algorithm 1 LC-CHECKPOINT-BASED SGD Input: u∗, u0, η 1: Initialize ˜u0 = u0. 2: for t= 1to T do 3: Update model state: ut = u∗ + η(ut−1 −u∗) +ϵ 4: Compute distance: δt = ut −˜ut−1 5: Quantize δt: ˜δt = QUANTIZE (δt) 6: Compress ˜δt by Huffman coding and save to disk 7: Update checkpoint state: ˜ut = ˜ut−1 + ˜δt 8: end for Output: uT, {˜δt |t∈[T]} random projections. 3. LC-Checkpoint-based SGD We now describe our solution LC-Checkpoint (LC refers to Lossy Compression). See Figure 1 for a working example and Algorithm 1 for a workﬂow. For simplicity, we assume that our system maintains a checkpoint ˜δt for each iteration. We slightly abuse δt to refer to both the compressed data and the real vector it represents. It is straightforward to downsample our operations to construct a checkpoint every k-iterations. Our solution consists of two major compo- nents. C1. Approximate tracking by delta-coding. At each step, our system maintains an approximation ˜ut of the ground-truth state. We simply set ˜ut = u0 + ∑ i≤t ˜δi, where u0 is the initial state of the model. Our system con- tinuously maintains and updates ˜ut at the background (line 7 in Algorithm 1). Our major compression task is to prop- erly track the “delta” between the approximate state and ground-truth. Speciﬁcally, the compression task for the t-th iteration is δt = ut −˜ut−1. See 3⃝in Figure 1. C2. Quantization and Huffman coding. This compo- nent compresses δt through two steps, Step 1. Two-stage quantization. We ﬁrst perform an exponent-based quantiza- tion, and then a priority promotion operation. This opera- tion intelligently drops inconsequential information between two consecutive states. Step 2. Lossless compression by Huffman. Finally, the quantized distance vector is further compressed using Huffman coding. One can see that to reconstruct the model state at iteration t from the checkpoints, we may simply compute ut = u0 + ∑t i=1 ˜δt. In what follows, Section 3.1 discusses C2 and Section 3.2 discusses additional system-level optimizations.On Efﬁcient Constructions of Checkpoints - 0.76 -0.48 0.2 0.07 0.18 0.49 0.14 0.39 0.82 0.09 0.070.76 0.82 e=-1,s=0 e=-2,s=0 e=-2,s=1 e=-3,s=0 e=-4,s=0 0.79 0.44 -0.48 0 0.79 -0.48 0 0 0 0.44 0 0.44 0.79 0+ 000 01 10 11 10 0 10 0111011100110 1.34 1.02 0.39 1.37 1.34 1.77 1.03 1.58 1.93 1.03 u0 ˜u0 ut−1 ˜ut−1 ut ˜ut … … … … 0.58 1.5 0.19 1.3 1.16 1.28 0.89 1.19 1.11 0.94 0.18 0.39 0.49 -0.48 0.14 0.2 0.09 1.37 1.02 0.19 1.3 1.16 1.72 0.89 1.63 1.9 0.94 ut ˜ut−1 ˜ut δt ˜δt Exponent-base Quantization Priority Promotion 0.79 0.44 -0.48 0.17 0.08 001 010 011 100 00 Huﬀman Encoding Checkpoint Saving ① ② ③ Figure 1.LC-Checkpoint overview. 3.1. Quantization and Huffman coding 3.1.1. T WO-STAGE QUANTIZATION LC-Checkpoint employs a novel two-stage pipeline to quan- tize δt, which consists of two main sub-steps: exponent- based quantization and priority promotion. Exponent-based Quantization. Recall that a ﬂoating point vis represented by v= (−1)s ×m×2e, where sis the sign, mis the mantissa, and eis the exponent. Recall that δt = ut −˜ut−1 ∈Rn is a high-dimensional vector we aim to encode. Our exponent-based quantization works as follows: ﬁrst, it partitions entries in δinto multiple buckets according to eand s, i.e., it assigns the elements with iden- tical exponents and signs to the same bucket. Our crucial observation from extensive experiments is that entries in ut usually drift towards the same direction, so δt typically have the same sign. Next, our algorithm represents each bucket by the average of maximum and minimum values in the bucket. Figure 1 2⃝shows an example, in which,δtis quantized into ﬁve buckets (marked with ﬁve different colors). All entries in each bucket are then represented by a unique value. Indexing kbuckets requires log2 kbits. Because δt consists of nﬂoating points, each of which usesb(e.g., b∈{32,64}) bits, the compression rate is r= nb nlog2 k+kb. For example, in Figure 1, δhas 10 elements (i.e., n= 10), each of which is represented by a single-precision ﬂoating point (i.e., b= 32). Thus, the original δhas nb, i.e., 320 bits in total. Exponent-based quantization uses 5 buckets (i.e., k= 5). Thus, after quantization, δhas (10×log 5+5×32 = 190) bits. Therefore, the compressing rate (r) is 1.68 (i.e., 320/190). It is critical to control the number of bucketskto achieve an optimal compression ratio. Fortunately, the exponent-based bucketing can control k≤29 for single-precision ﬂoating point elements, and control k ≤212 for double-precision. 1 Our evaluation results (Section 4.3) conﬁrm that usually k <25 sufﬁces. Figure 2(a) plots the distribution of all elements’ exponent parts in the last convolutional layer of AlexNet. Priority Promotion. We further improve the compres- sion ratio by limiting the number of buckets with a priority promotion approach. Our crucial observation is that when δt,i is excessively close to 0 (i.e., ˜ui,t−1 is close ui,t), it is more effective to batch the updates (i.e., do not update the i-th entry of δt until it becomes substantial). Note also this is conceptually different from minimizing construction er- rors. Minimizing construction errors corresponds to exactly keeping track of the heaviest entries in δt, whereas we both remove excessively small entries and quantize large entries 1Single-precision ﬂoating point numbers use 8 bits to store e, and together with a sign bit—that is why k ≤ 29. Similarly, double-precision numbers use 11 bits to store e.On Efﬁcient Constructions of Checkpoints −130 −1250 1 2 3 1e6 −30 −25 −20 −15 1e6 (a) Exponent distribution of δ. -127 -18 -17 -16 -15 -14 -13 -120.00 0.25 0.50 0.75 1.00 1.25 1.50 1e7 (b) Exponent distribution of ˜δ(3-bit promotion). Figure 2.The distribution of all elements’ exponent parts in the last convolutional layer of AlexNet. When eequals −127, the element value is 0. The x-axis denotes the exponent part value, and the y-axis indicates the count of elements with this value. (as done in the previous step). Speciﬁcally, we proposex-bit priority promotion. It keeps 2x −1 buckets with larger e only and merges the rest buckets into one with a unique value of 0. In other words, priority promotion updates ˜wi with a larger distance to wi with a higher priority. It limits the index of buckets within xbits. Figure 1 (Priority Promotion) uses 2-bit priority promotion to control the number of buckets under 4. It merges the green and purple buckets into a red one that is represented by a value 0. Indexing these buckets only needs 2 bits. Figure 2(b) gives a real example of 3-bit priority promotion for the last convolutional layer in AlexNet. 3.1.2. H UFFMAN CODING Finally, observing the number of elements in each bucket is highly non-uniform in most learning processes, we use Huffman coding (Van Leeuwen, 1976) to further compress the bucket. For example, Figure 2(a) plots the distribution of all elements’ exponent parts in the last convolutional layer of AlexNet. This distribution shows a skewed behavior, thus more suitable for Huffman coding. Our crucial ob- servation is that priority promotion further aggravates the skewness of this distribution (Figure 2(b)), thus marrying quantization with Huffman coding produces more than “sum of parts” beneﬁts. Our later evaluation validates it (Sec- tion 4.3). 3.2. System Optimizations LC-Checkpoint also comprises several novel system-level optimizations as follows: • Asynchronous Execution: Because only the ﬁrst step of LC-Checkpoint depends on the model state, the rest steps can run simultaneously with the next iteration of SGD computation. This asynchronous (non-blocking) execution signiﬁcantly reduces the checkpoint overhead, and mitigates the blocking of model execution. • Checkpoint Merging: To further reduce the recovery time, LC-Checkpoint employs a helper process to merge multiple checkpoints into super-step ones, periodically. In case of any system crash, LC-Checkpoint uses these super-step checkpoints for recovery. • Huffman Code Table Caching: The number of buckets may stay the same from one it- eration to another, speciﬁcally after priority promotion. Thus, it is possible to reuse the Huffman code table (with only a simple sort of buckets according to the number of entries in each bucket) among different iterations without any rebuilding. LC-Checkpoint comprises a lightweight cache to store the Huffman code table for each buckets count. 4. Experiments This section evaluates LC-Checkpoint on four typical ML applications with three benchmark datasets, and compares it with previous efforts (SCAR Qiao et al. (2018b) and a TOPN mechanism as mentioned in Section 2) on recovery (rework) cost, compression ratio, and execution overhead, demonstrating the superiority of LC-Checkpoint. 4.1. Methodology Evaluation Objective: This evaluation has four main ob- jectives: (1) comparing LC-Checkpoint’ recovery (rework) cost with previous work; (2) evaluating the compression beneﬁts brought by different approaches mentioned before; (3) speciﬁcally, validating the effectiveness of priority pro- motion; (4) conﬁrming that LC-Checkpoint incurs low over- head by an experiment case study. Our work is mainly compared with two state-of-the-art efforts: SCAR (Qiao et al., 2018b) and a TOPN mechanism. SCAR partitionsOn Efﬁcient Constructions of Checkpoints 5% 10% ckpt size 0 4 8 12rework cost SCAR TOPN LC (a) MLR on MNIST. 5% 10% ckpt size 0 4 8 12rework cost SCAR TOPN LC (b) LeNet on MNIST. 5% 10% ckpt size 0 4 8 12rework cost SCAR TOPN LC (c) AlexNet on MNIST. 5% 10% ckpt size 0 5 10 15rework cost SCAR TOPN LC (d) MF on MovieLens. 5% 10% ckpt size 0 5 10 15rework cost SCAR TOPN LC (e) MLR on FashionMNIST. 5% 10% ckpt size 0 5 10 15rework cost SCAR TOPN LC (f) LeNet on FashionMNIST. 5% 10% ckpt size 0 5 10 15rework cost SCAR TOPN LC (g) AlexNet on FashionMNIST. 5% 10% ckpt size 0 5 10 15rework cost SCAR TOPN LC (h) MF on Jester. Figure 3.Rework cost comparison among LC-Checkpoint, SCAR, and TOPN. The x-axis indicates the ratio of the compressed checkpoint size over the full checkpoint size. The y-axis shows the rework iterations. The error bars indicate 95% conﬁdence intervals, calculated by repeating each trial 50 times. the parameters and updates one partition in each iteration to reduce the checkpoint size. The TOPN mechanism only updates the parameters with the top-n largest distances to the previous iteration. The TOPN checkpoint is stored in a compressed sparse row (CSR) format. ML Applications and Datasets: LC-Checkpoint is evaluated on four typical ML applications: Multino- mial Logistic Regression ( MLR), LeNet-5 ( Lenet) (Le- Cun et al., 1998), AlexNet (Krizhevsky et al., 2012) and Matrix Factorization ( MF). The ﬁrst three applica- tions are trained on MNIST (LeCun et al., 1998) and FashionMNIST (Xiao et al., 2017) datasets. The last one, MF is trained on Jester (Goldberg et al., 2001) and MovieLens10M (Harper & Konstan, 2015). Platforms and Evaluation Conﬁgurations: Our experi- ments are conducted on a multi-core server with an Intel Xeon Gold 6138 Skylake CPU with 40 cores, each running at 2.0 GHz, and 192 GB DDR4 memory. The training is per- formed on a Tesla P100 GPU with 16GB High-bandwidth Memory (HBM). 4.2. Recovery/Rework Cost Comparison This section evaluates the recovery (or rework) cost of LC- Checkpoint, particularly comparing it to SCAR (Qiao et al., 2018b) and a TOPN mechanism2. 2Rework (or recovery) cost is deﬁned as the number of itera- tions from ˜ut to ut. All methods share the same SGD computation cost for each iteration. To evaluate their rework costs fairly, we use the same check- point size (update size) for all three methods. Two check- point sizes are tested: 5% and 10% of the full checkpoint size3. These checkpoint sizes can be set directly for SCAR and TOPN. However, LC-Checkpoint’s size is determined by the data distribution and thus changed dynamically. To address this issue, LC-Checkpoint employs 2-bit and 3-bit priority promotion that control its checkpoint size at 5% and 10%. Figure 4 reports more details of LC-Checkpoint’s checkpoint size information. Figure 3 compares the rework cost of three methods, SCAR, TOPN, and LC-Checkpoint, showing that LC-Checkpoint incurs the lowest rework cost for all ML applications and datasets among them. For the 5% checkpoint test case, LC-Checkpoint outperforms SCAR by 2.88×-5.77×, and TOPN by 2.17×-4.06×, respectively. With 10% checkpoint size, LC-Checkpoint outperforms SCAR by 1.9×-4.82×, and outperforms TOPN by 1.52×-2.17×, respectively. In addition, comparing two checkpoint sizes (5% v.s. 10%), LC-Checkpoint results in more stable rework cost as the checkpoint size decreasing. For example, decreasing the checkpoint size from 10% to 5%, LC-Checkpoint has a negligible rework cost increase on LeNet with MNIST (Figure 3(b)) and AlexNet (Figure 3(c), 3(g)). It does not have any rework cost change for other cases. In contrast, SCAR and TOPN increase 1.6×rework cost on average as the checkpoint size changing from 10% to 5%. 3Full checkpoint stores all model parameters after a speciﬁc iteration.On Efﬁcient Constructions of Checkpoints 2bits 3bits0.0% 5.0% 10.0% 15.0% 20.0% 25.0% E E+P E+P+H (a) MLR on MNIST. 2bits 3bits0.0% 5.0% 10.0% 15.0% 20.0% 25.0% E E+P E+P+H (b) LeNet on MNIST. 2bits 3bits0.0% 5.0% 10.0% 15.0% 20.0% 25.0% E E+P E+P+H (c) AlexNet on MNIST. 2bits 3bits0.0% 5.0% 10.0% 15.0% 20.0% 25.0% E E+P E+P+H (d) MF on MovieLens. 2bits 3bits0.0% 5.0% 10.0% 15.0% 20.0% 25.0% E E+P E+P+H (e) MLR on Fashion-MNIST. 2bits 3bits0.0% 5.0% 10.0% 15.0% 20.0% 25.0% E E+P E+P+H (f) LeNet on Fashion-MNIST. 2bits 3bits0.0% 5.0% 10.0% 15.0% 20.0% 25.0% E E+P E+P+H (g) AlexNet on Fashion-MNIST. 2bits 3bits0.0% 5.0% 10.0% 15.0% 20.0% 25.0% E E+P E+P+H (h) MF on Jester. Figure 4.The compression ratio with different compression methods. The x-axis denotes the bits count used in priority promotion, and the y-axis is the ratio of the checkpoint size after compression over the one before compression. E, P, H denote “exponent-base quantization”, “priority promotion”, and “Huffman coding”, respectively. 4.3. LC-Checkpoint Compression Effect Breakdown This section evaluates and analyzes the compression ef- fect of different approaches mentioned before, exponent- base quantization (E), priority promotion (P), and Huffman coding (H). Figure 4 reports the compression ratios with 2-bit and 3-bit priority promotion. With all compression approaches, the ultimate checkpoint sizes (E+P+H) are all below 5% with 2-bits, and below 10% with 3-bits over the uncompressed full checkpoint, i.e., the compression rates are above 20×and 10×, respectively. Exponent-base quantization yields a compression ratio of 85% on average. It proves that the exponent parts of all pa- rameters in δspan across a small range of all values that can be represented by single precision ﬂoating-point. 15% also indicates that the bucket number k< 25, because the aver- age bucket number can be estimated as k= 2(32×15%=4.8), where 32 is the width of single precision ﬂoating-point. Pri- ority promotion brings 9.26% extra compression ratio on average for 2-bit and 6.23% for 3-bit. For most cases, pri- ority promotion with smaller bits yields more beneﬁts for Huffman coding except MF (Figure 4(d), 4(h)). This is because MF’s parameters are sparse, thus Huffman coding can reach a sufﬁcient compression ratio without aggressive priority promotion. Across all models (and datasets), Huff- man coding brings 2% extra compression ratio with 2-bits priority promotion, and 1.6% with 3-bits one on average. 4.4. The Effectiveness of Priority Promotion This section further discusses the effectiveness of priority promotion. It aims to prove that priority promotion is able to save the majority of high priority parameters. We prove it by showing the exponent buckets result in a larger impact on the model state when their represented unique values are further from 0 (i.e., eis larger). Assume δis calculated from one state uθ to another for m iterations. Then, δi m is created by setting the parameters in the i-th exponent bucket to 0. The ground truth is calculated as Vgt = L(uθ+ δm) where L(x) denotes the loss function. Then the relative error is calculated as: Ei m = Vgt −L(uθ + δi m)  2 Vgt (2) Figure 5 reports the result of MLR with m= 10n,n ∈[1,6]. Both datasets (MNIST and FashionMNIST) on varied m prove that the elements in the buckets with the top-n largest distance impact more on the model (denotes as a higher relative error when the bucket represented value is set to 0). In addition, it is possible to preserve all important buckets with only a small number of index bits. For example, using 2-bit priority promotion (4 buckets with the last bucket storing 0) can easily preserve the most important buckets, and using 3-bit (8 buckets) can preserve all effective buckets. This result explains why priority promotion can compress the checkpoint with negligible accuracy loss.On Efﬁcient Constructions of Checkpoints (a) MLR on MNIST. (b) MLR on FashionMNIST. Figure 5.Evaluation on the priority of each exponent bucket. The x-axis denotes the id of the exponent bucket that is deleted. The y-axis shows the relative error to the ground-truth. Figure 6.MF on MovieLens25M.The x-axis denotes the iteration and the y-axis is the model’s RMSE (Root Mean Square Error). 4.5. A Case Study on LC-Checkpoint’s Overhead This section evaluates LC-Checkpoint’s execution overhead and overall impact on the model execution using a case study, i.e., trainingMF on MovieLens25M (Harper & Kon- stan, 2015) dataset. Each iteration costs 91 seconds on av- erage. LC-Checkpoint employs 3-bit priority promotion, resulting in a checkpoint size below 10% (of the uncom- pressed full checkpoint size). Default approach creates a full checkpoint every 10 iterations. A failure is triggered at the 7-th iteration. Figure 6 reports the result. LC-Checkpoint only incurs one extra iteration than the normal execution without any failure to convergence, and saves 6 iterations compared to the full checkpoint method, i.e., saving 546 seconds execution time. LC-Checkpoint introduces only less than 4 seconds (i.e., around 4%) overhead for each iteration, which is negligible. 5. Related Work Fault-tolerance is a key fundamental support for ML sys- tems. Li et al. (Li et al., 2014) propose a runtime parameter replication approach for recovery. Tensorﬂow (Abadi et al., 2016) employs periodic checkpoint to save the model state. Other efforts like (Harlap et al., 2017; Qiao et al., 2018a) aim to support strong consistency semantics. In contrast, our work relaxes the consistency guarantee of checkpoint based on the self-correcting behavior of ML applications. With a set of lossy compression mechanisms, our work can afford high frequent checkpoints, resulting in low rework cost and ﬁne-grained model state recovery. Similarly, Qiao et al. (Qiao et al., 2018b) also propose a fault-tolerant solu- tion (SCAR in our evaluation) based on weak consistency by partially updating parameters. SCAR is potential to store re- dundant information during checkpointing according to our evaluation, and our work aims to eliminate such redundancy by selectively saving the distance between two states. Model compression has been proposed to reduce model storage space and accelerate model execution time, simulta- neously. Weight pruning and weight quantization are two important categories of model compression. Some popular weight pruning techniques closely related to our work are summarized as follows. Guo et al. (Guo et al., 2016) present a dynamic network surgery approach with on-the-ﬂy connection pruning to reducing the network complexity. Dai et al. (Dai et al., 2019) combine the growth and the pruning phases in training to generate compact DNN architectures. Han et al. (Han et al., 2015b) design Deep Compression, a model compression approach by combining pruning, quantization, and Huffman coding. Mao et al. (Mao et al., 2017) carefully explore the impact of varied pruning granularity on model accuracy and propose a coarse-grained weight pruning approach. All effort above aims to prune model weights without compromising accuracy. Different from them, our work eliminates the redundancy between two checkpoints and reduces the rework cost during recovery by designing a reliable coding scheme working throughout the entire dynamic process of learning. Weight quantization is also widely used for model compres- sion. BinaryConnect (Courbariaux et al., 2015) introduces the binary weight for replacing multiplication by addition and subtraction. Binarized Neural Networks (Courbariaux et al., 2016) also use binary weights and activations to ac- celerate computation. Park et al. (Park et al., 2017) propose a clustering method based on weighted entropy for weight quantization. Leng et al. (Leng et al., 2018) formulate quan- tization as an optimization problem and solve it by ADMM. Our approach also employs quantization to reduce the bits ofOn Efﬁcient Constructions of Checkpoints parameters by designing a novel exponent-based quantiza- tion technique. Moreover, our approach emphasizes ﬁltering the parameters with a new priority promotion method. 6. Conclusion and Future Work This paper presents LC-Checkpoint, the ﬁrst checkpoint scheme based on lossy compression to achieve the maximal compression rate and efﬁcient recovery simultaneously. It employs a novel two-stage quantization method consisting of exponent-based quantization and priority promotion to identify and store the most critical information for SGD to recover, and leverages Huffman coding to further ben- eﬁt from the non-uniform distribution of gradient scales. Our evaluation demonstrates that LC-Checkpoint achieves a compression rate up to 28×and recovery speedup up to 5.77×over the state-of-the-art algorithm (SCAR). In the future, we plan to generalize LC-Checkpoint by re- laxing the assumption of SGD and equipping it with the capability of selecting checkpoint compression rates dynam- ically according to model and data changes. Acknowledgements We thank the anonymous reviewers for their valuable com- ments. This work is supported in part by NSF grants CRII- 1755646, CRII-1755769, OAC-1835821, CNS-1813487 and CCF-1918757, a Google Faculty Research Award, and an AWS Machine Learning Research Award. References Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M., et al. Tensorﬂow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016. Alistarh, D., Grubic, D., Li, J., Tomioka, R., and V ojnovic, M. Qsgd: Communication-efﬁcient sgd via gradient quan- tization and encoding. In Advances in Neural Information Processing Systems, pp. 1709–1720, 2017. Baylor, D., Breck, E., Cheng, H.-T., Fiedel, N., Foo, C. Y ., Haque, Z., Haykal, S., Ispir, M., Jain, V ., Koc, L., et al. Tfx: A tensorﬂow-based production-scale machine learn- ing platform. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1387–1395, 2017. Boyd, S. and Vandenberghe, L. Convex optimization. Cam- bridge university press, 2004. Courbariaux, M., Bengio, Y ., and David, J.-P. Binarycon- nect: Training deep neural networks with binary weights during propagations. In Advances in neural information processing systems, pp. 3123–3131, 2015. Courbariaux, M., Hubara, I., Soudry, D., El-Yaniv, R., and Bengio, Y . Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1. arXiv preprint arXiv:1602.02830, 2016. Dai, X., Yin, H., and Jha, N. K. Nest: A neural network syn- thesis tool based on a grow-and-prune paradigm. IEEE Transactions on Computers, 68(10):1487–1497, 2019. Goldberg, K., Roeder, T., Gupta, D., and Perkins, C. Eigen- taste: A constant time collaborative ﬁltering algorithm. information retrieval, 4(2):133–151, 2001. Goodfellow, I., Bengio, Y ., and Courville, A.Deep learning. MIT press, 2016. Guo, Y ., Yao, A., and Chen, Y . Dynamic network surgery for efﬁcient dnns. In Advances in neural information processing systems, pp. 1379–1387, 2016. Han, S., Mao, H., and Dally, W. J. Deep compres- sion: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015a. Han, S., Pool, J., Tran, J., and Dally, W. Learning both weights and connections for efﬁcient neural network. In Advances in neural information processing systems, pp. 1135–1143, 2015b. Harlap, A., Tumanov, A., Chung, A., Ganger, G. R., and Gibbons, P. B. Proteus: agile ml elasticity through tiered reliability in dynamic resource markets. InProceedings of the Twelfth European Conference on Computer Systems, pp. 589–604, 2017. Harper, F. M. and Konstan, J. A. The movielens datasets: History and context. Acm transactions on interactive intelligent systems (tiis), 5(4):1–19, 2015. Hong, M., Luo, Z.-Q., and Razaviyayn, M. Convergence analysis of alternating direction method of multipliers for a family of nonconvex problems. SIAM Journal on Optimization, 26(1):337–364, 2016. Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. Ivkin, N., Rothchild, D., Ullah, E., Stoica, I., Arora, R., et al. Communication-efﬁcient distributed sgd with sketching. In Advances in Neural Information Processing Systems, pp. 13144–13154, 2019.On Efﬁcient Constructions of Checkpoints Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classiﬁcation with deep convolutional neural networks. In Advances in neural information processing systems , pp. 1097–1105, 2012. Kushner, H. and Yin, G. G.Stochastic approximation and re- cursive algorithms and applications, volume 35. Springer Science & Business Media, 2003. Lai, T. L. Martingales in sequential analysis and time series, 1945–1985. Electronic Journal for history of probability and statistics, 5(1), 2009. LeCun, Y ., Bottou, L., Bengio, Y ., and Haffner, P. Gradient- based learning applied to document recognition. Proceed- ings of the IEEE, 86(11):2278–2324, 1998. Leng, C., Dou, Z., Li, H., Zhu, S., and Jin, R. Extremely low bit neural network: Squeeze the last bit out with admm. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018. Li, M., Andersen, D. G., Park, J. W., Smola, A. J., Ahmed, A., Josifovski, V ., Long, J., Shekita, E. J., and Su, B.-Y . Scaling distributed machine learning with the parameter server. In 11th {USENIX}Symposium on Operating Systems Design and Implementation ( {OSDI}14), pp. 583–598, 2014. Lin, D., Talathi, S., and Annapureddy, S. Fixed point quan- tization of deep convolutional networks. In International Conference on Machine Learning, pp. 2849–2858, 2016. Low, Y ., Gonzalez, J., Kyrola, A., Bickson, D., Guestrin, C., and Hellerstein, J. M. Distributed graphlab: A frame- work for machine learning in the cloud. arXiv preprint arXiv:1204.6078, 2012. Mao, H., Han, S., Pool, J., Li, W., Liu, X., Wang, Y ., and Dally, W. J. Exploring the regularity of sparse struc- ture in convolutional neural networks. arXiv preprint arXiv:1705.08922, 2017. Mogul, J. C., Douglis, F., Feldmann, A., and Krishnamurthy, B. Potential beneﬁts of delta encoding and data compres- sion for http. In Proceedings of the ACM SIGCOMM’97 conference on Applications, technologies, architectures, and protocols for computer communication, pp. 181–194, 1997. Park, E., Ahn, J., and Yoo, S. Weighted-entropy-based quantization for deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5456–5464, 2017. Qiao, A., Aghayev, A., Yu, W., Chen, H., Ho, Q., Gib- son, G. A., and Xing, E. P. Litz: Elastic frame- work for high-performance distributed machine learn- ing. In 2018 {USENIX}Annual Technical Conference ({USENIX}{ATC}18), pp. 631–644, 2018a. Qiao, A., Aragam, B., Zhang, B., and Xing, E. P. Fault tolerance in iterative-convergent machine learning. arXiv preprint arXiv:1810.07354, 2018b. Rastegari, M., Ordonez, V ., Redmon, J., and Farhadi, A. Xnor-net: Imagenet classiﬁcation using binary convo- lutional neural networks. In European conference on computer vision, pp. 525–542. Springer, 2016. Van Leeuwen, J. On the construction of huffman trees. In ICALP, pp. 382–410, 1976. Woodruff, D. P. et al. Sketching as a tool for numerical linear algebra. Foundations and TrendsR⃝in Theoretical Computer Science, 10(1–2):1–157, 2014. Wu, J., Leng, C., Wang, Y ., Hu, Q., and Cheng, J. Quantized convolutional neural networks for mobile devices. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4820–4828, 2016. Xiao, H., Rasul, K., and V ollgraf, R. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017. Zhou, A., Yao, A., Guo, Y ., Xu, L., and Chen, Y . Incre- mental network quantization: Towards lossless cnns with low-precision weights. arXiv preprint arXiv:1702.03044, 2017.",
      "meta_data": {
        "arxiv_id": "2009.13003v1",
        "authors": [
          "Yu Chen",
          "Zhenming Liu",
          "Bin Ren",
          "Xin Jin"
        ],
        "published_date": "2020-09-28T01:20:15Z",
        "venue": "International Conference on Machine Learning, 2020",
        "pdf_url": "https://arxiv.org/pdf/2009.13003v1.pdf"
      }
    },
    {
      "title": "The State of Sparse Training in Deep Reinforcement Learning",
      "abstract": "The use of sparse neural networks has seen rapid growth in recent years,\nparticularly in computer vision. Their appeal stems largely from the reduced\nnumber of parameters required to train and store, as well as in an increase in\nlearning efficiency. Somewhat surprisingly, there have been very few efforts\nexploring their use in Deep Reinforcement Learning (DRL). In this work we\nperform a systematic investigation into applying a number of existing sparse\ntraining techniques on a variety of DRL agents and environments. Our results\ncorroborate the findings from sparse training in the computer vision domain -\nsparse networks perform better than dense networks for the same parameter count\n- in the DRL domain. We provide detailed analyses on how the various components\nin DRL are affected by the use of sparse networks and conclude by suggesting\npromising avenues for improving the effectiveness of sparse training methods,\nas well as for advancing their use in DRL.",
      "full_text": "The State of Sparse Training in Deep Reinforcement Learning Laura Graesser* 1 2 Utku Evci* 2 Erich Elsen3 Pablo Samuel Castro2 Abstract The use of sparse neural networks has seen rapid growth in recent years, particularly in computer vision. Their appeal stems largely from the re- duced number of parameters required to train and store, as well as in an increase in learning efﬁ- ciency. Somewhat surprisingly, there have been very few efforts exploring their use in Deep Re- inforcement Learning (DRL). In this work we perform a systematic investigation into applying a number of existing sparse training techniques on a variety of DRL agents and environments. Our results corroborate the ﬁndings from sparse train- ing in the computer vision domain – sparse net- works perform better than dense networks for the same parameter count – in the DRL domain. We provide detailed analyses on how the various com- ponents in DRL are affected by the use of sparse networks and conclude by suggesting promising avenues for improving the effectiveness of sparse training methods, as well as for advancing their use in DRL1. 1. Introduction Deep neural networks are typically organized as a stack of layers. Each layer consists of multiple neurons, where each neuron is connected to all neurons in the next layer; this is often referred to as a dense network. Alternatively, each neuron can be wired to a subset of the neurons in the next layer, resulting in a sparse, and smaller, network. Such sparse neural networks have been shown to match the performance of their dense counterparts while requiring only 10%-to-20% of the connections in most cases (Han et al., 2015; Gale et al., 2019; Blalock et al., 2020) providing *Equal contribution 1Robotics at Google 2Google Research, Canada 3Adept AI. Correspondence to: Laura Graesser <laura- graesser@google.com>, Utku Evci <evcu@google.com>, Pablo Samuel Castro <psc@google.com>. Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s). 1Code for reproducing our results can be found at github.com/google-research/rigl/tree/master/rigl/rl signiﬁcant memory, storage, and latency gains. Deep networks have become a mainstay of scalable rein- forcement learning (RL), key to recent successes such as playing – at superhuman levels – Atari games (Mnih et al., 2015), Go (Silver et al., 2016), Dota 2 (Berner et al., 2019) and as well as controlling complex dynamical systems such as stratospheric balloons (Bellemare et al., 2020) and plasma in real-time (Degrave et al., 2022). Despite their importance, most deep reinforcement learning (DRL) research focuses on improving the algorithmic aspect of DRL, and less on the architecture aspect. Sparse networks in particular have received very little attention, likely due to the belief that net- work over-parameterization helps with learning. However, recent work suggests that RL agents may suffer from im- plicit under-parameterization when training deep networks with gradient descent (Kumar et al., 2021), suggesting that the network’s expressivity is in fact underused. In addition to this, Nikishin et al. (2022) suggests deep RL agents may have a tendency to overﬁt to early training data. Given this, one might expect there is substantial opportunity to com- press RL agents. Further, sparse networks might beneﬁt DRL by reducing the cost of training or aid running them in latency-constrained settings such as controlling plasma (Degrave et al., 2022). One limitation of current research on training sparse neural networks is that it almost solely focuses on image classiﬁca- tion benchmarks (Blalock et al., 2020; Hoeﬂer et al., 2021) creating the risk of over-ﬁtting to a speciﬁc domain. Do ad- vances observed in computer vision (CV) transfer to DRL? A few recent works (Sokar et al., 2021; Arnob et al., 2021) attempt to address this by applying individual sparse train- ing algorithms to DRL agents. However, it is still unknown if the key observation made in CV , thatsparse models per- form better than dense ones for the same parameter count, transfers to DRL. In this work we focus on answering that question and sys- tematically explore the effectiveness of different sparse learning algorithms in the online DRL setting. In order to achieve this, we benchmark four different sparse training algorithms using value-based (DQN (Mnih et al., 2015)) and actor-critic, (SAC (Haarnoja et al., 2018) and PPO (Schul- man et al., 2017)) agents. Our results also include a broad analysis of various components that play a role in the train- arXiv:2206.10369v1  [cs.LG]  17 Jun 2022The State of Sparse Training in Deep Reinforcement Learning ing of these sparse networks: sparsity distribution strategies, weight decay, layer initialization, signal-to-noise ratio for gradients, as well as batch size, topology update strategy and frequency. We summarize our main ﬁndings below: • In almost all cases, sparse neural networks perform bet- ter than their dense counterparts for a given parameter count, demonstrating their potential for DRL. • It is possible to train up to 80 - 90% sparse networks with minimal loss in performance compared to the standard dense networks. • Pruning often obtains the best results, and dynamic sparse training improves over static sparse training sig- niﬁcantly. However gradient based growth (Evci et al., 2020) seems to have a limited effect on performance. We argue this is due to low signal-to-noise ratio in gradients. • The distribution of parameters among the actor and critic networks, as well as among different layers, im- pact training greatly. We observe that the best perfor- mance is obtained by allocating the majority of parame- ters to the critic network and using Erdos Renyi Kernel (ERK) sparsity distributions. • We observe robust performance over various hyper- parameter variations. Somewhat surprisingly, when adding noise to the observations, sparse methods achieve better robustness in most cases. 2. Background 2.1. Sparse training Removing connections from neural networks was suggested at least as early as Mozer & Smolensky (1989), which coined the name “Skeleton” networks for what we today call sparse networks. Techniques for ﬁnding sparse neural networks can be grouped under two main categories. (1) Dense-to-sparse training approaches (Han et al., 2016; Molchanov et al., 2017; Wortsman et al., 2019; Kusu- pati et al., 2020; Peste et al., 2021) start with a dense neural network and gradually reduce the network size by pruning its weights. This approach often achieves state-of-the-art performance amongst sparse networks, however it requires the same (or more) computation as training a large dense net- work. An alternative to pruning is (2) sparse training (Mocanu et al., 2018). This family of methods sparsiﬁes the network at initialization and maintains this sparsity through- out training, thus reducing the training cost proportional to the sparsity of the network. However, training sparse neural networks from scratch is known to be difﬁcult, leading to sub-optimal solutions (Frankle & Carbin, 2019; Liu et al., 2019; Evci et al., 2019). DRL training is notoriously resource hungry, hence we fo- cus on the second family of methods (i.e. sparse training) in this work. There are various approaches to sparse training. One line of work (Lee et al., 2019; Wang et al., 2020; Tanaka et al., 2020), attempts to prune a dense networkimmediately on iteration 0. The resulting networks are used as an initial- ization for sparse training and kept ﬁxed throughout. These techniques have been shown to have marginal gains over random pruning (Frankle et al., 2020), especially when used in modern training pipelines. Furthermore they may not generalize well in the RL setting as the non-stationarity of the data make it less clear that any decision made at iteration 0 will remain optimal throughout training. Another line of work starts with randomly initialized sparse neural networks (both weights and masks) and focuses on improving sparse training by changing the sparse connec- tivity among neurons (Mocanu et al., 2018; Bellec et al., 2018) throughout the optimization. Known as Dynamic sparse training (DST), such approaches have been shown to match pruning results, making it possible to train sparse net- works efﬁciently without sacriﬁcing performance (Dettmers & Zettlemoyer, 2019; Evci et al., 2020). In this work we benchmark one dense-to-sparse and three sparse training methods, which we brieﬂy describe below: Pruning (Zhu & Gupta, 2018):uses a simple procedure to slowly make a dense network sparse over the course of one training run using weight magnitudes. We start pruning the network from 20% of the training steps and stop when we reach 80%, keeping the ﬁnal sparse network ﬁxed for the remaining of the training. This simple pruning algorithm is shown to exceed or match more complex pruning algorithms (Gale et al., 2019). Despite the fact it requires the same order of magnitude resources as training a dense network, we included this method since it serves as an upper bound on the sparse training performance. Static: prunes a given dense network randomly at initial- ization and the resulting sparse network is trained with a ﬁxed structure. This is an important baseline to show the effectiveness of DST algorithms explained below. Sparse Evolutionary Training (SET) (Mocanu et al., 2018): Similar to Static, SET starts training with a random sparse network. During training, a portion of the connec- tions are changed every N steps (the update interval) by replacing the lowest magnitude connections with new ran- dom ones. The fraction (drop fraction) of updated weights are decayed over the course of training to help the network converge to a minima. We use cosine decay as proposed by Dettmers & Zettlemoyer (2019). Rigged Lottery (RigL) (Evci et al., 2020):is the same as SET, except the new connections are activated using the gradient signal (highest magnitude) instead of at random.The State of Sparse Training in Deep Reinforcement Learning This criteria has been shown to improve results signiﬁcantly in image classiﬁcation and with enough training iterations matches or exceed accuracies obtained by pruning. 2.2. Reinforcement learning Reinforcement learning (RL) aims to design learning algo- rithms for solving sequential decision-making problems. Typically these are framed as an agent interacting with an environment at discrete time-steps by making action choices from a set of possible agent states; the environ- ment in turn responds to the action selection by (possibly) changing the agent’s state and/or providing a numerical reward (or cost); the agent’s objective is to ﬁnd a pol- icy mapping states to actions so as to maximize (mini- mize) the sum of rewards (costs). This is formalized as a Markov decision process (Puterman, 1994) deﬁned as a tuple ⟨X,A,P,R,γ⟩, where X is the state space, A is the action space, P : X ×A →∆(X) deﬁnes the transition dynamics2, R : X ×A →R is the reward function, and γ ∈[0,1) is a discount factor. A policy π : X →∆(A) formalizes an agent’s behaviour and induces a value func- tion Vπ : X →R deﬁned via the well-known Bellman recurrence: Vπ(x) := Ea∼π(x) [ R(x,a) + γEx′∼P(x,a)Vπ(x′) ] (1) It is convenient to deﬁne state-action value functions Qπ : X×A →R as: Qπ(x,a) := R(x,a)+γEx′∼P(x,a)Vπ(x′). The goal of an RL agent is to ﬁnd a policy π∗:= maxπVπ (which is guaranteed to exist); for notational convenience we denote V∗:= Vπ∗ and Q∗:= Qπ∗ . In online RL the agent achieves this by iteratively improving an initial policy π0: {π0,π1,··· ,πt,···} and using these intermediate policies to collect new experience from the environment in the form of transitions (x,a,r,x ′), where a ∼πt(x), r = R(x,a), and x′∼P(x,a). These transitions constitute the dataset the agent uses to improve its policies. In other words, the learning proocess is a type of closed feedback loop : an agent’s policy directly affects the data gathered from the environment, which in turn directly affects how the agent updates its policy. When X is very large, it is impractical to store Vπ and Qπ in a table, so a function approximator Vθ ≈Vπ (where θare the approximator’s parameters) is employed instead. This function approximator is usually one or more deep networks, and this type of RL is known as deep RL (DRL). DRL algorithms can be broadly categorized into two groups: Value-based: The function Qπ is approximated by a deep network Qθ. The policy is directly induced from the value 2∆(X) denotes the set of probability distributions over a ﬁnite set X. estimate via πt(x) = arg maxa∈A Qθt (x,a)3. The parame- ters θare trained using a temporal difference loss (based on Equation 1) from transitions sampled from D: L(θ) = E(x,a,r,x′)∼D [ Qθ(x,a) −(r+ γmax a′∈A Q¯θ(x′,a′)) ] (2) Here, ¯θs a copy of θthat is infrequently synced with θfor more stable training (Mnih et al., 2015). These methods are typically employed for discrete control environments, where there is a ﬁnite (and relatively small) set of actions (e.g. Atari games (Bellemare et al., 2013)). Policy-gradient: In contrast to value-based methods where the policy is implicitly improved by virtue of improving Qθ, policy-gradient methods maintain and directly improve upon a policy πψ parameterized by ψ. These methods typi- cally still make use of a value estimate Qθ as part of their learning process, and are thus often referred to as actor-critic methods (where πψ is the actor and Qθ the critic). Two po- tential advantages of these methods is that they can be more forgiving of errors in the Qθ estimates, and they can handle continuous action spaces (for instance, by having πψ(x) output mean and variance parameters from which actions may be sampled). These methods are typically employed for continuous control environments, where the action space is continuous (e.g. MuJoCo (Todorov et al., 2012)). 3. Experimental setup DRL algorithms We investigate both value-based and policy-gradient methods. We chose DQN (Mnih et al., 2015) as the value-based algorithm, as it is the algorithm that ﬁrst spurred the ﬁeld of DRL, and has thus been extensively stud- ied and extended. We chose two actor-critic algorithms for our investigations: an on-policy algorithm (PPO (Schulman et al., 2017)) and an off-policy one (SAC (Haarnoja et al., 2018)); both are generally considered to be state-of-the-art for many domains. Environments For discrete-control we focus on three classic control environments (CartPole, Acrobot, and Moun- tainCar) as well as 15 games from the ALE Atari suite (Bellemare et al., 2013) (see subsection A.4 for game selec- tion details). For continuous-control we use ﬁve environ- ments of varying difﬁculty from the MuJoCo suite (Todorov et al., 2012) (HalfCheetah, Hopper, Walker2d, Ant, and Humanoid). Rewards obtained by DRL algorithms have notoriously high variance (Agarwal et al., 2021). Therefore we repeat each experiment with at least 10 different seeds 3Although there are other mechanisms for deﬁning a policy, such as using a softmax, and there are exploration strategies to consider, we present only the argmax setup for simplicity, as the other variants are mostly orthogonal to our analyses.The State of Sparse Training in Deep Reinforcement Learning 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static SAC / s=0.9 Normalized Score 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static PPO, s = 0.9 Normalized Score 0.00 0.25 0.50 0.75 1.00 Dense:100% Dense Pruning RigL SET Static DQN (CNN) / s=0.9 Human Normalized Score 0 1 2 3 4 Dense:100% Dense Pruning RigL SET Static DQN (ResNet) / s=0.9 Human Normalized Score Figure 1.IQM plots for networks at 90% sparsity for various architecture and algorithm combinations. SAC and PPO are averaged over 5 MuJoCo environments, whereas DQN is averaged over 15 Atari environments. Results at different sparsities can be found at Appendix E. ”Dense: 100%” corresponds to the standard dense model. Atari scores were normalized using human performance per game. MuJoCo scores were normalized using the average returns obtained by the Dense: 100% SAC agent per game. and report the average reward obtained over the last 10% of evaluations. We also provide 95% conﬁdence intervals in all plots. See subsection A.1 for additional details. Training For each sparse training algorithm considered ( Pruning , Static, RigL, and SET) we train poli- cies ranging between 50% to 99% sparsity. To ensure a fair comparison between algorithms, we performed a hy- per parameter sweep for each algorithm separately. The exception is DQN experiments on Atari for which it was too computationally expensive to do a full hyper-parameter sweep and we used values found in previous experiments instead. Sparse results in these environments may therefore be conservative compared to the well tuned dense baseline. In addition to training the standard dense networks used in the literature, we also train smaller dense networks by scaling down layer widths to approximately match the pa- rameter counts of the sparse networks, thereby providing a ”parameter-equivalent” dense baseline. We share details of the hyper parameter sweeps and hyper parameters used for each algorithm in subsection A.3. Code Our code is built upon the TF-Agents (Guadarrama et al., 2018), Dopamine (Castro et al., 2018), and RigL (Evci et al., 2020) codebases. We use rliable (Agarwal et al., 2021) to calculate the interquartile mean (IQM) and plot the results. The IQM is calculated by discarding the bottom and top 25% of normalized scores aggregated from multiple runs and environments, then calculating the mean (reported with 95% conﬁdence intervals) over the remaining 50% runs. Code for reproducing our results can be found at github.com/google-research/rigl/tree/master/rigl/rl 4. The State of Sparse Networks in Deep RL We begin by presenting the outcome of our analyses in Figure 1 and Figure 2 for DQN (Atari), PPO and SAC (Mu- JoCo). Figure 1 presents the IQM at 90% sparsity, whilst in Figure 2 we evaluate ﬁnal performance relative to the number of parameters. We share results for classic control, 2 additional MuJoCo and 12 additional Atari environments in Appendix B. Three main conclusions emerge; (1) In most cases performance obtained by sparse networks signiﬁcantly exceeds that of their dense counterparts with a comparable number of parameters. Critically, in more difﬁcult environ- ments requiring larger networks (e.g. Humanoid, Atari), sparse networks can be obtained with efﬁcient sparse train- ing methods. (2) It is possible to train sparse networks with up to 80-90% fewer parameters and without loss in perfor- mance compared to the standard dense model. (3) Gradient based growing (i.e. RigL) seems to have limited impact on the performance of sparse networks. Next, we discuss each of these points in detail. Sparse networks perform better.Inline with previous ob- servations made in speech (Kalchbrenner et al., 2018), natu-The State of Sparse Training in Deep Reinforcement Learning 105 106 #Params 20 15 10 5 0 5 10 15 Reward 0.50 0.800.900.950.98 0.99 Pong / DQN (CNN) 105 106 #Params 0 1000 2000 3000 4000 5000 0.50 0.80 0.90 0.95 0.98 0.99 Qbert / DQN (CNN) 105 106 #Params 500 1000 1500 2000 0.500.800.900.95 0.98 0.99 MsPacman / DQN (CNN) Dense Pruning RigL SET Static 104 105 #Params 0 1000 2000 3000 4000 5000Reward 0.500.700.800.90 0.950.96 0.97 0.98 SAC / Walker2d-v2 104 105 #Params 0 2000 4000 6000 8000 10000 12000 0.500.700.800.90 0.950.96 0.97 0.98 SAC / HalfCheetah-v2 104 105 #Params 1000 2000 3000 4000 5000 0.500.700.800.90 0.95 0.96 0.970.98 SAC / Humanoid-v2 Dense Pruning Rigl Set Static 103 #Params 500 1000 1500 2000 2500 3000 3500 4000Reward 0.50 0.70 0.80 0.90 0.950.96 0.97 0.98 0.99 PPO / Walker2d-v2 103 #Params 0 1000 2000 3000 4000 5000 0.50 0.70 0.800.90 0.95 0.96 0.97 0.98 0.99 PPO / HalfCheetah-v2 103 104 #Params 500 1000 1500 2000 2500 0.50 0.70 0.80 0.90 0.950.960.97 0.980.99 PPO / Humanoid-v2 Dense Pruning Rigl Set Static Figure 2.Comparison of the ﬁnal reward relative to parameter count for the various methods considered: (row-1) DQN on Atari (CNN) (row-2) SAC on MuJoCo, (row-3) PPO on MuJoCo. We consider sparsities from 50% to 95% (annotated on the pruning curve) for sparse training methods and pruning. Parameter count for networks with 80% sparsity and the reward obtained by the dense baseline are highlighted with vertical and horizontal lines. Shaded areas represent 95% conﬁdence intervals. See Appendix B for results on additional environments. 10 20 30 40 50 60 70 80 90 Parameters allocated for Actor (%) 0 1000 2000 3000 4000 5000Reward Walker2d-v2 10 20 30 40 50 60 70 80 90 Parameters allocated for Actor (%) 0 2000 4000 6000 8000 10000 12000 HalfCheetah-v2 algorithm dense rigl % full dense params 100 20 10 10 20 30 40 50 60 70 80 90 Parameters allocated for Actor (%) 1000 2000 3000 4000 5000 Humanoid-v2 Figure 3.Evaluating how varying the actor-critic parameter ratio affects performance for a given parameter budget on different environment with policies trained using SAC. % of parameters allocated to the actor network is reported on the x axis. In SAC the parameter count of both critic networks is summed to give the overall critic parameter count. The vertical line corresponds the the standard parameter split and the horizontal line to the full dense training reward.The State of Sparse Training in Deep Reinforcement Learning ral language modelling (Li et al., 2020) and computer vision (Evci et al., 2020), in almost all environments, sparse net- works found by pruning achieve signiﬁcantly higher rewards than the dense baseline. However training these sparse net- works from scratch ( static) performs poorly. DST algo- rithms (RigL and SET) improve over static signiﬁcantly, however often fall short of matching the pruning perfor- mance. Critically, we observe that for more difﬁcult environment requiring larger networks such as Humanoid, MsPacman, Qbert and Pong, sparse networks found by efﬁcient DST algorithms exceed the performance of the dense baseline. How sparse?Next we asked how much sparsity is possible without loss in performance relative to that of the standard dense model (denoted by Dense:100% in Figure 1 and by the horizontal lines in Figure 2). We ﬁnd that on average DST algorithms maintain performance up to 90% sparsity using SAC (Figure 1 top left) or DQN ( Figure 1 bottom row), after which performance drops. However performance is variable. For example, DST algo- rithms maintain performance especially well in MsPacman and Humanoid. Whereas in Qbert none of the methods are able to match the performance of the standard dense model at any of the examined levels of sparsity. In the Atari environments, training a ResNet (He et al., 2015) following the architecture from Espeholt et al. (2018) instead of the standard CNN alone provided about 3x im- provement in IQM scores. We were also surprised to see that pruning at 90% sparsity exceeds the performance of the standard ResNet model. These observations indicate that while sparse training can bring very signiﬁcant efﬁciency gains in some environments, it is not a guaranteed beneﬁt. Unlike supervised learning, expected gains likely depend on both task and network, and merits further inquiry. RigL and SET:For most sparsities (50% - 95%) we ob- serve little difference between these two sparse training algo- rithms. At very high sparsities, RigL may outperform SET. The difference can be large (e.g. MsPacman), but is more often moderate (e.g. Pong) or negligible (e.g. Humanoid, Qbert) with overlapping conﬁdence intervals. This suggests that the gradient signal used by RigL may be less informa- tive in the DRL setting compared to image classiﬁcation, where it obtains state-of-the-art performance and consis- tently outperforms SET. Understanding this phenomenon could be a promising direction for improving sparse training methods for DRL. Perhaps unsurprisingly, the clarity of the differences be- tween sparse and dense training is affected by the stability of the underlying RL algorithm. Our results using SAC, designed for stability, were the clearest, as were the DQN results. In contrast, our results using PPO which has much higher variance, were less stark. For this reason, we used SAC and DQN when studying the different aspects of sparse agents in the rest of this work. 5. Where should sparsity be distributed? When searching for efﬁcient network architectures for DRL it is natural to ask where sparsity is best allocated. To that end, we consider both how to distribute parameters between network types and as well as within them. Actor or Critic? Although in value-based agents such as DQN there is a single network, in actor-critic methods such as PPO and SAC there are at least two: an actor and a critic network. It is believed that the underlying functions these networks approximate (e.g. a value function vs. a policy) may have signiﬁcantly different levels of complexity, and this complexity likely varies across environments. Actor and critic typically have near-identical network architectures. However, for a given parameter budget it is not clear that this is the best strategy, as the complexity of the functions being approximated may vary signiﬁcantly. We thus seek to understand how performance changes as the parameter ratio between the actor and critic is varied for a given parameter budget. In Figure 3 we assess three parameter budgets: 100%, 20% and 10% of the standard dense parameter count, and two training regimes, dense and sparse. Given the observed similarity in performance between RigL and SET in Figure 2, we selected one method, RigL, for this analysis. We observe that assigning a low proportion of parameters to the critic (10 - 20%) incurs a high performance cost across all regimes. When parameters are more scarce, in 20% and 10% of standard dense settings, performance degradation is highest. This effect is not symmetric. Reducing the actor parameters to just 10% rarely affects performance compared to the default actor-critic split of 34:66 (vertical line). Interestingly the default split appears well tuned, achieving the best performance in most settings. However in the more challenging Humanoid environment we see that for smaller dense networks, reducing the actor parameters to just 10% yields the best performance. Sparse networks follow a simi- lar trend, but we notice that they appear to be more sensitive to the parameter ratio, especially at higher sparsities. Overall this suggests that the value function is the more complex function to approximate in these settings, bene- ﬁting from the lion’s share of parameters. It also suggests that tuning the parameter ratio may improve performance. Furthermore, FLOPs at evaluation time is determined only by the actor network. Since the actor appears to be easier to compress, this suggests large potential FLOPs savings for real-time usage of these agents. Finally, this approachThe State of Sparse Training in Deep Reinforcement Learning 0.5 0.6 0.7 0.8 0.9 1.0 Sparsity 0 2000 4000 6000 8000 10000 12000Reward Half-Cheetah Algorithm rigl static Distribution erk uniform 0 10 6  10 5  10 4  10 3  10 2 Weight Decay 0 1000 2000 3000 4000 5000 6000Reward Walker2d Dense Pruning Rigl Set Static Static Rigl Set Sparsity-aware Initiliazation 0 1000 2000 3000 4000 5000 6000Reward Walker-2d False (ERK) True (ERK) False (Uniform) True (Uniform) Figure 4.Sensitivity analysis on policies trained with SAC: (left) uniform vs ERK sparsity distributions, (center) weight decay, (right) sparsity-aware vs dense initialization. We use 80% (ERK) sparse networks in all plots unless noted otherwise. Plots for the remaining hyper-parameters are shared in Appendix D. could be used to better understand the relative complexity of policies and values functions across different environments. Within network sparsity In Figure 4 (left) we turn our attention to the question of distributing parameters within networks and compare two strategies; uniform and ERK (Evci et al., 2020). Given a target sparsity of, say 90%, uniform achieves this by making each layer 90% sparse; ERK distributes them proportional to the sum of its dimen- sion, which has the effect of making large layers relatively more sparse than the smaller ones. Due to weight sharing in convolutional layers, ERK sparsity distribution doubles the FLOPs required at a given sparsity (Evci et al., 2020), which we also found to be the case with the convolutional networks used by DQN in the Atari environments (see subsection A.2 for further discussion). On the other hand, ERK has no effect on the FLOPs count of fully connected networks used in MuJoCo environments. Our results show that ERK sig- niﬁcantly improves performance over uniform sparsity and thus we use ERK distribution in all of our experiments and share results with uniform distribution in Appendix C. We hypothesize the advantage of ERK is because it leaves input and output layers relatively more dense, since they typ- ically have few incoming our outgoing connections, and this enables the network to make better use of (a) the observation and (b) the learned representations at the highest layers in the network. It is interesting to observe that maintaining a dense output layer is one of the key design decisions made by Sokar et al. (2021) for their proposed algorithm. 6. Sensitivity analysis In this section we assess the sensitivity of some key hyper- parameters for sparse training. Plots and commentary for the remaining hyper-parameters (drop fraction, topology update interval, and batch size) are shared in Appendix D. Weight decay In Figure 4 (center) we evaluate the effect of weight decay and ﬁnd that a small amount of weight decay is beneﬁcial for pruning, RigL, and SET. This is to be expected since network topology choices are made based on weight magnitude, although we do note that the improvements are quite minor. Surprisingly weight decay seems to help dense even though it is not often used in DRL. Findings: We recommend using small weight decay. Sparsity-aware initialization In Figure 4 (right) we eval- uate the effect of adjusting layer weight initialization based on a layer’s sparsity on static, RigL and SET. A common approach to initialization is to scale a weight’s initializa- tion inversely by the square root of the number of incoming connections. Consequently, when we drop incoming con- nections, the initialization distribution should be scaled pro- portionately to the number of incoming connections (Evci et al., 2022). Figure 4 (right) shows that this sparsity-aware initialization consistently improves performance when us- ing uniform distribution over layer sparsities. However the difference disappears when using ERK for RigL and SET and may even harm performance for static. Findings: Performance is not sensitive to sparsity-aware ini- tialization when using ERK and helps when using uniform layer sparsity. For RigL and SET we recommend always using sparsity-aware weight initialization (since it never ap- pears to harm performance) but for static this may depend on layer sparsity. 7. Signal-to-noise ratio in DRL environments Variance reduction is key to training deep models and of- ten achieved through using momentum based optimizers (Schmidt et al., 2011; Kingma & Ba, 2015a). However when new connections are grown such averages are not available, therefore noise in the gradients can provide mis- leading signals. In Figure 5 we share the signal-to-noise ratio (SNR) for the Classic control and MuJoCo environ- ments over the course of training. SNR is calculated as |µ| σ where µis the mean and σis the standard deviation of gradients over a mini-batch. A low SNR means the signalThe State of Sparse Training in Deep Reinforcement Learning 0 20000 40000 60000 80000 100000 Steps 10 3 10 2 10 1 Average SNR train_eval.env_name MountainCar-v0 CartPole-v0 Acrobot-v1 init_masks.sparsity 0.8 0.95 0.0 0.2 0.4 0.6 0.8 1.0 Steps 1e6 10 2 10 1 Average SNR Actor Environment Hopper-v2 Walker2d-v2 Humanoid-v2 HalfCheetah-v2 Sparsity 0.8 0.95 0.0 0.2 0.4 0.6 0.8 1.0 Steps 1e6 10 1 Average SNR Critic Environment Hopper-v2 Walker2d-v2 Humanoid-v2 HalfCheetah-v2 Sparsity 0.8 0.95 Figure 5.Signal-to-noise (SNR) ratio comparison of different gradient based DRL algorithms. We calculate SNR for every parameter in corresponding network (including the inactive/pruned weights) and report the mean SNR value. (left) DQN networks when training on classic control environments. SAC actor (center) and critic (right) networks during the training on MuJoCo environments. is dominated by the variance and thus the mean (the sig- nal) is uninformative. We calculate SNR for all parameters separately and report the mean. Mini-batch gradients can have average SNR values as low as 0.01 starting early in training. Higher sparsities seem to cause lower SNR values. Similarly, actor networks have lower SNR. Findings: We ﬁnd the average SNR for gradients to decrease with sparsity, potentially explaining the difﬁculty of using gradient based growing criteria in sparse training. 8. Are sparse networks robust to noise? Sparse neural networks can improve results on primary metrics such accuracy and rewards, yet they might have some unexpected behaviours in other aspects (Hooker et al., 2020). In Figure 6 we assess the effect of adding in- creasing amounts of noise to the observations and mea- suring their effect on a trained policy. Noise was sampled ∼N(0,σ),σ ∈[0,1,..., 30], quantized to an integer, and added to each observation’s pixel values (∈[0,255]) before normalization. Noise was sampled independently per pixel. We look at three data regimes; 100%, 50% and 10% of the standard dense model parameter count and compare dense and sparse training (RigL and SET). We made an effort to select policies with comparable performance for all the methods, chosen from the set of all policies trained during this work. We observe that (1) smaller models are generally more ro- bust to high noise than larger models, (2) sparse models are more robust to high noise than dense models on average, and (3) in most cases there are minimal differences when the noise is low. We can see that in the very low data regime (10% full pa- rameter count) policies trained using RigL are more robust to high noise compared with their dense counterparts, a fact observed across every environment. In the moderate data regime (50% full parameter count) the ordering is more mixed. In Qbert the dense model is most robust but the picture is reversed for Pong and McPacman. Finally, SET appears less robust to high noise than RigL, although we note this is not the case for Pong at 50% density. Although a preliminary analysis, it does suggest that sparse training can produce networks that are more robust to obser- vational noise, even when experienced post-training. 9. Related work Sparse training Though research on pruning has a rel- atively long story by deep learning standards (Mozer & Smolensky, 1989; Sietsma & Dow, 1988; Han et al., 2015; Molchanov et al., 2017; Louizos et al., 2017), training sparse networks from scratch has only recently gained popularity. Goyal et al. (2017), Frankle & Carbin (2019), and Evci et al. (2019) showed that training sparse networks from a ran- dom initialization is difﬁcult compared to dense neural net- works. Despite this, various approaches have been recently proposed to improve sparse training, most notably lottery tickets (Frankle et al., 2019; Zhou et al., 2019), dynamic sparse training (Dettmers & Zettlemoyer, 2019; Mostafa & Wang, 2019; Mocanu et al., 2018; Bellec et al., 2018; Evci et al., 2020; Liu et al., 2021) and one-shot pruning (Lee et al., 2019; Wang et al., 2020; Tanaka et al., 2020; Liu & Zenke, 2020). Solutions that focus on initialization alone have been shown to be ineffective for contemporary models (Evci et al., 2022; Frankle et al., 2020), possibly due to the catapult mechanism observed early in training (Lewkowycz et al., 2020). For an in-depth survey on the topic, please see Hoeﬂer et al. (2021). Sparse networks in RL Livne & Cohen (2020) used pruning as an intermediary step to guide the width of dense neural networks for DQN and A2C agents. Most of the work investigating the use of sparse sparse training in RL are in the context of the lottery ticket hypothesis; Morcos et al. (2019) studied the existence of lucky sparse initializa- tions using pruning and late-rewinding; Hasani et al. (2020) proposed an interesting approach by repurposing the sparseThe State of Sparse Training in Deep Reinforcement Learning 0 5 10 15 20 25 30 Observation noise StdDev 500 750 1000 1250 1500 1750 2000 2250Average return MsPacman 0 5 10 15 20 25 30 Observation noise StdDev 20 15 10 5 0 5 10 15 20 Average return Pong Density 100 50 10 Method Dense RigL SET 0 5 10 15 20 25 30 Observation noise StdDev 0 1000 2000 3000 4000 5000 6000Average return Qbert Figure 6.Robustness to observation noise. We test the robustness of networks trained using sparse and dense methods (denoted by line style) by adding Gaussian noise to the observations. We examine three parameter regimes, 100% (blue), 50% (pink) and 10% (purple) of the standard dense model parameter count. All policies were trained using DQN. circuitry of the C. elegans soil-worm for RL tasks; Vischer et al. (2021) observed that the success of such initializations dependens heavily on selecting correct features for the in- put data, and not on any general qualities of the different initializations. Lee et al. (2021) proposed the use of block- circulant masks during early steps of training to improve the efﬁciency of pruning on TD3 agents, while Arnob et al. (2021) applied one-shot pruning algorithms in an ofﬂine-RL setting. Perhaps the work closest to ours is the algorithm proposed by Sokar et al. (2021), where authors applied the SET algorithm for end-to-end training of sparse networks in two actor-critic algorithms (TD3 and SAC). By a carefully chosen topology update schedule and dynamic architecture design, the proposed algorithm was able to match the dense network with a sparsity of around 50%. Novel architectures in DRL A number of works have focused on evolving network architectures for RL policies. Nadizar et al. (2021) applied pruning together with evo- lution algorithms. Whiteson & Stone (2006) combined NEAT (Stanley & Miikkulainen, 2002) with Q-learning (Watkins, 1989) to evolve better learners, Gaier & Ha (2019) evolved strong architectural priors, resulting in networks that could solve tasks with a single randomly initialized shared weight, whilst Tang et al. (2020) evolve compact self-attention architectures as a form of indirect network encoding. Zambaldi et al. (2019) similarly explored self- attention enabling agents to perform relational reasoning and achieve state-of-the-art performance on the majority of StarCraft II mini-games. Another line of research seeks to improve the stability (Parisotto et al., 2020) and efﬁciency (Parisotto & Salakhutdinov, 2021) of transformers applied to DRL, whilst Shah & Kumar (2021) explore the utility of using features extracted from a pre-trained Resnet in the standard DRL pipeline. Consistent with our observations in this work, Ha & Schmidhuber (2018) showed it is possible to train very compact controllers (i.e. actors) albeit in a the context of model-based instead of the model-free RL setting considered here. 10. Discussion and Conclusion In this work we sought to understand the state of sparse training for DRL by applying pruning, static, SET and RigL to DQN, PPO, and SAC agents trained on a variety of envi- ronments. We found sparse training methods to be a drop-in alternative for their dense counterparts providing better re- sults for the same parameter count. From a practical stand- point we made recommendations regarding hyper-parameter settings and showed that non-uniform sparse initialization combined with tuning actor:critic parameter ratios improves performance. We hope this work establishes a useful foundation for fu- ture research into sparse DRL algorithms and highlights a number of interesting research questions. In contrast to the computer vision domain, we observe that RigL fails to match pruning results. Low SNR in high sparsity regimes offers a clue but more work is needed to understand this phenomena. Our results in section 8 also suggest that sparse networks may aid in generalization and robustness to obser- vational noise; this is an active area of interest and research in the DRL community, so a more thorough understanding could result in important algorithmic advances. Acknowledgements We thank Fabian Pedregosa, Rishabh Agarwal, and Adrien Ali Ta¨ıga for their helpful feedback on the manuscript, Oscar Ramirez for his help with on the TF-Agents codebase, and Trevor Gale and Sara Hooker for inspiring the title of this work. We also thank Brain Montreal RL team for their useful feedback on an early version of this work. Finally, we thank Bram Grooten for pointing out Degrave et al. (2022) and their contribution to the motivation for this work.The State of Sparse Training in Deep Reinforcement Learning References Agarwal, R., Schwarzer, M., Castro, P. S., Courville, A., and Bellemare, M. G. Deep reinforcement learning at the edge of the statistical precipice. Advances in Neural Information Processing Systems, 2021. Arnob, S. Y ., Ohib, R., Plis, S., and Precup, D. Single- shot pruning for ofﬂine reinforcement learning. ArXiv, abs/2112.15579, 2021. Bellec, G., Kappel, D., Maass, W., and Legenstein, R. A. Deep rewiring: Training very sparse deep networks. In International Conference on Learning Representations, 2018. Bellemare, M., Candido, S., Castro, P., Gong, J., Machado, M., Moitra, S., Ponda, S., and Wang, Z. Autonomous navigation of stratospheric balloons using reinforcement learning. Nature, 588:77–82, 12 2020. doi: 10.1038/ s41586-020-2939-8. Bellemare, M. G., Naddaf, Y ., Veness, J., and Bowling, M. The Arcade Learning Environment: An evaluation plat- form for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279, June 2013. Berner, C., Brockman, G., Chan, B., Cheung, V ., Debiak, P., Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse, C., J ´ozefowicz, R., Gray, S., Olsson, C., Pa- chocki, J., Petrov, M., de Oliveira Pinto, H. P., Raiman, J., Salimans, T., Schlatter, J., Schneider, J., Sidor, S., Sutskever, I., Tang, J., Wolski, F., and Zhang, S. Dota 2 with large scale deep reinforcement learning. CoRR, abs/1912.06680, 2019. URL http://arxiv.org/ abs/1912.06680. Blalock, D., Ortiz, J. J. G., Frankle, J., and Guttag, J. What is the state of neural network pruning? ArXiv, 2020. URL https://arxiv.org/abs/2003.03033. Castro, P. S., Moitra, S., Gelada, C., Kumar, S., and Belle- mare, M. G. Dopamine: A research framework for deep reinforcement learning. CoRR, abs/1812.06110, 2018. URL http://arxiv.org/abs/1812.06110. Degrave, J., Felici, F., Buchli, J., Neunert, M., Tracey, B., Carpanese, F., Ewalds, T., Hafner, R., Abdolmaleki, A., de las Casas, D., Donner, C., Fritz, L., Galperti, C., Hu- ber, A., Keeling, J., Tsimpoukelli, M., Kay, J., Merle, A., Moret, J.-M., Noury, S., Pesamosca, F., Pfau, D., Sauter, O., Sommariva, C., Coda, S., Duval, B., Fasoli, A., Kohli, P., Kavukcuoglu, K., Hassabis, D., and Riedmiller, M. Magnetic control of tokamak plasmas through deep rein- forcement learning. Nature, 2022. Dettmers, T. and Zettlemoyer, L. Sparse networks from scratch: Faster training without losing performance. ArXiv, 2019. URL http://arxiv.org/abs/1907. 04840. Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V ., Ward, T., Doron, Y ., Firoiu, V ., Harley, T., Dunning, I., Legg, S., and Kavukcuoglu, K. IMPALA: scalable dis- tributed deep-rl with importance weighted actor-learner architectures. CoRR, 2018. Evci, U., Pedregosa, F., Gomez, A. N., and Elsen, E. The difﬁculty of training sparse neural networks. ArXiv, 2019. URL http://arxiv.org/abs/1906.10732. Evci, U., Gale, T., Menick, J., Castro, P. S., and Elsen, E. Rigging the lottery: Making all tickets winners. In Pro- ceedings of Machine Learning and Systems 2020, 2020. Evci, U., Ioannou, Y . A., Keskin, C., and Dauphin, Y . Gradi- ent ﬂow in sparse neural networks and how lottery tickets win. In AAAI Conference on Artiﬁcial Intelligence, 2022. Frankle, J. and Carbin, M. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In 7th Interna- tional Conference on Learning Representations (ICLR), 2019. Frankle, J., Dziugaite, G. K., Roy, D. M., and Carbin, M. Stabilizing the lottery ticket hypothesis. ArXiv, 2019. URL https://arxiv.org/abs/1903.01611. Frankle, J., Dziugaite, G. K., Roy, D. M., and Carbin, M. Pruning neural networks at initialization: Why are we missing the mark? ArXiv, 2020. URL https: //arxiv.org/abs/2009.08576. Gaier, A. and Ha, D. Weight agnostic neural networks. 2019. URL https://weightagnostic.github. io. https://weightagnostic.github.io. Gale, T., Elsen, E., and Hooker, S. The state of sparsity in deep neural networks. ArXiv, 2019. URL http: //arxiv.org/abs/1902.09574. Goyal, P., Doll ´ar, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y ., and He, K. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. Guadarrama, S., Korattikara, A., Ramirez, O., Castro, P., Holly, E., Fishman, S., Wang, K., Gonina, E., Wu, N., Kokiopoulou, E., Sbaiz, L., Smith, J., Bart ´ok, G., Berent, J., Harris, C., Vanhoucke, V ., and Brevdo, E. TF-Agents: A library for reinforcement learning in tensorﬂow. https://github.com/tensorflow/ agents, 2018. URL https://github.com/ tensorflow/agents. [Online; accessed 25-June- 2019].The State of Sparse Training in Deep Reinforcement Learning Ha, D. and Schmidhuber, J. Recurrent world models facilitate policy evolution. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Pro- cessing Systems , volume 31. Curran Associates, Inc., 2018. URL https://proceedings. neurips.cc/paper/2018/file/ 2de5d16682c3c35007e4e92982f1a2ba-Paper. pdf. Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor-critic: Off-policy maximum entropy deep reinforce- ment learning with a stochastic actor. In International Conference on Machine Learning (ICML), 2018. Han, S., Pool, J., Tran, J., and Dally, W. Learning both weights and connections for efﬁcient neural network. In Advances in neural information processing systems, 2015. Han, S., Liu, X., Mao, H., Pu, J., Pedram, A., Horowitz, M. A., and Dally, W. J. EIE: Efﬁcient Inference Engine on compressed deep neural network. In Proceedings of the 43rd International Symposium on Computer Architecture, 2016. Hasani, R., Lechner, M., Amini, A., Rus, D., and Grosu, R. A natural lottery ticket winner: Reinforcement learning with ordinary neural circuits. In Proceedings of the 37th International Conference on Machine Learning. PMLR, 2020. URL https://proceedings.mlr.press/ v119/hasani20a.html. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. CoRR, 2015. Hoeﬂer, T., Alistarh, D., Ben-Nun, T., Dryden, N., and Peste, A. Sparsity in deep learning: Pruning and growth for efﬁcient inference and training in neural networks. ArXiv, abs/2102.00554, 2021. Hooker, S., Courville, A. C., Clark, G., Dauphin, Y ., and Frome, A. What do compressed deep neural networks forget. arXiv: Learning, 2020. Kalchbrenner, N., Elsen, E., Simonyan, K., Noury, S., Casagrande, N., Lockhart, E., Stimberg, F., Oord, A., Dieleman, S., and Kavukcuoglu, K. Efﬁcient neural au- dio synthesis. In International Conference on Machine Learning (ICML), 2018. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2015a. Kingma, D. P. and Ba, J. Adam: A method for stochas- tic optimization. In Bengio, Y . and LeCun, Y . (eds.), 3rd International Conference on Learning Representa- tions, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015b. URL http: //arxiv.org/abs/1412.6980. Kumar, A., Agarwal, R., Ghosh, D., and Levine, S. Im- plicit under-parameterization inhibits data-efﬁcient deep reinforcement learning. In International Conference on Learning Representations, 2021. URL https:// openreview.net/forum?id=O9bnihsFfXU. Kusupati, A., Ramanujan, V ., Somani, R., Wortsman, M., Jain, P., Kakade, S., and Farhadi, A. Soft threshold weight reparameterization for learnable sparsity. In Proceedings of the International Conference on Machine Learning , 2020. Langley, P. Crafting papers on machine learning. In Langley, P. (ed.),Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 1207–1216, Stan- ford, CA, 2000. Morgan Kaufmann. Lee, J., Kim, S., Kim, S., Jo, W., and Yoo, H.-J. Gst: Group- sparse training for accelerating deep reinforcement learn- ing. ArXiv, abs/2101.09650, 2021. Lee, N., Ajanthan, T., and Torr, P. H. S. SNIP: Single-shot Network Pruning based on Connection Sensitivity. In International Conference on Learning Representations (ICLR), 2019, 2019. Lewkowycz, A., Bahri, Y ., Dyer, E., Sohl-Dickstein, J., and Gur-Ari, G. The large learning rate phase of deep learning: the catapult mechanism. Arxiv, 2020. URL https://arxiv.org/pdf/2003.02218. Li, Z., Wallace, E., Shen, S., Lin, K., Keutzer, K., Klein, D., and Gonzalez, J. Train large, then compress: Re- thinking model size for efﬁcient training and inference of transformers. ArXiv, abs/2002.11794, 2020. Liu, S., Yin, L., Mocanu, D. C., and Pechenizkiy, M. Do we actually need dense over-parameterization? in-time over-parameterization in sparse training. In ICML, 2021. Liu, T. and Zenke, F. Finding trainable sparse networks through neural tangent transfer. In ICML, 2020. Liu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T. Re- thinking the value of network pruning. In International Conference on Learning Representations, 2019. Livne, D. and Cohen, K. Pops: Policy pruning and shrink- ing for deep reinforcement learning. IEEE Journal of Selected Topics in Signal Processing, 14:789–801, 2020. Louizos, C., Ullrich, K., and Welling, M. Bayesian compres- sion for deep learning. InAdvances in Neural Information Processing Systems, 2017. Mnih, V ., Kavukcuoglu, K., Silver, D., Rusu, A. A., Ve- ness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C.,The State of Sparse Training in Deep Reinforcement Learning Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wier- stra, D., Legg, S., and Hassabis, D. Human-level control through deep reinforcement learning. Nature, 518(7540): 529–533, 2015. Mocanu, D. C., Mocanu, E., Stone, P., Nguyen, P. H., Gibescu, M., and Liotta, A. Scalable training of arti- ﬁcial neural networks with adaptive sparse connectivity inspired by network science. Nature Communications, 2018. Molchanov, D., Ashukha, A., and Vetrov, D. P. Variational Dropout Sparsiﬁes Deep Neural Networks. In Proceed- ings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 Au- gust 2017, 2017. Morcos, A., Yu, H., Paganini, M., and Tian, Y . One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers. In Advances in Neural Information Processing Systems, 2019. Mostafa, H. and Wang, X. Parameter efﬁcient train- ing of deep convolutional neural networks by dynamic sparse reparameterization. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA , 2019. URL http://proceedings.mlr.press/ v97/mostafa19a.html. Mozer, M. C. and Smolensky, P. Skeletonization: A tech- nique for trimming the fat from a network via relevance assessment. In Advances in Neural Information Process- ing Systems 1, 1989. Nadizar, G., Medvet, E., Pellegrino, F. A., Zullich, M., and Nichele, S. On the effects of pruning on evolved neural controllers for soft robots. In Proceedings of the Genetic and Evolutionary Computation Conference Companion, 2021. Nikishin, E., Schwarzer, M., D’Oro, P., Bacon, P.-L., and Courville, A. The primacy bias in deep reinforcement learning. In Proceedings of the Thirty-ninth International Conference on Machine Learning (ICML’22), 2022. Parisotto, E. and Salakhutdinov, R. Efﬁcient transformers in reinforcement learning using actor-learner distillation. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum? id=uR9LaO_QxF. Parisotto, E., Song, F., Rae, J., Pascanu, R., Gulcehre, C., Jayakumar, S., Jaderberg, M., Kaufman, R. L., Clark, A., Noury, S., Botvinick, M., Heess, N., and Hadsell, R. Stabilizing transformers for reinforcement learning. In III, H. D. and Singh, A. (eds.), Pro- ceedings of the 37th International Conference on Ma- chine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 7487–7498. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/ v119/parisotto20a.html. Peste, A., Ioﬁnova, E., Vladu, A., and Alistarh, D. Ac/dc: Alternating compressed/decompressed training of deep neural networks. ArXiv, abs/2106.12379, 2021. Puterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc., New York, NY , USA, 1st edition, 1994. ISBN 0471619779. Schmidt, M., Roux, N. L., and Bach, F. Convergence rates of inexact proximal-gradient methods for convex optimiza- tion. In Proceedings of the 24th International Conference on Neural Information Processing Systems, 2011. Schulman, J., Moritz, P., Levine, S., Jordan, M. I., and Abbeel, P. High-dimensional continuous control using generalized advantage estimation. In Bengio, Y . and Le- Cun, Y . (eds.),4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1506.02438. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. Shah, R. M. and Kumar, V . Rrl: Resnet as representation for reinforcement learning. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning , volume 139 of Proceedings of Machine Learning Research, pp. 9465–9476. PMLR, 18– 24 Jul 2021. URL https://proceedings.mlr. press/v139/shah21a.html. Sietsma, J. and Dow, R. J. Neural net pruning-why and how. In IEEE International Conference on Neural Networks, 1988. doi: 10.1109/ICNN.1988.23864. Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V ., Lanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T., Leach, M., Kavukcuoglu, K., Graepel, T., and Hassabis, D. Mastering the game of go with deep neural networks and tree search. Nature, 529:484–503, 2016. URL http://www.nature.com/nature/journal/ v529/n7587/full/nature16961.html. Sokar, G., Mocanu, E., Mocanu, D. C., Pechenizkiy, M., and Stone, P. Dynamic sparse training for deep reinforcement learning. ArXiv, abs/2106.04217, 2021.The State of Sparse Training in Deep Reinforcement Learning Stanley, K. O. and Miikkulainen, R. Evolving neural networks through augmenting topologies. Evol. Com- put., 10(2):99–127, jun 2002. ISSN 1063-6560. doi: 10.1162/106365602320169811. URL https://doi. org/10.1162/106365602320169811. Tanaka, H., Kunin, D., Yamins, D. L. K., and Ganguli, S. Pruning neural networks without any data by iteratively conserving synaptic ﬂow. ArXiv, 2020. URL https: //arxiv.org/abs/2006.05467. Tang, Y ., Nguyen, D., and Ha, D. Neuroevolution of self- interpretable agents. In Proceedings of the 2020 Genetic and Evolutionary Computation Conference, GECCO ’20, pp. 414–424, New York, NY , USA, 2020. Association for Computing Machinery. ISBN 9781450371285. doi: 10. 1145/3377930.3389847. URL https://doi.org/ 10.1145/3377930.3389847. Todorov, E., Erez, T., and Tassa, Y . Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ Inter- national Conference on Intelligent Robots and Systems, pp. 5026–5033, 2012. doi: 10.1109/IROS.2012.6386109. Vischer, M. A., Lange, R., and Sprekeler, H. On lottery tickets and minimal task representations in deep rein- forcement learning. ArXiv, abs/2105.01648, 2021. Wang, C., Zhang, G., and Grosse, R. Picking winning tickets before training by preserving gradient ﬂow. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum? id=SkgsACVKPH. Watkins, C. J. C. H. Learning from Delayed Re- wards. PhD thesis, King’s College, Cambridge, UK, May 1989. URL http://www.cs.rhul.ac.uk/ ˜chrisw/new_thesis.pdf. Whiteson, S. and Stone, P. Evolutionary function approxi- mation for reinforcement learning. Journal of Machine Learning Research, 7(31):877–917, 2006. URL http: //jmlr.org/papers/v7/whiteson06a.html. Wortsman, M., Farhadi, A., and Rastegari, M. Discover- ing neural wirings. In Advances in Neural Information Processing Systems, 2019. Zambaldi, V ., Raposo, D., Santoro, A., Bapst, V ., Li, Y ., Babuschkin, I., Tuyls, K., Reichert, D., Lillicrap, T., Lockhart, E., Shanahan, M., Langston, V ., Pascanu, R., Botvinick, M., Vinyals, O., and Battaglia, P. Deep rein- forcement learning with relational inductive biases. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum? id=HkxaFoC9KQ. Zhou, H., Lan, J., Liu, R., and Yosinski, J. Deconstructing lottery tickets: Zeros, signs, and the supermask. In Ad- vances in Neural Information Processing Systems, 2019. Zhu, M. and Gupta, S. To Prune, or Not to Prune: Explor- ing the Efﬁcacy of Pruning for Model Compression. In International Conference on Learning Representations Workshop, 2018.The State of Sparse Training in Deep Reinforcement Learning A. Experimental Details A.1. Training schedule During training all agents are allowed M environment transitions, with policies being evaluated for Kepisodes / steps every N environment frames, where the values vary per suite and shared below. Atari experiments use a frame skip of 4, following (Mnih et al., 2015) thus 1 environment step = 4 environment frames. Environment M N K K = episodes Classic control 100,000 2 ,000 20 MujoCo (SAC) 1,000,000 10 ,000 30 MujoCo (PPO) 1,000,000 2 ,000 20 K = environment steps Atari Suite 40,000,000 1 ,000,000 125 ,000 A.2. FLOPs behaviour of Sparse ERK networks in DRL As reported in Evci et al. (2020), using ERK sparsity distribution often doubles the FLOPs needed for sparse models compared using the uniform sparsity distribution. This is due to the parameter sharing in convolutional layers. The spatial dimensions, kernel size and the stride of a convolutional layer affects how many times each weight is used during the convolution which in turn determines the contribution of each weight towards the total FLOPs count. In modern CNNs, the spatial dimensions of the feature maps often decreases monotonically towards the output of the network, making the contribution of the connections in later layers to the total FLOPs count smaller. Furthermore, the size of the layers typically increases towards the output and thus ERK removes a larger proportion of the connections from these later layers compared to uniform. Consequently a network sparsiﬁed using the ERK distribution will have a larger FLOPs count compared to one sparsiﬁed using a uniform distribution. Due to the lack of parameter sharing fully connected layers used in MuJoCo and classic control experiments, sparse networks with ERK have same amount of FLOPs as the uniform. Networks used in Atari experiments, however, uses convolutional networks and thus ERK doubles the FLOPs required compared to uniform. FLOPs scaling of sparse networks with ERK distributions used for Pong game can be found in Figure 7. Atari games have differently sized action spaces (Pong has 6 actions for example), which affects the number of neurons in the last layer. However since the last layer is very small and fully connected, it should have a very little effect on the results provided here. 0.5 0.6 0.7 0.8 0.9 1.0 Sparsity 106 107 FLOPs DQN-Atari CNN ERK 2x Uniform Uniform 0.5 0.6 0.7 0.8 0.9 1.0 Sparsity 107 108 FLOPs DQN-Atari Resnet ERK 2x Uniform Uniform Figure 7.FLOPs scaling of different sparsity distributions on (left) Nature CNN and (right) Impala ResNet architectures used in MsPacman environment.The State of Sparse Training in Deep Reinforcement Learning A.3. Hyper-parameter sweep We perform a grid search over different hyper parameters used in Dense, Prune, Static, SET and RigL algorithms. Unless otherwise noted, we use hyper-parameters used in regular dense training. When pruning, we start pruning around 20% of training steps and stop when 80% of training is completed following the ﬁndings of Gale et al. (2019). We use same default hyper-parameters for SET and RigL. Fort both algorithms we start updating the mask at initialization and decay the drop fraction over the course of the training using a cosine schedule, similar to pruning stopping the updates when 80% of training is completed. We search over the following parameters: 1. Weight decay ( ): Searched over the grid [0, 1e-6, 1e-4, 1e-3]. 2. Update Interval ( ): refers to how often models are pruned or sparse topology is updated. Searched over the grid [100, 250, 500, 1000, 5000]. 3. Drop Fraction ( ): refers to the maximum percentage of parameters that are dropped and added when network topology is updated. This maximum value is decayed during training according to a cosine decay schedule. Searched over the grid [0.0,0.1,0.2,0.3,0.5]. 4. Sparsity-aware initialization ( ): refers to whether sparse models are initialized with scaled initialization or not. We repeat the hyper-parameter search for each DRL algorithm using the Acrobot (for DQN) and Walker2D (for PPO and SAC) environments. Best hyper-parameters found in these environments are then used when training in other similar environments (i.e. classic control for DQN and MuJoCO for PPO and SAC). See Table 1, Table 2, and Table 3 for the best hyper parameters found in each setting. Table 1.DQN best hyper-parameters for Classic Control from sweep Algorithm Weight Decay Update Interval Drop Fraction Sparsity-aware init Dense 1 ·10−6 - - - Pruning 1 ·10−6 1,000 - True Static 1 ·10−6 - - True RigL 1 ·10−6 1,000 0.5 True SET 1 ·10−6 1,000 0.5 True Table 2.SAC best hyper-parameters from sweep Algorithm Weight Decay Update Interval Drop Fraction Sparsity-aware init Dense 1 ·10−4 - - - Pruning 1 ·10−4 1,000 - True Static 1 ·10−4 - - False RigL 1 ·10−6 1,000 0.5 True SET 1 ·10−4 250 0.3 TrueThe State of Sparse Training in Deep Reinforcement Learning Table 3.PPO best hyper-parameters from sweep Algorithm Weight Decay Update Interval Drop Fraction Sparsity-aware init Dense 1 ·10−6 - - - Pruning 1 ·10−6 500 - False Static 0 - - True RigL 1 ·10−4 250 0.3 True SET 1 ·10−6 100 0 True Atari hyper-parameters Due to computational constraints we did not search over hyper-parameters for the Atari environ- ments, except for a small grid-search to tune the dense ResNet. The CNN architecture from Mnih et al. (2015) has been used in many prior works thus was already well tuned. The ResNet hyper-parameter sweep for the original dense model is detailed below: 1. Weight decay:Searched over the grid [0, 1e-6, 1e-5, 1e-4]. 2. Learning rate:Searched over the grid [1e-4, 2.5e-4, 1e-3, 2.5e-3]. The ﬁnal hyper-parameters we used for the Atari environments are shown in in Table 4 for the CNN and Table 5 for the ResNet. Table 4.DQN (CNN) Atari hyper-parameters Algorithm Weight Decay Learning Rate Update Interval Drop Fraction Sparsity-aware init Dense 0 2.5 ·10−4 - - - Pruning 0 2.5 ·10−4 5,000 - False Static 0 2.5 ·10−4 - - True RigL 0 2.5 ·10−4 5,000 0.3 True SET 0 2.5 ·10−4 5,000 0.3 True Table 5.DQN (ResNet) Atari hyper-parameters Algorithm Weight Decay Learning Rate Update Interval Drop Fraction Sparsity-aware init Dense 1 ·10−5 1 ·10−4 - - - Pruning 1 ·10−5 1 ·10−4 5,000 - False Static 1 ·10−5 1 ·10−4 - - True RigL 1 ·10−5 1 ·10−4 5,000 0.3 True SET 1 ·10−5 1 ·10−4 5,000 0.3 True Remaining hyper-parameters Next, we include details of the DRL hyper-parameters used in all training settings for DQN (Table 6 and Table 7), SAC (Table 8), and PPO (Table 8).The State of Sparse Training in Deep Reinforcement Learning Table 6.DQN Hyperparameters/ Table format from Haarnoja et al. (2018). Parameter Value Shared optimizer Adam (Kingma & Ba, 2015b) discount (γ) 0.99 nonlinearity ReLU target smoothing coefﬁcient (τ) 1.0 gradient steps per training step 1 exploration policy epsilon greedy epsilon decay period (env steps) 2.5 ·104 Classic Control replay buffer size 105 learning rate 1 ·10−3 initial collect steps 1,000 target update interval 100 reward scale factor 1.0 gradient steps every k env steps, k = 1 ﬁnal epsilon 0.1 eval epsilon 0.1 number of samples per minibatch 128 network type MLP number of hidden dense layers 2 number of hidden units per layer 512 Atari replay buffer size 106 initial collect steps 20,000 target update interval 8000 reward scale factor 1.0 gradient steps every k env steps, k = 4 ﬁnal epsilon 0.01 eval epsilon 0.001 number of samples per minibatch 32 network type CNN or ResNetThe State of Sparse Training in Deep Reinforcement Learning Table 7.DQN Atari: CNN and ResNet Architectures Parameter Value CNN learning rate 2.5 ·10−4 Adam optimizer, epsilon 1 ·10−8 CNN Architecture number of hidden CNN layers 3 number of hidden dense layers 1 number of hidden units per dense layer 512 CNN params per layer (ﬁlters, kernel, stride) layer 1 (ﬁlters, kernel, stride) 32, 8, 4 layer 2 (ﬁlters, kernel, stride) 64, 4, 2 layer 3 (ﬁlters, kernel, stride) 64, 3, 1 ResNet learning rate 1 ·10−4 Adam optimizer, epsilon 3.125 ·10−4 ResNet Architecture number of stacks 3 number of hidden dense layers 1 number of hidden units per dense layer 512 use batch norm False ResNet stack layers num CNN layers 1 num max pooling layers 1 num residual-CNN layers 2 ResNet params per layer (ﬁlters, kernel, stride) stack 1 (ﬁlters, kernel, stride) 32, 3, 1 stack 2 (ﬁlters, kernel, stride) 64, 3, 1 stack 3 (ﬁlters, kernel, stride) 64, 3, 1The State of Sparse Training in Deep Reinforcement Learning Table 8.SAC Hyperparameters. Parameter Value optimizer Adam (Kingma & Ba, 2015b) learning rate 3 ·10−4 discount (γ) 0.99 replay buffer size 106 number of hidden layers (all networks) 2 number of hidden units per layer 256 number of samples per minibatch 256 nonlinearity ReLU target smoothing coefﬁcient (τ) 0.005 target update interval 1 train every k env steps, k = 1 gradient steps per training step = 1 Hopper, Walker, Humanoid initial collect steps 1,000 HalfCheetah, Ant initial collect steps 10,000 Table 9.PPO Hyperparameters. Parameter Value optimizer Adam (Kingma & Ba, 2015b) learning rate 3 ·10−4 discount (γ) 0.99 shared / separate networks separate number of hidden layers (all networks) 2 number of hidden units per layer 64 collect sequence length (batch size) 2048 minibatch size 64 num epochs 10 importance ratio clipping 0.2 use GAE (Schulman et al., 2016) True λ(GAE) 0.95 entropy regularization 0 value loss coeff 0.5 gradient clipping 0.5 A.4. Atari Game Selection Our original three games (MsPacman, Pong, Qbert) were selected to have varying levels of difﬁculty as measured by DQN’s human normalized score in Mnih et al. (2015), Figure 3. To this we added 12 games (Assault, Asterix, BeamRider, Boxing, Breakout, CrazyClimber, DemonAttack, Enduro, FishingDerby, SpaceInvaders, Tutankham, VideoPinball) selected to be roughly evenly distributed amongst the games ranked by DQN’s human normalized score in Mnih et al. (2015) with a lower cut off of approximately 100% of human performance.The State of Sparse Training in Deep Reinforcement Learning B. Sparse Scaling Plots in Other Environments Here we share results on additional environments, Acrobot, CartPole, MountainCar, Hopper, and Ant. Figure 8 compares ﬁnal reward relative to parameter count using DQN. ERK sparsity distribution was used in the top row whilst uniform was used in the bottom row. Figure 9 presents results on the two remaining MuJoCo environment, Hopper and Ant with SAC (top row) and PPO (bottom row). In Figures 10 and 11 we show sparsity scaling plots for 15 Atari games using the standard CNN and ResNet respectively. 104 105 #Params 200 180 160 140 120 100 80 60 Reward 0.500.700.800.900.950.960.97 0.98 DQN / Acrobot-v1 Dense Pruning Rigl Set Static 104 105 #Params 100 120 140 160 180 200 0.500.700.80 0.900.950.960.970.98 DQN / CartPole-v0 104 105 #Params 200 180 160 140 120 100 0.500.70 0.80 0.900.95 0.96 0.97 0.98 DQN / MountainCar-v0 104 105 #Params 200 180 160 140 120 100 80 60 Reward 0.500.600.700.80 0.90 0.950.96 0.970.98 DQN / Acrobot-v1 Dense Pruning Rigl Set Static 104 105 #Params 100 120 140 160 180 200 0.500.600.700.800.90 0.95 0.96 0.97 0.98 DQN / CartPole-v0 104 105 #Params 200 180 160 140 120 100 0.500.600.700.800.900.950.960.970.98 DQN / MountainCar-v0 Figure 8.DQN in the Classic Control environments with ERK network sparsity distribution (top) and uniform network sparsity (bottom). 104 105 #Params 500 1000 1500 2000 2500 3000 3500Reward 0.50 0.70 0.80 0.900.950.960.97 0.98 SAC / Hopper-v2 Dense Pruning Rigl Set Static 104 105 #Params 0 1000 2000 3000 4000 5000Reward 0.500.70 0.80 0.90 0.950.96 0.97 0.98 SAC / Ant-v2 Dense Pruning Rigl Set Static 103 #Params 500 1000 1500 2000 2500 3000Reward 0.50 0.70 0.80 0.90 0.95 0.96 0.970.98 0.99 PPO / Hopper-v2 Dense Pruning Rigl Set Static 103 104 #Params 1000 1500 2000 2500 3000 3500 4000Reward 0.500.70 0.800.90 0.95 0.96 0.970.98 0.99 PPO / Ant-v2 Dense Pruning Rigl Set Static Figure 9.Additional MuJoCO environments (Hopper and Ant) for SAC and PPO algorithms. Networks are initialized with ERK network sparsity distribution.The State of Sparse Training in Deep Reinforcement Learning 105 106 #Params 400 600 800 1000 1200Reward 0.500.80 0.900.95 0.98 0.99 Assault / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 500 1000 1500 2000Reward 0.500.80 0.90 0.95 0.98 0.99 Asterix / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 500 750 1000 1250 1500 1750 2000 2250 2500Reward 0.50 0.80 0.90 0.95 0.98 0.99 BeamRider / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 0 20 40 60 80Reward 0.50 0.80 0.900.95 0.98 0.99 Boxing / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 0 10 20 30 40 50 60 70Reward 0.50 0.80 0.90 0.95 0.98 0.99 Breakout / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 0 20000 40000 60000 80000 100000Reward 0.50 0.800.90 0.95 0.98 0.99 CrazyClimber / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 200 400 600 800 1000 1200 1400Reward 0.50 0.80 0.90 0.95 0.98 0.99 DemonAttack / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 0 100 200 300 400 500 600Reward 0.50 0.800.90 0.95 0.98 0.99 Enduro / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 100 80 60 40 20 Reward 0.50 0.80 0.90 0.950.98 0.99 FishingDerby / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 500 1000 1500 2000 0.500.800.900.95 0.98 0.99 MsPacman / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 20 15 10 5 0 5 10 15 Reward 0.50 0.800.900.950.98 0.99 Pong / DQN (CNN) 105 106 #Params 0 1000 2000 3000 4000 5000 0.50 0.80 0.90 0.95 0.98 0.99 Qbert / DQN (CNN) 105 106 #Params 300 400 500 600 700 800Reward 0.500.80 0.90 0.95 0.98 0.99 SpaceInvaders / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 20 40 60 80 100 120Reward 0.500.800.900.95 0.98 0.99 Tutankham / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 0 50000 100000 150000 200000 250000Reward 0.500.80 0.90 0.95 0.98 0.99 VideoPinball / DQN (CNN) Dense Pruning RigL SET Static Figure 10.CNN: Sparsity plots per game.The State of Sparse Training in Deep Reinforcement Learning 105 106 #Params 500 1000 1500 2000 2500Reward 0.50 0.80 0.90 0.95 0.98 0.99 Assault / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 20000 40000 60000 80000 100000 120000Reward 0.50 0.80 0.900.95 0.98 0.99 Asterix / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 2000 4000 6000 8000Reward 0.50 0.800.90 0.95 0.98 0.99 BeamRider / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 40 20 0 20 40 60 80 100Reward 0.500.800.900.950.98 0.99 Boxing / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 50 100 150 200 250 300Reward 0.50 0.80 0.90 0.95 0.98 0.99 Breakout / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 20000 40000 60000 80000 100000 120000Reward 0.500.800.900.950.98 0.99 CrazyClimber / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 10000 20000 30000 40000 50000 60000 70000Reward 0.50 0.80 0.90 0.95 0.98 0.99 DemonAttack / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 200 400 600 800 1000 1200 1400Reward 0.500.80 0.900.950.980.99 Enduro / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 100 80 60 40 20 0 20 Reward 0.500.800.900.950.98 0.99 FishingDerby / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 500 1000 1500 2000 2500 3000 3500 4000Reward 0.50 0.80 0.90 0.95 0.98 0.99 MsPacman / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 20 10 0 10 20 Reward 0.500.800.900.950.980.99 Pong / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 2000 4000 6000 8000Reward 0.500.80 0.900.95 0.98 0.99 Qbert / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 2000 4000 6000 8000 10000Reward 0.50 0.80 0.90 0.95 0.98 0.99 SpaceInvaders / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 25 50 75 100 125 150 175Reward 0.500.80 0.90 0.950.98 0.99 Tutankham / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 50000 100000 150000 200000 250000Reward 0.500.80 0.90 0.95 0.98 0.99 VideoPinball / DQN (ResNet) Dense Pruning RigL SET Static Figure 11.ResNet: Sparsity plots per game.The State of Sparse Training in Deep Reinforcement Learning C. Additional Results with Uniform Sparsity Distribution In Figure 12 we repeat the experiments presented in Figure 2, however this time using a uniform network sparsity distribution at initialization. These plots provide further evidence as the the beneﬁt of using ERK over uniform to distribute network sparsity. 105 106 #Params 20 15 10 5 0 5 10 15 Reward 0.500.800.90 0.95 0.98 0.99 Pong / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 0 1000 2000 3000 4000 5000 6000 0.50 0.80 0.90 0.950.980.99 Qbert / DQN (CNN) 105 106 #Params 500 1000 1500 2000 2500 0.500.80 0.90 0.95 0.98 0.99 MsPacman / DQN (CNN) 104 105 #Params 0 1000 2000 3000 4000 5000Reward 0.500.600.700.80 0.90 0.950.960.970.98 SAC / Walker2d-v2 Dense Pruning Rigl Set Static 104 105 #Params 0 2000 4000 6000 8000 10000 12000Reward SAC / HalfCheetah-v2 Dense Pruning Rigl Set Static 104 105 #Params 1000 2000 3000 4000 5000 0.50 0.60 0.70 0.800.900.95 0.96 0.97 0.98 SAC / Humanoid-v2 Dense Pruning Rigl Set Static 103 #Params 500 1000 1500 2000 2500 3000 3500Reward 0.50 0.70 0.80 0.90 0.950.96 0.97 0.980.99 PPO / Walker2d-v2 103 #Params 0 1000 2000 3000 4000 5000 0.50 0.700.80 0.90 0.95 0.96 0.97 0.98 0.99 PPO / HalfCheetah-v2 103 104 #Params 250 500 750 1000 1250 1500 1750 2000 2250 0.500.70 0.80 0.90 0.95 0.960.97 0.980.99 PPO / Humanoid-v2 Dense Pruning Rigl Set Static Figure 12.Uniform network sparsity initialization. Comparison of ﬁnal reward relative to parameter count for the various methods considered: (row-1) DQN on Atari (row-2) SAC on MuJoCo (row-3) PPO on MuJoCo. We consider sparsities from 50% to 95% for sparse training methods and pruning. Parameter count for networks with 80% sparsity and the reward obtained by the dense baseline are highlighted with vertical and horizontal lines. Shaded areas represent 95% conﬁdence intervals.The State of Sparse Training in Deep Reinforcement Learning D. Additional Hyper-parameter Sensitivity Plots Here we share the remaining plots for our analysis on the sensitivity of sparse training algorithms to various hyper-parameters. Policies area trained using SAC. We note that different architectures, environments and training algorithms might show different curves, which we omit due to high cost of running such analysis. 0.0 0.1 0.2 0.3 0.4 0.5 Drop Fraction 0 1000 2000 3000 4000 5000 6000Reward Walker2d Rigl Set 102 103 Update Interval 0 1000 2000 3000 4000 5000 6000Reward Walker2d Pruning Rigl Set 0 200 400 600 800 1000 Batch size 0 1000 2000 3000 4000 5000 6000Reward Walker2d Set Dense RigL Static Figure 13.Sensitivity analysis: (left) Drop fraction: Comparing different drop fractions for SET and RigL at 80% sparsity. (center) Comparing the effect of topology update interval on pruning, static, and RigL. (right) The effect of batch size. Drop fraction In Figure 13 (left) we evaluate the effect of drop fraction and observe that drop fractions >0 yield a small performance improvement over drop fraction = 0, which is equivalent to static sparse training. This indicates that changing the network topology during training helps performance. Surprisingly training does not appear particularly sensitive to the drop fraction chosen, with values from 10 - 50% yielding approximately equivalent performance. It is possible that this is because the environment is relatively easy, thus leading to little separation between different settings. The large improvement that RigL and SET give over static (drop fraction = 0) in the Humanoid and Atari environments is one reason to suspect this. Findings: Drop fractions of 10 - 50% worked well for RigL, whilst a drop fraction of 30% worked best for SET. 30% therefore appears to be a reasonable default for dynamic sparse training. However, the uniformity of performance merits investigation on a harder environment in which more separation between drop fractions may be observed. Topology update interval In Figure 13 (center) we evaluate the effect of topology update interval on pruning, RigL and SET and ﬁnd that training is not particularly sensitive to it. Findings: Updating the network every 1000 environment steps appears to be a reasonable default. Batch size In Figure 13 (right) we evaluate the effect of batch size. Batch size is critical for obtaining a good estimate for the gradients during training for all methods, however for RigL it is also used when selecting new connections. We observe performance degradation for all methods when smaller batch sizes are used, with no particular additional effect on RigL. Findings: Sparse networks seems to have similar sensitivity to the batch size as the dense networks. 0.5 0.6 0.7 0.8 0.9 1.0 Sparsity 0 2000 4000 6000 8000 10000 12000Reward Half-Cheetah Algorithm rigl static Distribution erk uniform 0.5 0.6 0.7 0.8 0.9 1.0 Sparsity 0 2000 4000 6000 8000 10000Reward Half-Cheetah Algorithm rigl static Distribution erk uniform 0.5 0.6 0.7 0.8 0.9 1.0 Sparsity 400 350 300 250 200 150 100 Reward Acrobot Algorithm rigl static Distribution erk uniform Figure 14.Evaluating non uniform sparsities on HalfCheetah using SAC when all layers pruned (left) and last layer is kept dense (center). On the right we prune all layers of the DQN MLP network on the Acrobot task.The State of Sparse Training in Deep Reinforcement Learning Within network sparsity In Figure 14 we share additional analysis on within network sparsity for SAC and also on DQN in the Acrobot environment. E. Additional Interquartile Mean (IQM) Plots Mujoco SAC In Figure 15 we present the interquartile mean (IQM) calculated over ﬁve Mujoco environments for SAC at four different sparsities, 50%, 90%, 95% and 99%, or the networks with the equivalent number of parameters in the case of Dense training. Note that for 99% sparsity the IQM is only calculated over three Mujoco environments. 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static SAC / s=0.5 Normalized Score 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static SAC / s=0.9 Normalized Score 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static SAC / s=0.95 Normalized Score 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static SAC / s=0.99 Normalized Score Figure 15.SAC: IQM plots calculated over ﬁve Mujoco games (Ant, HalfCheetah, Hopper, Humanoid, Walker2d), with 10 seeds per game. Except for sparsity = 0.99 which only includes results from HalfCheetah, Hopper, and Walker2d, each with 10 seeds.The State of Sparse Training in Deep Reinforcement Learning Mujoco PPO In Figure 16 we present the interquartile mean (IQM) calculated over ﬁve Mujoco environments for PPO at four different sparsities, 50%, 90%, 95% and 99%, or the networks with the equivalent number of parameters in the case of Dense training. Note that for 95% sparsity the IQM is calculated over four environments and for 99% sparsity the IQM is only calculated over three Mujoco environments. 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static PPO, s = 0.5 Normalized Score 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static PPO, s = 0.9 Normalized Score 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static PPO, s = 0.95 Normalized Score 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static PPO, s = 0.99 Normalized Score Figure 16.SAC: IQM plots calculated over multiple Mujoco games with 9 seeds per game. 50 - 90% sparsity include ﬁve games (Ant, HalfCheetah, Hopper, Humanoid, Walker2d), 95% sparsity includes Ant, HalfCheetah, Hopper, Walker2d, and 99% sparsity HalfCheetah, Hopper, Walker2d. Atari DQN Figure 17 presents IQM plots calculated over 15 Atari games for the standard CNN network architecture and Figure 18 presents IQM plots calculated over the same set of games for a ResNet architecture with an approximately equivalent number of parameters as the standard CNN (≈4M).The State of Sparse Training in Deep Reinforcement Learning 0.00 0.25 0.50 0.75 1.00 Dense:100% Dense Pruning RigL SET Static DQN (CNN) / s=0.5 Human Normalized Score 0.00 0.25 0.50 0.75 1.00 Dense:100% Dense Pruning RigL SET Static DQN (CNN) / s=0.9 Human Normalized Score 0.00 0.25 0.50 0.75 1.00 Dense:100% Dense Pruning RigL SET Static DQN (CNN) / s=0.95 Human Normalized Score 0.00 0.25 0.50 0.75 1.00 Dense:100% Dense Pruning RigL SET Static DQN (CNN) / s=0.99 Human Normalized Score Figure 17.CNN: IQM plots calculated over 15 Atari Games, with 9 seeds per game. 0 1 2 3 4 Dense:100% Dense Pruning RigL SET Static DQN (ResNet) / s=0.5 Human Normalized Score 0 1 2 3 4 Dense:100% Dense Pruning RigL SET Static DQN (ResNet) / s=0.9 Human Normalized Score 0 1 2 3 4 Dense:100% Dense Pruning RigL SET Static DQN (ResNet) / s=0.95 Human Normalized Score 0 1 2 3 4 Dense:100% Dense Pruning RigL SET Static DQN (ResNet) / s=0.99 Human Normalized Score Figure 18.ResNet: IQM plots calculated over 15 Atari Games, with 9 seeds per game.",
      "meta_data": {
        "arxiv_id": "2206.10369v1",
        "authors": [
          "Laura Graesser",
          "Utku Evci",
          "Erich Elsen",
          "Pablo Samuel Castro"
        ],
        "published_date": "2022-06-17T14:08:00Z",
        "pdf_url": "https://arxiv.org/pdf/2206.10369v1.pdf"
      }
    },
    {
      "title": "The State of Sparse Training in Deep Reinforcement Learning",
      "abstract": "The use of sparse neural networks has seen rapid growth in recent years,\nparticularly in computer vision. Their appeal stems largely from the reduced\nnumber of parameters required to train and store, as well as in an increase in\nlearning efficiency. Somewhat surprisingly, there have been very few efforts\nexploring their use in Deep Reinforcement Learning (DRL). In this work we\nperform a systematic investigation into applying a number of existing sparse\ntraining techniques on a variety of DRL agents and environments. Our results\ncorroborate the findings from sparse training in the computer vision domain -\nsparse networks perform better than dense networks for the same parameter count\n- in the DRL domain. We provide detailed analyses on how the various components\nin DRL are affected by the use of sparse networks and conclude by suggesting\npromising avenues for improving the effectiveness of sparse training methods,\nas well as for advancing their use in DRL.",
      "full_text": "The State of Sparse Training in Deep Reinforcement Learning Laura Graesser* 1 2 Utku Evci* 2 Erich Elsen3 Pablo Samuel Castro2 Abstract The use of sparse neural networks has seen rapid growth in recent years, particularly in computer vision. Their appeal stems largely from the re- duced number of parameters required to train and store, as well as in an increase in learning efﬁ- ciency. Somewhat surprisingly, there have been very few efforts exploring their use in Deep Re- inforcement Learning (DRL). In this work we perform a systematic investigation into applying a number of existing sparse training techniques on a variety of DRL agents and environments. Our results corroborate the ﬁndings from sparse train- ing in the computer vision domain – sparse net- works perform better than dense networks for the same parameter count – in the DRL domain. We provide detailed analyses on how the various com- ponents in DRL are affected by the use of sparse networks and conclude by suggesting promising avenues for improving the effectiveness of sparse training methods, as well as for advancing their use in DRL1. 1. Introduction Deep neural networks are typically organized as a stack of layers. Each layer consists of multiple neurons, where each neuron is connected to all neurons in the next layer; this is often referred to as a dense network. Alternatively, each neuron can be wired to a subset of the neurons in the next layer, resulting in a sparse, and smaller, network. Such sparse neural networks have been shown to match the performance of their dense counterparts while requiring only 10%-to-20% of the connections in most cases (Han et al., 2015; Gale et al., 2019; Blalock et al., 2020) providing *Equal contribution 1Robotics at Google 2Google Research, Canada 3Adept AI. Correspondence to: Laura Graesser <laura- graesser@google.com>, Utku Evci <evcu@google.com>, Pablo Samuel Castro <psc@google.com>. Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s). 1Code for reproducing our results can be found at github.com/google-research/rigl/tree/master/rigl/rl signiﬁcant memory, storage, and latency gains. Deep networks have become a mainstay of scalable rein- forcement learning (RL), key to recent successes such as playing – at superhuman levels – Atari games (Mnih et al., 2015), Go (Silver et al., 2016), Dota 2 (Berner et al., 2019) and as well as controlling complex dynamical systems such as stratospheric balloons (Bellemare et al., 2020) and plasma in real-time (Degrave et al., 2022). Despite their importance, most deep reinforcement learning (DRL) research focuses on improving the algorithmic aspect of DRL, and less on the architecture aspect. Sparse networks in particular have received very little attention, likely due to the belief that net- work over-parameterization helps with learning. However, recent work suggests that RL agents may suffer from im- plicit under-parameterization when training deep networks with gradient descent (Kumar et al., 2021), suggesting that the network’s expressivity is in fact underused. In addition to this, Nikishin et al. (2022) suggests deep RL agents may have a tendency to overﬁt to early training data. Given this, one might expect there is substantial opportunity to com- press RL agents. Further, sparse networks might beneﬁt DRL by reducing the cost of training or aid running them in latency-constrained settings such as controlling plasma (Degrave et al., 2022). One limitation of current research on training sparse neural networks is that it almost solely focuses on image classiﬁca- tion benchmarks (Blalock et al., 2020; Hoeﬂer et al., 2021) creating the risk of over-ﬁtting to a speciﬁc domain. Do ad- vances observed in computer vision (CV) transfer to DRL? A few recent works (Sokar et al., 2021; Arnob et al., 2021) attempt to address this by applying individual sparse train- ing algorithms to DRL agents. However, it is still unknown if the key observation made in CV , thatsparse models per- form better than dense ones for the same parameter count, transfers to DRL. In this work we focus on answering that question and sys- tematically explore the effectiveness of different sparse learning algorithms in the online DRL setting. In order to achieve this, we benchmark four different sparse training algorithms using value-based (DQN (Mnih et al., 2015)) and actor-critic, (SAC (Haarnoja et al., 2018) and PPO (Schul- man et al., 2017)) agents. Our results also include a broad analysis of various components that play a role in the train- arXiv:2206.10369v1  [cs.LG]  17 Jun 2022The State of Sparse Training in Deep Reinforcement Learning ing of these sparse networks: sparsity distribution strategies, weight decay, layer initialization, signal-to-noise ratio for gradients, as well as batch size, topology update strategy and frequency. We summarize our main ﬁndings below: • In almost all cases, sparse neural networks perform bet- ter than their dense counterparts for a given parameter count, demonstrating their potential for DRL. • It is possible to train up to 80 - 90% sparse networks with minimal loss in performance compared to the standard dense networks. • Pruning often obtains the best results, and dynamic sparse training improves over static sparse training sig- niﬁcantly. However gradient based growth (Evci et al., 2020) seems to have a limited effect on performance. We argue this is due to low signal-to-noise ratio in gradients. • The distribution of parameters among the actor and critic networks, as well as among different layers, im- pact training greatly. We observe that the best perfor- mance is obtained by allocating the majority of parame- ters to the critic network and using Erdos Renyi Kernel (ERK) sparsity distributions. • We observe robust performance over various hyper- parameter variations. Somewhat surprisingly, when adding noise to the observations, sparse methods achieve better robustness in most cases. 2. Background 2.1. Sparse training Removing connections from neural networks was suggested at least as early as Mozer & Smolensky (1989), which coined the name “Skeleton” networks for what we today call sparse networks. Techniques for ﬁnding sparse neural networks can be grouped under two main categories. (1) Dense-to-sparse training approaches (Han et al., 2016; Molchanov et al., 2017; Wortsman et al., 2019; Kusu- pati et al., 2020; Peste et al., 2021) start with a dense neural network and gradually reduce the network size by pruning its weights. This approach often achieves state-of-the-art performance amongst sparse networks, however it requires the same (or more) computation as training a large dense net- work. An alternative to pruning is (2) sparse training (Mocanu et al., 2018). This family of methods sparsiﬁes the network at initialization and maintains this sparsity through- out training, thus reducing the training cost proportional to the sparsity of the network. However, training sparse neural networks from scratch is known to be difﬁcult, leading to sub-optimal solutions (Frankle & Carbin, 2019; Liu et al., 2019; Evci et al., 2019). DRL training is notoriously resource hungry, hence we fo- cus on the second family of methods (i.e. sparse training) in this work. There are various approaches to sparse training. One line of work (Lee et al., 2019; Wang et al., 2020; Tanaka et al., 2020), attempts to prune a dense networkimmediately on iteration 0. The resulting networks are used as an initial- ization for sparse training and kept ﬁxed throughout. These techniques have been shown to have marginal gains over random pruning (Frankle et al., 2020), especially when used in modern training pipelines. Furthermore they may not generalize well in the RL setting as the non-stationarity of the data make it less clear that any decision made at iteration 0 will remain optimal throughout training. Another line of work starts with randomly initialized sparse neural networks (both weights and masks) and focuses on improving sparse training by changing the sparse connec- tivity among neurons (Mocanu et al., 2018; Bellec et al., 2018) throughout the optimization. Known as Dynamic sparse training (DST), such approaches have been shown to match pruning results, making it possible to train sparse net- works efﬁciently without sacriﬁcing performance (Dettmers & Zettlemoyer, 2019; Evci et al., 2020). In this work we benchmark one dense-to-sparse and three sparse training methods, which we brieﬂy describe below: Pruning (Zhu & Gupta, 2018):uses a simple procedure to slowly make a dense network sparse over the course of one training run using weight magnitudes. We start pruning the network from 20% of the training steps and stop when we reach 80%, keeping the ﬁnal sparse network ﬁxed for the remaining of the training. This simple pruning algorithm is shown to exceed or match more complex pruning algorithms (Gale et al., 2019). Despite the fact it requires the same order of magnitude resources as training a dense network, we included this method since it serves as an upper bound on the sparse training performance. Static: prunes a given dense network randomly at initial- ization and the resulting sparse network is trained with a ﬁxed structure. This is an important baseline to show the effectiveness of DST algorithms explained below. Sparse Evolutionary Training (SET) (Mocanu et al., 2018): Similar to Static, SET starts training with a random sparse network. During training, a portion of the connec- tions are changed every N steps (the update interval) by replacing the lowest magnitude connections with new ran- dom ones. The fraction (drop fraction) of updated weights are decayed over the course of training to help the network converge to a minima. We use cosine decay as proposed by Dettmers & Zettlemoyer (2019). Rigged Lottery (RigL) (Evci et al., 2020):is the same as SET, except the new connections are activated using the gradient signal (highest magnitude) instead of at random.The State of Sparse Training in Deep Reinforcement Learning This criteria has been shown to improve results signiﬁcantly in image classiﬁcation and with enough training iterations matches or exceed accuracies obtained by pruning. 2.2. Reinforcement learning Reinforcement learning (RL) aims to design learning algo- rithms for solving sequential decision-making problems. Typically these are framed as an agent interacting with an environment at discrete time-steps by making action choices from a set of possible agent states; the environ- ment in turn responds to the action selection by (possibly) changing the agent’s state and/or providing a numerical reward (or cost); the agent’s objective is to ﬁnd a pol- icy mapping states to actions so as to maximize (mini- mize) the sum of rewards (costs). This is formalized as a Markov decision process (Puterman, 1994) deﬁned as a tuple ⟨X,A,P,R,γ⟩, where X is the state space, A is the action space, P : X ×A →∆(X) deﬁnes the transition dynamics2, R : X ×A →R is the reward function, and γ ∈[0,1) is a discount factor. A policy π : X →∆(A) formalizes an agent’s behaviour and induces a value func- tion Vπ : X →R deﬁned via the well-known Bellman recurrence: Vπ(x) := Ea∼π(x) [ R(x,a) + γEx′∼P(x,a)Vπ(x′) ] (1) It is convenient to deﬁne state-action value functions Qπ : X×A →R as: Qπ(x,a) := R(x,a)+γEx′∼P(x,a)Vπ(x′). The goal of an RL agent is to ﬁnd a policy π∗:= maxπVπ (which is guaranteed to exist); for notational convenience we denote V∗:= Vπ∗ and Q∗:= Qπ∗ . In online RL the agent achieves this by iteratively improving an initial policy π0: {π0,π1,··· ,πt,···} and using these intermediate policies to collect new experience from the environment in the form of transitions (x,a,r,x ′), where a ∼πt(x), r = R(x,a), and x′∼P(x,a). These transitions constitute the dataset the agent uses to improve its policies. In other words, the learning proocess is a type of closed feedback loop : an agent’s policy directly affects the data gathered from the environment, which in turn directly affects how the agent updates its policy. When X is very large, it is impractical to store Vπ and Qπ in a table, so a function approximator Vθ ≈Vπ (where θare the approximator’s parameters) is employed instead. This function approximator is usually one or more deep networks, and this type of RL is known as deep RL (DRL). DRL algorithms can be broadly categorized into two groups: Value-based: The function Qπ is approximated by a deep network Qθ. The policy is directly induced from the value 2∆(X) denotes the set of probability distributions over a ﬁnite set X. estimate via πt(x) = arg maxa∈A Qθt (x,a)3. The parame- ters θare trained using a temporal difference loss (based on Equation 1) from transitions sampled from D: L(θ) = E(x,a,r,x′)∼D [ Qθ(x,a) −(r+ γmax a′∈A Q¯θ(x′,a′)) ] (2) Here, ¯θs a copy of θthat is infrequently synced with θfor more stable training (Mnih et al., 2015). These methods are typically employed for discrete control environments, where there is a ﬁnite (and relatively small) set of actions (e.g. Atari games (Bellemare et al., 2013)). Policy-gradient: In contrast to value-based methods where the policy is implicitly improved by virtue of improving Qθ, policy-gradient methods maintain and directly improve upon a policy πψ parameterized by ψ. These methods typi- cally still make use of a value estimate Qθ as part of their learning process, and are thus often referred to as actor-critic methods (where πψ is the actor and Qθ the critic). Two po- tential advantages of these methods is that they can be more forgiving of errors in the Qθ estimates, and they can handle continuous action spaces (for instance, by having πψ(x) output mean and variance parameters from which actions may be sampled). These methods are typically employed for continuous control environments, where the action space is continuous (e.g. MuJoCo (Todorov et al., 2012)). 3. Experimental setup DRL algorithms We investigate both value-based and policy-gradient methods. We chose DQN (Mnih et al., 2015) as the value-based algorithm, as it is the algorithm that ﬁrst spurred the ﬁeld of DRL, and has thus been extensively stud- ied and extended. We chose two actor-critic algorithms for our investigations: an on-policy algorithm (PPO (Schulman et al., 2017)) and an off-policy one (SAC (Haarnoja et al., 2018)); both are generally considered to be state-of-the-art for many domains. Environments For discrete-control we focus on three classic control environments (CartPole, Acrobot, and Moun- tainCar) as well as 15 games from the ALE Atari suite (Bellemare et al., 2013) (see subsection A.4 for game selec- tion details). For continuous-control we use ﬁve environ- ments of varying difﬁculty from the MuJoCo suite (Todorov et al., 2012) (HalfCheetah, Hopper, Walker2d, Ant, and Humanoid). Rewards obtained by DRL algorithms have notoriously high variance (Agarwal et al., 2021). Therefore we repeat each experiment with at least 10 different seeds 3Although there are other mechanisms for deﬁning a policy, such as using a softmax, and there are exploration strategies to consider, we present only the argmax setup for simplicity, as the other variants are mostly orthogonal to our analyses.The State of Sparse Training in Deep Reinforcement Learning 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static SAC / s=0.9 Normalized Score 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static PPO, s = 0.9 Normalized Score 0.00 0.25 0.50 0.75 1.00 Dense:100% Dense Pruning RigL SET Static DQN (CNN) / s=0.9 Human Normalized Score 0 1 2 3 4 Dense:100% Dense Pruning RigL SET Static DQN (ResNet) / s=0.9 Human Normalized Score Figure 1.IQM plots for networks at 90% sparsity for various architecture and algorithm combinations. SAC and PPO are averaged over 5 MuJoCo environments, whereas DQN is averaged over 15 Atari environments. Results at different sparsities can be found at Appendix E. ”Dense: 100%” corresponds to the standard dense model. Atari scores were normalized using human performance per game. MuJoCo scores were normalized using the average returns obtained by the Dense: 100% SAC agent per game. and report the average reward obtained over the last 10% of evaluations. We also provide 95% conﬁdence intervals in all plots. See subsection A.1 for additional details. Training For each sparse training algorithm considered ( Pruning , Static, RigL, and SET) we train poli- cies ranging between 50% to 99% sparsity. To ensure a fair comparison between algorithms, we performed a hy- per parameter sweep for each algorithm separately. The exception is DQN experiments on Atari for which it was too computationally expensive to do a full hyper-parameter sweep and we used values found in previous experiments instead. Sparse results in these environments may therefore be conservative compared to the well tuned dense baseline. In addition to training the standard dense networks used in the literature, we also train smaller dense networks by scaling down layer widths to approximately match the pa- rameter counts of the sparse networks, thereby providing a ”parameter-equivalent” dense baseline. We share details of the hyper parameter sweeps and hyper parameters used for each algorithm in subsection A.3. Code Our code is built upon the TF-Agents (Guadarrama et al., 2018), Dopamine (Castro et al., 2018), and RigL (Evci et al., 2020) codebases. We use rliable (Agarwal et al., 2021) to calculate the interquartile mean (IQM) and plot the results. The IQM is calculated by discarding the bottom and top 25% of normalized scores aggregated from multiple runs and environments, then calculating the mean (reported with 95% conﬁdence intervals) over the remaining 50% runs. Code for reproducing our results can be found at github.com/google-research/rigl/tree/master/rigl/rl 4. The State of Sparse Networks in Deep RL We begin by presenting the outcome of our analyses in Figure 1 and Figure 2 for DQN (Atari), PPO and SAC (Mu- JoCo). Figure 1 presents the IQM at 90% sparsity, whilst in Figure 2 we evaluate ﬁnal performance relative to the number of parameters. We share results for classic control, 2 additional MuJoCo and 12 additional Atari environments in Appendix B. Three main conclusions emerge; (1) In most cases performance obtained by sparse networks signiﬁcantly exceeds that of their dense counterparts with a comparable number of parameters. Critically, in more difﬁcult environ- ments requiring larger networks (e.g. Humanoid, Atari), sparse networks can be obtained with efﬁcient sparse train- ing methods. (2) It is possible to train sparse networks with up to 80-90% fewer parameters and without loss in perfor- mance compared to the standard dense model. (3) Gradient based growing (i.e. RigL) seems to have limited impact on the performance of sparse networks. Next, we discuss each of these points in detail. Sparse networks perform better.Inline with previous ob- servations made in speech (Kalchbrenner et al., 2018), natu-The State of Sparse Training in Deep Reinforcement Learning 105 106 #Params 20 15 10 5 0 5 10 15 Reward 0.50 0.800.900.950.98 0.99 Pong / DQN (CNN) 105 106 #Params 0 1000 2000 3000 4000 5000 0.50 0.80 0.90 0.95 0.98 0.99 Qbert / DQN (CNN) 105 106 #Params 500 1000 1500 2000 0.500.800.900.95 0.98 0.99 MsPacman / DQN (CNN) Dense Pruning RigL SET Static 104 105 #Params 0 1000 2000 3000 4000 5000Reward 0.500.700.800.90 0.950.96 0.97 0.98 SAC / Walker2d-v2 104 105 #Params 0 2000 4000 6000 8000 10000 12000 0.500.700.800.90 0.950.96 0.97 0.98 SAC / HalfCheetah-v2 104 105 #Params 1000 2000 3000 4000 5000 0.500.700.800.90 0.95 0.96 0.970.98 SAC / Humanoid-v2 Dense Pruning Rigl Set Static 103 #Params 500 1000 1500 2000 2500 3000 3500 4000Reward 0.50 0.70 0.80 0.90 0.950.96 0.97 0.98 0.99 PPO / Walker2d-v2 103 #Params 0 1000 2000 3000 4000 5000 0.50 0.70 0.800.90 0.95 0.96 0.97 0.98 0.99 PPO / HalfCheetah-v2 103 104 #Params 500 1000 1500 2000 2500 0.50 0.70 0.80 0.90 0.950.960.97 0.980.99 PPO / Humanoid-v2 Dense Pruning Rigl Set Static Figure 2.Comparison of the ﬁnal reward relative to parameter count for the various methods considered: (row-1) DQN on Atari (CNN) (row-2) SAC on MuJoCo, (row-3) PPO on MuJoCo. We consider sparsities from 50% to 95% (annotated on the pruning curve) for sparse training methods and pruning. Parameter count for networks with 80% sparsity and the reward obtained by the dense baseline are highlighted with vertical and horizontal lines. Shaded areas represent 95% conﬁdence intervals. See Appendix B for results on additional environments. 10 20 30 40 50 60 70 80 90 Parameters allocated for Actor (%) 0 1000 2000 3000 4000 5000Reward Walker2d-v2 10 20 30 40 50 60 70 80 90 Parameters allocated for Actor (%) 0 2000 4000 6000 8000 10000 12000 HalfCheetah-v2 algorithm dense rigl % full dense params 100 20 10 10 20 30 40 50 60 70 80 90 Parameters allocated for Actor (%) 1000 2000 3000 4000 5000 Humanoid-v2 Figure 3.Evaluating how varying the actor-critic parameter ratio affects performance for a given parameter budget on different environment with policies trained using SAC. % of parameters allocated to the actor network is reported on the x axis. In SAC the parameter count of both critic networks is summed to give the overall critic parameter count. The vertical line corresponds the the standard parameter split and the horizontal line to the full dense training reward.The State of Sparse Training in Deep Reinforcement Learning ral language modelling (Li et al., 2020) and computer vision (Evci et al., 2020), in almost all environments, sparse net- works found by pruning achieve signiﬁcantly higher rewards than the dense baseline. However training these sparse net- works from scratch ( static) performs poorly. DST algo- rithms (RigL and SET) improve over static signiﬁcantly, however often fall short of matching the pruning perfor- mance. Critically, we observe that for more difﬁcult environment requiring larger networks such as Humanoid, MsPacman, Qbert and Pong, sparse networks found by efﬁcient DST algorithms exceed the performance of the dense baseline. How sparse?Next we asked how much sparsity is possible without loss in performance relative to that of the standard dense model (denoted by Dense:100% in Figure 1 and by the horizontal lines in Figure 2). We ﬁnd that on average DST algorithms maintain performance up to 90% sparsity using SAC (Figure 1 top left) or DQN ( Figure 1 bottom row), after which performance drops. However performance is variable. For example, DST algo- rithms maintain performance especially well in MsPacman and Humanoid. Whereas in Qbert none of the methods are able to match the performance of the standard dense model at any of the examined levels of sparsity. In the Atari environments, training a ResNet (He et al., 2015) following the architecture from Espeholt et al. (2018) instead of the standard CNN alone provided about 3x im- provement in IQM scores. We were also surprised to see that pruning at 90% sparsity exceeds the performance of the standard ResNet model. These observations indicate that while sparse training can bring very signiﬁcant efﬁciency gains in some environments, it is not a guaranteed beneﬁt. Unlike supervised learning, expected gains likely depend on both task and network, and merits further inquiry. RigL and SET:For most sparsities (50% - 95%) we ob- serve little difference between these two sparse training algo- rithms. At very high sparsities, RigL may outperform SET. The difference can be large (e.g. MsPacman), but is more often moderate (e.g. Pong) or negligible (e.g. Humanoid, Qbert) with overlapping conﬁdence intervals. This suggests that the gradient signal used by RigL may be less informa- tive in the DRL setting compared to image classiﬁcation, where it obtains state-of-the-art performance and consis- tently outperforms SET. Understanding this phenomenon could be a promising direction for improving sparse training methods for DRL. Perhaps unsurprisingly, the clarity of the differences be- tween sparse and dense training is affected by the stability of the underlying RL algorithm. Our results using SAC, designed for stability, were the clearest, as were the DQN results. In contrast, our results using PPO which has much higher variance, were less stark. For this reason, we used SAC and DQN when studying the different aspects of sparse agents in the rest of this work. 5. Where should sparsity be distributed? When searching for efﬁcient network architectures for DRL it is natural to ask where sparsity is best allocated. To that end, we consider both how to distribute parameters between network types and as well as within them. Actor or Critic? Although in value-based agents such as DQN there is a single network, in actor-critic methods such as PPO and SAC there are at least two: an actor and a critic network. It is believed that the underlying functions these networks approximate (e.g. a value function vs. a policy) may have signiﬁcantly different levels of complexity, and this complexity likely varies across environments. Actor and critic typically have near-identical network architectures. However, for a given parameter budget it is not clear that this is the best strategy, as the complexity of the functions being approximated may vary signiﬁcantly. We thus seek to understand how performance changes as the parameter ratio between the actor and critic is varied for a given parameter budget. In Figure 3 we assess three parameter budgets: 100%, 20% and 10% of the standard dense parameter count, and two training regimes, dense and sparse. Given the observed similarity in performance between RigL and SET in Figure 2, we selected one method, RigL, for this analysis. We observe that assigning a low proportion of parameters to the critic (10 - 20%) incurs a high performance cost across all regimes. When parameters are more scarce, in 20% and 10% of standard dense settings, performance degradation is highest. This effect is not symmetric. Reducing the actor parameters to just 10% rarely affects performance compared to the default actor-critic split of 34:66 (vertical line). Interestingly the default split appears well tuned, achieving the best performance in most settings. However in the more challenging Humanoid environment we see that for smaller dense networks, reducing the actor parameters to just 10% yields the best performance. Sparse networks follow a simi- lar trend, but we notice that they appear to be more sensitive to the parameter ratio, especially at higher sparsities. Overall this suggests that the value function is the more complex function to approximate in these settings, bene- ﬁting from the lion’s share of parameters. It also suggests that tuning the parameter ratio may improve performance. Furthermore, FLOPs at evaluation time is determined only by the actor network. Since the actor appears to be easier to compress, this suggests large potential FLOPs savings for real-time usage of these agents. Finally, this approachThe State of Sparse Training in Deep Reinforcement Learning 0.5 0.6 0.7 0.8 0.9 1.0 Sparsity 0 2000 4000 6000 8000 10000 12000Reward Half-Cheetah Algorithm rigl static Distribution erk uniform 0 10 6  10 5  10 4  10 3  10 2 Weight Decay 0 1000 2000 3000 4000 5000 6000Reward Walker2d Dense Pruning Rigl Set Static Static Rigl Set Sparsity-aware Initiliazation 0 1000 2000 3000 4000 5000 6000Reward Walker-2d False (ERK) True (ERK) False (Uniform) True (Uniform) Figure 4.Sensitivity analysis on policies trained with SAC: (left) uniform vs ERK sparsity distributions, (center) weight decay, (right) sparsity-aware vs dense initialization. We use 80% (ERK) sparse networks in all plots unless noted otherwise. Plots for the remaining hyper-parameters are shared in Appendix D. could be used to better understand the relative complexity of policies and values functions across different environments. Within network sparsity In Figure 4 (left) we turn our attention to the question of distributing parameters within networks and compare two strategies; uniform and ERK (Evci et al., 2020). Given a target sparsity of, say 90%, uniform achieves this by making each layer 90% sparse; ERK distributes them proportional to the sum of its dimen- sion, which has the effect of making large layers relatively more sparse than the smaller ones. Due to weight sharing in convolutional layers, ERK sparsity distribution doubles the FLOPs required at a given sparsity (Evci et al., 2020), which we also found to be the case with the convolutional networks used by DQN in the Atari environments (see subsection A.2 for further discussion). On the other hand, ERK has no effect on the FLOPs count of fully connected networks used in MuJoCo environments. Our results show that ERK sig- niﬁcantly improves performance over uniform sparsity and thus we use ERK distribution in all of our experiments and share results with uniform distribution in Appendix C. We hypothesize the advantage of ERK is because it leaves input and output layers relatively more dense, since they typ- ically have few incoming our outgoing connections, and this enables the network to make better use of (a) the observation and (b) the learned representations at the highest layers in the network. It is interesting to observe that maintaining a dense output layer is one of the key design decisions made by Sokar et al. (2021) for their proposed algorithm. 6. Sensitivity analysis In this section we assess the sensitivity of some key hyper- parameters for sparse training. Plots and commentary for the remaining hyper-parameters (drop fraction, topology update interval, and batch size) are shared in Appendix D. Weight decay In Figure 4 (center) we evaluate the effect of weight decay and ﬁnd that a small amount of weight decay is beneﬁcial for pruning, RigL, and SET. This is to be expected since network topology choices are made based on weight magnitude, although we do note that the improvements are quite minor. Surprisingly weight decay seems to help dense even though it is not often used in DRL. Findings: We recommend using small weight decay. Sparsity-aware initialization In Figure 4 (right) we eval- uate the effect of adjusting layer weight initialization based on a layer’s sparsity on static, RigL and SET. A common approach to initialization is to scale a weight’s initializa- tion inversely by the square root of the number of incoming connections. Consequently, when we drop incoming con- nections, the initialization distribution should be scaled pro- portionately to the number of incoming connections (Evci et al., 2022). Figure 4 (right) shows that this sparsity-aware initialization consistently improves performance when us- ing uniform distribution over layer sparsities. However the difference disappears when using ERK for RigL and SET and may even harm performance for static. Findings: Performance is not sensitive to sparsity-aware ini- tialization when using ERK and helps when using uniform layer sparsity. For RigL and SET we recommend always using sparsity-aware weight initialization (since it never ap- pears to harm performance) but for static this may depend on layer sparsity. 7. Signal-to-noise ratio in DRL environments Variance reduction is key to training deep models and of- ten achieved through using momentum based optimizers (Schmidt et al., 2011; Kingma & Ba, 2015a). However when new connections are grown such averages are not available, therefore noise in the gradients can provide mis- leading signals. In Figure 5 we share the signal-to-noise ratio (SNR) for the Classic control and MuJoCo environ- ments over the course of training. SNR is calculated as |µ| σ where µis the mean and σis the standard deviation of gradients over a mini-batch. A low SNR means the signalThe State of Sparse Training in Deep Reinforcement Learning 0 20000 40000 60000 80000 100000 Steps 10 3 10 2 10 1 Average SNR train_eval.env_name MountainCar-v0 CartPole-v0 Acrobot-v1 init_masks.sparsity 0.8 0.95 0.0 0.2 0.4 0.6 0.8 1.0 Steps 1e6 10 2 10 1 Average SNR Actor Environment Hopper-v2 Walker2d-v2 Humanoid-v2 HalfCheetah-v2 Sparsity 0.8 0.95 0.0 0.2 0.4 0.6 0.8 1.0 Steps 1e6 10 1 Average SNR Critic Environment Hopper-v2 Walker2d-v2 Humanoid-v2 HalfCheetah-v2 Sparsity 0.8 0.95 Figure 5.Signal-to-noise (SNR) ratio comparison of different gradient based DRL algorithms. We calculate SNR for every parameter in corresponding network (including the inactive/pruned weights) and report the mean SNR value. (left) DQN networks when training on classic control environments. SAC actor (center) and critic (right) networks during the training on MuJoCo environments. is dominated by the variance and thus the mean (the sig- nal) is uninformative. We calculate SNR for all parameters separately and report the mean. Mini-batch gradients can have average SNR values as low as 0.01 starting early in training. Higher sparsities seem to cause lower SNR values. Similarly, actor networks have lower SNR. Findings: We ﬁnd the average SNR for gradients to decrease with sparsity, potentially explaining the difﬁculty of using gradient based growing criteria in sparse training. 8. Are sparse networks robust to noise? Sparse neural networks can improve results on primary metrics such accuracy and rewards, yet they might have some unexpected behaviours in other aspects (Hooker et al., 2020). In Figure 6 we assess the effect of adding in- creasing amounts of noise to the observations and mea- suring their effect on a trained policy. Noise was sampled ∼N(0,σ),σ ∈[0,1,..., 30], quantized to an integer, and added to each observation’s pixel values (∈[0,255]) before normalization. Noise was sampled independently per pixel. We look at three data regimes; 100%, 50% and 10% of the standard dense model parameter count and compare dense and sparse training (RigL and SET). We made an effort to select policies with comparable performance for all the methods, chosen from the set of all policies trained during this work. We observe that (1) smaller models are generally more ro- bust to high noise than larger models, (2) sparse models are more robust to high noise than dense models on average, and (3) in most cases there are minimal differences when the noise is low. We can see that in the very low data regime (10% full pa- rameter count) policies trained using RigL are more robust to high noise compared with their dense counterparts, a fact observed across every environment. In the moderate data regime (50% full parameter count) the ordering is more mixed. In Qbert the dense model is most robust but the picture is reversed for Pong and McPacman. Finally, SET appears less robust to high noise than RigL, although we note this is not the case for Pong at 50% density. Although a preliminary analysis, it does suggest that sparse training can produce networks that are more robust to obser- vational noise, even when experienced post-training. 9. Related work Sparse training Though research on pruning has a rel- atively long story by deep learning standards (Mozer & Smolensky, 1989; Sietsma & Dow, 1988; Han et al., 2015; Molchanov et al., 2017; Louizos et al., 2017), training sparse networks from scratch has only recently gained popularity. Goyal et al. (2017), Frankle & Carbin (2019), and Evci et al. (2019) showed that training sparse networks from a ran- dom initialization is difﬁcult compared to dense neural net- works. Despite this, various approaches have been recently proposed to improve sparse training, most notably lottery tickets (Frankle et al., 2019; Zhou et al., 2019), dynamic sparse training (Dettmers & Zettlemoyer, 2019; Mostafa & Wang, 2019; Mocanu et al., 2018; Bellec et al., 2018; Evci et al., 2020; Liu et al., 2021) and one-shot pruning (Lee et al., 2019; Wang et al., 2020; Tanaka et al., 2020; Liu & Zenke, 2020). Solutions that focus on initialization alone have been shown to be ineffective for contemporary models (Evci et al., 2022; Frankle et al., 2020), possibly due to the catapult mechanism observed early in training (Lewkowycz et al., 2020). For an in-depth survey on the topic, please see Hoeﬂer et al. (2021). Sparse networks in RL Livne & Cohen (2020) used pruning as an intermediary step to guide the width of dense neural networks for DQN and A2C agents. Most of the work investigating the use of sparse sparse training in RL are in the context of the lottery ticket hypothesis; Morcos et al. (2019) studied the existence of lucky sparse initializa- tions using pruning and late-rewinding; Hasani et al. (2020) proposed an interesting approach by repurposing the sparseThe State of Sparse Training in Deep Reinforcement Learning 0 5 10 15 20 25 30 Observation noise StdDev 500 750 1000 1250 1500 1750 2000 2250Average return MsPacman 0 5 10 15 20 25 30 Observation noise StdDev 20 15 10 5 0 5 10 15 20 Average return Pong Density 100 50 10 Method Dense RigL SET 0 5 10 15 20 25 30 Observation noise StdDev 0 1000 2000 3000 4000 5000 6000Average return Qbert Figure 6.Robustness to observation noise. We test the robustness of networks trained using sparse and dense methods (denoted by line style) by adding Gaussian noise to the observations. We examine three parameter regimes, 100% (blue), 50% (pink) and 10% (purple) of the standard dense model parameter count. All policies were trained using DQN. circuitry of the C. elegans soil-worm for RL tasks; Vischer et al. (2021) observed that the success of such initializations dependens heavily on selecting correct features for the in- put data, and not on any general qualities of the different initializations. Lee et al. (2021) proposed the use of block- circulant masks during early steps of training to improve the efﬁciency of pruning on TD3 agents, while Arnob et al. (2021) applied one-shot pruning algorithms in an ofﬂine-RL setting. Perhaps the work closest to ours is the algorithm proposed by Sokar et al. (2021), where authors applied the SET algorithm for end-to-end training of sparse networks in two actor-critic algorithms (TD3 and SAC). By a carefully chosen topology update schedule and dynamic architecture design, the proposed algorithm was able to match the dense network with a sparsity of around 50%. Novel architectures in DRL A number of works have focused on evolving network architectures for RL policies. Nadizar et al. (2021) applied pruning together with evo- lution algorithms. Whiteson & Stone (2006) combined NEAT (Stanley & Miikkulainen, 2002) with Q-learning (Watkins, 1989) to evolve better learners, Gaier & Ha (2019) evolved strong architectural priors, resulting in networks that could solve tasks with a single randomly initialized shared weight, whilst Tang et al. (2020) evolve compact self-attention architectures as a form of indirect network encoding. Zambaldi et al. (2019) similarly explored self- attention enabling agents to perform relational reasoning and achieve state-of-the-art performance on the majority of StarCraft II mini-games. Another line of research seeks to improve the stability (Parisotto et al., 2020) and efﬁciency (Parisotto & Salakhutdinov, 2021) of transformers applied to DRL, whilst Shah & Kumar (2021) explore the utility of using features extracted from a pre-trained Resnet in the standard DRL pipeline. Consistent with our observations in this work, Ha & Schmidhuber (2018) showed it is possible to train very compact controllers (i.e. actors) albeit in a the context of model-based instead of the model-free RL setting considered here. 10. Discussion and Conclusion In this work we sought to understand the state of sparse training for DRL by applying pruning, static, SET and RigL to DQN, PPO, and SAC agents trained on a variety of envi- ronments. We found sparse training methods to be a drop-in alternative for their dense counterparts providing better re- sults for the same parameter count. From a practical stand- point we made recommendations regarding hyper-parameter settings and showed that non-uniform sparse initialization combined with tuning actor:critic parameter ratios improves performance. We hope this work establishes a useful foundation for fu- ture research into sparse DRL algorithms and highlights a number of interesting research questions. In contrast to the computer vision domain, we observe that RigL fails to match pruning results. Low SNR in high sparsity regimes offers a clue but more work is needed to understand this phenomena. Our results in section 8 also suggest that sparse networks may aid in generalization and robustness to obser- vational noise; this is an active area of interest and research in the DRL community, so a more thorough understanding could result in important algorithmic advances. Acknowledgements We thank Fabian Pedregosa, Rishabh Agarwal, and Adrien Ali Ta¨ıga for their helpful feedback on the manuscript, Oscar Ramirez for his help with on the TF-Agents codebase, and Trevor Gale and Sara Hooker for inspiring the title of this work. We also thank Brain Montreal RL team for their useful feedback on an early version of this work. Finally, we thank Bram Grooten for pointing out Degrave et al. (2022) and their contribution to the motivation for this work.The State of Sparse Training in Deep Reinforcement Learning References Agarwal, R., Schwarzer, M., Castro, P. S., Courville, A., and Bellemare, M. G. Deep reinforcement learning at the edge of the statistical precipice. Advances in Neural Information Processing Systems, 2021. Arnob, S. Y ., Ohib, R., Plis, S., and Precup, D. Single- shot pruning for ofﬂine reinforcement learning. ArXiv, abs/2112.15579, 2021. Bellec, G., Kappel, D., Maass, W., and Legenstein, R. A. Deep rewiring: Training very sparse deep networks. In International Conference on Learning Representations, 2018. Bellemare, M., Candido, S., Castro, P., Gong, J., Machado, M., Moitra, S., Ponda, S., and Wang, Z. Autonomous navigation of stratospheric balloons using reinforcement learning. Nature, 588:77–82, 12 2020. doi: 10.1038/ s41586-020-2939-8. Bellemare, M. G., Naddaf, Y ., Veness, J., and Bowling, M. The Arcade Learning Environment: An evaluation plat- form for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279, June 2013. Berner, C., Brockman, G., Chan, B., Cheung, V ., Debiak, P., Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse, C., J ´ozefowicz, R., Gray, S., Olsson, C., Pa- chocki, J., Petrov, M., de Oliveira Pinto, H. P., Raiman, J., Salimans, T., Schlatter, J., Schneider, J., Sidor, S., Sutskever, I., Tang, J., Wolski, F., and Zhang, S. Dota 2 with large scale deep reinforcement learning. CoRR, abs/1912.06680, 2019. URL http://arxiv.org/ abs/1912.06680. Blalock, D., Ortiz, J. J. G., Frankle, J., and Guttag, J. What is the state of neural network pruning? ArXiv, 2020. URL https://arxiv.org/abs/2003.03033. Castro, P. S., Moitra, S., Gelada, C., Kumar, S., and Belle- mare, M. G. Dopamine: A research framework for deep reinforcement learning. CoRR, abs/1812.06110, 2018. URL http://arxiv.org/abs/1812.06110. Degrave, J., Felici, F., Buchli, J., Neunert, M., Tracey, B., Carpanese, F., Ewalds, T., Hafner, R., Abdolmaleki, A., de las Casas, D., Donner, C., Fritz, L., Galperti, C., Hu- ber, A., Keeling, J., Tsimpoukelli, M., Kay, J., Merle, A., Moret, J.-M., Noury, S., Pesamosca, F., Pfau, D., Sauter, O., Sommariva, C., Coda, S., Duval, B., Fasoli, A., Kohli, P., Kavukcuoglu, K., Hassabis, D., and Riedmiller, M. Magnetic control of tokamak plasmas through deep rein- forcement learning. Nature, 2022. Dettmers, T. and Zettlemoyer, L. Sparse networks from scratch: Faster training without losing performance. ArXiv, 2019. URL http://arxiv.org/abs/1907. 04840. Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V ., Ward, T., Doron, Y ., Firoiu, V ., Harley, T., Dunning, I., Legg, S., and Kavukcuoglu, K. IMPALA: scalable dis- tributed deep-rl with importance weighted actor-learner architectures. CoRR, 2018. Evci, U., Pedregosa, F., Gomez, A. N., and Elsen, E. The difﬁculty of training sparse neural networks. ArXiv, 2019. URL http://arxiv.org/abs/1906.10732. Evci, U., Gale, T., Menick, J., Castro, P. S., and Elsen, E. Rigging the lottery: Making all tickets winners. In Pro- ceedings of Machine Learning and Systems 2020, 2020. Evci, U., Ioannou, Y . A., Keskin, C., and Dauphin, Y . Gradi- ent ﬂow in sparse neural networks and how lottery tickets win. In AAAI Conference on Artiﬁcial Intelligence, 2022. Frankle, J. and Carbin, M. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In 7th Interna- tional Conference on Learning Representations (ICLR), 2019. Frankle, J., Dziugaite, G. K., Roy, D. M., and Carbin, M. Stabilizing the lottery ticket hypothesis. ArXiv, 2019. URL https://arxiv.org/abs/1903.01611. Frankle, J., Dziugaite, G. K., Roy, D. M., and Carbin, M. Pruning neural networks at initialization: Why are we missing the mark? ArXiv, 2020. URL https: //arxiv.org/abs/2009.08576. Gaier, A. and Ha, D. Weight agnostic neural networks. 2019. URL https://weightagnostic.github. io. https://weightagnostic.github.io. Gale, T., Elsen, E., and Hooker, S. The state of sparsity in deep neural networks. ArXiv, 2019. URL http: //arxiv.org/abs/1902.09574. Goyal, P., Doll ´ar, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y ., and He, K. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. Guadarrama, S., Korattikara, A., Ramirez, O., Castro, P., Holly, E., Fishman, S., Wang, K., Gonina, E., Wu, N., Kokiopoulou, E., Sbaiz, L., Smith, J., Bart ´ok, G., Berent, J., Harris, C., Vanhoucke, V ., and Brevdo, E. TF-Agents: A library for reinforcement learning in tensorﬂow. https://github.com/tensorflow/ agents, 2018. URL https://github.com/ tensorflow/agents. [Online; accessed 25-June- 2019].The State of Sparse Training in Deep Reinforcement Learning Ha, D. and Schmidhuber, J. Recurrent world models facilitate policy evolution. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Pro- cessing Systems , volume 31. Curran Associates, Inc., 2018. URL https://proceedings. neurips.cc/paper/2018/file/ 2de5d16682c3c35007e4e92982f1a2ba-Paper. pdf. Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor-critic: Off-policy maximum entropy deep reinforce- ment learning with a stochastic actor. In International Conference on Machine Learning (ICML), 2018. Han, S., Pool, J., Tran, J., and Dally, W. Learning both weights and connections for efﬁcient neural network. In Advances in neural information processing systems, 2015. Han, S., Liu, X., Mao, H., Pu, J., Pedram, A., Horowitz, M. A., and Dally, W. J. EIE: Efﬁcient Inference Engine on compressed deep neural network. In Proceedings of the 43rd International Symposium on Computer Architecture, 2016. Hasani, R., Lechner, M., Amini, A., Rus, D., and Grosu, R. A natural lottery ticket winner: Reinforcement learning with ordinary neural circuits. In Proceedings of the 37th International Conference on Machine Learning. PMLR, 2020. URL https://proceedings.mlr.press/ v119/hasani20a.html. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. CoRR, 2015. Hoeﬂer, T., Alistarh, D., Ben-Nun, T., Dryden, N., and Peste, A. Sparsity in deep learning: Pruning and growth for efﬁcient inference and training in neural networks. ArXiv, abs/2102.00554, 2021. Hooker, S., Courville, A. C., Clark, G., Dauphin, Y ., and Frome, A. What do compressed deep neural networks forget. arXiv: Learning, 2020. Kalchbrenner, N., Elsen, E., Simonyan, K., Noury, S., Casagrande, N., Lockhart, E., Stimberg, F., Oord, A., Dieleman, S., and Kavukcuoglu, K. Efﬁcient neural au- dio synthesis. In International Conference on Machine Learning (ICML), 2018. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2015a. Kingma, D. P. and Ba, J. Adam: A method for stochas- tic optimization. In Bengio, Y . and LeCun, Y . (eds.), 3rd International Conference on Learning Representa- tions, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015b. URL http: //arxiv.org/abs/1412.6980. Kumar, A., Agarwal, R., Ghosh, D., and Levine, S. Im- plicit under-parameterization inhibits data-efﬁcient deep reinforcement learning. In International Conference on Learning Representations, 2021. URL https:// openreview.net/forum?id=O9bnihsFfXU. Kusupati, A., Ramanujan, V ., Somani, R., Wortsman, M., Jain, P., Kakade, S., and Farhadi, A. Soft threshold weight reparameterization for learnable sparsity. In Proceedings of the International Conference on Machine Learning , 2020. Langley, P. Crafting papers on machine learning. In Langley, P. (ed.),Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 1207–1216, Stan- ford, CA, 2000. Morgan Kaufmann. Lee, J., Kim, S., Kim, S., Jo, W., and Yoo, H.-J. Gst: Group- sparse training for accelerating deep reinforcement learn- ing. ArXiv, abs/2101.09650, 2021. Lee, N., Ajanthan, T., and Torr, P. H. S. SNIP: Single-shot Network Pruning based on Connection Sensitivity. In International Conference on Learning Representations (ICLR), 2019, 2019. Lewkowycz, A., Bahri, Y ., Dyer, E., Sohl-Dickstein, J., and Gur-Ari, G. The large learning rate phase of deep learning: the catapult mechanism. Arxiv, 2020. URL https://arxiv.org/pdf/2003.02218. Li, Z., Wallace, E., Shen, S., Lin, K., Keutzer, K., Klein, D., and Gonzalez, J. Train large, then compress: Re- thinking model size for efﬁcient training and inference of transformers. ArXiv, abs/2002.11794, 2020. Liu, S., Yin, L., Mocanu, D. C., and Pechenizkiy, M. Do we actually need dense over-parameterization? in-time over-parameterization in sparse training. In ICML, 2021. Liu, T. and Zenke, F. Finding trainable sparse networks through neural tangent transfer. In ICML, 2020. Liu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T. Re- thinking the value of network pruning. In International Conference on Learning Representations, 2019. Livne, D. and Cohen, K. Pops: Policy pruning and shrink- ing for deep reinforcement learning. IEEE Journal of Selected Topics in Signal Processing, 14:789–801, 2020. Louizos, C., Ullrich, K., and Welling, M. Bayesian compres- sion for deep learning. InAdvances in Neural Information Processing Systems, 2017. Mnih, V ., Kavukcuoglu, K., Silver, D., Rusu, A. A., Ve- ness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C.,The State of Sparse Training in Deep Reinforcement Learning Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wier- stra, D., Legg, S., and Hassabis, D. Human-level control through deep reinforcement learning. Nature, 518(7540): 529–533, 2015. Mocanu, D. C., Mocanu, E., Stone, P., Nguyen, P. H., Gibescu, M., and Liotta, A. Scalable training of arti- ﬁcial neural networks with adaptive sparse connectivity inspired by network science. Nature Communications, 2018. Molchanov, D., Ashukha, A., and Vetrov, D. P. Variational Dropout Sparsiﬁes Deep Neural Networks. In Proceed- ings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 Au- gust 2017, 2017. Morcos, A., Yu, H., Paganini, M., and Tian, Y . One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers. In Advances in Neural Information Processing Systems, 2019. Mostafa, H. and Wang, X. Parameter efﬁcient train- ing of deep convolutional neural networks by dynamic sparse reparameterization. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA , 2019. URL http://proceedings.mlr.press/ v97/mostafa19a.html. Mozer, M. C. and Smolensky, P. Skeletonization: A tech- nique for trimming the fat from a network via relevance assessment. In Advances in Neural Information Process- ing Systems 1, 1989. Nadizar, G., Medvet, E., Pellegrino, F. A., Zullich, M., and Nichele, S. On the effects of pruning on evolved neural controllers for soft robots. In Proceedings of the Genetic and Evolutionary Computation Conference Companion, 2021. Nikishin, E., Schwarzer, M., D’Oro, P., Bacon, P.-L., and Courville, A. The primacy bias in deep reinforcement learning. In Proceedings of the Thirty-ninth International Conference on Machine Learning (ICML’22), 2022. Parisotto, E. and Salakhutdinov, R. Efﬁcient transformers in reinforcement learning using actor-learner distillation. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum? id=uR9LaO_QxF. Parisotto, E., Song, F., Rae, J., Pascanu, R., Gulcehre, C., Jayakumar, S., Jaderberg, M., Kaufman, R. L., Clark, A., Noury, S., Botvinick, M., Heess, N., and Hadsell, R. Stabilizing transformers for reinforcement learning. In III, H. D. and Singh, A. (eds.), Pro- ceedings of the 37th International Conference on Ma- chine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 7487–7498. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/ v119/parisotto20a.html. Peste, A., Ioﬁnova, E., Vladu, A., and Alistarh, D. Ac/dc: Alternating compressed/decompressed training of deep neural networks. ArXiv, abs/2106.12379, 2021. Puterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc., New York, NY , USA, 1st edition, 1994. ISBN 0471619779. Schmidt, M., Roux, N. L., and Bach, F. Convergence rates of inexact proximal-gradient methods for convex optimiza- tion. In Proceedings of the 24th International Conference on Neural Information Processing Systems, 2011. Schulman, J., Moritz, P., Levine, S., Jordan, M. I., and Abbeel, P. High-dimensional continuous control using generalized advantage estimation. In Bengio, Y . and Le- Cun, Y . (eds.),4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1506.02438. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. Shah, R. M. and Kumar, V . Rrl: Resnet as representation for reinforcement learning. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning , volume 139 of Proceedings of Machine Learning Research, pp. 9465–9476. PMLR, 18– 24 Jul 2021. URL https://proceedings.mlr. press/v139/shah21a.html. Sietsma, J. and Dow, R. J. Neural net pruning-why and how. In IEEE International Conference on Neural Networks, 1988. doi: 10.1109/ICNN.1988.23864. Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V ., Lanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T., Leach, M., Kavukcuoglu, K., Graepel, T., and Hassabis, D. Mastering the game of go with deep neural networks and tree search. Nature, 529:484–503, 2016. URL http://www.nature.com/nature/journal/ v529/n7587/full/nature16961.html. Sokar, G., Mocanu, E., Mocanu, D. C., Pechenizkiy, M., and Stone, P. Dynamic sparse training for deep reinforcement learning. ArXiv, abs/2106.04217, 2021.The State of Sparse Training in Deep Reinforcement Learning Stanley, K. O. and Miikkulainen, R. Evolving neural networks through augmenting topologies. Evol. Com- put., 10(2):99–127, jun 2002. ISSN 1063-6560. doi: 10.1162/106365602320169811. URL https://doi. org/10.1162/106365602320169811. Tanaka, H., Kunin, D., Yamins, D. L. K., and Ganguli, S. Pruning neural networks without any data by iteratively conserving synaptic ﬂow. ArXiv, 2020. URL https: //arxiv.org/abs/2006.05467. Tang, Y ., Nguyen, D., and Ha, D. Neuroevolution of self- interpretable agents. In Proceedings of the 2020 Genetic and Evolutionary Computation Conference, GECCO ’20, pp. 414–424, New York, NY , USA, 2020. Association for Computing Machinery. ISBN 9781450371285. doi: 10. 1145/3377930.3389847. URL https://doi.org/ 10.1145/3377930.3389847. Todorov, E., Erez, T., and Tassa, Y . Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ Inter- national Conference on Intelligent Robots and Systems, pp. 5026–5033, 2012. doi: 10.1109/IROS.2012.6386109. Vischer, M. A., Lange, R., and Sprekeler, H. On lottery tickets and minimal task representations in deep rein- forcement learning. ArXiv, abs/2105.01648, 2021. Wang, C., Zhang, G., and Grosse, R. Picking winning tickets before training by preserving gradient ﬂow. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum? id=SkgsACVKPH. Watkins, C. J. C. H. Learning from Delayed Re- wards. PhD thesis, King’s College, Cambridge, UK, May 1989. URL http://www.cs.rhul.ac.uk/ ˜chrisw/new_thesis.pdf. Whiteson, S. and Stone, P. Evolutionary function approxi- mation for reinforcement learning. Journal of Machine Learning Research, 7(31):877–917, 2006. URL http: //jmlr.org/papers/v7/whiteson06a.html. Wortsman, M., Farhadi, A., and Rastegari, M. Discover- ing neural wirings. In Advances in Neural Information Processing Systems, 2019. Zambaldi, V ., Raposo, D., Santoro, A., Bapst, V ., Li, Y ., Babuschkin, I., Tuyls, K., Reichert, D., Lillicrap, T., Lockhart, E., Shanahan, M., Langston, V ., Pascanu, R., Botvinick, M., Vinyals, O., and Battaglia, P. Deep rein- forcement learning with relational inductive biases. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum? id=HkxaFoC9KQ. Zhou, H., Lan, J., Liu, R., and Yosinski, J. Deconstructing lottery tickets: Zeros, signs, and the supermask. In Ad- vances in Neural Information Processing Systems, 2019. Zhu, M. and Gupta, S. To Prune, or Not to Prune: Explor- ing the Efﬁcacy of Pruning for Model Compression. In International Conference on Learning Representations Workshop, 2018.The State of Sparse Training in Deep Reinforcement Learning A. Experimental Details A.1. Training schedule During training all agents are allowed M environment transitions, with policies being evaluated for Kepisodes / steps every N environment frames, where the values vary per suite and shared below. Atari experiments use a frame skip of 4, following (Mnih et al., 2015) thus 1 environment step = 4 environment frames. Environment M N K K = episodes Classic control 100,000 2 ,000 20 MujoCo (SAC) 1,000,000 10 ,000 30 MujoCo (PPO) 1,000,000 2 ,000 20 K = environment steps Atari Suite 40,000,000 1 ,000,000 125 ,000 A.2. FLOPs behaviour of Sparse ERK networks in DRL As reported in Evci et al. (2020), using ERK sparsity distribution often doubles the FLOPs needed for sparse models compared using the uniform sparsity distribution. This is due to the parameter sharing in convolutional layers. The spatial dimensions, kernel size and the stride of a convolutional layer affects how many times each weight is used during the convolution which in turn determines the contribution of each weight towards the total FLOPs count. In modern CNNs, the spatial dimensions of the feature maps often decreases monotonically towards the output of the network, making the contribution of the connections in later layers to the total FLOPs count smaller. Furthermore, the size of the layers typically increases towards the output and thus ERK removes a larger proportion of the connections from these later layers compared to uniform. Consequently a network sparsiﬁed using the ERK distribution will have a larger FLOPs count compared to one sparsiﬁed using a uniform distribution. Due to the lack of parameter sharing fully connected layers used in MuJoCo and classic control experiments, sparse networks with ERK have same amount of FLOPs as the uniform. Networks used in Atari experiments, however, uses convolutional networks and thus ERK doubles the FLOPs required compared to uniform. FLOPs scaling of sparse networks with ERK distributions used for Pong game can be found in Figure 7. Atari games have differently sized action spaces (Pong has 6 actions for example), which affects the number of neurons in the last layer. However since the last layer is very small and fully connected, it should have a very little effect on the results provided here. 0.5 0.6 0.7 0.8 0.9 1.0 Sparsity 106 107 FLOPs DQN-Atari CNN ERK 2x Uniform Uniform 0.5 0.6 0.7 0.8 0.9 1.0 Sparsity 107 108 FLOPs DQN-Atari Resnet ERK 2x Uniform Uniform Figure 7.FLOPs scaling of different sparsity distributions on (left) Nature CNN and (right) Impala ResNet architectures used in MsPacman environment.The State of Sparse Training in Deep Reinforcement Learning A.3. Hyper-parameter sweep We perform a grid search over different hyper parameters used in Dense, Prune, Static, SET and RigL algorithms. Unless otherwise noted, we use hyper-parameters used in regular dense training. When pruning, we start pruning around 20% of training steps and stop when 80% of training is completed following the ﬁndings of Gale et al. (2019). We use same default hyper-parameters for SET and RigL. Fort both algorithms we start updating the mask at initialization and decay the drop fraction over the course of the training using a cosine schedule, similar to pruning stopping the updates when 80% of training is completed. We search over the following parameters: 1. Weight decay ( ): Searched over the grid [0, 1e-6, 1e-4, 1e-3]. 2. Update Interval ( ): refers to how often models are pruned or sparse topology is updated. Searched over the grid [100, 250, 500, 1000, 5000]. 3. Drop Fraction ( ): refers to the maximum percentage of parameters that are dropped and added when network topology is updated. This maximum value is decayed during training according to a cosine decay schedule. Searched over the grid [0.0,0.1,0.2,0.3,0.5]. 4. Sparsity-aware initialization ( ): refers to whether sparse models are initialized with scaled initialization or not. We repeat the hyper-parameter search for each DRL algorithm using the Acrobot (for DQN) and Walker2D (for PPO and SAC) environments. Best hyper-parameters found in these environments are then used when training in other similar environments (i.e. classic control for DQN and MuJoCO for PPO and SAC). See Table 1, Table 2, and Table 3 for the best hyper parameters found in each setting. Table 1.DQN best hyper-parameters for Classic Control from sweep Algorithm Weight Decay Update Interval Drop Fraction Sparsity-aware init Dense 1 ·10−6 - - - Pruning 1 ·10−6 1,000 - True Static 1 ·10−6 - - True RigL 1 ·10−6 1,000 0.5 True SET 1 ·10−6 1,000 0.5 True Table 2.SAC best hyper-parameters from sweep Algorithm Weight Decay Update Interval Drop Fraction Sparsity-aware init Dense 1 ·10−4 - - - Pruning 1 ·10−4 1,000 - True Static 1 ·10−4 - - False RigL 1 ·10−6 1,000 0.5 True SET 1 ·10−4 250 0.3 TrueThe State of Sparse Training in Deep Reinforcement Learning Table 3.PPO best hyper-parameters from sweep Algorithm Weight Decay Update Interval Drop Fraction Sparsity-aware init Dense 1 ·10−6 - - - Pruning 1 ·10−6 500 - False Static 0 - - True RigL 1 ·10−4 250 0.3 True SET 1 ·10−6 100 0 True Atari hyper-parameters Due to computational constraints we did not search over hyper-parameters for the Atari environ- ments, except for a small grid-search to tune the dense ResNet. The CNN architecture from Mnih et al. (2015) has been used in many prior works thus was already well tuned. The ResNet hyper-parameter sweep for the original dense model is detailed below: 1. Weight decay:Searched over the grid [0, 1e-6, 1e-5, 1e-4]. 2. Learning rate:Searched over the grid [1e-4, 2.5e-4, 1e-3, 2.5e-3]. The ﬁnal hyper-parameters we used for the Atari environments are shown in in Table 4 for the CNN and Table 5 for the ResNet. Table 4.DQN (CNN) Atari hyper-parameters Algorithm Weight Decay Learning Rate Update Interval Drop Fraction Sparsity-aware init Dense 0 2.5 ·10−4 - - - Pruning 0 2.5 ·10−4 5,000 - False Static 0 2.5 ·10−4 - - True RigL 0 2.5 ·10−4 5,000 0.3 True SET 0 2.5 ·10−4 5,000 0.3 True Table 5.DQN (ResNet) Atari hyper-parameters Algorithm Weight Decay Learning Rate Update Interval Drop Fraction Sparsity-aware init Dense 1 ·10−5 1 ·10−4 - - - Pruning 1 ·10−5 1 ·10−4 5,000 - False Static 1 ·10−5 1 ·10−4 - - True RigL 1 ·10−5 1 ·10−4 5,000 0.3 True SET 1 ·10−5 1 ·10−4 5,000 0.3 True Remaining hyper-parameters Next, we include details of the DRL hyper-parameters used in all training settings for DQN (Table 6 and Table 7), SAC (Table 8), and PPO (Table 8).The State of Sparse Training in Deep Reinforcement Learning Table 6.DQN Hyperparameters/ Table format from Haarnoja et al. (2018). Parameter Value Shared optimizer Adam (Kingma & Ba, 2015b) discount (γ) 0.99 nonlinearity ReLU target smoothing coefﬁcient (τ) 1.0 gradient steps per training step 1 exploration policy epsilon greedy epsilon decay period (env steps) 2.5 ·104 Classic Control replay buffer size 105 learning rate 1 ·10−3 initial collect steps 1,000 target update interval 100 reward scale factor 1.0 gradient steps every k env steps, k = 1 ﬁnal epsilon 0.1 eval epsilon 0.1 number of samples per minibatch 128 network type MLP number of hidden dense layers 2 number of hidden units per layer 512 Atari replay buffer size 106 initial collect steps 20,000 target update interval 8000 reward scale factor 1.0 gradient steps every k env steps, k = 4 ﬁnal epsilon 0.01 eval epsilon 0.001 number of samples per minibatch 32 network type CNN or ResNetThe State of Sparse Training in Deep Reinforcement Learning Table 7.DQN Atari: CNN and ResNet Architectures Parameter Value CNN learning rate 2.5 ·10−4 Adam optimizer, epsilon 1 ·10−8 CNN Architecture number of hidden CNN layers 3 number of hidden dense layers 1 number of hidden units per dense layer 512 CNN params per layer (ﬁlters, kernel, stride) layer 1 (ﬁlters, kernel, stride) 32, 8, 4 layer 2 (ﬁlters, kernel, stride) 64, 4, 2 layer 3 (ﬁlters, kernel, stride) 64, 3, 1 ResNet learning rate 1 ·10−4 Adam optimizer, epsilon 3.125 ·10−4 ResNet Architecture number of stacks 3 number of hidden dense layers 1 number of hidden units per dense layer 512 use batch norm False ResNet stack layers num CNN layers 1 num max pooling layers 1 num residual-CNN layers 2 ResNet params per layer (ﬁlters, kernel, stride) stack 1 (ﬁlters, kernel, stride) 32, 3, 1 stack 2 (ﬁlters, kernel, stride) 64, 3, 1 stack 3 (ﬁlters, kernel, stride) 64, 3, 1The State of Sparse Training in Deep Reinforcement Learning Table 8.SAC Hyperparameters. Parameter Value optimizer Adam (Kingma & Ba, 2015b) learning rate 3 ·10−4 discount (γ) 0.99 replay buffer size 106 number of hidden layers (all networks) 2 number of hidden units per layer 256 number of samples per minibatch 256 nonlinearity ReLU target smoothing coefﬁcient (τ) 0.005 target update interval 1 train every k env steps, k = 1 gradient steps per training step = 1 Hopper, Walker, Humanoid initial collect steps 1,000 HalfCheetah, Ant initial collect steps 10,000 Table 9.PPO Hyperparameters. Parameter Value optimizer Adam (Kingma & Ba, 2015b) learning rate 3 ·10−4 discount (γ) 0.99 shared / separate networks separate number of hidden layers (all networks) 2 number of hidden units per layer 64 collect sequence length (batch size) 2048 minibatch size 64 num epochs 10 importance ratio clipping 0.2 use GAE (Schulman et al., 2016) True λ(GAE) 0.95 entropy regularization 0 value loss coeff 0.5 gradient clipping 0.5 A.4. Atari Game Selection Our original three games (MsPacman, Pong, Qbert) were selected to have varying levels of difﬁculty as measured by DQN’s human normalized score in Mnih et al. (2015), Figure 3. To this we added 12 games (Assault, Asterix, BeamRider, Boxing, Breakout, CrazyClimber, DemonAttack, Enduro, FishingDerby, SpaceInvaders, Tutankham, VideoPinball) selected to be roughly evenly distributed amongst the games ranked by DQN’s human normalized score in Mnih et al. (2015) with a lower cut off of approximately 100% of human performance.The State of Sparse Training in Deep Reinforcement Learning B. Sparse Scaling Plots in Other Environments Here we share results on additional environments, Acrobot, CartPole, MountainCar, Hopper, and Ant. Figure 8 compares ﬁnal reward relative to parameter count using DQN. ERK sparsity distribution was used in the top row whilst uniform was used in the bottom row. Figure 9 presents results on the two remaining MuJoCo environment, Hopper and Ant with SAC (top row) and PPO (bottom row). In Figures 10 and 11 we show sparsity scaling plots for 15 Atari games using the standard CNN and ResNet respectively. 104 105 #Params 200 180 160 140 120 100 80 60 Reward 0.500.700.800.900.950.960.97 0.98 DQN / Acrobot-v1 Dense Pruning Rigl Set Static 104 105 #Params 100 120 140 160 180 200 0.500.700.80 0.900.950.960.970.98 DQN / CartPole-v0 104 105 #Params 200 180 160 140 120 100 0.500.70 0.80 0.900.95 0.96 0.97 0.98 DQN / MountainCar-v0 104 105 #Params 200 180 160 140 120 100 80 60 Reward 0.500.600.700.80 0.90 0.950.96 0.970.98 DQN / Acrobot-v1 Dense Pruning Rigl Set Static 104 105 #Params 100 120 140 160 180 200 0.500.600.700.800.90 0.95 0.96 0.97 0.98 DQN / CartPole-v0 104 105 #Params 200 180 160 140 120 100 0.500.600.700.800.900.950.960.970.98 DQN / MountainCar-v0 Figure 8.DQN in the Classic Control environments with ERK network sparsity distribution (top) and uniform network sparsity (bottom). 104 105 #Params 500 1000 1500 2000 2500 3000 3500Reward 0.50 0.70 0.80 0.900.950.960.97 0.98 SAC / Hopper-v2 Dense Pruning Rigl Set Static 104 105 #Params 0 1000 2000 3000 4000 5000Reward 0.500.70 0.80 0.90 0.950.96 0.97 0.98 SAC / Ant-v2 Dense Pruning Rigl Set Static 103 #Params 500 1000 1500 2000 2500 3000Reward 0.50 0.70 0.80 0.90 0.95 0.96 0.970.98 0.99 PPO / Hopper-v2 Dense Pruning Rigl Set Static 103 104 #Params 1000 1500 2000 2500 3000 3500 4000Reward 0.500.70 0.800.90 0.95 0.96 0.970.98 0.99 PPO / Ant-v2 Dense Pruning Rigl Set Static Figure 9.Additional MuJoCO environments (Hopper and Ant) for SAC and PPO algorithms. Networks are initialized with ERK network sparsity distribution.The State of Sparse Training in Deep Reinforcement Learning 105 106 #Params 400 600 800 1000 1200Reward 0.500.80 0.900.95 0.98 0.99 Assault / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 500 1000 1500 2000Reward 0.500.80 0.90 0.95 0.98 0.99 Asterix / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 500 750 1000 1250 1500 1750 2000 2250 2500Reward 0.50 0.80 0.90 0.95 0.98 0.99 BeamRider / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 0 20 40 60 80Reward 0.50 0.80 0.900.95 0.98 0.99 Boxing / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 0 10 20 30 40 50 60 70Reward 0.50 0.80 0.90 0.95 0.98 0.99 Breakout / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 0 20000 40000 60000 80000 100000Reward 0.50 0.800.90 0.95 0.98 0.99 CrazyClimber / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 200 400 600 800 1000 1200 1400Reward 0.50 0.80 0.90 0.95 0.98 0.99 DemonAttack / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 0 100 200 300 400 500 600Reward 0.50 0.800.90 0.95 0.98 0.99 Enduro / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 100 80 60 40 20 Reward 0.50 0.80 0.90 0.950.98 0.99 FishingDerby / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 500 1000 1500 2000 0.500.800.900.95 0.98 0.99 MsPacman / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 20 15 10 5 0 5 10 15 Reward 0.50 0.800.900.950.98 0.99 Pong / DQN (CNN) 105 106 #Params 0 1000 2000 3000 4000 5000 0.50 0.80 0.90 0.95 0.98 0.99 Qbert / DQN (CNN) 105 106 #Params 300 400 500 600 700 800Reward 0.500.80 0.90 0.95 0.98 0.99 SpaceInvaders / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 20 40 60 80 100 120Reward 0.500.800.900.95 0.98 0.99 Tutankham / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 0 50000 100000 150000 200000 250000Reward 0.500.80 0.90 0.95 0.98 0.99 VideoPinball / DQN (CNN) Dense Pruning RigL SET Static Figure 10.CNN: Sparsity plots per game.The State of Sparse Training in Deep Reinforcement Learning 105 106 #Params 500 1000 1500 2000 2500Reward 0.50 0.80 0.90 0.95 0.98 0.99 Assault / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 20000 40000 60000 80000 100000 120000Reward 0.50 0.80 0.900.95 0.98 0.99 Asterix / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 2000 4000 6000 8000Reward 0.50 0.800.90 0.95 0.98 0.99 BeamRider / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 40 20 0 20 40 60 80 100Reward 0.500.800.900.950.98 0.99 Boxing / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 50 100 150 200 250 300Reward 0.50 0.80 0.90 0.95 0.98 0.99 Breakout / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 20000 40000 60000 80000 100000 120000Reward 0.500.800.900.950.98 0.99 CrazyClimber / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 10000 20000 30000 40000 50000 60000 70000Reward 0.50 0.80 0.90 0.95 0.98 0.99 DemonAttack / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 200 400 600 800 1000 1200 1400Reward 0.500.80 0.900.950.980.99 Enduro / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 100 80 60 40 20 0 20 Reward 0.500.800.900.950.98 0.99 FishingDerby / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 500 1000 1500 2000 2500 3000 3500 4000Reward 0.50 0.80 0.90 0.95 0.98 0.99 MsPacman / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 20 10 0 10 20 Reward 0.500.800.900.950.980.99 Pong / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 2000 4000 6000 8000Reward 0.500.80 0.900.95 0.98 0.99 Qbert / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 2000 4000 6000 8000 10000Reward 0.50 0.80 0.90 0.95 0.98 0.99 SpaceInvaders / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 25 50 75 100 125 150 175Reward 0.500.80 0.90 0.950.98 0.99 Tutankham / DQN (ResNet) Dense Pruning RigL SET Static 105 106 #Params 0 50000 100000 150000 200000 250000Reward 0.500.80 0.90 0.95 0.98 0.99 VideoPinball / DQN (ResNet) Dense Pruning RigL SET Static Figure 11.ResNet: Sparsity plots per game.The State of Sparse Training in Deep Reinforcement Learning C. Additional Results with Uniform Sparsity Distribution In Figure 12 we repeat the experiments presented in Figure 2, however this time using a uniform network sparsity distribution at initialization. These plots provide further evidence as the the beneﬁt of using ERK over uniform to distribute network sparsity. 105 106 #Params 20 15 10 5 0 5 10 15 Reward 0.500.800.90 0.95 0.98 0.99 Pong / DQN (CNN) Dense Pruning RigL SET Static 105 106 #Params 0 1000 2000 3000 4000 5000 6000 0.50 0.80 0.90 0.950.980.99 Qbert / DQN (CNN) 105 106 #Params 500 1000 1500 2000 2500 0.500.80 0.90 0.95 0.98 0.99 MsPacman / DQN (CNN) 104 105 #Params 0 1000 2000 3000 4000 5000Reward 0.500.600.700.80 0.90 0.950.960.970.98 SAC / Walker2d-v2 Dense Pruning Rigl Set Static 104 105 #Params 0 2000 4000 6000 8000 10000 12000Reward SAC / HalfCheetah-v2 Dense Pruning Rigl Set Static 104 105 #Params 1000 2000 3000 4000 5000 0.50 0.60 0.70 0.800.900.95 0.96 0.97 0.98 SAC / Humanoid-v2 Dense Pruning Rigl Set Static 103 #Params 500 1000 1500 2000 2500 3000 3500Reward 0.50 0.70 0.80 0.90 0.950.96 0.97 0.980.99 PPO / Walker2d-v2 103 #Params 0 1000 2000 3000 4000 5000 0.50 0.700.80 0.90 0.95 0.96 0.97 0.98 0.99 PPO / HalfCheetah-v2 103 104 #Params 250 500 750 1000 1250 1500 1750 2000 2250 0.500.70 0.80 0.90 0.95 0.960.97 0.980.99 PPO / Humanoid-v2 Dense Pruning Rigl Set Static Figure 12.Uniform network sparsity initialization. Comparison of ﬁnal reward relative to parameter count for the various methods considered: (row-1) DQN on Atari (row-2) SAC on MuJoCo (row-3) PPO on MuJoCo. We consider sparsities from 50% to 95% for sparse training methods and pruning. Parameter count for networks with 80% sparsity and the reward obtained by the dense baseline are highlighted with vertical and horizontal lines. Shaded areas represent 95% conﬁdence intervals.The State of Sparse Training in Deep Reinforcement Learning D. Additional Hyper-parameter Sensitivity Plots Here we share the remaining plots for our analysis on the sensitivity of sparse training algorithms to various hyper-parameters. Policies area trained using SAC. We note that different architectures, environments and training algorithms might show different curves, which we omit due to high cost of running such analysis. 0.0 0.1 0.2 0.3 0.4 0.5 Drop Fraction 0 1000 2000 3000 4000 5000 6000Reward Walker2d Rigl Set 102 103 Update Interval 0 1000 2000 3000 4000 5000 6000Reward Walker2d Pruning Rigl Set 0 200 400 600 800 1000 Batch size 0 1000 2000 3000 4000 5000 6000Reward Walker2d Set Dense RigL Static Figure 13.Sensitivity analysis: (left) Drop fraction: Comparing different drop fractions for SET and RigL at 80% sparsity. (center) Comparing the effect of topology update interval on pruning, static, and RigL. (right) The effect of batch size. Drop fraction In Figure 13 (left) we evaluate the effect of drop fraction and observe that drop fractions >0 yield a small performance improvement over drop fraction = 0, which is equivalent to static sparse training. This indicates that changing the network topology during training helps performance. Surprisingly training does not appear particularly sensitive to the drop fraction chosen, with values from 10 - 50% yielding approximately equivalent performance. It is possible that this is because the environment is relatively easy, thus leading to little separation between different settings. The large improvement that RigL and SET give over static (drop fraction = 0) in the Humanoid and Atari environments is one reason to suspect this. Findings: Drop fractions of 10 - 50% worked well for RigL, whilst a drop fraction of 30% worked best for SET. 30% therefore appears to be a reasonable default for dynamic sparse training. However, the uniformity of performance merits investigation on a harder environment in which more separation between drop fractions may be observed. Topology update interval In Figure 13 (center) we evaluate the effect of topology update interval on pruning, RigL and SET and ﬁnd that training is not particularly sensitive to it. Findings: Updating the network every 1000 environment steps appears to be a reasonable default. Batch size In Figure 13 (right) we evaluate the effect of batch size. Batch size is critical for obtaining a good estimate for the gradients during training for all methods, however for RigL it is also used when selecting new connections. We observe performance degradation for all methods when smaller batch sizes are used, with no particular additional effect on RigL. Findings: Sparse networks seems to have similar sensitivity to the batch size as the dense networks. 0.5 0.6 0.7 0.8 0.9 1.0 Sparsity 0 2000 4000 6000 8000 10000 12000Reward Half-Cheetah Algorithm rigl static Distribution erk uniform 0.5 0.6 0.7 0.8 0.9 1.0 Sparsity 0 2000 4000 6000 8000 10000Reward Half-Cheetah Algorithm rigl static Distribution erk uniform 0.5 0.6 0.7 0.8 0.9 1.0 Sparsity 400 350 300 250 200 150 100 Reward Acrobot Algorithm rigl static Distribution erk uniform Figure 14.Evaluating non uniform sparsities on HalfCheetah using SAC when all layers pruned (left) and last layer is kept dense (center). On the right we prune all layers of the DQN MLP network on the Acrobot task.The State of Sparse Training in Deep Reinforcement Learning Within network sparsity In Figure 14 we share additional analysis on within network sparsity for SAC and also on DQN in the Acrobot environment. E. Additional Interquartile Mean (IQM) Plots Mujoco SAC In Figure 15 we present the interquartile mean (IQM) calculated over ﬁve Mujoco environments for SAC at four different sparsities, 50%, 90%, 95% and 99%, or the networks with the equivalent number of parameters in the case of Dense training. Note that for 99% sparsity the IQM is only calculated over three Mujoco environments. 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static SAC / s=0.5 Normalized Score 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static SAC / s=0.9 Normalized Score 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static SAC / s=0.95 Normalized Score 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static SAC / s=0.99 Normalized Score Figure 15.SAC: IQM plots calculated over ﬁve Mujoco games (Ant, HalfCheetah, Hopper, Humanoid, Walker2d), with 10 seeds per game. Except for sparsity = 0.99 which only includes results from HalfCheetah, Hopper, and Walker2d, each with 10 seeds.The State of Sparse Training in Deep Reinforcement Learning Mujoco PPO In Figure 16 we present the interquartile mean (IQM) calculated over ﬁve Mujoco environments for PPO at four different sparsities, 50%, 90%, 95% and 99%, or the networks with the equivalent number of parameters in the case of Dense training. Note that for 95% sparsity the IQM is calculated over four environments and for 99% sparsity the IQM is only calculated over three Mujoco environments. 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static PPO, s = 0.5 Normalized Score 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static PPO, s = 0.9 Normalized Score 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static PPO, s = 0.95 Normalized Score 0.0 0.4 0.8 1.2 Dense:100% Dense Pruning Rigl Set Static PPO, s = 0.99 Normalized Score Figure 16.SAC: IQM plots calculated over multiple Mujoco games with 9 seeds per game. 50 - 90% sparsity include ﬁve games (Ant, HalfCheetah, Hopper, Humanoid, Walker2d), 95% sparsity includes Ant, HalfCheetah, Hopper, Walker2d, and 99% sparsity HalfCheetah, Hopper, Walker2d. Atari DQN Figure 17 presents IQM plots calculated over 15 Atari games for the standard CNN network architecture and Figure 18 presents IQM plots calculated over the same set of games for a ResNet architecture with an approximately equivalent number of parameters as the standard CNN (≈4M).The State of Sparse Training in Deep Reinforcement Learning 0.00 0.25 0.50 0.75 1.00 Dense:100% Dense Pruning RigL SET Static DQN (CNN) / s=0.5 Human Normalized Score 0.00 0.25 0.50 0.75 1.00 Dense:100% Dense Pruning RigL SET Static DQN (CNN) / s=0.9 Human Normalized Score 0.00 0.25 0.50 0.75 1.00 Dense:100% Dense Pruning RigL SET Static DQN (CNN) / s=0.95 Human Normalized Score 0.00 0.25 0.50 0.75 1.00 Dense:100% Dense Pruning RigL SET Static DQN (CNN) / s=0.99 Human Normalized Score Figure 17.CNN: IQM plots calculated over 15 Atari Games, with 9 seeds per game. 0 1 2 3 4 Dense:100% Dense Pruning RigL SET Static DQN (ResNet) / s=0.5 Human Normalized Score 0 1 2 3 4 Dense:100% Dense Pruning RigL SET Static DQN (ResNet) / s=0.9 Human Normalized Score 0 1 2 3 4 Dense:100% Dense Pruning RigL SET Static DQN (ResNet) / s=0.95 Human Normalized Score 0 1 2 3 4 Dense:100% Dense Pruning RigL SET Static DQN (ResNet) / s=0.99 Human Normalized Score Figure 18.ResNet: IQM plots calculated over 15 Atari Games, with 9 seeds per game.",
      "meta_data": {
        "arxiv_id": "2206.10369v1",
        "authors": [
          "Laura Graesser",
          "Utku Evci",
          "Erich Elsen",
          "Pablo Samuel Castro"
        ],
        "published_date": "2022-06-17T14:08:00Z",
        "pdf_url": "https://arxiv.org/pdf/2206.10369v1.pdf"
      }
    },
    {
      "title": "Memory Based Trajectory-conditioned Policies for Learning from Sparse Rewards",
      "abstract": "Reinforcement learning with sparse rewards is challenging because an agent\ncan rarely obtain non-zero rewards and hence, gradient-based optimization of\nparameterized policies can be incremental and slow. Recent work demonstrated\nthat using a memory buffer of previous successful trajectories can result in\nmore effective policies. However, existing methods may overly exploit past\nsuccessful experiences, which can encourage the agent to adopt sub-optimal and\nmyopic behaviors. In this work, instead of focusing on good experiences with\nlimited diversity, we propose to learn a trajectory-conditioned policy to\nfollow and expand diverse past trajectories from a memory buffer. Our method\nallows the agent to reach diverse regions in the state space and improve upon\nthe past trajectories to reach new states. We empirically show that our\napproach significantly outperforms count-based exploration methods (parametric\napproach) and self-imitation learning (parametric approach with non-parametric\nmemory) on various complex tasks with local optima. In particular, without\nusing expert demonstrations or resetting to arbitrary states, we achieve the\nstate-of-the-art scores under five billion number of frames, on challenging\nAtari games such as Montezuma's Revenge and Pitfall.",
      "full_text": "Memory Based Trajectory-conditioned Policies for Learning from Sparse Rewards Yijie Guo1 Jongwook Choi1 Marcin Moczulski2 Shengyu Feng1 Samy Bengio2 Mohammad Norouzi2 Honglak Lee2,1 1University of Michigan 2Google Brain {guoyijie,jwook,shengyuf}@umich.edu moczulski@google.com {bengio,mnorouzi,honglak}@google.com Abstract Reinforcement learning with sparse rewards is challenging because an agent can rarely obtain non-zero rewards and hence, gradient-based optimization of param- eterized policies can be incremental and slow. Recent work demonstrated that using a memory buffer of previous successful trajectories can result in more ef- fective policies. However, existing methods may overly exploit past successful experiences, which can encourage the agent to adopt sub-optimal and myopic behaviors. In this work, instead of focusing on good experiences with limited diversity, we propose to learn a trajectory-conditioned policy to follow and expand diverse past trajectories from a memory buffer. Our method allows the agent to reach diverse regions in the state space and improve upon the past trajectories to reach new states. We empirically show that our approach signiﬁcantly outperforms count-based exploration methods (parametric approach) and self-imitation learning (parametric approach with non-parametric memory) on various complex tasks with local optima. In particular, without using expert demonstrations or resetting to arbitrary states, we achieve the state-of-the-art scores under ﬁve billion number of frames, on challenging Atari games such as Montezuma’s Revenge and Pitfall. 1 Introduction Deep reinforcement learning (DRL) algorithms with parameterized policy and value function have achieved remarkable success in various complex domains [32, 49, 48]. However, tasks that require reasoning over long horizons with sparse rewards remain exceedingly challenging for the parametric approaches. In these tasks, a positive reward could only be received after a long sequence of appro- priate actions. The gradient-based updates of parameters are incremental and slow and have a global impact on all parameters, which may cause catastrophic forgetting and performance degradation. Many parametric approaches rely on recent samples and do not explore the state space systematically. They might forget the positive-reward trajectories unless the good trajectories are frequently collected. Recently, non-parametric memory from past experiences is employed in DRL algorithms to improve policy learning and sample efﬁciency. Prioritized experience replay [45] proposes to learn from past experiences by prioritizing them based on temporal-difference error. Episodic reinforcement learning [43, 22, 28], self-imitation learning [36, 19], and memory-augmented policy optimization [27] build a memory to store past good experience and thus can rapidly latch onto past successful policies when encountering with states similar to past experiences. However, the exploitation of good experiences within limited directions might hurt performance in some cases. For example on Montezuma’s Revenge (Fig. 1), if the agent exploits the past good trajectories around the yellow path, it would receive the small positive rewards quickly but it loses the chance to achieve a higher score in the long term. Therefore, in order to ﬁnd the optimal path (red), it is better to consider past experiences in diverse directions, instead of focusing only on the good trajectories which lead to myopic behaviors. Inspired by recent work on memory-augmented generative models [21, 9], we note that generating a 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:1907.10247v3  [cs.LG]  15 Feb 2021Myopic Optimal : pick a key (+100): open a door (+300): get a diamond (+1000)               Myopic Optimal : pick a key (+100): open a door (+300): get a diamond (+1000)               Figure 1: Left: The map of the ﬁrst level in Montezuma’s Revenge. We simplify the agent’s paths and enlarge some objects to illustrate typical exploration challenges. The agent also needs to tackle control challenges (e.g., jumping between platforms, avoiding collision with moving enemies and electric ﬁelds, etc.), but they are not highlighted here. After getting two keys, the agent can easily expense the keys to open doors in the middle via the yellow path and achieve small incremental rewards, but as each key can only be used once, the agent is unlikely to open doors at the bottom ﬂoor to clear the level. The previous SOTA fails to open the last two doors. Ours visits the left-most room at the bottom ﬂoor, gets many diamonds, and goes to the next level. Right: Comparison to CoEX [13] (previous SOTA) with high-level state embedding. In a challenging setting with random initial delay, without using expert demonstrations or resetting to arbitrary state, ours explores more rooms and achieves a signiﬁcantly higher score. new sequence by editing prototypes in external memory is easier than generating one from scratch. In an RL setting, we aim to generate new trajectories visiting novel states by editing or augmenting the trajectories stored in the memory from past experiences. We propose a novel trajectory-conditioned policy where a full sequence of states is given as the condition. Then a sequence-to-sequence model with an attention mechanism learns to ‘translate’ the demonstration trajectory to a sequence of actions and generate a new trajectory in the environment with stochasticity. The single policy could take diverse trajectories as the condition, imitate the demonstrations to reach diverse regions in the state space, and allow for ﬂexibility in the action choices to discover novel states. Our main contributions are summarized as follows. (1) We propose a novel architecture for a trajectory-conditioned policy that can ﬂexibly imitate diverse demonstration trajectories. (2) We show the importance of exploiting diverse past experiences in the memory to indirectly drive exploration, by comparing with existing approaches on various sparse-reward RL tasks with stochasticity in the environments. (3) We achieve a performance superior to the state-of-the-art under 5 billion number of frames, on hard-exploration Atari games of Montezuma’s Revenge and Pitfall, without using expert demonstrations or resetting to arbitrary states. We also demonstrate the effectiveness of our method on other benchmarks. 2 Method 2.1 Background and Notation for DTSIL In the standard RL setting, at each time step t, an agent observes a state st, selects an action at ∈A, and receives a reward rt when transitioning to a next state st+1 ∈S, where Sand Ais a set of states and actions respectively. The goal is to ﬁnd a policy πθ(a|s) parameterized by θthat maximizes the expected return Eπθ [∑T t=0 γtrt], where γ ∈(0,1] is a discount factor. In our work, instead of directly maximizing expected return, we propose a novel way to ﬁnd best demonstrations g∗with (near-)optimal return and train the policyπθ(·|g) to imitate any trajectory gin the buffer, includingg∗. We assume a state st includes the observation ot (e.g., raw pixel image) and a high-level abstract state embedding et (e.g., the agent’s location in the abstract space). The embeddinget may be available as a part of st (e.g., the physical features in the robotics domain) or may be learnable from o≤t (e.g., [13, 54] could localize the agent in Atari games, as discussed in Sec. 5). A trajectory-conditioned policy πθ(at|e≤t,ot,g) (which can be viewed as a goal-conditioned policy and denoted as πθ(·|g)) takes a sequence of state embeddingsg= {eg 1,eg 2,··· ,eg |g|}as input for a demonstration, where|g|is the length of the trajectoryg. A sequence of the agent’s past state embeddingse≤t = {e1,e2,··· ,et} is provided to determine which part of the demonstration has been followed by the agent. Together with the current observation ot, it helps to determine the correct actionat to imitate the demonstration. Our goal is to ﬁnd a set of optimal state embedding sequence(s)g∗and the policy π∗ θ(·|g) to maximize the return: g∗,θ∗≜ arg maxg,θEπθ(·|g)[∑T t=0 γtrt]. We approximately solve this joint optimization 2(a) Buffer of Diverse Trajectories Ending with Diverse States(b) Sample a Statefor Exploration or Exploitation(c) Imitate the Demonstrationand Generate a New Trajectory: state.          : trajectory ending with       .           : visitation count of       .              : trajectory.    : ending state.  ……… ……… External Memory New Entry of Novel State(d) Update Memory Buffer with Novel States Demonstration Trajectory Agent’s Trajectory Sampled Demonstration Trajectory h g 1 <latexit sha1_base64=\"IV5x0gxtjcX97WeA+Eq95FEUYwA=\">AAACrXicbVFNbxMxEHW2fJTylcKRi0WExCGE3YJEj5W4cCwSaSo1aTT2Tnat+AvbC9qu9j+0V/hl/Bu8aQ40y0iWnt+b8YznMSuFD2n6Z5Ds3bv/4OH+o4PHT54+ez48fHHmTeU4TrmRxp0z8CiFxmkQQeK5dQiKSZyx9edOn/1A54XR30JtcaGg0GIlOIRIzcrLpmiX2XI4SifpJmgfZFswIts4XR4Orue54ZVCHbgE7y+y1IZFAy4ILrE9mFceLfA1FHgRoQaFftFs5m3pm8jkdGVcPDrQDftvRQPK+1qxmKkglH5X68j/ahw0R7nTPayOF43Qtgqo+W3zVSVpMLRbCM2FQx5kHQFwJ+L8lJfggIe4tjuvB7G+anvMu+/BYfzxRugIKZgDVzdK6FyBHXeyb3uyL8GinxRoFAYneD8DnDM//Ziy2K1wptJ5vHCQfExLw1g9ptZ40fkodHRxOMp2HeuDs6NJ9mFy9PXj6OR46+Y+eUVek7ckI5/ICflCTsmUcLImN+QX+Z28T6bJPLm8TU0G25qX5E4kxV/FF9kT</latexit> h g 0 <latexit sha1_base64=\"wTm5nCDfS/xNcfBEzKW+frXUet4=\">AAACrXicbVFNbxMxEHW2fJTylcKRi0WExCGE3YJEj5W4cCwSaSo1aTT2Tnat+AvbC9qu9j+0V/hl/Bu8aQ40y0iWnt+b8YznMSuFD2n6Z5Ds3bv/4OH+o4PHT54+ez48fHHmTeU4TrmRxp0z8CiFxmkQQeK5dQiKSZyx9edOn/1A54XR30JtcaGg0GIlOIRIzcrLpmiX6XI4SifpJmgfZFswIts4XR4Orue54ZVCHbgE7y+y1IZFAy4ILrE9mFceLfA1FHgRoQaFftFs5m3pm8jkdGVcPDrQDftvRQPK+1qxmKkglH5X68j/ahw0R7nTPayOF43Qtgqo+W3zVSVpMLRbCM2FQx5kHQFwJ+L8lJfggIe4tjuvB7G+anvMu+/BYfzxRugIKZgDVzdK6FyBHXeyb3uyL8GinxRoFAYneD8DnDM//Ziy2K1wptJ5vHCQfExLw1g9ptZ40fkodHRxOMp2HeuDs6NJ9mFy9PXj6OR46+Y+eUVek7ckI5/ICflCTsmUcLImN+QX+Z28T6bJPLm8TU0G25qX5E4kxV/C1NkS</latexit> h g | g | <latexit sha1_base64=\"0I7w7KUC75/+crUsojME2zUQP+o=\">AAACsXicbVFNbxMxEHW2fJTy0RaOXCwiJA4h2m2R6LESF45FIm1RGsLYO9m14i9sL2jZ7n/ohSv9Xf03eNMcaJaRLD2/N+MZz2NWCh/S9GaQbN27/+Dh9qOdx0+ePtvd239+6k3lOE64kcadM/AohcZJEEHiuXUIikk8Y8sPnX72A50XRn8OtcWZgkKLheAQIjUtvzZFO28ui8t2vjdMx+kqaB9kazAk6ziZ7w+uLnLDK4U6cAneT7PUhlkDLggusd25qDxa4EsocBqhBoV+1qxmbunryOR0YVw8OtAV+29FA8r7WrGYqSCUflPryP9qHDRHudE9LI5mjdC2Cqj5bfNFJWkwtFsKzYVDHmQdAXAn4vyUl+CAh7i6O68HsfzV9pi334PD+OOV0BFSMAeubpTQuQI76mTf9mRfgkU/LtAoDE7wfgY4Z376EWWxW+FMpfN44SD5iJaGsXpErfGi81LoovMw23SsD04Pxtnh+ODTu+Hx0drNbfKSvCJvSEbek2PykZyQCeHEkN/kD7lODpMvybeE3aYmg3XNC3InkuVfRuXbYQ==</latexit> ... <latexit sha1_base64=\"/ps7m72IRakV+XzlF0MmYPWJgH8=\">AAACqXicbVFNb9QwEPWGr1I+2sKRi8UKCYklSgoSPVbiwrEVbLuiXVWTyWzWWn9hO6AQ5ScgcYLfxr/B2e6BbhjJ0vN7M57xvMJK4UOW/Rklt27fuXtv5/7ug4ePHu/tHzw586Z2SFM00rhZAZ6k0DQNIkiaWUegCknnxep9r59/JeeF0Z9CY2muoNJiIRBCpD6maXq1P87SbB18CPINGLNNnFwdjH5elgZrRTqgBO8v8syGeQsuCJTU7V7WnizgCiq6iFCDIj9v17N2/EVkSr4wLh4d+Jr9t6IF5X2jipipICz9ttaT/9UQNJLc6h4WR/NWaFsH0njdfFFLHgzvl8FL4QiDbCIAdCLOz3EJDjDEld14PYjV927AvP4SHMUfr4WekKJw4JpWCV0qsJNe9t1A9kuw5NOKjKLgBA4zwDnzzU94EbtVztS6jBcEiRO+NEXRTLg1XvQeCl110cN827EhODtM8zfp4enb8fHRxs0d9ow9Zy9Zzt6xY/aBnbApQ1axH+wX+528Sk6TWfL5OjUZbWqeshuR4F8+rdbA</latexit> e g 0 <latexit sha1_base64=\"fxBD3yafJ27jLEz3rWmvyIeBIiU=\">AAACq3icbVFNbxMxEHWWj5by1cKRi0WExCGNdlskeqzEhWMRpK1oQjTrnWys+At7tmhZ7W+AI/w0/g3eNAeaZSRLz+/NeMbzcqdkoDT9M0ju3L13f2f3wd7DR4+fPN0/eHYebOUFToRV1l/mEFBJgxOSpPDSeQSdK7zIV+86/eIafZDWfKLa4UxDaeRCCqBITXCefinn+8N0nK6D90G2AUO2ibP5weDntLCi0mhIKAjhKksdzRrwJIXCdm9aBXQgVlDiVYQGNIZZs5625a8iU/CF9fEY4mv234oGdAi1zmOmBlqGba0j/6sJMALVVndanMwaaVxFaMRN80WlOFnerYMX0qMgVUcAwss4PxdL8CAoLu3W6yRX39sec/iVPMYfr4WOUDL34OtGS1NocKNODm1PDktwGMYlWo3kpehngPf2WxjxPHYrva1MES8ClBjxpc3zesSdDbJzUZqyjR5m2471wfnRODseH314Mzw92bi5y16wl+w1y9hbdsreszM2YYJJ9oP9Yr+Tw+Rj8jmZ3qQmg03Nc3YrEvwLOlzYAw==</latexit> e g | g | <latexit sha1_base64=\"j24CPU+MPvtbyUn1x2bZtma+zOU=\">AAACr3icbVFNbxMxEHWWr1K+WjhysYiQOIRot0Wix0pcOBaJNEVtGma9k40Vf2HPgpbt/gckrvDD+Dd40xxolpEsPb834xnPy52SgdL0zyC5dfvO3Xs793cfPHz0+Mne/tPTYCsvcCKssv4sh4BKGpyQJIVnziPoXOE0X73r9OlX9EFa85FqhzMNpZELKYAi9QnnzVV51V6W871hOk7Xwfsg24Ah28TJfH/w46KwotJoSCgI4TxLHc0a8CSFwnb3ogroQKygxPMIDWgMs2Y9cctfRqbgC+vjMcTX7L8VDegQap3HTA20DNtaR/5XE2AEqq3utDiaNdK4itCI6+aLSnGyvFsJL6RHQaqOAISXcX4uluBBUFzcjddJrr63Peb1F/IYf7wWOkLJ3IOvGy1NocGNOjm0PTkswWEYl2g1kpeinwHe229hxPPYrfS2MkW8CFBixJc2z+sRdzbIzklpyjZ6mG071genB+PscHzw4c3w+Gjj5g57zl6wVyxjb9kxe89O2IQJptlP9ov9TrJkmlwmn69Tk8Gm5hm7EYn8C7uc2lI=</latexit> g ⇠ D <latexit sha1_base64=\"20XvuQowmn/b4wZLNQNpO6yyMXw=\">AAACunicbVFNbxMxEHWWr1K+0nLkYhEhcQjRbotEDxwqwYFjkUhbqYmisXeyseKPxZ4FltX+B34BV/hL/Bu8aQ40y0iWnt+b8YzniVKrQGn6Z5Dcun3n7r29+/sPHj56/GR4cHgeXOUlTqXTzl8KCKiVxSkp0nhZegQjNF6I9btOv/iCPihnP1Fd4txAYdVSSaBILYaHBZ8FZfjMAK0k6OZ9uxiO0km6Cd4H2RaM2DbOFgeDH7PcycqgJakhhKssLWnegCclNbb7sypgCXINBV5FaMFgmDeb4Vv+IjI5XzofjyW+Yf+taMCEUBsRM7sZw67Wkf/VJFiJeqc7LU/mjbJlRWjldfNlpTk53m2H58qjJF1HANKrOD+XK/AgKe7wxuuk1t/bHvPqM3mMP94IHaGV8ODrxiibGyjHnRzanhxWUGKYFOgMkleynwHeu69hzEXsVnhX2TxeomFyzFdOiHrMSxdUZ6qyRedhtutYH5wfTbLjydHH16PTk62be+wZe85esoy9YafsAztjUybZN/aT/WK/k7eJSFSyvk5NBtuap+xGJPQXQeTduw==</latexit> e g 1 <latexit sha1_base64=\"lJw7uE4Ai+bJhyA+7BxJAfB3l8E=\">AAACrXicbVFNbxMxEHWWj5by1ZYjF4sIiUMIuwWJHiv1wrFIpKnUpNHYO9lY8Re2t2i72v9QrvDL+Dd4tznQLCNZen5vxjOex6wUPqTpn0Hy4OGjxzu7T/aePnv+4uX+weG5N6XjOOFGGnfBwKMUGidBBIkX1iEoJnHK1qetPr1G54XR30Jlca6g0GIpOIRITXGRXdVFs9gfpuO0C9oH2QYMySbOFgeD21lueKlQBy7B+8sstWFegwuCS2z2ZqVHC3wNBV5GqEGhn9fdvA19G5mcLo2LRwfasf9W1KC8rxSLmQrCym9rLflfjYPmKLe6h+XxvBbalgE1v2u+LCUNhrYLoblwyIOsIgDuRJyf8hU44CGu7d7rQaxvmh7z/ntwGH/cCS0hBXPgqloJnSuwo1b2TU/2K7DoxwUahcEJ3s8A58wPP6IsdiucKXUeLxwkH9GVYawaUWu8aH0UuvMw23asD86PxtnH8dHXT8OT442bu+Q1eUPekYx8JifkCzkjE8LJmvwkv8jv5EMySWbJ1V1qMtjUvCL3Iin+Ar0C2RA=</latexit> ... <latexit sha1_base64=\"/ps7m72IRakV+XzlF0MmYPWJgH8=\">AAACqXicbVFNb9QwEPWGr1I+2sKRi8UKCYklSgoSPVbiwrEVbLuiXVWTyWzWWn9hO6AQ5ScgcYLfxr/B2e6BbhjJ0vN7M57xvMJK4UOW/Rklt27fuXtv5/7ug4ePHu/tHzw586Z2SFM00rhZAZ6k0DQNIkiaWUegCknnxep9r59/JeeF0Z9CY2muoNJiIRBCpD6maXq1P87SbB18CPINGLNNnFwdjH5elgZrRTqgBO8v8syGeQsuCJTU7V7WnizgCiq6iFCDIj9v17N2/EVkSr4wLh4d+Jr9t6IF5X2jipipICz9ttaT/9UQNJLc6h4WR/NWaFsH0njdfFFLHgzvl8FL4QiDbCIAdCLOz3EJDjDEld14PYjV927AvP4SHMUfr4WekKJw4JpWCV0qsJNe9t1A9kuw5NOKjKLgBA4zwDnzzU94EbtVztS6jBcEiRO+NEXRTLg1XvQeCl110cN827EhODtM8zfp4enb8fHRxs0d9ow9Zy9Zzt6xY/aBnbApQ1axH+wX+528Sk6TWfL5OjUZbWqeshuR4F8+rdbA</latexit> h 0 <latexit sha1_base64=\"avtN/9POZOLHmx9Bng/AEJ4GIPc=\">AAACqXicbVFNb9NAEN2Yr1I+2sKRy4oICYkQ2QWJHitx6bFVSRvRRtF4PLFX2Q+zu6ZyLf8EpJ7gt/Fvuk5zoDEjrfT2vZmd2XlpKYXzcfx3ED14+Ojxk62n28+ev3i5s7v36syZyiJN0Ehjpyk4kkLTxAsvaVpaApVKOk+XXzv9/CdZJ4z+5uuSZgpyLRYCwQfqtJjH891hPI5XwfsgWYMhW8fxfG9wc5kZrBRpjxKcu0ji0s8asF6gpHb7snJUAi4hp4sANShys2Y1a8vfBSbjC2PD0Z6v2H8rGlDO1SoNmQp84Ta1jvyvhqCR5EZ3vziYNUKXlSeNd80XleTe8G4ZPBOW0Ms6AEArwvwcC7CAPqzs3uteLK/bHvPxh7cUfrwSOkKK1IKtGyV0pqAcdbJre7IroCQ3zsko8lZgPwOsNVduxNPQLbem0lm4IEgc8cKkaT3ipXGi81DovA0eJpuO9cHZ/jj5NN4/+Tw8PFi7ucXesLfsPUvYF3bIjtgxmzBkOfvFfrM/0YfoJJpG3+9So8G65jW7FxHeAjXo1y0=</latexit> h 1 <latexit sha1_base64=\"Y276hoMxSPiXPZ1bATcS675oyPE=\">AAACqXicbVFNb9NAEN2Yr1I+2sKRy4oICYkQ2QWJHitx6bFVSRvRRtF4PLFX2Q+zu6ZyLf8EpJ7gt/Fvuk5zoDEjrfT2vZmd2XlpKYXzcfx3ED14+Ojxk62n28+ev3i5s7v36syZyiJN0Ehjpyk4kkLTxAsvaVpaApVKOk+XXzv9/CdZJ4z+5uuSZgpyLRYCwQfqtJgn891hPI5XwfsgWYMhW8fxfG9wc5kZrBRpjxKcu0ji0s8asF6gpHb7snJUAi4hp4sANShys2Y1a8vfBSbjC2PD0Z6v2H8rGlDO1SoNmQp84Ta1jvyvhqCR5EZ3vziYNUKXlSeNd80XleTe8G4ZPBOW0Ms6AEArwvwcC7CAPqzs3uteLK/bHvPxh7cUfrwSOkKK1IKtGyV0pqAcdbJre7IroCQ3zsko8lZgPwOsNVduxNPQLbem0lm4IEgc8cKkaT3ipXGi81DovA0eJpuO9cHZ/jj5NN4/+Tw8PFi7ucXesLfsPUvYF3bIjtgxmzBkOfvFfrM/0YfoJJpG3+9So8G65jW7FxHeAjgr1y4=</latexit> h t <latexit sha1_base64=\"BeuVo79kS0eLzMo85XQ5j31wBg4=\">AAACqXicbVFNb9NAEN2Yr1I+2sKRy4oICYkQ2QWJHitx6bFVSRvRRtF4PbFX2Q+zO6ZyLf8EpJ7gt/Fvuk5zoDEjrfT2vZmd2XlpqaSnOP47iB48fPT4ydbT7WfPX7zc2d17deZt5QROhFXWTVPwqKTBCUlSOC0dgk4VnqfLr51+/hOdl9Z8o7rEmYbcyIUUQIE6LeY03x3G43gVvA+SNRiydRzP9wY3l5kVlUZDQoH3F0lc0qwBR1IobLcvK48liCXkeBGgAY1+1qxmbfm7wGR8YV04hviK/beiAe19rdOQqYEKv6l15H81AUag2uhOi4NZI01ZERpx13xRKU6Wd8vgmXQoSNUBgHAyzM9FAQ4EhZXde53k8rrtMR9/kMPw45XQEUqmDlzdaGkyDeWok33bk30BJfpxjlYjOSn6GeCcvfIjnoZuubOVycJFgBIjXtg0rUe8tF52HkqTt8HDZNOxPjjbHyefxvsnn4eHB2s3t9gb9pa9Zwn7wg7ZETtmEyZYzn6x3+xP9CE6iabR97vUaLCuec3uRSRuAc+013E=</latexit> e 0 <latexit sha1_base64=\"MFpP+xjfV7CWxGvcF7oeNiRozcM=\">AAACqXicbVFNbxMxEHWWr1I+2sKRi0WEhESIdgsSPVbiwrEVpI1oo2h2drKx4i9sL9V2tT8BiRP8Nv4N3jQHmmUkS8/vzXjG83IrhQ9p+meQ3Ll77/6DnYe7jx4/ebq3f/DszJvKIU3QSOOmOXiSQtMkiCBpah2ByiWd56uPnX7+nZwXRn8JtaWZglKLhUAIkfpM83S+P0zH6Tp4H2QbMGSbOJkfDH5eFgYrRTqgBO8vstSGWQMuCJTU7l5WnizgCkq6iFCDIj9r1rO2/FVkCr4wLh4d+Jr9t6IB5X2t8pipICz9ttaR/9UQNJLc6h4WR7NGaFsF0njTfFFJHgzvlsEL4QiDrCMAdCLOz3EJDjDEld16PYjVddtj3n4LjuKP10JHSJE7cHWjhC4U2FEn+7Yn+yVY8uOSjKLgBPYzwDlz5Uc8j91KZypdxAuCxBFfmjyvR9waLzoPhS7b6GG27VgfnB2Os3fjw9P3w+OjjZs77AV7yV6zjH1gx+wTO2EThqxkP9gv9jt5k5wm0+TrTWoy2NQ8Z7ciwb8vGdcq</latexit> e 1 <latexit sha1_base64=\"0zcSnWSaIZZ2kh70pAqk1+J9I74=\">AAACqXicbVFNbxMxEHWWr1I+2sKRi0WEhESIdgsSPVbiwrEVpI1oo2h2drKx4i9sL9V2tT8BiRP8Nv4N3jQHmmUkS8/vzXjG83IrhQ9p+meQ3Ll77/6DnYe7jx4/ebq3f/DszJvKIU3QSOOmOXiSQtMkiCBpah2ByiWd56uPnX7+nZwXRn8JtaWZglKLhUAIkfpM82y+P0zH6Tp4H2QbMGSbOJkfDH5eFgYrRTqgBO8vstSGWQMuCJTU7l5WnizgCkq6iFCDIj9r1rO2/FVkCr4wLh4d+Jr9t6IB5X2t8pipICz9ttaR/9UQNJLc6h4WR7NGaFsF0njTfFFJHgzvlsEL4QiDrCMAdCLOz3EJDjDEld16PYjVddtj3n4LjuKP10JHSJE7cHWjhC4U2FEn+7Yn+yVY8uOSjKLgBPYzwDlz5Uc8j91KZypdxAuCxBFfmjyvR9waLzoPhS7b6GG27VgfnB2Os3fjw9P3w+OjjZs77AV7yV6zjH1gx+wTO2EThqxkP9gv9jt5k5wm0+TrTWoy2NQ8Z7ciwb8xXNcr</latexit> e t <latexit sha1_base64=\"hCKyM/kT+CpmysNMb36hmbnX4ao=\">AAACqXicbVFNbxMxEHWWr1I+2sKRi0WEhESIdgsSPVbiwrEVpI1oo2jWO9lY8Rf2LNV2tT8BiRP8Nv4N3jQHmmUkS8/vzXjG83KnZKA0/TNI7ty9d//BzsPdR4+fPN3bP3h2FmzlBU6EVdZPcwiopMEJSVI4dR5B5wrP89XHTj//jj5Ia75Q7XCmoTRyIQVQpD7jnOb7w3ScroP3QbYBQ7aJk/nB4OdlYUWl0ZBQEMJFljqaNeBJCoXt7mUV0IFYQYkXERrQGGbNetaWv4pMwRfWx2OIr9l/KxrQIdQ6j5kaaBm2tY78rybACFRb3WlxNGukcRWhETfNF5XiZHm3DF5Ij4JUHQEIL+P8XCzBg6C4sluvk1xdtz3m7TfyGH+8FjpCydyDrxstTaHBjTo5tD05LMFhGJdoNZKXop8B3turMOJ57FZ6W5kiXgQoMeJLm+f1iDsbZOehNGUbPcy2HeuDs8Nx9m58ePp+eHy0cXOHvWAv2WuWsQ/smH1iJ2zCBCvZD/aL/U7eJKfJNPl6k5oMNjXP2a1IxF/I5ddu</latexit> Agent’s Trajectory ↵ t <latexit sha1_base64=\"fd7kY3QzsnmcM+2QguBPkZ8g0V8=\">AAACrnicbVFNbxMxEHWWj5by1cKRi0WExCFEuwWpPVbiwrFIJI3UrKKxd7JrxV+1vaBltf8BcYU/xr/Bm+ZAs4xk6fm9Gc94HrNS+JCmf0bJvfsPHh4cPjp6/OTps+fHJy/m3tSO44wbadyCgUcpNM6CCBIX1iEoJvGKbT72+tVXdF4Y/SU0FnMFpRZrwSFEarEEaStYhdXxOJ2m26BDkO3AmOzicnUy+rEsDK8V6sAleH+dpTbkLbgguMTuaFl7tMA3UOJ1hBoU+rzdDtzRN5Ep6Nq4eHSgW/bfihaU941iMVNBqPy+1pP/1ThojnKve1if563Qtg6o+W3zdS1pMLTfCC2EQx5kEwFwJ+L8lFfggIe4tzuvB7H53g2YdzfBYfzxVugJKZgD17RK6EKBnfSy7wayr8Cin5ZoFAYn+DADnDPf/ISy2K10ptZFvHCQfEIrw1gzodZ40RspdNlFD7N9x4ZgfjrN3k9PP38YX5zv3Dwkr8hr8pZk5IxckE/kkswIJ5L8JL/I7yRN5kmerG5Tk9Gu5iW5E0n1Fw6j2Z0=</latexit> AttentionReadoutc t <latexit sha1_base64=\"E8LSHhqg8MLY3BUkCbBYjm87LTA=\">AAACqXicbVFNbxMxEHWWr1I+2sKRi0WEhESIdgsSPVbiwrEVpI1oo2h2drKx4i9sL9V2tT8BiRP8Nv4N3jQHmmUkS8/vzXjG83IrhQ9p+meQ3Ll77/6DnYe7jx4/ebq3f/DszJvKIU3QSOOmOXiSQtMkiCBpah2ByiWd56uPnX7+nZwXRn8JtaWZglKLhUAIkfqM8zDfH6bjdB28D7INGLJNnMwPBj8vC4OVIh1QgvcXWWrDrAEXBEpqdy8rTxZwBSVdRKhBkZ8161lb/ioyBV8YF48OfM3+W9GA8r5WecxUEJZ+W+vI/2oIGkludQ+Lo1kjtK0CabxpvqgkD4Z3y+CFcIRB1hEAOhHn57gEBxjiym69HsTquu0xb78FR/HHa6EjpMgduLpRQhcK7KiTfduT/RIs+XFJRlFwAvsZ4Jy58iOex26lM5Uu4gVB4ogvTZ7XI26NF52HQpdt9DDbdqwPzg7H2bvx4en74fHRxs0d9oK9ZK9Zxj6wY/aJnbAJQ1ayH+wX+528SU6TafL1JjUZbGqes1uR4F/EW9ds</latexit> o t <latexit sha1_base64=\"pItX1dLeegvb2dVee+9NQ95+YfY=\">AAACqXicbVFNbxMxEHWWr1I+2sKRi0WEhESIdgsSPVbiwrEVpI1oo2jWO9lY8Rf2LNV2tT8BiRP8Nv4N3jQHmmUkS8/vzXjG83KnZKA0/TNI7ty9d//BzsPdR4+fPN3bP3h2FmzlBU6EVdZPcwiopMEJSVI4dR5B5wrP89XHTj//jj5Ia75Q7XCmoTRyIQVQpD7bOc33h+k4XQfvg2wDhmwTJ/ODwc/LwopKoyGhIISLLHU0a8CTFArb3csqoAOxghIvIjSgMcya9awtfxWZgi+sj8cQX7P/VjSgQ6h1HjM10DJsax35X02AEai2utPiaNZI4ypCI26aLyrFyfJuGbyQHgWpOgIQXsb5uViCB0FxZbdeJ7m6bnvM22/kMf54LXSEkrkHXzdamkKDG3VyaHtyWILDMC7RaiQvRT8DvLdXYcTz2K30tjJFvAhQYsSXNs/rEXc2yM5Daco2ephtO9YHZ4fj7N348PT98Pho4+YOe8FestcsYx/YMfvETtiECVayH+wX+528SU6TafL1JjUZbGqes1uRiL/fl9d4</latexit> FC Observation ⇡ ( a t | e  t ,o t ,g ) <latexit sha1_base64=\"UGeE0n2u4w752swz8aaRhd6qrDI=\">AAACx3icbVFNb9NAEN2Yr1I+msKRAysipCKZyClI9FiJC9yKRNpKdWSN1xNnlf0wu+NCsHzgwJ3/wRV+DP+GdZoDjRlppbfvzezszMsrJT0lyZ9BdOPmrdt3du7u3rv/4OHecP/Rqbe1EzgVVll3noNHJQ1OSZLC88oh6FzhWb582+lnl+i8tOYjrSqcaSiNnEsBFKhs+DSt5AFkxFMtC45Zkyrk1MbcZhTz8kU2HCXjZB28DyYbMGKbOMn2Bz/SwopaoyGhwPuLSVLRrAFHUihsd9PaYwViCSVeBGhAo58160la/jwwBZ9bF44hvmb/rWhAe7/SecjUQAu/rXXkfzUBRqDa6k7zo1kjTVUTGnHVfF4rTpZ3q+KFdChIrQIA4WT4PxcLcCAoLPTa6ySXX9se8/ITOQwTr4WOUDJ34FaNlqbQUMWd7Nue7BdQoR+XaDWSk6KfAc7Zzz7meehWOlubIlwEKBHzhc3zVcwr62XnsDRlGzycbDvWB6eH48mr8eGH16Pjo42bO+wJe8YO2IS9YcfsHTthUybYd/aT/WK/o/eRjS6jL1ep0WBT85hdi+jbX6nf4c0=</latexit> V ( e  t ,o t ,g ) <latexit sha1_base64=\"V2f7dB+EUwmFCVXvoaXbYa5NfYA=\">AAACvHicbVHbbtNAEN2YWymXpsAbLysipCKZyC5I9AlV4oXHIpG0UhNZ4/XEWWUvZndc5Fr+Bz6BV/gj/oZ1mgeaMNJKZ8+Z2Zmdk1dKekqSP4Pozt179x/sPdx/9PjJ04Ph4bOpt7UTOBFWWXeRg0clDU5IksKLyiHoXOF5vvrU6+dX6Ly05is1Fc41lEYupAAKVDZ8MT3CrJ0p5NTF3GYU8/JNNhwl42QdfBekGzBimzjLDgc/ZoUVtUZDQoH3l2lS0bwFR1Io7PZntccKxApKvAzQgEY/b9fjd/x1YAq+sC4cQ3zN/lvRgva+0XnI1EBLv6315H81AUag2upOi5N5K01VExpx03xRK06W9/vhhXQoSDUBgHAyzM/FEhwIClu89TrJ1XW3w7z9Rg7Dj9dCTyiZO3BNq6UpNFRxL/tuR/ZLqNCPS7QayUmxmwHO2e8+5nnoVjpbmyJcBCgR86XN8ybmlfWyt1WasgseptuO7YLp8Th9Nz7+8n50erJxc4+9ZK/YEUvZB3bKPrMzNmGCXbOf7Bf7HX2MimgV6ZvUaLCpec5uRXT1F85F3XY=</latexit> Concat Demonstration Trajectory Figure 2: Left: Overview of DTSIL. (a) We maintain a trajectory buffer. (b) For each episode, we sample a state from the buffer, (c) imitate the demonstration leading to the sampled state, obtain a new trajectory, (d) update the memory with the new trajectory and gradually expand the buffer. We repeat this process until training goes to the end. Right: Architecture of the trajectory-conditioned policy (see details in Sec. 2.3). problem via the sampling-based search for g∗ over the space of g realizable by the (trajectory- conditioned) policy πθ and gradient-based local search for θ∗. For robustness, we may want to ﬁnd multiple trajectories with high returns and a trajectory-conditioned policy executing them. We name our method as Diverse Trajectory-conditioned Self-Imitation Learning (DTSIL). 2.2 Overview of DTSIL Organizing Trajectory Buffer As shown in Fig. 2(a), we maintain a trajectory buffer D = {(e(1),τ(1),n(1)),(e(2),τ(2),n(2)),···} of diverse past trajectories. τ(i) is the best trajectory ending with a state with embedding e(i). n(i) is the number of times the cluster represented by the embedding e(i) has been visited during training. To maintain a compact buffer, similar state embeddings within the tolerance threshold δ are clustered together, and an existing entry is replaced if an improved trajectory τ(i) ending with a near-identical state is found. In the buffer, we keep a single representative state embedding for each cluster. If a state embedding et observed in the current episode is close to a representative state embedding e(k), we increase visitation count n(k) of the k-th cluster. If the sub-trajectory τ≤t of the current episode up to step tis better than τ(k), e(k) is replaced by et. Pseudocode for organizing clusters is in the appendix. Sampling States for Exploitation or ExplorationIn RL algorithms, the agent needs to exploit what it already knows to maximize reward and explore new behaviors to ﬁnd a potentially better policy. For exploitation, we aim at reaching the states with the highest total rewards, which probably means a good behavior of receiving high total rewards. For exploration, we would like to look around the rarely visited states, which helps discover novel states with higher total rewards. With probability 1 −p, in exploitation mode, we sample the states in the buffer with the highest cumulative rewards. With probability p, in exploration mode, we sample each state e(i) with the probability proportional to 1/ √ n(i), as inspired by count-based exploration [ 50, 7] and rank-based prioritization [ 45, 16]. To balance between exploration and exploitation, we decrease the hyper-parameterpof taking the exploration mode. The pseudo-code algorithm of sampling the states is in the appendix. Imitating Trajectory to State of InterestIn stochastic environments, in order to reach diverse states e(i) we sampled, the agent would need to learn a goal-conditioned policy [1, 34, 44, 40]. But it is difﬁcult to learn the goal-conditioned policy only with the ﬁnal goal state because the goal state might be far from the agent’s initial states and the agent might have few experiences reaching it. Therefore, we provide the agent with the full trajectory leading to the goal state. So the agent beneﬁts from richer intermediate information and denser rewards. We call this trajectory-conditioned policy πθ(·|g) where g= {eg 1,eg 2,··· ,eg |g|}, and introduce how to train the policy in detail in Sec. 2.3. Updating Buffer with New TrajectoryWith trajectory-conditioned policy, the agent takes actions to imitate the sampled demonstration trajectory. As shown in Fig. 2(c), because there could be stochasticity in the environment and our method does not require the agent to exactly follow the demonstration step by step, the agent’s new trajectory could be different from the demonstration and thus visit novel states. In a new trajectoryE= {(o0,e0,a0,r0),··· ,(oT,eT,aT,rT)}, if et is nearly identical to a state embedding e(k) in the buffer and the partial episode τ≤t is better than (i.e. higher return or shorter trajectory) the stored trajectory τ(k), we replace the existing entry (e(k),τ(k),n(k)) by (et,τ≤t,n(k) + 1). If et is not sufﬁciently similar to any state embedding in the buffer, a new entry (et,τ≤t,1) is pushed into the buffer, as shown in Fig. 2(d). Therefore we gradually increase the diversity of trajectories in the buffer. The detailed algorithm is described in the supplementary material. 32.3 Learning Trajectory-Conditioned Policy Policy Architecture For imitation learning with diverse demonstrations, we design a trajectory- conditioned policy πθ(at|e≤t,ot,g) that should ﬂexibly imitate any given trajectory g. Inspired by neural machine translation methods [51, 6], one can view the demonstration as the source sequence and view the incomplete trajectory of the agent’s state representations as the target sequence. We apply a recurrent neural network (RNN) and an attention mechanism Bahdanau et al. [6] to the sequence data to predict actions that would make the agent follow the demonstration. As illustrated in Fig. 2, RNN computes the hidden features hg i for each state embedding eg i (0 ≤i≤|g|) in the demonstration and derives the hidden features ht for the agent’s state representationet. Then the attention weight αt is computed by comparing the current agent’s hidden featuresht with the demonstration’s hidden features hg i (0 ≤i≤|g|). The attention readout ct is computed as an attention-weighted summation of the demonstration’s hidden features to capture the relevant information in the demonstration and to predict the action at. Training is performed by combining RL and supervised objectives as follows. Reinforcement Learning Objective Given a demonstration trajectory g= {eg 0,eg 1,··· ,eg |g|}, we provide rewards for imitating g and train the policy to maximize rewards. For each episode, we record uto denote the index of state in the given demonstration that is lastly visited by the agent. At the beginning of an episode, the index uof the lastly visited state embedding in the demonstration is initialized as u= −1, which means no state in the demonstration has been visited. At each step t, if the agent’s new state st+1 has an embedding et+1 and it is the similar enough to any of the next ∆tstate embeddings starting from the last visited state embedding eg u in the demonstration (i.e., ∥et+1 −eg u′ ∥<δ where u < u′≤u+ ∆t), then the index of the last visited state embedding in the demonstration is updated as u ←u′and the agent receives environment reward and positive imitation reward rDTSIL t = f(rt) + rim, where f(·) is a monotonically increasing function (e.g., clipping [32]) and rim is the imitation reward with a value of 0.1 in our experiments. Otherwise, the reward rDTSIL t is 0 (see appendix for an illustration example). This encourages the agent to visit states in the demonstration in a soft-order so that it can edit or augment the demonstration when executing a new trajectory. The demonstration plays a role to guide the agent to the region of interest in the state embedding space. After visiting the last (non-terminal) state in the demonstration, the agent performs random exploration (because rDTSIL t = 0) around and beyond the last state until the episode terminates, to push the frontier of exploration. With rDTSIL t , the trajectory-conditioned policy πθ can be trained with a policy gradient algorithm [52]: LRL = Eπθ [−log πθ(at|e≤t,ot,g) ˆAt], (1) where ˆAt = n−1∑ d=0 γdrDTSIL t+d + γnVθ(e≤t+n,ot+n,g) −Vθ(e≤t,ot,g) where Eπθ indicates the empirical average over a ﬁnite batch of on-policy samples and ndenotes the number of rollout steps taken in each iteration. We use Proximal Policy Optimization [48] as an actor-critic policy gradient algorithm for our experiments. Supervised Learning Objective To improve trajectory-conditioned imitation learning and to better leverage the past trajectories, we propose a supervised learning objective. We leverage the actions in demonstrations, similarly to behavior cloning, to help RL for imitation of diverse trajectories. We sample a trajectory τ = {(o0,e0,a0,r0),(o1,e1,a1,r1) ···}∈D , formulate the demonstration g= {e0,e1,··· ,e|g|}and assume the agent’s incomplete trajectory is the partial trajectoryg≤t = e≤t = {e0,e1,··· ,et}for any 1 ≤t≤|g|. Then at is the ‘correct’ action at steptfor the agent to imitate the demonstration. Our supervised learning objective is to maximize the log probability of taking such actions: LSL = −log πθ(at|e≤t,ot,g), where g= {e0,e1,··· ,e|g|}. (2) 2.4 Extensions of DTSIL for Improved Robustness and Generalization DTSIL can be easily extended for more challenging scenarios. Without hand-crafted high-level state embeddings, we can combine DTSIL with state representation learning approaches (Sec. 5.1). In highly stochastic environments, we modify DTSIL to construct and select proper demonstra- tions (Sec. 5.2). In addition, DTSIL can be extended with hierarchical reinforcement learning for generalization over multiple tasks (Sec. 5.3). See individual sections for more details. 43 Related Work Imitation Learning The goal of imitation learning is to train a policy to mimic a given demon- stration. Many previous works achieve good results on hard-exploration Atari games by imitating human demonstrations [23, 41]. Aytar et al. [3] learn embeddings from a variety of demonstration videos and proposes the one-shot imitation learning reward, which inspires the design of rewards in our method. All these successful attempts rely on the availability of human demonstrations. In contrast, our method treats the agent’s past trajectories as demonstrations. Memory Based RL An external memory buffer enables the storage and usage of past experiences to improve RL algorithms. Episodic reinforcement learning methods [43, 22, 28] typically store and update a look-up table to memorize the best episodic experiences and retrieve the episodic memory in the agent’s decision-making process. Oh et al.[36] and Gangwani et al. [19] train a parameterized policy to imitate only the high-reward trajectories with the SIL or GAIL objective. Unlike the previous work focusing on high-reward trajectories, we store the past trajectories ending with diverse states in the buffer, because trajectories with low reward in the short term might lead to high reward in the long term. Badia et al. [5] train a range of directed exploratory policies based on episodic memory. Gangwani et al. [19] propose to learn multiple diverse policies in a SIL framework but their exploration can be limited by the number of policies learned simultaneously and the exploration performance of every single policy, as shown in the supplementary material. Learning Diverse Policies Previous works [20, 17, 42] seek a diversity of policies by maximizing state coverage, the entropy of mixture skill policies, or the entropy of goal state distribution. Zhang et al. [56] learns a variety of policies, each performing novel action sequences, where the novelty is measured by a learned autoencoder. However, these methods focus more on tasks with relatively simple state space and dense rewards while DTSIL shows experimental results performing well on long-horizon, sparse-reward environments with a rich observation space like Atari games. Exploration Many exploration methods [46, 2, 12, 50] in RL tend to award a bonus to encourage an agent to visit novel states. Recently this idea was scaled up to large state spaces [53, 7, 38, 11, 39, 10]. Intrinsic curiosity uses the prediction error or pseudo count as intrinsic reward signals to incentivize visiting novel states. We propose that instead of directly taking a quantiﬁcation of novelty as an intrinsic reward, one can encourage exploration by rewarding the agent when it successfully imitates demonstrations that would lead to novel states. Ecoffet et al.[16] also shows the beneﬁt of exploration by returning to promising states. Our method can be viewed in general as an extension of [16], though we do not need to rely on the assumption that the environment is resettable to arbitrary states. Similar to previous off-policy methods, we use experience replay to enhance exploration. Many off-policy methods [25, 36, 1] tend to discard old experiences with low rewards and hence may prematurely converge to sub-optimal behaviors, but DTSIL using these diverse experiences has a better chance of ﬁnding higher rewards in the long term. Contemporaneous works [5, 4] as off-policy methods also achieved strong results on Atari games. NGU [5] constructs an episodic memory-based intrinsic reward using k-nearest neighbors over the agent’s recent experience to train the directed exploratory policies. Agent57 [4] parameterizes a family of policies ranging from very exploratory to purely exploitative and proposes an adaptive mechanism to choose which policy to prioritize throughout the training process. While these methods require a large number of interactions, ours perform competitively well on the hard-exploration Atari games with less than one-tenth of samples. Model-based reinforcement learning [24, 47, 26] generally improves the efﬁciency of policy learning. However, in the long-horizon, sparse-reward tasks, it is rare to collect precious transitions with non-zero rewards and thus it is difﬁcult to learn a model correctly predicting the dynamics of getting positive rewards. We instead perform efﬁcient policy learning in the hard-exploration tasks because of efﬁcient exploration with the trajectory-conditioned policy. 4 Experiments In the experiments, we aim to answer the following questions: (1) How well does the trajectory- conditioned policy imitate the diverse demonstration trajectories? (2) Does the imitation of the past diverse experience enable the agent to further explore more diverse directions and guide the exploration to ﬁnd the trajectory with a near-optimal total reward? (3) Is our method helpful for avoiding myopic behaviors and converging to near-optimal solutions? We compare our method with the following baselines: (1) PPO [48]; (2) PPO+EXP: PPO with reward f(rt) + λ/ √ N(et), where λ/ √ N(et) is the count-based exploration bonus, N(e) is the number of times the cluster which the state representation ebelongs to was visited during training and λis the hyper-parameter controlling the weight of exploration term; (3) PPO+SIL: PPO with Self-Imitation 5:apple (+1)       :gold (+10)      :rock (-0.05) Myopic Optimal (a) The map. 0M 8M 16M 24M 32M 40M Steps 0 2 4 6 8Average Reward PPO PPO+EXP PPO+SIL DTRA DTSIL (b) Average episode reward. PPO+SILDTSIL0M steps4.8M steps9.6M steps (c) Visualization of the trajectories stored in the buffer for PPO+SIL and DTSIL (ours) as training continues. The agent (gray), apple (red) and treasure (yellow) are shown as squares for simplicity. The rocky region is in dark blue. The reward of getting an apple, collecting the gold and stepping in rock is 1, 10, -0.05 respectively. (d) Visualization of at- tention in two samples. Figure 3: (a) The map of Apple-Gold domain. (b) Average reward of recent 40 episodes. The curves in dark colors are average over 5 curves in light colors. (c) Comparison of trajectories. (d) Attention in the learned trajectory-conditioned policy. The x-axis and y-axis correspond to the state (e.g. agent’s location) in the source sequence (demonstration) and the generated sequence (agent’s new trajectory), respectively. Each cell shows the weight αij of the j-th source state for the i-th target state. Learning [36]; (4) DTRA (“Diverse Trajectory-conditioned Repeat Actions”): we keep a buffer of diverse trajectories and sample the demonstrations as DTSIL, but we simply repeat the action sequence in the demonstration trajectory and then perform random exploration until the episode terminates. More details about the implementation can be found in the appendix. 4.1 Apple-Gold Domain The Apple-Gold domain (Fig. 3a) is a grid-world environment with misleading rewards that can lead the agent to local optima. At the start of each episode, the agent is placed randomly in the left bottom part of the maze. An observation consists of the agent’s location(xt,yt) and binary variables showing whether the agent has gotten the apples or the gold. A state is represented as the agent’s location and the cumulative positive reward indicating the collected objects, i.e. et = (xt,yt,∑ t i=1 max(ri,0)). In Fig. 3b, PPO+EXP achieves the average reward of 4. PPO+EXP agent can explore the environment and occasionally gather the gold to achieve the best episode reward around 8.5. However, it rarely encounters this optimal reward. Thus, this parametric approach might forget the good experience and fails to replicate the best past trajectory to achieve the optimal total reward. Fig. 3b shows that PPO+SIL agent is stuck with the sub-optimal policy of collecting the two apples with a total reward of 2 on average. Fig. 3c visualizes how the trajectories in the memory buffer evolve during the learning process. Obviously, PPO+SIL agent quickly exploits good experiences of collecting the apples and the buffer is ﬁlled with the trajectories in the nearby region. Therefore, the agent only adopts the myopic behavior and fails on this task. In the environment with the random initial location of the agent, repeating the previous action sequences is not sufﬁcient to reach the goal states. The DTRA agent has a difﬁculty in exploring the environment and achieving good performance. Unlike the baseline methods, DTSIL is able to obtain the near-optimal total reward of 8.5. Fig. 3c veriﬁes that DTSIL can generate new trajectories visiting novel states, gradually expand the explored region in the environment, and discover the optimal behavior. A visualization of attention weight in Fig. 3d investigates which states in the demonstration are considered more important when generating the new trajectory. Even though the agent’s random initial location is different from the demonstration, we can see a soft-alignment between the source sequence and the target sequence. The agent tends to pay more attention to states which are several steps away from its current state in the demonstration. Therefore, it is guided by these future states to determine the proper actions to imitate the demonstration. 4.2 Atari Games We evaluate our method on the hard-exploration games in the Arcade Learning Environment [8, 30]. The environment setting is the same as [13]. There is a sticky action [30] resulting in stochasticity in the dynamics. The observation is a frame of raw pixel images, and the state representation et = (roomt,xt,yt,∑ t i=1 max(ri,0)) consists of the agent’s ground truth location (obtained from 6Method DTSIL+EXP PPO+EXP SmartHash NGU* Abstract-HRL IDF A2C+SIL PPO+CoEX RND NGU Agent57 #Frames 3.2B 3.2B 4B 35B 2B 0.1B 0.2B 2B 16B 35B 100B Montezuma 22,616 12,338 6,600 15,000 11,000 2,505 2,500 11,618 10,070 10,400 9,352 Pitfall 12,446 0 - - 10,000 - - - -3 8,400 18,756 Venture 2,011 1,817 - - - 416 0 1,916 1,859 1,700 2,623 Table 1: Comparison with the state-of-the-art results. The top-2 scores for each game are in bold.Abstract-HRL [29] and NGU* (i.e., NGU with hand-crafted controllable states) [5] assume more high-level state information, including the agent’s location, inventory, etc. DTSIL, PPO+EXP [13], and SmartHash [53] only make use of agent’s location information from RAM. IDF [10], A2C+SIL [36], PPO+CoEX [13], RND [11], NGU [5] and Agent57 [4] (a contemporaneous work) do not use RAM information. The score is averaged over multiple runs, gathered from each paper, except PPO+EXP from our implementation. 0M 160M 320M 480M 640M 800M Steps 0 5000 10000 15000 20000 25000 30000Average Reward Montezuma's Revenge PPO+EXP DTRA+Exp DTSIL+EXP 0M 160M 320M 480M 640M 800M Steps 0 50000 100000 150000 200000 250000Best Reward Montezuma's Revenge 0M 160M 320M 480M 640M 800M Steps 0 2500 5000 7500 10000 12500 15000 17500Average Reward Pitfall PPO+Exp DTRA+Exp DTSIL+EXP 0M 160M 320M 480M 640M 800M Steps 0 5000 10000 15000 20000 25000Best Reward Pitfall Figure 4: Learning curves of the average episode reward and the best episode reward found on Montezuma’s Revenge and Pitfall, averaged over 3 runs. More statistics are reported in the appendix. RAM) and the accumulated positive environment reward, which implicitly indicates the objects the agent has collected. It is worth noting that even with the ground-truth location of the agent, on the two infamously difﬁcult games Montezuma’s Revenge and Pitfall, it is highly non-trivial to explore efﬁciently and avoid local optima without relying on expert demonstrations or being able to reset to arbitrary states. Many complicated elements such as moving entities, traps, and the agent’s inventory should be considered in decision-making process. Empirically, as summarized in Tab. 1, the previous SOTA baselines using the agent’s ground truth location information even fail to achieve high scores. Using the state representation et, we introduce a variant ‘DTSIL+EXP’ that adds a count-based exploration bonus r+ t = 1/ √ N(et) to rDTSIL t for faster exploration.1 DTSIL discovers novel states mostly by random exploration after the agent ﬁnishes imitating the demonstration. The pseudo-count bonus brings improvement over random exploration by explicitly encouraging the agent to visit novel states with less count. For a fair comparison, we also include count-based exploration bonus in DTRA. However, with stochasticity in the dynamics, it cannot avoid the dangerous obstacles and fails to reach the goal by just repeating the stored action sequence. Therefore, the performance of DTRA+EXP (Fig. 4) is poor compared to other methods. On Venture (Tab. 1), it is relatively easy to explore and gather positive environment rewards. DTSIL performs only slightly better than the baselines. On Montezuma’s Revenge (Fig. 4), in the early stage, the average episode reward of DTSIL+EXP is worse than PPO+EXP because our policy is trained to imitate diverse demonstrations rather than directly maximize the environment reward. Contrary to PPO+EXP, DTSIL is not eager to follow the myopic path (Fig. 1).2 As training continues, DTSIL+EXP successfully discovers trajectories to pass the ﬁrst level with a total reward more than 20,000. As we sample the best trajectories in the buffer as demonstrations, the average episode reward increases to surpass 20,000 in Fig. 4. On Pitfall, positive rewards are much sparser and most of the actions yield small negative rewards (time step penalty) that would discourage getting a high total reward in the long term. However, DTSIL+EXP stores trajectories with negative rewards, encourages the agent to visit these novel regions, discovers good paths with positive rewards and eventually attains an average episode reward over 0. In Fig. 4, different performances under different random seeds are due to huge positive rewards in some states on Montezuma’s Revenge and Pitfall. Once the agent luckily ﬁnds these states in some runs, DTSIL can exploit them and perform much better than other runs. 1The existing exploration methods listed in Table 1 take advantage of count-based exploration bonus (e.g., SmartHash, Abstract-HRL and PPO+CoEX). Therefore, a combination of DTSIL and the count-based exploration bonus does not introduce unfair advantages over other baselines. 2Demo videos of the learned policies for both PPO+EXP and DTSIL+EXP are available at https://sites. google.com/view/diverse-sil. In comparison to DTSIL+EXP, we can see the PPO+EXP agent does not explore enough to make best use of the tools (e.g. sword, key) collected in the game. 7(a)  (b) 0M 1.6M 3.2M 4.8M 6.4M 8M Steps 0 2 4 6 8Average Reward Nav A3C+D1D2L PPO+EXP DTSIL (c)  (d) 0M 4M 8M 12M 16M 20M Steps 0 20 40 60 80Average Reward DTSIL PPO+EXP (e) 0M 4M 8M 12M 16M 20M Steps 0 200 400 600 800Best Reward (f) Figure 5: (a) Indoor scene for navigation task. (b) A panoramic view from a speciﬁc viewpoint. (c) Learning curves of average reward on navigation task. (d) Bin picking. (e) Learning curves of average reward on manipulation task. (f) Learning curves of best reward on manipulation task. 4.3 Continuous Control Tasks When the initial condition is highly random, previous works imitating expert demonstrations (e.g. [3]) would also struggle. We slightly modify DTSIL to handle the highly random initial states: in each episode, from buffer D, we sample the demonstration trajectory with a start state similar to the current episode. The detailed algorithm is described in the supplementary material. Navigation We focus on a more realistic environment, a distant visual navigation task designed on Gibson dataset [ 55]. To make the task more challenging, the agent is randomly placed in the environment (red rectangle in Fig. 5a), a positive reward 10 is given only when it reaches a ﬁxed target location (green point in Fig. 5a) which is signiﬁcantly further away. The agent receives no information about the target (such as the target location or image) in advance. The observation is a ﬁrst-person view RGB image and the state embedding is the agent’s location and orientation (which is usually available in robotics navigation tasks) and the cumulative reward. This experiment setting is similar to the navigation task with a static goal deﬁned in [31]. Apart from the baseline PPO+EXP, we also compare with Nav A3C+D1D2L [31], which uses the agent’s location and RGB and depth image. This method performs well in navigation tasks on DeepMind Lab where apples with small positive rewards are randomly scattered to encourage exploration, but on our indoor navigation task, it fails to discover the distant goal without the exploration bonus. Fig. 5c shows that Nav A3C+D1D2L can never reach the target. PPO+EXP, as a parametric approach, is sample-inefﬁcient and fails to quickly exploit the previous successful experiences. However, DTSIL agent can successfully explore to ﬁnd the target and gradually imitate the best trajectories of reward 10 to replicate the good behavior. Manipulation Bin picking is one of the hardest tasks in Surreal Robotics Suite [18]. Fig. 5d shows the bin picking task with a single object, where the goal is to pick up the cereal and place it into the left bottom bin. With carefully designed dense rewards (i.e. positive rewards at each step when the robot arm moving near the object, touching it, lifting it, hovering it over the correct bin, or successfully placing it), the PPO agent can pick up, move and drop the object [ 18]. We instead consider a more challenging scenario with sparse rewards. The reward is 0.5 at the single step of picking up the object, -0.5 if the object is dropped in the wrong place, 1 at each step when the object keeps in the correct bin. The observation is the physical state of the robot arm and the object. The state embedding consists of the position of the object and gripper, a variable about whether the gripper is open, and cumulative environment reward. Each episode terminates at 1000 steps. In Fig. 5f, PPO+EXP agent never discovers a successful trajectory with episode reward over 0, because the agent has difﬁculty in lifting the cereal and easily drops it by mistake. In contrast, DTSIL imitates the trajectories lifting the object, explores to move the cereal over the bins, ﬁnds trajectories successfully placing the object, exploits the high-rewarding trajectories, and obtains a higher average reward than the baseline (Fig. 5e). 4.4 Other Domains: Deep Sea and Mujoco Maze In the supplementary material, we present additional details of the experimental results and also the experiments on other interesting domains. On Deep Sea[37], we show that the advantage of DTSIL becomes more obvious when the state space becomes larger and rewards become sparser. OnMujoco Maze [15, 33], we show that DTSIL helps avoid sub-optimal behavior in continuous action space. 5 Discussions: Robustness and Generalization of DTSIL 5.1 Robustness of DTSIL with Learned State Representations Learning a good state representation is an important open question and extremely challenging especially for long-horizon, sparse-reward environments, but it is not the main focus of this work. However, we ﬁnd that DTSIL can be combined with existing approaches of state representation learning if the high-level state embedding is not available. When the quality of the learned state 80M 240M 480M 720M 960M 1200M Steps 0 10000 20000 30000 40000Average Reward Montezuma's Revenge DTSIL+EXP (a)  (b) 0M 8M 16M 24M 32M 40M Steps 0 2 4 6 8Average Reward PPO+EXP DTSIL (c) 0M 8M 16M 24M 32M 40M Steps 0 1 2 3 4Average Reward PPO+EXP DTSIL (d) Figure 6: (a) Experiment with learned state representation. (b) Two samples of the random maze structure in Apple-Gold domain. (c) Learning curves of average episode reward on the training set of mazes in Apple-Gold domain. (d) Average episode reward on the test set of mazes for generalization experiment. representation is not satisfactory (e.g., on Montezuma’s Revenge, [13] fails to differentiate the dark rooms at the last ﬂoor), the trajectory-conditioned policy might be negatively inﬂuenced by the inaccuracy in eg i or ei. Thus, we modify DTSIL to handle this difﬁculty by feeding sequences of observations (instead of sequences of learned state embeddings) into the trajectory-conditioned policy. The learned state embeddings are merely used to cluster states when counting state visitation or determining whether to provide imitation reward rim. Then DTSIL becomes more robust to possible errors in the learned embeddings. With the learned state representation from [13], on Montezuma’s Revenge, DTSIL+EXP reaches the second level of the maze with a reward >20,000 (Fig 6a). 5.2 Robustness of DTSIL in Stochastic Environments In the single-task RL problem, a Markov Decision Process (MDP) is deﬁned by a state set S, an action set A, an initial state distribution p(s0), a state transition dynamics model p(st+1|st,at), a reward function r(st,at) and a discount factor γ. So the environment stochasticity falls into three categories: stochasticity in the initial state distribution, stochasticity in the transition function, and stochasticity in the reward function. For sparse-reward, long-horizon tasks, if the precious reward signals are unstable, the problem would be extremely difﬁcult to solve. Thus, in this paper, we mainly focus on the other two categories of stochasticity. In Sec. 4.2 & 4.3, we show the efﬁciency and robustness of DTSIL in the environment with sticky action (i.e. stochasticity in p(st+1|st,at)) or highly random initial states (i.e. stochasticity in p(s0)). 5.3 Generalization Ability of DTSIL While many previous works about exploration focus on the single-task RL problem with a single MDP [53, 13, 16], we step further to extend DTSIL for the multiple MDPs, where every single task is in a stochastic environment with local optima. For example, in the Apple-Gold domain, we design 12 different structures of the maze as a training set (Fig. 6b). In each episode, the structure of maze is randomly sampled and the location of agent and gold is randomized in a small region. If the structure in the demonstration is different from the current episode, DTSIL agent might fail to recover the state of interest by roughly following the demonstration. Thus, using the buffer of diverse trajectories, we alternatively learn a hierarchical policy, which can behave with higher ﬂexibility in the random mazes to reach the sampled states. We design the rewards so that the high-level policy is encouraged to propose the appropriate sub-goals (i.e., agent’s locations) sequentially to maximize the environment reward and goal-achieving bonus (i.e. positive reward when the low-level policy successfully reaches the long-term goal sampled from the buffer). The low-level policy learns to visit sub-goals given the current observation (i.e. RGB image of the maze). The diverse trajectories in the buffer are also used with a supervised learning objective to improve policy learning. Fig. 6c shows that the hierarchical policy outperforms PPO+EXP during training. When evaluated on 6 unseen mazes in the test set, it can generalize the good behavior to some unseen environments (Fig. 6d). More details of the algorithm and experiments are in the supplementary material. Solving multi-task RL is a challenging open problem [35, 14, 44]. Here we veriﬁed this variant of DTSIL is promising and the high-level idea of DTSIL to leverage and augment diverse past trajectories can help exploration in this scenario. We leave the study of improving DTSIL furthermore as future work. 6 Conclusion This paper proposes to learn diverse policies by imitating diverse trajectory-level demonstrations through count-based exploration over these trajectories. Imitation of diverse past trajectories can guide the agent to rarely visited states and encourages further exploration of novel states. We show that in a variety of stochastic environments with local optima, our method signiﬁcantly improves count-based exploration method and self-imitation learning. It avoids prematurely converging to a myopic solution and learns a near-optimal behavior to achieve a high total reward. 9Broader Impact DTSIL is likely to be useful in real-world RL applications, such as robotics-related tasks. Compared with previous exploration methods, DTSIL shows obvious advantages when the task requires rea- soning over long-horizon and the feedback from environment is sparse. We believe RL researchers and practitioners can beneﬁt from DTSIL to solve RL application problems requiring efﬁcient explo- ration. Especially, DTSIL helps avoid the cost of collecting human demonstration and the manual engineering burden of designing complicated reward functions. Also, as we discussed in Sec. 5, when deployed for more problems in the future, DTSIL has a good potential to perform robustly and avoid local optima in various stochastic environments when combined with other state representation learning approaches. DTSIL in its current form is applied to robotics tasks in the simulated environments. And it likely contributes to real robots in solving hard-exploration tasks in the future. Advanced techniques in robotics make it possible to eliminate repetitive, time-consuming, or dangerous tasks for human workers and might bring positive societal impacts. For example, the advancement in household robots will help reduce the cost for home care and beneﬁt people with disability or older adults who needs personalized care for a long time. However, it might cause negative consequences such as large-scale job disruptions at the same time. Thus, proper public policy is required to reduce the social friction. On the other hand, RL method without much reward shaping runs the risk of taking a step that is harmful for the environments. This generic issue faced by most RL methods is also applicable to DTSIL. To mitigate this issue, given any speciﬁc domain, one simple solution is to apply a constraint on the state space that we are interested to reach during exploration. DTSIL is complementary to the mechanisms to restrict the state space or action space. More principled way to ensure safety during exploration is a future work. In addition to AI safety, another common concern for most RL algorithms is the memory and computational cost. In the supplementary material we discuss how to control the size of the memory for DTSIL and report the cost. Empirically DTSIL provides ideas for solving various hard-exploration tasks with a reasonable computation cost. Acknowledgments and Disclosure of Funding This work was supported in part by NSF grant IIS-1526059 and Korea Foundation for Advanced Studies. Any opinions, ﬁndings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reﬂect the views of the sponsor. References [1] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin, O. P. Abbeel, and W. Zaremba. Hindsight experience replay. InAdvances in Neural Information Processing Systems, pages 5048–5058, 2017. [2] P. Auer. Using conﬁdence bounds for exploitation-exploration trade-offs. Journal of Machine Learning Research, 3(Nov):397–422, 2002. [3] Y . Aytar, T. Pfaff, D. Budden, T. Paine, Z. Wang, and N. de Freitas. Playing hard exploration games by watching youtube. In Advances in Neural Information Processing Systems, pages 2930–2941, 2018. [4] A. P. Badia, B. Piot, S. Kapturowski, P. Sprechmann, A. Vitvitskyi, D. Guo, and C. Blundell. Agent57: Outperforming the atari human benchmark. arXiv preprint arXiv:2003.13350, 2020. [5] A. P. Badia, P. Sprechmann, A. Vitvitskyi, D. Guo, B. Piot, S. Kapturowski, O. Tieleman, M. Arjovsky, A. Pritzel, A. Bolt, et al. Never give up: Learning directed exploration strategies. arXiv preprint arXiv:2002.06038, 2020. [6] D. Bahdanau, K. Cho, and Y . Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014. [7] M. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, and R. Munos. Unifying count-based exploration and intrinsic motivation. InAdvances in Neural Information Processing Systems, pages 1471–1479, 2016. 10[8] M. G. Bellemare, Y . Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279, 2013. [9] J. Bornschein, A. Mnih, D. Zoran, and D. J. Rezende. Variational memory addressing in generative models. In Advances in Neural Information Processing Systems, pages 3920–3929, 2017. [10] Y . Burda, H. Edwards, D. Pathak, A. Storkey, T. Darrell, and A. A. Efros. Large-scale study of curiosity-driven learning. arXiv preprint arXiv:1808.04355, 2018. [11] Y . Burda, H. Edwards, A. Storkey, and O. Klimov. Exploration by random network distillation. arXiv preprint arXiv:1810.12894, 2018. [12] N. Chentanez, A. G. Barto, and S. P. Singh. Intrinsically motivated reinforcement learning. In Advances in neural information processing systems, pages 1281–1288, 2005. [13] J. Choi, Y . Guo, M. Moczulski, J. Oh, N. Wu, M. Norouzi, and H. Lee. Contingency-aware exploration in reinforcement learning. arXiv preprint arXiv:1811.01483, 2018. [14] C. Devin, A. Gupta, T. Darrell, P. Abbeel, and S. Levine. Learning modular neural network policies for multi-task and multi-robot transfer. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pages 2169–2176. IEEE, 2017. [15] Y . Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel. Benchmarking deep reinforcement learning for continuous control. In International Conference on Machine Learning, pages 1329– 1338, 2016. [16] A. Ecoffet, J. Huizinga, J. Lehman, K. O. Stanley, and J. Clune. Go-explore: a new approach for hard-exploration problems. arXiv preprint arXiv:1901.10995, 2019. [17] B. Eysenbach, A. Gupta, J. Ibarz, and S. Levine. Diversity is all you need: Learning skills without a reward function. arXiv preprint arXiv:1802.06070, 2018. [18] L. Fan, Y . Zhu, J. Zhu, Z. Liu, O. Zeng, A. Gupta, J. Creus-Costa, S. Savarese, and L. Fei-Fei. Surreal: Open-source reinforcement learning framework and robot manipulation benchmark. In Conference on Robot Learning, 2018. [19] T. Gangwani, Q. Liu, and J. Peng. Learning self-imitating diverse policies. arXiv preprint arXiv:1805.10309, 2018. [20] K. Gregor, D. J. Rezende, and D. Wierstra. Variational intrinsic control. arXiv preprint arXiv:1611.07507, 2016. [21] K. Guu, T. B. Hashimoto, Y . Oren, and P. Liang. Generating sentences by editing prototypes. Transactions of the Association for Computational Linguistics, 6:437–450, 2018. [22] S. Hansen, A. Pritzel, P. Sprechmann, A. Barreto, and C. Blundell. Fast deep reinforcement learning using online adjustments from the past. In Advances in Neural Information Processing Systems, pages 10567–10577, 2018. [23] T. Hester, M. Vecerik, O. Pietquin, M. Lanctot, T. Schaul, B. Piot, D. Horgan, J. Quan, A. Sendonaris, I. Osband, et al. Deep q-learning from demonstrations. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018. [24] L. Kaiser, M. Babaeizadeh, P. Milos, B. Osinski, R. H. Campbell, K. Czechowski, D. Erhan, C. Finn, P. Kozakowski, S. Levine, et al. Model-based reinforcement learning for atari. arXiv preprint arXiv:1903.00374, 2019. [25] S. Kapturowski, G. Ostrovski, J. Quan, R. Munos, and W. Dabney. Recurrent experience replay in distributed reinforcement learning. In International conference on learning representations, 2018. [26] T. D. Kulkarni, A. Gupta, C. Ionescu, S. Borgeaud, M. Reynolds, A. Zisserman, and V . Mnih. Unsupervised learning of object keypoints for perception and control. In Advances in neural information processing systems, pages 10724–10734, 2019. [27] C. Liang, M. Norouzi, J. Berant, Q. V . Le, and N. Lao. Memory augmented policy optimization for program synthesis and semantic parsing. In Advances in Neural Information Processing Systems, pages 9994–10006, 2018. [28] Z. Lin, T. Zhao, G. Yang, and L. Zhang. Episodic memory deep q-networks. arXiv preprint arXiv:1805.07603, 2018. 11[29] E. Z. Liu, R. Keramati, S. Seshadri, K. Guu, P. Pasupat, E. Brunskill, and P. Liang. Learning abstract models for long-horizon exploration, 2019. URLhttps://openreview.net/forum? id=ryxLG2RcYX. [30] M. C. Machado, M. G. Bellemare, E. Talvitie, J. Veness, M. Hausknecht, and M. Bowling. Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. Journal of Artiﬁcial Intelligence Research, 61:523–562, 2017. [31] P. Mirowski, R. Pascanu, F. Viola, H. Soyer, A. J. Ballard, A. Banino, M. Denil, R. Goroshin, L. Sifre, K. Kavukcuoglu, et al. Learning to navigate in complex environments. arXiv preprint arXiv:1611.03673, 2016. [32] V . Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforcement learning. Nature, 2015. [33] O. Nachum, S. S. Gu, H. Lee, and S. Levine. Data-efﬁcient hierarchical reinforcement learning. In Advances in Neural Information Processing Systems, pages 3303–3313, 2018. [34] A. Nair, D. Chen, P. Agrawal, P. Isola, P. Abbeel, J. Malik, and S. Levine. Combining self-supervised learning and imitation for vision-based rope manipulation. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pages 2146–2153. IEEE, 2017. [35] J. Oh, S. Singh, H. Lee, and P. Kohli. Zero-shot task generalization with multi-task deep reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 2661–2670. JMLR. org, 2017. [36] J. Oh, Y . Guo, S. Singh, and H. Lee. Self-imitation learning.arXiv preprint arXiv:1806.05635, 2018. [37] I. Osband, Y . Doron, M. Hessel, J. Aslanides, E. Sezener, A. Saraiva, K. McKinney, T. Lattimore, C. Szepesvári, S. Singh, B. Van Roy, R. Sutton, D. Silver, and H. van Hasselt. Behaviour suite for reinforcement learning. 2019. [38] G. Ostrovski, M. G. Bellemare, A. van den Oord, and R. Munos. Count-based exploration with neural density models. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 2721–2730. JMLR. org, 2017. [39] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell. Curiosity-driven exploration by self- supervised prediction. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 16–17, 2017. [40] D. Pathak, P. Mahmoudieh, G. Luo, P. Agrawal, D. Chen, Y . Shentu, E. Shelhamer, J. Malik, A. A. Efros, and T. Darrell. Zero-shot visual imitation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 2050–2053, 2018. [41] T. Pohlen, B. Piot, T. Hester, M. G. Azar, D. Horgan, D. Budden, G. Barth-Maron, H. van Hasselt, J. Quan, M. Veˇcerík, et al. Observe and look further: Achieving consistent performance on atari. arXiv preprint arXiv:1805.11593, 2018. [42] V . H. Pong, M. Dalal, S. Lin, A. Nair, S. Bahl, and S. Levine. Skew-ﬁt: State-covering self-supervised reinforcement learning. arXiv preprint arXiv:1903.03698, 2019. [43] A. Pritzel, B. Uria, S. Srinivasan, A. P. Badia, O. Vinyals, D. Hassabis, D. Wierstra, and C. Blundell. Neural episodic control. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 2827–2836. JMLR. org, 2017. [44] T. Schaul, D. Horgan, K. Gregor, and D. Silver. Universal value function approximators. In International conference on machine learning, pages 1312–1320, 2015. [45] T. Schaul, J. Quan, I. Antonoglou, and D. Silver. Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015. [46] J. Schmidhuber. Adaptive conﬁdence and adaptive curiosity. Technical report, Citeseer, 1991. [47] J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez, E. Lock- hart, D. Hassabis, T. Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. arXiv preprint arXiv:1911.08265, 2019. [48] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 12[49] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V . Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484, 2016. [50] A. L. Strehl and M. L. Littman. An analysis of model-based interval estimation for markov decision processes. Journal of Computer and System Sciences, 74(8):1309–1331, 2008. [51] I. Sutskever, O. Vinyals, and Q. V . Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104–3112, 2014. [52] R. S. Sutton, D. A. McAllester, S. P. Singh, and Y . Mansour. Policy gradient methods for rein- forcement learning with function approximation. In Advances in neural information processing systems, pages 1057–1063, 2000. [53] H. Tang, R. Houthooft, D. Foote, A. Stooke, O. X. Chen, Y . Duan, J. Schulman, F. DeTurck, and P. Abbeel. # exploration: A study of count-based exploration for deep reinforcement learning. In Advances in neural information processing systems, pages 2753–2762, 2017. [54] D. Warde-Farley, T. Van de Wiele, T. Kulkarni, C. Ionescu, S. Hansen, and V . Mnih. Unsuper- vised control through non-parametric discriminative rewards. arXiv preprint arXiv:1811.11359, 2018. [55] F. Xia, A. R. Zamir, Z. He, A. Sax, J. Malik, and S. Savarese. Gibson env: Real-world perception for embodied agents. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9068–9079, 2018. [56] Y . Zhang, W. Yu, and G. Turk. Learning novel policies for tasks. arXiv preprint arXiv:1905.05252, 2019. 13",
      "meta_data": {
        "arxiv_id": "1907.10247v3",
        "authors": [
          "Yijie Guo",
          "Jongwook Choi",
          "Marcin Moczulski",
          "Shengyu Feng",
          "Samy Bengio",
          "Mohammad Norouzi",
          "Honglak Lee"
        ],
        "published_date": "2019-07-24T05:46:27Z",
        "pdf_url": "https://arxiv.org/pdf/1907.10247v3.pdf"
      }
    },
    {
      "title": "Replay Memory as An Empirical MDP: Combining Conservative Estimation with Experience Replay"
    },
    {
      "title": "Fuzzy Tiling Activations: A Simple Approach to Learning Sparse Representations Online",
      "abstract": "Recent work has shown that sparse representations -- where only a small\npercentage of units are active -- can significantly reduce interference. Those\nworks, however, relied on relatively complex regularization or meta-learning\napproaches, that have only been used offline in a pre-training phase. In this\nwork, we pursue a direction that achieves sparsity by design, rather than by\nlearning. Specifically, we design an activation function that produces sparse\nrepresentations deterministically by construction, and so is more amenable to\nonline training. The idea relies on the simple approach of binning, but\novercomes the two key limitations of binning: zero gradients for the flat\nregions almost everywhere, and lost precision -- reduced discrimination -- due\nto coarse aggregation. We introduce a Fuzzy Tiling Activation (FTA) that\nprovides non-negligible gradients and produces overlap between bins that\nimproves discrimination. We first show that FTA is robust under covariate shift\nin a synthetic online supervised learning problem, where we can vary the level\nof correlation and drift. Then we move to the deep reinforcement learning\nsetting and investigate both value-based and policy gradient algorithms that\nuse neural networks with FTAs, in classic discrete control and Mujoco\ncontinuous control environments. We show that algorithms equipped with FTAs are\nable to learn a stable policy faster without needing target networks on most\ndomains.",
      "full_text": "Published as a conference paper at ICLR 2021 FUZZY TILING ACTIVATIONS : A S IMPLE APPROACH TO LEARNING SPARSE REPRESENTATIONS ONLINE Yangchen Pan University of Alberta pan6@ualberta.ca Kirby Banman University of Alberta kdbanman@ualberta.ca Martha White University of Alberta whitem@ualberta.ca ABSTRACT Recent work has shown that sparse representations—where only a small percentage of units are active—can signiﬁcantly reduce interference. Those works, however, relied on relatively complex regularization or meta-learning approaches, that have only been used ofﬂine in a pre-training phase. In this work, we pursue a direction that achieves sparsity by design, rather than by learning. Speciﬁcally, we design an activation function that produces sparse representations deterministically by construction, and so is more amenable to online training. The idea relies on the simple approach of binning, but overcomes the two key limitations of binning: zero gradients for the ﬂat regions almost everywhere, and lost precision—reduced discrimination—due to coarse aggregation. We introduce a Fuzzy Tiling Activation (FTA) that provides non-negligible gradients and produces overlap between bins that improves discrimination. We ﬁrst show that FTA is robust under covariate shift in a synthetic online supervised learning problem, where we can vary the level of correlation and drift. Then we move to the deep reinforcement learning setting and investigate both value-based and policy gradient algorithms that use neural networks with FTAs, in classic discrete control and Mujoco continuous control environments. We show that algorithms equipped with FTAs are able to learn a stable policy faster without needing target networks on most domains. 1 1 I NTRODUCTION Representation learning in online learning systems can strongly impact learning efﬁciency, both positively due to generalization but also negatively due to interference (Liang et al., 2016; Heravi, 2019; Le et al., 2017; Liu et al., 2019; Chandak et al., 2019; Caselles-Dupré et al., 2018; Madjiheurem & Toni, 2019). Neural networks particularly suffer from interference—where updates for some inputs degrade accuracy for others—when training on temporally correlated data (McCloskey & Cohen, 1989; French, 1999; Kemker et al., 2018). Recent work (Liu et al., 2019; Ghiassian et al., 2020; Javed & White, 2019; Rafati & Noelle, 2019; Hernandez-Garcia & Sutton, 2019), as well as older work (McCloskey & Cohen, 1989; French, 1991), have shown that sparse representation can reduce interference in training parameter updates. A sparse representation is one where only a small number of features are active, for each input (Cheng et al., 2013). Each update only impacts a small number of weights and so is less likely to interfere with many state values. Further, when constrained to learn sparse features, the feature vectors are more likely to be orthogonal (Cover, 1965), which further mitigates interference. The learned features can still be highly expressive, and even more interpretable, as only a small number of attributes are active for a given input. However, learning sparse representations online remains relatively open. Some previous work has relied on representations pre-trained before learning, either with regularizers that encourage sparsity (Tibshirani, 1996; Xiang et al., 2011; Liu et al., 2019) or with meta-learning (Javed & White, 2019). Other work has trained the sparse-representation neural network online, by using sparsity regularizers online with replay buffers (Hernandez-Garcia & Sutton, 2019) or using a winner-take-all strategy where all but the top activations are set to zero (Rafati & Noelle, 2019). Hernandez-Garcia & 1Code is available at https://github.com/yannickycpan/reproduceRL.git 1 arXiv:1911.08068v3  [cs.LG]  16 Mar 2021Published as a conference paper at ICLR 2021 Sutton (2019) found that many of these sparsity regularizers were ineffective for obtaining sparse representations without high levels of dead neurons, though the regularizers did still often improve learning. The Winner-Take-All (WTA) approach is non-differentiable, and there are mixed results on it’s efﬁcacy, some positive (Rafati & Noelle, 2019) and some negative (Liu et al., 2019). Finally, kernel representations can be used online, and when combined with a WTA approach, provide sparse representations. There is some evidence that using only the closest prototypes—and setting kernel values to zero for the further prototypes—may not hurt approximation quality (Schlegel et al., 2017). However, kernel-based methods can be difﬁcult to scale to large problems, due to computation and difﬁculties in ﬁnding a suitable distance metric. Providing a simpler approach to obtain sparse representations, that are easy to train online, would make it easier for researchers from the broad online learning community to adopt sparse representations and further explore their utility. In this work, we pursue a strategy for what we call natural sparsity—an approach where we achieve sparsity by design rather than by encoding sparsity in the loss. We introduce an activation function that facilitates sparse representation learning in an end-to-end manner without the need of additional losses, pre-training or manual truncation. Speciﬁcally, we introduce a Fuzzy Tiling Activation (FTA) function that naturally produce sparse representation with controllable sparsity and can be conveniently used like other activation functions in a neural network. FTA relies on the idea of designing a differentiable approximate binning operation—where inputs are aggregated into intervals. We prove that the FTA guarantees sparsity by construction. We empirically investigate the properties of FTA in an online supervised learning problem, where we can carefully control the level of correlation. We then empirically show FTA’s practical utility in a more challenging online learning setting—the deep Reinforcement Learning (RL) setting. On a variety of discrete and continuous control domains, deep RL algorithms using FTA can learn more quickly and stably compared to both those using ReLU activations and several online sparse representation learning approaches. 2 P ROBLEM FORMULATION FTA is a generic activation that can be applied in a variety of settings. A distinct property of FTA is that it does not need to learn to ensure sparsity; instead, it provides an immediate, deterministic sparsity guarantee. We hypothesize that this property is suitable for handling highly nonstationary data in an online learning setting, where there is highly correlated data stream and a strong need for interference reduction. We therefore explicitly formalize two motivating problems: the online supervised learning problem and the reinforcement learning (RL) problem. Online Supervised Learning problem setting. The agent observes a temporally correlated stream of data, generated by a stochastic process {(Xt,Yt)}t∈N, where the observations Xt depend on the past {Xt−i}i∈N. In our supervised setting, Xt depends only on Xt−1, and the target Yt depends only on Xt according to a stationary underlying mean function f(x) = E[Yt|Xt = x]. On each time step, the agent observes Xt, makes a prediction fθ(Xt) with its parameterized function fθ, receives target Yt and incurs a prediction error. The goal of the agent is to approximate function f—the ideal predictor—by learning from correlated data in an online manner, unlike standard supervised learning where data is independent and identically distributed (iid). RL problem setting. We formalize the interaction using Markov decision processes (MDPs). An MDP consists of (S,A,P,R,γ ), where Sis the state space, Ais the action space, P is the transition probability kernel, R is the reward function, and γ ∈[0,1] is the discount factor. At each time step t = 1,2,... , the agent observes a state st ∈S and takes an action at ∈A. Then the environment transits to the next state according to the transition probability distribution, i.e., st+1 ∼ P(·|st,at), and the agent receives a scalar reward rt+1 ∈ R according to the reward function R : S×A×S → R. A policy is a mapping from a state to an action (distribution) π: S×A→ [0,1]. For a given state-action pair (s,a), the action-value function under policy πis deﬁned as Qπ(s,a) = E[Gt|St = s,At = a; At+1:∞∼π] where Gt def = ∑∞ t=0 γtR(st,at,st+1) is the return of a sequence of transitions s0,a0,s1,a1,... by following the policy π. The goal of an agent is to ﬁnd an optimal policy that obtains maximal expected return from each state. The policy is either directly learned, as in policy gradient methods (Sutton et al., 1999; Sutton & Barto, 2018), or the action-values are learned and the policy inferred by acting greedily with respect to the action-values, as in Q-learning (Watkins & Dayan, 1992). In either setting, we often parameterize the policy/value function by a neural network (NN). For example, Deep QNetworks (DQN) (Mnih 2Published as a conference paper at ICLR 2021 et al., 2015) parameterizes the action-value function Qθ : S×A↦→ R by a NN. The bootstrap target for updating a state-action value is computed by using a separate target NN Qθ− : S×A↦→ R parameterized by θ−: yt = rt+1 + γmaxa′Qθ−(st+1,a′). The target NN parameter θ−is updated by copying from θevery certain number of time steps. Online deep RL control problems can be highly nonstationary, for two primary reasons. First, the environment itself could be highly nonstationary, or alternatively, partially observable. Second, the data distribution is constantly shifting because of both the changing policy and shifting training targets. The latter can be mitigated by using a target NN as described above, which has become critical in successfully training many deep RL algorithms. However, it potentially slows learning as the new information is not immediately used to update action-values; instead, the slower moving and potentially out-dated target NN is used. Several works reported that successful training without a target NN can improve sample efﬁciency of RL algorithms (Liu et al., 2019; van Hasselt et al., 2018; Fan et al., 2020; Kim et al., 2019; Rafati & Noelle, 2019; Fan et al., 2020; Ghiassian et al., 2020). We show that deep RL algorithms using our activation is able to achieve superior performance without using a target NN, indicating the beneﬁt of applying our method to nonstationary, online problems. 3 B INNING WITH NON-NEGLIGIBLE GRADIENTS In this section, we develop the Fuzzy Tiling Activation (FTA), as a new modular component for neural networks that provides sparse representations. We ﬁrst introduce a new way to compute the binning of an input, using indicator functions. This activation provides guaranteed sparsity but has a gradient of zero almost everywhere. Then, we provide a smoothed version, resulting in non-negligible gradients that make it compatible with back-propagation algorithms. We then prove that the fuzzy version is still guaranteed to provide sparse representation and the sparsity can be easily tuned. 3.1 T ILING ACTIVATION The tiling activation inputs a scalar z and outputs a binned vector. This vector is one-hot, with a 1 in the bin corresponding to the value of z, and zeros elsewhere. Note that a standard activation typically maps a scalar to a scalar. However, the tiling activation maps a scalar to a vector, as depicted in Figure 1(a). This resembles tile coding, which inspires the name Tiling Activation; to see this connection, we include a brief review of tile coding in the Appendix A.1. In this section, we show how to write the tiling activation compactly, using element-wise max and indicator functions. z h1 h2 h3 h4 (a) TA, k= 4 z h1 h2 h3 h4 (b) FTA, k= 4,η = 0.1 z h1 h2 h3 h4 (c) FTA, k= 4,η = 0.25 Figure 1: a) The regular TA mapping R →Rk, with each output element hi corresponds to a different bin. b) The FTA with η >0, permitting both overlap in activation, and nonzero gradient between the vertical red and gray lines. c) Larger values for ηextends the sloped lines further from either side of each plateau, increasing the region that has non-negligible gradients. Assume we are given a range [l,u] for constants l,u ∈R, where we expect the input z∈[l,u]. The goal is to convert the input, to a one-hot encoding, with evenly spaced bins of size δ∈R+. Without loss of generality, we assume that u−lis evenly divisible by δ; if it is not, the range [l,u] could be slightly expanded, evenly on each side, to ensure divisibility. Deﬁne the k-dimensional tiling vector c def = (l,l + δ,l + 2δ,...,u −2δ,u −δ). (1) where k= (u−l)/δ. The tiling activationis deﬁned as φ(z) def = 1 −I+(max(c −z,0) + max(z−δ−c,0)) (2) where I+(·) is an indicator function, which returns 1 if the input is positive, and zero otherwise. The indicator function for vectors is applied element-wise. In Proposition 1, we prove that φ returns a 3Published as a conference paper at ICLR 2021 binned encoding: if ci <z <ci+1, then φ(z) returns ei the one-hot (standard basis) vector with a 1 in the i-th entry and zero elsewhere. For values of zthat fall on the boundary, z= ci, the encoding returns a vector with ones in both thei−1th and ith entries. Consider the below example for intuition. Example. Assume [l,u] = [0 ,1] and set the tile width to δ = 0.25. Then the tiling vector c has four tiles (k = 4): c = (0,0.25,0.5,0.75). If we apply the tiling activation to z = 0.3, because 0.25 <0.3 <0.5, the output should be (0,1,0,0). To see φ(z) does in fact return this vector, we compute each max term max(c −z,0) = (0,0,0.2,0.45) and max(z−δ−c,0) = max(0.05 −c,0) = (0.05,0,0,0). The addition of the two is (0.05,0,0.2,0.45) and so 1 −I+(0.05,0,0.2,0.45) = 1 −(1,0,1,1) = (0,1,0,0). The ﬁrst max extracts those components in c that are strictly greater than z, and the second max extracts those strictly less than z. The addition gives the bins that are strictly greater and strictly less than the bin for z, leaving only the entry corresponding to that activated bin as 0, with all others positive. The indicator function sets all nonzero entries to one and then using one minus this indicator function’s output provides us the desired binary encoding. We rigorously characterize the possible output cases for the activation in the Appendix A.2.1. 3.2 F UZZY TILING ACTIVATION (FTA) The Tiling Activation provides a way to obtain sparse, binary encodings for features learned within a NN. Unfortunately, the tiling activation has a zero derivative almost everywhere as visualized in Figure 1(a). In this section, we provide a fuzzy tiling activation, that has non-zero derivatives and so is amenable to use with backpropagation. To design the FTA, we deﬁne the fuzzy indicator function2 Iη,+(x) def = I+(η−x)x+ I+(x−η) (3) where ηis a small constant for controlling the sparsity. The ﬁrst term I+(η−x) is 1 if x<η , and 0 otherwise. The second term I+(x−η) is 1 if x>η , and 0 otherwise. If x<η , then Iη,+(x) = x, and else Iη,+(x) = 1. The original indicator function I+ can be acquired by setting η= 0. When η >0, the derivative is non-zero for x < η, and zero otherwise. Hence the derivative can be propagated backwards through those nonzero entries. Using this fuzzy indicator function, we deﬁne the following Fuzzy Tiling Activation(FTA) φη(z) def = 1 −Iη,+(max(c −z,0) + max(z−δ−c,0)) (4) where again Iη,+ is applied elementwise. We depict FTA with differentηs in Figure 3.1. For the smaller η, the FTA extends the activation to the neighbouring bins. The activation in these neighbouring bins is sloped, resulting in non-zero deriva- tives. For this smaller η, however, there are still regions where the derivative is zero (e.g.,z= 0.3 in Figure 1(b)). The regions where derivatives are non-zero can be expanded by increasing ηas shown in Figure 1(c). Hence we can adjust ηto control the sparsity level as we demonstrate in Section A.5. F F Figure 2: A visualization of an FTA layer Figure 2 shows a neural network with FTA applied to the second hidden layer and its output yis linear in the sparse representation. FTA itself does not introduce any new train- ing parameters, just like other activation func- tions. For input x, after computing ﬁrst layer h1 = xW1, we apply φη(z) to h1W2 ∈Rd to get the layer h2 of size kd. This layer consists of stacking the k-dimensional sparse encodings, for each element in h1W2. 3.3 G UARANTEED SPARSITY FROM THE FTA We now show that the FTA maintains one of the key properties of the tiling activation: sparsity. The distinction with many existing approaches is that our sparsity is guaranteed by design and hence is 2The word fuzzy reﬂects that an input can partially activate a tile, with a lower activation than 1, as an analogy to the concept of partial inclusion and degrees of membership from fuzzy sets. 4Published as a conference paper at ICLR 2021 not a probabilistic guarantee. We ﬁrst characterize the vectors produced by the FTA, in Proposition 2 with proof in Appendix A.2.2. Then, we provide an upper bound on the proportion of nonzero entries in the generated vector in Theorem 1, with in Appendix A.2.3. Assumption 1. δ <u−l, where kδ = u−lfor k∈N. Theorem 1(Sparsity guarantee for FTA.). For any z∈[l,u],η >0, φη(z) outputs a vector whose number of nonzero entries ∥φη(z)∥0 satisﬁes: ∥φη(z)∥0 ≤2 ⌊η δ ⌋ + 3 Corollary 1. Let ρ∈[0,1) be the desired sparsity level: the maximum proportion of nonzero entries of φη(z),∀z∈[l,u]. Assume ρk≥3, i.e., some inputs have three active indices or more (even with η= 0, this minimal active number is 2). Then ηshould be chosen such that ⌊η δ ⌋ ≤kρ−3 2 or equivalently η≤δ 2 (⌊kρ⌋−1) (5) As an example, for k= 100, δ= 0.05 and a desired sparsity of at least 10% (ρ= 0.1), we can use η = 0.05 2 (⌊100 ×0.1⌋−1) = 0.225. Note that the bound in Theorem 1 is loose in practice as the bound is for any input z. In Appendix A.2.3 and A.5, we theoretically and empirically show that the actual sparsity is usually lower than the upper bound, and quite consistent across inputs. 4 E XPERIMENTS IN SUPERVISED LEARNING UNDER COVARIATE SHIFT In this section, we focus on testing the hypothesis that FTA provides representations that are more robust to learning online on correlated data. Speciﬁcally, we hypothesize that convergence speed and stability for ReLU networks suffer under strongly correlated training data, whereas comparable FTA networks are nearly unaffected. We create a synthetic supervised problem with a relatively simply target function, and focus the investigation on the impact of a drifting distribution on inputs, which results both in covariate shift and creates temporal correlation during training. We also report results on two benchmark image classiﬁcation tasks in the Appendix A.6. The Piecewise Random Walk Problem has Gaussian Xt ∼N(St,β2) with ﬁxed variance β2 and a mean St that drifts every T steps. More precisely, the mean St stays ﬁxed for T timesteps, then takes a step according to a ﬁrst order autoregressive random walk: St = (1 −c)St + Zt where c∈(0,1] and Zt ∼N (0,σ2) for ﬁxed variance σ2. If c = 0, then this process is a standard random walk; otherwise, with c< 1, it keeps St in a bounded range with high probability. Forxt ∼Xt, the training label yt is deﬁned as yt = sin(2πx2 t). This process is designed to modulate the level of correlation—which we call correlation difﬁculty— without changing the equilibrium distribution over Xt. As the correlation difﬁculty dvaries from 0 to 1, the training data varies from low to high correlation: d = 0 recovers iid sampling. All d∈[0,1) share the same equilibrium distribution in Xt. Xt is ergodic and has Gaussian equilibrium distribution N(0,ξ2), with variance ξ2 dependent upon β2,σ2 and c. In particular, the visitation distribution Xt for any training run will converge to the equilibrium distribution. This ensures that measuring loss with respect to the stationary distribution is a fair comparison, because the visitation distribution of Xt is identical across all settings. We depict sample trajectories with low dand high din Figure 3(a). For a rigorous construction of Xt and St, and justiﬁcation for this equilibrium distribution and implementation details, see Appendix A.7. We measure the mean squared error over the equilibrium distribution inXt, for neural networks using FTA and ReLU activations across a range of correlation difﬁculty values. In Figure 3, we can see that FTA outperforms ReLU in two ways. First, FTA converges to a lower loss with less variance across all correlation difﬁculties. Second, FTA only marginally suffers under high difﬁculties d >0.9, whereas the performance of ReLU begins to deteriorate for relatively mild d> 0.5. Note that the FTA reaches a lower error, even on iid data (d= 0). We hypothesize this gap arises because the networks are trained online, with one sample from each Xt being used for each weight update. Figure 23 in the Appendix supports this hypothesis, with the gap vanishing in an identical experiment where 50 samples are drawn from each Xt. 5Published as a conference paper at ICLR 2021 0 500 1000 Training Iterations 1.0 0.5 0.0 0.5 1.0 Xt St = E[Xt] Equil. Density (0, 2) (a) Mild Correlation 0 500 1000 Training Iterations 1.0 0.5 0.0 0.5 1.0 Xt St = E[Xt] Equil. Density (0, 2)  (b) High Correlation 0.0 0.2 0.4 0.6 0.8 1.0 Covariate Shift Difficulty 0.00 0.05 0.15 Mean Final Loss FTA ReLU (c) ReLU and FTA, d∈[0,1) Figure 3: (a) and (b) contain sample trajectories of Xt which are (a) mildly correlated with d= 0.41 and (b) severely correlated with d= 0.98. Both share the same equilibrium distribution (in gray). (c) A plot of the prediction error (under the equilibrium distribution), averaged over the ﬁnal 2.5K iterations, across a range of difﬁculty settings. All networks are trained for 20k online updates. The lines correspond to the mean of 30 runs, with the shaded region corresponding to 99.9% conﬁdence intervals. The iid setting d= 0 is shown as a dotted line for baseline comparison. 5 E XPERIMENTAL RESULTS IN REINFORCEMENT LEARNING In this section, we empirically study the effect of using FTA in RL. First, we show overall performance on several benchmark discrete and continuous control environments. Second, we compare our method with other simple strategies to obtain sparse representations. Third, we provide insight into different hyper-parameter choices of FTA and suggest potential future directions. Appendix A.4 includes details for reproducing experiments and Appendix A.5 has additional RL experiments. 5.1 A LGORITHMS AND NAMING CONVENTIONS All the algorithms use a two-layer neural network, with the primary difference being the activation used on the last hidden layer. See Appendix A.5 for results using FTA in all the hidden layers. DQN is used for the discrete action environments, and Deep Deterministic Policy Gradient (DDPG) (Lillicrap et al., 2016) for continuous action environments. On each step, all algorithms sample a mini-batch size of 64 from an experience replay (Lin, 1992; Mnih et al., 2015) buffer with maximum size 100k. Note that we keep the same FTA setting across all experiments: we set [l,u] = [ −20,20], δ= η= 2.0, and hence c = {−20,−18,−16,..., 18}, k= 40/2 = 20. We ﬁrst compare to standard DQN agents, with the same architectures except the last layer. DQN: DQN with tanh or ReLU on the last layer (best performance reported). DQN-FTA: DQN with FTA on the last layer. DQN-Large: DQN, but with the last layer of the same size as DQN-FTA. If DQN has a last (i.e., the second) hidden layer of size d, then DQN-FTA has a last layer of size dk, since the FTA activation simply expands the number of features due to binning. Hence, we include DQN-Large with the same feature dimension as DQN-FTA in the last hidden layer. Note that DQN-Large has ktimes more parameters in this last hidden layer than DQN or DQN-FTA. 3 We also compare to several simple strategies to obtain local or sparse features. Radial basis functions (RBFs) have traditionally been used to obtain local features in RL, and recent work has used ℓ2 and ℓ1 regularization directly on activations as a simple baseline (Arpit et al., 2016; Liu et al., 2019). All of these strategies have the same sized last layer as the sparse feature dimension of DQN-FTA. DQN-RBF: DQN using radial basis functions (RBFs) on the last layer, with the centers deﬁned by the same c as FTA: φ(z) = [exp (−(z−c1)2 σ ),..., exp (−(z−ck)2 σ )] where σis the bandwidth parameter. DQN-L2/L1: DQN with ℓ2 or ℓ1 regularization on the activation functions of the ﬁnal hidden layer where there is the same number of units as that in DQN-Large. To the best of our knowledge, no suitable sparse representation approaches exist for RL. SR-NNs for RL were only developed for ofﬂine training (Liu et al., 2019). That work also showed that k-sparse NNs (Makhzani & Frey, 2013) and Winner-Take-All NNs (Makhzani & Frey, 2015) performed signiﬁcantly worse than ℓ2 regularization, where ℓ2 actually performed quite well in most of their 3Please refer to Figure 2. FTA does not augment the number of training parameters in the hidden layer where it is applied; it only augments the training parameters in the immediately next layer. 6Published as a conference paper at ICLR 2021 0.0 0.5 1.0 1.5 2.0 Time Steps 1e5 500 50 Average Return per Episode DQN-FTA DQN DQN-Large (a) Acrobot (v1) 0.0 0.5 1.0 1.5 2.0 2.5 3.0 1e5 2000 1500 1000 500  (b) MountainCar (v0) 0.0 0.5 1.0 1.5 2.0 2.5 3.0 1e5 100 200 300 400 500 (c) CartPole (v1) 0 1 2 3 4 5 1e5 400 200 0 200 (d) LunarLander (v2) Figure 4: Evaluation learning curves of DQN-FTA(black), DQN(red), and DQN-Large(blue), showing episodic return versus environment time steps. The dotted line indicates algorithms trained with target networks. The results are averaged over 20 runs and the shading indicates standard error. The learning curve is smoothed over a window of size 10 before averaging across runs. experiments. One other possible option is to use Tile Coding NNs (Ghiassian et al., 2020), which ﬁrst tile code inputs before feeding them into the neural network. This approach focuses on using discretization to break, or overcome, incorrect generalization in the inputs; their goal is not to learn sparse representations. This approach is complementary, in that it could be added to all the agents in this work. Nonetheless, because it is one of the only papers with a lightweight approach to mitigate interference in online RL, we do compare to it in the Appendix A.5.2. 5.2 O VERALL PERFORMANCE In this section, we demonstrate the overall performance of using FTAs on both discrete and continuous control environments. Our goals are to investigate if we can 1) obtain improved performance with FTA, with ﬁxed parameter choices across different domains; 2) improve stability in learning with FTA; and 3) see if we can remove the need to use target networks with FTA, including determining that learning can in fact be faster without target networks and so that it is beneﬁcial to use FTA without them. All experiments are averaged over 20 runs (20 different random seeds), with ofﬂine evaluation performed on the policy every 1000 training/environment time steps. 1 2 3 4 5 Time Steps 1e5 200 1050 Average Return per Episode DDPG DDPG-Large DDPG-FTA (a) InvertedPendu 0.0 0.5 1.0 1.5 2.0 1e6 0 500 1000 1500 2000 (b) Hopper 0.0 0.5 1.0 1.5 2.0 1e6 0 1000 2000 3000 (c) Walker2d 0 1 2 3 4 5 1e5 0 2000 4000 6000 8000 (d) InvertedDouble 0.0 0.5 1.0 1.5 2.0 1e6 0 25 50 75 100 125 150 (e) Swimmer Figure 5: Evaluation learning curves of DDPG-FTA(black), DDPG(red), and DDPG-Large(blue), averaged over 10 runs with shading indicating standard error. The dotted line indicates algorithms trained with target networks. The learning curve is smoothed over a window of size 30 before averaging across runs. Discrete control. We compare performance on four discrete-action environments from Ope- nAI (Brockman et al., 2016): MountainCar, CartPole, Acrobot and LunarLander. We use 64 ×64 ReLU hidden units on all these domains. Since FTA has k = 20 , this yields 64 ×20 = 1280 dimensional sparse features. Hence, DQN-Large, use two layers ReLU with size 64 ×1280. We plot evaluation learning curves, averaged over20 runs, in Figure 4. The results show the following. 1) With or without using a target network, DQN with FTA can signiﬁcantly outperform the version without using FTA. 2) FTA has signiﬁcantly lower variability across runs (smaller standard errors) in most of the ﬁgures. 3) DQN-FTA trained without a target network outperforms DQN-FTA trained with a target network, which indicates a potential gain by removing the target network. 4) Without using FTA, DQN trained without a target network cannot perform well in general, providing further evidence for the utility of sparse feature highlighted in previous works (Liu et al., 2019; Rafati & Noelle, 2019). 5) Simply using a larger neural network does not obtain the same performance improvements, and in some cases signiﬁcantly degrades performance. The exception to these conclusions is LunarLander, where DQN-FTA performed similarly to DQN and actually performed slightly better with target networks. On deeper investigation, we found that this was due to using a tiling bound of [−20,20], which we discuss further in Section 5.4. When using [−1,1], DQN-FTA performs much better and results are consistent with the other domains. 7Published as a conference paper at ICLR 2021 Continuous control.We compare performance on continuous control tasks from Mujoco (Todorov et al., 2012). For these experiments, we use DDPG with exactly the same FTA settings as in the above discrete control domains to show its generality. Corresponding to the discrete control domains, we apply FTA to the critic network and do not use a target network to train it. As seen in Figure 5, on most domains, DDPG-FTA achieves comparable and sometimes signiﬁcantly better performance to DDPG with a target network. However, Figure 5 (e) highlights that FTA is not always sufﬁcient on its own to achieve superior performance. Factors such as the exploration strategy, other hyper-parameters, and neural network architecture may also play an important role on such challenging tasks. 5.3 C OMPARISON WITH OTHER REPRESENTATION LEARNING APPROACHES FTA provides clear beneﬁts, but it is natural to ask if other simple strategies that provide sparse or local features could have provided similar beneﬁts. We compare to DQN-RBF, DQN-L2 and DQN-L1, on the discrete action environments, shown in Figure 6. We ﬁnd the following. 1) FTA performs consistently well across all environments using a ﬁxed parameter setting; none of the other approaches achieve consistent performance, even though we tuned their parameters per environment. 2) Both the ℓ1 and ℓ2 approaches have a high variance across different random seeds. 3) The RBF variant can do better than the ℓ1 and ℓ2 approaches but is worse than our algorithm. It is a known issue that RBF is sensitive to the bandwidth parameter choice and we observe similar phenomenon. It is also known that the exponential activation can be problematic in the back-propagation process. We empirically investigate the overlap and instance sparsities of each algorithm in the Appendix A.5. 0.0 0.5 1.0 1.5 2.0 Time Steps 1e5 500 50 Average Return per Episode DQN-L2 DQN-L1 DQN-RBF DQN-FTA (a) Acrobot (v1) 0.0 0.5 1.0 1.5 2.0 2.5 3.0 1e5 2000 100  (b) MountainCar (v0) 0.0 0.5 1.0 1.5 2.0 2.5 3.0 1e5 100 200 300 400 500 (c) CartPole (v1) 0 1 2 3 4 5 1e5 200 100 0 100 200 (d) LunarLander (v2) Figure 6: Evaluation learning curves of DQN-FTA(black), DQN-RBF(forest green), DQN- L2(red), and DQN-L1(blue), averaging over 20 runs with the shade indicating standard error. All algorithms are trained without using target networks. The learning curve is smoothed over a window of size 10 before averaging across runs. 5.4 C HOOSING THE HYPER -PARAMETERS FOR FTA In this section, we provide some insight into selecting the hyper-parameters of FTA. We argue that η= δcan be used as a rule of thumb. Then we show that given an appropriate tiling bound (i.e., u value), DQN-FTA performs well, with a reasonably large number of bins, i.e., a reasonably small tile width δ. However, we do observe a certain level of sensitivity to the tiling bound, which may partially explain the worse performance of DQN-FTA on LunarLander and some Mujoco domains. Sparsity control parameter. The purpose of the parameter ηis to provide a nonzero gradient for training an NN via backpropagation. Since FTA is an one-to-many mapping, it gives a nonzero gradient as long as any single element in the resulting vector provides a nonzero gradient. Setting η= δ(i.e., the tile width) is the minimum value to guarantee nonzero gradient as we visualize in the Figure 1(c) as long as FTA’s input is within the tiling bound. In all of our experiments, we ﬁxη= δ unless otherwise speciﬁed. In the Appendix A.5.4, we show that DQN-FTA does reasonably well across a broad range of η,δ parameter settings. Number of tiles/tile width. In Figure 7(c), we show that on LunarLander, with u = 1.0, DQN- FTA performs well when the tile width δ = 2 u/k is reasonably small (i.e., number of tiles k is reasonably large). We show the learning curves of DQN-FTA with u = 1 .0, in Figure 7(a), by using number of tiles from k ∈{2,4,6,8,10,12,14,16}, providing sparse feature dimension 64 ×k ∈{128,256,384,512,640,768,896,1024}. We also examine the performance of DQN trained by using NN size 64 ×k, for k ∈{2,4,6,8,10,12,14,16}. In Figure 7(b), one can see 8Published as a conference paper at ICLR 2021 0 1 2 3 4 5 1e5 400 300 200 100 0 100 200 300 2 4 6 8 10 12 14 16 (a) Vary # of tiles k 0 1 2 3 4 5 1e5 400 300 200 100 0 100 200 300 128 256 384 512 640 768 896 1024 (b) Vary # of hidden units 0 1 2 3 4 5 1e5 200 100 0 100 200 300 0.01 0.1 1.0 10.0 20.0 (c) Bound sensitivity 0 1 2 3 4 5 1e5 0.0 0.2 0.4 0.6 0.8 1.0 (d) Sparsity ratio Figure 7: (a) Evaluation learning curves showing episodic return versus environment time steps of DQN-FTA using different number of tiles kas labeled. This is equivalent to varying tile width δas δ= 2u/k. The results are averaged over 5 random seeds. (b) Evaluation learning curves of DQN without using a target NN as we change the number of the second hidden layer units as labeled in the ﬁgure. (c) Evaluation learning curves of DQN-FTA uses [−u,u] as tiling bound where u∈{0.01,0.1,1.0,10.0,20.0}. The results are averaged over 10 runs. The standard error is not shown but is sufﬁcient small to differentiate the two groups (i.e., {0.1,1.0} and {0.01,10.0,20.0}) corresponding to appropriate bound and too large/small bound. To generate this ﬁgure, we ﬁx on using FTA with 20 tiles (i.e., tile width δ = 2u/20). (d) The overlap-instance sparsity ratio v.s. environment time steps. The standard error is very small and is ignored. The results are averaged over 10 runs. that increasing NN size does not provide a clear beneﬁt to DQN; however, DQN-FTA performs signiﬁcantly better as kincreases, in Figure 7(a). Sensitivity to tiling bound. We ﬁnd that the tiling bound [l,u] can be sensitive. Intuitively, if we set l,u extremely small, then the input of FTA may go out of the boundary and FTA provides zero gradient again. When we set the bound too large, many inputs may hit the same bins, both resulting in many dead neurons and increasing interference. In Figure 7(c), we see that very small u= 0.01 and big u= 10,20 perform poorly in LunarLander, and interim values of u= 0.1,1 perform well. We further examine the corresponding representation interference measured by the overlap sparsity divided by instance sparsity, in Figure 7(d). Instance sparsity is the proportion of nonzero entries in the feature vector for each instance. Overlap sparsity (French, 1991) is deﬁned as overlap(φ,φ′) =∑ iI(φi ̸= 0)I(φ′ i ̸= 0)/(kd), given two kd-dimensional sparse vectors φ,φ′. Low overlap sparsity potentially indicates less feature interference between different input samples. Overlap sparsity divided by instance sparsity represents the proportion of overlapped entries among activated ones. We can see in Figure 7(d) that large uincreases this ratio, indicating increased interference, which may explain the worse performance of DQN-FTA withu= 10,20 in Figure 7(c). On the other extreme, when the bound is too small, u = 0.01, the sparsity ratio is low but performance is poor. This is likely because generalization actually becomes too low at this level, reducing sample efﬁciency. We refer to Appendix A.5.6 for additional experiments about gradient interference. 6 D ISCUSSION In this work, we proposed the idea of natural sparsity, aiming at achieving sparsity without learning in a deep learning setting. We design an activation by drawing on the idea of binning which produces a one-hot encoding of an input. We provide a Fuzzy Tiling Activation (FTA) with a sparsity control mechanism that enables backpropagation of gradients, and potentially improves generalization by increasing active feature units. We show that the FTA still has sparsity guarantees, related to the choice of η. The FTA provides sparse representations by construction, and so it is much easier to use when learning online than conventional sparse representation learning methods. We highlight that FTA is robust to high levels of covariate shift, in a synthetic supervised learning problem. We then show across several discrete and continuous control RL environments that using the FTA signiﬁcantly improve learning efﬁciency and stability, and in most cases even removes the need of target networks. Our work suggests several promising future research directions. The ﬁrst is to further investigate the choices for the tiling bound. An adaptive approach for the tiling bound is particularly relevant for ease-of-use, as we found more sensitivity to this choice. Second, FTA is actually complementary to approaches (Liu et al., 2018; Riemer et al., 2019; Javed & White, 2019) designed to explicitly mitigate interference. An interesting direction is to combine FTA with these methods to reduce interference. Last, it is important to study how we should balance generalization and discrimination. We observe that an algorithm’s performance degrades when either the overlap sparsity is too high or too low. Learning sparse representation with an appropriate overlap sparsity may be most desirable. 9Published as a conference paper at ICLR 2021 7 A CKNOWLEDGEMENTS We would like to thank all anonymous reviewers for their helpful feedback. We thank Han Wang for carefully reading our paper and providing helpful suggestions to improve our presentation. We acknowledge the funding from the Canada CIFAR AI Chairs program and Alberta Machine Intelligence Institute. REFERENCES Abadi, M. and et. al. TensorFlow: Large-scale machine learning on heterogeneous systems. Software available from tensorﬂow.org, 2015. Arpit, D., Zhou, Y ., Ngo, H., and Govindaraju, V . Why regularized auto-encoders learn sparse representation? In International Conference on Machine Learning, pp. 136–144, 2016. Belkin, M., Hsu, D., Ma, S., and Mandal, S. Reconciling modern machine-learning practice and the classical bias–variance trade-off. Proceedings of the National Academy of Sciences , pp. 15849–15854, 2019. Brockman, G., Cheung, V ., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W. Openai gym. arXiv, 2016. Caselles-Dupré, H., Ortiz, M. G., and Filliat, D. Continual state representation learning for reinforce- ment learning using generative replay. arXiv:1810.03880, 2018. Chandak, Y ., Theocharous, G., Kostas, J., Jordan, S., and Thomas, P. Learning action representations for reinforcement learning. In International Conference on Machine Learning, pp. 941–950, 2019. Cheng, H., Liu, Z., Yang, L., and Chen, X. Sparse representation and learning in visual recognition: Theory and applications. Signal Processing, pp. 1408–1425, 2013. Special issue on Machine Learning in Intelligent Image Processing. Cover, T. M. Geometrical and Statistical Properties of Systems of Linear Inequalities with Applica- tions in Pattern Recognition. IEEE Trans. Electronic Computers, 1965. Fan, J., Wang, Z., Xie, Y ., and Yang, Z. A theoretical analysis of deep q-learning. InProceedings of the 2nd Conference on Learning for Dynamics and Control, pp. 486–489, 2020. French, R. M. Using semi-distributed representations to overcome catastrophic forgetting in connec- tionist networks. In Annual Cognitive Science Society Conference, 1991. French, R. M. Catastrophic forgetting in connectionist networks. Trends in Cognitive Sciences, pp. 128–135, 1999. Ghiassian, S., Raﬁee, B., Lo, Y . L., and White, A. Improving performance in reinforcement learning by breaking generalization in neural networks. International Conference on Autonomous Agents and Multi-agent Systems, 2020. Glorot, X. and Bengio, Y . Understanding the difﬁculty of training deep feedforward neural networks. International Conference on Artiﬁcial Intelligence and Statistics, 2010. Grunwald, G., Hyndman, R., and Tedesco, L. A uniﬁed view of linear ar (1) models. 1995. Gu, S., Lillicrap, T. P., Sutskever, I., and Levine, S. Continuous Deep Q-Learning with Model-based Acceleration. In International Conference on Machine Learning, pp. 2829–2838, 2016. Heravi, J. R. Learning Representations in Reinforcement Learning . PhD thesis, University of California, Merced, 2019. Hernandez-Garcia, J. F. and Sutton, R. S. Learning sparse representations incrementally in deep reinforcement learning. Workshop on Continual Learning at Advances in Neural Information Processing Systems, 2019. 10Published as a conference paper at ICLR 2021 Javed, K. and White, M. Meta-learning representations for continual learning. In Advances in Neural Information Processing Systems, pp. 1818–1828. 2019. Kemker, R., McClure, M., Abitino, A., Hayes, T. L., and Kanan, C. Measuring catastrophic forgetting in neural networks. In AAAI conference on Artiﬁcial Intelligence, 2018. Kim, S., Asadi, K., Littman, M., and Konidaris, G. Deepmellow: Removing the need for a target network in deep q-learning. International Joint Conference on Artiﬁcial Intelligence, pp. 2733– 2739, 2019. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. International Conference on Learning Representations, 2015. Le, L., Kumaraswamy, R., and White, M. Learning sparse representations in reinforcement learning with sparse coding. In International Joint Conference on Artiﬁcial Intelligence, pp. 2067–2073, 2017. LeCun, Y . and Cortes, C. MNIST handwritten digit database. http://yann.lecun.com/exdb/mnist/, 2010. Leurent, E. An environment for autonomous driving decision-making, 2018. URL https:// github.com/eleurent/highway-env. Liang, Y ., Machado, M. C., Talvitie, E., and Bowling, M. State of the art control of atari games using shallow reinforcement learning. International Conference on Autonomous Agents & Multiagent Systems, pp. 485–493, 2016. Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y ., Silver, D., and Wierstra, D. Continuous control with deep reinforcement learning. In International Conference on Learning Representations, 2016. Lin, L.-J. Self-Improving Reactive Agents Based On Reinforcement Learning, Planning and Teaching. Machine Learning, 1992. Liu, V ., Kumaraswamy, R., Le, L., and White, M. The utility of sparse representations for control in reinforcement learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, pp. 4384–4391, 2019. Liu, X., Masana, M., Herranz, L., Van de Weijer, J., López, A. M., and Bagdanov, A. D. Rotate your networks: Better weight consolidation and less catastrophic forgetting. In International Conference on Pattern Recognition, pp. 2262–2268, 2018. Madjiheurem, S. and Toni, L. Representation learning on graphs: A reinforcement learning applica- tion. International Conference on Artiﬁcial Intelligence and Statistics, 2019. Makhzani, A. and Frey, B. k-sparse autoencoders. arXiv:1312.5663, 2013. Makhzani, A. and Frey, B. Winner-take-all autoencoders. In Advances in Neural Information Processing Systems, 2015. McCloskey, M. and Cohen, N. J. Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem. Psychology of Learning and Motivation, 1989. Mnih, V ., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., and others. Human-level control through deep reinforcement learning. Nature, 2015. Moore, A. W. and Atkeson, C. G. Prioritized sweeping: Reinforcement learning with less data and less time. Machine learning, pp. 103–130, 1993. Pan, Y ., Zaheer, M., White, A., Patterson, A., and White, M. Organizing experience: a deeper look at replay mechanisms for sample-based planning in continuous state domains. In International Joint Conference on Artiﬁcial Intelligence, pp. 4794–4800, 2018. 11Published as a conference paper at ICLR 2021 Pan, Y ., Yao, H., Farahmand, A.-m., and White, M. Hill climbing on value estimates for search-control in dyna. International Joint Conference on Artiﬁcial Intelligence, 2019. Rafati, J. and Noelle, D. C. Learning sparse representations in reinforcement learning. arXiv:1909.01575, 2019. Riemer, M., Cases, I., Ajemian, R., Liu, M., Rish, I., Tu, Y ., , and Tesauro, G. Learning to learn without forgetting by maximizing transfer and minimizing interference. In International Conference on Learning Representations, 2019. Schlegel, M., Pan, Y ., Chen, J., and White, M. Adapting Kernel Representations Online Using Submodular Maximization. In International Conference on Machine Learning, 2017. Sutton, R. S. Dyna, an integrated architecture for learning, planning, and reacting. SIGART Bulletin, 2(4):160–163, 1991. Sutton, R. S. Generalization in reinforcement learning: Successful examples using sparse coarse coding. In Advances in Neural Information Processing Systems 8 , pp. 1038–1044. MIT Press, 1996. Sutton, R. S. and Barto, A. G. Reinforcement Learning: An Introduction. The MIT Press, second edition, 2018. Sutton, R. S., McAllester, D., Singh, S., and Mansour, Y . Policy gradient methods for reinforce- ment learning with function approximation. In International Conference on Neural Information Processing Systems, 1999. Sutton, R. S., Szepesvári, C., Geramifard, A., and Bowling, M. Dyna-style planning with linear function approximation and prioritized sweeping. In Conference on Uncertainty in Artiﬁcial Intelligence, pp. 528–536, 2008. Tibshirani, R. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, pp. 267–288, 1996. Todorov, E., Erez, T., and Tassa, Y . Mujoco: A physics engine for model-based control. In International Conference on Intelligent Robots and Systems, 2012. van Hasselt, H., Doron, Y ., Strub, F., Hessel, M., Sonnerat, N., and Modayil, J. Deep reinforcement learning and the deadly triad. arXiv:1812.02648, 2018. Watkins, C. J. C. H. and Dayan, P. Q-learning. Machine Learning, 1992. Xiang, Z., Xu, H., and Ramadge, P. J. Learning sparse representations of high dimensional data on large scale dictionaries. In Advances in Neural Information Processing Systems, 2011. Xiao, H., Rasul, K., and V ollgraf, R. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. 2017. 12Published as a conference paper at ICLR 2021 A A PPENDIX This appendix includes the following contents: 1. Section A.1 brieﬂy reviews tile coding which inspires FTA and the naming. 2. Section A.2 shows the proofs for theorems about sparsity guarantee in this paper. 3. Section A.3 discusses an alternative way to handle the case when the inputs of FTA go out of the boundary of the tiling vector c. 4. Section A.4 includes experimental details of Section 5 for reproducible research. 5. Section A.5 presents additional experiments in reinforcement learning setting. 6. Section A.6 reports the results of FTA on two popular image classiﬁcation datasets: Mnist (LeCun & Cortes, 2010) and Mnistfashion (Xiao et al., 2017). 7. Section A.7 includes the details of the synthetic supervised learning experiment from Section 4. A.1 T ILE CODING REVIEW We give a brief review of tile coding here, as tile coding inspires our Fuzzy Tiling Activation (and its naming). Tile coding is a generalization of state aggregation, that uses multiple tilings (aggregations) to improve discrimination. For input z∈[0,1], state-aggregation would map zto a one-hot vector with a 1 in the corresponding bin (bin can be also called tile), with kthe number of bins discretized into intervals of length δ. In tile coding, multiple such tilings of the input are concatenated, where each tiling is offset by a small amount. This is depicted in Figure 8, where we show two tilings, one covering from [−0.05,1] and the other from [0.0,1.05] both with k= 4 and δ = 1.05/4 = 0.2625. The resulting feature vector is a concatenation of these two tilings, to produce 8 features. Two nearby inputs z = 0.3 and z′= 0.55 would be aggregated together in state-aggregation, making it impossible to produce different values for those inputs. Tile coding, on the other hand, would cause them to share one feature in the second tiling, but each have a distinct feature that enables them to have different values. The large bins/tiles allows for some information sharing, for fast generalization; and the overlapping tilings allows for improved discrimination. 1 2 3 4  -0.05 1.05  tiling1 tiling25 6 7 8  z z'  1  Figure 8: Tile coding maps a scalar to an 8 di- mensional binary vector. For example, z activates the second tile on both tilings which gives the fea- ture vector (0,1,0,0,0,1,0,0). Similarly, z′ ↦→ (0,0,1,0,0,1,0,0). Unfortunately, tile coding can scale exponentially in the dimensiondof the inputs. Each dimension is discretized to a granularity of δ(kregions), with the cross-product between all these regions resulting in kd bins. One natural approach to make binning more scalable and more practically usable, is to combine it with neural networks (NN). Our activation function—Fuzzy Tiling Activation—enables a differentiable binning operation. It is known that those simple domains which are able to use tile coding never require a separate slowly moving weight vector (i.e., the counterpart of a target network in linear function approximation setting) (Sutton, 1996; Sutton & Barto, 2018). Based on this observation, Liu et al. (2019) indicates that in a deep learning setting, when using sparse representation, the target network can be removed and the sample efﬁciency can be improved, coinciding with many of our empirical results. It is sensible that the target network possibly slower down learning. Because at each environment time step, the new information is not immediately utilized to estimate the bootstrap target and this could slower down learning. Accurately estimating bootstrap targets may be highly beneﬁcial in Dyna- style model-based reinforcement learning Sutton (1991); Sutton et al. (2008), as the planning stage typically involves efﬁcient improvement of value estimates (Moore & Atkeson, 1993; Pan et al., 2018; Gu et al., 2016; Pan et al., 2019). It may be an interesting future direction to carefully study the effect of removing the target network in Dyna-style planning. 13Published as a conference paper at ICLR 2021 A.2 P ROOFS We now provide the proof for Proposition 1, Proposition 2, Theorem 1 and Corollary 1 below. A.2.1 P ROOF FOR PROPOSITION 1 For convenience, we deﬁne the k-dimensional one-hot vector ei whose ith entry is one and otherwise zero. Proposition 1. Under Assumption 1, for any z∈[l,u], 1. If ci <z <ci+1 for i∈[k−1] or ck <z, then φ(z) = ei 2. If z= ci for i∈{2,...,k }, then φ(z) = ei−1 + ei 3. If z= c1, then φ(z) = e1 Proof. In all thee cases, the ﬁrst max operation in φ is max(c −z,0) = (0,0,..., ci+1 −x,ci+2 −z,..., ck −z). To understand the output of the second max operation, we look at the three cases separately. Case 1:Because ci < z <ci+1, we know that ci−1 < z−δ <ci. This implies the second max operation in φ is: max(z−δ−c,0) = (z−δ−c1,z −δ−c2,...,z −δ−ci−1,0,0,..., 0) Therefore, the sum of both max operations max(c −z,0) + max(z−δ−c,0) has positive elements everywhere except the ith position, which is zero. Hence I+(max(c −z,0) + max(z−δ−c,0)) gives a vector where every entry is 1 except the ith entry which is 0. Then 1 −I+(max(c −z,0) + max(z−δ−c,0)) = ei. Case 2:If z= ci,i ∈{2,...,k }, then ci−1 = z−δ, and max(z−δ−c,0) = (z−δ−c1,z −δ−c2,...,z −δ−ci−2,0,..., 0). It follows that max(c −z,0) + max(z−δ−c,0) has exactly two zero entries, at indices i−1 and i. This gives 1 −I+(max(c −z,0) + max(z−δ−c,0)) = ei−1 + ei, a vector with ones at indices i−1 and i. Case 3:When z= c1, max(z−δ−c,0) is a zero vector and max(c −z,0) is positive everywhere except the ﬁrst entry, which is zero. Again this gives 1 −I+(max(c −z,0) + max(z−δ−c,0)) = e1. A.2.2 P ROOF FOR PROPOSITION 2 Proposition 2. Let Ibe the set of indices where φ(z) is active (from Theorem 1 we know it can only contain one or two indices). Under Assumption 1, for any z∈[l,u], the function φη(z) = φ(z) + ∆, where ∆ is mostly zero, with a few non-zero entries. The vector ∆ has the following properties: 1. ∆ is zero at indices i∈I, i.e., φη(z) equals φ(z) at indices i∈I. 2. ∆ is non-zero at indices {j|j /∈I,j ∈[k],0 <z −δ−cj ≤η,0 <cj −z≤η}. Proof. Part 1.Let the ith entry be active in φ(z). We have one of the three cases hold as stated in Theorem 1. Assume ci <z <ci+1. Note that max(c −z,0) = (0,0,..., ci+1 −z,..., ck −z) max(z−δ−c,0) = (z−δ−c1,...,z −δ−ci−1,0,..., 0), taking the sum of the above two equations gives us a vector as following: (z−δ−c1,...,z −δ−ci−1,0,ci+1 −z,..., ck −z) (6) Then applying Iη,+(·) to vector equation 6 gives us a vector retaining elements ≤ηand all other elements become 1. Hence the ith position is zero after applying Iη,+(·) to vector equation 6. Using 14Published as a conference paper at ICLR 2021 one minus this vector would give us a vector with only the ith is one. But this is exactly φη(z). And since φη(z) = φ(z) + ∆, and the ith entry of φ(z) is also one, the ith entry of ∆ must be zero. Similar reasoning applies to the cases when z= ci,i ∈{2,...,k }or z= c1. Part 2.Note that applyingIη,+(·) to the vectormax(c−z,0)+max(z−δ−c,0) keeps all elements no more than ηand making all other elements one. As a result,1−Iη,+(max(c−z,0)+max(z−δ−c,0)) would give a vector zero everywhere except those entries in max(c −z,0) + max(z−δ−c,0) which are ≤η. The set of indices which are ≤ηin the vector max(c −z,0) + max(z−δ−c,0) can be written as {j|j ∈[k],0 < z−δ −cj ≤η,0 < cj −z ≤η}, which is also the set of indices where φη(z) is nonzero. Since φη(z) = φ(z) + ∆ and φ(z) has nonzero entries in the indices Iand ∆ has zero values at those entries by Part 1, then ∆ must have nonzero entries at {j|j /∈I,j ∈[k],0 <z −δ−cj ≤η,0 <cj −z≤η}. A.2.3 P ROOF FOR THEOREM 1 Theorem 1. Sparsity guarantee for FTA.For any z∈[l,u],η >0, φη(z) outputs a vector whose number of nonzero entries ∥φη(z)∥0 satisﬁes: ∥φη(z)∥0 ≤2 ⌊η δ ⌋ + 3 Proof. Similar to the proof of Theorem 1, we divide to three cases to prove the result. Case 1.Consider the case that ci <z <ci+1,i ∈[k−1]. Note that the number of nonzero entries in φη is equal to the number of entries less than ηin the vector equation 6, hence we can count the number of entries less than ηin (z−δ−c1,...,z −δ−ci−1,0,ci+1 −z,ci+2 −z,..., ck −z). First, we count the number of entries that are less than or equal to ηon the left side of the ith position. Since the ith position is zero, which indicates z−δ−ci <0, hence z−δ−ci−1 −δ <0 and it follows that 0 < z−δ−ci−1 < δ. Then δ < z−δ−ci−1 + δ = z−δ−ci−2 < 2δ. Hence (j−1)δ < z−δ−ci−j < jδ,j∈{1,...,i −1}. Assume there are mentries ≤ηon the the left side of the ith. Then z−δ−ci−m ≤η. It follows (m−1)δ <ηand hence m≤⌊η δ⌋+ 1. Hence the total number of elements ≤ηon the left side of the ith position is at most ⌊η δ⌋+ 1. Second, count the number of entries ≤η on the right side of the ith position. Since ci −z <0, 0 <ci + δ−z= ci+1 −z <δ,0 <ci+2 −z <2δ,.... Hence the possible number of entries ≤η on the right side of ith position is at most ⌊η δ⌋+ 1. As a result, together with the ith position which is 0 ≤η, the possible number of nonzero entries in this case is at most 2⌊η δ⌋+ 3. Case 2.When z= ci,i ∈{2,...,k }, we count the number of entries less than ηin the vector max(c −z,0) + max(z−δ−c,0) = ((i−1)δ,...,2δ,δ,0,0,δ,2δ,...,(k−i)δ) Again, we attempt to count the number of entries less than η in this vector by considering (i− 1)δ,...,2δ,δ and δ,2δ,...,(k−i)δrespectively. We follow the exactly same argument as above and now we have two zero entries ati−1,ith positions. The difference is that, the number of entries less than ηin the vector ((i−1)δ,...,2δ,δ can be at most ⌊η δ⌋. As a result, the number of nonzero entries in this case is still at most 2⌊η δ⌋+ 2. Case 3.When z= c1, max(z−δ−c,0) is a zero vector and max(c −z,0) + max(z−δ−c,0) is positive everywhere except the ﬁrst entry, which is zero. Then we simply count the number of entries ≤ηon the right side of the 0th position, i.e. j ∈{1,...,k }. Similar to the analysis in the above Case 1, the possible number of entries ≤ηon the right side of 0th position is at most ⌊η δ⌋. Hence in this case, the number of nonzero entries is at most ⌊η δ⌋+ 1. In summary, the number of nonzero entries does not exceed 2⌊η δ⌋+ 3. This completes the proof. Remark. As we empirically demonstrated in Table 1 and below Figure 10 and Figure 9, the actual sparsity achieved by FTA is lower than the upper bound. This is because our upper bound is for any possible input z. Consider that in Case 1 in the above proof, we count the number of entries that 15Published as a conference paper at ICLR 2021 are less than or equal to ηon the left side and right side of the ith position. There are ⌊η δ⌋+ 1 such entries on both sides only when zis exactly equal to ci+ci−1 2 , which is unlikely to happen in practice. A.2.4 P ROOF FOR COROLLARY 1 Corollary 1 Let ρ∈[0,1) be the desired sparsity level: the maximum proportion of nonzero entries of φη(z),∀z∈[l,u]. Assume ρk≥3, i.e., some inputs have three active indices or more (even with η= 0, this minimal active number is 2). Then ηshould be chosen such that ⌊η δ ⌋ ≤kρ−3 2 or equivalently η≤δ 2 (⌊kρ⌋−1) (7) Proof. Because ∥φη(z)∥0 ≤⌊kρ⌋, from Theorem 1 it is sufﬁcient to pick ηsuch that 2⌊η δ⌋+ 3 ≤ ⌊kρ⌋≤ kρ. This gives ⌊η δ⌋≤ (⌊kρ⌋−3)/2 ≤(kρ−3)/2. Additionally, we know η δ −1 ≤⌊η δ⌋≤ (⌊kρ⌋−3)/2, giving the second inequality. A.3 M ORE DISCUSSION ABOUT FTA Our development of the FTA assumed that the inputs are bounded in the range [l,u] (recall that c = [l,l + δ,l + 2δ,...,u]). This is not guaranteed for the standard inputs to activations, namely z= x⊤w for some weight vector w. The FTA can still be used if z /∈[l,u], but then the gradient of the FTA will be zero. This means that the weights w cannot be adjusted for inputs where zbecomes too large or too small. This issue is usually called gradient vanish, which is common in many popular, existing activation functions such as ReLU, tanh, sigmoid, etc. In our main paper, we proposed to use different tiling vectors (i.e., cs) with a broad range of bounds to different components in the input vector of FTA to reduce the chance of vanishing gradients. Here we discuss two other possible ways to avoid this issue: 1) use a squashing activation before handingz to the FTA and 2) regularize (penalize) zthat falls outside the range [l,u]. For example, tanh can be applied ﬁrst to produce z= tanh(x⊤w) ∈[−1,1]. Though the simplest strategy, using tanh function can be problematic in deep neural networks due to gradient vanishing problems. An alternative is to use a penalty, that pushes out-of-boundary zback into the chosen range [−u,u] r(z) def = I(|z|>u) ◦|z| (8) This penalty is easily added to the loss, giving gradient∂r(z) ∂z = I(z >u)−I(z <−u). For example, z= max(x⊤w,0), which might produce a value greater than u. If z >u, then the gradient pushes the weights w to decrease z. It should be noted that the number of nonzero entries can only decrease when zgoes out of boundary. However, in our experiments in our main paper, we found such out of boundary loss is unnecessary on all of our tested domains. Furthermore, we further verify that our FTA performs stably across different weights for the above regularization in Section A.5. A.4 R EPRODUCING EXPERIMENTS FROM SECTION 5 Common settings.All discrete action domains are from OpenAI Gym (Brockman et al., 2016) with version 0.14.0. Deep learning implementation is based on tensorﬂow with version 1.13.0 (Abadi & et. al, 2015). We use Adam optimizer (Kingma & Ba, 2015), Xavier initializer (Glorot & Bengio, 2010), mini-batch size b= 64, buffer size 100k, and discount rate γ = 0.99 across all experiments. We evaluate each algorithm every1k training/environment time steps. Algorithmic details.We use64×64 ReLU units neural network for DQN and200×100 ReLU units neural network for DDPG. All activation functions are ReLU except: the output layer of the Qvalue is linear. The weights in output layers were initialized from a uniform distribution [−0.003,0.003]. Note that we keep the same FTA setting across all experiments: we set [l,u] = [−20,20]; we set δ = η = 2.0, c = {−20,−18,−16,..., 18}, and hence k = 40/2 = 20 . This indicates that the DQN-Large and DDPG-Large versions have 64 ×20 = 1280 and 100 ×20 = 2000 ReLU units in the second hidden layer. For RBF coding, we set the bandwidth as σ= 2.0, and uses the same tiling (i.e. c vector) as our FTA. 16Published as a conference paper at ICLR 2021 Meta-parameter details.For DQN, the learning rate is 0.0001 and the target network is updated every 1k steps. For DDPG, the target network moving rate is 0.001 and the actor network learning rate is 0.0001, critic network learning rate is 0.001. For l1,l2 regularization variants, we optimize its regularization weight from {0.1,0.01,0.001,0.0001}on MountainCar, then we ﬁx the chosen optimal weight 0.01 across all domains. Environmental details on discrete control domains.We set the episode length limit as 2000 for MountainCar and keep all other episode limit as default settings. We use warm-up steps 5000 for populating the experience replay buffer before training. Exploration noise is 0.1 without decaying. During policy evaluation, we keep a small noise ϵ= 0.05 when taking action. Environmental details on continuous control domains.On Mujoco domains, we use default settings for maximum episodic length. We use 5,000 warm-up time steps to populate the experience replay buffer. The exploration noise is as suggested in the original paper by Lillicrap et al. (2016). A.5 A DDITIONAL EXPERIMENTS ON REINFORCEMENT LEARNING PROBLEMS This section shows the following additional empirical results which cannot be put in the main body due to space limitations. • A.5.1 empirically investigates the representation sparsity generated by FTA. • A.5.2 shows the comparison with Tile Coding NNs (Ghiassian et al., 2020), which ﬁrst tile code inputs before feeding them into a neural network. • A.5.3 shows the empirical results of DQN-FTA using linear activation function with regular- ization weights ∈{0.0,0.01,1.0}for the out of bound loss in Eq 8. • A.5.4 shows the sensitivity of FTA to the sparsity control parameter η and tile width parameter δ. • A.5.5 shows the results when applying FTA to both hidden layers rather than just the second hidden layer in RL problems. • A.5.6 shows gradient interference analysis result. • A.5.7 shows the result of using FTA on an autonomous driving application. A.5.1 E MPIRICALLY MEASURING SPARSITY We also report two sparsity measures of the learned representations averaged across all steps in Table 1. We compute an estimate of sparsity by sampling a mini-batch of samples from experience replay buffer and taking the average of them. Instance sparsity corresponds to proportion of nonzero entries in the feature vector for each instance. Overlap sparsity French (1991) is deﬁned as overlap(φ,φ′) =∑ iI(φi ̸= 0)I(φ′ i ̸= 0)/(kd), given two kd-dimensional sparse vectors φ,φ′. Low overlap sparsity potentially indicates less feature interference between different input samples. Theorem 1 guarantees that the instance sparsity, with FTA, should be no more than12.5% for our setting of k= 40,η = δ; and should be no more than 25% when k= 20,η = δ. In the table, we can see that FTA has lower (better) instance sparsity than the upper bound provided by the theorem. Further, FTA achieves the lowest instance and overlap sparsity among all baselines in an average sense. Figure 9 and Figure 10 are corresponding to the learning curves as shown in Figure 6 in Section 5.3. We show learning curves of instance/overlap sparsity as a function of training steps for FTA,l1,l2 regularization and RBF. The instance sparsity is computed by randomly sampling a mini-batch of states from replay buffer and count the average number of nonzero entries in that mini-batch divided by feature dimension kd. The overlap sparsity is computed by randomly sampling two mini-batches of states from the replay buffer and count the average number of simultaneously activated (i.e. nonzero) entries in both mini-batches. From the sparsity learning curves, one can see that our FTA has very stable sparsity since the beginning of the learning, and the sparsity is almost constant cross domains. Particularly, the overlap sparsity is very low, which possibly indicates low representation interference. A.5.2 C OMPARISON WITH TC-NN We include the ﬁgure of comparing with TC-NN from the work by Ghiassian et al. (2020), which use a regular tile coding to process the input before feeding into the neural network. It should be noted 17Published as a conference paper at ICLR 2021 Table 1: Sparsity on LunarLander (average across time steps) Sparsity FTA(k=40) FTA(k=20) L1 L2 RBF Instance 7% 14% 16% 44% 99% Overlap 4% 8% 10% 34% 99% 0.5 1.0 1.5 2.0 Time Steps 1e5 0.0 0.2 0.4 0.6 0.8 1.0 (a) Acrobot-v1 0.5 1.0 1.5 2.0 2.5 3.0 Time Steps 1e5 0.0 0.2 0.4 0.6 0.8 1.0 (b) MountainCar-v0 0.5 1.0 1.5 2.0 2.5 3.0 Time Steps 1e5 0.0 0.2 0.4 0.6 0.8 1.0 DQN-L2 DQN-L1 DQN-RBF DQN-FTA (c) CartPole-v1 1 2 3 4 5 Time Steps 1e5 0.0 0.2 0.4 0.6 0.8 1.0 (d) LunarLander-v2 Figure 9: Instance sparsity v.s. number of time steps on MountainCar, CartPole, Acrobot, LunarLan- der. The results are averaged over 20 random seeds and the shade indicates standard error. 0.5 1.0 1.5 2.0 Time Steps 1e5 0.0 0.2 0.4 0.6 0.8 1.0 (a) Acrobot-v1 0.5 1.0 1.5 2.0 2.5 3.0 Time Steps 1e5 0.0 0.2 0.4 0.6 0.8 1.0 (b) MountainCar-v0 0.5 1.0 1.5 2.0 2.5 3.0 Time Steps 1e5 0.0 0.2 0.4 0.6 0.8 1.0 DQN-L2 DQN-L1 DQN-RBF DQN-FTA (c) CartPole-v1 1 2 3 4 5 Time Steps 1e5 0.0 0.2 0.4 0.6 0.8 1.0 (d) LunarLander-v2 Figure 10: Overlap sparsity (number of simultaneously activated entries in the sparse feature vectors) v.s. number of time steps by averaging over 20 random seeds and the shade indicates standard error. 18Published as a conference paper at ICLR 2021 0 1 2 3 4 1e5 2000 1500 1000 500 DQN-L2 DQN-L1 DQN-RBF DQN-FTA TC-NN Figure 11: DQN-FTA compares with TCNN. Evaluation learning curves are averaged over 20 random seeds and the shade indicates standard error. All variants are trained without target networks. 0.0 0.5 1.0 1.5 2.0 Time Steps 1e5 500 50 Average Return per Episode reg-1.0 reg-0.01 reg-0.0 (a) Acrobot-v1 0 1 2 3 4 1e5 2000 1500 1000 500  (b) MountainCar-v0 0 1 2 3 4 1e5 100 200 300 400 500 (c) CartPole-v1 0 1 2 3 4 5 1e5 200 100 0 100 200 (d) LunarLander-v2 Figure 12: DQN-FTA trained with linear activation function with different regularization weight (-reg-1.0 means regularization is set as 1.0). One can see that our FTA isnot sensitive to this choice as those learning curves are almost overlapping with each other. Evaluation learning curves are averaged over 10 random seeds and the shade indicates standard error. All variants are trained without target networks. that this method itself is not a sparse representation learning technique, and requires the original observation space to be scaled within [0,1]. Figure 11 shows that TC-NN does indeed improve performance for DQN, but not compared to FTA. DQN-FTA both learns faster and reaches a better, more stable solution. In our TC-NN implementation, as a correspondent to FTA, we use binning operation to turn each raw input variable to 20 binary variables through binning operation. We use the same learning rate 0.0001 as other baselines. The result is generated by using FTA with a single tiling: [l,u] = [−20,20], δ= η= 2.0, and hence c = {−20,−18,−16,..., 18}, k= 40/2 = 20. A.5.3 I NSENSITIVITY TO OUT OF BOUNDARY LOSS We demonstrate the effect of using an out of boundary loss as discussed in Section A.3. This is supplementary to our experiments in the main body, where we did not use an out of boundary loss, i.e. the regularization weight is 0. To highlight the issue of out of boundary, we intentionally use a tiling vector c with a small range [−1,1] with 20 tiles, i.e. δ = 2/20 = 0.1 and set the sparsity control parameter η= δ= 0.1. In Figure 12, one can see that our algorithm typically does not need such a regularization even if we use a small tiling. A.5.4 FTA’ S STRONG PERFORMANCE WITH DIFFERENT TILE WIDTHS AND SPARSITY CONTROL PARAMETERS The purpose of this section is to show that given an appropriate bound of the tiling vector c, our approach is insensitive to tile width δand sparsity control parameter η. Since we already showed in Section 5 that DQN-FTA works well on LunarLander with tiling vector bound [−1,1], we ﬁx using this setting in this section. For both ηand δ, we sweep over 0.8/2i,i ∈{0,1,2,..., 8}. Note that this range is extreme (δ= 0.8 gives two bins, and η= 0.8 signiﬁcantly increases overlap); we do so to see a clear pattern. 19Published as a conference paper at ICLR 2021 Figure 13 shows the early learning performance of all possible 9 ×9 = 81 combinations of η,δ. We report the average episodic return (rounded to integers) within the ﬁrst 300k time steps, for each combination. The results are averaged over 5 runs. The pattern is as expected. 1) The algorithms performs best with a reasonably small δ and η. 2) For extremely small η and δ, performance is poor, as expected because the gradient is very small and so learning is slow. 3) Given a ﬁxed tile width δ, the performance degrades as ηbecomes smaller than δ, since again the activation has many regions with zero derivative. 4) If ηgets too large, the sparsity level increases and again performance degrades, though performance remained quite good for a broad range between 0.025 and 0.2. 5) If δ gets large (big bins), performance degrades more so that with larger η, which matches the intuition that ηprovides overlap rather than just increasing bin size and so losing precision. tile width eta 8 11 -65 -25 -24 25 -41 -118 -155 29 116 74 46 46 59 -66 -127 -138 68 105 124 95 28 43 -107 -115 -121 37 114 153 149 126 50 -101 -125 -145 31 96 153 173 175 47 -77 -105 -93 -24 114 149 164 138 10 -30 -81 -86 -5 61 108 121 124 -41 -16 48 -148 -117 31 73 67 15 -42 12 -99 -51 -104 24 15 30 62 -72 -85 -99 -123 Figure 13: Sensitivity of η,δ on LunarLander-v2. A.5.5 E MPIRICAL RESULTS WHEN USING FTA TO BOTH HIDDEN LAYERS As an activation function, FTA can be naturally applied in any hidden layer in a NN. As a convention, we typically use the same activation functions in a fully connected NN. In this section, we present such results on the reinforcement learning domains, as a supplement to the results in Section 5. All setting is the same as those in Section 5, except that we apply FTA to both hidden layers in DQN-FTA and DDPG-FTA. It should be noted that, in this case, DQN-FTA and DDPG-FTA have the same number of training parameters as their -Large correspondents respectively. Figure 14 shows the results on the discrete domains, and Figure 15 shows the results on the continuous control domains. It can be seen that, the FTA versions are better than ReLu versions in the case of not using a target network. This further validates the utility of our FTA in dealing with nonstationary problems. A.5.6 G RADIENT INTERFERENCE ANALYSIS We provide gradient interference analysis in this section. The results in this section are pro- duced by using FTA with a single tiling: [l,u] = [ −20,20], δ = η = 2 .0, and hence c = {−20,−18,−16,..., 18}, k= 40/2 = 20. We consider three measures for gradient interference. Let lθ be the DQN loss parameterized by θ. Given two random samples (i.e. experiences) X,X′from ER buffer, we estimate 1. m1: E[∇θlθ(X)⊤∇θlθ(X′)], this is to measure on average, whether the algorithm general- izes positively or interfere. 2. m2: E[∇θlθ(X)⊤∇θlθ(X′)] ONLY for those pairs of gradient vectors who have negative inner product. This is to check for those likely interfered gradient directions, how much they interfere with each other. 3. m3: Within a minibatch of pairs of (X,X′), the proportion of negative gradient inner products (i.e. if 32 out of 64 pairs has negative inner products, then the proportion is 50%). 20Published as a conference paper at ICLR 2021 0.0 0.5 1.0 1.5 2.0 Time Steps 1e5 500 50 Average Return per Episode DQN-FTA DQN DQN-Large (a) Acrobot (v1) 0.0 0.5 1.0 1.5 2.0 2.5 3.0 1e5 2000 1500 1000 500  (b) MountainCar (v0) 0.0 0.5 1.0 1.5 2.0 2.5 3.0 1e5 100 200 300 400 500 (c) CartPole (v1) 0 1 2 3 4 5 1e5 200 100 0 100 200 (d) LunarLander (v2) Figure 14: Evaluation learning curves of of DQN-FTA(black), DQN(red), and DQN-Large(blue), showing episodic return versus environment time steps. The dotted line indicates algorithms trained with target networks. The results are averaged over 20 runs and the shading indicates standard error. The learning curve is smoothed over a window of size 10 before averaging across runs. (a) InvertedPendu 0.0 0.5 1.0 1.5 2.0 1e6 0 500 1000 1500 2000 2500 (b) Hopper 0.0 0.5 1.0 1.5 2.0 1e6 0 1000 2000 3000 4000 (c) Walker2d 0 1 2 3 4 5 1e5 0 2000 4000 6000 8000 (d) InvertedDouble 0.0 0.5 1.0 1.5 2.0 1e6 0 25 50 75 100 125 150 (e) Swimmer Figure 15: Evaluation learning curves of DDPG-FTA(black), DDPG(red), and DDPG-Large(blue) on Mujoco environments, averaged over 20 runs with shading indicating standard error. All algorithms are trained without target networks. The learning curve is smoothed over a window of size 30 before averaging across runs. This is to roughly check how likely it is to get conﬂict gradient directions by randomly drawing two experiences from ER buffer. It should be noted that we normalize the gradient vectors to unit length before computing inner product. For each point in the learning curve, we randomly draw 64 samples from the ER buffer to estimate the interference. Figure 17 shows the algorithms’ performances in terms of number of time steps taken to reach a near-optimal policy: DQN-FTA >DQN-RBF > DQN-L1 >DQN-L2 ≥DQN (i.e. >means better (use fewer time steps) and ≥means slightly better). There are several interesting observations. First, in Figure 16(b)(e), for the weights in the second hidden layer (m2 measurement), DQN-RBF has lower interference strength than DQN-FTA, but DQN-FTA performs better. This discrepancy indicates that it is not necessarily true that the lower interference, the better. Second, DQN tends to over generalize during early learning, which may hurt and result in bad performance. Third, ﬁgure (e) shows that DQN-L1 is similar to DQN-FTA during late learning stage in terms of m2. But DQN-L1 seems to have extremely interfered gradient directions during early learning, which may explain why it learns slower than DQN-FTA. These observations may indicate that DQN-FTA helps generalize appropriately, but not overly generalize or highly interfered. We believe it is worth a separate work to thoroughly study the gradient interference issue to draw some interesting conclusions. A.5.7 T ESTING STABILITY IN A SIMULATED AUTONOMOUS DRIVING DOMAIN Our results have shown improved stability with FTA. In this section, we test FTA in an environment focused on stability, namely an autonomous driving task (Leurent, 2018). In the real world, a stable policy is of vital importance to ensure safety in autonomous driving. In this simulated task, the goal is not only to obtain high return, but also keep the number of car crashes as low as possible. FTA setting is the same as those in Section 5 on discrete control domains. Figure 18(a) shows the domain, where the agent—the green car—has to learn to switch lanes, avoid car crashes, and go as fast as possible. The observations are 25-dimensional, with vehicle dynamics that follow the Kinematic Bicycle Model. The action space is discrete. FTA learns faster, with signiﬁcantly fewer car crashes incurred during the evaluation time steps as shown in Figure 18(c). Target networks are harmful in this environment, potentially because they slow early learning; the agent to accumulate a signiﬁcant number of crashes before improving. 21Published as a conference paper at ICLR 2021 0.0 0.5 1.0 1.5 2.0 1e5 0.0 0.2 0.4 0.6 DQN-L2 DQN-L1 DQN-RBF DQN DQN-FTA (a) m1 0.0 0.5 1.0 1.5 2.0 1e5 0.8 0.6 0.4 0.2 0.0 DQN-L2 DQN-L1 DQN-RBF DQN DQN-FTA (b) m2 0.0 0.5 1.0 1.5 2.0 1e5 0.0 0.1 0.2 0.3 0.4 0.5 DQN-L2 DQN-L1 DQN-RBF DQN DQN-FTA (c) m3 0.0 0.5 1.0 1.5 2.0 1e5 0.0 0.2 0.4 0.6 0.8 1.0 DQN-L2 DQN-L1 DQN-RBF DQN DQN-FTA (d) m1 in 2nd 0.0 0.5 1.0 1.5 2.0 1e5 1.0 0.8 0.6 0.4 0.2 0.0 DQN-L2 DQN-L1 DQN-RBF DQN DQN-FTA (e) m2 in 2nd 0.0 0.5 1.0 1.5 2.0 1e5 0.0 0.1 0.2 0.3 0.4 0.5 DQN-L2 DQN-L1 DQN-RBF DQN DQN-FTA (f) m3 in 2nd Figure 16: Gradient inference measurements on mountain car domains, averaging over 10 runs with the shade indicating standard error. (d)(e)(f) are measured for those parameters in the second hidden layer only (which are supposed to directly affect representation). All algorithms are trained without using target networks except DQN. The curve is smoothed over a window of size20 before averaging across runs. 0.0 0.5 1.0 1.5 2.0 1e5 2000 100 DQN-L2 DQN-L1 DQN-RBF DQN-FTA Figure 17: Evaluation learning curves on mountain car. The results are averaged over 20 runs with the shade indicating standard error. All algorithms are trained without using target networks. A.6 R ESULTS ON IMAGE CLASSIFICATION TASKS We now report the empirical results on two popular image classiﬁcation tasks: MNIST (LeCun & Cortes, 2010) and Mnistfashion (Xiao et al., 2017). We found that FTA does not present any clear advantage or disadvantage in such a conventional supervised learning setting. On MNIST, FTA achieves testing error1.22%, and ReLu achieves 1.38%. On Mnistfashion, FTA achieves testing error 10.67%, and ReLu achieves 10.87%. Details. The NN architecture we use is two convolution layer followed by two fully connected 32 ×32 layers. The ﬁrst convolutional layer has 6 ﬁlters with size 5 ×5 and is followed by a max pooling operation. The second convolutional layer has 16 ﬁlters with the same size followed by a max pooling. FTA is applied to the second hidden layer and the setting is exactly the same as we used in the RL experiment A.4. To optimize learning rate, we optimize over the range 22Published as a conference paper at ICLR 2021 (a) Highway 0.0 0.5 1.0 1.5 Time Steps 1e4 18 20 30 32 Average Return per Episode DQN-FTA DQN (b) Evaluation LC 0 1 2 3 4 5 Time Steps 1e3 0 5 25 30 35 Cumulative Number of Car Crashes DQN-FTA DQN (c) Car Crashes LC Figure 18: (a) The Highway environment. (b) The evaluation learning curve. (c) The cumulative number of car crashes as a function of driving time steps. Results are averaged over 30 runs with shading indicating standard error. The learning curve is smoothed over a window of size 10 before averaging across runs. {0.00003,0.00001,0.0003,0.0001,0.003,0.001}. We use 10-fold cross validation to choose the best learning rate and the above error rate is reported on testing set by using the optimal learning rate at the end of learning. The standard deviation of the testing error (randomness comes from NN initialization and random shufﬂing) is sufﬁciently small to get ignored. A.7 C ONSTRUCTION OF PIECEWISE RANDOM WALK PROBLEM In Section 4, recall that we train with data generating process {(Xt,Yt)}t∈N so that interference difﬁculty is controllable, while permitting fair comparison via a ﬁxed equilibrium distribution. The temporal correlation in high difﬁculty {Xt}t∈N is designed to mimic state space trajectories through MDPs with a high degree of local state space connectedness. For example, an agent can only move to adjacent cells in GridWorld, the paddle and ball in Atari Breakout can only move so far in a single frame, and most successor states in Go only differ by a few pieces. Figure 19 depicts sample trajectories across a range of difﬁculties, alongside the ﬁxed equilibrium distribution. 0 500 1000 Training Iterations 1.0 0.5 0.0 0.5 1.0 Xt St = E[Xt] Equil. Density (0, 2) (a) d= 0 0 500 1000 Training Iterations 1.0 0.5 0.0 0.5 1.0 Xt St = E[Xt] Equil. Density (0, 2)  (b) d= 0.21 0 500 1000 Training Iterations 1.0 0.5 0.0 0.5 1.0 Xt St = E[Xt] Equil. Density (0, 2)  (c) d= 0.41 0 500 1000 Training Iterations 1.0 0.5 0.0 0.5 1.0 Xt St = E[Xt] Equil. Density (0, 2) (d) d= 0.62 0 500 1000 Training Iterations 1.0 0.5 0.0 0.5 1.0 Xt St = E[Xt] Equil. Density (0, 2)  (e) d= 0.83 0 500 1000 Training Iterations 1.0 0.5 0.0 0.5 1.0 Xt St = E[Xt] Equil. Density (0, 2)  (f) d= 0.98 Figure 19: Sample trajectories of {Xt}t∈N (blue) and {St}t∈N (black) across a range of difﬁculties. Note in particular the ﬁxed equilibrium distribution (gray) and i.i.d sampling for d= 0 (top left). Here we rigorously construct {Xt}t∈N and {St}t∈N, and show they have the claimed properties. Let random variable Et ∼N (ϵ; 0,σ2) be a source of Gaussian noise for the recursive random variable St 23Published as a conference paper at ICLR 2021 St+1 = (1 −c)St + Et (9) If c∈(0,1], then {St}t∈N is a linear Gaussian ﬁrst order autoregressive process, denoted AR(1).4 Then the equilibrium distribution of {St}t∈N is also Gaussian. If S0 is sampled from the equilibrium distribution, then the process {St}t∈N is stationary with E[St] = 0 and variance ν2 .= E[S2 t] = σ2 2c−c2 (10) It also follows that {St}t∈N is ergodic. See Grunwald et al. (1995) for reference on AR(1) processes, especially the discussion of equation (3.3) for linear Gaussian AR(1) processes, and section 5.3 for their stationarity and ergodicity. Let st be a realization of St. We deﬁne Xt|St = st as a Gaussian r.v. with mean st and variance β2: Xt|St = st ∼N(x; st,β2) (11) The process {Xt}t∈N is also a linear Gaussian AR(1) process, so we can again rely on established AR(1) process theory to conclude that theequilibrium distribution of {Xt}t∈N is Gaussian. Moreover, if X0 is sampled from the equilibrium distribution, then the process {Xt}t∈N is stationary with E[Xt] = 0 and variance ξ2 ξ2 .= E[X2 t] = β2 + ν2 = β2 + σ2 2c−c2 (12) Refer to Theorem 4 in Section A.7.3 for proof that {Xt}t∈N is a linear Gaussian AR(1) process, and that the resulting equilibrium distribution has variance speciﬁed by equation 12. Note there are several variance terms in the above construction. • σ2 is the noise variance in the random walk, i.e. the source of jumps in {St}t∈N • ν2 is the equilibrium distributon variance of {St}t∈N • β2 is the variance in any particular Xt given st at time step t • ξ2 is the equilibrium distribution variance of {Xt}t∈N The processes {St}t∈N and {Xt}t∈N are fully determined by the quantities c,σ2,β2. For our experiments we wish to have a single parameter dsuch that the equilibrium distribution of {Xt}t∈N is ﬁxed for all d∈[0,1), but temporal correlation is variable. Also, d= 0 should correspond to zero temporal correlation. The following two theorems specify this behaviour more rigorously. Theorem 2. Let difﬁculty parameter d∈[0,1) and ξ2 = ( B 2 )2 for some B ∈R+. Then {Xt}t∈N will have ﬁxed equilibrium distribution N(0,ξ2), invariant to d, if parameters c,σ2,β2 are set as follows c= 1 − √ 1 −d σ2 = d2(B 2 )2 β2 = (1 −d)(B 2 )2 ξ2 is deﬁned in terms of B ∈R+ simply because Bis a more intuitive design parameter. In particular, Bis a high probability bound with P(Xt ∈[−B,B]) ≤0.95 for all t, since {Xt}t∈N is ergodic. Theorem 3. d = 0 and S0 = 0 induces i.i.d Xt from N(x; 0,ξ2), the equilibrium distribution of {Xt}t∈N. 4If c= 0, then St is a simple Gaussian random walk. We do not use such a process, because sample paths are unbounded, hitting ± √ tinﬁnitely often as t→∞ 24Published as a conference paper at ICLR 2021 We defer proof of both theorems to Section A.7.3. Correlation difﬁculty dis intuitively characterized as follows. • As dincreases, β2 decreases, so a smaller portion of the overall state space is supported at any given time. i.e. P(Xt = x|St = st) becomes increasingly narrow, inducing higher temporal correlation. • As dincreases, the noise in random walk {St}t∈N increases in amplitude, so that larger jumps in the mean of any particular Xt are more likely. • At d= 0, all correlation difﬁculty from {Xt}t∈N has been removed, in that we recover iid sampling. • As d→1, {Xt}t∈N converges towards pathological correlation, withXt d − →δ(st), where δ(·) is the Dirac delta distribution. i.e. Xt becomes constant everywhere except the jumps in realization st of St. Despite the above relationships between dand the other process parameters, the equilibrium distribu- tion N(x; 0,ξ2) is identical for all difﬁculty settings d∈[0,1), because the increase (or decrease) in parameters σ2,c is tuned speciﬁcally to counteract the decrease (or increase) in β2. Having identical equilibrium distributions means that a hypothetical ideal algorithm, capable of perfectly mitigating interference, would train the identical approximatorfθ for any correlation difﬁculty d∈[0,1). Hence, we use approximation loss on the equilibrium distribution as a measure of robustness to interference. To summarize, correlation difﬁculty d ∈[0,1) controls the likelihood of interference-inducing samples throughout training, but also permits fair comparison between values of dby the (ﬁxed) equilibrium distribution of {Xt}t∈N. A.7.1 P IECEWISE RANDOM WALK ADDITIONAL PLOTS Figure 20 depicts learning curves of loss over training time at different levels of correlation difﬁculty. Figure 21 shows learning rate sensitivity curves for the FTA and ReLU networks for three different difﬁculty settings. The ADAM optimizer Kingma & Ba (2015) was used in all experiments. For the experiment in Figure 3, where one sample from each Xt is used for each weight update, FTA outperforms ReLU even on i.i.d data (d= 0). In order to ﬁnd the conditions under which ReLU and FTA both perform equally well, we repeat the same experiment, but with 50 samples drawn from each Xt. Figure 23 depicts the results, where the performance is indeed equalized for iid sampling (d= 0), but ReLU’s performance still diverges similarly to other experiments as correlation difﬁculty is increased. The size of the FTA and ReLU neural networks used on the Piecewise Random Walk Problem were chosen based on best performance on iid data, and the best performers do not have the same number of parameters. (67K vs 5.2K learnable parameters.) In order to verify that the difference in parameter count was not a signiﬁcant factor in handling high correlation difﬁculty, we additionally include results for a ReLU network with wider hidden layers totalling 81K learnable parameters. See Figure 22. Both ReLU networks perform very similarly—in most cases agreeing within p= 0.05—with loss steeply running away as correlation difﬁculty increases. So we conclude FTA’s robustness to high correlation difﬁculties is not explained simply by parameter count. A.7.2 P IECEWISE RANDOM WALK PROBLEM HYPERPARAMETER SELECTION Experimental Parameters Conﬁguration for Piecewise Random Walk problem across all experiments: 25Published as a conference paper at ICLR 2021 0 5000 10000 15000 20000 Training Iterations 0.0 0.1 0.3 0.4 Loss Over Equilib. Dist. d=0.88 d=0 (a) FTA 0 5000 10000 15000 20000 Training Iterations d=0.88 d=0 (b) ReLU 0.0 0.2 0.4 0.6 0.8 1.0 Covariate Shift Difficulty 0.00 0.05 0.15 Mean Final Loss FTA ReLU (c) ReLU and FTA, d∈[0,1) Figure 20: Left, Middle: Learning curve of loss over stationary distribution during training on low difﬁculty (dotted) and high difﬁculty (solid) settings for two layer neural nets. The curves are smoothed over a window of 50. Right: The ﬁnal loss over stationary distribution after 20K training iterations across a range of difﬁculty settings, shown as the mean of 30 runs with the shaded region corresponding to p = 0.001. The ﬁnal loss per run is computed as the mean over the ﬁnal 2.5K iterations, with the iid setting d= 0 (dotted) shown for baseline comparison. 10 5  10 4  10 3  10 2 Learning Rate 10 3 10 2 10 1 100 Mean Final Loss d= 0.00 FTA ReLU (a) Lowest Difﬁculty 10 5  10 4  10 3  10 2 Learning Rate 10 3 10 2 10 1 100 Mean Final Loss d= 0.41 FTA ReLU (b) Moderate Difﬁculty 10 5  10 4  10 3  10 2 Learning Rate 10 3 10 2 10 1 100 Mean Final Loss d= 0.98 FTA ReLU (c) High Difﬁculty Figure 21: Learning rate sensitivity of FTA and ReLU for iid, mildly correlated, and severely correlated Xt (left, middle, right, respectively.) Final loss performance is shown as the mean of 30 runs with the shaded region corresponding p= 0.001. These curves corroborate our ﬁndings that, in general, FTA prefers lower learning rates (but converges more quickly nonetheless.) 0 5000 10000 15000 20000 Training Iterations 0.0 0.1 0.3 0.4 Loss Over Equilib. Dist. d=0.88 d=0 (a) FTA 0 5000 10000 15000 20000 Training Iterations d=0.88 d=0 (b) ReLU-large 0.0 0.2 0.4 0.6 0.8 1.0 Covariate Shift Difficulty 0.00 0.05 0.15 Mean Final Loss FTA ReLU ReLU (large) (c) ReLU-large and FTA, d∈[0,1) Figure 22: Left, Middle: Learning curve for single run using loss over stationary distribution during training on low difﬁculty (dotted) and high difﬁculty (solid) settings for two layer neural nets, both having similar numbers of learnable parameters. The curves are smoothed over a window of 50. Right: The ﬁnal loss over stationary distribution after 20K training iterations across a range of difﬁculty settings, shown as the mean of 10 runs with the shaded region corresponding p= 0.05. The ﬁnal loss per run is computed as the mean over the ﬁnal 2.5K iterations, with the iid setting d= 0 (dotted) shown for baseline comparison. 26Published as a conference paper at ICLR 2021 0.0 0.2 0.4 0.6 0.8 1.0 Covariate Shift Difficulty 0.00 0.05 0.15 Mean Final Loss FTA ReLU Figure 23: The ﬁnal loss over stationary distribution after 20K training iterations across a range of difﬁculty settings, shown as the mean of 10 runs with the shaded region corresponding p= 0.001. For each weight update, a batch of 50 samples are drawn from each Xt, rather than a single sample as was done in other similar ﬁgures. Iterations per training run 20K Test batch size per iteration 100 Train batch size per iteration 1 Min, max difﬁculty settings d (0.0, 0.98) Number of difﬁculty settings swept 20 (linear range over (min, max), inclusive) Training runs per difﬁculty setting 10 for ReLU Large (Figure 22) 10 for batched run (Figure 23) 30 otherwise Number of piecewise stationary segments 50 Target function Yt = sin(2πX2 t) High probability bound B (−1,1) Conﬁguration for the neural networks used on the Piecewise Random Walk problem: FTA Hidden layers 2 Layer width w 40 Layer binning interval bounds (l,u) ( −1,1) Layer bins k 40 Layer sparsity parameter η 1 40 ReLU Hidden layers 2 Layer width w 50 ReLU Large Hidden layers 2 Layer width w 200 Layer widths w, bin count k, and sparsity parameterηwere chosen by sweeping a range, and choosing the best performing conﬁguration on the iid setting (i.e. difﬁculty d= 0). In the sweep, the same w,k,η are used for each hidden layer of FTA, and the same wis used for each hidden layer of ReLU. See the Parameter Selection Details below. For all three networks, the ADAM optimizer Kingma & Ba (2015) was used with parameters (β1,β2) = (0.9,0.999), and the following learning rates were swept for each difﬁculty setting. Learning rates λ 0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005, 0.00001, 0.000005 The results from the best performing learning rate are reported in Figures 3(c), 20(c), 22(c), and 23. That is, the performing learning rate is determined separately for each difﬁculty setting, using the mean ﬁnal loss across 30 runs for the FTA and ReLU networks. 10 runs were used for ReLU Large and the batching run. Results from all learning rates are reported in Figure 21 for three different difﬁculty settings. Parameter Selection Details 27Published as a conference paper at ICLR 2021 Before sweeping any difﬁculty settings, ﬁrst the network architectures were established for the FTA and ReLU networks so that loss was minimized on the iid setting ( d = 0 ). We avoid the overparametrized regime, so that standard bias-variance tradeoff applies here Belkin et al. (2019). (However, the ReLU Large network is overparametrized.) Learning rate was also swept during the architecture sweep, to ensure that the chosen architecture performed best within the learning rate range chosen for the main experiment depicted in Figure 3(c). For the 2 layer FTA network, each hidden layer uses the same width w, and sparsity parameter η. Those parameters, along with learning rate λwere optimized in the following ranges with grid search: • Layer width w∈{10,15,20,30,40} • Sparsity parameter η= 1 w • Learning rate λ∈{0.0005,0.0001,0.00005,0.00001,0.000005} For the 2 layer ReLU network, fully connected hidden layers were used. Width wand learning rate λ were found from the following ranges, also with grid search: • Layer width w∈{5,10,20,30,40,50,60,70,80,100,120} • Learning rate λ∈{0.005,0.001,0.0005,0.0001,0.00005} The learning rates are of different magnitude between FTA and ReLU because they perform best in different ranges, as explained in A.7.1. For the main experiments, where a range of difﬁculties are swept, the range of tested learning rates is shared between FTA and ReLU, and is broad enough to cover the optimal range for both. Also note that we set η = 1 k, rather than separately sweeping it. This is because FTA performs consistently well with ηin this order of magnitude. A.7.3 P ROOFS FOR COVARIATE SHIFT PROPERTIES Theorem 4. Let {Et}t∈N,{St}t∈N,{Xt}t∈N be stochastic processes such that Et ∼N(ϵ; 0,σ2) St+1 = (1 −c)St + Et (13) Xt|St = st ∼N(x; st,β2) (14) where c∈(0,1] and st is a realization of St. Then {Xt}t∈N is a linear Gaussian AR(1) process with equilibrium distribution N(x; 0,β2 + σ2 2c−c2 ). Proof. Begin by rewriting equation 14 to give Xt as a sum of St and a mean zero Gaussian r.v. Bt Xt = St + Bt Bt ∼N(0,β2), iid (15) Now substitute according to equation 13 Xt = (1 −c)St−1 + Et−1 + Bt (16) Let Gaussian r.v. Θ ∼N(0,α2) with some variance α2. Et−1,Bt are independent Gaussian, so Et−1 +Bt can be written as a linear combination ofBt−1,Θt, since all are independent. Speciﬁcally, ﬁx α2 so that the following holds Et−1 + Bt = (1 −c)Bt−1 + Θt (17) and substitute into equation 16 28Published as a conference paper at ICLR 2021 Xt = (1 −c)St−1 + (1 −c)Bt−1 + Θt = (1 −c)(St−1 + Bt−1) + Θt = (1 −c)Xt−1 + Θt By inspection, {Xt}t∈N is a linear Gaussian AR(1) process with coefﬁcient(1−c) and noise variance α2. Elementary AR process theory gives the equilibrium distribution as N ( 0, α2 1 −(1 −c)2 ) (18) For equation 17 to hold, we need the ﬁrst and second moments of LHS and RHS to be equal. All terms are mean zero Gaussian, so it sufﬁces to show when the LHS and RHS have equal variance: Var(Et−1 + Bt) = Var((1−c)Bt−1 + Θt) σ2 + β2 = (1 −c)2β2 + α2 α2 = σ2 + β2 −(1 −c)2β2 α2 = σ2 + (1 −(1 −c)2)β2 Substituting into equation 18 N ( 0, α2 1 −(1 −c)2 ) = N ( 0,σ2 + (1 −(1 −c)2)β2 1 −(1 −c)2 ) = N ( 0, σ2 1 −(1 −c)2 + β2 ) = N ( 0, σ2 2c−c2 + β2 ) So {Xt}t∈N is linear Gaussian AR(1) with the desired equilibrium distribution. Theorem 2 Let difﬁculty parameter d∈[0,1) and ξ2 = (B 2 )2 for some B ∈R+. Then {Xt}t∈N will have ﬁxed equilibrium distribution N(0,ξ2), invariant to d, if parameters c,σ2,β2 are set as follows c= 1 − √ 1 −d σ2 = d2(B 2 )2 β2 = (1 −d)(B 2 )2 Proof. By the above theorem 4, ξ2 = β2 + σ2 2c−c2 = (1 −d)(B 2 )2 + d2(B 2 )2/d = ( B 2 )2. This completes the proof. Theorem 3 d= 0 and S0 = δ(0) induces iid Xt from N(x; 0,ξ2), the equilibrium distribution of {Xt}t∈N. 29Published as a conference paper at ICLR 2021 Proof. St+1 = (1 −c)St + Et (by9) = St + Et (d= 0 =⇒ c= 0) = St (d= 0 =⇒ σ2 = 0 =⇒ Et ∼δ(0)) Hence, d= 0 implies St is constant over all time t, and S0 = δ(0) gives St = δ(0) for all time t. Let r.v. Bt ∼N(0,β2) iid, then Xt = St + Bt = Bt when St is Dirac delta concentrating at zero. β2 = (1 −d)(B 2 )2 = (B 2 )2 by the setting from Theorem 2 and setting d= 0. Hence, d= 0 induces iid sampling from the equilibrium distribution of Xt. This completes the proof. 30",
      "meta_data": {
        "arxiv_id": "1911.08068v3",
        "authors": [
          "Yangchen Pan",
          "Kirby Banman",
          "Martha White"
        ],
        "published_date": "2019-11-19T03:12:06Z",
        "pdf_url": "https://arxiv.org/pdf/1911.08068v3.pdf"
      }
    },
    {
      "title": "Online Learned Continual Compression with Adaptive Quantization Modules",
      "abstract": "We introduce and study the problem of Online Continual Compression, where one\nattempts to simultaneously learn to compress and store a representative dataset\nfrom a non i.i.d data stream, while only observing each sample once. A naive\napplication of auto-encoders in this setting encounters a major challenge:\nrepresentations derived from earlier encoder states must be usable by later\ndecoder states. We show how to use discrete auto-encoders to effectively\naddress this challenge and introduce Adaptive Quantization Modules (AQM) to\ncontrol variation in the compression ability of the module at any given stage\nof learning. This enables selecting an appropriate compression for incoming\nsamples, while taking into account overall memory constraints and current\nprogress of the learned compression. Unlike previous methods, our approach does\nnot require any pretraining, even on challenging datasets. We show that using\nAQM to replace standard episodic memory in continual learning settings leads to\nsignificant gains on continual learning benchmarks. Furthermore we demonstrate\nthis approach with larger images, LiDAR, and reinforcement learning\nenvironments.",
      "full_text": "Online Learned Continual Compression with Adaptive Quantization Modules Lucas Caccia 1 2 3 Eugene Belilovsky 2 4 Massimo Caccia 2 4 5 Joelle Pineau 1 2 3 Abstract We introduce and study the problem of Online Continual Compression, where one attempts to simultaneously learn to compress and store a rep- resentative dataset from a non i.i.d data stream, while only observing each sample once. A naive application of auto-encoders in this setting en- counters a major challenge: representations de- rived from earlier encoder states must be usable by later decoder states. We show how to use discrete auto-encoders to effectively address this challenge and introduce Adaptive Quantization Modules (AQM) to control variation in the com- pression ability of the module at any given stage of learning. This enables selecting an appropriate compression for incoming samples, while taking into account overall memory constraints and cur- rent progress of the learned compression. Unlike previous methods, our approach does not require any pretraining, even on challenging datasets. We show that using AQM to replace standard episodic memory in continual learning settings leads to sig- niﬁcant gains on continual learning benchmarks. Furthermore we demonstrate this approach with larger images, LiDAR, and reinforcement learn- ing agents. 1. Introduction Interest in machine learning in recent years has been fueled by the plethora of data being generated on a regular basis. Effectively storing and using this data is critical for many applications, especially those involving continual learning. In general, compression techniques can greatly improve data storage capacity, and, if done well, reduce the memory and compute usage in downstream machine learning tasks (Gueguen et al., 2018; Oyallon et al., 2018). Thus, learned compression has become a topic of great interest (Theis 1McGill 2Mila 3Facebook AI Research 4University of Montreal 5ElementAI. Correspondence to: Lucas Caccia <lucas.page-caccia@mail.mcgill.ca>, Eugene Belilovsky <eu- gene.belilovsky@umontreal.ca>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). et al., 2017; Ball´e et al., 2016; Johnston et al., 2018). Yet its application in reducing the size of datasets bound for machine learning applications has been limited. This work focuses on the following familiar setting: new training data arrives continuously for a learning algorithm to exploit, however this data might not be iid, and further- more there is insufﬁcient storage capacity to preserve all the data uncompressed. We may want to train classiﬁers, reinforcement learning policies, or other models continu- ously from this data as it’s being collected, or use samples randomly drawn from it at a later point for a downstream task. For example, an autonomous vehicle (with bounded memory) collects large amounts of high-dimensional train- ing data (video, 3D lidar) in a non-stationary environment (e.g. changing weather conditions), and overtime applies an ML algorithm to improve its behavior using this data. This data might be transferred at a later point for use in down- stream learning. Current learned compression algorithms, e.g. (Torfason et al., 2018), are not well designed to deal with this case, as their convergence speed is too slow to be usable in an online setting. In the ﬁeld of continual/lifelong learning (Thrun & Mitchell, 1995), which has for now largely focused on classiﬁcation, approaches based on storing memories for later use have emerged as some of the most effective in online settings (Lopez-Paz et al., 2017; Aljundi et al., 2018; Chaudhry et al.; 2019; Aljundi et al., 2019). These memories can be stored as is, or via a generative model (Shin et al., 2017). Then, they can either be used for rehearsal (Chaudhry et al., 2019; Aljundi et al., 2019) or for constrained optimization (Lopez-Paz et al., 2017; Chaudhry et al.; Aljundi et al., 2018). Indeed many continual learning applications would be greatly improved with replay approaches if one could afford to store all samples. These approaches are however inherently limited by the amount of data that can be stored Learning a generative model to compress the previous data stream thus seems like an appealing idea. However, learn- ing generative models, particularly in the online and non- stationary setting, continues to be challenging, and can greatly increase the complexity of the continual learning task. Furthermore, such models are susceptible to catas- trophic forgetting (Aljundi et al., 2019). An alternate ap- proach is to learn a compressed representation of the data, arXiv:1911.08019v3  [cs.LG]  20 Aug 2020Online Learned Continual Compression with Adaptive Quantization Modules Online Continual Compression z_1 z_1 z_2 z_3z_1 z_2 X ~ non-iid t=1 t=2 t=3 query  samples query  samples Learner Learner Encoder Encoder Encoder Decoder     t=3 Decoder     t=1 Decoder     t=2 storage storage storage  Decoder     t=3 z_1 Decoder     t=1 Representation Drift z_1 X_3 X_2 X_1 Figure 1.Illustration of the challenges in the Online Continual Compression problem. A model must be able to decode representations encoded by previous versions of the autoencoder, permitting anytime access to data for the learner. This must be accomplished while dealing with a time-varying data distribution and ﬁxed memory constraints which can be more stable than learning generative models. Learned compression in the online and non-stationary set- ting itself introduces several challenges illustrated in Fig 1. Firstly the learned compression module must be able to de- code representations encoded by earlier versions of itself, introducing a problem we refer to as representation drift. Secondly, the learned compressor is itself susceptible to catastrophic forgetting. Finally, the learned compression needs to be adaptive to maintain a prescribed level of re- construction quality even if it has not fully adapted to the current distribution. In this work we demonstrate that the VQ-V AE framework (van den Oord et al., 2017; Razavi et al., 2019), originally introduced in the context of generative modeling and density estimation, can be used online to effectively address repre- sentation drift while achieving high compression. Further- more, when augmented with an internal replay mechanism it can overcome forgetting. Finally we use propose to use multiple gradient-isolated compression levels to allow the compressor to adaptively store samples at different compres- sion scales, based on the amount of data, storage capacity, and effectiveness of the model in compressing samples. The main contributions of this work are: (a) we intro- duce and highlight the online learned continual compres- sion (OCC) problem and its challenges. (b) We show how representation drift, one of the key challenges, can be tackled by effective use of codebooks in the VQ-V AE framework. (c) We propose an architecture using multi- ple VQ-V AE’s, adaptive compression scheme, stream sam- pling scheme, and self-replay mechanism that work together to effectively tackle the OCC problem. (d) We demon- strate this can yield state-of-the-art performance in stan- dard online continual image classiﬁcation benchmarks and demonstrate the applications of our OCC solution in a va- riety of other contexts. The code to reproduce our results is available at https://github.com/pclucas14/ adaptive-quantization-modules 2. Related Work Learned compression has been recently studied for the case of image compression. Works by Theis et al. (2017); Ball´e et al. (2016); Johnston et al. (2018) have shown learned compressions can outperform standard algorithms like JPEG. These methods however are difﬁcult to adapt for online settings as they do not directly address the challenges of the OCC problem (e.g. representation drift). Continual Learning research currently focuses on over- coming catastrophic forgetting (CF) in the supervised learn- ing setting, with some limited work in the generative mod- eling and reinforcement learning settings. Most continual learning methods can be grouped into three major families. Some algorithms dynamically change the model’s architec- ture to incorporate learning from each task separately (Rusu et al., 2016; Li & Hoiem, 2018; Fernando et al., 2017). Al- though these methods can perform well in practice, their introduction of task-speciﬁc weights requires growing com- pute and memory costs which are problematic for the online setting. Another set of techniques employ regularization to constrain weights updates in the hope of maintaining knowl- edge from previous tasks. Notable methods in this class include (Kirkpatrick et al., 2017; Husz´ar, 2017; Zenke et al., 2017; Nguyen et al., 2017; Chaudhry et al., 2018). However, this set of approaches has been shown to be inefﬁcient in the online setting (Chaudhry et al., 2019). The last family of methods encapsulates all that have a mech- anism to store information about the previous data distribu- tions. This memory then serves as a tool for the continual learner to rehearse previous tasks. The simplest instantia- tion of this method is to keep and sample from a buffer of old data to retrain the model after every update (Chaudhry et al., 2019). This approach is widely used in RL where it is known as Experience Replay (ER) (Lin, 1993; Mnih et al., 2013; Andrychowicz et al., 2016). Another method, known as Generative Replay (GR) (Shin et al., 2017), uses generative modeling to store past task distributions. The con- tinual learner then trains on generated samples to alleviateOnline Learned Continual Compression with Adaptive Quantization Modules CF. Other notable examples are Gradient Episodic Memory (GEM) (Lopez-Paz et al., 2017), iCarl (Rebufﬁ et al., 2017), and Maximally Interfered Retrieval (MIR) (Aljundi et al., 2019), as well as (Aljundi et al., 2018; Hu et al., 2018). Most closely related to our work, Riemer et al. (2017) consider compressing memories for use in the continual classiﬁcation task. They also employ a discrete latent variable model but with the Gumbel approximation, which shows to be less effective than our approach. Furthermore a separate ofﬂine iid pre-training step for the learned compression is required in order to surpass the ER baseline, distinctly different from the online continual compression we consider. Lidar compression is considered in (Tu et al., 2019) and (Caccia et al., 2018). Both approaches use a similar projec- tion from 3D (x,y,z ) coordinates to 2D cylindrical coordi- nates, and leverage deep generative models to compress the data. However, neither accounts for potential distribution shift, nor for online learning. In this work we show that us- ing this 2D projection in conjunction with our model allows us to mitigate the two issues above for lidar data. 3. Methodology In this section we outline our approach to the online con- tinual compression problem. First we will review the VQ- V AE and highlight the properties making it effective for representational drift. Then we will describe our adaptive architecture, storage, and sampling scheme. 3.1. Problem Setting: Online Continual Compression We consider the problem setting where a stream of samples x ∼Dt arrives from different distributions Dt changing over time t= 1...T . We have a ﬁxed storage capacity ofC bytes where we would like to store the most representative information from all data distributions D1,...DT . There is notably a trade-off in quality of information versus the amount of samples stored. We propose to use a learned compression model, and most crucially, this model must also be stored within the Cbytes, to encode and decode the data samples. Another critical requirement is that at anytime tthe content of the storage (data and/or compression model) be usable for downstream applications. An important chal- lenge, illustrated in Figure 1, is that the learned compression module will change over time, while we still need to be able to decode the memories in storage. 3.2. Vector Quantized V AE for Online Compression The VQ-V AE is a discrete auto-encoder which relies on a vector quantization step to obtain discrete latent repre- sentations. An embedding table, E ∈RK×D consisting of K vectors of size D, is used to quantize encoder out- puts. Given an input (e.g. an RGB image), the encoder ﬁrst encodes it as a Hh ×Wh ×D tensor, where Hh Encoder     t=1      Encoder     t=3      same after VQ Vector Quantization different codebook Figure 2.Illustration of reduced representation drift from Vector Quantization and Wh denote the height and width of the latent repre- sentation. Then, every Ddimensional vector is quantized using a nearest-neighbor lookup on the embedding table. Speciﬁcally, zij = arg mine∈E ||enc(x)ij −e||2, where i,j refers to a spatial location. The output of the quanti- zation step is then fed through the decoder. The gradient of this non-differentiable step is approximated using the straight-through estimator. An important property to notice is that to reconstruct the input, only the Hh ×Wh indices are required, thus yielding high compression (van den Oord et al., 2017). Critically, the embedding tables are updated independently from the encoder and decoder, namely by minimizing mine ||sg[enc(x)ij] −e||, where sg is the stop gradient operator. Observe in the case of online compression, if the embedding table is ﬁxed, then a change in the encoder parameters and therefore a change in the encoder output for a given input will not change the ﬁnal quantized representation z, unless it is sufﬁciently large, thus we can observe that if the em- beddings change slowly or are ﬁxed we can greatly improve our control of the representational drift. This effect is illus- trated in Figure 2. On the other hand we do need to adapt the embedding table, since randomly selected embeddings would not cover well the space of encoder outputs. 3.3. Adaptive Quantization Modules To address issues of how to optimize storage and sampling in the context of Online Continual Compression we intro- duce Adaptive Quantization Modules (AQM). We use AQM to collectively describe the architecture, adaptive multi-level storage mechanism, and data sampling method used. To- gether they allow effectively constraining individual sample quality and memory usage while keeping in storage a faith- ful representation of the overall distribution.Online Learned Continual Compression with Adaptive Quantization Modules Algorithm 1: AQM L EARNING WITH SELF -REPLAY Input: Learning rate α, EXTERNAL LEARNER 1 Initialize: AQM Memory M; AQM Parameters θaqm 2 for t∈1..T do 3 % Fetch data from current task 4 for Binc ∼Dt do 5 for n∈1..N do 6 B←Binc 7 if t> 1 then 8 % Fetch data from buffer 9 Bre ∼SAMPLE (M,θaqm) 10 B ←(Binc,Bre) 11 end 12 % Update AQM 13 θaqm ←ADAM(θaqm,B,α ) 14 % Send data to external learner 15 EXTERNAL LEARNER (B) 16 if t> 1 then UPDATE BUFFER REP(M,θaqm) 17 %Save current indices 18 ADDTOMEMORY (M,Binc,θaqm) 19 end 20 end 21 end AQM uses an architecture consisting of a sequence of VQ- V AEs, each with a buffer. The AQM approach to online continual compression is overall summarized in Algorithm 1 (note ADDTOMEMORY is described in Appendix A). In- coming data is added to storage using an adaptive compres- sion scheme described in Algorithm 2. Incoming data is also used along with randomly sampled data from storage (self- replay) to update the current AQM model. The randomly sampled data also updates its representation in storage as per Algorithm 3. As illustrated by the optional lines in blue, Algorithm 1 can run concurrently with a downstream learn- ing task (e.g. online continual classiﬁcation) which would use the same batch order. It can also be run independently as part of a data collection. In the sequel we give further details on all these elements 3.3.1. A RCHITECTURE AND TRAINING Each AQM module contains a VQ-V AE and a corresponding buffer of adaptive capacity. A diagram of the architecture is given in Figure 3. We will denote the output after quan- tization of each module i as zi q and the set of codebook indexes used to obtain zi q as ai. Note that ai are the discrete representations we actually store. Each subsequent module produces and stores an ai requiring fewer bits to represent. For RGB images, the compression rate at a given level is given by H×W×3×log2(256) Nc×Hhi×Whi×⌈log2 (Ki)⌉. Here Ki is the number of embeddings in the codebooks, ( Hhi, Whi) the spatial dimension of the latent representation and Nci the number of codebooks. Algorithm 2: ADAPTIVE COMPRESS Input: datapoint x, AQM with Lmodules, threshold dth 1 % Forward all modules, store encodings 2 {zi q,ai}i=1..L= ENCODE (x) 3 for i∈L...1 do 4 % Decode from level i to output space 5 ˆx= DECODE (zi q) 6 % Check reconstruction error 7 if MSE(ˆx,x) <dth then return ai, i 8 end 9 % Otherwise, return original input 10 return x,0 Algorithm 3: UpdateBufferRep Input: Memory M, AQM with Llevels, data D, distortion threshold dth 1 for x∈Ddo 2 hidx, blockid = ADAPTIVE COMPRESS (x, AQM, dth) 3 % Delete Old Repr. 4 DELETE (M[x]) 5 % Add new one 6 ADD(M, hidx) 7 end VQV AE-2 (Razavi et al., 2019) also uses a multi-scale hier- archical organization, where unlike our AQM, the top level models global information such as shape, while the bottom level, conditioned on the top one, models local information. While this architecture is tailored for generative modeling, it is less attractive for compression, as both the bottom and top quantized representations must be stored for high qual- ity reconstructions. Furthermore in AQM each module is learned in a greedy manner using the current estimate of z(i−1) q without passing gradients between modules similar to (Belilovsky et al., 2019; Nøkland & Eidnes, 2019). A subsequent module is not required to build representations which account for all levels of compression, thus minimiz- ing interference across resolutions. This allows the modules to each converge as quickly as possible with minimal drift at their respective resolution, particularly important in the online continual learning case. 3.3.2. M ULTI -LEVEL STORAGE Our aim is to store the maximum number of samples in an allotted Cbytes of storage, while assuring their quality, and our ability to reconstruct them. Samples are thus stored at different levels based on the compressors’ current ability. The process is summarized in Algorithm 2. Such an approach is particularly helpful in the online non- stationary setting, allowing knowledge retention before the compressor network has learned well the current distribution. Note in Alg. 2 samples can be completely uncompressed until the ﬁrst module is able to effectively encode them. This can be crucial in some cases, if the compressor has not yet converged, to avoid storing poorly compressed rep-Online Learned Continual Compression with Adaptive Quantization Modules Figure 3.Architecture of Adaptive Quantization Modules. Each level uses its own loss and maintains its own replay buffer. Yello dotted lines indicate gradient isolation between modules resentations. Further taking into account that compression difﬁculty is not the same for all datapoints, this allows use of more capacity for harder data, and fewer for easier. We also note, since we maintain stored samples at each mod- ule and the modules are decoupled, that such an approach allows to easily distribute training in an asynchronous man- ner as per Belilovsky et al. (2019). 3.3.3. S ELF -REPLAY AND STREAM SAMPLING As shown in Alg. 1 our AQM is equipped with an internal experience replay mechanism (Mnih et al., 2013), which reconstructs a random sample from storage and uses it to perform an update to the AQM modules, while simultane- ously freeing up overall memory if the sample can now be compressed at a later AQM module. This has the effect of both reducing forgetting and freeing memory. In practice we replay at the same rate as incoming samples arrive and thus replay will not increase asymptotic complexity of the online learning. Finally, for efﬁciency the replay can be coupled to an external online learner querying for random samples from the overall memory. Since we would like the AQM to work in cases of a ﬁxed memory capacity it must also be equipped with a mecha- nism for selecting which samples from the stream to store and which to delete from memory. Reservoir Sampling (RS) is a simple yet powerful approach to this problem, used successful in continual learning (Chaudhry et al., 2019). It adds a sample from the stream with prob. p= buffer capacity points seen so far while remove a random sample. However, RS is not directly compatible with AQM primarily because the amount of sam- ples that can be stored varies over time. This is because samples at different levels have different memory usage and memory can be freed by replay. We thus propose an alternative scheme, which maximally ﬁlls available memory and selects non-uniformly samples for deletion. Speciﬁ- cally when a larger amount of samples are added at one point in the stream, they become more likely to be removed. The details of this stream sampling method are provided in Appendix A. 3.3.4. DRIFT CONTROL VIA CODEBOOK STABILIZATION As mentioned previously, a good online compressor must control its representational drift, which occurs when up- dates in the auto-encoder parameters creates a mismatch with the static representations in the buffer. Throughout the paper we measure representational drift by compar- ing the following time varying quantity: DRIFT t(z) = RECON ERR ( Decode(θt; z),x ) where θt the model pa- rameters at timetand (z,x) is a stored compressed represen- tation and its original uncompressed datapoint respectively. For all experiments on images, RECON ERR is simply the mean squared error. As illustrated in Sec 3.2 a slow changing codebook can allow to control drifting representations. This can be in part accomplished by updating the codebook with an exponential moving average as described in (van den Oord et al., 2017, Appendix A), where it was used to reduce the variance of codebook updates. This alone is insufﬁcient to fully control drift, thus once a given module yields satisfactory compressions on the data stream, we freeze the module’s embedding matrix but leave encoder and decoder parameters free to change and adapt to new data. Moreover, we note that ﬁxing the codebook for a given module does not affect the reconstruction performance of subsequent modules, as they only need access to the current module’s decoder which can still freely change. 4. Experiments We evaluate the efﬁcacy of the proposed methods on a suite of canonical and new experiments. In Section 4.1 we present results on standard supervised continual learning benchmarks on CIFAR-10. In Section 4.2 we evaluate other downstream tasks such as standard iid training applied on the storage at the end of online continual compression. For this evaluation we consider larger images from Imagenet, as well as on lidar data. Finally we apply AQM on observations of an agent in an RL environment.Online Learned Continual Compression with Adaptive Quantization Modules Accuracy (↑) M= 20 M= 50 iid online 60.8 ±1.0 60 .8 ±1.0 iid ofﬂine 79.2 ±0.4 79 .2 ±0.4 GEM (Lopez-Paz et al., 2017) 16.8 ±1.1 17 .1 ±1.0 iCarl (5 iter) (Rebufﬁ et al., 2017) 28.6 ±1.2 33 .7 ±1.6 ﬁne-tuning 18.4 ±0.3 18 .4 ±0.3 ER 27.5 ±1.2 33 .1 ±1.7 ER-MIR (Aljundi et al., 2019) 29.8 ±1.1 40 .0 ±1.1 ER-JPEG 33.9 ±1.0 43 .1 ±0.6 Gumbel AE (Riemer et al., 2018) 25.5 ±2.0 28 .8 ±2.9 AQM (ours) 43.5 ±0.7 47 .0 ±0.8 Forgetting (↓) M= 20 M= 50 N/A N/A N/A N/A 73.5 ±1.7 70 .7 ±4.5 49 ±2.4 40 .6 ±1.1 85.4 ±0.7 85 .4 ±0.7 50.5 ±2.4 35 .4 ±2.0 50.2 ±2.0 30 .2 ±2.3 54.8 ±1.2 44 .3 ±0.9 71.5 ±2.8 67 .2 ±3.9 23.0 ±1.0 19 .0 ±1.4 Table 1.Shared head results on disjoint CIFAR-10. Total memory per class M measured in sample memory size. We report (a) Accuracy, (b) Forgetting (lower is better). 4.1. Online Continual Classiﬁcation Although CL has been studied in generative modeling (Ramapuram et al., 2017; Lesort et al., 2018; Zhai et al., 2019; Lesort et al., 2019) and reinforcement learning (Kirk- patrick et al., 2017; Fernando et al., 2017; Riemer et al., 2018), supervised learning is still the standard for evalua- tion of new methods. Thus, we focus on the online continual classiﬁcation of images for which our approach can provide a complement to experience replay. In this setting, a new task consists of new image classes that the classiﬁer must learn, while not forgetting the previous ones. The model is only allowed one pass through the data (Lopez-Paz et al., 2017; Chaudhry et al.; Aljundi et al., 2019; Chaudhry et al., 2019). The online compression here takes the role of replay buffer in replay based methods such as (Chaudhry et al., 2019; Aljundi et al., 2019). We thus run Algorithm 1, with an additional online classiﬁer being updated performed at line 15. Here we consider the more challenging continual classiﬁca- tion setting often referred to as using ashared-head (Aljundi et al., 2019; Farquhar & Gal, 2018; Aljundi et al., 2018). Here the model is not informed of the task (and thereby the subset of classes within it) at test time. This is in contrast to other (less realistic) CL classiﬁcation scenarios where the task, and therefore subset of classes, is provided explicitly to the learner (Farquhar & Gal, 2018; Aljundi et al., 2019). For this set of experiments, we report accuracy, i.e. 1 T ∑T i=1 RT,i, and forgetting, i.e. 1 T−1 ∑T−1 i=1 max(R:,i) − RT,i with R ∈ RT×T representing the accuracy matrix where Ri,j is the test classiﬁcation accuracy on task jwhen task iis completed. Baselines A basic baseline for continual supervised learn- ing is Experience Replay ( ER). It consists of storing old data in a buffer to replay old memories. Although relatively simple recent research has shown it is a critical baseline to consider, and in some settings is actually state-of-the- art (Chaudhry et al., 2019; Aljundi et al., 2019; Rolnick et al., 2018). AQM can be used as an add-on to ER that in- corporates online continual compression. We also compare against ER with standard JPEG compression. In addition we consider the following baselines. iid online (upper-bound) trains the model with a single-pass through the data on the same set of samples, but sampled iid. iid ofﬂine (upper- bound) evaluates the model using multiple passes through the data, sampled iid. We use 5 epochs in all the experiments for this baseline. ﬁne-tuning trains continuously upon ar- rival of new tasks without any forgetting avoidance strategy. iCarl (Rebufﬁ et al., 2017) incrementally classiﬁes using a nearest neighbor algorithm, and prevents catastrophic for- getting by using stored samples. GEM (Lopez-Paz et al., 2017) uses stored samples to avoid increasing the loss on previous task through constrained optimization. It has been shown to be a strong baseline in the online setting. It gives similar results to the recent A-GEM (Chaudhry et al.). ER- MIR (Aljundi et al., 2019) controls the sampling of the replays to bias sampling towards samples that will be for- gotten. We note that the ER-MIR critera is orthogonal to AQM, and both can be applied jointly.Gumbel AE (Riemer et al., 2018) learns an autoencoder for ER using the Gumbel softmax to obtain discrete representations. We evaluate with the standard CIFAR-10 split (Aljundi et al., 2018), where 5 tasks are presented sequentially, each adding two new classes. Evaluations are shown in Table 1. Due to our improved storage of previous data, we observe sig- niﬁcant improvement over other baselines at various mem- ory sizes. We can contrast AQM’s performance with ER’s to understand the net impact of our compression scheme. Speciﬁcally, AQM improves over ER by 16.0% and 13.9% in the M=20 and M=50 case, highlighting the effectiveness of online compression. Our approach is also superior in forgetting by a signiﬁcant margin in both memory settings. To compare directly to reporting in (Riemer et al., 2018) we also benchmarked our implementation on the Incremental CIFAR-100 multi-head experiment (Lopez-Paz et al., 2017) with the same settings as in (Riemer et al., 2018). By using AQM we were able to get 65.3 vs the reported 43.7 using a buffer of size 200. To speciﬁcally isolate the advantageOnline Learned Continual Compression with Adaptive Quantization Modules Accuracy RS 5.2 ±0.2 2 Module AQM (ours) 23.2 ± 1.1 Ablate 2nd Module 20.5 ±1.3 Ablate Fixing Codebook 19.2 ±0.6 Ablate Decoupled Training 16.5 ±0.7 Ablate Adaptive Compression 13.1 ±3.2 Table 2.Imagenet ofﬂine training evaluation from online continual compression. We see a clear gain over a standard Reservoir sam- pling approach. We then ablate each component of our proposal showing each component is important. Note storage used in each experiment is identical (including accounting for model sizes). of gumbel softmax versus the vector quantization for drift, we replaced the vector quantization approach with gumbel softmax in an AQM. We observed signﬁcantly less drift in the case where vector quantization is used. Full details of this experiment are described in the supplementary materials along with visualizations. The CIFAR-10 dataset has a low resolution (3 ×32 ×32) and uses a lot of data per task (10K samples). These two characteristics might leave the online compression problem easier than in a real-life scenario. Speciﬁcally, if the ﬁrst tasks are long enough and the compression rate is not too large, the model can quickly converge and thus not incur too much representation drift. Indeed, we found that us- ing a single module is already sufﬁcient for this task. For these reasons, we now study the AQM in more challenging settings presented in the next section. 4.2. Ofﬂine Evaluation on Larger Images Besides the standard continual classiﬁcation setup, we pro- pose several other evaluations to determine the effectiveness of the stored data and compression module after learning online compression. We also perform a detailed ablation to study the efﬁcacy of each component in AQM. Ofﬂine training on Imagenet We compare the effective- ness of the stored memories of AQM after a certain amount of online continual compression. We do this by training in a standard iid way an ofﬂine classiﬁcation model using only reconstructions obtained from the storage sampled after on- line continual compression has progressed for a period of time. In each case we would have the same sized storage available. We note that simply having more stored memo- ries does not amount to better performance as their quality may be severely degraded and affected by drift. Using this evaluation we ﬁrst compare a standard reservoir sampling approach on uncompressed data to a 2 module AQM using the same size storage. We observe that perfor- mance is drastically increased using the compressed sam- ples. We then use this to perform a series of ablations to Figure 4.Impact of codebook freezing. Vertical black line indi- cates freezing point. We see that AQM is still able to adapt and reduce its reconstruction loss, while having stable compressed representations. Results averaged over 5 runs demonstrate each component of our proposal is important. Speciﬁcally (a) we restrict AQM to have only one module, (b) instead of decoupled training we train modules end-to- end, (c) we remove adaptive compression, thus all samples are stored in the most compressed block, regardless of qual- ity, and (d) we do not stabilize the codebook, the embedding matrices of every block are never ﬁxed. We observe that all these elements contribute to successfully storing a represen- tative set of data for the distribution online. Drift Ablation We have seen the importance of codebook freezing when dealing with high dimensional datasets. How- ever, judging solely from the ﬁnal downstream task perfor- mance it’s difﬁcult to see if the model continues adapting after freezing. As alluded in Sec 3.2 there is a tradeoff be- tween keeping recoverable representations and a model’s ability to continue to adapt. To shed some light on this, we run the following experiment: we run a vanilla VQ-V AE on the same 20 task mini-imagenet stream, without storing any samples. When it reaches a pre-speciﬁed performance threshold, we ﬁx the codebook, and store compressed held- out data from the ﬁrst task. We then continue to update the VQ-V AE parameters, and the memory is kept ﬁxed for the rest of the stream. We apply self-replay but no other AQM mechanisms (e.g. no sampling from the input stream and no adaptive compression). We monitor how well the VQ-V AE can adapt by looking at the streaming reconstruction cost, measured on the in- coming data before an update. We also monitor the drift of samples stored in the buffer. Results are presented in Figure 4. They demonstrate that drift is controlled by stabilizing the codebook, while the model can still improve at nearly the same rate. Further analysis, along with an additional ex- periment showcasing the robustness of vector quantization to small perturbations is included in the Appendix.Online Learned Continual Compression with Adaptive Quantization Modules Figure 5.Top: Sample decoded from the buffer at the end of train- ing from scratch (32x compression rate). Bottom: Original lidar LiDAR Range data enables autonomous vehicles to scan the topography of their surrounding, giving precise mea- surements of an obstacle’s relative location. In its raw form, range data can be very large, making it costly to transmit in real time, or for long term storage. Equipping self-driving cars with a good lidar compressor can enable fast vehicle- to-vehicle (V2V) communication, leading to safer driving (Eckelmann et al., 2017). Moreover, since data collected by autonomous vehicles can be highly non-stationary (new objects on the road, changing weather or trafﬁc conditions), having a compressor which can quickly adapt to this distri- bution change will reduce the required memory for storage (or bandwidth for real time transmission). We proceed to train AQM on the Kitti Dataset (Geiger et al., 2013), which contains 61 LiDAR scan recordings, each belonging to either the “residential”, “road”, “city” envi- ronments. The data is processed as in (Caccia et al., 2018), where points from the same elevation angle are sorted in increasing order of azimuth angle along the same row. This yield a 2D grid, making it compatible with the same architec- ture used in the previous experiments. As in (Caccia et al., 2018; Tu et al., 2019), we report the reconstruction cost in Symmetric Nearest Neighbor Root Mean Squared Error (SNNRMSE) which allows to compare two point clouds. Note AQM can also be adapted to use task relevant criteria besides MSE. We consider two settings. In the ﬁrst, we train AQM from scratch on a data stream consisting of recordings from all three environments. We present (once) all the recordings of an environment before moving on to another, in order to maximise the distribution shift. We show qualitative results in Figure 5 and in the supplementary materials. Ob- serve that we are able to effectively reconstruct the LiDAR samples and can easily tradeoff quality with compression. Overall we obtain 18.8 cm SNNRMSE with 32×compres- Size in Mb Raw 1326.8 Gzip 823.0 AQM 35.5 ±.06 AQM + ﬁnetune 33.0 ±.07 AQM + ﬁnetune + PNG 27.9 ±.01 Table 3.Compression results for the data transmission of the city lidar recordings. We require that each compressed scan has an SNNRMSE under 15 cm. sion, which lies in a range that has been shown in (Tu et al., 2019) to be sufﬁcient to enable SLAM localization with very minimal error. In the second setting, we wish to simulate a scenario where some data is available a priori for the model to leverage. However, this data is limited and does not cover all the possible modalities to which an autonomous vehicle could be exposed. To this end, we pretrain AQM in a fully ofﬂine iid manner on the road and residential recordings. We then simulate the deployment of the compressor on a vehicle, where it must compress and transmit in real time the lidar data feed from a new distribution. We therefore stream the held-out city recordings and show that AQM can be ﬁne-tuned on the ﬂy to reduce the required bandwidth for data transmission. Quantitative results are presented in table 3. We ensure that the reconstructed lidar scans have a SNNRMSE smaller than 15.0 cm. Moreover, since the stored representations in AQM are 2D and discrete, we can apply lossless compression schemes such as Portable Network Graphics (PNG). 4.3. Atari RL Environments Another application of online continual compression is for preserving the states of an reinforcement learning agent op- erating online. These agents may often learn new tasks or enter new rooms thus the observations will often be highly non-iid. Furthermore many existing reinforcement learning algorithms already rely on potentially large replay buffers which can be prohibitive (Mnih et al., 2014; Rolnick et al., 2018) to run and may greatly beneﬁt from an approach such as the AQM to run concurrently with reinforcement learning algorithms. We thus perform a proof of concept for the AQM for storing the state sequence encountered by an RL learner in the atari environment(Bellemare et al., 2013). We use the dataset and tasks introduced in (Anand et al., 2019), which runs a random or learned policy in the atari environments and provides a set of classiﬁcation tasks to evaluate whether key information about the state is preserved. Results are shown Table 4. We run the online learning with the AQM on the data stream observed by the random agent. We use the same observations and optimiza- tion as in (Anand et al., 2019) and report the F1 resultsOnline Learned Continual Compression with Adaptive Quantization Modules Figure 6.Top: original. Bottom: reconstructed from AQM Game Cls Input F1 Pong Orig. State 86.7 AQM Recon 86.8 Ms Pacman Orig. State 89.4 AQM Recon 88.3 Pitfall Orig. State 68.2 AQM Recon 66.7 Table 4.Results on RL probing tasks from (Anand et al., 2019) with linear probe applied to original observation and to reconstruc- tions from AQM after online compression. Acc is averaged for each game over game speciﬁc prediction. of a linear probe directly on states for our reconstructions after online compression and the originals. Results for 3 environments are shown in Table 4 and examples in in Fig 6 and the Appendix. We ﬁnd that AQM can well preserve the critical information while compressing the state by 16x. The reference accuracies achieved by our classiﬁer are sim- ilar to those in (Anand et al., 2019). However, we do not control for the representation size unlike those evaluations of various unsupervised models. 5. Conclusion We have introduced online continual compression. We demonstrated vector quantization can be used to control drift and how to create mechanisms that allow maintain- ing quality and maximizing memory usage. These allowed learning compression while compressing. We have shown effectiveness of this online compression approach on stan- dard continual classiﬁcation benchmarks, as well as for compressing larger images, lidar, and atari data. We believe future work can consider dealing with temporal correlations for video and reinforcement learning tasks, as well as im- proved prioritization of samples for storage. References Aljundi, R., Lin, M., Goujaud, B., and Bengio, Y . Online continual learning with no task boundaries. In arXiv, 2018. Aljundi, R., Caccia, L., Belilovsky, E., Caccia, M., Charlin, L., and Tuytelaars, T. Online continual learning with maximally interfered retrieval. In Advances in Neural Information Processing (NeurIPS), 2019. Anand, A., Racah, E., Ozair, S., Bengio, Y ., C ˆot´e, M.- A., and Hjelm, R. D. Unsupervised state representation learning in atari. arXiv preprint arXiv:1906.08226, 2019. Andrychowicz, M., Denil, M., G´omez, S., Hoffman, M. W., Pfau, D., Schaul, T., and de Freitas, N. Learning to learn by gradient descent by gradient descent. In Lee, D. D., Sugiyama, M., Luxburg, U. V ., Guyon, I., and Garnett, R. (eds.), Advances in Neural Information Processing Sys- tems 29, pp. 3981–3989. Curran Associates, Inc., 2016. Ball´e, J., Laparra, V ., and Simoncelli, E. P. End-to-end optimization of nonlinear transform codes for perceptual quality. In 2016 Picture Coding Symposium (PCS), pp. 1–5. IEEE, 2016. Belilovsky, E., Eickenberg, M., and Oyallon, E. De- coupled greedy learning of cnns. arXiv preprint arXiv:1901.08164, 2019. Bellemare, M. G., Naddaf, Y ., Veness, J., and Bowling, M. The arcade learning environment: An evaluation plat- form for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279, 2013. Caccia, L., van Hoof, H., Courville, A., and Pineau, J. Deep generative modeling of lidar data. arXiv preprint arXiv:1812.01180, 2018. Chaudhry, A., Ranzato, M., Rohrbach, M., and Elhoseiny, M. Efﬁcient lifelong learning with a-gem. In ICLR 2019. Chaudhry, A., Dokania, P. K., Ajanthan, T., and Torr, P. H. Riemannian walk for incremental learning: Un- derstanding forgetting and intransigence. arXiv preprint arXiv:1801.10112, 2018. Chaudhry, A., Rohrbach, M., Elhoseiny, M., Ajanthan, T., Dokania, P. K., Torr, P. H., and Ranzato, M. Continual learning with tiny episodic memories. arXiv preprint arXiv:1902.10486, 2019. Eckelmann, S., Trautmann, T., Ußler, H., Reichelt, B., and Michler, O. V2v-communication, lidar system and posi- tioning sensors for future fusion algorithms in connected vehicles. Transportation research procedia, 27:69–76, 2017. Farquhar, S. and Gal, Y . Towards robust evaluations of continual learning. arXiv preprint arXiv:1805.09733 , 2018.Online Learned Continual Compression with Adaptive Quantization Modules Fernando, C., Banarse, D., Blundell, C., Zwols, Y ., Ha, D., Rusu, A. A., Pritzel, A., and Wierstra, D. Pathnet: Evolu- tion channels gradient descent in super neural networks. arXiv preprint arXiv:1701.08734, 2017. Geiger, A., Lenz, P., Stiller, C., and Urtasun, R. Vision meets robotics: The kitti dataset. The International Journal of Robotics Research, pp. 0278364913491297, 2013. Gueguen, L., Sergeev, A., Kadlec, B., Liu, R., and Yosinski, J. Faster neural networks straight from jpeg. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 31 , pp. 3933–3944. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/ 7649-faster-neural-networks-straight-from-jpeg. pdf. Hu, W., Lin, Z., Liu, B., Tao, C., Tao, Z., Ma, J., Zhao, D., and Yan, R. Overcoming catastrophic forgetting for continual learning via model adaptation. 2018. Husz´ar, F. On quadratic penalties in elastic weight consoli- dation. arXiv preprint arXiv:1712.03847, 2017. Johnston, N., Vincent, D., Minnen, D., Covell, M., Singh, S., Chinen, T., Jin Hwang, S., Shor, J., and Toderici, G. Improved lossy image compression with priming and spatially adaptive bit rates for recurrent networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4385–4393, 2018. Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Des- jardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, pp. 201611835, 2017. Lesort, T., Caselles-Dupr´e, H., Garcia-Ortiz, M., Stoian, A., and Filliat, D. Generative models from the perspective of continual learning. arXiv preprint arXiv:1812.09111, 2018. Lesort, T., Gepperth, A., Stoian, A., and Filliat, D. Marginal replay vs conditional replay for continual learning. In International Conference on Artiﬁcial Neural Networks, pp. 466–480. Springer, 2019. Li, Z. and Hoiem, D. Learning without forgetting. IEEE Transactions on Pattern Analysis and Machine Intelli- gence, 40(12):2935–2947, 2018. Lin, L.-J. Reinforcement learning for robots using neu- ral networks. Technical report, Carnegie-Mellon Univ Pittsburgh PA School of Computer Science, 1993. Lopez-Paz, D. et al. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems, pp. 6467–6476, 2017. Mnih, V ., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. Mnih, V ., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje- land, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., and Hassabis, D. Human-level control through deep reinforcement learning. Nature, 2014. Nguyen, C. V ., Li, Y ., Bui, T. D., and Turner, R. E. Variational continual learning. arXiv preprint arXiv:1710.10628, 2017. Nøkland, A. and Eidnes, L. H. Training neural networks with local error signals. arXiv preprint arXiv:1901.06656, 2019. Oyallon, E., Belilovsky, E., Zagoruyko, S., and Valko, M. Compressing the input for cnns with the ﬁrst-order scatter- ing transform. In The European Conference on Computer Vision (ECCV), September 2018. Ramapuram, J., Gregorova, M., and Kalousis, A. Lifelong generative modeling. arXiv preprint arXiv:1705.09847, 2017. Razavi, A., Oord, A. v. d., and Vinyals, O. Generating diverse high-ﬁdelity images with vq-vae-2.arXiv preprint arXiv:1906.00446, 2019. Rebufﬁ, S.-A., Kolesnikov, A., Sperl, G., and Lampert, C. H. icarl: Incremental classiﬁer and representation learning. In Proc. CVPR, 2017. Riemer, M., Franceschini, M., and Klinger, T. Genera- tion and consolidation of recollections for efﬁcient deep lifelong learning. CoRR, abs/1711.06761, 2017. URL http://arxiv.org/abs/1711.06761. Riemer, M., Cases, I., Ajemian, R., Liu, M., Rish, I., Tu, Y ., and Tesauro, G. Learning to learn without forgetting by maximizing transfer and minimizing interference. arXiv preprint arXiv:1810.11910, 2018. Rolnick, D., Ahuja, A., Schwarz, J., Lillicrap, T. P., and Wayne, G. Experience replay for continual learning, 2018. Rusu, A. A., Rabinowitz, N. C., Desjardins, G., Soyer, H., Kirkpatrick, J., Kavukcuoglu, K., Pascanu, R., and Had- sell, R. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.Online Learned Continual Compression with Adaptive Quantization Modules Shin, H., Lee, J. K., Kim, J., and Kim, J. Continual learn- ing with deep generative replay. In Advances in Neural Information Processing Systems, pp. 2990–2999, 2017. Theis, L., Shi, W., Cunningham, A., and Husz ´ar, F. Lossy image compression with compressive autoencoders. arXiv preprint arXiv:1703.00395, 2017. Thrun, S. and Mitchell, T. M. Lifelong robot learning. Robotics and autonomous systems, 15(1-2):25–46, 1995. Torfason, R., Mentzer, F., ´Ag´ustsson, E., Tschannen, M., Timofte, R., and Gool, L. V . Towards image under- standing from deep compression without decoding. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum? id=HkXWCMbRW. Tu, C., Takeuchi, E., Carballo, A., and Takeda, K. Point cloud compression for 3d lidar sensor using recurrent neural network with residual blocks. In 2019 Interna- tional Conference on Robotics and Automation (ICRA), pp. 3274–3280. IEEE, 2019. van den Oord, A., Vinyals, O., et al. Neural discrete repre- sentation learning. In Advances in Neural Information Processing Systems, pp. 6306–6315, 2017. Zenke, F., Poole, B., and Ganguli, S. Continual learn- ing through synaptic intelligence. arXiv preprint arXiv:1703.04200, 2017. Zhai, M., Chen, L., Tung, F., He, J., Nawhal, M., and Mori, G. Lifelong gan: Continual learning for conditional im- age generation. ArXiv, abs/1907.10107, 2019.Online Learned Continual Compression with Adaptive Quantization Modules A. Buffer Memory Management We consider two settings to manage the samples inside the memory. In the ﬁrst setting, we do not perform any codebook freezing. Therefore, as the model trains, the amount of compressed representations AQM can store increases smoothly. In this setting, the current amount of samples stored by AQM is a good approximation of the model’s capacity. Therefore, we simply use this estimate instead of the total buffer size in the regular reservoir sampling scheme. The algorithm is presented in Alg 4. Algorithm 4: AddToMemory Input: Memory Mwith capacity C(bytes), sample x 1 Nreg = C BY TES(x) 2 capacity = max( Nreg, NUM SAMPLES (M) ) 3 %Probability of adding x 4 add ∼B( capacity SAMPLE AMT SEEN SO FAR ) %Bernoulli 5 if add then 6 hidx, blockid = ADAPTIVE COMPRESS (x, AE, dth) 7 while BITS(hidx) - FREE SPACE(M) >0 do 8 DELETE RANDOM (M) 9 end 10 end This is not the case when we perform codebook freezing. In the latter setting, consider the moment when the ﬁrst codebook is ﬁxed; suddenly, the amount of samples the model can store has increased by a factor equal to the compression rate of the ﬁrst block. Therefore, at this moment, the amount of samples currently stored by AQM is not a good approximation for the model’s capacity. Moreover, when performing codebook freezing, since the capacity suddenly spikes, we must decide between a) having an imbalance in the buffer, where certain temporal regions of the streams are not equally represented, or not utilising all available memory and storing less incoming samples so they are in similar quantities as previous samples. We opt for the former approach, and propose a procedure that allows the buffer to rebalance itself as new training data becomes available. We illustrate the procedure with an example. Consider an AQM where the distribution of samples in the buffer is the one plotted in Fig 10. Speciﬁcally, we show the number of samples stored for each minibatch processed by the model. In this example, very few (<20) samples are stored from the earliest part of the stream, while a much larger number comes from the more recent part of the stream. Assuming that the model is over its memory capacity, we need to remove samples until the memory requirement is met. Ideally, we would like to remove more samples from parts of the stream where samples are abundant. In order to do so, we use Kernel Density Estimation on the histogram in Fig 10. Doing so gives us the line labelled iter0 in Fig 10. We then sample points according to the distribution given by iter0, remove them, and ﬁt a new KDE with the remaining points (labelled iter1). In this example we repeat this procedure 10 times, until iter9. As we can see, the distribution of stored samples becomes closer to the uniform distribution, i.e. the setting where all parts of the streeam are equally represented. Therefore, in the setting where codebook freezing is performed, we ﬁrst add all incoming points to the buffer. Then, points are removed according to the procedure described above. This allows for maximal memory usage while ensuring that the buffer self balances over time. Note that we need to store the timestamp alongside each sample, which has a negligible cost when dealing with high-dimensional inputs. B. Further Details of Experiments We include here further details regarding the models used in our experiments. For all reported results, almost all hyperpa- rameters are kept the same. we set D the size of the embedding table equal to 100, we use a decay value of 0.6 for the Embedding EMA update, and the same architectural blocks. Across problems, we mainly vary the reconstruction threshold, as well how the blocks are stacked and their compression rates (by e.g. changing the number of codebooks per block).Online Learned Continual Compression with Adaptive Quantization Modules Figure 7.(left) histogram of samples in AQM where no buffer balancing was performed. (right) iterative buffer balancing procedure B.1. Cifar For CIFAR-10, we use a 1 block AQM, latent size (16 x 16 x 100) is quantized with (16 x 16 x 1) indices where the last index represents the number of codebooks. The codebook here contains 128 embeddings, giving a compression factor of 13.7×. Due to the simplistic nature of the task and low resolution of the images, AQM already yields good compression before the end of the ﬁrst task, hence adaptive compression and codebook freezing are not required. For the (Riemer et al., 2018) baseline, we ran a hyperparameter search to vary the compression size. Speciﬁcally, we ran a grid search for the number of categories per latent variable, as well as for the number of latent variables. We found the gumbel softmax much less stable during training and harder to cross-validate than vector quantization. Below we show an example of the image quality of our approach compared to (Riemer et al., 2018). We ran both AQM and (Riemer et al., 2018) on the split CIFAR-10 task, then extracted images which happened to be in the buffer of both methods. Figure 8.Bottom row: random buffer reconstructions using (Riemer et al., 2018). Middle row: random buffer reconstructions using SQM. Top row: corresponding original image. Columns are ordered w.r.t their entry time in the buffer, from oldest to newest. All samples above were obtained from the disjoint CIFAR-10 task, and are 12×smaller than their original image. B.2. Imagenet For the Ofﬂine 128 x 128 Imagenet experiment, we use the following three blocks to build AQM, with the following latent sizes : 1. (64 x 64 x 100) quantized using (64 x 64 x 1) indices, with a codebook of 16 vectors, giving a 24 ×compression. 2. (32 x 32 x 100) quantized using (32 x 32 x 1) indices, with a codebook of 256 vectors, giving a 48 ×compression. 3. (32 x 32 x 100) quantized using (32 x 32 x 1) indices, with a codebook of 32 vectors, giving a 76.8 ×compression. For the 2 block AQM, we searched over using blocks (1-2) (2-3) and (1-3). For 1 block AQM we simply tried all three blocks independently. When stacking two blocks with the same latent sizes (e.g. block 2 and 3) the encoder and decoder functions for the second block are the identity. In other words, the second block simply learns another embedding matrix.Online Learned Continual Compression with Adaptive Quantization Modules C. Drift Ablation Here we provide additional results for the drift ablation studied in 4. We repeat the same experiment for different values of the reconstruction threshold parameter, which controls when the codebook freezing occurs. The table below shows that the same conclusion holds across multiple values for this parameter: codebook freezing yields little to no drift, while only negligibly hindering the model’s ability to adapt to a distribution shift. It is worth noting that in cases of severe drift (e.g. with recon th = 7.5) the model diverges because it is rehearsing on samples of poor quality. In this setting, codebook freezing performs better on both the streaming MSE and the drift MSE. recon th Freezing Streaming MSE Drift MSE Streaming + Drift MSE 1 No 0.59 ±0.03 1.01 ±0.14 1.60 ±0.17 1 Yes 0.60 ±0.03 0.49 ±0.02 1.09±0.05 2.5 No 0.63 ±0.06 13.24 ±5.36 13.87 ±5.42 2.5 Yes 0.81 ±0.02 1.07 ±0.05 1.88±1.12 5 No 0.65 ±0.04 55.31 ±36.82 55.96 ±36.86 5 Yes 0.92 ±0.03 1.69 ±0.18 2.61±0.21 7.5 nan nan nan nan 7.5 Yes 0.98 ±0.11 2.10 ±0.28 3.08±0.39 Table 5.Here we provide additional results for the drift ablation discussion in section shown in Fig 4. For clarity all results are multiplied by 100. (e.g. recon th of 2.5 corresponds to 0.025). Results averaged over 5 runs. Figure 9.Visualization of results reported in Table C. We kept the scale of the y axis consistent across the four graphs for easy comparison.Online Learned Continual Compression with Adaptive Quantization Modules D. Atari Below are sample reconstructions used in the Atari experiments. For each game, reconstructions are in the ﬁrst row and the original uncompressed samples in the second row. Reconstructions are 16×smaller than the original RGB images. Figure 10.Pong Figure 11.Pitfall Figure 12.Ms PacmanOnline Learned Continual Compression with Adaptive Quantization Modules E. Lidar Samples Here we show reveral lidar compressions (left) and their original counterpart (right). The compression rate is 32 ×. We note that unlike RGB images, raw lidar scans are stored using ﬂoating points. We calculate the compression rate from the (smaller) polar projection instead of the 3 channel cartesian representation. Figure 13.Lidar reconstruction (left) vs original (right)Online Learned Continual Compression with Adaptive Quantization Modules Figure 14.Lidar reconstruction (left) vs original (right)",
      "meta_data": {
        "arxiv_id": "1911.08019v3",
        "authors": [
          "Lucas Caccia",
          "Eugene Belilovsky",
          "Massimo Caccia",
          "Joelle Pineau"
        ],
        "published_date": "2019-11-19T00:43:16Z",
        "pdf_url": "https://arxiv.org/pdf/1911.08019v3.pdf"
      }
    },
    {
      "title": "Vector Quantization Prompting for Continual Learning",
      "abstract": "Continual learning requires to overcome catastrophic forgetting when training\na single model on a sequence of tasks. Recent top-performing approaches are\nprompt-based methods that utilize a set of learnable parameters (i.e., prompts)\nto encode task knowledge, from which appropriate ones are selected to guide the\nfixed pre-trained model in generating features tailored to a certain task.\nHowever, existing methods rely on predicting prompt identities for prompt\nselection, where the identity prediction process cannot be optimized with task\nloss. This limitation leads to sub-optimal prompt selection and inadequate\nadaptation of pre-trained features for a specific task. Previous efforts have\ntried to address this by directly generating prompts from input queries instead\nof selecting from a set of candidates. However, these prompts are continuous,\nwhich lack sufficient abstraction for task knowledge representation, making\nthem less effective for continual learning. To address these challenges, we\npropose VQ-Prompt, a prompt-based continual learning method that incorporates\nVector Quantization (VQ) into end-to-end training of a set of discrete prompts.\nIn this way, VQ-Prompt can optimize the prompt selection process with task loss\nand meanwhile achieve effective abstraction of task knowledge for continual\nlearning. Extensive experiments show that VQ-Prompt outperforms\nstate-of-the-art continual learning methods across a variety of benchmarks\nunder the challenging class-incremental setting. The code is available at\n\\href{https://github.com/jiaolifengmi/VQ-Prompt}{this https URL}.",
      "full_text": "Vector Quantization Prompting for Continual Learning Li Jiao1, Qiuxia Lai 1∗, Yu Li 2, Qiang Xu 3 1 Communication University of China 2 Harbin Institute of Technology, Shenzhen 3 The Chinese University of Hong Kong {jl0930,qxlai}@cuc.edu.cn;li.yu@hit.edu.cn;qxu@cse.cuhk.edu.hk Abstract Continual learning requires to overcome catastrophic forgetting when training a single model on a sequence of tasks. Recent top-performing approaches are prompt- based methods that utilize a set of learnable parameters (i.e., prompts) to encode task knowledge, from which appropriate ones are selected to guide the fixed pre- trained model in generating features tailored to a certain task. However, existing methods rely on predicting prompt identities for prompt selection, where the identity prediction process cannot be optimized with task loss. This limitation leads to sub-optimal prompt selection and inadequate adaptation of pre-trained features for a specific task. Previous efforts have tried to address this by directly generating prompts from input queries instead of selecting from a set of candidates. However, these prompts are continuous, which lack sufficient abstraction for task knowledge representation, making them less effective for continual learning. To address these challenges, we propose VQ-Prompt, a prompt-based continual learning method that incorporates Vector Quantization (VQ) into end-to-end training of a set of discrete prompts. In this way, VQ-Prompt can optimize the prompt selection process with task loss and meanwhile achieve effective abstraction of task knowledge for continual learning. Extensive experiments show that VQ- Prompt outperforms state-of-the-art continual learning methods across a variety of benchmarks under the challenging class-incremental setting. The code is available at https://github.com/jiaolifengmi/VQ-Prompt. 1 Introduction Humans have the remarkable capability to continually acquire and integrate knowledge of new concepts or categories without forgetting old ones, whereas deep learning models struggle with catastrophic forgetting[40] when tasked with learning a sequence of classes [42, 10, 39]. Continual learning aims at addressing catastrophic forgetting in deep neural networks (DNNs) by striking a balance between plasticity for learning new incoming data effectively and stability to retain prior knowledge. Approaches in this field vary: some methods dynamically expand network architec- tures [64, 29, 58] or reconfigure their internal structures [48, 17, 24] for new tasks. Others penalize the update of crucial parameters from previous tasks [20, 27, 65, 1, 49] or alter parameter update rules to prevent interference across tasks [34, 7, 47, 23]. Additionally, certain methods interleave stored past data with current ones for training [18, 8, 44, 45, 4, 6, 36]. Despite recent advances, continual learning remains an open challenge for DNNs. Recently, prompt-based continual learning has emerged as a promising solution to mitigate catas- trophic forgetting in sequential task learning. This approach enhances a pre-trained Vision Trans- ∗Corresponding author 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2410.20444v2  [cs.LG]  20 Jul 2025Diffrentiable?Discrete Prompt?  Diffrentiable?Discrete Prompt?  Diffrentiable?Discrete Prompt? (a) (b) (c) Query Prompt  Query Module Input-dependent Prompt Query Module Vector Quantization  Input-dependent Prompt  Gradient  Gradient  Select  Gradient Estimation  Module Id  Gradient  ... Prompt Pool ... Prompt Pool ������ Figure 1: Concept comparison. (a) Prior prompt-based continual learning methods predict prompt identities for prompt selection, which cannot be optimized end-to-end with task loss. (b) Some methods enable end-to-end training by directly generating prompts from the queries using learnable parameters. However, these prompts are continuous, lacking the necessary abstraction to effectively represent the task knowledge essential for generating features tailored to a certain task. (c) Our method incorporates Vector Quantization (VQ) into the prompt generation pipeline to enable end-to- end training of discrete prompts with task loss. See §1 for details. former (ViT) [11] with a small set of learnable parameters, known as “prompts”. These prompts encapsulate task-specific knowledge, shifting the learning focus from the entire model to the prompts themselves, which guide the pre-trained model in generating task-relevant outputs. During inference, the most suitable prompt containing necessary task knowledge is selected from the prompt pool based on the input image to direct the behavior of the frozen pre-trained model. Current prompt-based methods either involve a key-query matching mechanism to select prompts based on the similarity between the image features and the key parameters paired with prompts [62, 61], or explicitly predict the prompt indices and perform the selection accordingly [60, 59]. However, the non-differentiable nature of indexing impedes the prompt selection from being optimized end-to- end with task loss. This limitation can lead to diminished performance, as inaccurate prompt selection may fail to tailor the pre-trained features for the specific task. Efforts to address this issue include implementing a differentiable prompt selection, such as generating prompts as a weighted sum from the prompt pool [50], or deriving prompts from the intermediate features of the input image [51, 26]. However, the resulting prompts are continuous, which lack the necessary abstraction to effectively represent the task knowledge essential for guiding the pre-trained model to generate features tailored to a certain task. A concept comparison is shown in Fig. 1. The assumption that discrete prompts better represent task knowledge than continuous prompts for continual learning can be supported by both theoretical insights from cognitive science and empirical evidence. Discrete prompts mimic the organizational structure of memory and knowledge in the human brain, which is typically understood to consist of discrete units such as concepts and facts [19]. This clear separation of information helps prevent interference among different knowledge domains, and enables models to provide distinct guidance for feature extraction specific to each task. Such knowledge abstraction aligns with categorical perception in human cognition, where sensory inputs are perceived as distinct categories (e.g., colors, phonemes) rather than continuous spectrum [41]. Furthermore, empirical comparisons in §5.2 demonstrate the effectiveness of discrete prompts when optimized end-to-end with task loss (e.g., VQ-Prompt V .S. CODA-P or EvoPrompt). In summary, discrete prompts hold significant promise for improving the continual learning capabilities of models, bringing them more in line with human learning. Optimizing prompts with task loss while preserving their discrete properties as representations of concepts poses a non-trivial challenge. In this paper, we introduce Vector Quantization Prompting (VQ-Prompt) for continual learning, which can optimize prompts using task loss while preserving their discrete characteristics as concept representations. This method involves initially generating a continuous prompt and then replacing it with its nearest match from a predefined prompt pool. To address the non-differentiability inherent in prompt quantization, we apply gradient estimation to propagate task loss to the continuous prompt, while additional vector quantization regularization terms further refine the learning of the prompt pool. To further stabilize task knowledge learning, we use representation statistics to mitigate the classification bias towards previous tasks, thereby enhancing continual learning performance. Our contributions are three-folds: (1) We propose VQ-Prompt, an end-to-end learnable discrete prompting mechanism for continual learning, addressing a critical yet overlooked aspect in the current literature. (2) We leverage gradient estimation to pass the task loss to prompt-related parameters while regularizing the learning of the prompts with vector quantization terms, which facilitates the 2end-to-end training of the discrete prompt pool. (3) We incorporate representation statistics during training to further stabilize task knowledge learning and improve the overall continual learning performance. Extensive experiments show that VQ-Prompt consistently outperforms state-of-the-art continual learning methods on a variety of benchmarks. 2 Related work Continual Learning refers to the process where multiple tasks are learned sequentially without forgetting [42, 10, 39]. Generally, continual learning has three scenarios [ 54]. Task-incremental learning (TIL) learns different classes for each task and assumes having task identities available at test time. Domain-incremental learning (DIL) maintains the same set of classes for different tasks while changing the data distributions across tasks, and task identities are not provided for inference. For Class-incremental learning (CIL), each task involves new classes and all the learned classes are to be classified without task identities available during inference. In this paper, we focus on the more representative and challenging CIL scenario. Numerous efforts have been devoted to alleviating catastrophic forgetting. Architecture-based methodsaddress this by either dynamically expanding network architectures [64, 29, 58] or modifying internal network structures [48, 17, 24] for new tasks. Regularization-based methodsfocus on limiting updates to vital parameters from earlier tasks [ 20, 27, 65, 1, 49], or modifying the rules for parameter updates to reduce task interference [34, 7, 47, 23]. Rehearsal-based methodsincorporate previous data with current data during training to mitigate forgetting [ 18, 8, 44, 45, 4, 6, 36]. Despite recent advances, continual learning remains a challenging and evolving field. Prompt-based Continual Learning Methods.Recently, there has been a surge in methods leveraging prompting techniques from natural language processing (NLP) [ 28, 30] for continual learning. These methods instruct a frozen pre-trained transformer using learnable prompts that encode task knowledge. During training, prompt selection is either through key-query similarity matching [62] or indicated by task identity [61, 12, 60, 59]. In inference, the appropriate prompt is chosen through similarity matching with key or feature centroids. However, both kinds of prompt selection are non-differentiable, making it challenging to optimize them end-to-end with the task loss, particularly when the gap between the pre-training task and unknown future tasks is large. To address this, CODA-Prompt [50] adopts a soft prompt selection, i.e., generating prompts as a weighted sum from the prompt pool. APG [ 51] and EvoPrompt [26] learn to derive prompts from intermediate image features. Nevertheless, all three methods generate prompts that are continuous, which lack the necessary abstraction to effectively represent the task knowledge essential for instructing the pre-trained model to produce features tailored to a certain task. In this paper, we present a new prompting framework for continual learning capable of optimizing prompts with task loss while preserving their discrete properties as the representation of task knowledge. Vector Quantization in Representation Learning. Vector Quantization (VQ) is a technique used in signal processing and data compression to represent a set of vectors (data points) with a smaller set of “coding vectors” (CVs). Unsupervised VQ algorithms such as Self-organizing Maps (SOMs) [22] and Neural Gas (NG) networks [38] attempt to obtain a set of CVs that optimally represent the data. Supervised VQ algorithms such as Learning Vector Quantization (LVQ) [21] focus on reducing the misclassification rates by refining decision boundaries between classes. In generative modeling, VQ has been used to learn structured discrete latent spaces in VQ-V AE [55] and VQ-GAN [13] to achieve higher fidelity images. Recently studies have explored combining VQ with continual learning to constrain the feature space, aiming to enhance class separation and retrain prior knowledge across increments [52, 53, 9, 37]. In this paper, instead of utilizing VQ to confine the feature space, we employ VQ to enable end-to-end learning a set of discrete prompts that effectively encode task knowledge in a learning system that evolves over time. 3 Preliminary Problem Formulation. In class-incremental learning (CIL), a model is required to sequentially learn a series of tasks with disjoint class sets, and to accurately classify all seen classes during evaluation. Formally, let Dt = {(xt i, yt i)}Nt i=1 denote the training set of the t-th task, where xt i ∈Xt is an input image, yt i ∈Yt is the target label, and Nt is the number of samples. The label spaces of all the tasks are mutually exclusive, i.e., ∩T t=1Yt = ∅, where T is the total number of tasks. Consider a deep 3learning model M = ϕ ◦ f with a backbone f(·) and a classifier ϕ(·). During training on task t, the model only has access to Dt, which raises a risk of forgetting old tasks. After learning task t, the model is expected to perform well on all classes in Y1:t = ∪t k=1Yk, and further on Y1:T after completing training on all T tasks. Prompt-based Learning is an emerging approach in NLP [ 32] that involves incorporating extra instructions into pre-trained models to guide their performance on specific tasks. Rather than relying on extensive retraining or task-specific fine-tuning, this technique leverages prompts to shape the behavior of pre-trained models, providing adaptable instructions that helps them handle a wide range of downstream tasks more effectively. In vision-related continual learning, prompting is typically employed with Vision Transformer (ViT) [11]. ViT consists of a sequence of multi-head self-attention (MSA) blocks [56]. For clarity, we take one MSA block as an example to illustrate the prompting. We denote the input query, key, and value of the MSA block as hQ, hK and hV , respectively. Here, h∗ ∈RL×D, L is the sequence length, and D is the embedding dimension. The output of the MSA is computed as: MSA(hQ, hK, hV ) =Concat(h1, . . . ,hM )WO, hm = Attention(hQWQ m, hKWK m , hV WV m ), (1) where WO, WQ m, WK m and WV m are projection matrices, m=1, ··· , Mis the head index, M is the number of heads, and hQ =hK =hV =h for SA. Previous prompt-based continual learning methods mainly implement Prompt Tuning (Pro-T) [28] and Prefix Tuning (Pre-T) [30]. Pro-T prepends the same prompt p∈RLp×D to hQ, hK, and hV . The prompting function of Pro-T is defined as: fPro-T(p, h) =MSA([p; hQ], [p; hK], [p; hV ]), (2) where [·; ·] means concatenating along the sequence length dimension. The output dimension is (Lp+L)×D. Pre-T splits p along the sequence length dimension into pK, pV ∈RLp/2×D, which are prepended to hK and hV , respectively: fPre-T(p, h) =MSA(hQ, [pK; hK], [pV ; hV ]). (3) The output dimension is the same as that of h. In continual learning, the pre-trained ViT backbone is kept frozen as a general feature extractor, and the prompt parameters p are trained to capture task knowledge. Proper prompts corresponding to the input samples are selected to guide the feature extraction during inference. Following [61, 50, 59], we adopt Pre-T strategy in our method. 4 Method As shown in Fig. 2, our VQ-Prompt approach begins by constructing a continuous prompt through a soft selection from the prompt pool (§4.1). The continuous prompt is then quantized to an element in the prompt pool, which is inserted into an MSA block of a frozen pre-trained transformer. This process is made end-to-end trainable through gradient estimation and vector quantization regularization (§4.2), such that the prompting parameters, namely the keys and the prompt pool could all be optimized using the task loss. In this way, VQ-Prompt can yield a discrete prompt for each input while maintaining end-to-end optimization. To better stabilize task knowledge learning, representation statistics of previously learned classes are employed to mitigate the classification bias (§4.3). 4.1 Prompt Formation Most previous prompting-based continual learning approaches construct their prompts by selecting from the prompt pool based on key-query similarity [62, 61] or other task identity prediction mecha- nisms [60, 59], making the prompt selection process non-differentiable. In our prompt formation, we first generate a continuous prompt by aggregating all the elements in the prompt pool based on the similarity scores between the query and the keys. Specifically, given a query q from the input image, the similarity score is calculated as: α = Softmax(Kq), (4) 4Input  Image Query Function Cosine Similarity Weighted Sum  Gradient Estimation   NN Look-up  ... Prompt Keys K ... Prompt Pool P Pre-trained Transformer Classifier ...  Insert Prompts  Statistics Figure 2: VQ-Prompt framework. An input image is passed through a query function (e.g., a fixed pre-trained ViT) to generate a query q, which is then used to compute similarity scores with prompt keys K. These scores α serve as weights to aggregate elements from the prompt pool P to form a continuous prompt p′. This prompt is subsequently quantized to an element within the prompt pool p, and then fed into a specific MSA block of a frozen pre-trained transformer. To ensure differentiability, the prompt quantization process employs gradient estimation and prompt pool regularization. The representation statistics of features from learned classes are used to stabilize task knowledge learning. More details are shown in §4. where K ∈RN×D is the prompt key matrix, q∈RD is the query, N is the number of keys, and D is the embedding dimension. Then, the continuous prompt is obtained by: p′ = X i αiPi, i = 1, ··· , N, (5) where Pi ∈RLp×D is the i-th element in the prompt pool, and Lp is the length of the prompt. Such a prompt formation process is differentiable and can be viewed as a simplified version of CODA-P [50]. Here, we do not learn an extra attention parameter for weighting the query, nor do we increase the number of elements in the prompt pool or the number of keys during sequential task learning. 4.2 Vector Quantization Prompting (VQ-Prompt) Nearest-neighbour Look-up. The continuous prompt p′ obtained in Eq. (5) is conditioned on a specific instance, i.e., it varies with the input images, making it insufficiently abstract to capture task knowledge effectively. We further perform prompt quantization by performing the nearest neighbour (NN) look-up in the prompt pool P using p′. The quantized prompt to be fed to the MSA block is obtained by: p = Pk, k= arg min j ∥p′ − Pj∥2, p∈RLp×D. (6) Such a prompt selection pipeline can be viewed as a specific non-linearity that maps the continuous prompt to 1-of-N elements in the prompt pool. Gradient Estimation. Because the arg minoperation in Eq. (6) is non-differentiable, we use the straight-through estimator [3] to approximate the gradient of p′ using the gradient of p. Despite its simplicity, this estimator has demonstrated its effectiveness in our experiments. Specifically, in the forward process, the quantized prompt p is passed to the MSA block in the pre-trained transformer. During the backward computation, the gradient ofp is transferred unaltered top′, and the optimization of prompt pool P and keys K guided by the similarity scores (c.f., Eq. (4)). This gradient estimation is justified, as p and p′ share the same Lp ×D-dimensional space, and the gradient of p provides valuable information on how prompt parameter learning could instruct the transformer features to minimize the cross-entropy (CE) loss during task learning. In this way, each prompt and key element is adjusted according to its relevance to the current learning context, rather than undergoing wholesale changes. This allows for more updates of task-relevant elements without disrupting less relevant ones, thereby maintaining previously acquired knowledge while adapting to new tasks. Vector Quantization (VQ) Regularization. Though the prompt pool P could receive gradients from the task loss through straight-through gradient estimation of mapping from p to p′, to enhance the learning of the prompt embedding space, we add an extra VQ objective. This VQ objective uses 5the L2 error to move the selected element p of prompt pool towards the continuous prompt p′: LVQ = ∥sg[p′] − p∥2 2. (7) Here, sg[·] stands for the stop-gradient operation [ 55], which constrains its operand to be a non- updated constant during training. To ensure that the learning processes of the prompt keys K and the continuous prompt p′ align closely with the characteristics of the element p from the prompt pool P, we further introduce a commitment regularization term. This term is defined mathematically as follows: LCommit = ∥p′ − sg[p]∥2 2. (8) The incorporation of this commitment loss ensures that the prompt formation process described in §4.1 is optimized to yield prompts to commit to the elements in the prompt poolP, thereby promoting consistency and stability in the prompt learning process. 4.3 Stabilizing Task Knowledge Learning with Representation Statistics Though prompts effectively capture task knowledge to guide the backbonef(·) in producing instructed representations, the classifier ϕ(·) may develop a bias towards new classes in continual learning scenarios [16, 2]. This bias can adversely affect the learning of the prompts for subsequent tasks. To mitigate this issue and stabilize task knowledge learning, we employ a strategy similar to [59], which leverages the representation statistics of previously learned classes to correct classifier bias and stabilize prompt learning. Specifically, after completing task t, we calculate the mean µc and variance σc for each class c ∈ Y1:t with the learned prompt parameters and the pre-trained backbone. By modeling each class as a Gaussian distribution, we generate pseudo features through sampling from these distributions. These pseudo features are then used to fine-tune the classifier, thereby mitigating its bias towards recent classes. The balanced classifier could help stabilize task knowledge learning in the prompts, alleviate catastrophic forgetting, and enhance overall continual learning performance. 4.4 Overall Optimization Objective The overall loss function is defined in Eq. (9), which extends the task lossLCE with two terms, namely a quantization objective LVQ weighted by λq, and a commitment term LCommit weighted by λc. L = LCE + λqLVQ + λcLCommit. (9) Here, LCE is the cross-entropy loss that supervises the learning of the image classification task, LVQ is the VQ regularization defined in Eq. (7) that updates the prompt pool elements to move towards the continuous prompt p′, and LCommit is the commitment term defined in Eq. (8) that forces the prompt formation process to commit to the prompt pool elements. During training, the pre-trained backbone f(·) is frozen, while the classifier ϕ(·) and prompting parameters K and P are optimized across all the tasks. During task knowledge stabilization, only the classifier ϕ(·) is actively trained. 5 Experiment 5.1 Experimental Setups Datasets. We consider three representative benchmarks for evaluating CIL.ImageNet-R [14] includes 200-class images that are either hard samples for ImageNet or newly collected data with different styles, thus can serve as a challenging benchmark for continual learning with pre-trained models. In the experiments, we divide it into 5, 10, and 20 disjoint tasks and report the corresponding performance. Split CIFAR-100randomly splits the original CIFAR-100 [25] into 10 disjoint tasks, each containing 10 classes. Split CUB-200is built on CUB-200-2011 [57], a fine-grained classification dataset, by randomly splitting the 200 classes into 10 tasks, where each task contains 20 classes. Baselines. We evaluate our approach against a comprehensive set of baselines to contextualize its performance. Following [ 50], we include “Joint Training” as the upper-bound performance, setting a benchmark for optimal results. To establish lower bounds, we employ two sequential learning baselines, denoted as “FT” and “FT++”, with the latter refraining from updating the logits of previously learned classes during the training of new tasks. We also consider 5 prompt-based approaches: L2P [62], DualPrompt [61], HiDe-Prompt [59], CODA- Prompt [50] and EvoPrompt [26], where the last two yield continuous prompts. For L2P, we include 6Table 1: Comparison on ImageNet-R. Results on “5-task”, “10-task”, and “20-task” settings are included. Backbones are pre-trained on ImageNet-1K. ↑ denotes larger values are better. See §5.2. Method Pub. 5-task 10-task 20-task FAA (↑) CAA ( ↑) FAA ( ↑) CAA ( ↑) FAA ( ↑) CAA ( ↑) Joint-Train. 82.06 82.06 82.06 FT 18.74 ±0.44 48.39±0.58 10.12±0.51 35.23±0.92 4.75±0.40 22.8±0.37 FT++ 60.42 ±0.87 71.59±0.50 48.93±1.15 66.79±0.92 35.98±1.38 59.68±0.95 L2P++ [62] CVPR22 70.83 ±0.58 78.34±0.47 69.29±0.73 78.30±0.69 65.89±1.30 77.15±0.65 Deep L2P++ [62] CVPR22 73.93±0.37 80.14±0.54 71.66±0.64 79.63±0.90 68.42±1.20 78.68±1.03 DualPrompt [61] ECCV22 73.05±0.50 79.47±0.40 71.32±0.62 78.94±0.72 67.87±1.39 77.42±0.80 CODA-P [50] CVPR23 76.51 ±0.38 82.04±0.54 75.45±0.56 81.59±0.82 72.37±1.19 79.88±1.06 HiDe-Prompt*[59] NeurIPS23 76.29±0.10 78.77±0.11 76.74±0.18 78.76±0.11 76.46±0.06 78.76±0.11 EvoPrompt [26] AAAI24 77.16±0.18 82.22±0.54 76.83±0.08 82.09±0.68 74.41±0.23 80.96±1.42 VQ-Prompt — 79.23±0.29 82.96±0.50 78.71±0.22 83.24±0.68 78.10±0.22 82.70±1.16 * denotes results obtained by running the official code with ImageNet-1K pre-trained weights. Table 2: Comparison on Split CIFAR-100. Backbones are pre-trained on ImageNet-1K. See §5.2 for details. Method Pub. 10-task FAA (↑) CAA ( ↑) Joint-Train. 91.38 FT 29.21 ±0.18 37.37±0.89 FT++ 49.91 ±0.42 74.76±0.93 LwF [31] TPAMI17 64.83 ±1.03 - L2P++ [62] CVPR22 82.50 ±1.10 88.96±0.82 Deep L2P++ [62] CVPR22 84.30±1.03 90.50±0.69 DualPrompt [61] ECCV22 66.00±0.57 77.92±0.50 CODA-P [50] CVPR23 70.03±0.47 74.26±0.24 EvoPrompt [26] AAAI24 87.97±0.30 92.26±0.86 VQ-Prompt — 88.73±0.2792.84±0.73 Table 3: Comparison on Split CUB-200. Back- bones are pre-trained on ImageNet-21K. ∗ de- notes backbone is not frozen. See §5.2. Method Pub. 10-task FAA (↑) CAA ( ↑) Joint-Train. 88.00 FT 11.04 ±0.78 31.96±0.74 FT++ 37.81 ±2.86 63.55±1.62 LwF [31] TPAMI17 69.75 ±1.37 80.45±2.08 BiC [63] CVPR19 81.91 ±2.59 89.92±1.57 DualPrompt [61] ECCV22 66.00±0.57 77.92±0.50 CODA-P [50] CVPR23 70.03±0.47 74.26±0.24 ∗SLCA [67] ICCV23 84.71 ±0.4090.94±0.68 HiDe-Prompt [59] NeurIPS23 86.61±0.18 87.01±0.03 VQ-Prompt — 86.72±0.9490.33±1.03 its two variations from [50]: “L2P++” and “Deep L2P++”. L2P++ uses Pre-T instead of Pro-T and inserts the prompts to the first MSA block, which achieves better performance than the original L2P [33]. Deep L2P++ extends L2P++ by incorporating prompts into the first five MSA blocks. In addition to prompt-based methods, we include a classical regularization-based method LwF [31], and a rehearsal-based method BiC [63], providing a more comprehensive overview for evaluation. Evaluation Metrics. We present Final Average Accuracy (FAA)and Cumulative Average Accuracy (CAA) for comparison. FAA refers to the last average accuracy after learning all the tasks, which is equivalent to “Last-Acc” in [67]. CAA is the average of historical FAA values after learning each task, which is equivalent to “Inc-Acc” in [67]. The formal definitions of the metrics are in §A.1. Implementation Details. We follow prior works [62, 61, 50, 59, 67, 26] and use ViT-Base [11] pre-trained with supervised learning on ImageNet-1K [46] or ImageNet-21K [43] as the backbone. The number of keys and prompt elements N is 10. The prompt length Lp is 8. The embedding dimension D=768 which is the same as the feature dimension of ViT-Base. Our method is trained using an AdamW optimizer [35] with an initial learning rate of 0.0025 and a cosine decay schedule. The batch size is 128 for Split CIFAR-100 and Split CUB-200, and 64 for ImageNet-R. The number of epochs is set to be 20 for training on all three datasets. The classifier bias mitigation process described in §4.3 requires ten epochs of training. Each experiment is run on a single NVIDIA GeForce RTX 4090 GPU. More details are presented in §A.2. 5.2 Comparison Results In this section, we present a comprehensive comparison with established baselines across various datasets and pre-training regimes. The performances of different methods are reported in separate 7Table 4: Results on 10-task ImageNet-R with different self-supervised pre-training paradigms. Method Pub. iBOT-1K [68] DINO-1K [5] FAA (↑) CAA ( ↑) FAA ( ↑) CAA ( ↑) DualPrompt [61] ECCV22 61.51 ±1.05 67.11±0.08 58.57±0.45 64.89±0.15 CODA-Prompt [50] CVPR23 66.56±0.68 73.14±0.57 63.15±0.39 69.73±0.25 HiDe-Prompt [59] NeurIPS23 71.33±0.21 73.62±0.13 68.11±0.18 71.70±0.01 VQ-Prompt — 71.68±0.72 76.66±0.40 68.42±0.28 74.43±0.58 tables due to the varying experimental settings such as the number of tasks and the pre-training dataset, to ensure a fair and accurate comparison. Results on ImageNet-R. Table 1 shows the results across five runs on ImageNet-R with 5-task, 10-task, and 20-task splits using the ViT-Base backbone pre-trained with supervised learning on ImageNet-1K. Our VQ-Prompt consistently outperforms other methods across key metrics such as FAA and CAA for all task splits, including the latest prompt-based method EvoPrompt. Results on Split CIFAR-100. Table 2 presents the results across five runs on CIFAR-100 split into 10 tasks, with the ViT-Base backbone also pre-trained on ImageNet-1K with supervised learning. Our VQ-Prompt achieves superior results compared to other methods using the same pre-training weights. Results on Split CUB-200. Table 3 displays the results on Split CUB-200. Following [67], we use the ViT-Base backbone pre-trained on ImageNet-21K for this dataset. VQ-Prompt achieves superior or comparable performance compared with all other methods, including SLCA [67], which trains the entire network without freezing the pre-trained feature extraction backbone. This highlights its potential and efficacy in continual learning for fine-grained classification tasks. Other Pre-training Regimes. Table 4 summarizes the experimental results across three runs on the 10-task ImageNet-R dataset, utilizing different self-supervised pre-training paradigms, namely iBOT-1K [68] and DINO-1K [5]. These results demonstrate that our method consistently outperforms state-of-the-art prompt-based continual learning methods, underscoring its robustness and efficiency in leveraging self-supervised pre-training for continual learning tasks. Specifically, VQ-Prompt shows a greater advantage in CAA than FAA, indicating its superior ability to leverage past knowledge for aiding current tasks, despite a slightly higher degree of forgetting relative to some baselines. This trade-off between adaptation to new tasks and forgetting of previous ones is a common challenge in continual learning. With self-supervised pre-training, our method tends to prioritize adaptability to new tasks to ensure that the model remains relevant and effective in dynamic environments. 5.3 Ablation Study and Additional Analysis In this section, we assess the effectiveness of different components illustrated in §4. The experiments are performed on 10-task ImageNet-R with the ViT-Base backbone pre-trained on ImageNet-1K. Effectiveness of VQ Design. Our VQ design (c.f., §4.2) enables end-to-end training of the discrete prompt selection in continual learning. Here, we compare it with an alternative intuition design choice, i.e., rewriting Eq. (4) as α = Softmax(Kq/τ), and reducing the temperature τ of the softmax operation. A lower temperature leads to a “sharper” distribution of α, allowing the prompt formation in Eq. (5) to more closely approximate discrete prompt selection during end-to-end training with task loss. This baseline is denoted as “Soft-Prompt”. Fig. 3 (a) presents the FAA values of Soft-Prompt with different τ values. Surprisingly, Soft-Prompt achieves its best performance of 77.15 at τ = 1.0 instead of at lower values. While its performance is comparable to other prompt- based methods, Soft-Prompt falls short of “VQ-Prompt-S”, which achieves an FAA value of78.05 on 10-task ImageNet-R. Here, VQ-Prompt-S is a simplified version of VQ-Prompt that does not use representation statistics. Our standard version VQ-Prompt further achieves an FAA value of 78.83. This observation underscores the effectiveness of our VQ design compared to the intuitive low-temperature soft prompt selection. The rationale behind this is that reducing the temperature makes the softmax operation more sensitive to differences in logits. While this heightened sensitivity is acceptable when the model is confident in its predictions, it can lead to more aggressive prompt choices at the beginning of the training when the model is less fully trained. In contrast, our VQ-Prompt utilizes a standard softmax for prompt formation, substitutes the resulting prompt with the nearest one in the prompt pool, and enables 8Softmax Temperature  FAAFAA FAA   (a) (b) (c) VQ-Prompt-S   78.05 VQ-Prompt       78.83 Figure 3: Ablation study. (a) VQ Design. We show the performance of an alternative of VQ Design, “Soft-Prompt”, that generates the continuous prompt with low-temperature softmax operation only without using VQ. Here, “VQ-Prompt-S” is a simplified version of VQ-Prompt without using representation statistics. (b) Prompt Hyperparameters. The results of varying the size of the prompt pool N and the length of a single prompt L p are displayed. (c) Loss Weights. The results of different combinations of λq and λc values are presented. See §5.3 for details. Table 5: Effectiveness of classifier bias mitigation. Results for “5-task”, “10-task”, and “20-task” settings on ImageNet-R are included. “C.B.M.” denotes “Classifier Bias Mitigation”. Backbones are pre-trained on ImageNet-1K. ↑ denotes larger values are better. See §5.3 for details. Method C.B.M. 5-task 10-task 20-task FAA (↑) CAA ( ↑) FAA ( ↑) CAA ( ↑) FAA ( ↑) CAA ( ↑) L2P++ [62] No 70.83 ±0.58 78.34±0.47 69.29±0.73 78.30±0.69 65.89±1.30 77.15±0.65 L2P++ V2 [62] Yes 74.11 ±0.08 78.44±0.63 72.93±0.27 78.63±0.80 70.99±0.26 77.65±0.79 EvoPrompt [26] No 77.16 ±0.18 82.22±0.54 76.83±0.08 82.09±0.68 74.41±0.23 80.96±1.42 VQ-Prompt-SNo 78.52±0.34 82.64±0.68 78.00±0.39 82.83±0.69 76.19±0.26 81.68±1.02 VQ-Prompt Yes 79.23±0.29 82.96±0.50 78.71±0.22 83.24±0.68 78.10±0.22 82.70±1.16 end-to-end learning through gradient estimation and VQ regularization, which proves to be more robust for task knowledge learning compared with Soft-Prompt. Hyperparameters for Prompting. There are two key hyperparameters: i) the size of the prompt pool N that represents the total capacity of the learnable prompts, and ii) the length of a single prompt L p which determines the capacity of a single prompt to encode certain aspects of task knowledge. The total size of the prompts to be prepended to the input of one MSA block is given by N ×L p. Fig. 3 (b) illustrates the impact of varying L p and N on FAA performance. Across different parameter configurations, our method consistently outperforms existing approaches, demonstrating its robustness. Specifically, an excessively small L p value consistently yields sub-optimal results, as indicated by the lower FAA scores across different N values. Increasing N can partially compensate for small L p values, leading to improved performance. In contrast, increasing L p generally enhances performance up to a certain threshold, beyond which an overly large L p may cause knowledge overfitting, as reflected by the stable or slightly declining FAA scores for larger L p values. We selected L p =8 and N =10 as our default configuration. This configuration achieves superior results with fewer parameters compared to other prompt-based methods (c.f., § A.3). Our competitive performance is primarily attributed to the use of VQ, which offers several key benefits for prompt-based continual learning. First, VQ enables the encoding of task knowledge into discrete prompts, which provide a more compact representation than continuous prompts. This discrete nature helps in capturing essential task-specific features with the necessary level of abstraction. Second, integrating VQ within the prompt-based framework facilitates end-to-end optimization with task loss, ensuring that the selected prompts are highly relevant to the task at hand, thereby enhancing the task 9knowledge learning of the prompts. This enables the use of shorter prompts while maintaining strong performance, making our approach more parameter-efficient and effective. Impact of λq and λc. To further enhance the learning of prompt-related parameters, we introduce two regularization terms LVQ and LCommit to guide the learning (c.f., §4.2). We investigate the impact of various loss weights, as shown in Eq. (9). The outcomes are detailed in Fig. 3 (c). As can be observed, these two terms can contribute to good performance when assigned with a relatively broad range of values. According to Fig. 3 (c), we set λq =0.4 and λc =0.1 in all of our experiments. Effectiveness of Classifier Bias Mitigation. The classifier bias mitigation utilizes representation statistics to stabilize task knowledge learning (c.f., §4.3), which can also be applied to other methods. To evaluate its potential advantage, we integrated this component into L2P++. As shown in Table 5, L2P++ with representation statistics (“L2P++ V2”) achieves improved performance over the original L2P++ across all three ImageNet-R settings, but remains inferior to our method. Additionally, we include the results of “VQ-Prompt-S”, a simplified version of VQ-Prompt that omits classifier bias mitigation. Notably, VQ-Prompt-S still outperforms other methods such as EvoPrompt, demonstrating the effectiveness of our approach. This indicates that the classifier bias mitigation process can contribute to performance improvements, but is not the sole determinant of the final performance. 6 Discussion and Conclusion This study focuses on one critical deficiency inherent in current prompt-based continual learning methodologies, specifically the end-to-end optimization of the prompt selection process with task loss while keeping its discrete nature as the representation of task knowledge. Our proposed Vector Quantization Prompting (VQ-Prompt) framework mitigates the challenge by substituting continuous prompts with their nearest counterparts from the prompt pool, thereby enhancing task accuracy through a more aligned and abstract representation of conceptual task knowledge. To overcome the non-differentiability inherent in this process, we employed gradient estimation along with vector quantization regularization terms, which allows for optimizing prompt retrieval with task loss. Repre- sentation statistics are utilized to further stabilize task knowledge learning. Extensive experiments in class-incremental scenarios consistently demonstrate VQ-Prompt’s superiority over SOTA methods. Limitations and Future Work. One limitation of VQ-Prompt is its dependence on pre-trained models. While these models offer rich initial knowledge, enabling a more mature learning process akin to that of an adult, they also inherit the limitations of the pre-trained data distribution and the high computational costs associated with their use. This challenge is not unique to VQ-Prompt but applies broadly to other continual learning methods that rely on pre-trained models. Another limitation is the absence of constraints in calculating similarity scores and prompt keys, which can result in suboptimal prompt utilization, i.e., some prompts are more frequently selected for samples from different tasks while others are less frequently used. To alleviate this, one possible strategy is to introduce constraints on prompt selection, such as limiting the reuse of prompts that have already been heavily utilized by previous tasks to enhance the diversity and utility of the prompts. We leave a more thorough exploration of such prompt selection constraints for future research. Broader Impacts This paper presents a prompt-based continual learning method to tackle the challenges of knowledge preservation in sequential task learning. Our approach facilitates continual adaptation and learning, thereby contributing to the advancement of intelligent, adaptive, and efficient technologies applicable to various domains, including autonomous vehicles and personalized AI agents. While VQ-Prompt advances class-incremental continual learning, its robustness could be compromised if poisonous samples are introduced after a concept has been learned. One possible mitigation strategy is to implement robust anomaly detection methods to identify and filter out suspicious input samples before they are introduced into the training process. Acknowledgments and Disclosure of Funding This work is supported by the National Natural Science Foundation of China (No. 62306292) and the Fundamental Research Funds for the Central Universities (No. CUC24QT06). 10References [1] R. Aljundi, F. Babiloni, M. Elhoseiny, M. Rohrbach, and T. Tuytelaars. Memory aware synapses: Learning what (not) to forget. In ECCV, pages 139–154, 2018. [2] E. Belouadah and A. Popescu. Il2m: Class incremental learning with dual memory. In ICCV, pages 583–592, 2019. [3] Y . Bengio, N. Léonard, and A. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013. [4] P. Buzzega, M. Boschini, A. Porrello, D. Abati, and S. Calderara. Dark experience for general continual learning: a strong, simple baseline. NeurIPS, 33:15920–15930, 2020. [5] M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging properties in self-supervised vision transformers. In CVPR, pages 9650–9660, 2021. [6] S. Cha, S. Cho, D. Hwang, S. Hong, M. Lee, and T. Moon. Rebalancing batch normalization for exemplar- based class-incremental learning. In CVPR, pages 20127–20136, 2023. [7] A. Chaudhry, M. Ranzato, M. Rohrbach, and M. Elhoseiny. Efficient lifelong learning with a-gem. In ICLR, 2019. [8] A. a. Chaudhry, M. Rohrbach, M. Elhoseiny, T. Ajanthan, P. K. Dokania, P. H. Torr, and M. Ranzato. On tiny episodic memories in continual learning. arXiv preprint arXiv:1902.10486, 2019, 2019. [9] K. Chen and C.-G. Lee. Incremental few-shot learning via vector quantization in deep embedded space. In ICLR, 2021. [10] M. De Lange, R. Aljundi, M. Masana, S. Parisot, X. Jia, A. Leonardis, G. Slabaugh, and T. Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE TPAMI, 44(7):3366–3385, 2021. [11] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Min- derer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. [12] A. Douillard, A. Ramé, G. Couairon, and M. Cord. Dytox: Transformers for continual learning with dynamic token expansion. In CVPR, pages 9285–9295, 2022. [13] P. Esser, R. Rombach, and B. Ommer. Taming transformers for high-resolution image synthesis. In CVPR, pages 12873–12883, 2021. [14] D. Hendrycks, S. Basart, N. Mu, S. Kadavath, F. Wang, E. Dorundo, R. Desai, T. Zhu, S. Parajuli, M. Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In ICCV, pages 8340–8349, 2021. [15] D. Hendrycks, K. Zhao, S. Basart, J. Steinhardt, and D. Song. Natural adversarial examples. In CVPR, pages 15262–15271, 2021. [16] S. Hou, X. Pan, C. C. Loy, Z. Wang, and D. Lin. Learning a unified classifier incrementally via rebalancing. In CVPR, pages 831–839, 2019. [17] C.-Y . Hung, C.-H. Tu, C.-E. Wu, C.-H. Chen, Y .-M. Chan, and C.-S. Chen. Compacting, picking and growing for unforgetting continual learning. NeurIPS, 32, 2019. [18] D. Isele and A. Cosgun. Selective experience replay for lifelong learning. In AAAI, volume 32, 2018. [19] M. Kiefer and F. Pulvermüller. Conceptual representations in mind and brain: Theoretical developments, current evidence and future directions. Cortex, 48(7):805–825, 2012. [20] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. PNAS, 114(13):3521–3526, 2017. [21] T. Kohonen. Improved versions of learning vector quantization. In IJCNN, pages 545–550, 1990. [22] T. Kohonen. The self-organizing map. Proceedings of the IEEE, 78(9):1464–1480, 1990. 11[23] Y . Kong, L. Liu, Z. Wang, and D. Tao. Balancing stability and plasticity through advanced null space in continual learning. In ECCV, pages 219–236, 2022. [24] T. Konishi, M. Kurokawa, C. Ono, Z. Ke, G. Kim, and B. Liu. Parameter-level soft-masking for continual learning. In ICML, 2023. [25] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009. [26] M. R. Kurniawan, X. Song, Z. Ma, Y . He, Y . Gong, Y . Qi, and X. Wei. Evolving parameterized prompt memory for continual learning. In AAAI, volume 38, pages 13301–13309, 2024. [27] S.-W. Lee, J.-H. Kim, J. Jun, J.-W. Ha, and B.-T. Zhang. Overcoming catastrophic forgetting by incremental moment matching. NeurIPS, 30, 2017. [28] B. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-efficient prompt tuning. In EMNLP, pages 3045–3059, 2021. [29] X. Li, Y . Zhou, T. Wu, R. Socher, and C. Xiong. Learn to grow: A continual structure learning framework for overcoming catastrophic forgetting. In ICML, pages 3925–3934, 2019. [30] X. L. Li and P. Liang. Prefix-tuning: Optimizing continuous prompts for generation. In ACL-IJCNLP, pages 4582–4597, 2021. [31] Z. Li and D. Hoiem. Learning without forgetting. IEEE TPAMI, 40(12):2935–2947, 2017. [32] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1–35, 2023. [33] Y . Liu and T. Tuytelaars. Residual tuning: Toward novel category discovery without labels.IEEE TNNLS, 2022. [34] D. Lopez-Paz and M. Ranzato. Gradient episodic memory for continual learning. NeurIPS, 30, 2017. [35] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In ICLR, 2018. [36] Z. Luo, Y . Liu, B. Schiele, and Q. Sun. Class-incremental exemplar compression for class-incremental learning. In CVPR, pages 11371–11380, 2023. [37] T. Malepathirana, D. Senanayake, and S. Halgamuge. Napa-vq: Neighborhood-aware prototype augmenta- tion with vector quantization for continual learning. In CVPR, pages 11674–11684, 2023. [38] T. Martinetz, K. Schulten, et al. A “neural-gas” network learns topologies. ANN, pages 397–402, 1991. [39] M. Masana, X. Liu, B. Twardowski, M. Menta, A. D. Bagdanov, and J. van de Weijer. Class-incremental learning: Survey and performance evaluation on image classification. IEEE TPAMI, 2022. [40] M. McCloskey and N. J. Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of learning and motivation, volume 24, pages 109–165. 1989. [41] G. L. Murphy and D. L. Medin. The role of theories in conceptual coherence. Psychological Review, 92 (3):289, 1985. [42] G. I. Parisi, R. Kemker, J. L. Part, C. Kanan, and S. Wermter. Continual lifelong learning with neural networks: A review. Neural Networks, 113:54–71, 2019. [43] T. Ridnik, E. Ben-Baruch, A. Noy, and L. Zelnik-Manor. Imagenet-21k pretraining for the masses. In NeurIPS, 2021. [44] M. Riemer, I. Cases, R. Ajemian, M. Liu, I. Rish, Y . Tu, and G. Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interference. In ICLR, 2019. [45] D. Rolnick, A. Ahuja, J. Schwarz, T. Lillicrap, and G. Wayne. Experience replay for continual learning. NeurIPS, 32, 2019. [46] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. IJCV, 115(3):211–252, 2015. [47] G. Saha, I. Garg, and K. Roy. Gradient projection memory for continual learning. In ICLR, 2021. [48] J. Serra, D. Suris, M. Miron, and A. Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. In ICML, pages 4548–4557, 2018. 12[49] Y . Shi, K. Zhou, J. Liang, Z. Jiang, J. Feng, P. H. Torr, S. Bai, and V . Y . Tan. Mimicking the oracle: an initial phase decorrelation approach for class incremental learning. In CVPR, pages 16722–16731, 2022. [50] J. S. Smith, L. Karlinsky, V . Gutta, P. Cascante-Bonilla, D. Kim, A. Arbelle, R. Panda, R. Feris, and Z. Kira. Coda-prompt: Continual decomposed attention-based prompting for rehearsal-free continual learning. In CVPR, pages 11909–11919, 2023. [51] Y .-M. Tang, Y .-X. Peng, and W.-S. Zheng. When prompt-based incremental learning does not meet strong pretraining. In ICCV, pages 1706–1716, 2023. [52] X. Tao, X. Chang, X. Hong, X. Wei, and Y . Gong. Topology-preserving class-incremental learning. In ECCV, pages 254–270, 2020. [53] X. Tao, X. Hong, X. Chang, S. Dong, X. Wei, and Y . Gong. Few-shot class-incremental learning. InCVPR, pages 12183–12192, 2020. [54] G. M. Van de Ven and A. S. Tolias. Three scenarios for continual learning.arXiv preprint arXiv:1904.07734, 2019. [55] A. Van Den Oord, O. Vinyals, et al. Neural discrete representation learning. In NeurIPS, 2017. [56] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. NeurIPS, 30, 2017. [57] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The caltech-ucsd birds-200-2011 dataset. 2011. [58] F.-Y . Wang, D.-W. Zhou, L. Liu, H.-J. Ye, Y . Bian, D.-C. Zhan, and P. Zhao. Beef: Bi-compatible class-incremental learning via energy-based expansion and fusion. In ICLR, 2023. [59] L. Wang, J. Xie, X. Zhang, M. Huang, H. Su, and J. Zhu. Hierarchical decomposition of prompt-based continual learning: Rethinking obscured sub-optimality. In NeurIPS, 2023. [60] Y . Wang, Z. Huang, and X. Hong. S-prompts learning with pre-trained transformers: An occam’s razor for domain incremental learning. NeurIPS, 2022. [61] Z. Wang, Z. Zhang, S. Ebrahimi, R. Sun, H. Zhang, C.-Y . Lee, X. Ren, G. Su, V . Perot, J. Dy, et al. Dualprompt: Complementary prompting for rehearsal-free continual learning. In ECCV, pages 631–648, 2022. [62] Z. Wang, Z. Zhang, C.-Y . Lee, H. Zhang, R. Sun, X. Ren, G. Su, V . Perot, J. Dy, and T. Pfister. Learning to prompt for continual learning. In CVPR, pages 139–149, 2022. [63] Y . Wu, Y . Chen, L. Wang, Y . Ye, Z. Liu, Y . Guo, and Y . Fu. Large scale incremental learning. InCVPR, pages 374–382, 2019. [64] J. Yoon, E. Yang, J. Lee, and S. J. Hwang. Lifelong learning with dynamically expandable networks. ICLR, 2018. [65] G. Zeng, Y . Chen, B. Cui, and S. Yu. Continual learning of context-dependent processing in neural networks. Nature Machine Intelligence, 1(8):364–372, 2019. [66] X. Zhai, J. Puigcerver, A. Kolesnikov, P. Ruyssen, C. Riquelme, M. Lucic, J. Djolonga, A. S. Pinto, M. Neumann, A. Dosovitskiy, et al. A large-scale study of representation learning with the visual task adaptation benchmark. arXiv preprint arXiv:1910.04867, 2019. [67] G. Zhang, L. Wang, G. Kang, L. Chen, and Y . Wei. Slca: Slow learner with classifier alignment for continual learning on a pre-trained model. In ICCV, 2023. [68] J. Zhou, C. Wei, H. Wang, W. Shen, C. Xie, A. Yuille, and T. Kong. Image bert pre-training with online tokenizer. In ICLR, 2022. 13A Appendix / supplemental material In this section, we provide detailed supplementary information. §A.1 outlines the evaluation metrics used to assess performance, providing detailed descriptions and formulas for clarity. §A.3 elaborates on the configurations of prompt-based methods compared in our experiments. §A.2 offers additional implementation details, including model training procedures, detailed parameter settings, and method- ology for obtaining results of other methods compared in the experiments. Finally, §A.4 provides more results on two challenging datasets. A.1 Evaluation Metrics To assess the performance of continual learning, we record the average classification accuracy of all seen classes at the end of each task training, and denote the average accuracy on the i-th task after learning the j-th task as Aij. The formal definitions of FAA and CAA are introduced as follows. i) Final Average Accuracy (FAA)refers to the last average accuracy after learning all the tasks: FAA = 1 T TX i=1 AiT , (10) where AiT is the average accuracy of taski after learning task T, and T is the number of tasks. Larger FAA indicates greater learning capacity and less forgetting. FAA is also denoted as “Last-Acc”. ii) Cumulative Average Accuracy (CAA)is the average of historical FAA values after learning each task, which is calculated as: CAA = 1 T TX j=1 1 j jX i=1 Aij. (11) CAA reflects the overall performance after learning each incremental task, which can also be denoted as “Inc-Acc”. A.2 More Implementation Details We use AdamW [35] with β1 = 0.9 and β2 = 0.999. Our batch size is 64 for ImageNet-R, and 128 for Split CIFAR-100 and Split CUB-200. We resize the input images to 224×224 and perform data transform following [50], including random horizontal flip and normalization. Following DualPrompt [61] and CODA-Prompt [50], we use 20% of the training data as validation data, and perform hyperparameters tuning on it. After hyperparameter searching, we use a learning rate of 0.0025 for our method. For all other prompt-based methods, we use the hyperparameters following [50] We search the values of prompt length LP from 4 to 24 with a step of 4. We search the number of prompt elements N in {10, 30, 50, 100}. We found that a prompt length of 8 and 10 prompt elements already work fine. We insert prompts at the same locations as all other implemented prompt-based methods in this paper, namely, the first 5 MSA blocks. A detailed comparison of the prompt configurations can be found in §A.3. Finally, we run FT, FT++, L2P++, Deep L2P++, DualPrompt, and CODA-Prompt by using the official implementation provided by CODA-Prompt [50]. We set the predicted logits for past task classes to be 0 to prevent gradients from flowing to the linear heads of these classes. This is recommended by CODA-Prompt to improve the performance of these methods during code reproduction, as it could alleviate the bias towards new classes in CIL for rehearsal-free methods. For HiDe-Prompt [59] and EvoPrompt [26], we reproduce the results using their respective official implementations. A.3 Configurations of Prompt-based Methods Table 6 presents the configurations of all the prompt-based continual learning methods compared in our experiment. Here, “Pro-T” denotes Prompt Tuning [28], and “Pre-T” denotes Prefix Tuning strategy [30], “Locations” indicates the MSA blocks to insert the prompts, N is the number of prompts/components in the prompt pool, and Lp is the length of a single prompt/component. L2P++ 14Table 6: Prompt configurations for prompt-based approaches in our experiments. See §A.3. Approaches Strategy Locations Datasets Hyperparameters L2P++ [62] Pre-T [0] All N=30, Lp=20 Deep L2P++ [62] Pre-T [0 1 2 3 4] All N=30, Lp=20 Dual-Prompt [61] Pre-T [0 1] All G: N=1, Lp=6 Pre-T [2 3 4] All E: N=10, Lp=20 CODA-P [50] Pre-T [0 1 2 3 4] All N=100, Lp=8 HiDe-Prompt [59] Pre-T [0 1 2 3 4] ImageNet-R N=10, Lp=40 Split CIFAR-100N=10, Lp=10 Split CUB-200 N=10, Lp=40 EvoPrompt [26] Pro-T [0 1 2 3 4 5 6 7 8 9 10 11] All Input-cond., Lp=5 VQ-Prompt (Ours) Pre-T [0 1 2 3 4] All N=10, Lp=8 and Deep L2P++ are two variants of L2P for fair comparison. Specifically, L2P++ uses Pre-T instead of Pro-T prompting, and inserts the prompts to the first MSA block. Deep L2P++ is an extension of L2P++ with prompts incorporated into the same 5 MSA blocks as DualPrompt. For DualPrompt, “G” denotes the general prompt shared by all the tasks, and “E” denotes the expert prompt pool where only one of the elements is selected for a certain query. As can be observed, our method requires fewer prompting parameters while consistently achieving superior or comparable performance across the benchmarks in continual learning. A.4 More Experiment Results This section presents results on two challenging datasets, namely ImageNet-A [15] and VTAB [66], for evaluating continual learning methods based on pre-trained models. ImageNet-A contains adversarial images that fool current ImageNet pre-trained classifiers, while VTAB includes 19 datasets with diverse classes that do not overlap with ImageNet-1K. For ImageNet-A, we split the 200 classes into 20 tasks. For VTAB, we sample five 10-class datasets from it to construct the cross-domain CIL setting. We used a batch size of 64 for ImageNet-A and 8 for VTAB, with other training hyperparameters consistent with those used on other datasets. As shown in Table 7, our VQ-Prompt outperforms other SOTA methods such as HiDe-Prompt when evaluated using FAA. Table 7: Results evaluated using the FAA metric on the ImageNet-A and VTAB datasets. Backbones are pre-trained on ImageNet-1K. Larger values are better. Method ImageNet-A [15] VTAB [66] HiDe-Prompt [59] 51.67 86.38 VQ-Prompt 52.96 90.46 15",
      "meta_data": {
        "arxiv_id": "2410.20444v2",
        "authors": [
          "Li Jiao",
          "Qiuxia Lai",
          "Yu Li",
          "Qiang Xu"
        ],
        "published_date": "2024-10-27T13:43:53Z",
        "pdf_url": "https://arxiv.org/pdf/2410.20444v2.pdf"
      }
    },
    {
      "title": "Mitigating Forgetting in Online Continual Learning with  Neuron Calibration"
    },
    {
      "title": "Uncertainty-guided Continual Learning with Bayesian Neural Networks",
      "abstract": "Continual learning aims to learn new tasks without forgetting previously\nlearned ones. This is especially challenging when one cannot access data from\nprevious tasks and when the model has a fixed capacity. Current\nregularization-based continual learning algorithms need an external\nrepresentation and extra computation to measure the parameters'\n\\textit{importance}. In contrast, we propose Uncertainty-guided Continual\nBayesian Neural Networks (UCB), where the learning rate adapts according to the\nuncertainty defined in the probability distribution of the weights in networks.\nUncertainty is a natural way to identify \\textit{what to remember} and\n\\textit{what to change} as we continually learn, and thus mitigate catastrophic\nforgetting. We also show a variant of our model, which uses uncertainty for\nweight pruning and retains task performance after pruning by saving binary\nmasks per tasks. We evaluate our UCB approach extensively on diverse object\nclassification datasets with short and long sequences of tasks and report\nsuperior or on-par performance compared to existing approaches. Additionally,\nwe show that our model does not necessarily need task information at test time,\ni.e. it does not presume knowledge of which task a sample belongs to.",
      "full_text": "Published as a conference paper at ICLR 2020 UNCERTAINTY -GUIDED CONTINUAL LEARNING WITH BAYESIAN NEURAL NETWORKS Sayna Ebrahimi∗ UC Berkeley Mohamed Elhoseiny† KAUST, Stanford University Trevor Darrell UC Berkeley Marcus Rohrbach Facebook AI Research ABSTRACT Continual learning aims to learn new tasks without forgetting previously learned ones. This is especially challenging when one cannot access data from previous tasks and when the model has a ﬁxed capacity. Current regularization-based continual learning algorithms need an external representation and extra computation to measure the parameters’ importance. In contrast, we propose Uncertainty- guided Continual Bayesian Neural Networks (UCB), where the learning rate adapts according to the uncertainty deﬁned in the probability distribution of the weights in networks. Uncertainty is a natural way to identify what to rememberand what to change as we continually learn, and thus mitigate catastrophic forgetting. We also show a variant of our model, which uses uncertainty for weight pruning and retains task performance after pruning by saving binary masks per tasks. We evaluate our UCB approach extensively on diverse object classiﬁcation datasets with short and long sequences of tasks and report superior or on-par performance compared to existing approaches. Additionally, we show that our model does not necessarily need task information at test time, i.e. it does not presume knowledge of which task a sample belongs to. 1 I NTRODUCTION Humans can easily accumulate and maintain knowledge gained from previously observed tasks, and continuously learn to solve new problems or tasks. Artiﬁcial learning systems typically forget prior tasks when they cannot access all training data at once but are presented with task data in sequence. Overcoming these challenges is the focus ofcontinual learning, sometimes also referred to as lifelong learning or sequential learning. Catastrophic forgetting(McCloskey & Cohen, 1989; McClelland et al., 1995) refers to the signiﬁcant drop in the performance of a learner when switching from a trained task to a new one. This phenomenon occurs because trained parameters on the initial task change in favor of learning new objectives. Given a network of limited capacity, one way to address this problem is to identify the importance of each parameter and penalize further changes to those parameters that were deemed to be important for the previous tasks (Kirkpatrick et al., 2017; Aljundi et al., 2018; Zenke et al., 2017). An alternative is to freeze the most important parameters and allow future tasks to only adapt the remaining parameters to new tasks (Mallya & Lazebnik, 2018). Such models rely on the explicit parametrization of importance. We propose here implicit uncertainty-guided importance representation. Bayesian approaches to neural networks (MacKay, 1992b) can potentially avoid some of the pitfalls of explicit parameterization of importance in regular neural networks. Bayesian techniques, naturally account for uncertainty in parameters estimates. These networks represent each parameter with a distribution deﬁned by a mean and variance over possible values drawn from a shared latent probability distribution (Blundell et al., 2015). Variational inference can approximate posterior distributions using Monte Carlo sampling for gradient estimation. These networks act like ensemble methods in that they reduce the prediction variance but only use twice the number of parameters present in a regular neural network. We propose to use the predicted mean and variance of the latent distributions to characterize the importance of each parameter. We perform continual learning with ∗Corresponding author: sayna@berkeley.edu †Work done while at Facebook AI Research 1 arXiv:1906.02425v2  [cs.LG]  20 Feb 2020Published as a conference paper at ICLR 2020 (a) (b) (c)  Illustration of evolution of weight distributions through learning two tasks. (a) circles represent  weight parameters, initialized by distributions with mean and variance values randomly sampled  from Ɲ(0,0.1).  As an example we show five color-coded and plot their distributions. (b) Shows  posterior distribution after learning Task 1. While W1 and W2 exhibit lower uncertainties (more  contributions in learning Task 1), W3, W4, and W5 appear to have larger uncertainties, with the  highest STD in W5, making them available to learn more tasks. (c) Task 2 is learned using higher  learning rates for previously uncertain parameters (W3 and W4, W5) while learning rates for W1  and W2 are moderated according to their predicted low uncertainty after finishing task 1.  p(𝜃) 𝜃 𝜃 𝜃 Training Task 1 Training Task 2 1 3 2 4 p(𝜃) p(𝜃) 5 p(y| x,[𝜃1,𝜃2,𝜃3,𝜃4,𝜃5]) ⩰ p(y) p(y| x,[𝜃1,𝜃2,𝜃3,𝜃4,𝜃5]) ⩰  p(y|x,[𝜃1,𝜃2]) p(y| x, [𝜃1,𝜃2,𝜃3,𝜃4,𝜃5]) ⩰  p(y|x,[𝜃1,𝜃2,𝜃3,𝜃4]) Figure 1: Illustration of the evolution of weight distributions – uncertain weights adapt more quickly – when learning two tasks using UCB. (a) weight parameter initialized by distributions initialized with mean and variance values randomly sampled from N(0,0.1). (b) posterior distribution after learning task one; while θ1 and θ2 exhibit lower uncertainties after learning the ﬁrst task, θ3, θ4, and θ5 have larger uncertainties, making them available to learn more tasks. (c) a second task is learned using higher learning rates for previously uncertain parameters (θ1, θ2, θ3, and θ4) while learning rates for θ1 and θ2 are reduced. Size of the arrows indicate the magnitude of the change of the distribution mean upon gradient update. Bayesian neural networks by controlling the learning rate of each parameter as a function of its uncertainty. Figure 1 illustrates how posterior distributions evolve for certain and uncertain weight distributions while learning two consecutive tasks. Intuitively, the more uncertain a parameter is, the more learnable it can be and therefore, larger gradient steps can be taken for it to learn the current task. As a hard version of this regularization technique, we also show that pruning, i.e., preventing the most important model parameters from any change and learning new tasks with the remaining parameters, can be also integrated into UCB. We refer to this method as UCB-P. Contributions: We propose to perform continual learning with Bayesian neural networks and develop a new method which exploits the inherent measure of uncertainty therein to adapt the learning rate of individual parameters (Sec. 4). Second, we introduce a hard-threshold variant of our method that decides which parameters to freeze (Sec. 4.2). Third, in Sec. 5, we extensively validate our approach experimentally, comparing it to prior art both on single datasets split into different tasks, as well as for the more difﬁcult scenario of learning a sequence of different datasets. Forth, in contrast to most prior work, our approach does not rely on knowledge about task boundaries at inference time, which humans do not need and might not be always available. We show in Sec. 6 that our approach naturally supports this scenario and does not require task information at test time, sometimes also referred to as a “single head” scenario for all tasks. We refer to evaluation metric of a “single head” model without task information at test time as “generalized accuracy”. Our code is available at https://github.com/SaynaEbrahimi/UCB. 2 R ELATED WORK Conceptually, approaches to continual learning can be divided into the following categories: dynamic architectural methods, memory-based methods, and regularization methods. Dynamic architectural methods: In this setting, the architecture grows while keeping past knowl- edge ﬁxed and storing new knowledge in different forms such as additional layers, nodes, or modules. In this approach, the objective function remains ﬁxed whereas the model capacity grows –often exponentially– with the number of tasks. Progressive networks (Rusu et al., 2016; Schwarz et al., 2018) was one of the earliest works in this direction and was successfully applied to reinforcement learning problems; the base architecture was duplicated and lateral connections added in response to new tasks. Dynamically Expandable Network (DEN) (Yoon et al., 2018) also expands its network by selecting drifting units and retraining them on new tasks. In contrast to our method, these approaches require the architecture grow with each new task. Memory-based methods:In this regime, previous information is partially stored to be used later as a form of rehearsal (Robins, 1995). Gradient episodic memory (GEM) (Lopez-Paz et al., 2017) uses this idea to store the data at the end of each episode to be used later to prevent gradient updates from deviating from their previous values. GEM also allows for positive backward knowledge transfer, i.e, 2Published as a conference paper at ICLR 2020 an improvement on previously learned tasks, and it was the ﬁrst method capable of learning using a single training example. Recent approaches in this category have mitigated forgetting by using external data combined with distillation loss and/or conﬁdence-based sampling strategies to select the most representative samples. (Castro et al., 2018; Wu et al., 2019; Lee et al., 2019) Regularization methods: In these approaches, signiﬁcant changes to the representation learned for previous tasks are prevented. This can be performed through regularizing the objective function or directly enforced on weight parameters. Typically, thisimportance measure is engineered to represent the importance of each parameter. Inspired by Bayesian learning, in elastic weight consolidation (EWC) method (Kirkpatrick et al., 2017) important parameters are those to have the highest in terms of the Fisher information matrix. In Synaptic Intelligence (SI) (Zenke et al., 2017) this parameter importance notion is engineered to correlate with the loss function: parameters that contribute more to the loss are more important. Similar to SI, Memory-aware Synapses (MAS) (Aljundi et al., 2018) proposed an online way of computing importance adaptive to the test set using the change in the model outputs w.r.t the inputs. While all the above algorithms are task-dependent, in parallel development to this work, (Aljundi et al., 2019) has recently investigated task-free continual learning by building upon MAS and using a protocol to update the weights instead of waiting until the tasks are ﬁnished. PackNet (Mallya & Lazebnik, 2018) used iterative pruning to fully restrict gradient updates on important weights via binary masks. This method requires knowing which task is being tested to use the appropriate mask. PackNet also ranks the weight importance by their magnitude which is not guaranteed to be a proper importance indicative. HAT (Serra et al., 2018) identiﬁes important neurons by learning an attention vector to the task embedding to control the gradient propagation. It maintains the information learned on previous tasks using an almost-binary mask per previous tasks. Bayesian approaches: Using Bayesian approach in learning neural networks has been studied for few decades (MacKay, 1992b;a). Several approaches have been proposed for Bayesian neural networks, based on, e.g., the Laplace approximation (MacKay, 1992a), Hamiltonian Monte Carlo (Neal, 2012), variational inference (Hinton & Van Camp, 1993; Graves, 2011), and probabilistic backpropagation (Hern´andez-Lobato & Adams, 2015). Variational continual learning (Nguyen et al., 2018) uses Bayesian inference to perform continual learning where new posterior distribution is simply obtained by multiplying the previous posterior by the likelihood of the dataset belonging to the new task. They also showed that by using a core-set, a small representative set of data from previous tasks, VCL can experience less forgetting. In contrast, we rely on Bayesian neural networks to use their predictive uncertainty to perform continual learning. Moreover, we do not use episodic memory or any other way to access or store previous data in our approach. Natural gradient descent methods:A fast natural gradient descent method for variational inference was introduced in (Khan & Nielsen, 2018) in which, the Fisher Information matrix is approximated using the generalized Gauss-Newton method. In contrast, in our work, we use classic gradient descent. Although second order optimization algorithms are proven to be more accurate than the ﬁrst order methods, they add considerable computational cost. Tseran et al. (2018); Chen et al. (2019) both investigate the effect of natural gradient descent methods as an alternative to classic gradient descent used in VCL and EWC methods. GNG (Chen et al., 2019) uses Gaussian natural gradients in the Adam optimizer (Kingma & Ba, 2014) in the framework of VCL because as opposed to conventional gradient methods which perform in Euclidian space, natural gradients cause a small difference in terms of distributions following the changes in parameters in the Riemannian space. Similar to VCL, they obtained their best performance by adding a coreset of previous examples. Tseran et al. (2018) introduce two modiﬁcations to VCL called Natural-VCL (N-VCL) and VCL-Vadam. N-VCL (Tseran et al., 2018) uses a Gauss-Newton approximation introduced by (Schraudolph, 2002; Graves, 2011) to estimate the VCL objective function and used natural gradient method proposed in (Khan et al., 2018) to exploit the Riemannian geometry of the variational posterior by scaling the gradient with an adaptive learning rate equal to σ−2 obtained by approximating the Fisher Information matrix in an online fashion. VCL-Vadam (Tseran et al., 2018) is a simpler version of N-VCL to trade-off accuracy for simplicity which uses Vadam (Khan et al., 2018) to update the gradients by perturbing the weights with a Gaussian noise using a reparameterization trick and scaling by σ−1 instead of its squared. N-VCL/VCL-Vadam both use variational inference to adapt the learning rate within Adam optimizer at every time step, whereas in our method below, gradient decent is used with constant learning rate during each task where learning rate scales with uncertainty only after ﬁnishing a task. We show extensive comparison with state-of-the-art results on short and relatively long sequence of vision datasets with Bayesian convolutional neural networks, whereas VCL-Vadam only rely on 3Published as a conference paper at ICLR 2020 multi-layer perceptron networks. We also like to highlight that this is the ﬁrst work which evaluates and shows the working of convolutional Bayesian Neural Networks rather than only fully connected MLP models for continual learning. 3 B ACKGROUND : VARIATIONAL BAYES -BY-BACKPROP In this section, we review the Bayes-by-Backprop (BBB) framework which was introduced by (Blundell et al., 2015); to learn a probability distribution over network parameters. (Blundell et al., 2015) showed a back-propagation-compatible algorithm which acts as a regularizer and yields comparable performance to dropout on the MNIST dataset. In Bayesian models, latent variables are drawn from a prior density p(w) which are related to the observations through the likelihood p(x|w). During inference, the posterior distribution p(w|x) is computed conditioned on the given input data. However, in practice, this probability distribution is intractable and is often estimated through approximate inference. Markov Chain Monte Carlo (MCMC) sampling (Hastings, 1970) has been widely used and explored for this purpose, see (Robert & Casella, 2013) for different methods under this category. However, MCMC algorithms, despite providing guarantees for ﬁnding asymptotically exact samples from the target distribution, are not suitable for large datasets and/or large models as they are bounded by speed and scalability issues. Alternatively, variational inference provides a faster solution to the same problem in which the posterior is approximated using optimization rather than being sampled from a chain (Hinton & Van Camp, 1993). Variational inference methods always take advantage of fast optimization techniques such as stochastic methods or distributed methods, which allow them to explore data models quickly. See (Blei et al., 2017) for a complete review of the theory and (Shridhar et al., 2018) for more discussion on how to use Bayes by Backprop (BBB) in convolutioal neural networks. 3.1 B AYES BY BACKPROP (BBB) Let x ∈I Rn be a set of observed variables and w be a set of latent variables. A neural network, as a probabilistic model P(y|x,w), given a set of training examples D= (x,y) can output y which belongs to a set of classes by using the set of weight parameters w. Variational inference aims to calculate this conditional probability distribution over the latent variables by ﬁnding the closest proxy to the exact posterior by solving an optimization problem. We ﬁrst assume a family of probability densities over the latent variables w parametrized by θ, i.e., q(w|θ). We then ﬁnd the closest member of this family to the true conditional probability of interest P(w|D) by minimizing the Kullback-Leibler (KL) divergence between qand P which is equivalent to minimizing variational free energy or maximizing the expected lower bound: θ∗= arg minθKL ( q(w|θ)∥P(w|D) ) (1) The objective function can be written as: LBBB(θ,D) = KL [ q(w|θ)∥P(w) ] −Eq(w|θ) [ log(P(D|w)) ] (2) Eq. 2 can be approximated using N Monte Carlo samples wi from the variational posterior (Blundell et al., 2015): LBBB(θ,D) ≈ N∑ i=1 log q(wi|θ) −log P(wi) −log(P(D|wi)) (3) We assume q(w|θ) to have a Gaussian pdf with diagonal covariance and parametrized by θ= (µ,ρ). A sample weight of the variational posterior can be obtained by sampling from a unit Gaussian and reparametrized by w = µ+ σ◦ϵ where ϵ is the noise drawn from unit Gaussian, and ◦is a pointwise multipliation. Standard deviation is parametrized as σ = log(1 + exp( ρ)) and thus is always positive. For the prior, as suggested by Blundell et al. (2015), a scale mixture of two Gaussian pdfs are chosen which are zero-centered while having different variances ofσ2 1 and σ2 2. The uncertainty obtained for every parameter has been successfully used in model compression (Han et al., 2015) and uncertainty-based exploration in reinforcement learning (Blundell et al., 2015). In this work we propose to use this framework to learn sequential tasks without forgetting using per-weight uncertainties. 4Published as a conference paper at ICLR 2020 4 U NCERTAINTY -GUIDED CONTINUAL LEARNING IN BAYESIAN NEURAL NETWORKS In this section, we introduce Uncertainty-guided Continual learning approach with Bayesian neural networks (UCB), which exploits the estimated uncertainty of the parameters’ posterior distribution to regulate the change in “important” parameters both in a soft way (Section 4.1) or setting a hard threshold (Section 4.2). 4.1 UCB WITH LEARNING RATE REGULARIZATION A common strategy to perform continual learning is to reduce forgetting by regularizing further changes in the model representation based on parameters’importance. In UCB the regularization is performed with the learning rate such that the learning rate of each parameter and hence its gradient update becomes a function of its importance. As shown in the following equations, in particular, we scale the learning rate of µand ρfor each parameter distribution inversely proportional to its importance Ω to reduce changes in important parameters while allowing less important parameters to alter more in favor of learning new tasks. αµ ←αµ/Ωµ (4) αρ ←αρ/Ωρ (5) The core idea of this work is to base the deﬁnition of importance on the well-deﬁned uncertainty in parameters distribution of Bayesian neural networks, i.e., setting the importance to be inversely proportional to the standard deviation σwhich represents the parameter uncertainty in the Baysian neural network: Ω ∝1/σ (6) We explore different options to set Ω in our ablation study presented in Section A.2 of the appendix, Table 1. We empirically found that Ωµ = 1/σand not adapting the learning rate for ρ(i.e. Ωρ = 1) yields the highest accuracy and the least forgetting. The key beneﬁt of UCB with learning rate as the regularizer is that it neither requires additional memory, as opposed to pruning technique nor tracking the change in parameters with respect to the previously learned task, as needed in common weight regularization methods. More importantly, this method does not need to be aware of task switching as it only needs to adjust the learning rates of the means in the posterior distribution based on their current uncertainty. The complete algorithm for UCB is shown in Algorithm 1 with parameter update function given in Algorithm 2. 4.2 UCB USING WEIGHT PRUNING (UCB-P) In this section, we introduce a variant of our method, UCB-P, which is related to recent efforts in weight pruning in the context of reducing inference computation and network compression (Liu et al., 2017; Molchanov et al., 2016). More speciﬁcally, weight pruning has been recently used in continual learning (Mallya & Lazebnik, 2018), where the goal is to continue learning multiple tasks using a single network’s capacity. (Mallya & Lazebnik, 2018) accomplished this by freeing up parameters deemed to be unimportant to the current task according to their magnitude. Forgetting is prevented in pruning by saving a task-speciﬁc binary mask of important vs. unimportant parameters. Here, we adapt pruning to Bayesian neural networks. Speciﬁcally, we propose a different criterion for measuring importance: the statistically-grounded uncertainty deﬁned in Bayesian neural networks. Unlike regular deep neural networks, in a BBB model weight parameters are represented by proba- bility distributions parametrized by their mean and standard deviation. Similar to (Blundell et al., 2015), in order to take into account both mean and standard deviation, we use the signal-to-noise ratio (SNR) for each parameter deﬁned as Ω = SNR = |µ|/σ (7) 5Published as a conference paper at ICLR 2020 Algorithm 1Uncertainty-guided Continual Learning with Bayesian Neural Networks UCB 1: Require Training data for all tasks D= (x,y), µ(mean of posterior), ρ, σ1 and σ2 (std for the scaled mixture Gaussian pdf of prior), π(weighting factor for prior), N (number of samples in a mini-batch), M (Number of minibatches per epoch), initial learning rate (α0) 2: αµ = αρ = α0 3: for every task do 4: repeat 5: ϵ∼N(0,I) 6: σ= log(1 + exp(ρ)) ⊿ Ensures σis always positive 7: w = µ+ σ◦ϵ ⊿w = {w1,..., wi,..., wN}posterior samples of weights 8: l1 = ∑N i=1 log N(wi|µ,σ2) ⊿ l1 := Log-posterior 9: l2 = ∑N i=1 log ( πN(wi |0,σ2 1) + (1−π)N(wi |0,σ2 2) ) ⊿ l2 := Log-prior 10: l3 = ∑N i=1 log(p(D|wi)) ⊿ l3 := Log-likelihood of data 11: LBBB = 1 M(l1 −l2 −l3 ) 12: µ←µ−αµ∇LBBBµ 13: ρ←ρ−αρ∇LBBBρ 14: until loss plateaus 15: αµ,αρ ←LearningRateUpdate(αµ,αρ,σ,µ) ⊿ See Algorithm 2 for UCB and 3 for UCB-P 16: end for Algorithm 2LearningRateUpdate in UCB 1: function LearningRateUpdate(αµ,αρ,σ) 2: for each parameter do 3: Ωµ ←1/σ 4: Ωρ ←1 5: αµ ←αµ/Ωµ 6: αρ ←αρ/Ωρ 7: end for 8: end function Algorithm 3LearningRateUpdate in UCB-P 1: function LearningRateUpdate(αµ,αρ,σ,µ) 2: for each parameter jin each layer ldo 3: Ω ←|µ|/σ ⊿ Signal to noise ratio 4: if Ω[j] ∈top p% of Ωs in lthen 5: αµ = αρ = 0 6: end if 7: end for 8: end function SNR is a commonly used measure in signal processing to distinguish between “useful” information from unwanted noise contained in a signal. In the context of neural models, the SNR can be thought as an indicative of parameter importance; the higher the SNR, the more effective or important the parameter is to the model predictions for a given task. UCB-P, as shown in Algorithms 1 and 3, is performed as follows: for every layer, convolutional or fully-connected, the parameters are ordered by their SNR value and those with the lowest importance are pruned (set to zero). The pruned parameters are marked using a binary mask so that they can be used later in learning new tasks whereas the important parameters remain ﬁxed throughout training on future tasks. Once a task is learned, an associated binary mask is saved which will be used during inference to recover key parameters and hence the exact performance to the desired task. The overhead memory per parameter in encoding the mask as well as saving it on the disk is as follows. Assuming we have ntasks to learn using a single network, the total number of required bits to encode an accumulated mask for a parameter is at max log2 nbits assuming a parameter deemed to be important from task 1 and kept being encoded in the mask. 5 R ESULTS 5.1 E XPERIMENTAL SETUP Datasets: We evaluate our approach in two common scenarios for continual learning: 1) class- incremental learning of a single or two randomly alternating datasets, where each task covers only a subset of the classes in a dataset, and 2) continual learning of multiple datasets, where each task is a dataset. We use Split MNIST with 5 tasks (5-Split MNIST) similar to (Nguyen et al., 2018; Chen et al., 2019; Tseran et al., 2018) and permuted MNIST (Srivastava et al., 2013) for class incremental learning with similar experimental settings as used in (Serra et al., 2018; Tseran et al., 2018). Furthermore, to have a better understanding of our method, we evaluate our approach on continually learning a sequence of 8 datasets with different distributions using the identical sequence 6Published as a conference paper at ICLR 2020 as in (Serra et al., 2018), which includes FaceScrub (Ng & Winkler, 2014), MNIST, CIFAR100, NotMNIST (Bulatov, 2011), SVHN (Netzer et al., 2011), CIFAR10, TrafﬁcSigns (Stallkamp et al., 2011), and FashionMNIST (Xiao et al., 2017). Details of each are summarized in Table 4 in appendix. No data augmentation of any kind has been used in our analysis. Baselines: Within the Bayesian framework, we compare to three models which do not incorporate the importance of parameters, namely ﬁne-tuning, feature extraction, and joint training. In ﬁne-tuning (BBB-FT), training continues upon arrival of new tasks without any forgetting avoidance strategy. Feature extraction, denoted as (BBB-FE), refers to freezing all layers in the network after training the ﬁrst task and training only the last layer for the remaining tasks. In joint training (BBB-JT) we learn all the tasks jointly in a multitask learning fashion which serves as the upper bound for average accuracy on all tasks, as it does not adhere to the continual learning scenario. We also perform the counterparts for FT, FE, and JT using ordinary neural networks and denote them as ORD-FT, ORD- FE, and ORD-JT. From the prior work, we compare with state-of-the-art approaches including Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017), Incremental Moment Matching (IMM) (Lee et al., 2017), Learning Without Forgetting (LWF) (Li & Hoiem, 2016), Less-Forgetting Learning (LFL) (Jung et al., 2016), PathNet (Fernando et al., 2017), Progressive neural networks (PNNs) (Rusu et al., 2016), and Hard Attention Mask (HAT) (Serra et al., 2018) using implementations provided by (Serra et al., 2018). On Permuted MNIST results for SI (Zenke et al., 2017) are reported from (Serra et al., 2018). On Split and Permuted MNIST, results for VCL (Nguyen et al., 2018) are obtained using their original provided code whereas for VCL-GNG (Chen et al., 2019) and VCL-Vadam (Tseran et al., 2018) results are reported from the original work without re-implementation. Because our method lies into the regularization-based regime, we only compare against baselines which do not beneﬁt from episodic or coreset memory. Hyperparameter tuning: Unlike commonly used tuning techniques which use a validation set composed of all classes in the dataset, we only rely on the ﬁrst two task and their validations set, similar to the setup in (Chaudhry et al., 2019). In all our experiments we consider a 0.15 split for the validation set on the ﬁrst two tasks. After tuning, training starts from the beginning of the sequence. Our scheme is different from (Chaudhry et al., 2019), where the models are trained on the ﬁrst (e.g. three) tasks for validation and then training is restarted for the remaining ones and the reported performance is only on the remaining tasks. Training details:It is important to note that in all our experiments, no pre-trained model is used. We used stochastic gradient descent with a batch size of 64 and a learning rate of 0.01, decaying it by a factor of 0.3 once the loss plateaued. Dataset splits and batch shufﬂe are identically in all UCB experiments and all baselines. Pruning procedure and mask size: Once a task is learned, we compute the performance drop for a set of arbitrary pruning percentages from the maximum training accuracy achieved when no pruning is applied. The pruning portion is then chosen using a threshold beyond which the performance drop is not accepted. Mask size is chosen without having the knowledge of how many tasks to learn in the future. Upon learning each task we used a uniform distribution of pruning ratios (50-100%) and picked the ratio resulted in at most 1%, 2%, and 3% forgetting for MNIST, CIFAR, and 8tasks experiments, respectively. We did not tune this parameter because in our hyperparameter tuning, we only assume we have validation sets of the ﬁrst two tasks. Parameter regularization and importance measurement:Table 1 ablates different ways to com- pute the importance Ω of an parameter in Eq. 4 and 5. As shown in Table 1 the conﬁguration that yields the highest accuracy and the least forgetting (maximum BWT) occurs when the learning rate regularization is performed only on µof the posteriors using Ωµ = 1/σas the importance and Ωρ = 1. Performance measurement:Let nbe the total number of tasks. Once all are learned, we evaluate our model on all n tasks. ACC is the average test classiﬁcation accuracy across all tasks. To measure forgetting we report backward transfer, BWT, which indicates how much learning new tasks has inﬂuenced the performance on previous tasks. While BWT <0 directly reports catastrophic forgetting, BWT >0 indicates that learning new tasks has helped with the preceding tasks. Formally, BWT and ACC are as follows: BWT = 1 n n∑ i=1 Ri,n −Ri,i, ACC = 1 n n∑ i=1 Ri,n (8) 7Published as a conference paper at ICLR 2020 Table 1: Variants of learning rate regularization and importance measurement on 2-Split MNIST Method µ ρ Importance Ω BWT (%) ACC (%) UCB x - 1/σ 0.00 99 .2 UCB - x 1/σ −0.04 98 .7 UCB x x 1/σ −0.02 98 .0 UCB x - |µ|/σ −0.03 98 .4 UCB - x |µ|/σ −0.52 98 .7 UCB x x |µ|/σ −0.32 98 .8 UCB-P x x |µ|/σ −0.01 99 .0 UCB-P x x 1/σ −0.01 98 .9 Table 2: Continually learning on different datasets. BWT and ACC in %. (*) denotes that methods do not adhere to the continual learning setup: BBB-JT and ORD-JT serve as the upper bound for ACC for BBB/ORD networks, respectively. ‡denotes results reported by (Serra et al., 2018). †denotes the result reported from original work. BWT was not reported in ‡and †. All others results are (re)produced by us and are averaged over 3 runs with standard deviations given in Section A.3 of the appendix. (a) 5-Split MNIST, 5 tasks. Method BWT ACC VCL-Vadam† - 99.17 VCL-GNG† - 96.50 VCL - 0.56 98 .20 IMM - 11.20 88 .54 EWC - 4.20 95 .78 HAT 0.00 99 .59 ORD-FT - 9.18 90 .60 ORD-FE 0.00 98 .54 BBB-FT - 6.45 93 .42 BBB-FE 0.00 98 .76 UCB-P (Ours) - 0.72 99 .32 UCB (Ours) 0.00 99.63 ORD-JT∗ 0.00 99 .78 BBB-JT∗ 0.00 99 .87 (b) Permuted MNIST, 10 permutations. Method #Params BWT ACC SI ‡ 0.1M - 86.0 EWC ‡ 0.1M - 88.2 HAT ‡ 0.1M - 91.6 VCL-Vadam†0.1M - 86.34 VCL-GNG† 0.1M - 90.50 VCL 0.1M - 7.90 88.80 UCB (Ours) 0.1M -0.38 91.44 LWF 1.9M - 31.17 65.65 IMM 1.9M - 7.14 90.51 HAT 1.9M 0.03 97.34 BBB-FT 1.9M - 0.58 90.01 BBB-FE 1.9M 0.02 93.54 UCB-P (Ours) 1.9M - 0.95 97.24 UCB (Ours) 1.9M 0.03 97.42 BBB-JT∗ 1.9M 0.00 98.12 (c) Alternating CIFAR10/100 Method BWT ACC PathNet 0.00 28 .94 LWF - 37.9 42 .93 LFL - 24.22 47 .67 IMM - 12.23 69 .37 PNN 0.00 70 .73 EWC - 1.53 72 .46 HAT - 0.04 78 .32 BBB-FE - 0.04 51 .04 BBB-FT - 7.43 68 .89 UCB-P (Ours) - 1.89 77 .32 UCB (Ours) -0.72 79.44 BBB-JT∗ 1.52 83 .93 (d) Sequence of 8 tasks Method BWT ACC LFL - 10.0 8 .61 PathNet 0.00 20 .22 LWF - 54.3 28 .22 IMM - 38.5 43 .93 EWC - 18.04 50 .68 PNN 0.00 76 .78 HAT - 0.14 81 .59 BBB-FT - 23.1 43 .09 BBB-FE - 0.01 58 .07 UCB-P (Ours) - 2.54 80 .38 UCB (Ours) -0.84 84.04 BBB-JT∗ -1.2 84 .1 where Ri,n is the test classiﬁcation accuracy on task iafter sequentially ﬁnishing learning the nth task. Note that in UCB-P, Ri,i refers the test accuracy on taskibefore pruning and Ri,n after pruning which is equivalent to the end of sequence performance. In Section 6, we show that our UCB model can be used when tasks labels are not available at inference time by training it with a “single head” architecture with a sum of number of classes for all tasks. We refer to the ACC measured for this scenario as “Generalized Accuracy”. 5.2 5-S PLIT MNIST We ﬁrst present our results for class incremental learning of MNIST (5-Split MNIST) in which we learn the digits 0 −9 in ﬁve tasks with 2 classes at a time in 5 pairs of 0/1, 2/3, 4/5, 6/7, and 8/9. Table 2a shows the results for reference baselines in Bayesian and non-Bayesian neural networks including ﬁne-tuning ( BBB-FT, ORD-FT), feature extraction ( BBB-FE, ORD-FE) and, joint training (BBB-JT, ORD-JT) averaged over 3 runs and standard deviations are given in Table 9 in the appendix. Although the MNIST dataset is an “easy” dataset, we observe throughout all experiments that Bayesian ﬁne-tuning and joint training perform signiﬁcantly better than their counterparts, ORD-FT and ORD-JT. For Bayesian methods, we compare against VCL and its variations named as VCL with Variational Adam (VCL-Vadam), VCL with Adam and Gaussian natural gradients (VCL-GNG). For non-Bayesian methods, we compare against HAT, IMM, and EWC (EWC can be regarded as Bayesian-inspired). VCL-Vadam (ACC= 99.17%) appears to be outperforming VCL (ACC=98.20%) and VCL-GNG (ACC=96.50%) in average accuracy. However, full comparison is not possible because forgetting was not reported for Vadam and GNG. Nevertheless, UCB (ACC=99.63%) is able to surpass all the baselines including VCL-Vadam in average accuracy while in zero forgetting it is on par with HAT (ACC=99.59%). We also report results on incrementally learning MNIST in two tasks (2-Split MNIST) in Table 8 in the appendix, where we compare it 8Published as a conference paper at ICLR 2020 against PackNet, HAT, and LWF where PackNet, HAT,UCB-P, and UCB have zero forgetting while UCB has marginally higher accuracy than all others. 5.3 P ERMUTED MNIST Permuted MNIST is a popular variant of the MNIST dataset to evaluate continual learning approaches in which each task is considered as a random permutation of the original MNIST pixels. Following the literature, we learn a sequence of 10 random permutations and report average accuracy at the end. Table 2b shows ACC and BWT of UCB and UCB-P in comparison to state-of-the-art models using a small and a large network with 0.1M and 1.9M parameters, respectively (architecture details are given in Section A.2 of the appendix). The accuracy achieved by UCB (ACC=91.44 ±0.04%) using the small network outperforms the ACC reported by Serra et al. (2018) for SI (ACC=86.0%), EWC (ACC=88.2%), while HAT attains a slightly better performance (ACC=91.6%). Comparing the average accuracy reported in VCL-Vadam (ACC=86.34%) and VCL-GNG (ACC=90.50%) as well as obtained results for VCL (ACC=88.80%) shows UCB with BWT=(0.03% ±0.00%) is able to outperform other Bayesian approaches in accuracy while forgetting signiﬁcantly less compared to VCL with BWT=−7.9%. While we do not experiment with memory in this work, not surprisingly adding memory to most approaches will improve their performance signiﬁcantly as it allows looking into past tasks. E.g. Chen et al. (2019) report ACC=94.37% for VCL-GNC when adding a memory of size 200. Next, we compare the results for the larger network (1.9M). While HAT andUCB have zero forgetting, UCB, reaching ACC=97.42±0.01%, performs better than all baselines including HAT which obtains ACC=97.34 ±0.05% using 1.9M parameters. We also observe again that BBB-FT, despite being not speciﬁcally penalized to prevent forgetting, exhibits reasonable negative BWT values, performing better than IMM and LWF baselines. It is close to joint training, BBB-JT, with ACC=98.1%, which can be seen as an upper bound. 5.4 A LTERNATING CIFAR10 AND CIFAR100 In this experiment, we randomly alternate between class incremental learning of CIFAR10 and CIFAR100. Both datasets are divided into 5 tasks each with 2 and 20 classes per task, respectively. Table 2c presents ACC and BWT obtained with UCB-P, UCB, and three BBB reference methods compared against various continual learning baselines. Among the baselines presented in Table 2c, PNN and PathNet are the only zero-forgetting-guaranteed approaches. It is interesting to note that in this setup, some baselines (PathNet, LWF, and LFL) do not perform better than the naive accuracy achieved by feature extraction. PathNet suffers from bad pre-assignment of the network’s capacity per task which causes poor performance on the initial task from which it never recovers. IMM performs almost similar to ﬁne-tuning in ACC, yet forgets more. PNN, EWC, and HAT are the only baselines that perform better than BBB-FE and BBB-FT. EWC and HAT are both allowed to forget by construction, however, HAT shows zero forgetting behavior. While EWC is outperformed by both of our UCB variants, HAT exhibits 1% better ACC over UCB-P. Despite having a slightly higher forgetting, the overall accuracy of UCB is higher, reaching 79.4%. BBB-JT in this experiment achieves a positive BWT which shows that learning the entire sequence improves the performance on earlier tasks. 5.5 M ULTIPLE DATASETS LEARNING Finally, we present our results for continual learning of 8 tasks using UCB-P and UCB in Table 2d. Similar to the previous experiments we look at both ACC and BWT obtained forUCB-P, UCB, BBB references (FT, FE, JT) as well as various baselines. Considering the ACC achieved by BBB-FE or BBB-FT (58.1%) as a lower bound we observe again that some baselines are not able to do better than BBB-FT including LFL, PathNet, LWF, IMM, and EWC while PNN and HAT remain the only strong baselines for our UCB-P and UCB approaches. UCB-P again outperforms PNN by 3.6% in ACC. HAT exhibits only−0.1% BWT, but our UCB achieves2.4% higher ACC. 6 S INGLE HEAD AND GENERALIZED ACCURACY OF UCB UCB can be used even if the task information is not given at test time. For this purpose, at training time, instead of using a separate fully connected classiﬁcation head for each task, we use a single 9Published as a conference paper at ICLR 2020 Table 3: Single Head vs. Multi-Head architecture and Generalized vs. Standard Accuracy. Generalized accuracy means that task information is not available at test time. SM, PM, CF, and 8T denote the 5-Split MNIST, Permuted MNIST, Alternating CIFAR10/100, and sequence of8 tasks, respectively. Generalized ACC ACC Single Head Single Head Multi Head Exp UCB BBB-FT UCB BBB-FT UCB BBB-FT SM 98.7 98 .1 98.9 98 .7 99.2 98 .4 PM 92.5 86 .1 95.1 88 .3 97.7 90 .0 CF 71.2 65 .2 74.3 67 .8 79.4 68 .9 8T 76.8 47 .6 79.9 53 .2 84.0 43 .1 head with the total number of outputs for all tasks. For example in the 8-dataset experiment we only use one head with 293 number of output classes, rather than using 8 separate heads, during training and inference time. Table 3 presents our results for UCB and BBB-FT trained with a single head against having a multi-head architecture, in columns 4-7. Interestingly, we see only a small performance degrade for UCB from training with multi-head to a single head. The ACC reduction is 0.3%, 2.6%, 5.1%, and 4.1% for 2-Split MNIST, Permuted MNIST, Alternating CIFAR10/100, and sequence of8 tasks experiments, respectively. We evaluatedUCB and BBB-FT with a more challenging metric where the prediction space covers the classes across all the tasks. Hence, confusion of similar class labels across tasks can be measured. Performance for this condition is reported as Generalized ACC in Table 3 in columns2-3. We observe a small performance reduction in going from ACC to Generalized ACC, suggesting non-signiﬁcant confusion caused by the presence of more number of classes at test time. The performance degradation from ACC to Generalized ACC is0.2%, 2.6%, 3.1%, and 3.1% for 2-Split MNIST, Permuted MNIST, Alternating CIFAR10/100, and sequence of 8 tasks, respectively. This shows that UCB can perform competitively in more realistic conditions such as unavailability of task information at test time. We believe the main insight of our approach is that instead of computing additional measurements of importance, which are often task, input or output dependent, we directly use predicted weight uncertainty to ﬁnd important parameters. We can freeze them using a binary mask, as in UCB-P, or regularize changes conditioned on current uncertainty, as in UCB. 7 C ONCLUSION In this work, we propose a continual learning formulation with Bayesian neural networks, called UCB, that uses uncertainty predictions to perform continual learning: important parameters can be either fully preserved through a saved binary mask (UCB-P) or allowed to change conditioned on their uncertainty for learning new tasks (UCB). We demonstrated how the probabilistic uncertainty distributions per weight are helpful to continually learning short and long sequences of benchmark datasets compared against baselines and prior work. We show that UCB performs superior or on par with state-of-the-art models such as HAT (Serra et al., 2018) across all the experiments. Choosing between the two UCB variants depends on the application scenario: While UCB-P enforces no forgetting after the initial pruning stage by saving a small binary mask per task, UCB does not require additional memory and allows for more learning ﬂexibility in the network by allowing small forgetting to occur. UCB can also be used in a single head setting where the right subset of classes belonging to the task is not known during inference leading to a competitive model that can be deployed where it is not possible to distinguish tasks in a continuous stream of the data at test time. UCB can also be deployed in a single head scenario and where tasks information is not available at test time. REFERENCES Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. Memory aware synapses: Learning what (not) to forget. InProceedings of the European Conference on Computer Vision (ECCV), pp. 139–154, 2018. 10Published as a conference paper at ICLR 2020 Rahaf Aljundi, Klaas Kelchtermans, and Tinne Tuytelaars. Task-free continual learning. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 11254–11263, 2019. David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. Journal of the American Statistical Association, 112(518):859–877, 2017. Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural network. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 1613–1622. PMLR, 2015. Yaroslav Bulatov. Notmnist dataset. Google (Books/OCR), Tech. Rep.[Online]. Available: http://yaroslavvb. blogspot. it/2011/09/notmnist-dataset. html, 2011. Francisco M Castro, Manuel J Mar´ın-Jim´enez, Nicol´as Guil, Cordelia Schmid, and Karteek Alahari. End-to-end incremental learning. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 233–248, 2018. Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efﬁcient lifelong learning with A-GEM. In International Conference on Learning Representations, 2019. Yu Chen, Tom Diethe, and Neil Lawrence. Facilitating bayesian continual learning by natural gradients and stein gradients. arXiv preprint arXiv:1904.10644, 2019. Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A Rusu, Alexander Pritzel, and Daan Wierstra. Pathnet: Evolution channels gradient descent in super neural networks. arXiv preprint arXiv:1701.08734, 2017. Alex Graves. Practical variational inference for neural networks. In Advances in neural information processing systems, pp. 2348–2356, 2011. Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015. W Keith Hastings. Monte carlo sampling methods using markov chains and their applications. Biometrika, 1970. Jos´e Miguel Hern´andez-Lobato and Ryan Adams. Probabilistic backpropagation for scalable learning of bayesian neural networks. In International Conference on Machine Learning, pp. 1861–1869, 2015. Geoffrey E Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing the description length of the weights. In Proceedings of the sixth annual conference on Computational learning theory, pp. 5–13. ACM, 1993. Heechul Jung, Jeongwoo Ju, Minju Jung, and Junmo Kim. Less-forgetting learning in deep neural networks. arXiv preprint arXiv:1607.00122, 2016. Mohammad Emtiyaz Khan and Didrik Nielsen. Fast yet simple natural-gradient descent for variational inference in complex models. In 2018 International Symposium on Information Theory and Its Applications (ISITA), pp. 31–35. IEEE, 2018. Mohammad Emtiyaz Khan, Didrik Nielsen, V oot Tangkaratt, Wu Lin, Yarin Gal, and Akash Srivas- tava. Fast and scalable bayesian deep learning by weight-perturbation in adam. arXiv preprint arXiv:1806.04854, 2018. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, pp. 201611835, 2017. 11Published as a conference paper at ICLR 2020 Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009. Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. Kibok Lee, Kimin Lee, Jinwoo Shin, and Honglak Lee. Overcoming catastrophic forgetting with unlabeled data in the wild. In Proceedings of the IEEE International Conference on Computer Vision, pp. 312–321, 2019. Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, and Byoung-Tak Zhang. Overcoming catastrophic forgetting by incremental moment matching. In Advances in Neural Information Processing Systems, pp. 4652–4662, 2017. Zhizhong Li and Derek Hoiem. Learning without forgetting. In European Conference on Computer Vision, pp. 614–629. Springer, 2016. Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learn- ing efﬁcient convolutional networks through network slimming. In Proceedings of the IEEE International Conference on Computer Vision, pp. 2736–2744, 2017. David Lopez-Paz et al. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems, pp. 6467–6476, 2017. David JC MacKay. A practical bayesian framework for backpropagation networks. Neural computa- tion, 4(3):448–472, 1992a. David JC MacKay. Bayesian methods for adaptive models. PhD thesis, California Institute of Technology, 1992b. Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative pruning. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. James L McClelland, Bruce L McNaughton, and Randall C O’reilly. Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory. Psychological review, 102(3):419, 1995. Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of learning and motivation, volume 24, pp. 109–165. Elsevier, 1989. Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional neural networks for resource efﬁcient inference. In International Conference on Learning Repre- sentations (ICLR), 2016. Radford M Neal. Bayesian learning for neural networks, volume 118. Springer Science & Business Media, 2012. Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning, 2011. Hong-Wei Ng and Stefan Winkler. A data-driven approach to cleaning large face datasets. In Image Processing (ICIP), 2014 IEEE International Conference on, pp. 343–347. IEEE, 2014. Cuong V . Nguyen, Yingzhen Li, Thang D. Bui, and Richard E. Turner. Variational continual learning. In International Conference on Learning Representations, 2018. Christian Robert and George Casella. Monte Carlo statistical methods. Springer Science & Business Media, 2013. Anthony Robins. Catastrophic forgetting, rehearsal and pseudorehearsal. Connection Science, 7(2): 123–146, 1995. 12Published as a conference paper at ICLR 2020 Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016. Nicol N Schraudolph. Fast curvature matrix-vector products for second-order gradient descent. Neural computation, 14(7), 2002. Jonathan Schwarz, Jelena Luketina, Wojciech M Czarnecki, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable frame- work for continual learning. arXiv preprint arXiv:1805.06370, 2018. Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 4548–4557. PMLR, 2018. Kumar Shridhar, Felix Laumann, and Marcus Liwicki. Uncertainty estimations by softplus nor- malization in bayesian convolutional neural networks with variational inference. arXiv preprint arXiv:1806.05978, 2018. Rupesh K Srivastava, Jonathan Masci, Sohrob Kazerounian, Faustino Gomez, and J¨urgen Schmidhu- ber. Compete to compute. In Advances in neural information processing systems, pp. 2310–2318, 2013. Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The german trafﬁc sign recognition benchmark: a multi-class classiﬁcation competition. In Neural Networks (IJCNN), The 2011 International Joint Conference on, pp. 1453–1460. IEEE, 2011. Hanna Tseran, Mohammad Emtiyaz Khan, Tatsuya Harada, and Thang D Bui. Natural variational continual learning. In Continual Learning Workshop@ NeurIPS, volume 2, 2018. Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, and Yun Fu. Large scale incremental learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 374–382, 2019. Han Xiao, Kashif Rasul, and Roland V ollgraf. Fashion-mnist: a novel image dataset for bench- marking machine learning algorithms, The MIT License (MIT) Copyright c⃝2017 Zalando SE. https://tech.zalando.com, arXiv preprint arXiv:1708.07747, 2017. Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with dynamically expandable networks. In International Conference on Learning Representations, 2018. Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In Doina Precup and Yee Whye Teh (eds.),Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 3987–3995. PMLR, 2017. 13Published as a conference paper at ICLR 2020 A A PPENDIX A.1 D ATASETS Table 4 shows a summary of the datasets utilized in our work along with their size and number of classes. In all the experiments we resized images to 32 ×32 ×3 if necessary. For datasets with monochromatic images, we replicate the image across all RGB channels. Table 4: Utilized datasets summary Names #Classes Train Test FaceScrub (Ng & Winkler, 2014) 100 20,600 2,289 MNIST (LeCun et al., 1998) 10 60,000 10,000 CIFAR100 (Krizhevsky & Hinton, 2009) 100 50,000 10,000 NotMNIST (Bulatov, 2011) 10 16,853 1,873 SVHN (Netzer et al., 2011) 10 73,257 26,032 CIFAR10 (Krizhevsky & Hinton, 2009) 10 39,209 12,630 TrafﬁcSigns (Stallkamp et al., 2011) 43 39,209 12,630 FashionMNIST (Xiao et al., 2017) 10 60,000 10,000 A.2 I MPLEMENTATION DETAILS In this section we take a closer look at elements of our UCB model on MNIST and evaluate variants of parameter regularization, importance measurement, as well as the effect of the number of samples drawn from the posited posterior. Bayes-by-backprop (BBB) Hyperparamters:Table 5 shows the search space for hyperparamters in the BBB algorithm Blundell et al. (2015) which we used for tuning on the validation set of the ﬁrst two tasks. Table 5: Search space for hyperparamters in BBB given by Blundell et al. (2015) BBB hyperparamters −log σ1 −log σ2 π Search space {0,1,2} {6,7,8} {0.25,0.5,0.75} Network architecture:For Split MNIST and Permuted MNIST experiments, we have used a two- layer perceptron which has 1200 units. Because there is more number of parameters in our Bayesian neural network compared to its equivalent regular neural net, we ensured fair comparison by matching the total number of parameters between the two to be 1.9M unless otherwise is stated. For the multiple datasets learning scenario, as well as alternating incremental CIFAR10/100 datasets, we have used a ResNet18 Bayesian neural network with 7.1-11.3M parameters depending on the experiment. However, the majority of the baselines provided in this work are originally developed using some variants of AlexNet structure and altering that, e.g. to ResNet18, resulted in degrading in their reported and experimented performance as shown in Table 6. Therefore, we kept the architecture for baselines as AlexNet and ours as ResNet18 and only matched their number of parameters to ensure having equal capacity across different approaches. Table 6: Continually learning on CIFAR10/100 using AlexNet and ResNet18 for UCB (our method) and HAT (Serra et al., 2018). BWT and ACC in %. All results are (re)produced by us. Method BWT ACC HAT (AlexNet) 0.0 78 .3 HAT (ResNet18) −9.0 56 .8 UCB (AlexNet) −0.7 79 .44 UCB (ResNet18) −0.7 79 .70 14Published as a conference paper at ICLR 2020 Number of Monte Carlo samples:UCB is ensured to be robust to random noise using multiple samples drawn from posteriors. Here we explore different number of samples and the effect on ﬁnal performance for ACC and BWT. We have usedΩµ = 1/σas importance and regularization has been performed on mean values only. Following the result in Table 7 we chose the number of samples to be 10 for all experiments. Table 7: Number of Monte Carlo samples (N) in 2-Split MNIST Method N BWT (%) ACC (%) UCB 1 0 .00 98 .0 UCB 2 0 .00 98 .3 UCB 5 −0.15 99 .0 UCB 10 0 .00 99 .2 UCB 15 −0.01 98 .3 A.3 A DDITIONAL RESULTS Here we include some additional results such as Table 8 for 2-split MNIST and some complementary results for tables in the main text as follows: 9, 10, and 11 include standard deviation for results shown in Table 2a, 2b, 2c, respectively. Table 8: Continually learning on 2-Split MNIST. BWT and ACC in %. (*) denotes that methods do not adhere to the continual learning setup: BBB-JT and ORD-JT serve as the upper bound for ACC for BBB/ORD networks, respectively. All results are (re)produced by us. Method BWT ACC PackNet (Mallya & Lazebnik, 2018) 0.04 ±0.01 98 .91 ±0.03 LWF (Li & Hoiem, 2016) −0.22 ±0.04 99 .12 ±0.03 HAT (Serra et al., 2018) 0.01 ±0.00 99 .02 ±0.00 ORD-FT −6.81 ±0.03 92 .42 ±0.02 ORD-FE 0.04 ±0.04 97 .90 ±0.04 BBB-FT −0.61 ±0.03 98 .44 ±0.03 BBB-FE 0.02 ±0.05 98 .03 ±0.05 UCB-P (Ours) 0.03 ±0.04 99 .02 ±0.01 UCB (Ours) 0.01 ±0.00 99.18 ±0.01 ORD-JT∗ 0.02 ±0.03 99 .13 ±0.03 BBB-JT∗ 0.03 ±0.02 99 .51 ±0.02 Table 9: Continually learning on 5-Split MNIST. BWT and ACC in %. (*) denotes that methods do not adhere to the continual learning setup: BBB-JT and ORD-JT serve as the upper bound for ACC for BBB/ORD networks, respectively. All results are (re)produced by us. Method BWT ACC VCL-Vadam (Tseran et al., 2018) - 99.17 ±0.05 VCL-GNG (Chen et al., 2019) - 96.50 ±0.07 VCL (Nguyen et al., 2018) - 0.56 ±0.03 98 .20 ±0.03 IMM (Lee et al., 2017) - 11.20 ±1.57 88 .54 ±1.56 EWC (Kirkpatrick et al., 2017) - 4.20 ±1.08 95 .78 ±1.08 HAT (Serra et al., 2018) 0.00 ±0.02 99 .59 ±0.02 ORD-FT∗ -9.18 ±1.12 90 .60 ±1.12 ORD-FE∗ 0.00 ±1.56 98 .54 ±1.57 BBB-FT∗ -6.45 ±1.99 93 .42 ±1.98 BBB-FE∗ 0.00 ±2.23 98 .76 ±2.23 UCB-P (Ours) - 0.72 ±0.04 99 .32 ±0.04 UCB (Ours) 0.00 ±0.04 99.63 ±0.03 ORD-JT∗ 0.00 ±0.02 99 .78 ±0.02 BBB-JT∗ 0.00 ±0.01 99 .87 ±0.01 15Published as a conference paper at ICLR 2020 Table 10: Continually learning on Permuted MNIST. BWT and ACC in %. (*) denotes that method does not adhere to the continual learning setup: BBB-JT serves as the upper bound for ACC for BBB network. ‡denotes results reported by (Serra et al., 2018). †denotes the result reported from original work. BWT was not reported in ‡and †. All others results are (re)produced by us. Method #Params BWT ACC SI (Zenke et al., 2017)‡ 0.1M - 86.0 EWC (Kirkpatrick et al., 2017)‡ 0.1M - 88.2 HAT (Serra et al., 2018)‡ 0.1M - 91.6 VCL-Vadam† 0.1M - 93.34 VCL-GNG† 0.1M - 94.62 VCL 0.1M −7.90 ±0.23 88 .80 ±0.23 UCB (Ours) 0.1M −0.38 ±0.02 91 .44 ±0.04 LWF (Li & Hoiem, 2016) 1.9M −31.17 ±0.05 65 .65 ±0.05 IMM (Lee et al., 2017) 1.9M −7.14 ±0.07 90 .51 ±0.08 HAT (Serra et al., 2018) 1.9M 0 .03 ±0.05 97 .34 ±0.05 BBB-FT 1.9M −0.58 ±0.05 90 .01 ±0.05 BBB-FE 1.9M 0 .02 ±0.03 93 .54 ±0.04 UCB-P (Ours) 1.9M −0.95 ±0.06 97 .24 ±0.06 UCB (Ours) 1.9M 0 .03 ±0.00 97.42 ±0.01 BBB-JT∗ 1.9M 0 .00 ±0.00 98 .12 ±0.01 Table 11: Continually learning on CIFAR10/100. BWT and ACC in %. (*) denotes that method does not adhere to the continual learning setup: BBB-JT serves as the upper bound for ACC for BBB network. All results are (re)produced by us. Method BWT ACC PathNet (Fernando et al., 2017) 0.00 ±0.00 28 .94 ±0.03 LWF (Li & Hoiem, 2016) −37.9 ±0.32 42 .93 ±0.30 LFL (Jung et al., 2016) −24.22 ±0.21 47 .67 ±0.22 IMM (Lee et al., 2017) −12.23 ±0.06 69 .37 ±0.06 PNN (Rusu et al., 2016) 0.00 ±0.00 70 .73 ±0.08 EWC (Kirkpatrick et al., 2017) −1.53 ±0.07 72 .46 ±0.06 HAT (Serra et al., 2018) 0.04 ±0.06 78 .32 ±0.06 BBB-FE 0.04 ±0.02 51 .04 ±0.03 BBB-FT −7.43 ±0.07 68 .89 ±0.07 UCB-P (Ours) −1.89 ±0.03 77 .32 ±0.03 UCB (Ours) −0.72 ±0.02 79.44 ±0.02 BBB-JT∗ 1.52 ±0.04 83 .93 ±0.04 16",
      "meta_data": {
        "arxiv_id": "1906.02425v2",
        "authors": [
          "Sayna Ebrahimi",
          "Mohamed Elhoseiny",
          "Trevor Darrell",
          "Marcus Rohrbach"
        ],
        "published_date": "2019-06-06T05:40:25Z",
        "pdf_url": "https://arxiv.org/pdf/1906.02425v2.pdf"
      }
    },
    {
      "title": "Lifelong Domain Adaptation via Consolidated Internal Distribution"
    }
  ]
}