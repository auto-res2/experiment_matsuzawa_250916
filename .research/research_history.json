{
  "research_topic": "KVキャッシュ量子化 × 精度保持を改善したい（2bit域）",
  "queries": [
    "KV-cache quantization",
    "2-bit KV quantization",
    "accuracy preservation quantization",
    "post-training 2-bit quantization",
    "mixed-precision KV cache"
  ],
  "research_study_list": [
    {
      "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization",
      "abstract": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model.",
      "full_text": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization Coleman Hooper1 Sehoon Kim1 Hiva Mohammadzadeh1 Michael W. Mahoney1,2,3 Yakun Sophia Shao1 Kurt Keutzer1 Amir Gholami1,2 1University of California, Berkeley 2ICSI 3LBNL {chooper, sehoonkim, hiva, mahoneymw, ysshao, keutzer, amirgh}@berkeley.edu Abstract LLMs are seeing growing use for applications which require large context windows, and with these large context windows KV cache activations surface as the dominant contributor to memory consumption during inference. Quantization is a promising approach for compressing KV cache activations; however, existing solutions fail to represent activations accurately in sub-4-bit precision. Our work, KVQuant, facili- tates low precision KV cache quantization by incorporating several novel methods: (i) Per-Channel Key Quantization, where we adjust the dimension along which we quantize the Key activations to better match the distribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations before the rotary positional em- bedding to mitigate its impact on quantization; (iii) Non-Uniform KV Cache Quan- tization, where we derive per-layer sensitivity-weighted non-uniform datatypes that better represent the distributions; and (iv) Per-Vector Dense-and-Sparse Quanti- zation, where we isolate outliers separately for each vector to minimize skews in quantization ranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral models, we achieve < 0.1 perplexity degradation with 3-bit quantiza- tion on both Wikitext-2 and C4, outperforming existing approaches. Our method enables serving LLaMA-7B with a context length of up to 1 million on a single A100-80GB GPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for KVQuant, showing that we can achieve up to ∼1.7× speedups, compared to baseline fp16 matrix-vector multiplications, for the LLaMA-7B model. Code is available at https://github.com/SqueezeAILab/KVQuant. 1 Introduction Large language models (LLMs) have revolutionized many natural language processing (NLP) tasks. In order to improve the capabilities of LLMs, there is significant interest in increasing the context lengths of LLMs. Longer context lengths enable new applications, including long document summarization, retrieval for answering questions about long documents, extended multi-turn applications [6], and code analysis. To support this pull from applications, there have been significant recent advances in long-context length models in industry [2, 30], as well as in academia [6, 22]. Given the importance of LLM workloads, there is strong motivation to improve their inference efficiency. LLM inference with large context lengths can be incredibly resource-intensive; serving LLMs requires high-end GPUs, and the largest LLMs require costly multi-GPU inference setups. When analyzing the computational nature of generative inference with LLMs, it becomes quickly apparent that, for relatively small batch sizes, the computation is memory bound [ 17]. With the growing divergence between computational speeds and memory speeds, this problem is only going to get worse over time [12]. This makes reducing the memory bottleneck preeminently important. Further analysis shows that the memory bottleneck is strongly related to context size. For short 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2401.18079v6  [cs.LG]  28 May 202516% 84%98% 2% Long Sequence LengthsKV Cache is the bottleneckShort sequence length Weightsare the bottleneck BaselineKVQuant int3 +Grouping 10.87 Pre-RoPEKeyQuantizationNon-UniformQuantization5.755.94 1%Outliers6.23 Per-Channel KeyQuantization7.05 Perplexity on Wikitext2fp16 Baseline Figure 1: Left: Model size versus activation memory size for the LLaMA-7B model with sequence length 512 and 128K. For longer context lengths, the KV cache becomes the dominant memory bottleneck. Memory consumption of model weights and KV cache activations for different LLaMA models with different sequence lengths are provided in Table 7 in Appendix A. Right: Overview of the different components used in KVQuant that result in less than 0.1 perplexity degradation over the fp16 baseline when quantizing the KV cache for the LLaMA-7B model to 3-bit precision. As shown in Table 1, our 3-bit approach results in4.8× reduction in cached activation memory footprint. sequence lengths, the dominant contributor to memory consumption is the weight matrices, and therefore the optimal strategy is to minimize the model size in order to reduce memory consumption as well as bandwidth requirements [ 18, 17]. However, as shown in Figure 1, the main bottleneck for long sequence lengths is the memory requirements for caching Key and Value (KV) activations throughout inference. This challenge is further exacerbated when one considers batched inference. It is therefore crucial to develop methods for compressing the KV cache to enable efficient long- sequence length inference. Existing approaches lead to unacceptable accuracy degradation due to the outlier structures in KV cache activations as well as suboptimal bit allocation with existing uniform and non-uniform approaches. In this work, we perform an extensive analysis of KV cache activations in recent LLMs, revealing patterns which can be exploited to enable ultra-low precision quantization with minimal accuracy loss. In particular, we make the following contributions (summarized in Figure 1): • We find that the Keys exhibit outliers in specific channels before applying RoPE. However, the outlier channel magnitudes become less consistent after applying RoPE, posing a distinct challenge for low precision quantization. We address this by quantizing Keys per-channel before RoPE is applied (see Section 3.1 and Section 3.2). • We find that existing uniform and non-uniform quantization methods result in sub-optimal quanti- zation signpost placement. Instead, we propose a Non-Uniform Quantization (NUQ) method which considers sensitivity and not just magnitude when quantizing activations. We show that one can apply sensitivity-weighted non-uniform quantization offline on a calibration set to derive accurate datatypes for KV cache quantization (see Section 3.3). • Even with the above, we find that outlier values in cached KV activations can significantly de- grade quantization resolution. Unlike for weights, it is non-trivial to extract outlier values from activations, given the dynamic nature of activations. However, we find that we can efficiently and accurately identify and compress outlier values in order to store them compactly in a separate sparse representation. We also find that per-vector outlier detection outperforms per-matrix outlier detection with no additional memory overhead. By removing only 1% of outliers, we can attain under 0.1 perplexity degradation on both Wikitext-2 and C4 for 3-bit KV cache quantization with the LLaMA, Llama-2, Llama-3, and Mistral models, thereby facilitating accurate inference with 4.8× longer context length. • We implement custom CUDA kernels to perform activation quantization efficiently during inference, achieving up to ∼1.7× speedups for Key and Value matrix-vector multiplications for LLaMA-7B at 4-bit precision relative to the fp16 baseline (see Section 3.7 and 4.4). These results demonstrate how our methodology allows for accurate and efficient low-bit KV cache quantization. 22 Background 2.1 LLM Inference When inferring a decoder-only LLM, inference proceeds in two distinct phases. In the prefill phase, the model takes in an input prompt, which it processes in parallel. During the generation phase, the model then generates the output sequence autoregressively, meaning that each token generation is dependent on all previously generated tokens. As such, for small batch sizes, the generation phase of LLM inference is typically memory-bandwidth bound, as the only available parallelism is across different sequences in a given batch. Additionally, during generation, the model needs to store intermediate Key and Value activations at each layer in order to condition generations on previously generated output tokens. Otherwise, we would need to recompute all prior Keys and Values at each timestep, which would be prohibitively expensive. These stored activations are referred to as the Key-Value (KV) cache. Throughout this paper, we will capitalize Key and Value to distinguish when we are referring to the KV cache tensors. Assuming a model with n layers and h attention heads with dimension d that is stored using e bytes per element, the KV cache size for batch size b and sequence length l is 2 · n · h · d · e · b · l, meaning that it grows linearly with both batch size and sequence length. As shown in Table 7, the KV cache becomes the dominant contributor to memory consumption for longer sequence lengths and larger batch sizes. Note that since each sequence in batched inference depends on separate past context, there is no available batch-level parallelism when loading the cached Keys and Values for their respective computations in batched inference. KV cache loading is therefore always memory-bandwidth bound. This motivates pursuing methods to optimally compress the KV cache, even at the expense of a more complex dequantization process. 2.2 LLM Quantization There have been many prior works on LLM quantization. Several have focused on weight-only quantization for LLMs, due to the greater contribution to memory consumption and runtime for small sequence lengths and batch sizes [21, 9, 17]. Prior works have noted the presence of distinct outliers in both weights and activations [7, 9, 17]. One approach that has been developed to address this outlier issue in the context of weight quantization is dense-and-sparse quantization, where each weight matrix is decomposed into a sparse outlier matrix and a dense low-precision matrix [9, 17]. Prior works have also leveraged non-uniform quantization methods to improve accuracy for the same bit precision by allowing for flexible quantization signpost placement [17, 8]. These approaches have either used a fixed non-uniform datatype such as NormalFloat [8], or derived quantization signposts using a sensitivity-weighted k-means approach [17]. Appendix B provides a more detailed overview of related work for outlier-aware LLM quantization and non-uniform LLM quantization. There has also been work on quantizing both weights and activations (including KV cache) [41, 33]. However, there is still a significant perplexity degradation when quantizing KV cache activations to low precision; [34, 44] quantized KV cache activations to 4-bits, but required fine-grained grouping for 4-bit quantization, while still observing some perplexity degradation, and [34] observed that 3-bit KV cache quantization with fine-grained grouping leads to unacceptable accuracy loss. Other works quantized KV cache activations to 4-bits but required retraining to maintain performance [24]. One concurrent work also explores low precision KV cache quantization in order to enable larger batch size inference by reducing the KV cache size [26]. 2.3 KV Cache Compression There have also been several prior works on compressing the KV cache. Some of these methods aim to only store important tokens in the KV cache and to evict less important tokens, thereby maintaining low memory usage [25, 43, 11, 20]. Other methods aim to only retrieve a subset of tokens at each step to achieve memory bandwidth savings [32]. In this work, we explore KV cache quantization as an orthogonal direction for compressing the KV cache in order to enable long context inference. 3Channel 0 10002000 3000 4000 T oken 0 500 1000 1500 2000 Magnitude 0.0 2.5 5.0 7.5 10.0 12.5 15.0 Layer 10 Keys (pre-RoPE) Channel 0 10002000 3000 4000 T oken 0 500 1000 1500 2000 Magnitude 0.0 2.5 5.0 7.5 10.0 12.5 15.0 Layer 10 Keys (post-RoPE) Channel 0 10002000 3000 4000 T oken 0 500 1000 1500 2000 Magnitude 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Layer 10 Values Figure 2: Example distributions of the activation values for Keys pre-RoPE, Keys post-RoPE, and Values for LLaMA-7B on a sample with 2K sequence length from Wikitext-2. We observe several patterns: (i) Keys pre-RoPE exhibit clear outliers in specific channels across different tokens; (ii) after applying RoPE, the distribution becomes less structured and there are less consistent magnitudes for outlier channels (this is expected, as RoPE applies a rotation operation between pairs of channels); and (iii) Values exhibit no fixed outlier pattern with outlier values across channels and tokens. 3 Method 3.1 Per-Channel Key Quantization To inform our approach, we first performed a detailed analysis to understand the KV cache distribu- tions. Figure 2 shows sample distributions for the KV cache activations. We observe that the Key matrices tend to have distinct outlier channels, which have larger average magnitudes than other channels; this corroborates previous observations about outlier channels in LLM activations [7, 41]. The Value matrices exhibit both outlier channels as well as outlier tokens (although these outliers are less extreme than the outlier Key channels). Existing KV cache quantization approaches perform per-token quantization (meaning that the scaling factor and zero-point are shared by elements in the same token) [ 34, 44]. However, due to the differing average magnitudes between channels, the values within a channel are easier to quantize when grouped together than the values across different channels. As such, to better match the distributions, we investigate per-channel KV cache quantization, meaning that the scaling factor and zero-point are shared by elements in the same channel. By sharing the scaling factor and zero-point along the channel dimension, this will naturally group together values with similar magnitudes, thereby mitigating the impacts of outlier channels on other channels when quantizing to low precision. As outlined in Appendix G, we find that per-channel quantization provides significant accuracy benefits for Keys but not for Values. By leveraging per-channel quantization for Keys and per- token quantization for Values, we observe a 3.82 perplexity improvement on Wikitext-2 for 3-bit LLaMA-7B quantization. Note that this can potentially add runtime overhead since the quantization dimension is now misaligned with the reduction dimension for the Keys when performing matrix- vector multiplications. However, we find that we are able to efficiently dequantize Keys and perform the Query-Key matrix-vector multiplication without adding runtime overhead, as shown in Section 4.4. Additionally, as outlined in Section 3.6, per-channel quantization can also be challenging due to the need to recompute scaling factors as tokens are added to the Key cache. We show that we can calibrate offline for scaling factors, thereby avoiding expensive online recomputation. Per-channel Key quantization was also explored in another concurrent work [26], which leveraged similar intuition about grouping together large magnitude values in the same channel to minimize quantization error. Their methodology requires fine-grained grouping for per-channel quantization while maintaining a residual subset of the KV cache in fp16. In our work, we demonstrate that by leveraging offline calibration, we can accurately perform per-channel quantization without grouping. 3.2 Pre-RoPE Key Quantization One issue when quantizing Keys is handling the rotary positional embedding (RoPE), which is applied to Keys and Queries in most public LLMs, including LLaMA and Llama-2 [35]. Given Query and Key vectors Qm = Wq ∗ xm and Kn = Wk ∗ xn at positions m and n in the sequence, RoPE is applied as position-dependent rotations to each of these vectors to obtain ˜Qm = Rd θ,m · Qm and 4˜Kn = Rd θ,n · Kn. This embeds the relative position between a Query and Key vector as an amount of an angle that is a multiple of its position index. When caching Key vectors, we therefore need to either cache ˜Kn, or else we need to cache Kn and apply Rd θ,n on-the-fly during inference. The challenge with caching Key vectors after applying this rotation is that it leads to mixing pairs of channels by different amounts for different positions in the sequence, as shown in Appendix C (since it jointly rotates pairs of channels by different angles depending on the position in the sequence). The post-RoPE activation distribution is also shown in Figure 2, demonstrating how the rotation between pairs of channels leads to less consistent channel magnitudes. This makes it harder to quantize Key activation channels which would typically have consistent large-magnitude values. This motivated our investigation into whether we could perform pre-RoPE Key quantization (meaning that we quantize Kn) and then efficiently apply the positional embeddings on-the-fly after dequantization. The benefits of pre-RoPE Key quantization are highlighted in Appendix H, yielding 0.82 perplexity improvement on Wikitext-2 for 3-bit LLaMA-7B quantization. To be able to quantize Keys pre-RoPE, we develop a fused kernel to efficiently apply RoPE post-dequantization (the details of this approach will be discussed in Section 3.7). 3.3 nuqX: An X-Bit Per-Layer Sensitivity-Weighted Non-Uniform Datatype Uniform quantization is suboptimal for KV cache quantization since the Query and Key activations are non-uniform. Additionally, KV cache loading is memory bandwidth bound, regardless of batch size or sequence length, meaning that the dequantization overhead introduced by non-uniform quantization methods is not problematic (since the added computation does not introduce any additional latency). It is therefore desirable to leverage non-uniform quantization methods for KV cache quantization. In [17], the authors computed non-uniform quantization signposts using a sensitivity-weighted k-means approach. However, this cannot be applied directly to KV cache quantization as the Values are quantized dynamically at runtime, which means that we would need to apply K-means online during inference, and it is also difficult to estimate sensitivity for activations online. We therefore facilitate efficient online non-uniform KV cache quantization by deriving a per-tensor non-uniform datatype offline on calibration data, which is then rescaled per-channel or per-token to accurately represent the key and value distributions. We compute sensitivity-weighted quantization signposts offline on a calibration set prior to inference, while maintaining compatibility with per-vector quantization by separately normalizing each channel to the range [−1, 1] prior to deriving the shared datatype. Using the diagonal Fisher information matrix (derived in Appendix D), along with the quantization error for activation A, we formulate the error minimization objective as follows, where A is flattened to one dimension and where N is the number of elements from all of the samples in our calibration set: Q(A)∗ ≃ arg min Q NX i=1 Fii \u0000 Ai − Q(Ai) \u00012 . (1) We modify the objective in Equation 1 as described in Appendix E in order to apply it using the normalized activation values. We then minimize it offline on a calibration set using a k-means solver in order to obtain the quantization signposts for the non-uniform datatype for each Key or Value layer. Appendix I compares our non-uniform quantization approach with existing uniform and non-uniform quantization baselines [8], demonstrating how our non-uniform approach provides 0.29 perplexity improvement on Wikitext-2 for LLaMA-7B relative to 3-bit uniform methods. Table 16 in Appendix L shows how computing the required Fisher information for the LLaMA-65B model takes only a few minutes, and how using the k-means solver takes only a few minutes per layer (with the computation for each layer being parallelizable). 3.4 Per-Vector Dense-and-Sparse Quantization As shown in Figure 4 in Appendix F, for both Keys and Values, the majority of elements are contained within a small percentage of the dynamic range. This means that by leveraging dense-and-sparse quantization, as demonstrated in [17], in order to isolate a small percentage of numerical outliers, we can restrict the range that we need to represent, thereby allowing us to represent the remaining elements with greater precision. However, when looking at the Key and Value distributions in Figure 2, different channels and tokens have different average magnitudes. Therefore, an element which counts as an outlier in one channel may not be an outlier in another channel (since that channel 5may have a greater average magnitude), making naive application of dense-and-sparse quantization suboptimal. It is therefore crucial to directly target the outlier values that skew the dynamic range at the granularity that we are quantizing in order to address the values that are exaggerating the range along that particular dimension. In this work, we leverage per-vector dense-and-sparse quantization, where we use a different outlier threshold per-vector (either a separate threshold per-channel for per-channel quantization, or a separate threshold per-token for per-token quantization), rather than a single outlier threshold for each layer. Note that computing outlier thresholds for per-vector dense-and-sparse quantization poses potential accuracy and efficiency challenges. However, in Section 3.6, we show that we are able to accurately calibrate for per-channel outlier thresholds offline and efficiently compute per-token outlier thresholds online. After determining the upper and lower outlier thresholds, the remaining numbers in the vector are normalized to the range [−1, 1], and we then minimize Equation 1 (ignoring outliers) in order to obtain the quantization signposts for the non-uniform datatype for the remaining numbers. Appendix J will demonstrate the benefits of removing a small percentage of numerical outliers and keeping them in full precision, as well as the advantages of per-vector dense-and-sparse quantization over using a single global outlier threshold for each layer. As shown in Figure 1, by removing 1% of numerical outliers using per-vector outlier thresholds, we achieve an additional 0.19 perplexity improvement on Wikitext-2 for 3-bit LLaMA-7B quantization, which is within 0.07 perplexity of the fp16 baseline. 3.5 Attention Sink-Aware Quantization Prior work has demonstrated that after the first few layers in LLMs, the model tends to allocate a large attention score to the first token [42]. This occurs even when the initial token is not semantically important. This phenomenon happens because the model tends to use the inital token as a “sink”. In our work, we demonstrate that due to the Attention Sink phenomenon, the model is disproportionately sensitive to quantization error in the first token. By keeping only the first token in fp16, we can attain perplexity benefits, particularly for 2-bit quantization. A similar observation has also been made in another concurrent work [ 23]. Note that when retaining the first token in fp16, we account for this during the calibration process as well, meaning that we ignore the first token when deriving the nuqX datatype and when calibrating the scaling factors and zero points offline for the Keys. As demonstrated in Appendix K, this approach persistently yields performance benefits, particularly with lower bit widths and without dense-and-sparse quantization. 3.6 Offline Calibration versus Online Computation A crucial challenge for activation quantization is that we either need to compute statistics on-the-fly (which is potentially expensive) or else we need to use offline calibration data (which potentially has negative accuracy implications). The challenges with computing scaling factors (and zero-points) online versus offline for both Keys and Values are shown in Figure 5 in Appendix L. In per-channel quantization, it is challenging to update scaling factors online since the scaling factors corresponding to each incoming channel would potentially need to be updated whenever a new token is added to the KV cache. It is therefore desirable to be able to compute statistics offline (i.e., using calibration data before running inference). While this can have negative effects on model accuracy, in Appendix L we show that we can effectively calibrate offline for per-channel quantization, obviating the need for online updates of scaling factors for per-channel quantization. For per-token quantization, it is challenging to calibrate for scaling factors offline due to the presence of outlier Value tokens. It is therefore desirable to be able to compute scaling factors and outlier thresholds online for each incoming token. As shown in Appendix L, we can efficiently compute outlier thresholds online per-token by offloading to the CPU. By leveraging custom quantization function implementations for compressing activations, we are able to perform online per-token Value quantization without compromising on performance. 3.7 Kernel Implementation In order to efficiently perform activation quantization on-the-fly, we leverage dedicated kernel implementations with our 4-bit quantization method for compressing vectors to reduced precision and extracting the sparse outliers, performing matrix-vector multiplications using the compressed 6Table 1: Evaluation of our method for different models using the perplexity (PPL) on Wikitext-2. KVQuant results are using pre-RoPE per-channel quantization for Keys. KV cache sizes are estimated assuming a sequence length of 128K (ignoring context length limits for the models). Note that ATOM and FlexGen use 4-bit quantization with group sizes of 128 and 64 with uniform quantization, respectively, and we extend their methods to 3-bit and 2-bit quantization. We leverage Attention Sink-Aware quantization for all bit widths. We used post-RoPE quantization for all baseline methods since it achieves higher accuracy when quantizing Keys per-token as shown in Appendix P. Table 18 in Appendix O demonstrates a full evaluation on all LLaMA, Llama-2, Llama-3, and Mistral models. Method LLaMA-7B LLaMA-13B LLaMA-30B LLaMA-65B PPL KV Cache (GB) PPL KV Cache (GB) PPL KV Cache (GB) PPL KV Cache (GB) baseline 5.68 64.0 5.09 100.0 4.10 195.0 3.53 320.0 int4 5.98 16.0 5.32 25.0 4.34 48.8 3.73 80.1 nf4 5.87 16.0 5.23 25.0 4.25 48.8 3.63 80.1 ATOM-4bit 5.77 16.6 5.16 26.0 4.16 50.7 3.57 83.1 FlexGen-4bit 5.73 17.3 5.14 27.0 4.14 52.6 3.56 86.3 KVQuant-4bit 5.72 16.0 5.13 25.0 4.13 48.8 3.55 80.0 KVQuant-4bit-1% 5.69 17.3 5.10 27.0 4.11 52.7 3.54 86.5 int3 10.87 12.0 8.69 18.8 6.82 36.6 6.37 60.1 nf3 7.33 12.0 6.21 18.8 5.46 36.6 4.44 60.1 ATOM-3bit 6.17 12.6 5.47 19.7 4.44 38.4 3.78 63.0 FlexGen-3bit 5.93 13.2 5.29 20.6 4.26 40.2 3.66 65.9 KVQuant-3bit 5.87 12.0 5.25 18.8 4.25 36.6 3.63 60.0 KVQuant-3bit-1% 5.75 13.3 5.14 20.8 4.15 40.5 3.57 66.5 int2 11779 8.0 69965 12.5 1470 24.4 7272 40.1 nf2 3210 8.0 5786 12.5 2044 24.4 5367 40.1 ATOM-2bit 37.37 8.6 41.77 13.4 16.49 26.1 13.63 42.8 FlexGen-2bit 11.09 9.1 9.84 14.3 6.60 27.8 5.54 45.6 KVQuant-2bit 7.23 8.0 5.82 12.5 4.87 24.4 4.03 40.0 KVQuant-2bit-1% 6.01 9.3 5.36 14.5 4.35 28.3 3.70 46.5 vectors, and performing sparse matrix-dense vector multiplications using the sparse outliers. We store the quantized Key and Value matrices as 4-bit elements which are used as indices into lookup tables to recover the corresponding fp16 values. We store the sparse outlier matrices in either Compressed- Sparse Row (CSR) or Compressed-Sparse Column (CSC) format (depending on which aligns better with appending new Key and Value tokens). The kernels for the Key matrix-vector operations apply RoPE on-the-fly in order to support pre-RoPE quantization. More kernel implementation details are provided in Appendix R. 4 Results 4.1 Main Evaluation We used the LLaMA-7B/13B/30B/65B, Llama-2-7B/13B/70B, Llama-3-8B/70B, and Mistral-7B models to evaluate our methodology by measuring perplexity on both Wikitext-2 and C4 [36, 37, 1, 16, 27, 31]. Perplexity has been measured using teacher forcing with the output logits of all input tokens. We compared our method against (i) uniform quantization without grouping (intX), (ii) nonuniform quantization using NormalFloat [8] without grouping (nfX), as well as (iii) Atom [44] and FlexGen [34]. Note that Atom and FlexGen use uniform quantization with group sizes of 64 and 128, respectively. All the KVQuant models throughout this experiment section are calibrated using 16 calibration samples of sequence length 2K from the Wikitext-2 training set. See Appendix M for details on our experimental setup, including our methodology for computing KV cache size estimates. Table 1 shows the results for LLaMA models for the Wikitext-2 dataset. We compared our method with per-token quantization with and without grouping. The baseline configurations used by Atom and FlexGen are included for reference [44, 34]. We find that our method consistently outperforms baseline approaches by an especially large margin with 3-bit and 2-bit quantization. Once we incorporate outliers, we further push the performance of low-precision quantization, achieving 4-bit quantization with less than 0.02 perplexity degradation, 3-bit quantization with under 0.1 perplexity degradation, and 2-bit quantization with under 0.5 perplexity degradation on Wikitext-2, relative to the fp16 baseline, across all models (while attaining 3.7 ×, 4.8×, and 6.9 × memory savings, respectively). 73.7x4.8x6.8x 3.7x4.8x6.8x Figure 3: Perplexity results for the LLaMA-2-7B-32K model [5] as well as the Llama-2-70B-32K LongLoRA model [6] on the Wikitext-2 dataset, evaluated using different sequence lengths. Table 2: Passkey retrieval results across different context lengths for the LLaMA-2-7B-32K model (uptrained for long sequence lengths using positional interpolation [ 5]) as well as the Llama-2- 70B-32K LongLoRA model [6]. The values reported are the success rate for retrieving the passkey, computed over 50 samples. We also include comparisons with KIVI for reference, using the 2-bit configuration with group size of 32 and 128-element fp16 residual [ 26]. Average bit widths are estimated for each approach assuming 32K context length. Note that the open-source code for running KIVI with LLaMA does not support grouped-query attention, so we did not include comparisons with KIVI for Llama-2-70B-32K. Model Method 2K 4K 8K 16K 32K Avg. Bit Width LLaMA-2-7B-32K fp16 1 1 1 1 1 16 KIVI-2-gs32-r128 0.76 0.72 0.72 0.68 0.7 3.05 nuq4-1% 1 1 1 1 1 4.33 nuq3-1% 0.98 1 1 1 1 3.33 nuq2-1% 1 1 0.98 1 1 2.33 Llama-2-70B-32K fp16 1 1 1 1 1 16 nuq4-1% 1 1 1 1 1 4.35 nuq3-1% 1 1 1 1 1 3.35 nuq2-1% 0.98 0.98 0.96 1 0.74 2.35 4.2 Long Context Length Evaluation Perplexity Evaluation. We evaluated long context length performance using the LLaMA-2-7B- 32K model (uptrained for long sequence lengths using positional interpolation [ 5]) as well as the Llama-2-70B-32K LongLoRA model [6]. For evaluating performance on longer context lengths, we first evaluated perplexity on Wikitext-2 using larger amounts of input context, as shown in Figure 3 [6, 13]. The results demonstrate how our method maintains accuracy even for longer amounts of input context, thereby enabling efficient and accurate long sequence length inference. Passkey Retrieval Evaluation. We also evaluated the performance of our quantization method on passkey retrieval to assess the model’s ability to use its context. Passkey retrieval involves evaluating the model’s capacity to locate specific information in long texts [19], and this can be used to effectively measure the maximum distance over which a token can attend during the inference stage. We used the passkey evaluation framework from [45] (which is based on the methodology from [28]) to evaluate retrieval performance. The passkey retrieval results are provided in Table 2, demonstrating how our method is able to maintain the retrieval performance for long context length models. We also include comparisons with the passkey retrieval when using KIVI for the LLaMA-2-7B-32K model [26], demonstrating that our approach can attain higher retrieval rate for the same compression level. Our improved performance on retrieval tasks can be attributed to our improved representation of all tokens equally. This approach differs from KIVI, which preserves a local window of residual tokens in fp16. Therefore, while KIVI is effective at representing the tail part of the context, it may provide less benefit for tasks requiring the utilization of the full context window. LongBench Evaluation. Table 3 shows evaluation on LongBench [3] for the LLaMA-2-7B-32K model. LongBench contains a suite of long-context length evaluation benchmarks including QA tasks, summarization, and few-shot learning [ 3]. The max input context length is set at 31500, and results using KIVI are also included for reference [26]. Our results demonstrate that our 3-bit 8Table 3: LongBench evaluation for the LLaMA-2-7B-32K model using KVQuant-3bit-1%. Com- parisons with KIVI are included for reference, using the configuration with group size of 32 and 128-element fp16 residual [26]. Average bit widths are estimated for each approach assuming 12.2K context length, which was the average number of tokens across all tasks. Config Avg. bitNtrvQAQasperMF-enHotpot2WikiMusiqueGovRepQMSumMNewsTRECTriviQASamSumRBenchLCC PsgRetrPsgCntAvg. fp16 Baseline 16 17.96 10.51 33.43 12.55 12.53 6.19 29.65 16.99 22.15 71 87.79 43.97 59.99 62.14 23 1.50 31.96 KIVI-2-gs32-r128 3.17 19.25 10.66 24.78 12.48 11.19 6.38 27.05 16.36 23.37 71 80.80 43.93 57.74 60.61 13.58 1.50 30.04 KVQuant-3bit-1% 3.33 18.87 13.67 30.93 12.07 12.55 6.25 27.10 16.53 16.54 71 87.55 43.95 59.50 61.52 19.5 1.75 31.21 Table 4: RULER evaluation results for the LLaMA-2-7B-32K model with KVQuant quantization methods. We report accuracy across RULER tasks, comparing our KVQuant configurations to baseline and KIVI approaches. A maximum context length of 32K is used for evaluation. Our results show that our method retains baseline accuracy even with aggressive quantization and pruning. Config Avg. bit Niah1 Niah2 Niah3 MKey1MKey2MKey3MValueMQueryVT CWE FWE QA1 QA2 Avg. fp16 Baseline 16 100 99.8 98.6 94 68.2 11 55.95 64.5 37.88 9.64 30.4 31.6 31.6 56.40 KIVI-2-gs32-r128 3.05 76 85.6 59.6 72.6 11.4 0 34.7 46.45 39.6 8.26 30.53 24.8 27.6 39.78 KVQuant-3bit-1% 3.33 99.8 98.8 95.2 92.8 61.6 6.4 47.5 54.45 41.04 8.52 29.33 31.0 31.0 53.65 KVQuant-2bit-1% 2.33 95.4 86.8 49.8 73.6 23.4 0 16.65 22.95 22.52 5.14 24.0 26.4 28.4 36.54 model can attain minimal degradation relative to the fp16 baseline, outperforming KIVI for a similar compression level. RULER Evaluation. Finally in Table 4, we provide evaluation of KVQuant and KIVI on the RULER benchmark suite [15] using LLaMA-2-7B-32K. As can be seen in the table, 3-bit KVQuant achieves 14% better score against KIVI with a similar average bit-width. Furthermore, our 2-bit KVQuant achieves similar accuracy to KIVI with 1.5× smaller bit-width. 4.3 Joint Weight and KV Cache Quantization Table 5 provides results for our KV cache quantization method when the weights are also quantized using the methodology in SqueezeLLM [ 17]. We observe minimal perplexity degradation when leveraging our KV cache quantization approach, even when weights are also quantized to reduced precision. In particular, we observe small 0.02 and 0.1 perplexity degradation of 4-bit and 3-bit weight-only quantization, respectively, when quantizing the KV cache using nuq4-1% for the LLaMA- 7B and LLaMA-13B models. These results demonstrate how our method is compatible with existing weight-only quantization methods. 4.4 Performance Analysis and Memory Savings Table 6 shows kernel benchmarking results using a batch size of 1 for the 4-bit dense-and-sparse compression and matrix-vector kernel implementations. We show results across different sequence lengths to assess the performance of the kernels at different points during generation. We report latency benchmarked on an A6000 GPU. The results show that for the Key and Value multiplications, we can achieve 1.2-1.6× and 1.3-1.7× latency savings, respectively, relative to the baseline. We have integrated these kernels into an end-to-end generation pipeline that is able to compress activations dynamically during inference, thereby achieving significant memory savings and allowing for either larger batch sizes or longer sequence lengths. Appendix A highlights the benefits of KVQuant in supporting longer context lengths through reducing KV cache memory footprint. As shown in Table 8 in Appendix A, our nuq2 method provides 8× KV cache compression and enables serving the quantized LLaMA-7B model with a context length of 1M tokens on a single A100 GPU, as well as enabling serving the LLaMA-7B model with 10M context length on an 8-GPU system. Our results show little degradation compared to baseline fp16 inference 9Table 5: KV cache quantization results when KVQuant is applied in conjunction with the weight quantization methodology in SqueezeLLM [17]. w4-s45 and w3-s45 for weights refer to the 4-bit and 3-bit dense-and-sparse weight quantization approaches in [ 17], respectively. See Appendix M for experimental details. Weights KV Cache LLaMA-7B LLaMA-13B Avg. Bits (KV Cache) fp16 fp16 5.68 5.09 16 w4-s45 fp16 5.77 5.17 16 nuq4-1% 5.79 5.18 4.32-4.33 w3-s45 fp16 6.13 5.45 16 nuq3-1% 6.23 5.52 3.32-3.33 Table 6: Average latency (in microseconds) for the Key and Value nuq4-1% kernels, benchmarked on an A6000 GPU for the LLaMA-2-7B-32K model across different sequence lengths ( l). fp16 matrix-vector multiplication latencies are included for reference, and the fp16 Key multiplication time also includes the time to apply RoPE to the newly appended Key vector. Section 3.7 and Appendix R provide additional details for our kernel implementation, Appendix R describes our benchmarking methodology, and Table 22 provides a detailed breakdown of kernel runtime on an A6000 GPU. Activation Operation l=2K l=4K l=16K Key fp16 Matvec 33.3 59.1 219.4 Key nuq4-1% 25.6 39.9 126.3 Value fp16 Matvec 26.0 50.2 203.7 Value nuq4-1% 22.1 37.9 124.5 while providing significant compression, demonstrating the benefits of our approach for enabling accurate and efficient long sequence length inference. 5 Conclusion As context lengths in LLMs increase, the KV cache activations surface as the dominant contributor to memory consumption. Quantization is a promising approach to reduce the size of KV cache activations, but prior solutions failed to represent activations accurately in ultra-low precisions, such as sub-4-bit. In contrast, we achieve accurate ultra-low precision KV cache quantization. By quantizing Keys per-channel before applying RoPE, we are able to better match the outlier distribution and mitigate the impacts of RoPE on quantization (due to it mixing pairs of channels which may have different average magnitudes). We use non-uniform quantization to better allocate the small number of quantization signposts at low precision. We observe significant accuracy improvements when employing dense-and-sparse quantization, particularly when detecting outliers at the same granularity as we compute quantization scaling factors. Crucially, we demonstrate that we can perform accurate calibration offline for Keys, as well as efficient online scaling factor and outlier threshold computation for Values. By leveraging these methods, we are able to enable accurate low-precision activation quantization, achieving 4.8x compression (nuq3-1% outliers) with only 0.1 perplexity degradation across different LLaMA, Llama-2, Llama-3, and Mistral models. Our methodology therefore supports inferring the LLaMA-7B model with a context length of 10M on an 8-GPU serving system. Through our efficient kernel implementation, we are able to show improved latency relative to the fp16 baseline, demonstrating how our method allows for improved latency in addition to the memory savings. Acknowledgements The authors would like to acknowledge Nicholas Lee for helpful discussions and feedback. We acknowledge gracious support from Intel, Furiosa, Apple, Samsung SAIT, POSCO HOLDINGS, and NVIDIA. We also appreciate the support from Microsoft through their Accelerating Foundation Model Research, including great support from Sean Kuno. Furthermore, we appreciate support from Google Cloud, the Google TRC team, and specifically Jonathan Caton, and Prof. David Patterson. Prof. Keutzer’s lab is sponsored by the Intel corporation, Intel One-API, Intel VLAB team, the Intel 10One-API center of excellence, as well as funding through BDD and BAIR. We appreciate great feedback and support from Ellick Chan, Saurabh Tangri, Andres Rodriguez, and Kittur Ganesh. Sehoon Kim would like to acknowledge the support from the Korea Foundation for Advanced Studies (KFAS). Michael W. Mahoney would also like to acknowledge a J. P. Morgan Chase Faculty Research Award as well as the DOE, NSF, and ONR. Our conclusions do not necessarily reflect the position or the policy of our sponsors, and no official endorsement should be inferred. References [1] AI@Meta. Llama 3 model card. 2024. [2] Anthropic. Introducing claude 2.1, Nov 2023. [3] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. [4] Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding and overcoming the challenges of efficient transformer quantization. arXiv preprint arXiv:2109.12948, 2021. [5] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023. [6] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. arXiv preprint arXiv:2309.12307, 2023. [7] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. [8] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023. [9] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: A sparse-quantized representation for near-lossless llm weight compression. arXiv preprint arXiv:2306.03078, 2023. [10] Goran Flegar and Enrique S Quintana-Ortí. Balanced csr sparse matrix-vector product on graphics processors. In Euro-Par 2017: Parallel Processing: 23rd International Conference on Parallel and Distributed Computing, Santiago de Compostela, Spain, August 28–September 1, 2017, Proceedings 23, pages 697–709. Springer, 2017. [11] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [12] Amir Gholami, Zhewei Yao, Sehoon Kim, Michael Mahoney, and Kurt Keutzer. Ai and memory wall. RiseLab Medium Post, 2021. [13] Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Simple on-the-fly length generalization for large language models. arXiv preprint arXiv:2308.16137, 2023. [14] Jung Hwan Heo, Jeonghoon Kim, Beomseok Kwon, Byeongwook Kim, Se Jung Kwon, and Dongsoo Lee. Rethinking channel dimensions to isolate outliers for low-bit weight quantization of large language models. arXiv preprint arXiv:2309.15531, 2023. [15] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. Ruler: What’s the real context size of your long-context language models? arXiv preprint arXiv:2404.06654, 2024. [16] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [17] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023. 11[18] Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W Mahoney, et al. Full stack optimization of transformer inference: a survey. arXiv preprint arXiv:2302.14017, 2023. [19] Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can context length of open-source llms truly promise? In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023. [20] Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. Snapkv: Llm knows what you are looking for before generation, 2024. [21] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023. [22] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with blockwise ringattention, 2024. [23] Ruikang Liu, Haoli Bai, Haokun Lin, Yuening Li, Han Gao, Zhengzhuo Xu, Lu Hou, Jun Yao, and Chun Yuan. Intactkv: Improving large language model quantization by keeping pivot tokens intact, 2024. [24] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free quantiza- tion aware training for large language models. arXiv preprint arXiv:2305.17888, 2023. [25] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. arXiv preprint arXiv:2305.17118, 2023. [26] Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: Plug-and-play 2bit kv cache quantization with streaming asymmetric quantization. 2023. [27] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016. [28] Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context length for transformers. arXiv preprint arXiv:2305.16300, 2023. [29] Roman Novak, Yasaman Bahri, Daniel A Abolafia, Jeffrey Pennington, and Jascha Sohl- Dickstein. Sensitivity and generalization in neural networks: an empirical study. arXiv preprint arXiv:1802.08760, 2018. [30] OpenAI. New models and developer products announced at devday 2023, Nov 2023. [31] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv e-prints, 2019. [32] Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo Luschi, and Douglas Orr. Sparq attention: Bandwidth-efficient llm inference. arXiv preprint arXiv:2312.04985, 2023. [33] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally calibrated quantiza- tion for large language models. arXiv preprint arXiv:2308.13137, 2023. [34] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. Flexgen: High-throughput generative inference of large language models with a single gpu. In International Conference on Machine Learning, pages 31094–31116. PMLR, 2023. [35] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [36] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 12[37] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [38] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, and Xianglong Liu. Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling. arXiv preprint arXiv:2304.09145, 2023. [39] Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier suppression: Pushing the limit of low-bit transformer language models. Advances in Neural Information Processing Systems, 35:17402–17414, 2022. [40] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the- art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38–45, Online, October 2020. Association for Computational Linguistics. [41] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pages 38087–38099. PMLR, 2023. [42] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks, 2023. [43] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H _2 o: Heavy-hitter oracle for efficient generative inference of large language models. arXiv preprint arXiv:2306.14048, 2023. [44] Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom: Low-bit quantization for efficient and accurate llm serving. arXiv preprint arXiv:2310.19102, 2023. [45] Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. Pose: Efficient context window extension of llms via positional skip-wise training. arXiv preprint arXiv:2309.10400, 2023. 13A Memory Bottlenecks for Long Context Length Inference Table 7 shows the model size and KV cache memory requirements for different LLaMA models with different sequence lengths. For short sequence lengths, the model weights are the primary memory bottleneck. However, for longer sequence lengths and larger batch sizes, the KV cache memory is the main bottleneck. This is particularly pronounced when the weights are already quantized to low precision. In our work, we demonstrate that we can help address the KV cache memory bottleneck through low-precision KV cache quantization. By compressing the KV cache to 2-bit precision, we can enable 1M context length inference with the LLaMA-7B model on a single A100-80GB GPU, and we can also enable 10M context length inference with the LLaMA-7B model on an 8-GPU system. Table 8 shows the KV cache memory requirements for 128K, 1M, and 10M sequence lengths, with the KV cache stored in fp16 as well as 4-bit, 3-bit, and 2-bit precision with KVQuant. As one can see, our method provides 3.7× KV cache compression (nuq4-1%) and enables serving the quantized LLaMA-65B model with a context length of 32K tokens on a single A100-80GB GPU (requiring 30.3GB for the model weights compressed to 4-bit, and 46.5GB for the KV cache when compressed with nuq2-1%), and our nuq2 method enables serving the LLaMA-7B model with a context length of 1M tokens on a single A100 GPU (requiring 64GB for the KV cache). Additionally, when considering an 8-GPU serving system, we enable serving the LLaMA-7B model with 10M context length (with nuq2), or the LLaMA-65B model with 1M context length (with nuq3). Our results show little degradation compared to baseline fp16 inference while providing significant compression, demonstrating the benefits of our approach for enabling accurate and efficient long sequence length inference. Table 7: Model size and activation memory size estimates for different sequence lengths and batch sizes (BS) for different LLaMA models. For long sequence lengths and larger batch sizes, activation memory is the main bottleneck (particularly when weights are already quantized to low precision). By compressing the KV cache to 2-bit precision, we can enable1M context length inference with the LLaMA-7B model on a single A100-80GB GPU, and we can also enable 10M context length inference with the LLaMA-7B model on an 8-GPU system. BS Model Model Size (GB) KV Cache Size w/ Diff. Seq Len (GB) 16 → 2-bit 32K 128K 1M 10M (16 → 2-bit) 1 7B 12.6 → 1.6 16 64 512 4883 → 610 13B 24.1 → 3.0 25 100 800 7629 → 954 30B 60.3 → 7.5 49 195 1560 14877 → 1860 65B 121.1 → 15.1 80 320 2560 24414 → 3052 4 7B 12.6 → 1.6 64 256 2048 19531 → 2441 13B 24.1 → 3.0 100 400 3200 30518 → 3815 30B 60.3 → 7.5 195 780 6240 59509 → 7439 65B 121.1 → 15.1 320 1280 10240 97656 → 12207 B Additional Related Works B.1 Outlier-Aware LLM Quantization LLMs have been known to have distinct outliers both in weights and activations [ 7, 9, 17]. SqueezeLLM and SpQR both decompose the weight matrix into a sparse matrix containing a small portion of outliers and a dense matrix that can be accurately quantized to low precision (referred to as dense-and-sparse or sparse-quantized representation) [9, 17]. LLM.int8() [7] handled particular outlier channels separately in higher precision, and SmoothQuant [41] migrates quantization difficulty due to outlier channels to weights in order to support joint weight-activation quantization. Other works reconsidered the dimension along which we quantize in order to reduce quantization error (or else added per-channel compensation to improve quantization performance) [4, 14, 39, 38]. In this work, we demonstrate that per-channel pre-RoPE Key quantization provides significant accuracy benefits given the outlier structure in Keys, and that dense-and-sparse quantization can be efficiently applied for KV cache quantization. 14Table 8: Activation memory size estimates (GB) for 128K, 1M, and 10M sequence length ( l) for different LLaMA models. By compressing the KV cache to 2-bit precision, we can enable 1M context length inference with the LLaMA-7B model on a single A100-80GB GPU, and we can also enable 10M context length inference with the LLaMA-7B model on an 8-GPU system. Model Method l=128K l=1M l=10M LLaMA-7B fp16 64.0 512.0 4882.8 nuq4 16.0 128.1 1221.9 nuq4-1% 17.3 138.4 1319.6 nuq3 12.0 96.1 916.7 nuq3-1% 13.3 106.4 1014.4 nuq2 8.0 64.1 611.5 nuq2-1% 9.3 74.4 709.2 LLaMA-65B fp16 320.0 2560.0 24414 nuq4 80.0 640.3 6106.5 nuq4-1% 86.5 691.5 6595.0 nuq3 60.0 480.3 4580.6 nuq3-1% 66.5 531.5 5069.1 nuq2 40.0 320.3 3054.7 nuq2-1% 46.5 371.5 3543.3 B.2 Non-uniform LLM Quantization Non-uniform quantization has also been applied in the context of LLMs. Non-uniform quantization allows for more flexible quantization signpost placement relative to uniform quantization methods, enabling improved accuracy for the same bit precision [17, 8]. Building on the observation that model parameters tend to be approximately normally-distributed, prior work has proposed the NormalFloat datatype [8]. SqueezeLLM [ 17] derived per-channel non-uniform quantization signposts using a sensitivity-weighted k-means approach. In this work, we show that we can derive accurate per-layer non-uniform datatypes using a sensitivity-weighted k-means approach with KV cache activations. C RoPE Equation The rotation matrix for RoPE is provided in Equation 2, where c and s are cosine and sine functions, θi = 10000−2(i−1)/d, d is the attention head dimension, and n is the current position in the sequence:   c(nθ1) −s(nθ1) ··· 0 0 s(nθ1) c( nθ1) ··· 0 0 ... ... ... ... ... 0 0 ··· c(nθd/2) −s(nθd/2) 0 0 ··· s(nθd/2) c( nθd/2)   (2) The Query vectors computed at each iteration will have RoPE applied (to obtain ˜Qm = Rd θ,m ∗ Qm). When caching Key vectors, we therefore need to either cache ˜Kn = Rd θ,n ∗ Kn, or else we need to cache Kn and apply Rd θ,n on-the-fly during inference. In order to apply Rd θ,n efficiently on-the-fly, we leverage the element-wise formulation of RoPE rather than the matrix-multiplication formulation from Equation 2. The element-wise formulation for Rd θ,nx is as follows, where ⊙ is the element- wise multiplication operator (note that the formulation that we use matches the implementation in the Transformers library for LLaMA [ 40], and it is a different but equivalent formulation to the element-wise expression in [35]): 15  x1 x2 ... xd 2 xd 2 +1 ... xd−1 xd   ⊙   c(θ1n) c(θ2n) ... c(θd 2 n) c(θ1n) ... c(θd 2 −1n) c(θd 2 n)   +   −xd 2 +1 −xd 2 +2 ... −xd x1 ... xd 2 −1 xd 2   ⊙   s(θ1n) s(θ2n) ... s(θd 2 n) s(θ1n) ... s(θd 2 −1n) s(θd 2 n)   (3) By leveraging this element-wise implementation, we can apply RoPE on-the-fly to the Key activations (after dequantizing the Key activations and before multiplying them with the corresponding elements in the Query vector). D Derivation for Sensitivity Analysis To compute the sensitivity analysis we largely follow the derivation in [29], which was originally provided to compute sensitivity of a NN based classifier but can be extended with minor modifications for quantization. In particular, the sensitivity measure is based on how much the loss output of the model is perturbed. To compute this we denote activations at a layer before quantization as A, after quantization as AQ, quantization perturbation in activation as A − AQ, and the gradient of the Loss function w.r.t. activation as J(A) = ∂L ∂A (A). By making the assumption that the quantization perturbation of different activations follow a Gaussian distribution with zero mean, we can show that the sensitivity of an activation value is proportional to Fii \u0000 A − Q(A) \u00012 as was given in Equation 1: E∆A h |L(A) − L(A + ∆A)|2 i ≈ E∆A h\u0000 J(A)T ∆A \u00012i = E∆A \"\u0010X i Ji∆Ai \u00112 # = E∆A \"X i J2 i ∆A2 i # = X i J2 i E∆A \u0002 ∆A2 i \u0003 . Here note that we first assume a first order Taylor series expansion to approximate the perturbation to the loss, and then use the zero mean assumption of the quantization perturbation to derive the second line. To approximate the E∆A \u0002 ∆A2 i \u0003 we use empirical evaluation of the expectation by sampling multiple different inputs and empirically computing the resulting quantization perturbation which results in Equation 1. E Derivation for Quantization Error In our work, before applying the sensitivity-weighted K-means to derive quantization signposts, we normalize each element Ai in the flattened activation A. This normalization for Ai involves a shift by a zero-point zi followed by rescaling the quantization signposts by a scaling factor si, where si and zi are the scaling factor and zeropoint corresponding to element Ai: Ai,norm = Ai − zi si , (4) where Ai and Ai,norm are element i from activation A before and after normalization, respectively. We then quantize Ai,norm to Q(Ai,norm) with quantization error ∆Ai,norm. After we dequantized, we rescale each element by si and add zi to get the recovered quantized activation value Q(Ai): 16Q(Ai) = si Q(Ai,norm) + zi. (5) As such, if there is quantization error ∆Ai,norm in Ai,norm, this will be scaled by si in terms of the error in Ai, i.e., ∆Ai = si∆Ai,norm. For activation A which is normalized to Anorm (with corresponding scaling factors si for each element Ai), minimizing the sensitivity-weighted quantization error as expressed in Equation 1 gives us the following expression, which we can minimize using the normalized activations across all N elements from the samples in a calibration set: Q(A)∗ ≃ arg min Q NX i=1 Fii \u0000 Ai − Q(Ai) \u00012 (6) = arg min Q NX i=1 Fii \u0010 s2 i \u0000 Ai,norm − Q(Ai,norm) \u00012\u0011 (7) F Key and Value Dynamic Range Figure 4 shows the portion of the elements contained within difference percentages of the dynamic range for both Keys and Values. The majority of values (∼ 99%) are contained in a small portion of the dynamic range, and a small portion of numerical outliers skew the dynamic range that must be represented. This motivates our dense-and-sparse approach which removes numerical outliers and stores them in a separate sparse matrix, thereby restricting the range that needs to be represented in the dense component. 0 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Normalized Magnitude Keys 0 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Normalized Magnitude Values t100 t99.99 t99.9 t99 Figure 4: Distribution of the magnitude of elements of Key (Pre-RoPE) and Value activations for different layers of LLaMA-7B, computed on a single sample with sequence length 2K from the Wikitext-2 dataset. The normalized magnitude is computed by dividing by the largest magnitude value in that layer. As one can see, for both Key and Value activations, the majority of values lie in a small portion of the dynamic range, with a few numerical outliers skewing the dynamic range (and thereby reducing the fidelity when quantizing to low precision). G Per-Channel Key Quantization Ablations As shown in Table 9, per-channel quantization for Keys and per-token quantization for Values outperforms the standard per-token quantization approach for both Keys and Values, yielding an improvement of 3.82 perplexity for the LLaMA-7B model at 3-bit precision. This demonstrates the 17benefits of per-channel Key quantization to mitigate the large outlier channels in Keys. Note that for all experiments using per-channel quantization, we use an fp16 zeropoint rather than a low-precision zeropoint that is rounded to the nearest integer value. We do this since for some of the Key channels, all of the elements are positive or all of the elements are negative, meaning that the zeropoint will fall outside of the range that is representable by a low-precision integer (and rounding it to the nearest low-precision value can degrade performance). Additionally, we observe that per-channel quantization for Values actually performs worse than per-token quantization. We hypothesize that this behavior is because per-channel Value quantization leads to greater error accumulation in particular output values (since the result of the attention scores multiplied by one channel of the Values will be localized to a single value in the output vector), which leads to greater quantization error at later model layers. Another concurrent work, KIVI [26], observes similar behavior for per-channel Value quantization, which they attribute to the fact that per-token Value quantization confines the error to each token. Assuming that the output is a weighted sum of only a few important tokens (as only a few attention scores are large), a perturbation in these tokens can lead to significant degradation. Per-token Value quantization therefore ensures that the quantization of unimportant tokens does not adversely impact the important tokens. Table 9: Ablation Study: Perplexity comparison of per-token and per-channel quantization for KV cache activations for LLaMA-7B. PT refers to per-token quantization, and PC refers to per-channel quantization. Datatype Key Dim. Value Dim. Perplexity KV Cache Size (GB) Seqlen 128K fp16 - - 5.68 64.0 int3 PT PT 10.87 12.0 int3 PC PC 223 12.0 int3 PC PT 7.05 12.0 H Pre-RoPE Key Quantization Ablations As shown in Table 10, pre-RoPE Key quantization achieves higher accuracy than post-RoPE quanti- zation, with an improvement of 0.82 perplexity for 3-bit quantization with the LLaMA-7B model. These results show that the rotary positional embeddings make Key quantization more challenging due to mixing pairs of channels with different magnitudes. Pre-RoPE quantization thereby allows for more accurate quantization at low precision. Table 10: Ablation Study: Perplexity comparison of Pre-RoPE and post-RoPE Key quantization for LLaMA-7B (using per-channel Key quantization and per-token Value quantization). Pre-RoPE quantization leads to significant improvement (see Section 3.2 for more details). Datatype Scheme Perplexity KV Cache Size (GB) Seqlen 128K fp16 - 5.68 64.0 int3 post-RoPE 7.05 12.0 int3 pre-RoPE 6.23 12.0 I Sensitivity-Weighted Non-Uniform Quantization Ablations Table 11 shows perplexity evaluation results across different LLaMA, Llama-2, and Mistral models on Wikitext-2 for different datatypes, including nuq3, nuq3 without using sensitivity-weighting, as well as nuq3 with sensitivity-weighting but without accounting for the per-channel scaling factors when performing k-means. We observe particularly noticeable gains relative to uniform quantization for 3- bit and 2-bit quantization, where the benefits of non-uniform quantization are more pronounced due to the reduced precision. These results demonstrate the benefits of our sensitivity-weighted non-uniform quantization approach relative to NormalFloat quantization [8], as we achieve consistent accuracy improvements of up to 0.33 perplexity across different models. These results also demonstrate the 18necessity of our sensitivity-weighting approach in order to derive performant non-uniform datatypes using a k-means based approach. Additionally, we observe distinct benefits when also accounting for the per-channel scaling factors when performing k-means. Table 11: Ablation Study: Ablation of our sensitivity-weighted non-uniform datatype for different models on Wikitext-2. All experiments use pre-RoPE per-channel quantization for Keys and per-token quantization for Values (meaning that all configurations are the same as in KVQuant, except for the datatype). We compare against both uniform (int3) and non-uniform (nf3) [8] approaches, as well as with using “unweighted” k-means (i.e., not sensitivity-weighted) and “Fisher-weighted k- means” (without accounting for per-channel scaling factors) to compute the non-uniform quantization signposts. Note that there is slight variation in average bitwidth across models due to the differing hidden dimensions. Results report perplexity with 2K/4K/8K sequence length for LLaMA, Llama-2, and Mistral, respectively. Method LLaMA Llama-2 Mistral-7B Avg. Num. Bits7B 13B 30B 65B 7B 13B 70B baseline 5.68 5.09 4.10 3.53 5.12 4.57 3.12 4.76 16 int3 6.23 5.60 5.09 5.18 5.95 5.98 3.26 5.26 3.00-3.02 nf3 6.05 5.42 4.51 3.84 5.55 5.15 3.27 5.13 3.00-3.02 nuq3 (unweighted) 6.84 6.16 5.37 4.57 8.52 7.66 3.67 5.29 3.00-3.02 nuq3 (Fisher-weighted) 6.01 5.34 4.41 3.74 5.49 4.83 3.26 5.03 3.00-3.02 nuq3 (KVQuant) 5.94 5.32 4.34 3.68 5.39 4.82 3.23 4.98 3.00-3.02 J Per-Vector Dense-and-Sparse Quantization Ablations Table 12 shows the performance improvements we observe when isolating a small portion of outliers and storing them in a sparse format. We provide results both with using a single per-matrix outlier threshold, as well as with applying separate outlier thresholds per-vector. In particular, we see greater improvements by employing outlier detection with a different threshold per-channel for Keys and per-token for Values. This provides additional benefits since some values which would be considered outliers for the entire matrix are not actually outliers within a particular channel (so they are not hard to quantize). It is therefore better to directly target the outliers that will skew the quantization range for a particular channel. By removing 1% of outliers using per-vector thresholds, we can achieve an additional 0.19 reduction in perplexity for the LLaMA-7B model at 3 bits, thereby enabling 3-bit quantization with under 0.1 degradation in perplexity. Table 12: Ablation Study: Perplexity comparison of different outlier isolation methods for LLaMA- 7B on Wikitext-2. Per-vector outlier detection allows for significant accuracy improvements relative to per-tensor outlier detection. All experiments use per-token quantization for Values and per-channel quantization for Keys (pre-RoPE). “PV” refers to using per-vector outlier thresholds, and “PM” refers to using a single per-matrix outlier threshold. Datatype % Outliers Outlier Dim. Perplexity KV Cache Size (GB) Seqlen 128K fp16 - - 5.68 64.0 nuq3 - - 5.94 12.0 nuq3 0.1% PM 5.89 12.2 nuq3 0.1% PV 5.82 12.2 nuq3 1% PM 5.85 13.3 nuq3 1% PV 5.75 13.3 K Attention Sink-Aware Quantization Ablations Table 13 provides perplexity with and without Attention Sink-Aware quantization across different LLaMA, Llama-2, and Llama-3 models. For all bit widths (4, 3, and 2-bit) and in both dense- only and dense-and-sparse quantization settings, Attention Sink-Aware quantization consistently yields perplexity improvement. Notably, the perplexity gain is more pronounced at lower bit widths 19and without sparsity. We observe particularly significant improvements for Llama-3 models, with perplexity improvements of 0.25 and 0.13 PPL for nuq2-1% on Llama-3-8B and Llama-3-70B, respectively. Table 13: Ablation Study: Perplexity with and without Attention Sink-Aware quantization on Wikitext-2 with various models. Attention Sink-Aware quantization consistently improves perplexity across different models and bit widths, particularly with lower bit widths and without sparsity. Datatype Attention Sink-Aware LLaMA-7B LLaMA-13B Llama-2-7B Llama-2-13B Llama-3-8B Llama-3-70B fp16 - 5.68 5.09 5.12 4.57 5.54 2.59 nuq4 X 5.72 5.14 5.17 4.62 5.64 2.65 nuq4 O 5.72 5.13 5.16 4.60 5.60 2.62 nuq4-1% X 5.69 5.10 5.13 4.59 5.57 2.60 nuq4-1% O 5.69 5.10 5.13 4.58 5.56 2.60 nuq3 X 5.94 5.32 5.39 4.82 6.10 2.99 nuq3 O 5.87 5.25 5.34 4.71 5.84 2.72 nuq3-1% X 5.75 5.15 5.18 4.62 5.67 2.66 nuq3-1% O 5.75 5.14 5.17 4.61 5.64 2.63 nuq2 X 8.47 7.29 11.20 23.34 16.63 6.79 nuq2 O 7.23 5.82 7.03 9.59 7.04 3.49 nuq2-1% X 6.05 5.39 5.47 4.85 6.29 2.95 nuq2-1% O 6.01 5.36 5.41 4.78 6.04 2.82 L Calibration Ablations Figure 5 outlines the accuracy and efficiency challenges which our work addresses in order to enable accurate and efficient KV cache quantization. We demonstrate that we can circumvent the challenges related to efficiently performing online per-channel scaling factor computation by instead calibrating offline for the scaling factor without hurting accuracy. Additionally, we show that we can perform per-token scaling factor and outlier threshold computation efficiently online during inference, thereby enabling accurate per-token quantization without compromising on efficiency. Per-channel Quantization  dim sequence length KeysNew Key Per-token Quantization  dim sequence length ValuesNew ValueChallenge: need to potentially recompute the scaling factor every time a new key is added if we computeitonline Challenge: Need to compute the scaling factor for each incoming token per-channel scaling factor per-token scaling factor Figure 5: One typically achieves better performance when the scaling factor/zero point are computed online. However, this is quite challenging to do for per-channel quantization, as these factors will not only need to be recomputed for every new Key appended to the Key cache, but also all the prior cached Keys will need to be updated. As such, we use a calibration set to compute per-channel scaling factors offline. A similar challenge exists for per-token quantization, but online calibration for this does not require updating prior cached Values. In Section 3.6 and Appendix L, we discuss how we are able to efficiently compute outlier thresholds / scaling factors for per-token calibration, thereby enabling online computation. Table 14 shows accuracy results when using offline calibration for computing the scaling factors for the Keys. For 3-bit quantization, we observe minor accuracy degradation when not employing outlier extraction methods. However, if we remove a small percentage of outliers, then the accuracy with offline calibration is the same as computing the scaling factors online per-channel during evaluation. This demonstrates that when incorporating outlier extraction methods, we are better able to perform offline calibration due to reduced sensitivity to outliers (either to outliers during calibration that exaggerate the quantization range, or to outliers during evaluation that cannot be represented accurately if there weren’t large outliers observed during calibration). 20Table 15 shows the runtime for the topk operation for the LLaMA-7B model (which is required for computing outlier thresholds online). It compares the runtime of the topk operation with the runtime for the QKV projections, finding that the topk runtime is 45% of the matrix-vector operation runtime for a single projection layer. The topk operation can also be performed efficiently on the CPU, so we can actually run this operation in parallel with the subsequent linear layer matrix-vector operations on the GPU (which is possible by computing the Value projection before the Key and Query projections). This allows us to compress the activations dynamically without added runtime overhead, thereby enabling online scaling factor computation for the Value tensors. Table 16 shows how both Fisher information computation and calibration (including k-means) per- layer take only a few minutes for the LLaMA-65B model on a typical server machine. Even if we perform calibration sequentially for each layer, the entire calibration process would take a maximum of 6 hours for the LLaMA-65B model at 4-bit precision. Table 14: Ablation Study: Model accuracy when using offline calibration for Keys with LLaMA-7B. When incorporating outlier detection, offline calibration for Keys is able to perform comparably with online calibration. All nuq3 experiments use per-token quantization for Values and per-channel quantization for Keys (pre-RoPE), and experiments with outliers use per-vector outlier detection. Datatype % Outliers Perplexity Perplexity (Online for K) (Offline for K) fp16 - 5.68 5.68 nuq3 - 5.91 5.94 nuq3 1% 5.75 5.75 Table 15: topk runtime on a vector of length 4096 for computing outlier thresholds when using 1% sparsity (compared with the runtime for the QKV matrix multiplications, which are 4096×4096 by 4096 matrix-vector multiplications for the LLaMA-7B model). The runtime is reported on a system with an A6000 GPU and an Intel Xeon Gold 6126 CPU. We find that the runtime for the topk operation is only 45% of the runtime of each matvec operation. Additionally, the topk operation can be performed efficiently on the CPU; we can therefore run this operation in parallel with subsequent linear layer operations on the GPU to compress the activations dynamically without added overhead. Operation Device Outlier % Runtime (ms) QKV Projection GPU - 0.172 topk CPU 1% 0.026 topk GPU 1% 0.088 QKV Projection / topk (Fused) GPU / CPU 1% 0.173 Table 16: Runtime for computing Fisher information as well as for calibration (including k-means) with 16 samples for LLaMA-65B quantization. Runtime for computing Fisher information was computed on an 8-GPU A100-80GB system. Runtime for calibration (including k-means) was performed on an Intel Xeon Gold 6442Y CPU, and is shown for a single layer. Note that calibration is independent for each layer, so it can be easily parallelized. Operation Runtime (minutes) Computing Fisher Information 2.8 4-bit Calibration Per-Layer (including k-means) 4.5 3-bit Calibration Per-Layer (including k-means) 2.7 2-bit Calibration Per-Layer (including k-means) 1.9 M Additional Experimental Details For our empirical evaluation, we use 16 calibration samples of sequence length 2K from the Wikitext- 2 training set (as well as the corresponding gradients) to derive the per-channel scaling factors and zero-points, and to derive the non-uniform datatypes for both Keys and Values. While we use KVQuant models that are calibrated on the Wikitext-2 dataset for all experiments, in Appendix L, we 21include an additional experiment that demonstrates the robustness of the calibration process to the choice of data. We measured perplexity on both Wikitext-2 and on C4 using a sequence length equal to the maximum context length of the model (2K for LLaMA, 4K for Llama-2, and 8K for Llama-3 and Mistral- 7B). For generative tasks, when processing the input prompt, the Key/Value matrix multiplications are computed using the fp16 Keys and Values, and then they are separately compressed into low precision. For baseline experiments, we use post-RoPE quantization, both since this is required from an efficiency perspective without a dedicated kernel implementation, and because it provides better accuracy when quantizing Keys per-token as shown in Appendix P. We make several assumptions in order to estimate average bit widths and KV cache sizes for different approaches. We compute these estimates assuming a sequence length of 128K (unless otherwise specified). For integer quantization, we assume a low-precision integer offset and a 16-bit scaling factor, whereas for NormalFloat and NUQ we assume that the zero-point and offset are each 16-bit. For the sparse matrices, 32-bit integers are assumed for the per-token indices (since we need to support long sequence lengths), and the elements and per-element indices are assumed to be 16-bit. This means that for CSR, the rows are assumed to be 32-bit and the columns and values are assumed to be 16-bit, whereas for CSC, the columns are assumed to be 32-bit and the rows and values are assumed to be 16-bit. N Comparison Between Different Datatypes and Sparsity Levels Table 17 shows perplexity across LLaMA, Llama-2, Llama-3, and Mistral models on Wikitext-2 using different datatypes. The results highlight that the NUQ datatype generally outperforms both uniformly quantized INT datatype and non-uniformly quantized NF datatype even after they are incorporated with grouping at the expense of increased bit-widths. This performance can be further improved by extracting 0.1% to 1.0% of outliers. Note that NUQ with a sparsity level 1.0% has a similar memory requirement as INT with a group size of 64, but achieves significant perplexity improvement across all models and bit widths. O Full Perplexity Evaluation Tables 18 and 19 show perplexity evaluation across all LLaMA, Llama-2, Llama-3, and Mistral models on Wikitext-2 and C4, respectively. These results demonstrate the benefits of our approach for KV cache compression across different models and bit widths. 22Table 17: Comparison of our NUQ datatype with and without sparsity against other data types on different models using the perplexity (PPL) measured on Wikitext-2. Non-uniform quantization (“nuq”) results are using pre-RoPE per-channel quantization for Keys. “gs64/128” refers to baseline experiments using grouping with group size 64/128. Note that there is a slight variation in average bitwidth across models due to the differing hidden dimensions. Method LLaMA Llama-2 Llama-3 Mistral-7B Avg. Num. Bits7B 13B 30B 65B 7B 13B 70B 8B 70B baseline 5.68 5.09 4.10 3.53 5.12 4.57 3.12 5.54 2.59 4.76 16 int4 5.98 5.32 4.34 3.73 5.66 5.01 3.31 7.89 14.03 4.97 4.00-4.02 int4-gs128 5.77 5.16 4.16 3.57 5.32 4.71 3.16 5.78 2.73 4.82 4.16 int4-gs64 5.73 5.14 4.14 3.56 5.25 4.66 3.14 5.68 2.66 4.80 4.31 nf4 5.87 5.23 4.25 3.63 5.47 4.90 3.22 6.42 3.86 4.91 4.00-4.03 nf4-gs128 5.77 5.17 4.17 3.58 5.30 4.71 3.16 5.84 2.75 4.83 4.25 nuq4 5.72 5.14 4.14 3.56 5.17 4.62 3.14 5.64 2.65 4.81 4.00-4.02 + 0.1% outliers 5.70 5.12 4.12 3.55 5.15 4.60 3.13 5.63 2.66 4.79 4.04-4.06 + 0.5% outliers 5.70 5.11 4.12 3.54 5.14 4.59 3.13 5.59 2.61 4.78 4.16-4.19 + 1.0% outliers 5.69 5.10 4.11 3.54 5.13 4.59 3.13 5.57 2.60 4.78 4.32-4.35 int3 10.87 8.69 6.82 6.37 22.71 18.26 7.68 125 158 7.64 3.00-3.02 int3-gs128 6.17 5.47 4.44 3.78 6.15 5.34 3.33 7.50 4.11 5.16 3.15 int3-gs64 5.93 5.29 4.26 3.66 5.64 4.98 3.23 6.38 3.23 5.00 3.30 nf3 7.33 6.21 5.46 4.44 9.96 9.50 4.06 61.07 98.64 6.30 3.00-3.03 nf3-gs128 6.26 5.52 4.54 3.83 6.21 5.43 3.38 7.79 4.74 5.23 3.25 nuq3 5.94 5.32 4.34 3.68 5.39 4.82 3.23 6.10 2.99 4.98 3.00-3.02 + 0.1% outliers 5.82 5.22 4.21 3.62 5.27 4.69 3.19 5.96 2.96 4.91 3.04-3.06 + 0.5% outliers 5.76 5.16 4.16 3.59 5.20 4.65 3.16 5.74 2.69 4.84 3.16-3.19 + 1.0% outliers 5.75 5.15 4.15 3.57 5.18 4.62 3.15 5.67 2.66 4.82 3.32-3.35 int2 11779 69965 1470 7272 4708 3943 976 2841 2164 573 2.00-2.02 int2-gs128 37.37 41.77 16.49 13.63 118 93.09 18.31 200 2092 51.96 2.14 int2-gs64 11.09 9.84 6.60 5.54 25.69 26.83 5.93 57.82 43.69 12.47 2.28 nf2 3210 5785 2044 5367 13601 4036 3680 30492 9486 903 2.00-2.03 nf2-gs128 351 141 60.97 31.69 635 642 71.21 1024 3091 253 2.25 nuq2 8.47 7.29 6.08 9.19 11.20 23.34 4.18 16.63 6.79 6.87 2.00-2.02 + 0.1% outliers 6.82 5.72 4.83 4.00 6.38 5.33 3.54 7.97 4.77 5.83 2.04-2.06 + 0.5% outliers 6.24 5.49 4.45 3.80 5.59 4.95 3.33 6.53 3.18 5.28 2.16-2.19 + 1.0% outliers 6.05 5.39 4.41 3.72 5.47 4.85 3.28 6.29 2.95 5.14 2.32-2.35 Table 18: Full evaluation of our method for different models on all LLaMA, Llama-2, Llama-3, and Mistral models using the perplexity on Wikitext-2. KVQuant results are using pre-RoPE per-channel quantization for Keys. Average bit widths assume a sequence length of 128K (ignoring context length limits for the models). Note that ATOM and FlexGen use 4-bit quantization with group sizes of 128 and 64 with uniform quantization, respectively, and we extend their methods to 3-bit and 2-bit quantization. We leverage Attention-Sink Aware quantization for all bit widths. We used post-RoPE quantization for all baseline methods since it achieves higher accuracy when quantizing Keys per-token as shown in Appendix P. Method LLaMA Llama-2 Llama-3 Mistral-7B Avg. Num. Bits7B 13B 30B 65B 7B 13B 70B 8B 70B baseline 5.68 5.09 4.10 3.53 5.12 4.57 3.12 5.54 2.59 4.76 16 int4 5.98 5.32 4.34 3.73 5.66 5.01 3.31 7.89 14.03 4.97 4.00-4.02 nf4 5.87 5.23 4.25 3.63 5.47 4.90 3.22 6.42 3.86 4.91 4.00-4.03 ATOM-4bit 5.77 5.16 4.16 3.57 5.32 4.71 3.16 5.78 2.73 4.82 4.16 Flexgen-4bit 5.73 5.14 4.14 3.56 5.25 4.66 3.14 5.68 2.66 4.80 4.31 KVQuant-4bit 5.72 5.13 4.13 3.55 5.16 4.60 3.13 5.60 2.62 4.80 4.00-4.02 KVQuant-4bit-1% 5.69 5.10 4.11 3.54 5.13 4.58 3.13 5.56 2.60 4.78 4.32-4.35 int3 10.87 8.69 6.82 6.37 22.71 18.26 7.68 125 158 7.64 3.00-3.02 nf3 7.33 6.21 5.46 4.44 9.96 9.50 4.06 61.07 98.64 6.30 3.00-3.03 ATOM-3bit 6.17 5.47 4.44 3.78 6.15 5.34 3.33 7.50 4.11 5.16 3.15 FlexGen-3bit 5.93 5.29 4.26 3.66 5.64 4.98 3.23 6.38 3.23 5.00 3.30 KVQuant-3bit 5.87 5.25 4.25 3.63 5.34 4.71 3.18 5.84 2.72 4.98 3.00-3.02 KVQuant-3bit-1% 5.75 5.14 4.15 3.57 5.17 4.61 3.15 5.64 2.63 4.82 3.32-3.35 int2 11779 69965 1470 7272 4708 3943 976 2841 2164 573 2.00-2.02 nf2 3210 5786 2044 5367 13601 4036 3680 30492 9486 903 2.00-2.03 ATOM-2bit 37.37 41.77 16.49 13.63 118 93.09 18.31 200 2092 51.96 2.14 FlexGen-2bit 11.09 9.84 6.60 5.54 25.69 26.83 5.93 57.82 43.69 12.47 2.28 KVQuant-2bit 7.23 5.82 4.87 4.03 7.03 9.59 3.45 7.04 3.49 6.84 2.00-2.02 KVQuant-2bit-1% 6.01 5.36 4.35 3.70 5.41 4.78 3.26 6.04 2.82 5.14 2.32-2.35 23Table 19: Full evaluation of our method for different models on all LLaMA, Llama-2, Llama-3, and Mistral models using the perplexity on C4. KVQuant results are using pre-RoPE per-channel quantization for Keys. Average bit widths assume a sequence length of 128K (ignoring context length limits for the models). Note that ATOM and FlexGen use 4-bit quantization with group sizes of 128 and 64 with uniform quantization, respectively, and we extend their methods to 3-bit and 2-bit quantization. We leverage Attention-Sink Aware quantization for all bit widths. We used post-RoPE quantization for all baseline methods since it achieves higher accuracy when quantizing Keys per-token as shown in Appendix P. Method LLaMA Llama-2 Llama-3 Mistral-7B Avg. Num. Bits7B 13B 30B 65B 7B 13B 70B 8B 70B baseline 7.08 6.61 5.98 5.62 6.63 6.05 4.97 7.10 5.78 5.71 16 int4 7.40 6.82 6.18 5.75 7.31 6.59 5.12 8.79 14.36 5.91 4.00-4.02 nf4 7.27 6.74 6.10 5.69 7.09 6.45 5.06 7.93 6.58 5.85 4.00-4.03 ATOM-4bit 7.16 6.67 6.02 5.65 6.87 6.20 5.00 7.32 6.03 5.76 4.16 FlexGen-4bit 7.12 6.64 6.00 5.63 6.79 6.15 4.99 7.23 5.84 5.75 4.31 KVQuant-4bit 7.11 6.64 6.00 5.63 6.68 6.08 4.99 7.18 5.80 5.75 4.00-4.02 KVQuant-4bit-1% 7.09 6.62 5.99 5.62 6.64 6.06 4.98 7.12 5.78 5.72 4.32-4.35 int3 12.97 10.95 9.13 8.27 30.14 28.57 16.00 63.75 301 8.84 3.00-3.02 nf3 8.90 7.84 7.43 6.37 14.92 13.75 5.96 68.38 148 7.27 3.00-3.03 ATOM-3bit 7.62 6.93 6.24 5.79 8.00 7.06 5.16 9.00 6.72 6.08 3.15 FlexGen-3bit 7.34 6.78 6.11 5.70 7.29 6.59 5.08 7.92 6.18 5.92 3.30 KVQuant-3bit 7.25 6.72 6.06 5.68 6.89 6.18 5.03 7.43 5.90 5.92 3.00-3.02 KVQuant-3bit-1% 7.13 6.64 6.00 5.63 6.69 6.09 4.99 7.19 5.81 5.76 3.32-3.35 int2 10892 100870 1411 7216 4708 4220 814 2113 1977 477 2.00-2.02 nf2 2850 4680 1617 5190 13081 4176 3217 78331 7616 1102 2.00-2.03 ATOM-2bit 43.49 56.25 21.07 17.05 113 97.04 23.67 135 3734 50.73 2.14 FlexGen-2bit 13.91 13.36 8.49 7.34 35.21 40.40 8.28 50.78 42.27 13.83 2.28 KVQuant-2bit 8.52 7.32 6.67 5.96 9.49 15.36 5.30 10.06 6.45 7.63 2.00-2.02 KVQuant-2bit-1% 7.33 6.78 6.11 5.70 6.96 6.25 5.08 7.60 5.96 6.05 2.32-2.35 24P Post-RoPE Per-Token Quantization Ablation Table 20 shows perplexity evaluation on Wikitext-2 for the LLaMA-7B model with uniform quan- tization, with Keys quantized pre-RoPE and post-RoPE. These results show that post-RoPE Key quantization is superior to pre-RoPE Key quantization when quantizing Keys per-token. This is because when rotating an outlier channel with large average magnitude and another channel with smaller average magnitude together, at some positions in the sequence, part of the magnitude from the outlier channel will be shifted to the smaller channel. This partially mitigates the impact of the outlier channel on skewing the quantization range for some of the tokens in the sequence. As such, for our baseline comparisons, we use post-RoPE per-token Key quantization to serve as a stronger baseline. Table 20: Model accuracy when using pre-RoPE and post-RoPE quantization for LLaMA-7B with per-token Key quantization. Our experiments demonstrate that post-RoPE quantization is superior when using per-token Key quantization. Therefore, we decided to use these results for baseline comparison with per-token quantization. Datatype Perplexity (Pre-RoPE) Perplexity (Post-RoPE) fp16 5.68 5.68 int4 6.02 5.98 int4-gs128 5.76 5.77 int3 14.68 10.87 int3-gs128 6.28 6.17 Q Experiments on Calibration Data Robustness To evaluate the robustness of our quantization method to the choice of calibration datasets, we measure the perplexity on Wikitext-2 and C4 using different quantized models calibrated with Wikitext-2 and C4. As shown in Table 21, the resulting perplexity numbers remain similar even when the models are calibrated with out-of-domain examples (e.g., calibrated with Wikitext-2 and evaluated on C4, and vice versa), demonstrating the robustness of the calibration process to the choice of data. Table 21: Perplexity (PPL) results on Wikitext-2 and C4 using different quantization schemes, calibrated using Wikitext-2 and C4. Datatype Wikitext-2 C4 Calib. with Wikitext-2 Calib. with C4 Calib. with Wikitext-2 Calib. with C4 4-bit, 1% sparsity 5.69 5.70 7.09 7.09 3-bit, 1% sparsity 5.75 5.75 7.13 7.13 2-bit, 1% sparsity 6.05 6.07 7.38 7.38 R Kernel Implementation Details We implemented 4-bit lookup table-based kernels for matrix-vector multiplication between the Key or Value activations (packed as a lookup table (LUT) plus indices into the LUT per-element) and a full-precision activation vector. These kernels load the compressed Key and Value activations and dequantize them only as needed in order to minimize memory bandwidth utilization. All arithmetic is performed in fp16. The lookup table entries are the values of the sensitivity-weighted non-uniform datatype for that particular layer scaled according to the range of activations that need to be represented [8]. When selecting between the Compressed-Sparse Column (CSC format) and the Compressed-Sparse Row (CSR) format for storing the outliers for the Keys and Values, we needed to consider how easy it would be to append new vectors. When using CSC format for the Key matrix, we only need to append a single element to the column vector, as well as one new element to the row and value vectors per nonzero element in that new column. If we used CSR format, we would need to insert the new column and value elements in the middle of the existing column and value vectors, and we would 25need to recompute the elements of the row vector. When using CSR format for the Value matrix, we only need to append a single element to the row vector, as well as one new element to the column and value vectors per nonzero element in that new row. If we used CSC format, we would need to insert the new row and value elements in the middle of the existing row and value vectors, and we would need to recompute the elements of the column vector. We therefore used the CSC format for the Key matrices and the CSR format for the Value matrices. One challenge with efficiently processing the sparse matrix-dense vector operation is that the sparsity distribution may be unbalanced. This poses a challenge for efficiently processing the sparse matrix on a GPU as there can be different numbers of nonzeros to process per thread. We therefore leverage a balanced sparse matrix-dense vector kernel based on [10, 17], which assigns an equal number of nonzeros per thread. This has greater synchronization overhead than assigning a single thread for an entire row or column when processing CSR/CSC matrices, but it leads to a more balanced work assignment between threads. We set the number of threads such that there were 10 nonzero values assigned to each thread. The dense non-uniform kernel and balanced sparse kernels are launched in one call to avoid overhead from summing the output vectors from these separate operations. A second potential challenge is that the quantization dimension for the Keys and Values is not aligned with the reduction dimension for the respective matrix-vector operations. If we used per-channel or per-token lookup tables for dequantization, we would need to load from a different lookup table for each element as we iterate along the reduction dimension. However, since our implementation uses a single per-layer datatype that is rescaled per-vector, we can load a single scaling factor and zero-point for each channel / token and broadcast it across all threads (while doing a table lookup from the shared per-layer datatype, which is duplicated such that each thread can access it efficiently in parallel). The use of a single per-layer datatype that is rescaled per-vector therefore allows for much more efficient GPU kernel implementations in the context of per-channel Key / per-token Value quantization. Table 22 shows a detailed breakdown of kernel runtime, including how much time is spent packing vectors into the compressed format and how much time is spent on the dense and sparse matrix- vector multiplications. For the fp16 baselines, we benchmarked runtime by averaging across 1000 iterations. When benchmarking our dense-and-sparse kernels (both for packing and matrix-vector multiplication), since the runtime of the sparse kernels can vary based on the sparsity pattern, we ran the LLaMA-2-7B-32K model and collected activations at each layer, and we then measured the average runtime across 1000 iterations for each layer in the model. We find that even with 1% sparsity, we can attain significant speedups of up to 1.7× relative to the fp16 matrix-vector multiply kernels, demonstrating how our methodology facilitates efficient inference with a low-precision quantized KV cache. Table 22: Average latency (in microseconds) for the Key and Value dense nuq4 kernels as well as for the sparse kernels (with 1% outliers), benchmarked on an A6000 GPU for the LLaMA-2-7B-32K model. Benchmarking results are reported for different sequence lengths ( l). fp16 matrix-vector multiplication latencies are included for reference, and the Key multiplication time also includes the time to apply RoPE to the newly appended Key vector. We find that our dense-and-sparse approach (even with 1% outliers) provides latency benefits relative to the fp16 baseline, even when accounting for the time to compress activations online. Activation Operation l=2K l=4K l=16K Key fp16 Matvec 33.3 59.1 219.4 Key (nuq4-1%) Packing 4.5 4.5 4.5 Dense Matvec 13.0 24.1 87.6 Sparse Matvec 8.1 11.2 34.2 Total Latency 25.6 39.9 126.3 Value fp16 Matvec 26.0 50.2 203.7 Value (nuq4-1%) Packing 4.1 4.1 4.1 Dense Matvec 10.0 17.8 62.0 Sparse Matvec 7.9 15.9 58.2 Total Latency 22.1 37.9 124.5 26S Limitations While our work enables accurate long-context length inference by reducing the memory requirements, there is significant work required for training long context length models with greater than 100K context length. This work is orthogonal to our efforts, which are constrained to efficient inference with long context length models. Additionally, our latency benchmarking results currently focus on memory-bandwidth bound generation rather than prompt processing (where we need to compress multiple Keys and Values at once). Finally, in the current end-to-end implementation, there are inefficiencies in how memory allocation is handled for updating the sparse matrix (where the data corresponding to the previous tokens have to be copied when concatenating them with the data from the new token). In future work, we plan to optimize this by doing blocked allocation to avoid overheads from reallocating memory. 27",
      "meta_data": {
        "arxiv_id": "2401.18079v6",
        "authors": [
          "Coleman Hooper",
          "Sehoon Kim",
          "Hiva Mohammadzadeh",
          "Michael W. Mahoney",
          "Yakun Sophia Shao",
          "Kurt Keutzer",
          "Amir Gholami"
        ],
        "published_date": "2024-01-31T18:58:14Z",
        "pdf_url": "https://arxiv.org/pdf/2401.18079v6.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "KVQuant addresses the memory consumption bottleneck of KV cache activations in LLM inference for large context windows, where existing solutions fail at sub-4-bit precision. The main contributions include achieving <0.1 perplexity degradation with 3-bit quantization across LLaMA, Llama-2, Llama-3, and Mistral models on Wikitext-2 and C4 datasets, outperforming existing methods. It enables serving LLaMA-7B with a context length of up to 1 million on a single A100-80GB GPU and up to 10 million on an 8-GPU system. Furthermore, custom CUDA kernels deliver up to ~1.7x speedups compared to fp16 baseline matrix-vector multiplications for LLaMA-7B.",
        "methodology": "KVQuant integrates several novel methods: (i) Per-Channel Key Quantization before rotary positional embedding (RoPE) to better match Key activation distributions and mitigate RoPE's impact. (ii) Non-Uniform KV Cache Quantization (nuqX) that derives per-layer sensitivity-weighted non-uniform datatypes offline on calibration data using a k-means solver and Fisher information matrix. (iii) Per-Vector Dense-and-Sparse Quantization, which isolates a small percentage of numerical outliers per-vector (per-channel for Keys, per-token for Values) into a separate sparse representation, allowing the remaining dense elements to be represented with greater precision. (iv) Attention Sink-Aware Quantization, which retains the first token in fp16 due to its disproportionate sensitivity. Key scaling factors are calibrated offline for Keys (per-channel) and computed online for Values (per-token), with outlier thresholds for Values also computed online. Custom CUDA kernels are implemented for efficient on-the-fly 4-bit quantization, including fused kernels for applying RoPE post-dequantization and balanced sparse matrix-dense vector operations.",
        "experimental_setup": "The method was evaluated on LLaMA-7B/13B/30B/65B, Llama-2-7B/13B/70B, Llama-3-8B/70B, and Mistral-7B models. Perplexity was measured on Wikitext-2 and C4 datasets. Long context length performance was assessed using LLaMA-2-7B-32K and Llama-2-70B-32K LongLoRA models through perplexity on Wikitext-2, passkey retrieval, LongBench (QA, summarization, few-shot learning), and RULER benchmarks. Calibration was performed using 16 samples of 2K sequence length from the Wikitext-2 training set. Comparisons were made against uniform (intX) and non-uniform (nfX) quantization, Atom, FlexGen, and KIVI. Kernel benchmarking was conducted on an A6000 GPU, while memory estimates considered an A100-80GB GPU. Joint weight and KV cache quantization experiments utilized SqueezeLLM for weight quantization.",
        "limitations": "The work focuses on efficient inference and does not address the significant work required for training long context length models greater than 100K. Latency benchmarking is currently focused on memory-bandwidth bound generation, not prompt processing where multiple Keys and Values need to be compressed simultaneously. Furthermore, the current end-to-end implementation has inefficiencies in memory allocation for updating the sparse matrix, requiring data corresponding to previous tokens to be copied when concatenating with new token data.",
        "future_research_directions": "Future work plans to optimize memory allocation for updating the sparse matrix by implementing blocked allocation to avoid overheads from reallocating memory. Implicitly, further research could address the challenges of training long context models and improving prompt processing latency."
      }
    },
    {
      "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache",
      "abstract": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI.",
      "full_text": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Zirui Liu * 1 Jiayi Yuan* 1 Hongye Jin 2 Shaochen (Henry) Zhong 1 Zhaozhuo Xu 3 Vladimir Braverman 1 Beidi Chen 4 Xia Hu 1 Abstract Efficiently serving large language models (LLMs) requires batching of many requests to reduce the cost per request. Yet, with larger batch sizes and longer context lengths, the key-value (KV) cache, which stores attention keys and values to avoid re- computations, significantly increases memory de- mands and becomes the new bottleneck in speed and memory usage. Additionally, the loading of the KV cache causes the computational core to be idle, which limits the inference speed. A straight- forward and effective solution to reduce KV cache size is quantization, which decreases the total bytes taken by KV cache. However, there is a lack of in-depth studies that explore the element dis- tribution of KV cache to understand the hardness and limitation of KV cache quantization. To fill the gap, we conducted a comprehensive study on the element distribution in KV cache of popular LLMs. Our findings indicate that the key cache should be quantized per-channel, i.e., group ele- ments along the channel dimension and quantize them together. In contrast, the value cache should be quantized per-token. From this analysis, we de- veloped a tuning-free 2bit KV cache quantization algorithm named KIVI. With hardware-friendly implementation, KIVI can enable Llama, Falcon, and Mistral models to maintain almost the same quality while using 2.6× less peak memory (in- cluding model weight). This reduction in memory usage enables up to 4× larger batch size, bringing 2.35× ∼3.47× throughput on real LLM infer- ence workload. The source code is available at https://github.com/jy-yuan/KIVI. *Equal contribution . The order of authors is determined by flipping a coin. 1Rice University 2Texas A&M University 3Stevens Institute of Technology 4Carnegie Mellon University. Correspondence to: Zirui Liu <zl105@rice.edu>, Jiayi Yuan <jy101@rice.edu>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). 1. Introduction Large Language Models (LLMs) have demonstrated strong performance across a wide range of tasks (Brown et al., 2020; Taylor et al., 2022; Yuan et al., 2023; Chuang et al., 2024). However, their deployment is very costly, requiring a large number of hardware accelerators such as GPUs. Given these substantial costs, one natural way to reduce the cost per request is to combine a sufficient number of requests together for batch processing. However, in this batch infer- ence scenario, the key-value cache (KV cache), which holds the attention keys and values during generation to prevent re-computations, is becoming the new memory and speed bottleneck. This bottleneck becomes more pronounced with larger batch sizes and longer context lengths. For instance, in 540B PaLM, with a batch size of 512 and a context length of 2048, KV cache alone can take 3TB. This is 3 times the size of the model’s parameters (Pope et al., 2023). Also, the GPU SRAM has to load the whole KV cache from the GPU device memory for every token generated, during which the computational cores are idle. Thus, reducing KV cache size in LLMs while maintaining accuracy is important. Existing works towards this problem can be roughly divided into three categories. First, some work suggests reducing the number of heads in KV cache, such as multi-query attention (Shazeer, 2019) and multi-group attention (Ainslie et al., 2023). However, these methods require either training the model from scratch or fine-tuning the existing model. Second, another research line reduces KV cache size by evicting unimportant tokens (Zhang et al., 2023). Third, some other works try to solve this problem from the system perspective, e.g., offloading KV cache (Sheng et al., 2023) or extending virtual memory and paging techniques into the attention mechanism (Kwon et al., 2023). To reduce the size of KV cache, the most simple and ef- fective way is to reduce the total bytes taken by KV cache, namely, quantization. Unlike the well-studied weight quan- tization (Lin et al., 2023; Xiao et al., 2023a; Zhao et al., 2024), to the best of our knowledge, only a few studies applied the vanilla 4bit round-to-nearest quantization to KV cache (Sheng et al., 2023; Zhang et al., 2023; Zhao et al., 2024) due to the streaming nature of KV cache or other complications. There is a lack of in-depth studies that ex- 1 arXiv:2402.02750v2  [cs.CL]  25 Jul 2024KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache per-token  quantization  per-channel  quantization  Figure 1: Definition of per-token and per-channel quantiza- tion. X ∈ Rlprompt×d is key/value cache where lprompt is the number of tokens and d is the number of channels. zX is the zero-point, sX is the scaling factor. plore the element distribution of KV cache to understand the hardness and limitation of KV cache quantization. To fill the gap, we study the element distribution of KV cache. Our analysis suggests: • For key cache, there are a few fixed channels whose magnitudes are very large, which is consistent with previous finding (Lin et al., 2023; Xiao et al., 2023a). Thus, as shown in Figure 1 right, key cache should be quantized per-channel, i.e., group elements along the channel dimension and quantize them together. In this way, it can confine the error to each individual channel, without impacting the other normal channels. • For value cache, there is no obvious outlier pattern. Al- though value cache has no obvious outlier pattern, we experimentally show that it can only be quantized per- token because it is used to calculate the attention output, which is essentially a value cache mixer. As shown in Figure 1 left, the per-token quantization can confine the error inside each individual token and ensure that the quantization of one token does not adversely impact the others. Based on the above insights, we propose KIVI, a plug-and- play extreme low-bit KV cache quantization method. KIVI quantizes key cache per-channel and quantizes value cache per-token. The per-token value cache quantization aligns well with the streaming nature of auto-regressive inference, allowing newly quantized tensors to be directly appended to the existing quantized value cache by token dimension. However, for per-channel key cache quantization, the quan- tization process spans different tokens, which cannot be directly implemented in this streaming setting. Since the number of tokens in key cache can be arbitrary, our key idea is to split key cache into two parts. The first part is the grouped key cache, which contains several groups of tokens and each group has a certain number of tokens. The second part is the residual key cache, which does not have a suffi- cient number of tokens to form a complete group. Similarly, we split value cache into the grouped and residual parts to maintain the accuracy. We only apply group-wise quanti- zation to the grouped key cache and value cache, while the residual key cache and value cache are kept in full precision. The grouped and residual parts can be combined using tiled matrix multiplication when computing attention scores. Our contributions are summarized as follows: • Extensive analysis regarding the outlier patterns and quantization error of KV cache in commonly- used LLMs. Our observations suggest that key cache should be quantized per-channel and value cache should be quantized per-token. We also explain in depth why these caches require different quantization approaches. • A new plug-and-play 2bit KV cache quantization algorithm without any fine-tuning, KIVI, with hardware-friendly implementation. We conduct an extensive evaluation for KIVI with Llama, Mistral, and Falcon on popular generation tasks. KIVI can efficiently compress KV cache to 2bit and bring 2.6× peak memory usage reduction for Llama-2-7B, with little to no accuracy drop. With our efficient system im- plementation, this memory reduction, in return, enables up to 4× larger batch size and brings2.35× ∼3.47× throughput. 2. Background: Attention Inference-Time Workflow The LLM attention inference-time workflow involves two phases: i) the prefill phase, where the input prompt is used to generate KV cache for each transformer layer of LLMs; and ii) the decoding phase, where the model uses and updates KV cache to generate the next token, one at a time. Prefill Phase. Let X ∈ Rb×lprompt×d be the input tensor, where b is the batch size, lprompt is the length of the input prompt, and d is the model hidden size. For convenience, we ignore the layer index here. The key, value tensors can be computed by XK = XWK, XV = XWV , where WK, WV ∈ Rd×d are the key and value layer weight, respectively. After obtaining XK and XV , they are cached in the memory for the ease of decoding. Decoding Phase. Let t ∈ Rb×1×d be the current input token embedding. Let tK = tWK and tV = tWV be the key and value layer output, respectively. We first update KV cache: XK ← Concat(XK, tK), XV ← Concat(XV , tV ), 2KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache then calculate the attention output as: tQ = tWQ, A = Softmax(tQX⊤ K), tO = AXV , (1) where WQ is the weight matrix of the query layer. For ease of illustration, we ignore the attention output layer and the other parts of the inference workflow. Memory and Speed Analysis. The above process is re- peated until a special token indicating the sentence’s con- clusion is reached. Let lgen be the number of generated tokens. From the above analysis, the shape of KV cache is b ×(lprompt + lgen) ×d. To get a sense of the scale, consider the OPT-175B model with a batch size b 512, a prompt length lprompt 512, and an output length lgen 32. The KV cache requires 1.2TB, which is 3.8 times the model weights (Sheng et al., 2023). Besides the memory, the inference speed is also decided by the KV cache size. The GPU needs to load KV cache from GPU main memory to GPU SRAM once for every token generated during which the computa- tional core of the chip is essentially idle (Pope et al., 2023; Kwon et al., 2023). 3. Methodology In scenarios with long contexts or batched inferences, the memory and speed bottlenecks are storing and loading KV cache. The most simple and effective way to alleviate this problem is to reduce the total bytes occupied by KV cache, specifically, quantization. Following this motivation, we first evaluate the performance of the existing quantization method in Section 3.1. Our observations suggest that key and value cache should be quantized along different dimen- sions. We analyze the rationale behind this observation in Section 3.2. Then based on the analysis, we propose KIVI, a new KV cache quantization method along with its streaming data structure, detailed in Section 3.3. 3.1. Preliminary Study of KV Cache Quantization As we analyzed in Section 2, KV cache functions as a streaming data structure, where the new tensor arrives se- quentially. Thus, optimization-based methods like GPTQ (Frantar et al., 2022) are unsuitable for quantizing KV cache due to the overhead. To the best of our knowledge, the most flexible way for quantizing KV cache is the round- to-nearest quantization. The B−bit integer quantization- dequantization process can be expressed as: Q(X) = ⌊X − zX sX ⌉, X′ = Q(X) · sX + zX, where zX = min X is the zero-point, sX = (max X − min X)/(2B −1) is the scaling factor, and ⌊·⌉ is the round- ing operation. Here we ignore the batch size for ease of Table 1: The results of simulated KV cache group-wise quantization with various configurations. The group size is set as 32. C stands for per-channel quantization and T stands for per-token quantization. Please check the whole evaluation in Table 3. Llama-2-13B CoQA TruthfulQA 16bit 66.37 29.53 4bit (K - T, V - T) 66.48 29.51 2bit (K - T, V - T) 52.93 24.98 2bit (K - C, V - C) 2.88 0.74 2bit (K - T, V - C) 2.80 0.26 2bit (K - C, V - T) 63.53 28.60 understanding. As shown in Figure 1, X is quantized along either the token or channel dimension group-wisely. Considering the streaming nature of KV cache, previous studies often apply per-token quantization to both key and value cache since the newly quantized KV cache can be naively added to the existing quantized one along the token dimension (Sheng et al., 2023). While per-channel quanti- zation is non-trivial, we have designed a padding method to implement per-channel quantization to explore its effect on both key and value cache. Setting. In Table 1, we show the results of fake KV cache group-wise quantization with different configurations on the Llama-2-13B model for the CoQA and TruthfulQA tasks. We use a group size of 32 for all configurations. Here fake quantization means we simulate the quantization process by first quantizing KV cache into lower precision and then dequantizing it in the attention layer. For per-channel quan- tization, if the number of tokens is not divided evenly into groups, we add zero-padding to ensure it can be grouped perfectly. In this way, we ensure that all tokens in KV cache are quantized for a fair comparison. The detailed experi- mental setting can be found in Section 4.1. Specifically, we observe that: OB 1. When using the commonly used per-token quanti- zation to both key and value caches, INT4 precision can maintain accuracy. However, reducing it to INT2 results in a notable accuracy drop. OB 2. When value cache is quantized per-channel, the accuracy significantly worsens regardless of how key cache is quantized. OB 3. When using a lower numerical precision such as INT2, the most accurate approach is to quantize key cache per-channel and value cache per-token. 3KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache 0 50 100 150 200 Token 0 25 50 75 100 125 Channel 0.0 2.5 5.0 7.5 10.0 12.5Absolute Value Llama-2-13B Layer 16 Head 0 Key Cache 0 50 100 150 200 Token 0 25 50 75 100 125 Channel 0.5 1.0 1.5 2.0Absolute Value Llama-2-13B Layer 16 Head 0 Value Cache 0 50 100 150 200 Token 0 25 50 75 100 125 Channel 0.0 2.5 5.0 7.5 10.0 12.5 Llama-2-13B Layer 31 Head 0 Key Cache 0 50 100 150 200 Token 0 25 50 75 100 125 Channel 0.5 1.0 1.5 2.0 2.5 Llama-2-13B Layer 31 Head 0 Value Cache 0 50 100 150 200 Token 0 20 40 60 Channel 0 2 4 6 8 10Absolute Value Falcon-7B Layer 16 Head 0 Key Cache 0 50 100 150 200 Token 0 20 40 60 Channel 0.5 1.0 1.5Absolute Value Falcon-7B Layer 16 Head 0 Value Cache 0 50 100 150 200 Token 0 20 40 60 Channel 0.0 2.5 5.0 7.5 10.0 Falcon-7B Layer 20 Head 0 Key Cache 0 50 100 150 200 Token 0 20 40 60 Channel 0.5 1.0 1.5 Falcon-7B Layer 20 Head 0 Value Cache Figure 2: Magnitude of key and value cache for Llama-2-13B and Falcon-7B. We observe (1) for key cache, there are a few channels whose magnitudes are very large. (2) for value cache, there is no obvious outlier pattern. 3.2. Why Key and Value Cache Should Quantize Along Different Dimensions? In Table 1, we observe that quantizing key cache per-channel and value cache per-token to 2bit results in a very small accu- racy drop. Here we analyze why this configuration delivers better accuracy. In Figure 2 we visualize the original KV cache distribution at different layers. We observe that in key cache, some fixed channels exhibit very large mag- nitudes, whereas in value cache, there is no significant pattern for outliers. Analysis of Key Cache. The above observation for key cache aligns with previous findings that certain fixed columns in activations exhibit larger outliers (Dettmers et al., 2022; Lin et al., 2023). The persistence of outliers within each channel means that per-channel quantization can con- fine the quantization error to each individual channel with- out impacting the other normal channels. Thus, Figure 2 explains why key cache should be quantized per-channel. In Table 2 we show key cache relative reconstruction error ∥XK−X′ K XK ∥F , along with the relative attention score error ∥A−A′ A ∥F where A′ = Softmax(tQX ′⊤ K ). We observe that the per-token quantization can lead to almost 5× larger at- tention score error than per-channel quantization, which is consistent with Figure 2. Table 2: The relative error statistics averaged over all layers and all heads Llama-2-13B K Per-Token K Per-Channel Avg. ∥ XK−X′ K XK ∥F 13.67 4.55 Avg. ∥ A−A′ A ∥F 47.00 9.60 Attention sparsity 84.3% V Per-Token V Per-Channel Avg. ∥ XV −X′ V XV ∥F 4.57 3.73 Avg. ∆ 3.55 49.89 Analysis of Value Cache. Unlike key cache, value cache does not show the channel-wise outlier pattern. Furthermore, Figure 2 alone cannot explain OB2, which indicates value cache should only be quantized per-token. This is because Figure 2 implies that errors should be comparable for both per-token and per-channel quantization, given the absence of a clear pattern. As shown in Equation (1), value cache is used to calculate the attention output tO. Instead of analyzing the quantization error of value cache XV , in Table 2 we analyze the relative error ∆ = ∥AXV −AX′ V AXV ∥F with different quantization configurations. Surprisingly, we observe that the per-token quantization error is almost 15× 4KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Q_MatMul Concat Quantization by Channel Quantization by Token MatMul Prefill Phase Decoding Phase* KVCache * we omit the value cache and  attention output calculation Full Precision Tensor Low Precision Tensor Figure 3: The overview of KIVI algorithm. For ease of illustration, we omit the value cache and attention output parts. The detailed pseudo-code is provided in Algorithm 1. Here “Q_Matmul” is the mix-precision matrix multiplication which fuses the dequantization with matrix multiplication at the tiling level. smaller than per-channel quantization, which explains why OB2 happens. The intuition behind this observation stems from the attention sparsity. Equation (1) can be written as: [AXV ]i∗ = lpromptX j=1 Aij[XV ]j∗, (2) where [XV ]j∗ is the j-th row of XV . From Equation (2), the attention output is the weighted summation of value cache across various tokens, with the weights being the attention scores. Since the attention score is highly sparse (Tian et al., 2023), the output is just the combination of value caches of a few important tokens. The per-token quantization can confine the error to each individual token. Thus, quantizing other tokens does not affect the accuracy of important tokens. Consequently, per-token quantization leads to a much smaller relative error ∆. 3.3. KIVI: Algorithm and System Support Algorithm. As we previously analyzed, key cache should be quantized per-channel and value cache should be quan- tized per-token. Recall that key and value cache of newly generated tokens arrive sequentially. From the implemen- tation perspective, per-token quantization aligns well with streaming settings, allowing newly quantized tensors to be directly appended to the existing quantized value cache by token dimension. However, for per-channel quantiza- tion, the quantization process spans across different tokens, which cannot be directly implemented in the streaming set- ting. As shown in Figure 3, our key idea to solve this prob- lem is to group key cache everyG tokens and quantize them separately. Because the number of tokens in XK can be ar- bitrary, we splitXK into two parts, namely, the grouped part XKg = XK[: l − r] and residual part XKr = XK[l − r :], where l is the number of tokens inside the current key cache XK, r is the number of residual tokens, where l − r can be divisible by G. Since XKg can be evenly divided into (l − r)/G groups, we only store Q(XKg ) with group-wise quantization, while XKr is kept in full precision. During the decoding process, each newly arrived key cache tK is added to XKr and once XKr reaches R tokens, which is a hyperparameter - residual length, we quantize and concatenate it with the previously quantized Q(XKG). Then we reset XKr to an empty tensor. We note that R should be divisible by G. With tiled matrix multiplication, the raw attention logits is then calculated as: Ag = tQQ(X⊤ Kg ), XKr = Concat([XKr , tK]), Ar = tQX⊤ Kr , A = Concat([Ag, Ar]). (3) For value cache, similar to key cache, we also split it into two parts and keep the most recent value cache in full pre- cision, namely, XVg and XVr . Specifically, we maintain a queue and each newly arrived value cache is pushed into the queue. Once the queue reaches the predefined residual length R, the most outdated value cache is poped. Then the poped value cache is quantized per-token and concatenated with the previously quantized value cache along the token dimension. As shown in Figure 3, we also emphasize that during the prefill phase, the exact key and value tensors are passed to the next layers, although only the quantized KV cache is retained in memory. The whole algorithm can be found in Appendix A Algorithm 1. Analysis. In KIVI, the grouped key cacheXKg and value cache XVg is quantized, while the residual key cache XKr and value cache XVr is kept in full precision. By design, there are at most R tokens inside XKr or XVr . In practice, we set R ≤ 128 and the sequence length lprompt + lgen is often much longer than R. Thus the memory overhead from XKr and XVr is negligible when considering the benefit from extreme low-bit quantization, especially for the long context scenarios. Also, since the newly arrived key and 5KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache value tensors are added to XKr and XVr in full precision, KIVI maintains a full precision KV cache sliding window for the local relevant tokens. This window size is expected to be R 2 for key cache, and R for value cache. Later in the experiment section, we show that this full precision sliding window is crucial for obtaining desirable perfor- mance on hard tasks, such as GSM8K. System Support. We provide a hardware-friendly imple- mentation for running KIVI on GPUs. To minimize the overhead, we have fused the dequantization process with matrix multiplication, e.g., Q_MatMul in Figure 3, using CUDA. We also implement the group-wise quantization ker- nel in Triton. Our method is fully compatible with weight- only quantization. 4. Experiments 4.1. Settings Models. We evaluate KIVI using three popular model families: Llama/Llama-2 (Touvron et al., 2023a;b), Fal- con (Penedo et al., 2023) and Mistral (Jiang et al., 2023). Llama and Mistral model is based on multi-head attention, while Falcon is based on multi-query attention (Shazeer, 2019). We use the Hugging Face Transformers codebase and implement the KIVI algorithm upon it. Following previous work (Sheng et al., 2023), the group size G in Algorithm 1 for quantization is set as 32 across all experi- ments, the residual length R for key and value cache is set to 128. Tasks. As we analyzed in Section 2, the KV cache size grows larger with a longer context. Thus, we evaluateKIVI under the normal context length and long context setting, respectively. Specifically, we adopt generation tasks from LM-Eval (Gao et al., 2021) for normal context length eval- uation and LongBench (Bai et al., 2023) for long context evaluation, respectively1. For LM-eval, we adopt CoQA (Exact match accuracy), TruthfulQA (BLEU score), and GSM8K (Exact match accuracy). For LongBench, we chose tasks from four subgroups. Specifically, Qasper (F1 score) is a Single-Document QA task; QMSum (ROUGE score) and MultiNews (ROUGE score) are Summarization tasks; TREC (classification score), TriviaQA (F1 score), and SAM- Sum (ROUGE score) are Few-shot Learning tasks; and LCC (similarity score) and RepoBench-P (similarity score) is Code Completion task. The maximum sequence length in LongBench was set to 8192 for the Mistral model and 4096 for other models. We also consider the needle-in-a-haystack 1The closed-end tasks such as MMLU are not ideal to evaluate KIVI since they only involve one decoding step and directly fetch the output logits, which is not suitable for studying the impact of compressed KV cache. task (NIAH) to evaluate the model’s long context retrieval ability after quantizing KV cache. Detailed NIAH setting can be found in Appendix B. 4.2. Accuracy and Efficiency Analysis 4.2.1. C OMPARISON BETWEEN DIFFERENT QUANTIZATION CONFIGURATIONS We first utilize the fake quantization to demonstrate the effec- tiveness of our asymmetric quantization, namely, quantizing key cache per-channel and value cache per-token. Here fake quantization is exactly the same as in Table 1. The results are shown in Table 3. We observe that “2bit (K per-channel, V per-token)” consistently achieves the best results com- pared to all other configurations. This is consistent with our previous analysis. We also note that for hard generation tasks such as GSM8K, the fake “2bit (K per-channel, V per-token)” quantization results are significantly worse than the full precision counterparts. However, for KIVI in Table 3, we observe that the accuracy drop is only around 2% for GSM8K across different models. As we analyzed in Section 3.3, the difference between fake “2bit (K per-channel, V per-token)” quantization and KIVI is that KIVI maintains a full precision key and value cache sliding window for the local relevant tokens. This sliding window is crucial to maintaining accuracy for hard generation tasks such as mathematical reasoning. 4.2.2. A CCURACY COMPARISON ON GENERATION TASKS LM-Eval Results. We benchmark KIVI in CoQA, Truth- fulQA and GSM8K tasks using the LM-Eval framework. All dataset parameters were set to default. We compare the standard 16bit configuration with our KIVI compression techniques in Llama-2-7B, Llama-2-13B, Falcon-7B and Mistral-7B. As shown in Table 3, we observe that for the Llama and Mistral model, KIVI only has up to 2% accu- racy drop despite the KV cache being stored in 2bit. For instance, in the Llama-2-7B model, the transition from 16bit to 2bit only slightly decreases accuracy. Similar trends are observed in other Llama-family models. Since Falcon-7B adopts multi-query attention and only has one head for KV cache, it is already highly compressed compared to Llama- based models. Thus, in Table 3, 4bit KIVI is needed to maintain the accuracy, while 2bit KIVI may have a large accuracy drop in this case. LongBench Results. The performance of KIVI over vari- ous models in the LongBench dataset is summarised in Table 4. We apply KIVI to Llama2-7B, Llama2-13B, Llama2-7B- Chat, Llama2-13B-Chat, Falcon-7B and Mistral-7B. Table 4 suggests that KIVI is an effective method for KV cache compression with minimal impact on accuracy across var- 6KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  27K tokens (a) Llama-3-8B-Instruct Baseline 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  27K tokens  (b) Llama-3-8B-Instruct + KIVI-2 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  27K tokens  (c) Llama-3-8B-Instruct + KIVI-4 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  30K tokens (d) Mistral-7B-Instruct-v0.2 Baseline 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  30K tokens  (e) Mistral-7B-Instruct-v0.2 + KIVI-2 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  30K tokens  (f) Mistral-7B-Instruct-v0.2 + KIVI-4 0.0 0.2 0.4 0.6 0.8 1.0 Figure 4: Needle-in-a-Haystack results on Llama-3-8B-Instruct and Mistral-7B-Instruct-v0.2. Here we count the number of words instead of tokens to better account for the tokenizer difference. The final token length is noted in the upper right corners of each figure. Detailed setting can be found in Appendix B. ious hard long context generation tasks. We present ad- ditional results using Llama3-8b, Mistral-7B-v0.2, and LongChat-7B-v1.5, which can be found in Appendix D. NIAH Results. From Figure 4, we observe that KIVI can still maintain the retrieval ability of LLMs even with 2bit KV Cache. Detailed NIAH setting can be found in Appendix B. 4.2.3. A BLATION In this section, we benchmark KIVI on GSM8K, one of the hardest generation tasks, to show the effect of hyperpa- rameters group size G and residual length R on the model performance. For full results of KIVI with a residual length of 32, please refer to Appendix C. The effect of group size. We fix the residual length at 128 and vary the group sizes to 32, 64, and 128. From Table 5, we observe that group sizes 32 and 64 yield similar results, whereas the performance significantly decreases when the group size reaches 128. Note the zero-point and the scaling factor mentioned in Section 3.1 are calculated according to this group size; where the choice of group size will greatly impact the KV cache compression effect under a long input. The effect of residual length. We fix the group size at 32 and vary the residual length across 32, 64, 96, and 128. As shown in Table 5, there is no consistent pattern between residual lengths and model accuracy. Namely, while a resid- ual length of 128 achieves good results, 32 and 96 yield similar outcomes, but a residual length of 64 results in the worst performance. We emphasize that while we observe no significance among residual lengths of {32, 96, 128}, hav- ing a reasonably large residual length is important; as it brings much performance boosts on hard tasks like GSM8K, again as shown in Table 5. 4.2.4. E FFICIENCY COMPARISON To evaluate the wall-clock time efficiency ofKIVI, follow- ing vLLM (Kwon et al., 2023), we synthesize workloads based on ShareGPT (sha, 2023), which contain input and output texts of real LLM services. On average, the data set has an input prompt length lprompt of 161 and an output length lgen of 338 (Kwon et al., 2023). We increase the batch 7KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Table 3: Performance comparison between 16bit, 4-bit per- token quantization, four fake 2bit KV cache quantization similar to those in Table 1, KIVI-2 (2bit) / KIVI-4 (4bit) across various models. We emphasize that unlike KIVI, which preserves a small portion of full precision key cache XKr and value cache XVr , all tokens in fake KV cache quantization are quantized for a fair comparison. C stands for per-channel quantization and T stands for per-token quantization. Model CoQA TruthfulQA GSM8K Llama-2-7B 16bit 63.88 30.76 13.50 4bit (K -T, V -T) 64.82 29.85 12.28 2bit (K -C, V -T) 59.08 33.10 5.76 2bit (K -T, V -T) 39.88 18.29 0.83 2bit (K -C, V -C) 3.60 0.27 0.00 2bit (K -T, V -C) 1.30 0.49 0.08 KIVI-4 63.78 30.80 13.80 KIVI-2 63.05 33.95 12.74 Llama-2-13B 16bit 66.37 29.53 22.67 4bit (K -T, V -T) 66.73 29.14 20.92 2bit (K -C, V -T) 63.53 28.60 12.21 2bit (K -T, V -T) 52.93 24.98 4.55 2bit (K -C, V -C) 2.88 0.74 0.00 2bit (K -T, V -C) 2.80 0.26 0.08 KIVI-4 66.38 29.49 23.65 KIVI-2 66.23 29.84 20.77 Falcon-7B 16bit 59.83 23.20 4.55 4bit (K -T, V -T) 58.53 22.94 3.26 2bit (K -C, V -T) 43.93 20.82 1.29 2bit (K -T, V -T) 25.72 0.91 0.53 2bit (K -C, V -C) 41.95 17.11 1.52 2bit (K -T, V -C) 19.53 0.94 0.15 KIVI-4 59.67 22.58 4.47 KIVI-2 57.48 24.98 3.41 Mistral-7B 16bit 67.40 30.45 38.36 4bit (K -T, V -T) 67.80 29.83 36.85 2bit (K -C, V -T) 61.65 29.64 26.46 2bit (K -T, V -T) 54.55 25.86 5.00 2bit (K -C, V -C) 24.40 24.86 2.27 2bit (K -T, V -C) 10.73 19.12 0.99 KIVI-4 66.95 30.49 37.30 KIVI-2 66.35 32.17 36.01 size until out of memory and report the peak memory usage and throughput between KIVI (with residual length 32 and 128) and FP16 baseline for the Llama-2-7B model. The hardware here is a single NVIDIA A100 GPU (80GB). As shown in Figure 5, with similar maximum memory us- age, KIVI enables up to 4× larger batch size and gives 2.35× ∼3.47× larger throughput. This throughput num- ber can grow larger with longer context length and output length. We also note that this speed-up can be greatly increased if we further fuse the KV cache quantization process with previous operations. We leave it as one of future work. Figure 5: Memory usage and throughput comparison be- tween 2bit KIVI and 16bit baseline. KIVI can achieve higher throughput by enabling a larger batch size. 5. Related Work Many machine learning systems and benchmark works con- sider scaling up LLM inference process (Pope et al., 2023; Yuan et al., 2024). Among them, quantization techniques have been widely applied (Frantar et al., 2022; Lin et al., 2023; Kim et al., 2023; Xu et al., 2023). A main branch of LLM quantization is weight-only quantization, which in- volves the quantization of model weights to lower precision. For instance, AWQ (Lin et al., 2023) cleverly quantizes model weights to INT4 and INT3 using an activation-aware manner. GPTQ (Frantar et al., 2022) utilizes approximate second-order information to quantize model weights both accurately and efficiently. SqueezeLLM (Kim et al., 2023) adopts the concept of non-uniform quantization based on sensitivity along with dense-and-sparse decomposition. This line of work is orthogonal to ours, as they can be combined. SmoothQuant (Xiao et al., 2023a) is a post-training quan- tization method that is more closely related to our work. This method uses equivalent transformations to balance the quantization complexity for both activation and weight, making the activation easier to quantize. SmoothQuant can compress KV cache to 8bit with minor performance loss. However, it faces a significant accuracy drop when scaled down to 4bit or less (Zhao et al., 2024). FlexGen (Sheng et al., 2023) adopts 4-bit group-wise quantization for both key and value cache. 8KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Table 4: Performance evaluation of KIVI on various models across a range of benchmarks in LongBench. We highlight the average performance of our method. More similar results on Mistral-7B-v0.2 and LongChat-7b-v1.5 can be found in Table 10 and Table 9 Model Qasper QMSum MultiNews TREC TriviaQA SAMSum LCC RepoBench-P Average Llama2-7B 16bit 9.52 21.28 3.51 66.00 87.72 41.69 66.66 59.82 44.52 KIVI-4 9.28 21.42 3.88 66.00 87.72 41.82 66.80 59.83 44.59 KIVI-2 9.31 20.50 1.14 66.00 87.42 42.71 66.88 60.23 44.27 Llama2-13B 16bit 9.32 21.38 3.71 70.00 87.87 43.55 66.61 56.42 44.85 KIVI-4 9.16 20.86 3.21 69.00 86.97 44.26 65.30 57.08 44.48 KIVI-2 8.58 20.69 6.19 69.50 87.78 44.30 65.08 55.46 44.69 Llama2-7B-Chat 16bit 19.65 20.54 26.36 63.00 84.28 41.12 59.75 52.93 45.95 KIVI-4 19.62 20.70 25.49 63.00 84.13 40.87 59.27 53.56 45.83 KIVI-2 19.32 20.46 25.48 63.00 84.84 40.60 58.71 52.97 45.67 Llama2-13B-Chat 16bit 24.18 20.37 25.69 67.50 86.90 42.18 50.23 50.64 45.96 KIVI-4 23.00 20.36 26.06 67.50 87.20 42.04 52.55 52.77 46.44 KIVI-2 23.59 20.76 25.25 67.5 87.17 41.56 49.93 48.45 45.52 Falcon-7B 16bit 1.48 2.35 11.09 13.00 5.84 2.44 23.86 9.69 8.71 KIVI-4 1.04 2.41 11.98 13.00 5.84 2.36 23.72 9.92 8.78 KIVI-2 1.98 3.61 6.78 10.00 6.24 2.73 22.18 10.12 7.95 Mistral-7B 16bit 8.12 19.98 19.99 67.50 89.80 41.69 66.59 58.99 46.58 KIVI-4 7.89 20.06 20.58 67.50 89.80 41.56 66.45 58.62 46.56 KIVI-2 6.92 19.71 17.92 66.50 89.63 41.66 65.52 58.99 45.85 Table 5: Ablation study of KIVI by changing group size G and residual length R. Model Group Size GSM8K Llama2-13B 32 20.77 64 21.00 128 17.29 Model Residual Length GSM8K Llama2-13B 32 20.62 64 19.86 96 20.55 128 20.77 One essential recipe of KIVI is the per-channel quantiza- tion scheme designed based on the observation made in Section 3.2 and Figure 2. ATOM (Zhao et al., 2024) also indicates that key cache exhibits more outliers compared to the value cache. KIVI provides further extensive analysis and leverages this observation to implement per-channel quantization. A similar observation and approach has been independently discovered and developed in the concurrent work KVQuant (Hooper et al., 2024). vLLM (Kwon et al., 2023) and S3 (Jin et al., 2023) are system-level works, which include memory management through the use of PagedAttention or memory usage pre- diction. They can lower the memory requirements of KV cache and simultaneously increase model throughput. This research direction is orthogonal to our work, since system- level optimizations can also be applied upon our algorithm. Several other works also consider compressing KV cache by evicting tokens. H2O (Zhang et al., 2023) retains only a small portion of tokens that contribute significantly to the attention scores. Similarly, Scissorhands (Liu et al., 2024) exploit the persistence of the importance hypothesis in KV cache sparsification. StreamingLLM (Xiao et al., 2023b) is based on the observation of “attention sink” and maintains only a few initial tokens to preserve performance. Unlike these works, our KIVI retains all input tokens and compresses them into lower precision. This line of work is orthogonal to ours, as they can also be combined together. 6. Conclusion and Future Work In this paper, we systematically analyze KV cache element distribution in popular LLMs. We conclude that key cache should be quantized per-channel and value cache should be quantized per token. Based on these observations, we propose KIVI, a plug-and-play 2bit KV cache quantization algorithm without the need for any tuning. In real LLM workload, KIVI allows up to 4× larger batch sizes and 3.47× throughput. In the future, we will further optimize the implementation to reduce the overhead of quantization process during the prefill and decoding phase. Acknowledgments The authors thank the anonymous reviewers for their help- ful comments. Dr. Beidi Chen is supported by a research gift from Moffett.AI. Dr. Vladimir Braverman is partially 9KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache supported by the Ministry of Trade, Industry and Energy (MOTIE) and Korea Institute for Advancement of Technol- ogy (KIAT) through the International Cooperative R&D pro- gram, the Naval Research (ONR) grant N00014-23-1-2737, and NSF CNS 2333887 award. Dr. Xia Hu is supported by NSF grants NSF IIS-2224843. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. References ShareGPT Team. https://sharegpt.com/, 2023. Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan- guage models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. Yu-Neng Chuang, Songchen Li, Jiayi Yuan, Guanchu Wang, Kwei-Herng Lai, Leisheng Yu, Sirui Ding, Chia-Yuan Chang, Qiaoyu Tan, Daochen Zha, et al. Understand- ing different design choices in training large time series models. arXiv preprint arXiv:2406.14045, 2024. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, An- thony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al. A frame- work for few-shot language model evaluation. Version v0. 0.1. Sept, page 8, 2021. Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W Mahoney, Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cache quantization. arXiv preprint arXiv:2401.18079, 2024. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Yunho Jin, Chun-Feng Wu, David Brooks, and Gu-Yeon Wei. S3: Increasing gpu utilization during genera- tive inference for higher throughput. arXiv preprint arXiv:2306.06000, 2023. Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611–626, 2023. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023. Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and An- shumali Shrivastava. Scissorhands: Exploiting the per- sistence of importance hypothesis for llm kv cache com- pression at test time. Advances in Neural Information Processing Systems, 36, 2024. Amirkeivan Mohtashami and Martin Jaggi. Landmark at- tention: Random-access infinite context length for trans- formers. arXiv preprint arXiv:2305.16300, 2023. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobei- dli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outper- forming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023. 10KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Ja- cob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling trans- former inference. Proceedings of Machine Learning and Systems, 5, 2023. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrit- twieser, et al. Gemini 1.5: Unlocking multimodal un- derstanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150, 2019. Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. Flexgen: High-throughput generative inference of large language models with a single gpu. In International Conference on Machine Learning, pages 31094–31116. PMLR, 2023. Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022. Yuandong Tian, Yiping Wang, Beidi Chen, and Simon Du. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. arXiv preprint arXiv:2305.16380, 2023. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar- tinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and ef- ficient post-training quantization for large language mod- els. In International Conference on Machine Learning, pages 38087–38099. PMLR, 2023a. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023b. Zhaozhuo Xu, Zirui Liu, Beidi Chen, Yuxin Tang, Jue Wang, Kaixiong Zhou, Xia Hu, and Anshumali Shrivastava. Compress, then prompt: Improving accuracy-efficiency trade-off of llm inference with transferable prompt. arXiv preprint arXiv:2305.11186, 2023. Jiayi Yuan, Ruixiang Tang, Xiaoqian Jiang, and Xia Hu. Large language models for healthcare data augmentation: An example on patient-trial matching. In AMIA Annual Symposium Proceedings, volume 2023, page 1324. Amer- ican Medical Informatics Association, 2023. Jiayi Yuan, Hongyi Liu, Shaochen Zhong, Yu-Neng Chuang, Songchen Li, Guanchu Wang, Duy Le, Hongye Jin, Vipin Chaudhary, Zhaozhuo Xu, Zirui Liu, and Xia Hu. Kv cache compression, but what must we give in return? a comprehensive benchmark of long context capable ap- proaches. arXiv preprint arXiv:2407.01527, 2024. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H _2 o: Heavy-hitter oracle for efficient generative inference of large language models. arXiv preprint arXiv:2306.14048, 2023. Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom: Low-bit quanti- zation for efficient and accurate llm serving. Proceedings of Machine Learning and Systems, 6:196–209, 2024. 11KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache A. Detailed Implementations In this section, we present the algorithm for KIVI as discussed in Section 3.3. Specifically, we provide the pseudocode for KIVI when calculating the attention output in the prefill and decoding phases. Algorithm 1: The KIVI Prefill & Decoding Algorithm parameter: group size G, residual length R procedure Prefill: Input: X ∈ Rlprompt×d XK = XWK, XV = XWV XVg = XV [: lprompt − R], XVr = XV [lprompt − R :] Q(XVg ) ← GroupQuant(XVg , dim=token, numGroup=d//G) Q(XKg ), XKr ← KeyQuant(XK) KV cache ← Q(XKg ), XKr , Q(XVg ), XVr return XK, XV end procedure Decoding: Input: KV cache, t ∈ R1×d tQ = tWQ, tK = tWK, tV = tWV Q(XKg ), XKr , Q(XVg ), XVr ← KV cache XKr ← Concat([XKr , tK], dim=token) XVr ← Concat([XVr , tV ], dim=token) if len(XKr ) = R then Q(XKr ), _ ←KeyQuant(XKr ) Q(XKg ) ← Concat([Q(XKg ), Q(XKr )], dim=token) XKr ← empty tensor. end if len(XVr ) > Rthen Q(XV ′r ) ← GroupQuant(XVr [: −R], dim=token, numGroup = d//G) Q(XVg ) ← Concat([Q(XVg ), Q(XV ′r )], dim=token) XVr ← XVr [−R :] end A ← Concat([tQQ(XKg )⊤, tQX⊤ Kr ], dim=token) Ag = Softmax(A)[: −R], Ar = Softmax(A)[−R :] tO ← AgQ(XVg ) + ArXVr KV cache ← Q(XKg ), XKr , Q(XVg ), XVr return tO end function KeyQuant(XK ∈ Rl×d): r = l%R, XKg = XK[: l − r], XKr = XK[l − r :] Q(XKg ) ← GroupQuant(XKg , dim=channel, numGroup=l//G) return Q(XKg ), XKr end 12KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache B. NIAH Setting We largely follows the passkey retrieval prompt template of Mohtashami and Jaggi (2023) but using 7-digit passkey and Paul Graham Essays2 as the background filler, as set forth in Arize-ai and Reid et al. (2024): There is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I will quiz you about the important information there. <prefix filled by Paul Graham Essays> The pass key is <7-DIGIT PASS KEY>. Remember it. <7-DIGIT PASS KEY> is the pass key. <suffix filler> What is the pass key? The pass key is C. More Ablation Results In our efficiency evaluation, we observe that with a residual length of 32, KIVI achieves a significantly higher memory compression rate, which in turn leads to increased throughput. Additionally, our ablation study reveals that changing the residual length from 128 to 32 does not result in a substantial performance gap. We demonstrate KIVI with a residual length of 32 across all benchmark datasets. As shown in Tables 6 and 7, KIVI with a residual length of 32 also delivers performance comparable to that of the 16-bit full model. Table 6: Performance comparison between 16bit, KIVI-2 (2bit) / KIVI-4 (4bit) with residual length 128 and 32 across various models. R32 stands for residual length 32. Model CoQA TruthfulQA GSM8K Llama-2-7B 16bit 63.88 30.76 13.50 KIVI-2 R128 63.05 33.95 12.74 KIVI-2 R32 62.85 33.01 13.57 Llama-2-13B 16bit 66.37 29.53 22.67 KIVI-2 R128 66.23 29.84 20.77 KIVI-2 R32 66.57 29.35 20.62 Falcon-7B 16bit 59.83 23.20 4.55 KIVI-4 R128 59.67 22.58 4.47 KIVI-4 R32 59.73 22.96 3.94 KIVI-2 R128 57.48 24.98 3.41 KIVI-2 R32 57.50 25.70 2.20 Mistral-7B 16bit 67.40 30.45 38.36 KIVI-2 R128 66.35 32.17 36.01 KIVI-2 R32 65.90 31.21 34.34 D. More Experimental Results We present additional results using Llama3-8B, Mistral-7B-v0.2, and LongChat-7B-v1.5 in LongBench, which can be found in Table 8, Table 9 and Table 10, respectively. We also show result of Needle-in-a-Haystack Test in Figure 4. The settings largely follow the format of the original passkey retrieval task (Mohtashami and Jaggi, 2023) while including some modern modifications set forward by Arize-ai and the technical report of Gemini 1.5 (Reid et al., 2024). 2https://paulgraham.com/articles.html 13KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Table 7: Performance evaluation of KIVI with residual length 128 and 32 on various models across a range of benchmarks in LongBench. R32 stands for residual length 32. Model Qasper QMSum MultiNews TREC TriviaQA SAMSum LCC RepoBench-P Average Llama2-7B 16bit 9.52 21.28 3.51 66.00 87.72 41.69 66.66 59.82 44.52 KIVI-2R128 9.31 20.50 1.14 66.00 87.42 42.71 66.88 60.23 44.27 KIVI-2R32 9.26 20.53 0.97 66.00 87.42 42.61 66.22 59.67 44.08 Llama2-13B 16bit 9.32 21.38 3.71 70.00 87.87 43.55 66.61 56.42 44.85 KIVI-2R128 8.58 20.69 6.19 69.50 87.78 44.30 65.08 55.46 44.69 KIVI-2R32 8.38 20.74 7.01 69.50 87.78 44.43 64.89 55.31 44.75 Llama2-7B-Chat 16bit 19.65 20.54 26.36 63.00 84.28 41.12 59.75 52.93 45.95 KIVI-2R128 19.32 20.46 25.48 63.00 84.84 40.60 58.71 52.97 45.67 KIVI-2R32 19.10 20.08 25.33 63.00 85.04 39.80 57.91 52.38 45.33 Llama2-13B-Chat 16bit 24.18 20.37 25.69 67.50 86.90 42.18 50.23 50.64 45.96 KIVI-2R128 23.59 20.76 25.25 67.50 87.17 41.56 49.93 48.45 45.52 KIVI-2R32 23.56 20.90 25.45 67.50 87.42 41.40 48.93 48.81 45.49 Falcon-7B 16bit 1.48 2.35 11.09 13.00 5.84 2.44 23.86 9.69 8.71 KIVI-4R128 1.04 2.41 11.98 13.00 5.84 2.36 23.72 9.92 8.78 KIVI-4R32 1.03 2.45 11.99 13.50 5.84 2.46 23.88 9.95 8.88 KIVI-2R128 1.98 3.61 6.78 10.00 6.24 2.73 22.18 10.12 7.95 KIVI-2R32 2.28 3.23 6.73 10.00 6.31 2.88 22.71 10.45 8.07 Mistral-7B 16bit 8.12 19.98 19.99 67.50 89.80 41.69 66.59 58.99 46.58 KIVI-2R128 6.92 19.71 17.92 66.50 89.63 41.66 65.52 58.99 45.85 KIVI-2R32 6.84 19.81 17.20 66.50 89.63 42.82 65.13 58.06 45.74 Table 8: The results of Llama-3-8B-Instruct with KIVI on LongBench. The model has 8K context length and applies group query attention, which uses 8 heads for KV cache instead of the full 32 heads. We use a 32 group size and 128 residual length for both KIVI-2 and KIVI-4. The baseline is of full precision. NarrativeQA Qasper MultiFieldQA HotpotQA MuSiQue 2WikiMQA GovReport QMSum Baseline 21.71 44.24 44.54 46.82 21.49 36.42 30.03 22.67 w./KIVI-2 21.35 43.17 44.49 46.79 20.56 37.05 29.98 22.07 w./KIVI-4 21.01 44.83 44.60 46.96 21.43 36.48 30.22 22.44 MultiNews LCC RepoBench-P TriviaQA SAMSum TRec PR Avg Baseline 27.79 57.00 51.22 90.23 42.53 74.50 67.00 45.21 w./KIVI-2 27.77 50.84 46.65 90.54 42.26 74.50 67.50 44.37 w./KIVI-4 27.97 57.36 52.03 90.33 42.97 74.50 66.50 45.31 Table 9: The results of Mistral-7B-Instruct-v0.2 with KIVI on LongBench. The model has 32K context length and applies group query attention, which uses 8 heads for KV cache instead of the full 32 heads. We use a 32 group size and 128 residual length for both KIVI-2 and KIVI-4. The baseline is of full precision. NarrativeQA Qasper MultiFieldQA HotpotQA MuSiQue 2WikiMQA GovReport QMSum Baseline 21.02 29.41 47.13 36.53 19.13 21.76 32.59 23.99 w./KIVI-2 20.61 28.73 44.88 35.47 17.95 20.68 32.55 23.65 w./KIVI-4 20.97 29.41 46.52 36.25 19.53 21.66 32.97 24.06 MultiNews LCC RepoBench-P TriviaQA SAMSum TRec PR Avg Baseline 27.09 53.49 51.40 86.23 43.04 71.00 89.33 43.54 w./KIVI-2 26.54 53.03 51.16 86.00 43.34 71.00 80.83 42.43 w./KIVI-4 26.89 53.33 51.41 86.23 43.34 71.00 89.42 43.53 14KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Table 10: The results of LongChat-7B-v1.5-32K with KIVI on LongBench. The model has 32K context length. We use a 32 group size and 128 residual length for both KIVI-2 and KIVI-4. The baseline is of full precision. NarrativeQA Qasper MultiFieldQA HotpotQA MuSiQue 2WikiMQA GovReport QMSum Baseline 20.65 29.42 43.15 33.05 14.66 24.14 30.85 22.84 w./KIVI-2 20.79 28.69 41.02 32.91 13.82 23.00 30.47 22.59 w./KIVI-4 20.49 28.90 43.24 33.07 14.66 24.86 31.40 22.84 MultiNews LCC RepoBench-P TriviaQA SAMSum TRec PR Avg Baseline 26.55 54.83 58.94 83.99 40.75 66.50 30.50 38.72 w./KIVI-2 26.28 54.11 57.62 83.19 41.28 66.50 32.25 38.30 w./KIVI-4 26.52 54.06 58.77 83.88 40.62 67.00 31.50 38.79 15",
      "meta_data": {
        "arxiv_id": "2402.02750v2",
        "doi": "10.13140/RG.2.2.28167.37282",
        "authors": [
          "Zirui Liu",
          "Jiayi Yuan",
          "Hongye Jin",
          "Shaochen Zhong",
          "Zhaozhuo Xu",
          "Vladimir Braverman",
          "Beidi Chen",
          "Xia Hu"
        ],
        "published_date": "2024-02-05T06:06:47Z",
        "pdf_url": "https://arxiv.org/pdf/2402.02750v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper identifies the KV cache as a major memory and speed bottleneck in LLM inference, especially with increasing batch sizes and context lengths. It addresses the lack of in-depth studies on KV cache element distribution for quantization. The main contributions include a comprehensive analysis revealing that key cache should be quantized per-channel due to large outliers, while value cache should be quantized per-token because of its role in attention output calculation. Based on these insights, the authors propose KIVI, a tuning-free, asymmetric 2-bit KV cache quantization algorithm with hardware-friendly implementation. KIVI achieves a 2.6x reduction in peak memory usage for models like Llama, Falcon, and Mistral, with minimal accuracy degradation. This memory reduction enables up to 4x larger batch sizes, leading to a 2.35x to 3.47x increase in throughput on real LLM inference workloads.",
        "methodology": "The methodology involves a detailed study of KV cache element distribution to understand quantization challenges. Based on observations, key cache is quantized per-channel and value cache per-token using a tuning-free, asymmetric round-to-nearest quantization with zero-point and scaling factors (2-bit, with 4-bit option for some models). To accommodate per-channel quantization in a streaming inference setting, KIVI splits the KV cache into a 'grouped part' (quantized group-wise) and a 'residual part' (kept in full precision, acting as a sliding window for recent tokens). This full-precision sliding window is crucial for maintaining accuracy on challenging tasks. For system support, the dequantization process is fused with matrix multiplication (Q_MatMul) using CUDA, and a group-wise quantization kernel is implemented in Triton, ensuring compatibility with weight-only quantization.",
        "experimental_setup": "KIVI was evaluated using popular LLMs including Llama/Llama-2 (7B, 13B), Falcon (7B), Mistral (7B), Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.2, and LongChat-7B-v1.5, implemented on the Hugging Face Transformers codebase. The group size for quantization (G) was set to 32, and the residual length (R) to 128, with ablation studies exploring other values (32, 64, 96). Performance was assessed on generation tasks from LM-Eval (CoQA, TruthfulQA, GSM8K) for normal context lengths and LongBench (Qasper, QMSum, MultiNews, TREC, TriviaQA, SAMSum, LCC, RepoBench-P) for long contexts, including a Needle-in-a-Haystack (NIAH) test for long-context retrieval. Experiments were conducted on a single NVIDIA A100 GPU (80GB), comparing KIVI against 16-bit full precision and various fake quantization configurations in terms of accuracy, peak memory usage, and throughput.",
        "limitations": "The 2-bit KIVI quantization for Falcon-7B (which uses multi-query attention and already has a compressed KV cache design) resulted in a larger accuracy drop compared to Llama and Mistral models, requiring 4-bit KIVI to maintain accuracy. This suggests that the 2-bit quantization might not be universally optimal across all LLM architectures. Additionally, while KIVI provides significant throughput improvements, the authors note that the speed-up could be 'greatly increased if we further fuse the KV cache quantization process with previous operations,' implying that the current implementation still has potential overhead that could be further optimized.",
        "future_research_directions": "Future work will focus on further optimizing the implementation to reduce the overhead of the quantization process during both the prefill and decoding phases. The authors also suggest exploring the combination of KIVI with other orthogonal KV cache compression techniques, such as token eviction methods (e.g., H2O, Scissorhands, StreamingLLM), and system-level optimizations like PagedAttention or memory usage prediction."
      }
    },
    {
      "title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization",
      "abstract": "Efficient deployment of Large Language Models (LLMs) requires batching\nmultiple requests together to improve throughput. As the batch size, context\nlength, or model size increases, the size of the key and value (KV) cache can\nquickly become the main contributor to GPU memory usage and the bottleneck of\ninference latency. Quantization has emerged as an effective technique for KV\ncache compression, but existing methods still fail at very low bit widths. We\nobserve that distinct channels of a key/value activation embedding are highly\ninter-dependent, and the joint entropy of multiple channels grows at a slower\nrate than the sum of their marginal entropies. Based on this insight, we\npropose Coupled Quantization (CQ), which couples multiple key/value channels\ntogether to exploit their inter-dependency and encode the activations in a more\ninformation-efficient manner. Extensive experiments reveal that CQ outperforms\nor is competitive with existing baselines in preserving model quality.\nFurthermore, we demonstrate that CQ can preserve model quality with KV cache\nquantized down to 1-bit.",
      "full_text": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization Tianyi Zhang1, Jonah Yi1, Zhaozhuo Xu2, and Anshumali Shrivastava1,3 1Department of Computer Science, Rice University 2Department of Computer Science, Stevens Institute of Technology 3ThirdAI Corp. {tz21, jwy4, anshumali}@rice.edu, zxu79@stevens.edu May 8, 2024 Abstract Efficient deployment of Large Language Models (LLMs) requires batching multiple requests together to improve throughput. As the batch size, context length, or model size increases, the size of the key and value (KV) cache can quickly become the main contributor to GPU memory usage and the bottleneck of inference latency. Quantization has emerged as an effective technique for KV cache compression, but existing methods still fail at very low bit widths. We observe that distinct channels of a key/value activation embedding are highly inter-dependent, and the joint entropy of multiple channels grows at a slower rate than the sum of their marginal entropies. Based on this insight, we propose Coupled Quantization (CQ), which couples multiple key/value channels together to exploit their inter-dependency and encode the activations in a more information-efficient manner. Extensive experiments reveal that CQ outperforms or is competitive with existing baselines in preserving model quality. Furthermore, we demonstrate that CQ can preserve model quality with KV cache quantized down to 1-bit. 1 Introduction Large Language Models (LLMs) have showcased remarkable generalization abilities across various tasks, including text generation, language translation, and reasoning, without needing specific fine- tuning [25]. These impressive capabilities have empowered LLMs to find applications in numerous domains, such as law [14], education [15], and patient care [35]. However, the high computational demands and the prohibitive deployment costs of LLMs have created significant barriers, hindering their widespread adoption [14, 3]. Particularly, as LLMs move towards larger model size [9] and longer context length [34], they require faster graphics processing units (GPUs), or other special- ized processors, with higher memory capacity for efficient inference. Hence it is crucial to develop approaches for reducing the computational costs and memory requirement of LLMs. To accelerate LLM inference, key and value (KV) caching [20] has been proven to be an effective technique without affecting model quality. In autoregressive decoder-only LLMs, KV caching works through trading off memory for computations: the key and value activations of all previous tokens in the current batch are saved in memory to avoid their recomputation for generating the next token. However, since KV cache scales linearly with the number of tokens and batch size, it can quickly overwhelm the memory capacity of existing GPUs under long context or large batch size 1 arXiv:2405.03917v1  [cs.LG]  7 May 2024settings. In addition, since past key and value activations are not shared between sequences (except for maybe a common prompt), reading the KV cache from GPU memory becomes the primary inference bottleneck as opposed to the computation of the attention scores and value activations of the next token [10]. Thus, it is worthwhile to explore techniques for compressing KV cache for two primary benefits: 1. speeding up LLM inference through reducing the amount of memory reads for KV cache, 2. lowering the GPU memory requirements of inference for a given batch size and context length. Existing approaches typically compress KV cache through token eviction [37, 20] or activation quantization [21, 10]. While they can preserve model quality at moderate compression rates (4× compression or 4 bits per floating-point number), model quality quickly deteriorates at high compression rates (16× compression or 1 bit per floating-point number). In this work, we propose Coupled Quantization (CQ), a novel KV cache quantization method that preserves model quality up to 16× compression or 1 bit per floating-point number. Our approach is motivated by the observation that different channels within the same key/value activation embedding are highly inter-dependent. Thus, it is more information efficient to encode multiple channels of a key/value activation embedding than quantizing each channel independently. Existing KV cache quantization methods employ per-channel or per-token quantization strategies, which fail to exploit the inter-dependence between channels and suffer catastrophic model quality degradation at high compression rates. Our proposed method exploits the mutual dependency be- tween channels by jointly quantizing multiple channels and achieves better preservation of model quality than existing approaches in most cases, especially under low bit width settings. We sum- marize our contributions as follows, 1. We empirically observe the phenomenon that different channels within the same key/value activation embedding in an LLM share a high amount of dependency or mutual information, which is a key insight not leveraged by existing KV cache compression approaches. 2. We propose Coupled Quantization (CQ), a novel KV cache quantization method that takes advantage of the reduced entropy of jointly encoding multiple channels. 3. Through extensive experiments, we demonstrate the effectiveness of CQ against the most competitive existing methods. Furthermore, we showcase the ability of CQ in preserving model quality at an extreme KV cache compression level of 1-bit. 2 Background In this section, we introduce the relevant background and context including the KV caching tech- nique, the von Neumann bottleneck of KV cache, and channel-wise quantization. 2.1 LLM Attention and KV Cache Decoder-only transformer-based LLMs employ masked self-attention [32], in which activations of the current token is only dependent on previous tokens and unaffected by future ones. This prop- erty enables training parallelism for the next-token prediction objective, and gives rise to the KV caching technique for efficient inference decoding. Consider the decoding step for thet-th token in a single head of attention in an LLM. The input embedding of thet-th token (a column vector),et, goes through three distinct transformations to become key, query, and value activation embeddings fK(et), fQ(et), fV (et), where the transformationsfK, fQ, fV are composed of linear projection and positional encoding such as RoPE [29]. The output embedding of attention for thet-th token is 2computed as attention(et) = \u0014 fV (e1) . . . fV (et) \u0015 softmax  \u0014 fK(e1) . . . fK(et) \u0015⊤ fQ(et) ! (1) Thus, computing the output embedding of the current token requires the key and value activation embeddings of all previous tokens,fK(ei) and fV (ei) where i ∈ {1, . . . , t− 1}. These embeddings are cached in memory from previous decoding steps asKV cacheto avoid redundant computations and reduce inference latency. The size of KV cache can be calculated asb × n × l × 2 × h × c floating-point numbers, whereb is the batch size,n is the number of tokens in each sequence,l is the number of layers in the model,2 is for key and value,h is the number of key/value attention heads, and c is the number of channels in a single head of key/value activation embedding. As batch size, context length, or model size increases, the size of KV quickly overwhelms limited GPU memory. 2.2 The von Neumann Bottleneck of KV Cache The attention computation in Equation 1 is primarily bottlenecked by GPU memory bandwidth, known as the von Neumann bottleneck, due to low compute-to-global-memory-access ratio. GPUs are able to perform computations significantly faster than reading from global memory, and previous works have shown that attention computation can be accelerated by avoiding unnecessary global memory reads/writes through kernel fusion [5]. During the decoding phase in LLM attention, com- puting the output for the current token requires fetching the cached key and value embeddings of all previous tokens from global memory, resulting in a low ratio of arithmetic operations to global memory reads [10]. Furthermore, each sequence in a batch retains its own KV cache, leading to poor utilization of the parallel processing capabilities of GPUs. Therefore, KV cache compression algorithms can mitigate the von Neumann bottleneck and improve LLM inference efficiency, even if the approach introduces additional computational overheads in decompression or dequantization. As GPU compute cores are mostly stalled by KV cache memory accesses in attention, quantization approaches can effectively reduce memory reads while introducing negligible latency from dequan- tization computations. 2.3 Channel-wise Quantization Existing KV cache quantization methods [10, 21] employ channel-wise quantization for keys and token-wise quantization for values, based on the observation that certain key channels exhibit outlier patterns in magnitude while value channels do not. Channel-wise and token-wise quantization are similar, except the direction along which the quantization centroids are learned. In non-uniform channel-wise quantization, a set of centroids is learned for each channel. SupposeA is a key or value activation matrix, andAi,∗ denotes the i-th channel ofA. Then, non-uniform b-bit channel-wise quantization aims to learn a set of centroidsC⋆ i ⊂ R for each channeli of A independently through the objective C⋆ i = arg min C⊂R |C|=2b \r\r\rAi,∗ − q(Ai,∗) \r\r\r 2 2 (2) where q quantizes each value inAi,∗ to the nearest centroid inC. 31 2 3 4 5 10 15Key Entropy (bits) Layer 1 1 2 3 4 5 10 Layer 2 1 2 3 4 5 10 Layer 3 1 2 3 4 5 10 Layer 4 1 2 3 4 Num. of Joint Channels 5 10Value Entropy (bits) 1 2 3 4 Num. of Joint Channels 5 10 1 2 3 4 Num. of Joint Channels 2.5 5.0 7.5 10.0 1 2 3 4 Num. of Joint Channels 2.5 5.0 7.5 10.0 Sum of Marginal Entropy Joint Entropy Figure 1: Growth rate of joint entropy versus sum of marginal entropies of the LLaMA-7b key/value activation embeddings on 262k tokens of WikiText-2. Entropy is estimated using Equation 4. The slower growth rate of joint entropy implies that jointly quantizing more channels requires fewer bits than quantizing each channel independently. 3 Methodology In this section, we motivate our proposal using information theory and introduce the Coupled Quantization (CQ) approach for KV cache compression. 3.1 Motivations Our proposed approach is inspired by concepts in information theory [28]. We consider each channel in a key/value activation embedding as a random variableX1, X2, . . .. The amount of information (oruncertainty)inchannel X canbemeasuredby entropy, definedasH(X) =− R X p(x) log2 p(x) dx, where p(·) is the probability density function andX is the support of X. H(X) measures the theoretical number of bits needed for losslessly encoding the channelX, so it can be used to gauge how “quantizable” a channel is: ifH(X1) < H(X2), then channelX1 may be quantized to fewer bits than channelX2 while achieving the same quantization error. Our insight is that different channels from the same key/value activation embedding may be interdependent, which would reduce the number of bits required for jointly encoding multiple channels together compared to encoding them independently. The total amount of information (or uncertainty) in two channelsX1, X2 is measured by joint entropy, defined as H(X1, X2) = − R X1 R X2 p(x1, x2) log2 p(x1, x2) dx2 dx1, wherep(·, ·) is the joint probability density function. The joint entropy of two channels is the difference between the sum of their marginal entropies and their mutual information, i.e.,H(X1, X2) =H(X1) +H(X2) − I(X1, X2), whereI(·, ·) is a non-negative quantity for measuring the mutual dependency of two random variables. Thus, we have H(X1, X2) ≤ H(X1) +H(X2) (3) which implies the number of bits needed for jointly encoding two channels is no more than the total number of bits needed for encoding them independently. Previous works have demonstrated that 41 16 32 LLaMA-7b Key Correlations Layer 1  Layer 2  Layer 13  Layer 14  Layer 28  Layer 29  Layer 31  Layer 32 1 16 32 Channels 1 16 32 LLaMA-7b Value Correlations 1 16 32 Channels 1 16 32 Channels 1 16 32 Channels 1 16 32 Channels 1 16 32 Channels 1 16 32 Channels 1 16 32 Channels 1.0 0.5 0.0 0.5 1.0 Figure 2: Correlation matrices of the first 32 channels of 8 layers of LLaMA-7b key and value activation embeddings on WikiText-2. Channel pairs exhibit high levels of linear dependency, shown by high magnitudes of the correlation coefficients. deep neural networks [11] and attention-based networks [7] tend to produce low-rank embeddings, which suggests that channels of key/value embedding in LLM may exhibit high amount of mutual dependency. It is hence beneficial to measure the difference between the joint entropy of multiple channels and the sum of their marginal entropies in key and value activation embeddings. A significant difference would suggest that encoding these channels together is more information-efficient than encoding them independently. However, it is intractable to derive the exact entropy or joint entropy of channels, since their probability density functions are not known. Therefore, we employ the “binning” trick [17] to estimate entropy. We first observe an empirical distribution of key and value channels by saving the KV cache on a dataset, and partition the support of each channel into equally sized bins. Then, values of each channel are discretized to the index of the bin they fall into. Finally, the joint entropy ofn channels X1, . . . , Xn is estimated with the Riemann sum, H(X1, . . . , Xn) ≈ X x1∈B1 ··· X xn∈Bn bp(x1, . . . , xn) log2 bp(x1, . . . , xn) (4) where Bi is the support of the binned or discretizedXi and bp(·) is the empirical probability density function. Specifically, we divide the channels of key and value embeddings of LLaMA-7b into non- overlapping groups ofc contiguous channels, wherec ∈ {1, 2, 3, 4}, and estimate the joint entropy and the sum of marginal entropies of each group. The support of each channel is partitioned into 16 equally sized bins. Figure 1 shows the mean and standard deviation of the estimated joint entropy and sum of marginal entropies of three layers of LLaMA-7b on 262k tokens of the WikiText-2 dataset, averaged over groups. We only show a maximum group size of 4, since increasing the group size requires saving exponentially more key and value embeddings to avoid empty bins and maintain estimation quality. As shown in Figure 1, the sum of marginal entropies grows at a linear rate while the joint entropy increases slower at a sub-linear rate. This implies that as the number of jointly quantized channels increases, the total amount of information needed for encoding decreases. This phenomenon is the foundation that motivates our proposed approach. In addition to presenting the estimated marginal and joint entropy, we also show the Pearson correlation coefficients between channels of LLaMA-7b key and value activation embeddings on WikiText-2. Correlation coefficient captures the linear dependency between two random variables. Heat maps for the correlation matrices of 8 layers are shown in Figure 2, while the correlation matrices of all layers are presented in Section 6 of the Appendix. The key and value channels 5Channel-wise Quantization Coupled Quantization q0 q1 channel 0 -0.57 0  0.21 1  -0.38 0  -0.13 1  key or value activation embedding channel 1 channel-wise centroids q -0.62 0  0.11 1  -0.38-0.10 0.30 2  -0.38 3  -0.13-0.24 coupled channels 0 & 1 key or value activation embedding multi-channel centroids Figure 3: A comparison of 1-bit channel-wise quantization and our proposed Coupled Quantization (using 2 bits per 2 channels as an example). The quantization results on the first two channels of the first-layer key activation embeddings of LLaMA-7b on the WikiText-2 dataset are shown. Channel-wise quantization is ineffective at capturing the original values at low widths, while CQ leverages the dependency between channels to achieve low quantization errors. exhibit high levels of linear dependency and they are clearly not independently distributed, as shown by high magnitudes of the correlation coefficients. 3.2 Coupled Quantization Motivated by the finding that distinct key/value channels exhibit high dependency, we propose Coupled Quantization (CQ), an information-efficient quantization approach for compressing LLM KV cache. Unlike existing KV cache quantization methods which quantizes channel-wise or token- wise, CQ performs channel-coupled quantization for keys and values. More concretely, channels of a key or value activation embedding are divided into equally sized, non-overlapping groups of contiguous channels. The channels in each group arecoupled, as they are jointly quantized and share a single quantization code. For each group of coupled channels, a distinct set of multi-channel centroids are learned, where each centroid has dimensionality equal to the number of channels in that group. When quantizing a key or value activation embedding, each group of coupled channels are quantized to the nearest centroid in terms of L2 distance. We use the CQ-<c>c<b>b notation to denote the configuration of channel coupling and quantization bit width, where<c> is the number of channels in each group and<b> indicates the number of bits in a quantized code for a group. For example, CQ-4c8b means that every 4 contiguous channels are coupled together and each coupled group shares an 8-bit code, which is equivalent to 2-bit channel-wise quantization in terms of storage overhead of quantized codes. A illustrative comparison of channel-wise quantization and CQ is shown in Figure 3. Although previous works [10, 21] opt to quantize keys channel-wise and values token-wise, we adopt channel-coupled quantization for both keys and values. Similar to existing approaches [10, 21], CQ quantizes keys before RoPE [29] is applied, which increases the quantization difficulty by introducing more outliers in key activations. 3.2.1 Centroid Learning In CQ, the multi-channel centroids for each channel group are learned offline on a calibration dataset by leveraging uniform clustering or second-order-information-informed clustering. Specifically, for uniform centroid learning of the CQ-c cb b configuration, a set of centroidsC⋆ i ⊂ Rc is learned 61c1b 2c2b 4c4b 8c8b CQ Conﬁg. 101 102 103 Perplexity on WikiText-2 2642.72 1614.02 883.60 32.12 620.08 28.39 10.47 8.095.68 LLaMA-7b 1-bit CQ Uniform CQ Fisher-guided CQ FP16 Baseline 1c2b 2c4b 4c8b CQ Conﬁg. 101 102 Perplexity on WikiText-2 204.56 24.46 6.866.37 6.08 5.97 LLaMA-7b 2-bit CQ 1c1b 2c2b 4c4b 8c8b CQ Conﬁg. 750 1000 1250 1500 1750 2000Quant. Error on WikiText-2 LLaMA-7b 1-bit CQ Uniform CQ Key Uniform CQ Value Fisher-guided CQ Key Fisher-guided CQ Value 1c2b 2c4b 4c8b CQ Conﬁg. 600 800 1000Quant. Error on WikiText-2 LLaMA-7b 2-bit CQ Figure 4: Perplexity and key/value quantization errors (averaged over all layers) of LLaMA-7b on WikiText-2. Channels coupling and Fisher-guided centroid learning are effective for improving perplexity. independently for each channel groupi through the objective C⋆ i = arg min C∈Rc |C|=2b \r\r\rAic:(ic+c−1), ∗ − cq \u0000 Aic:(ic+c−1), ∗ \u0001\r\r\r 2 F (5) where Aic:(ic+c−1), ∗ is the sub-matrix ofA containing all coupled channels of thei-th group, and cq quantizes each column vector to the nearest centroid inC in terms of L2 distance. We use the k-means algorithm [22] with k-means++ initialization [1] to optimize the objective. LLMs are more sensitive to the quantization precision of certain weights than others [16]. To better preserve model quality, centroids of CQ should be learned to be biased towards preserving the precision of more important activations. To this end, we leverage an approximation to the Hessian to perform second-order-information-informed centroid learning. More concretely, we use the diagonals of the Fisher information matrixF to identify the more influential key/value activations and guide the centroid learning process. This method was proposed by Li et al. [18] and used by Kim et al. [16] for channel-wise weight quantization, and we extend it to multi-channel CQ. For performing Fisher-guided centroid learning, we first save a key/value activation matrixA and its gradient g(A) = ∂ ∂A L(A) on a calibration dataset, whereL is the training loss function. We approximate the Hessian matrix using the diagonals of the Fisher information matrix, which is the element-wise square of the gradient matrixdiag(F) = g(A) ⊙ g(A). We use the sum of diagonal entries of the Fisher information matrix as a measure of importance for each group of channel-coupled activations, and obtain the centroid setC⋆ i for thei-th channel group using the objective C⋆ i = arg min C⊂Rc |C|=2b X j g \u0000 Aic:(ic+c−1), j \u0001⊤g \u0000 Aic:(ic+c−1), j \u0001\r\r\rAic:(ic+c−1), j− cq(Aic:(ic+c−1), j) \r\r\r 2 F (6) We leverage weighted k-means to optimize the objective. The overhead of the centroid learning process and centroid storage are discussed in Section 4.3. We validate the effectiveness of our proposed channel-coupling and Fisher-guided centroid learn- ing by compressing LLaMA-7b KV cache to 1-bit and 2-bit, and present the perplexity results and quantization errors (∥A − cq(A)∥2 F averaged over layers) on WikiText-2 under different CQ con- figurations in Figure 4. The experimental setup is given in Section 4. As the number of coupled channels increases, perplexity and quantization errors improve significantly, approaching the FP16 7Table 1: Perplexity on WikiText-2 under different KV cache quantization methods at varying bit- width. The results of INT, NF, and KVQuant (excluding 1b and 1b-1%) are from [10]. “NaN” means Not a Number, which is caused by numerical stability issues of quantization. Our method CQ consistently outperforms non-dense-and-sparse quantization methods, and performs better or on par with the dense-and-sparse method KVQuant-<b>b-1%. Bits Per FPN LLaMA-7b LLaMA-13b LLaMA-2-7b LLaMA-2-13b Mistral-7b FP16 16 5.68 5.09 5.12 4.57 4.76 INT4 4.00-4.01 5.98 5.32 5.66 5.01 4.97 INT4-gs128 4.16 5.77 5.16 5.32 4.71 4.82 NF4 4.00-4.01 5.87 5.23 5.47 4.90 4.91 NF4-gs128 4.16 5.77 5.17 5.30 4.71 4.83 KVQuant-4b 4.00-4.02 5.73 5.15 5.18 4.63 4.81 KVQuant-4b-1% 4.32-4.35 5.70 5.11 5.14 4.59 4.78 CQ-2c8b 4.00 5.70 5.11 5.14 4.59 4.79 INT2 2.00-2.01 11779 69965 4708 3942 573 INT2-gs128 2.14 37.37 41.77 117.88 93.09 51.96 NF2 2.00-2.02 3210.5 5785.6 13601 4035.6 902.51 NF2-gs128 2.14 351.23 141.19 634.59 642.44 252.85 KVQuant-2b 2.00-2.02 8.17 7.29 9.75 29.25 7.33 KVQuant-2b-1% 2.32-2.35 6.06 5.40 5.50 4.92 5.16 CQ-4c8b 2.00 5.97 5.32 5.42 4.81 5.11 KVQuant-1b 1.00-1.02 321.58 1617.40 NaN 4709.83 203.73 KVQuant-1b-1% 1.32-1.35 9.93 7.97 9.50 13.76 10.07 CQ-8c8b 1.00 8.09 7.02 7.75 6.55 7.25 CQ-8c10b 1.25 6.78 6.00 6.25 5.47 5.90 baseline performance. Although Fisher-guided centroid learning increases the quantization error, it better preserves the salient activations and achieve lower perplexity. 4 Experiments In this section, we perform extensive experiments to validate the effectiveness of our proposed CQ approach for KV cache compression. We first introduce the experimental setups including hardware, software, metrics, datasets, and baselines used. Then, we present the detailed empirical results and provide discussions. Finally, we perform an ablation study to validate the effectiveness of each component of our proposal. Hardware Testbed Experiments are performed on a Linux server running Ubuntu 20.04, equipped with 2 AMD EPYC 7742 CPUs, 1.5TB RAM, and 4 NVIDIA A100 40GB GPUs. Software ImplementationOur software implementation of CQ is based on PyTorch [24] and the HuggingFace Transformers library [33]. Evaluation Metrics and BenchmarksWe evaluate the quality of 5 popular open-source LLMs on various benchmarks under different KV cache quantization algorithms. The 5 LLMs con- sidered are 1. LLaMA-7b, 2. LLaMA-13b [30], 3. LLaMA-2-7b, 4. LLaMA-2-13b [31], 5. Mistral-7b [13]. We evaluate LLM quality using the perplexity metric on 2 datasets: WikiText-2 [23] and C4 [26], and accuracy on 3 benchmarks in zero-shot setting: WinoGrande [27], PIQA [2], and ARC Challenge [4]. Perplexity is evaluated on the test set of the datasets at the maximum context length 8Table 2: Perplexity on C4 under different KV cache quantization methods at varying bit-width. The results of INT, NF, and KVQuant (excluding 1b and 1b-1%) are from [10]. Our method CQ consistently outperforms non-dense-and-sparse quantization methods, and performs better or on par with the dense-and-sparse method KVQuant-<b>b-1%. Bits Per FPN LLaMA-7b LLaMA-13b LLama-2-7b LLaMA-2-13b Mistral-7b FP16 16 7.08 6.61 6.63 6.05 5.71 INT4 4.00-4.01 7.40 6.82 7.31 6.59 5.91 INT4-gs128 4.16 7.16 6.67 6.87 6.20 5.76 NF4 4.00-4.01 7.27 6.74 7.09 6.45 5.85 NF4-gs128 4.16 7.16 6.66 6.86 6.20 5.77 KVQuant-4b 4.00-4.02 7.13 6.65 6.70 6.11 5.75 KVQuant-4b-1% 4.32-4.35 7.09 6.62 6.65 6.06 5.72 CQ-2c8b 4.00 7.11 6.64 6.67 6.09 5.74 INT2 2.00-2.01 10892 100870 4708 4220 477 INT2-gs128 2.14 43.49 56.25 113.49 97.04 50.73 NF2 2.00-2.02 2850.1 4680.3 13081.2 4175.6 1102.3 NF2-gs128 2.14 248.32 118.18 420.05 499.82 191.73 KVQuant-2b 2.00-2.02 10.28 9.05 15.16 43.77 8.40 KVQuant-2b-1% 2.32-2.35 7.38 6.83 7.06 6.38 6.08 CQ-4c8b 2.00 7.52 6.96 7.23 6.52 6.17 KVQuant-1b 1.00-1.02 168.90 1316.41 362.94 4223.37 127.07 KVQuant-1b-1% 1.32-1.35 11.18 9.56 16.04 22.87 10.53 CQ-8c8b 1.00 12.13 10.53 12.49 10.53 9.89 CQ-8c10b 1.25 9.12 8.23 9.03 8.01 7.46 of the LLM (2048 for LLaMA, 4096 for LLaMA-2, and 8192 for Mistral). Baselines We compare our proposed approach with uncompressed FP16 KV cache and com- petitive KV cache quantization methods, including 1. uniform integer (INT) quantization (without grouping and with a group size of 128), 2. NormalFloat (NF) quantization [6] (without grouping and with a group size of 128), 3. KVQuant [10] (without sparse outliers and with 1% outliers stored in sparse format). KVQuant-<b>b-1% is a dense-and-sparse method that requires an additional sparse matrix multiplication in addition to the dense matrix multiplication during inference, which introduces additional computational overhead. KVQuant and our proposed CQ both require learn- ing centroids on a calibration dataset, and we use the same calibration set of 16 sequences (each with 2048 tokens) of WikiText-2 for both methods. Calibration is performed once on the training set of WikiText-2, while perplexity and accuracy are evaluated on the test sets of different datasets and benchmarks. For each method, we report bits per floating-point number (FPN) to measure the compression rate, which is calculated as the number of bits in the quantized KV cache of each token divided by the number of FPN in the uncompressed KV cache of each token, excluding the constant storage overheads of centroids, scaling factors, and zero points. 4.1 Results The results of perplexity on WikiText-2 are presented in Table 1 and the results on C4 are presented in Table 2. CQ consistently outperforms non-dense-and-sparse quantization methods, especially in low bit-width regions. Despite using lower bit-width, CQ performs better or on par with the dense-and-sparse quantization method KVQuant-<b>b-1%. Dense-and-sparse quantization methods 9Table 3: Accuracy on 3 benchmarks under different KV cache quantization methods at varying bit-width. Bits Per FPN Task LLaMA-7b LLaMA-13b LLaMA-2-7b LLaMA-2-13b Mistral-7b FP16 16 WinoGrande 69.93 72.69 68.90 71.98 73.88 PIQA 78.67 79.16 78.07 79.16 80.58 ARC Challenge 41.72 46.42 43.43 48.29 50.34 KVQuant-4b 4.00-4.02 WinoGrande 69.53 72.61 67.96 71.59 73.88 PIQA 78.62 79.22 77.86 78.94 80.58 ARC Challenge 42.32 45.99 42.75 46.67 49.06 KVQuant-4b-1% 4.32-4.35 WinoGrande 70.72 73.40 68.67 72.30 73.72 PIQA 78.40 79.16 78.07 79.27 80.74 ARC Challenge 41.38 46.76 43.17 47.87 49.91 CQ-2c8b 4.00 WinoGrande 70.40 72.45 68.27 72.53 73.48 PIQA 78.61 79.11 77.91 78.62 80.52 ARC Challenge 41.55 45.99 43.34 47.78 49.15 KVQuant-2b 2.00-2.02 WinoGrande 53.59 59.35 51.70 51.30 63.46 PIQA 72.47 74.81 63.38 65.40 75.46 ARC Challenge 32.00 34.47 22.44 24.66 38.57 KVQuant-2b-1% 2.32-2.35 WinoGrande 68.03 71.43 67.64 70.17 70.80 PIQA 77.69 78.51 76.60 78.51 79.65 ARC Challenge 38.74 45.14 41.47 44.97 47.53 CQ-4c8b 2.00 WinoGrande 67.48 70.72 66.45 69.06 69.38 PIQA 76.11 78.29 76.12 77.42 79.49 ARC Challenge 38.48 44.03 39.93 44.11 45.65 KVQuant-1b 1.00-1.02 WinoGrande 50.51 48.46 50.91 49.41 49.80 PIQA 53.26 53.54 53.37 50.92 54.73 ARC Challenge 21.76 21.33 20.65 21.67 19.88 KVQuant-1b-1% 1.32-1.35 WinoGrande 56.67 61.01 57.77 57.30 58.17 PIQA 71.38 75.46 69.91 70.89 73.83 ARC Challenge 29.69 35.32 31.48 32.59 33.19 CQ-8c8b 1.00 WinoGrande 56.51 61.56 55.01 57.14 58.25 PIQA 71.16 73.99 71.22 73.01 75.24 ARC Challenge 30.20 33.79 30.20 34.30 33.79 CQ-8c10b 1.25 WinoGrande 60.46 65.27 59.19 62.98 63.93 PIQA 73.45 75.90 73.07 74.37 77.31 ARC Challenge 33.28 37.12 34.64 38.74 39.59 introduce additional inference overhead due to the extra sparse matrix multiplications for activation outliers which is inefficient on GPUs, while CQ does not have this limitation. The accuracy results on different benchmarks are shown in Table 3. CQ consistently outperforms the non-dense-and-sparse baseline KVQuant-<b>b at bit-width of 1 and 2, and performs better or on par with the dense-and-sparse baseline KVQuant-<b>b-1%. 4.2 Ablation Study We perform a set of ablation experiments to study the effectiveness of each component of our proposed approach. The results of the ablation experiments are shown in Table 4. We evaluate the perplexity of 2 models Mistral-7b and LLaMA-2-13b on WikiText-2 using CQ at 2 bits per FPN, withvaryingnumberofchannelscoupledandcomparinguniformcentroidlearningandFisher-guided centroid learning. Fisher-guided centroids significantly improve model quality as demonstrated by lower perplexity. With either uniform centroids or Fisher-guided centroids, perplexity improves as the number of coupled channels increases. Hence, our proposed techniques of channel coupling and 10Table 4: Ablation study: perplexity on WikiText-2 using CQ with varying number of coupled channels and fisher-guide centroids. Perplexity consistently improves as the number of coupled channels increases. Mistral-7b LLaMA-2-13b Bits Per FPN 2 2 2 2 2 2 2 2 2 2 2 2 Num. of Channels Coupled 1 2 4 1 2 4 1 2 4 1 2 4 Fisher-guided Centroids ✗ ✗ ✗ ✓ ✓ ✓ ✗ ✗ ✗ ✓ ✓ ✓ Perplexity ↓ 97.76 16.29 5.42 5.34 5.20 5.11 890.42 171.96 6.62 6.06 4.91 4.81 Fisher-guided centroid learning are both effective for maintaining model quality. 4.3 Overhead of Centroid Learning and Storage In this section, we discuss the computational overhead of centroid learning and the memory overhead of centroid storage for CQ. The centroid learning process of CQ consists of many independent k- means runs, which can be time-consuming on CPUs. Hence, we leverage a GPU implementation to accelerate the learning process. In all our experiments, we use k-means++ initialization and run 100 iterations of k-means on a single GPU to obtain the centroids. The memory overhead of storing the centroids can be calculated asl × 2 × h × c × 2b 16-bit floating-point numbers, wherel is the number of layers,2 is for keys and values,h is the number of key/value attention heads,c is the number of channels in a single-head key/value activation embedding, andb is the bit width of quantized codes. The detailed learning and memory overhead for different CQ configurations and models are given in Table 5. CQ can easily scale to large model sizes with low learning and memory overheads. Table 5: Learning and memory overhead of different CQ configurations and models. The number of centroid parameters are shown in millions, and the percentage to the model weights is shown in brackets. Centroid Learning Time Parameter Count in Centroids CQ Config. 2c8b 4c8b 8c8b 2c8b 4c8b 8c8b LLaMA-7b 53 mins 28 mins 14 mins 67.11M (0.996%) 67.11M (0.996%) 67.11M (0.996%) LLaMA-13b 94 mins 44 mins 22 mins 104.86M (0.806%) 104.86M (0.806%) 104.86M (0.806%) LLaMA-2-7b 54 mins 28 mins 14 mins 67.11M (0.996%) 67.11M (0.996%) 67.11M (0.996%) LLaMA-2-13b 83 mins 44 mins 23 mins 104.86M (0.806%) 104.86M (0.806%) 104.86M (0.806%) Mistral-7b 13 mins 7 mins 4 mins 16.78M (0.232%) 16.78M (0.232%) 16.78M (0.232%) 5 Related Works The high memory requirements and computational demands of LLMs pose a great challenge to effi- cient inference. Post-training model weight quantization has been shown to be effective for reducing inference latency and memory requirements. GPTQ [8] scales approximate Hessian-based weight quantization to large-scale models, and AWQ [19] preserves the salient weights in LLMs to achieve better quantized model quality. SqueezeLLM [16] leverages sensitivity-based non-uniform clustering and dense-and-sparse quantization for preserving salient weights and outliers. In addition to weight quantization, KV cache compression approaches are also effective for improving inference efficiency. Scissorhands [20] and H2O [37] achieve KV cache compression while preserving model quality by 11evicting unimportant tokens and only storing pivotal tokens. KIVI [21] quantizes key activations channel-wise and value activations token-wise, and uses a residual to achieve better model quality. KVQuant [10] proposes sensitivity-based quantization and dense-and-sparse quantization for KV cache. FlashAttention [5] improves the inference efficiency of attention-based models on GPUs by fusing kernels to eliminate unnecessary global memory reads/writes, while NoMAD-Attention [36] accelerates attention computations on CPUs by leveraging in-register shuffles. Product Quantiza- tion [12] is an approximate nearest neighbor search method that compresses vectors by decomposing the vector space into a Cartesian product of low-dimensional subspaces, and jointly quantizing the dimensions within each subspace. 6 Conclusion In this work, we propose Coupled Quantization (CQ) for enabling more efficient LLM inference by compressing KV cache, which is the latency and throughput bottleneck in long context or large batch size settings. We observe that distinct channels of key/value activation embeddings exhibit high levels of dependency, which has not been leveraged by existing compression methods. Motivated by this insight, we propose channel coupling for exploiting the inter-channel dependency to achieve more information-efficient encoding of key/value activations. Furthermore, we propose Fisher-guided centroid learning to better preserve salient activations and model quality. Extensive experiments demonstrate that our method mostly outperforms existing methods in terms of model quality under the same quantization bit width. Moreover, CQ can preserve model quality reasonably well with KV cache quantized down to 1-bit. References [1] David Arthur, Sergei Vassilvitskii, et al. k-means++: The advantages of careful seeding. In Soda, volume 7, pages 1027–1035, 2007. [2] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about phys- ical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 7432–7439, 2020. [3] Lingjiao Chen, Matei Zaharia, and James Zou. Frugalgpt: How to use large language models while reducing cost and improving performance.arXiv preprint arXiv:2305.05176, 2023. [4] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [5] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness.Advances in Neural Information Processing Systems, 35:16344–16359, 2022. [6] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient fine- tuning of quantized llms.Advances in Neural Information Processing Systems, 36, 2024. [7] Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: Pure attention loses rank doubly exponentially with depth. InInternational Conference on Machine Learning, pages 2793–2803. PMLR, 2021. 12[8] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post- training quantization for generative pre-trained transformers.arXiv preprint arXiv:2210.17323, 2022. [9] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models.arXiv preprint arXiv:2203.15556, 2022. [10] Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W Mahoney, Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cache quantization.arXiv preprint arXiv:2401.18079, 2024. [11] Minyoung Huh, Hossein Mobahi, Richard Zhang, Brian Cheung, Pulkit Agrawal, and Phillip Isola. The low-rank simplicity bias in deep networks.arXiv preprint arXiv:2103.10427, 2021. [12] Herve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search. IEEE transactions on pattern analysis and machine intelligence, 33(1):117–128, 2010. [13] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chap- lot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b.arXiv preprint arXiv:2310.06825, 2023. [14] Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. Challenges and applications of large language models. arXiv preprint arXiv:2307.10169, 2023. [15] Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke Hüllermeier, et al. Chatgpt for good? on opportunities and challenges of large language models for education.Learning and individual differences, 103:102274, 2023. [16] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023. [17] Alexander Kraskov, Harald Stögbauer, and Peter Grassberger. Estimating mutual information. Physical review E, 69(6):066138, 2004. [18] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. arXiv preprint arXiv:2102.05426, 2021. [19] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023. [20] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time.Advances in Neural Information Process- ing Systems, 36, 2024. 13[21] Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: A tuning-free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750, 2024. [22] Stuart Lloyd. Least squares quantization in pcm.IEEE transactions on information theory, 28(2):129–137, 1982. [23] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. [24] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An impera- tive style, high-performance deep learning library.Advances in neural information processing systems, 32, 2019. [25] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.OpenAI blog, 1(8):9, 2019. [26] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1–67, 2020. [27] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale.Communications of the ACM, 64(9):99–106, 2021. [28] Claude Elwood Shannon. A mathematical theory of communication.The Bell system technical journal, 27(3):379–423, 1948. [29] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding.Neurocomputing, 568:127063, 2024. [30] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models.arXiv preprint arXiv:2302.13971, 2023. [31] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models.arXiv preprint arXiv:2307.09288, 2023. [32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need.Advances in neural information processing systems, 30, 2017. [33] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Transformers: State-of- the-art natural language processing. InProceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, pages 38–45, 2020. [34] Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective long- context scaling of foundation models.arXiv preprint arXiv:2309.16039, 2023. 14[35] Xi Yang, Aokun Chen, Nima PourNejatian, Hoo Chang Shin, Kaleb E Smith, Christopher Parisien, Colin Compas, Cheryl Martin, Anthony B Costa, Mona G Flores, et al. A large language model for electronic health records.NPJ digital medicine, 5(1):194, 2022. [36] Tianyi Zhang, Jonah Wonkyu Yi, Bowen Yao, Zhaozhuo Xu, and Anshumali Shrivastava. Nomad-attention: Efficient llm inference on cpus through multiply-add-free attention.arXiv preprint arXiv:2403.01273, 2024. [37] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H2o: Heavy-hitter oracle for effi- cient generative inference of large language models.Advances in Neural Information Processing Systems, 36, 2024. 15Appendix A Correlation Matrices and Scatter Plots 0 15 31 Layer 1  Layer 2  Layer 3  Layer 4  Layer 5  Layer 6  Layer 7  Layer 8 0 15 31 Layer 9  Layer 10  Layer 11  Layer 12  Layer 13  Layer 14  Layer 15  Layer 16 0 15 31 Layer 17  Layer 18  Layer 19  Layer 20  Layer 21  Layer 22  Layer 23  Layer 24 0 15 31 0 15 31 Layer 25 0 15 31 Layer 26 0 15 31 Layer 27 0 15 31 Layer 28 0 15 31 Layer 29 0 15 31 Layer 30 0 15 31 Layer 31 0 15 31 Layer 32 1.0 0.5 0.0 0.5 1.0 Channel Channel Figure 5: Correlation matrix for the first 32 channels of pre-RoPEkey activation embeddings of all LLaMA-7b layers on WikiText-2. 160 15 31 Layer 1  Layer 2  Layer 3  Layer 4  Layer 5  Layer 6  Layer 7  Layer 8 0 15 31 Layer 9  Layer 10  Layer 11  Layer 12  Layer 13  Layer 14  Layer 15  Layer 16 0 15 31 Layer 17  Layer 18  Layer 19  Layer 20  Layer 21  Layer 22  Layer 23  Layer 24 0 15 31 0 15 31 Layer 25 0 15 31 Layer 26 0 15 31 Layer 27 0 15 31 Layer 28 0 15 31 Layer 29 0 15 31 Layer 30 0 15 31 Layer 31 0 15 31 Layer 32 1.0 0.5 0.0 0.5 1.0 Channel Channel Figure 6: Correlation matrix for the first 32 channels ofvalue activation embeddings of all LLaMA- 7b layers on WikiText-2. Figure 7: 2-dimensional scatter plots of pairs of channels inkey activation embeddings of 4 LLaMA- 7b layers on WikiText-2. 17Figure 8: 2-dimensional scatter plots of pairs of channels in value activation embeddings of 4 LLaMA-7b layers on WikiText-2. 18",
      "meta_data": {
        "arxiv_id": "2405.03917v1",
        "authors": [
          "Tianyi Zhang",
          "Jonah Yi",
          "Zhaozhuo Xu",
          "Anshumali Shrivastava"
        ],
        "published_date": "2024-05-07T00:25:20Z",
        "pdf_url": "https://arxiv.org/pdf/2405.03917v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper identifies that KV cache is a primary bottleneck for LLM inference, especially at large batch sizes or long context lengths, and existing quantization methods fail at very low bit widths. It proposes Coupled Quantization (CQ), a novel KV cache quantization method, by observing that distinct key/value activation channels are highly inter-dependent. CQ exploits this inter-dependency by jointly encoding multiple channels, enabling high compression rates (down to 1-bit) while preserving model quality, outperforming or being competitive with existing baselines.",
        "methodology": "Coupled Quantization (CQ) divides key/value activation embedding channels into equally sized, non-overlapping groups of contiguous channels. These coupled channels are jointly quantized, sharing a single multi-channel centroid, with dimensionality equal to the number of channels in the group. Quantization maps each channel group to its nearest centroid using L2 distance. Centroids are learned offline on a calibration dataset using k-means algorithm with k-means++ initialization (for uniform centroid learning) or Fisher-guided centroid learning. Fisher-guided learning uses the diagonals of the Fisher information matrix (element-wise square of gradients) to bias centroid selection towards preserving precision of more influential activations. CQ applies channel-coupled quantization for both keys and values, quantizing keys before RoPE application.",
        "experimental_setup": "Experiments were conducted on a Linux server with 2 AMD EPYC 7742 CPUs, 1.5TB RAM, and 4 NVIDIA A100 40GB GPUs, using PyTorch and HuggingFace Transformers. Five LLMs were evaluated: LLaMA-7b, LLaMA-13b, LLaMA-2-7b, LLaMA-2-13b, and Mistral-7b. Model quality was assessed using perplexity on WikiText-2 and C4 datasets, and accuracy on WinoGrande, PIQA, and ARC Challenge benchmarks in a zero-shot setting. Evaluation used the maximum context length of each LLM (2048 for LLaMA, 4096 for LLaMA-2, 8192 for Mistral). Baselines included uncompressed FP16, uniform integer (INT) quantization, NormalFloat (NF) quantization (both with and without group size 128), and KVQuant (with and without 1% sparse outliers). Centroid learning for CQ and KVQuant used a calibration set of 16 WikiText-2 sequences (2048 tokens each). Performance was measured in bits per floating-point number, learning time, and centroid memory overhead.",
        "limitations": "Centroid learning for CQ is an offline process that can be computationally intensive, though accelerated by GPU implementation. Storing centroids introduces a constant memory overhead (e.g., 0.232%-0.996% of model weights for Mistral-7b and LLaMA models, respectively), which is not accounted for in the 'bits per FPN' compression rate metric. Fisher-guided centroid learning, while improving model quality (perplexity), can increase the overall quantization error. The empirical estimation of joint entropy for motivating CQ was limited to a maximum group size of 4, as larger groups required exponentially more data to maintain estimation quality.",
        "future_research_directions": "Future work could explore more sophisticated or adaptive channel coupling strategies beyond contiguous groups to further exploit inter-channel dependencies. Investigating methods to reduce the computational and memory overheads of centroid learning and storage, possibly through online learning or more efficient representations, would be beneficial. Combining Coupled Quantization with other KV cache compression techniques, such as token eviction or more advanced sparse outlier handling, could lead to even higher compression ratios or improved performance. Optimizing CQ for various hardware architectures or specific LLM model types could further enhance inference efficiency."
      }
    },
    {
      "title": "MiniCache: KV Cache Compression in Depth Dimension for Large Language Models",
      "abstract": "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance.",
      "full_text": "MiniCache: KV Cache Compression in Depth Dimension for Large Language Models Akide Liu1 Jing Liu1 Zizheng Pan1 Yefei He2 Gholamreza Haffari1 Bohan Zhuang1,2† 1ZIP Lab, Monash University, Australia 2ZIP Lab, Zhejiang University, China Abstract A critical approach for efficiently deploying computationally demanding large lan- guage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value states of previously generated tokens, significantly reducing the need for repetitive computations and thereby lowering latency in autoregressive generation. However, the size of the KV cache grows linearly with sequence length, posing challenges for applications requiring long context input and extensive sequence generation. In this paper, we present a simple yet effective approach, called MiniCache, to compress the KV cache across layers from a novel depth perspective, significantly reducing the memory footprint for LLM inference. Our approach is based on the observation that KV cache states exhibit high similarity between the adjacent layers in the middle-to-deep portion of LLMs. To facilitate merging, we propose disentangling the states into the magnitude and direction components, interpolating the directions of the state vectors while preserving their lengths unchanged. Furthermore, we in- troduce a token retention strategy to keep highly distinct state pairs unmerged, thus preserving the information with minimal additional storage overhead. Our Mini- Cache is training-free and general, complementing existing KV cache compression strategies, such as quantization and sparsity. We conduct a comprehensive evalua- tion of MiniCache utilizing various models including LLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks, demonstrating its exceptional performance in achieving superior compression ratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit MiniCache achieves a remarkable compression ratio of up to 5.02×, enhances inference throughput by approximately 5×, and reduces the memory footprint by 41% compared to the FP16 full cache baseline, all while maintaining near-lossless performance. Project is available at https://minicache.vmv.re 1 Introduction Large Language Models (LLMs), exemplified by the GPT series [ 1, 2, 3] and the LLaMA series [4, 5, 6], have emerged as pivotal innovations within the artificial general intelligence, significantly enhancing the capabilities of natural language processing. However, these models are meticulously trained using extensive computational resources [7] and massive datasets [8], which enables them to produce text that effectively mimics human writing styles and conducts complex reasoning analysis, yet raises the challenge of efficient deployment and serving. Within the inference framework of †Corresponding author. Email: bohan.zhuang@gmail.com Preprint. Under review. arXiv:2405.14366v2  [cs.CL]  7 Sep 2024(a)Cross-layer KV cachesimilarity (b)Merged layersvs EMscore on GSM8K Layer𝑙−1 Layer𝑙−2  ... Layer2 Layer1 LMHead Input Layer𝑙−3 Pruning/Quant. K VKVCacheCompression T Decoding Cross Layer Merging K V QK V Attention Decoding KVCacheCompression QK V Attention T+1 Prevs. MiniCache (c)Comparisonbetween MiniCacheand previous methods ...... CosineSimilarity  T T+1 Number of Layers Merged on LLaMA-3-70B Figure 1: Overview of our MiniCache strategy and example results: (a) shows the observation that the KV cache states between two adjacent layers are highly similar, particularly across the middle to deep layers. The x-axis uses index/2 to represent the similarities for each pair of layers. (b) compares the performance of MiniCache, and the mean baseline, which simply averages the KV caches of two layers, using the LLaMA-3-70B model [6] on the GSM8K dataset [10]. MiniCache, which begins merging from the half-layer depth, achieves near-lossless performance. (c) highlights the primary difference between MiniCache and previous approaches. MiniCache investigates the inter-layer redundancy of KV caches along the depth dimension of LLMs, an aspect overlooked by intra-layer-based methods. Here, T refers to the last timestamp of pre-filling, and T + 1 des to the first timestamp of decoding. LLMs, KV caches [9] are crucial for storing pre-computed keys and values, thus avoiding repeated calculations over the preceding context and substantially enhancing LLMs’ deployment efficiency. However, the increasing demand for longer sequence lengths results in voluminous cached states, leading to significant memory consumption during generation. For instance, a 175B GPT-3 model [2], with a batch size of 64 and a sequence length of 4,096 tokens (both prefilled and generated), requires approximately 1,208GB of GPU memory. This requirement is3.45× greater than the memory used to store the model’s weights. In this context, KV cache compression is of paramount importance due to its clear benefits: 1) it largely reduces the memory footprint allowing for faster generation and larger batch serving; 2) it significantly lowers the cost per token, demonstrating substantial commercial benefits. Existing KV cache compression efforts can be roughly categorized into two types, namely quantization and sparsity. The quantization approaches [11, 12] propose storing the KV states in low-bit numerical values. Typically, FlexGen [13] demonstrates that 4-bit KV cache quantization can achieve lossless performance. In contrast, sparsity-driven methods aim to retain only the salient tokens while evicting the rest, either heuristically [14, 15] or adaptively [16]. Some approaches [11] explore the intersection of these two types, by assigning high-bit to salient tokens and extremely low-bit to the rest of the tokens, achieving more aggressive memory gain. Despite these innovations, existing literature merely consider the intra-layer redundancy, while neglecting another important complementary direction – the inter-layer redundancy, as illustrated in the Figure 1(c). Our analysis begins by exploring the redundancy of KV cachesalong the depth dimension, as shown in Figure 1(a). We observe that KV cache states exhibit high similarity between neighbouring layers in 2the middle-to-deep portions of LLMs. This intriguing property suggests that states paired by position between adjacent layers can be accurately merged into a single state space with a strong performance guarantee, as illustrated in Figure 1(b). This approach significantly reduces the memory footprint without the need to retain individual states for each attention layer. Note that these observations are pertinent to dynamic inference strategies such as mixture-of-depths [17] and layer-wise early exiting [18, 19], which optimize computational paths by skipping non-critical layers to enhance training and inference efficiency. Furthermore, layer pruning methods [20] highlight considerable redundancy in deeper layers. However, despite these advancements, the redundancy of KV caches along the depth dimension has largely been overlooked. In this paper, we propose MiniCache, a simple yet effective cross-layer KV cache compression method aimed at advancing the inference efficiency of LLMs. MiniCache consists of two essential components. Firstly, we introduce an accurate cache merging strategy, employing a reparameteri- zation of state vectors that decompose them into the magnitude and direction components, akin to weight normalization [21]. This approach allows for effective interpolation of the directional compo- nent in polar coordinates while preserving the original state norms to retain as much information as possible. This interpolation refers to the cross-layer merging as shown in the Figure 1(c). Secondly, we recognize that a small subset of state pairs, characterized by low similarities but carrying largely distinct semantic meanings, are unsuitable for inter-layer merging. To address this, we propose a token retention strategy to minimize performance degradation, which involves separately retaining these outlier pairs. Our framework is notably memory-efficient, requiring storage for only a single high-dimensional directional component, along with minimal extra memory overhead. The over- head consists of a few unmergeable tokens and their corresponding indexes, as well as token-wise magnitudes to accurately restore the original states. We conduct extensive experiments with representative LLMs, including Mixtral-8x7B [ 22], Phi- 3-Mini [23], and LLaMA-3 [ 6] 8B and 70B, respectively. Our method is benchmarked across a diverse range of question answering and generation datasets [24, 25, 26, 27, 28, 29, 30, 31] using the lm-eval-harness [32]. Additionally, we evaluate our results on LongBench [33] for long-sequence generation. The results demonstrate that MiniCache can reduce the memory footprint required for LLM inference by up to 41%, while simultaneously enhancing throughput by approximately 5× compared to fully cached baseline, clearly surpassing existing methods [11, 12, 14, 15]. Our contributions are summarized as follows: • We introduce MiniCache, a simple yet highly effective framework for KV cache compression. MiniCache pioneers the exploration of KV cache compression along the depth dimension, thereby significantly expanding its capabilities. • We observe a fascinating characteristic of cross-layer KV cache states: high similarity between adjacent layers in the middle to later stages of LLMs. Additionally, we find that not all state pairs are suitable for merging. • We propose an accurate and memory-efficient method for cross-layer cache merging, comprising a reparameterization strategy and a token retention mechanism. Our method complements existing KV cache compression approaches, further enhancing LLM serving efficiency. • Our MiniCache performs favourably against the state-of-the-art methods. Notably, our 4-bit MiniCache achieves a strong compression ratio of up to5.02×, 5× higher inference throughput, and 41% memory reduction compared to the FP16 full cache baseline with near-lossless performance. 2 Related Work Efficient inference for LLMs. Large Language Models (LLMs) are constrained by considerable computational and memory requirements during inference, particularly in resource-constrained environments. To mitigate these challenges, a variety of efficient inference techniques have been developed. For instance, dynamic inference methods [ 18, 34, 35, 36], represented by mixture-of- experts (MoE) [37, 38, 39, 40, 41], adaptively select specific sub-structures of the model during the inference process based on the input data, significantly improving inference efficiency while keeping model capacity. Techniques like Multi-Query Attention [42, 43], Kernel-driven attentions [44, 45, 46, 47], and low-rank attentions [41, 48, 49, 50] approximate the functionality of traditional attention mechanisms but with more efficient implementations. Quantization strategies [51, 52, 53, 54] 3involve converting the model’s weights and activations into a low bit-width format, thereby reducing memory footprint and computational intensity. Sparsification approaches [14, 15, 55, 56] eliminate unnecessary elements in both model weights and token representations, further enhancing efficiency. Some closely related works, such as MoD [17] and LayerSkips [19], considered the dynamic inference nature to ignore unimportant layers according to input. However, these methods require an additional fine-tuning process or carefully designed pre-training stages, which reduces the adaptivity of these methods. MiniCache relies on inter-layer similarity observations to perform cross-layer merging, significantly reducing memory demand. Model merging. Merging compression involves the aggregation of a model’s parameters and activations at various granularities. This process enhances the efficiency of inference in large models and facilitates huge redundancy [57]. Linear Mode Connectivity (LMC) [58] enables the fine-tuning of models from a shared pre-trained base. Commonly, weight averaging [ 59] is employed as an efficient technique to perform merge compression. Notably, Model Soup [60] utilizes linear averaging in this context. Advanced methods like TIES Merging [61], Model Breadcrumbs [62], and DARE [63] further enhance this process by sparsifying and combining model parameters, enabling the merging of models without sacrificing performance capabilities. Additionally, Spherical Linear intERPolation (SLERP) [64] extends beyond simple weight averaging by interpolating between model parameters. The Fisher information matrix [65] and RegMean-based methods [66] further optimize merges to produce ideal weights, minimizing the ℓ2 distance to generation outputs while preserving the privacy of the training data. However, most existing works focus on merging model parameters, with the concept of depth-wise mergeability not being thoroughly explored in prior research. MiniCache focuses on the KV cache token merging in the depth dimensional of LLMs. 3 Motivation In the below, we present our new observations in a novel cross-layer perspective. LLama 2 7BLLama 2 30BLLama 3 8BMixtral 8x7B Models 0 10 20 30 40 50Exact Match (%) Baseline Mean KV (a) Simple average baseline vs. full cache on GSM8K 0 20 40 60 80 100 T okens Index 0.6 0.4 0.2 0.0 0.2 0.4 0.6 Cosine Similarity Layer 16 - 17 Layer 18 - 19 Layer 20 - 21 Layer 22 - 23 Layer 24 - 25 Layer 26 - 27 Layer 28 - 29 Layer 30 - 31 (b) Pairwise similarity in adjacent layers KV cache MathQA OpenBookQA PiQA RTE Winogrande 0.00.20.40.60.8 MiniCache Baseline Mean (c) Benchmark on five QA datasets Figure 2: Overall of our explorations and observations : (a) shows the strong baseline by performing average merging on the KV cache. (b) shows the pairwise similarity of cache states between adjacent layers. (c) compares the MiniCache, simple average, and full cache baseline across five different datasets. 3.1 Cross-Layer Redundancy in KV Cache Prior studies have revealed the ineffectiveness of middle-to-deep layers in LLMs [20]. Thus, layer- wise early exiting in this case can effectively avoid the redundant computation with minor effect on LLM performance [19, 67]. Inspired by this, we explore a layer-wise merging of KV cache in LLMs, starting with a simple baseline by averaging full tokens across adjacent layers. We provide our key observations as follows. Observation 1: KV cache shares a high similarity between adjacent layers. Based on LLaMA-3- 70B [6], we conduct zero-shot inference on the validation sets of three widely recognized benchmarks: COQA [68], GSM8K [10] and TruthfulQA [69]. In general, we find that KV cache in the shallow layers exhibits low similarity, whereas those in the middle-to-deep layers are very similar to one another based on angular distance, as shown in Figure 1(b). Next, we merge the KV cache across 4adjacent layers by conducting five-shot inference with LLaMA-2-7B, LLaMA-2-13B [5], LLaMA- 3-8B [6], and Mixtral-8x7B [22] on GSM8K [10]. Specifically, starting from the middle layer of each model, we merge the KV cache in the adjacent two layers. As shown in Figure 2(a), we observe a favourable performance for different LLMs, which reveals the huge potential for efficiency improvement by sharing the KV cache across adjacent layers during LLM decoding. Observation 2: Not all tokens are equally important to merge, a few distinct pairs require retention. Recent works [15, 16] in KV cache compression have found that keeping a few salient tokens at each layer, which contribute the majority of attention scores, could be sufficient to maintain LLM performance. In our case, we speculate that certain token pairs in adjacent layers also exhibit outlier behaviours, showing strong semantic differences that make them unsuitable for merging. Based on COQA [ 68] and LLaMA-2-7B [ 5], we investigate the similarities at the level of token pairs. As shown in Figure 2(b), we find a significant portion of token pairs share high similarities across adjacent layers. However, we observe a few outlier pairs, such as indices 0 and 15, with large margins of difference. We consider these tokens non-mergeable due to their significant differences. We also show that merging these distinct tokens results in performance degradation, corresponding to γ = 0 row in Table 2. Thus, while cross-layer merging is a promising strategy for reducing memory burdens, it must be implemented with careful consideration of token-level similarities to ensure optimal performance, as shown in Figure 2(c). 4 Method In this section, we introduce our MiniCache, a simple yet effective method aimed at trimming the KV cache redundancy in the depth dimension. This framework exploits the high similarity of KV cache states between adjacent layers and consists of two primary components: a reparameterization-based merging strategy and a token retention mechanism. The merging strategy compresses the KV cache states in adjacent layers to aggregate them into a single shared memory space, beginning from the middle of the model. The token retention mechanism mitigates information lost by retaining the highly distinct state pairs with minimal additional memory cost. With the merged cache, retention tokens, and magnitudes, we can accurately restore the original cache states for token decoding. 4.1 Cross-Layer Compression Our method commences with the identification of an optimal starting layer S. Observations in Section 3.1 indicate that the KV cache from middle-to-deep layers consistently exhibits patterns of high similarity across adjacent layers. Consequently, we select the starting layer from the middle of the LLM, specifically S = L/2. From this layer onward, the KV pairs are assumed to be sufficiently similar across adjacent layers to warrant their consolidation. Central to this approach is a merge function, F, which is designed to integrate the KV caches of consecutive layers into a single, unified cache. We define x as the vectorized cache state of a single token, where the superscript indicates the layer index and the subscripts k and v denote the keys and values, respectively. Specifically, for a pair of key/value tokens at the same position in layers l and l − 1, the merged cache is computed as cl,l−1 k = F(xl k, xl−1 k ), cl,l−1 v = F(xl v, xl−1 v ). (1) This consolidation process effectively eliminates the need to store and process the original memory- intensive keys and values in each layer independently. Instead, it approximates a shared cache across the adjacent layers. 4.2 KV Cache Merging and Restoration Reparameterization-based cache merging. To perform the pairwise merging, one solution is to directly average a pair of KV tokens, analogous to model merging [60, 61]. However, we observe that direct averaging can cause significant information loss. We conjecture that the distance between activations can be larger than that of weights due to the presence of outlier activation channels with extremely large magnitudes in LLMs [70, 71], while weights typically have relatively quite small magnitudes. A potential method to compensate for this information loss is to project from c to xl−1 and xl, then rescale the projected vectors based on their relative magnitudes to exactly restore the 5(a)Cross-layer Compression KV Store𝐶 Keep Rescale Recover (b)Restoration𝑙−1𝑙 𝑙−1 𝑙 Keep × Fetch𝐶\tforlayer 𝑙and𝑙−1KVcachecompressionat layer𝑙 original KV Cache merged KV Cache retentiontokencachemagnitudemergeoperation Figure 3: The illustration of the proposed methodMiniCache. (a) depicts the cross-layer compression process. We fetch the KV caches, from layers l and l − 1, and merge them into shared states via Eq. (3). Additionally, we compute the ℓ2 norm for the caches to obtain their magnitudes. Furthermore, we select unmergable tokens for retention, then store merged cache, retention tokens, and magnitudes at layer l in C. (b) illustrates the restoration process for layers l and l − 1, which includes magnitude rescaling in Eq. (2) and retention token recovery. original states. However, this approach requires extensive additional storage and computations; for example, restoring xl−1 needs both c and xl, which undermines the benefits of cache merging. To efficiently merge token pairs, we draw inspiration from weight normalization [21], which disentangles model parameters into the magnitude and direction components to accelerate the convergence of stochastic gradient descent. Additionally, we take cues from DoRA [72], which employs a similar way to resemble the learning behavior of parameter-efficient fine-tuning compared to full fine-tuning. In our case, the reparameterization can be formulated as follows: ˆxl = el,l−1 · ∥xl∥ ∥el,l−1∥, ˆxl−1 = el,l−1 · ∥xl−1∥ ∥el,l−1∥, (2) where e is the directional vector. This decomposition ensures that el,l−1 ∥el,l−1∥ is a unit vector, and allows the restored states to match the ℓ2 norm of the original states, thereby preserving the cache’s information as much as possible. The restoration is shown as Figure 3(b). For brevity, we omit the subscripts k and v, as keys and values are decomposed in the same way. For estimating the directional component el,l−1, we follow SLERP [64], which adaptively handles the interpolation, which often resembles rotation-like transformations. The choice of SLERP as the merging function is strategic, as it facilitates interpolation along the shortest path on the unit sphere between two high-dimensional vectors, thereby preserving their geometric integrity, which refers to merge operation in Figure 3(a). This is crucial for maintaining the semantic and syntactic properties of the KV caches. The formula for SLERP in our context is: el,l−1 = sin((1 − t)Ωl,l−1) sin(Ωl,l−1) · xl−1 ∥xl−1∥ + sin(tΩl,l−1) sin(Ωl,l−1) · xl ∥xl∥, (3) where Ωl,l−1 = arccos \u0010 xl·xl−1 ∥xl∥∥xl−1∥ \u0011 represents the angle between vectors xl and xl−1, and sin(·) is the sine function. t is an interpolation hyperparameter that adjusts the relative influence of each vector on the resulting direction, tailored to the layer depth and specific characteristics of the KV pairs. Note that when we set t = 0.5, it will become an average merging along the geometry surface, which we consider special cases in Eq. (A). The merged cache for each token pair is then a concatenation of the directional vector, magnitude and Ωl,l−1, denoting as cl,l−1 = [el,l−1, ∥xl−1∥, ∥xl∥, Ωl,l−1], cached components as shown in Figure 3(a). Note that apart from storing the merged directional vector, we only need to store additional token-wise magnitude and angle scalars, which is memory efficient. In this way, we achieve substantial memory efficiencies through reduced redundancy while ensuring the retention of the critical functional characteristics of the original KV pairs across transformer layers. Unmergeable token retention. Highly distinct pairs are sensitive to merging operations, leading us to propose unmergeable token retention, as shown in Figure 3(a). Despite the high similarity between KV cache states across neighbouring layers, a few sensitive distinct pairs remain that are significantly 6difficult to merge and share. Aligned with previous studies, these distinct tokens carry substantial semantic meanings [15, 16]. We observe that merging sensitive tokens, which results in the loss of layer-specific information, can lead to significant performance degradation. Therefore, it is crucial to properly disentangle the shared and unique information between adjacent layers. To address this issue, we designed a token retention strategy to selectively retain tokens that cannot be merged based on their angular distance, defined as: d(xl, xl−1) = 1 π Ω. For the KV caches, the minimum and maximum angular distances are determined to identify the unmergeable tokens. The set of required token indices to keep, I, is obtained by: I = {i | di < dmin + (dmax − dmin) · γ}, (4) where γ is a predefined hyperparameter that controls the retention threshold. The tokens with indices in I are retained and not compressed during the merge, which ensures that performance does not decline by preventing the loss of unmergeable tokens. Next, let X ∈ Rn×h be either the key or value cache at one attention layer, where n denotes the number of tokens and h is the number of hidden dimensions, and E ∈ Rn×h be the shared KV cache states. For each pair of neighbouring two layers, the unmergeable tokens are selected along with the token dimension by Rl = Xl[I], Rl−1 = Xl−1[I], then restoring to our compressed caches by ˆXl[I] = Rl, ˆXl−1[I] = Rl−1, as shown in Figure 3(b). Overall, we share the final cache for the two layers as Cl,l−1 = [El,l−1, Rl, Rl−1, ∥Xl−1∥, ∥Xl∥, I]. This cache includes the shared KV cache states, retention of unmerged tokens, magnitude vectors for each layer, and the token-keeping index, respectively. These additional components are quite lightweight. Thus compared to full-layer caches, our method remains memory-efficient, as discussed in Sec. 4.3. Cache restoration. After obtaining the shared cacheCl,l−1, we further need to approximately restore the original cache states for the current token decoding, as shown in Fig. 3(b). Specifically, to restore Xl, we first rescale the directional shared states with the corresponding magnitude vector along the token dimension, denoted as El,l−1∥Xl∥. Subsequently, we perform retention token recovery by placing the sensitive tokens according to their token indices. 4.3 Efficiency Discussion Compression efficiency. We primarily analyze our memory efficiency in terms of the number of tokens used. Next, let r be the number of layers and and b is the batch size, s and n are input and output sequence length respectively. We consider the FP16 storage for KV cache. The full cache memory usage is given by 4brh(s + n). In our study, we begin merging layers from the middle to the deeper layers, consolidating the KV cache states of every two layers into a single shared state space. As a result, we effectively reduce the GPU memory usage in the decoding inference to 3brh(s + n), demonstrating a significant compression rate. Restoration efficiency. We then analyze the additional memory cost incurred during the restora- tion process, which During the magnitude rescaling phase, we save an additional norm vector for the corresponding layers in the KV cache. It is important to note that the norm vector is in the shape of Rb×s×1, which has a single channel dimension compared to the fully ranked original KV states. Additionally, we suppose that the retention threshold can be set to 0.05. Therefore, we have brh(0.05(s + n)) tokens retained without compression. Finally, our overall memory requirement is given by (3.1h + 2)br(s + n). The detailed derivation is shown in the Appendix E. 5 Experiments We demonstrated that our MiniCache can perform merging compression on the latter half of the layers of LLMs with minimal performance degradation. Implementation details. Our experiments are based on representative model families of LLMs, including a compact LLM Phi-3-Mini [23] and an MoE LLM Mixtral-8x7B [22]. Additionally, we adopt LLaMA-3 [6] 8B and 70B models to explore how our method generalizes to larger LLMs. We sample ten tasks from lm-eval-harness [32], including COPA [24], MathQA [25], OpenBookQA [26], PIQA [27], RTE [28], WinoGrande [29], XSUM [30], and CNN/Daily Mail [31]. We also evaluate long-sequence generation on LongBench [33]. We compare our method with a fully cached baseline, 7and other methods such as round-to-nearest quantization (RTN) [73], SmoothQuant [70] and KIVI [11]. For the proposed MiniCache, we set the interpolation parameter t to 0.6, indicating that the merged results have a smaller rotation angle to the next layer. Furthermore, we set the token retention threshold γ to 0.05, according to the statistics of unmergeable tokens across multiple datasets. In addition to our merging method, we also consider a strong baseline of average merging. For sequential loading of large models, we utilize NVIDIA 4 A100 80GB GPUs, more details refers to Appendix D. Main results. We evaluate MiniCache by merging KV caches across all layers on GSM8K, COQA, and TruthfulQA. The results are shown in Figure 4. In general, we demonstrate the general effective- ness of merging KV caches from middle-to-deep layers across different sized LLMs. Moreover, the proposed MiniCache demonstrates a consistent and significant advantage over the averaging baseline. We also illustrate the performance of merging KV caches across half of the layers with the blue lines, where MiniCache still maintains a robust performance and achieves the best compression ratio. Besides, we find that our method is even more effective for larger LLMs. For instance, based on LLaMA-3-70B, MiniCache shows nearly zero performance drop even with the KV cache in 87.5% of the layers merged on the COQA dataset. This highlights the adaptability and efficiency of our approach in handling large-scale models while ensuring minimal performance degradation. LongBench. We also conduct experiments to evaluate performance and quality in long-sequence gen- eration using the LongBench dataset [33], as shown in Table 1. Our experiments applied MiniCache over several models: LLaMA-2-7B-Chat, LLaMA-2-13B-Chat, Mistral-7B, and Mistral-7B-Instruct. It is important to note that our MiniCache method maintains orthogonality with all existing quan- tization and sparsity (refers to Table A) methods at both the model and token-wise levels. When combined with KIVI-4bit KV cache quantization, our approach achieves a compression rate of5.02×, with minimal impact on accuracy across various challenging long-context generation tasks. The combination of MiniCache and KIVI-4bit KV cache quantization demonstrates significant memory savings without compromising the model’s ability to handle long sequences effectively. This high- Figure 4: Performance comparisons between our proposed MiniCache with the “averaging baseline” and the “unmerged full cache baseline” on multiple datasets with Phi3-Mini, Mixtral-8x7B, LLaMA- 3-8B, and LLaMA-3-70B. More result details are shown in Appendix F. The x-axis indicates the number of layers merged. As more layers are merged, a greater reduction in memory usage is achieved. 8Table 1: Evaluation of different KV cache compression methods on LongBench. MiniCache builds on top of 4-bit KIVI [11] and achieves the best performance with the strongest compression rate. Model Method LCC RepoBench-P PR-en TREC 2wikimqa GovReport MQA-zh AverageCompressionRatio Llama-2-7B-Chat Baseline 58.16 52.19 10.12 64.00 31.12 27.09 10.12 36.41 1xRTN [73] 15.44 8.76 0.79 4.00 0.30 1.93 0.07 4.90 3.21xSmoothQuant [70] 35.31 32.18 0.79 28.75 7.45 11.83 1.68 16.28 2.15xKIVI-2 [11] 49.32 43.71 4.50 63.00 24.07 24.73 10.24 31.51 3.95xMiniCache 58.03 52.01 9.00 64.00 30.58 25.32 10.13 35.44 5.02x Llama-2-13B-Chat Baseline 48.06 50.08 14.25 68.50 13.09 27.76 7.23 32.71 1xRTN [73] 20.89 18.62 0.33 0.00 0.52 1.68 0.16 6.03 3.21xSmoothQuant [70] 32.17 33.86 2.65 48.00 3.53 12.47 0.47 19.16 2.15xKIVI-2 [11] 48.60 48.81 13.50 68.00 14.32 25.70 7.01 32.42 3.95xMiniCache 48.75 48.59 13.00 68.00 14.36 26.57 7.99 32.61 5.02x Mistral-7B Baseline 68.06 60.46 17.71 68.00 10.87 20.09 17.10 37.33 1xRTN [73] 27.98 26.18 3.34 13.00 1.11 2.49 0.45 10.51 3.21xSmoothQuant [70] 40.63 35.14 3.40 30.50 6.03 5.00 4.12 17.55 2.15xKIVI-2 [11] 65.16 58.33 12.43 65.00 11.03 13.22 13.87 33.43 3.95xMiniCache 68.89 60.98 13.92 67.00 10.50 18.06 7.88 35.75 5.02x Mistral-7B-Instruct Baseline 55.51 48.96 60.00 71.00 27.33 32.85 42.74 48.32 1xRTN [73] 32.36 33.23 0.67 1.00 2.25 10.03 2.30 11.55 3.21xSmoothQuant [70] 43.84 38.63 4.79 39.50 10.34 23.61 8.33 24.43 2.15xKIVI-2 [11] 53.13 48.60 47.50 69.00 20.68 29.37 33.88 43.74 3.95xMiniCache 54.79 51.02 64.14 71.00 24.97 31.46 27.54 46.99 5.02x lights the potential of our method to optimize large language models for tasks requiring extensive context, making them more efficient and scalable for real-world applications. Efficiency analysis. To assess the acceleration capabilities of MiniCache, we conduct evaluations based on the methodologies employed in vLLM [74] and KIVI [11]. We generate synthetic workloads derived from ShareGPT, which include real input and output texts from LLM services. The dataset features an average input prompt length of 161 tokens and an output length of 338 tokens. Using the LLaMA-2-7B model on a single 80GB NVIDIA A100 GPU, we benchmark our method in a batch-serving scenario, comparing peak memory usage and throughput among 2-bit KIVI, 4-bit MiniCache, and an FP16 baseline. As illustrated in Figure 5, with a batch size of 128, MiniCache reduces memory usage by 25GB, achieving a 41% memory saving . In terms of throughput, MiniCache outperforms the FP16 baseline by approximately 5×. Additionally, despite utilizing 4-bit quantization, MiniCache benefits from merging and sharing KV caches across adjacent layers, resulting in a 1.29× higher throughput compared to the 2-bit KIVI. These results demonstrate that MiniCache offers a state-of-the-art trade-off between efficiency and performance. 50 100 150 200 250 300 Batch Size 20 30 40 50 60 70 80Peak Memory Usage (GB) Baseline FP16 KIVI 2 MINICache 4 (a) BS. vs. Peak Memory Usage 50 100 150 200 250 300 Batch Size 1000 1500 2000 2500 3000Throughput (tokens/sec)  Baseline FP16 KIVI 2 MINICache 4 (b) BS. vs. Decoding Throughput Figure 5: Memory usage and throughput comparison between our 4-bit MiniCache, 2-bit KIVI, and 16-bit Baseline. MiniCache can achieve higher throughput by enabling a larger batch size while reducing memory footprints via LLaMA-2-7B [5]. 0.3 0.4 0.5 0.6 0.7 Interpolation Parameter t 0.350 0.375 0.400 0.425 0.450 0.475 0.500Exact Match Scores GSM8k 0.0 0.2 0.4 0.6 0.8 1.0 1.2 Normalized Frequency Frequency Figure 6: LLaMA-3-8B [ 6] to experiment on the GSM8K [10]. The right axis is the normalized frequency of the relative magni- tude ratio. Optional t shows a strong correlation with frequency. 6 Ablation Study Table 2: Comparisons of various token retention thresholds γ by LLaMA-2-7B [5] on three benchmarks. γ COQA GSM8K TruthfulQA 0 0.603 0.108 29.813 0.01 0.620 0.126 30.226 0.02 0.630 0.143 33.903 0.05 0.647 0.152 33.213 0.1 0.643 0.152 33.903 1 0.643 0.159 33.743 The effect of interpretation parameter t. We explore the effects of the interpretation parameter t on perfor- mance, particularly in relation to the relative magnitude ratio of adjacent layers, as shown in Figure 6. We maintain all settings constant, starting from layer S = 16 (halfway 9through the layers of LLaMA-3-8B), and vary the interpretation parameter t from 0.3 to 0.7. Our findings reveal several key points. When t = 0.5, the process resembles average merging, which is less effective for cross-layer merging. In contrast, when t = 0.6 is optimal, the merged representation exhibits the most robust performance, while indicating that more information is derived from the second term (xl) of the SLERP. The frequency results also indicate that the high frequencies are clustered around 0.4 and 0.6, corroborating our optimal t. Moreover, there is a strong correlation between the optimalt and the high frequency of the relative magnitude ratio of adjacent layers. This finding provides an opportunity to utilize the relative magnitude ratio to dynamically determine the interpretation parameter t. Dynamic t allows for more flexible weight control in SLERP merging for each layer-wise operation, thereby showing potential for further exploration. The effect of token retention thresholdγ. We investigate the impact of the token retention threshold γ on model performance across the three datasets, as shown in Table 2. A larger t generally means retaining more tokens for improved performance, but this comes at the cost of increased memory demand. The results suggest that setting γ to 0.05 achieves the best balance between performance and efficiency. 7 Conclusion and Future Work This paper presents a pioneering exploration of KV cache compression in the depth dimension, addressing a significant memory bottleneck in LLMs. Our proposed MiniCache offers a simple, effective, and training-free approach to compressing KV caches by leveraging the notable high similarities between KV caches in neighboring layers, starting from the midpoint of LLMs. We have demonstrated that MiniCache can significantly reduce the memory footprint required for LLM inference by up to 41%, while simultaneously enhancing throughput by approximately five times compared to the FP16 baseline. In conclusion, MiniCache significantly advances the field of KV cache compression, offering a state-of-the-art balance between efficiency and performance. Future work will focus on enhancing the compression ratio by cross-multiple-layer merging, developing advanced merging algorithms such as Spherical Cubic Interpolation [ 75], and further optimizing memory usage for large-scale deployments in diverse application scenarios. References [1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., “Language models are few-shot learners,” in NeurIPS, vol. 33, pp. 1877–1901, 2020. [2] OpenAI, J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, and OTHERS, “Gpt-4 technical report,” 2023. [3] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray,et al., “Training language models to follow instructions with human feedback,” in NeurIPS, vol. 35, pp. 27730–27744, 2022. [4] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar,et al., “Llama: Open and efficient foundation language models,” arXiv preprint arXiv:2302.13971, 2023. [5] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale,et al., “Llama 2: Open foundation and fine-tuned chat models,” arXiv preprint arXiv:2307.09288, 2023. [6] “Introducing meta llama 3: The most capable openly available llm to date.”https://ai.meta. com/blog/meta-llama-3/, 2024. Accessed: 2024-05-04. [7] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Rad- ford, J. Wu, and D. Amodei, “Scaling laws for neural language models,” arXiv preprint arXiv:2001.08361, 2020. 10[8] G. Team, R. Anil, S. Borgeaud, Y . Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al., “Gemini: a family of highly capable multimodal models,” arXiv preprint arXiv:2312.11805, 2023. [9] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, J. Heek, K. Xiao, S. Agrawal, and J. Dean, “Efficiently scaling transformer inference,” Proceedings of Machine Learning and Systems, vol. 5, 2023. [10] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al., “Training verifiers to solve math word problems,”arXiv preprint arXiv:2110.14168, 2021. [11] Z. Liu, J. Yuan, H. Jin, S. Zhong, Z. Xu, V . Braverman, B. Chen, and X. Hu, “Kivi: Plug- and-play 2bit kv cache quantization with streaming asymmetric quantization,” arXiv preprint arXiv:2402.02750, 2024. [12] H. Kang, Q. Zhang, S. Kundu, G. Jeong, Z. Liu, T. Krishna, and T. Zhao, “Gear: An efficient kv cache compression recipefor near-lossless generative inference of llm,” arXiv preprint arXiv:2403.05527, 2024. [13] Y . Sheng, L. Zheng, B. Yuan, Z. Li, M. Ryabinin, B. Chen, P. Liang, C. Ré, I. Stoica, and C. Zhang, “Flexgen: High-throughput generative inference of large language models with a single gpu,” in ICML, pp. 31094–31116, PMLR, 2023. [14] Z. Zhang, Y . Sheng, T. Zhou, T. Chen, L. Zheng, R. Cai, Z. Song, Y . Tian, C. Ré, C. Barrett, et al., “H2o: Heavy-hitter oracle for efficient generative inference of large language models,” in NeurIPS, vol. 36, 2024. [15] G. Xiao, Y . Tian, B. Chen, S. Han, and M. Lewis, “Efficient streaming language models with attention sinks,” in ICLR, 2024. [16] S. Ge, Y . Zhang, L. Liu, M. Zhang, J. Han, and J. Gao, “Model tells you what to discard: Adaptive kv cache compression for llms,” ICLR, 2024. [17] D. Raposo, S. Ritter, B. Richards, T. Lillicrap, P. C. Humphreys, and A. Santoro, “Mixture-of- depths: Dynamically allocating compute in transformer-based language models,” arXiv preprint arXiv:2404.02258, 2024. [18] W. Zhou, C. Xu, T. Ge, J. McAuley, K. Xu, and F. Wei, “Bert loses patience: Fast and robust inference with early exit,” in NeurIPS, vol. 33, pp. 18330–18341, 2020. [19] M. Elhoushi, A. Shrivastava, D. Liskovich, B. Hosmer, B. Wasti, L. Lai, A. Mahmoud, B. Acun, S. Agarwal, A. Roman, et al., “Layer skip: Enabling early exit inference and self-speculative decoding,” arXiv preprint arXiv:2404.16710, 2024. [20] A. Gromov, K. Tirumala, H. Shapourian, P. Glorioso, and D. A. Roberts, “The unreasonable ineffectiveness of the deeper layers,”arXiv preprint arXiv:2403.17887, 2024. [21] T. Salimans and D. P. Kingma, “Weight normalization: A simple reparameterization to accelerate training of deep neural networks,” in NeurIPS, vol. 29, 2016. [22] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand, et al., “Mixtral of experts,” arXiv preprint arXiv:2401.04088, 2024. [23] M. Abdin, S. A. Jacobs, A. A. Awan, J. Aneja, A. Awadallah, H. Awadalla, N. Bach, A. Bahree, A. Bakhtiari, H. Behl, et al., “Phi-3 technical report: A highly capable language model locally on your phone,” arXiv preprint arXiv:2404.14219, 2024. [24] M. Roemmele, C. A. Bejan, and A. S. Gordon, “Choice of plausible alternatives: An evaluation of commonsense causal reasoning.,” in AAAI spring symposium: logical formalizations of commonsense reasoning, pp. 90–95, 2011. 11[25] A. Amini, S. Gabriel, S. Lin, R. Koncel-Kedziorski, Y . Choi, and H. Hajishirzi, “Mathqa: Towards interpretable math word problem solving with operation-based formalisms,” inNAACL, pp. 2357–2367, 2019. [26] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal, “Can a suit of armor conduct electricity? a new dataset for open book question answering,” in EMNLP, 2018. [27] Y . Bisk, R. Zellers, R. L. Bras, J. Gao, and Y . Choi, “Piqa: Reasoning about physical common- sense in natural language,” in AAAI, 2020. [28] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman, “Glue: A multi- task benchmark and analysis platform for natural language understanding,” arXiv preprint arXiv:1804.07461, 2018. [29] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y . Choi, “Winogrande: An adversarial winograd schema challenge at scale,” Communications of the ACM, vol. 64, no. 9, pp. 99–106, 2021. [30] S. Narayan, S. B. Cohen, and M. Lapata, “Don’t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization,” arXiv preprint arXiv:1808.08745, 2018. [31] R. Nallapati, B. Zhou, C. Gulcehre, B. Xiang, et al., “Abstractive text summarization using sequence-to-sequence rnns and beyond,” arXiv preprint arXiv:1602.06023, 2016. [32] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, A. Le Noac’h, H. Li, K. McDonell, N. Muennighoff, C. Ociepa, J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron, L. Sutawika, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou, “A framework for few-shot language model evaluation,” 12 2023. [33] Y . Bai, X. Lv, J. Zhang, H. Lyu, J. Tang, Z. Huang, Z. Du, X. Liu, A. Zeng, L. Hou, et al., “Longbench: A bilingual, multitask benchmark for long context understanding,” arXiv preprint arXiv:2308.14508, 2023. [34] L. Del Corro, A. Del Giorno, S. Agarwal, B. Yu, A. Awadallah, and S. Mukherjee, “Skipdecode: Autoregressive skip decoding with batching and caching for efficient llm inference,” arXiv preprint arXiv:2307.02628, 2023. [35] T. Schuster, A. Fisch, J. Gupta, M. Dehghani, D. Bahri, V . Tran, Y . Tay, and D. Metzler, “Confident adaptive language modeling,” in NeurIPS, vol. 35, pp. 17456–17472, 2022. [36] H. Wu and K. Tu, “Layer-condensed kv cache for efficient inference of large language models,” 2024. [37] W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity,” Journal of Machine Learning Research, vol. 23, no. 120, pp. 1–39, 2022. [38] D. Lepikhin, H. Lee, Y . Xu, D. Chen, O. Firat, Y . Huang, M. Krikun, N. Shazeer, and Z. Chen, “Gshard: Scaling giant models with conditional computation and automatic sharding,” arXiv preprint arXiv:2006.16668, 2020. [39] D. Dai, C. Deng, C. Zhao, R. Xu, H. Gao, D. Chen, J. Li, W. Zeng, X. Yu, Y . Wu, et al., “Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models,” arXiv preprint arXiv:2401.06066, 2024. [40] C. Hwang, W. Cui, Y . Xiong, Z. Yang, Z. Liu, H. Hu, Z. Wang, R. Salas, J. Jose, P. Ram,et al., “Tutel: Adaptive mixture-of-experts at scale,” Proceedings of Machine Learning and Systems, vol. 5, 2023. [41] DeepSeek-AI, “Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model,” arXiv preprint arXiv:2405.04434, 2024. [42] J. Ainslie, J. Lee-Thorp, M. de Jong, Y . Zemlyanskiy, F. Lebrón, and S. Sanghai, “Gqa: Training generalized multi-query transformer models from multi-head checkpoints,” arXiv preprint arXiv:2305.13245, 2023. 12[43] N. Shazeer, “Fast transformer decoding: One write-head is all you need,” arXiv preprint arXiv:1911.02150, 2019. [44] K. Choromanski, V . Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, et al., “Rethinking attention with performers,” arXiv preprint arXiv:2009.14794, 2020. [45] H. Peng, N. Pappas, D. Yogatama, R. Schwartz, N. A. Smith, and L. Kong, “Random feature attention,” in ICLR, 2021. [46] T. Dao, D. Fu, S. Ermon, A. Rudra, and C. Ré, “Flashattention: Fast and memory-efficient exact attention with io-awareness,” in NeurIPS, vol. 35, pp. 16344–16359, 2022. [47] T. Dao, “Flashattention-2: Faster attention with better parallelism and work partitioning,” arXiv preprint arXiv:2307.08691, 2023. [48] S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma, “Linformer: Self-attention with linear complexity,”arXiv preprint arXiv:2006.04768, 2020. [49] X. Ma, X. Kong, S. Wang, C. Zhou, J. May, H. Ma, and L. Zettlemoyer, “Luna: Linear unified nested attention,” in NeurIPS, vol. 34, pp. 2441–2453, 2021. [50] J. Lee, Y . Lee, J. Kim, A. Kosiorek, S. Choi, and Y . W. Teh, “Set transformer: A framework for attention-based permutation-invariant neural networks,” in ICML, pp. 3744–3753, PMLR, 2019. [51] J. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and S. Han, “Awq: Activation-aware weight quantization for llm compression and acceleration,” arXiv preprint arXiv:2306.00978, 2023. [52] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “Qlora: Efficient finetuning of quantized llms,” in NeurIPS, vol. 36, 2023. [53] T. Dettmers, M. Lewis, Y . Belkada, and L. Zettlemoyer, “Llm.int8(): 8-bit matrix multiplication for transformers at scale,” in NeurIPS, vol. 35, pp. 30318–30332, 2022. [54] J. Liu, R. Gong, X. Wei, Z. Dong, J. Cai, and B. Zhuang, “Qllm: Accurate and efficient low-bitwidth quantization for large language models,” in ICLR, 2024. [55] M. Zhang, C. Shen, Z. Yang, L. Ou, X. Yu, B. Zhuang, et al., “Loraprune: Pruning meets low-rank parameter-efficient fine-tuning,” inACL findings, 2024. [56] N. Kitaev, Ł. Kaiser, and A. Levskaya, “Reformer: The efficient transformer,”arXiv preprint arXiv:2001.04451, 2020. [57] S. K. Ainsworth, J. Hayase, and S. Srinivasa, “Git re-basin: Merging models modulo permutation symmetries,” in ICLR, 2023. [58] R. Entezari, H. Sedghi, O. Saukh, and B. Neyshabur, “The role of permutation invariance in linear mode connectivity of neural networks,” arXiv preprint arXiv:2110.06296, 2021. [59] T. Garipov, P. Izmailov, D. Podoprikhin, D. P. Vetrov, and A. G. Wilson, “Loss surfaces, mode connectivity, and fast ensembling of dnns,” inNeurIPS, vol. 31, 2018. [60] M. Wortsman, G. Ilharco, S. Y . Gadre, R. Roelofs, R. Gontijo-Lopes, A. S. Morcos, H. Namkoong, A. Farhadi, Y . Carmon, S. Kornblith,et al., “Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time,” in ICML, pp. 23965–23998, PMLR, 2022. [61] P. Yadav, D. Tam, L. Choshen, C. A. Raffel, and M. Bansal, “Ties-merging: Resolving interfer- ence when merging models,” in NeurIPS, vol. 36, 2023. [62] M. Davari and E. Belilovsky, “Model breadcrumbs: Scaling multi-task model merging with sparse masks,” arXiv preprint arXiv:2312.06795, 2023. [63] L. Yu, B. Yu, H. Yu, F. Huang, and Y . Li, “Language models are super mario: Absorbing abilities from homologous models as a free lunch,” arXiv preprint arXiv:2311.03099, 2023. 13[64] K. Shoemake, “Animating rotation with quaternion curves,” inProceedings of the 12th annual conference on Computer graphics and interactive techniques, pp. 245–254, 1985. [65] M. S. Matena and C. A. Raffel, “Merging models with fisher-weighted averaging,” inNeurIPS, vol. 35, pp. 17703–17716, 2022. [66] X. Jin, X. Ren, D. Preotiuc-Pietro, and P. Cheng, “Dataless knowledge fusion by merging weights of language models,” arXiv preprint arXiv:2212.09849, 2022. [67] Y . Chen, X. Pan, Y . Li, B. Ding, and J. Zhou, “Ee-llm: Large-scale training and inference of early-exit large language models with 3d parallelism,” arXiv preprint arXiv:2312.04916, 2023. [68] S. Reddy, D. Chen, and C. D. Manning, “Coqa: A conversational question answering challenge,” Transactions of the Association for Computational Linguistics, vol. 7, pp. 249–266, 2019. [69] S. Lin, J. Hilton, and O. Evans, “Truthfulqa: Measuring how models mimic human falsehoods,” arXiv preprint arXiv:2109.07958, 2021. [70] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han, “Smoothquant: Accurate and efficient post-training quantization for large language models,” in International Conference on Machine Learning, pp. 38087–38099, PMLR, 2023. [71] J. Liu, R. Gong, X. Wei, Z. Dong, J. Cai, and B. Zhuang, “Qllm: Accurate and efficient low-bitwidth quantization for large language models,” in ICLR, 2024. [72] S.-Y . Liu, C.-Y . Wang, H. Yin, P. Molchanov, Y .-C. F. Wang, K.-T. Cheng, and M.-H. Chen, “Dora: Weight-decomposed low-rank adaptation,” arXiv preprint arXiv:2402.09353, 2024. [73] M. Nagel, R. A. Amjad, M. Van Baalen, C. Louizos, and T. Blankevoort, “Up or down? adaptive rounding for post-training quantization,” in International Conference on Machine Learning, pp. 7197–7206, PMLR, 2020. [74] W. Kwon, Z. Li, S. Zhuang, Y . Sheng, L. Zheng, C. H. Yu, J. Gonzalez, H. Zhang, and I. Stoica, “Efficient memory management for large language model serving with pagedattention,” in Proceedings of the 29th Symposium on Operating Systems Principles, pp. 611–626, 2023. [75] D. H. Eberly, “Quaternion algebra and calculus,” 2002. [76] “Stanford crfm.” https://crfm.stanford.edu/2023/10/12/flashdecoding.html, 2024. Accessed: 2024-05-04. [77] Y . Liu, H. Li, K. Du, J. Yao, Y . Cheng, Y . Huang, S. Lu, M. Maire, H. Hoffmann, A. Holtzman, et al. , “Cachegen: Fast context loading for language model applications,” arXiv preprint arXiv:2310.07240, 2023. [78] L. Chen, H. Zhao, T. Liu, S. Bai, J. Lin, C. Zhou, and B. Chang, “An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models,” arXiv preprint arXiv:2403.06764, 2024. [79] S. Wei, T. Ye, S. Zhang, Y . Tang, and J. Liang, “Joint token pruning and squeezing towards more aggressive compression of vision transformers,” in CVPR, pp. 2092–2101, 2023. [80] L. Gao, J. Tow, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, K. McDonell, N. Muennighoff, J. Phang, L. Reynolds, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou, “A framework for few-shot language model evaluation,” 2021. [81] W. Kwon, Z. Li, S. Zhuang, Y . Sheng, L. Zheng, C. H. Yu, J. Gonzalez, H. Zhang, and I. Stoica, “Efficient memory management for large language model serving with pagedattention,” in Proceedings of the 29th Symposium on Operating Systems Principles, pp. 611–626, 2023. 14Appendix A Additional Experiment Results Comparisons with token sparsity methods. We also compare MiniCache with the sparsity-based method H2O [14]. Our method outperforms H2O in most tasks on LongBench. Notably, our approach focuses on reducing inter-layer redundancy, whereas H2O focuses on intra-layer redundancy. Our approaches are orthogonal to sparsity-based methods. Table A: Comparison between MiniCache with token sparsity based method H2O, using mistral-7B- instruct on LongBench dataset. MethodsNrtvQA Qasper MF-en HotpotQA 2WikiMQA Musique GovReport QMSum MultiNews TREC TriviaQA SAMSum PCount PRe Lcc Baseline 26.82 33.06 49.28 42.77 27.33 19.27 32.85 24.25 27.06 71.0 86.23 42.98 2.75 86.98 55.51H2O[14] 22.61 29.06 47.22 36.54 20.6 16.25 30.0 23.8 26.75 70.5 86.16 42.97 3.46 86.38 53.72MiniCache27.0432.5949.38 43.91 24.97 18.3 31.46 23.85 26.64 71.0 86.93 43.6 3.04 79.5654.79 B Additional Related Work Preliminaries of KV cache in LLMs. LLM inference has been significantly improved in recent works [9, 13, 74] by optimizing the KV cache management. Overall, this line of research is typically done in two steps: 1) First, at the prefilling stage, LLM calculates the initial KV caches at each attention layer based on the input sequence and decodes the first token. Second, 2) at the decoding stage, LLM autoregressively predicts the next token, where its KV cache at each attention layer is added to the overall caches. Existing works have compressed KV cache in different aspects (e.g., quantization [11, 12], token pruning [14, 16] ). KV cache compression. In the prior study, various strategies for enhancing efficient transformer architectures are discussed, covering a spectrum of techniques aimed at optimizing performance and managing resource constraints. These methods include attention optimization [ 46, 47, 76], grouping queries [42, 43], sparse KV caching [16, 77, 78], shrinking tokens [15, 79], and improving long-context generation. Significant contributions come from projects such as H2O [ 15], GEAR [15], and KIVI [11]. Additionally, efforts to alleviate KV cache bottlenecks include strategies like multi-query attention [42] and multi-group attention [43], which propose reducing the number of heads in the KV cache. However, these methods often necessitate retraining or fine-tuning models. Other approaches focus on diminishing the size of the KV cache by selectively evicting less important tokens [14] and enhancing the system architecture through technologies like offloading the KV cache [80] or integrating techniques such as virtual memory and paging [81] into the attention mechanism. C Discussions and Limitations Alternative merging function. During our preliminary exploration, we initially considered an alter- native, simpler merge function for cross-layer compression: maximum norm-preserving interpolation. This function is designed to maintain the maximum norm of the vectors involved, ensuring that the most significant features are preserved during the merging process. The maximum norm-preserving interpolation in terms of Fmax can be defined as follows: Fmax(xl, xl−1) = ¯xl,l−1 ∥¯xl,l−1∥ · Max(∥xl∥, ∥xl−1∥). (A) Here ¯xl,l−1 represents the average vector between xl and xl−1. The function Fmax ensures that the merged vector preserves the direction of the average vector while scaling it to the maximum norm of the original KV states. Compared to the SLERP-based merge function, Fmax has less computational overhead and lower memory consumption. However, it is less accurate than SLERP. The choice between using FSLERP or Fmax depends on the specific requirements of the application. In our study, we primarily use SLERP to maximize performance. Societal impact. Our work shows a preliminary exploration of KV cache Compression in the depth dimension, a relatively unexplored yet critically bottlenecked area in large language models 15(LLMs). The proposed MiniCache provides a solution to improve the efficiency of LLM generation and is adaptable to existing intra-layer-based KV cache pruning and quantization technologies. Additionally, we proposed a simple yet effective approach that merges similar KV cache states in a cross-layer manner and effectively restores performance through a novel restoration technique. Our observations indicate that cross-layer similarity and mergeability can be applied in broad-efficient applications for post-training optimization in low-resource scenarios, such as deployment on mobile devices. Moreover, with effective optimization of KV cache memory demand, MiniCache further enhances long-context generation, which is a crucial paradigm for real-world applications, such as understanding concepts in textbooks. We aim for our work to advance the boundaries of two key challenges in the LLM industry and research: batch inference and long-context generation. Furthermore, in addition to the significant contributions of MiniCache for KV cache Compression, several challenges remain that are common to LLMs. Issues such as the truthfulness and security of LLMs are still unresolved. Ensuring the accuracy and reliability of generated content is critical, as LLMs can sometimes produce plausible but incorrect or misleading information. Additionally, safeguarding against security vulnerabilities, such as adversarial attacks or data leakage, is paramount to maintaining the integrity and confidentiality of user interactions. Addressing these challenges requires ongoing research and development to enhance the robustness and trustworthiness of LLMs. This effort must proceed alongside advancements in computational efficiency and performance, as exemplified by innovations like MiniCache. Limitations. The current merging algorithm based on Spherical Linear Interpolation (SLERP) has its limitations. SLERP is suitable only for merging two vectors and uses an interpolation approach that restricts our algorithm from merging multiple layers simultaneously and maximizing the compression ratio in further states. This limitation impacts the overall efficiency of KV cache compression and underscores the need for advanced techniques capable of handling more complex merging scenarios. Future research should focus on developing more sophisticated algorithms that can overcome these constraints, thereby enhancing the compression capabilities and overall performance of LLMs. D Additional Implementation Details Overview of the inference algorithm. The MiniCache inference implementation, as shown in Alg. 1, involves several key steps. When the interface starts, In the prefilling phase, we define the merging starting layer S. Before reaching layer S, the inference uses the original attention and cache logic. From layer S onward, we implement our merging algorithm, which operates in a cross-layer manner and is applied only at odd-numbered layers. During merging, we fetch the KV cache from the previous layer and save the merged shared states into the current layer’s KV cache. To reduce memory usage, we then remove the cache of the previous layer. Additionally, we store the magnitudes vector and retention-sensitive tokens for each layer. During the decoding phase, two scenarios arise after layer S. For even-numbered layers (first round), since the KV cache has been removed during the prefilling phase, we refer to the next layer (l + 1) to fetch the shared KV cache states. We then perform approximated scale restoration and retention token recovery. The new KV states from this phase are stored for use in the next round. In the second round, which involves odd-numbered layers, we use the new KV tokens from both the previous and current layers. After the restoration phase, we perform the merge operations and update the shared KV cache states in the stack. Cross-Layer merging and restoration algorithm. As outlined in Alg. 2, MiniCache algorithm involves several crucial steps to ensure efficient memory usage while maintaining the integrity of the Key-Value (KV) Cache states. Initially, given the KV cacheEl k,v, norm values ∥Xl k,v∥ unmerged tokens Rl k,v, retention indices Ik,v, and the next tokens tl, tl−1, the algorithm proceeds by rescaling the magnitude of the KV pairs. Specifically, ˆXl k and ˆXl v are computed by multiplying the normalized KV pairs El k,v with their respective magnitude norms ∥Xl k,v∥. Following this, the algorithm restores unmerged tokens using the retention indices, updating ˆXl k and ˆXl v accordingly. Next, the new tokens tk and tv are concatenated to the rescaled KV pairs along the token dimension. This augmented KV cache undergoes a softmax attention mechanism where the attention scores A are computed by taking the dot product of the query token tq with the transposed keys ( ˆXl k)⊤. The output token tO is then obtained by multiplying the attention scores A with the values ˆXl v. In cases where the previous token tl−1 exists, the algorithm performs a compression step. It concatenates the existing KV cache 16El k,v with the merged tokens resulting from the current and previous layers, effectively reducing redundancy and optimizing memory. If tl−1 is not available, the KV cache is updated by simply concatenating El k,v with the new tokens tl k,v, deferring the compression until the next iteration. The final output token tO is then returned, concluding the decoding process. In the merging function, the algorithm normalizes the KV pairs from both the current and previous layers. It calculates the angular distance Ω between the normalized vectors, ensuring that the interpolation occurs along the shortest path on the unit sphere. The merged KV cache is then obtained by weighted interpolation of the normalized vectors, preserving the geometric and semantic integrity of the original states. This comprehensive process allows MiniCache to achieve substantial memory efficiencies while maintaining the functional characteristics of the KV pairs across transformer layers. Layer𝑙 QK V Attention Cross-Layer Merging K V KVcachecompressionat layer𝑙 Layer𝑙−1  ... 𝜒! 𝜒!\"# 𝐾,𝑉 C 1 Keep Fetch 2Merge 3 Cache 4Delete Recovery Rescale Retention Recovery 5Fetch Contact New Token QK V Recovery Attention...... 6Fetch Cross Layer Merging Time 7 Prefilling Decoding Figure A: Overall prefilling and decoding logic for MiniCache involves performing cross-layer merging and recovery within our framework. MiniCache execution flow. Figure A delineates the pre-filling and decoding logic for the MiniCache framework, which incorporates cross-layer merging and error suppression to achieve memory effi- ciency and maintain functional integrity. Initially, in Step 1, the KV cache is fetched from the previous layer (Layer L−1) during the pre-filling phase. In Step 2, the fetched KV pairs from the current layer χL are merged with the KV pairs from the preceding layer χL−1, reducing redundancy through a 17merge function. Subsequently, in Step 3, the merged KV pairs are cached for future use, representing a consolidated data set from multiple layers. During the decoding phase, Step 4 involves deleting unnecessary or redundant KV pairs to optimize memory usage. In Step 5, the decoder fetches the required KV pairs from the cache for output generation. Step 6 applies error suppression mechanisms to the fetched KV pairs, including rescaling and retention recovery, to minimize errors introduced during the merging and compression processes. Finally, in Step 7, the cache is updated with the final KV pairs post-error suppression and adjustments, ensuring the cache contains the most accurate and efficient representation of the KV pairs for subsequent layers. This comprehensive approach guarantees substantial memory efficiencies while preserving the critical functional characteristics of the original KV pairs across transformer layers. Algorithm 1: The MiniCache Inference Algorithm 1 procedure MiniCache Inference: Input: Input Tokens: T ∈ Rtinput×d, number of layers L, merging beginning layer S Output: Output Tokens: O ∈ Rtoutput×d 2 for l ← 0 to S − 1 do 3 procedure Standard Prefill: 4 Standard Attention & Standard Cache 5 procedure Standard Decoding: 6 Standard Attention & Standard Cache // Start Merging from layer S 7 for l ← S to L do // Perform Merging in every two layers l%2 == 1 8 if l%2 == 1 and prefilling then 9 procedure MiniCache Prefill: Input: KV cache from Current layer l: Xl k,v, KV cache from Previous layer l − 1: Xl−1 k,v , token retention threshold: γ 10 Delete KV cache of the l − 1-th layer // layer l, l− 1 shares one Cache 11 Standard Attention & Standard Cache // Perform Merging in the second layer 12 if and decoding then 13 if l%2 == 0 then // first round in the cross-layer merging, fetch shared KV cache states from Cl+1 k,v 14 procedure MiniCache Decoding: Input: KV cache: Cl+1 k,v , Norm: ∥Xl k,v∥, Unmerged Tokens: Rl k,v, Retention indices: Ik,v, Next Token: tl 15 else // second round in cross-layer merging, while tl−1 exist 16 procedure MiniCache Decoding: Input: KV cache: Cl k,v, Norm: ∥Xl k,v∥, Unmerged Tokens: Rl k,v, Retention indices: Ik,v, Next Token: tl, tl−1 17 return O 18Algorithm 2: The MiniCache Prefill & Decoding Compression Algorithm Hyperparameter: number of layers L, merging beginning layer S 1 procedure MiniCache Prefill: Input: KV cache from current layer l: Xl k,v ∈ R2×tprompt×d, KV cache from previous layer l − 1: Xl−1 k,v ∈ R2×tprompt×d, token retention threshold: γ 2 El k, ∥Xl k∥, ∥Xl−1 k ∥, Ωk ← Merge(Xl k, Xl−1 k ) ; 3 El v, ∥Xl v∥, ∥Xl−1 v ∥, Ωv ← Merge(Xl v, Xl−1 v ) ; 4 El k,v ∈ Rtprompt×d | {z } compression output , ∥Xl,l−1 k,v ∥ ∈R4×tprompt×1 | {z } norms for rescaling ; 5 d(Xl, Xl−1)k,v = 1 π · Ωk,v // distance metrics 6 Ik,v = {i | di < dmin + (dmax − dmin) · γ} // retention indices 7 Rl,l−1 k ← Xl,l−1 k [Ik], Rl,l−1 v ← Xl,l−1 v [Iv] // unmerged tokens 8 return El k,v, ∥Xl,l−1 k,v ∥, Rl,l−1 k , Rl,l−1 v , Ik,v 9 procedure MiniCache Decoding: Input: KV cache: El k,v ∈ R2×tprompt×d, Norm: ∥Xl k,v∥ ∈R2×tprompt×1, Unmerged Tokens: Rl k,v ∈ R2×γ·tprompt×d, Retention indices: Ik,v ∈ R2×γ·tprompt×1, Next Token: tl ∈ R1×d, tl−1 ∈ R1×d 10 ˆXl k ← El k · ∥Xl k∥ ∥El k∥ ˆXl v ← El v · ∥Xl v∥ ∥Elv∥ // magnitude rescale 11 ˆXl k[Ik] = Rl k ˆXl v[Iv] = Rl v // token restoration 12 ˆXl k ← Concat( ˆXl k, tk, dim=token) ˆXl v ← Concat( ˆXl v, tv, dim=token) A ← Softmax(tq · ( ˆXl k)⊤) tO ← A · ˆXl v if tl−1 exists then 13 KV cache ← Concat(El k,v, Merge(tl k,v, tl−1 k,v ), dim=token) // perform compression 14 else 15 KV cache ← Concat(El k,v, tl k,v, dim=token) // wait for compression 16 return tO 17 function MiniCache Merge(Xl, Xl−1, t): 18 ⃗Xl ← Xl ∥Xl∥ 19 ⃗Xl−1 ← Xl−1 ∥Xl−1∥ 20 Ω ← arccos \u0010 Xl T ·Xl−1 T ∥Xl T ∥∥Xl−1 T ∥ \u0011 21 E ← sin((1−t)Ω) sin(Ω) ⃗X l + sin(tΩ) sin(Ω) ⃗X l−1 22 return E, ∥Xl∥, ∥Xl−1∥, Ω E Detailed Efficiency Derivation In this section, we provide a detailed derivation of the memory efficiency improvements outlined in Section 4.3. First, we consider the original KV cache memory usage, which is given by: 4brh(s + n). Here, r is the number of layers, b is the batch size, h is the hidden size, s is the input sequence length, and n is the output sequence length. To improve efficiency, we begin merging layers starting from the midpoint, S = 1 2 r, by consolidating the KV cache states of every two layers into a single shared state space. The memory usage derivation proceeds as follows: for the unmerged part of the cache (from layer 1 to S): 194brh(s + n) · 1 2 = 2brh(s + n). For the merged part of the cache (from layer S + 1 to r): 4brh(s + n) · 1 2 · 1 2 = brh(s + n). Combining these two parts, the total memory usage is: 2brh(s + n) + brh(s + n) = 3brh(s + n). Next, we consider the additional memory cost incurred during the restoration process. During this phase, we save additional normalized vectors for the layers in the KV cache. These vectors are of shape Rb×s×1, which means they have a single channel dimension compared to the fully ranked original KV states. The additional normalized vectors for layers from S onwards are given by: br(s + n) · 2 = 2br(s + n). We also introduce a retention threshold, which we set to 0.05. This means that 5% of the KV cache tokens are retained without compression: brh(0.05(s + n)). Combining these terms, the total additional memory for the restoration process is: 2br(s + n) + 0.1brh(s + n). Finally, summing the compressed memory usage and the restoration memory cost, the overall memory requirement is: 3brh(s + n) + 2br(s + n) + 0.1brh(s + n). This can be simplified by grouping the common factors: br(s + n) (3h + 2 + 0.1h) . Simplifying the expression inside the parentheses, we get: br(s + n) (3.1h + 2). Therefore, the total memory cost for the KV cache in the MiniCache Framework is: br(s + n)(3.1h + 2). This detailed derivation confirms the efficiency improvements discussed in Section 4.3, highlighting the significant reduction in memory usage achieved through our layer merging and restoration strategies. F Detailed Experiment Results 20Table B: Detailed performance comparison on GSM8K dataset with LLaMA-3-70B. Layer LLaMA-3-70B MiniCache LLaMA-3-70B Mean 0 0.799 0.729 5 0.800 0.640 10 0.792 0.578 15 0.815 0.545 20 0.801 0.560 25 0.812 0.544 30 0.799 0.556 35 0.810 0.557 40 0.790 0.551 45 0.725 0.539 50 0.638 0.541 55 0.638 0.501 60 0.625 0.497 65 0.635 0.511 70 0.623 0.497 75 0.615 0.493 Table C: Detailed performance comparison on COQA dataset with LLaMA-3-70B. Layer LLaMA-3-70B MiniCache LLaMA-3-70B Mean 0 0.705 0.706 4 0.705 0.699 8 0.706 0.696 12 0.704 0.691 16 0.704 0.690 20 0.703 0.690 24 0.701 0.690 28 0.702 0.690 32 0.702 0.688 36 0.703 0.688 40 0.697 0.687 44 0.698 0.685 48 0.699 0.678 52 0.699 0.672 56 0.701 0.668 60 0.704 0.657 64 0.706 0.635 68 0.691 0.611 72 0.689 0.565 76 0.641 0.526 21Table D: Detailed performance comparison on TruthfulQA dataset with LLaMA-3-70B. Layer LLaMA-3-70B MiniCache LLaMA-3-70B Mean 0 22.615 22.130 4 22.512 22.005 8 22.451 21.876 12 22.413 21.303 16 22.387 21.209 20 22.387 20.752 24 22.387 20.657 28 22.276 20.501 32 22.130 20.479 36 22.130 20.335 40 22.073 19.834 44 21.356 17.024 48 21.356 12.440 52 21.333 9.127 56 21.316 3.255 60 21.172 2.349 64 21.153 2.250 68 21.002 1.721 72 20.940 1.119 76 20.683 0.784 Table E: Detailed performance comparison on GSM8K dataset with LLaMA-3-8B. Layer LLaMA-3-8B MiniCache LLaMA-3-8B Mean 0 0.488 0.467 2 0.476 0.369 4 0.489 0.388 6 0.487 0.387 8 0.489 0.359 10 0.479 0.388 12 0.486 0.384 14 0.472 0.368 16 0.477 0.343 18 0.446 0.291 20 0.447 0.271 22 0.433 0.234 24 0.399 0.155 26 0.396 0.140 28 0.395 0.052 30 0.391 0.024 32 0.397 0.025 22Table F: Detailed performance comparison on COQA dataset with LLaMA-3-8B. Layer LLaMA-3-8B MiniCache LLaMA-3-8B Mean 0 0.676 0.676 2 0.676 0.571 4 0.675 0.566 6 0.674 0.564 8 0.674 0.561 10 0.673 0.560 12 0.672 0.560 14 0.670 0.559 16 0.670 0.558 18 0.669 0.555 20 0.669 0.552 22 0.668 0.549 24 0.667 0.543 26 0.667 0.537 28 0.666 0.536 30 0.666 0.531 32 0.665 0.528 Table G: Detailed performance comparison on TruthfulQA dataset with LLaMA-3-8B. Layer LLaMA-3-8B MiniCache LLaMA-3-8B Mean 0 32.520 32.524 2 32.231 28.431 4 31.645 28.197 6 31.485 27.894 8 31.008 27.796 10 30.964 27.704 12 30.798 27.371 14 30.798 27.093 16 30.798 26.643 18 30.798 26.517 20 30.798 26.355 22 30.798 26.011 24 30.798 25.044 26 30.798 15.254 28 30.798 14.791 30 30.765 9.419 32 30.390 6.068 23Table H: Detailed performance comparison on GSM8K dataset with Mixtral-8x7B. Layer Mixtral-8x7B MiniCache Mixtral-8x7B Mean 0 0.589 0.575 2 0.592 0.480 4 0.593 0.491 6 0.591 0.469 8 0.580 0.472 10 0.592 0.492 12 0.582 0.485 14 0.572 0.480 16 0.562 0.462 18 0.522 0.432 20 0.526 0.426 22 0.540 0.416 24 0.519 0.398 26 0.515 0.436 28 0.502 0.401 30 0.515 0.386 32 0.490 0.258 Table I: Detailed performance comparison on COQA dataset with Mixtral-8x7B. Layer Mixtral-8x7B MiniCache Mixtral-8x7B Mean 0 0.672 0.675 2 0.671 0.612 4 0.670 0.601 6 0.672 0.590 8 0.674 0.582 10 0.671 0.571 12 0.674 0.561 14 0.670 0.546 16 0.672 0.544 18 0.672 0.530 20 0.675 0.522 22 0.671 0.512 24 0.660 0.455 26 0.657 0.447 28 0.640 0.440 30 0.634 0.424 32 0.459 0.430 24Table J: Detailed performance comparison on TruthfulQA dataset with Mixtral-8x7B. Layer Mixtral-8x7B MiniCache Mixtral-8x7B Mean 0 21.686 19.465 2 21.385 19.405 4 21.368 19.251 6 21.038 19.094 8 21.038 18.265 10 20.216 17.019 12 20.026 15.902 14 19.723 15.505 16 19.641 15.028 18 19.641 14.723 20 19.546 14.543 22 18.756 14.122 24 18.402 13.834 26 18.366 13.789 28 17.738 12.091 30 16.827 12.008 32 16.635 0.430 Table K: Detailed performance comparison on GSM8K dataset with Phi-3-Mini. Layer Phi-3-Mini MiniCache Phi-3-Mini Mean 0 0.774 0.774 2 0.765 0.667 4 0.757 0.661 6 0.754 0.659 8 0.748 0.657 10 0.750 0.645 12 0.750 0.616 14 0.752 0.575 16 0.739 0.491 18 0.742 0.417 20 0.692 0.272 22 0.685 0.206 24 0.640 0.110 26 0.545 0.061 28 0.500 0.039 30 0.460 0.036 32 0.447 0.028 25Table L: Detailed performance comparison on COQA dataset with Phi-3-Mini. Layer Phi-3-Mini MiniCache Phi-3-Mini Mean 0 0.665 0.665 2 0.662 0.562 4 0.657 0.557 6 0.656 0.556 8 0.656 0.556 10 0.654 0.554 12 0.646 0.546 14 0.648 0.538 16 0.647 0.537 18 0.637 0.527 20 0.627 0.487 22 0.591 0.461 24 0.567 0.437 26 0.548 0.408 28 0.527 0.407 30 0.506 0.406 32 0.503 0.403 Table M: Detailed performance comparison on TruthfulQA dataset with Phi-3-Mini. Layer Phi-3-Mini MiniCache Phi-3-Mini Mean 0 19.686 19.465 2 19.385 19.365 4 19.368 19.221 6 19.100 18.255 8 19.038 17.019 10 19.500 15.912 12 19.216 15.525 14 20.026 15.195 16 19.641 15.058 18 18.756 14.763 20 17.738 14.593 22 19.546 14.182 24 19.723 13.954 26 18.366 13.919 28 18.402 12.231 30 16.827 12.158 32 16.635 10.333 26",
      "meta_data": {
        "arxiv_id": "2405.14366v2",
        "authors": [
          "Akide Liu",
          "Jing Liu",
          "Zizheng Pan",
          "Yefei He",
          "Gholamreza Haffari",
          "Bohan Zhuang"
        ],
        "published_date": "2024-05-23T09:43:52Z",
        "pdf_url": "https://arxiv.org/pdf/2405.14366v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "MiniCache addresses the memory bottleneck of Key-Value (KV) caches in Large Language Models (LLMs) by pioneering compression in the depth dimension. It leverages the observed high similarity between adjacent KV cache states in the middle-to-deep layers of LLMs. The method proposes a training-free and general framework that significantly reduces memory footprint and enhances inference throughput while maintaining near-lossless performance. Key findings include achieving up to 5.02x compression ratio, approximately 5x higher inference throughput, and 41% memory reduction compared to the FP16 full cache baseline.",
        "methodology": "MiniCache consists of a reparameterization-based cache merging strategy and a token retention mechanism. It starts merging from the middle layer (S=L/2) where KV cache states show high similarity. State vectors are reparameterized into magnitude and direction components, inspired by weight normalization. The directional component is estimated using Spherical Linear Interpolation (SLERP), which interpolates along the shortest path on the unit sphere, while the original state norms are preserved. The interpolation hyperparameter 't' is set to 0.6. A token retention strategy is introduced to identify and separately retain 'unmergeable' token pairs that have low similarities (high angular distance) to prevent performance degradation. This is controlled by a threshold 'γ' (set to 0.05). The overall framework stores the merged directional vector, token-wise magnitudes, retention tokens, and their indices, making it memory-efficient. Cache restoration involves rescaling directional shared states with magnitudes and recovering retained tokens.",
        "experimental_setup": "The method was evaluated on various LLMs, including LLaMA-2 (7B, 13B), LLaMA-3 (8B, 70B), Phi-3-Mini, Mistral (7B, 7B-Instruct), and Mixtral-8x7B. Benchmarks included a diverse range of question answering and generation datasets from lm-eval-harness (e.g., COPA, MathQA, OpenBookQA, PIQA, RTE, WinoGrande, XSUM, CNN/Daily Mail) and LongBench for long-sequence generation. Performance comparisons were made against baselines such as FP16 full cache, round-to-nearest quantization (RTN), SmoothQuant, KIVI (2-bit, 4-bit), simple average merging, and the token sparsity method H2O. Experiments were conducted using NVIDIA 4 A100 80GB GPUs for sequential model loading and a single 80GB NVIDIA A100 GPU for batch-serving efficiency analysis, utilizing synthetic workloads derived from ShareGPT.",
        "limitations": "The current SLERP-based merging algorithm is suitable only for merging two vectors, which restricts the ability to merge multiple layers simultaneously and limits the maximization of the compression ratio. This limitation underscores the need for more sophisticated algorithms to handle complex merging scenarios. Furthermore, the paper acknowledges that common LLM challenges such as truthfulness and security remain unresolved by this specific work.",
        "future_research_directions": "Future work will focus on enhancing the compression ratio by enabling cross-multiple-layer merging. This involves developing more advanced merging algorithms, such as Spherical Cubic Interpolation, to overcome the current two-vector merging constraint. Other directions include further optimizing memory usage for large-scale deployments in diverse application scenarios and exploring dynamic determination of the interpolation parameter 't' based on the relative magnitude ratio of adjacent layers. Additionally, addressing general LLM challenges like truthfulness and security is an ongoing effort that proceeds alongside advancements in computational efficiency."
      }
    },
    {
      "title": "ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification",
      "abstract": "KV cache stores key and value states from previous tokens to avoid\nre-computation, yet it demands substantial storage space, especially for long\nsequences. Adaptive KV cache compression seeks to discern the saliency of\ntokens, preserving vital information while aggressively compressing those of\nless importance. However, previous methods of this approach exhibit significant\nperformance degradation at high compression ratios due to inaccuracies in\nidentifying salient tokens. In this paper, we present ZipCache, an accurate and\nefficient KV cache quantization method for LLMs. First, we construct a strong\nbaseline for quantizing KV cache. Through the proposed channel-separable\ntokenwise quantization scheme, the memory overhead of quantization parameters\nare substantially reduced compared to fine-grained groupwise quantization. To\nenhance the compression ratio, we propose normalized attention score as an\neffective metric for identifying salient tokens by considering the lower\ntriangle characteristics of the attention matrix. Moreover, we develop an\nefficient approximation method that decouples the saliency metric from full\nattention scores, enabling compatibility with fast attention implementations\nlike FlashAttention. Extensive experiments demonstrate that ZipCache achieves\nsuperior compression ratios, fast generation speed and minimal performance\nlosses compared with previous KV cache compression methods. For instance, when\nevaluating Mistral-7B model on GSM8k dataset, ZipCache is capable of\ncompressing the KV cache by $4.98\\times$, with only a $0.38\\%$ drop in\naccuracy. In terms of efficiency, ZipCache also showcases a $37.3\\%$ reduction\nin prefill-phase latency, a $56.9\\%$ reduction in decoding-phase latency, and a\n$19.8\\%$ reduction in GPU memory usage when evaluating LLaMA3-8B model with a\ninput length of $4096$.",
      "full_text": "ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification Yefei He1 Luoming Zhang1 Weijia Wu2 Jing Liu3 Hong Zhou1† Bohan Zhuang1,3† 1Zhejiang University, China 2National University of Singapore, Singapore 3ZIP Lab, Monash University, Australia Abstract KV cache stores key and value states from previous tokens to avoid re-computation, yet it demands substantial storage space, especially for long sequences. Adaptive KV cache compression seeks to discern the saliency of tokens, preserving vital information while aggressively compressing those of less importance. However, previous methods of this approach exhibit significant performance degradation at high compression ratios due to inaccuracies in identifying salient tokens. Ad- ditionally, the compression process introduces excessive overhead, substantially increasing memory burdens and the generation latency. In this paper, we present ZipCache, an accurate and efficient KV cache quantization method for large lan- guage models (LLMs). First, we construct a strong baseline for quantizing KV cache. Through the proposed channel-separable tokenwise quantization scheme, the memory overhead of quantization parameters are substantially reduced com- pared to fine-grained groupwise quantization. To enhance the compression ratio, we propose normalized attention score as an effective metric for identifying salient tokens by considering the lower triangle characteristics of the attention matrix. The quantization bit-width for each token is then adaptively assigned based on their saliency. Moreover, we develop an efficient approximation method that decouples the saliency metric from full attention scores, enabling compatibility with fast attention implementations like FlashAttention. Extensive experiments demonstrate that ZipCache achieves superior compression ratios, fast generation speed and min- imal performance losses compared with previous KV cache compression methods. For instance, when evaluating Mistral-7B model on GSM8k dataset, ZipCache is capable of compressing the KV cache by 4.98×, with only a 0.38% drop in accuracy. In terms of efficiency, ZipCache also showcases a 37.3% reduction in prefill-phase latency, a 56.9% reduction in decoding-phase latency, and a 19.8% reduction in GPU memory usage when evaluating LLaMA3-8B model with a input length of 4096. 1 Introduction LLMs with the next-token-prediction scheme have achieved remarkable advancements in various text-related tasks, such as language understanding [13, 34, 10], content creation [1, 5, 36], coding [3, 29, 42] and mathematics [33, 23, 35]. In this generation scheme, the forthcoming token interacts with all previous tokens via the attention mechanism [38], where the query, key and value states will be †Corresponding author. Email: zhouhong_zju@zju.edu.cn, bohan.zhuang@gmail.com Preprint. Under review. arXiv:2405.14256v1  [cs.LG]  23 May 2024calculated for each token. As the past tokens will not be altered, previously computed key and value states can be stored as KV cache to prevent re-computations, significantly improving the generation speed. However, as the batch size and the input context length grows, the stored KV cache emerges as a new memory bottleneck for LLMs. For example, when serving a 175B-parameter LLM [1] with a batch size of 64 and a context length of 4096, the KV cache can occupy 1.2TB of memory space, while the model weights only require 350GB. Meanwhile, the size of KV cache will continue to increase as decoding progresses. Therefore, the compression of KV cache is crucial for the efficient deployment of LLMs. Recent compression methods for KV cache can be broadly categorized into two types. The first type of methods compresses the KV cache uniformly, without considering the significance of individual tokens. To preserve performance, these methods often rely on either high-precision quantization [21] or maintaining recent tokens in full-precision [32], which undoubtedly compromise the compression ratio. Additionally, if salient tokens are not among the most recent ones, such as in information retrieval tasks, it may result in degraded performance. The other type of methods [ 46, 43, 16] compress KV cache adaptively by identifying salient tokens and compresses them separately. This approach aligns with the observation that a minority of tokens contribute the majority of attention scores [41], potentially achieving higher compression ratios than non-adaptive methods. However, current adaptive KV cache compression methods [ 46, 43] use accumulated attention scores as a metric of token saliency, which is insufficient in two aspects. First, accumulated attention scores is inaccurate in identifying important tokens. Due to the presence of attention masks, the attention matrix is a lower triangular matrix. Earlier tokens tend to have larger softmax attention values and more attention scores to be accumulated, as illustrated in Figure 3. Under this metric, the saliency of the most recent tokens can never surpass that of the first token, thereby introducing a bias in determining token saliency. Additionally, to obtain accumulated attention scores, full attention matrices must be explicitly computed and stored, which can be inefficient for serving LLMs. Given an input context length of l, fast attention implementations such as FlashAttention [8, 7] only require O(l) memory by computing attention output in blocks without retaining complete attention matrices. By contrast, storing full attention matrices requires O(l2) memory, and the large number of memory accesses significantly slows down the inference speed, as depicted in Figure 4. Figure 1: Accuracy and efficiency compar- isons across various KV cache compression methods. Data is collected with LLaMA3- 8B model on Line Retrieval dataset. Among these methods, ZipCache achieves the high- est accuracy, generation speed and compres- sion ratio. Details can be found in the sup- plementary material. To address these challenges, we introduce ZipCache, an efficient KV cache compression method that attains ex- ceptionally high compression ratios by accurate salient token identification. Figure 1 presents an overview of latency-accuracy comparisons among ZipCache and diverse KV cache compression methods. We start by designing an efficient quantization baseline for com- pressing the KV cache. To preserve performance, prede- cessor methods [32, 21] employ fine-grained groupwise quantization, which involves independent quantization for a small channel group within each token. However, this method necessitates storing extensive quantization parameters and results in significant memory overhead. By contrast, we introduce a channel-separable quanti- zation scheme that decouples the quantization along channel and token dimensions. This method signifi- cantly reduces the quantization overhead without com- promising performance. To accurately recognize salient tokens, we introduce a new token saliency metric based on normalized attention scores, which alleviates the bias towards earlier tokens that accumulate more val- ues. All tokens, without exception, will be quantized to the target bit-width based on their estimated saliency, boosting the overall compression ratio. Moreover, to ease integration with fast attention implementa- tions, we introduce an efficient approximation of the token saliency metric. This approximation only relies on computing and storing attention scores from a few number of tokens, which we refer to as probe tokens. An effective probe token selection strategy is then introduced to minimize performance loss. As a result, the majority of tokens can benefit from fast attention implementations, significantly enhancing the generation speed. 2In summary, our contributions are as follows: • We establish an efficient channel-separable quantization scheme for KV cache, which significantly reduces the overhead of quantization parameters without compromising performance compared to fine-grained groupwise quantization approach. • We propose an accurate metric for assessing token saliency based on normalized attention scores. All tokens are adaptively quantized according to their assessed saliency, thereby improving the overall compression ratio. • We further develop an efficient approximation method for the token saliency metric that integrates seamlessly with fast attention implementations, enhancing generation speed. • By integrating these three techniques, we present ZipCache, an accurate and efficient framework for KV cache compression. Extensive experiments demonstrate that ZipCache reaches a new state-of-the-art performance for KV cache compression in terms of compression ratio, accuracy and generation efficiency. 2 Related Work 2.1 Model Quantization Quantization is a prevalent technique for compressing deep neural networks by representing model weights and activations with lower numerical bit-widths. This technique can be categorized into two primary approaches based on the necessity of fine-tuning: post-training quantization (PTQ) [26, 17, 14] and quantization-aware training (QAT) [28, 31]. For large language models (LLMs), where fine-tuning can be data- and computation-intensive, PTQ is often the preferred method [40, 11, 45, 27]. In this paper, we also quantize KV cache in a post-training manner. For both approaches, quantization can be implemented at various levels of granularity, including channelwise, tokenwise, and groupwise approach. Typically, a finer quantization granularity involves the independent quantization of smaller parameter groups, which often results in improved performance albeit at the cost of more quantization parameters and increased memory overhead. In the context of LLMs, fine-grained quantization is frequently utilized due to the presence of outliers [22, 45]. However, for KV cache compression, this will greatly reduce the overall compression ratio. Mixed precision quantization [39, 44, 12, 2] allocates varying bit-widths to distinct parts of a model or tensor, enabling a more compact compression. This approach originates from the observation that model components exhibit differing sensitivities to quantization. Consequently, components with low sensitivity can utilize reduced bit-widths without impairing performance. For LLMs, previous studies [46, 43, 30, 18] have shown significant disparities in the importance of tokens, indicating that heavy compression of non-critical tokens has minimal impact on overall performance. This insight highlights the applicability of mixed precision quantization for compressing the KV cache. 2.2 KV Cache Compression While KV cache effectively prevents re-computation and significantly enhances generation speed, its memory footprint is notably substantial with long-context input. To alleviate this, many efforts have been made to reduce the KV cache size. Based on the compression method, these methods can be categorized into two groups: token dropping [46, 16, 30] and KV cache quantization [43, 21, 32]. The former identifies and drops unimportant tokens in the KV cache. For example, H2O [46] only maintain 20% heavy-hitted tokens and 20% recent tokens while evicting the rest. However, discarding tokens permanently erases their information, which proves to be suboptimal for tasks such as retrieval [43]. Conversely, the latter category employs quantization on the cached key and value states, and mixed precision quantization can further be applied once token importance is identified [ 43]. To tackle the outliers present in the KV cache, these methods extract the outlier as full precision [21] or use finer-grained quantization scheme [32], which increases the quantization overhead. In this study, we propose an efficient channel-separable quantization scheme with reduced quantization overhead and strong performance. Additionally, both categories of methods commonly adopt accumulated attention scores as the metric for token importance [46, 43]. However, we observe that this criterion is inaccurate and can result in significant performance deterioration at low bit-widths. In contrast, we achieve superior compression performance by utilizing a more accurate metric for identifying salient tokens. 33 Preliminary 3.1 Attention Block in LLMs Given an input prompt, the generation process of LLMs can be broadly categorized into two distinct phases: the prefill phase, which computes and stores the KV cache for input tokens, and the decoding phase, where new tokens are generated through a next-token-prediction scheme. Given input data X and an attention block with its weight matrices WQ, WK and WV, the prefill phase can be formulated as: Q = XWQ, K = XWK, V = XWV, (1) A = Softmax \u0012QKT √dk \u0013 , O = AV. (2) Here, dk is the dimension of the key, and A refers to the attention scores. K and V will be stored as KV cache. For clarity, we have omitted the output projection. For the decoding phase, given x as the embedding vector of the current token, the query q becomes a vector and the KV cache matrices will be updated as follow: q = xWQ, K = Concat(K, xWK), V = Concat(V, xWV). (3) The attention output are then computed as follows: a = Softmax \u0012qKT √dk \u0013 , o = aV. (4) To ensure clarity and consistency, we introduce notation to define the hyper-parameters used in the paper. Specifically, we denote the batch size as b, the number of attention heads as h, the sequence length as l, and the head dimension as d. 3.2 Model Quantization Uniform quantization is adopted in our study and all experiments. Given a floating-point vector x, it can be uniformly quantized to k-bit as follows: ˆx = QU (x, k) = clip(⌊x s ⌉ + z, 0, 2k − 1) · s. (5) Here, ⌊·⌉ denotes the round operation, s = max(x)−min(x) 2k−1 and z = −⌊min(x) s ⌉ are quantization parameters. It should be noted that the quantization parameters are stored in full-precision, which can lead to significant overhead if the quantization is fine-grained. 4 Method 4.1 A Strong Baseline for KV Cache Quantization Tokenwise quantization, as depicted in Figure 2(b) is prevalent in quantizing large language models (LLMs) due to the distinct representations of individual tokens. However, it has been widely observed, as illustrated in Figure 2(a), that outliers emerge within the channel dimensions of key and value matrices [43, 32], posing challenges for tokenwise quantization. To address this, recent work [32] resorts to groupwise quantization, where outlier channels are processed in distinct groups, as illustrated in Figure 2(c). However, this fine-grained quantization approach introduces excessive memory overhead, thereby significantly impacting the compression ratio. For instance, considering X ∈ Rb×h×l×d as the data to be quantized and a group size of n, tokenwise quantization only results in 2bl quantization parameters, while groupwise quantization would yield 2bhld n quantization parameters. Since these parameters are usually stored in full precision, this overhead would constitute a substantial portion of the storage cost for quantized data. Motivated by depthwise separable convolution [ 19], we introduce an efficient channel-separable tokenwise quantization scheme, which disentangles the channel and token dimensions. As shown in 4𝒔,𝒛∈𝑅𝑙 (b) Tokenwise Quantization (c) Groupwise Quantization (d) Channel-separable  Tokenwise Quantization 𝒄 ∈ 𝑅ℎ𝑑 (a) Visualization of  Key and Value States 𝐗 ∈ 𝑅𝑙∗ℎ𝑑 𝐗 ∈ 𝑅𝑙∗ℎ𝑑 𝐗 ∈ 𝑅𝑙∗ℎ𝑑 𝒔,𝒛∈𝑅𝑙∗ℎ𝑑/𝑛 𝒔,𝒛∈𝑅𝑙 Figure 2: Visualization and different quantization granularities for key and value states. Here, we omit the batch dimension for simplicity. For keys, channel outliers emerge, yet token representations exhibit minimal differences. For values, both channel outliers and distinct token representations exist. Figure 2(d), our approach initiates by normalizing each channel of data X with a scaling factor c. For the i-th channel in X, the normalization process can be formulated as: Xi = Xi ci , where ci = p max(|Xi|). (6) After normalization, each channel is scaled to a closed magnitude, mitigating the influence of outliers during tokenwise quantization. Subsequently, tokenwise quantization can be reliably applied and the scales c are multiplied back to restore the magnitude of each channel. The process of channel-separable tokenwise quantization is summarized in the supplementary material. Within this quantization scheme, the total number of quantization parameters amounts to hd + 2bl, representing a notable reduction compared to groupwise quantization, while effectively balancing the outlier channels and the representation of each token. Table 1: Performance comparisons of different quantization granularities for KV cache. The KV cache is quantized to 4-bit and the compression ratio is calculated with b = 8, hd = l = 4096and n = 32. Data is collected with LLaMA3-8B model on GSM8k dataset. Key Cache Quantization Granularity Value Cache Quantization Granularity Quantization Parameters Compression Ratio Acc.(%) / / 0 1× 55.88 Groupwise Groupwise 4bhld/n 3.2× 54.51 Tokenwise Tokenwise 4bl 3.99× 49.81 Channelwise Tokenwise 2hd+ 2bl 4.00× 52.77 Channelwise Channel-separable Tokenwise 3hd+ 2bl 4.00× 54.74 As referred to Figure 2(a), since the differences in token representations are small in key cache, we employ channelwise quantization for the key cache to further reduce overhead and employ channel- separable tokenwise quantization for the value cache. As depicted in Table 1, this configuration yields superior performance with reduced quantization overhead compared with groupwise quantization, thereby establishing a robust baseline for KV cache quantization. 4.2 Accurate Salient Token Identification Adaptive KV cache compression [46, 43, 16] aims to discern the saliency of each token, keeping the information of salient tokens while evicting or aggressively compressing the rest, to achieve a higher compression ratio. These salient tokens, also referred to as \"Heavy Hitters\" [46], are often identified based on accumulated attention scores. Given attention score matrix A ∈ Rl×l, the saliency of token i is estimated by: pi = lX k=1 Ak,i. (7) 5So   there  are   five   eggs 1.00 0.00 0.00 0.00 0.00 0.33 0.67 0.00 0.00 0.00 0.35 0.30 0.35 0.00 0.00 0.04 0.06 0.14 0.76 0.00 0.02 0.02 0.04 0.06 0.86 egs five   are   there   So : 40% salient tokens 1.74 1.05 0.53 0.82 0.86 0.35 0.26 0.18 0.41 0.86 Question: There are 15 trees in the  grove… Let's think step by step… Question: If there are 3 cars… Let's think step by step… Question: Leah had 32 chocolates… Let's think step by step… … Question: Olivia has $23… Let's think step by step… Question: Janet’s ducks lay 16 eggs per  day…How much in dollars does she  make every day at the farmers' market? (a) (b) (c) 𝒑𝒊 ෥𝒑𝒊 Figure 3: (a) A toy example to illustrate accumulated attention scores and normalized attention scores. Initial tokens have larger attention scores and more values to be accumulated. (b) A sample from GSM8k dataset with chain-of-thoughts (CoT) prompting. (c) The probability of each token being selected as a salient token, measured by both accumulated and normalized attention scores. Tokens correspond to the final question are identified as low saliency by accumulated attention scores. Tokens with large saliency values are then considered salient tokens. However, this approach has inherent limitations due to the lower triangular nature of the attention score matrix, as illustrated in Figure 3(a). There are two primary issues. Firstly, earlier tokens benefit from having more values accumulated since the elements above the diagonal are all zero. For instance, in a sequence of length l, the initial token accumulates l positive values, whereas the final token only accumulates one. Secondly, Softmax function converts real numbers into probabilities, so that the earlier rows of the attention matrix tending to have higher values, as fewer numbers are involved in the Softmax calculation. Consequently, the accumulated attention score of the final token will always be smaller than that of the first, which exceeds 1. To address this, previous works, such as H2O [46], always maintain recent caches in full precision. Nevertheless, this solution is suboptimal since recent tokens are not necessarily the most significant ones. To enhance the evaluation of each token’s saliency, we introduce an accurate token saliency metric based on normalized attention scores ˜pi: ˜pi = Pl k=1 Ak,i nnz(A:,i) (8) Here, nnz(A:,i) denotes the number of non-zero elements in the i-th column of A. As evidenced in Figure 3(a), normalizing the accumulated attention scores mitigates the influence of excessively large values in the initial rows of the attention score matrix, thereby delivering a more precise assessment. To validate the efficacy of our new metric, we input a sample from GSM8k dataset with chain-of- thoughts (CoT) prompting to the LLaMA3-8B model and identify saliency of each token by Eq. 7 and Eq. 8, respectively. As depicted in Figure 3(b) and (c), the salient tokens are at the end of the prompt, which correspond to the question for LLM to answer. However, these tokens are identified as low saliency by accumulated attention scores. Under the KV cache compression framework, these tokens would either be discarded or quantized to extremely low bit-width, resulting in a significant performance deterioration. In contrast, our method accurately identifies the salient tokens. Additional experimental results regarding the accuracy of our method will be detailed in Section 5.2. 4.3 Efficient Approximation of Saliency Metric As analyzed in Section 4.2, adaptive KV cache compression requires the explicit computation of full attention scores, as referred to Figure 4(b), which clashes with fast attention implementations like FlashAttention [8, 7, 9]. As shown in Figure 4(c), FlashAttention computes attention outputs in tiles without storing the intermediate attention scores. To reconcile the efficiency of FlashAttention with the substantial compression offered by adaptive KV caching, we devise an effective approximation for Eq. 8 as a measure of token saliency. Specifically, we sample a small group of tokens, designated 6𝐀 = (b) Standard Attention 𝐀 (c) FlashAttention 𝑸 𝑽 𝑶 𝑽 𝐊𝑻 𝑸 𝐊𝑻 𝑶 Memory=𝑶(𝒍𝟐) More memory access & slower Memory=𝑶(𝒍) Less memory access & faster (a) Efficient Saliency Metric with Probe Tokens Input tokens 𝐀𝑝𝑟𝑜𝑏𝑒 Probe tokens (b) Standard Attention (c) FlashAttention Regular tokens Output : Intermediate attention scores Figure 4: (a): Efficient saliency metric only requires attention scores of probe tokens through standard attention, enabling fast computation for the majority of tokens through FlashAttention. (b): In standard attention, full attention scores are computed before deriving the attention output. (c): FlashAttention avoids large attention matrix memory transfers by partitioning input matrices into blocks for incremental computation. as probe tokens, and compute their attention scores Aprobe as follows: Aprobe = Softmax \u0012QprobeKT √dk \u0013 . (9) By substituting Aprobe into Eq. 8, we can approximate the saliency of all tokens. For the remaining non-probe tokens, their attention scores do not have to be computed explicitly, enabling the integration of fast attention implementations to expedite the generation process, as illustrated in Figure 4(a). Table 2: Performance comparisons of various probe strategies. Data is col- lected from LLaMA3-8B model on GSM8k dataset. We quantize 40% salient tokens to 4-bit and the remaining 60% tokens to 2-bit. The proportion of probe tokens is 10%. Probe Strategy Acc.(%) All tokens 52.54 Random tokens 47.46 Special tokens 46.78 Recent tokens 51.10 Random+recent tokens 52.08 However, the positions of the probe tokens will un- doubtedly affects the accuracy of the approximated token saliency and the selection of probe tokens is under explored. In this study, we suggest four strategies for sampling probe tokens: • Random tokens. The probe tokens are randomly sam- pled from all positions. • Special tokens. The special tokens and punctuation tokens will be treated as probe tokens. • Recent tokens. The most recent tokens are selected as probe tokens. • Random+recent tokens. The probe tokens will be di- vided into two parts, one using recent tokens and the other randomly selecting from the remaining tokens. It should be emphasized that our study diverges from prior research [16] in that, rather than directly choosing special or recent tokens as salient tokens, we opt to sample a subset of tokens as \"probes\" to detect the salient ones. As depicted in Table 2, we present a comprehensive comparison of the performance among four distinct sampling strategies. Among the four strategies examined, a hybrid approach that combines recent tokens with randomly selected tokens emerges as the most effective. Unless otherwise specified, this hybrid strategy with 5% recent tokens and 5% random tokens will be employed in our method. 5 Experiment 5.1 Implementation Details Models and datasets. To validate the efficacy of our proposed method, we conduct experiments with three open-source LLMs: Mistral [ 20], LLaMA2 [37] and LLaMA3. These models are evaluated on three challenging benchmarks: GSM8k [6] for math problem solving, HumanEval [4] for code 7generation, and Line Retrieval [25] for data retrieval. To ensure reproducibility, the reported results are obtained using the Language Model Evaluation Harness [15] and LongEval [24]. Quantization and generation settings. We employ mixed precision quantization for KV cache where salient tokens will be quantized to 4-bit while the remaining will be quantized to 2-bit. For both subsets, we apply channelwise quantization for the key cache and channel-separable tokenwise quantization for the value cache. The proportion of salient tokens will be denoted by \"Saliency Ratio\" in the experimental results. During the decoding process, ZipCache adopts a streaming strategy [21] and repeats the compression process for the KV cache whenever 100 new tokens are generated. 5.2 Comparison with SOTA methods 5.2.1 Evaluation on GSM8k We begin our evaluation on GSM8k dataset with chain-of-thoughts (CoT) prompting, and the results are presented in Table 3. This task requires LLM to solve mathematical problems and return the final answer without multiple options. This task poses considerable challenges and previous KV cache compression methods manifest notable declines in accuracy. For instance, KIVI [32] shows an accuracy drop of 7.89% on LLaMA3-8B model, indicating the suboptimality of preserving recent tokens in full precision instead of identifying salient ones. Moreover, there is a substantial decrease in accuracy, amounting to 20.4%, for MiKV [43] under the high compression ratio. This suggests that accumulated attention scores mistakenly identify salient tokens, resulting in the loss of vital information during compression. By contrast, the proposed normalized attention scores can accurately measure token saliency, leading to a substantial enhancement in accuracy by 18.27% for LLaMA3-8B models in comparison to MiKV . In comparison to GEAR [21], which quantizes the entire KV cache to 4-bit, our approach additionally quantizes 40% tokens to 2-bit with enhanced performance on Mistral-7B model. This underscores the superiority of accurate adaptive compression of KV cache. Table 3: Performance comparisons on GSM8k with CoT prompts. Here, \"H/L\" denotes the bit-width for salient tokens (high-precision) and regular tokens (low-precision), respectively. The compression ratio is calculated with an average input length of l = 840. Model Method Bit-width (H/L) Saliency Ratio Compression Ratio Acc.(%) Mistral-7B FP16 16/16 100% 1 × 41.62 H2O [46] 16/0 40.0% 2.50 × 1.67 GEAR [21] 4/4 100% 3.00 × 39.42 KIVI [32] 16/2 15.2% 3.46 × 39.04 MiKV [43] 4/2 60.0% 4.98 × 36.32 ZipCache 4/2 60.0% 4.98× 41.24 LLaMA2-7B FP16 16/16 100% 1 × 14.18 H2O [46] 16/0 40.0% 2.50 × 13.50 GEAR [21] 4/4 100% 3.00 × 12.96 KIVI [32] 16/2 15.2% 3.46 × 13.19 MiKV [43] 4/2 60.0% 4.98 × 9.02 ZipCache 4/2 60.0% 4.98× 13.50 LLaMA2-13B FP16 16/16 100% 1 × 28.05 H2O [46] 16/0 40.0% 2.50 × 26.00 GEAR [21] 4/4 100% 3.00 × 25.40 KIVI [32] 16/2 15.2% 3.46 × 27.29 MiKV [43] 4/2 60.0% 4.98 × 23.65 ZipCache 4/2 60.0% 4.98× 27.85 LLaMA3-8B FP16 16/16 100% 1 × 55.88 H2O [46] 16/0 40.0% 2.50 × 27.82 GEAR [21] 4/4 100% 3.00 × 49.43 KIVI [32] 16/2 15.2% 3.46 × 47.99 MiKV [43] 4/2 70.0% 4.69 × 35.48 ZipCache 4/2 70.0% 4.69× 53.75 5.2.2 Evaluation on Line Retrival We further evaluate the data retrieval performance of various KV cache compression methods on Line Retrieval [25] dataset, where LLMs are required to retrieve specific content from a record 8of lines using a corresponding line index. The accuracy results under various number of lines are depicted in Figure 5. Notably, all quantization-based compression methods exhibit superior performance compared to the eviction-based approach H2O [ 46]. For eviction-based methods, information is permanently discarded upon eviction, whereas quantization introduces only minor errors while preserving the integrity of the data. Additionally, in comparison to KIVI [32], which always maintains recent caches at full precision, our approach consistently achieves better retrieval accuracy. This can be attributed to the nature of retrieval tasks, where salient tokens may appear at any position within the context, rather than being confined to the most recent caches. Moreover, when compared to MiKV [43], which employs accumulated attention scores as a saliency metric, our method yields a remarkable 42% accuracy improvement when evaluated using 200 lines on the Mistral-7b model. This substantial enhancement once more highlights the effectiveness of normalized attention scores in identifying salient tokens. Additional experimental results on HumanEval [4] can be found in the supplementary material. 50 100 150 200 Number of Lines 0 20 40 60 80 100Accuracy (%) LLaMA2-13B Full Cache ZipCache KIVI-2 MiKV H2O 100 200 300 400 Number of Lines 0 20 40 60 80 100Accuracy (%) LLaMA3-8B Full Cache ZipCache KIVI-2 MiKV H2O 50 100 150 200 Number of Lines 0 20 40 60 80 100Accuracy (%) Mistral-7B Full Cache ZipCache KIVI-2 MiKV H2O Figure 5: Performance comparisons of various KV cache compression methods on Line Retrieval. 5.3 Generation Efficiency In this subsection, we compare the latency and memory consumption of ZipCache and MiKV [43] under various input lengths, as depicted in Figure 6. Data is collected by serving LLaMA3-8B model on a Nvidia A100 GPU. MiKV employs accumulated attention scores to estimate token saliency, necessitating the use of standard attention for both prefill and decoding phases. Conversely, through an efficient approximate saliency metric, ZipCache requires only the calculation of the attention matrix for 10% of the tokens, while the remaining 90% tokens can be computed using either FlashAttention [7] or FlashDecoding [9]. Consequently, ZipCache achieves faster inference speed and lower memory usage, boasting a 37.3% reduction in prefill-phase latency, a 56.9% reduction in decoding-phase latency, and a 19.8% reduction in GPU memory usage when the input length scales to 4096. (a) Prefill phase latency  (b) Decoding phase latency  (c) GPU memory Figure 6: Comparisons of prefill-phase, decoding-phase latency and memory consumption between MiKV and ZipCache. 96 Conclusion and Future Work In this paper, we have proposed ZipCache, an accurate and efficient mixed-precision quantization framework for compressing KV cache. To commence, we introduce a channel-separable quantization scheme for KV cache, effectively reducing the overhead of storing quantization parameters compared to traditional fine-grained quantization schemes without performance degradation. Additionally, we present a novel metric for accurately assessing token saliency based on normalized attention scores. This metric enables adaptive quantization of all tokens according to their saliency, leading to improved compression ratios without sacrificing model performance. Moreover, we introduce an efficient approximation method for the token saliency metric, seamlessly integrating with fast attention implementations such as FlashAttention and FlashDecoding. This enhancement signifi- cantly boosts generation speed and reduces GPU memory requirements. Our extensive experiments have demonstrated that ZipCache achieves state-of-the-art compression performance in terms of compression ratio, accuracy and generation speed. We believe that ZipCache will pave the way for more practical and scalable deployment of LLMs in various real-world applications. Limitations and Broader Impacts. While ZipCache presents promising advancements in KV cache mixed-quantization frameworks for LLMs, the saliency ratio is manually specified before evaluation and cannot be automatically adjusted based on task datasets. Moreover, similar to other generative models, ZipCache can potentially be used to generate malicious content. References [1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. [2] A. Chauhan, U. Tiwari, et al. Post training mixed precision quantization of neural networks using first- order information. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1343–1352, 2023. [3] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y . Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [4] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y . Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [5] W.-L. Chiang, Z. Li, Z. Lin, Y . Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y . Zhuang, J. E. Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, march 2023. URL https://lmsys. org/blog/2023-03-30-vicuna, 3(5), 2023. [6] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [7] T. Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023. [8] T. Dao, D. Y . Fu, S. Ermon, A. Rudra, and C. Ré. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022. [9] T. Dao, D. Haziza, F. Massa, and G. Sizov. Flash-decoding for long-context inference, 2023. [10] J. C. de Winter. Can chatgpt pass high school exams on english language comprehension? International Journal of Artificial Intelligence in Education, pages 1–16, 2023. [11] T. Dettmers, M. Lewis, Y . Belkada, and L. Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems, 35:30318–30332, 2022. [12] Z. Dong, Z. Yao, A. Gholami, M. W. Mahoney, and K. Keutzer. Hawq: Hessian aware quantization of neural networks with mixed-precision. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 293–302, 2019. [13] M. Du, F. He, N. Zou, D. Tao, and X. Hu. Shortcut learning of large language models in natural language understanding. Communications of the ACM, 67(1):110–120, 2023. [14] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. [15] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, A. Le Noac’h, H. Li, K. McDonell, N. Muennighoff, C. Ociepa, J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron, 10L. Sutawika, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou. A framework for few-shot language model evaluation, 12 2023. [16] S. Ge, Y . Zhang, L. Liu, M. Zhang, J. Han, and J. Gao. Model tells you what to discard: Adaptive kv cache compression for llms. In The Twelfth International Conference on Learning Representations, 2024. [17] Y . He, L. Liu, J. Liu, W. Wu, H. Zhou, and B. Zhuang. Ptqd: Accurate post-training quantization for diffusion models. Advances in Neural Information Processing Systems, 36, 2023. [18] L. Hou, R. Y . Pang, T. Zhou, Y . Wu, X. Song, X. Song, and D. Zhou. Token dropping for efficient bert pretraining. arXiv preprint arXiv:2203.13240, 2022. [19] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. [20] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [21] H. Kang, Q. Zhang, S. Kundu, G. Jeong, Z. Liu, T. Krishna, and T. Zhao. Gear: An efficient kv cache compression recipefor near-lossless generative inference of llm. arXiv preprint arXiv:2403.05527, 2024. [22] Y . J. Kim, R. Henry, R. Fahim, and H. H. Awadalla. Finequant: Unlocking efficiency with fine-grained weight-only quantization for llms. arXiv preprint arXiv:2308.09723, 2023. [23] C. Li, W. Wang, J. Hu, Y . Wei, N. Zheng, H. Hu, Z. Zhang, and H. Peng. Common 7b language models already possess strong math capabilities. arXiv preprint arXiv:2403.04706, 2024. [24] D. Li, R. Shao, A. Xie, Y . Sheng, L. Zheng, J. Gonzalez, I. Stoica, X. Ma, , and H. Zhang. How long can open-source llms truly promise on context length?, June 2023. [25] D. Li, R. Shao, A. Xie, Y . Sheng, L. Zheng, J. Gonzalez, I. Stoica, X. Ma, and H. Zhang. How long can context length of open-source llms truly promise? In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023. [26] Y . Li, R. Gong, X. Tan, Y . Yang, P. Hu, Q. Zhang, F. Yu, W. Wang, and S. Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. In International Conference on Learning Representations, 2020. [27] J. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and S. Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023. [28] J. Liu, R. Gong, X. Wei, Z. Dong, J. Cai, and B. Zhuang. Qllm: Accurate and efficient low-bitwidth quanti- zation for large language models. In The Twelfth International Conference on Learning Representations, 2024. [29] J. Liu, C. S. Xia, Y . Wang, and L. Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36, 2024. [30] Z. Liu, A. Desai, F. Liao, W. Wang, V . Xie, Z. Xu, A. Kyrillidis, and A. Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. Advances in Neural Information Processing Systems, 36, 2024. [31] Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y . Mehdad, Y . Shi, R. Krishnamoorthi, and V . Chandra. Llm- qat: Data-free quantization aware training for large language models. arXiv preprint arXiv:2305.17888, 2023. [32] Z. Liu, J. Yuan, H. Jin, S. Zhong, Z. Xu, V . Braverman, B. Chen, and X. Hu. Kivi: A tuning-free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750, 2024. [33] H. Luo, Q. Sun, C. Xu, P. Zhao, J. Lou, C. Tao, X. Geng, Q. Lin, S. Chen, and D. Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct.arXiv preprint arXiv:2308.09583, 2023. [34] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. Improving language understanding by generative pre-training. 2018. [35] M. Tan, L. Wang, L. Jiang, and J. Jiang. Investigating math word problems using pretrained multilingual language models. arXiv preprint arXiv:2105.08928, 2021. [36] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [37] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 11[38] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [39] K. Wang, Z. Liu, Y . Lin, J. Lin, and S. Han. Haq: Hardware-aware automated quantization with mixed precision. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8612–8620, 2019. [40] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pages 38087–38099. PMLR, 2023. [41] G. Xiao, Y . Tian, B. Chen, S. Han, and M. Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2023. [42] F. F. Xu, U. Alon, G. Neubig, and V . J. Hellendoorn. A systematic evaluation of large language models of code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming, pages 1–10, 2022. [43] J. Y . Yang, B. Kim, J. Bae, B. Kwon, G. Park, E. Yang, S. J. Kwon, and D. Lee. No token left behind: Reliable kv cache compression via importance-aware mixed precision quantization. arXiv preprint arXiv:2402.18096, 2024. [44] Z. Yao, Z. Dong, Z. Zheng, A. Gholami, J. Yu, E. Tan, L. Wang, Q. Huang, Y . Wang, M. Mahoney, et al. Hawq-v3: Dyadic neural network quantization. In International Conference on Machine Learning, pages 11875–11886. PMLR, 2021. [45] Z. Yao, R. Yazdani Aminabadi, M. Zhang, X. Wu, C. Li, and Y . He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers.Advances in Neural Information Processing Systems, 35:27168–27183, 2022. [46] Z. Zhang, Y . Sheng, T. Zhou, T. Chen, L. Zheng, R. Cai, Z. Song, Y . Tian, C. Ré, C. Barrett, et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36, 2023. 12Appendix A Calculation of Overhead for Different Quantization Schemes Assuming b = 8, hd = l = 4096, and that the KV cache is quantized to4-bit, we proceed to calculate the actual compression ratio for different quantization granularities. For groupwise quantization with a group size of n = 32, the compression ratio Rgroup is given by: Rgroup = 2 × bhld × 16 2 × bhld × 4 +4bhld n × 16 = 3.200 (A) For tokenwise quantization, the compression ratio Rtoken can be calculated as: Rtoken = 2 × bhld × 16 2 × bhld × 4 + 4× bl × 16 = 3.992 (B) For our proposed quantization baseline, the compression ratio Rbaseline is determined by: Rbaseline = 2 × bhld × 16 2 × bhld × 4 + 3× hd × 16 + 2× bl × 16 = 3.995 (C) B Implementation Details of ZipCache In this section, we provide an overview of the channel-separable tokenwise quantization scheme in Algorithm 1. Additionally, we present the process of ZipCache’s prefill phase as described in Algorithm 2, as well as its decoding phase detailed in Algorithm 3. It is worth mentioning that during both the prefill and decoding phases, rather than calculating attention outputs separately for probe tokens and regular tokens followed by merging, FlashAttention [7] is utilized to compute the attention output for all tokens simultaneously. Additionally, attention scores of probe tokens are calculated. By bypassing the substantial memory accesses associated with matrix splitting and merging, this strategy enhances generation speed. Algorithm 1: Channel-separable Tokenwise Quantization (CSTQuant) procedure CSTQuant: Input: data X ∈ Rl×hd, target bit-width k for i ← 0 to hd do ci = p max(|Xi|) Xi = Xi ci // Normalizing each channel of X ˆX =TokenQuant(X, k) // Do tokenwise quantization for i ← 0 to hd do ˆXi = ˆXi × ci // Rescale each channel of X return ˆX C Additional Experimental Results C.1 Accuracy and Efficiency Comparisons of various KV cache compression methods In this section, we present the accuracy and efficiency comparisons of various KV cache compression methods, as presented in Table A. Data is collected by evaluating LLaMA3-8B model on200-line retrieval task with a Nvidia A100 GPU. We use a batch size of 8 and an average input length of 3072. Among these methods, ZipCache achieves the highest accuracy, compression ratio and generation speed. Specifically, in comparison to MiKV [43], which identifies salient tokens through accumulated attention scores, our method achieves a notable 10.0% accuracy improvement by accurately pinpointing salient tokens and a substantial38.0% decrease in prefill latency by integrating FlashAttention [7]. 13Algorithm 2: ZipCache for Prefill Phase procedure ZipCachePrefill: Input: Query states Q, key states K, value states V, saliency ratio r%, bit-width for salient tokens kh, bit-width for regular tokens kl // Salient Token Identification Select probe tokens and compute their attention scores Aprobe by Eq. 9 Measure the token saliency ˜p with Aprobe by Eq. 8 // Computing Attention Output with FlashAttention O = FlashAttention(Q, K, V) // Compressing KV Cache Partition key states: Ksalient, Kregular = Split(K, ˜p, r%) Partition value states: Vsalient, Vregular = Split(V, ˜p, r%) Ksalient = ChannelQuant(Ksalient, kh), Vsalient = CSTQuant(Vsalient, kh) Kregular = ChannelQuant(Kregular, kl), Vregular = CSTQuant(Vregular, kl) ˆK = Concat(Ksalient, Kregular) ˆV = Concat(Vsalient, Vregular) // Return Attention Output and Compressed KV Cache return O, ( ˆK, ˆV) Algorithm 3: ZipCache for Decoding Phase procedure ZipCacheDecoding: Input: Query vector q, key vector k, value vector v, KV cache ( ˆK, ˆV), saliency ratio r%, bit-width for salient tokens kh, bit-width for regular tokens kl, decoding token index i, probe attention score Aprobe K = Concat(k, ˆK) // Concatenate key cache V = Concat(v, ˆV) // Concatenate value cache o = FlashAttention(q, K, V) // Compute attention output i = i + 1 if i == 100then // Re-compress every 100 tokens Extract K[: −100] and V[: −100] and adaptively compress them with Aprobe Reset i = 0, Aprobe = None else if i >95 or randint(0, 100) < 5 then // probe tokens consists of 5% recent and 5% random tokens. Compute attention scores a of current token by Eq. 4 Aprobe = Concat(a, Aprobe) // Return Attention Output, KV Cache and Attention Scores from Probe Tokens return o, (K, V), Aprobe C.2 Evaluation on HumanEval In this section, we assess the performance of code generation across various KV cache compression methods, as summarized in Table B. Remarkably, ZipCache attains a compression ratio of 4.94× without sacrificing performance when tested with the Mistral-7B model, outperforming predecessor methods. Moreover, when evaluating on LLaMA3-8B model, our approach outperforms KIVI-2 [32] by 7.32% with a significantly higher compression ratio (4.39× vs. 2.55×). It should be noted that the average input length for this task is only119, while KIVI retains the recent 32 tokens in full-precision, thereby considerably diminishing its overall compression ratio. This underscores the advantage of ZipCache over methods that consistently retain information of recent tokens. 14Table A: Accuracy and efficiency comparisons over LLaMA3-8B on the200-line retrieval task. Here, \"H/L\" denotes the bit-width for salient tokens (high-precision) and regular tokens (low-precision), respectively. 0-bit denotes the tokens are evicted. The compression ratio is calculated with an average input length of l = 3072. Method Bit-width (H/L) Saliency Ratio Compression Ratio Acc.(%) Prefill-phase Latency (ms) FP16 16/16 100% 1 × 100 2340.11 H2O [46] 16/0 40.0% 2.50 × 0 4335.01 GEAR [21] 4/4 100% 3.00 × 100 5957.76 KIVI [32] 16/2 8.33% 4.36 × 96 4010.14 MiKV [43] 4/2 80.0% 4.43 × 90 4170.61 ZipCache 4/2 80.0% 4.43× 100 2584.01 Table B: Performance comparisons on HumanEval for code generation. Here, \"H/L\" denotes the bit-width for salient tokens (high-precision) and regular tokens (low-precision), respectively.0-bit denotes the tokens are evicted. The compression ratio is calculated with an average input length of l = 120. Model Method Bit-width (H/L) Saliency Ratio Compression Ratio Acc.(%) Mistral-7B FP16 16/16 100% 1 × 29.27 H2O [46] 16/0 40.0% 2.50 × 14.63 GEAR [21] 4/4 100% 3.00 × 28.05 KIVI [32] 16/2 26.7% 2.55 × 28.05 MiKV [43] 4/2 60.0% 4.94 × 27.44 ZipCache 4/2 60.0% 4.94× 29.27 LLaMA2-7B FP16 16/16 100% 1 × 14.02 H2O [46] 16/0 40.0% 2.50 × 11.59 GEAR [21] 4/4 100% 3.00 × 13.02 KIVI [32] 16/2 26.7% 2.55 × 11.59 MiKV [43] 4/2 80.0% 4.39 × 10.37 ZipCache 4/2 80.0% 4.39× 12.80 LLaMA3-8B FP16 16/16 100% 1 × 33.54 H2O [46] 16/0 40.0% 2.50 × 15.85 GEAR [21] 4/4 100% 3.00 × 28.66 KIVI [32] 16/2 26.7% 2.55 × 25.61 MiKV [43] 4/2 80.0% 4.39 × 29.88 ZipCache 4/2 80.0% 4.39× 32.93 15",
      "meta_data": {
        "arxiv_id": "2405.14256v1",
        "authors": [
          "Yefei He",
          "Luoming Zhang",
          "Weijia Wu",
          "Jing Liu",
          "Hong Zhou",
          "Bohan Zhuang"
        ],
        "published_date": "2024-05-23T07:37:16Z",
        "pdf_url": "https://arxiv.org/pdf/2405.14256v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "ZipCache addresses the challenges of substantial KV cache storage demands and the inaccuracies/overheads of prior adaptive compression methods. Its main contributions include: 1) Proposing an efficient channel-separable tokenwise quantization scheme that significantly reduces quantization parameter overhead without performance degradation. 2) Introducing normalized attention scores as an accurate metric for identifying salient tokens, which mitigates bias towards earlier tokens. 3) Developing an efficient approximation method for the saliency metric, compatible with fast attention implementations like FlashAttention, to enhance generation speed. ZipCache achieves superior compression ratios (e.g., 4.98x), fast generation speed (e.g., 37.3% prefill, 56.9% decoding latency reduction), and minimal performance losses (e.g., 0.38% accuracy drop) compared to existing methods.",
        "methodology": "ZipCache employs a multi-faceted approach. For quantization, it uses a channel-separable tokenwise quantization scheme, normalizing each channel before tokenwise quantization to handle outliers and reduce parameter overhead (e.g., channelwise for keys, channel-separable tokenwise for values). To identify salient tokens, it introduces normalized attention scores (sum of column attention scores divided by the number of non-zero elements) which are less biased than accumulated attention scores. Tokens are adaptively assigned bit-widths based on this saliency (e.g., 4-bit for salient, 2-bit for regular tokens). For efficiency, it approximates the saliency metric by computing attention scores only for a small subset of 'probe tokens' (a hybrid of recent and randomly selected tokens), allowing the majority of tokens to be processed by fast attention implementations (FlashAttention/FlashDecoding). During decoding, a streaming strategy is used, re-compressing the KV cache every 100 new tokens.",
        "experimental_setup": "Experiments were conducted using three open-source LLMs: Mistral-7B, LLaMA2-7B, LLaMA2-13B, and LLaMA3-8B. The models were evaluated on three challenging benchmarks: GSM8k (math problem solving with chain-of-thoughts prompting), HumanEval (code generation), and Line Retrieval (data retrieval). Performance was measured in terms of accuracy (%), compression ratio (x), prefill-phase latency (ms), decoding-phase latency (ms), and GPU memory usage (%). ZipCache was compared against FP16 (full precision), H2O, GEAR, KIVI, and MiKV. Mixed precision quantization was applied with salient tokens quantized to 4-bit and regular tokens to 2-bit. The proportion of salient tokens ('Saliency Ratio') varied per task (e.g., 60-70% for GSM8k, 80% for Line Retrieval/HumanEval). The probe token strategy used a hybrid of 5% recent and 5% random tokens. Evaluations were performed on an Nvidia A100 GPU, utilizing the Language Model Evaluation Harness and LongEval for reproducibility.",
        "limitations": "The current version of ZipCache requires the saliency ratio (proportion of salient tokens) to be manually specified before evaluation, lacking automatic adjustment based on different task datasets. Additionally, like other generative models, ZipCache has a broader impact limitation in that it could potentially be used to generate malicious content.",
        "future_research_directions": "A key future research direction is to enable automatic adjustment of the saliency ratio based on task datasets, as it is currently manually specified. The paper also broadly suggests that ZipCache will facilitate more practical and scalable deployment of LLMs in various real-world applications, implying further work on integration and widespread use."
      }
    },
    {
      "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization",
      "abstract": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model.",
      "full_text": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization Coleman Hooper1 Sehoon Kim1 Hiva Mohammadzadeh1 Michael W. Mahoney1,2,3 Yakun Sophia Shao1 Kurt Keutzer1 Amir Gholami1,2 1University of California, Berkeley 2ICSI 3LBNL {chooper, sehoonkim, hiva, mahoneymw, ysshao, keutzer, amirgh}@berkeley.edu Abstract LLMs are seeing growing use for applications which require large context windows, and with these large context windows KV cache activations surface as the dominant contributor to memory consumption during inference. Quantization is a promising approach for compressing KV cache activations; however, existing solutions fail to represent activations accurately in sub-4-bit precision. Our work, KVQuant, facili- tates low precision KV cache quantization by incorporating several novel methods: (i) Per-Channel Key Quantization, where we adjust the dimension along which we quantize the Key activations to better match the distribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations before the rotary positional em- bedding to mitigate its impact on quantization; (iii) Non-Uniform KV Cache Quan- tization, where we derive per-layer sensitivity-weighted non-uniform datatypes that better represent the distributions; and (iv) Per-Vector Dense-and-Sparse Quanti- zation, where we isolate outliers separately for each vector to minimize skews in quantization ranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral models, we achieve < 0.1 perplexity degradation with 3-bit quantiza- tion on both Wikitext-2 and C4, outperforming existing approaches. Our method enables serving LLaMA-7B with a context length of up to 1 million on a single A100-80GB GPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for KVQuant, showing that we can achieve up to ∼1.7× speedups, compared to baseline fp16 matrix-vector multiplications, for the LLaMA-7B model. Code is available at https://github.com/SqueezeAILab/KVQuant. 1 Introduction Large language models (LLMs) have revolutionized many natural language processing (NLP) tasks. In order to improve the capabilities of LLMs, there is significant interest in increasing the context lengths of LLMs. Longer context lengths enable new applications, including long document summarization, retrieval for answering questions about long documents, extended multi-turn applications [6], and code analysis. To support this pull from applications, there have been significant recent advances in long-context length models in industry [2, 30], as well as in academia [6, 22]. Given the importance of LLM workloads, there is strong motivation to improve their inference efficiency. LLM inference with large context lengths can be incredibly resource-intensive; serving LLMs requires high-end GPUs, and the largest LLMs require costly multi-GPU inference setups. When analyzing the computational nature of generative inference with LLMs, it becomes quickly apparent that, for relatively small batch sizes, the computation is memory bound [ 17]. With the growing divergence between computational speeds and memory speeds, this problem is only going to get worse over time [12]. This makes reducing the memory bottleneck preeminently important. Further analysis shows that the memory bottleneck is strongly related to context size. For short 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2401.18079v6  [cs.LG]  28 May 202516% 84%98% 2% Long Sequence LengthsKV Cache is the bottleneckShort sequence length Weightsare the bottleneck BaselineKVQuant int3 +Grouping 10.87 Pre-RoPEKeyQuantizationNon-UniformQuantization5.755.94 1%Outliers6.23 Per-Channel KeyQuantization7.05 Perplexity on Wikitext2fp16 Baseline Figure 1: Left: Model size versus activation memory size for the LLaMA-7B model with sequence length 512 and 128K. For longer context lengths, the KV cache becomes the dominant memory bottleneck. Memory consumption of model weights and KV cache activations for different LLaMA models with different sequence lengths are provided in Table 7 in Appendix A. Right: Overview of the different components used in KVQuant that result in less than 0.1 perplexity degradation over the fp16 baseline when quantizing the KV cache for the LLaMA-7B model to 3-bit precision. As shown in Table 1, our 3-bit approach results in4.8× reduction in cached activation memory footprint. sequence lengths, the dominant contributor to memory consumption is the weight matrices, and therefore the optimal strategy is to minimize the model size in order to reduce memory consumption as well as bandwidth requirements [ 18, 17]. However, as shown in Figure 1, the main bottleneck for long sequence lengths is the memory requirements for caching Key and Value (KV) activations throughout inference. This challenge is further exacerbated when one considers batched inference. It is therefore crucial to develop methods for compressing the KV cache to enable efficient long- sequence length inference. Existing approaches lead to unacceptable accuracy degradation due to the outlier structures in KV cache activations as well as suboptimal bit allocation with existing uniform and non-uniform approaches. In this work, we perform an extensive analysis of KV cache activations in recent LLMs, revealing patterns which can be exploited to enable ultra-low precision quantization with minimal accuracy loss. In particular, we make the following contributions (summarized in Figure 1): • We find that the Keys exhibit outliers in specific channels before applying RoPE. However, the outlier channel magnitudes become less consistent after applying RoPE, posing a distinct challenge for low precision quantization. We address this by quantizing Keys per-channel before RoPE is applied (see Section 3.1 and Section 3.2). • We find that existing uniform and non-uniform quantization methods result in sub-optimal quanti- zation signpost placement. Instead, we propose a Non-Uniform Quantization (NUQ) method which considers sensitivity and not just magnitude when quantizing activations. We show that one can apply sensitivity-weighted non-uniform quantization offline on a calibration set to derive accurate datatypes for KV cache quantization (see Section 3.3). • Even with the above, we find that outlier values in cached KV activations can significantly de- grade quantization resolution. Unlike for weights, it is non-trivial to extract outlier values from activations, given the dynamic nature of activations. However, we find that we can efficiently and accurately identify and compress outlier values in order to store them compactly in a separate sparse representation. We also find that per-vector outlier detection outperforms per-matrix outlier detection with no additional memory overhead. By removing only 1% of outliers, we can attain under 0.1 perplexity degradation on both Wikitext-2 and C4 for 3-bit KV cache quantization with the LLaMA, Llama-2, Llama-3, and Mistral models, thereby facilitating accurate inference with 4.8× longer context length. • We implement custom CUDA kernels to perform activation quantization efficiently during inference, achieving up to ∼1.7× speedups for Key and Value matrix-vector multiplications for LLaMA-7B at 4-bit precision relative to the fp16 baseline (see Section 3.7 and 4.4). These results demonstrate how our methodology allows for accurate and efficient low-bit KV cache quantization. 22 Background 2.1 LLM Inference When inferring a decoder-only LLM, inference proceeds in two distinct phases. In the prefill phase, the model takes in an input prompt, which it processes in parallel. During the generation phase, the model then generates the output sequence autoregressively, meaning that each token generation is dependent on all previously generated tokens. As such, for small batch sizes, the generation phase of LLM inference is typically memory-bandwidth bound, as the only available parallelism is across different sequences in a given batch. Additionally, during generation, the model needs to store intermediate Key and Value activations at each layer in order to condition generations on previously generated output tokens. Otherwise, we would need to recompute all prior Keys and Values at each timestep, which would be prohibitively expensive. These stored activations are referred to as the Key-Value (KV) cache. Throughout this paper, we will capitalize Key and Value to distinguish when we are referring to the KV cache tensors. Assuming a model with n layers and h attention heads with dimension d that is stored using e bytes per element, the KV cache size for batch size b and sequence length l is 2 · n · h · d · e · b · l, meaning that it grows linearly with both batch size and sequence length. As shown in Table 7, the KV cache becomes the dominant contributor to memory consumption for longer sequence lengths and larger batch sizes. Note that since each sequence in batched inference depends on separate past context, there is no available batch-level parallelism when loading the cached Keys and Values for their respective computations in batched inference. KV cache loading is therefore always memory-bandwidth bound. This motivates pursuing methods to optimally compress the KV cache, even at the expense of a more complex dequantization process. 2.2 LLM Quantization There have been many prior works on LLM quantization. Several have focused on weight-only quantization for LLMs, due to the greater contribution to memory consumption and runtime for small sequence lengths and batch sizes [21, 9, 17]. Prior works have noted the presence of distinct outliers in both weights and activations [7, 9, 17]. One approach that has been developed to address this outlier issue in the context of weight quantization is dense-and-sparse quantization, where each weight matrix is decomposed into a sparse outlier matrix and a dense low-precision matrix [9, 17]. Prior works have also leveraged non-uniform quantization methods to improve accuracy for the same bit precision by allowing for flexible quantization signpost placement [17, 8]. These approaches have either used a fixed non-uniform datatype such as NormalFloat [8], or derived quantization signposts using a sensitivity-weighted k-means approach [17]. Appendix B provides a more detailed overview of related work for outlier-aware LLM quantization and non-uniform LLM quantization. There has also been work on quantizing both weights and activations (including KV cache) [41, 33]. However, there is still a significant perplexity degradation when quantizing KV cache activations to low precision; [34, 44] quantized KV cache activations to 4-bits, but required fine-grained grouping for 4-bit quantization, while still observing some perplexity degradation, and [34] observed that 3-bit KV cache quantization with fine-grained grouping leads to unacceptable accuracy loss. Other works quantized KV cache activations to 4-bits but required retraining to maintain performance [24]. One concurrent work also explores low precision KV cache quantization in order to enable larger batch size inference by reducing the KV cache size [26]. 2.3 KV Cache Compression There have also been several prior works on compressing the KV cache. Some of these methods aim to only store important tokens in the KV cache and to evict less important tokens, thereby maintaining low memory usage [25, 43, 11, 20]. Other methods aim to only retrieve a subset of tokens at each step to achieve memory bandwidth savings [32]. In this work, we explore KV cache quantization as an orthogonal direction for compressing the KV cache in order to enable long context inference. 3Channel 0 10002000 3000 4000 T oken 0 500 1000 1500 2000 Magnitude 0.0 2.5 5.0 7.5 10.0 12.5 15.0 Layer 10 Keys (pre-RoPE) Channel 0 10002000 3000 4000 T oken 0 500 1000 1500 2000 Magnitude 0.0 2.5 5.0 7.5 10.0 12.5 15.0 Layer 10 Keys (post-RoPE) Channel 0 10002000 3000 4000 T oken 0 500 1000 1500 2000 Magnitude 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Layer 10 Values Figure 2: Example distributions of the activation values for Keys pre-RoPE, Keys post-RoPE, and Values for LLaMA-7B on a sample with 2K sequence length from Wikitext-2. We observe several patterns: (i) Keys pre-RoPE exhibit clear outliers in specific channels across different tokens; (ii) after applying RoPE, the distribution becomes less structured and there are less consistent magnitudes for outlier channels (this is expected, as RoPE applies a rotation operation between pairs of channels); and (iii) Values exhibit no fixed outlier pattern with outlier values across channels and tokens. 3 Method 3.1 Per-Channel Key Quantization To inform our approach, we first performed a detailed analysis to understand the KV cache distribu- tions. Figure 2 shows sample distributions for the KV cache activations. We observe that the Key matrices tend to have distinct outlier channels, which have larger average magnitudes than other channels; this corroborates previous observations about outlier channels in LLM activations [7, 41]. The Value matrices exhibit both outlier channels as well as outlier tokens (although these outliers are less extreme than the outlier Key channels). Existing KV cache quantization approaches perform per-token quantization (meaning that the scaling factor and zero-point are shared by elements in the same token) [ 34, 44]. However, due to the differing average magnitudes between channels, the values within a channel are easier to quantize when grouped together than the values across different channels. As such, to better match the distributions, we investigate per-channel KV cache quantization, meaning that the scaling factor and zero-point are shared by elements in the same channel. By sharing the scaling factor and zero-point along the channel dimension, this will naturally group together values with similar magnitudes, thereby mitigating the impacts of outlier channels on other channels when quantizing to low precision. As outlined in Appendix G, we find that per-channel quantization provides significant accuracy benefits for Keys but not for Values. By leveraging per-channel quantization for Keys and per- token quantization for Values, we observe a 3.82 perplexity improvement on Wikitext-2 for 3-bit LLaMA-7B quantization. Note that this can potentially add runtime overhead since the quantization dimension is now misaligned with the reduction dimension for the Keys when performing matrix- vector multiplications. However, we find that we are able to efficiently dequantize Keys and perform the Query-Key matrix-vector multiplication without adding runtime overhead, as shown in Section 4.4. Additionally, as outlined in Section 3.6, per-channel quantization can also be challenging due to the need to recompute scaling factors as tokens are added to the Key cache. We show that we can calibrate offline for scaling factors, thereby avoiding expensive online recomputation. Per-channel Key quantization was also explored in another concurrent work [26], which leveraged similar intuition about grouping together large magnitude values in the same channel to minimize quantization error. Their methodology requires fine-grained grouping for per-channel quantization while maintaining a residual subset of the KV cache in fp16. In our work, we demonstrate that by leveraging offline calibration, we can accurately perform per-channel quantization without grouping. 3.2 Pre-RoPE Key Quantization One issue when quantizing Keys is handling the rotary positional embedding (RoPE), which is applied to Keys and Queries in most public LLMs, including LLaMA and Llama-2 [35]. Given Query and Key vectors Qm = Wq ∗ xm and Kn = Wk ∗ xn at positions m and n in the sequence, RoPE is applied as position-dependent rotations to each of these vectors to obtain ˜Qm = Rd θ,m · Qm and 4˜Kn = Rd θ,n · Kn. This embeds the relative position between a Query and Key vector as an amount of an angle that is a multiple of its position index. When caching Key vectors, we therefore need to either cache ˜Kn, or else we need to cache Kn and apply Rd θ,n on-the-fly during inference. The challenge with caching Key vectors after applying this rotation is that it leads to mixing pairs of channels by different amounts for different positions in the sequence, as shown in Appendix C (since it jointly rotates pairs of channels by different angles depending on the position in the sequence). The post-RoPE activation distribution is also shown in Figure 2, demonstrating how the rotation between pairs of channels leads to less consistent channel magnitudes. This makes it harder to quantize Key activation channels which would typically have consistent large-magnitude values. This motivated our investigation into whether we could perform pre-RoPE Key quantization (meaning that we quantize Kn) and then efficiently apply the positional embeddings on-the-fly after dequantization. The benefits of pre-RoPE Key quantization are highlighted in Appendix H, yielding 0.82 perplexity improvement on Wikitext-2 for 3-bit LLaMA-7B quantization. To be able to quantize Keys pre-RoPE, we develop a fused kernel to efficiently apply RoPE post-dequantization (the details of this approach will be discussed in Section 3.7). 3.3 nuqX: An X-Bit Per-Layer Sensitivity-Weighted Non-Uniform Datatype Uniform quantization is suboptimal for KV cache quantization since the Query and Key activations are non-uniform. Additionally, KV cache loading is memory bandwidth bound, regardless of batch size or sequence length, meaning that the dequantization overhead introduced by non-uniform quantization methods is not problematic (since the added computation does not introduce any additional latency). It is therefore desirable to leverage non-uniform quantization methods for KV cache quantization. In [17], the authors computed non-uniform quantization signposts using a sensitivity-weighted k-means approach. However, this cannot be applied directly to KV cache quantization as the Values are quantized dynamically at runtime, which means that we would need to apply K-means online during inference, and it is also difficult to estimate sensitivity for activations online. We therefore facilitate efficient online non-uniform KV cache quantization by deriving a per-tensor non-uniform datatype offline on calibration data, which is then rescaled per-channel or per-token to accurately represent the key and value distributions. We compute sensitivity-weighted quantization signposts offline on a calibration set prior to inference, while maintaining compatibility with per-vector quantization by separately normalizing each channel to the range [−1, 1] prior to deriving the shared datatype. Using the diagonal Fisher information matrix (derived in Appendix D), along with the quantization error for activation A, we formulate the error minimization objective as follows, where A is flattened to one dimension and where N is the number of elements from all of the samples in our calibration set: Q(A)∗ ≃ arg min Q NX i=1 Fii \u0000 Ai − Q(Ai) \u00012 . (1) We modify the objective in Equation 1 as described in Appendix E in order to apply it using the normalized activation values. We then minimize it offline on a calibration set using a k-means solver in order to obtain the quantization signposts for the non-uniform datatype for each Key or Value layer. Appendix I compares our non-uniform quantization approach with existing uniform and non-uniform quantization baselines [8], demonstrating how our non-uniform approach provides 0.29 perplexity improvement on Wikitext-2 for LLaMA-7B relative to 3-bit uniform methods. Table 16 in Appendix L shows how computing the required Fisher information for the LLaMA-65B model takes only a few minutes, and how using the k-means solver takes only a few minutes per layer (with the computation for each layer being parallelizable). 3.4 Per-Vector Dense-and-Sparse Quantization As shown in Figure 4 in Appendix F, for both Keys and Values, the majority of elements are contained within a small percentage of the dynamic range. This means that by leveraging dense-and-sparse quantization, as demonstrated in [17], in order to isolate a small percentage of numerical outliers, we can restrict the range that we need to represent, thereby allowing us to represent the remaining elements with greater precision. However, when looking at the Key and Value distributions in Figure 2, different channels and tokens have different average magnitudes. Therefore, an element which counts as an outlier in one channel may not be an outlier in another channel (since that channel 5may have a greater average magnitude), making naive application of dense-and-sparse quantization suboptimal. It is therefore crucial to directly target the outlier values that skew the dynamic range at the granularity that we are quantizing in order to address the values that are exaggerating the range along that particular dimension. In this work, we leverage per-vector dense-and-sparse quantization, where we use a different outlier threshold per-vector (either a separate threshold per-channel for per-channel quantization, or a separate threshold per-token for per-token quantization), rather than a single outlier threshold for each layer. Note that computing outlier thresholds for per-vector dense-and-sparse quantization poses potential accuracy and efficiency challenges. However, in Section 3.6, we show that we are able to accurately calibrate for per-channel outlier thresholds offline and efficiently compute per-token outlier thresholds online. After determining the upper and lower outlier thresholds, the remaining numbers in the vector are normalized to the range [−1, 1], and we then minimize Equation 1 (ignoring outliers) in order to obtain the quantization signposts for the non-uniform datatype for the remaining numbers. Appendix J will demonstrate the benefits of removing a small percentage of numerical outliers and keeping them in full precision, as well as the advantages of per-vector dense-and-sparse quantization over using a single global outlier threshold for each layer. As shown in Figure 1, by removing 1% of numerical outliers using per-vector outlier thresholds, we achieve an additional 0.19 perplexity improvement on Wikitext-2 for 3-bit LLaMA-7B quantization, which is within 0.07 perplexity of the fp16 baseline. 3.5 Attention Sink-Aware Quantization Prior work has demonstrated that after the first few layers in LLMs, the model tends to allocate a large attention score to the first token [42]. This occurs even when the initial token is not semantically important. This phenomenon happens because the model tends to use the inital token as a “sink”. In our work, we demonstrate that due to the Attention Sink phenomenon, the model is disproportionately sensitive to quantization error in the first token. By keeping only the first token in fp16, we can attain perplexity benefits, particularly for 2-bit quantization. A similar observation has also been made in another concurrent work [ 23]. Note that when retaining the first token in fp16, we account for this during the calibration process as well, meaning that we ignore the first token when deriving the nuqX datatype and when calibrating the scaling factors and zero points offline for the Keys. As demonstrated in Appendix K, this approach persistently yields performance benefits, particularly with lower bit widths and without dense-and-sparse quantization. 3.6 Offline Calibration versus Online Computation A crucial challenge for activation quantization is that we either need to compute statistics on-the-fly (which is potentially expensive) or else we need to use offline calibration data (which potentially has negative accuracy implications). The challenges with computing scaling factors (and zero-points) online versus offline for both Keys and Values are shown in Figure 5 in Appendix L. In per-channel quantization, it is challenging to update scaling factors online since the scaling factors corresponding to each incoming channel would potentially need to be updated whenever a new token is added to the KV cache. It is therefore desirable to be able to compute statistics offline (i.e., using calibration data before running inference). While this can have negative effects on model accuracy, in Appendix L we show that we can effectively calibrate offline for per-channel quantization, obviating the need for online updates of scaling factors for per-channel quantization. For per-token quantization, it is challenging to calibrate for scaling factors offline due to the presence of outlier Value tokens. It is therefore desirable to be able to compute scaling factors and outlier thresholds online for each incoming token. As shown in Appendix L, we can efficiently compute outlier thresholds online per-token by offloading to the CPU. By leveraging custom quantization function implementations for compressing activations, we are able to perform online per-token Value quantization without compromising on performance. 3.7 Kernel Implementation In order to efficiently perform activation quantization on-the-fly, we leverage dedicated kernel implementations with our 4-bit quantization method for compressing vectors to reduced precision and extracting the sparse outliers, performing matrix-vector multiplications using the compressed 6Table 1: Evaluation of our method for different models using the perplexity (PPL) on Wikitext-2. KVQuant results are using pre-RoPE per-channel quantization for Keys. KV cache sizes are estimated assuming a sequence length of 128K (ignoring context length limits for the models). Note that ATOM and FlexGen use 4-bit quantization with group sizes of 128 and 64 with uniform quantization, respectively, and we extend their methods to 3-bit and 2-bit quantization. We leverage Attention Sink-Aware quantization for all bit widths. We used post-RoPE quantization for all baseline methods since it achieves higher accuracy when quantizing Keys per-token as shown in Appendix P. Table 18 in Appendix O demonstrates a full evaluation on all LLaMA, Llama-2, Llama-3, and Mistral models. Method LLaMA-7B LLaMA-13B LLaMA-30B LLaMA-65B PPL KV Cache (GB) PPL KV Cache (GB) PPL KV Cache (GB) PPL KV Cache (GB) baseline 5.68 64.0 5.09 100.0 4.10 195.0 3.53 320.0 int4 5.98 16.0 5.32 25.0 4.34 48.8 3.73 80.1 nf4 5.87 16.0 5.23 25.0 4.25 48.8 3.63 80.1 ATOM-4bit 5.77 16.6 5.16 26.0 4.16 50.7 3.57 83.1 FlexGen-4bit 5.73 17.3 5.14 27.0 4.14 52.6 3.56 86.3 KVQuant-4bit 5.72 16.0 5.13 25.0 4.13 48.8 3.55 80.0 KVQuant-4bit-1% 5.69 17.3 5.10 27.0 4.11 52.7 3.54 86.5 int3 10.87 12.0 8.69 18.8 6.82 36.6 6.37 60.1 nf3 7.33 12.0 6.21 18.8 5.46 36.6 4.44 60.1 ATOM-3bit 6.17 12.6 5.47 19.7 4.44 38.4 3.78 63.0 FlexGen-3bit 5.93 13.2 5.29 20.6 4.26 40.2 3.66 65.9 KVQuant-3bit 5.87 12.0 5.25 18.8 4.25 36.6 3.63 60.0 KVQuant-3bit-1% 5.75 13.3 5.14 20.8 4.15 40.5 3.57 66.5 int2 11779 8.0 69965 12.5 1470 24.4 7272 40.1 nf2 3210 8.0 5786 12.5 2044 24.4 5367 40.1 ATOM-2bit 37.37 8.6 41.77 13.4 16.49 26.1 13.63 42.8 FlexGen-2bit 11.09 9.1 9.84 14.3 6.60 27.8 5.54 45.6 KVQuant-2bit 7.23 8.0 5.82 12.5 4.87 24.4 4.03 40.0 KVQuant-2bit-1% 6.01 9.3 5.36 14.5 4.35 28.3 3.70 46.5 vectors, and performing sparse matrix-dense vector multiplications using the sparse outliers. We store the quantized Key and Value matrices as 4-bit elements which are used as indices into lookup tables to recover the corresponding fp16 values. We store the sparse outlier matrices in either Compressed- Sparse Row (CSR) or Compressed-Sparse Column (CSC) format (depending on which aligns better with appending new Key and Value tokens). The kernels for the Key matrix-vector operations apply RoPE on-the-fly in order to support pre-RoPE quantization. More kernel implementation details are provided in Appendix R. 4 Results 4.1 Main Evaluation We used the LLaMA-7B/13B/30B/65B, Llama-2-7B/13B/70B, Llama-3-8B/70B, and Mistral-7B models to evaluate our methodology by measuring perplexity on both Wikitext-2 and C4 [36, 37, 1, 16, 27, 31]. Perplexity has been measured using teacher forcing with the output logits of all input tokens. We compared our method against (i) uniform quantization without grouping (intX), (ii) nonuniform quantization using NormalFloat [8] without grouping (nfX), as well as (iii) Atom [44] and FlexGen [34]. Note that Atom and FlexGen use uniform quantization with group sizes of 64 and 128, respectively. All the KVQuant models throughout this experiment section are calibrated using 16 calibration samples of sequence length 2K from the Wikitext-2 training set. See Appendix M for details on our experimental setup, including our methodology for computing KV cache size estimates. Table 1 shows the results for LLaMA models for the Wikitext-2 dataset. We compared our method with per-token quantization with and without grouping. The baseline configurations used by Atom and FlexGen are included for reference [44, 34]. We find that our method consistently outperforms baseline approaches by an especially large margin with 3-bit and 2-bit quantization. Once we incorporate outliers, we further push the performance of low-precision quantization, achieving 4-bit quantization with less than 0.02 perplexity degradation, 3-bit quantization with under 0.1 perplexity degradation, and 2-bit quantization with under 0.5 perplexity degradation on Wikitext-2, relative to the fp16 baseline, across all models (while attaining 3.7 ×, 4.8×, and 6.9 × memory savings, respectively). 73.7x4.8x6.8x 3.7x4.8x6.8x Figure 3: Perplexity results for the LLaMA-2-7B-32K model [5] as well as the Llama-2-70B-32K LongLoRA model [6] on the Wikitext-2 dataset, evaluated using different sequence lengths. Table 2: Passkey retrieval results across different context lengths for the LLaMA-2-7B-32K model (uptrained for long sequence lengths using positional interpolation [ 5]) as well as the Llama-2- 70B-32K LongLoRA model [6]. The values reported are the success rate for retrieving the passkey, computed over 50 samples. We also include comparisons with KIVI for reference, using the 2-bit configuration with group size of 32 and 128-element fp16 residual [ 26]. Average bit widths are estimated for each approach assuming 32K context length. Note that the open-source code for running KIVI with LLaMA does not support grouped-query attention, so we did not include comparisons with KIVI for Llama-2-70B-32K. Model Method 2K 4K 8K 16K 32K Avg. Bit Width LLaMA-2-7B-32K fp16 1 1 1 1 1 16 KIVI-2-gs32-r128 0.76 0.72 0.72 0.68 0.7 3.05 nuq4-1% 1 1 1 1 1 4.33 nuq3-1% 0.98 1 1 1 1 3.33 nuq2-1% 1 1 0.98 1 1 2.33 Llama-2-70B-32K fp16 1 1 1 1 1 16 nuq4-1% 1 1 1 1 1 4.35 nuq3-1% 1 1 1 1 1 3.35 nuq2-1% 0.98 0.98 0.96 1 0.74 2.35 4.2 Long Context Length Evaluation Perplexity Evaluation. We evaluated long context length performance using the LLaMA-2-7B- 32K model (uptrained for long sequence lengths using positional interpolation [ 5]) as well as the Llama-2-70B-32K LongLoRA model [6]. For evaluating performance on longer context lengths, we first evaluated perplexity on Wikitext-2 using larger amounts of input context, as shown in Figure 3 [6, 13]. The results demonstrate how our method maintains accuracy even for longer amounts of input context, thereby enabling efficient and accurate long sequence length inference. Passkey Retrieval Evaluation. We also evaluated the performance of our quantization method on passkey retrieval to assess the model’s ability to use its context. Passkey retrieval involves evaluating the model’s capacity to locate specific information in long texts [19], and this can be used to effectively measure the maximum distance over which a token can attend during the inference stage. We used the passkey evaluation framework from [45] (which is based on the methodology from [28]) to evaluate retrieval performance. The passkey retrieval results are provided in Table 2, demonstrating how our method is able to maintain the retrieval performance for long context length models. We also include comparisons with the passkey retrieval when using KIVI for the LLaMA-2-7B-32K model [26], demonstrating that our approach can attain higher retrieval rate for the same compression level. Our improved performance on retrieval tasks can be attributed to our improved representation of all tokens equally. This approach differs from KIVI, which preserves a local window of residual tokens in fp16. Therefore, while KIVI is effective at representing the tail part of the context, it may provide less benefit for tasks requiring the utilization of the full context window. LongBench Evaluation. Table 3 shows evaluation on LongBench [3] for the LLaMA-2-7B-32K model. LongBench contains a suite of long-context length evaluation benchmarks including QA tasks, summarization, and few-shot learning [ 3]. The max input context length is set at 31500, and results using KIVI are also included for reference [26]. Our results demonstrate that our 3-bit 8Table 3: LongBench evaluation for the LLaMA-2-7B-32K model using KVQuant-3bit-1%. Com- parisons with KIVI are included for reference, using the configuration with group size of 32 and 128-element fp16 residual [26]. Average bit widths are estimated for each approach assuming 12.2K context length, which was the average number of tokens across all tasks. Config Avg. bitNtrvQAQasperMF-enHotpot2WikiMusiqueGovRepQMSumMNewsTRECTriviQASamSumRBenchLCC PsgRetrPsgCntAvg. fp16 Baseline 16 17.96 10.51 33.43 12.55 12.53 6.19 29.65 16.99 22.15 71 87.79 43.97 59.99 62.14 23 1.50 31.96 KIVI-2-gs32-r128 3.17 19.25 10.66 24.78 12.48 11.19 6.38 27.05 16.36 23.37 71 80.80 43.93 57.74 60.61 13.58 1.50 30.04 KVQuant-3bit-1% 3.33 18.87 13.67 30.93 12.07 12.55 6.25 27.10 16.53 16.54 71 87.55 43.95 59.50 61.52 19.5 1.75 31.21 Table 4: RULER evaluation results for the LLaMA-2-7B-32K model with KVQuant quantization methods. We report accuracy across RULER tasks, comparing our KVQuant configurations to baseline and KIVI approaches. A maximum context length of 32K is used for evaluation. Our results show that our method retains baseline accuracy even with aggressive quantization and pruning. Config Avg. bit Niah1 Niah2 Niah3 MKey1MKey2MKey3MValueMQueryVT CWE FWE QA1 QA2 Avg. fp16 Baseline 16 100 99.8 98.6 94 68.2 11 55.95 64.5 37.88 9.64 30.4 31.6 31.6 56.40 KIVI-2-gs32-r128 3.05 76 85.6 59.6 72.6 11.4 0 34.7 46.45 39.6 8.26 30.53 24.8 27.6 39.78 KVQuant-3bit-1% 3.33 99.8 98.8 95.2 92.8 61.6 6.4 47.5 54.45 41.04 8.52 29.33 31.0 31.0 53.65 KVQuant-2bit-1% 2.33 95.4 86.8 49.8 73.6 23.4 0 16.65 22.95 22.52 5.14 24.0 26.4 28.4 36.54 model can attain minimal degradation relative to the fp16 baseline, outperforming KIVI for a similar compression level. RULER Evaluation. Finally in Table 4, we provide evaluation of KVQuant and KIVI on the RULER benchmark suite [15] using LLaMA-2-7B-32K. As can be seen in the table, 3-bit KVQuant achieves 14% better score against KIVI with a similar average bit-width. Furthermore, our 2-bit KVQuant achieves similar accuracy to KIVI with 1.5× smaller bit-width. 4.3 Joint Weight and KV Cache Quantization Table 5 provides results for our KV cache quantization method when the weights are also quantized using the methodology in SqueezeLLM [ 17]. We observe minimal perplexity degradation when leveraging our KV cache quantization approach, even when weights are also quantized to reduced precision. In particular, we observe small 0.02 and 0.1 perplexity degradation of 4-bit and 3-bit weight-only quantization, respectively, when quantizing the KV cache using nuq4-1% for the LLaMA- 7B and LLaMA-13B models. These results demonstrate how our method is compatible with existing weight-only quantization methods. 4.4 Performance Analysis and Memory Savings Table 6 shows kernel benchmarking results using a batch size of 1 for the 4-bit dense-and-sparse compression and matrix-vector kernel implementations. We show results across different sequence lengths to assess the performance of the kernels at different points during generation. We report latency benchmarked on an A6000 GPU. The results show that for the Key and Value multiplications, we can achieve 1.2-1.6× and 1.3-1.7× latency savings, respectively, relative to the baseline. We have integrated these kernels into an end-to-end generation pipeline that is able to compress activations dynamically during inference, thereby achieving significant memory savings and allowing for either larger batch sizes or longer sequence lengths. Appendix A highlights the benefits of KVQuant in supporting longer context lengths through reducing KV cache memory footprint. As shown in Table 8 in Appendix A, our nuq2 method provides 8× KV cache compression and enables serving the quantized LLaMA-7B model with a context length of 1M tokens on a single A100 GPU, as well as enabling serving the LLaMA-7B model with 10M context length on an 8-GPU system. Our results show little degradation compared to baseline fp16 inference 9Table 5: KV cache quantization results when KVQuant is applied in conjunction with the weight quantization methodology in SqueezeLLM [17]. w4-s45 and w3-s45 for weights refer to the 4-bit and 3-bit dense-and-sparse weight quantization approaches in [ 17], respectively. See Appendix M for experimental details. Weights KV Cache LLaMA-7B LLaMA-13B Avg. Bits (KV Cache) fp16 fp16 5.68 5.09 16 w4-s45 fp16 5.77 5.17 16 nuq4-1% 5.79 5.18 4.32-4.33 w3-s45 fp16 6.13 5.45 16 nuq3-1% 6.23 5.52 3.32-3.33 Table 6: Average latency (in microseconds) for the Key and Value nuq4-1% kernels, benchmarked on an A6000 GPU for the LLaMA-2-7B-32K model across different sequence lengths ( l). fp16 matrix-vector multiplication latencies are included for reference, and the fp16 Key multiplication time also includes the time to apply RoPE to the newly appended Key vector. Section 3.7 and Appendix R provide additional details for our kernel implementation, Appendix R describes our benchmarking methodology, and Table 22 provides a detailed breakdown of kernel runtime on an A6000 GPU. Activation Operation l=2K l=4K l=16K Key fp16 Matvec 33.3 59.1 219.4 Key nuq4-1% 25.6 39.9 126.3 Value fp16 Matvec 26.0 50.2 203.7 Value nuq4-1% 22.1 37.9 124.5 while providing significant compression, demonstrating the benefits of our approach for enabling accurate and efficient long sequence length inference. 5 Conclusion As context lengths in LLMs increase, the KV cache activations surface as the dominant contributor to memory consumption. Quantization is a promising approach to reduce the size of KV cache activations, but prior solutions failed to represent activations accurately in ultra-low precisions, such as sub-4-bit. In contrast, we achieve accurate ultra-low precision KV cache quantization. By quantizing Keys per-channel before applying RoPE, we are able to better match the outlier distribution and mitigate the impacts of RoPE on quantization (due to it mixing pairs of channels which may have different average magnitudes). We use non-uniform quantization to better allocate the small number of quantization signposts at low precision. We observe significant accuracy improvements when employing dense-and-sparse quantization, particularly when detecting outliers at the same granularity as we compute quantization scaling factors. Crucially, we demonstrate that we can perform accurate calibration offline for Keys, as well as efficient online scaling factor and outlier threshold computation for Values. By leveraging these methods, we are able to enable accurate low-precision activation quantization, achieving 4.8x compression (nuq3-1% outliers) with only 0.1 perplexity degradation across different LLaMA, Llama-2, Llama-3, and Mistral models. Our methodology therefore supports inferring the LLaMA-7B model with a context length of 10M on an 8-GPU serving system. Through our efficient kernel implementation, we are able to show improved latency relative to the fp16 baseline, demonstrating how our method allows for improved latency in addition to the memory savings. Acknowledgements The authors would like to acknowledge Nicholas Lee for helpful discussions and feedback. We acknowledge gracious support from Intel, Furiosa, Apple, Samsung SAIT, POSCO HOLDINGS, and NVIDIA. We also appreciate the support from Microsoft through their Accelerating Foundation Model Research, including great support from Sean Kuno. Furthermore, we appreciate support from Google Cloud, the Google TRC team, and specifically Jonathan Caton, and Prof. David Patterson. Prof. Keutzer’s lab is sponsored by the Intel corporation, Intel One-API, Intel VLAB team, the Intel 10One-API center of excellence, as well as funding through BDD and BAIR. We appreciate great feedback and support from Ellick Chan, Saurabh Tangri, Andres Rodriguez, and Kittur Ganesh. Sehoon Kim would like to acknowledge the support from the Korea Foundation for Advanced Studies (KFAS). Michael W. Mahoney would also like to acknowledge a J. P. Morgan Chase Faculty Research Award as well as the DOE, NSF, and ONR. Our conclusions do not necessarily reflect the position or the policy of our sponsors, and no official endorsement should be inferred. References [1] AI@Meta. Llama 3 model card. 2024. [2] Anthropic. Introducing claude 2.1, Nov 2023. [3] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. [4] Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding and overcoming the challenges of efficient transformer quantization. arXiv preprint arXiv:2109.12948, 2021. [5] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023. [6] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. arXiv preprint arXiv:2309.12307, 2023. [7] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. [8] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023. [9] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: A sparse-quantized representation for near-lossless llm weight compression. arXiv preprint arXiv:2306.03078, 2023. [10] Goran Flegar and Enrique S Quintana-Ortí. Balanced csr sparse matrix-vector product on graphics processors. In Euro-Par 2017: Parallel Processing: 23rd International Conference on Parallel and Distributed Computing, Santiago de Compostela, Spain, August 28–September 1, 2017, Proceedings 23, pages 697–709. Springer, 2017. [11] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [12] Amir Gholami, Zhewei Yao, Sehoon Kim, Michael Mahoney, and Kurt Keutzer. Ai and memory wall. RiseLab Medium Post, 2021. [13] Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Simple on-the-fly length generalization for large language models. arXiv preprint arXiv:2308.16137, 2023. [14] Jung Hwan Heo, Jeonghoon Kim, Beomseok Kwon, Byeongwook Kim, Se Jung Kwon, and Dongsoo Lee. Rethinking channel dimensions to isolate outliers for low-bit weight quantization of large language models. arXiv preprint arXiv:2309.15531, 2023. [15] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. Ruler: What’s the real context size of your long-context language models? arXiv preprint arXiv:2404.06654, 2024. [16] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [17] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023. 11[18] Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W Mahoney, et al. Full stack optimization of transformer inference: a survey. arXiv preprint arXiv:2302.14017, 2023. [19] Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can context length of open-source llms truly promise? In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023. [20] Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. Snapkv: Llm knows what you are looking for before generation, 2024. [21] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023. [22] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with blockwise ringattention, 2024. [23] Ruikang Liu, Haoli Bai, Haokun Lin, Yuening Li, Han Gao, Zhengzhuo Xu, Lu Hou, Jun Yao, and Chun Yuan. Intactkv: Improving large language model quantization by keeping pivot tokens intact, 2024. [24] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free quantiza- tion aware training for large language models. arXiv preprint arXiv:2305.17888, 2023. [25] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. arXiv preprint arXiv:2305.17118, 2023. [26] Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: Plug-and-play 2bit kv cache quantization with streaming asymmetric quantization. 2023. [27] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016. [28] Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context length for transformers. arXiv preprint arXiv:2305.16300, 2023. [29] Roman Novak, Yasaman Bahri, Daniel A Abolafia, Jeffrey Pennington, and Jascha Sohl- Dickstein. Sensitivity and generalization in neural networks: an empirical study. arXiv preprint arXiv:1802.08760, 2018. [30] OpenAI. New models and developer products announced at devday 2023, Nov 2023. [31] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv e-prints, 2019. [32] Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo Luschi, and Douglas Orr. Sparq attention: Bandwidth-efficient llm inference. arXiv preprint arXiv:2312.04985, 2023. [33] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally calibrated quantiza- tion for large language models. arXiv preprint arXiv:2308.13137, 2023. [34] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. Flexgen: High-throughput generative inference of large language models with a single gpu. In International Conference on Machine Learning, pages 31094–31116. PMLR, 2023. [35] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [36] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 12[37] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [38] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, and Xianglong Liu. Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling. arXiv preprint arXiv:2304.09145, 2023. [39] Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier suppression: Pushing the limit of low-bit transformer language models. Advances in Neural Information Processing Systems, 35:17402–17414, 2022. [40] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the- art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38–45, Online, October 2020. Association for Computational Linguistics. [41] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pages 38087–38099. PMLR, 2023. [42] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks, 2023. [43] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H _2 o: Heavy-hitter oracle for efficient generative inference of large language models. arXiv preprint arXiv:2306.14048, 2023. [44] Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom: Low-bit quantization for efficient and accurate llm serving. arXiv preprint arXiv:2310.19102, 2023. [45] Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. Pose: Efficient context window extension of llms via positional skip-wise training. arXiv preprint arXiv:2309.10400, 2023. 13A Memory Bottlenecks for Long Context Length Inference Table 7 shows the model size and KV cache memory requirements for different LLaMA models with different sequence lengths. For short sequence lengths, the model weights are the primary memory bottleneck. However, for longer sequence lengths and larger batch sizes, the KV cache memory is the main bottleneck. This is particularly pronounced when the weights are already quantized to low precision. In our work, we demonstrate that we can help address the KV cache memory bottleneck through low-precision KV cache quantization. By compressing the KV cache to 2-bit precision, we can enable 1M context length inference with the LLaMA-7B model on a single A100-80GB GPU, and we can also enable 10M context length inference with the LLaMA-7B model on an 8-GPU system. Table 8 shows the KV cache memory requirements for 128K, 1M, and 10M sequence lengths, with the KV cache stored in fp16 as well as 4-bit, 3-bit, and 2-bit precision with KVQuant. As one can see, our method provides 3.7× KV cache compression (nuq4-1%) and enables serving the quantized LLaMA-65B model with a context length of 32K tokens on a single A100-80GB GPU (requiring 30.3GB for the model weights compressed to 4-bit, and 46.5GB for the KV cache when compressed with nuq2-1%), and our nuq2 method enables serving the LLaMA-7B model with a context length of 1M tokens on a single A100 GPU (requiring 64GB for the KV cache). Additionally, when considering an 8-GPU serving system, we enable serving the LLaMA-7B model with 10M context length (with nuq2), or the LLaMA-65B model with 1M context length (with nuq3). Our results show little degradation compared to baseline fp16 inference while providing significant compression, demonstrating the benefits of our approach for enabling accurate and efficient long sequence length inference. Table 7: Model size and activation memory size estimates for different sequence lengths and batch sizes (BS) for different LLaMA models. For long sequence lengths and larger batch sizes, activation memory is the main bottleneck (particularly when weights are already quantized to low precision). By compressing the KV cache to 2-bit precision, we can enable1M context length inference with the LLaMA-7B model on a single A100-80GB GPU, and we can also enable 10M context length inference with the LLaMA-7B model on an 8-GPU system. BS Model Model Size (GB) KV Cache Size w/ Diff. Seq Len (GB) 16 → 2-bit 32K 128K 1M 10M (16 → 2-bit) 1 7B 12.6 → 1.6 16 64 512 4883 → 610 13B 24.1 → 3.0 25 100 800 7629 → 954 30B 60.3 → 7.5 49 195 1560 14877 → 1860 65B 121.1 → 15.1 80 320 2560 24414 → 3052 4 7B 12.6 → 1.6 64 256 2048 19531 → 2441 13B 24.1 → 3.0 100 400 3200 30518 → 3815 30B 60.3 → 7.5 195 780 6240 59509 → 7439 65B 121.1 → 15.1 320 1280 10240 97656 → 12207 B Additional Related Works B.1 Outlier-Aware LLM Quantization LLMs have been known to have distinct outliers both in weights and activations [ 7, 9, 17]. SqueezeLLM and SpQR both decompose the weight matrix into a sparse matrix containing a small portion of outliers and a dense matrix that can be accurately quantized to low precision (referred to as dense-and-sparse or sparse-quantized representation) [9, 17]. LLM.int8() [7] handled particular outlier channels separately in higher precision, and SmoothQuant [41] migrates quantization difficulty due to outlier channels to weights in order to support joint weight-activation quantization. Other works reconsidered the dimension along which we quantize in order to reduce quantization error (or else added per-channel compensation to improve quantization performance) [4, 14, 39, 38]. In this work, we demonstrate that per-channel pre-RoPE Key quantization provides significant accuracy benefits given the outlier structure in Keys, and that dense-and-sparse quantization can be efficiently applied for KV cache quantization. 14Table 8: Activation memory size estimates (GB) for 128K, 1M, and 10M sequence length ( l) for different LLaMA models. By compressing the KV cache to 2-bit precision, we can enable 1M context length inference with the LLaMA-7B model on a single A100-80GB GPU, and we can also enable 10M context length inference with the LLaMA-7B model on an 8-GPU system. Model Method l=128K l=1M l=10M LLaMA-7B fp16 64.0 512.0 4882.8 nuq4 16.0 128.1 1221.9 nuq4-1% 17.3 138.4 1319.6 nuq3 12.0 96.1 916.7 nuq3-1% 13.3 106.4 1014.4 nuq2 8.0 64.1 611.5 nuq2-1% 9.3 74.4 709.2 LLaMA-65B fp16 320.0 2560.0 24414 nuq4 80.0 640.3 6106.5 nuq4-1% 86.5 691.5 6595.0 nuq3 60.0 480.3 4580.6 nuq3-1% 66.5 531.5 5069.1 nuq2 40.0 320.3 3054.7 nuq2-1% 46.5 371.5 3543.3 B.2 Non-uniform LLM Quantization Non-uniform quantization has also been applied in the context of LLMs. Non-uniform quantization allows for more flexible quantization signpost placement relative to uniform quantization methods, enabling improved accuracy for the same bit precision [17, 8]. Building on the observation that model parameters tend to be approximately normally-distributed, prior work has proposed the NormalFloat datatype [8]. SqueezeLLM [ 17] derived per-channel non-uniform quantization signposts using a sensitivity-weighted k-means approach. In this work, we show that we can derive accurate per-layer non-uniform datatypes using a sensitivity-weighted k-means approach with KV cache activations. C RoPE Equation The rotation matrix for RoPE is provided in Equation 2, where c and s are cosine and sine functions, θi = 10000−2(i−1)/d, d is the attention head dimension, and n is the current position in the sequence:   c(nθ1) −s(nθ1) ··· 0 0 s(nθ1) c( nθ1) ··· 0 0 ... ... ... ... ... 0 0 ··· c(nθd/2) −s(nθd/2) 0 0 ··· s(nθd/2) c( nθd/2)   (2) The Query vectors computed at each iteration will have RoPE applied (to obtain ˜Qm = Rd θ,m ∗ Qm). When caching Key vectors, we therefore need to either cache ˜Kn = Rd θ,n ∗ Kn, or else we need to cache Kn and apply Rd θ,n on-the-fly during inference. In order to apply Rd θ,n efficiently on-the-fly, we leverage the element-wise formulation of RoPE rather than the matrix-multiplication formulation from Equation 2. The element-wise formulation for Rd θ,nx is as follows, where ⊙ is the element- wise multiplication operator (note that the formulation that we use matches the implementation in the Transformers library for LLaMA [ 40], and it is a different but equivalent formulation to the element-wise expression in [35]): 15  x1 x2 ... xd 2 xd 2 +1 ... xd−1 xd   ⊙   c(θ1n) c(θ2n) ... c(θd 2 n) c(θ1n) ... c(θd 2 −1n) c(θd 2 n)   +   −xd 2 +1 −xd 2 +2 ... −xd x1 ... xd 2 −1 xd 2   ⊙   s(θ1n) s(θ2n) ... s(θd 2 n) s(θ1n) ... s(θd 2 −1n) s(θd 2 n)   (3) By leveraging this element-wise implementation, we can apply RoPE on-the-fly to the Key activations (after dequantizing the Key activations and before multiplying them with the corresponding elements in the Query vector). D Derivation for Sensitivity Analysis To compute the sensitivity analysis we largely follow the derivation in [29], which was originally provided to compute sensitivity of a NN based classifier but can be extended with minor modifications for quantization. In particular, the sensitivity measure is based on how much the loss output of the model is perturbed. To compute this we denote activations at a layer before quantization as A, after quantization as AQ, quantization perturbation in activation as A − AQ, and the gradient of the Loss function w.r.t. activation as J(A) = ∂L ∂A (A). By making the assumption that the quantization perturbation of different activations follow a Gaussian distribution with zero mean, we can show that the sensitivity of an activation value is proportional to Fii \u0000 A − Q(A) \u00012 as was given in Equation 1: E∆A h |L(A) − L(A + ∆A)|2 i ≈ E∆A h\u0000 J(A)T ∆A \u00012i = E∆A \"\u0010X i Ji∆Ai \u00112 # = E∆A \"X i J2 i ∆A2 i # = X i J2 i E∆A \u0002 ∆A2 i \u0003 . Here note that we first assume a first order Taylor series expansion to approximate the perturbation to the loss, and then use the zero mean assumption of the quantization perturbation to derive the second line. To approximate the E∆A \u0002 ∆A2 i \u0003 we use empirical evaluation of the expectation by sampling multiple different inputs and empirically computing the resulting quantization perturbation which results in Equation 1. E Derivation for Quantization Error In our work, before applying the sensitivity-weighted K-means to derive quantization signposts, we normalize each element Ai in the flattened activation A. This normalization for Ai involves a shift by a zero-point zi followed by rescaling the quantization signposts by a scaling factor si, where si and zi are the scaling factor and zeropoint corresponding to element Ai: Ai,norm = Ai − zi si , (4) where Ai and Ai,norm are element i from activation A before and after normalization, respectively. We then quantize Ai,norm to Q(Ai,norm) with quantization error ∆Ai,norm. After we dequantized, we rescale each element by si and add zi to get the recovered quantized activation value Q(Ai): 16Q(Ai) = si Q(Ai,norm) + zi. (5) As such, if there is quantization error ∆Ai,norm in Ai,norm, this will be scaled by si in terms of the error in Ai, i.e., ∆Ai = si∆Ai,norm. For activation A which is normalized to Anorm (with corresponding scaling factors si for each element Ai), minimizing the sensitivity-weighted quantization error as expressed in Equation 1 gives us the following expression, which we can minimize using the normalized activations across all N elements from the samples in a calibration set: Q(A)∗ ≃ arg min Q NX i=1 Fii \u0000 Ai − Q(Ai) \u00012 (6) = arg min Q NX i=1 Fii \u0010 s2 i \u0000 Ai,norm − Q(Ai,norm) \u00012\u0011 (7) F Key and Value Dynamic Range Figure 4 shows the portion of the elements contained within difference percentages of the dynamic range for both Keys and Values. The majority of values (∼ 99%) are contained in a small portion of the dynamic range, and a small portion of numerical outliers skew the dynamic range that must be represented. This motivates our dense-and-sparse approach which removes numerical outliers and stores them in a separate sparse matrix, thereby restricting the range that needs to be represented in the dense component. 0 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Normalized Magnitude Keys 0 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Normalized Magnitude Values t100 t99.99 t99.9 t99 Figure 4: Distribution of the magnitude of elements of Key (Pre-RoPE) and Value activations for different layers of LLaMA-7B, computed on a single sample with sequence length 2K from the Wikitext-2 dataset. The normalized magnitude is computed by dividing by the largest magnitude value in that layer. As one can see, for both Key and Value activations, the majority of values lie in a small portion of the dynamic range, with a few numerical outliers skewing the dynamic range (and thereby reducing the fidelity when quantizing to low precision). G Per-Channel Key Quantization Ablations As shown in Table 9, per-channel quantization for Keys and per-token quantization for Values outperforms the standard per-token quantization approach for both Keys and Values, yielding an improvement of 3.82 perplexity for the LLaMA-7B model at 3-bit precision. This demonstrates the 17benefits of per-channel Key quantization to mitigate the large outlier channels in Keys. Note that for all experiments using per-channel quantization, we use an fp16 zeropoint rather than a low-precision zeropoint that is rounded to the nearest integer value. We do this since for some of the Key channels, all of the elements are positive or all of the elements are negative, meaning that the zeropoint will fall outside of the range that is representable by a low-precision integer (and rounding it to the nearest low-precision value can degrade performance). Additionally, we observe that per-channel quantization for Values actually performs worse than per-token quantization. We hypothesize that this behavior is because per-channel Value quantization leads to greater error accumulation in particular output values (since the result of the attention scores multiplied by one channel of the Values will be localized to a single value in the output vector), which leads to greater quantization error at later model layers. Another concurrent work, KIVI [26], observes similar behavior for per-channel Value quantization, which they attribute to the fact that per-token Value quantization confines the error to each token. Assuming that the output is a weighted sum of only a few important tokens (as only a few attention scores are large), a perturbation in these tokens can lead to significant degradation. Per-token Value quantization therefore ensures that the quantization of unimportant tokens does not adversely impact the important tokens. Table 9: Ablation Study: Perplexity comparison of per-token and per-channel quantization for KV cache activations for LLaMA-7B. PT refers to per-token quantization, and PC refers to per-channel quantization. Datatype Key Dim. Value Dim. Perplexity KV Cache Size (GB) Seqlen 128K fp16 - - 5.68 64.0 int3 PT PT 10.87 12.0 int3 PC PC 223 12.0 int3 PC PT 7.05 12.0 H Pre-RoPE Key Quantization Ablations As shown in Table 10, pre-RoPE Key quantization achieves higher accuracy than post-RoPE quanti- zation, with an improvement of 0.82 perplexity for 3-bit quantization with the LLaMA-7B model. These results show that the rotary positional embeddings make Key quantization more challenging due to mixing pairs of channels with different magnitudes. Pre-RoPE quantization thereby allows for more accurate quantization at low precision. Table 10: Ablation Study: Perplexity comparison of Pre-RoPE and post-RoPE Key quantization for LLaMA-7B (using per-channel Key quantization and per-token Value quantization). Pre-RoPE quantization leads to significant improvement (see Section 3.2 for more details). Datatype Scheme Perplexity KV Cache Size (GB) Seqlen 128K fp16 - 5.68 64.0 int3 post-RoPE 7.05 12.0 int3 pre-RoPE 6.23 12.0 I Sensitivity-Weighted Non-Uniform Quantization Ablations Table 11 shows perplexity evaluation results across different LLaMA, Llama-2, and Mistral models on Wikitext-2 for different datatypes, including nuq3, nuq3 without using sensitivity-weighting, as well as nuq3 with sensitivity-weighting but without accounting for the per-channel scaling factors when performing k-means. We observe particularly noticeable gains relative to uniform quantization for 3- bit and 2-bit quantization, where the benefits of non-uniform quantization are more pronounced due to the reduced precision. These results demonstrate the benefits of our sensitivity-weighted non-uniform quantization approach relative to NormalFloat quantization [8], as we achieve consistent accuracy improvements of up to 0.33 perplexity across different models. These results also demonstrate the 18necessity of our sensitivity-weighting approach in order to derive performant non-uniform datatypes using a k-means based approach. Additionally, we observe distinct benefits when also accounting for the per-channel scaling factors when performing k-means. Table 11: Ablation Study: Ablation of our sensitivity-weighted non-uniform datatype for different models on Wikitext-2. All experiments use pre-RoPE per-channel quantization for Keys and per-token quantization for Values (meaning that all configurations are the same as in KVQuant, except for the datatype). We compare against both uniform (int3) and non-uniform (nf3) [8] approaches, as well as with using “unweighted” k-means (i.e., not sensitivity-weighted) and “Fisher-weighted k- means” (without accounting for per-channel scaling factors) to compute the non-uniform quantization signposts. Note that there is slight variation in average bitwidth across models due to the differing hidden dimensions. Results report perplexity with 2K/4K/8K sequence length for LLaMA, Llama-2, and Mistral, respectively. Method LLaMA Llama-2 Mistral-7B Avg. Num. Bits7B 13B 30B 65B 7B 13B 70B baseline 5.68 5.09 4.10 3.53 5.12 4.57 3.12 4.76 16 int3 6.23 5.60 5.09 5.18 5.95 5.98 3.26 5.26 3.00-3.02 nf3 6.05 5.42 4.51 3.84 5.55 5.15 3.27 5.13 3.00-3.02 nuq3 (unweighted) 6.84 6.16 5.37 4.57 8.52 7.66 3.67 5.29 3.00-3.02 nuq3 (Fisher-weighted) 6.01 5.34 4.41 3.74 5.49 4.83 3.26 5.03 3.00-3.02 nuq3 (KVQuant) 5.94 5.32 4.34 3.68 5.39 4.82 3.23 4.98 3.00-3.02 J Per-Vector Dense-and-Sparse Quantization Ablations Table 12 shows the performance improvements we observe when isolating a small portion of outliers and storing them in a sparse format. We provide results both with using a single per-matrix outlier threshold, as well as with applying separate outlier thresholds per-vector. In particular, we see greater improvements by employing outlier detection with a different threshold per-channel for Keys and per-token for Values. This provides additional benefits since some values which would be considered outliers for the entire matrix are not actually outliers within a particular channel (so they are not hard to quantize). It is therefore better to directly target the outliers that will skew the quantization range for a particular channel. By removing 1% of outliers using per-vector thresholds, we can achieve an additional 0.19 reduction in perplexity for the LLaMA-7B model at 3 bits, thereby enabling 3-bit quantization with under 0.1 degradation in perplexity. Table 12: Ablation Study: Perplexity comparison of different outlier isolation methods for LLaMA- 7B on Wikitext-2. Per-vector outlier detection allows for significant accuracy improvements relative to per-tensor outlier detection. All experiments use per-token quantization for Values and per-channel quantization for Keys (pre-RoPE). “PV” refers to using per-vector outlier thresholds, and “PM” refers to using a single per-matrix outlier threshold. Datatype % Outliers Outlier Dim. Perplexity KV Cache Size (GB) Seqlen 128K fp16 - - 5.68 64.0 nuq3 - - 5.94 12.0 nuq3 0.1% PM 5.89 12.2 nuq3 0.1% PV 5.82 12.2 nuq3 1% PM 5.85 13.3 nuq3 1% PV 5.75 13.3 K Attention Sink-Aware Quantization Ablations Table 13 provides perplexity with and without Attention Sink-Aware quantization across different LLaMA, Llama-2, and Llama-3 models. For all bit widths (4, 3, and 2-bit) and in both dense- only and dense-and-sparse quantization settings, Attention Sink-Aware quantization consistently yields perplexity improvement. Notably, the perplexity gain is more pronounced at lower bit widths 19and without sparsity. We observe particularly significant improvements for Llama-3 models, with perplexity improvements of 0.25 and 0.13 PPL for nuq2-1% on Llama-3-8B and Llama-3-70B, respectively. Table 13: Ablation Study: Perplexity with and without Attention Sink-Aware quantization on Wikitext-2 with various models. Attention Sink-Aware quantization consistently improves perplexity across different models and bit widths, particularly with lower bit widths and without sparsity. Datatype Attention Sink-Aware LLaMA-7B LLaMA-13B Llama-2-7B Llama-2-13B Llama-3-8B Llama-3-70B fp16 - 5.68 5.09 5.12 4.57 5.54 2.59 nuq4 X 5.72 5.14 5.17 4.62 5.64 2.65 nuq4 O 5.72 5.13 5.16 4.60 5.60 2.62 nuq4-1% X 5.69 5.10 5.13 4.59 5.57 2.60 nuq4-1% O 5.69 5.10 5.13 4.58 5.56 2.60 nuq3 X 5.94 5.32 5.39 4.82 6.10 2.99 nuq3 O 5.87 5.25 5.34 4.71 5.84 2.72 nuq3-1% X 5.75 5.15 5.18 4.62 5.67 2.66 nuq3-1% O 5.75 5.14 5.17 4.61 5.64 2.63 nuq2 X 8.47 7.29 11.20 23.34 16.63 6.79 nuq2 O 7.23 5.82 7.03 9.59 7.04 3.49 nuq2-1% X 6.05 5.39 5.47 4.85 6.29 2.95 nuq2-1% O 6.01 5.36 5.41 4.78 6.04 2.82 L Calibration Ablations Figure 5 outlines the accuracy and efficiency challenges which our work addresses in order to enable accurate and efficient KV cache quantization. We demonstrate that we can circumvent the challenges related to efficiently performing online per-channel scaling factor computation by instead calibrating offline for the scaling factor without hurting accuracy. Additionally, we show that we can perform per-token scaling factor and outlier threshold computation efficiently online during inference, thereby enabling accurate per-token quantization without compromising on efficiency. Per-channel Quantization  dim sequence length KeysNew Key Per-token Quantization  dim sequence length ValuesNew ValueChallenge: need to potentially recompute the scaling factor every time a new key is added if we computeitonline Challenge: Need to compute the scaling factor for each incoming token per-channel scaling factor per-token scaling factor Figure 5: One typically achieves better performance when the scaling factor/zero point are computed online. However, this is quite challenging to do for per-channel quantization, as these factors will not only need to be recomputed for every new Key appended to the Key cache, but also all the prior cached Keys will need to be updated. As such, we use a calibration set to compute per-channel scaling factors offline. A similar challenge exists for per-token quantization, but online calibration for this does not require updating prior cached Values. In Section 3.6 and Appendix L, we discuss how we are able to efficiently compute outlier thresholds / scaling factors for per-token calibration, thereby enabling online computation. Table 14 shows accuracy results when using offline calibration for computing the scaling factors for the Keys. For 3-bit quantization, we observe minor accuracy degradation when not employing outlier extraction methods. However, if we remove a small percentage of outliers, then the accuracy with offline calibration is the same as computing the scaling factors online per-channel during evaluation. This demonstrates that when incorporating outlier extraction methods, we are better able to perform offline calibration due to reduced sensitivity to outliers (either to outliers during calibration that exaggerate the quantization range, or to outliers during evaluation that cannot be represented accurately if there weren’t large outliers observed during calibration). 20Table 15 shows the runtime for the topk operation for the LLaMA-7B model (which is required for computing outlier thresholds online). It compares the runtime of the topk operation with the runtime for the QKV projections, finding that the topk runtime is 45% of the matrix-vector operation runtime for a single projection layer. The topk operation can also be performed efficiently on the CPU, so we can actually run this operation in parallel with the subsequent linear layer matrix-vector operations on the GPU (which is possible by computing the Value projection before the Key and Query projections). This allows us to compress the activations dynamically without added runtime overhead, thereby enabling online scaling factor computation for the Value tensors. Table 16 shows how both Fisher information computation and calibration (including k-means) per- layer take only a few minutes for the LLaMA-65B model on a typical server machine. Even if we perform calibration sequentially for each layer, the entire calibration process would take a maximum of 6 hours for the LLaMA-65B model at 4-bit precision. Table 14: Ablation Study: Model accuracy when using offline calibration for Keys with LLaMA-7B. When incorporating outlier detection, offline calibration for Keys is able to perform comparably with online calibration. All nuq3 experiments use per-token quantization for Values and per-channel quantization for Keys (pre-RoPE), and experiments with outliers use per-vector outlier detection. Datatype % Outliers Perplexity Perplexity (Online for K) (Offline for K) fp16 - 5.68 5.68 nuq3 - 5.91 5.94 nuq3 1% 5.75 5.75 Table 15: topk runtime on a vector of length 4096 for computing outlier thresholds when using 1% sparsity (compared with the runtime for the QKV matrix multiplications, which are 4096×4096 by 4096 matrix-vector multiplications for the LLaMA-7B model). The runtime is reported on a system with an A6000 GPU and an Intel Xeon Gold 6126 CPU. We find that the runtime for the topk operation is only 45% of the runtime of each matvec operation. Additionally, the topk operation can be performed efficiently on the CPU; we can therefore run this operation in parallel with subsequent linear layer operations on the GPU to compress the activations dynamically without added overhead. Operation Device Outlier % Runtime (ms) QKV Projection GPU - 0.172 topk CPU 1% 0.026 topk GPU 1% 0.088 QKV Projection / topk (Fused) GPU / CPU 1% 0.173 Table 16: Runtime for computing Fisher information as well as for calibration (including k-means) with 16 samples for LLaMA-65B quantization. Runtime for computing Fisher information was computed on an 8-GPU A100-80GB system. Runtime for calibration (including k-means) was performed on an Intel Xeon Gold 6442Y CPU, and is shown for a single layer. Note that calibration is independent for each layer, so it can be easily parallelized. Operation Runtime (minutes) Computing Fisher Information 2.8 4-bit Calibration Per-Layer (including k-means) 4.5 3-bit Calibration Per-Layer (including k-means) 2.7 2-bit Calibration Per-Layer (including k-means) 1.9 M Additional Experimental Details For our empirical evaluation, we use 16 calibration samples of sequence length 2K from the Wikitext- 2 training set (as well as the corresponding gradients) to derive the per-channel scaling factors and zero-points, and to derive the non-uniform datatypes for both Keys and Values. While we use KVQuant models that are calibrated on the Wikitext-2 dataset for all experiments, in Appendix L, we 21include an additional experiment that demonstrates the robustness of the calibration process to the choice of data. We measured perplexity on both Wikitext-2 and on C4 using a sequence length equal to the maximum context length of the model (2K for LLaMA, 4K for Llama-2, and 8K for Llama-3 and Mistral- 7B). For generative tasks, when processing the input prompt, the Key/Value matrix multiplications are computed using the fp16 Keys and Values, and then they are separately compressed into low precision. For baseline experiments, we use post-RoPE quantization, both since this is required from an efficiency perspective without a dedicated kernel implementation, and because it provides better accuracy when quantizing Keys per-token as shown in Appendix P. We make several assumptions in order to estimate average bit widths and KV cache sizes for different approaches. We compute these estimates assuming a sequence length of 128K (unless otherwise specified). For integer quantization, we assume a low-precision integer offset and a 16-bit scaling factor, whereas for NormalFloat and NUQ we assume that the zero-point and offset are each 16-bit. For the sparse matrices, 32-bit integers are assumed for the per-token indices (since we need to support long sequence lengths), and the elements and per-element indices are assumed to be 16-bit. This means that for CSR, the rows are assumed to be 32-bit and the columns and values are assumed to be 16-bit, whereas for CSC, the columns are assumed to be 32-bit and the rows and values are assumed to be 16-bit. N Comparison Between Different Datatypes and Sparsity Levels Table 17 shows perplexity across LLaMA, Llama-2, Llama-3, and Mistral models on Wikitext-2 using different datatypes. The results highlight that the NUQ datatype generally outperforms both uniformly quantized INT datatype and non-uniformly quantized NF datatype even after they are incorporated with grouping at the expense of increased bit-widths. This performance can be further improved by extracting 0.1% to 1.0% of outliers. Note that NUQ with a sparsity level 1.0% has a similar memory requirement as INT with a group size of 64, but achieves significant perplexity improvement across all models and bit widths. O Full Perplexity Evaluation Tables 18 and 19 show perplexity evaluation across all LLaMA, Llama-2, Llama-3, and Mistral models on Wikitext-2 and C4, respectively. These results demonstrate the benefits of our approach for KV cache compression across different models and bit widths. 22Table 17: Comparison of our NUQ datatype with and without sparsity against other data types on different models using the perplexity (PPL) measured on Wikitext-2. Non-uniform quantization (“nuq”) results are using pre-RoPE per-channel quantization for Keys. “gs64/128” refers to baseline experiments using grouping with group size 64/128. Note that there is a slight variation in average bitwidth across models due to the differing hidden dimensions. Method LLaMA Llama-2 Llama-3 Mistral-7B Avg. Num. Bits7B 13B 30B 65B 7B 13B 70B 8B 70B baseline 5.68 5.09 4.10 3.53 5.12 4.57 3.12 5.54 2.59 4.76 16 int4 5.98 5.32 4.34 3.73 5.66 5.01 3.31 7.89 14.03 4.97 4.00-4.02 int4-gs128 5.77 5.16 4.16 3.57 5.32 4.71 3.16 5.78 2.73 4.82 4.16 int4-gs64 5.73 5.14 4.14 3.56 5.25 4.66 3.14 5.68 2.66 4.80 4.31 nf4 5.87 5.23 4.25 3.63 5.47 4.90 3.22 6.42 3.86 4.91 4.00-4.03 nf4-gs128 5.77 5.17 4.17 3.58 5.30 4.71 3.16 5.84 2.75 4.83 4.25 nuq4 5.72 5.14 4.14 3.56 5.17 4.62 3.14 5.64 2.65 4.81 4.00-4.02 + 0.1% outliers 5.70 5.12 4.12 3.55 5.15 4.60 3.13 5.63 2.66 4.79 4.04-4.06 + 0.5% outliers 5.70 5.11 4.12 3.54 5.14 4.59 3.13 5.59 2.61 4.78 4.16-4.19 + 1.0% outliers 5.69 5.10 4.11 3.54 5.13 4.59 3.13 5.57 2.60 4.78 4.32-4.35 int3 10.87 8.69 6.82 6.37 22.71 18.26 7.68 125 158 7.64 3.00-3.02 int3-gs128 6.17 5.47 4.44 3.78 6.15 5.34 3.33 7.50 4.11 5.16 3.15 int3-gs64 5.93 5.29 4.26 3.66 5.64 4.98 3.23 6.38 3.23 5.00 3.30 nf3 7.33 6.21 5.46 4.44 9.96 9.50 4.06 61.07 98.64 6.30 3.00-3.03 nf3-gs128 6.26 5.52 4.54 3.83 6.21 5.43 3.38 7.79 4.74 5.23 3.25 nuq3 5.94 5.32 4.34 3.68 5.39 4.82 3.23 6.10 2.99 4.98 3.00-3.02 + 0.1% outliers 5.82 5.22 4.21 3.62 5.27 4.69 3.19 5.96 2.96 4.91 3.04-3.06 + 0.5% outliers 5.76 5.16 4.16 3.59 5.20 4.65 3.16 5.74 2.69 4.84 3.16-3.19 + 1.0% outliers 5.75 5.15 4.15 3.57 5.18 4.62 3.15 5.67 2.66 4.82 3.32-3.35 int2 11779 69965 1470 7272 4708 3943 976 2841 2164 573 2.00-2.02 int2-gs128 37.37 41.77 16.49 13.63 118 93.09 18.31 200 2092 51.96 2.14 int2-gs64 11.09 9.84 6.60 5.54 25.69 26.83 5.93 57.82 43.69 12.47 2.28 nf2 3210 5785 2044 5367 13601 4036 3680 30492 9486 903 2.00-2.03 nf2-gs128 351 141 60.97 31.69 635 642 71.21 1024 3091 253 2.25 nuq2 8.47 7.29 6.08 9.19 11.20 23.34 4.18 16.63 6.79 6.87 2.00-2.02 + 0.1% outliers 6.82 5.72 4.83 4.00 6.38 5.33 3.54 7.97 4.77 5.83 2.04-2.06 + 0.5% outliers 6.24 5.49 4.45 3.80 5.59 4.95 3.33 6.53 3.18 5.28 2.16-2.19 + 1.0% outliers 6.05 5.39 4.41 3.72 5.47 4.85 3.28 6.29 2.95 5.14 2.32-2.35 Table 18: Full evaluation of our method for different models on all LLaMA, Llama-2, Llama-3, and Mistral models using the perplexity on Wikitext-2. KVQuant results are using pre-RoPE per-channel quantization for Keys. Average bit widths assume a sequence length of 128K (ignoring context length limits for the models). Note that ATOM and FlexGen use 4-bit quantization with group sizes of 128 and 64 with uniform quantization, respectively, and we extend their methods to 3-bit and 2-bit quantization. We leverage Attention-Sink Aware quantization for all bit widths. We used post-RoPE quantization for all baseline methods since it achieves higher accuracy when quantizing Keys per-token as shown in Appendix P. Method LLaMA Llama-2 Llama-3 Mistral-7B Avg. Num. Bits7B 13B 30B 65B 7B 13B 70B 8B 70B baseline 5.68 5.09 4.10 3.53 5.12 4.57 3.12 5.54 2.59 4.76 16 int4 5.98 5.32 4.34 3.73 5.66 5.01 3.31 7.89 14.03 4.97 4.00-4.02 nf4 5.87 5.23 4.25 3.63 5.47 4.90 3.22 6.42 3.86 4.91 4.00-4.03 ATOM-4bit 5.77 5.16 4.16 3.57 5.32 4.71 3.16 5.78 2.73 4.82 4.16 Flexgen-4bit 5.73 5.14 4.14 3.56 5.25 4.66 3.14 5.68 2.66 4.80 4.31 KVQuant-4bit 5.72 5.13 4.13 3.55 5.16 4.60 3.13 5.60 2.62 4.80 4.00-4.02 KVQuant-4bit-1% 5.69 5.10 4.11 3.54 5.13 4.58 3.13 5.56 2.60 4.78 4.32-4.35 int3 10.87 8.69 6.82 6.37 22.71 18.26 7.68 125 158 7.64 3.00-3.02 nf3 7.33 6.21 5.46 4.44 9.96 9.50 4.06 61.07 98.64 6.30 3.00-3.03 ATOM-3bit 6.17 5.47 4.44 3.78 6.15 5.34 3.33 7.50 4.11 5.16 3.15 FlexGen-3bit 5.93 5.29 4.26 3.66 5.64 4.98 3.23 6.38 3.23 5.00 3.30 KVQuant-3bit 5.87 5.25 4.25 3.63 5.34 4.71 3.18 5.84 2.72 4.98 3.00-3.02 KVQuant-3bit-1% 5.75 5.14 4.15 3.57 5.17 4.61 3.15 5.64 2.63 4.82 3.32-3.35 int2 11779 69965 1470 7272 4708 3943 976 2841 2164 573 2.00-2.02 nf2 3210 5786 2044 5367 13601 4036 3680 30492 9486 903 2.00-2.03 ATOM-2bit 37.37 41.77 16.49 13.63 118 93.09 18.31 200 2092 51.96 2.14 FlexGen-2bit 11.09 9.84 6.60 5.54 25.69 26.83 5.93 57.82 43.69 12.47 2.28 KVQuant-2bit 7.23 5.82 4.87 4.03 7.03 9.59 3.45 7.04 3.49 6.84 2.00-2.02 KVQuant-2bit-1% 6.01 5.36 4.35 3.70 5.41 4.78 3.26 6.04 2.82 5.14 2.32-2.35 23Table 19: Full evaluation of our method for different models on all LLaMA, Llama-2, Llama-3, and Mistral models using the perplexity on C4. KVQuant results are using pre-RoPE per-channel quantization for Keys. Average bit widths assume a sequence length of 128K (ignoring context length limits for the models). Note that ATOM and FlexGen use 4-bit quantization with group sizes of 128 and 64 with uniform quantization, respectively, and we extend their methods to 3-bit and 2-bit quantization. We leverage Attention-Sink Aware quantization for all bit widths. We used post-RoPE quantization for all baseline methods since it achieves higher accuracy when quantizing Keys per-token as shown in Appendix P. Method LLaMA Llama-2 Llama-3 Mistral-7B Avg. Num. Bits7B 13B 30B 65B 7B 13B 70B 8B 70B baseline 7.08 6.61 5.98 5.62 6.63 6.05 4.97 7.10 5.78 5.71 16 int4 7.40 6.82 6.18 5.75 7.31 6.59 5.12 8.79 14.36 5.91 4.00-4.02 nf4 7.27 6.74 6.10 5.69 7.09 6.45 5.06 7.93 6.58 5.85 4.00-4.03 ATOM-4bit 7.16 6.67 6.02 5.65 6.87 6.20 5.00 7.32 6.03 5.76 4.16 FlexGen-4bit 7.12 6.64 6.00 5.63 6.79 6.15 4.99 7.23 5.84 5.75 4.31 KVQuant-4bit 7.11 6.64 6.00 5.63 6.68 6.08 4.99 7.18 5.80 5.75 4.00-4.02 KVQuant-4bit-1% 7.09 6.62 5.99 5.62 6.64 6.06 4.98 7.12 5.78 5.72 4.32-4.35 int3 12.97 10.95 9.13 8.27 30.14 28.57 16.00 63.75 301 8.84 3.00-3.02 nf3 8.90 7.84 7.43 6.37 14.92 13.75 5.96 68.38 148 7.27 3.00-3.03 ATOM-3bit 7.62 6.93 6.24 5.79 8.00 7.06 5.16 9.00 6.72 6.08 3.15 FlexGen-3bit 7.34 6.78 6.11 5.70 7.29 6.59 5.08 7.92 6.18 5.92 3.30 KVQuant-3bit 7.25 6.72 6.06 5.68 6.89 6.18 5.03 7.43 5.90 5.92 3.00-3.02 KVQuant-3bit-1% 7.13 6.64 6.00 5.63 6.69 6.09 4.99 7.19 5.81 5.76 3.32-3.35 int2 10892 100870 1411 7216 4708 4220 814 2113 1977 477 2.00-2.02 nf2 2850 4680 1617 5190 13081 4176 3217 78331 7616 1102 2.00-2.03 ATOM-2bit 43.49 56.25 21.07 17.05 113 97.04 23.67 135 3734 50.73 2.14 FlexGen-2bit 13.91 13.36 8.49 7.34 35.21 40.40 8.28 50.78 42.27 13.83 2.28 KVQuant-2bit 8.52 7.32 6.67 5.96 9.49 15.36 5.30 10.06 6.45 7.63 2.00-2.02 KVQuant-2bit-1% 7.33 6.78 6.11 5.70 6.96 6.25 5.08 7.60 5.96 6.05 2.32-2.35 24P Post-RoPE Per-Token Quantization Ablation Table 20 shows perplexity evaluation on Wikitext-2 for the LLaMA-7B model with uniform quan- tization, with Keys quantized pre-RoPE and post-RoPE. These results show that post-RoPE Key quantization is superior to pre-RoPE Key quantization when quantizing Keys per-token. This is because when rotating an outlier channel with large average magnitude and another channel with smaller average magnitude together, at some positions in the sequence, part of the magnitude from the outlier channel will be shifted to the smaller channel. This partially mitigates the impact of the outlier channel on skewing the quantization range for some of the tokens in the sequence. As such, for our baseline comparisons, we use post-RoPE per-token Key quantization to serve as a stronger baseline. Table 20: Model accuracy when using pre-RoPE and post-RoPE quantization for LLaMA-7B with per-token Key quantization. Our experiments demonstrate that post-RoPE quantization is superior when using per-token Key quantization. Therefore, we decided to use these results for baseline comparison with per-token quantization. Datatype Perplexity (Pre-RoPE) Perplexity (Post-RoPE) fp16 5.68 5.68 int4 6.02 5.98 int4-gs128 5.76 5.77 int3 14.68 10.87 int3-gs128 6.28 6.17 Q Experiments on Calibration Data Robustness To evaluate the robustness of our quantization method to the choice of calibration datasets, we measure the perplexity on Wikitext-2 and C4 using different quantized models calibrated with Wikitext-2 and C4. As shown in Table 21, the resulting perplexity numbers remain similar even when the models are calibrated with out-of-domain examples (e.g., calibrated with Wikitext-2 and evaluated on C4, and vice versa), demonstrating the robustness of the calibration process to the choice of data. Table 21: Perplexity (PPL) results on Wikitext-2 and C4 using different quantization schemes, calibrated using Wikitext-2 and C4. Datatype Wikitext-2 C4 Calib. with Wikitext-2 Calib. with C4 Calib. with Wikitext-2 Calib. with C4 4-bit, 1% sparsity 5.69 5.70 7.09 7.09 3-bit, 1% sparsity 5.75 5.75 7.13 7.13 2-bit, 1% sparsity 6.05 6.07 7.38 7.38 R Kernel Implementation Details We implemented 4-bit lookup table-based kernels for matrix-vector multiplication between the Key or Value activations (packed as a lookup table (LUT) plus indices into the LUT per-element) and a full-precision activation vector. These kernels load the compressed Key and Value activations and dequantize them only as needed in order to minimize memory bandwidth utilization. All arithmetic is performed in fp16. The lookup table entries are the values of the sensitivity-weighted non-uniform datatype for that particular layer scaled according to the range of activations that need to be represented [8]. When selecting between the Compressed-Sparse Column (CSC format) and the Compressed-Sparse Row (CSR) format for storing the outliers for the Keys and Values, we needed to consider how easy it would be to append new vectors. When using CSC format for the Key matrix, we only need to append a single element to the column vector, as well as one new element to the row and value vectors per nonzero element in that new column. If we used CSR format, we would need to insert the new column and value elements in the middle of the existing column and value vectors, and we would 25need to recompute the elements of the row vector. When using CSR format for the Value matrix, we only need to append a single element to the row vector, as well as one new element to the column and value vectors per nonzero element in that new row. If we used CSC format, we would need to insert the new row and value elements in the middle of the existing row and value vectors, and we would need to recompute the elements of the column vector. We therefore used the CSC format for the Key matrices and the CSR format for the Value matrices. One challenge with efficiently processing the sparse matrix-dense vector operation is that the sparsity distribution may be unbalanced. This poses a challenge for efficiently processing the sparse matrix on a GPU as there can be different numbers of nonzeros to process per thread. We therefore leverage a balanced sparse matrix-dense vector kernel based on [10, 17], which assigns an equal number of nonzeros per thread. This has greater synchronization overhead than assigning a single thread for an entire row or column when processing CSR/CSC matrices, but it leads to a more balanced work assignment between threads. We set the number of threads such that there were 10 nonzero values assigned to each thread. The dense non-uniform kernel and balanced sparse kernels are launched in one call to avoid overhead from summing the output vectors from these separate operations. A second potential challenge is that the quantization dimension for the Keys and Values is not aligned with the reduction dimension for the respective matrix-vector operations. If we used per-channel or per-token lookup tables for dequantization, we would need to load from a different lookup table for each element as we iterate along the reduction dimension. However, since our implementation uses a single per-layer datatype that is rescaled per-vector, we can load a single scaling factor and zero-point for each channel / token and broadcast it across all threads (while doing a table lookup from the shared per-layer datatype, which is duplicated such that each thread can access it efficiently in parallel). The use of a single per-layer datatype that is rescaled per-vector therefore allows for much more efficient GPU kernel implementations in the context of per-channel Key / per-token Value quantization. Table 22 shows a detailed breakdown of kernel runtime, including how much time is spent packing vectors into the compressed format and how much time is spent on the dense and sparse matrix- vector multiplications. For the fp16 baselines, we benchmarked runtime by averaging across 1000 iterations. When benchmarking our dense-and-sparse kernels (both for packing and matrix-vector multiplication), since the runtime of the sparse kernels can vary based on the sparsity pattern, we ran the LLaMA-2-7B-32K model and collected activations at each layer, and we then measured the average runtime across 1000 iterations for each layer in the model. We find that even with 1% sparsity, we can attain significant speedups of up to 1.7× relative to the fp16 matrix-vector multiply kernels, demonstrating how our methodology facilitates efficient inference with a low-precision quantized KV cache. Table 22: Average latency (in microseconds) for the Key and Value dense nuq4 kernels as well as for the sparse kernels (with 1% outliers), benchmarked on an A6000 GPU for the LLaMA-2-7B-32K model. Benchmarking results are reported for different sequence lengths ( l). fp16 matrix-vector multiplication latencies are included for reference, and the Key multiplication time also includes the time to apply RoPE to the newly appended Key vector. We find that our dense-and-sparse approach (even with 1% outliers) provides latency benefits relative to the fp16 baseline, even when accounting for the time to compress activations online. Activation Operation l=2K l=4K l=16K Key fp16 Matvec 33.3 59.1 219.4 Key (nuq4-1%) Packing 4.5 4.5 4.5 Dense Matvec 13.0 24.1 87.6 Sparse Matvec 8.1 11.2 34.2 Total Latency 25.6 39.9 126.3 Value fp16 Matvec 26.0 50.2 203.7 Value (nuq4-1%) Packing 4.1 4.1 4.1 Dense Matvec 10.0 17.8 62.0 Sparse Matvec 7.9 15.9 58.2 Total Latency 22.1 37.9 124.5 26S Limitations While our work enables accurate long-context length inference by reducing the memory requirements, there is significant work required for training long context length models with greater than 100K context length. This work is orthogonal to our efforts, which are constrained to efficient inference with long context length models. Additionally, our latency benchmarking results currently focus on memory-bandwidth bound generation rather than prompt processing (where we need to compress multiple Keys and Values at once). Finally, in the current end-to-end implementation, there are inefficiencies in how memory allocation is handled for updating the sparse matrix (where the data corresponding to the previous tokens have to be copied when concatenating them with the data from the new token). In future work, we plan to optimize this by doing blocked allocation to avoid overheads from reallocating memory. 27",
      "meta_data": {
        "arxiv_id": "2401.18079v6",
        "authors": [
          "Coleman Hooper",
          "Sehoon Kim",
          "Hiva Mohammadzadeh",
          "Michael W. Mahoney",
          "Yakun Sophia Shao",
          "Kurt Keutzer",
          "Amir Gholami"
        ],
        "published_date": "2024-01-31T18:58:14Z",
        "pdf_url": "https://arxiv.org/pdf/2401.18079v6.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces KVQuant, a novel method for ultra-low precision KV cache quantization to enable efficient Large Language Model (LLM) inference with extremely long context lengths. It addresses the memory bottleneck caused by KV cache activations in long-context scenarios. Key contributions include achieving less than 0.1 perplexity degradation with 3-bit quantization, enabling LLaMA-7B inference with up to 1 million context length on a single A100-80GB GPU and 10 million on an 8-GPU system, and demonstrating up to 1.7x speedups for Key and Value matrix-vector multiplications compared to fp16 baselines. The method incorporates Per-Channel Key Quantization, Pre-RoPE Key Quantization, Non-Uniform KV Cache Quantization, and Per-Vector Dense-and-Sparse Quantization.",
        "methodology": "KVQuant employs several novel methods for KV cache quantization: (i) **Per-Channel Key Quantization** groups Key activations by channel to better match their distribution and mitigate outlier impacts, calibrated offline. Value activations use per-token quantization. (ii) **Pre-RoPE Key Quantization** quantizes Key activations before the rotary positional embedding (RoPE) is applied, using a fused kernel to apply RoPE post-dequantization, which preserves accuracy by avoiding RoPE's mixing of channel magnitudes. (iii) **Non-Uniform KV Cache Quantization** derives per-layer sensitivity-weighted non-uniform datatypes offline on calibration data using a k-means solver and Fisher information matrix, then rescales them per-channel/per-token for accurate online representation. (iv) **Per-Vector Dense-and-Sparse Quantization** isolates a small percentage (e.g., 1%) of numerical outliers for each vector (per-channel for Keys, per-token for Values) to minimize quantization range skews, storing them in a separate sparse representation (CSC for Keys, CSR for Values). Outlier thresholds are calibrated offline for Keys and computed online for Values. (v) **Attention Sink-Aware Quantization** keeps the first token in fp16 to address its disproportionate sensitivity to quantization error. Custom CUDA kernels are developed to efficiently perform on-the-fly activation quantization, dequantization, and sparse matrix-vector multiplications, including RoPE post-dequantization.",
        "experimental_setup": "The methodology was evaluated on LLaMA-7B/13B/30B/65B, Llama-2-7B/13B/70B, Llama-3-8B/70B, and Mistral-7B models. Perplexity (PPL) was measured using teacher forcing on Wikitext-2 and C4 datasets, with sequence lengths up to the model's maximum context length (2K for LLaMA, 4K for Llama-2, 8K for Llama-3/Mistral-7B). Calibration was performed using 16 samples of 2K sequence length from the Wikitext-2 training set. Long context length performance was assessed using LLaMA-2-7B-32K and Llama-2-70B-32K LongLoRA models. Additional evaluations included passkey retrieval (over 50 samples, using framework from [45]), LongBench [3] for QA, summarization, and few-shot learning (max 31500 context length), and RULER benchmark suite [15] for LLaMA-2-7B-32K. Comparisons were made against uniform quantization (intX), NormalFloat (nfX) [8], Atom [44], and FlexGen [34]. Kernel performance (latency for matrix-vector multiplications) was benchmarked on an A6000 GPU across different sequence lengths.",
        "limitations": "The current work does not address the significant efforts required for training long context length models greater than 100K, focusing instead on efficient inference. Latency benchmarking primarily targets memory-bandwidth bound generation, with less focus on prompt processing where multiple Keys and Values are compressed simultaneously. Additionally, the existing end-to-end implementation has inefficiencies in memory allocation for updating the sparse matrix, as data for previous tokens must be copied when concatenating with new token data.",
        "future_research_directions": "Future work will focus on optimizing memory allocation for updating sparse matrices by implementing blocked allocation to avoid reallocating memory overheads. Further research could also explore efficient inference for prompt processing, where the challenge lies in compressing multiple Keys and Values at once, a scenario not fully covered by the current latency benchmarks."
      }
    },
    {
      "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache",
      "abstract": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI.",
      "full_text": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Zirui Liu * 1 Jiayi Yuan* 1 Hongye Jin 2 Shaochen (Henry) Zhong 1 Zhaozhuo Xu 3 Vladimir Braverman 1 Beidi Chen 4 Xia Hu 1 Abstract Efficiently serving large language models (LLMs) requires batching of many requests to reduce the cost per request. Yet, with larger batch sizes and longer context lengths, the key-value (KV) cache, which stores attention keys and values to avoid re- computations, significantly increases memory de- mands and becomes the new bottleneck in speed and memory usage. Additionally, the loading of the KV cache causes the computational core to be idle, which limits the inference speed. A straight- forward and effective solution to reduce KV cache size is quantization, which decreases the total bytes taken by KV cache. However, there is a lack of in-depth studies that explore the element dis- tribution of KV cache to understand the hardness and limitation of KV cache quantization. To fill the gap, we conducted a comprehensive study on the element distribution in KV cache of popular LLMs. Our findings indicate that the key cache should be quantized per-channel, i.e., group ele- ments along the channel dimension and quantize them together. In contrast, the value cache should be quantized per-token. From this analysis, we de- veloped a tuning-free 2bit KV cache quantization algorithm named KIVI. With hardware-friendly implementation, KIVI can enable Llama, Falcon, and Mistral models to maintain almost the same quality while using 2.6× less peak memory (in- cluding model weight). This reduction in memory usage enables up to 4× larger batch size, bringing 2.35× ∼3.47× throughput on real LLM infer- ence workload. The source code is available at https://github.com/jy-yuan/KIVI. *Equal contribution . The order of authors is determined by flipping a coin. 1Rice University 2Texas A&M University 3Stevens Institute of Technology 4Carnegie Mellon University. Correspondence to: Zirui Liu <zl105@rice.edu>, Jiayi Yuan <jy101@rice.edu>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). 1. Introduction Large Language Models (LLMs) have demonstrated strong performance across a wide range of tasks (Brown et al., 2020; Taylor et al., 2022; Yuan et al., 2023; Chuang et al., 2024). However, their deployment is very costly, requiring a large number of hardware accelerators such as GPUs. Given these substantial costs, one natural way to reduce the cost per request is to combine a sufficient number of requests together for batch processing. However, in this batch infer- ence scenario, the key-value cache (KV cache), which holds the attention keys and values during generation to prevent re-computations, is becoming the new memory and speed bottleneck. This bottleneck becomes more pronounced with larger batch sizes and longer context lengths. For instance, in 540B PaLM, with a batch size of 512 and a context length of 2048, KV cache alone can take 3TB. This is 3 times the size of the model’s parameters (Pope et al., 2023). Also, the GPU SRAM has to load the whole KV cache from the GPU device memory for every token generated, during which the computational cores are idle. Thus, reducing KV cache size in LLMs while maintaining accuracy is important. Existing works towards this problem can be roughly divided into three categories. First, some work suggests reducing the number of heads in KV cache, such as multi-query attention (Shazeer, 2019) and multi-group attention (Ainslie et al., 2023). However, these methods require either training the model from scratch or fine-tuning the existing model. Second, another research line reduces KV cache size by evicting unimportant tokens (Zhang et al., 2023). Third, some other works try to solve this problem from the system perspective, e.g., offloading KV cache (Sheng et al., 2023) or extending virtual memory and paging techniques into the attention mechanism (Kwon et al., 2023). To reduce the size of KV cache, the most simple and ef- fective way is to reduce the total bytes taken by KV cache, namely, quantization. Unlike the well-studied weight quan- tization (Lin et al., 2023; Xiao et al., 2023a; Zhao et al., 2024), to the best of our knowledge, only a few studies applied the vanilla 4bit round-to-nearest quantization to KV cache (Sheng et al., 2023; Zhang et al., 2023; Zhao et al., 2024) due to the streaming nature of KV cache or other complications. There is a lack of in-depth studies that ex- 1 arXiv:2402.02750v2  [cs.CL]  25 Jul 2024KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache per-token  quantization  per-channel  quantization  Figure 1: Definition of per-token and per-channel quantiza- tion. X ∈ Rlprompt×d is key/value cache where lprompt is the number of tokens and d is the number of channels. zX is the zero-point, sX is the scaling factor. plore the element distribution of KV cache to understand the hardness and limitation of KV cache quantization. To fill the gap, we study the element distribution of KV cache. Our analysis suggests: • For key cache, there are a few fixed channels whose magnitudes are very large, which is consistent with previous finding (Lin et al., 2023; Xiao et al., 2023a). Thus, as shown in Figure 1 right, key cache should be quantized per-channel, i.e., group elements along the channel dimension and quantize them together. In this way, it can confine the error to each individual channel, without impacting the other normal channels. • For value cache, there is no obvious outlier pattern. Al- though value cache has no obvious outlier pattern, we experimentally show that it can only be quantized per- token because it is used to calculate the attention output, which is essentially a value cache mixer. As shown in Figure 1 left, the per-token quantization can confine the error inside each individual token and ensure that the quantization of one token does not adversely impact the others. Based on the above insights, we propose KIVI, a plug-and- play extreme low-bit KV cache quantization method. KIVI quantizes key cache per-channel and quantizes value cache per-token. The per-token value cache quantization aligns well with the streaming nature of auto-regressive inference, allowing newly quantized tensors to be directly appended to the existing quantized value cache by token dimension. However, for per-channel key cache quantization, the quan- tization process spans different tokens, which cannot be directly implemented in this streaming setting. Since the number of tokens in key cache can be arbitrary, our key idea is to split key cache into two parts. The first part is the grouped key cache, which contains several groups of tokens and each group has a certain number of tokens. The second part is the residual key cache, which does not have a suffi- cient number of tokens to form a complete group. Similarly, we split value cache into the grouped and residual parts to maintain the accuracy. We only apply group-wise quanti- zation to the grouped key cache and value cache, while the residual key cache and value cache are kept in full precision. The grouped and residual parts can be combined using tiled matrix multiplication when computing attention scores. Our contributions are summarized as follows: • Extensive analysis regarding the outlier patterns and quantization error of KV cache in commonly- used LLMs. Our observations suggest that key cache should be quantized per-channel and value cache should be quantized per-token. We also explain in depth why these caches require different quantization approaches. • A new plug-and-play 2bit KV cache quantization algorithm without any fine-tuning, KIVI, with hardware-friendly implementation. We conduct an extensive evaluation for KIVI with Llama, Mistral, and Falcon on popular generation tasks. KIVI can efficiently compress KV cache to 2bit and bring 2.6× peak memory usage reduction for Llama-2-7B, with little to no accuracy drop. With our efficient system im- plementation, this memory reduction, in return, enables up to 4× larger batch size and brings2.35× ∼3.47× throughput. 2. Background: Attention Inference-Time Workflow The LLM attention inference-time workflow involves two phases: i) the prefill phase, where the input prompt is used to generate KV cache for each transformer layer of LLMs; and ii) the decoding phase, where the model uses and updates KV cache to generate the next token, one at a time. Prefill Phase. Let X ∈ Rb×lprompt×d be the input tensor, where b is the batch size, lprompt is the length of the input prompt, and d is the model hidden size. For convenience, we ignore the layer index here. The key, value tensors can be computed by XK = XWK, XV = XWV , where WK, WV ∈ Rd×d are the key and value layer weight, respectively. After obtaining XK and XV , they are cached in the memory for the ease of decoding. Decoding Phase. Let t ∈ Rb×1×d be the current input token embedding. Let tK = tWK and tV = tWV be the key and value layer output, respectively. We first update KV cache: XK ← Concat(XK, tK), XV ← Concat(XV , tV ), 2KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache then calculate the attention output as: tQ = tWQ, A = Softmax(tQX⊤ K), tO = AXV , (1) where WQ is the weight matrix of the query layer. For ease of illustration, we ignore the attention output layer and the other parts of the inference workflow. Memory and Speed Analysis. The above process is re- peated until a special token indicating the sentence’s con- clusion is reached. Let lgen be the number of generated tokens. From the above analysis, the shape of KV cache is b ×(lprompt + lgen) ×d. To get a sense of the scale, consider the OPT-175B model with a batch size b 512, a prompt length lprompt 512, and an output length lgen 32. The KV cache requires 1.2TB, which is 3.8 times the model weights (Sheng et al., 2023). Besides the memory, the inference speed is also decided by the KV cache size. The GPU needs to load KV cache from GPU main memory to GPU SRAM once for every token generated during which the computa- tional core of the chip is essentially idle (Pope et al., 2023; Kwon et al., 2023). 3. Methodology In scenarios with long contexts or batched inferences, the memory and speed bottlenecks are storing and loading KV cache. The most simple and effective way to alleviate this problem is to reduce the total bytes occupied by KV cache, specifically, quantization. Following this motivation, we first evaluate the performance of the existing quantization method in Section 3.1. Our observations suggest that key and value cache should be quantized along different dimen- sions. We analyze the rationale behind this observation in Section 3.2. Then based on the analysis, we propose KIVI, a new KV cache quantization method along with its streaming data structure, detailed in Section 3.3. 3.1. Preliminary Study of KV Cache Quantization As we analyzed in Section 2, KV cache functions as a streaming data structure, where the new tensor arrives se- quentially. Thus, optimization-based methods like GPTQ (Frantar et al., 2022) are unsuitable for quantizing KV cache due to the overhead. To the best of our knowledge, the most flexible way for quantizing KV cache is the round- to-nearest quantization. The B−bit integer quantization- dequantization process can be expressed as: Q(X) = ⌊X − zX sX ⌉, X′ = Q(X) · sX + zX, where zX = min X is the zero-point, sX = (max X − min X)/(2B −1) is the scaling factor, and ⌊·⌉ is the round- ing operation. Here we ignore the batch size for ease of Table 1: The results of simulated KV cache group-wise quantization with various configurations. The group size is set as 32. C stands for per-channel quantization and T stands for per-token quantization. Please check the whole evaluation in Table 3. Llama-2-13B CoQA TruthfulQA 16bit 66.37 29.53 4bit (K - T, V - T) 66.48 29.51 2bit (K - T, V - T) 52.93 24.98 2bit (K - C, V - C) 2.88 0.74 2bit (K - T, V - C) 2.80 0.26 2bit (K - C, V - T) 63.53 28.60 understanding. As shown in Figure 1, X is quantized along either the token or channel dimension group-wisely. Considering the streaming nature of KV cache, previous studies often apply per-token quantization to both key and value cache since the newly quantized KV cache can be naively added to the existing quantized one along the token dimension (Sheng et al., 2023). While per-channel quanti- zation is non-trivial, we have designed a padding method to implement per-channel quantization to explore its effect on both key and value cache. Setting. In Table 1, we show the results of fake KV cache group-wise quantization with different configurations on the Llama-2-13B model for the CoQA and TruthfulQA tasks. We use a group size of 32 for all configurations. Here fake quantization means we simulate the quantization process by first quantizing KV cache into lower precision and then dequantizing it in the attention layer. For per-channel quan- tization, if the number of tokens is not divided evenly into groups, we add zero-padding to ensure it can be grouped perfectly. In this way, we ensure that all tokens in KV cache are quantized for a fair comparison. The detailed experi- mental setting can be found in Section 4.1. Specifically, we observe that: OB 1. When using the commonly used per-token quanti- zation to both key and value caches, INT4 precision can maintain accuracy. However, reducing it to INT2 results in a notable accuracy drop. OB 2. When value cache is quantized per-channel, the accuracy significantly worsens regardless of how key cache is quantized. OB 3. When using a lower numerical precision such as INT2, the most accurate approach is to quantize key cache per-channel and value cache per-token. 3KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache 0 50 100 150 200 Token 0 25 50 75 100 125 Channel 0.0 2.5 5.0 7.5 10.0 12.5Absolute Value Llama-2-13B Layer 16 Head 0 Key Cache 0 50 100 150 200 Token 0 25 50 75 100 125 Channel 0.5 1.0 1.5 2.0Absolute Value Llama-2-13B Layer 16 Head 0 Value Cache 0 50 100 150 200 Token 0 25 50 75 100 125 Channel 0.0 2.5 5.0 7.5 10.0 12.5 Llama-2-13B Layer 31 Head 0 Key Cache 0 50 100 150 200 Token 0 25 50 75 100 125 Channel 0.5 1.0 1.5 2.0 2.5 Llama-2-13B Layer 31 Head 0 Value Cache 0 50 100 150 200 Token 0 20 40 60 Channel 0 2 4 6 8 10Absolute Value Falcon-7B Layer 16 Head 0 Key Cache 0 50 100 150 200 Token 0 20 40 60 Channel 0.5 1.0 1.5Absolute Value Falcon-7B Layer 16 Head 0 Value Cache 0 50 100 150 200 Token 0 20 40 60 Channel 0.0 2.5 5.0 7.5 10.0 Falcon-7B Layer 20 Head 0 Key Cache 0 50 100 150 200 Token 0 20 40 60 Channel 0.5 1.0 1.5 Falcon-7B Layer 20 Head 0 Value Cache Figure 2: Magnitude of key and value cache for Llama-2-13B and Falcon-7B. We observe (1) for key cache, there are a few channels whose magnitudes are very large. (2) for value cache, there is no obvious outlier pattern. 3.2. Why Key and Value Cache Should Quantize Along Different Dimensions? In Table 1, we observe that quantizing key cache per-channel and value cache per-token to 2bit results in a very small accu- racy drop. Here we analyze why this configuration delivers better accuracy. In Figure 2 we visualize the original KV cache distribution at different layers. We observe that in key cache, some fixed channels exhibit very large mag- nitudes, whereas in value cache, there is no significant pattern for outliers. Analysis of Key Cache. The above observation for key cache aligns with previous findings that certain fixed columns in activations exhibit larger outliers (Dettmers et al., 2022; Lin et al., 2023). The persistence of outliers within each channel means that per-channel quantization can con- fine the quantization error to each individual channel with- out impacting the other normal channels. Thus, Figure 2 explains why key cache should be quantized per-channel. In Table 2 we show key cache relative reconstruction error ∥XK−X′ K XK ∥F , along with the relative attention score error ∥A−A′ A ∥F where A′ = Softmax(tQX ′⊤ K ). We observe that the per-token quantization can lead to almost 5× larger at- tention score error than per-channel quantization, which is consistent with Figure 2. Table 2: The relative error statistics averaged over all layers and all heads Llama-2-13B K Per-Token K Per-Channel Avg. ∥ XK−X′ K XK ∥F 13.67 4.55 Avg. ∥ A−A′ A ∥F 47.00 9.60 Attention sparsity 84.3% V Per-Token V Per-Channel Avg. ∥ XV −X′ V XV ∥F 4.57 3.73 Avg. ∆ 3.55 49.89 Analysis of Value Cache. Unlike key cache, value cache does not show the channel-wise outlier pattern. Furthermore, Figure 2 alone cannot explain OB2, which indicates value cache should only be quantized per-token. This is because Figure 2 implies that errors should be comparable for both per-token and per-channel quantization, given the absence of a clear pattern. As shown in Equation (1), value cache is used to calculate the attention output tO. Instead of analyzing the quantization error of value cache XV , in Table 2 we analyze the relative error ∆ = ∥AXV −AX′ V AXV ∥F with different quantization configurations. Surprisingly, we observe that the per-token quantization error is almost 15× 4KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Q_MatMul Concat Quantization by Channel Quantization by Token MatMul Prefill Phase Decoding Phase* KVCache * we omit the value cache and  attention output calculation Full Precision Tensor Low Precision Tensor Figure 3: The overview of KIVI algorithm. For ease of illustration, we omit the value cache and attention output parts. The detailed pseudo-code is provided in Algorithm 1. Here “Q_Matmul” is the mix-precision matrix multiplication which fuses the dequantization with matrix multiplication at the tiling level. smaller than per-channel quantization, which explains why OB2 happens. The intuition behind this observation stems from the attention sparsity. Equation (1) can be written as: [AXV ]i∗ = lpromptX j=1 Aij[XV ]j∗, (2) where [XV ]j∗ is the j-th row of XV . From Equation (2), the attention output is the weighted summation of value cache across various tokens, with the weights being the attention scores. Since the attention score is highly sparse (Tian et al., 2023), the output is just the combination of value caches of a few important tokens. The per-token quantization can confine the error to each individual token. Thus, quantizing other tokens does not affect the accuracy of important tokens. Consequently, per-token quantization leads to a much smaller relative error ∆. 3.3. KIVI: Algorithm and System Support Algorithm. As we previously analyzed, key cache should be quantized per-channel and value cache should be quan- tized per-token. Recall that key and value cache of newly generated tokens arrive sequentially. From the implemen- tation perspective, per-token quantization aligns well with streaming settings, allowing newly quantized tensors to be directly appended to the existing quantized value cache by token dimension. However, for per-channel quantiza- tion, the quantization process spans across different tokens, which cannot be directly implemented in the streaming set- ting. As shown in Figure 3, our key idea to solve this prob- lem is to group key cache everyG tokens and quantize them separately. Because the number of tokens in XK can be ar- bitrary, we splitXK into two parts, namely, the grouped part XKg = XK[: l − r] and residual part XKr = XK[l − r :], where l is the number of tokens inside the current key cache XK, r is the number of residual tokens, where l − r can be divisible by G. Since XKg can be evenly divided into (l − r)/G groups, we only store Q(XKg ) with group-wise quantization, while XKr is kept in full precision. During the decoding process, each newly arrived key cache tK is added to XKr and once XKr reaches R tokens, which is a hyperparameter - residual length, we quantize and concatenate it with the previously quantized Q(XKG). Then we reset XKr to an empty tensor. We note that R should be divisible by G. With tiled matrix multiplication, the raw attention logits is then calculated as: Ag = tQQ(X⊤ Kg ), XKr = Concat([XKr , tK]), Ar = tQX⊤ Kr , A = Concat([Ag, Ar]). (3) For value cache, similar to key cache, we also split it into two parts and keep the most recent value cache in full pre- cision, namely, XVg and XVr . Specifically, we maintain a queue and each newly arrived value cache is pushed into the queue. Once the queue reaches the predefined residual length R, the most outdated value cache is poped. Then the poped value cache is quantized per-token and concatenated with the previously quantized value cache along the token dimension. As shown in Figure 3, we also emphasize that during the prefill phase, the exact key and value tensors are passed to the next layers, although only the quantized KV cache is retained in memory. The whole algorithm can be found in Appendix A Algorithm 1. Analysis. In KIVI, the grouped key cacheXKg and value cache XVg is quantized, while the residual key cache XKr and value cache XVr is kept in full precision. By design, there are at most R tokens inside XKr or XVr . In practice, we set R ≤ 128 and the sequence length lprompt + lgen is often much longer than R. Thus the memory overhead from XKr and XVr is negligible when considering the benefit from extreme low-bit quantization, especially for the long context scenarios. Also, since the newly arrived key and 5KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache value tensors are added to XKr and XVr in full precision, KIVI maintains a full precision KV cache sliding window for the local relevant tokens. This window size is expected to be R 2 for key cache, and R for value cache. Later in the experiment section, we show that this full precision sliding window is crucial for obtaining desirable perfor- mance on hard tasks, such as GSM8K. System Support. We provide a hardware-friendly imple- mentation for running KIVI on GPUs. To minimize the overhead, we have fused the dequantization process with matrix multiplication, e.g., Q_MatMul in Figure 3, using CUDA. We also implement the group-wise quantization ker- nel in Triton. Our method is fully compatible with weight- only quantization. 4. Experiments 4.1. Settings Models. We evaluate KIVI using three popular model families: Llama/Llama-2 (Touvron et al., 2023a;b), Fal- con (Penedo et al., 2023) and Mistral (Jiang et al., 2023). Llama and Mistral model is based on multi-head attention, while Falcon is based on multi-query attention (Shazeer, 2019). We use the Hugging Face Transformers codebase and implement the KIVI algorithm upon it. Following previous work (Sheng et al., 2023), the group size G in Algorithm 1 for quantization is set as 32 across all experi- ments, the residual length R for key and value cache is set to 128. Tasks. As we analyzed in Section 2, the KV cache size grows larger with a longer context. Thus, we evaluateKIVI under the normal context length and long context setting, respectively. Specifically, we adopt generation tasks from LM-Eval (Gao et al., 2021) for normal context length eval- uation and LongBench (Bai et al., 2023) for long context evaluation, respectively1. For LM-eval, we adopt CoQA (Exact match accuracy), TruthfulQA (BLEU score), and GSM8K (Exact match accuracy). For LongBench, we chose tasks from four subgroups. Specifically, Qasper (F1 score) is a Single-Document QA task; QMSum (ROUGE score) and MultiNews (ROUGE score) are Summarization tasks; TREC (classification score), TriviaQA (F1 score), and SAM- Sum (ROUGE score) are Few-shot Learning tasks; and LCC (similarity score) and RepoBench-P (similarity score) is Code Completion task. The maximum sequence length in LongBench was set to 8192 for the Mistral model and 4096 for other models. We also consider the needle-in-a-haystack 1The closed-end tasks such as MMLU are not ideal to evaluate KIVI since they only involve one decoding step and directly fetch the output logits, which is not suitable for studying the impact of compressed KV cache. task (NIAH) to evaluate the model’s long context retrieval ability after quantizing KV cache. Detailed NIAH setting can be found in Appendix B. 4.2. Accuracy and Efficiency Analysis 4.2.1. C OMPARISON BETWEEN DIFFERENT QUANTIZATION CONFIGURATIONS We first utilize the fake quantization to demonstrate the effec- tiveness of our asymmetric quantization, namely, quantizing key cache per-channel and value cache per-token. Here fake quantization is exactly the same as in Table 1. The results are shown in Table 3. We observe that “2bit (K per-channel, V per-token)” consistently achieves the best results com- pared to all other configurations. This is consistent with our previous analysis. We also note that for hard generation tasks such as GSM8K, the fake “2bit (K per-channel, V per-token)” quantization results are significantly worse than the full precision counterparts. However, for KIVI in Table 3, we observe that the accuracy drop is only around 2% for GSM8K across different models. As we analyzed in Section 3.3, the difference between fake “2bit (K per-channel, V per-token)” quantization and KIVI is that KIVI maintains a full precision key and value cache sliding window for the local relevant tokens. This sliding window is crucial to maintaining accuracy for hard generation tasks such as mathematical reasoning. 4.2.2. A CCURACY COMPARISON ON GENERATION TASKS LM-Eval Results. We benchmark KIVI in CoQA, Truth- fulQA and GSM8K tasks using the LM-Eval framework. All dataset parameters were set to default. We compare the standard 16bit configuration with our KIVI compression techniques in Llama-2-7B, Llama-2-13B, Falcon-7B and Mistral-7B. As shown in Table 3, we observe that for the Llama and Mistral model, KIVI only has up to 2% accu- racy drop despite the KV cache being stored in 2bit. For instance, in the Llama-2-7B model, the transition from 16bit to 2bit only slightly decreases accuracy. Similar trends are observed in other Llama-family models. Since Falcon-7B adopts multi-query attention and only has one head for KV cache, it is already highly compressed compared to Llama- based models. Thus, in Table 3, 4bit KIVI is needed to maintain the accuracy, while 2bit KIVI may have a large accuracy drop in this case. LongBench Results. The performance of KIVI over vari- ous models in the LongBench dataset is summarised in Table 4. We apply KIVI to Llama2-7B, Llama2-13B, Llama2-7B- Chat, Llama2-13B-Chat, Falcon-7B and Mistral-7B. Table 4 suggests that KIVI is an effective method for KV cache compression with minimal impact on accuracy across var- 6KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  27K tokens (a) Llama-3-8B-Instruct Baseline 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  27K tokens  (b) Llama-3-8B-Instruct + KIVI-2 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  27K tokens  (c) Llama-3-8B-Instruct + KIVI-4 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  30K tokens (d) Mistral-7B-Instruct-v0.2 Baseline 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  30K tokens  (e) Mistral-7B-Instruct-v0.2 + KIVI-2 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  30K tokens  (f) Mistral-7B-Instruct-v0.2 + KIVI-4 0.0 0.2 0.4 0.6 0.8 1.0 Figure 4: Needle-in-a-Haystack results on Llama-3-8B-Instruct and Mistral-7B-Instruct-v0.2. Here we count the number of words instead of tokens to better account for the tokenizer difference. The final token length is noted in the upper right corners of each figure. Detailed setting can be found in Appendix B. ious hard long context generation tasks. We present ad- ditional results using Llama3-8b, Mistral-7B-v0.2, and LongChat-7B-v1.5, which can be found in Appendix D. NIAH Results. From Figure 4, we observe that KIVI can still maintain the retrieval ability of LLMs even with 2bit KV Cache. Detailed NIAH setting can be found in Appendix B. 4.2.3. A BLATION In this section, we benchmark KIVI on GSM8K, one of the hardest generation tasks, to show the effect of hyperpa- rameters group size G and residual length R on the model performance. For full results of KIVI with a residual length of 32, please refer to Appendix C. The effect of group size. We fix the residual length at 128 and vary the group sizes to 32, 64, and 128. From Table 5, we observe that group sizes 32 and 64 yield similar results, whereas the performance significantly decreases when the group size reaches 128. Note the zero-point and the scaling factor mentioned in Section 3.1 are calculated according to this group size; where the choice of group size will greatly impact the KV cache compression effect under a long input. The effect of residual length. We fix the group size at 32 and vary the residual length across 32, 64, 96, and 128. As shown in Table 5, there is no consistent pattern between residual lengths and model accuracy. Namely, while a resid- ual length of 128 achieves good results, 32 and 96 yield similar outcomes, but a residual length of 64 results in the worst performance. We emphasize that while we observe no significance among residual lengths of {32, 96, 128}, hav- ing a reasonably large residual length is important; as it brings much performance boosts on hard tasks like GSM8K, again as shown in Table 5. 4.2.4. E FFICIENCY COMPARISON To evaluate the wall-clock time efficiency ofKIVI, follow- ing vLLM (Kwon et al., 2023), we synthesize workloads based on ShareGPT (sha, 2023), which contain input and output texts of real LLM services. On average, the data set has an input prompt length lprompt of 161 and an output length lgen of 338 (Kwon et al., 2023). We increase the batch 7KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Table 3: Performance comparison between 16bit, 4-bit per- token quantization, four fake 2bit KV cache quantization similar to those in Table 1, KIVI-2 (2bit) / KIVI-4 (4bit) across various models. We emphasize that unlike KIVI, which preserves a small portion of full precision key cache XKr and value cache XVr , all tokens in fake KV cache quantization are quantized for a fair comparison. C stands for per-channel quantization and T stands for per-token quantization. Model CoQA TruthfulQA GSM8K Llama-2-7B 16bit 63.88 30.76 13.50 4bit (K -T, V -T) 64.82 29.85 12.28 2bit (K -C, V -T) 59.08 33.10 5.76 2bit (K -T, V -T) 39.88 18.29 0.83 2bit (K -C, V -C) 3.60 0.27 0.00 2bit (K -T, V -C) 1.30 0.49 0.08 KIVI-4 63.78 30.80 13.80 KIVI-2 63.05 33.95 12.74 Llama-2-13B 16bit 66.37 29.53 22.67 4bit (K -T, V -T) 66.73 29.14 20.92 2bit (K -C, V -T) 63.53 28.60 12.21 2bit (K -T, V -T) 52.93 24.98 4.55 2bit (K -C, V -C) 2.88 0.74 0.00 2bit (K -T, V -C) 2.80 0.26 0.08 KIVI-4 66.38 29.49 23.65 KIVI-2 66.23 29.84 20.77 Falcon-7B 16bit 59.83 23.20 4.55 4bit (K -T, V -T) 58.53 22.94 3.26 2bit (K -C, V -T) 43.93 20.82 1.29 2bit (K -T, V -T) 25.72 0.91 0.53 2bit (K -C, V -C) 41.95 17.11 1.52 2bit (K -T, V -C) 19.53 0.94 0.15 KIVI-4 59.67 22.58 4.47 KIVI-2 57.48 24.98 3.41 Mistral-7B 16bit 67.40 30.45 38.36 4bit (K -T, V -T) 67.80 29.83 36.85 2bit (K -C, V -T) 61.65 29.64 26.46 2bit (K -T, V -T) 54.55 25.86 5.00 2bit (K -C, V -C) 24.40 24.86 2.27 2bit (K -T, V -C) 10.73 19.12 0.99 KIVI-4 66.95 30.49 37.30 KIVI-2 66.35 32.17 36.01 size until out of memory and report the peak memory usage and throughput between KIVI (with residual length 32 and 128) and FP16 baseline for the Llama-2-7B model. The hardware here is a single NVIDIA A100 GPU (80GB). As shown in Figure 5, with similar maximum memory us- age, KIVI enables up to 4× larger batch size and gives 2.35× ∼3.47× larger throughput. This throughput num- ber can grow larger with longer context length and output length. We also note that this speed-up can be greatly increased if we further fuse the KV cache quantization process with previous operations. We leave it as one of future work. Figure 5: Memory usage and throughput comparison be- tween 2bit KIVI and 16bit baseline. KIVI can achieve higher throughput by enabling a larger batch size. 5. Related Work Many machine learning systems and benchmark works con- sider scaling up LLM inference process (Pope et al., 2023; Yuan et al., 2024). Among them, quantization techniques have been widely applied (Frantar et al., 2022; Lin et al., 2023; Kim et al., 2023; Xu et al., 2023). A main branch of LLM quantization is weight-only quantization, which in- volves the quantization of model weights to lower precision. For instance, AWQ (Lin et al., 2023) cleverly quantizes model weights to INT4 and INT3 using an activation-aware manner. GPTQ (Frantar et al., 2022) utilizes approximate second-order information to quantize model weights both accurately and efficiently. SqueezeLLM (Kim et al., 2023) adopts the concept of non-uniform quantization based on sensitivity along with dense-and-sparse decomposition. This line of work is orthogonal to ours, as they can be combined. SmoothQuant (Xiao et al., 2023a) is a post-training quan- tization method that is more closely related to our work. This method uses equivalent transformations to balance the quantization complexity for both activation and weight, making the activation easier to quantize. SmoothQuant can compress KV cache to 8bit with minor performance loss. However, it faces a significant accuracy drop when scaled down to 4bit or less (Zhao et al., 2024). FlexGen (Sheng et al., 2023) adopts 4-bit group-wise quantization for both key and value cache. 8KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Table 4: Performance evaluation of KIVI on various models across a range of benchmarks in LongBench. We highlight the average performance of our method. More similar results on Mistral-7B-v0.2 and LongChat-7b-v1.5 can be found in Table 10 and Table 9 Model Qasper QMSum MultiNews TREC TriviaQA SAMSum LCC RepoBench-P Average Llama2-7B 16bit 9.52 21.28 3.51 66.00 87.72 41.69 66.66 59.82 44.52 KIVI-4 9.28 21.42 3.88 66.00 87.72 41.82 66.80 59.83 44.59 KIVI-2 9.31 20.50 1.14 66.00 87.42 42.71 66.88 60.23 44.27 Llama2-13B 16bit 9.32 21.38 3.71 70.00 87.87 43.55 66.61 56.42 44.85 KIVI-4 9.16 20.86 3.21 69.00 86.97 44.26 65.30 57.08 44.48 KIVI-2 8.58 20.69 6.19 69.50 87.78 44.30 65.08 55.46 44.69 Llama2-7B-Chat 16bit 19.65 20.54 26.36 63.00 84.28 41.12 59.75 52.93 45.95 KIVI-4 19.62 20.70 25.49 63.00 84.13 40.87 59.27 53.56 45.83 KIVI-2 19.32 20.46 25.48 63.00 84.84 40.60 58.71 52.97 45.67 Llama2-13B-Chat 16bit 24.18 20.37 25.69 67.50 86.90 42.18 50.23 50.64 45.96 KIVI-4 23.00 20.36 26.06 67.50 87.20 42.04 52.55 52.77 46.44 KIVI-2 23.59 20.76 25.25 67.5 87.17 41.56 49.93 48.45 45.52 Falcon-7B 16bit 1.48 2.35 11.09 13.00 5.84 2.44 23.86 9.69 8.71 KIVI-4 1.04 2.41 11.98 13.00 5.84 2.36 23.72 9.92 8.78 KIVI-2 1.98 3.61 6.78 10.00 6.24 2.73 22.18 10.12 7.95 Mistral-7B 16bit 8.12 19.98 19.99 67.50 89.80 41.69 66.59 58.99 46.58 KIVI-4 7.89 20.06 20.58 67.50 89.80 41.56 66.45 58.62 46.56 KIVI-2 6.92 19.71 17.92 66.50 89.63 41.66 65.52 58.99 45.85 Table 5: Ablation study of KIVI by changing group size G and residual length R. Model Group Size GSM8K Llama2-13B 32 20.77 64 21.00 128 17.29 Model Residual Length GSM8K Llama2-13B 32 20.62 64 19.86 96 20.55 128 20.77 One essential recipe of KIVI is the per-channel quantiza- tion scheme designed based on the observation made in Section 3.2 and Figure 2. ATOM (Zhao et al., 2024) also indicates that key cache exhibits more outliers compared to the value cache. KIVI provides further extensive analysis and leverages this observation to implement per-channel quantization. A similar observation and approach has been independently discovered and developed in the concurrent work KVQuant (Hooper et al., 2024). vLLM (Kwon et al., 2023) and S3 (Jin et al., 2023) are system-level works, which include memory management through the use of PagedAttention or memory usage pre- diction. They can lower the memory requirements of KV cache and simultaneously increase model throughput. This research direction is orthogonal to our work, since system- level optimizations can also be applied upon our algorithm. Several other works also consider compressing KV cache by evicting tokens. H2O (Zhang et al., 2023) retains only a small portion of tokens that contribute significantly to the attention scores. Similarly, Scissorhands (Liu et al., 2024) exploit the persistence of the importance hypothesis in KV cache sparsification. StreamingLLM (Xiao et al., 2023b) is based on the observation of “attention sink” and maintains only a few initial tokens to preserve performance. Unlike these works, our KIVI retains all input tokens and compresses them into lower precision. This line of work is orthogonal to ours, as they can also be combined together. 6. Conclusion and Future Work In this paper, we systematically analyze KV cache element distribution in popular LLMs. We conclude that key cache should be quantized per-channel and value cache should be quantized per token. Based on these observations, we propose KIVI, a plug-and-play 2bit KV cache quantization algorithm without the need for any tuning. In real LLM workload, KIVI allows up to 4× larger batch sizes and 3.47× throughput. In the future, we will further optimize the implementation to reduce the overhead of quantization process during the prefill and decoding phase. Acknowledgments The authors thank the anonymous reviewers for their help- ful comments. Dr. Beidi Chen is supported by a research gift from Moffett.AI. Dr. Vladimir Braverman is partially 9KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache supported by the Ministry of Trade, Industry and Energy (MOTIE) and Korea Institute for Advancement of Technol- ogy (KIAT) through the International Cooperative R&D pro- gram, the Naval Research (ONR) grant N00014-23-1-2737, and NSF CNS 2333887 award. Dr. Xia Hu is supported by NSF grants NSF IIS-2224843. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. References ShareGPT Team. https://sharegpt.com/, 2023. Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan- guage models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. Yu-Neng Chuang, Songchen Li, Jiayi Yuan, Guanchu Wang, Kwei-Herng Lai, Leisheng Yu, Sirui Ding, Chia-Yuan Chang, Qiaoyu Tan, Daochen Zha, et al. Understand- ing different design choices in training large time series models. arXiv preprint arXiv:2406.14045, 2024. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, An- thony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al. A frame- work for few-shot language model evaluation. Version v0. 0.1. Sept, page 8, 2021. Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W Mahoney, Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cache quantization. arXiv preprint arXiv:2401.18079, 2024. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Yunho Jin, Chun-Feng Wu, David Brooks, and Gu-Yeon Wei. S3: Increasing gpu utilization during genera- tive inference for higher throughput. arXiv preprint arXiv:2306.06000, 2023. Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611–626, 2023. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023. Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and An- shumali Shrivastava. Scissorhands: Exploiting the per- sistence of importance hypothesis for llm kv cache com- pression at test time. Advances in Neural Information Processing Systems, 36, 2024. Amirkeivan Mohtashami and Martin Jaggi. Landmark at- tention: Random-access infinite context length for trans- formers. arXiv preprint arXiv:2305.16300, 2023. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobei- dli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outper- forming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023. 10KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Ja- cob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling trans- former inference. Proceedings of Machine Learning and Systems, 5, 2023. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrit- twieser, et al. Gemini 1.5: Unlocking multimodal un- derstanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150, 2019. Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. Flexgen: High-throughput generative inference of large language models with a single gpu. In International Conference on Machine Learning, pages 31094–31116. PMLR, 2023. Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022. Yuandong Tian, Yiping Wang, Beidi Chen, and Simon Du. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. arXiv preprint arXiv:2305.16380, 2023. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar- tinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and ef- ficient post-training quantization for large language mod- els. In International Conference on Machine Learning, pages 38087–38099. PMLR, 2023a. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023b. Zhaozhuo Xu, Zirui Liu, Beidi Chen, Yuxin Tang, Jue Wang, Kaixiong Zhou, Xia Hu, and Anshumali Shrivastava. Compress, then prompt: Improving accuracy-efficiency trade-off of llm inference with transferable prompt. arXiv preprint arXiv:2305.11186, 2023. Jiayi Yuan, Ruixiang Tang, Xiaoqian Jiang, and Xia Hu. Large language models for healthcare data augmentation: An example on patient-trial matching. In AMIA Annual Symposium Proceedings, volume 2023, page 1324. Amer- ican Medical Informatics Association, 2023. Jiayi Yuan, Hongyi Liu, Shaochen Zhong, Yu-Neng Chuang, Songchen Li, Guanchu Wang, Duy Le, Hongye Jin, Vipin Chaudhary, Zhaozhuo Xu, Zirui Liu, and Xia Hu. Kv cache compression, but what must we give in return? a comprehensive benchmark of long context capable ap- proaches. arXiv preprint arXiv:2407.01527, 2024. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H _2 o: Heavy-hitter oracle for efficient generative inference of large language models. arXiv preprint arXiv:2306.14048, 2023. Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom: Low-bit quanti- zation for efficient and accurate llm serving. Proceedings of Machine Learning and Systems, 6:196–209, 2024. 11KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache A. Detailed Implementations In this section, we present the algorithm for KIVI as discussed in Section 3.3. Specifically, we provide the pseudocode for KIVI when calculating the attention output in the prefill and decoding phases. Algorithm 1: The KIVI Prefill & Decoding Algorithm parameter: group size G, residual length R procedure Prefill: Input: X ∈ Rlprompt×d XK = XWK, XV = XWV XVg = XV [: lprompt − R], XVr = XV [lprompt − R :] Q(XVg ) ← GroupQuant(XVg , dim=token, numGroup=d//G) Q(XKg ), XKr ← KeyQuant(XK) KV cache ← Q(XKg ), XKr , Q(XVg ), XVr return XK, XV end procedure Decoding: Input: KV cache, t ∈ R1×d tQ = tWQ, tK = tWK, tV = tWV Q(XKg ), XKr , Q(XVg ), XVr ← KV cache XKr ← Concat([XKr , tK], dim=token) XVr ← Concat([XVr , tV ], dim=token) if len(XKr ) = R then Q(XKr ), _ ←KeyQuant(XKr ) Q(XKg ) ← Concat([Q(XKg ), Q(XKr )], dim=token) XKr ← empty tensor. end if len(XVr ) > Rthen Q(XV ′r ) ← GroupQuant(XVr [: −R], dim=token, numGroup = d//G) Q(XVg ) ← Concat([Q(XVg ), Q(XV ′r )], dim=token) XVr ← XVr [−R :] end A ← Concat([tQQ(XKg )⊤, tQX⊤ Kr ], dim=token) Ag = Softmax(A)[: −R], Ar = Softmax(A)[−R :] tO ← AgQ(XVg ) + ArXVr KV cache ← Q(XKg ), XKr , Q(XVg ), XVr return tO end function KeyQuant(XK ∈ Rl×d): r = l%R, XKg = XK[: l − r], XKr = XK[l − r :] Q(XKg ) ← GroupQuant(XKg , dim=channel, numGroup=l//G) return Q(XKg ), XKr end 12KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache B. NIAH Setting We largely follows the passkey retrieval prompt template of Mohtashami and Jaggi (2023) but using 7-digit passkey and Paul Graham Essays2 as the background filler, as set forth in Arize-ai and Reid et al. (2024): There is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I will quiz you about the important information there. <prefix filled by Paul Graham Essays> The pass key is <7-DIGIT PASS KEY>. Remember it. <7-DIGIT PASS KEY> is the pass key. <suffix filler> What is the pass key? The pass key is C. More Ablation Results In our efficiency evaluation, we observe that with a residual length of 32, KIVI achieves a significantly higher memory compression rate, which in turn leads to increased throughput. Additionally, our ablation study reveals that changing the residual length from 128 to 32 does not result in a substantial performance gap. We demonstrate KIVI with a residual length of 32 across all benchmark datasets. As shown in Tables 6 and 7, KIVI with a residual length of 32 also delivers performance comparable to that of the 16-bit full model. Table 6: Performance comparison between 16bit, KIVI-2 (2bit) / KIVI-4 (4bit) with residual length 128 and 32 across various models. R32 stands for residual length 32. Model CoQA TruthfulQA GSM8K Llama-2-7B 16bit 63.88 30.76 13.50 KIVI-2 R128 63.05 33.95 12.74 KIVI-2 R32 62.85 33.01 13.57 Llama-2-13B 16bit 66.37 29.53 22.67 KIVI-2 R128 66.23 29.84 20.77 KIVI-2 R32 66.57 29.35 20.62 Falcon-7B 16bit 59.83 23.20 4.55 KIVI-4 R128 59.67 22.58 4.47 KIVI-4 R32 59.73 22.96 3.94 KIVI-2 R128 57.48 24.98 3.41 KIVI-2 R32 57.50 25.70 2.20 Mistral-7B 16bit 67.40 30.45 38.36 KIVI-2 R128 66.35 32.17 36.01 KIVI-2 R32 65.90 31.21 34.34 D. More Experimental Results We present additional results using Llama3-8B, Mistral-7B-v0.2, and LongChat-7B-v1.5 in LongBench, which can be found in Table 8, Table 9 and Table 10, respectively. We also show result of Needle-in-a-Haystack Test in Figure 4. The settings largely follow the format of the original passkey retrieval task (Mohtashami and Jaggi, 2023) while including some modern modifications set forward by Arize-ai and the technical report of Gemini 1.5 (Reid et al., 2024). 2https://paulgraham.com/articles.html 13KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Table 7: Performance evaluation of KIVI with residual length 128 and 32 on various models across a range of benchmarks in LongBench. R32 stands for residual length 32. Model Qasper QMSum MultiNews TREC TriviaQA SAMSum LCC RepoBench-P Average Llama2-7B 16bit 9.52 21.28 3.51 66.00 87.72 41.69 66.66 59.82 44.52 KIVI-2R128 9.31 20.50 1.14 66.00 87.42 42.71 66.88 60.23 44.27 KIVI-2R32 9.26 20.53 0.97 66.00 87.42 42.61 66.22 59.67 44.08 Llama2-13B 16bit 9.32 21.38 3.71 70.00 87.87 43.55 66.61 56.42 44.85 KIVI-2R128 8.58 20.69 6.19 69.50 87.78 44.30 65.08 55.46 44.69 KIVI-2R32 8.38 20.74 7.01 69.50 87.78 44.43 64.89 55.31 44.75 Llama2-7B-Chat 16bit 19.65 20.54 26.36 63.00 84.28 41.12 59.75 52.93 45.95 KIVI-2R128 19.32 20.46 25.48 63.00 84.84 40.60 58.71 52.97 45.67 KIVI-2R32 19.10 20.08 25.33 63.00 85.04 39.80 57.91 52.38 45.33 Llama2-13B-Chat 16bit 24.18 20.37 25.69 67.50 86.90 42.18 50.23 50.64 45.96 KIVI-2R128 23.59 20.76 25.25 67.50 87.17 41.56 49.93 48.45 45.52 KIVI-2R32 23.56 20.90 25.45 67.50 87.42 41.40 48.93 48.81 45.49 Falcon-7B 16bit 1.48 2.35 11.09 13.00 5.84 2.44 23.86 9.69 8.71 KIVI-4R128 1.04 2.41 11.98 13.00 5.84 2.36 23.72 9.92 8.78 KIVI-4R32 1.03 2.45 11.99 13.50 5.84 2.46 23.88 9.95 8.88 KIVI-2R128 1.98 3.61 6.78 10.00 6.24 2.73 22.18 10.12 7.95 KIVI-2R32 2.28 3.23 6.73 10.00 6.31 2.88 22.71 10.45 8.07 Mistral-7B 16bit 8.12 19.98 19.99 67.50 89.80 41.69 66.59 58.99 46.58 KIVI-2R128 6.92 19.71 17.92 66.50 89.63 41.66 65.52 58.99 45.85 KIVI-2R32 6.84 19.81 17.20 66.50 89.63 42.82 65.13 58.06 45.74 Table 8: The results of Llama-3-8B-Instruct with KIVI on LongBench. The model has 8K context length and applies group query attention, which uses 8 heads for KV cache instead of the full 32 heads. We use a 32 group size and 128 residual length for both KIVI-2 and KIVI-4. The baseline is of full precision. NarrativeQA Qasper MultiFieldQA HotpotQA MuSiQue 2WikiMQA GovReport QMSum Baseline 21.71 44.24 44.54 46.82 21.49 36.42 30.03 22.67 w./KIVI-2 21.35 43.17 44.49 46.79 20.56 37.05 29.98 22.07 w./KIVI-4 21.01 44.83 44.60 46.96 21.43 36.48 30.22 22.44 MultiNews LCC RepoBench-P TriviaQA SAMSum TRec PR Avg Baseline 27.79 57.00 51.22 90.23 42.53 74.50 67.00 45.21 w./KIVI-2 27.77 50.84 46.65 90.54 42.26 74.50 67.50 44.37 w./KIVI-4 27.97 57.36 52.03 90.33 42.97 74.50 66.50 45.31 Table 9: The results of Mistral-7B-Instruct-v0.2 with KIVI on LongBench. The model has 32K context length and applies group query attention, which uses 8 heads for KV cache instead of the full 32 heads. We use a 32 group size and 128 residual length for both KIVI-2 and KIVI-4. The baseline is of full precision. NarrativeQA Qasper MultiFieldQA HotpotQA MuSiQue 2WikiMQA GovReport QMSum Baseline 21.02 29.41 47.13 36.53 19.13 21.76 32.59 23.99 w./KIVI-2 20.61 28.73 44.88 35.47 17.95 20.68 32.55 23.65 w./KIVI-4 20.97 29.41 46.52 36.25 19.53 21.66 32.97 24.06 MultiNews LCC RepoBench-P TriviaQA SAMSum TRec PR Avg Baseline 27.09 53.49 51.40 86.23 43.04 71.00 89.33 43.54 w./KIVI-2 26.54 53.03 51.16 86.00 43.34 71.00 80.83 42.43 w./KIVI-4 26.89 53.33 51.41 86.23 43.34 71.00 89.42 43.53 14KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Table 10: The results of LongChat-7B-v1.5-32K with KIVI on LongBench. The model has 32K context length. We use a 32 group size and 128 residual length for both KIVI-2 and KIVI-4. The baseline is of full precision. NarrativeQA Qasper MultiFieldQA HotpotQA MuSiQue 2WikiMQA GovReport QMSum Baseline 20.65 29.42 43.15 33.05 14.66 24.14 30.85 22.84 w./KIVI-2 20.79 28.69 41.02 32.91 13.82 23.00 30.47 22.59 w./KIVI-4 20.49 28.90 43.24 33.07 14.66 24.86 31.40 22.84 MultiNews LCC RepoBench-P TriviaQA SAMSum TRec PR Avg Baseline 26.55 54.83 58.94 83.99 40.75 66.50 30.50 38.72 w./KIVI-2 26.28 54.11 57.62 83.19 41.28 66.50 32.25 38.30 w./KIVI-4 26.52 54.06 58.77 83.88 40.62 67.00 31.50 38.79 15",
      "meta_data": {
        "arxiv_id": "2402.02750v2",
        "doi": "10.13140/RG.2.2.28167.37282",
        "authors": [
          "Zirui Liu",
          "Jiayi Yuan",
          "Hongye Jin",
          "Shaochen Zhong",
          "Zhaozhuo Xu",
          "Vladimir Braverman",
          "Beidi Chen",
          "Xia Hu"
        ],
        "published_date": "2024-02-05T06:06:47Z",
        "pdf_url": "https://arxiv.org/pdf/2402.02750v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the KV cache bottleneck in LLMs, which significantly increases memory demands and limits inference speed. It conducts a comprehensive study on KV cache element distribution, revealing that key cache should be quantized per-channel due to large magnitude outliers, while value cache should be quantized per-token because it acts as a mixer where per-token errors are better confined. Based on these insights, the paper proposes KIVI, a tuning-free asymmetric 2bit KV cache quantization algorithm. KIVI reduces peak memory usage by 2.6x, enabling up to 4x larger batch sizes and achieving 2.35x to 3.47x throughput on real LLM inference workloads with minimal quality loss for Llama, Falcon, and Mistral models.",
        "methodology": "KIVI is a plug-and-play, tuning-free, asymmetric 2bit quantization method. It quantizes key cache per-channel and value cache per-token. To handle the streaming nature of auto-regressive inference, KIVI splits KV cache into two parts: a grouped part (quantized) and a residual part (kept in full precision, maintaining a full precision sliding window for recent tokens). During decoding, newly arrived tokens are added to the residual part, and once it reaches a certain length, it's quantized and appended to the grouped part. Hardware-friendly implementation includes fusing dequantization with matrix multiplication using CUDA and implementing group-wise quantization kernels in Triton. The method is compatible with weight-only quantization.",
        "experimental_setup": "KIVI was evaluated on Llama/Llama-2 (7B, 13B, 7B-Chat, 13B-Chat), Falcon-7B, and Mistral-7B models, along with additional results on Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.2, and LongChat-7B-v1.5-32K. Experiments used Hugging Face Transformers. The quantization group size (G) was set to 32, and the residual length (R) for the full-precision sliding window was 128 (with ablation studies also using 32, 64, 96). Evaluation tasks included: 1) Normal context length: LM-Eval framework with CoQA, TruthfulQA, and GSM8K. 2) Long context length: LongBench, covering Single-Document QA (Qasper), Summarization (QMSum, MultiNews, SAMSum), Few-shot Learning (TREC, TriviaQA), and Code Completion (LCC, RepoBench-P). Maximum sequence lengths were 8192 for Mistral and 4096 for others. 3) Long context retrieval: Needle-in-a-Haystack (NIAH) test. Performance was measured by accuracy metrics, peak memory usage, and throughput on a single NVIDIA A100 GPU (80GB). Comparisons were made against 16bit baseline, 4bit per-token quantization, and various fake 2bit quantization configurations.",
        "limitations": "For models like Falcon-7B that utilize multi-query attention, 2bit KIVI may lead to a significant accuracy drop, necessitating 4bit quantization to preserve performance. The paper also implies that the current system implementation for speed-up could be further optimized by fusing the KV cache quantization process with previous operations, suggesting it is not yet fully optimized for maximum throughput.",
        "future_research_directions": "Future work will focus on further optimizing the implementation to reduce the overhead of the quantization process during both the prefill and decoding phases. The paper also notes that KIVI is orthogonal to other research directions, such as weight-only quantization, system-level optimizations (e.g., memory management, PagedAttention), and KV cache compression through token eviction, implying potential for combining KIVI with these methods."
      }
    },
    {
      "title": "FrameQuant: Flexible Low-Bit Quantization for Transformers",
      "abstract": "Transformers are the backbone of powerful foundation models for many Vision\nand Natural Language Processing tasks. But their compute and memory/storage\nfootprint is large, and so, serving such models is expensive often requiring\nhigh-end hardware. To mitigate this difficulty, Post-Training Quantization\nseeks to modify a pre-trained model and quantize it to eight bits or lower,\nsignificantly boosting compute/memory/latency efficiency. Such models have been\nsuccessfully quantized to four bits with some performance loss. In this work,\nwe outline a simple scheme to quantize Transformer-based models to just two\nbits (plus some overhead) with only a small drop in accuracy. Key to our\nformulation is a concept borrowed from Harmonic analysis called Fusion Frames.\nOur main finding is that the quantization must take place not in the original\nweight space, but instead in the Fusion Frame representations. If quantization\nis interpreted as the addition of noise, our casting of the problem allows\ninvoking an extensive body of known consistent recovery and noise robustness\nguarantees. Further, if desired, de-noising filters are known in closed form.\nWe show empirically, via a variety of experiments, that (almost) two-bit\nquantization for Transformer models promises sizable efficiency gains. The code\nis available at https://github.com/vsingh-group/FrameQuant",
      "full_text": "FrameQuant: Flexible Low-Bit Quantization for Transformers Harshavardhan Adepu 1 Zhanpeng Zeng 1 Li Zhang 2 Vikas Singh 1 2 Abstract Transformers are the backbone of powerful foun- dation models for many Vision and Natural Lan- guage Processing tasks. But their compute and memory/storage footprint is large, and so, serv- ing such models is expensive often requiring high-end hardware. To mitigate this difficulty, Post-Training Quantization seeks to modify a pre- trained model and quantize it to eight bits or lower, significantly boosting compute/memory/latency efficiency. Such models have been successfully quantized to four bits with some performance loss. In this work, we outline a simple scheme to quan- tize Transformer-based models to just two bits (plus some overhead) with only a small drop in accuracy. Key to our formulation is a concept borrowed from Harmonic analysis called Fusion Frames. Our main finding is that the quantization must take place not in the original weight space, but instead in the Fusion Frame representations. If quantization is interpreted as the addition of noise, our casting of the problem allows invoking an extensive body of known consistent recovery and noise robustness guarantees. Further, if de- sired, de-noising filters are known in closed form. We show empirically, via a variety of experiments, that (almost) two-bit quantization for Transformer models promises sizable efficiency gains. The code is available at https://github.com/ vsingh-group/FrameQuant 1. Introduction Transformer-based Large Language Models (LLMs) domi- nate the landscape for Natural Language Processing tasks such as language translation and text summarization (Zhang et al., 2023; Touvron et al., 2023; Zhang et al., 2022b). Vision Transformers (VITs) adapt this idea for computer 1University of Wisconsin-Madison 2Google Research. Corre- spondence to: Harshavardhan Adepu <adepu@wisc.edu>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). vision, and achieve state-of-the-art results on image classi- fication (Zhai et al., 2022), object detection (Zhang et al., 2022a), generation (Chang et al., 2022; Hudson & Zitnick, 2021) and segmentation (Cheng et al., 2022; Ranftl et al., 2021). There is general agreement that scale provides re- markable new capabilities. While large models offer strong performance improvements, their deployment as a module within a product creates unique challenges. For example, serving these models on ex- pensive hardware can drastically increase data center costs. Even loading these models on consumer-grade machines is difficult, and the ability to handle heterogeneous resource- constrained devices is almost infeasible. This has led to various efficiency-focused strategies for model compression including but not limited to distillation (Hinton et al., 2015; Zhu et al., 2021), pruning (Chen & Zhao, 2019), sparsity (Yu et al., 2012; Yun et al., 2020) and quantization (Han et al., 2016; Banner et al., 2019). Among these methods, Post-Training Quantization offers unique advantages in that it does not change the model architecture or training scheme. This paper presents a new Post-Training Quantization scheme, FrameQuant, that offers much more flexibility to strike a balance between reducing model size and preserving model quality. Specifically, FrameQuant offers what may be considered equivalent to using a fractional number of bits for quantization, e.g., 2.1 or 2.2 bits: this is valuable because for large Transformer-based models like GPT, model quality deteriorates fast (Frantar et al., 2023) as we reduce bit width in the low-bit quantization regime (e.g., 2-bit). Further, de- pending on the accuracy needs of the downstream task at hand or a desire to control the worst-off error, more flexi- bility offers the user more control. Towards this goal, our main idea is to compute a specific type of redundant/over- complete representation of a pre-trained weight matrix and quantize the matrix in that representation. We will see how robustness to quantization error will follow naturally from our choice of representation. The de-quantization step uses a straightforward scheme to re-construct the full-precision weights. We leverage a mature concept from Harmonic analysis, Fusion Frames, as the foundation for our proposal. Fusion Frames (Donoho et al., 1998; Christensen, 2018) serve an important role in signal processing in analog-to- digital conversion and signal transmission. Frames are guar- 1 arXiv:2403.06082v2  [cs.LG]  31 Jul 2024Flexible Low-Bit Quantization for Transformers anteed to be robust when the Frame coefficients are cor- rupted by additive noise. They are numerically stable, and if additional compute/memory overhead is acceptable, de- noising filters with good theoretical properties or provably optimal recovery schemes are known. To our knowledge, Frame theory for neural network quantization is unexplored. Our key contributions include (a) an approach that offers fractional bit quantization capabilities with theoretical guar- antees. (b) We empirically verify that Transformer-based models can be quantized to two bits (or 2.x bits), on an ex- tensive basket of 15 popular Vision Transformers and Large Language Models from the OPT (Zhang et al., 2022b) as well as Llama2 (Touvron et al., 2023) classes. We achieve consistent improvements over all existing baselines. 1.1. Related Work Given the growth in the scale of foundation models com- mon in our community, model compression is an active topic of research. Distillation (Hinton et al., 2015; Zhu et al., 2021), pruning/shrinking (Chen & Zhao, 2019) and the use of sparsity is quite common (Yu et al., 2012; Yun et al., 2020). There is growing interest (Rokh et al., 2023; Namburi et al., 2023; Gholami et al., 2022) in approaches that perform model compression via quantization either (i) during training or (ii) post-training since minimal changes to the architecture are needed. Quantization during training works well (Gholami et al., 2022; Nagel et al., 2021), but models must be re-trained. Post-training quantization (PTQ) methods (Nagel et al., 2019) simply quantize a pre-trained model on a small calibration set, and involve much less work. These methods are effective for large language models like OPT (Zhang et al., 2022b), BLOOM (Scao et al., 2023) and can reduce the bit-width with only a small degradation in performance. For example, (Nagel et al., 2020) analyzed the effect of data-dependent rounding. A layer-wise proxy loss was studied and AdaRound quantization was proposed to efficiently minimize this loss. The approach in (Frantar & Alistarh, 2022) minimizes the squared error similar to (Nagel et al., 2020), but quantizes each layer individually while adjusting the remaining unquantized weights using the Hessian of the proxy loss term following (Lecun et al., 1989; Hassibi et al., 1993). OPTQ (Frantar et al., 2023)(for- merly GPTQ) extended upon the ideas in OBQ (Frantar & Alistarh, 2022), and offered other adjustments that gives a stable scheme that can compress large language models like OPT-175B or BLOOM-176B to 3 or 4 bits per parameter without a large loss in accuracy. For Vision Transformers, PTQ4ViT (Yuan et al., 2022) quantifies the weights in two stages, and uses a Hessian-guided search for the optimal scale for the weights. In (Liu et al., 2021b), a feature map is used to search for the optimal quantization interval for maintaining similarity between the quantized and original feature maps. The method also chooses different bit widths for each layer. Other strategies proposed for PTQ include (Ding et al., 2022; Li et al., 2023). We note a recent con- current result for two-bit quantization for language models reported in (Chee et al., 2023). Our approaches are based on different starting points: our choice of Frame theory to min- imize quantization error versus the choice in (Chee et al., 2023) of using incoherence as a pre and post-processing step, which is later shown to offer desirable theoretical prop- erties. But fundamentally, both methods work well due to similar underlying principles related to basis expansions (on a space-filling basis). We discuss later how (Chee et al., 2023) can be viewed as a special version of our formulation (but with no redundancy). 2. Finite Frame Theory and Fusion Frames Frames generalize the Orthogonal basis decomposition of a Hilbert space and provide redundant representations. Finite frames find applications in robust signal transmission with quantization and erasures (Goyal et al., 1998; 2001; Casazza & Kovaˇcevi´c, 2003), Coding theory (Strohmer & Heath Jr, 2003), distributed processing (Casazza et al., 2008), Com- pressed Sensing (Boufounos et al., 2009) among others. We start with a brief review of relevant concepts. Readers familiar with these concepts may skim this section. Consider a finite-dimensional Hilbert space H of dimension d. Throughout the paper, we denote this space as Hd. Definition 2.1 (Frames). A family of k vectors ϕ = (φi)k i=1 in Hd is called a frame for Hd if there exist con- stants 0 < A≤ B <∞ such that A||x||2 ≤ kX i=1 |⟨x, φi⟩|2 ≤ B||x||2 (1) for all x ∈ Hd where ⟨·, ·⟩ is the dot-product. The constants A and B are the lower and upper frame bounds. The sandwich expression suggests that x will not be poorly distorted when we calculate its inner products with a frame. When A = B, ϕ is called a A-tight frame. When A = B = 1, we get a Parseval’s frame. Fig. 1 shows examples of Tight Frames for R2 for different k’s. The lower bound is equivalent to asking that ϕ span H. So, for a frame, we always have k ≥ d. If k = 3d, the redundancy is r = 3. Fusion Frames provide a way for fusing “smaller” frames to construct large frames, offering various efficiency and robustness properties (Eldar & Michaeli, 2008). Formally, Definition 2.2 (Fusion Frames). Let (Wi)k i=1 be a family of subspaces in Hd, and let (wi)k i=1 ⊆ R+ be a family of weights. Then, ((Wi, wi))k i=1 is a fusion frame for Hd, if 2Flexible Low-Bit Quantization for Transformers Figure 1.Examples of Tight frames of k = 4, 5, ...,11 in R2 there exists constants 0 < A≤ B <∞ such that A||x||2 ≤ kX i=1 w2 i ||Ui(x)||2 ≤ B||x||2 for all x ∈ Hd where Ui denotes the orthonormal projection onto the sub- space Wi for each i. The constants A and B still denote the lower and upper fusion frame bounds respectively. Similar to the Frames case, the Fusion Frame((Wi, wi))k i=1 is referred to as a tight fusion frame if A = B and as a Parseval fusion frame if A = B = 1. Finally, if wi = 1 for all i, we simply utilize the notation (Wi)k i=1. 2.1. Operators in Fusion Frames Fusion Frame (FF) operators can be formally defined using a Hilbert direct sum. Since we use the operators for model quantization, without loss of generality, we describe them in terms of vectors and matrices, to keep notations simple. Let ((Wi, wi))k i=1 be a Fusion Frame for Hd with orthonormal basis (Pi)k i=1 respectively. The Analysis operator TW takes a signal x ∈ Hd and com- putes its dot product with all the basis (Pi)k i=1. The results represent x w.r.t. the FF as TW : x 7→ (wiPT i (x))k i=1 (2) The Synthesis operator T ∗ W is the adjoint of the analysis operator, and takes a sequence of representation vectors (yi)k i=1 and outputs a signal in Hd: the reconstruction of the original signal from its FF representation is defined as T ∗ W : (yi)k i=1 7→ kX i=1 wiPi(yi) (3) The Fusion Frame operator SW is defined as the compo- sition of these two operators. It first computes the FF rep- resentation of a signal in Hd in different subspaces using the Analysis operator. Then, when needed, we can recon- struct the signal back from these representations using the Synthesis operator. When the Fusion Frame is tight, the reconstruction is exact (Casazza et al., 2011). Formally, SW = T ∗ WTW : x 7→ kX i=1 w2 i Ui(x) (4) Here, Ui = PiPiT is the orthogonal projection onto the subspace Wi. If the Fusion Frame is tight, we have SW = AId where Id is the d × d Identity Matrix. Throughout, we will use Parseval Fusion Frames, where the frame bounds A = B = 1. Fusion Frames offer many other properties but due to space, we will keep the presentation focused. How will Fusion Frames be used? An easy way to see Fusion Frames in practice is to work out a simple example, Example 1. Consider the Euclidean space Hd = R4. Say, an oracle gives us a Fusion Frame where we have k = 3 subspaces, and each subspace is of equal dimension ρ = 2. For notational ease, we represent these subspaces with their Synthesis operator T ∗ W =     0.57 0 .00 0.00 0 .57 0.57 0 .00 0.00 0 .57  ,   0.57 0 .00 0.00 0 .57 −0.28 0 .50 −0.50 −0.28  ,   0.57 0 .00 0.00 0 .57 −0.28 −0.50 0.50 −0.28     We want to compute the FF representation of a signal x = \u0002 −1 −0.5 0 .5 1 \u0003T . To do so, we must apply the Analysis operator TW on x. The Analysis operator is simply based on the individual transposes in T ∗ W defined above. \u0014 0.57 0 .00 0 .57 0 .00 0.00 0 .57 0 .00 0 .57 \u0015 , \u0014 0.57 0 .00 −0.28 −0.50 0.00 0 .57 0 .50 −0.28 \u0015 · · · Applying TW on x, we get the FF representations TW(x) = \u0012\u0014−0.28 0.28 \u0015 , \u0014−1.22 −0.32 \u0015 , \u0014−0.22 −0.82 \u0015\u0013 To get the actual projections ofx onto different subspaces Wi, we multiply these coefficients with the scaled orthonor- mal basis (wiPi)k i=1 of their corresponding subspaces (w2 i Ui(x))3 i=1 =     −0.1667 0.1667 −0.1667 0.1667  ,   −0.7053 −0.1890 0.1890 0.7053  ,   −0.1280 −0.4777 0.4777 0.1280     We can verify by checking the identity SW = Id or check- ing that P3 i=1 w2 i Ui(x) = x (only accurate up to rounding errors) that this Fusion Frame is a Parseval’s frame. Ap- plying the Synthesis operator T ∗ W on the projections above recovers x perfectly. Corrupting FF representations by noise.What happens when the Fusion frame representations are corrupted by noise, say due to erasure or quantization? Because of re- dundancy in the representation of a signal, we expect some immunity to corruptions in the representations due to noise. 3Flexible Low-Bit Quantization for Transformers In the current example, this is indeed the case. If we add noise to TW(x) with an SNR of 10dB and use the noisy coefficients to reconstruct x back, we observe an MSE re- duction of 33% at a redundancy factor of r = 1.5× and 50% MSE reduction r = 2×, consistent with theory (Goyal et al., 1998). Quantizing Transformer layers. Let us consider quan- tizing each layer in a Transformer model as in (Nagel et al., 2020; Frantar & Alistarh, 2022; Frantar et al., 2023; Yuan et al., 2022), e.g., by quantizing individual weights or columns, one by one. First, notice that the quantization error/noise is weight-dependent. Further, the error will also depend on how all other weights are quantized. The only way to guide a quantization scheme is the evaluation of a loss (to be described shortly) on a small calibration dataset D. In this regime, even with strong assumptions on the noise, it is difficult to say much about the quality of the de- quantization. On the other hand, far more is known (Goyal et al., 1998; Waldron, 2019; Casazza & Kutyniok, 2012) about the behavior of quantization of data given in an appro- priate Frame basis (e.g., Fusion Frames), and error bounds on the reconstruction are available. Put simply, quantiza- tion noise in the space of Frame projections incurs far less error in the reconstructions due to the robustness of Frame representations. §3 will leverage this principle. 2.2. Tight Fusion Frames and their construction We first define the type of Fusion Frames we will use and then describe how they can be constructed. Definition 2.3 (Tight Fusion Frames or TFF). For A >0 and with Id giving the d × d Identity matrix, a (k, ρ, d)- TFF is a sequence {Ui}k i=1 of d × d orthogonal projection matrices of rank ρ and scalars {wi}k i=1, wi > 0 such that kX i=1 w2 i Ui = AId. (5) A (k, ρ, d)-TFF is a sequence of k equidimensional sub- spaces of dimension ρ in a d-dimensional space, and Ui is the orthogonal projection matrix onto the ith sub-space. Constructing TFFs. The algorithm in (Casazza et al., 2011) can be used to generate TFFs if we provide the dimension d, the number k of subspaces we need, and the dimension ρ of each of these subspaces. The algorithm has two main steps. First, one generates a Tight Frame of d unit norm vectors for the complex domain Cρ. Then, this Frame is modulated with the square roots of unity to generate the k subspaces for Cd. We use a simple construction described in (Fickus et al., 2023) to extend these Fusion Frames to Rd. Since it can be used as a black-box module, we skip the details and include a brief synopsis in Appendix §J. Remarks. A few properties are useful to note. This Fusion Frame construction is sparse/block diagonal and can be gen- Figure 2.Illustration of standard calculation (on top) versus the corresponding calculations in FF space (bottom) erated one subspace at a time. To generate another Fusion Frame, we can hit it with a random rotation. Depending on the Transformer model at hand, the dimension of the acti- vations of the layer determines d. For a desired redundancy factor (k × ρ ≥ d) in our frames, given d we simply choose a k and ρ such that they are valid (i.e., a TFF exists for the triple (k, ρ, d)) according to (Casazza et al., 2011). If not, we use a slightly lower redundancy factor r knowing that we will always have a trivial solution for k = 1 and ρ = d. 3. Fusion Frames based Quantization We can now leverage the ideas described in the preceding sections for quantizing the parameters of a Transformer model. Consistent with common PTQ approaches (Nagel et al., 2020; Frantar & Alistarh, 2022; Frantar et al., 2023; Yuan et al., 2022), we perform quantization layer-by-layer, minimizing the proxy loss between the quantized and non- quantized output of the layer. What are analogous calculations in FF space? Consider a layer l in a Transformer model, with parameters Θl. Let ˘Aprev be the activation of the already quantized previous layer for the examples in the calibration set D. The (non- quantized) output Zl of layer l is Zl = Θl ˘Aprev (6) Here, Θl maps the input ˘Aprev to the output Zl. To avoid directly quantizing Θl, we want the quantization noise to instead impact the analogous terms in the Fusion Frame representation (but equivalent calculation as (6)). To this end, let us set up some notations. In general, the dimension of Zl and ˘Aprev may not be the same. So, the number of subspaces in their respective Fusion Frames will be differ- ent. Let k, kprev denote the number of subspaces for Zl and ˘Aprev respectively. In other words, Wl = ( Wl i)k i=1 and Wprev = (Wprev i )kprev i=1 . Let the sequence of orthonor- mal basis for the subspaces of Wl and Wprev be given by (Pl i )k i=1 and (Pprev i )kprev i=1 respectively. To reduce notational clutter, we absorb the scalars wi into Pi. To write down the expression in FF space, for simplicity, let us vectorize the 4Flexible Low-Bit Quantization for Transformers set of orthogonal basis above and define Pl = [Pl 1Pl 2 . . . Pl k] and Pprev = [Pprev 1 Pprev 2 . . . Pprev kprev ] Taking the FF representations of the output Zl means PT l Zl = PT l Θl ˘Aprev| {z } =Zl (7) Rearranging brackets, PT l Θl ˘Aprev = PT l Θl(PprevPT prev) ˘Aprev (8) = (PT l ΘlPprev)(PT prev ˘Aprev) (9) In the above expression, the object (PT l ΘlPprev) maps the FF representation of ˘Aprev, i.e., (PT prev ˘Aprev), to the FF representation of (PT l Zl). This operation is completely in the FF representation space as desired. A notation simplification allows us to cross-reference what our FF-space calculations are doing w.r.t. the objective function. Let Cprev = PT prev ˘Aprev and Dl = PT l ΘlPprev. Our objective is to quantize Dl to ˆDl while minimizing the proxy loss in terms of FF representations, L( ˆDl) = ||DlCprev − ˆDlCprev||2 F = tr((Dl − ˆDl)T CT prevCprev(Dl − ˆDl)) = tr((Dl − ˆDl)CprevCT prev(Dl − ˆDl)T ) The term ˜H = CprevCT prev corresponds to the Hessian prominent in most published results on PTQ strategies (Nagel et al., 2020; Frantar & Alistarh, 2022; Frantar et al., 2023; Chee et al., 2023). So, our loss is the same as other approaches, except that we are operating in the FF represen- tation space and enjoy all the associated noise robustness properties. Further, because the loss for quantizing the trans- formed weights Dl is the same as e.g., (Frantar et al., 2023), we can directly use the Hessian-based iterative quantization algorithms in (Frantar & Alistarh, 2022; Frantar et al., 2023) with minimal changes. Finally, following recent results in Post-training Quantization (Nagel et al., 2020; Frantar & Alistarh, 2022; Frantar et al., 2023; Chee et al., 2023) we primarily focus on quantizing the transformed weights (Dl) but include one experiment with a simple activation quan- tization in §F. We note that there are standalone activation quantization strategies for smaller Vision models for up to four bits, see (Ding et al., 2022; Yuan et al., 2022). Details of the quantization procedure.Other than working in the FF space, the quantization itself is almost identical to (Frantar et al., 2023). We use the iterative method from (Frantar et al., 2023) with some modifications to improve the stability of our algorithm. For example, we found that clipping the weights before calling the iterative scheme Figure 3.Inference for a FrameQuant quantized model. from GPTQ reduces the weight range during quantization. This effectively adds more quantization noise to the outlier weights that are too large. Since Fusion Frames spread out the energy uniformly among different subspaces, we observe that there are only a few outliers in the transformed Weight matrices, and hence clipping them boosts performance. We found that simply clipping the weights at 2σ (assuming a Normal distribution), where σ is the standard deviation of Dl, works well in practice. We observe that this change also helps the method in (Chee et al., 2023) (and this modified algorithm is also included in our baselines). Alg. 1 shows the sequence of steps in FrameQuant. Algorithm 1 FrameQuant Require: Weight matrix Θl, previous layer activations ˘Aprev, input and output Fusion Frames Pl, Pprev, block size B 1: Compute Cprev = PT prevAprev, Dl = PT l ΘlPprev 2: Compute σ = std(Dl), µ = mean(Dl) 3: Dl = 2σ clip(Dl, µ− 2σ, µ+ 2σ) 4: ˆDl = quantize(Dl, Cprev, B) // modified GPTQ 5: Store ˆDl // store the quantized matrix ˆDl return Pl ˆDlCprev // return quantized layer activations 3.1. Robustness of Fusion Frames We now state some technical results that apply to both Frames and Fusion Frames. (a) Redundancy related guarantees. During quantization, the Fusion Frame coefficients are corrupted. This can be modeled as an additive noise being added to these coeffi- cients. Assume that the redundancy factor is r >1. Even with classical analysis, the result in (Rozell & Johnson, 2005; Goyal et al., 1998) shows that when using Tight Frames to reconstruct the signal from noisy coefficients, for memoryless quantization, we get an MSE reduction of O(1/r). A rate of O(1/r2) for consistent reconstruction can also be achieved by solving an LP during the dequan- tization step (Goyal et al., 1998). While this may not be preferred in practice, we know that if adopted, this matches 5Flexible Low-Bit Quantization for Transformers the lower bound of (1/r2), see Ch. 2 in (Goyal et al., 1998). (b) Another benefit of Frame representations is that recon- struction can “denoise” using filters available in closed form. For example, with Tight Frames, it is known that the Wiener filter provably minimizes the MSE, see Ch. 13 in (Casazza & Kutyniok, 2012), (Kutyniok et al., 2009). In our exper- iments, we found that even a diagonal approximation of the Wiener filter helps. But our experimental results are reported without utilizing this boost. 3.2. Inference Procedure During inference, the quantized model is loaded into mem- ory. At each layer, the inputs to the layer ( ˘Aprev) are first transformed into their Fusion Frame representations using the analysis operator PT prev. The FF representations are then transformed by the quantized weights (Dl) for this layer into the FF representations of the output. Finally the synthesis operator Pl is used to compute the layer outputs. Figure 3 shows this dequantization process and the bit widths of each of these operations for a single layer in a network. 4. Experiments We performed an extensive set of experiments comparing FrameQuant with several quantization baselines for Vision models and Language models. The goal is to assess (a) per- formance metrics of different methods on benchmark tasks and (b) how close low-bit quantization can approach the full precision performance with a small degree of represen- tation redundancy. We use image classification task (Deng et al., 2009) for Vision models and Perplexity for Language models. We start with an overview of our experimental setup. We present the evaluation results of FrameQuant on 15+ Vision Transformer architectures+configurations for image clas- sification. Next, we conduct an ablation study on image classification task to better understand the behavior of dif- ferent components of FrameQuant. We then present results on Language models such as OPT (Zhang et al., 2022b) and Llama2 (Touvron et al., 2023) by comparing perplexity and accuracy in downstream tasks. The appendix includes many additional experiments. 4.1. Experimental Setup We evaluate our method on the ImageNet-1K classification task. For quantizing the model weights of the pre-trained models obtained from the Huggingface hub (Wightman, 2019), we use 128 images randomly selected images from the training dataset as calibration dataset D. We quantize the parameter matrices of the layers sequentially from shal- low layers to deep layers, similar to (Frantar et al., 2023). After quantizing each layer, we pass the inputs to the layer again and send the output with the quantized weights to the next layer. Finally, we evaluate the quantized models on the ImageNet-1K validation dataset and report the top-1 accu- racy. All our “base” experiments correspond to 2 bits. We note that one of the baselines, PTQ4ViT (Yuan et al., 2022), performs activation quantization together with weight quan- tization, but was not tested in the extreme 2 bit quantiza- tion setting. To ensure fair comparisons to that method, we switch off activation quantization in their method and also add another experiment with 3 bits. For additional ex- periments with activation quantization, Segmentation and Object Detection tasks, we refer the reader to the Appendix sections F, G respectively. 4.2. Results on ImageNet Classification Task We use model architectures (including ViT (Dosovitskiy et al., 2021), DeiT (Touvron et al., 2021), DeiT III (Touvron et al., 2022), and Swin (Liu et al., 2021a)) and model sizes (including small, medium, large, huge) that are available on the Huggingface hub (Wightman, 2019). Our main results for these experiments are shown in Tab. 1–2. Figure 4a shows the performance of the different classes of models on the ImageNet-1K dataset. We observed that clipping the weights at 2σ also helps QuIP (Chee et al., 2023), so we include it as an additional baseline. Even with a redundancy factor of r = 1, FrameQuant achieves better accuracy com- pared to most baselines under consideration. Further, with a redundancy factor of r = 1.1, FrameQuant outperforms all baselines by a good margin and is respectably close to the full precision model, underscoring the robustness of Fusion Frames in the presence of quantization noise. We observe that adding more redundancy to the Frame representations continues to improve the performance of the quantized mod- els, especially when the models are small. See §A for more details. We note that the codebase for PTQ4ViT (Yuan et al., 2022) was not compatible with the Swin-L model, so we could not report their performance for this model. 4.3. Ablation Study In this section, we dissect FrameQuant to understand the contribution of different components of our algorithm. Ta- ble 3 shows the results of this experiment. We use GPTQ (Frantar et al., 2023) as our starting point. With GPTQ (Frantar et al., 2023) alone, the performance drops in the quantized models are significant: as high as 82% for the DeiT III (Touvron et al., 2022) Base model. Simply with the FF representation added (column TFF), we see improve- ments in performance across all models, with a maximum improvement of 56% for DeiT III-H. We note that some of the smaller-sized models are yet to see all the benefits of FF representations. That is because these models have outliers in the weights (much larger than the remaining weights) which results in higher quantization errors. The FF repre- 6Flexible Low-Bit Quantization for Transformers Method #bits ViT DeiT Swin T S S/32 B T S B S B B/384 Full-Precision 32 75.46 81.39 75.99 85.10 72.16 79.85 81.98 82.88 84.67 86.02 PTQ4ViT 2 0.33 0.55 0.71 0.41 1.51 4.47 25.54 12.54 0.15 0.15 GPTQ 2 0.40 0.40 0.39 29.26 1.60 4.23 41.00 43.54 47.38 57.52 QuIP 2 1.42 21.98 19.00 77.54 12.93 51.62 75.51 71.58 74.91 79.85 QuIP (with our 2σ clip) 2 9.10 48.96 41.41 79.54 30.49 65.70 77.69 76.34 79.17 82.40 FrameQuant (r = 1.0) 2 8.92 48.10 41.16 79.53 31.73 66.35 77.62 77.91 80.16 82.44 FrameQuant (r = 1.1) 2.2 25.79 61.51 53.85 80.93 46.48 70.43 78.67 78.77 81.33 83.42 PTQ4ViT 3 18.32 36.18 22.20 21.43 51.73 69.65 75.35 73.01 69.46 70.68 Table 1.ImageNet-1k Top-1 validation accuracy of Tiny to Base sized Vision Transformer-based models when quantized to 2 (or 3) bits by different methods. FrameQuant with a redundancy factor of r = 1already performs better or on par with Quip (Chee et al., 2023). With a slightly higher redundancy factor of r = 1.1, we get the best performance of all the methods under consideration. (a) Validation accuracies for different classes of Transformer models for Vision on ImageNet-1K (b) Weights distribution in a ViT layer and the 2σ thresholds Figure 4.(a) Validation accuracies of Vision Transformers on ImageNet-1K dataset. We can see FrameQuant closing the gap between the full precision model with increasing redundancy. Each dot in the plot corresponds to a model from tables 1-2 combined. (b) shows the distribution of weights in a ViT layer and the 2σ thresholds for clipping. We see that our thresholding keeps most of the mass while removing outliers. Method #bits ViT DeiT III Swin L H L H L Full-Precision 32 85.84 87.59 86.97 87.19 85.95 PTQ4ViT 2 37.05 00.18 2.14 55.57 - GPTQ 2 63.08 42.63 68.43 28.20 71.69 QuIP 2 82.22 84.58 84.76 86.27 83.61 QuIP (our 2σ clip) 2 83.17 85.31 85.48 86.38 84.27 FrameQuant (r = 1.0) 2 83.22 85.49 85.45 86.62 84.25 FrameQuant (r = 1.1) 2.2 83.67 85.99 85.75 86.68 84.42 PTQ4ViT 3 81.26 78.92 83.63 85.39 - Table 2.ImageNet-1K Top-1 validation accuracy of Large and Huge sized Vision Transformer-based models when quantized to 2 (or 3) bits by different methods. sentation yields a nice enough distribution that we can fit a Normal distribution. So, after we clip these weights at the ±2σ level, we see improvements in performance because of the outlier removal. Clipping is most effective once the weights are nicely distributed. A direct application of clip- ping on the weights has limited efficacy and incurs errors for weight configurations that are degenerate/poorly distributed, see D.1 for more details. Finally, we add a redundancy fac- tor of r = 1.1 and the FF representations take advantage of this redundancy: we see the best performance across the board. Impact of Gaussian assumption on the weights distri- bution. Figure 4b shows a representative example of the distribution of weights in a model from the ViT family and why the 2σ clipping seems reasonable for capturing most of the mass. The weights distribution for models from DeiT and Swin Transformer are shown in Figure §13. 4.4. Results on Language Models In this experiment, we evaluate the perplexity of quantized models from the OPT (Zhang et al., 2022b) and Llama2 (Touvron et al., 2023) family on two datasets - WikiText2 (Merity et al., 2017) and C4 (Raffel et al., 2020). Figure 5 shows the perplexity of models from the OPT family as the size is increased. We see that FrameQuant at 1× redun- dancy performs better than all other quantization methods. With a redundancy factor of 1.1×, FrameQuant reduces the performance gap with the full precision models as suggested by the theory. We see similar results for models from the Llama2 family as well. We also finetuned the Llama2-7B model quantized by various methods on diverse downstream 7Flexible Low-Bit Quantization for Transformers GPTQ TFF 2σ clip Redundancy ViT DeiT III Swin r = 1.1 S B H S B H S B L ON OFF OFF OFF 0.4 29 .26 42 .63 0 .45 8 .5 28 .2 43 .54 47 .38 71 .69 ON ON OFF OFF 0.88 59 .87 68 .75 1 .48 29 .92 84 .33 61 .01 60 .21 79 .52 ON ON ON OFF 48.10 79 .53 85 .49 51 .13 77 .99 86 .62 77 .91 80 .16 84 .25 ON ON ON ON 61.51 80.93 85.99 65.33 80.91 86.68 78.77 81.33 84.42 Full Precision 81.39 85 .1 87 .59 83 .06 85 .7 87 .19 82 .79 84 .7 85 .95 Table 3.Incremental impact of various steps in FrameQuant on ImageNet-1k accuracy for different Transformer models in Vision Method #bits acc mm-acc Full-Precision 32 84.19 84 .67 ZeroQuant 4.33 78.69 78 .07 ZeroQuant 3.66 54.91 56 .45 ZeroQuant 2.66 38.00 38 .30 FrameQuant 2.2 80.02 79.37 Table 4.Performance of the BERT model quantized with Zero- Quant and FrameQuant on the MNLI dataset. FrameQuant per- forms better than ZeroQuant even with a lower bit-width than ZeroQuant. Method bits OPT Llama2 125M 1.3B 2.7B 6.7B 7B 70B Full-Precision 16 27.65 14.62 12.47 10.86 5.68 3.32 GPTQ 2 5.7e3 8.9e3 9.1e3 3.1e3 6.4e3 140.5 QuIP 2 913.0 37.59 22.86 15.67 26.02 6.21 FrameQuant 2 345.7 30.54 20.67 15.72 14.85 5.50 FrameQuant 2.2 131.2 22.68 15.86 13.53 8.48 4.67 Table 5.Perplexity (lower is better) of Llama2 and OPT models on WikiText2 dataset when quantized to 2 (or 2.2) bits by different methods. tasks and observed a maximum accuracy boost of 41% by FrameQuant at r = 1.1× compared to vanilla GPTQ. Table 5 summarizes the perplexity of all the models on the Wiki- Text2 (Merity et al., 2017) dataset. Results on downstream tasks/additional datasets is in Appendix §H. 4.5. Comparision with Mixed-precision Quantization A redundancy factor of 1.1 is the same as an average bit- width of 2.2 per weight parameter. Mixed-precision quan- tization methods can achieve fractional bit-widths by us- ing different bit-widths for different weights in the model. We compare FrameQuant with a recent Mixed-precision method, ZeroQuant (Yao et al., 2022). We test FrameQuant with a bit-width of 2 and a redundancy factor of 1.1 relative to ZeroQuant at different fractional bit-widths. As shown in Table 4. FrameQuant performs favorably with ZeroQuant, even at low bit widths. (a) Perplexity on WikiText2  (b) Perplexity on C4 Figure 5.Perplexity of models from OPT family on WikiText2 and C4 datasets. FrameQuant performs better than all other quan- tization methods under consideration. We can also see that the performance gap between the quantized models and the unquan- tized model goes down as the size of the models increases. Llama2 7B Llama2 13B Original model 13G 25G FrameQuant 2.1G 3.6G Table 6.Size of original and quantized model with FrameQuant. 5. Other Practical Considerations 5.1. Storage requirements Weight quantization has a direct improvement on the storage needs of the models. Table 6 shows the sizes of compressed Llama2 models. FrameQuant reduces the size of the models by around 85% on average. 5.2. Inference speeds Since FrameQuant involves additional operations to com- pute and transform the weights from the low-bit Fusion Frame representations to the regular weight space, the raw inference speed is expected to be lower than GPTQ. On the other hand, at 2 bits, the accuracy/perplexity of FrameQuant is much better than GPTQ. So, there is a trade-off. Table 7 shows the inference speeds of the quantized models on a Nvidia A100 GPU. Here, we used the block diagonal struc- ture of Fusion Frames and a Hadamard transform-based fast random projection based on (Dao, 2023; Zeng et al., 2023) for the rotation matrices. This inference speed can be improved by using efficient kernels to load the weights into the GPU and perform the transformations. 8Flexible Low-Bit Quantization for Transformers Method Llama2 7B Llama2 13B GPTQ 1425.07t/s 844.03t/s FrameQuant 974.20t/s 607.01t/s Table 7.Inference speed in tokens/sec (t/s) of quantized models with GPTQ and FrameQuant. 6. Discussion We cover a few additional aspects that were not explicitly discussed thus far. (1) Can we reduce to one bit? We per- formed experiments with redundancy of 1.8× with 1 bit per weight but were unsuccessful. For one bit, once the redundancy has exceeded r = 2, it makes more sense to just use two bits. (2) Can FrameQuant run as QuIP? For each layer, if we choose a Fusion Frame with a redundancy factor r = 1 and the random orthonormal basis Pl, Pprev, we get a setup similar to QuIP (Chee et al., 2023) after removing the 2σ weight clipping. This is also why when QuIP is augmented with our 2σ clipping we see similar results to FrameQuant with 1× redundancy. (3) Additional storage needed?: Since there are efficient deterministic al- gorithms to generate Fusion Frames, during inference, only knowledge of (k, ρ, d) is needed. For rotations, we only need knowledge of the seed. Also, since many layers in a Transformer model have the same shape, these parameters can be shared across layers. Additional details on the stor- age benefits are in §K.1 (4) Why is flexibility useful? If the performance hit at the two-bit level is unacceptable for an application, the only recourse currently is to move up to three bits for existing methods ( 50% increase). However, FrameQuant allows flexibility through the choice of the re- dundancy factor r. (5) Higher bitwidths? The main focus of this work is to evaluate 2-bit quantization of the weights in Vision and Language models and to check the benefits of applying Fusion Frames in terms of flexible bit-widths. Higher bit widths such as 3 or 4-bit quantization have been studied (Frantar et al., 2023; Chee et al., 2023) and also used in practice (Gerganov, 2023). (6) Computational complexity during Inference: The core FF-related compute is similar to alternatives (Chee et al., 2023) with a small overhead related to the number of subspaces k. During inference, we need an additional compute of O(d2(kr + logd)) for transforming the weights from the Fusion Frame representation space to the regular weight space. Any quantization scheme in the low-bit regime will incur a cost of O(d2) to transform the quantized weights by scaling and shifting them. More de- tails are provided in §K.2. (7) Quantization aware training: FrameQuant can be modified to be applicable during QAT although we do not include such experiments here. One option is to use it during fine-tuning where the quantization loss is simulated, which can then be used to regularize the loss to make it more robust to quantization. Fusion Frames can meaningfully inform this bias, via an estimate of the “out of subspace error” to minimize degradation due to quan- tization. (8) Scaling laws vis- `a-vis FrameQuant? During quantization, the number of parameters does not change. Instead, each parameter has a lower degree of freedom since the number of states it can represent is reduced. We can use the (number of parameters × bit-width) as a proxy for the degree of freedom for each (quantized) model. Taking the quantization bit width into account, a line plot of test loss (on the vertical-axis) as a function of (number of parameters × bit-width) on the horizontal axis may have a different slope compared to (Kaplan et al., 2020), Fig. 1. (9) Ratio- nale for clipping: Let u be a vector in p dimensions. Let P be a projection onto a random subspace in p′ dimensions. Projecting u using P gives v as v = Pu. Assume that the entries in u have finite mean and variance and are uncorre- lated. Then each entry of v is effectively a sum of many scaled random variables. The distribution of these entries (sum of scaled variables, suitably standardized) approaches a normal distribution as the dimensionality p grows. Weak dependence or mixing can also be handled. 7. Conclusions This paper describesFrameQuant, a Frames based algorithm for flexible low-bit quantization. Quantization is motivated by the need to efficiently serve Large Language Models on heterogeneous devices and flexibility here means that while we retain the option to go as low as two bits; depend- ing on the needs of the downstream task, the user also has the flexibility to seek models with a net footprint of 2.x bits on average. Across most widely used Vision Trans- former models and Large Language models, we find that effective quantization is possible with only a small loss in performance relative to the full-precision model. Further, flexibility for a minor increase in redundancy is available and uniformly helps close the gap with full precision models. We observe, consistent with the literature, that quantization to low bit width is more favorable for larger models (in terms of a performance hit) than a similar quantization ap- plied to smaller models. While some benefits (e.g., model loading time, loading larger models) are immediate, tighter integration with the hardware can unlock far more efficiency gains. The code is publicly available. Impact Statement This paper introduces a low precision quantization method for inference. The objective is to decrease memory needs and facilitate the implementation of larger models on less powerful devices, thereby reducing costs (economic impact) and the carbon footprint (environmental impact). We have not identified any particular ethical issues that need to be emphasized. 9Flexible Low-Bit Quantization for Transformers Acknowledgments Support through the Google Cloud Platform provided the computational resources for conducting our experiments. The research was also supported in part by a Google gift award to UW-Foundation and funding from the Vilas Board of Trustees. References Banner, R., Nahshan, Y ., and Soudry, D. Post training 4-bit quantization of convolutional networks for rapid- deployment. Advances in Neural Information Processing Systems, 32, 2019. Bisk, Y ., Zellers, R., Gao, J., Choi, Y ., et al. Piqa: Reasoning about physical commonsense in natural language. In Pro- ceedings of the AAAI conference on artificial intelligence, 2020. Boufounos, P., Kutyniok, G., and Rauhut, H. Compressed sensing for fusion frames. Proceedings of SPIE - The International Society for Optical Engineering, 10 2009. doi: 10.1117/12.826327. Casazza, P. G. and Kovaˇcevi´c, J. Equal-norm tight frames with erasures. Advances in Computational Mathematics, 18, 2003. Casazza, P. G. and Kutyniok, G. Finite frames, theory and applications, 2012. URL https: //link.springer.com/book/10.1007/ 978-0-8176-8373-3 . Casazza, P. G., Kutyniok, G., and Li, S. Fusion frames and distributed processing. Applied and computational harmonic analysis, 25(1), 2008. Casazza, P. G., Fickus, M., Mixon, D. G., Wang, Y ., and Zhou, Z. Constructing tight fusion frames. Applied and Computational Harmonic Analysis, 30(2), 2011. ISSN 1063-5203. doi: https://doi.org/10.1016/j.acha.2010.05. 002. URL https://www.sciencedirect.com/ science/article/pii/S1063520310000850. Chang, H., Zhang, H., Jiang, L., Liu, C., and Freeman, W. T. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2022. Chee, J., Cai, Y ., Kuleshov, V ., and Sa, C. D. QuIP: 2-bit quantization of large language models with guarantees. In Thirty-seventh Conference on Neural Information Pro- cessing Systems, 2023. URL https://openreview. net/forum?id=xrk9g5vcXR. Chen, S. and Zhao, Q. Shallowing deep networks: Layer- wise pruning based on feature representations. IEEE Transactions on Pattern Analysis and Machine Intelli- gence, 41(12):3048–3056, 2019. doi: 10.1109/TPAMI. 2018.2874634. Cheng, B., Misra, I., Schwing, A. G., Kirillov, A., and Gird- har, R. Masked-attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022. Christensen, O. An introduction to frames and riesz bases, 2018. URL https://link.springer. com/book/10.1007/978-3-319-25613-9 . Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Dao, T. fast-hadamard-transform, 2023. URL https://github.com/Dao-AILab/ fast-hadamard-transform. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, 2009. Ding, Y ., Qin, H., Yan, Q., Chai, Z., Liu, J., Wei, X., and Liu, X. Towards accurate post-training quantization for vision transformer. In Proceedings of the 30th ACM International Conference on Multimedia, MM ’22, New York, NY , USA, 2022. ISBN 9781450392037. doi: 10. 1145/3503161.3547826. URL https://doi.org/ 10.1145/3503161.3547826. Donoho, D., Vetterli, M., DeV ore, R., and Daubechies, I. Data compression and harmonic analysis. IEEE Transactions on Information Theory, 44(6), 1998. doi: 10.1109/18.720544. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. URL https:// openreview.net/forum?id=YicbFdNTTy. Eldar, Y . C. and Michaeli, T. Beyond bandlimited sampling: Nonlinearities, smoothness and sparsity. ArXiv, abs/0812.3066, 2008. URL https://api. semanticscholar.org/CorpusID:8702589. 10Flexible Low-Bit Quantization for Transformers Fickus, M., Iverson, J. W., Jasper, J., and Mixon, D. G. Harmonic grassmannian codes. Applied and Computational Harmonic Analysis , 65, 2023. ISSN 1063-5203. doi: https://doi.org/10.1016/j.acha.2023.01. 009. URL https://www.sciencedirect.com/ science/article/pii/S1063520323000106. Frantar, E. and Alistarh, D. Optimal brain compression: A framework for accurate post-training quantization and pruning. Advances in Neural Information Processing Systems, 35, 2022. Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. OPTQ: Accurate quantization for generative pre-trained transformers. In The Eleventh International Confer- ence on Learning Representations, 2023. URL https: //openreview.net/forum?id=tcbBPnfwxS. Gao, L., Tow, J., Abbasi, B., et al. A framework for few- shot language model evaluation, 2023. URL https: //zenodo.org/records/10256836. Gerganov, G. llama.cpp, 2023. URL https://github. com/ggerganov/llama.cpp. Gholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M. W., and Keutzer, K. A survey of quantization methods for effi- cient neural network inference. In Low-Power Computer Vision. Chapman and Hall/CRC, 2022. Goyal, V ., Vetterli, M., and Thao, N. Quantized overcom- plete expansions in Rn: analysis, synthesis, and algo- rithms. IEEE Transactions on Information Theory, 44(1), 1998. doi: 10.1109/18.650985. Goyal, V . K., Kova ˇcevi´c, J., and Kelner, J. A. Quan- tized frame expansions with erasures. Applied and Computational Harmonic Analysis, 10(3), 2001. ISSN 1063-5203. doi: https://doi.org/10.1006/acha.2000. 0340. URL https://www.sciencedirect.com/ science/article/pii/S1063520300903403. Han, S., Mao, H., and Dally, W. J. Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding. In Bengio, Y . and Le- Cun, Y . (eds.),4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1510.00149. Hassibi, B., Stork, D. G., and Wolff, G. J. Optimal brain sur- geon and general network pruning. In IEEE international conference on neural networks. IEEE, 1993. Hinton, G., Vinyals, O., and Dean, J. Distilling the knowl- edge in a neural network, 2015. Hudson, D. A. and Zitnick, L. Generative adversarial trans- formers. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learn- ing, volume 139 of Proceedings of Machine Learning Research. PMLR, 18–24 Jul 2021. Kaplan, J., McCandlish, S., Henighan, T., et al. Scaling laws for neural language models, 2020. Kutyniok, G., Pezeshki, A., Calderbank, R., and Liu, T. Robust dimension reduction, fusion frames, and grassmannian packings. Applied and Computational Harmonic Analysis , 26(1):64–76, 2009. ISSN 1063- 5203. doi: https://doi.org/10.1016/j.acha.2008.03. 001. URL https://www.sciencedirect.com/ science/article/pii/S1063520308000249. Le, Q., Sarl´os, T., Smola, A., et al. Fastfood-approximating kernel expansions in loglinear time. In Proceedings of the international conference on machine learning, 2013. Lecun, Y ., Denker, J., and Solla, S. Optimal brain damage. In Advances in Neural Information Processing Systems, volume 2, 01 1989. Li, Z., Xiao, J., Yang, L., and Gu, Q. Repq-vit: Scale repa- rameterization for post-training quantization of vision transformers. In Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision, 2023. Liu, Z., Lin, Y ., Cao, Y ., Hu, H., Wei, Y ., Zhang, Z., Lin, S., and Guo, B. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, 2021a. Liu, Z., Wang, Y ., Han, K., Zhang, W., Ma, S., and Gao, W. Post-training quantization for vision transformer. Ad- vances in Neural Information Processing Systems , 34, 2021b. Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models. In International Conference on Learning Representations, 2017. URL https:// openreview.net/forum?id=Byj72udxe. Nagel, M., Baalen, M. v., Blankevoort, T., and Welling, M. Data-free quantization through weight equalization and bias correction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019. Nagel, M., Amjad, R. A., Van Baalen, M., Louizos, C., and Blankevoort, T. Up or down? Adaptive rounding for post-training quantization. In III, H. D. and Singh, A. (eds.), Proceedings of the 37th International Con- ference on Machine Learning, volume 119 of Proceed- ings of Machine Learning Research. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/ v119/nagel20a.html. 11Flexible Low-Bit Quantization for Transformers Nagel, M., Fournarakis, M., Amjad, R. A., Bondarenko, Y ., van Baalen, M., and Blankevoort, T. A white paper on neural network quantization. ArXiv, abs/2106.08295, 2021. URL https://api.semanticscholar. org/CorpusID:235435934. Namburi, S. S. S., Sreedhar, M., Srinivasan, S., et al. The cost of compression: Investigating the impact of com- pression on parametric knowledge in language models. In Findings of the Association for Computational Lin- guistics: EMNLP 2023. Association for Computational Linguistics, 2023. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., and Liu, P. J. Ex- ploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020. URL http://jmlr. org/papers/v21/20-074.html. Ranftl, R., Bochkovskiy, A., and Koltun, V . Vision trans- formers for dense prediction. In Proceedings of the IEEE/CVF international conference on computer vision, 2021. Rokh, B., Azarpeyvand, A., and Khanteymoori, A. A com- prehensive survey on model quantization for deep neural networks in image classification. ACM Transactions on Intelligent Systems and Technology, 14(6):1–50, Novem- ber 2023. ISSN 2157-6912. doi: 10.1145/3623402. URL http://dx.doi.org/10.1145/3623402. Rozell, C. and Johnson, D. Analysis of noise reduction in re- dundant expansions under distributed processing require- ments. In Proceedings. (ICASSP ’05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005., volume 4, 04 2005. ISBN 0-7803-8874-7. doi: 10.1109/ICASSP.2005.1415976. Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y . Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM , 64(9):99–106, 2021. Scao, T. L., Fan, A., et al. Bloom: A 176b-parameter open- access multilingual language model, 2023. Strohmer, T. and Heath Jr, R. W. Grassmannian frames with applications to coding and communication. Applied and computational harmonic analysis, 14(3), 2003. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jegou, H. Training data-efficient image transform- ers; distillation through attention. In International Con- ference on Machine Learning, volume 139, July 2021. Touvron, H., Cord, M., and J ´egou, H. Deit iii: Revenge of the vit. In European Conference on Computer Vision. Springer, 2022. Touvron, H., Martin, L., Stone, K., et al. Llama 2: Open foundation and fine-tuned chat models, 2023. Waldron, S. F. D. An introduction to finite tight frames, 2019. URL https://link.springer. com/book/10.1007/978-0-8176-4815-2 . Wightman, R. Pytorch image models. https://github. com/rwightman/pytorch-image-models, 2019. Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han, S. SmoothQuant: Accurate and efficient post-training quantization for large language models. InProceedings of the 40th International Conference on Machine Learning, 2023. Yao, Z., Yazdani Aminabadi, R., Zhang, M., et al. Zero- quant: Efficient and affordable post-training quantization for large-scale transformers. In Advances in Neural Infor- mation Processing Systems, 2022. Yu, D., Seide, F., Li, G., and Deng, L. Exploiting sparse- ness in deep neural networks for large vocabulary speech recognition. In 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 4409–4412, 2012. doi: 10.1109/ICASSP.2012.6288897. Yuan, Z., Xue, C., Chen, Y ., Wu, Q., and Sun, G. Ptq4vit: Post-training quantization for vision transformers with twin uniform quantization. In European Conference on Computer Vision, 2022. Yun, C., Chang, Y .-W., Bhojanapalli, S., Rawat, A. S., Reddi, S., and Kumar, S. O (n) connections are expressive enough: Universal approximability of sparse transform- ers. Advances in Neural Information Processing Systems, 33:13783–13794, 2020. Zellers, R., Holtzman, A., Bisk, Y ., Farhadi, A., and Choi, Y . Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Zeng, Z., Davies, M., Pulijala, P., et al. Lookupffn: making transformers compute-lite for cpu inference. In Proceed- ings of the 40th International Conference on Machine Learning, ICML’23. JMLR.org, 2023. Zhai, X., Kolesnikov, A., Houlsby, N., and Beyer, L. Scaling vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. Zhang, B., Haddow, B., and Birch, A. Prompting large language model for machine translation: A case study. In Proceedings of the 40th International Conference on Machine Learning, ICML’23, 2023. 12Flexible Low-Bit Quantization for Transformers Zhang, H., Li, F., Liu, S., Zhang, L., Su, H., Zhu, J., Ni, L. M., and Shum, H.-Y . Dino: Detr with improved denois- ing anchor boxes for end-to-end object detection, 2022a. Zhang, S., Roller, S., Goyal, N., et al. Opt: Open pre-trained transformer language models, 2022b. Zhu, Z., Hong, J., and Zhou, J. Data-free knowledge distil- lation for heterogeneous federated learning. In Interna- tional conference on machine learning, pp. 12878–12889. PMLR, 2021. 13Flexible Low-Bit Quantization for Transformers Appendix In this Appendix, we provide additional details related to the experiments reported in the main paper. This Appendix is organized as follows. In Section A we analyze the impact of redundancy on the performance of the model in terms of classification accuracy on the ImageNet-1K dataset. In Section B, we study this effect on the performance of Vision Transformer models, evaluated using activation maps. Next, in Section C, we study the effect of the size of the calibration data used for quantizing various Vision Models. In Section D, we analyze the choice of the 2σ threshold for clipping the weights during quantization. We provide empirical evidence for different classes of Vision models. We also show that 2σ clipping alone cannot improve quantization performance. On the contrary, it can degrade the performance for weight configurations that are poorly distributed. Section E shows the distribution of weights in the DeiT and Swin Transformer models. In Section F, we present a framework for quantizing activations and show how the FF representation of activations inherently addresses the key pain points described in previous works. We follow this with a simple experiment with activation quantization enabled. In Section G, we provide experiments on Segmentation and Object detection tasks. In Section H, we present more experiments on Language models on different datasets and downstream tasks as mentioned in the main paper. Then, in Section I, we provide an expanded synopsis of the theoretical results that apply to our setting, as briefly described in the main paper. In Section J we provide a brief synopsis of the algorithm used to generate a TFF for the curious reader. Finally in Section K we give a detailed analysis of the storage benefits of FrameQuant and the computational complexity during inference. A. Impact of redundancy in representations We consider the impact of redundancy in our Frame representation moving forward from 2 bits, incrementally increasing redundancy. Table 8 shows the performance of different models at different levels of redundancy. We observe that for large models, the original performance without any redundancy was already high, and adding redundancy did not impact their performance significantly. However, this is not the case for smaller models. Here, we see significant performance improvements (around +21% for the ViT-S model). Redundancy bits ViT DeiT III Swin S B H S B H S B L r = 1.00 (2 .0 bits) 48 .10 79 .53 85 .49 51 .13 77 .99 86 .62 77 .91 80 .16 84 .25 r = 1.05 (2 .1 bits) 56 .19 79 .97 85 .67 58 .74 79 .59 86 .58 78 .47 80 .41 84 .26 r = 1.10 (2 .2 bits) 61 .51 80 .93 85 .99 65 .33 80 .91 86 .68 78 .77 81 .33 84 .42 r = 1.15 (2 .3 bits) 65 .17 81 .27 86 .04 69 .54 81 .69 86 .67 78 .87 81 .88 84 .51 r = 1.20 (2 .4 bits) 66 .53 81 .59 86 .11 71 .07 81 .98 86 .61 79 .56 82 .02 84 .56 r = 1.25 (2 .5 bits) 68 .57 81 .74 86 .06 73 .48 82 .51 86 .55 79 .99 82 .26 84 .51 r = 1.30 (2 .6 bits) 69 .02 81 .77 85 .99 74 .40 82 .54 86 .38 79 .92 82 .39 84 .65 Full Precision - 81.39 85 .1 87 .59 83 .06 85 .7 87 .19 82 .79 84 .7 85 .95 Table 8.Performance of various quantized models on ImageNet-1K classification task as the redundancy in FrameQuant is increased. We see that increasing the redundancy closes the gap between the performance of the quantized model and the Full precision model B. Does redundancy impact attention maps? In the main paper, we discussed how the performance of the models improves as we increase the redundancy in the Fusion Frames during quantization. In this section, we provide additional details on how redundancy affects the attention maps of Vision Transformers from different classes. We will focus mainly on the small and base models where we see significant improvement in the validation accuracy on ImageNet, as we increase the redundancy. Figures 6, 7 and 8 show the attention maps of Vit-S, DeiT III -S, and Deit III - B models respectively. These models show an improvement in the range of 4.55% to 23.27% as we increase the redundancy from r = 1 to r = 1.3. This is reflected in the attention maps as well. We see that as the redundancy is increased, the attention regions concentrate around the objects of interest systematically. This is consistent with the improvement in accuracy and can also be seen in Figure 9. 14Flexible Low-Bit Quantization for Transformers Figure 6.Effect of flexibility/redundancy on activation maps for ViT-S.Figure showing attention maps of ViT-S as the redundancy is increased from r = 1to r = 1.3 in the increments of 0.1 from left to right. The first column shows the image and the ground truth label, and the rest of the columns show the regions that the model is attending to in the final transformer block. We see that as the redundancy is increased, the model gets more focused, with the attention regions concentrating on the objects of interest. #images ViT DeiT III Swin S B H S B H S B L 128 48 .10 79 .53 85 .49 51 .13 77 .99 86 .62 77 .91 80 .16 84 .25 200 51 .48 79 .84 85 .62 53 .74 78 .38 86 .61 77 .66 80 .19 84 .09 256 51 .69 79 .84 85 .74 54 .73 79 .06 86 .47 77 .96 80 .68 84 .31 Table 9.ImageNet-1K Top-1 validation accuracies of models from different classes as the number of calibration images is increased. C. Does the calibration set size matter? In the main paper, we noted that a small calibration set size was sufficient. In this section, we report on experiments varying the number of calibration images and observe the performance of different classes of models on ImageNet-1K. We use a redundancy factor of r = 1 in this experiment. Table 9 shows the validation accuracies for different classes of models as the number of calibration images is increased from 128 to 256. We can see that the performance improvement is seen only in the small-sized models from the ViT and DeiT III classes. So, we will focus on reporting results for these models. Figure 10 shows the accuracies of ViT-S and DeiT III-S models as the number of calibration images is increased from 128 to 512. We 15Flexible Low-Bit Quantization for Transformers Figure 7.Effect of flexibility/redundancy on activation maps for DeiT III-S. Figure showing attention maps of DeiT III -S as the redundancy is increased from r = 1to r = 1.3 in the increments of 0.1 from left to right. can see that there is a small improvement as the number of images is increased from 128 to 200, but the benefits taper off quickly as we increase it further. This shows that if access to the calibration is not limited, a small increase in the number of images used for quantization can benefit the final accuracies of the models, especially for smaller models. D. How does 2σ clipping affect performance? In the main paper, we discussed a simple clipping threshold at the 2σ level. In this section, we analyze the benefits of this choice and its effect on the performance of different classes of models on ImageNet-1K. As in the previous section, we use a redundancy factor of r = 1 for these experiments and focus on the impact of clipping the weights at different levels based on their distribution. Figure 11 shows the accuracies of different classes of models as the threshold for the weights is varied from ±σ to ±3σ. We can see that the performance of all the models peaks in the neighborhood of ±2σ. Clipping at ±σ restricts the range of the weights too aggressively, incurring errors. At ±3σ level, which is close to allowing the entire range, we are stretching the effective scale of the weights to allow all the extreme entries to be represented within the range. This, in turn, increases the width of the quantization levels, which affects the majority of the weights impacting performance. ±2σ seems to be the sweet spot. 16Flexible Low-Bit Quantization for Transformers Figure 8.Effect of flexibility/redundancy on activation maps for DeiT III-B. Figure showing attention maps of DeiT III -B as the redundancy is increased from r = 1to r = 1.3 in the increments of 0.1 from left to right. 1.00 1.05 1.10 1.15 1.20 1.25 1.30 Redundancy (r) 0 20 40 60 80Imagenet 1k Accuracy ViT-S DeiT-S DeiT-B Figure 9.Trend of accuracies in small size models as we increase the redundancy 17Flexible Low-Bit Quantization for Transformers 150 200 250 300 350 400 450 500 # images 0 20 40 60 80ImageNet-1K Accuracy ViT-S DeiT-S Figure 10.Trend of accuracies in small size models as we increase the number of calibration images Model Quantization method WikiText2 C4 Llama2 7B GPTQ without clipping 6.40e3 2.27e3 Llama2 7B GPTQ with clipping 9.45e3 7.40e3 Llama2 7B FrameQuant with clipping 14.85 19.62 Llama2 70B GPTQ without clipping 140.5 68.83 Llama2 70B GPTQ with clipping 2.08e3 1.12e3 Llama2 70B FrameQuant with clipping 5.5 7.85 Table 10.Table showing the impact of clipping on GPTQ. FrameQuant computes the FF representations of the weights that are nicely distributed and can take advantage of clipping to remove outliers. 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 3.00 Clipping Threshold (× ) 0 20 40 60 80Imagenet 1k Accuracy ViT-S DeiT-S Swin-S (a) Accuracies of Small models 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 3.00 Clipping Threshold (× ) 0 20 40 60 80Imagenet 1k Accuracy ViT-B DeiT-B Swin-B (b) Accuracies of Base models 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 3.00 Clipping Threshold (× ) 0 20 40 60 80Imagenet 1k Accuracy ViT-H DeiT-H Swin-L (c) Accuracies of Large models Figure 11.Figure showing the impact of clipping at different thresholds based on σ D.1. Does 2σ clipping alone improve performance? From our alation study 4.3, it might seem that 2σ clipping is doing the heavy lift in improving the performance. However, clipping is most effective once the weights are nicely distributed. A direct application of clipping on the weights has limited efficacy and incurs errors for weight configurations that are degenerate/poorly distributed. Projecting onto a space-filling basis makes clipping effective. To demonstrate this point quantitatively, we run GPTQ on Llama2 models with the 2σ clipping applied directly to the weights. Table 10 shows that the performance degrades when the weights are clipped instead of their Fusion Frame representations as in FrameQuant. E. Distribution of weights in the DeiT and Swin Transformer models This section presents the distribution of the weights in the DeiT and Swin Transformer models. Figure 13 shows the distribution of weights in a linear layer from the DeiT and Swin Transformer families. We can see that the distribution is well behaved and the 2σ threshold captures most of the mass well. 18Flexible Low-Bit Quantization for Transformers (a) Activations of the first block in ViT-M  (b) Activation FF representations of the first block in ViT-M Figure 12.Activations of the first Transformer Block of Vit-M model and their FF representations. We can see the outliers in the activations (shown in red) on the left, while the FF representations are well-behaved. (a) Weight distribution in DeiT  (b) Weight distribution in Swin Transformer Figure 13.Weights distribution in DeiT and Swin Transformer models. F. FrameQuant for Activation Quantization? In the main paper, we restricted the experimental setup of Vision Transformer models to weight quantization for meaningful comparisons to recent PTQ papers. This is because activation quantization in this low-bit regime has not been reported and each baseline will need modifications to report the best possible results. In this section, we provide some details regarding applying FrameQuant for activation Quantization with the caveat that a comprehensive head-to-head comparison to all reported baselines is difficult for the reasons above. Rounding activations to the nearest. For smaller Transformer models, the inference efficiency bottleneck also largely lies in activations. So, we focus on these models to consider activation quantization. We performed activation quantization on ViT-S/B models with a simple rounding to the nearest, and we found that even when the weights are quantized to 2 (or 2.2) bits using FrameQuant, the performance drops are not large. This is promising and shows that FrameQuant is robust in preserving activations even at a 2.x bit level for weights. Table 11 shows the ImageNet-1K accuracy at different bit-widths for weights and activations. Benefits of well-behaved FF representations. Since we operate in the FF representation space, we can first compute the FF representations of the previous layer activations, Cprev = PT prevAprev (10) 19Flexible Low-Bit Quantization for Transformers Method bits ViT-S ViT-B Full Precision W32/A32 81.39 85 .10 FrameQuant W2/A32 48.17 79 .53 FrameQuant W2.2/A32 61.51 80 .93 FrameQuant W2/A8 48.02 79 .51 FrameQuant W2.2/A8 60.96 80 .64 FrameQuant W2/A6 47.41 78 .59 FrameQuant W2.2/A6 58.35 80 .14 Table 11.Performance of quantized ViT-S and ViT-B models on ImageNet-1K validation set. We used FrameQuant to quantize the weights while the activations are rounded to the nearest. and quantize these directly. Also, since activation quantization happens dynamically, during inference time, we keep the activation quantization procedure simple and just use the nearest rounding method. This can be written as: ¯Cprev = ⌊Cprev ∆C ⌉, ∆C = max |Cprev| 2N−1 − 1 (11) where ¯Cprev is in INT8 form and is the quantized version of the FF representations of the activations (Cprev). ⌊·⌉ represents nearest rounding. We can substitute with ⌊·⌋ or ⌈·⌉ to get the floor or the ceil operation. As noted by (Xiao et al., 2023), we also observe that the activations have large outliers in some of the channels whose values are more than 100× larger than the activations of other channels on average and this behavior is consistent across the tokens. This is shown in Figure 12a. So, to quantize the outliers, we need a large-scale factor ∆C, which will quantize all small values to zero. The other option is to use per-channel quantization – where we have different scale factors for different channels. This would solve the outlier problem, but it is not ideal because we cannot use integer kernels for matrix multiplications in the Linear Layers. To use integer arithmetic for the matrix multiplications in the Linear layers, we can only perform per-token quantization for the activations and per-channel quantization for the weights. To solve this problem, (Xiao et al., 2023) shifts the scale from activations to weights that are well-behaved. They dynamically search for different amounts of shifts between the weights and activations using a calibration set and use that during inference. Since we operate in the FF representation space, we observe that after we compute the FF representations of the activations, they are well-behaved. Figure 12b shows the FF representation of activation of the first Transformer block in the ViT-M model. So, we do not need to perform further scaling to reduce the range. This makes FrameQuant to be amenable to activation quantization if necessary in practice. G. Quantizing Segmentation and Object Detection models We used FrameQuant to quantize the Swin backbone for Object Detection and Segmentation Models. We compare our results with RepQ-ViT (Li et al., 2023), one of the state-of-the-art publicly available quantization methods in this regime. Since our primary focus is quantizing the weights of the Transformer, for a fair comparison, we use RepQ-ViT to quantize the rest of the parameters, such as activations and norm layers. From Table 12, we can see that FrameQuant performs similarly to RepQ-ViT, and the main benefits of frameQuant kick in at very low bit widths. H. Additional Experiments on Language models H.1. Evaluation on the C4 dataset This section is a continuation of section 4.4. Here, we present the perplexity of different models from OPT and Llama2 classes on the C4 (Raffel et al., 2020) dataset. Consistent with our previous experiments, we see that FrameQuant with 1× the redundancy performs better than all the methods under consideration. With an additional redundancy of r = 1.1×, FrameQuant closes the gap between the full precision model across all the sizes from different families of Large Language Models. The results are shown in table 13. 20Flexible Low-Bit Quantization for Transformers Method Precision of Swin Backbone Precision of rest of the network MoBY Mask RCNN w. Swin-T (APbox / APmask) MoBY Cascade Mask RCNN with Swin-T (APbox / APmask) Full Precision W32/A32 W32/A32 43.6/39.6 48 .1/41.5 RepQ-ViT W6/A6 W6/A6 42.6/39.0 47 .7/41.3 FrameQuant W6/A6 W6/A6 42.7/39.0 47 .8/41.3 RepQ-ViT W4/A4 W4/A4 34.2/32.3 43 .8/38.6 FrameQuant W4/A4 W4/A4 34.5/32.5 44 .3/39.1 RepQ-ViT W3/A4 W4/A4 27.5/26.4 38 .9/34.8 FrameQuant W3/A4 W4/A4 29.3/27.9 41 .2/36.7 RepQ-ViT W3/A4 W3/A4 16.9/16.9 32 .4/29.2 FrameQuant W3/A4 W3/A4 21.7/21.5 35 .2/31.4 Table 12.Performance of quantized models with Swin-T backbone on the Object Detection and Segmentation tasks. We can see that FrameQuant performs similarly to RepQ-Vit at higher bit widths. The main benefits of Frame representations kick in at very low bit-widths. Method #bits OPT Llama2 125M 350M 1.3B 2.7B 6.7B 7B 70B Full-Precision 16 26.56 22 .58 16 .07 14 .34 12 .71 7.26 5 .71 GPTQ 2 2203.89 5325 .65 4139 .91 4058 .41 528 .41 2265.09 68 .83 QuIP 2 543.63 432 .56 28 .91 21 .49 16 .92 26.61 8 .65 FrameQuant (r = 1.0) 2 226.15 95.38 27.90 20.74 17.28 19.62 7.85 FrameQuant (r = 1.1) 2.2 91.29 47.62 22.39 17.75 15.33 11.23 6.86 Table 13.Perplexity (smaller the better) of Llama2 and OPT models on C4 dataset when quantized to 2 (or 2.2) bits by different methods. Method #bits ARC (challenge) ARC (easy) BoolQ HellaSwag PIQA WinoGrande Full-Precision 16 43.43 76 .35 77 .71 57 .16 78 .07 69 .06 GPTQ 2 22.44 24 .58 41 .19 25 .93 51 .85 50 .43 QuIP 2 22.27 42 .76 50 .31 34 .04 61 .75 52 .64 FrameQuant (r = 1.0) 2 23.98 55.39 63.52 36.76 66.65 55.80 FrameQuant (r = 1.1) 2.2 31.91 65.53 67.95 46.46 73.07 63.61 Table 14.Evaluating Llama2-7B model quantized with different methods on a range of downstream tasks. H.2. Perplexity of Quantized Llama2 7B Figure 14 shows the perplexity of Llama2-7B model quantized by different quantization schemes. We see that FrameQuant with a redundancy of 1x already performs better than all other methods. With increasing redundancy, the performance becomes closer to the Full precision model. H.3. Performance on Downstream tasks In this experiment, we finetune the Llama2-7B model on downstream tasks. We ran experiments on ARC challenge, ARC easy (Clark et al., 2018), BoolQ (Clark et al., 2019), HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020) and WinoGrande (Sakaguchi et al., 2021). We used LM-evaluation harness (Gao et al., 2023) for running our experiments on these diverse tasks. The results are presented in table 14. We can see that again in line with our previous experiments, the LLM quantized with FrameQuant with no redundancy already performs better than all the other methods on the downstream tasks. With added redundancy, this performance goes up across all the tasks under consideration. Based on our previous experiments and as observed in (Chee et al., 2023), we expect the performance gap between the full precision model and the quantized model to go down as the size of the models increases. 21Flexible Low-Bit Quantization for Transformers (a) Perplexity of Llama2-7B model on WikiText2 dataset  (b) Perplexity of Llama2-7B model on C4 dataset Figure 14.Perplexity of Llama2-7B model on WikiText2 and C4 datasets. FrameQuant performs better than all quantization methods tested. With increasing redundancy, we see that the performance of the model also improves as indicated by the theory. I. Robustness guarantees We provide additional details on two specific results (mentioned in the main paper) that apply to our construction. We encourage the interested reader to refer to (Christensen, 2018; Casazza & Kutyniok, 2012) for a more comprehensive treatment of the topic. LMMSE estimation from fusion frame measurements. For a given layer l FrameQuant quantizes the transformed weights matrix Dl which is given by Dl = PT l (ΘlPprev). We can treat ˆDl as a projection of ΘlPprev which is corrupted by noise. During inference, the activations of this layer are given by Zl = Pl ˆDlCprev. But, can we do better? Instead of directly applying the synthesis operator Pl to compute Zl from its FF representations ˆDlCprev, we can design a simple linear filter F that minimizes the MSE in Zl because we are using a quantized ˆDl. The final expression for the computation of the output of the layer will be Zl = F ˆDlCprev. This linear MSE minimizer F is known to be the Wiener Filter and has a closed-form expression with various levels of approximation. The following theorem states that the Wiener filter minimizes MSE when the Fusion Frame is tight. Theorem I.1. (Kutyniok et al., 2009) For the model described above, the MSE in linearly estimating the signal from its noisy projections is minimized when the Fusion Frame is tight Consistent Reconstruction. Assuming the same mode of representing the modified weights Dl as above, during inference, we can get a consistent estimate of the weights ( ˆΘl) from ˆDl if one were to solve a linear program for ˆX \u0014 Pl −Pl \u0015 ˆXl ≤ \u0014∆ 2 + ˆDl ∆ 2 − ˆDl \u0015 , where ∆ is the quantization level. Here, the constraints in the Linear Program make sure that ˆX belongs to the regions where valid unquantized values must lie, thereby removing the out-of-sub-space error (Goyal et al., 1998). We can get the estimated weights from ˆXl as ˆΘl = ˆXlPT prev. Using this consistent reconstruction yields estimates with an MSE which is upper bounded by O(1/r2) (Goyal et al., 1998) J. Synopsis of Construction of Tight Fusion Frames Here, we give a brief synopsis of an algorithm for generating Tight Fusion Frames for the curious reader. (Casazza et al., 2011) was the first to introduce a systematic method for constructing UNTFs (Unit Norm Tight Frames) that play a key role in constructing Tight Fusion Frames. They also characterize the (k, ρ, d) values for which a Tight Fusion Frame exists. 22Flexible Low-Bit Quantization for Transformers Whenever such a TFF exists, we can construct Tight Fusion Frames by using their algorithm. There are two main parts to the algorithm. 1. Play Spectral Tetris to generate a UNTF of d elements in Cρ 2. Modulate this UNTF with complex roots of unity to generate a (k, ρ, d) TFF for Cd So, the first step is to generate a “smaller” frame and in the next step, we modulate the smaller frame to generate a “larger” Tight Fusion Frame. After generating a TFF for Cd we can easily extend it to the Real Field by applying the entrywise map x + iy 7→ \u0014x −y y x \u0015 . We describe the algorithm with the help of an example for the simplicity of explanation. We aim to construct a (5,4,11) TFF. So, k = 5, ρ= 3, d= 11. J.1. Spectral Tetris As the name suggests UNTFs are Tight frames where each frame vector has a unit norm. We construct a 4 × 11 matrix F whose columns are the frame vectors for C4 which satisfies • Columns of unit norm • Orthogonal rows, meaning FF ∗ is diagonal • Rows of constant norm, meaning FF ∗ is a constant multiple of identity matrix with the constant being 11 4 We start with a matrix F =   1 1 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?   This leaves a norm of 11 4 − 2 = 3 4 to be filled in the first row. This can easily be added using a 2 × 2 matrix T(x) where x = 3 4 . T(x) is defined as: T(x) := 1√ 2 \u0014 √x √x√2 − x −√2 − x \u0015 , T (x)T∗(x) = \u0014x 0 0 2 − x \u0015 After inserting T(x), F is now F =   1 1 √ 3√ 8 √ 3√ 8 0 0 0 0 0 0 0 0 0 √ 5√ 8 − √ 5√ 8 ? ? ? ? ? ? ? 0 0 0 0 ? ? ? ? ? ? ? 0 0 0 0 ? ? ? ? ? ? ?   Then we continue adding ones in row two until the norm becomes less than 11 4 . F =   1 1 √ 3√ 8 √ 3√ 8 0 0 0 0 0 0 0 0 0 √ 5√ 8 − √ 5√ 8 1 ? ? ? ? ? ? 0 0 0 0 0 ? ? ? ? ? ? 0 0 0 0 0 ? ? ? ? ? ?   Now we insert T(x) with the remaining norm. We repeat this process until all the rows are filled. The Final F is given by F =   1 1 √ 3√ 8 √ 3√ 8 0 0 0 0 0 0 0 0 0 √ 5√ 8 − √ 5√ 8 1 √ 2√ 8 √ 2√ 8 0 0 0 0 0 0 0 0 0 √ 6√ 8 − √ 6√ 8 1 √ 7√ 8 √ 7√ 8 0 0 0 0 0 0 0 0 0 √ 7√ 8 − √ 7√ 8 1   23Flexible Low-Bit Quantization for Transformers   1 1 √ 3√ 8 √ 3√ 8 0 0 0 0 0 0 0 1 ω √ 3√ 8 ω2 √ 3√ 8 ω3 0 0 0 0 0 0 0 1 ω2 √ 3√ 8 ω4 √ 3√ 8 ω 0 0 0 0 0 0 0 1 ω3 √ 3√ 8 ω √ 3√ 8 ω4 0 0 0 0 0 0 0 1 ω4 √ 3√ 8 ω3 √ 3√ 8 ω2 0 0 0 0 0 0 0 0 0 √ 5√ 8 − √ 3√ 8 1 √ 2√ 8 √ 2√ 8 0 0 0 0 0 0 √ 5√ 8 ω2 − √ 3√ 8 ω3 ω4 √ 2√ 8 √ 2√ 8 ω 0 0 0 0 0 0 √ 5√ 8 ω4 − √ 3√ 8 ω ω 3 √ 2√ 8 √ 2√ 8 ω2 0 0 0 0 0 0 √ 5√ 8 ω − √ 3√ 8 ω4 ω2 √ 2√ 8 √ 2√ 8 ω3 0 0 0 0 0 0 √ 5√ 8 ω3 − √ 3√ 8 ω2 ω √ 2√ 8 √ 2√ 8 ω4 0 0 0 0 0 0 0 0 0 √ 6√ 8 − √ 6√ 8 1 √ 7√ 8 √ 7√ 8 0 0 0 0 0 0 √ 6√ 8 − √ 6√ 8 ω ω 2 √ 7√ 8 ω3 √ 7√ 8 ω4 0 0 0 0 0 0 √ 6√ 8 − √ 6√ 8 ω2 ω4 √ 7√ 8 ω √ 7√ 8 ω3 0 0 0 0 0 0 √ 6√ 8 − √ 6√ 8 ω3 ω √ 7√ 8 ω4 √ 7√ 8 ω2 0 0 0 0 0 0 √ 6√ 8 − √ 6√ 8 ω4 ω3 √ 7√ 8 ω2 √ 7√ 8 ω1 0 0 0 0 0 0 0 0 0 √ 7√ 8 − √ 7√ 8 1 0 0 0 0 0 0 0 0 √ 7√ 8 ω3 − √ 7√ 8 ω4 1 0 0 0 0 0 0 0 0 √ 7√ 8 ω − √ 7√ 8 ω3 1 0 0 0 0 0 0 0 0 √ 7√ 8 ω4 − √ 7√ 8 ω2 1 0 0 0 0 0 0 0 0 √ 7√ 8 ω2 − √ 7√ 8 ω 1   Table 15.(5, 4, 11)-TFF for C11. Here, ω = ei2π/5. Each pair of rows belongs to the same subspace if their indices differ by a multiple of 5 J.2. Modulation In the second step, we modulate the F matrix with complex roots of unity, one subspace at a time. So, for each ki = 0, 1, 2, . . . k− 1, we construct a row vector wki = \u0014\u0010 e i2πki k \u00110 \u0010 e i2πki k \u00111 \u0010 e i2πki k \u00112 . . . \u0010 e i2πki k \u0011d−1\u0015 We multiply each row of F with wki to generate the orthogonal basis for different subspaces indexed by ki. Theorem 14 by Casazza et al. (2011) proves that the Fusion Frames generated by this algorithm are Tight. The Final Fusion Frame vectors are shown in Table 15. K. Storage benefits and Computational complexity during inference K.1. Storage benefits Consider an example where we are quantizing a weight matrix Θl of dimension 1024 × 1024 using FrameQuant with a redundancy factor of r = 1.1×. The size of the original matrix using FP32 is 4MB. After transforming the weights to map within the FF representation space, the transformed weights Dl have dimensions 1126 × 1126, which are quantized and represented using 2 bits. This quantized weight ˆDl has a size of 0.3MB. Along with the quantized weights, we need to store the bias and scale values for each row leading to an additional storage of 1024 FP32 values, which will incur an additional cost of 0.007MB. All this sums up to a storage of 0.307MB from an initial 4MB giving a savings of 13x in the storage requirements. Since we can generate the Fusion Frames on the fly, we just need to store the (k, ρ, d) values, and a seed to 24Flexible Low-Bit Quantization for Transformers generate the random rotation matrix which incurs negligible storage costs. Table 6 shows the sizes of Llama2 models when compressed with FrameQuant. K.2. Computational Complexity during Inference Consider a linear layer in a transformer model with weights Θl of dimensions d × d. Using FrameQuant these weights are transformed to Dl and the quantized weights ˆDl are stored. Let the parameters of the TFF used for quantization be (k, ρ, d). As a recap, k is the number of subspaces, ρ is the dimension of each subspace and d is the dimension of the Hilbert space we are operating in. So, the redundancy in Frame representations is r = kρ d . Let, Tl, Tprev ∈ Rd×kρ be the vectorized Orthonormal basis for the current layer, and the previous layer respectively. During inference, the quantized weights ˆDl are transformed to the weight space as ˆΘl = Pl ˆDlPT prev. Here, Pl = Rl(Tl), Pprev = Rprev(Tprev), where Rl, Rprev ∈ Rd×d denote the rotation matrices for the current and the previous layers respectively. So, the overall operation is ˆΘl = RlTl ˆDlTprevT RT prev. Let us first look at the ˆDlTprevT operation. TprevT is a block diagonal matrix constructed as defined in section 2.2. It has ρ blocks along the diagonal, each with k rows and at most ⌈d ρ ⌉ + 2 columns. The order of the computations required to generate this matrix is O(dk). The computation complexity of ˆDlTprevT is O(d ρ kρdr) = O(d2kr). So, the overall computational complexity for the computation of TprevT and multiplication with ˆDl is O(d2kr). Now, consider the left multiplication with Tl. Tl is again a block diagonal matrix similar to TprevT . But it is multiplying a quantity with dimensions kρ × d. Hence this multiplication has a computational complexity of O(d2k). The worst-case computational complexity of multiplication with the TFF orthonormal basis of current and previous layers is O(d2kr). The final Rl, RT prev are orthogonal rotation matrices which can be efficiently computed in O(d2 log d) time using random projections such as (Le et al., 2013) or any other efficient implementation. Combining all these calculations, the overall computational complexity of transforming the weights during inference is O(d2(kr + logd)). Note that since all of these are matrix operations, they run on GPU in a vectorized manner. Table 7 shows the inference speeds of the quantized models on a Nvidia A100 GPU. 25",
      "meta_data": {
        "arxiv_id": "2403.06082v2",
        "authors": [
          "Harshavardhan Adepu",
          "Zhanpeng Zeng",
          "Li Zhang",
          "Vikas Singh"
        ],
        "published_date": "2024-03-10T04:01:49Z",
        "pdf_url": "https://arxiv.org/pdf/2403.06082v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces FrameQuant, a flexible low-bit (2-bit or 2.x-bit) Post-Training Quantization scheme for Transformer-based models. Its main contribution is demonstrating that quantizing model weights in redundant Fusion Frame representations, rather than the original weight space, significantly improves robustness to quantization noise. This approach achieves 2-bit quantization with only a small accuracy drop, offering fractional bit quantization capabilities (e.g., 2.1 or 2.2 bits) with theoretical guarantees and outperforming existing baselines across a wide range of Vision Transformers and Large Language Models.",
        "methodology": "FrameQuant leverages Fusion Frames from Harmonic analysis to create redundant representations of pre-trained weight matrices. The core idea is to transform the weight matrix (Θl) and previous layer activations (˘Aprev) into their respective Fusion Frame (FF) representations, and then perform quantization on the transformed weight matrix (Dl = PT l ΘlPprev) within this FF space. This minimizes a proxy loss similar to existing PTQ strategies, but benefits from the noise robustness properties of Fusion Frames. The quantization procedure itself is an iterative Hessian-based algorithm, a modified version of GPTQ, with an additional step of clipping the transformed weights at ±2σ (standard deviations) to handle outliers, which is effective due to the well-behaved distribution of weights in the FF space. During inference, layer inputs are first transformed into FF representations, multiplied by the quantized FF weights, and then a synthesis operator reconstructs the layer outputs.",
        "experimental_setup": "The method was evaluated on ImageNet-1K for Vision Transformers (ViT, DeiT, DeiT III, Swin) and Perplexity for Language Models (OPT and Llama2 families) on WikiText2 and C4 datasets. A small calibration dataset of 128 randomly selected images was used for weight quantization, applied layer-by-layer. Main experiments focused on 2-bit weight quantization, with explorations into fractional bit-widths (e.g., 2.2 bits) through redundancy. Baselines included PTQ4ViT, GPTQ, QuIP (with and without a 2σ clipping modification), and ZeroQuant. Performance was measured by Top-1 accuracy for image classification and perplexity for language models. Additional experiments covered activation quantization, object detection, and segmentation tasks using a Swin backbone, and downstream tasks for Llama2-7B (ARC, BoolQ, HellaSwag, PIQA, WinoGrande). Inference speeds were measured on Nvidia A100 GPUs.",
        "limitations": "The raw inference speed of FrameQuant is expected to be lower than methods like GPTQ due to additional operations required to transform weights between original and Fusion Frame representations during inference. While efficient kernels can mitigate this, it remains an overhead. The paper also notes that 1-bit quantization proved unsuccessful, suggesting a practical lower bound around 2 bits. Computational complexity during inference incurs an additional O(d^2(kr + logd)) cost for weight transformations. Furthermore, quantization to low bit-widths tends to be more favorable for larger models, implying smaller models might experience a relatively larger performance hit, even with FrameQuant's benefits.",
        "future_research_directions": "Future work could focus on optimizing inference speeds by developing more efficient kernels for loading weights into GPUs and performing the Fusion Frame transformations. Tighter integration of FrameQuant with hardware could unlock significantly greater efficiency gains. The authors also suggest exploring the applicability of FrameQuant in Quantization-Aware Training (QAT), potentially during fine-tuning, where Fusion Frames could inform regularization to enhance robustness against quantization errors by estimating 'out of subspace error'. Lastly, an investigation into scaling laws considering the 'number of parameters × bit-width' as a proxy for the degree of freedom in quantized models is also proposed."
      }
    },
    {
      "title": "2DQuant: Low-bit Post-Training Quantization for Image Super-Resolution",
      "abstract": "Low-bit quantization has become widespread for compressing image\nsuper-resolution (SR) models for edge deployment, which allows advanced SR\nmodels to enjoy compact low-bit parameters and efficient integer/bitwise\nconstructions for storage compression and inference acceleration, respectively.\nHowever, it is notorious that low-bit quantization degrades the accuracy of SR\nmodels compared to their full-precision (FP) counterparts. Despite several\nefforts to alleviate the degradation, the transformer-based SR model still\nsuffers severe degradation due to its distinctive activation distribution. In\nthis work, we present a dual-stage low-bit post-training quantization (PTQ)\nmethod for image super-resolution, namely 2DQuant, which achieves efficient and\naccurate SR under low-bit quantization. The proposed method first investigates\nthe weight and activation and finds that the distribution is characterized by\ncoexisting symmetry and asymmetry, long tails. Specifically, we propose\nDistribution-Oriented Bound Initialization (DOBI), using different searching\nstrategies to search a coarse bound for quantizers. To obtain refined quantizer\nparameters, we further propose Distillation Quantization Calibration (DQC),\nwhich employs a distillation approach to make the quantized model learn from\nits FP counterpart. Through extensive experiments on different bits and scaling\nfactors, the performance of DOBI can reach the state-of-the-art (SOTA) while\nafter stage two, our method surpasses existing PTQ in both metrics and visual\neffects. 2DQuant gains an increase in PSNR as high as 4.52dB on Set5 (x2)\ncompared with SOTA when quantized to 2-bit and enjoys a 3.60x compression ratio\nand 5.08x speedup ratio. The code and models will be available at\nhttps://github.com/Kai-Liu001/2DQuant.",
      "full_text": "2DQuant: Low-bit Post-Training Quantization for Image Super-Resolution Kai Liu1, Haotong Qin2, Yong Guo3, Xin Yuan4, Linghe Kong1∗, Guihai Chen1, Yulun Zhang1∗ 1Shanghai Jiao Tong University, 2ETH Zürich, 3Max Planck Institute for Informatics, 4Westlake University Abstract Low-bit quantization has become widespread for compressing image super- resolution (SR) models for edge deployment, which allows advanced SR models to enjoy compact low-bit parameters and efficient integer/bitwise constructions for storage compression and inference acceleration, respectively. However, it is notorious that low-bit quantization degrades the accuracy of SR models compared to their full-precision (FP) counterparts. Despite several efforts to alleviate the degradation, the transformer-based SR model still suffers severe degradation due to its distinctive activation distribution. In this work, we present a dual-stage low- bit post-training quantization (PTQ) method for image super-resolution, namely 2DQuant, which achieves efficient and accurate SR under low-bit quantization. The proposed method first investigates the weight and activation and finds that the distribution is characterized by coexisting symmetry and asymmetry, long tails. Specifically, we propose Distribution-Oriented Bound Initialization (DOBI), using different searching strategies to search a coarse bound for quantizers. To obtain refined quantizer parameters, we further propose Distillation Quantization Calibration (DQC), which employs a distillation approach to make the quantized model learn from its FP counterpart. Through extensive experiments on different bits and scaling factors, the performance of DOBI can reach the state-of-the-art (SOTA) while after stage two, our method surpasses existing PTQ in both met- rics and visual effects. 2DQuant gains an increase in PSNR as high as 4.52dB on Set5 (×2) compared with SOTA when quantized to 2-bit and enjoys a 3.60× compression ratio and 5.08× speedup ratio. The code and models will be available at https://github.com/Kai-Liu001/2DQuant. 1 Introduction As one of the most classical low-level computer vision tasks, image super-resolution (SR) has been widely studied with the significant development of deep neural networks. With the abil- ity to reconstruct high-resolution (HR) image from the corresponding low-resolution (LR) image, SR has been widely used in many real-world scenarios, including medical imaging [ 13, 21, 19], surveillance [44, 37], remote sensing [1], and mobile phone photography. With massive parameters, DNN-based SR models always require expensive storage and computation in the actual application. Some works have been proposed to reduce the demand for computational power of SE models, like lightweight architecture design and compression. One kind of approach investigates lightweight and efficient models as the backbone for image SR. This progression has moved from the earliest convolutional neural network (CNNs) [10, 11, 25, 47] to Transformers [46, 29, 42, 40, 4, 3] and their combinations. The parameter number significantly decreased while maintaining or even enhancing performance. The other kind of approach is compression, which focuses on reducing the parameter (e.g., pruning and distillation) or bit-width (quantization) of existing SR models. Model quantization [7, 9, 20, 28] is a technology that compresses the floating-point parameters of a neural network into lower bit-width. The discretized parameters are homogenized into restricted ∗Corresponding authors: Yulun Zhang, yulun100@gmail.com, Linghe Kong, linghe.kong@sjtu.edu.cn Preprint. Under review. arXiv:2406.06649v1  [eess.IV]  10 Jun 2024candidate values and cause heterogenization between the FP and quantized models, leading to severe performance degradation. Considering the process, quantization approaches can be divided into quantization-aware training (QAT) and post-training quantization (PTQ). QAT simultaneously optimizes the model parameters and the quantizer parameters [6, 16, 26, 48], allowing them to adapt mutually, thereby more effectively alleviating the degradation caused by quantization. However, QAT often suffers from a heavy training cost and a long training time, and the burden is even much heavier than the training process of the FP counterparts, which necessitates a large amount of compatibility and makes it still far from practical in training-resource-limited scenarios. Urban100: img_092 HR Bicubic Percentile [27] DBDC+Pac [39] Ours FP Figure 1: Existing methods suffer from blurring artifacts. Fortunately, post-training quantization emerges as a promising way to quantize models at a low training cost. PTQ fixes the model parameters and only determines the quantizer parameters through search or optimization. Previous researches [39, 26] on PTQ for SR has primarily focused on CNN-based models such as EDSR [30] and SRResNet [24]. However, these quantization methods are not practical for deployment for two reasons. Firstly, these CNN-based models themself require huge space and calculation resources. Their poor starting point makes them inferior to advanced models in terms of parameters and computational cost, even after quantization. As shown in Table 1, the light version of SwinIR needs only 16.2% parameters and 15.9% FLOPs compared with quantized EDSR. But its PSNR metric is close to that of the FP EDSR. While the previous PTQ algorithm, DBDC+Pac, suffers from unacceptable degradation in both visual and metrics. Secondly, most of these methods can not adapt well to Transformer-based models because of the unadaptable changes in weight and activation distributions. As shown in Figure 1, when applied on SwinIR, the existing methods still suffer from distorted artifacts compared with FP or HR. Table 1: Complexity and performance (×4). Model EDSR [30]EDSR (4bit) [39]SwinIR-light [29]DBDC+Pac (4bit) [39]Ours (4bit) Params (MB) 172.36 21.55 3.42 1.17 1.17Ops (G) 823.34 103.05 16.74 4.19 4.19PNSR on Urban100 26.64 25.56 26.47 24.94 25.71 Therefore, we conducted a post-training quantization analysis on super-resolution with a classical Transformer-based model SwinIR [29]. The weight and activation distribution is characterized by coexisting symmetry and asymmetry, long tails. Firstly, if the previous symmetric quantization method is applied for asymmetric distribution, at least half of the candidates are completely ineffective. Besides, the long tail effect causes the vast majority of floating-point numbers to be compressed into one or two candidates, leading to worse parameter homogenization. Furthermore, with such a small number of parameters, SwinIR’s information has been highly compressed, and quantizing the model often results in significant performance degradation. Nevertheless, the excellent performance and extremely low computational requirements of Transformer-based models are precisely what is needed for deployment in real-world scenarios. In this paper, we propose 2DQuant, a two-stage PTQ algorithm for image super-resolution tasks. To enhance the representational capacity in asymmetry scenarios, we employ a quantization method with two bounds. The bounds decide the candidate for numbers out of range and the interval of candidates in range. First, we propose distribution-oriented Bound Initialization(DOBI), a fast MSE-based searching method. It is designed to minimize the value heterogenization between quantized and FP models. Two different MSE [5] search strategies are applied for different distributions to avoid nonsense traversal. This guarantees minimum value shift while maintaining high speed and efficiency in the search process. Second, we propose Distillation Quantization Calibration(DQC), a training- based method. It is designed to adjust each bound to its best position finely. This ensures that the outputs and intermediate feature layers of the quantized model and that of the FP model should remain as consistent as possible. Thereby DQC allows the quantizer parameters to be finely optimized toward the task goal. The contributions of this paper can be summarized as follows: (1) To the best of our knowledge, we are the first to explore PTQ with Transformer-based model in SR thoroughly. We design 2DQuant, a unique and efficient two-stage PTQ method (see Figure 2) for image super-resolution, which utilizes DOBI and DQC to optimize the bound from coarse to fine. (2) In the first stage of post-quantization, we use DOBI to search for quantizer parameters, employing customized search strategies for different distributions to balance speed and accuracy. In the second stage, we design DQC, a more fine-grained optimization-based training strategy, for the quantized model, ensuring it aligns with the FP model on the calibration set. 2FP Model Quant Model Moving Simultaneously Moving Upper Bound Only FP weights and activations Distribution Oriented Bound Initialization Distillation Quantization Calibration Searching  Direction Initial Bound DQC Loss Forward Full-precision Layer Quantized Layer Figure 2: The overall pipeline of our proposed 2DQuant method. The whole pipeline contains two stages, optimizing the clipping bound from coarse to fine. In stage 1, we design DOBI to efficiently obtain the coarse bound. In stage 2, DQC is performed to finetune clipping bounds and guarantee the quantized model learns the full-precision (FP) model’s feature and output information. (3) Our 2DQuant can compress Transformer-based model to 4,3,2 bits with the compression ratio being 3.07×, 3.31×, and 3.60× and speedup ratio being 3.99×, 4.47×, and 5.08×. No additional module is added so 2DQuant enjoys the theoretical upper limit of compression and speedup. (4) Through extensive experiments, our 2DQuant surpasses existing SOTA on all benchmarks. We gain an increase in PSNR by as high as 4.52dB in Set5 ( ×2) when compressed to 2 bits, and our method has a more significant increase when compressed to lower bits. 2 Related work Image super-resolution. Deep CNN networks have shown excellent performance in the field of image super-resolution. The earliest SR-CNN [ 10, 11] method adopted a CNN architecture. It surpassed previous methods in the image super-resolution domain. In 2017, EDSR [ 30] won the NTIRE2017 [38] championship, becoming a representative work of CNNs in the SR by its excellent performance. Thereafter, with the continuous development of Vision Transformers (ViT) [ 12], models based on the ViT architecture have surpassed many CNN networks. These Transformer- based models achieve significant performance improvements and they have fewer parameters and lower computational costs. Many works have modified the ViT architecture, achieving continuous improvements. A notable example is SwinIR [29]. With a simple structure, it outperforms many CNN- based models. However, previous explorations of post-quantization in the super-resolution domain have been limited to CNN-based models. They focus on models like EDSR [30] or SRResNet [24]. It is a far cry from advanced models no matter in parameters, FLOPs, or performance. Currently, there is still a research gap in post-quantization for Transformer architectures. Model quantization. In the field of quantization, quantization methods are mainly divided into PTQ and QAT. QAT is widely accepted due to its minimal performance degradation. PAMS [26] utilizes a trainable truncated parameter to dynamically determine the upper limit of the quantization range. DAQ [17] proposed a channel-wise distribution-aware quantization scheme. CADyQ [16] is proposed as a technique designed for SR networks and optimizes the bit allocation for local regions and layers in the input image. However, QAT usually requires training for as long as or even longer than the original model, which becomes a barrier for real scenarios deployment. Instead of training the model from scratch, existing PTQ methods use the pre-trained models. PTQ algorithms only find the just right clipping bound for quantizers, saving time and costs. DBDC+Pac [ 39] is the first to optimize the post-training quantization for image super-resolution task. It outperforms other existing PTQ algorithms. Whereas, they only focus on EDSR [30] and SRResNet [24]. Their 4-bit quantized version is inferior to advanced models in terms of parameters and computational cost, let alone performance. It reveals a promising result for PTQ applying on SR, but using a more advanced model could bridge the gap between high-performance models and limited calculation resource scenarios. 3 Methodology To simulate the precision loss caused by quantization, we use fake-quantize [ 22],i.e.quantization- dequantization, for activations and weights. and the process can be written as vc = Clip(v, l, u), v r = Round(2N − 1 u − l (vc − l)), v q = u − l 2N − 1vr + l, (1) 3LayerNorm X WQ WK WV Q K V BMM Scale SoftMax BMM Projection LayerNorm FC2 GELU FC1 FP32 INT4 Figure 3: Quantization scheme for SwinIR Transformer blocks. Fake quantization and INT arithmetic are performed in all compute-intensive operators including all linear layers and batch matmul. Lower bits such as 3 or even 2 are also permitted. Dropout of attention and projection is ignored where v denotes the value being fake quantized, which can be weight or activation. l and u are the lower and upper bounds for clipping, respectively. Clip(v, l, u) = max(min(v, u), l), and Round rounds the input value to the nearest integer. vc denotes the value after clipping, and vr denotes the integer representation of v, and vq denotes the value after fake quantization. The Clip and Round operations contribute to reducing the parameters and FLOPs but also introduce quantization errors. Table 2: FLOPs distribution. Module FLOPs (G)Ratio (%) Linear & BMM 14.34 85.66Conv 2.33 13.90Other 0.07 0.44Total 16.74 100.00 Figure 3 shows the basic structure of the Transformer block. We have quantized all the modules with a significant computational load within them, effectively reducing the model’s FLOPs. Table 2 shows the FLOPs needed for each module. The Linear layers and matrix multiplication account for approximately 86% of the compu- tation load, which are all transformed into integer arithmetic. When performing gradient backpropagation, we follow the Straight-Through Estimator [8] (STE) style: ∂vq ∂u = ∂vc ∂u + 1 2n − 1vr − vc − l u − l , ∂vq ∂l = ∂vc ∂l − 1 2n − 1vr + vc − l u − l , (2) where ∂vc ∂u = H(u −v) and vc ∂l = H(l −v), H(·) denotes Heaviside step function [45]. This formula approximates the direction of gradient backpropagation, allowing training-based optimization to proceed. The derivation of the formula can be found in the supplementary material. Figure 2 shows the whole pipeline of 2DQuant, which is a two-stage coarse-to-fine post-training quantization method. The first stage is DOBI, using two strategies to minimize the value shift while the second stage is DQC, optimizing two bound of each quantizer towards the task goal. 3.1 Analysis of data distribution To achieve better quantization results, we need to analyze the distribution of the model’s weights and activations in detail. We notice that the data distribution shows a significantly different pattern from previous explorations, invalidating many of the previous methods. The weights and activations distri- bution of SwinIR is shown in Figure 4. More can be found in supplemental material. Specifically, the weights and activations of SwinIR exhibit noticeable long-tail, coexisting symmetry and asymmetry. Weight. The weights of all linear layers are symmetrically distributed around zero, showing clear symmetry, and are generally similar to a normal distribution. This is attributed to the weight decay applied to weights, which provides quantization-friendly distributions. From the value shift perspective, both symmetric and asymmetric quantization are tolerable. Whereas, from the vantage point of task objectives, asymmetric quantization possesses the potential to offer a markedly enhanced information density, thus elevating the overall precision of the computational processes involved. Activations. As for activations, they exhibit obvious periodicity in different Transformer Blocks. For V or the input of FC1, the obtained activation values are symmetrically distributed around 0. However, for the attention map or the input of FC2 in each Transformer Block, due to the Softmax calculation or the GELU [14] activation function, the minimum value is almost fixed, and the overall distribution is similar to an exponential distribution. Therefore, the data in SwinIR’s weights and activations exhibit two distinctly different distribution characteristics. Setting asymmetric quantization and different search strategies for both can make the search rapid and accurate. 3.2 Distribution-oriented bound initialization Because the data distribution exhibits a significant long-tail effect, we must first clip the range to avoid low effective bits. Common clipping methods include density-based, ratio-based, and MSE-based approaches. The first two require manually specifying the clipping ratio, which significantly affects the clipping outcome and necessitates numerous experiments to determine the optimal ratio. Thus we proposed the Distribution-Oriented Bound Initialization (DOBI) to search the bound for weight and 40.0 0.2 0.4 0.6 0.8 1.0 /uni0000003e/uni00000013/uni00000011/uni00000013/uni00000013/uni0000000f/uni00000014/uni00000011/uni00000013/uni00000013/uni00000040 /uni00000024/uni00000046/uni00000057/uni0000001d/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000011/uni00000013/uni00000011/uni00000045/uni0000004f/uni00000052/uni00000046/uni0000004e/uni00000011/uni00000013/uni00000003/uni00000024/uni00000057/uni00000057/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000052/uni00000051 2  0 2 /uni0000003e/uni00000010/uni00000017/uni00000011/uni00000019/uni00000014/uni0000000f/uni00000016/uni00000011/uni0000001b/uni0000001a/uni00000040 /uni00000024/uni00000046/uni00000057/uni0000001d/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000011/uni00000014/uni00000011/uni00000045/uni0000004f/uni00000052/uni00000046/uni0000004e/uni00000011/uni00000014/uni00000003/uni00000039 5.0  2.5  0.0 2.5 5.0 7.5 /uni0000003e/uni00000010/uni00000018/uni00000011/uni00000019/uni00000014/uni0000000f/uni0000001a/uni00000011/uni00000016/uni00000014/uni00000040 /uni00000024/uni00000046/uni00000057/uni0000001d/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000011/uni00000015/uni00000011/uni00000045/uni0000004f/uni00000052/uni00000046/uni0000004e/uni00000011/uni00000015/uni00000003/uni00000029/uni00000026/uni00000014 0 2 4 6 /uni0000003e/uni00000010/uni00000013/uni00000011/uni00000014/uni0000001a/uni0000000f/uni00000019/uni00000011/uni00000019/uni00000014/uni00000040 /uni00000024/uni00000046/uni00000057/uni0000001d/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000011/uni00000016/uni00000011/uni00000045/uni0000004f/uni00000052/uni00000046/uni0000004e/uni00000011/uni00000016/uni00000003/uni00000029/uni00000026/uni00000015 0.2  0.0 0.2 /uni0000003e/uni00000010/uni00000013/uni00000011/uni00000016/uni00000018/uni0000000f/uni00000013/uni00000011/uni00000015/uni0000001c/uni00000040 /uni0000003a/uni00000048/uni0000004c/uni0000004a/uni0000004b/uni00000057/uni0000001d/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000011/uni00000013/uni00000011/uni00000045/uni0000004f/uni00000052/uni00000046/uni0000004e/uni00000011/uni00000013/uni00000003/uni00000033/uni00000055/uni00000052/uni0000004d/uni00000048/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051 0.50  0.25  0.00 0.25 0.50 /uni0000003e/uni00000010/uni00000013/uni00000011/uni00000019/uni00000013/uni0000000f/uni00000013/uni00000011/uni00000019/uni00000014/uni00000040 /uni0000003a/uni00000048/uni0000004c/uni0000004a/uni0000004b/uni00000057/uni0000001d/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000011/uni00000014/uni00000011/uni00000045/uni0000004f/uni00000052/uni00000046/uni0000004e/uni00000011/uni00000014/uni00000003/uni00000034/uni0000002e/uni00000039 0.75  0.50  0.25  0.00 0.25 0.50 /uni0000003e/uni00000010/uni00000013/uni00000011/uni00000019/uni0000001c/uni0000000f/uni00000013/uni00000011/uni00000019/uni00000019/uni00000040 /uni0000003a/uni00000048/uni0000004c/uni0000004a/uni0000004b/uni00000057/uni0000001d/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000011/uni00000015/uni00000011/uni00000045/uni0000004f/uni00000052/uni00000046/uni0000004e/uni00000011/uni00000015/uni00000003/uni00000029/uni00000026/uni00000014 0.5  0.0 0.5 /uni0000003e/uni00000010/uni00000013/uni00000011/uni0000001b/uni00000015/uni0000000f/uni00000013/uni00000011/uni0000001b/uni00000017/uni00000040 /uni0000003a/uni00000048/uni0000004c/uni0000004a/uni0000004b/uni00000057/uni0000001d/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000011/uni00000013/uni00000011/uni00000045/uni0000004f/uni00000052/uni00000046/uni0000004e/uni00000011/uni00000014/uni00000003/uni00000029/uni00000026/uni00000015 Figure 4: The selected representative distribution of activations (Row 1) and weights (Row 2). The range of data is marked in the figure. All weights obey symmetric distribution. The attention map and the input of FC2 are asymmetric due to softmax function and GELU function. activation, avoiding manually adjusting hyperparameters. The global optimizing goal is as follows {(li, ui)}N i=1 = arg min li,ui NX i=1 ∥vi − vqi∥2 . (3) The collection of all quantizers’ bounds {(li, ui)}N i=1 is the linchpin of quantized model performance as it indicates the candidate value for weights and activations. We note that the data distribution falls into two categories: one resembling a bell-shaped distribution and the other resembling an exponential distribution. For the bell-shaped distribution, we use a symmetric boundary-narrowing search method. Whereas, for the exponential distribution, we fix the lower bound to the minimum value of the data and only traverse the right bound. The specific search method is shown in Algorithm 1. The time complexity of Algorithm 1 is O(MK ), where M is the number of elements in data v and K is the number of search points. The condition v is symmetricalis obtained by observing the visualization of v and the activations are from the statistics on a small calibration set. 3.3 Distillation quantization calibration To further fine-tune the clipping range, we propose distillation quantization calibration (DQC) to transfer the knowledge from the FP model to the quantized model. It leverages the knowl- edge distillation [ 15] where the FP model acts as the teacher while the quantized model is the student. Specifically, for the same input image, the student model needs to continu- ously minimize the discrepancy with the teacher model on the final super-resolution output. Algorithm 1:DOBI pipeline Data: Data to be quantized v, the number of search point K, bit b Result: Clip bound l, u l ← min(v),u ← max(v); min_mse ← +∞; if v is symmetricalthen ∆l ← (max(v) − min(v))/2K; else ∆l ← 0; end ∆u ← (max(v) − min(v))/2K; while i ≤ K do li ← l + i ×∆l, ui ← u + i ×∆u; get vq based on Eq. (1); mse ← ∥v − vq∥2; if mse ≤ min_mse then min_mse ← mse; l_best ← li, u_best ← ui; end end The loss for the final output can be written as LO = 1 COHOWO ∥O − Oq∥1 , (4) where O and Oq are the final outputs of the teacher and student models, CO, HO, and WO represent the number of output channels, height, and width, respec- tively. we adopt the L1 loss for the final output, as it tends to converge more easily compared to the L2 loss [30]. As the quantized model shares the same structure with the FP model and is quantized from the FP model, the student model also need to learn to extract the same feature of the teacher model, which can be written as LF = NX i 1 CiHiWi \r\r\r\r Fi ∥Fi∥2 − Fqi ∥Fqi∥2 \r\r\r\r 2 , (5) where Fi and Fqi are the intermediate features of the teacher and student models respectively andi is the in- dex of the layer. In the field of super-resolution, there is a clear correspondence between the feature maps and the final reconstructed images, making training on feature maps crucial. since the quantized network 5and the full-precision network have identical structures, we do not need to add extra adaptation layers for feature distillation. The final loss function can be written as L = LO + λLF , (6) where λ is the co-efficient ofLF . In the second stage, based on training optimization methods, the gap between the quantized model and the full-precision model will gradually decrease. The performance of the quantized model will progressively improve and eventually converge to the optimal range. 4 Experiments 4.1 Experimental settings Data and evaluation. We use DF2K [38, 31] as the training data, which combines DIV2K [38] and Flickr2K [31], as utilized by most SR models. During training, since we employ a distillation training method, we do not need to use the high-resolution parts of the DF2K images. For validation, we use the Set5 [2] as the validation set. After selecting the best model, we tested it on five commonly used benchmarks in the SR field: Set5 [2], Set14 [43], B100 [34], Urban100 [18], and Manga109 [35]. On the benchmarks, we input low-resolution images into the quantized model to obtain reconstructed images, which we then compared with the high-resolution images to calculate the metrics. We do not use self-ensemble in the test stage as it increases the computational load eightfold, but the improvement in metrics is minimal The evaluation metrics we used are the most common metrics PSNR and SSIM [41], which are calculated on the Y channel (i.e., luminance) of the YCbCr space. Implementation details. We use SwinIR-light [29] as the backbone and provide its structure in the supplementary materials. We conduct comprehensive experiments with scale factors of 2, 3, and 4 and with 2, 3, and 4 bits, where Our hyperparameter settings remain consistent. During DOBI, we use a search step number of K=100, and the statistics of activations are obtained from 32 images in DF2K being randomly cropped to retain only 3 ×64×64. During DQC, we use the Adam [ 23] optimizer with a learning rate of 1 ×10−2, betas set to (0.9, 0.999), and a weight decay of 0. We employ CosineAnnealing [33] as the learning rate scheduler to stabilize the training process. Data augmentation is also performed. We randomly utilize rotation of 90°, 180°, and 270° and horizontal flips to augment the input image. The total iteration for training is 3,000 with batch size of 32. Our code is written with Python and PyTorch [36] and runs on an NVIDIA A800-80G GPU. 4.2 Comparison with state-of-the-art methods The methods we compared include MinMax [ 22], Percentile [ 27], and the current SOTA post- quantization method in the super-resolution field, DBDC+Pac [39]. For a fair comparison, we report the performance of DBDC+Pac [39] on EDSR [30], as the authors performed detailed parameter adjustments and model training on EDSR. We directly used the results reported by the authors, recorded in the table as EDSR †. It should be noted that the EDSR method uses self-ensemble in the final test, which can improve performance to some extent but comes at the cost of 8 times the computational load. Additionally, we applied DBDC+Pac [39] to SwinIR-light [29], using the same hyperparameters as those set by the authors for EDSR, recorded in the table as DBDC+Pac [39]. The following are the quantitative and qualitative results of the comparison. Quantitative results. Table 3 shows the extensive results of comparing different quantization methods with bit depths of 2, 3, and 4, as well as different scaling factors of ×2, ×3, and ×4. 0 20 40 60 80 100 120 140 0.00 0.05 0.10 0.15 0.20 0.25 0.30 /uni00000034/uni00000058/uni00000044/uni00000051/uni00000057/uni0000004c/uni0000005d/uni00000048/uni00000055/uni00000003/uni0000004f/uni00000052/uni0000005a/uni00000048/uni00000055/uni00000003/uni00000045/uni00000052/uni00000058/uni00000051/uni00000047/uni00000003/uni00000053/uni00000048/uni00000055/uni00000046/uni00000048/uni00000051/uni00000057/uni0000004c/uni0000004f/uni00000048/uni00000011 DOBI DOBI+DQC 0 20 40 60 80 100 120 140 0.70 0.75 0.80 0.85 0.90 0.95 1.00 /uni00000034/uni00000058/uni00000044/uni00000051/uni00000057/uni0000004c/uni0000005d/uni00000048/uni00000055/uni00000003/uni00000058/uni00000053/uni00000053/uni00000048/uni00000055/uni00000003/uni00000045/uni00000052/uni00000058/uni00000051/uni00000047/uni00000003/uni00000053/uni00000048/uni00000055/uni00000046/uni00000048/uni00000051/uni00000057/uni0000004c/uni0000004f/uni00000048/uni00000011 DOBI DOBI+DQC Figure 5: The bound percentile of DOBI and DQC. DBDC+Pac [39] performs poorly mainly because 1. The DBDC process requires manually specifying the clipping ratio, which signif- icantly affects performance. 2. DBDC does not prune weights, and the learning rate in the Pac process is too low, causing slow con- vergence of weight quantizer parameters. However, both adverse factors are eliminated in our 2DQuant algorithm. When using only DOBI algorithm, our performance has already reached a level comparable to that of DBDC+Pac algorithms. Upon applying DQC, our performance experienced a remarkable and discernible enhancement, elevating it to new heights. In the case of ×2, 4-bit on Set5 and Urban100, DOBI has an improvement of 1.11dB and 0.39 dB compared to EDSR, while 6Table 3: Quantitative comparison with SOTA methods. EDSR† means applying DBDC+Pac [39] on CNN-based backbone EDSR [31]. Its results are cited from the paper [39]. Set5 (×2) Set14 (×2) B100 (×2) Urban100 (×2) Manga109 (×2)Method Bit PSNR↑ SSIM↑ PSNR↑ SSIM↑ PSNR↑ SSIM↑ PSNR↑ SSIM↑ PSNR↑ SSIM↑ SwinIR-light [29]32 38.15 0.9611 33.86 0.9206 32.31 0.9012 32.76 0.9340 39.11 0.9781 Bicubic 32 32.25 0.9118 29.25 0.8406 28.68 0.8104 25.96 0.8088 29.17 0.9128 MinMax [22] 4 34.39 0.9202 30.55 0.8512 29.72 0.8409 28.40 0.8520 33.70 0.9411 Percentile [27] 4 37.37 0.9568 32.96 0.9113 31.61 0.8917 31.17 0.9180 37.19 0.9714 EDSR†[30, 39] 4 36.33 0.9420 32.75 0.9040 31.48 0.8840 30.90 0.9130 N/A N/A DBDC+Pac [39]4 37.18 0.9550 32.86 0.9106 31.56 0.8908 30.66 0.9110 36.76 0.9692 DOBI (Ours) 4 37.44 0.9568 33.15 0.9132 31.75 0.8937 31.29 0.9193 37.93 0.9743 2DQuant (Ours)4 37.87 0.9594 33.41 0.9161 32.02 0.8971 31.84 0.9251 38.31 0.9761 MinMax [22] 3 28.19 0.6961 26.40 0.6478 25.83 0.6225 25.19 0.6773 28.97 0.7740 Percentile [27] 3 34.37 0.9170 31.04 0.8646 29.82 0.8339 28.25 0.8417 33.43 0.9214 DBDC+Pac [39]3 35.07 0.9350 31.52 0.8873 30.47 0.8665 28.44 0.8709 34.01 0.9487 DOBI (Ours) 3 36.37 0.9496 32.33 0.9041 31.12 0.8836 29.65 0.8967 36.18 0.9661 2DQuant (Ours)3 37.32 0.9567 32.85 0.9106 31.60 0.8911 30.45 0.9086 37.24 0.9722 MinMax [22] 2 33.88 0.9185 30.81 0.8748 29.99 0.8535 27.48 0.8501 31.86 0.9306 Percentile [27] 2 30.82 0.8016 28.80 0.7616 27.95 0.7232 26.30 0.7378 30.37 0.8351 DBDC+Pac [39]2 34.55 0.9386 31.12 0.8912 30.27 0.8706 27.63 0.8649 32.15 0.9467 DOBI (Ours) 2 35.25 0.9361 31.72 0.8917 30.62 0.8699 28.52 0.8727 34.65 0.9529 2DQuant (Ours)2 36.00 0.9497 31.98 0.9012 30.91 0.8810 28.62 0.8819 34.40 0.9602 Set5 (×3) Set14 (×3) B100 (×3) Urban100 (×3) Manga109 (×3)Method Bit PSNR↑ SSIM↑ PSNR↑ SSIM↑ PSNR↑ SSIM↑ PSNR↑ SSIM↑ PSNR↑ SSIM↑ SwinIR-light [29]32 34.63 0.9290 30.54 0.8464 29.20 0.8082 28.66 0.8624 33.99 0.9478 Bicubic 32 29.54 0.8516 27.04 0.7551 26.78 0.7187 24.00 0.7144 26.16 0.8384 MinMax [22] 4 31.66 0.8784 28.17 0.7641 27.19 0.7257 25.60 0.7485 29.98 0.8854 Percentile [27] 4 33.34 0.9137 29.61 0.8275 28.49 0.7899 27.06 0.8242 32.10 0.9303 DBDC+Pac [39]4 33.42 0.9143 29.69 0.8261 28.51 0.7869 27.05 0.8217 31.89 0.9274 DOBI (Ours) 4 33.78 0.9200 29.87 0.8338 28.72 0.7970 27.53 0.8391 32.57 0.9367 2DQuant (Ours)4 34.06 0.9231 30.12 0.8374 28.89 0.7988 27.69 0.8405 32.88 0.9389 MinMax [22] 3 26.01 0.6260 23.41 0.4944 22.46 0.4182 21.70 0.4730 24.68 0.6224 Percentile [27] 3 30.91 0.8426 28.02 0.7545 27.23 0.7183 25.32 0.7349 29.43 0.8537 DBDC+Pac [39]3 30.91 0.8445 28.02 0.7538 26.99 0.6937 25.10 0.7122 28.84 0.8403 DOBI (Ours) 3 32.85 0.9075 29.33 0.8200 28.27 0.7820 26.36 0.8036 31.14 0.9178 2DQuant (Ours)3 33.24 0.9135 29.56 0.8255 28.50 0.7873 26.65 0.8116 31.46 0.9235 MinMax [22] 2 26.05 0.5827 24.74 0.5302 24.42 0.4973 22.87 0.5155 24.66 0.5652 Percentile [27] 2 25.30 0.5677 23.60 0.4890 23.77 0.4751 22.33 0.4965 24.65 0.5882 DBDC+Pac [39]2 29.96 0.8254 27.53 0.7507 27.05 0.7136 24.57 0.7117 27.23 0.8213 DOBI (Ours) 2 30.54 0.8321 27.74 0.7312 26.69 0.6643 24.80 0.6797 28.18 0.7993 2DQuant (Ours)2 31.62 0.8887 28.54 0.8038 27.85 0.7679 25.30 0.7685 28.46 0.8814 Set5 (×4) Set14 (×4) B100 (×4) Urban100 (×4) Manga109 (×4)Method Bit PSNR↑ SSIM↑ PSNR↑ SSIM↑ PSNR↑ SSIM↑ PSNR↑ SSIM↑ PSNR↑ SSIM↑ SwinIR-light [29]32 32.45 0.8976 28.77 0.7858 27.69 0.7406 26.48 0.7980 30.92 0.9150 Bicubic 32 27.56 0.7896 25.51 0.6820 25.54 0.6466 22.68 0.6352 24.19 0.7670 MinMax [22] 4 28.63 0.7891 25.73 0.6657 25.10 0.6061 23.07 0.6216 26.97 0.8104 Percentile [27] 4 30.64 0.8679 27.61 0.7563 26.96 0.7151 24.96 0.7479 28.78 0.8803 EDSR†[30, 39] 4 31.20 0.8670 27.98 0.7600 27.09 0.7140 25.56 0.7640 N/A N/A DBDC+Pac [39]4 30.74 0.8609 27.66 0.7526 26.97 0.7104 24.94 0.7369 28.52 0.8697 DOBI (Ours) 4 31.10 0.8770 28.03 0.7672 27.18 0.7237 25.43 0.7631 29.31 0.8916 2DQuant (Ours)4 31.77 0.8867 28.30 0.7733 27.37 0.7278 25.71 0.7712 29.71 0.8972 MinMax [22] 3 19.41 0.3385 18.35 0.2549 18.79 0.2434 17.88 0.2825 19.13 0.3097 Percentile [27] 3 27.55 0.7270 25.15 0.6043 24.45 0.5333 22.80 0.5833 26.15 0.7569 DBDC+Pac [39]3 27.91 0.7250 25.86 0.6451 25.65 0.6239 23.45 0.6249 26.03 0.7321 DOBI (Ours) 3 29.59 0.8237 26.87 0.7156 26.24 0.6735 24.17 0.6880 27.62 0.8349 2DQuant (Ours)3 30.90 0.8704 27.75 0.7571 26.99 0.7126 24.85 0.7355 28.21 0.8683 MinMax [22] 2 23.96 0.4950 22.92 0.4407 22.70 0.3943 21.16 0.4053 22.94 0.5178 Percentile [27] 2 23.03 0.4772 22.12 0.4059 21.83 0.3816 20.45 0.3951 20.88 0.3948 DBDC+Pac [39]2 25.01 0.5554 23.82 0.4995 23.64 0.4544 21.84 0.4631 23.63 0.5854 DOBI (Ours) 2 28.82 0.7699 26.46 0.6804 25.97 0.6319 23.67 0.6407 26.32 0.7718 2DQuant (Ours)2 29.53 0.8372 26.86 0.7322 26.46 0.6927 23.84 0.6912 26.07 0.8163 2DQuant has an improvement of 0.69 dB and 1.18 dB compared to the SOTA method. All these results indicate that our two-stage PTQ method can effectively mitigate the degradation caused by quantization and ensure the quality of the reconstructed images. Figure 5 shows the bound percentile of DOBI searching and DQC. Overall, the bound of DQC is tighter as the values around the zero point enjoy greater importance. Besides, the shallow layers’ 7Urban100: img_004 ( ×4) HR Bicubic MinMax [22] Percentile [27] DBDC+Pac [39] DOBI (Ours) 2DQuant (Ours) FP Urban100: img_046 ( ×4) HR Bicubic MinMax [22] Percentile [27] DBDC+Pac [39] DOBI (Ours) 2DQuant (Ours) FP Urban100: img_023 ( ×4) HR Bicubic MinMax [22] Percentile [27] DBDC+Pac [39] DOBI (Ours) 2DQuant (Ours) FP Figure 6: Visual comparison for image SR (×4) in some challenging cases. Learning rate PSNR↑ SSIM↑ 10−1 37.82 0.9594 10−2 37.87 0.9594 10−3 37.78 0.9592 10−4 37.74 0.9587 (a) Learning rate Batch size PSNR↑ SSIM↑ 4 37.82 0.9594 8 37.83 0.9594 16 37.84 0.9593 32 37.87 0.9594 (b) Batch size DOBI DQC PSNR↑ SSIM↑ 34.39 0.9202 ✓ 37.44 0.9568 ✓ 37.32 0.9563 ✓ ✓ 37.87 0.9594 (c) DOBI and DQC Table 4: Ablation studies. The models are trained on DIV2K and Flickr2K, and tested on Set5 (×2). bounds vary more significantly due to the elevated significance of these layers within the neural network. Detailedly, the bound for the second MLP fully connected layer’s weight in Layer 0 Block 1 only remains 46% data in its range. It has the second-highest lower bound percentile and the smallest upper bound percentile among the network. Its percentiles are 0.2401 and 0.7035 respectively while its bound values are -0.062 and 0.047 and its distribution is visualized in Figure 4. In conclusion, only through task-oriented optimization of each bound at a fine-grained level can redundant information be maximally excluded and useful information be maximally retained. Qualitative results. We show the visual comparison results for×4 in Figure 6. Since quantized models are derived from full-precision models with information loss, their global performance will rarely exceed that of full-precision models. As seen in the three images for Minmax, after quantization, if no clipping is performed, the long tail effect will lead to a large number of useless bits, resulting in a significant amount of noise and repeated distorted patterns in the reconstructed images. In these challenging cases, our training method allows the model to retain edge information of objects better, preventing blurring and distorted effects. For example, in img_046 and img_023, we have the highest similarity to the full-precision model, while other methods show varying degrees of edge diffusion, significantly affecting image quality. Compared to the DBDC+Pac method, our DOBI and DQC allow for better representation of edge and texture information in the images and effectively avoid distortions and misalignments in the graphics. The visual results demonstrate that our proposed DQC is essential for improving performance in both metric and visual comparisons. 4.3 Ablation study Learning rate and batchsize. We first study the performance variations of the model under different hyperparameters. From Tables 4a and 4b, it can be seen that our DQC enables the model to 8converge within a range of outstanding performance for most learning rates and batch sizes. Due to the non-smooth impact of quantization parameters on the model, the quantized model is more prone to local optima compared to the full-precision model, resulting in a noticeable performance drop when the learning rate is too low. Additionally, as shown in Table 4b, the larger the batch size, the better the model’s performance, and the smoother the convergence process. However, even with a smaller batch size, we can still achieve a performance of 37.82dB on Set5, indicating that our two-stage method has good robustness to different hyperparameters. DOBI and DQC. Moreover, we also study the impact of different stages on performance, with the results shown in Table 4c, from which we can draw the following conclusions:Firstly, the goal of DOBI is to minimize the value shift for weights and activations. Although it is not the task goal, it can still enjoy significant enhancement due to better bit representational ability. Secondly, DQC alone cannot achieve the optimization effect of DOBI. This is because the impact of quantizer parameters on model performance is oscillatory, and training alone is prone to converge to local optima. In contrast, search-based methods can naturally avoid local optima. So it’s necessary to use results from the search-based method to initialize training-based method in PTQ. Thirdly, when DOBI and DQC are combined, namely our 2DQuant, the 4-bit quantized model has only a 0.28dB decrease on Set5 compared to the FP model, which maximally mitigates the accuracy loss caused by quantization. 5 Discussion Why our results surpass FP outcomes.While our method’s performance metrics do not yet fully match those of full-precision models, visual results reveal a compelling advantage. As observed in image img_092 of Figure 1 of Urban100, our approach correctly identifies the direction of the stripes in the image. Whereas the full-precision model erroneously selects the wrong direction. This discrepancy arises because the lower-resolution image, affected by aliasing, creates an illusion of slanted stripes, misleading the FP model’s reconstruction. This phenomenon demonstrates that our PTQ algorithm allows more accurate restored results in certain localized and challenging tasks without being misled. More examples are in the supplementary materials. It suggests that full-precision models contain not only redundant knowledge but also incorrect information. The latter is hard to get rid of by training the FP model. Our quantization method can effectively reduce model parameters and computational demands while eliminating erroneous information, achieving multiple benefits simultaneously. This also suggests that the FP model doesn’t represent the pinnacle of what a quantized model can achieve. Limitations. Despite achieving excellent results, this study still has some limitations. During the DOBI process, the data distribution of activations and weights is required to approximate a bell curve or exponential distribution; otherwise, the DOBI method cannot find the most suitable positions. Additionally, increasing the number of search points for a single tensor in MSE does not necessarily guarantee better performance. However, the second-stage training can somewhat alleviate this issue. Moreover, our method requires a calibration set; without which, the first-stage DOBI and the second-stage DQC cannot be carried out at all. Societal impacts. Our super-resolution quantization method effectively saves computational re- sources, facilitating the deployment of super-resolution models at the cutting edge 6 Conclusion This paper studies the post-training quantization in the field of image super-resolution. We first conducted a detailed analysis of the data distribution of Transformer-based model in SR. These data exhibit a clear long-tail effect and symmetry and asymmetry coexisting effect. We designed 2DQuant, a dual-stage PTQ algorithms. In the first stage DOBI, we designed two different search strategies for the two different distributions. In the second stage DQC, we designed a distillation- based training method that let the quantized model learn from the FP model, minimizing the accuracy loss caused by quantization. Our 2DQuant can compress Transformer-based model to 4,3,2 bits with the compression ratio being 3.07×, 3.31×, and 3.60× and speedup ratio being 3.99×, 4.47×, and 5.08×. No additional module is added so 2DQuant enjoys the theoretical upper limit of compression and speedup. Extensive experiments demonstrate that 2DQuant surpasses all existing PTQ methods in the field of SR and even surpasses the FP model in some challenging cases. In the future, recognizing the significant impact of the model on performance, we will conduct PTQ research on more advanced super-resolution models and attempt to deploy quantized super-resolution algorithms to actual photography tasks, providing a more detailed evaluation of the performance of PTQ algorithms. 9References [1] Wele Gedara Chaminda Bandara and Vishal M. Patel. Hypertransformer: A textural and spectral feature fusion transformer for pansharpening. In CVPR, 2022. 1 [2] Marco Bevilacqua, Aline Roumy, Christine Guillemot, and Marie Line Alberi-Morel. Low-complexity single-image super-resolution based on nonnegative neighbor embedding. In BMVC, 2012. 6 [3] Zheng Chen, Yulun Zhang, Jinjin Gu, Linghe Kong, Xiaokang Yang, and Fisher Yu. Dual aggregation transformer for image super-resolution. In CVPR, 2023. 1 [4] Zheng Chen, Yulun Zhang, Jinjin Gu, Yongbing Zhang, Linghe Kong, and Xin Yuan. Cross aggregation transformer for image restoration. In NeurIPS, 2022. 1 [5] Jungwook Choi, Pierce I-Jen Chuang, Zhuo Wang, Swagath Venkataramani, Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan. Bridging the accuracy gap for 2-bit quantized neural networks (qnn). arXiv, 2018. 2 [6] Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan. Pact: Parameterized clipping activation for quantized neural networks. arXiv preprint arXiv:1805.06085, 2018. 2 [7] Yoni Choukroun, Eli Kravchik, Fan Yang, and Pavel Kisilev. Low-bit quantization of neural networks for efficient inference. In ICCVW, 2019. 1 [8] Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1. arXiv preprint arXiv:1602.02830, 2016. 4, 17 [9] Yifu Ding, Haotong Qin, Qinghua Yan, Zhenhua Chai, Junjie Liu, Xiaolin Wei, and Xianglong Liu. Towards accurate post-training quantization for vision transformer. In ACM MM, 2022. 1 [10] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Learning a deep convolutional network for image super-resolution. In ECCV, 2014. 1, 3 [11] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Image super-resolution using deep convolutional networks. TPAMI, 2016. 1, 3 [12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv, 2020. 3 [13] Hayit Greenspan. Super-resolution in medical imaging. The Computer Journal, 2008. 1 [14] Dan Hendrycks and Kevin Gimpel. Bridging nonlinearities and stochastic regularizers with gaussian error linear units. CoRR, 2016. 4 [15] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. In NeurIPS Workshop, 2014. 5 [16] Cheeun Hong, Sungyong Baik, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee. Cadyq: Content-aware dynamic quantization for image super-resolution. In ECCV, 2022. 2, 3 [17] Cheeun Hong, Heewon Kim, Sungyong Baik, Junghun Oh, and Kyoung Mu Lee. Daq: Channel-wise distribution-aware quantization for deep image super-resolution networks. In WACV, 2022. 3 [18] Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja. Single image super-resolution from transformed self-exemplars. In CVPR, 2015. 6 [19] Yawen Huang, Ling Shao, and Alejandro F Frangi. Simultaneous super-resolution and cross-modality synthesis of 3d medical images using weakly-supervised joint convolutional sparse coding. In CVPR, 2017. 1 [20] Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. Accurate post training quantization with small calibration sets. In ICML, 2021. 1 [21] Jithin Saji Isaac and Ramesh Kulkarni. Super resolution techniques for medical image processing. In ICTSD, 2015. 1 [22] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer- arithmetic-only inference. In CVPR, 2018. 3, 6, 7, 8, 18 [23] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 6 [24] Christian Ledig, Lucas Theis, Ferenc Huszár, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, and Wenzhe Shi. Photo-realistic single image super-resolution using a generative adversarial network. In CVPR, 2017. 2, 3 10[25] Christian Ledig, Lucas Theis, Ferenc Huszár, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew P Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image super-resolution using a generative adversarial network. In CVPR, 2017. 1 [26] Huixia Li, Chenqian Yan, Shaohui Lin, Xiawu Zheng, Baochang Zhang, Fan Yang, and Rongrong Ji. Pams: Quantized super-resolution via parameterized max scale. In ECCV, 2020. 2, 3 [27] Rundong Li, Yan Wang, Feng Liang, Hongwei Qin, Junjie Yan, and Rui Fan. Fully quantized network for object detection. In CVPR, 2019. 2, 6, 7, 8, 18 [28] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. In ICLR, 2021. 1 [29] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir: Image restoration using swin transformer. In ICCVW, 2021. 1, 2, 3, 6, 7 [30] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee. Enhanced deep residual networks for single image super-resolution. In CVPRW, 2017. 2, 3, 5, 6, 7 [31] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee. Enhanced deep residual networks for single image super-resolution. In CVPRW, 2017. 6, 7 [32] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021. 13 [33] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv, 2016. 6 [34] David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In ICCV, 2001. 6 [35] Yusuke Matsui, Kota Ito, Yuji Aramaki, Azuma Fujimoto, Toru Ogawa, Toshihiko Yamasaki, and Kiyoharu Aizawa. Sketch-based manga retrieval using manga109 dataset. Multimedia Tools and Applications, 2017. 6 [36] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. NeurIPS, 2019. 6 [37] Pejman Rasti, Tõnis Uiboupin, Sergio Escalera, and Gholamreza Anbarjafari. Convolutional neural network super resolution for face recognition in surveillance monitoring. In AMDO, 2016. 1 [38] Radu Timofte, Eirikur Agustsson, Luc Van Gool, Ming-Hsuan Yang, Lei Zhang, Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, Kyoung Mu Lee, et al. Ntire 2017 challenge on single image super-resolution: Methods and results. In CVPRW, 2017. 3, 6 [39] Zhijun Tu, Jie Hu, Hanting Chen, and Yunhe Wang. Toward accurate post-training quantization for image super resolution. In CVPR, 2023. 2, 3, 6, 7, 8, 18 [40] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li. Uformer: A general u-shaped transformer for image restoration. In CVPR, 2022. 1 [41] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. TIP, 2004. 6 [42] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Restormer: Efficient transformer for high-resolution image restoration. In CVPR, 2022. 1 [43] Roman Zeyde, Michael Elad, and Matan Protter. On single image scale-up using sparse-representations. In Proc. 7th Int. Conf. Curves Surf., 2010. 6 [44] Liangpei Zhang, Hongyan Zhang, Huanfeng Shen, and Pingxiang Li. A super-resolution reconstruction algorithm for surveillance images. Elsevier Signal Processing, 2010. 1 [45] Weihong Zhang and Ying Zhou. Chapter 2 - level-set functions and parametric functions. In The Feature- Driven Method for Structural Optimization. Elsevier. 4 [46] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu. Image super-resolution using very deep residual channel attention networks. In ECCV, 2018. 1 [47] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and Yun Fu. Residual dense network for image super-resolution. In CVPR, 2018. 1 [48] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016. 2 11Appendix / Supplemental material A Detailed structure of SwinIR SwinIR comprises three core modules: shallow feature extraction, deep feature extraction, and high-quality (HQ) image reconstruction. Shallow and deep feature extraction.Given a low-quality (LQ) input ILQ ∈ RH×W×Cin (where H, W, and Cin represent the image height, width, and input channel number, respectively), a 3 × 3 convolutional layer HSF(·) is employed to extract shallow features F0 ∈ RH×W×C as follows: F0 = HSF(ILQ), (7) where C denotes the number of feature channels. Subsequently, deep features FDF ∈ RH×W×C are extracted from F0 as: FDF = HDF(F0), (8) where HDF(·) represents the deep feature extraction module, comprising K residual Swin Transformer blocks (RSTB) and a 3 × 3 convolutional layer. Specifically, intermediate features F1, F2, . . . , FK and the output deep feature FDF are sequentially extracted as follows: Fi = HRSTBi(Fi−1), i = 1, 2, . . . , K, FDF = HCONV(FK), (9) where HRSTBi(·) denotes the i-th RSTB, and HCONV is the concluding convolutional layer. Incorporating a convolutional layer at the end of feature extraction introduces the inductive bias of the convolution operation into the Transformer-based network, laying a robust foundation for subsequent aggregation of shallow and deep features. Image reconstruction. In the context of image super-resolution (SR), the high-quality image IRHQ is reconstructed by combining shallow and deep features as follows: IRHQ = HREC(F0 + FDF), (10) where HREC(·) is the reconstruction module’s function. The reconstruction module is implemented using a sub-pixel convolution layer to upsample the feature.Additionally, residual learning is utilized to reconstruct the residual between the LQ and HQ images instead of the HQ image itself, formulated as: IRHQ = HSwinIR(ILQ) +ILQ, (11) where HSwinIR(·) represents the SwinIR function. A.1 Residual Swin Transformer block The residual Swin Transformer block (RSTB) is a residual block incorporating Swin Transformer layers (STL) and convolutional layers. Given the input featureFi,0 of the i-th RSTB, intermediate featuresFi,1, Fi,2, . . . , Fi,L are first extracted by L Swin Transformer layers as follows: Fi,j = HSTLi,j (Fi,j−1), j = 1, 2, . . . , L, (12) where HSTLi,j (·) is the j-th Swin Transformer layer in the i-th RSTB. A convolutional layer is added before the residual connection, and the output of RSTB is formulated as: Fi,out = HCONVi(Fi,L) +Fi,0, (13) where HCONVi(·) is the convolutional layer in the i-th RSTB. Swin Transformer layer.Given an input of size H × W × C, the Swin Transformer first reshapes the input into a HW M2 × M2 × C feature by partitioning the input into non-overlapping M × M local windows, where HW M2 is the total number of windows. It then computes the standard self-attention for each window (i.e., local attention). For a local window feature X ∈ RM2×C, the query, key, and value matrices Q, K, and V are computed as follows: Q = XPQ, K = XPK, V = XPV , (14) where PQ, PK, and PV are projection matrices shared across different windows. Typically, Q, K, V∈ RM2×d. The attention matrix is then computed via the self-attention mechanism within a local window as follows: Attention(Q, K, V) =SoftMax(QKT / √ d + B)V, (15) where B is the learnable relative positional encoding. In practice, the attention function is performed h times in parallel, and the results are concatenated for multi-head self-attention (MSA). 12Next, a multi-layer perceptron (MLP) with two fully-connected layers and GELU non-linearity between them is used for further feature transformations. The LayerNorm (LN) layer is added before both MSA and MLP, with residual connections employed for both modules. The entire process is formulated as: X = MSA(LN(X)) +X, X = MLP(LN(X)) +X. (16) However, when the partition is fixed across different layers, there are no connections between local windows. Thus, regular and shifted window partitioning are used alternately to enable cross-window connections [32], with shifted window partitioning involving shifting the feature by (⌊M 2 ⌋, ⌊M 2 ⌋) pixels before partitioning. A.2 Our settings We use the SwinIR light version provided by the original authors. The light version has only 4 RSTBs in the body part while for each RSTB, there are only 6 STLs. For each STL’s MSA, the number of heads is 6, the embedding dimension is 60, the window size is 8, and the MLP ratio is 2. B Detailed distribution of weights and activations In code implementation, the RSTB is called layers while the STL is called blocks. We visualize all layers’ distribution of the pre-trained SwinIR light model’s weights in Figure 7. Bias is ignored as it is not quantized. Also, we visualize the distribution of activations from 32 image patches with a size of 3×64×64 in Figure 8, Figure 9, Figure 10, and Figure 11. We can safely ignore the detailed value of each axis but just care about the shape of distributions. 13Figure 7: Visualization of SwinIR weights. 14Figure 8: Visualization of SwinIR first layer activation. Figure 9: Visualization of SwinIR second layer activation. 15Figure 10: Visualization of SwinIR third layer activation. Figure 11: Visualization of SwinIR fourth layer activation. 16C The derivation of the backward gradient propagation formula In this section, we provide the derivation of our backpropagation formula.We follow the STE [8] style to process the round term, which is ∂Round (x) ∂x = 1 (17) As for the clip function, we take a similar approach, which is ∂Clip(x, l, u) ∂x = ( 1 if l ≤ x ≤ u 0 if x < lor x > u ∂Clip(x, l, u) ∂l = ( 1 if x < l 0 if x ≥ l ∂Clip(x, l, u) ∂u = ( 1 if x > u 0 if x ≤ u (18) With Eqs. (1), (17), and (18), we first derive ∂vq ∂u ∂vq ∂u = ∂ ∂u ( u − l 2N − 1vr + l) = 1 2N − 1vr + u − l 2N − 1 ∂vr ∂u = 1 2N − 1vr + u − l 2N − 1(− 2N − 1 (u − l)2(vc − l) + 2N − 1 u − l ∂vc ∂u ) = ∂vc ∂u + 1 2n − 1vr − vc − l u − l (19) ∂vq ∂l can be derived roughly the same, which can be written as ∂vq ∂l = ∂ ∂l ( u − l 2N − 1vr + l) = − 1 2N − 1vr + u − l 2N − 1 ∂vr ∂u + 1 = − 1 2N − 1vr + u − l 2N − 1( 2N − 1 (u − l)2(vc − l) + 2N − 1 u − l ( ∂vc ∂u − 1)) + 1 = ∂vc ∂u − 1 2n − 1vr + vc − l u − l (20) D More visual examples We provide more visual illustrations to demonstrate the superiority of our method, as shown in Figure 12. In img_016, our method does not distort straight lines. In img_040, our method does not introduce noise to the camera and does not alter the shape at the camera lens. In img_072, we once again outperform the full-precision model by not adding vertical stripes to the curtains. In img_096, we ensure the shape of each window to the greatest extent. These images prove that we can surpass the current SOTA methods in visual effects and avoid misleading results in some tricky cases, generating correct results. 17Urban100: img_016 ( ×4) HR Bicubic MinMax [22] Percentile [27] DBDC+Pac [39] DOBI (Ours) 2DQuant (Ours) FP Urban100: img_040 ( ×4) HR Bicubic MinMax [22] Percentile [27] DBDC+Pac [39] DOBI (Ours) 2DQuant (Ours) FP Urban100: img_072 ( ×4) HR Bicubic MinMax [22] Percentile [27] DBDC+Pac [39] DOBI (Ours) 2DQuant (Ours) FP Urban100: img_096 ( ×4) HR Bicubic MinMax [22] Percentile [27] DBDC+Pac [39] DOBI (Ours) 2DQuant (Ours) FP Figure 12: Visual comparison for image SR (×4) in some challenging cases. 18",
      "meta_data": {
        "arxiv_id": "2406.06649v1",
        "authors": [
          "Kai Liu",
          "Haotong Qin",
          "Yong Guo",
          "Xin Yuan",
          "Linghe Kong",
          "Guihai Chen",
          "Yulun Zhang"
        ],
        "published_date": "2024-06-10T06:06:11Z",
        "pdf_url": "https://arxiv.org/pdf/2406.06649v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the significant accuracy degradation of low-bit post-training quantization (PTQ) for image super-resolution (SR) models, particularly for transformer-based architectures like SwinIR, which suffer from distinctive activation distributions. It proposes 2DQuant, a novel dual-stage PTQ method to achieve efficient and accurate SR. The first stage, Distribution-Oriented Bound Initialization (DOBI), performs a fast MSE-based search for coarse quantizer bounds, specifically adapting to the coexisting symmetric and asymmetric, long-tailed distributions of weights and activations. The second stage, Distillation Quantization Calibration (DQC), refines these bounds using a knowledge distillation approach where the quantized model learns from its full-precision (FP) counterpart to minimize output and intermediate feature discrepancies. 2DQuant achieves state-of-the-art performance, gaining up to a 4.52dB PSNR increase on Set5 (×2) when quantized to 2-bit compared to existing SOTA methods, along with significant compression (3.60×) and speedup (5.08×) ratios. This is the first thorough exploration of PTQ for Transformer-based SR models.",
        "methodology": "The 2DQuant method is a two-stage coarse-to-fine post-training quantization pipeline. It simulates precision loss using fake-quantization (quantization-dequantization) for weights and activations, and employs the Straight-Through Estimator (STE) for gradient backpropagation during training. A detailed analysis of SwinIR's weight and activation distributions revealed coexisting symmetry and asymmetry, along with long tails. Stage one, Distribution-Oriented Bound Initialization (DOBI), minimizes the mean squared error (MSE) between full-precision and quantized values to find coarse clipping bounds. It applies different search strategies: a symmetric boundary-narrowing search for bell-shaped distributions (weights, V-activations, FC1 input activations) and a fixed-lower-bound, right-bound traversal for exponential distributions (attention map, FC2 input activations). Stage two, Distillation Quantization Calibration (DQC), fine-tunes these bounds using knowledge distillation. The FP model acts as a teacher, guiding the quantized student model by minimizing a combined loss function. This function consists of an L1 loss for the final super-resolution output (LO) and an L2 loss for intermediate feature consistency (LF), with a tunable coefficient λ.",
        "experimental_setup": "The research used SwinIR-light as the backbone model for all experiments. Training data was DF2K (combining DIV2K and Flickr2K), with high-resolution parts being unnecessary for the distillation training. Set5 was used as the validation set. Performance was evaluated on five widely used benchmarks: Set5, Set14, B100, Urban100, and Manga109. The primary evaluation metrics were PSNR and SSIM, calculated on the Y channel of the YCbCr space, without using self-ensemble during testing. Experiments were conducted for quantization bit depths of 2, 3, and 4 bits, and scaling factors of ×2, ×3, and ×4. DOBI utilized a search step number of K=100, with activation statistics collected from 32 randomly cropped 3×64×64 images from DF2K. DQC training employed the Adam optimizer with a learning rate of 1 ×10−2, betas (0.9, 0.999), and no weight decay, using CosineAnnealing as the learning rate scheduler. Data augmentation included random 90°, 180°, 270° rotations and horizontal flips. Training ran for 3,000 iterations with a batch size of 32 on an NVIDIA A800-80G GPU. Comparisons were made against MinMax, Percentile, and the SOTA PTQ method DBDC+Pac (applied on both EDSR and SwinIR-light).",
        "limitations": "The DOBI process assumes that the data distribution of activations and weights approximates either a bell curve or an exponential distribution. If distributions deviate significantly from these assumed shapes, DOBI may not effectively find the most suitable clipping bounds. Additionally, simply increasing the number of search points for a single tensor in the MSE-based search does not guarantee improved performance, although the second stage, DQC, helps to mitigate this issue through fine-grained optimization. A critical dependency for the entire method is the availability of a calibration set; without it, both the DOBI and DQC stages cannot be carried out.",
        "future_research_directions": "Recognizing the impact of the model on performance, future work will involve conducting PTQ research on more advanced super-resolution models. The researchers also plan to attempt deploying quantized super-resolution algorithms to actual photography tasks, which will allow for a more detailed and practical evaluation of the performance of PTQ algorithms in real-world scenarios. This suggests an interest in exploring the generalization and practical utility of their approach beyond benchmark datasets."
      }
    },
    {
      "title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization",
      "abstract": "Efficient deployment of Large Language Models (LLMs) requires batching\nmultiple requests together to improve throughput. As the batch size, context\nlength, or model size increases, the size of the key and value (KV) cache can\nquickly become the main contributor to GPU memory usage and the bottleneck of\ninference latency. Quantization has emerged as an effective technique for KV\ncache compression, but existing methods still fail at very low bit widths. We\nobserve that distinct channels of a key/value activation embedding are highly\ninter-dependent, and the joint entropy of multiple channels grows at a slower\nrate than the sum of their marginal entropies. Based on this insight, we\npropose Coupled Quantization (CQ), which couples multiple key/value channels\ntogether to exploit their inter-dependency and encode the activations in a more\ninformation-efficient manner. Extensive experiments reveal that CQ outperforms\nor is competitive with existing baselines in preserving model quality.\nFurthermore, we demonstrate that CQ can preserve model quality with KV cache\nquantized down to 1-bit.",
      "full_text": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization Tianyi Zhang1, Jonah Yi1, Zhaozhuo Xu2, and Anshumali Shrivastava1,3 1Department of Computer Science, Rice University 2Department of Computer Science, Stevens Institute of Technology 3ThirdAI Corp. {tz21, jwy4, anshumali}@rice.edu, zxu79@stevens.edu May 8, 2024 Abstract Efficient deployment of Large Language Models (LLMs) requires batching multiple requests together to improve throughput. As the batch size, context length, or model size increases, the size of the key and value (KV) cache can quickly become the main contributor to GPU memory usage and the bottleneck of inference latency. Quantization has emerged as an effective technique for KV cache compression, but existing methods still fail at very low bit widths. We observe that distinct channels of a key/value activation embedding are highly inter-dependent, and the joint entropy of multiple channels grows at a slower rate than the sum of their marginal entropies. Based on this insight, we propose Coupled Quantization (CQ), which couples multiple key/value channels together to exploit their inter-dependency and encode the activations in a more information-efficient manner. Extensive experiments reveal that CQ outperforms or is competitive with existing baselines in preserving model quality. Furthermore, we demonstrate that CQ can preserve model quality with KV cache quantized down to 1-bit. 1 Introduction Large Language Models (LLMs) have showcased remarkable generalization abilities across various tasks, including text generation, language translation, and reasoning, without needing specific fine- tuning [25]. These impressive capabilities have empowered LLMs to find applications in numerous domains, such as law [14], education [15], and patient care [35]. However, the high computational demands and the prohibitive deployment costs of LLMs have created significant barriers, hindering their widespread adoption [14, 3]. Particularly, as LLMs move towards larger model size [9] and longer context length [34], they require faster graphics processing units (GPUs), or other special- ized processors, with higher memory capacity for efficient inference. Hence it is crucial to develop approaches for reducing the computational costs and memory requirement of LLMs. To accelerate LLM inference, key and value (KV) caching [20] has been proven to be an effective technique without affecting model quality. In autoregressive decoder-only LLMs, KV caching works through trading off memory for computations: the key and value activations of all previous tokens in the current batch are saved in memory to avoid their recomputation for generating the next token. However, since KV cache scales linearly with the number of tokens and batch size, it can quickly overwhelm the memory capacity of existing GPUs under long context or large batch size 1 arXiv:2405.03917v1  [cs.LG]  7 May 2024settings. In addition, since past key and value activations are not shared between sequences (except for maybe a common prompt), reading the KV cache from GPU memory becomes the primary inference bottleneck as opposed to the computation of the attention scores and value activations of the next token [10]. Thus, it is worthwhile to explore techniques for compressing KV cache for two primary benefits: 1. speeding up LLM inference through reducing the amount of memory reads for KV cache, 2. lowering the GPU memory requirements of inference for a given batch size and context length. Existing approaches typically compress KV cache through token eviction [37, 20] or activation quantization [21, 10]. While they can preserve model quality at moderate compression rates (4× compression or 4 bits per floating-point number), model quality quickly deteriorates at high compression rates (16× compression or 1 bit per floating-point number). In this work, we propose Coupled Quantization (CQ), a novel KV cache quantization method that preserves model quality up to 16× compression or 1 bit per floating-point number. Our approach is motivated by the observation that different channels within the same key/value activation embedding are highly inter-dependent. Thus, it is more information efficient to encode multiple channels of a key/value activation embedding than quantizing each channel independently. Existing KV cache quantization methods employ per-channel or per-token quantization strategies, which fail to exploit the inter-dependence between channels and suffer catastrophic model quality degradation at high compression rates. Our proposed method exploits the mutual dependency be- tween channels by jointly quantizing multiple channels and achieves better preservation of model quality than existing approaches in most cases, especially under low bit width settings. We sum- marize our contributions as follows, 1. We empirically observe the phenomenon that different channels within the same key/value activation embedding in an LLM share a high amount of dependency or mutual information, which is a key insight not leveraged by existing KV cache compression approaches. 2. We propose Coupled Quantization (CQ), a novel KV cache quantization method that takes advantage of the reduced entropy of jointly encoding multiple channels. 3. Through extensive experiments, we demonstrate the effectiveness of CQ against the most competitive existing methods. Furthermore, we showcase the ability of CQ in preserving model quality at an extreme KV cache compression level of 1-bit. 2 Background In this section, we introduce the relevant background and context including the KV caching tech- nique, the von Neumann bottleneck of KV cache, and channel-wise quantization. 2.1 LLM Attention and KV Cache Decoder-only transformer-based LLMs employ masked self-attention [32], in which activations of the current token is only dependent on previous tokens and unaffected by future ones. This prop- erty enables training parallelism for the next-token prediction objective, and gives rise to the KV caching technique for efficient inference decoding. Consider the decoding step for thet-th token in a single head of attention in an LLM. The input embedding of thet-th token (a column vector),et, goes through three distinct transformations to become key, query, and value activation embeddings fK(et), fQ(et), fV (et), where the transformationsfK, fQ, fV are composed of linear projection and positional encoding such as RoPE [29]. The output embedding of attention for thet-th token is 2computed as attention(et) = \u0014 fV (e1) . . . fV (et) \u0015 softmax  \u0014 fK(e1) . . . fK(et) \u0015⊤ fQ(et) ! (1) Thus, computing the output embedding of the current token requires the key and value activation embeddings of all previous tokens,fK(ei) and fV (ei) where i ∈ {1, . . . , t− 1}. These embeddings are cached in memory from previous decoding steps asKV cacheto avoid redundant computations and reduce inference latency. The size of KV cache can be calculated asb × n × l × 2 × h × c floating-point numbers, whereb is the batch size,n is the number of tokens in each sequence,l is the number of layers in the model,2 is for key and value,h is the number of key/value attention heads, and c is the number of channels in a single head of key/value activation embedding. As batch size, context length, or model size increases, the size of KV quickly overwhelms limited GPU memory. 2.2 The von Neumann Bottleneck of KV Cache The attention computation in Equation 1 is primarily bottlenecked by GPU memory bandwidth, known as the von Neumann bottleneck, due to low compute-to-global-memory-access ratio. GPUs are able to perform computations significantly faster than reading from global memory, and previous works have shown that attention computation can be accelerated by avoiding unnecessary global memory reads/writes through kernel fusion [5]. During the decoding phase in LLM attention, com- puting the output for the current token requires fetching the cached key and value embeddings of all previous tokens from global memory, resulting in a low ratio of arithmetic operations to global memory reads [10]. Furthermore, each sequence in a batch retains its own KV cache, leading to poor utilization of the parallel processing capabilities of GPUs. Therefore, KV cache compression algorithms can mitigate the von Neumann bottleneck and improve LLM inference efficiency, even if the approach introduces additional computational overheads in decompression or dequantization. As GPU compute cores are mostly stalled by KV cache memory accesses in attention, quantization approaches can effectively reduce memory reads while introducing negligible latency from dequan- tization computations. 2.3 Channel-wise Quantization Existing KV cache quantization methods [10, 21] employ channel-wise quantization for keys and token-wise quantization for values, based on the observation that certain key channels exhibit outlier patterns in magnitude while value channels do not. Channel-wise and token-wise quantization are similar, except the direction along which the quantization centroids are learned. In non-uniform channel-wise quantization, a set of centroids is learned for each channel. SupposeA is a key or value activation matrix, andAi,∗ denotes the i-th channel ofA. Then, non-uniform b-bit channel-wise quantization aims to learn a set of centroidsC⋆ i ⊂ R for each channeli of A independently through the objective C⋆ i = arg min C⊂R |C|=2b \r\r\rAi,∗ − q(Ai,∗) \r\r\r 2 2 (2) where q quantizes each value inAi,∗ to the nearest centroid inC. 31 2 3 4 5 10 15Key Entropy (bits) Layer 1 1 2 3 4 5 10 Layer 2 1 2 3 4 5 10 Layer 3 1 2 3 4 5 10 Layer 4 1 2 3 4 Num. of Joint Channels 5 10Value Entropy (bits) 1 2 3 4 Num. of Joint Channels 5 10 1 2 3 4 Num. of Joint Channels 2.5 5.0 7.5 10.0 1 2 3 4 Num. of Joint Channels 2.5 5.0 7.5 10.0 Sum of Marginal Entropy Joint Entropy Figure 1: Growth rate of joint entropy versus sum of marginal entropies of the LLaMA-7b key/value activation embeddings on 262k tokens of WikiText-2. Entropy is estimated using Equation 4. The slower growth rate of joint entropy implies that jointly quantizing more channels requires fewer bits than quantizing each channel independently. 3 Methodology In this section, we motivate our proposal using information theory and introduce the Coupled Quantization (CQ) approach for KV cache compression. 3.1 Motivations Our proposed approach is inspired by concepts in information theory [28]. We consider each channel in a key/value activation embedding as a random variableX1, X2, . . .. The amount of information (oruncertainty)inchannel X canbemeasuredby entropy, definedasH(X) =− R X p(x) log2 p(x) dx, where p(·) is the probability density function andX is the support of X. H(X) measures the theoretical number of bits needed for losslessly encoding the channelX, so it can be used to gauge how “quantizable” a channel is: ifH(X1) < H(X2), then channelX1 may be quantized to fewer bits than channelX2 while achieving the same quantization error. Our insight is that different channels from the same key/value activation embedding may be interdependent, which would reduce the number of bits required for jointly encoding multiple channels together compared to encoding them independently. The total amount of information (or uncertainty) in two channelsX1, X2 is measured by joint entropy, defined as H(X1, X2) = − R X1 R X2 p(x1, x2) log2 p(x1, x2) dx2 dx1, wherep(·, ·) is the joint probability density function. The joint entropy of two channels is the difference between the sum of their marginal entropies and their mutual information, i.e.,H(X1, X2) =H(X1) +H(X2) − I(X1, X2), whereI(·, ·) is a non-negative quantity for measuring the mutual dependency of two random variables. Thus, we have H(X1, X2) ≤ H(X1) +H(X2) (3) which implies the number of bits needed for jointly encoding two channels is no more than the total number of bits needed for encoding them independently. Previous works have demonstrated that 41 16 32 LLaMA-7b Key Correlations Layer 1  Layer 2  Layer 13  Layer 14  Layer 28  Layer 29  Layer 31  Layer 32 1 16 32 Channels 1 16 32 LLaMA-7b Value Correlations 1 16 32 Channels 1 16 32 Channels 1 16 32 Channels 1 16 32 Channels 1 16 32 Channels 1 16 32 Channels 1 16 32 Channels 1.0 0.5 0.0 0.5 1.0 Figure 2: Correlation matrices of the first 32 channels of 8 layers of LLaMA-7b key and value activation embeddings on WikiText-2. Channel pairs exhibit high levels of linear dependency, shown by high magnitudes of the correlation coefficients. deep neural networks [11] and attention-based networks [7] tend to produce low-rank embeddings, which suggests that channels of key/value embedding in LLM may exhibit high amount of mutual dependency. It is hence beneficial to measure the difference between the joint entropy of multiple channels and the sum of their marginal entropies in key and value activation embeddings. A significant difference would suggest that encoding these channels together is more information-efficient than encoding them independently. However, it is intractable to derive the exact entropy or joint entropy of channels, since their probability density functions are not known. Therefore, we employ the “binning” trick [17] to estimate entropy. We first observe an empirical distribution of key and value channels by saving the KV cache on a dataset, and partition the support of each channel into equally sized bins. Then, values of each channel are discretized to the index of the bin they fall into. Finally, the joint entropy ofn channels X1, . . . , Xn is estimated with the Riemann sum, H(X1, . . . , Xn) ≈ X x1∈B1 ··· X xn∈Bn bp(x1, . . . , xn) log2 bp(x1, . . . , xn) (4) where Bi is the support of the binned or discretizedXi and bp(·) is the empirical probability density function. Specifically, we divide the channels of key and value embeddings of LLaMA-7b into non- overlapping groups ofc contiguous channels, wherec ∈ {1, 2, 3, 4}, and estimate the joint entropy and the sum of marginal entropies of each group. The support of each channel is partitioned into 16 equally sized bins. Figure 1 shows the mean and standard deviation of the estimated joint entropy and sum of marginal entropies of three layers of LLaMA-7b on 262k tokens of the WikiText-2 dataset, averaged over groups. We only show a maximum group size of 4, since increasing the group size requires saving exponentially more key and value embeddings to avoid empty bins and maintain estimation quality. As shown in Figure 1, the sum of marginal entropies grows at a linear rate while the joint entropy increases slower at a sub-linear rate. This implies that as the number of jointly quantized channels increases, the total amount of information needed for encoding decreases. This phenomenon is the foundation that motivates our proposed approach. In addition to presenting the estimated marginal and joint entropy, we also show the Pearson correlation coefficients between channels of LLaMA-7b key and value activation embeddings on WikiText-2. Correlation coefficient captures the linear dependency between two random variables. Heat maps for the correlation matrices of 8 layers are shown in Figure 2, while the correlation matrices of all layers are presented in Section 6 of the Appendix. The key and value channels 5Channel-wise Quantization Coupled Quantization q0 q1 channel 0 -0.57 0  0.21 1  -0.38 0  -0.13 1  key or value activation embedding channel 1 channel-wise centroids q -0.62 0  0.11 1  -0.38-0.10 0.30 2  -0.38 3  -0.13-0.24 coupled channels 0 & 1 key or value activation embedding multi-channel centroids Figure 3: A comparison of 1-bit channel-wise quantization and our proposed Coupled Quantization (using 2 bits per 2 channels as an example). The quantization results on the first two channels of the first-layer key activation embeddings of LLaMA-7b on the WikiText-2 dataset are shown. Channel-wise quantization is ineffective at capturing the original values at low widths, while CQ leverages the dependency between channels to achieve low quantization errors. exhibit high levels of linear dependency and they are clearly not independently distributed, as shown by high magnitudes of the correlation coefficients. 3.2 Coupled Quantization Motivated by the finding that distinct key/value channels exhibit high dependency, we propose Coupled Quantization (CQ), an information-efficient quantization approach for compressing LLM KV cache. Unlike existing KV cache quantization methods which quantizes channel-wise or token- wise, CQ performs channel-coupled quantization for keys and values. More concretely, channels of a key or value activation embedding are divided into equally sized, non-overlapping groups of contiguous channels. The channels in each group arecoupled, as they are jointly quantized and share a single quantization code. For each group of coupled channels, a distinct set of multi-channel centroids are learned, where each centroid has dimensionality equal to the number of channels in that group. When quantizing a key or value activation embedding, each group of coupled channels are quantized to the nearest centroid in terms of L2 distance. We use the CQ-<c>c<b>b notation to denote the configuration of channel coupling and quantization bit width, where<c> is the number of channels in each group and<b> indicates the number of bits in a quantized code for a group. For example, CQ-4c8b means that every 4 contiguous channels are coupled together and each coupled group shares an 8-bit code, which is equivalent to 2-bit channel-wise quantization in terms of storage overhead of quantized codes. A illustrative comparison of channel-wise quantization and CQ is shown in Figure 3. Although previous works [10, 21] opt to quantize keys channel-wise and values token-wise, we adopt channel-coupled quantization for both keys and values. Similar to existing approaches [10, 21], CQ quantizes keys before RoPE [29] is applied, which increases the quantization difficulty by introducing more outliers in key activations. 3.2.1 Centroid Learning In CQ, the multi-channel centroids for each channel group are learned offline on a calibration dataset by leveraging uniform clustering or second-order-information-informed clustering. Specifically, for uniform centroid learning of the CQ-c cb b configuration, a set of centroidsC⋆ i ⊂ Rc is learned 61c1b 2c2b 4c4b 8c8b CQ Conﬁg. 101 102 103 Perplexity on WikiText-2 2642.72 1614.02 883.60 32.12 620.08 28.39 10.47 8.095.68 LLaMA-7b 1-bit CQ Uniform CQ Fisher-guided CQ FP16 Baseline 1c2b 2c4b 4c8b CQ Conﬁg. 101 102 Perplexity on WikiText-2 204.56 24.46 6.866.37 6.08 5.97 LLaMA-7b 2-bit CQ 1c1b 2c2b 4c4b 8c8b CQ Conﬁg. 750 1000 1250 1500 1750 2000Quant. Error on WikiText-2 LLaMA-7b 1-bit CQ Uniform CQ Key Uniform CQ Value Fisher-guided CQ Key Fisher-guided CQ Value 1c2b 2c4b 4c8b CQ Conﬁg. 600 800 1000Quant. Error on WikiText-2 LLaMA-7b 2-bit CQ Figure 4: Perplexity and key/value quantization errors (averaged over all layers) of LLaMA-7b on WikiText-2. Channels coupling and Fisher-guided centroid learning are effective for improving perplexity. independently for each channel groupi through the objective C⋆ i = arg min C∈Rc |C|=2b \r\r\rAic:(ic+c−1), ∗ − cq \u0000 Aic:(ic+c−1), ∗ \u0001\r\r\r 2 F (5) where Aic:(ic+c−1), ∗ is the sub-matrix ofA containing all coupled channels of thei-th group, and cq quantizes each column vector to the nearest centroid inC in terms of L2 distance. We use the k-means algorithm [22] with k-means++ initialization [1] to optimize the objective. LLMs are more sensitive to the quantization precision of certain weights than others [16]. To better preserve model quality, centroids of CQ should be learned to be biased towards preserving the precision of more important activations. To this end, we leverage an approximation to the Hessian to perform second-order-information-informed centroid learning. More concretely, we use the diagonals of the Fisher information matrixF to identify the more influential key/value activations and guide the centroid learning process. This method was proposed by Li et al. [18] and used by Kim et al. [16] for channel-wise weight quantization, and we extend it to multi-channel CQ. For performing Fisher-guided centroid learning, we first save a key/value activation matrixA and its gradient g(A) = ∂ ∂A L(A) on a calibration dataset, whereL is the training loss function. We approximate the Hessian matrix using the diagonals of the Fisher information matrix, which is the element-wise square of the gradient matrixdiag(F) = g(A) ⊙ g(A). We use the sum of diagonal entries of the Fisher information matrix as a measure of importance for each group of channel-coupled activations, and obtain the centroid setC⋆ i for thei-th channel group using the objective C⋆ i = arg min C⊂Rc |C|=2b X j g \u0000 Aic:(ic+c−1), j \u0001⊤g \u0000 Aic:(ic+c−1), j \u0001\r\r\rAic:(ic+c−1), j− cq(Aic:(ic+c−1), j) \r\r\r 2 F (6) We leverage weighted k-means to optimize the objective. The overhead of the centroid learning process and centroid storage are discussed in Section 4.3. We validate the effectiveness of our proposed channel-coupling and Fisher-guided centroid learn- ing by compressing LLaMA-7b KV cache to 1-bit and 2-bit, and present the perplexity results and quantization errors (∥A − cq(A)∥2 F averaged over layers) on WikiText-2 under different CQ con- figurations in Figure 4. The experimental setup is given in Section 4. As the number of coupled channels increases, perplexity and quantization errors improve significantly, approaching the FP16 7Table 1: Perplexity on WikiText-2 under different KV cache quantization methods at varying bit- width. The results of INT, NF, and KVQuant (excluding 1b and 1b-1%) are from [10]. “NaN” means Not a Number, which is caused by numerical stability issues of quantization. Our method CQ consistently outperforms non-dense-and-sparse quantization methods, and performs better or on par with the dense-and-sparse method KVQuant-<b>b-1%. Bits Per FPN LLaMA-7b LLaMA-13b LLaMA-2-7b LLaMA-2-13b Mistral-7b FP16 16 5.68 5.09 5.12 4.57 4.76 INT4 4.00-4.01 5.98 5.32 5.66 5.01 4.97 INT4-gs128 4.16 5.77 5.16 5.32 4.71 4.82 NF4 4.00-4.01 5.87 5.23 5.47 4.90 4.91 NF4-gs128 4.16 5.77 5.17 5.30 4.71 4.83 KVQuant-4b 4.00-4.02 5.73 5.15 5.18 4.63 4.81 KVQuant-4b-1% 4.32-4.35 5.70 5.11 5.14 4.59 4.78 CQ-2c8b 4.00 5.70 5.11 5.14 4.59 4.79 INT2 2.00-2.01 11779 69965 4708 3942 573 INT2-gs128 2.14 37.37 41.77 117.88 93.09 51.96 NF2 2.00-2.02 3210.5 5785.6 13601 4035.6 902.51 NF2-gs128 2.14 351.23 141.19 634.59 642.44 252.85 KVQuant-2b 2.00-2.02 8.17 7.29 9.75 29.25 7.33 KVQuant-2b-1% 2.32-2.35 6.06 5.40 5.50 4.92 5.16 CQ-4c8b 2.00 5.97 5.32 5.42 4.81 5.11 KVQuant-1b 1.00-1.02 321.58 1617.40 NaN 4709.83 203.73 KVQuant-1b-1% 1.32-1.35 9.93 7.97 9.50 13.76 10.07 CQ-8c8b 1.00 8.09 7.02 7.75 6.55 7.25 CQ-8c10b 1.25 6.78 6.00 6.25 5.47 5.90 baseline performance. Although Fisher-guided centroid learning increases the quantization error, it better preserves the salient activations and achieve lower perplexity. 4 Experiments In this section, we perform extensive experiments to validate the effectiveness of our proposed CQ approach for KV cache compression. We first introduce the experimental setups including hardware, software, metrics, datasets, and baselines used. Then, we present the detailed empirical results and provide discussions. Finally, we perform an ablation study to validate the effectiveness of each component of our proposal. Hardware Testbed Experiments are performed on a Linux server running Ubuntu 20.04, equipped with 2 AMD EPYC 7742 CPUs, 1.5TB RAM, and 4 NVIDIA A100 40GB GPUs. Software ImplementationOur software implementation of CQ is based on PyTorch [24] and the HuggingFace Transformers library [33]. Evaluation Metrics and BenchmarksWe evaluate the quality of 5 popular open-source LLMs on various benchmarks under different KV cache quantization algorithms. The 5 LLMs con- sidered are 1. LLaMA-7b, 2. LLaMA-13b [30], 3. LLaMA-2-7b, 4. LLaMA-2-13b [31], 5. Mistral-7b [13]. We evaluate LLM quality using the perplexity metric on 2 datasets: WikiText-2 [23] and C4 [26], and accuracy on 3 benchmarks in zero-shot setting: WinoGrande [27], PIQA [2], and ARC Challenge [4]. Perplexity is evaluated on the test set of the datasets at the maximum context length 8Table 2: Perplexity on C4 under different KV cache quantization methods at varying bit-width. The results of INT, NF, and KVQuant (excluding 1b and 1b-1%) are from [10]. Our method CQ consistently outperforms non-dense-and-sparse quantization methods, and performs better or on par with the dense-and-sparse method KVQuant-<b>b-1%. Bits Per FPN LLaMA-7b LLaMA-13b LLama-2-7b LLaMA-2-13b Mistral-7b FP16 16 7.08 6.61 6.63 6.05 5.71 INT4 4.00-4.01 7.40 6.82 7.31 6.59 5.91 INT4-gs128 4.16 7.16 6.67 6.87 6.20 5.76 NF4 4.00-4.01 7.27 6.74 7.09 6.45 5.85 NF4-gs128 4.16 7.16 6.66 6.86 6.20 5.77 KVQuant-4b 4.00-4.02 7.13 6.65 6.70 6.11 5.75 KVQuant-4b-1% 4.32-4.35 7.09 6.62 6.65 6.06 5.72 CQ-2c8b 4.00 7.11 6.64 6.67 6.09 5.74 INT2 2.00-2.01 10892 100870 4708 4220 477 INT2-gs128 2.14 43.49 56.25 113.49 97.04 50.73 NF2 2.00-2.02 2850.1 4680.3 13081.2 4175.6 1102.3 NF2-gs128 2.14 248.32 118.18 420.05 499.82 191.73 KVQuant-2b 2.00-2.02 10.28 9.05 15.16 43.77 8.40 KVQuant-2b-1% 2.32-2.35 7.38 6.83 7.06 6.38 6.08 CQ-4c8b 2.00 7.52 6.96 7.23 6.52 6.17 KVQuant-1b 1.00-1.02 168.90 1316.41 362.94 4223.37 127.07 KVQuant-1b-1% 1.32-1.35 11.18 9.56 16.04 22.87 10.53 CQ-8c8b 1.00 12.13 10.53 12.49 10.53 9.89 CQ-8c10b 1.25 9.12 8.23 9.03 8.01 7.46 of the LLM (2048 for LLaMA, 4096 for LLaMA-2, and 8192 for Mistral). Baselines We compare our proposed approach with uncompressed FP16 KV cache and com- petitive KV cache quantization methods, including 1. uniform integer (INT) quantization (without grouping and with a group size of 128), 2. NormalFloat (NF) quantization [6] (without grouping and with a group size of 128), 3. KVQuant [10] (without sparse outliers and with 1% outliers stored in sparse format). KVQuant-<b>b-1% is a dense-and-sparse method that requires an additional sparse matrix multiplication in addition to the dense matrix multiplication during inference, which introduces additional computational overhead. KVQuant and our proposed CQ both require learn- ing centroids on a calibration dataset, and we use the same calibration set of 16 sequences (each with 2048 tokens) of WikiText-2 for both methods. Calibration is performed once on the training set of WikiText-2, while perplexity and accuracy are evaluated on the test sets of different datasets and benchmarks. For each method, we report bits per floating-point number (FPN) to measure the compression rate, which is calculated as the number of bits in the quantized KV cache of each token divided by the number of FPN in the uncompressed KV cache of each token, excluding the constant storage overheads of centroids, scaling factors, and zero points. 4.1 Results The results of perplexity on WikiText-2 are presented in Table 1 and the results on C4 are presented in Table 2. CQ consistently outperforms non-dense-and-sparse quantization methods, especially in low bit-width regions. Despite using lower bit-width, CQ performs better or on par with the dense-and-sparse quantization method KVQuant-<b>b-1%. Dense-and-sparse quantization methods 9Table 3: Accuracy on 3 benchmarks under different KV cache quantization methods at varying bit-width. Bits Per FPN Task LLaMA-7b LLaMA-13b LLaMA-2-7b LLaMA-2-13b Mistral-7b FP16 16 WinoGrande 69.93 72.69 68.90 71.98 73.88 PIQA 78.67 79.16 78.07 79.16 80.58 ARC Challenge 41.72 46.42 43.43 48.29 50.34 KVQuant-4b 4.00-4.02 WinoGrande 69.53 72.61 67.96 71.59 73.88 PIQA 78.62 79.22 77.86 78.94 80.58 ARC Challenge 42.32 45.99 42.75 46.67 49.06 KVQuant-4b-1% 4.32-4.35 WinoGrande 70.72 73.40 68.67 72.30 73.72 PIQA 78.40 79.16 78.07 79.27 80.74 ARC Challenge 41.38 46.76 43.17 47.87 49.91 CQ-2c8b 4.00 WinoGrande 70.40 72.45 68.27 72.53 73.48 PIQA 78.61 79.11 77.91 78.62 80.52 ARC Challenge 41.55 45.99 43.34 47.78 49.15 KVQuant-2b 2.00-2.02 WinoGrande 53.59 59.35 51.70 51.30 63.46 PIQA 72.47 74.81 63.38 65.40 75.46 ARC Challenge 32.00 34.47 22.44 24.66 38.57 KVQuant-2b-1% 2.32-2.35 WinoGrande 68.03 71.43 67.64 70.17 70.80 PIQA 77.69 78.51 76.60 78.51 79.65 ARC Challenge 38.74 45.14 41.47 44.97 47.53 CQ-4c8b 2.00 WinoGrande 67.48 70.72 66.45 69.06 69.38 PIQA 76.11 78.29 76.12 77.42 79.49 ARC Challenge 38.48 44.03 39.93 44.11 45.65 KVQuant-1b 1.00-1.02 WinoGrande 50.51 48.46 50.91 49.41 49.80 PIQA 53.26 53.54 53.37 50.92 54.73 ARC Challenge 21.76 21.33 20.65 21.67 19.88 KVQuant-1b-1% 1.32-1.35 WinoGrande 56.67 61.01 57.77 57.30 58.17 PIQA 71.38 75.46 69.91 70.89 73.83 ARC Challenge 29.69 35.32 31.48 32.59 33.19 CQ-8c8b 1.00 WinoGrande 56.51 61.56 55.01 57.14 58.25 PIQA 71.16 73.99 71.22 73.01 75.24 ARC Challenge 30.20 33.79 30.20 34.30 33.79 CQ-8c10b 1.25 WinoGrande 60.46 65.27 59.19 62.98 63.93 PIQA 73.45 75.90 73.07 74.37 77.31 ARC Challenge 33.28 37.12 34.64 38.74 39.59 introduce additional inference overhead due to the extra sparse matrix multiplications for activation outliers which is inefficient on GPUs, while CQ does not have this limitation. The accuracy results on different benchmarks are shown in Table 3. CQ consistently outperforms the non-dense-and-sparse baseline KVQuant-<b>b at bit-width of 1 and 2, and performs better or on par with the dense-and-sparse baseline KVQuant-<b>b-1%. 4.2 Ablation Study We perform a set of ablation experiments to study the effectiveness of each component of our proposed approach. The results of the ablation experiments are shown in Table 4. We evaluate the perplexity of 2 models Mistral-7b and LLaMA-2-13b on WikiText-2 using CQ at 2 bits per FPN, withvaryingnumberofchannelscoupledandcomparinguniformcentroidlearningandFisher-guided centroid learning. Fisher-guided centroids significantly improve model quality as demonstrated by lower perplexity. With either uniform centroids or Fisher-guided centroids, perplexity improves as the number of coupled channels increases. Hence, our proposed techniques of channel coupling and 10Table 4: Ablation study: perplexity on WikiText-2 using CQ with varying number of coupled channels and fisher-guide centroids. Perplexity consistently improves as the number of coupled channels increases. Mistral-7b LLaMA-2-13b Bits Per FPN 2 2 2 2 2 2 2 2 2 2 2 2 Num. of Channels Coupled 1 2 4 1 2 4 1 2 4 1 2 4 Fisher-guided Centroids ✗ ✗ ✗ ✓ ✓ ✓ ✗ ✗ ✗ ✓ ✓ ✓ Perplexity ↓ 97.76 16.29 5.42 5.34 5.20 5.11 890.42 171.96 6.62 6.06 4.91 4.81 Fisher-guided centroid learning are both effective for maintaining model quality. 4.3 Overhead of Centroid Learning and Storage In this section, we discuss the computational overhead of centroid learning and the memory overhead of centroid storage for CQ. The centroid learning process of CQ consists of many independent k- means runs, which can be time-consuming on CPUs. Hence, we leverage a GPU implementation to accelerate the learning process. In all our experiments, we use k-means++ initialization and run 100 iterations of k-means on a single GPU to obtain the centroids. The memory overhead of storing the centroids can be calculated asl × 2 × h × c × 2b 16-bit floating-point numbers, wherel is the number of layers,2 is for keys and values,h is the number of key/value attention heads,c is the number of channels in a single-head key/value activation embedding, andb is the bit width of quantized codes. The detailed learning and memory overhead for different CQ configurations and models are given in Table 5. CQ can easily scale to large model sizes with low learning and memory overheads. Table 5: Learning and memory overhead of different CQ configurations and models. The number of centroid parameters are shown in millions, and the percentage to the model weights is shown in brackets. Centroid Learning Time Parameter Count in Centroids CQ Config. 2c8b 4c8b 8c8b 2c8b 4c8b 8c8b LLaMA-7b 53 mins 28 mins 14 mins 67.11M (0.996%) 67.11M (0.996%) 67.11M (0.996%) LLaMA-13b 94 mins 44 mins 22 mins 104.86M (0.806%) 104.86M (0.806%) 104.86M (0.806%) LLaMA-2-7b 54 mins 28 mins 14 mins 67.11M (0.996%) 67.11M (0.996%) 67.11M (0.996%) LLaMA-2-13b 83 mins 44 mins 23 mins 104.86M (0.806%) 104.86M (0.806%) 104.86M (0.806%) Mistral-7b 13 mins 7 mins 4 mins 16.78M (0.232%) 16.78M (0.232%) 16.78M (0.232%) 5 Related Works The high memory requirements and computational demands of LLMs pose a great challenge to effi- cient inference. Post-training model weight quantization has been shown to be effective for reducing inference latency and memory requirements. GPTQ [8] scales approximate Hessian-based weight quantization to large-scale models, and AWQ [19] preserves the salient weights in LLMs to achieve better quantized model quality. SqueezeLLM [16] leverages sensitivity-based non-uniform clustering and dense-and-sparse quantization for preserving salient weights and outliers. In addition to weight quantization, KV cache compression approaches are also effective for improving inference efficiency. Scissorhands [20] and H2O [37] achieve KV cache compression while preserving model quality by 11evicting unimportant tokens and only storing pivotal tokens. KIVI [21] quantizes key activations channel-wise and value activations token-wise, and uses a residual to achieve better model quality. KVQuant [10] proposes sensitivity-based quantization and dense-and-sparse quantization for KV cache. FlashAttention [5] improves the inference efficiency of attention-based models on GPUs by fusing kernels to eliminate unnecessary global memory reads/writes, while NoMAD-Attention [36] accelerates attention computations on CPUs by leveraging in-register shuffles. Product Quantiza- tion [12] is an approximate nearest neighbor search method that compresses vectors by decomposing the vector space into a Cartesian product of low-dimensional subspaces, and jointly quantizing the dimensions within each subspace. 6 Conclusion In this work, we propose Coupled Quantization (CQ) for enabling more efficient LLM inference by compressing KV cache, which is the latency and throughput bottleneck in long context or large batch size settings. We observe that distinct channels of key/value activation embeddings exhibit high levels of dependency, which has not been leveraged by existing compression methods. Motivated by this insight, we propose channel coupling for exploiting the inter-channel dependency to achieve more information-efficient encoding of key/value activations. Furthermore, we propose Fisher-guided centroid learning to better preserve salient activations and model quality. Extensive experiments demonstrate that our method mostly outperforms existing methods in terms of model quality under the same quantization bit width. Moreover, CQ can preserve model quality reasonably well with KV cache quantized down to 1-bit. References [1] David Arthur, Sergei Vassilvitskii, et al. k-means++: The advantages of careful seeding. In Soda, volume 7, pages 1027–1035, 2007. [2] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about phys- ical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 7432–7439, 2020. [3] Lingjiao Chen, Matei Zaharia, and James Zou. Frugalgpt: How to use large language models while reducing cost and improving performance.arXiv preprint arXiv:2305.05176, 2023. [4] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [5] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness.Advances in Neural Information Processing Systems, 35:16344–16359, 2022. [6] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient fine- tuning of quantized llms.Advances in Neural Information Processing Systems, 36, 2024. [7] Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: Pure attention loses rank doubly exponentially with depth. InInternational Conference on Machine Learning, pages 2793–2803. PMLR, 2021. 12[8] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post- training quantization for generative pre-trained transformers.arXiv preprint arXiv:2210.17323, 2022. [9] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models.arXiv preprint arXiv:2203.15556, 2022. [10] Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W Mahoney, Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cache quantization.arXiv preprint arXiv:2401.18079, 2024. [11] Minyoung Huh, Hossein Mobahi, Richard Zhang, Brian Cheung, Pulkit Agrawal, and Phillip Isola. The low-rank simplicity bias in deep networks.arXiv preprint arXiv:2103.10427, 2021. [12] Herve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search. IEEE transactions on pattern analysis and machine intelligence, 33(1):117–128, 2010. [13] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chap- lot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b.arXiv preprint arXiv:2310.06825, 2023. [14] Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. Challenges and applications of large language models. arXiv preprint arXiv:2307.10169, 2023. [15] Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke Hüllermeier, et al. Chatgpt for good? on opportunities and challenges of large language models for education.Learning and individual differences, 103:102274, 2023. [16] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023. [17] Alexander Kraskov, Harald Stögbauer, and Peter Grassberger. Estimating mutual information. Physical review E, 69(6):066138, 2004. [18] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. arXiv preprint arXiv:2102.05426, 2021. [19] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023. [20] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time.Advances in Neural Information Process- ing Systems, 36, 2024. 13[21] Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: A tuning-free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750, 2024. [22] Stuart Lloyd. Least squares quantization in pcm.IEEE transactions on information theory, 28(2):129–137, 1982. [23] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. [24] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An impera- tive style, high-performance deep learning library.Advances in neural information processing systems, 32, 2019. [25] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.OpenAI blog, 1(8):9, 2019. [26] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1–67, 2020. [27] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale.Communications of the ACM, 64(9):99–106, 2021. [28] Claude Elwood Shannon. A mathematical theory of communication.The Bell system technical journal, 27(3):379–423, 1948. [29] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding.Neurocomputing, 568:127063, 2024. [30] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models.arXiv preprint arXiv:2302.13971, 2023. [31] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models.arXiv preprint arXiv:2307.09288, 2023. [32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need.Advances in neural information processing systems, 30, 2017. [33] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Transformers: State-of- the-art natural language processing. InProceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, pages 38–45, 2020. [34] Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective long- context scaling of foundation models.arXiv preprint arXiv:2309.16039, 2023. 14[35] Xi Yang, Aokun Chen, Nima PourNejatian, Hoo Chang Shin, Kaleb E Smith, Christopher Parisien, Colin Compas, Cheryl Martin, Anthony B Costa, Mona G Flores, et al. A large language model for electronic health records.NPJ digital medicine, 5(1):194, 2022. [36] Tianyi Zhang, Jonah Wonkyu Yi, Bowen Yao, Zhaozhuo Xu, and Anshumali Shrivastava. Nomad-attention: Efficient llm inference on cpus through multiply-add-free attention.arXiv preprint arXiv:2403.01273, 2024. [37] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H2o: Heavy-hitter oracle for effi- cient generative inference of large language models.Advances in Neural Information Processing Systems, 36, 2024. 15Appendix A Correlation Matrices and Scatter Plots 0 15 31 Layer 1  Layer 2  Layer 3  Layer 4  Layer 5  Layer 6  Layer 7  Layer 8 0 15 31 Layer 9  Layer 10  Layer 11  Layer 12  Layer 13  Layer 14  Layer 15  Layer 16 0 15 31 Layer 17  Layer 18  Layer 19  Layer 20  Layer 21  Layer 22  Layer 23  Layer 24 0 15 31 0 15 31 Layer 25 0 15 31 Layer 26 0 15 31 Layer 27 0 15 31 Layer 28 0 15 31 Layer 29 0 15 31 Layer 30 0 15 31 Layer 31 0 15 31 Layer 32 1.0 0.5 0.0 0.5 1.0 Channel Channel Figure 5: Correlation matrix for the first 32 channels of pre-RoPEkey activation embeddings of all LLaMA-7b layers on WikiText-2. 160 15 31 Layer 1  Layer 2  Layer 3  Layer 4  Layer 5  Layer 6  Layer 7  Layer 8 0 15 31 Layer 9  Layer 10  Layer 11  Layer 12  Layer 13  Layer 14  Layer 15  Layer 16 0 15 31 Layer 17  Layer 18  Layer 19  Layer 20  Layer 21  Layer 22  Layer 23  Layer 24 0 15 31 0 15 31 Layer 25 0 15 31 Layer 26 0 15 31 Layer 27 0 15 31 Layer 28 0 15 31 Layer 29 0 15 31 Layer 30 0 15 31 Layer 31 0 15 31 Layer 32 1.0 0.5 0.0 0.5 1.0 Channel Channel Figure 6: Correlation matrix for the first 32 channels ofvalue activation embeddings of all LLaMA- 7b layers on WikiText-2. Figure 7: 2-dimensional scatter plots of pairs of channels inkey activation embeddings of 4 LLaMA- 7b layers on WikiText-2. 17Figure 8: 2-dimensional scatter plots of pairs of channels in value activation embeddings of 4 LLaMA-7b layers on WikiText-2. 18",
      "meta_data": {
        "arxiv_id": "2405.03917v1",
        "authors": [
          "Tianyi Zhang",
          "Jonah Yi",
          "Zhaozhuo Xu",
          "Anshumali Shrivastava"
        ],
        "published_date": "2024-05-07T00:25:20Z",
        "pdf_url": "https://arxiv.org/pdf/2405.03917v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper identifies the KV cache as a major bottleneck for LLM inference, especially at large batch sizes or context lengths, and notes that existing quantization methods fail at very low bit widths. It empirically observes that distinct channels of key/value activation embeddings are highly inter-dependent, and their joint entropy grows slower than the sum of marginal entropies. Based on this, the authors propose Coupled Quantization (CQ), a novel KV cache quantization method that couples multiple key/value channels to exploit their inter-dependency for more information-efficient encoding. CQ is shown to preserve model quality effectively, even down to 1-bit compression, outperforming or being competitive with existing baselines, including dense-and-sparse methods without their associated inference overheads.",
        "methodology": "Coupled Quantization (CQ) is motivated by information theory, specifically the observation that joint entropy is less than or equal to the sum of marginal entropies, suggesting efficiency in joint encoding. CQ divides channels of a key or value activation embedding into equally sized, non-overlapping groups of contiguous channels. These coupled channels are jointly quantized and share a single quantization code, where each multi-channel centroid has a dimensionality equal to the number of channels in its group. Quantization maps each channel group to its nearest centroid using L2 distance. Centroids are learned offline on a calibration dataset using either uniform clustering (k-means with k-means++ initialization) or Fisher-guided centroid learning. Fisher-guided learning uses an approximation to the Hessian (diagonals of the Fisher information matrix) to bias centroid learning towards preserving important activations via weighted k-means. Keys are quantized before RoPE application.",
        "experimental_setup": "Experiments were conducted on a Linux server with 2 AMD EPYC 7742 CPUs, 1.5TB RAM, and 4 NVIDIA A100 40GB GPUs, using PyTorch and HuggingFace Transformers. The evaluation involved 5 LLMs: LLaMA-7b, LLaMA-13b, LLaMA-2-7b, LLaMA-2-13b, and Mistral-7b. Model quality was assessed using perplexity on WikiText-2 and C4 datasets, and accuracy on WinoGrande, PIQA, and ARC Challenge benchmarks in a zero-shot setting. Baselines included uncompressed FP16 KV cache, uniform integer (INT) quantization (with/without group size 128), NormalFloat (NF) quantization (with/without group size 128), and KVQuant (with/without 1% sparse outliers). Centroids for CQ and KVQuant were learned on a calibration set of 16 sequences (2048 tokens each) from WikiText-2, using 100 iterations of GPU-accelerated k-means.",
        "limitations": "The exact entropy or joint entropy of channels is intractable to derive, necessitating estimation via a 'binning' trick. This estimation process limits the practical maximum group size for joint entropy analysis (e.g., to 4 channels for LLaMA-7b) because increasing group size requires exponentially more key/value embeddings to avoid empty bins and maintain estimation quality. While centroid learning is accelerated on GPUs, it can be time-consuming on CPUs. Fisher-guided centroid learning, while improving perplexity, might lead to increased quantization error as it prioritizes salient activations over overall error reduction.",
        "future_research_directions": "Not mentioned"
      }
    },
    {
      "title": "Towards Cheaper Inference in Deep Networks with Lower Bit-Width Accumulators",
      "abstract": "The majority of the research on the quantization of Deep Neural Networks\n(DNNs) is focused on reducing the precision of tensors visible by high-level\nframeworks (e.g., weights, activations, and gradients). However, current\nhardware still relies on high-accuracy core operations. Most significant is the\noperation of accumulating products. This high-precision accumulation operation\nis gradually becoming the main computational bottleneck. This is because, so\nfar, the usage of low-precision accumulators led to a significant degradation\nin performance. In this work, we present a simple method to train and fine-tune\nhigh-end DNNs, to allow, for the first time, utilization of cheaper, $12$-bits\naccumulators, with no significant degradation in accuracy. Lastly, we show that\nas we decrease the accumulation precision further, using fine-grained gradient\napproximations can improve the DNN accuracy.",
      "full_text": "TOWARDS CHEAPER INFERENCE IN DEEP NETWORKS WITH LOWER BIT-WIDTH ACCUMULATORS Yaniv Blumenfeld Technion, Israel yanivblm6@gmail.com Itay Hubara Technion, Israel itayhubara@gmail.com Daniel Soudry Technion, Israel daniel.soudry@gmail.com ABSTRACT The majority of the research on the quantization of Deep Neural Networks (DNNs) is focused on reducing the precision of tensors visible by high-level frameworks (e.g., weights, activations, and gradients). However, current hardware still relies on high-accuracy core operations. Most significant is the operation of accumulating products. This high-precision accumulation operation is gradually becoming the main computational bottleneck. This is because, so far, the usage of low-precision accumulators led to a significant degradation in performance. In this work, we present a simple method to train and fine-tune high-end DNNs, to allow, for the first time, utilization of cheaper, 12-bits accumulators, with no significant degradation in accuracy. Lastly, we show that as we decrease the accumulation precision further, using fine-grained gradient approximations can improve the DNN accuracy. 1 I NTRODUCTION Deep Neural Networks (DNNs) quantization (Hubara et al., 2017; Sun et al., 2020; Banner et al., 2018; Nagel et al., 2022; Chmiel et al., 2021) have been generally successful at improving the efficiency of neural networks’ computation without harming the accuracy of the network Liang et al. (2021). The suggested methods aim to reduce the cost of the Multiply-And-Accumulate (MAC) operations for both training and inference. To this end, they quantize the weights, activations, and gradients. For applications utilizing such quantization methods, the cost of multiplications, commonly considered to be the computational bottleneck, can be substantially reduced. However, the accumulation of computed products is still performed with high-precision data types. Consequently, the cost of the accumulation, as a component of MAC operations, becomes increasingly dominant in performance breakdowns (Sakr et al., 2019; Ni et al., 2020; Chmiel et al., 2021). For example, when the weights and activations are in the common FP8 format, van Baalen et al. (2023) showed the accumulation becomes a computational bottleneck. For example, they conducted experiments to estimate the raw gate count for various FP8 implementations (a first-order approx- imation for power and area) and observed a 2× reduction in gate count when employing FP16 accumulators instead of FP32. Similarly, Ni et al. (2020) reported analogous findings for INT8, demonstrating that an 8-bit×8-bit multiplier consumes a comparable amount of power and silicon area to a 32-bit accumulator. In this study, we focus on reducing the numerical precision of the accumulation operation in DNNs. Building our solution on top of the emerging FP8 format, which has gained prominence for both training and inference on the most prevalent hardware Andersch et al. (2022), we aim to optimize such DNNs, to enable inference on hardware with Low Bit-width Accumulators (LBAs). Our main contributions are: • We propose a simple scheme for fine-tuning models with 12-bit accumulators for a variety of tasks, and show this method can already achieve strong performance. For example, we show for the first time that 12-bits accumulators can be used in ResNets on ImageNet, with no significant degradation in accuracy. • We examine more fine-grained approaches, in which, for the first time, we backpropagate through the entire accumulation-computation graph. Though much more expensive during training, such fine-grained backpropagation can be used to significantly improve the accuracy of DNNs with LBAs at lower bit-widths. 1 arXiv:2401.14110v1  [cs.LG]  25 Jan 20242 P RELIMINARIES : Q UANTIZED NEURAL NETWORKS 2.1 Q UANTIZED WEIGHTS AND ACTIVATIONS The quantization of neural networks is, by now, a standard practice for achieving efficient neural networks. Unlike traditional scientific computation, that often (Bailey, 2005) requires high-precision floating point arithmetic (e.g., FP64) to achieve accurate results, it was observed (Gupta et al., 2015) that deep neural networks can maintain high accuracy when the weights and activations in the network are represented in low bit representation. As a result, training deep neural networks (DNNs) using half-precision (FP16) arithmetic became the default setup for modern Deep Learning applications (Brown et al., 2020). Lower precision representation (INT8, FP8, INT4, FP4, and Binary) (Sun et al., 2019; 2020; Courbariaux et al., 2016) is also used for a variety of deep learning applications, for either training or inference, albeit using them is more experimental and may result in lower model performance, depending on the specific application. Quantization of Weights and Activations (W/A) has two main benefits. • Lower memory footprint: By reducing the number of bits used for representation of each numerical value, W/A quantization can significantly reduce the memory required for storing and using a neural network. Consequently, W/A quantization enables storing larger models (with more parameters and activations) on DL accelerators with finite storage and improves the computation efficiency of smaller models by mitigating memory bottlenecks. • Reduced complexity of multiplication operation: Neural networks commonly compute mul- tiplications of weight and activation pairs. When both weight and activation are represented at a lower precision, it is possible to perform the multiplication operation with cheaper hardware (smaller area, less energy). This allows us to do more multiplication operations per second, provided that the hardware was designed to support these lower-precision operations. Numerical values are typically represented using either fixed, or floating point format. Methods for quantization of DNNs can be divided accordingly. 2.2 F IXED POINT QUANTIZATION Given a full-precision value x, a fixed number of bits B, and an integer b (exponent-bias), we define the fixed-point quantization of x as: Rmin ≡ −2B−b−1 Rmax ≡ 2−b \u0000 2B−1 − 1 \u0001 QFIXED B,b (x) ≡    Rmin x ≤ Rmin Rmax x ≥ Rmax 2−bRound \u0000 x · 2b\u0001 else (1) As we can see from Eq. (1), the process of fixed-point quantization involves two explicit changes to the value of x. First, we round x · 2b to an integer value. The rounding operation can be done using a variety of operations (such as Floor, Ceil, Nearest-Neighbour, or Stochastic Rounding (Wang et al., 2018)), but will result in a loss of information either way, with a rounding error that decreases as we increase the parameter b: ∆ ∼ 2−b. If the value of x is sufficiently small |x| < 2−b, the quantization noise will exceed the represented value (∆ > |x|) and we have no way to accurately represent the value of x. We will refer to this event as underflow. Second, we have a limited range for representation, that increases with the number of bits B and decreases with b. We refer to the event when x is outside the range (Rmin, Rmax) as overflow, noting that the quantization error in this case is unbounded. Integer quantization is a specific case of fixed-point quantization, where the exponent biasb is set to 0. While we defined the exponent-bias b to be an integer, it is important to note that non-integer values could have worked mathematically just as well to define valid quantization operations. The main benefit of choosing b to be an integer is the efficiency of computing power-of-two multiplications in hardware. The main advantage of fixed point quantization comes from its relative simplicity. Integer multiplica- tion (and addition) are generally considered to be cheaper on hardware when compared with floating point operations. 22.3 F LOATING POINT QUANTIZATION Given a full-precision scalar value x, number of mantissa bits M, number of exponent bits E, and an integer b (exponent-bias), we define the floating point (Dekker, 1971) quantization M/E: s ≡ 1 2 (1 − sign(x)) , e ≡ ⌊log2(|x|)⌋ m = 2−M Round \u0000 2M (|x|2−e − 1) \u0001 ROF ≡ 22E−b−1 \u0000 2 − 2−M \u0001 , R UF = 2−b QFLOAT M,E,b (x) ≡ (−1)s    ROF |x| ≥ROF 0 |x| < RUF 2e (m + 1) else (2) Note that 1 ≤ |x|2−e < 2, due to the definition of e, which helps make sense of the quantization operation in Eq. (2). The total number of bits used for this representation is B = M + E + 1: 1 sign bit (s), M mantissa bits (m) and E exponent bits (e). As we can see, floating point representation can cover a larger range of values when compared with a fixed point representation that uses the same amount of bits and exponent bias, (ROF/UF depends on 22±E while Rmax, Rmin depends on 2B), reducing the occurrence of overflow and underflow events. Unlike fixed-point representation, which had a fixed bound for quantization error (∆ ∼ 2−b) within the represented range, the quantization error for floating point representation varies, depending on the magnitude of x: ∆ ∼ 2e−M . As a direct result, floating point’s arithmetic also adds additional complexity, in the form of swamping Higham (1993). When performing an addition over two floating points values ¯z = z1 +(FP) z2 ≡ QFLOAT M,E,b (z1 + z2), it is possible that the precision of ¯z will not be sufficient for full-representation of its summands, causing the least significant bits to be swamped out — resulting in a ‘noisy‘ addition operation. In the extreme case, denoted as Full-Swamping, if |z1| > 2M+1|z2|, z2 is swamped out entirely, so ¯z = z1 despite z2 being non-zero. In contrast, fixed-point addition will always be exact, as long as the sum remains within the representation range (no overflow). 2.4 L OW BIT-WIDTH ACCUMULATORS When performing a general matrix multiplication (GEMM) operation, (e.g. matrix-multiplication, or convolution), each individual scalar computed during the operation can be expressed as the sum of product pairs y = N−1X i=0 xiwi. (3) Here, y is a scalar component of the output tensor of the GEMM operation, N is the accumulations size (i.e., the number of summands used per scalar output), while {xi}N−1 i=0 and {wi}N−1 i=0 are two series of scalar inputs used for the calculation of y. The values in both series originate from the input tensors, but the exact mapping, from tensors to series, will depend on the performed operation (see Appendix A for more details). Due to the common structure of the multiply-accumulate operation, hardware implementations of GEMM operation often rely on the fundamental Fused Multiply-Add (FMA) operation, defined as FMA(x, w, s) ≡ x · w + s, with x, w, sbeing scalars. Our goal in this work will be to decrease the cost of the FMA component. Previous discussed methods, such as W/A quantization, have been helpful in reducing the cost of the multiplication of FMA. In contrast, the accumulation component of FMA has been studied to a much lesser extent. In (Wang et al., 2018), the authors show that training a neural network with FP16 accumulators can result in noisy training, with a modest loss of accuracy. To mitigate this, the paper recommends chunk-based accumulation and floating-point stochastic rounding. Chunk-based accumulation changes the order of accumulation, while stochastic rounding is a method where a small, random noise is added to the result of high-precision summation, before the result is cast to a low-precision representation. While successful at closing the gap (e.g., for ResNet18 on ImageNet), both methods may prove difficult to implement on modern hardware. Specifically, the order of accumulation on DL accelerators will usually depend on their block architecture and is not easily configured. Moreover, stochastic rounding requires an implicit addition operation, which is projected to increase the cost of hardware addition, negating the benefit of using LBAs. Sakr et al. (2019) examined the effect of low precision accumulators on training through the accu- mulation variance statistic, which they theoretically derive, given several statistical assumptions on the distribution of the summands. In Ni et al. (2020), the authors propose WrapNet, where the 3additions are performed with 8 and 12 integer accumulators with wrap-around. WrapNet is shown to perform complex inference tasks (e.g. ImageNet classification) with extreme quantization (e.g., 7 bits activations, 2 bit weights, and 12 bits accumulators), but it does suffer a noticeable accuracy degradation in this setup, for tasks such as ImageNet classification. Although mostly experimental, FP16 accumulation was integrated in the design of several commercial products (Agrawal et al., 2021), including the tensor cores in the Hopper architecture (NVIDIA) Andersch et al. (2022). 3 F INE -TUNING NEURAL NETWORKS WITH LOW-BIT ACCUMULATORS One key difference between W/A quantization and quantization of the accumulators is that accumula- tion is an internal FMA operation, which is not generally visible to the software user. To simulate the effect of quantized FMA component, we implement the GEMM operations (convolution/ matrix multiply) in CUDA, where the FMA operation is replaced with our custom FMAq operation: FMAq(x, w, s) ≡ Qacc (Qprod (x · w) + s) , (4) as illustrated in Fig. 1. In all experiments, we use a constant chunk size of 16, based on the sizes exposed to the user of NVIDIA’s tensor cores. It is important to highlight that the product and W i X i  Multiply  Q W Q A  S i - 1  Q p r o d  Add S i Q a c c  FMA  w 0 x 0  FMA  w 1 x 1  FMA  w n - 1 x n - 1  FMA  w n x n  FMA  w n + 1 x n + 1  FMA FMA  w 2 n - 1 x 2 n - 1  Add  w N - n x N - n  FMA FMA FMA  w N - 1 x N - 1 w N - n + 1 x N - n + 1  Q acc  Add  Q acc  Figure 1: Left: an illustration of quantized FMA component, as simulated in our work. Unlike the W/A quantization operations (QW (w), QA(x)) that can be efficiently performed in software, Qprod and Qacc are explicitly internal hardware operations, intended to simulate the logic of a cheaper hardware component. Right: Illustration of chunk-based accumulation, with chunk base of n. Chunk- based accumulation is useful for reducing error caused by swamping, but the chunk size is not easily configured and will usually depend on the architecture design of the systolic array. accumulator quantization functions (Qprod and Qacc) are intended to simulate the hardware, rather than suggest an implementation for it. Breaking down the FMA to components in hardware would, in practice, undermine its efficiency — as it will no longer be ‘fused’. Taking this into account, Qprod and Qacc must remain simple and computationally efficient. For example, ‘round to nearest’ and stochastic rounding methods, which are taken for granted for W/A quantization, will not be available to us during inference, as their hardware implementation would still perform addition internally with a higher number of bits. Our quantization will instead rely on the simple ‘floor’ operation, implemented in software via bit-mask. For Hardware analysis of our implementation, see Appendix E. As discussed in section 2.3, floating point quantization can be broken down into 3-distinct events: underflow, overflow and swamping. Eventually, our low-precision model will have to handle all three events. We will, however, start by examining their individual properties, as displayed in Tab. 1. Our main insight from Tab. 1, is that underflow events are expected to have the least significant effect over the network output (They have the lowest absolute error, since the default value for the exponent bias b, as used by the common FP32/FP16 definitions, is b = 2E−1.). In Fig. 2, we evaluate the correctness of this claim, and show that the wide-scope loss-landscape of an LBA ResNet is barely affected when we ignore UF events. And yet, the large relative error induced during underflow 4Event Condition Key Parameters Absolute Error (bound): ∆ = |Q(x) − x| Relative Error: ∆ |x| Overflow (OF) |x| ≳ 22E−b E, −b ∞ (0%, ∞) Underflow (UF) |x| < 2−b E, b 2−b 100% Swamping No OF/UF M 2⌊log2(|x|)⌋−M \u0002 2−M−1, 2−M \u0003 Table 1: Properties of each type of floating-point quantization event. (small elements are effectively replaced with zero), will cause significant optimization errors for gradient-based methods: During fine-tuning, we can expect the magnitude of the weight updates to be proportional to the magnitude of the corresponding weights, causing the underflow region to be particular hard region to ‘escape’ from. Values that are stuck at underflow are effectively excluded from the training since the forced value of zero prevents them from learning meaningful correlations. (see Appendix F for more details.) 0.2 0.4 0.6 0.8 1.0 0.2 0.4 0.6 0.8 1.0 2 4 6 8 10 2 4 6 8 (a) Baseline LBA 0.2 0.4 0.6 0.8 1.0 0.2 0.4 0.6 0.8 1.0 2 4 6 8 10 2 4 6 8 (b) Excluding UF 0.2 0.4 0.6 0.8 1.0 0.2 0.4 0.6 0.8 1.0 2 4 6 8 10 12 2 4 6 8 10 (c) Excluding Swamping Figure 2: Wide scope loss landscapes Li et al. (2018) of an LBA resnet50, using pre-trained ResNet50 weights (CIFAR10, FP32). Here, we compare the qualitative effect of different components in floating points quantization over the network output: In (a), we use a complete implementation of FP quantization during convolution accumulation, with 7 Mantissa and 4 Exponent bits. In (b), we repeat the previous experiment but ignore underflow events during quantization. For comparison, in (c), we repeat the original experiment, but add 16 additional bits to the mantissa, greatly diminishing the effect of swamping, without affecting the role of underflow. All landscapes appear similar, but while the effect of excluding swamping events (c) is visible, the loss landscapes of networks with (a) and without (b) underflow are hardly distinguishable. Therefore, we propose the following method: Starting off with the weights of a pre-trained net- work (trained in full-precision), we will design a network that utilizes quantized FMA for forward- propagation, excluding underflow events, and perform a standard gradient-based optimization (i.e. Stochastic gradient decent, while keeping the backward implementation of each operation as it was with full-precision FMAs). Once we converge to some accuracy value, we will enable the underflow implementation and proceed with further fine-tuning. As seen in Tab. 1, the exponent bias (b) can be configured to control underflow and overflow events, with a clear trade-off between the former and the latter. Previous works Kuzmin et al. (2022) have made the insight, that the default value b = 2E−1 is not always suitable for neural networks. For our purposes, we note that the different quantization functions Qprod and Qacc as seen in Fig. 1, are likely to require different ranges for representation: Assuming the product terms ui = wixi are i.i.d, the accumulator’s value will follow the central limit theorem, and is therefore more likely to reach overflow, resulting unbounded quantization noise. To try and avoid this scenario, our setup will give a smaller exponent bias to the accumulator. In our experiments, we use a relative factor based on the chunk-size, so that bacc = bprod − 1 2 log2 (Chunk-Size). Following the same reasoning, one may suggest that the exponent bias should depend on the sequence number in which the FMA is applied within every GEMM operation. Nevertheless, for the context of this work, we will treat all FMA units as homogeneous, with the same exponent bias. 53.1 E XPERIMENTS : I MAGE CLASSIFICATION For our first set of experiments, we aim to check the effect low-bit accumulators have on residual neural networks He et al. (2016). For each experiment, we use the standard ResNet architecture and replace each GEMM operation used during forward-propagation (convolution and matrix multiplica- tion) with our custom implementation, as described in section 3. With no adjustments, the effect of these changes on the network accuracy (zero-shot), can be severe, as we show in Appendix B. For Qprod, Qacc, we used the same amount of mantissa and exponent bits, M = 7, E= 4, a setup we will denote as M7E4. For overflow, we used the exponent biases: bacc = 10, bprod = 12, but disabled underflow events for the first part of the experiment. After loading the networks with pre-trained weights, we proceed to train the network for 5 epochs, using Adam optimizer with a learning rate of η0 = 10 −6, and a cosine scheduler, so that η5 = 10 −8). Then, we enable underflow events and run a fine-tuning again for a single epoch, using a reduced learning rate of ηUF = 10−7. To evaluate the benefit of the two-staged fine-tuning, we also ran the same experiment with a single stage, where underflow is enabled for 10 epochs. The baseline numbers were obtained by repeating the fine-tuning process in a non-LBA setup, which resulted in an improvement of up to 0.65% over the zero-shot accuracy. Our full setup and implementation are detailed in Appendix C. The results of this experiment are presented in Tab. 2. Model Baseline 1-stage no UF* no UF → with UF ResNet18 70.23% 69.94% 70.01% 70.06% ResNet34 73.87% 73.64% 73.61% 73.45% ResNet50 76.80% 74.70% 76.60% 76.40% Table 2: Top-1 Accuracy results: Fine-tuning ResNets with low-bit accumulators for ImageNet classification. *Intermediate Stage: Both training and evaluation are done without underflow. For LBA ResNets with full-precision W/A, our results indicate that the models we suggest can train surprisingly well even without a dedicated fine-tuning regime. The dual-stage approach (Training without UF first and enabling it later) only shows clear benefit, so far, in the case of the larger, ResNet50 model. That being said, scaling the method for larger models is important, and tasks will only become more difficult from now on. In order for a model with low-bit accumulators to be commercially viable, it is vital to show that quantized accumulation still works when the weights and activations are quantized. Therefore, our next set of experiments will test the feasibility of LBA ResNets in this setting. For weights and activations, we will use 8-bit floating point representation (Wang et al., 2018). Following the results presented in Kuzmin et al. (2022), we use M4E3 representation with flex-bias for both weights and activations, implemented using the qtorch library Zhang et al. (2019). For our flex-bias implementation, we evaluate the maximal exponent for each tensor during forward propagation, and use the maximal integer exponent bias that is sufficient to prevent overflows (single value per tensor). The results of fine-tuning LBA ResNets in this setup can be seen in Tab. 3, as well as a comparison of our results with previous works that also used lower-bit accumulators. We note that a direct comparison between the methods based on final accuracy alone will not be valid: the method presented in Wang et al. (2018) is intended for quantized training, and includes several more quantized components, as well as several methods that are projected to reduce hardware efficiency. Meanwhile, Ni et al. (2020) proposes the cheapest implementation (Fewer bits for Weights and activations, Integer quantization), sacrificing model accuracy for hardware efficiency. Nevertheless, when aiming for cheaper inference, our LBA models were the only models to achieve accuracy on par with non-LBA models, while providing a cheaper alternative compared to models with standard accumulation. 3.2 E XPERIMENTS : L ANGUAGE MODELS To assess the capability of LBA language models, our next set of experiments will focus on the common Bert (Devlin et al., 2018) architecture, and the SQUAD (Question-Answering) task. In 6Model Data Type Weights Activations Accumulator Top-1 Accuracy ResNet18 Baseline FP 32 32 32 70.23% Baseline (FP8) FP 8 8 32 69.90% Wang et al. (2018) FP 8 8 16 66.95% Ni et al. (2020) INT 7 2 12 63.84% Ours (1-stage) FP 8 8 12 69.54% Ours (dual-stage) FP 8 8 12 69.70% ResNet34 Baseline FP 32 32 32 73.87% Baseline (FP8) FP 8 8 32 73.49% Ours (1-stage) FP 8 8 12 73.18% Ours (dual-stage) FP 8 8 12 73.42% ResNet50 Baseline FP 32 32 32 76.80% Baseline (FP8) FP 8 8 32 76.25% Wang et al. (2018) FP 8 8 16 71.72% Ours (1-stage) FP 8 8 12 74.15% Ours (dual-stage) FP 8 8 12 76.22% Table 3: Top-1 Accuracy results: Fine-tuning ResNets with low-bit accumulators and FP8 weights and activations for ImageNet classification. Results are compared with similar models utilizing LBAs in the literature. this case, fine-tuning a pre-trained model is already the standard. In contrast to our experience with residual networks, breaking down the fine-tuning process into separate phases was not, in general, beneficial for the accuracy of the larger models. The exponent biases we used for the different LBA models also had to be changed, to avoid overflow events. In table4, we compare the results of fine-tuning LBA Bert models with the results of fine-tuning non-LBA models, as described in C.2. While LBA Bert-small has a small (∆f1 = 0.37%) performance degradation compared with the non-LBA model, the gap is closed completely for the Bert ( ∆f1 = −0.09%) and Bert-Large (∆f1 = −0.26%). Baseline LBA (M7E4) bacc,bprod=7,9 LBA (M7E4) bacc,bprod=8,10 Model Exact (%) f1 (%) Exact (%) f1 (%) Exact (%) f1 (%) Bert-Small 71.32 80.96 70.88 80.24 71.35 80.59 Bert-Base 79.84 87.53 79.60 87.62 79.80 87.52 Bert-Large 83.22 90.40 82.97 89.97 83.25 90.66 Table 4: SQUAD v1 fine-tuning for LBA-Bert models. Inspired by our LBA-Bert model results (which were favorable toward larger models), we tested our LBA-aware fine-tuning method on the LLama-v2-7B model (Touvron et al., 2023). We used the same settings and scripts as QLoRA paper (Dettmers et al., 2023), which uses frozen 4-bit weights with an additional trainable low-rank matrix in BF16. To measure performance on a range of language understanding tasks, we used the MMLU (Massively Multitask Language Understanding) benchmark Hendrycks et al. (2020), a multiple-choice benchmark covering 57 tasks. The fine-tuning was done over the Open Assistant (OASSA1) dataset Köpf et al. (2023) using official training scripts found in the QLoRA code (i.e., llama2_guanaco_7b). We report 5-shot test accuracy in tabel 5. Model Baseline M10E5 M6E5 M7E4* LLamma v2 (OASSA1) 45.3 45.4 44.3 45.1 Table 5: MMLU 5-shot test accuracy with and without LBA, for QLORA+ LLama v2 (7B parameters). * For runs with 4 exponent bits, we used dynamic (per-layer) exponent-bias. 74 BELOW 12 BITS : F INE -GRAINED GRADIENT FOR LOW BIT ACCUMULATORS Thus far, we have shown that 12 bits are sufficient for inference in a variety of deep neural networks. However, the simple methods described in section 3 are not sufficient for training neural networks with lower amounts of accumulation bits. For example, a shallow fully-connected DNN trained over MNIST, will fail when using a M4E3 accumulator, even when excluding underflow events. The cause of the failure is known as it is similar to quantization failures in other areas of deep neural network: Quantization changes the output of the network during forward pass, and when the change is significant enough, it is no longer feasible to rely on the gradients of non-quantized operations for optimization. Of course, we cannot use the “real” gradients with respect to quantized operation, since they are zero almost everywhere. The common solution to this problem, with relation to the quantization of weights and activations, is to replace the derivative of the quantized function with a Straight-Through-Estimator (STE). In our case, we would like to use the STE with respect to the derivatives of the quantizers Qacc and Qprod inside the FMAq operation from Eq. (4). So far in this work, we used the naive “identity STE” (Bengio et al., 2013) which makes the replacement “ d dx Q(x)” = 1 (we will use the quotation marks to denote an STE replacement of a derivative). However, the more common STE for quantization zeros out gradients outside of the representation range (Hubara et al., 2017). For the quantizers in Eq. (1) and Eq. (2), we get: “ d dxQFIXED B,b (x)” = 1(Rmin < x < Rmax) ; “ d dxQFLOAT M,E,b (x)” = 1(|x| < ROF), (5) where we defined 1(·) as the indicator function which is equal 1 if its input is ‘true’ and zero otherwise. Many alternative forms of STEs exist and have been studied in the context of W/A quantization. The implementation of STEs for LBA networks, on the other hand, has several additional difficulties. The first, most immediate problem, is that the values of the inputs of the quantization functions within the FMAq (Qacc and Qprod) are not exposed to the software or stored in memory during forward propagation. Saving these internal values is generally not feasible, since the quantization operation occurs in each FMAq, and the number of FMAqs in DNNs typically exceeds the size of weights and activations by many orders of magnitude. However, if the hardware operation is deterministic and well-known, we found we are still able to use software for re-computation of the GEMM operation, to retrieve the required values during backpropagation (1 bit per operation). Such a re-computation operation is expensive (training time is doubled, at the very least), and so far feasible only in fully connected and attention layers (not convolutional layers). To the best our knowledge, this is the first time backpropagation is used on the full computational graph of the summation operation. Another possible problem for using standard STEs for the accumulation process stems from the recursive nature of the summation operation. The STE in equation Eq. (5) sets the corresponding gradient of any overflowing value to zero. As explained in Appendix D, if this STE is used for the accumulator’s quantization function, each overflow event will eliminate the gradients of all previously accumulated product pairs. Lastly, another possible problem is that, for floating point summation, other events besides overflow can potentially be important when estimating the gradient. Motivated by the last two potential problems, in appendix D, we propose, describe, and justify the practicality of several alternative methods for estimating the gradients of FMAq(x, w, s). The different methods use different types of STE: OF passes zero on overflow of Qacc (using Eq. (5), while DIFF passes zero on overflow, underflow, and full-swamping events of the FMAq. We also distinguish between a method where we apply identity STE with respect to the partial sum s, and the non-identity STE over the product-pair (x, w) (a.k.a Immediate), to the standard method, where the STE is applied with respect to all inputs (x, w, s) (a.k.a Recursive). For example, defining z ≡ FMAq(x, w, s) = Qacc (Qprod (x · w) + s) and ϵ1, ϵ2 as some small constants, we get: Immediate / DIFF: “dz ds” = 1 ; 1 w “ dz dx” = 1 x“ dz dw ” = 1 \u0012 |z − s| |xw| + ϵ1 > ϵ2 \u0013 , (6) Recursive / OF: “dz ds” = 1 w “ dz dx” = 1 x“ dz dw ” = 1(|Qprod (xw) + s| < ROF) ) (7) In Tab. 6, we compare the accuracy achieved using the proposed STE variants over the MNIST dataset. We see that such fine-grained gradient methods can indeed enable high accuracy in models with only 8-bit accumulators. 8STE Underflow Accuracy (Top-1, %) STE Underflow Accuracy (Top-1, %) Baseline - 98.65 Immediate / OF Yes 98.47 Identity Yes 18.28 Immediate / DIFF Yes 11.35 Identity No 18.28 Immediate / DIFF No 97.67 +Identity* Yes 42.28 Recursive / OF Yes 98.47 Table 6: Training a fully-connected NN with 8-bit (M4E3) accumulators for MNIST classification. The reported accuracy matches the final accuracy of the experiment. The model’s loss does not converge when using naive (Identity) STE for accumulation. Full details in Appendix C.3. *The mantissa for the accumulator was extended by 2 additional bits in this run. As we saw in the case of residual neural networks (Tab. 2 and 3) with 1-stage training, successful implementation of LBA is not guaranteed to scale to larger models. To evaluate the quality of our estimated gradients, we would like to compare the optimization of the different approaches. To that end, we train a small LBA transformer from scratch for masked language modeling, over a modest-sized dataset (200K rows), for 15 epochs. In Tab. 7, we compare different STE variants for a variety of very-low precision accumulators. Accumulator Identity (%) Recursive / OF (%) Immediate / OF (%) Immediate / DIFF (%) FP32 51.31 - - - M3E3 20.86 19.20 14.80 24.60 M4E3 13.88 39.57 37.23 41.94 M5E3 9.47 45.28 44.76 50.12 M6E3 14.71 46.17 46.13 50.03 M3E4 15.2 15.15 15.43 25.53 M4E4 42.93 42.81 42.81 41.50 M5E4 47.87 48.76 48.76 47.93 Table 7: Accuracy of LBA transformer for the task of Masked Language Modelling (200K rows), when using different STEs for the accumulator operation. Full details of the experiments are available in Appendix C.4. Based on our results for training masked language models, using fine-grained STEs becomes crucial when the number of accumulation bits is decreased below M = 4 or E = 4 (hence, this includes all possible FP8 formats). While successful at improving the optimization, none of the STEs we have tried were successful at closing the gap with the baseline completely, when extreme accumulator quantization was applied. Out of the three proposed STEs, we recommend Immediate/ DIFF STE, which generally achieved better accuracy in the areas where naive, identity STE was insufficient, despite its higher cost. The Immediate/ DIFF STE may also prove more suitable in cases where the exact behavior of the FMAq is unknown (i.e., ’black-box’) since its definition is agnostic to the FMAq internals. 5 D ISCUSSION The quantization of the accumulator in deep neural networks is a hard but necessary task in the effort to improve neural networks’ efficiency, reduce cost, and cut down carbon footprint. Despite the many difficulties involving the training, the implementation, and the theoretical analysis of networks with low-bit-accumulators, our results show that LBA networks are surprisingly easy to fine-tune. By applying simple optimization methods over pre-trained networks, we show it is possible to adjust the models for inference with cheaper hardware, that utilizes12 bits accumulators. When the accumulators bit width is further reduced we alleviate the accuracy degradation by using fine-grained approaches for estimating the gradient. 9ACKNOWLEDGMENTS The research of DS was Funded by the European Union (ERC, A-B-C-Deep, 101039436). Views and opinions expressed are however those of the author only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency (ERCEA). Neither the European Union nor the granting authority can be held responsible for them. DS also acknowledges the support of the Schmidt Career Advancement Chair in AI. REFERENCES Ankur Agrawal, Sae Kyu Lee, Joel Silberman, Matthew Ziegler, Mingu Kang, Swagath Venkatara- mani, Nianzheng Cao, Bruce Fleischer, Michael Guillorn, Matthew Cohen, et al. 9.1 a 7nm 4-core ai chip with 25.6 tflops hybrid fp8 training, 102.4 tops int4 inference and workload-aware throttling. In 2021 IEEE International Solid-State Circuits Conference (ISSCC), volume 64, pp. 144–146. IEEE, 2021. (Cited on 4) Michael Andersch, Greg Palmer, Ronny Krashinsky, Nick Stam, Vishal Mehta, Gonzalo Brito, and Sridhar Ramaswamy. Nvidia hopper architecture in-depth, Apr 2022. URL https: //developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/ . (Cited on 1, 4) David H Bailey. High-precision floating-point arithmetic in scientific computation. Computing in science & engineering, 7(3):54–61, 2005. (Cited on 2) Ron Banner, Itay Hubara, Elad Hoffer, and Daniel Soudry. Scalable methods for 8-bit training of neural networks. In Advances in Neural Information Processing Systems, pp. 5145–5153, 2018. (Cited on 1, 17) Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation, 2013. (Cited on 8) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. (Cited on 2) Brian Chmiel, Ron Banner, Elad Hoffer, Hilla Ben Yaacov, and Daniel Soudry. Logarithmic unbiased quantization: Practical 4-bit training in deep learning. arXiv preprint arXiv:2112.10769, 2021. (Cited on 1) Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks. Advances in Neural Information Processing Systems, 2016. (Cited on 2) Theodorus Jozef Dekker. A floating-point technique for extending the available precision.Numerische Mathematik, 18(3):224–242, 1971. (Cited on 3) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023. (Cited on 7) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. (Cited on 6) Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited numerical precision. In International Conference on Machine Learning, pp. 1737–1746, 2015. (Cited on 2) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. (Cited on 6) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. (Cited on 7) 10Nicholas J Higham. The accuracy of floating point summation.SIAM Journal on Scientific Computing, 14(4):783–799, 1993. (Cited on 3) Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized neural networks: Training neural networks with low precision weights and activations.The Journal of Machine Learning Research, 18(1):6869–6898, 2017. (Cited on 1, 8) Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richárd Nagyfi, et al. Openassistant conversations–democratizing large language model alignment. arXiv preprint arXiv:2304.07327, 2023. (Cited on 7) Andrey Kuzmin, Mart Van Baalen, Yuwei Ren, Markus Nagel, Jorn Peters, and Tijmen Blankevoort. Fp8 quantization: The power of the exponent. Advances in Neural Information Processing Systems, 35:14651–14662, 2022. (Cited on 5, 6) Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. In Neural Information Processing Systems, 2018. (Cited on 5) Tailin Liang, John Glossner, Lei Wang, Shaobo Shi, and Xiaotong Zhang. Pruning and quantization for deep neural network acceleration: A survey. Neurocomputing, 461:370–403, 2021. (Cited on 1) Markus Nagel, Marios Fournarakis, Yelysei Bondarenko, and Tijmen Blankevoort. Overcoming oscillations in quantization-aware training. arXiv preprint arXiv:2203.11086, 2022. (Cited on 1) Renkun Ni, Hong-min Chu, Oscar Castañeda, Ping-yeh Chiang, Christoph Studer, and Tom Gold- stein. Wrapnet: Neural net inference with ultra-low-resolution arithmetic. arXiv preprint arXiv:2007.13242, 2020. (Cited on 1, 3, 6, 7) Charbel Sakr, Naigang Wang, Chia-Yu Chen, Jungwook Choi, Ankur Agrawal, Naresh Shanbhag, and Kailash Gopalakrishnan. Accumulation bit-width scaling for ultra-low precision training of deep networks. arXiv preprint arXiv:1901.06588, 2019. (Cited on 1, 3) Xiao Sun, Jungwook Choi, Chia-Yu Chen, Naigang Wang, Swagath Venkataramani, Xiaodong Cui, Wei Zhang, Kailash Gopalakrishnan, et al. Hybrid 8-bit floating point (hfp8) training and inference for deep neural networks. 2019. (Cited on 2, 17) Xiao Sun, Naigang Wang, Chia-Yu Chen, Jiamin Ni, Ankur Agrawal, Xiaodong Cui, Swagath Venkataramani, Kaoutar El Maghraoui, Vijayalakshmi Viji Srinivasan, and Kailash Gopalakrishnan. Ultra-low precision 4-bit training of deep neural networks. Advances in Neural Information Processing Systems, 33:1796–1807, 2020. (Cited on 1, 2) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. (Cited on 7) Mart van Baalen, Andrey Kuzmin, Suparna S Nair, Yuwei Ren, Eric Mahurin, Chirag Patel, Sundar Subramanian, Sanghyuk Lee, Markus Nagel, Joseph Soriaga, et al. Fp8 versus int8 for efficient deep learning inference. arXiv preprint arXiv:2303.17951, 2023. (Cited on 1, 16) Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan. Training deep neural networks with 8-bit floating point numbers. In Advances in neural information processing systems, pp. 7675–7684, 2018. (Cited on 2, 3, 6, 7) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Huggingface’s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019. (Cited on 13, 14) Tianyi Zhang, Zhiqiu Lin, Guandao Yang, and Christopher De Sa. Qpytorch: A low-precision arithmetic simulation framework, 2019. (Cited on 6, 13) 11A G ENERAL MATRIX MULTIPLICATION : E XAMPLE In section 2.4, we defined the FMA operation, and presented Eq. (3) as a general formula for all GEMM operation. It is worth taking a moment to illustrate the connection between the known tensor operations and the formula. For example, let us look at the simple case of matrix-multiplication (Y = XW T , X∈ Rd0×d1 , W∈ Rd2×d1 ). Here, if we wish to calculate the scalar y = Ykl, we can use the mapping: xi = Xki, wi = Wli. In this case, all values of X and W were used exactly once in the calculation of Y . This is not always the case, however. In batch matrix multiplication, values ofW will be used multiple times, paired with values of X of different batch dimension. In convolution, the same values of W will be used to calculate every scalar in the same output channel, and the neuron in the input channel may be used to calculate a multiple values in multiple output channel. This level of repetition will be, in part, what prevents us from using fine-grained STE methods on convolutional neural networks, in Sec. 4. B E FFECT OF QUANTIZED FMA ON ZERO -SHOT ACCURACY To give the reader a sense of the effect of low bit accumulators on deep neural networks, we include Tab. 8, where we measure the zero-shot accuracy of different ResNet architectures, pretrained with full-precision, after replacing all FMA components with FMAq (as described in C). Mantissa Effect Model Baseline M10E5 M9E5 M8E5 M7E5 M6E5 ResNet18 69.75 69.50 68.95 66.70 57.09 20.49 ResNet34 73.31 73.17 72.68 70.46 60.07 17.19 ResNet50 76.12 75.95 75.57 73.70 64.94 19.48 Exponent Bias Effect (M7E4) Model b = 8 b = 9 b = 10 b = 11 b = 12 bacc, bprod = 10, 12 ResNet18 55.68 60.64 60.00 58.84 56.96 60.14 ResNet34 50.80 63.30 63.88 62.46 59.90 63.65 ResNet50 26.41 64.25 68.69 67.57 66.12 68.49 Table 8: Zeroshot Accuracies for LBA-ResNets, with weights of pre-trained, full precision ResNets [%] The accuracies presented in Tab. 8 illustrates well why M7E4 quantization was chosen: Increasing the mantissa below M = 7 bits would result a much lower zero-shot accuracy, too far for proper fine-tuning. Likewise, reducing the number of bits to E = 4 already resulted lower accuracy due to overflow and underflow events, as indicated by the effect of the exponent bias. For example, the default exponent bias for E = 4 is b = 8, and using it for the accumulator in Resnet50 results in a significant degradation in accuracy. A small increase to b = 9, increases both underflow and overflow thresholds by a factor of 2 and is sufficient for increasing the accuracy by almost 40%. C E XPERIMENTS IMPLEMENTATION DETAILS C.1 I MAGE NET Each of the ImageNet experiments were performed on a single server, containing 8 NVIDIA GPUs (RTX 2080 Ti, RTX A6000). We used a total mini-batch size of 256, equally divided across the 8 workers. For the training datasets, we used the standard RandomResizedCrop and RandomHorizon- talFlip augmentations only. With no quantization, our model architecture was identical to the standard torchvision architecture, for all ImageNet models, while our custom GEMM kernels were used to override all forward GEMM operations (convolutions and matrix multiplications). For optimization, we used the Adam optimizer, with the hyperparameters β = (0.9, 0.999), ϵ= 10−8, λ= 10−4. Dropout was not used. As mentioned in the main text, we used cosine scheduling, the parameters of which depend on the phase in which it was used. We used 10 epochs in the 1-stage compared 12with 5 epochs for the dual-stage to support our claims that the gaps between the methods (where they exist) are not simply a result of better hyperparameters. The epoch count was initially chosen due to time-constraints, and was kept since the benefit of running more epochs was small. For W/A quantization, we used the qtorch (Zhang et al., 2019) library, which provides reliable quantization functions. Weights quantization was applied during every optimization step, while the activations were quantized using dedicated modules, preceding all convolutions, except the first one, and the downsample convolutions. The input of the final fully-connected layer was not quantized as well, in accordance with prior works. The quantization function we applied used stochastic-rounding (which is not considered expensive in this case, as we are not implementing the FMAq internals and the number of quantized components is significantly lower). No other components (e.g. Gradients or Momentum) were quantized since our solution is only aimed at inference. In addition to hyperparameters used in this experiment, we have also ran a set of experiments using fixed-learning rates (no cosine annealing). In the other set, we tested a few initial learning rate values (1E-7, 3E-8, 1E-8) for few epochs, and used enough epochs to reach convergence (for training accuracy/loss). The results in this regime were slightly better than the results published in the paper: For 8bit quantized ResNets with 4ME3, we achieved 69.6% for Resnet18, 73.48% for ResNet34 and 76.35% for ResNet50. However, this required more epochs and finer-tuned hyperparameters (different models used different learning rates). In the paper, we used the regime with cosine annealing since it was more robust to hyperparameter changes. C.2 SQUAD For the SQUAD fine-tuning experiment, we use 8 NVIDIA GPUs (RTX 2080 Ti, RTX A6000). We used the SQUAD training script of the transformers library (Wolf et al., 2019), while using our custom, LBA-model. In our LBA model, all fully connected layers and matrix multiplication operations were modified to use LBA GEMM operations during forward propagation, with the exception of the final fully connected layer (qa-outputs). For pre-trained models, we used either bert-base-uncased for Bert or prajjwal1/bert-small for Bert-small. For optimization, the Adam optimizer, with 1000 warmup steps to a learning rate of 3 · 10−5, from which we applied a cosine annealing scheduler. The batch size was configured to be 8. Our run was set for 20 epochs, but we applied early stopping once the model performance reached its peak (usually after 3 − 5 epochs). C.3 MNIST For each experiment with the MNIST setting, we used a single RTX 2080 Ti GPU with a mini- batch size of 16. Our neural network consisted of 4 fully connected layers (with LBA), and ReLU activations, with all hidden layers being1024 neurons wide. Outside of the accumulator, all data types were with full precision. Dropout wasn’t used (although it was shown to benefit the results slightly), and no data augmentation wasn’t used during training. For optimization, we used Adam optimizer, with an initial learning rate of10−3, with the hyper-parameters: β = (0.9, 0.999), ϵ= 10−8, λ= 0.0, and StepLR scheduler (γ = 0.95). We used 100 epochs per experiment, which was usually much more than needed for convergence or divergence. To test the STE, we replaced the default linear operations with our custom implementation, this time also implementing a custom backward operation. During backpropagation, we used a new, cuda kernel that imitated the (deterministic) operation of the original GEMM operation (using the available weights and activations), but outputted a binary tensor, that indicated the value of all STEs involved in the operation (the type of STE was configurable). For recursive implementation, we modified the tensor ad-hoc to account for the recursive nature of the STE (although less efficient than the optimal implementation). After running the kernel, we used the output to adjust the computation of the weights/ neural gradients as described in section D. In this experiment, we used a fixed exponent bias of 5, which was shown to perform the best among all values in its vicinity. C.4 M ASKED LANGUAGE MODELLING Each of the Masked Language Modelling (MLM) experiments was performed on a single server, containing 8 NVIDIA GPUs (RTX 2080 Ti, RTX A6000, or A100). Our tests were run over theoscar: unshuffled-original-af dataset, with a single tokenizer we trained over the same dataset (vocabulary 13size of 1000). The dataset was chosen due to its moderate size (200K rows), being difficult enough to show gaps in convergence while allowing us to perform meaningful optimizations with simulation kernels in moderate time. For the transformer, we used the Bert architecture, with the hidden size of 512, 2 hidden layers, 4 attention heads, and maximum position embedding of 1024 (All other parameters were according to transformers library defaults). We used the available Huggingface infrastructure (Wolf et al., 2019) to train/ evaluate the model, with Adam optimizer, an initial learning rate for 10−3, a drop-on-plateau scheduler (evaluating every 250 step, γ = 0.1), and a global mini- batch size of 64. In practice, the drop-on-plateau scheduling was only applied to ‘failed’ runs, to give them another shot for optimization, with no success (They did not converge, even well passed the 15 specified epochs). When the number of exponent bits was set to E = 3, we used a fixed exponent bias of b = 6 for the product and accumulator. D G RADIENT ESTIMATION FOR LBAS Following the general equation for GEMM operation (Eq. (3)), the operation can be expressed, using the recursive expression: S0 = 0; Si+1 = FMA (xi, wi, Si) ; y = SN−1 (8) In this example, we add the values of the product to the intermediate accumulator sum, ( S), in a sequential manner. Different orderings of the FMA operation are possible and can have an effect on the output (i.e., floating point addition is not commutative as ‘ideal’ addition, due to swamping). Let us write the recursive expression in Eq. (8) explicitly Sq i ≡ FMAq (xi−1, wi−1, FMAq (xi−2, wi−2, FMAq (...FMAq(x0, w0, 0)))) ; yq = Sq N−1. (9) Our goal in this section is to find a good estimate for the derivative for ∂yq ∂xi and ∂yq ∂wi . D.1 R ECURSIVE STE S The first, and most obvious method to estimate the derivative is by using the common STE (Eq. (5)). Per our definition, FMAq contains two quantization functions. Our main concern, however, is for the post-accumulator quantization, Qacc, and our first attempt will be to quantize it directly using a general STE function (STE : R3 → R). By doing so, we get the gradients: “ dyq dSq i ” = 1 wi “dyq dxi ” = 1 xi “ dyq dwi ” = dyq dSq i+1 STE (xi, wi, Sq i ) , (10) which, when expanded upon, will give us: “ dyq dSq i ” = NY k=i+1 STE (xk, wk, Sq k) . (11) Eq. (11) reveals an additional, possible issue for using standard STEs for the accumulation process. Usually, when applied over an overflowed activation neuron, the STE in equation Eq. (5) will set the corresponding neural gradient to zero. If the same STE is used for the accumulator’s quantization function, as per Eq. (11), each overflow event will eliminate the neural gradients of all previously accumulated product-pairs. Still, this approach may help us calculate a more accurate gradient, provided that conditions in which the STE returns 0 are not commonly met. We will denote the approach in Eq. (11) as Recursive. To perform the recursive correction to the gradient, all we really need to know is the last index (if any), in which the accumulator was overflown. While this may be more complex in cases where the values are added in non-sequential ordering, this is still a feasible calculation to perform, with modest-sized output. Computationally, calculating the underflow indexes is no more difficult as a task than computing the original GEMM operation, and this calculation (as well the calculation that will be presented in the following section), can be done during backpropagation, to avoid using essential memory for a long term. 14D.2 I MMEDIATE STE S To avoid setting too many gradients to zero, we suggest an alternative approach. First, we will re-write Eq. (9) as: Sq i = α0x0w0 + α1x1w1 + α2x2jw2 + ... + αi−1xi−1wi−1 , (12) where αi ≡ FMAq (xi, wi, Sq i ) − Sq i xiwi . (13) If xi = 0 or wi = 0 , we will define αi = 0 for simplicity. Recall Sq i here is the value of the “quantized\" accumulator in step i (Eq. (9)). From its definition, αi is the correction we make to the product xi · wi, to account for the FMA quantization error. Our choice to express the correction this way is based on the assumption that for most steps, |Sq i | ≫ |xi · wi|. This is true, because Sq i is, approximately, the accumulated sum of many such products. During a floating point addition, the bits of the lesser value will be the first to be swamped out, and thus we entangle the quantization error with this component. Moving on to the gradients, we are interested in the gradients of the operation inputs, dyq dxi and dyq dwi . We can use the chain rule to get: dyq dxi = wiαi + N−1X k=i+1 dαk dxi xkwk. (14) The exact expression we got for the gradient remains complex, sincedαk dxi ̸= 0, for k > i. Nevertheless, moving forward, we will take the approximation that ∀k, dαk dxi = 0. i.e., we neglect the cumulative effect that any individual scalar value has on the quantization correction we make in the following steps of the GEMM operation. We then get: dyq dxi = wiαi, dyq dwi = xiαi (15) Eq. (15) suggests a correction we can make to the standard backpropagation operation, which we will denote as an Immediate STE. However, to make any use of this correction, we must first have the values αi. In terms of computation, calculating α is quite similar to performing the original GEMM operation. In terms of memory, however, αi scales with the number of overall FMA operations. This is feasible in the case of fully connected operations, but not for GEMM operations that include a large amount of shared weights. To make sure the evaluation of αi by itself does not overburden the memory, it is possible to quantize the values of αi. By doing so, we get the equation: 1 wi “dyq dxi ” = 1 xi “ dyq dwi ” = QFIXED B,0 \u0012FMAq (xi, wi, Sq i ) − Sq i xiwi + ϵ1 \u0013 . (16) where ϵ1 is a small constant, added with flexible sign to prevent invalid denominator. In our experi- ments, we have observed that the quantization of αi does not harm the quality of the optimization process, and proceeded to binarize the value. The result is that we ended up suggesting an alternative STE to the one presented in Eq. (5), which is designed to address overflow only. We denote the new STE as DIFF: STEDIFF (xi, wi, Sq i ) = ( 1 |FMAq(xi,wi,Sq i )−Sq i | |xiwi|+ϵ1 > ϵ2 0 Else STEOF (xi, wi, Sq i ) = \u001a1 |Qprod(xiwi) + Sq i | < ROF 0 Else (17) The DIFF STE is similar to the common Overflow STE, but is tuned to detect cases of full-swamping and cases of product underflow in addition to cases of overflow. In our experiments, we tested the immediate approach with both STEs. One unique advantage of the DIFF STE, is that is agnostic to the specific implementation of the FMAq component. Therefore, the DIFF STE remains relevant in 15the general cases where the FMAq operation has an unspecified, black-box behavior, as common for hardware modules. Our derivation in this section was done in respect to the sequential ordering of FMAq operations, which is not commonplace in hardware accelerators that try to achieve large degree of parallelism. A more typical case, presumably common for systolic-array-like accelerators, is the chunk-based accumulation, where the accumulation is performed in two hierarchies, as seen in Fig. 1. In our experiments, all simulated GEMM operations and gradient estimation used a chunk size of 16, which means that an operation with an accumulation width of N is initiated with N 16 parallel operations (i.e., the first hierarchy), before aggregating the results (i.e., the second hierarchy). For example, in the case of recursive STE, every detection of OF or DIFF during re-computation will result in a ‘0’ in all preceding operations, just as we saw for sequential accumulation. The only difference for parallel accumulation is that the hierarchy tree can expand in two directions (Like 1 (right), with all the arrows reversed). E H ARDWARE ANALYSIS In this section, we try to give an estimate for the effect of incorporation of LBA models on hardware cost (area/ power), by estimating the number of gates needed to implement qLBA with different levels of quantization. Following an existing design of FMAq component (van Baalen et al. (2023),figure 2b), we adjusted the design for the case of FMAq with m/e quantization of weights and activations and M/E quantization of intermediate values (product, accumulator), and suggested the following gate counts, as seen in table 9. The gate counts are all based on the gate count assumptions listed in [van Baalen et al. (2023), appendix B], and common block designs. FMA Components breakdown Gate Count Exponent Adder (e − 1) · CFA + CHA Exponent Differ (min(E, e+ 1) − 1) · CFA + CHA · (1 + |e + 1 − E|) Exponent Max E · CMUX Mantissa MUL (m + 3)2 · CAND + (m + 2)2 · CFA + (m + 2) · CHA Sort Exponent (M + 1) · CMUX 1st Shift (M + 1 >> k→ F) (F − 1) · log2(kmax) · CMUX Mantissa Adder (F, F→ F) (M) · CFA + CHA Leading Zero Detector F(CAND + COR) + log2(kmax)2COR 2nd Shift (F >> k→ M + 1) (M + 1) · log2(kmax) · CMUX − kmax · (CFA − CAND) Exponent Rebase (E − 1) · CFA + CHA Final Incrementor (M + 1)CHA Table 9: FMA components gate-count breakdown. For the gate count, we used CAND = COR = 1 for the standard gates AND2/OR2, CMUX = 3 for MUX2, and CHA = 3, CFA = 7 for half and full adder. We do not include Flip-Flops in our gate count. For the value of F (Canvas bits, after shifting to fixed-point representation), we used 2M +1, the maximum bit width in which two 2’s complementary values with M + 1 bits can interact during addition. For kmax (the maximum shift distance), we used min(log2(F), E), as the magnitude of the shift is bounded by both the number of exponent bits and the size of the canvas F. Weights/Activations bits FMAq Bits Canvas Gates m e M E F log2(kmax) Count Ratio [%] 4 3 23 8 47 6 2208 100 4 3 10 5 21 5 1082 49 4 3 7 4 15 4 808 37 Table 10: Gate estimation for Quantized FMA We summarize our numerical results in table 10. Our results show that for 8bit activations and weights (at FP format M4E3), as we used in the paper, any half-precision FMAq that follows our quantization 16scheme is expected to reduce the number of gates by about 50% from the gate count of full-precision accumulators. Reducing the accumulator to M7E4, as was done in the paper, will cut the number of gates by an additional 25%, compared to half precision. We conclude our 12bit accumulators will reduce the gate count by 63% compared to 32 bit FP accumulation. We note that the 16-bit accumulation gate count in our analysis is not directly applicable to previous works that used16-bits accumulators– This is because in Sun et al. (2019), only the output of the accumulator was quantized with no explicit quantization of the internals. Presumably, the majority of the gain there was achieved by the reduction of communication bandwidth between FMAq components, which does not affect the gate count in this analysis. F W HY IS IT HARD TO TRAIN WITH UNDERFLOW ? Our claim that underflow cause unique problems during SGD optimization is based, first and foremost, on empirical observations (see: Tab. 2, Tab. 3). In addition, we suggest several explanations to why this problem may occur when underflow is introduced during training, despite it having a small effect on the loss landscape. Consider a neural network, where a specific scalar weight w is connected to its scalar activation x as input, and their product is z = xw. Suppose w is small enough so that z consistently results in product underflow, i.e. Qprod(z) = 0. In this case, during forward propagation, the value of x has little to no direct effect on the output neuron to which w is connected. Therefore, it is reasonable to assume that the computed neural gradient g ≡ dL dz (where L is the loss) will be uncorrelated with x. Consequently, the gradient update of the weight w will be ∆w ∝ gx, with the expected value E[∆w] ∝ E[gx] = E[g]E[x]. Based on previous quantization literature Banner et al. (2018), we have approximately E[g] = 0, and so E[∆w] = 0. Therefore, any sufficiently small weight w will become “stuck\", so that its z cannot escape underflow for a long time. The issue is excavated by the ratio between updates magnitude, and the magnitude a weight has to be updated to surpass the underflow threshold. In a fully-trained model, the gradients are expected to be dL dW ≃ 0. When transitioning to an LBA-model, we make sure to avoid significant changes to the loss landscape (as indicated by the zero-shot accuracy). As a result, we can expect the relative change in gradient to remain small, |∆w| = |η dL dw | ∼ |w|. (Otherwise, the loss landscape would change rapidly during SGD, and we can no longer consider the process as fine-tuning). When dealing with quantized values, it is always possible that a gradient step will be too small to change the value. (This is the main motivation behind stochastic rounding, which is not suitable for our case). For example, for floating point quantization without underflow/overflow, the gradient step must be approximately |∆w| = |η dL dw | ≥2−M |w| for the quantized value of w to ‘jump‘ quantization level. In this case, |∆w| ∼ |w| means that the ability of all weights to change during fine-tuning only depends on M, and the learning rate. In the case of underflow, however, values must surpass an absolute threshold (2−b), for the gradient step to have any effect. Consequently, under previous assumptions, any small enough value subjected to floating point quantization is expected to receive updates which are too small to result a state-change. This is what we referred to when mentioning values being ’stuck’ and ’escaping’. In LBA networks, the quantization is performed over intermediate values (products and partial accumulation values). These value do not get the explicit updates, but they will still get implicit updates by passing their respective neural gradient backwards. 17",
      "meta_data": {
        "arxiv_id": "2401.14110v1",
        "authors": [
          "Yaniv Blumenfeld",
          "Itay Hubara",
          "Daniel Soudry"
        ],
        "published_date": "2024-01-25T11:46:01Z",
        "pdf_url": "https://arxiv.org/pdf/2401.14110v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the growing computational bottleneck of high-precision accumulation operations in Deep Neural Networks (DNNs) when weights and activations are quantized to low precision (e.g., FP8). It proposes a simple method for the first time to train and fine-tune high-end DNNs, enabling the utilization of cheaper, 12-bit accumulators without significant accuracy degradation. Furthermore, it demonstrates that for even lower accumulation precisions, fine-grained gradient approximations can significantly improve DNN accuracy, particularly by backpropagating through the entire accumulation-computation graph.",
        "methodology": "The core methodology involves simulating quantized Fused Multiply-Add (FMA) operations (FMAq) in CUDA, replacing standard FMA with custom FMAq using bit-mask based 'floor' quantization. A two-staged fine-tuning scheme is proposed: initially fine-tuning a pre-trained full-precision network using FMAq but excluding underflow events, then enabling underflow and continuing fine-tuning. Exponent biases for Qprod and Qacc are configured to manage overflow and underflow, with `bacc = bprod - 1/2 log2 (Chunk-Size)`. For very low bit-width accumulators, fine-grained Straight-Through-Estimators (STEs) are introduced, including 'Recursive/OF' and 'Immediate/DIFF' variants, which require software re-computation of GEMM operations during backpropagation to retrieve internal quantization values.",
        "experimental_setup": "Experiments were conducted on ImageNet classification using ResNet18, ResNet34, and ResNet50 architectures, with accumulators quantized to M7E4 (12-bit) and FP8 (M4E3) for weights/activations. Language models, including Bert-Small, Bert-Base, Bert-Large on SQUAD v1, and LLama-v2-7B (using QLoRA) on the MMLU benchmark and Open Assistant (OASSA1) dataset, were fine-tuned with LBA. For extreme low-bit accumulators (e.g., 8-bit M4E3), a shallow fully-connected DNN was trained on MNIST, and a small LBA transformer was trained from scratch for Masked Language Modelling on the 'oscar: unshuffled-original-af' dataset. Evaluation metrics included Top-1 Accuracy for ImageNet, f1 score and Exact Match for SQUAD, 5-shot test accuracy for MMLU, and accuracy for Masked Language Modelling. All experiments utilized NVIDIA GPUs and standard optimizers (Adam) with cosine or StepLR schedulers.",
        "limitations": "The hardware implementation of certain techniques, such as chunk-based accumulation and stochastic rounding, may be difficult on modern hardware or could negate efficiency benefits due to implicit addition operations. The fine-grained backpropagation, while effective for lower bit-widths, is computationally expensive during training (doubling training time) and is currently feasible only for fully-connected and attention layers, not convolutional layers. Standard STEs for accumulation suffer from the recursive nature of the summation, potentially zeroing out gradients for all previously accumulated products upon an overflow event. Additionally, even the proposed fine-grained STEs did not fully close the accuracy gap with full-precision baselines when extreme accumulator quantization was applied. Training with underflow also poses a challenge as small weights can become 'stuck' due to update steps being too small to surpass the underflow threshold.",
        "future_research_directions": "Future research could focus on developing more efficient and universally applicable fine-grained gradient estimation methods to fully close the accuracy gap for extremely low bit-width accumulators. Extending the applicability of fine-grained backpropagation to convolutional layers is also a key direction. Further hardware exploration is needed to enable efficient implementation of techniques like chunk-based accumulation and stochastic rounding without negating their benefits. Investigation into dynamic adjustment of exponent biases or other quantization parameters during training, tailored to different FMA components, could yield further improvements. Additionally, developing more robust optimization strategies to mitigate the 'stuck' weight problem caused by underflow during training could enhance model performance and stability."
      }
    },
    {
      "title": "Searching for Low-Bit Weights in Quantized Neural Networks",
      "abstract": "Quantized neural networks with low-bit weights and activations are attractive\nfor developing AI accelerators. However, the quantization functions used in\nmost conventional quantization methods are non-differentiable, which increases\nthe optimization difficulty of quantized networks. Compared with full-precision\nparameters (i.e., 32-bit floating numbers), low-bit values are selected from a\nmuch smaller set. For example, there are only 16 possibilities in 4-bit space.\nThus, we present to regard the discrete weights in an arbitrary quantized\nneural network as searchable variables, and utilize a differential method to\nsearch them accurately. In particular, each weight is represented as a\nprobability distribution over the discrete value set. The probabilities are\noptimized during training and the values with the highest probability are\nselected to establish the desired quantized network. Experimental results on\nbenchmarks demonstrate that the proposed method is able to produce quantized\nneural networks with higher performance over the state-of-the-art methods on\nboth image classification and super-resolution tasks.",
      "full_text": "Searching for Low-Bit Weights in Quantized Neural Networks Zhaohui Yang1,2, Yunhe Wang2, Kai Han2, Chunjing Xu2, Chao Xu1, Dacheng Tao3∗, Chang Xu3 1 Key Lab of Machine Perception (MOE), Dept. of Machine Intelligence, Peking University. 2 Noah’s Ark Lab, Huawei Technologies. 3 School of Computer Science, Faculty of Engineering, University of Sydney. zhaohuiyang@pku.edu.cn; {yunhe.wang,kai.han,xuchunjing}@huawei.com xuchao@cis.pku.edu.cn; {dacheng.tao, c.xu}@sydney.edu.au Abstract Quantized neural networks with low-bit weights and activations are attractive for developing AI accelerators. However, the quantization functions used in most conventional quantization methods are non-differentiable, which increases the optimization difﬁculty of quantized networks. Compared with full-precision param- eters (i.e., 32-bit ﬂoating numbers), low-bit values are selected from a much smaller set. For example, there are only 16 possibilities in 4-bit space. Thus, we present to regard the discrete weights in an arbitrary quantized neural network as searchable variables, and utilize a differential method to search them accurately. In particular, each weight is represented as a probability distribution over the discrete value set. The probabilities are optimized during training and the values with the highest prob- ability are selected to establish the desired quantized network. Experimental results on benchmarks demonstrate that the proposed method is able to produce quantized neural networks with higher performance over the state-of-the-art methods on both image classiﬁcation and super-resolution tasks. 1 Introduction The huge success of deep learning is well demonstrated in considerable computer vision tasks, including image recognition [ 20], object detection [ 11, 40], visual segmentation [ 18], and image processing [26]. On the other side, these deep neural architectures are often oversized for accuracy reason. A great number of network compression and acceleration methods have been proposed to eliminate the redundancy and explore the efﬁciency in neural networks , including pruning [16, 48], low-bit quantization [6, 39, 56], weight decompostion [53, 31], neural architecture search [34, 49] and efﬁcient block design [22, 41, 37]. Among these algorithms, quantization is very particular which represents parameters in deep neural networks as low-bit values. Since the low costs of quantized networks on both memory usage and computation, they can be easily deployed on mobile devices with speciﬁc hardware design. For example, compared with conventional 32-bit networks, binary neural networks (BNNs) can directly obtain a 32×compression ratio, and an extreme computational complexity reduction by executing bit-wise operations (e.g., 57×speed-up ratio in XNORNet [39]). However, the performance of the low-bit neural networks is usually worse than that of full-precision baselines, due to the optimization difﬁculty raised by the low-bit quantization functions. ∗Corresponding Author. Preprint. Under review. arXiv:2009.08695v1  [cs.CV]  18 Sep 2020To reduce the accuracy drop of quantized neural networks, some methods have been proposed in recent years. BiRealNet [35] inserts more shortcuts to help optimization. Structured Binary [59] and ABCNet [32] explore some sophisticated binary blocks and achieve comparable performance to that of full-precision networks, etc. Admittedly, the aforementioned methods have made great efforts to improve the performance of the quantized neural networks. However, the accuracy gap between the full-precision network and its quantized version is still very huge, especially the binarized model. For example, the state-of- the-art accuracy of binarized ResNet-18 is about 10% lower than that of the full-precision baseline. A common problem in existing quantization methods is the estimated gradients for quantization functions, using either STE [6, 39] or self-designed gradient computation manner [34]. The estimated gradients may provide inaccurate optimization direction and consequently lead to worse performance. Therefore, an effective approach for learning quantized neural networks without estimated gradients is urgently required. In this paper, we present a novel weight searching method for training the quantized deep neural network without gradient estimation. Since there are only a few values of low-bit weights (e.g., +1 and -1 in binary networks), we develop a weight searching algorithm to avoid the non-differentiable problem of quantization functions. All low-bit values of an arbitrary weight in the given network are preserved with different probabilities. These probabilities will be optimized during the training phase. To further eliminate the performance gap after searching, we explore the temperature factor and a state batch normalization for seeking consistency in both training and testing. The effectiveness of the proposed differentiable optimization method is then veriﬁed on image classiﬁcation and super-resolution tasks, in terms of accuracy and PSNR values. 2 Related Works To reduce memory usage and computational complexity of deep neural networks, a series of methods are explored, including efﬁcient block design, network pruning, weight decomposition, and network quantization. MobileNet [22, 41] and ShufﬂeNet [37] design novel blocks and construct the networks with high efﬁciency. Weight pruning methods such as DeepCompression [ 16] propose to remove redundant neurons in pre-trained models, while requires speciﬁc hardware for acceleration. Structured pruning methods [21, 48, 51] discard a group of weights (e.g., an entire ﬁlter) in pre-trained networks so that the compressed model can be directly accelerated in any off-the-shelf platforms. Low-rank decomposition methods [ 8, 25, 53, 50] explore the relationship between weights and ﬁlters and explore more compact representations. Gradient-based neural architecture search methods [34, 47] use the continuous relaxation strategy to search discrete operations for each layer. All the candidate operations are mixed with a learnable distribution during searching and the operation with the highest probability is selected as the ﬁnal operation. Quantization methods [ 29, 1, 38, 12, 43, 9, 46, 55] represent weights or activations in a given neural architecture as low-bit values. The model size and computational complexity of the quantized network are much smaller than those of its original version. Compared with previous model compression methods, network quantization does not change the architecture and the calculation process of original ones, which is supported by some mainstream platforms. To maintain the high performance of quantized neural networks, considerable approaches have been explored for training low-bit weights and activations. For instance, Fixed point quantization [ 30] reduces about 20% complexity without loss of accuracy on CIFAR-10 dataset. BinaryConnect [6] represents weights by binary values {−1,+1}, TTQ [ 57] uses the ternary weights {−1,0,+1}. Activations in these two methods are still full-precision values. DoReFa [56] quantizes both weights and activations to low-bit values. Among all the quantization methods, in extreme cases, if activations and weights are both 1 bit, the binary neural network not only occupies less storage space but also utilizes low-level bit operations xnor and bitcount to accelerate inference. ABCNet [32], CBCN [33], BENN [58] and SBNN [59] explored how to modify the BNN to match the performance with full- precision networks. In [ 60], a clearly different viewpoint is investigated into gradient descent, which provides the ﬁrst attempt for the ﬁeld and is worth further exploration. The hidden relationship between different variables is carefully studied, which will beneﬁt many applications. 2Nevertheless, most of the quantization functions in existing methods are non-differentiable, which increases the optimization difﬁculty. Thus, this paper aims to explore a more effective algorithm for quantizing deep neural networks. 3 Methods In this section, we ﬁrst brieﬂy introduce the optimization approach in the conventional weight quantization methods and the existing problems. Then, a low-bit weight search approach is developed for accurately learning quantized deep neural networks. The strategies, including gradually decreasing temperature and state batch normalization, are proposed to eliminate the quantization gap. 3.1 Optimization Difﬁculty in Network Quantization Representing massive weights and activations in deep neural networks using low-bit values is effective for reducing memory usage and computational complexity. However, it is very hard to learn these low-bit weights in practice. For the case of q-bit quantization, the coordinate axis is divided into m = 2q intervals, and each interval corresponds to one discrete value in setV = {v1,v2,··· ,vm},v1 <··· <vm. Denoting the thresholdsas T = {t1,...,t m−1},v1 <t1 ≤v2 ...v m−1 <tm−1 ≤vm, the quantization function Q(x) can be deﬁned as, Q(x) =    v1 if x<t 1, v2 if t1 ≤x<t 2, ... vm if tm−1 ≤x, (1) where x ∈R is the original full-precision values. Given full-precision latent convolutional ﬁlters Wlat q , the weights are quantized as Wq = Q(Wlat q ), which are used to calculate the output features (i.e., activations). In the training phase, the gradients ∇Wq w.r.t the quantized ﬁlters Wq can be calculated with standard back-propagation algorithm. However, the gradients∇Wlatq is hard to obtain, because the derivative ofQis zero almost everywhere and inﬁnite at zero point. The Straight Through Estimator (STE) [6] is a widely used strategy to estimate the gradient of Wlat q , i.e., ∇Wlatq = ∇Wq (2) where the approximate gradients ∇Wlatq are use to update weights Wlat q , ˆWlatq = Wlat q −η·∇Wlatq ·σ(∇Wlatq ), (3) where ηis the learning rate and σ(·) is the gradient clip function. Although Eq. 2 can provide the approximate gradients of Wlat q , the estimation error cannot be ignored if we aim to further improve the performance of quantized networks. Considering that there are a number of layers and learnable parameters in modern neural networks, the optimization difﬁculty for learning accurate quantized models is still very large. 3.2 Low-bit Weight Searching The parameters in a quantized neural network can be divided into two parts: the non-quantized parameters Wf (e.g., in batch normalization, fully connected layer), and the quantized convolutional parameters Wq. The target of training the network is to ﬁnd W∗ f,W∗ q that minimizes W∗ f,W∗ q = arg min Wf ∈R,Wq∈V L(Wf,Wq), (4) where the Wf and Wq belong to different domains, which are real number R and discrete set V, respectively. To optimize modern neural networks, the stochastic gradient descent (SGD) strategy is usually utilized. However, the discrete variables Wq are not feasible to be optimized by SGD. The previously described solution (Eq. 2- 3) by introducing the latent variables can be viewed as a heuristic solution 3that enables updating all the parameters in an end-to-end manner. However, the estimated and inaccurate gradients limit the upper bound of the performance. In contrast, inspired by gradient-based neural architecture search methods [34, 49, 17, 52], we introduce the continuous relaxation strategy to search discrete weights. Consider optimizing an n-dimensional discrete variable W of size (d1,...,d n), where each element w in W is chosen from the m discrete values V = {v1,v2,··· ,vm}. A new auxiliary tensor A ∈Rm×d1×···×dn is created to learn the distribution of W, and the probability over mdiscrete variables is computed according to the following formula, Pi = expAi/τ ∑ jexpAj/τ, i∈{1,··· ,m}, (5) where Pi is probability that the elements in W belong to the i-th discrete value vi, and τ is the temperature controlling the entropy of this system. The expectation of the quantized values for W can be obtained: Wc = ∑ i Pi ·vi, (6) where the continuous state tensor Wc is calculated according to the probability over all the discrete values. Wc is used for convolution during training, and the process of Eq. 6 is differentiable and friendly for end-to-end training. Here we optimize the auxiliary tensor Awhose gradients can be accurately calculated so that we avoid the gradient estimation in previous works. In the inference stage, the weights are quantized by selecting the discrete value with the maximum probability for each position. In formal, the quantized state weights are Wq = ∑ i Ii ·vi, (7) where I = onehot(arg maxi(Pi)), i∈{1,··· ,m}indicates the one-hot vectors with the maximum probability. For a convolution layer in neural networks, a four-dimensional weight tensorW of size M ×N × DK ×DK is to be learned, where M,N , and DK are output channels, input channels, and kernel size, respectively. By using the proposed method to search forq-bit quantized weight W, the problem can be transformed into a special case of dimension n= 4and discrete values m= 2q (Eq. 1). The tensor A∈Rm×M×N×DK×DK is constructed to learn the distribution over the discrete value set V according to Eq. 5. By using the continuous relaxation, the learning of Eq. 4 could be transformed to learn the tensor A, W∗ f,A∗= arg min Wf ∈R,A∈R L(Wf,A), (8) where the entire network is differentiable and can be optimized end-to-end with accurate gradients. 3.3 Optimization Details We use the full-precision tensor Wc for training and quantized tensor Wq for inference. Although the continuous relaxation method solves the problem of inaccurate gradient during training, there is still a quantization gap after converting Wc to Wq. This will lead to the mismatch between the Wq and other parameters trained according to Wc, such as the statistics in the batch normalization layers. 3.3.1 Reducing the Quantization Gap In the proposed low-bit weight searching scheme, the continuous Wc for training and the discrete Wq for inference are not exactly the same. The quantization gap is introduced to measure the quantization error in the process of transforming the softmax distribution to the one-hot vector (Eq. 6- 7), Wgap = Wq −Wc. (9) Fortunately, As stated in the temperature limit Theorem 1, the quantization gap Wgap could be an inﬁnitesimal quantity if the temperature τ is small enough. Theorem 1 ( Temperature Limit Theorem ) Assuming a ∈Rm is a vector in tensor A. If the temperature τ is gradually decreasing to zero, then the quantization gap is an inﬁnitesimal quantity. 4Algorithm 1 Training algorithm of SLB Input: The network Nconstructed by convolution and state batch normalization, training iterations I, dataset D, initialize temperature τs, end temperature τe and temperature decay scheduler T. 1: for iter iin 1,...,I do 2: Update temperature parameter τ according to the temperature decay scheduler T. 3: Get minibatch data X and target Y from dataset D, calculate continuous weights Wc and quantized weights Wq of Naccording to the current temperature τ. 4: The continuous state weights Wc (Eq. 6) is computed and prediction P = N(X). The continuous state statistics in SBN are also updated. 5: Compute the discrete weights Wq (Eq. 7) and the discrete state statistics in SBN layer are updated (Eq. 13). 6: The loss is calculated according to the prediction P and target Y, and the back-propogated gradients are used to update auxiliary matrix A. 7: end for 8: Record quantized tensors Wq and the discrete state statistics in SBN. Output: A trained quantized neural network N∗. proof 1 The distribution pis computed as, pi = expai/τ ∑ jexpaj/τ = 1∑ jexp(aj−ai)/τ, (10) by gradually decreasing τ to zero, the index of the maximum value is k= arg maxipi, the quantiza- tion gap wgap is computed as, lim τ→0 pk = 1, lim τ→0 pi,i̸=k = 0 lim τ→0 wgap = vk − ∑ i pi ·vi = (1−pk) ·vk + ∑ i,i̸=k pi ·vi = 0. (11) We propose the gradually decreasing temperature strategy during training so that limτ→0 Wc = Wq. In particular, at the beginning of optimization, the temperature factor τ is high, and the distribution P is relatively smooth. With the change of temperature τ, the distribution P becomes sharper. In this way, the quantization gap will gradually decrease as τ changes. 3.3.2 State Batch Normalization With the gradually decreasing temperature strategy, the continuous Wc will converge to the discrete Wq. Nevertheless, one problem that cannot be ignored is that the temperature cannot be decreased to a minimal value, i.e., 0. If a neuron w’s probability of choosing its value to be a discrete value is greater than a relatively high threshold (e.g., max(p) >0.999), we may infer that the discrete value will not change as we continue to decrease the temperature and make the distribution sharper. Under this assumption, the quantized weights are well optimized and the only difference between training and inference is the statistics difference for batch normalization layers. Thus we proceed to develop a corresponding state batch normalization that acts as a bridge between a sharp softmax distribution and a one-hot distribution. Batch Normalization (BN) [24] is a widely-used module for increasing the training stability of deep neural networks. The conventional BN for a given input feature ycan be written as, ˆyi = 1 σi (yi −µi), (12) where iis the index of channel and µi,σi are the mean and standard deviation values for the i-th channel, respectively. In the proposed method, Wc is used for convolution during training, so the normalization is formulated as, yc = x⊗Wc, ˆyc,i = 1 σc,i (yc,i −µc,i), (13) 5where xis the input data, ⊗is the convolution operation and yc is the output. µc,i and σc,i are the mean value and standard deviation of the i-th channel in yc. Therefore, the statistics of the quantized weights Wq does not match with the statistics σc and µc of BN , which results in a precision decline in inference. To address this problem, we proposed the State Batch Normalization (SBN). To be speciﬁc, the SBN calculates two groups of statistics during training, one is for yc and the other is for yq where yq is the convolution output using quantized weights Wq. The normalization process of yq is yq = x⊗Wq, ˆyq,i = 1 σq,i (yq,i −µq,i), (14) where µq,i and σq,i are the mean and standard deviation of the i-th channel in yq. Both ˆyc and ˆyq have a mean of zero and a standard deviation of one. We make they share the same group of afﬁne coefﬁcients, zc,i = γi ˆyc,i + βi, zq,i = γi ˆyq,i + βi (15) In this case, the proposed SBN eliminates the small quantization gap in the statistics for batch normalization layers. 3.3.3 Overall Algorithm We view the training for quantized networks as a search problem, and utilize a differentiable method for learning the distribution of quantized weights. The strategies including gradually decreasing temperature and state batch normalization are proposed to eliminate the quantization gap. The overall training algorithm of searching for low-bit weights (SLB) is summarized in Algorithm 1. 4 Experiments 0 100 200 300 400 500 Epochs 40 50 60 70 80 Accuracy (%) exp cos linear Figure 1: The diagram of dif- ferent temperature scheduler. We examine our proposed method on quantized neural networks with different bitwidths. Following common practice in most works, we use the CIFAR-10 [28] and large scale ILSVRC2012 [7] recognition datasets to demonstrate the effectiveness of our method. For all the quantized neural networks, following previous methods [35, 54, 56], all convolution layers except for the ﬁrst one are quantized. The low-bit set V is constructed by uniform distribution from −1 to 1. 4.1 Experiments on CIFAR-10 In this section, we ﬁrst examine different temperature decay schedulers and then compare the results with other methods. We train the network for 500 epochs in total and decay the learning rate by a factor of 10 at 350, 440, and 475 epochs. The matrix Ais initialized by kaiming initialization [19]. The quantization function for the activations is the same as DoReFa [56]. We test several different decay schedulers for temperature changing, includingsin, linear and exp, which are widely used in adjusting the learning rate [ 36, 22]. For convenience, we used T in this section to represent 1 τ in Eq. 5. The temperature T is used to control the entropy of the system. Given the start temperature factor Ts, the end temperature Te and the total number of training iterations I. At iteration i, the temperature Tt in different schedulers is calculated as follow, Tt,linear = Ts + i I ×(Te −Ts), Tt,sin = Ts + sin( iπ 2I) ×(Te −Ts), Tt,exp = Ts ×(Te Ts ) i I . In Figure 1, we comprehensively compare different schedulers on binary ResNet20. We setTs = 0.01 and Te = 10. For sin and linear schedulers, the accuracies converge rapidly. However, some weights are not adequately trained, which results in relatively low accuracies. For the exp scheduler, the accuracies are much higher which indicates the weights converge to better local minima. In the following experiments, we adopt the exp scheduler for temperature adjusting. 6Table 1: Results of ResNet20 on CIFAR-10. Methods Bit-width Acc. (W/A) (%) FP32 [20] 32/32 92.1 DoReFa [56, 13] 1/1 79.3 DSQ [12] 1/1 84.1 SQ [13] 1/1 84.1 SLB 1/1 85.5 LQ-Net [54] 1/2 88.4 SLB 1/2 89.5 SLB 1/4 90.3 SLB 1/8 90.5 LQ-Net [54] 1/32 90.1 DSQ [12] 1/32 90.2 SLB 1/32 90.6 LQ-Net [54] 2/2 90.2 SLB 2/2 90.6 SLB 2/4 91.3 SLB 2/8 91.7 SLB 2/32 92.0 SLB 4/4 91.6 SLB 4/8 91.8 SLB 4/32 92.1 Table 2: Results of VGG-Small on CIFAR-10. Methods Bit-width Acc. (W/A) (%) FP32 [44] 32/32 94.1 BNN [23] 1/1 89.9 XNORNet [39] 1/1 89.8 DoReFa [56, 13] 1/1 90.2 SQ [13] 1/1 91.7 SLB 1/1 92.0 LQ-Net [54] 1/2 93.4 SLB 1/2 93.4 SLB 1/4 93.5 SLB 1/8 93.8 LQ-Net [54] 1/32 93.5 SLB 1/32 93.8 LQ-Net [54] 2/2 93.5 SLB 2/2 93.5 SLB 2/4 93.9 SLB 2/8 94.0 SLB 2/32 94.0 SLB 4/4 93.8 SLB 4/8 94.0 SLB 4/32 94.1 We compare our results with other SOTA quantization methods including BNN, XNORNet, DoReFa, DSQ, SQ, and LQ-Net, on two different network architectures, i.e., VGG-Small [ 44] and ResNet20 [20]. As shown in Table 1 and Table 2, our method outperforms other methods and achieves state-of-the-art results on different bit-widths. 4.2 Experiments on ILSVRC2012 In the ILSVRC2012 classiﬁcation experiments, we use ResNet18 [ 20, 35] as our backbone and compare the results with other state-of-the-art methods. The learning rate starts from 1e-3, weight decay is set to 0, and Adam optimizer is used to update parameters. Table 3 presents the results of our method and a number of other quantization methods. The binary neural network is the most potential method because the xnor and bitcount operations are relatively efﬁcient. Our proposed method on the binary case achieves state-of-the-art accuracy with a Top-1 accuracy of 61.3% . When using more bits on the activations, the Top-1 accuracies gradually increase. Our method consistently outperforms other methods by a signiﬁcant margin. By adding the bitwidth of weights, the proposed SLB also achieves state-of-the-art accuracies on different settings. 4.2.1 The Impact on State Batch Normalization We further verify the importance of state batch normalization by removing it in binary ResNet18. In particular, the discrete weights and discrete state statistics are not calculated during training, and the continuous state statistics are used after training. Because of the quantization gap, the SLB (w/o SBN) in Table 3 decreases the Top-1 accuracy by 0.3%. This indicates the essentiality to maintain two groups of statistics, which are estimated by the continuous state outputs and discrete state outputs, respectively. 4.3 Experiment on Super Resolution To verify the generalization of the proposed method, we apply SLB to the image super-resolution task. The typical model VDSR [27] is selected as the baseline model. Following the original paper, 7Table 3: Overall comparison of quantized ResNet18 on ILSVRC2012 large scale classiﬁcation dataset. ’W/A’ denotes the bitwidth of weights and activations, respectively. Methods Bit-width Top-1 Top-5 (W/A) (%) (%) FP32 [20] 32/32 69.3 89.2 BNN [23] 1/1 42.2 67.1 ABCNet [32] 1/1 42.7 67.6 XNORNet [39] 1/1 51.2 73.2 BiRealNet [35] 1/1 56.4 79.5 PCNN [14] 1/1 57.3 80.0 SQ [13] 1/1 53.6 75.3 ResNetE [2] 1/1 58.1 80.6 BONN [15] 1/1 59.3 81.6 SLB 1/1 61.3 83.1 SLB (w/o SBN) 1/1 61.0 82.9 DoReFa [56] 1/2 53.4 - LQ-Net [54] 1/2 62.6 84.3 HWGQ [4] 1/2 59.6 82.2 TBN [45] 1/2 55.6 79.0 HWGQ [4] 1/2 59.6 82.2 SLB 1/2 64.8 85.5 DoReFa [56] 1/4 59.2 - SLB 1/4 66.0 86.4 SYQ [10] 1/8 62.9 84.6 SLB 1/8 66.2 86.5 Methods Bit-width Top-1 Top-5 (W/A) (%) (%) FP32 [20] 32/32 69.3 89.2 BWN [39] 1/32 60.8 83.0 DSQ [12] 1/32 63.7 - SQ [13] 1/32 66.5 87.3 SLB 1/32 67.1 87.2 PACT [5] 2/2 64.4 - LQ-Net [54] 2/2 64.9 85.9 DSQ [12] 2/2 65.2 - SLB 2/2 66.1 86.3 SLB 2/4 67.5 87.4 SYQ [10] 2/8 67.7 87.8 SLB 2/8 68.2 87.7 TTQ [57] 2/32 66.6 87.2 LQ-Net [54] 2/32 68.0 88.0 SLB 2/32 68.4 88.1 Input Groundtruth (PSNR) FP32 (37.68) DoReFa (36.34) SLB (36.84) (a) Super-resolution results with scale factor ×2. Input Groundtruth (PSNR) FP32 (33.87) DoReFa (32.29) SLB (33.01) (b) Super-resolution results with scale factor ×3. Figure 2: Super-resolution results on Set5 dataset with different scale factors. we use the 291 images as in [ 42] for training and test on Set5 dataset [3]. Each image in the training data is split into patches and augmented with rotation or ﬂip. The baseline model and the binarized models using DoReFa and SLB algorithms are trained with the same setting as in [27]. The models are compared at different scale factors, i.e.×2 and ×3. From the results in Figure 2, we can see that SLB achieves much higher PSNR than DoReFa at both ×2 and ×3 scale factors. In Figure 2(a), the eyebrow in DoReFa’s result is somewhat red while the result of SLB is normal and closer to the raw image. In Figure 2(b), the texture on the wing in DoReFa’s result is blurry and hard to see clearly. SLB could well recover the texture and performs close to the full-precision model. 85 Conclusions In this paper, we use the continuous relaxation strategy which addresses the gradient mismatch problem. To learn the discrete convolution kernels, an auxiliary probability matrix is constructed to learn the distribution for each weight from soft to hard, and the gradients are calculated to update the distribution. The state batch normalization is also proposed to minimize the gap between continuous state outputs and discrete state outputs. Our SLB can be applied to optimize quantized networks on a number of bitwidth cases and achieved state-of-the-art performance, which demonstrates the effectiveness of our method. References [1] Thalaiyasingam Ajanthan, Puneet Dokania, Richard Hartley, and Philip Torr. Proximal mean- ﬁeld for neural network quantization. In ddV, 2019. [2] Joseph Bethge, Haojin Yang, Marvin Bornstein, and Christoph Meinel. Back to simplicity: How to train accurate bnns from scratch? arXiv, 2019. [3] Marco Bevilacqua, Aline Roumy, Christine Guillemot, and Marie Line Alberi-Morel. Low- complexity single-image super-resolution based on nonnegative neighbor embedding. BMVC, 2012. [4] Zhaowei Cai, Xiaodong He, Jian Sun, and Nuno Vasconcelos. Deep learning with low precision by half-wave gaussian quantization. CVPR, 2017. [5] Jungwook Choi. Pact: Parameterized clipping activation for quantized neural networks. arXiv, 2018. [6] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: training deep neural networks with binary weights during propagations. NeurIPS, 2015. [7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. CVPR, 2009. [8] Emily Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Exploiting linear structure within convolutional networks for efﬁcient evaluation. arXiv, 2014. [9] Ruizhou Ding, Ting-Wu Chin, Zeye Liu, and Diana Marculescu. Regularizing activation distribution for training binarized deep networks. CVPR, 2019. [10] Julian Faraone, Nicholas Fraser, Michaela Blott, and Philip H.W. Leong. Syq: Learning symmetric quantization for efﬁcient deep neural networks. CVPR, 2018. [11] Ross Girshick. Fast r-cnn. ICCV, 2015. [12] Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu, Jiazhen Lin, Fengwei Yu, and Junjie Yan. Differentiable soft quantization: Bridging full-precision and low-bit neural networks. CVPR, 2019. [13] Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu, Jiazhen Lin, Fengwei Yu, and Junjie Yan. Differentiable soft quantization: Bridging full-precision and low-bit neural networks. ICCV, 2019. [14] Jiaxin Gu, Ce Li, Baochang Zhang, Jungong Han, Xianbin Cao, Jianzhuang Liu, and David Doermann. Projection convolutional neural networks for 1-bit cnns via discrete back propagation. arXiv, 2018. [15] Jiaxin Gu, Junhe Zhao, Xiaolong Jiang, Baochang Zhang, Liu Jianzhuang, Guodong Guo, and Rongrong Ji. Bayesian optimized 1-bit cnns. ICCV, 2019. [16] Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. ICLR, 2016. [17] Chaoyang He, Haishan Ye, Li Shen, and Tong Zhang. Milenas: Efﬁcient neural architecture search via mixed-level reformulation. arXiv, 2020. [18] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. ICCV, 2017. [19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation. ICCV, 2015. 9[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CVPR, 2016. [21] Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural networks. ICCV, 2017. [22] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolutional neural networks for mobile vision applications. arXiv, 2017. [23] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Bina- rized neural networks. NeurIPS, 2016. [24] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. ICML, 2015. [25] Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks with low rank expansions. BMVC, 2014. [26] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. ECCV, 2016. [27] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate image super-resolution using very deep convolutional networks. CVPR, 2016. [28] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, 2009. [29] Zefan Li, Bingbing Ni, Wenjun Zhang, Xiaokang Yang, and Wen Gao. Performance guaranteed network acceleration via high-order residual quantization. ICCV, 2017. [30] Darryl D. Lin, Sachin S. Talathi, and V . Sreekanth Annapureddy. Fixed point quantization of deep convolutional networks. ICML, 2016. [31] Shaohui Lin, Rongrong Ji, Chao Chen, Dacheng Tao, and Jiebo Luo. Holistic cnn compression via low-rank decomposition with knowledge transfer. TPAMI, 2018. [32] Xiaofan Lin, Cong Zhao, and Wei Pan. Towards accurate binary convolutional neural network. NeurIPS, 2017. [33] Chunlei Liu, Wenrui Ding, Xin Xia, Baochang Zhang, Jiaxin Gu, Jianzhuang Liu, Rongrong Ji, and David Doermann. Circulant binary convolutional networks: Enhancing the performance of 1-bit dcnns with circulant back propagation. CVPR, 2019. [34] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. In ICLR, 2019. [35] Zechun Liu, Baoyuan Wu, Wenhan Luo, Xiaodong Yang, Weiwei Liu, and Kwang-Ting Cheng. Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm. ECCV, 2018. [36] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. ICLR, 2017. [37] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufﬂenet v2: Practical guidelines for efﬁcient cnn architecture design. ECCV, 2018. [38] Hai Phan, Dang Huynh, Yihui He, Marios Savvides, and Zhiqiang Shen. Mobinet: A mobile binary network for image classiﬁcation. arXiv, 2019. [39] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classiﬁcation using binary convolutional neural networks. ECCV, 2016. [40] Joseph Redmon, Santosh Kumar Divvala, Ross B. Girshick, and Ali Farhadi. You only look once: Uniﬁed, real-time object detection. CVPR, 2016. [41] Mark B. Sandler, Andrew G. Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. CVPR, 2018. [42] Samuel Schulter, Christian Leistner, and Horst Bischof. Fast and accurate image upscaling with super-resolution forests. CVPR, 2015. [43] Mingzhu Shen, Xianglong Liu, Kai Han, Ruihao Gong, Yunhe Wang, and Chang Xu. Balanced binary neural networks with gated residual. arXiv, 2019. 10[44] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. ICLR, 2015. [45] Diwen Wan, Fumin Shen, Li Liu, Fan Zhu, Jie Qin, Ling Shao, and Heng Tao Shen. Tbn: Convolutional neural network with ternary inputs and binary weights. ECCV, 2018. [46] Ziwei Wang, Jiwen Lu, Chenxin Tao, Jie Zhou, and Qi Tian. Learning channel-wise interactions for binary convolutional neural networks. CVPR, 2019. [47] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, and Kurt Keutzer. Fbnet: Hardware-aware efﬁcient convnet design via differentiable neural architecture search. CVPR, 2019. [48] Zuxuan Wu, Tushar Nagarajan, Abhishek Kumar, Steven Rennie, Larry S. Davis, Kristen Grauman, and Rogério Schmidt Feris. Blockdrop: Dynamic inference paths in residual networks. CVPR, 2018. [49] Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin. Snas: stochastic neural architecture search. In ICLR, 2019. [50] Zhaohui Yang, Yunhe Wang, Chuanjian Liu, Hanting Chen, Chunjing Xu, Boxin Shi, Chao Xu, and Chang Xu. Legonet: Efﬁcient convolutional neural networks with lego ﬁlters. ICML, 2019. [51] Jiahui Yu and Thomas Huang. Autoslim: Towards one-shot architecture search for channel numbers. arXiv, 2019. [52] Jiahui Yu, Pengchong Jin, Hanxiao Liu, Gabriel Bender, Pieter-Jan Kindermans, Mingxing Tan, Thomas S. Huang, Xiaodan Song, Ruoming Pang, and Quoc Le. Bignas: Scaling up neural architecture search with big single-stage models. arXiv, 2020. [53] Xiyu Yu, Tongliang Liu, Xinchao Wang, and Dacheng Tao. On compressing deep models by low rank and sparse decomposition. CVPR, 2017. [54] Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua. Lq-nets: Learned quantiza- tion for highly accurate and compact deep neural networks. ECCV, 2018. [55] Feng Zheng, Cheng Deng, and Heng Huang. Binarized neural networks for resource-efﬁcient hashing with minimizing quantization loss. IJCAI, 2019. [56] Shuchang Zhou, Zekun Ni, Xinyu Zhou, He Wen, Yuxin Wu, and Yuheng Zou. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv, 2016. [57] Chenzhuo Zhu, Song Han, Huizi Mao, and William J. Dally. Trained ternary quantization. ICLR, 2017. [58] Shilin Zhu, Xin Dong, and Hao Su. Binary ensemble neural network: More bits per network or more networks per bit? CVPR, 2018. [59] Bohan Zhuang, Chunhua Shen, Mingkui Tan, Lingqiao Liu, and Ian D. Reid. Structured binary neural networks for accurate image classiﬁcation and semantic segmentation. CVPR, 2018. [60] Li’an Zhuo, Baochang Zhang, Linlin Yang, Hanlin Chen, Qixiang Ye, David Doermann, Rongrong Ji, and Guodong Guo. Cogradient descent for bilinear optimization. CVPR, 2020. 11",
      "meta_data": {
        "arxiv_id": "2009.08695v1",
        "authors": [
          "Zhaohui Yang",
          "Yunhe Wang",
          "Kai Han",
          "Chunjing Xu",
          "Chao Xu",
          "Dacheng Tao",
          "Chang Xu"
        ],
        "published_date": "2020-09-18T09:13:26Z",
        "pdf_url": "https://arxiv.org/pdf/2009.08695v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the significant accuracy degradation in quantized neural networks, primarily caused by the non-differentiable nature of quantization functions and the inaccurate gradients estimated by conventional methods like Straight-Through Estimator (STE). The main contribution is a novel differentiable low-bit weight searching method, termed SLB (Searching for Low-Bit Weights). This method re-conceptualizes discrete weights as searchable variables, represented by a probability distribution over a discrete value set. Instead of gradient estimation, the probabilities are directly optimized during training using accurate gradients. To further mitigate the performance gap between continuous training and discrete inference, two key strategies are introduced: a gradually decreasing temperature factor and State Batch Normalization (SBN). The proposed SLB method achieves state-of-the-art performance on both image classification (CIFAR-10, ILSVRC2012) and super-resolution tasks across various bit-widths, demonstrating its effectiveness.",
        "methodology": "The core methodology involves a differentiable low-bit weight searching approach based on a continuous relaxation strategy. Each low-bit weight is represented by an auxiliary tensor A, which learns a probability distribution Pi = exp(Ai/τ) / sum(exp(Aj/τ)) over a predefined discrete value set V, where τ is a temperature parameter. During training, a continuous state tensor Wc = sum(Pi * vi) is derived from this distribution and used for convolution. This Wc is differentiable, allowing for accurate gradient calculation and end-to-end optimization of the auxiliary tensor A, thereby avoiding inaccurate gradient estimations. For inference, the quantized weights Wq are obtained by a one-hot selection based on the maximum probability (Wq = sum(Ii * vi), where I is one-hot(argmax(Pi))). To reduce the 'quantization gap' between Wc (training) and Wq (inference), two optimization details are employed: 1. Gradually Decreasing Temperature: The temperature τ is progressively reduced during training (using an exponential decay scheduler), making the probability distribution sharper and causing Wc to converge towards Wq. 2. State Batch Normalization (SBN): This module calculates and maintains two distinct sets of statistics during training: one for the continuous convolution outputs (yc = x ⊗ Wc) and another for the discrete convolution outputs (yq = x ⊗ Wq). Both normalized outputs then share the same affine coefficients, ensuring consistency in batch normalization statistics between training and inference.",
        "experimental_setup": "The method was extensively evaluated on standard computer vision tasks: Image Classification and Super-Resolution. For **Image Classification**, experiments were conducted on the CIFAR-10 and large-scale ILSVRC2012 datasets. Network architectures used include ResNet20 and VGG-Small for CIFAR-10, and ResNet18 for ILSVRC2012. For all quantized networks, all convolution layers except the first were quantized, and the low-bit set V was uniformly distributed from -1 to 1. Activations were quantized using the DoReFa method. Training for CIFAR-10 involved 500 epochs, with learning rate decay at 350, 440, and 475 epochs, and Kaiming initialization for the auxiliary matrix A. For ILSVRC2012, an Adam optimizer was used with a starting learning rate of 1e-3 and zero weight decay. Different temperature decay schedulers (sin, linear, exp) were compared, with the exponential scheduler (Ts=0.01, Te=10) being adopted due to its superior accuracy convergence. Performance was measured using Top-1 and Top-5 accuracy. For **Super-Resolution**, the method was applied to the VDSR model, trained on 291 images from [42] (augmented with rotation/flip) and tested on the Set5 dataset. Performance was evaluated using PSNR values at ×2 and ×3 scale factors. The importance of State Batch Normalization was further validated by comparing performance with and without SBN on binary ResNet18.",
        "limitations": "The primary limitation identified in the paper is that the temperature factor (τ) cannot be practically decreased to an absolute minimal value (i.e., zero) during training. While the gradually decreasing temperature strategy significantly reduces the quantization gap between the continuous weights used for training (Wc) and the discrete weights used for inference (Wq), a small residual mismatch may persist. This practical constraint necessitates additional mechanisms like State Batch Normalization (SBN) to compensate for the remaining statistical discrepancies. The paper also implies an assumption that if a neuron's probability for a discrete value exceeds a high threshold (e.g., max(p) > 0.999), it is considered sufficiently converged, which is a practical approximation rather than a theoretical absolute.",
        "future_research_directions": "Not mentioned"
      }
    },
    {
      "title": "PowerQuant: Automorphism Search for Non-Uniform Quantization",
      "abstract": "Deep neural networks (DNNs) are nowadays ubiquitous in many domains such as\ncomputer vision. However, due to their high latency, the deployment of DNNs\nhinges on the development of compression techniques such as quantization which\nconsists in lowering the number of bits used to encode the weights and\nactivations. Growing concerns for privacy and security have motivated the\ndevelopment of data-free techniques, at the expanse of accuracy. In this paper,\nwe identity the uniformity of the quantization operator as a limitation of\nexisting approaches, and propose a data-free non-uniform method. More\nspecifically, we argue that to be readily usable without dedicated hardware and\nimplementation, non-uniform quantization shall not change the nature of the\nmathematical operations performed by the DNN. This leads to search among the\ncontinuous automorphisms of $(\\mathbb{R}_+^*,\\times)$, which boils down to the\npower functions defined by their exponent. To find this parameter, we propose\nto optimize the reconstruction error of each layer: in particular, we show that\nthis procedure is locally convex and admits a unique solution. At inference\ntime, we show that our approach, dubbed PowerQuant, only require simple\nmodifications in the quantized DNN activation functions. As such, with only\nnegligible overhead, it significantly outperforms existing methods in a variety\nof configurations.",
      "full_text": "Arxiv version POWER QUANT : A UTOMORPHISM SEARCH FOR NON- UNIFORM QUANTIZATION Edouard Yvinec1,2 , Arnaud Dapogny2 , Matthieu Cord1 , Kevin Bailly1,2 Sorbonne Universit´e1, CNRS, ISIR, f-75005, 4 Place Jussieu 75005 Paris, France Datakalab2, 114 boulevard Malesherbes, 75017 Paris, France ey@datakalab.com ABSTRACT Deep neural networks (DNNs) are nowadays ubiquitous in many domains such as computer vision. However, due to their high latency, the deployment of DNNs hinges on the development of compression techniques such as quantization which consists in lowering the number of bits used to encode the weights and acti- vations. Growing concerns for privacy and security have motivated the devel- opment of data-free techniques, at the expanse of accuracy. In this paper, we identity the uniformity of the quantization operator as a limitation of existing ap- proaches, and propose a data-free non-uniform method. More speciﬁcally, we argue that to be readily usable without dedicated hardware and implementation, non-uniform quantization shall not change the nature of the mathematical oper- ations performed by the DNN. This leads to search among the continuous auto- morphisms of (R∗ +,×), which boils down to the power functions deﬁned by their exponent. To ﬁnd this parameter, we propose to optimize the reconstruction error of each layer: in particular, we show that this procedure is locally convex and admits a unique solution. At inference time, we show that our approach, dubbed PowerQuant, only require simple modiﬁcations in the quantized DNN activation functions. As such, with only negligible overhead, it signiﬁcantly outperforms existing methods in a variety of conﬁgurations. 1 I NTRODUCTION Deep neural networks (DNNs) tremendously improved algorithmic solutions for a wide range of tasks. In particular, in computer vision, these achievements come at a consequent price, as DNNs deployment bares a great energetic price. Consequently, the generalization of their usage hinges on the development of compression strategies. Quantization is one of the most promising such technique, that consists in reducing the number of bits needed to encode the DNN weights and/or activations, thus limiting the cost of data processing on a computing device. Existing DNN quantization techniques, for computer vision tasks, are numerous and can be distin- guished by their constraints. One such constraint is data usage, as introduced in Nagel et al. (2019), and is based upon the importance of data privacy and security concerns. Data-free approaches such as Banner et al. (2019); Cai et al. (2020); Choukroun et al. (2019); Fang et al. (2020); Garg et al. (2021); Zhao et al. (2019); Nagel et al. (2019), exploit heuristics and weight properties in order to perform the most efﬁcient weight quantization without having access to the training data. As com- pared to data-driven methods, the aforementioned techniques are more convenient to use but usually come with higher accuracy loss at equivalent compression rates. Data-driven methods performance offer an upper bound on what can be expected from data-free approaches and in this work, we aim at further narrowing the gap between these methods. To achieve this goal, we propose to leverage a second aspect of quantization: uniformity. For sim- plicity reasons, most quantization techniques such as Nagel et al. (2019); Zhao et al. (2019); Cong et al. (2022) perform uniform quantization, i.e. they consist in mapping ﬂoating point values to an evenly spread, discrete space. However, non-uniform quantization can theoretically provide a closer ﬁt to the network weight distributions, thus better preserving the network accuracy. Previous work on non-uniform quantization either focused on the search of binary codes (Banner et al., 2019; 1 arXiv:2301.09858v1  [cs.CV]  24 Jan 2023Arxiv version W8/A8 W6/A6 W4/A4 PowerQuant SPIQ SQuant OCS DFQ 80 70 60 50 40 ImageNet top1 Accuracy Figure 1: Comparison of the proposed method to other data-free quantization schemes on DenseNet 121 pre- trained on ImageNet. The proposed method (right bin in blue) drastically improves upon the existing data-free methods especially in the challenging W4/A4 quantization. Hubara et al., 2016; Jeon et al., 2020; Wu et al., 2016; Zhang et al., 2018) or leverage logarithmic distribution (Miyashita et al., 2016; Zhou et al., 2017). However, these approaches map ﬂoating point multiplications operations to other operations that are hard to leverage on current hardware (e.g. bit-shift) as opposed to uniform quantization which maps ﬂoating point multiplications to in- teger multiplications (Gholami et al., 2021; Zhou et al., 2016). To circumvent this limitation and reach a tighter ﬁt between the quantized and original weight distributions, in this work, we propose to search for the best possible quantization operator that preserves the nature of the mathematical operations. We show that this search boils down to the space deﬁned by the continuous automor- phisms of (R∗ +,×), which is limited to power functions deﬁned by their exponent. We optimize the value of this parameter by minimizing the error introduced by quantization. This allows us to reach superior accuracy, as illustrated in Fig 1. To sum it up, our contributions are: • We search for the best quantization operator that do not change the nature of the mathemat- ical operations performed by the DNN, i.e. the automorphisms of (R∗ +,×). We show that this search can be narrowed down to ﬁnding the best exponent for power functions. • We ﬁnd the optimal exponent parameter to more closely ﬁt the original weight distribution compared with existing (e.g. uniform and logarithmic) baselines. To do so, we propose to optimize the quantization reconstruction error. We show that this problem is locally convex and admits a unique solution. • In practice, we show that the proposed approach, dubbed PowerQuant, only requires simple modiﬁcations in the quantized DNN activation functions. Furthermore, we demonstrate through extensive experimentation that our method achieves outstanding results on various and challenging benchmarks with only negligible computational overhead. 2 R ELATED WORK 2.1 Q UANTIZATION In this section, we provide a background on the current state of DNNs quantization. Notice that while certain approaches are geared towards memory footprint reduction (e.g. without quantizing inputs and activations) (Chen et al., 2015; Gong et al., 2014; Han et al., 2016; Zhou et al., 2017), in what follows, we essentially focus on methods that aim at reducing the inference time. In particular, motivated by the growing concerns for privacy and security, data-free quantization methods (Banner et al., 2019; Cai et al., 2020; Choukroun et al., 2019; Fang et al., 2020; Garg et al., 2021; Zhao et al., 2019; Nagel et al., 2019; Cong et al., 2022) are emerging and have signiﬁcantly improved over the recent years. The ﬁrst breakthrough in data-free quantization (Nagel et al., 2019) was based on two mathematical ingenuities. First, they exploited the mathematical properties of piece-wise afﬁne activation func- 2Arxiv version tions (such as e.g. ReLU based DNNs) in order to balance the per-channel weight distributions by iteratively applying scaling factors to consecutive layers. Second, they proposed a bias correction scheme that consists in updating the bias terms of the layers with the difference between the ex- pected quantized prediction and the original predictions. They achieved near full-precision accuracy in int8 quantization. Since this seminal work, two main categories of data-free quantization methods have emerged. First, data-generation based methods, such as Cai et al. (2020); Garg et al. (2021), that used samples generated by Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) as samples to ﬁne-tune the quantized model through knowledge distillation (Hinton et al., 2014). Nevertheless, these methods are time-consuming and require signiﬁcantly more computational re- sources. Other methods, such as Banner et al. (2019); Choukroun et al. (2019); Fang et al. (2020); Zhao et al. (2019); Nagel et al. (2019); Cong et al. (2022), focus on improving the quantization operator but usually achieve lower accuracy. One limitation of these approaches is that they are essentially restricted to uniform quantization, while considering non-uniform mappings between the ﬂoating point and low-bit representation might be key to superior performance. 2.2 N ON-UNIFORM QUANTIZATION Indeed, in uniform settings, continuous variables are mapped to an equally-spaced grid in the orig- inal, ﬂoating point space. Such mapping introduces an error: however, applying such uniform mapping to an a priori non-uniform weight distribution is likely to be suboptimal in the general case. To circumvent this limitation, non-uniform quantization has been introduced (Banner et al., 2019; Hubara et al., 2016; Jeon et al., 2020; Wu et al., 2016; Zhang et al., 2018; Miyashita et al., 2016; Zhou et al., 2017) and (Zhang et al., 2021a; Li et al., 2019). We distinguish two categories of non-uniform quantization approaches. First, methods that introduce a code-base and require very sophisticated implementations for actual inference beneﬁts such as Banner et al. (2019); Hubara et al. (2016); Jeon et al. (2020); Wu et al. (2016); Zhang et al. (2018). Second, methods that simply modify the quantization operator such as Miyashita et al. (2016); Zhou et al. (2017). In particular, (Zhang et al., 2021a) propose a log-quantization technique. Similarly, Li et al. (Li et al., 2019) use log quantization with basis 2. In both cases, in practice, such logarithmic quantization scheme changes the nature of the mathematical operations involved, with multiplications being replaced by bit shifts. Nevertheless, one limitation of this approach is that because the very nature of the mathematical operations is intrinsically altered, in practice, it is hard to leverage without dedicated hardware and implementation. Instead of transforming ﬂoating point multiplications in integer mul- tiplications, they change ﬂoating point multiplications into bit-shifts or even look up tables (LUTs). Some of these operations are very speciﬁc to some hardware (e.g. LUTs are thought for FPGAs) and may not be well supported on most hardware. Conversely, in this work, we propose a non-uniform quantization scheme that preserves the nature of the mathematical operations by mapping ﬂoating point multiplications to standard integer multiplications. As a result, our approach boils down to simple modiﬁcations of the computations in the quantized DNN, hence allowing higher accuracies than uniform quantization methods while leading to straightforward, ready-to-use inference speed gains. Below we describe the methodology behind the proposed approach. 3 M ETHODOLOGY Let F be a trained feed forward neural network with L layers, each comprising a weight tensor Wl. Let Qbe a quantization operator such that the quantized weights Q(Wl) are represented on b bits. The most popular such operator is the uniform one. We argue that, despite its simplicity, the choice of such a uniform operator is responsible for a signiﬁcant part of the quantization error. In fact, the weights Wl most often follow a bell-shaped distribution for which uniform quantization is ill-suited: intuitively, in such a case, we would want to quantize more precisely the small weights on the peak of the distribution. For this reason, the most popular non-uniform quantization scheme is logarithmic quantization, outputting superior performance. Practically speaking, however, it consists in replacing the quantized multiplications by bit-shift operations. As a result, these methods have limited adaptability as the increment speed is hardware dependent. To adress this problem, we look for the non-uniform quantization operators that preserve the nature of matrix multiplications. Formally, taking aside the rounding operation in quantization, we want to 3Arxiv version square root distribution uniform distribution square distribution a=0.5 a=1 a=2 Figure 2: Inﬂuence of the power parameter a on the quantized distribution for weights distributed following a Gaussian prior. In such a case, the reconstruction error is typically minimized for a <1. deﬁne the space Qof functions Qsuch that ∀Q∈Q,∃Q−1 ∈Q s.t. ∀x,y Q −1(Q(x) ∗Q(y)) =x×y (1) where ∗is the intern composition law of the quantized space and ×is the standard multiplication, and Q, Q−1 are the quantization and de-quantization operators, respectively. In the case of uniform quantization and our work ∗will be the multiplication while in other non-uniform works it often corresponds to other operations that are harder to leverage, e.g. bit-shift. Recall that, for now, we omit the rounding operator. The proposed PowerQuant method consists in the search for the best suited operator Qfor a given trained neural network and input statistics. 3.1 A UTOMORPHISMS OF (R∗ +,×) Let Qbe a transformation from R+ to R+. In this case, ∗, the intern composition law in quantized space in (1), simply denote the scalar multiplication operator ×and (1) becomes Q(x) ×Q(y) = Q(x×y) ∀x,y ∈R2 +. In order to deﬁne a de-quantization operation, we need Q−1 to be deﬁned i.e. Qis bijective. Thus, by deﬁnition, Qis a group automorphism of (R∗ +,×). Thus, quantization operators that preserve the nature of multiplications are restricted to automorphisms of (R∗ +,×). The following lemma further restricts the search to power functions. Lemma 1.The set of continuous automorphisms of(R∗ +,×) is deﬁned by the set of power functions Q= {Q: x↦→xa|a∈R}. A proof of this result can be found in Appendix A. For the sake of clarity, we will now include the rounding operation in the quantization operators. Q= { Qa : W ↦→ ⌊ (2b−1 −1)sign(W) ×|W|a max |W|a ⌋⏐⏐⏐a∈R } (2) where W is a tensor and all the operations are performed element-wise. As functions of W, the quantization operators deﬁned in equation 2 are (signed) power functions. Fig 2 illustrates the effect of the power parameter a on quantization (vertical bars). Uniform quantization and a = 1 are equivalent and correspond to a quantization invariant to the weight distribution. For a <1, the quantization is more ﬁne-grained on weight values with low absolute value and coarser on high absolute values. Conversely, fora> 1, the quantization becomes more ﬁne-grained on high absolute values. We now deﬁne the search protocol in the proposed search space Q. 3.2 A UTOMORPHISM SEARCH AS A MINIMIZATION PROBLEM We propose to use the error introduced by quantization on the weights as a proxy on the distance between the quantized and the original model. Reconstruction Error Minimization: The operator Qa is not a bijection. Thus, quantization introduces a reconstruction error summed over all the layers of the network, and deﬁned as follows: ϵ(F,a) = L∑ l=1 Wl −Q−1 a (Qa(Wl))  p (3) 4Arxiv version where ∥·∥p denotes the Lp vector norm (in practice p= 2, see appendix B) and the de-quantization operator Q−1 a is deﬁned as: Q−1 a (W) =sign(W) × ⏐⏐⏐⏐W ×max |W| 2b−1 −1 ⏐⏐⏐⏐ 1 a (4) In practice, the problem of ﬁnding the best exponent a∗= argminaϵ(F,a) in (3) is a locally convex optimization problem (Appendix C.1) which has a unique minimum (see Appendix C.2). We ﬁnd the optimal value for ausing the Nelder–Mead method (Nelder & Mead, 1965) which solves problems for which derivatives may not be known or, in our case, are almost-surely zero (due to the rounding operation). In practice, more recent solvers are not required in order to reach the optimal solution (see Appendix D). Lastly, we discuss the limitations of the proposed metric in Appendix H. 3.3 F USED DE-QUANTIZATION AND ACTIVATION FUNCTION Based on equation 2, the quantization process of the weights necessitates the storage and multipli- cation of W along with a signs tensor, which is memory and computationally intensive. For the weights, however, this can be computed once during the quantization process, inducing no overhead during inference. As for activations, we do not have to store the sign of ReLU activations as they are always positive. In this case, the power function has to be computed at inference time (see al- gorithm 2). However, it can be efﬁciently computed Kim et al. (2021), using Newton’s method to approximate continuous functions in integer-only arithmetic. This method is very efﬁcient in prac- tice as it converges in 2 steps for low bit representations (four steps for int32). Thus, PowerQuant leads to signiﬁcant accuracy gains with limited computational overhead. Conversely, for non-ReLU feed forward networks such as EfﬁcientNets (SiLU) or Image Transformers (GeLU), activations are signed. This can be tackled using asymmetric quantization which consists in the use of a zero-point. In general, asymmetric quantization allows one to have a better coverage of the quantized values support. In our case, we use asymmetric quantization to work with positive values only. Formally, for both SiLU and GeLU, the activations are analytically bounded below by CSiLU = 0.27846 and CGeLU = 0.169971 respectively. Consequently, assuming a layer with SiLU activation with input x and weights W, we have: Q−1 a (Qa(x+ CSiLU)Qa(W)) ≈((x+ CSiLU)aWa) 1 a = xW + CSiLUW (5) The bias term CSiLUW induces a very slight computation overhead which is standard in asymmetric quantization. We provide a detailed empirical evaluation of this cost in Appendix G. Using the adequate value for the bias corrector, we can generalize equation 5 to any activation functionσ. The quantization process and inference with the quantized DNN are summarized in Algorithm 1 and 2. The proposed representation is fully compatible with integer multiplication as deﬁned in Jacob et al. (2018), thus it is fully compatible with integer only inference (see appendix F for more details). Algorithm 1Weight Quantization Algorithm Require: trained neural network F with Llayers to quantize, number of bits b a←solver(min{error(F,a)}) ⊿in practice we use the Nelder–Mead method for l∈{1,...,L }do Wsign ←sign(Wl) ⊿save the sign of the scalar values in W Wl ←Wsign ×|Wl|a ⊿power transformation s←max |Wl| 2b−1−1 ⊿get quantization scale Q: Wl ↦→ ⌊Wl s ⌉ and Q−1 : W ↦→Wsign ×|W ×s| 1 a ⊿qdeﬁne Qand Q−1 end for 4 E XPERIMENTS In this section, we empirically validate our method. First, we discuss the optimization of the expo- nent parameter aof PowerQuant using the reconstruction error, showing its interest as a proxy for the quantized model accuracy from an experimental standpoint. We show that the proposed approach preserves this reconstruction error signiﬁcantly better, allowing a closer ﬁt to the original weight dis- tribution through non-uniform quantization. Second, we show through a variety of benchmarks that 5Arxiv version Algorithm 2Simulated Inference Algorithm Require: trained neural network F quantized with Llayers, input X and exponent a∗ for l∈{1,...,L }do X ←Xa∗ ⊿X is assumed positive (see equation (5)) XQ ←⌊XsX⌉ ⊿where sX is a scale in the input range O←Fl(XQ) ⊿O contains the quantized output of the layer X ← ( σ(O) sXsW )1 a∗ ⊿where σis the activation function and sW the weight scale end for return X the proposed approach signiﬁcantly outperforms state-of-the-art data-free methods, thanks to more efﬁcient power function quantization with optimized exponent. Third, we show that the proposed approach comes at a negligible cost in term of inference speed. 4.1 D ATASETS AND IMPLEMENTATION DETAILS We validate the proposed PowerQuant method on ImageNet classiﬁcation (Deng et al., 2009) (≈ 1.2M images train/50k test). In our experiments we used pre-trained MobileNets (Sandler et al., 2018), ResNets (He et al., 2016), EfﬁcientNets (Tan & Le, 2019) and DenseNets (Huang et al., 2017). We used Tensorﬂow implementations of the baseline models from ofﬁcial repositories, achieving standard baseline accuracies. The quantization process was done using Numpy library. Activations are quantized as unsigned integers and weights are quantized using a symmetric repre- sentation. We fold batch-normalization layers as in Yvinec et al. (2022a). We performed ablation study using the uniform quantization operator over weight values from Kr- ishnamoorthi (2018) and logarithmic quantization from Miyashita et al. (2016). For our compari- son with state-of-the-art approaches in data-free quantization, we implemented the more complex quantization operator from SQuant (Cong et al., 2022). To compare with strong baselines, we also implement bias correction (Nagel et al., 2019) (which measures the expected difference between the outputs of the original and quantized models and updates the biases terms to compensate for this difference) as well as input weight quantization (Nagel et al., 2019). 4.2 E XPONENT PARAMETER FITTING Fig 3 illustrates the evolution of both the accuracy of the whole DNN and the reconstruction error summed over all the layers of the network, as functions of the exponent parameter a. Our target is the highest accuracy with respect to the value of a: however, in a data-free context, we only have access to the reconstruction error. Nevertheless, as shown on Fig 3, these metrics are strongly anti- correlated. Furthermore, while the reconstruction curve is not convex it behaves well for simplex based optimization method such as the Nelder-Mead method. This is due to two properties: locally convex (Appendix C.1) and has a unique minimum (Appendix C.2). Empirically, optimal values a∗ for the exponent parameter are centered on 0.55, which approxi- mately corresponds to the ﬁrst distribution in Fig 2. Still, as shown on Table 1 we observe some variations on the best value for awhich motivates the optimization of afor each network and bit- width. Furthermore, our results provide a novel insight on the difference between pruning and quantization. In the pruning literature (Han et al., 2015; Frankle & Carbin, 2018; Molchanov et al., 2019), the baseline method consists in setting the smallest scalar weight values to zero and keeping unchanged the highest non-zero values, assuming that small weights contribute less to the network prediction. In a similar vein, logarithmic or power quantization with a > 1 roughly quantizes (almost zeroing it out) small scalar values to better preserve the precision on larger values. In prac- tice, in our case, lower reconstruction errors, and better accuracies, are achieved by setting a <1: this suggests that the assumption behind pruning can’t be straightforwardly applied to quantization, where in fact we argue that ﬁnely quantizing smaller weights is paramount to preserve the patterns learned at each layer, and the representation power of the whole network. 6Arxiv version Figure 3: Accuracy/reconstruction error relationship for ResNet and DenseNet quantized in W4/A4. Table 1: Comparison between logarithmic, uniform and the proposed quantization scheme on ResNet 50 trained for ImageNet classiﬁcation task. We report for different quantization conﬁguration (weights noted W and activations noted A) both the top1 accuracy and the reconstruction error (equation 3). Architecture Method W-bit A-bit a∗ Accuracy Reconstruction Error ResNet 50 Baseline 32 32 - 76.15 - uniform 8 8 1 76.15 1.1 ×10−4 logarithmic 8 8 - 76.12 2.0 ×10−4 PowerQuant 8 8 0.55 76.15 1.0 ×10−4 uniform 4 4 1 54.68 3.5 ×10−3 logarithmic 4 4 - 57.07 2.1 ×10−3 PowerQuant 4 4 0.55 70.29 1.9 ×10−3 Another approach that puts more emphasis on the nuances between low valued weights is logarith- mic based non-uniform quantization. In Table 1 and Appendix E, we compare the proposed power method to both uniform and logarithmic approaches. By deﬁnition, the proposed power method nec- essarily outperforms the uniform method in every scenario as uniform quantization is included in the search space. For instance, in int4, the proposed method improves the accuracy by 13.22 points on ResNet 50. This improvement can also be attributed to a better input quantization of each layer, especially on ResNet 50 where the gap in the reconstruction error (over the weights) is smaller. 4.3 C OMPARISON WITH DATA-FREE QUANTIZATION METHODS In table 2, we report the performance of several data-free quantization approaches on ResNet 50. Although no real training data is involved in these methods, some approaches such as ZeroQ (Cai et al., 2020), DSG (Zhang et al., 2021b) or GDFQ (Xu et al., 2020) rely on data generation (DG) in order to calibrate parameters of the method or to apply ﬁne-tuning to preserve the accuracy through quantization. As shown in table 2, in the W8/A8 setup, the proposed PowerQuant method outper- forms other data-free solutions, fully preserving the accuracy of the ﬂoating point model. The gap is even wider on the more challenging low bit quantization W4/A4 setup, where the PowerQuant im- proves the accuracy by1.93 points over SQuant (Cong et al., 2022) and by14.88 points over GDFQ. This shows the effectiveness of the method on ResNet 50. We provide more results on DenseNet (Huang et al., 2017), MobileNet (Sandler et al., 2018), Efﬁcient Net (Tan & Le, 2019) in Appendix J. These results demonstrate the versatility of the method on both large and very compact convnets. In summary, the proposed PowerQuant vastly outperforms other data-free quantization schemes. Last but not least, when compared to recent QAT methods such as OCTA V Sakr et al. (2022), PowerQuant achieves competitive results on both ResNets and MobileNets using either both static or dynamic quantization. This is remarkable since PowerQuant does not involve any ﬁne-tuning of the network. We provide more details on this benchamrk in Appendix I. In what follows, we evaluate PowerQuant on recent transformer architectures for both image and language applications. 7Arxiv version Table 2: Comparison between state-of-the-art post training quantization techniques on ResNet 50 on ImageNet. We distinguish methods relying on data (synthetic or real) or not. In addition to being fully data-free, our approach signiﬁcantly outperforms existing methods. Architecture Method Data W-bit A-bit Accuracy gap ResNet 50 Baseline - 32 32 76.15 - DFQ Nagel et al. (2019) No 8 8 75.45 -0.70 ZeroQ Cai et al. (2020) Synthetic 8 8 75.89 -0.26 DSG Zhang et al. (2021b) Synthetic 8 8 75.87 -0.28 GDFQ Xu et al. (2020) Synthetic 8 8 75.71 -0.44 SQuant Cong et al. (2022) No 8 8 76.04 -0.11 PowerQuant No 8 8 76.15 0.00 DFQ Nagel et al. (2019) No 4 4 0.10 -76.05 ZeroQ Cai et al. (2020) Synthetic 4 4 7.75 -68.40 DSG Zhang et al. (2021b) Synthetic 4 4 23.10 -53.05 GDFQ Xu et al. (2020) Synthetic 4 4 55.65 -20.50 SQuant Cong et al. (2022) No 4 4 68.60 -7.55 PowerQuant No 4 4 70.53 -5.62 Table 3: Comparison of data-free quantization methods on ViT and DeiT trained on ImageNet. model method W / A accuracy ViT baseline -/- 78.05% DFQ (ICCV 2019) 8/8 70.33% SQuant (ICLR 2022) 8/8 68.85% PSAQ (arxiv 2022) 8/8 37.36% PowerQuant 8/8 77.46% DFQ (ICCV 2019) 4/8 66.63% SQuant (ICLR 2022) 4/8 64.62% PSAQ (arxiv 2022) 4/8 25.34% PowerQuant 4/8 75.24% (a) Evaluation for ViT Base model method W / A accuracy DeiT T baseline -/- 72.21% DFQ (ICCV 2019) 8/8 71.32% SQuant (ICLR 2022) 8/8 71.11% PSAQ (arxiv 2022) 8/8 71.56% PowerQuant 8/8 72.23% DFQ (ICCV 2019) 4/8 67.71% SQuant (ICLR 2022) 4/8 67.58% PSAQ (arxiv 2022) 4/8 65.57% PowerQuant 4/8 69.77% (b) Evaluation for DeiT Tiny model method W / A accuracy DeiT S baseline -/- 79.85% DFQ (ICCV 2019) 8/8 78.76% SQuant (ICLR 2022) 8/8 78.94% PSAQ (arxiv 2022) 8/8 76.92% PowerQuant 8/8 79.33% DFQ (ICCV 2019) 4/8 76.75% SQuant (ICLR 2022) 4/8 76.61% PSAQ (arxiv 2022) 4/8 73.23% PowerQuant 4/8 78.16% (c) Evaluation for DeiT Small model method W / A accuracy DeiT B baseline -/- 81.85% DFQ (ICCV 2019) 8/8 80.72% SQuant (ICLR 2022) 8/8 80.60% PSAQ (arxiv 2022) 8/8 79.10% PowerQuant 8/8 81.26% DFQ (ICCV 2019) 4/8 79.41% SQuant (ICLR 2022) 4/8 79.21% PSAQ (arxiv 2022) 4/8 77.05% PowerQuant 4/8 80.67% (d) Evaluation for DeiT Base 4.4 E VALUATION ON TRANSFORMER ARCHITECTURES In Table 3, we quantized the weight tensors of a ViT Dosovitskiy et al. (2021) with85M parameters and baseline accuracy≈78 as well as DeiT T,S and B Touvron et al. (2021) with baseline accuracies 72.2, 79.9 and 81.8 and ≈5M, ≈22M, ≈87M parameters respectively. Similarly to ConvNets, the image transformer is better quantized using PowerQuant rather than standard uniform quantization schemes such as DFQ. Furthermore, more complex and recent data-free quantization schemes such as SQuant, tend to under-perform on the novel Transformer architectures as compared to ConvNets. This is not the case for PowerQuant which maintains its very high performance even in low bit representations. This is best illustrated on ViT where PowerQuant W4/A8 out performs both DFQ and SQuant even when they are allowed 8 bits for the weights (W8/A8) by a whopping 4.91 points. The proposed PowerQuant even outperforms methods dedicated to transformer quantization such as PSAQ Li et al. (2022) on every image transformer tested. 8Arxiv version Table 4: Complementary Benchmarks on the GLUE task quantized in W4/A8. We consider the BERT trans- former architecture. We provide the original performance (from the article) of BERT on GLUE as well as our reproduced results (baseline). task original baseline uniform log SQuant PowerQuant CoLA 49.23 47.90 45.60 45.67 46.88 47.11 SST-2 91.97 92.32 91.81 91.53 91.09 92.23 MRPC 89.47/85.29 89.32/85.41 88.24/84.49 86.54/82.69 88.78/85.24 89.26/85.34 STS-B 83.95/83.70 84.01/83.87 83.89/83.85 84.01/83.81 83.80/83.65 84.01/83.87 QQP 88.40/84.31 90.77/84.65 89.56/83.65 90.30/84.04 90.34/84.32 90.61/84.45 MNLI 80.61/81.08 80.54/80.71 78.96/79.13 78.96/79.71 78.35/79.56 79.02/80.28 QNLI 87.46 91.47 89.36 89.52 90.08 90.23 RTE 61.73 61.82 60.96 60.46 60.21 61.45 WNLI 45.07 43.76 39.06 42.19 42.56 42.72 Table 5: ACE cost of the overhead computations introduced by PowerQuant. Architecture overhead cost accuracy in W6/A6 ResNet 50 0.63% 75.07 DenseNet 121 0.97% 72.71 MobileNet V2 0.57% 52.20 EfﬁcientNet B0 0.80% 58.24 We further compare the proposed power quantization, in W4/A8, on natural language processing (NLP) tasks and report results in Table 4. We evaluate a BERT model (Devlin et al., 2018) on GLUE (Wang et al., 2018) and report both the original (reference) and our reproduced (baseline) results. We compare the three quantization processes: uniform, logarithmic and PowerQuant. Similarly to com- puter vision tasks, the power quantization outperforms the other methods in every instances which further conﬁrms its ability to generalize well to transformers and NLP tasks. In what follows, we show experimentally that our approach induces very negligible overhead at inference time, making this accuracy enhancement virtually free from a computational standpoint. 4.5 I NFERENCE COST AND PROCESSING TIME The ACE metrics was recently introduced in Zhang et al. (2022) to provide a hardware-agnostic measurement of the overhead computation cost in quantized neural networks. In Table 5, we evaluate the cost in the inference graph due to the change in the activation function. We observe very similar results to Table 17. The proposed changes are negligible in terms of computational cost on all tested networks. Furthermore, DenseNet has the highest cost due to its very dense connectivity. On the other hand, using this metric it seems that the overhead cost due to the zero-point technique from section 3.3 for EfﬁcientNet has no signiﬁcant impact as compared to MobileNet and ResNet. In addition, we provide a more detailed discussion on the inference and processing cost of PowerQuant on speciﬁc hardware using dedicated tools in Appendix K. 5 C ONCLUSION In this paper, we pinpointed the uniformity of the quantization as a limitation of existing data- free methods. To address this limitation, we proposed a novel data-free method for non-uniform quantization of trained neural networks for computer vision tasks, with an emphasis on not chang- ing the nature of the mathematical operations involved (e.g. matrix multiplication). This led us to search among the continuous automorphisms of(R∗ +,×), which are restricted to the power functions x→xa. We proposed an optimization of this exponent parameter based upon the reconstruction er- ror between the original ﬂoating point weights and the quantized ones. We show that this procedure is locally convex and admits a unique solution. At inference time, the proposed approach, dubbed PowerQuant, involves only very simple modiﬁcations in the quantized DNN activation functions. We empirically demonstrate that PowerQuant allows a closer ﬁt to the original weight distributions compared with uniform or logarithmic baselines, and signiﬁcantly outperforms existing methods in 9Arxiv version a variety of benchmarks with only negligible computational overhead at inference time. In addi- tion, we also discussed and addressed some of the limitations in terms of optimization (per-layer or global) and generalization (non-ReLU networks). Future work involves the search of a better proxy error as compared with the proposed weight re- construction error as well as the extension of the search space to other internal composition law of R+ that are suited for efﬁcient calculus and inference. ACKNOWLEDGMENTS This work has been supported by the french National Association for Research and Technology (ANRT), the company Datakalab (CIFRE convention C20/1396) and by the French National Agency (ANR) (FacIL, project ANR-17-CE33-0002). This work was granted access to the HPC resources of IDRIS under the allocation 2022-AD011013384 made by GENCI. REFERENCES Ron Banner, Yury Nahshan, and Daniel Soudry. Post training 4-bit quantization of convolutional networks for rapid-deployment. NeurIPS, pp. 7950–7958, 2019. Yash Bhalgat, Jinwon Lee, Markus Nagel, Tijmen Blankevoort, and Nojun Kwak. Lsq+: Improving low-bit quantization through learnable offsets and better initialization. CVPR Workshops, pp. 696–697, 2020. Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Zeroq: A novel zero shot quantization framework. CVPR, pp. 13169–13178, 2020. Wenlin Chen, James Wilson, et al. Compressing neural networks with the hashing trick. ICML, pp. 2285–2294, 2015. Vladimir Chikin and Vladimir Kryzhanovskiy. Channel balancing for accurate quantization of wino- grad convolutions. In CVPR, pp. 12507–12516, 2022. Yoni Choukroun, Eli Kravchik, Fan Yang, and Pavel Kisilev. Low-bit quantization of neural net- works for efﬁcient inference. ICCV Workshops, pp. 3009–3018, 2019. Guo Cong, Qiu Yuxian, Leng Jingwen, Gao Xiaotian, Zhang Chen, Liu Yunxin, Yang Fan, Zhu Yuhao, and Guo Minyi. Squant: On-the-ﬂy data-free quantization via diagonal hessian approxi- mation. ICLR, 2022. Andrew R Conn, Katya Scheinberg, and Ph L Toint. On the convergence of derivative-free methods for unconstrained optimization. Approximation theory and optimization: tributes to MJD Powell, pp. 83–108, 1997. J. Deng, W. Dong, et al. ImageNet: A Large-Scale Hierarchical Image Database. CVPR, 2009. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021. Jun Fang, Ali Shaﬁee, Hamzah Abdel-Aziz, David Thorsley, Georgios Georgiadis, and Joseph H Hassoun. Post-training piecewise linear quantization for deep neural networks. ECCV, pp. 69– 86, 2020. Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. ICLR, 2018. Sahaj Garg, Anirudh Jain, Joe Lou, and Mitchell Nahmias. Confounding tradeoffs for neural net- work quantization. arXiv preprint arXiv:2102.06366, 2021. 10Arxiv version Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. A survey of quantization methods for efﬁcient neural network inference. arXiv preprint arXiv:2103.13630, 2021. Yunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev. Compressing deep convolutional net- works using vector quantization. arXiv preprint arXiv:1412.6115, 2014. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. NeurIPS, 27, 2014. Zbigniew Hajduk. High accuracy fpga activation function implementation for neural networks. Neurocomputing, 247:59–61, 2017. Kun Han, Yuxuan Wang, DeLiang Wang, William S Woods, Ivo Merks, and Tao Zhang. Learning spectral mapping for speech dereverberation and denoising. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 23(6):982–992, 2015. Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. ICLR, 2016. Kaiming He, Xiangyu Zhang, et al. Deep residual learning for image recognition. CVPR, pp. 770–778, 2016. Horst Herrlich. Axiom of choice, volume 1876. Springer, 2006. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. NeurIPS, 2014. Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. CVPR, pp. 4700–4708, 2017. Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks. NeurIPS, 29, 2016. Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efﬁcient integer-arithmetic-only inference. CVPR, pp. 2704–2713, 2018. Yongkweon Jeon, Baeseong Park, Se Jung Kwon, Byeongwook Kim, Jeongin Yun, and Dongsoo Lee. Biqgemm: matrix multiplication with lookup table for binary-coding-based quantized dnns. SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 1–14, 2020. Yongkweon Jeon, Chungman Lee, Eulrang Cho, and Yeonju Ro. Mr. biq: Post-training non-uniform quantization based on minimizing the reconstruction error. In CVPR, pp. 12329–12338, 2022. Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. I-bert: Integer- only bert quantization. In International conference on machine learning, pp. 5506–5518. PMLR, 2021. Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efﬁcient inference: A whitepaper. arXiv preprint arXiv:1806.08342, 2018. Maximilian Lam, Michael Mitzenmacher, Vijay Janapa Reddi, Gu-Yeon Wei, and David Brooks. Tabula: Efﬁciently computing nonlinear activation functions for secure neural network inference. arXiv preprint arXiv:2203.02833, 2022. Yuhang Li, Xin Dong, and Wei Wang. Additive powers-of-two quantization: An efﬁcient non- uniform discretization for neural networks. arXiv preprint arXiv:1909.13144, 2019. Zhikai Li, Liping Ma, Mengjuan Chen, Junrui Xiao, and Qingyi Gu. Patch similarity aware data-free quantization for vision transformers. arXiv preprint arXiv:2203.02250, 2022. Daisuke Miyashita, Edward H Lee, and Boris Murmann. Convolutional neural networks using logarithmic data representation. arXiv preprint arXiv:1603.01025, 2016. 11Arxiv version Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. Importance estimation for neural network pruning. CVPR, pp. 11264–11272, 2019. Markus Nagel, Mart van Baalen, et al. Data-free quantization through weight equalization and bias correction. ICCV, pp. 1325–1334, 2019. John A Nelder and Roger Mead. A simplex method for function minimization. The computer journal, 7(4):308–313, 1965. Michael JD Powell. An efﬁcient method for ﬁnding the minimum of a function of several variables without calculating derivatives. The computer journal, 7(2):155–162, 1964. Charbel Sakr, Steve Dai, Rangha Venkatesan, Brian Zimmer, William Dally, and Brucek Khailany. Optimal clipping and magnitude-aware differentiation for improved quantization-aware training. In ICML, pp. 19123–19138. PMLR, 2022. Mark Sandler, Andrew Howard, et al. Mobilenetv2: Inverted residuals and linear bottlenecks.CVPR, pp. 4510–4520, 2018. Mingxing Tan and Quoc V Le. Efﬁcientnet: Rethinking model scaling for convolutional neural networks. ICML, pp. 6105–6114, 2019. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv´e J ´egou. Training data-efﬁcient image transformers & distillation through attention. In International Conference on Machine Learning, pp. 10347–10357. PMLR, 2021. Mart van Baalen, Brian Kahne, Eric Mahurin, Andrey Kuzmin, Andrii Skliar, Markus Nagel, and Tijmen Blankevoort. Simulated quantization, real power savings. InCVPR, pp. 2757–2761, 2022. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceed- ings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 353–355, Brussels, Belgium, November 2018. Association for Computational Lin- guistics. doi: 10.18653/v1/W18-5446. URL https://aclanthology.org/W18-5446. Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, and Jian Cheng. Quantized convolutional neural networks for mobile devices. CVPR, pp. 4820–4828, 2016. Shoukai Xu, Haokun Li, Bohan Zhuang, Jing Liu, Jiezhang Cao, Chuangrun Liang, and Mingkui Tan. Generative low-bitwidth data free quantization. ECCV, pp. 1–17, 2020. Edouard Yvinec, Arnaud Dapogny, and Kevin Bailly. To fold or not to fold: a necessary and sufﬁcient condition on batch-normalization layers folding. IJCAI, 2022a. Edouard Yvinec, Arnaud Dapogny, Matthieu Cord, and Kevin Bailly. Spiq: Data-free per-channel static input quantization. arXiv preprint arXiv:2203.14642, 2022b. Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua. Lq-nets: Learned quantization for highly accurate and compact deep neural networks. ECCV, pp. 365–382, 2018. Sai Qian Zhang, Bradley McDanel, HT Kung, and Xin Dong. Training for multi-resolution inference using reusable quantization terms. In Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, pp. 845–860, 2021a. Xiangguo Zhang, Haotong Qin, Yifu Ding, Ruihao Gong, Qinghua Yan, Renshuai Tao, Yuhang Li, Fengwei Yu, and Xianglong Liu. Diversifying sample generation for accurate data-free quantiza- tion. CVPR, pp. 15658–15667, 2021b. Yichi Zhang, Zhiru Zhang, and Lukasz Lew. Pokebnn: A binary pursuit of lightweight accuracy. In CVPR, pp. 12475–12485, 2022. Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, and Zhiru Zhang. Improving neural network quantization without retraining using outlier channel splitting. ICML, pp. 7543–7552, 2019. 12Arxiv version Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen. Incremental network quantiza- tion: Towards lossless cnns with low-precision weights. ICLR, 2017. Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Train- ing low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016. 13Arxiv version A P ROOF OF LEMMA 1 In this section, we provide a simple proof for lemma 1 as well as a discussion on the continuity hypothesis. Proof. We have that ∀x ∈R+,Q(x) ×Q(0) =Q(0) and ∀x ∈R+,Q(x) ×Q(1) =Q(x) which induces that Qis either the constant 1 or Q(0) = 0and Q(1) = 1. Because Qis an automorphism we can eliminate the ﬁrst option. Now, we will demonstrate that Qis necessarily a power function. Let nbe an integer, then Q(xn) =Q(x) ×Q(xn−1) =Q(x)2 ×Q(xn−2) =··· = Q(x)n. (6) Similarly, for fractions, we get Q(x 1 n ) ×···× Q(x 1 n ) =Q(x) ⇔Q(x 1 n ) =Q(x) 1 n . Assuming Q is continuous, we deduce that for any rational a∈R, we have Q(xa) =Q(x)a (7) In order to verify that the solution is limited to power functions, we use a reductio ad absurdum. Assume Q is not a power function. Therefore, there exists (x,y) ∈ R2 + and a ∈ R such that Q(x) ̸= xa and Q(y) =ya. By deﬁnition of the logarithm, there exists bsuch that xb = y. We get the following contradiction, from (7), { Q(xba ) =Q(ya) =ya Q(xba ) =Q(xab) =Q(xa)b ̸= ( xab = ya) (8) Consequently, the suited functions Qare limited to power functions i.e. Q= {Q : x ↦→xa|a ∈ R}. We would also like to put the emphasis on the fact that there are other Automorphisms of (R,×). However, the construction of such automorphisms require the axiom of choice Herrlich (2006). Such automorphisms are not applicable in our case which is why the key constraint is being an automorphism rather than the continuous property. B N ORM SELECTION In the minimization objective, we need to select a norm to apply. In this section, we provide the- oretical arguments in favor of the l2 vector norm. Let F be a feed forward neural network with L layers to quantize, each deﬁned by a set of weights Wl = (wl)i,j ∈Rnl×ml and bias bl ∈Rnl . We note (λ(i) l )i the eigenvalues associated with Wl. We want to study the distance d(F,Fa) between the predictive function F and its quantized version Fa deﬁned as d(F,Fa) = max x∈D ∥F(x) −Fa(x)∥p (9) where Dis the domain of F. We prove that minimizing the reconstruction error with respect to ais equivalent to minimizing d(F,Fa) with respect to a. Assume L = 1for the sake of simplicity and we drop the notation l. With the proposed PowerQuant method, we minimize the vector norm ∥W −Q−1 a (Qa(W))∥p p = ∑ i<=n max j<=m |wi,j −Q−1 a (Qa(wi,j))|p (10) For p= 2, the euclidean norm is equal to the spectral norm, thus minimizing∥W−Q−1 a (Qa(W))∥2 is equivalent to minimizing d(F,Fa) for L = 1. However, we know that minimizing for another value of pmay result in a different optimal solution and therefore not necessarily minimized(F,Fa). In the context of data-free quantization, we want to avoid uncontrollable changes on F, which is why we recommend the use of p= 2. 14Arxiv version C M ATHEMATICAL PROPERTIES C.1 L OCAL CONVEXITY We prove that the minimization problem deﬁned in equation 3 is locally convex around the solution a∗. Formally we prove that x↦→ x−Q−1 a (Qa(x))  p (11) is locally convex around a∗deﬁned as arg mina x−Q−1 a (Qa(x))  p. Lemma 2. The minimization problem deﬁned as arg min a {x−Q−1 a (Qa(x))  p } (12) is locally convex around any solution a∗. Proof. We recall that ∂xa ∂a = xalog(x). The function x−Q−1 a (Qa(x)) is differentiable. We assume x∈R, then we can simplify the sign functions (assumexpositive without loss of generality) and note y= max|x|, then ∂Q−1 a (Qa(x)) ∂a = ∂ ⏐⏐⏐ ⌊ (2b−1 −1)xa ya ⌋ ya 2b−1−1 ⏐⏐⏐ 1 a ∂a . (13) This simpliﬁes to ∂Q−1 a (Qa(x)) ∂a = y ∂ ( ⌊B(x y ) a ⌋ B )1 a ∂a , (14) with B = 2b−1 −1. By using the standard differentiation rules, we know that the rounding operator has a zero derivative a.e.. Consequently we get, ∂Q−1 a (Qa(x)) ∂a = −a2y   ⌊ B ( x y )a⌋ B   1 a log   ⌊ B ( x y )a⌋ B  . (15) Now we can compute the second derivative of Q−1 a (Qa(x)), ∂2Q−1 a (Qa(x)) ∂a2 = a4y   ⌊ B ( x y )a⌋ B   1 a log2   ⌊ B ( x y )a⌋ B  . (16) From this expression, we derive the second derivative, using the property(f◦g)′′= f′′◦g×g′2 + f′◦g×g′′and the derivatives |·| 1 p ′ = x|x| 1 p −2 p and |·| 1 p ′′ = 1−p p2 |x| 1 p x2 , then for any xi ∈x ∂2 ⏐⏐xi −Q−1 a (Qa(xi)) ⏐⏐ ∂a2 = 1 −p p2 |xi −Q−1 a (Qa(xi)| 1 p (xi −Q−1a (Qa(xi))2 (∂Q−1 a (Qa(x)) ∂a )2 + (xi −Q−1 a (Qa(xi))|xi −Q−1 a (Qa(xi)| 1 p −2 p ∂2Q−1 a (Qa(x)) ∂a2 (17) We now note the ﬁrst term in the previous additionT1 = 1−p p2 |xi−Q−1 a (Qa(xi)| 1 p (xi−Q−1 a (Qa(xi))2 ( ∂Q−1 a (Qa(x)) ∂a )2 and the second term as a product ofT2 = (xi−Q−1 a (Qa(xi))|xi−Q−1 a (Qa(xi)| 1 p −2 p times T3 = ∂2Q−1 a (Qa(x)) ∂a2 . We know that T1 > 0 and T3 > 0, consequently, and T2 is continuous in a. At a∗ the terms with |xi −Q−1 a (Qa(xi)) |are negligible in comparison with ∂2Q−1 a (Qa(x)) ∂a2 and ( ∂Q−1 a (Qa(x)) ∂a )2 . Consequently, there exists an open set around a∗where T1 >|T2|T3, and ∂2|xi−Q−1 a (Qa(xi))| ∂a2 >0. This concludes the proof. 15Arxiv version Table 6: Minimization of the reconstruction error on a MobileNet V2 for W6/A6 quantization with different solvers. Solver a∗ reconstruction error accuracy Nelder-Mead 0.750 1.12 64.248 Powell (Powell, 1964) 0.744 1.10 64.104 COBYLA (Conn et al., 1997) 0.752 1.11 64.364 C.2 U NIQUENESS OF THE SOLUTION In this section we provide the elements of proof on the uniqueness of the solution of the minimization of the quantization reconstruction error. Lemma 3. The minimization problem over x∈RN deﬁned as arg min a {x−Q−1 a (Qa(x))  p } (18) has almost surely a unique global minimum a∗. Proof. We assume that x can not be exactly quantized, i.e. mina {x−Q−1 a (Qa(x))  p } > 0 which is true almost everywhere. We use a reductio ad absurdum and assume that there ex- ist two optimal solutions a1 and a2 to the optimization problem. We expand the expressionx−Q−1 a (Qa(x))  p and get x−Q−1 a (Qa(x))  p = x− ⏐⏐⏐⏐ ⌊ (2b−1 −1)sign(x) ×|x|a max |x|a ⌋max |x|a 2b−1 −1 ⏐⏐⏐⏐ 1 a sign(x) .p (19) We note the rounding term Ra and get x−Q−1 a (Qa(x))  p = x− ⏐⏐⏐⏐Ra max |x|a 2b−1 −1 ⏐⏐⏐⏐ 1 a sign(x)  p . (20) Assume Ra1 = Ra2 = R, the minimization problem arg mina x− ⏐⏐⏐Rmax |x|a 2b−1−1 ⏐⏐⏐ 1 a sign(x)  p is convex and has a unique solution, thus a1 = a2. Now assume Ra1 ̸= Ra2 . Let’s denoteD(R) the domain of power valuesaover which we have ⌊ (2b−1 −1)sign(x)×|x|a max |x|a ⌋ = R. If there is a value aoutside of D(Ra1 ) ∪D(Ra2 ) such that R′has each of its coordinate strictly between the coordinates of Ra1 and Ra2 , then, without loss of generality, assume that at least half of the coordinates of Ra1 are further away from the corresponding coordinates of xthan one quan- tization step. This implies that there exists a value a′in D(R′) such that x−Q−1 a′ (Qa′(x))  p <x−Q−1 a1 (Qa1 (x))  p. which goes against our hypothesis. Thus, there are up to N possible values for Rthat minimize the problem which happens iff xsatisﬁes at least one coordinate can be either ceiled or ﬂoored by the rounding. The set deﬁned by this condition has a zero measure. D S OLVER FOR MINIMIZATION In the main article we state that we can use Nelder-Mead (Nelder & Mead, 1965) solver to ﬁnd the optimal a∗. We tested several other solvers and report the results in Table 6. The empirical results show that basically any popular solver can be used, and that the Nelder-Mead solver is sufﬁcient for the minimization problem. E C OMPARISON BETWEEN LOG, NAIVE AND POWER QUANTIZATION COMPLEMENTARY RESULTS To complement the results provided in the main paper on ResNet 50, we list in Table 7 more quan- tization setups on ResNet 50 as well as DenseNet 121. To put it in a nutshell, The proposed power 16Arxiv version Table 7: Comparison between logarithmic, uniform and the proposed quantization scheme on ResNet 50 and DenseNet 121 trained for ImageNet classiﬁcation task. We report for different quantization conﬁguration (weights noted W and activations noted A) both the top1 accuracy and the reconstruction error (equation 3). Architecture Method W-bit A-bit a∗ Accuracy Reconstruction Error ResNet 50 Baseline 32 32 - 76.15 - uniform 8 8 1 76.15 1.1 ×10−4 logarithmic 8 8 - 76.12 2.0 ×10−4 PowerQuant 8 8 0.55 76.15 1.0 ×10−4 uniform 6 6 1 75.07 8.0 ×10−4 logarithmic 6 6 - 75.37 4.6 ×10−4 power (ours) 6 6 0.50 75.95 4.3 ×10−4 uniform 4 4 1 54.68 3.5 ×10−3 logarithmic 4 4 - 57.07 2.1 ×10−3 PowerQuant 4 4 0.55 70.29 1.9 ×10−3 DenseNet 121 Baseline 32 32 - 75.00 - uniform 8 8 1 75.00 2.8 ×10−4 logarithmic 8 8 - 74.91 2.5 ×10−4 PowerQuant 8 8 0.60 75.00 2.2 ×10−4 uniform 6 6 1 74.47 1.1 ×10−3 logarithmic 6 6 - 72.71 1.0 ×10−3 power (ours) 6 6 0.50 74.84 0.7 ×10−3 uniform 4 4 1 54.83 4.7 ×10−3 logarithmic 4 4 - 5.28 4.8 ×10−3 PowerQuant 4 4 0.55 68.04 3.1 ×10−3 quantization systematically achieves signiﬁcantly higher accuracy and lower reconstruction error than the logarithmic and uniform quantization schemes. On a side note, the poor performance of the logarithmic approach on DenseNet 121 can be attributed to the skewness of the weight distributions. Formally, ResNet 50 and DenseNet 121 weight values show similar average standard deviations across layers (0.0246 and 0.0264 respectively) as well as similar kurtosis (6.905 and 6.870 respec- tively). However their skewness are signiﬁcantly different: 0.238 for ResNet 50 and more than twice as much for DenseNet 121, with 0.489. The logarithmic quantization, that focuses on very small value is very sensible to asymmetry which explains the poor performance on DenseNet 121. In contrast, the proposed method offers a robust performance in all situations. F H OW TO PERFORM MATRIX MULTIPLICATION WITH POWER QUANT The proposed PowerQuant method preserves the multiplication operations, i.e. a multiplication in the ﬂoating point space remains a multiplication in the quantized space (integers). This allows one to leverage current implementations of uniform quantization available on most hardware Gholami et al. (2021); Zhou et al. (2016). However, while PowerQuant preserves multiplications it doesn’t preserve additions which are signiﬁcantly less costly than multiplications. Consequently, in order to infer under the PowerQuant transformation, instead of accumulating the quantized products, as done in standard quantization Jacob et al. (2018), one need to accumulate the powers of said products. Formally, let’s consider two quantized weightsw1,w2 and their respective quantized inputs x1,x2. The standard accumulation would be performed as followsw1x1+w2x2. In the case of PowerQuant, this would be done as(w1x1) 1 a +(w2x2) 1 a . Previous studies on quantization have demonstrated that such power functions can be computed with very high ﬁdelity at almost no latency cost Kim et al. (2021). G O VERHEAD COST OF ZERO -POINTS IN ACTIVATION QUANTIZATION The overhead cost introduced in equation 5 is well known in general in quantization as it arises from asymmetric quantization. Nonetheless, we share here (as well as in the article) some empirical values. 17Arxiv version Table 8: Overhead induced by asymmetric quantization Architecture parameters overhead run-time overhead (CPU intel-m3) ResNet50 0.25% 4.35% EfﬁcientNet 0.20% 3.38% ViT b16 0.73% 5.14% Table 9: Comparison between the per-layer and global method of power parameter a ﬁtting on a ResNet 5 `a trained for ImageNet classiﬁcation task. Architecture Method W-bit A-bit Accuracy Reconstruction Error ResNet 50 Baseline 32 32 76.15 - per-layer 8 8 76.14 0.8 ×10−4 global 8 8 76.15 1.0 ×10−4 per-layer 4 4 64,19 1.7 ×10−3 global 4 4 70.29 1.9 ×10−3 These are empirical results from our own implementation. We include ResNet50 as it can also be quantized using asymmetric quantization although in our research, we only applied asymmetric quantization to SilU and GeLU based architectures. We included these results in the appendix of the revised article. It is worth noting that according to LSQ+ Bhalgat et al. (2020), asymmetric quantization can be achieved at virtually not run-time cost. H L IMITATIONS OF THE RECONSTRUCTION ERROR METRIC In the proposed PowerQuant method, we ﬁt the parameter abased on the reconstruction error over all the weights, i.e. over all layers in the whole network. Then, we perform per-channel quantization layer by layer independently. However, if the ﬁnal objective is to minimize the reconstruction error from equation (3), a more efﬁcient approach would consist in ﬁtting the parameter aseparately for each layer. We note a∗ l such that for every layer lwe have a∗ l = arg min a {Wl −Q−1 a (Qa(Wl))  p } (21) Then the network (F,(al)∗) quantized with a per-layer ﬁt of the power parameter will satisfy L∑ l=1 Wl −Q−1 al (Qal (Wl))  p < L∑ l=1 Wl −Q−1 a (Qa(Wl))  p (22) if and only if their exists at least one lsuch that al ̸= a. Consequently, if the reconstruction error was a perfect estimate of the resulting accuracy, the per-layer strategy would offer an even higher accuracy than the proposed PowerQuant method. Unfortunately, the empirical evidence, in table 9, shows that the proposed PowerQuant method achieves better results in every benchmark. This observation demonstrates the limits of the measure of the reconstruction error. We explain this phe- nomenon by the importance of inputs and activations quantization. This can be seen as some form of overﬁtting the parameters al on the weights which leads to poor performance on the activation quantization and prediction. In the general sens, this highlights the limitations of the reconstruction error as a proxy for maximizing the accuracy. Previous results can be interpreted in a similar way. For instance, in SQuant Cong et al. (2022) the author claim that it is better to minimize the abso- lute sum of errors rather than the sum of absolute errors and achieve good performance in data-free quantization. I I MPROVEMENT WITH RESPECT TO QAT In the introduction, we argued that data-driven quantization schemes performance deﬁne an upper- bound on data-free performance. Our goal was to narrow the resulting gap between these methods. In Table 10, we report the evolution in the gap between data-free and data-driven quantization tech- niques. These empirical results validate the signiﬁcant improvement of the proposed method at narrowing the gap between data-free and data-driven quantization methods by 26.66% to 29.74%. 18Arxiv version Table 10: Performance Gap as compared to Data-driven techniques on ResNet 50 quantization in W4/A4. The relative gap improvement to the state-of-the-art SQuant [6], is measured as gs−gp gs with gs = ∗−SQuant ∗ and gp = ∗−PowerQuant ∗ where ∗ is the performance of a data-driven method data-driven method SQuant PowerQuant relative gap OCTA V Sakr et al. (2022) (ICML) 8,72% 6,15% +29,47% SQ van Baalen et al. (2022) (CVPR) 8,64% 6,07% +29,74% WinogradQ Chikin & Kryzhanovskiy (2022) (CVPR) 9,55% 7,00% +26,66% Mr BiQ Jeon et al. (2022) (CVPR) 8,74% 6,17% +29,38% Table 11: Performance gap between data-free PowerQuant and short-retraining OCTA V Sakr et al. (2022). method architecture quantization accuracy PowerQuant ResNet 50 W4/A4 70.53 OCTA V ResNet 50 W4/A4 75.84 PowerQuant MobileNet V2 W4/A4 45.84 OCTA V MobileNet V2 W4/A4 0.66 Table 12: Performance gap between PowerQuant and OCTA V Sakr et al. (2022) (using an additional short retraining), both using dynamic range estimation. method architecture quantization accuracy PowerQuant ResNet 50 W4/A4 76.02 OCTA V ResNet 50 W4/A4 76.46 PowerQuant MobileNet V2 W4/A4 71.65 OCTA V MobileNet V2 W4/A4 71.23 In order to complete our comparison to QAT methods, we considered the short-re-training (30 epochs) regime from OCTA V in Table 11. We can draw two observations from this comparison. First, on ResNet 50, OCTA V achieves remarkable results by reach near full-precision accuracy. Still the proposed method does not fall too far back with only 5.31 points lower accuracy while being data-free. Second, on very small models such as MobileNet V2, using a strong quantization oper- ator rather than a short re-training leads to a huge accuracy improvement as PowerQuant achieves 45.18 points higher accuracy. This is also the ﬁnding of the author in OCTA V , as they conclude that models such as MobileNet tend to be very challenging to quantize using static quantization and short re-training. In Table 12, we draw a comparison between the proposed PowerQuant and the QAT method OCTA V Sakr et al. (2022), both using dynamic quantization (i.e. estimating the ranges of the activations on- the-ﬂy depending on the input). As expected, the use of dynamic ranges has a considerable inﬂuence on the performance of both quantization methods. As can be observed the QAT method OCTA V achieved very impressive results and even outperforming the full-precision model on ResNet 50. Nevertheless, it is on MobileNet that the inﬂuence of dynamic ranges is the most impressive. For OCTA V , we observe a boost of almost 71 points going from almost random predictions to near exact full-precision accuracy. It is to be noted that PowerQuant does not fall shy in front of these perfor- mances, as using static quantization we still manage to preserve some of the predictive capability of the model. Furthermore, using dynamic quantization, Powerquant achieves similar accuracies than OCTA V while not involving any ﬁne-tuning, contrary to OCTA V . All in all, we can conclude that the proposed data-free method manages to hold close results to a state-of-the-art QAT method in some context. An interesting future work could be the extension of PowerQuant as a QAT method and possibly learning the power parameter athat we use in our quantization operator. J C OMPARISON TO STATE-OF-THE-ART DATA-FREE QUANTIZATION ON OTHER CONV NETS In addition to our evaluation on ResNet, we propose some complementary results on DenseNet in Table 13 as well as the challenging and compact architectures MobileNet and EfﬁcientNet in Table 19Arxiv version Table 13: Comparison between state-of-the-art post-training quantization techniques on DenseNet 121 on Im- ageNet. We distinguish methods relying on data (synthetic or real) or not. In addition to being fully data-free. our approach signiﬁcantly outperforms existing methods. Architecture Method Data W-bit A-bit Accuracy gap DenseNet 121 Baseline - 32 32 75.00 - DFQ Nagel et al. (2019) No 8 8 74.75 -0.25 SQuant Cong et al. (2022) No 8 8 74.70 -0.30 OMSE Choukroun et al. (2019) Real 8 8 74.97 -0.03 SPIQ Yvinec et al. (2022b) No 8 8 75.00 -0.00 PowerQuant No 8 8 75.00 -0.00 DFQ Nagel et al. (2019) No 4 4 0.10 -74.90 SQuant Cong et al. (2022) No 4 4 47.14 -27.86 SPIQ Yvinec et al. (2022b) No 4 4 51.83 -23.17 OMSE Choukroun et al. (2019) Real 4 4 57.07 -17.93 PowerQuant No 4 4 69.37 -5.63 Table 14: Complementary Benchmarks on ImageNet Architecture Method Data W-bit A-bit Accuracy gap MobileNet V2 Baseline - 32 32 71.80 - DFQ (ICCV 2019) No 8 8 70.92 -0.88 SQuant (ICLR 2022) No 8 8 71.68 -0.12 SPIQ (W ACV 2023) No 8 8 71.79 -0.01 PowerQuant No 8 8 71.81 +0.01 DFQ (ICCV 2019) No 4 4 27.1 -44.70 SQuant (ICLR 2022) No 4 4 28.21 -43.59 SPIQ (W ACV 2023) No 4 4 31.28 -40.52 PowerQuant No 4 4 45.84 25.96 EfﬁcientNet B0 Baseline - 32 32 77.10 - DFQ (ICCV 2019) No 8 8 76.89 -0.21 SQuant (ICLR 2022) No 8 8 76.93 -0.17 SPIQ (W ACV 2023) No 8 8 77.02 -0.08 PowerQuant No 8 8 77.05 -0.05 DFQ (ICCV 2019) No 6 6 43.08 -34.02 SQuant (ICLR 2022) No 6 6 54.51 -32.59 SPIQ (W ACV 2023) No 6 6 74.67 -2.43 PowerQuant No 6 6 75.13 -1.97 14 as well as weights only for Bert in Table 16. In table 13, we report the performance of other data-free quantization processes on DenseNet 121. The OMSE method (Choukroun et al., 2019) is a post-training quantization method that leverages validation examples during quantization, thus cannot be labelled as data-free. Yet, we include this work in our comparison as they show strong performance in terms of accuracy at a very low usage of real data. As showcased in table 13, the proposed PowerQuant method almost preserves the ﬂoating point accuracy in W8/A8 quantization. Additionally, on the challenging W4/A4 setup, our approach improves the accuracy by a remarkable 12.30 points over OMSE and 17.54 points over SQuant. This is due to the overall better efﬁciency of non-uniform quantization, that allows a theoretically closer ﬁt to the weight distributions of each DNN layer. The results on MobileNet and EfﬁcientNet from Table 14 conﬁrm our previous ﬁndings. We observe a signiﬁcant boost in performance from PowerQuant as compared to the other very competitive data-free solutions. K O VERHEAD COST DISCUSSION In this section, we provide more empirical results on the inference cost of the proposed method. Table 17 shows the inference time of DNNs quantized with our approach (which only implies modi- ﬁcations of the activation function and a bias correction-see Section 3.3). For DenseNet, ResNet and MobileNet V2, the baseline activation function is the ReLU, which is particularly fast to compute. 20Arxiv version Table 15: Complementary Benchmarks on Vision Transformers for ImageNet Architecture Method Data W-bit A-bit Accuracy gap CaiT xxs24 Baseline - 32 32 78.524 - DFQ (ICCV 2019) No 8 8 77.612 -0.912 SQuant (ICLR 2022) No 8 8 77.638 -0.886 PowerQuant No 8 8 77.718 -0.806 DFQ (ICCV 2019) No 4 8 74.192 -4.332 SQuant (ICLR 2022) No 4 8 74.224 -4.300 PowerQuant No 4 8 75.104 -3.420 CaiT xxs36 Baseline - 32 32 79.760 - DFQ (ICCV 2019) No 8 8 79.000 -0.760 SQuant (ICLR 2022) No 8 8 78.914 -0.846 PowerQuant No 8 8 79.150 -0.610 DFQ (ICCV 2019) No 4 8 76.906 -2.854 SQuant (ICLR 2022) No 4 8 76.896 -2.864 PowerQuant No 4 8 77.702 -2.058 CaiT s24 Baseline - 32 32 83.368 - DFQ (ICCV 2019) No 8 8 82.802 -0.566 SQuant (ICLR 2022) No 8 8 82.784 -0.584 PowerQuant No 8 8 82.766 -0.602 DFQ (ICCV 2019) No 4 8 81.474 -1.894 SQuant (ICLR 2022) No 4 8 81.486 -1.882 PowerQuant No 4 8 81.612 -1.756 Table 16: Complementary Benchmarks on the GLUE task. We consider the BERT transformer architecture. We provide the reference performance of BERT on GLUE as well as our reproduced results (baseline). task (reference) baseline uniform log power CoLA 49.23 47.90 46.24 46.98 47.77 SST-2 91.97 92.32 91.28 91.85 92.32 MRPC 89.47/85.29 89.32/85.41 86.49/81.37 86.65/82.86 89.32/85.41 STS-B 83.95/83.70 84.01/83.87 83.25/83.14 84.01/83.81 84.01/83.87 QQP 88.40/84.31 90.77/84.65 90.23/84.61 90.76/84.65 90.77/84.65 MNLI 80.61/81.08 80.54/80.71 79.72/79.13 79.22/79.71 80.54/80.71 QNLI 87.46 91.47 90.32 91.43 91.47 RTE 61.73 61.82 59.23 61.27 61.68 WNLI 45.07 43.76 40.85 42.80 42.85 Nevertheless, our results show that our approach leads to only increasing by1% the whole inference time on most networks. More precisely, in the case of ResNet 50, the change in activation function induces a slowdown of0.15%. The largest runtime increase is obtained on DenseNet with a 3.4% in- crease. Lastly, note that our approach is also particularly fast and efﬁcient on EfﬁcientNet B0, which uses SiLU activation, thanks to the bias correction technique introduced in Section 3.3. Overall, the proposed approach can be easily implemented and induces negligible overhead in inference on GPU. To furthermore justify the practicality of the proposed quantization process, we recall that the only practicality concern that may arise is on the activation function as the other operations are strictly identical to standard uniform quantization. According to Kim et al. (2021) efﬁcient power functions can be implemented for generic hardware as long as they support standard integer arithmetic, i.e. as long as they support uniform quantization. When it comes to Field-Programmable Gate Array (FPGA), activation functions are implemented using look-up tables (LUT) as detailed in Hajduk (2017). More precisely, they are pre-computed using Pad ´e approximation which are quotients of polynomial functions. Consequently the proposed approach would simply change the polynomial values but not the inference time as it would still rely on the same number of LUTs. In general, activation functions that are non-linear can be very effectively implemented in quantiza- tion runtime Lam et al. (2022). However these considerations are hardware agnostic. In order to circumvent this limitation and address any concerns to our best, we conducted a small study using 21Arxiv version Table 17: Inference time, in seconds, over ImageNet using batches of size 16 of several networks on a 2070 RTX GPU. We also report the accuracy for W6/A6 quantization setup. Architecture Method inference time (gap) accuracy ResNet 50 Uniform 164 75.07 Power Function 164 (+0.2) 75.95 DenseNet 121 Uniform 162 72.71 Power Function 167 (+4.8) 74.84 MobileNet V2 Uniform 85 52.20 Power Function 86 (+0.7) 64.09 EfﬁcientNet B0 Uniform 125 58.24 Power Function 127 (+2.2) 66.38 Table 18: Inference cost each component of a convolutional layer and percentage of total in terms of number of cycles on a wide range of simulated hardware using nntool from GreenWaves. operation number of cycles number of ops % of total cycles % of total ops convolution 22950 442368 85.482% 99.310% bias 2033 1024 7.573% 0.229% relu 924 1024 3.442% 0.229% power function 940 1024 3.502% 0.229% Table 19: We report the processing time in seconds (on an Intel(R) Core(TM) i9-9900K CPU) required to quantize a trained neural network such as ResNet, MobileNet, DenseNet or EfﬁcientNet. Architecture GDFQ SQuant Uniform Power MobileNet V2 7.103 134 <1 <1 ResNet 50 11.103 320 <1 1.3 the simulation tool nntool from GreenWaves, a risc-v chips manufacturer that enables to simulate inference cost of quantized neural networks on their gap unit. We tested a single convolutional layer with bias and relu activation plus our power quantization operation and reported the number of cycles and operations. These results demonstrate that even without any optimization the proposed method has a marginal computational cost on MCU inference which corroborates our previous em- pirical results. We would like to put the emphasis on the fact that this cost could be further reduced via optimizing the computation of the power function using existing methods such as Kim et al. (2021). Similarly, we measure the empirical time required to perform the proposed quantization method on several neural networks and report the results in table 19. These results show that the proposed PowerQuant method offers outstanding trade-offs in terms of compression and accuracy at virtually no cost over the processing and inference time as compared to other data-free quantiza- tion methods. For instance, SQuant is a sophisticated method that requires heavy lifting in order to efﬁciently process a neural network. On a CPU, it requires at least 100 times more time to reach a lower accuracy than the proposed method as we will showcase in our comparison to state-of-the-art quantization schemes. 22",
      "meta_data": {
        "arxiv_id": "2301.09858v1",
        "authors": [
          "Edouard Yvinec",
          "Arnaud Dapogny",
          "Matthieu Cord",
          "Kevin Bailly"
        ],
        "published_date": "2023-01-24T08:30:14Z",
        "pdf_url": "https://arxiv.org/pdf/2301.09858v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper identifies the uniformity of the quantization operator as a key limitation of existing data-free methods, leading to accuracy loss. It proposes PowerQuant, a novel data-free, non-uniform quantization method that preserves the nature of mathematical operations (i.e., floating-point multiplications map to integer multiplications). The core contributions include searching for optimal quantization operators among continuous automorphisms of (R*+,×), which are narrowed down to power functions (x→x^a). The optimal exponent 'a' is found by minimizing the reconstruction error of each layer, a problem shown to be locally convex with a unique solution. PowerQuant is demonstrated to significantly outperform existing data-free methods across various configurations and architectures, including CNNs and Transformers, with negligible inference overhead, by requiring only simple modifications in DNN activation functions.",
        "methodology": "PowerQuant's methodology is based on searching for non-uniform quantization operators that preserve the nature of mathematical operations (multiplications). This search space is restricted to continuous automorphisms of (R*+,×), which are precisely power functions of the form x→x^a. The optimal exponent 'a' is determined by minimizing the quantization reconstruction error, defined as the Lp norm (L2 in practice) between the original floating-point weights and their de-quantized counterparts across all network layers. This optimization problem is solved using the Nelder–Mead method, leveraging its locally convex nature and unique minimum. For inference, activations are handled differently: ReLU activations are assumed positive, while for signed activations (e.g., SiLU, GeLU), asymmetric quantization with a zero-point is used to ensure positive values. The power function calculation at inference time is efficiently approximated using Newton's method, ensuring compatibility with integer-only arithmetic and standard hardware.",
        "experimental_setup": "The proposed PowerQuant method was validated on ImageNet classification (approx. 1.2M training images/50k test images) using pre-trained MobileNets, ResNets, EfficientNets, and DenseNets. Transformer architectures, including ViT and DeiT, were also evaluated on ImageNet. For Natural Language Processing (NLP) tasks, a BERT model was evaluated on the GLUE benchmark. The quantization process used Numpy, with activations quantized as unsigned integers and weights using a symmetric representation, and batch-normalization layers were folded. Comparisons were made against various state-of-the-art data-free quantization methods (e.g., uniform, logarithmic, SQuant, DFQ, ZeroQ, DSG, GDFQ, PSAQ) and, for certain benchmarks, against Quantization-Aware Training (QAT) methods like OCTA V. Performance metrics included Top-1 accuracy and reconstruction error. Inference cost and processing time were also measured using ACE metrics, GPU (2070 RTX), and CPU (Intel i9-9900K) runtimes, as well as a simulated hardware environment (nntool from GreenWaves for RISC-V chips).",
        "limitations": "The primary limitation identified is that the proposed weight reconstruction error is not a perfect proxy for maximizing the resulting accuracy. Empirical results showed that a per-layer optimization of the power parameter 'a', which theoretically should yield lower reconstruction error, actually led to lower overall accuracy compared to a globally optimized 'a'. This suggests that focusing solely on weight reconstruction might 'overfit' the parameters to the weights, leading to suboptimal performance in activation quantization and overall prediction accuracy.",
        "future_research_directions": "Future research directions include exploring better proxy errors than the current weight reconstruction error for optimizing the quantization parameters. Another promising avenue is extending the search space for quantization operators to include other internal composition laws of R+ that are compatible with efficient calculus and inference on hardware. Additionally, there is potential to extend PowerQuant into a Quantization-Aware Training (QAT) method, possibly by making the power parameter 'a' learnable during the training process."
      }
    },
    {
      "title": "Outlier-aware Slicing for Post-Training Quantization in Vision Transformer"
    },
    {
      "title": "Bit-Shrinking: Limiting Instantaneous Sharpness for Improving Post-Training Quantization"
    },
    {
      "title": "Towards Accurate Post-training Network Quantization via Bit-Split and Stitching"
    },
    {
      "title": "Bit-Shrinking: Limiting Instantaneous Sharpness for Improving Post-Training Quantization"
    },
    {
      "title": "Accurate Post Training Quantization With Small Calibration Sets"
    },
    {
      "title": "Shifted and Squeezed 8-bit Floating Point format for Low-Precision Training of Deep Neural Networks",
      "abstract": "Training with larger number of parameters while keeping fast iterations is an\nincreasingly adopted strategy and trend for developing better performing Deep\nNeural Network (DNN) models. This necessitates increased memory footprint and\ncomputational requirements for training. Here we introduce a novel methodology\nfor training deep neural networks using 8-bit floating point (FP8) numbers.\nReduced bit precision allows for a larger effective memory and increased\ncomputational speed. We name this method Shifted and Squeezed FP8 (S2FP8). We\nshow that, unlike previous 8-bit precision training methods, the proposed\nmethod works out-of-the-box for representative models: ResNet-50, Transformer\nand NCF. The method can maintain model accuracy without requiring fine-tuning\nloss scaling parameters or keeping certain layers in single precision. We\nintroduce two learnable statistics of the DNN tensors - shifted and squeezed\nfactors that are used to optimally adjust the range of the tensors in 8-bits,\nthus minimizing the loss in information due to quantization.",
      "full_text": "Published as a conference paper at ICLR 2020 SHIFTED AND SQUEEZED 8-BIT FLOATING POINT FOR - MAT FOR LOW-PRECISION TRAINING OF DEEP NEU- RAL NETWORKS L´eopold Cambier1∗†, Anahita Bhiwandiwalla2†, Ting Gong2, Mehran Nekuii2, Oguz H Elibol2 and Hanlin Tang2 1ICME, Stanford University 2Intel AI Lab lcambier@stanford.edu {anahita.bhiwandiwalla,ting.gong}@intel.com {mehran.nekuii,oguz.h.elibol,hanlin.tang}@intel.com ABSTRACT Training with larger number of parameters while keeping fast iterations is an in- creasingly adopted strategy and trend for developing better performing Deep Neu- ral Network (DNN) models. This necessitates increased memory footprint and computational requirements for training. Here we introduce a novel methodology for training deep neural networks using 8-bit ﬂoating point (FP8) numbers. Re- duced bit precision allows for a larger effective memory and increased computa- tional speed. We name this method Shifted and Squeezed FP8 (S2FP8). We show that, unlike previous 8-bit precision training methods, the proposed method works out-of-the-box for representative models: ResNet-50, Transformer and NCF. The method can maintain model accuracy without requiring ﬁne-tuning loss scaling parameters or keeping certain layers in single precision. We introduce two learn- able statistics of the DNN tensors - shifted and squeezed factors that are used to optimally adjust the range of the tensors in 8-bits, thus minimizing the loss in information due to quantization. 1 I NTRODUCTION Deep neural networks have achieved state-of-the-art performance on a wide variety of computer vision, audio, and natural language processing (NLP) tasks. This has resulted in an explosion of in- terest around techniques to reduce the memory footprint and energy consumption of neural network training and inference (Guo, 2018). Although there are a number of methods to address some of these issues for inference, the most effective method for training is using reduced precision numeri- cal formats. While 32-bit ﬂoating point (FP32) is the most common data format for neural network training, recent hardware have leveraged techniques that allow for training with 16-bit data formats (K ¨oster et al., 2017; Micikevicius et al., 2018). However, 8-bit precision training remains an open challenge (Johnson, 2018; Kalamkar et al., 2019). Current FP8 training methodologies (Wang et al., 2018; Mellempudi et al., 2019) require either specialized chunk-based accumulation, stochastic rounding techniques, loss scaling or maintaining some layers of the network in higher precision. Tuning these knobs is non-intuitive and requires signiﬁcant experimentation for each individual network. Accelerating the adoption of 8-bit data in training DNNs requires a hardware-friendly and out-of- the-box implementation of FP8. Due to the reduced number of mantissa bits, 8-bit multipliers are smaller and consume less power compared to higher bit representations. In this work we describe a novel 8-bit ﬂoating point (FP8) format - shifted and squeezed FP8 (S2FP8) - which has the following advantages compared to previously proposed 8-bit training methodologies: ∗Work performed during an internship at Intel †Equal contribution 1 arXiv:2001.05674v1  [cs.LG]  16 Jan 2020Published as a conference paper at ICLR 2020 •S2FP8 eliminates the need for loss scaling, which requires signiﬁcant tuning of the loss scale values and schedule for individual topologies •Leveraged by the forward and backward passes of model training, S2FP8 is effective in adjusting the range of gradients and also of activations and weights •S2FP8 does not require keeping the ﬁrst and last layer in FP32 precision, which is needed for other approaches (Mellempudi et al., 2019), however maintains the master weights and accumulations inside the matrix multipliers in FP32 We demonstrate across image classiﬁcation, translation, and recommendation models that S2FP8 outperforms previous 8-bit approaches, and reaches the accuracy of FP32 models without any addi- tional hyperparameter tuning. 2 R ELATED WORK The success of 32-bit ﬂoating point data type in training deep neural networks has increased interest in the feasibility of even lower precision training. The exponential demand for compute involved in training these deep neural networks has lead to multiple advancements in lower precision data types. Several studies have developed techniques such as loss scaling, stochastic rounding, and others to train effectively in 16-bit (Micikevicius et al., 2018; Das et al., 2018; Azim), along with associated hardware support (Markidis et al., 2018). Using 16-bit ﬁxed point, (Gupta et al., 2015) showed that stochastic rounding techniques were crucial for model convergence even for simple convolutional neural networks. As noted in (Kalamkar et al., 2019), Google’s bﬂoat16 format has the same number of exponent bits as FP32, leading the success of that format without commonly requiring hardware intensive requirements such as stochastic rounding or other framework level techniques such as loss scaling. Although 8-bit formats have signiﬁcant performance and memory advantages, convergence is es- pecially challenging due to loss of accuracy in the backpropogated gradient values. Wang et al. (2018) demonstrated training models with matrix multiplications and convolutions in FP8 but they use FP16 with chunk-based accumulations and stochastic rounding hardware. Mellempudi et al. (2019) also demonstrated success with FP8, accumulating in FP32 and using loss scaling techniques on ResNets, Transformer and GNMT networks. However, they too require the ﬁrst and last layers of the model to be in FP32, and similar to (Banner et al., 2018) leverage Stochastic Rounding tech- niques to maintain model accuracy. Unlike S2FP8 proposed in this work, both of these FP8 training techniques emphasize the need for efﬁcient loss scaling, rounding hardware and restriction on some layers being in higher precision. Zhou et al. (2016) quantized weights, activations and gradients of AlexNet (Krizhevsky et al., 2012) to 1, 2 and 6 bits respectively. But they also need to maintain the ﬁrst and last convolution layers in full precision and stochastically quantize the gradients. Wu et al. (2018) demonstrate using integers for training LeNet-5 (LeCun et al., 1998) and AlexNet with 8-bits for activations, error and gradi- ents and 2-bits for weights. However, these approaches also required custom tuning such as novel initialization techniques and layer wise scaling instead of Batch Normalization and Softmax. These approaches lack generalizability to other models, requiring signiﬁcant ﬁne tuning. To the best of our knowledge, there does not exist an out-of-the-box solution using FP8 in training deep learning topologies without the need for tuned loss scaling techniques, requirements of cer- tain layers being in full precision along with efﬁcient hardware rounding schemes like Stochastic Rounding. 3 S HIFTED AND SQUEEZED 8-BIT FLOATING POINT FORMAT 3.1 C HALLENGES OF 8-BIT FLOATING POINT FORMAT The FP8 format, with 2 bits of mantissa and 5 bits of exponent (Mellempudi et al., 2019) is both nar- row (i.e., its dynamic range is very limited, from 2−16 to 216) and has lower accuracy (the machine epsilon is only 2−3). Figure A1 illustrates the range and accuracy of FP8. In contrast, FP32 ranges from 2−149 to 2128 with a machine-epsilon of 2−24 (Table A1). 2Published as a conference paper at ICLR 2020 Figure 1: The distribution of tensor elements over the course of training for three tensors from the Transformer tiny model on the English-Vietnamese translation dataset. Blue bar indicates the representable range of FP8. Left: Many of the tensor elements fall outside of FP8’s representable range. Center: Few tensor elements fall outside of FP8’s representable range. Right: Initially, most elements are within FP8’s representable range, but after training, many fall outside of the representable range On the other hand, tensors involved in neural networks (weights, activations and gradients) are spread across varying scales. As illustrated in Figure 1, the tensor distributions change over the course of training, spanning different orders of magnitude. As a result, 8-bit training usually requires a combination of multiple techniques to capture the full dynamic range of values for model training. Some of these techniques include: • Loss scaling (Micikevicius et al., 2018) scales the loss L(w) by a constant λbefore back- propagation . This makes the gradients artiﬁcially larger, allowing them to ﬁt within the FP8 range. Gradients are then scaled down before being accumulated into the trainable weights as shown in Equation 6 • Stochastic rounding (Maxﬁeld, 2006) alleviate quantization errors by capturing some of the information discarded when truncating to lower precision at the output of a GEMM operation Between these two techniques, loss scaling is more critical; once the magnitude of the gradients can no longer be represented in the FP8 range, training convergence will not be possible. However, loss scaling only modiﬁes the gradients. Weights and activations can also (albeit admittedly less frequently) exceed the FP8’s representable range of[2−16,216]. In those scenarios, convergence can also be affected. The issue with loss scaling is that it requires user interaction. Models have to be modiﬁed, and, more importantly, tedious empirical tuning is required to ﬁnd the correct loss scaling schedule. While some networks can be trained with constant loss scaling, some, notably Transformers (Mellempudi et al., 2019), require dynamic “back-off” and improved loss scaling. This requires signiﬁcant trial and error to tune the scaling schedule, slowing down wide adoption of low-precision numerical formats. 3.2 S HIFTED AND SQUEEZED FP8 To alleviate these issues and make neural network training possible with no model modiﬁcations or hyperparameter tuning, we propose a new 8-bit ﬂoating point format. Consider a tensor X of size N, i.e., X = {Xi}N i=1. Instead of directly encoding each Xi in FP8, we store X using N FP8 numbers {Yi}N i=1 accompanied by two (squeeze and shift) factors αand β (the “statistics” — see Figure 2). Figure 2: The S2FP8 format. A tensor Xof N numbers is represented by α, βand N FP8 numbers Y, related to X through Equation 1. 3Published as a conference paper at ICLR 2020 -16 0 16 log2 |Y| (a) Y, the usual FP8 distribution. 0 32 log2 |X| (b) X, for α= 1and β <0 -32 0 32 log2 |X| (c) X, for α< 1 and β = 0 Figure 3: Impact of the Shifted and Squeezed transformation log2 |Y|= αlog2 |X|+ β. αlet the distribution be as wide as necessary (though, with an associated loss of precision), and βlet us shift the distribution around any value. For Xi ̸= 0, X and Y are then related through log2(|Yi|) = αlog2(|Xi|) + β ⇔Yi = ±2β|Xi|α (1) where the ±is chosen so that Xi and Yi have the same sign. This representation allows for αand βbe chosen so that together with tensor Y they capture most of the dynamic range of the tensor X. As we will see in section 4, this is all that is necessary to train networks using 8-bit ﬂoating point numbers. In order for Y to be a tensor suitable to be represented by FP8 numbers, we enforce that it has zero mean and a maximum value within the dynamic range of FP8 (e.g. 15): N′ ∑ i=1 log2(|Yi|) = 0 and max i=1,...,N′ log2(|Yi|) = 15(= log2(215)) (2) where the ′notation indicates that the sum and the max, respectively, ignore any isuch that Yi = 0. Those equations ensure that log2(|Y|) values are distributed with zero mean and each is less than 15, which is ideal for an FP8 format. By inserting Equation 2 into Equation 1, and by denoting µ= N′ ∑ i=1 log2(|Xi|) and m= max i log2(|Xi|) (3) we ﬁnd α= 15 m−µ, β = −αµ (4) This new tensor format results in the training procedure (forward pass, backward pass, weight up- date) described in Figure 4. Forward and backward MatMul use this new S2FP8 format. Master weights are kept in FP32 and updated using S2FP8 gradients. Accumulations inside the GEMM kernel are kept in full FP32 precision. Figure 3 illustrates the impact of αand β. By having those two extra degrees of freedom for each tensor, majority of the dynamic range of each tensor can now be captured, whether very small ( β >0), very large ( β <1), very narrow ( α >1)) or very wide (α< 1). 3.3 L EARNING THE TENSOR DISTRIBUTION One way to interpret αand βis to consider them as parameters of a distribution generating the ten- sor values log2(|Xi|). We can then say that, by continuously computing αand β, we are effectively learning the distribution of log2(|Xi|). Figure 5c shows the evolution of µ, m, αand βfor a partic- ular tensor of ResNet-20. We see that αand β converge to, approximately, 5 and 21, respectively. From Equation 1, we conclude that: 4Published as a conference paper at ICLR 2020 FP32àS2FP8 T T T Master weights layer ℓ (FP32) Weights  gradients layer ℓ (S2FP8) Loss gradients layer ℓ (S2FP8) Activations  layer ℓ (S2FP8 ) Activations layer ℓ+1 (S2FP8) Loss gradients layer ℓ+1 (S2FP8) ⨉Σ ⨉Σ Σ⨉ T Update FWD GEMM WG GEMM FP32àS2FP8 FP32àS2FP8 FP32àS2FP8FP32 FP32 FP32 BWD GEMM Figure 4: Low precision training with S2FP8. T represent the truncation described in Equation 5, from FP32 to S2FP8. When using S2FP8 for training, forward and backward GEMM’s only use S2FP8. The master weights are kept in FP32 and updated during the update step. • since α> 1, this means that X is expanded into Y, i.e., X is more narrow than what FP8 allows •since β >0, this means that X is right-shifted into Y, i.e., X is smaller than what FP8 allows At convergence, thoseαand βvalues represent the distribution of each converged tensor. Notice that all statistics stabilize in the last third of the training, where the learning rate is decreased, indicating the network is converging to its ﬁnal state. 4 E XPERIMENTAL RESULTS In this section, we compare S2FP8 training with baseline FP32 and FP8 training with and with- out loss scaling for: Residual Networks (He et al., 2016) of varying depths on the CIFAR-10 and ImageNet (Deng et al., 2009) datasets, Transformer (Vaswani et al., 2017) on IWSLT’15 English- Vietnamese dataset (Luong & Manning, 2015), and Neural Collaborative Filtering (NCF) (He et al., 2017) on MovieLens 1 Million dataset (Harper & Konstan, 2016). For our experiments, we use the open source Tensorﬂow Models 1 repository for ResNet and NCF, Tensor2Tensor (Vaswani et al., 2018) for Transformer with added S2FP8 data type simulation sup- port using the methodology described in subsection 4.1. For a given model, we keep the hyperpa- rameters consistent across FP32, FP8 and S2FP8 evaluations. 4.1 S IMULATION METHODOLOGY We simulated S2FP8 by inserting appropriate truncation function throughout the network, before and after every convolution and matrix-matrix product operations, during both the forward and backward passes. The rest of the network is kept in FP32, and those truncation simulate the low-precision training described in subsection 3.2. The truncation function takes as input a tensor X, computes its magnitude mean and maximum, computes the appropriate αand βand ﬁnally truncates X by computing Xtruncated = [ 2−β{ truncateFP8 ( 2β|X|α)}]1/α (5) where truncateFP8 is a usual FP8 truncation function with RNE (round-to-nearest, with ties broken by rounding to even) rounding which is easier to implement and most widely supported in hardware. 1https://github.com/tensorflow/models 5Published as a conference paper at ICLR 2020 (a) Distribution of the magnitude log2(|X|) of original tensor Xbefore scaling using αand β (b) Distribution of the magnitude log2(|Y|) of shifted and squeezed tensor Y with |Yi| = 2β|Xi|α 0 50k 100k −4.6 −4.4 −4.2 −4 −3.8 Step µ 0 50k 100k −3 −2 −1 Step m 0 50k 100k 4 6 8 Step α 0 50k 100k 20 30 40 Step β (c) The computed statistics during training for the scale (β), shift (α), as well as the mean of the log values (µ) and the maximum log value (m). Figure 5: Evolution of the average and maximum magnitude, as well asαand βfor CIFAR-10 with ResNet-20. This illustrates how the network is actually implicitly learning the tensors distribution, by repeatedly computing magnitudes αand βthrough µand m. 4.2 R ESIDUAL NETWORKS We ﬁrst present results with Residual Networks of varying depths on the CIFAR-10 image recogni- tion dataset. We trained the model on 1 GPU using standard parameters: 250 epochs, batchsize of 128, SGD with momentum of 0.9, initial learning rate of 0.1 decreased by a factor of 10 after epochs 100, 150 and 200. Table 1 and Figure A2 presents the results. We observe that S2FP8 reaches almost exactly the FP32 baseline, sometimes even improving over it. Out-of-the-box FP8 does not converge and has very poor accuracy. Finally, FP8 with constant loss scaling of 100 (FP8+LS(100)) can reach the baseline. Both S2FP8 and FP8+LS(100) have similar performances, but S2FP8 can do so without any extra hyperparameters or tuning from the user’s perspective. CIFAR-10 FP32 S2FP8 ∆ FP8 FP8+LS(100) ResNet-20 91.5 91.1 0.4 17.9 91.1 ResNet-34 92.5 92.0 0.5 13.5 92.0 ResNet-50 93.0 93.2 -0.2 11.5 92.9 Table 1: Validation accuracy (in %) for image recognition on CIFAR-10 with ResNet-20/34/50. We also evaluate S2FP8 on the 1000 class ImageNet dataset. Here, we trained the network on 4 GPUs using standard parameters: 90 epochs, batchsize of 256, SGD with momentum of 0.9, initial learning rate of 0.1 decreased by a factor of 10 after epochs 30, 60, 80 and 90. Table 2 and Figure 6 present the results. Again, we observe that S2FP8 gets very close to the FP32 baseline. Out-of-the-box FP8 quickly diverges and does not converge at all. For FP8 with loss scaling to converge, one has to not truncate the ﬁrst and last layer, as consistent with (Mellempudi et al., 2019), which we denote as Ex in Table 2 below. A loss scaling of 10,000 can then be used to reach the baseline (FP8+LS(10k)+Ex). Finally, stochastic rounding can be added and it slightly improves the precision (FP8+LS(100k)+Ex+SR). However, both those cases are not out-of-the-box, as they require loss scaling tuning and some layers 6Published as a conference paper at ICLR 2020 to be kept in full precision. S2FP8 does not suffer from that, thanks to its improved quantization: all layers can be truncated and no loss scaling is required. Imagenet1k FP32 S2FP8 ∆ FP8 FP8+LS(10k)+Ex FP8+LS(100k)+Ex+SR ResNet-18 70.3 69.6 -0.7 NaN 68.7 68.9 ResNet-50 76.2 75.2 -1.0 NaN 75.3 75.5 Table 2: Validation accuracy (in %) for image recognition on Imagenet1k with ResNet-18/50 0 250k 500k 20 40 60 80 Step Top-1 accuracy (%) FP32 S2FP8 0 250k 500k 2 4 6 8 Step Loss FP32 S2FP8 0 250k 500k 0.4 0.6 0.8 1 1.2 Step L2 Loss FP32 S2FP8 Figure 6: Comparing Top-1 accuracy and Loss of S2FP8 with FP32 for ResNet-50 on Imagenet1k 4.3 T RANSFORMER We also tested S2FP8 on a small Transformer (Transformer Tiny) on the English-Vietnamese dataset. The model has 2 hidden layers of size 128, and a ﬁlter of size 512, and is trained using Adam optimizer (Kingma & Ba, 2014). Table 3 and Figure 7 show the result, where we compare FP32, S2FP8 and FP8 with exponential loss scaling. We tried many loss scaling schedules (constant and exponential, with various initializations) and report the best result. As one can see, S2FP8 reaches the baseline with no hyperparameter tuning. FP8, on the other hand, does not, even after extensive loss scaling tuning. This shows the value of an out-of-the-box method for the user. En-Vi FP32 S2FP8 ∆ FP8 FP8+LS(exp) Transformer tiny 25.3 25.3 0.0 NaN 21.3 Table 3: BLEU Score (Papineni et al., 2002) (from 0 to 100) for translation task on the English- Vietnamese dataset with Transformer tiny. 4.4 N EURAL COLLABORATIVE FILTERING The Neural Collaborative Filtering (NCF) network comprises of embeddings for users and items from the MovieLens dataset, that are then passed to a Multi-Layer Perceptron(MLP) network to learn the user-item interaction. Matrix-multiplication operations are the building blocks of such models. We compare S2FP8 with FP32 and FP8 without loss scaling. We simulate Matrix-Multiplications and look-ups from the embeddings in S2FP8 and compare it to FP8 with RNE. We trained the model on the MovieLens 1 Million dataset with the following standard paramaters: 20 iterations, batchsize of 1024 on 4 GPUs, 8 predictive factors, learning rate of 0.0005 using the Adam optimizer. Figure 8 and Table 4 show the result, where we compare FP32, S2FP8 and FP8 without loss scaling. This again shows that S2FP8 easily reaches the baseline out-of-the-box, without tuning of any sort. FP8 gets relatively close, but cannot reach the baseline. 7Published as a conference paper at ICLR 2020 0 125k 250k 5 10 15 20 25 Step BLEU Score FP32 S2FP8 0 125k 250k 2 4 6 Step Loss FP32 S2FP8 Figure 7: Comparing BLEU score and Loss of S2FP8 and FP32 for Transformer tiny on En-Vi dataset 1 10 20 0.5 0.55 0.6 0.65 Iteration Hit Ratio FP32 S2FP8 1 10 20 0.3 0.35 0.4 Iteration NDCG FP32 S2FP8 1 10 20 0.2 0.25 0.3 Iteration Loss FP32 S2FP8 Figure 8: Comparing Hit Ratio, NDCG and Loss of S2FP8 and FP32 for NCF on MovieLens-1M 5 H ARDWARE ASPECTS S2FP8 is a new data type and requires its own circuitry to be implemented in a tensor processing en- gine. However, the added overhead is very minimal and affects neither data throughput nor compute speed. In order to convert FP32 tensors into S2FP8, two hardware (HW) components are needed. One is to calculate each tensor’s statistics (Equation 3), which bring minimal HW complexity. To make compute operations even easier these statistics could be stored in lower precision such as FP8/INT8. The other component is to adjust the exponent and mantissa of all those tensor elements by applying the squeeze ( α) and shift ( β) factors in Equation 4 before truncating them into their 8-bit placeholders. The shift could be done using simple element-wise add/subtract operations on the exponents, and element-wise squeeze could be applied to the mantissa portions. Another con- sideration is within the tensor processing engine(e.g., GEMM engine) which requires the αand β factors while doing the calculations. The FP32 result will be converted back to S2FP8 when needed (e.g., to store back in memory) as shown in Figure 4. 6 C ONCLUSION We introduce a novel 8-bit ﬂoating point data type (S2FP8), that gives competitive performance in comparison to state-of-the-art FP32 baselines over a range of representative networks. S2FP8 makes use of shifted and squeezed factors to shift and rescale the range of tensors prior to truncation. S2FP8 allows training of neural networks with an 8-bit format while eliminating the need for loss scaling tuning, hardware-complex rounding techniques. In addition, compared to existing FP8 implemen- tations we also eliminate the restriction of maintaining the ﬁrst and last layers in FP32. Decreasing Movielens 1 million FP32 S2FP8 ∆ FP8 NCF 0.666 0.663 0.003 0.633 Table 4: HR Score for NCF on the Movielens 1 million dataset. 8Published as a conference paper at ICLR 2020 the number of bits enables larger models to ﬁt on a single device and results in faster training. As part of future work, we plan to extend the use of S2FP8 to train additional DNN topologies and also simplify the squeeze and shift statistics from a hardware implementation point of view. We also plan to explore the use of reduced precision to store the statistics and the extendability of this ap- proach to efﬁciently represent a broader suite of low precision formats like 8-bit POSIT (Gustafson & Yonemoto, 2017), 4-bit ﬂoating and integer data types. ACKNOWLEDGMENTS We would like to thank Naveen Mellempudi, Pratap Prasad, Prasanna Singamsetty and Cory Stephenson for insightful discussions. REFERENCES Anwarul Azim. Low precision arithmetic operations in deep neural networks: An overview. Ron Banner, Itay Hubara, Elad Hoffer, and Daniel Soudry. Scalable methods for 8-bit training of neural networks. In Advances in Neural Information Processing Systems, pp. 5145–5153, 2018. Dipankar Das, Naveen Mellempudi, Dheevatsa Mudigere, Dhiraj Kalamkar, Sasikanth Avancha, Kunal Banerjee, Srinivas Sridharan, Karthik Vaidyanathan, Bharat Kaul, Evangelos Georganas, et al. Mixed precision training of convolutional neural networks using integer operations. arXiv preprint arXiv:1802.00930, 2018. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi- erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009. Yunhui Guo. A survey on methods and theories of quantized neural networks. CoRR, abs/1808.04752, 2018. URL http://arxiv.org/abs/1808.04752. Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited numerical precision. In International Conference on Machine Learning, pp. 1737–1746, 2015. John L Gustafson and Isaac T Yonemoto. Beating ﬂoating point at its own game: Posit arithmetic. Supercomputing Frontiers and Innovations, 4(2):71–86, 2017. F Maxwell Harper and Joseph A Konstan. The movielens datasets: History and context. Acm transactions on interactive intelligent systems (tiis), 5(4):19, 2016. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European conference on computer vision, pp. 630–645. Springer, 2016. Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. Neural col- laborative ﬁltering. In Proceedings of the 26th international conference on world wide web, pp. 173–182. International World Wide Web Conferences Steering Committee, 2017. Jeff Johnson. Rethinking ﬂoating point for deep learning. CoRR, abs/1811.01721, 2018. URL http://arxiv.org/abs/1811.01721. Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja V ooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen, et al. A study of bﬂoat16 for deep learning training. arXiv preprint arXiv:1905.12322, 2019. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Urs K ¨oster, Tristan Webb, Xin Wang, Marcel Nassar, Arjun K Bansal, William Constable, Oguz Elibol, Scott Gray, Stewart Hall, Luke Hornof, et al. Flexpoint: An adaptive numerical format for efﬁcient training of deep neural networks. In Advances in neural information processing systems, pp. 1742–1752, 2017. 9Published as a conference paper at ICLR 2020 Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convo- lutional neural networks. In Advances in neural information processing systems, pp. 1097–1105, 2012. Yann LeCun, L´eon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. Minh-Thang Luong and Christopher D. Manning. Stanford neural machine translation systems for spoken language domain. In International Workshop on Spoken Language Translation, Da Nang, Vietnam, 2015. Stefano Markidis, Steven Wei Der Chien, Erwin Laure, Ivy Bo Peng, and Jeffrey S Vetter. Nvidia tensor core programmability, performance & precision. In 2018 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW), pp. 522–531. IEEE, 2018. Clive Maxﬁeld. An introduction to different rounding algorithms. Programmable Logic Design Line, pp. 1–15, 2006. Naveen Mellempudi, Sudarshan Srinivasan, Dipankar Das, and Bharat Kaul. Mixed precision train- ing with 8-bit ﬂoating point. arXiv preprint arXiv:1905.12334, 2019. Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=r1gs9JgRZ. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pp. 311–318, Stroudsburg, PA, USA, 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL https://doi.org/10. 3115/1073083.1073135. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pp. 5998–6008, 2017. Ashish Vaswani, Samy Bengio, Eugene Brevdo, Francois Chollet, Aidan N. Gomez, Stephan Gouws, Llion Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki Parmar, Ryan Sepassi, Noam Shazeer, and Jakob Uszkoreit. Tensor2tensor for neural machine translation. CoRR, abs/1803.07416, 2018. URL http://arxiv.org/abs/1803.07416. Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan. Train- ing deep neural networks with 8-bit ﬂoating point numbers. In Advances in neural information processing systems, pp. 7675–7684, 2018. Shuang Wu, Guoqi Li, Feng Chen, and Luping Shi. Training and inference with integers in deep neural networks. arXiv preprint arXiv:1802.04680, 2018. Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Train- ing low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016. 10Published as a conference paper at ICLR 2020 A A PPENDIX A.1 SUPPLEMENTARY TABLES AND FIGURES Format Bits s/e/m Min sub- normal Min nor- mal (Approx.) Max normal Machine epsilon Range IEEE-FP32 32 1/8/23 2−149 2−126 2128 2−24 2277 IEEE-FP16 16 1/5/10 2−24 2−14 216 2−11 240 BF16 16 1/8/7 2−133 2−126 2128 2−8 2261 FP8 8 1/5/2 2−16 2−14 216 2−3 232 Table A1: Comparing several ﬂoating point formats. s/e/m indicates the number of sign (s), exponent (e) and mantissa (m) bits. Models Datasets FP32 BF16 FP8 FP8+other recipes S2FP8 ResNet-20 CIFAR-10 91.5 91.7 17.9 91.1(Loss Scale=100) 91.1 ResNet-50 CIFAR-10 93.0 93.2 11.5 92.9(Loss Scale=100) 93.2 ResNet-50 ImageNet 76.2 76.5 NaN 75.3(Loss Scale=10K, FP32 for ﬁrst and last layers) 75.2 NCF MovieLens1M 0.666 0.653 0.633 - 0.663 Transformer- tiny En-Vi 25.3 25.6 NaN 21.3(Loss Scale=Exp) 25.3 Table A2: Comparing FP32, BF16, vanilla FP8, FP8 with tuning and S2FP8 on the model ResNet(Top1-accuracy), NCF(Hit Ratio),Transformer-tiny(BLEU score). −16 −8 0 8 16 1 2 3 4 log2(|X|) Numbers density Figure A1: The range and precision of FP8. Bar indicate the number density between each power of 2. Since FP8 has 2 mantissa bit, the density is 4 (except in the denormals), and the associated machine epsilon is 2−3 = 1/8. The normal representable range goes from 2−14 to (1 −2−3)216, with denormals from 2−16 to 2−14. A.2 S UPPLEMENTARY EQUATIONS ∂(λL) ∂w (w) = λ∂L ∂w(w) ⇒w(k+1) = w(k) −α1 λ ∂(λL) ∂w (w(k)). (6) 11Published as a conference paper at ICLR 2020 0 50k 100k 60 80 100 Step Top-1 accuracy (%) FP32 S2FP8 0 50k 100k 0 1 2 3 Step Loss FP32 S2FP8 0 50k 100k 0.2 0.3 Step L2 Loss FP32 S2FP8 Figure A2: Convergence of ResNet-50 with the CIFAR-10 dataset 12",
      "meta_data": {
        "arxiv_id": "2001.05674v1",
        "authors": [
          "Léopold Cambier",
          "Anahita Bhiwandiwalla",
          "Ting Gong",
          "Mehran Nekuii",
          "Oguz H Elibol",
          "Hanlin Tang"
        ],
        "published_date": "2020-01-16T06:38:27Z",
        "pdf_url": "https://arxiv.org/pdf/2001.05674v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Shifted and Squeezed FP8 (S2FP8), a novel 8-bit floating point format for training deep neural networks. It addresses the challenges of low-precision training, such as loss of accuracy due to limited dynamic range and the need for extensive tuning of techniques like loss scaling or maintaining certain layers in higher precision. S2FP8 achieves state-of-the-art FP32 model accuracy without requiring loss scaling, stochastic rounding, or keeping specific layers in FP32. It uses two learnable tensor statistics (shifted and squeezed factors, alpha and beta) to optimally adjust the range of tensors in 8-bits, thereby minimizing information loss due to quantization.",
        "methodology": "The S2FP8 methodology represents a tensor X using N 8-bit FP numbers {Yi} and two factors, alpha (squeeze) and beta (shift). These factors are dynamically computed for each tensor during training based on its magnitude's mean (µ) and maximum (m) of log2(|Xi|) values: alpha = 15 / (m - µ) and beta = -alpha * µ. This ensures that the transformed tensor Y has zero mean and a maximum value within FP8's dynamic range. The training procedure involves using S2FP8 for forward and backward matrix multiplications (GEMMs), while master weights are maintained in FP32 and updated using S2FP8 gradients. Accumulations within the GEMM kernels are also kept in full FP32 precision. The method was simulated by inserting a truncation function before and after every convolution and matrix-matrix product, which calculates alpha and beta and then truncates X using `Xtruncated = [ 2−beta{ truncateFP8 ( 2beta|X|alpha)}]1/alpha` with round-to-nearest-even (RNE) rounding.",
        "experimental_setup": "The S2FP8 method was evaluated against FP32 baselines, vanilla FP8, and FP8 with various tuning techniques (loss scaling, FP32 layers, stochastic rounding). Experiments were conducted on: 1) Residual Networks (ResNet-18, ResNet-20, ResNet-34, ResNet-50) for image classification on CIFAR-10 and ImageNet datasets. 2) Transformer Tiny for English-Vietnamese translation on the IWSLT’15 dataset. 3) Neural Collaborative Filtering (NCF) on the MovieLens 1 Million dataset. For ResNets on CIFAR-10, models were trained on 1 GPU for 250 epochs with a batch size of 128 using SGD. For ResNets on ImageNet, 4 GPUs were used for 90 epochs with a batch size of 256 using SGD. NCF was trained for 20 iterations with a batch size of 1024 on 4 GPUs using the Adam optimizer. Hyperparameters were kept consistent across different precision evaluations for each model. The experiments used open-source TensorFlow Models and Tensor2Tensor with S2FP8 data type simulation support.",
        "limitations": "S2FP8 requires dedicated circuitry for hardware implementation in a tensor processing engine, although the authors state the added overhead is minimal and does not affect data throughput or compute speed. The conversion process from FP32 to S2FP8 involves calculating tensor statistics and adjusting exponents/mantissas, which requires specific hardware components. The current experimental results are based on simulation, and while hardware implications are discussed, full hardware-level validation is beyond the scope of this paper.",
        "future_research_directions": "Future work includes extending the use of S2FP8 to train additional Deep Neural Network (DNN) topologies. The authors also plan to simplify the squeeze and shift statistics from a hardware implementation perspective. Exploring the use of reduced precision (e.g., FP8/INT8) to store the statistics themselves is another direction. Additionally, the research aims to investigate the extendability of this approach to efficiently represent a broader suite of low-precision formats, such as 8-bit POSIT, 4-bit floating point, and integer data types."
      }
    },
    {
      "title": "BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction",
      "abstract": "We study the challenging task of neural network quantization without\nend-to-end retraining, called Post-training Quantization (PTQ). PTQ usually\nrequires a small subset of training data but produces less powerful quantized\nmodels than Quantization-Aware Training (QAT). In this work, we propose a novel\nPTQ framework, dubbed BRECQ, which pushes the limits of bitwidth in PTQ down to\nINT2 for the first time. BRECQ leverages the basic building blocks in neural\nnetworks and reconstructs them one-by-one. In a comprehensive theoretical study\nof the second-order error, we show that BRECQ achieves a good balance between\ncross-layer dependency and generalization error. To further employ the power of\nquantization, the mixed precision technique is incorporated in our framework by\napproximating the inter-layer and intra-layer sensitivity. Extensive\nexperiments on various handcrafted and searched neural architectures are\nconducted for both image classification and object detection tasks. And for the\nfirst time we prove that, without bells and whistles, PTQ can attain 4-bit\nResNet and MobileNetV2 comparable with QAT and enjoy 240 times faster\nproduction of quantized models. Codes are available at\nhttps://github.com/yhhhli/BRECQ.",
      "full_text": "Published as a conference paper at ICLR 2021 BRECQ : PUSHING THE LIMIT OF POST -TRAINING QUANTIZATION BY BLOCK RECONSTRUCTION Yuhang Li12∗, Ruihao Gong2∗, Xu Tan2, Yang Yang2, Peng Hu2, Qi Zhang2, Fengwei Yu2, Wei Wang, Shi Gu1\u0000 1University of Electronic Science and Technology of China, 2SenseTime Research liyuhang699@gmail.com, gongruihao@sensetime.com, gus@uestc.edu.cn ABSTRACT We study the challenging task of neural network quantization without end-to- end retraining, called Post-training Quantization (PTQ). PTQ usually requires a small subset of training data but produces less powerful quantized models than Quantization-Aware Training (QAT). In this work, we propose a novel PTQ frame- work, dubbed BRECQ , which pushes the limits of bitwidth in PTQ down to INT2 for the ﬁrst time. B RECQ leverages the basic building blocks in neural networks and reconstructs them one-by-one. In a comprehensive theoretical study of the second-order error, we show that BRECQ achieves a good balance between cross- layer dependency and generalization error. To further employ the power of quan- tization, the mixed precision technique is incorporated in our framework by ap- proximating the inter-layer and intra-layer sensitivity. Extensive experiments on various handcrafted and searched neural architectures are conducted for both im- age classiﬁcation and object detection tasks. And for the ﬁrst time we prove that, without bells and whistles, PTQ can attain 4-bit ResNet and MobileNetV2 com- parable with QAT and enjoy 240×faster production of quantized models. Codes are available at https://github.com/yhhhli/BRECQ. 1 I NTRODUCTION The past decade has witnessed the rapid development of deep learning in many tasks, such as com- puter vision, autonomous driving, etc. However, the issue of huge computation cost and memory footprint requirements in deep learning has received considerable attention. Some works such as neural architecture search (Zoph & Le, 2016) try to design and search a tiny network, while oth- ers, like quantization (Hubara et al., 2017), and network pruning (Han et al., 2015) are designed to compress and accelerate off-the-shelf well-trained redundant networks. Many popular quantization and network pruning methods follow a simple pipeline: training the original model and then ﬁnetune the quantized/pruned model. However, this pipeline requires a full training dataset and many computation resources to perform end-to-end backpropagation, which will greatly delay the production cycle of compressed models. Besides, not all training data are always ready-to-use considering the privacy problem. Therefore, there is more demand in industry for quan- tizing the neural networks without retraining, which is called Post-training Quantization. Although PTQ is fast and light, it suffers from severe accuracy degeneration when the quantization precision is low. For example, DFQ (Nagel et al., 2019) can quantize ResNet-18 to 8-bit without accuracy loss (69.7% top-1 accuracy) but in 4-bit quantization, it can only achieve 39% top-1 accuracy. The primary reason is the approximation in the parameter space is not equivalent to the approximation in model space thus we cannot assure the optimal minimization on the ﬁnal task loss. Recent works like (Nagel et al., 2020) recognized the problem and analyzed the loss degradation by Taylor series expansion. Analysis of the second-order error term indicates we can reconstruct each layer output to approximate the task loss degeneration. However, their work cannot further quantize the weights into INT2 because the cross-layer dependency in the Hessian matrix cannot be ignored when the perturbation on weight is not small enough. In this work, we analyze the second-order ∗Equal contribution, \u0000 Corresponding author. 1 arXiv:2102.05426v2  [cs.LG]  25 Jul 2021Published as a conference paper at ICLR 2021 error based on the Gauss-Newton matrix. We show that the second-order error can be transformed into network ﬁnal outputs but suffer from bad generalization. To achieve the best tradeoff, we adopt an intermediate choice, block reconstruction. In addition, our contributions are threefold: 1. Based on the second-order analysis, we deﬁne a set of reconstruction units and show that block reconstruction is the best choice with the support from theoretical and empirical evidence. We also use Fisher Information Matrix to assign each pre-activation with an importance measure during reconstruction. 2. We incorporate genetic algorithm and the well-deﬁned intra-block sensitivity measure to generate latency and size guaranteed mixed precision quantized neural networks, which fulﬁlls a general improvement on both specialized hardware (FPGA) and general hardware (ARM CPU). 3. We conduct extensive experiments to verify our proposed methods. We ﬁnd that our method is applicable to a large variety of tasks and models. Moreover, we show that post-training quanti- zation can quantize weights to INT2 without signiﬁcant accuracy loss for the ﬁrst time. 2 P RELIMINARIES Notations Vectors are denoted by small bold letters and matrices (or tensors) are denoted by capital bold letters. For instance, W and w represent the weight tensor and its ﬂattened version. Bar accent denotes the expectation over data points, e.g.¯a. Bracketed superscript w(ℓ) indicates the layer index. For a convolutional or a fully-connected layer, we mark its input and output vectors byx and z. Thus given a feedforward neural network with nlayers, we can denote the forward process by x(ℓ+1) = h(z(ℓ)) = h(W(ℓ)x(ℓ) + b(ℓ)), 1 ≤ℓ≤n, (1) where h(·) indicates the activation function (ReLU in this paper). For simplicity, we omit the anal- ysis of bias b(ℓ) as it can be merged into activation. ||·||F denotes the Frobenius norm. Quantization Background Uniform symmetric quantization maps the ﬂoating-point numbers to several ﬁxed-points. These points (or grids) have the same interval and are symmetrically dis- tributed. We denote the set that contains these grids asQu,sym b = s×{−2b−1,..., 0,..., 2b−1 −1}. Here, sis the step size between two grids and bis the bit-width. Quantization function, denoted by q(·) : R→Q u,sym b , is generally designed to minimize the quantization error: min ||ˆw −w||2 F.s.t. ˆw ∈Qu,sym b (2) Solving this minimization problem, one can easily get theq(·) by leveraging the rounding-to-nearest operation ⌊·⌉. Rounding-to-nearest is a prevalent method to perform quantization, e.g. PACT (Choi et al., 2018). However, recently some empirical or theoretical evidence supports that simply mini- mizing the quantization error in parameter space does not bring optimal task performances. Specif- ically, Esser et al. (2020) propose to learn the step size sby gradient descent in quantization-aware training (QAT). LAPQ (Nahshan et al., 2019) ﬁnds the optimal step size when the loss function is minimized without re-training the weights. Their motivations are all towards minimizing a ﬁnal objective, which is the task loss, i.e., min E[L( ˆw)], s.t. ˆw ∈Qu,sym b . (3) While this optimization objective is simple and can be well-optimized in QAT scenarios, it is not easy to learn the quantized weight without end-to-end ﬁnetuning as well as sufﬁcient training data and computing resources. In post-training quantization settings, we only have full precision weights that w⋆ = arg minw E[L(w)] where w ∈R and a small subset of training data to do calibration. Taylor ExpansionIt turns out that the quantization imposed on weights could be viewed as a special case of weight perturbation. To quantitatively analyze the loss degradation caused by quantization, Nagel et al. (2020) use Taylor series expansions and approximates the loss degradation by E[L(w + ∆w)] −E[L(w)] ≈∆wT¯g(w) + 1 2∆wT ¯H(w)∆w, (4) where ¯g(w) = E[∇wL] and ¯H(w) = E[∇2 wL] are the gradients and the Hessian matrix and ∆w is the weight perturbation. Given the pre-trained model is converged to a minimum, the gradients can be safely thought to be close to0. However, optimizing with the large-scale full Hessian is memory- infeasible on many devices as the full Hessian requires terabytes of memory space. To tackle this problem, they make two assumptions: 2Published as a conference paper at ICLR 2021 1. Layers are mutual-independent, thus the Hessian is in the form of layer-diagonal1 and Kronecker- factored, i.e., ¯H(w(ℓ)) = E[x(ℓ)x(ℓ),T ⊗H(z(ℓ))], where ⊗is the Kronecker product. 2. The second-order derivatives of pre-activations are constant diagonal matrix (H(z(ℓ)) = c×I) which is independent of input data points. At last, the objective is transformed into a practical proxy signal, the change in feature-maps (z = Wx), and the quantized model can be obtained by a layer-by-layer feature map reconstruc- tion algorithm (with few calibration images). Recent works, like Bit-Split (Wang et al., 2020) and AdaQuant (Hubara et al., 2020), also take this layer-wise objective to improve the post-training quantization. However, they failed to quantize weights to INT2. We think the inherent reason is that when ∆w grows higher, the former assumptions do not hold and an accurate signal is required. 3 P ROPOSED METHOD 3.1 C ROSS -LAYER DEPENDENCY Denote the neural network output z(n) = f(θ), the loss function can be represented by L(f(θ)) where θ = vec[w(1),T,..., w(n),T]T is the stacked vector of weights in all nlayers. The Hessian matrix can be computed by ∂2L ∂θi∂θj = ∂ ∂θj (m∑ k=1 ∂L ∂z(n) k ∂z(n) k ∂θi ) = m∑ k=1 ∂L ∂z(n) k ∂2z(n) k ∂θi∂θj + m∑ k,l=1 ∂z(n) k ∂θi ∂2L ∂z(n) k ∂z(n) l ∂z(n) l ∂θj , (5) where z(n) ∈ Rm. Since the pretrained full precision model is converged to a local minimum, we can assume the Hessian is positive-semideﬁnite (PSD). Speciﬁcally, the converged model has ∇z(n) Lclose to 0 so the ﬁrst term in Eq. (5) is neglected and Hessian becomes the Gauss-Newton (GN) matrix G(θ). GN matrix can be written in matrix form (Botev et al., 2017) as H(θ) ≈G(θ) = Jz(n) (θ)TH(z(n))Jz(n) (θ), (6) where Jz(n) (θ) is the Jacobian matrix of the network output with respect to the network parameters. However, in practice, we cannot explicitly compute and store the Jacobian for each input data point in such a raw form. To reduce the computation and memory budget, we will transform the second- order error into the network output, as shown in the following theorem. Theorem 3.1. Consider an n-layer feedforward neural network with ReLU activation function. Assuming all weights are quantized, the second-order error optimization can be transformed by: arg min ˆθ ∆θT ¯H(θ)∆θ≈arg min ˆθ E [ ∆z(n),TH(z(n))∆z(n) ] . (7) Remark 3.1. The same transformation is also applicable for activation quantization. The quadratic loss is deﬁned as E[∆γTH(γ)∆γ] where ∆γ = vec[∆x(1),T,..., ∆x(n),T]T. We prove the theorem using the quadratic form, details can be found in Appendix A.1. Here we provide a sketch of the proof by matrix form. The product of perturbation and Jacobian can be thought as the ﬁrst-order Taylor approximation of the change in network output ∆z(n): ∆z(n) = ˆz(n) −z(n) ≈Jz(n) (θ)∆θ. (8) Therefore, combining Eq. (8) and Eq. (6) we can transform the large-scale second-order error into the change in network outputs characterized by the output Hessian H(z(n)). The theorem indicates a simple observation, suppose a well-trained teacher model and an initialized student model, we can minimize their discrepancy by reconstructing the network’s ﬁnal output z(n), which coincides with and generalizes the distillation (Hinton et al., 2015; Polino et al., 2018). LAPQ (Nahshan et al., 2019) also considers the dependency but their optimization does not rely on second-order information. However, we should emphasize that distillation requires the same computation and data resources as in normal training procedure, which is impractical for PTQ with limited data. 1To prevent ambiguity, we hereby use layer-diagonal Hessian to replace the common name “block-diagonal Hessian” because the block in this paper means a building block in the CNNs. 3Published as a conference paper at ICLR 2021 stem Network Structure                             Body Structure                             Stage Structure                     Block Structure (w, h, 3) body (w/2, h/2, c0) head (w/32, h/32, c4) (1, 1, #classes) stage 1 (w/2, h/2, c0) stage 2 (w/4, h/4, c1) stage 4 ... ... (w/32, h/32, c4) block 1 (w, h, ci) block 2 (w/2, h/2, ci+ 1) block n ... ... (w/2, h/2, ci+ 1) layer 1 (w, h, ci) layer 2 (w, h, ci*e) layer 3 (w, h, ci*e) (w, h, ci) + (a) A typical structure of CNN (taken from Radosavovic et al. (2020)). Network is composed of a stem layer (ﬁrst convolu- tion on input images), a body and a head layer (average pool- ing with a fully connected layer). A body contains several stages, and a stage contains several blocks. A representative block is the bottleneck block with residual path. layer-diagonal  block-diagonal  Full matrix (b) An example illustration of Hessian (or Fisher) matrix. Blue sub-block means the layer-diagonal and each layer are mutual- independent; orange sub-block consider the dependency inside a building block and green parts measure all dependencies. Figure 1: We deﬁne 4 kinds of reconstruction granularity, namely net-wise, stage-wise, block-wise and layer- wise optimization, each of them corresponds an essential component of CNN. 3.2 B LOCK RECONSTRUCTION Although the network output reconstruction has an accurate estimation of the second-order error, we ﬁnd in practice it is worse than the layer-by-layer reconstruction in PTQ. The primary reason for this is optimizing the whole networks over 1024 calibration data samples leads to over-ﬁtting easily. As Jakubovitz et al. (2019) explained, the networks can have perfect expressivity when the number of parameters exceeds the number of data samples during training, but lower training error does not ensure lower test error. We ﬁnd layer-wise reconstruction acts like a regularizer which reduces the generalization error by matching each layer’s output distribution. In other words, both layer-wise and network-wise output reconstruction has their own drawbacks. And there should be a better bias-variance trade-off choice to conduct reconstruction at an intermediate granularity. The layer-wise optimization corresponds to layer-diagonal Hessian (Fig. 1b blue parts) and the network-wise optimization corresponds to full Hessian (Fig. 1b green parts). Similarly, we can de- ﬁne an intermediate block-diagonal Hessian. Formally, if layer kto layer ℓ(where 1 ≤k <ℓ≤n) form a block, the weight vector is deﬁned as ˜θ = vec[w(k),T,..., w(ℓ),T]T and the Hessian can be also transformed by ∆˜θT ¯H(˜θ)∆˜θ= E[∆z(ℓ),TH(z(ℓ))∆z(ℓ)]. Such block-diagonal Hessian ignores the inter-block dependency and considers the intra-block dependency but it produces less general- ization error. Then we can block-by-block reconstruct the intermediate output. To this end, we deﬁne 2 extra kinds of intermediate reconstruction granularity: Stage-wise recon- struction and Block-wise reconstruction. These 4 reconstruction granularities are described below: 1. Layer-wise Reconstruction: Assume the Hessian matrix is layer-diagonal and optimize the layer output one-by-one. It does not consider cross-layer dependency and resemble existing methods (Nagel et al., 2020; Hubara et al., 2020; Wang et al., 2020). 2. Block-wise Reconstruction: A block is the core component in modern CNN, such as the Residual Bottleneck Block as shown in Fig. 1a. This method assumes the Hessian matrix is block- diagonal and block-by-block perform reconstruction, which ignores inter-block dependencies. 3. Stage-wise Reconstruction: A stage is where the featuremaps will be downsampled and gener- ate more channels, which is believed to produce higher-level features. Typical CNN in ImageNet dataset contains 4 or 5 different stages. This method simultaneously optimizes all layers within a stage and thus considers more dependencies than the block-wise method. 4. Network-wise Reconstruction: Optimize the whole quantized network by reconstructing the output of the ﬁnal layers. This method resembles distillation but does not result in good perfor- mances with few images because of high generalization error. The relationship between network, stage, block, and layer is illustrated in Fig. 1a. We test these 4 kinds of reconstruction granularity and ﬁnd that block-wise optimization outperforms others . We think this is because the main off-diagonal loss in the Hessian is concentrated in each block, as Fig. 1b orange part illustrated, while the inter-block loss is small and can be ignored in the opti- 4Published as a conference paper at ICLR 2021 Algorithm 1:BRECQ optimization Input: Pretrained FP model; Calibration dataset, iteration T for all i= 1,2,...,N -th block in the FP model do Collect input data to the block x(i), the FP output z(i) and its gradient g(z(i)) ; for all j = 1,2,...,T -iteration do Get quantized output ˆz(i) and compute ∆z(i) = z(i) −ˆz(i); Descend Eq. (10) and update the rounding of all the weights in this block (Eq. (16)); if Activation Quantization is triggered then Update the activation quantization step size (Eq. (18)). After optimization, compute the sensitivity for each layer and between layers (2-bit only); return Quantized model, Sensitivities for mixed precision; mization. The shortcut connections, which is proposed in (He et al., 2016), may also increase the dependencies within a block. Also, the stage-wise or net-wise optimization suffer from the bad generalization on the validation set and degenerate the ﬁnal performances. We report the quanti- tative comparison in Sec. 4.1. We name our algorithm B RECQ , because we choose block as our base reconstruction unit. It is necessary to point out that our analysis does not give the optimal conﬁguration of the reconstruction granularity. The choice of block-wise optimization comes from our experiments and we ﬁnd this choice has two merits. (1) No hyper-parameters included and (2) applicable for all models and all tasks we tested. 3.3 A PPROXIMATING PRE-ACTIVATION HESSIAN With block-diagonal approximated Hessian matrix, we can measure the cross-layer dependency inside each block and transform any block’s second-order error to the output of this block E[∆z(ℓ),TH(z(ℓ))∆z(ℓ)]. This objective requires the further computation of the knowledge in the rest of the network, i.e., pre-activation Hessian H(z(ℓ)). One way is to follow Nagel et al. (2020) and assume H(z(ℓ)) = c×I. Therefore the quadratic loss becomes ||∆z(ℓ)||2. This method might be easy to implement but lose too much information. We use the diagonal Fisher Information Matrix (FIM) to replace the pre-activation Hessian. For- mally, given a probabilistic model p(x|θ), the FIM is deﬁned as: ¯F(θ) = E [ ∇θlog pθ(y|x)∇θlog pθ(y|x)T] = −E [ ∇2 θlog pθ(y|x) ] = −¯H(θ) log p(x|θ). (9) The FIM is equal to the negative expected Hessian of the log-likelihood function, therefore, a simple corollary is that the Hessian of task loss will become FIM if the model distribution matches the true data distribution (LeCun et al., 2012). Although matching true data distribution seems impossible, this is the best we can do since the pretrained model is converged. The diagonal of the pre-activation FIM is equal to the squared gradients of each elements, which is successfully used in Adam (Kingma & Ba, 2014) for the second momentum. The optimization objective becomes min ˆw E [ ∆z(ℓ),TH(z(ℓ))∆z(ℓ) ] = min ˆw E [ ∆z(ℓ),Tdiag ( ( ∂L ∂z(ℓ) 1 )2,..., ( ∂L ∂z(ℓ) a )2 ) ∆z(ℓ) ] . (10) Compared with the MSE minimization, the above minimization incorporates the squared gradient information. If the output has higher absolute gradients, it will receive more attention when being reconstructed. A similar method for pruning the pre-activation has been proposed in Theis et al. (2018). Note that BRECQ is compatible with any optimization method, like STE (Hubara et al., 2017). Here we adopt adaptive rounding (Nagel et al., 2020) for weights and learned step size (Esser et al., 2020) for activation step size because we observe they generally perform better in PTQ, see details in Appendix B.4.1. We formulate the overall calibration algorithm for a uniﬁed precision model in algorithm 1. We should emphasize that we only need a small subset (1024 in our experiments) of the 5Published as a conference paper at ICLR 2021 whole training dataset to calibrate the quantized model. And we can obtain a quantized ResNet-18 within 20 minutes on a single GTX 1080TI GPU. 3.4 M IXED PRECISION To further push the limit of post-training quantization, we employ mixed precision techniques, which can be formulated by min c L( ˆw,c), s.t. H(c) ≤δ, c ∈{2,4,8}n. (11) Here, c is the bit-width vector with the shape of number of layers. H(·) is a hardware performance measurement function, which is used to ensure the mixed precision model has the same or lower hardware performance (e.g., memory and speed) than a predeﬁned threshold δ. We choose 2, 4, 8-bit for mixed precision because they are most common in practical deployment. Regarding the training lossL, we ﬁnd that nearly all existing literature (Cai et al., 2020; Hubara et al., 2020; Dong et al., 2019) uses layer-wise measurement. They all assume the sensitivity within a layer is independent and can be summed together. Therefore, the mixed precision problem becomes an integer programming problem. However, we argue that the loss measurement should contain two parts: diagonal loss and off-diagonal loss, the ﬁrst is the same with previous works and measure the sensitivity of each layer independently, while the off-diagonal loss is used to measure the cross-layer sensitivity. Theoretically, we should examine all permutations, which results in 3n possibilities and prohibits the search algorithm. Our ﬁrst attempt is to reduce the off-diagonal loss into the block- level as we mentioned that the Hessian can be approximated to a block-diagonal matrix. Granted, we still ﬁnd the search space is large, for example, if a block has four layers, then we have to consider the 34 = 81 permutations for a single block. Based on our preliminary experiments, we ﬁnd that 4-bit and 8-bit quantization nearly do not drop the ﬁnal accuracy. Hence we only take 2-bit permutations into consideration and drastically reduce the search space. We use genetic algorithm (Guo et al., 2020) to search the optimal bitwidth conﬁguration with hardware performance threshold, the algorithm is located in algorithm 2. Due to space limits, we put related works in Appendix 5. Readers can refer to related works for a brief discussion on quantization and second- order analysis. 4 E XPERIMENTS In this section, we report experimental results for the ImageNet classiﬁcation task and MS COCO object detection task. The detailed implementation of the experiments can be found in the Ap- pendix B.4.4. The rest of this section will contain ablation study on reconstruction granularity, classiﬁcation and detection results, mixed precision results and comparison with quantization-aware training. In Appendix B, we conduct more experiments, including the impact of the ﬁrst and the last layer, the impact of calibration dataset size and data source. 4.1 A BLATION STUDY We test four kinds of reconstruction granularity: Net-wise, Stage-wise, Block-wise, and Layer-wise Table 1: Ablation study. Model Layer Block Stage Net ResNet-18 65.19 66.39 66.01 54.15 MobileNetV2 52.13 59.67 54.23 40.76 reconstruction. We conduct ImageNet experi- ments using MobileNetV2 and ResNet-18 with 2- bit weight quantization for all layers except for the ﬁrst and the last layer. It can be seen from Table 1 that Block-wise optimization outperforms other methods. This result implies that the general- ization error in net-wise and stage-wise optimiza- tion outweighs their off-diagonal loss. In ResNet- 18, we ﬁnd the difference is not signiﬁcant, this can be potentially attributed to that ResNet-18 only has 19 layers in the body and the block size, as well as the stage size, is small, therefore leading to indistinct results. 4.2 I MAGE NET We conduct experiments on a variety of modern deep learning architectures, including ResNet (He et al., 2016) with normal convolution, MobileNetV2 (Sandler et al., 2018) with depthwise separa- 6Published as a conference paper at ICLR 2021 Table 2: Accuracy comparison on weight-only quantized post-training models. Activations here are un- quantized and kept full precision. We also conduct variance study for our experiments. Bold values indicates best results. * indicates our implementation based on open-source codes. Methods Bits (W/A) ResNet-18 ResNet-50 MobileNetV2 RegNet-600MF RegNet-3.2GF MnasNet-2.0 Full Prec. 32/32 71.08 77.00 72.49 73.71 78.36 76.68 Bias Correction* 4/32 50.43 64.64 62.82 67.09 71.73 72.31 OMSE (Choukroun et al., 2019) 4/32 67.12 74.67 - - - - AdaRound (Nagel et al., 2020) 4/32 68.71 75.23 69.78 71.97* 77.12* 74.87* AdaQuant (Hubara et al., 2020) 4/32 68.82 75.22 44.78 - - - Bit-Split (Wang et al., 2020) 4/32 69.11 75.58 - - - - BRECQ(Ours) 4/32 70.70±0.07 76.29±0.04 71.66±0.04 73.02±0.09 78.04±0.04 76.00±0.02 Bias Correction* 3/32 12.85 7.97 10.89 28.82 17.95 40.72 AdaRound (Nagel et al., 2020)* 3/32 68.07 73.42 64.33 67.71 72.31 69.33 AdaQuant (Hubara et al., 2020)* 3/32 58.12 67.61 12.56 - - - Bit-Split (Wang et al., 2020) 3/32 66.75 73.24 - - - BRECQ(Ours) 3/32 69.81±0.05 75.61±0.09 69.50±0.12 71.48±0.07 77.22±0.04 74.58±0.08 Bias Correction* 2/32 0.13 0.12 0.14 0.18 0.11 0.11 AdaRound (Nagel et al., 2020)* 2/32 55.96 47.95 32.54 25.66 24.70 30.60 AdaQuant (Hubara et al., 2020)* 2/32 0.30 0.49 0.11 - - - BRECQ(Ours) 2/32 66.30±0.12 72.40±0.12 59.67±0.13 65.83±0.13 73.88±0.14 67.13±0.13 Table 3: Accuracy comparison on fully quantized post-training models. Activations here are quantized to 4-bit. Notations follows the upper table. Methods Bits (W/A) ResNet-18 ResNet-50 MobileNetV2 RegNet-600MF RegNet-3.2GF MNasNet-2.0 Full Prec. 32/32 71.08 77.00 72.49 73.71 78.36 76.68 ACIQ-Mix (Banner et al., 2019) 4/4 67.0 73.8 - - - - ZeroQ (Cai et al., 2020)* 4/4 21.71 2.94 26.24 28.54 12.24 3.89 LAPQ (Nahshan et al., 2019) 4/4 60.3 70.0 49.7 57.71* 55.89* 65.32* AdaQuant (Hubara et al., 2020) 4/4 67.5 73.7 34.95* - - - Bit-Split (Wang et al., 2020) 4/4 67.56 73.71 - - - BRECQ(Ours) 4/4 69.60±0.04 75.05±0.09 66.57±0.67 68.33±0.28 74.21±0.19 73.56±0.24 ZeroQ (Cai et al., 2020)* 2/4 0.08 0.08 0.10 0.10 0.05 0.12 LAPQ (Nahshan et al., 2019)* 2/4 0.18 0.14 0.13 0.17 0.12 0.18 AdaQuant (Hubara et al., 2020)* 2/4 0.21 0.12 0.10 - - BRECQ(Ours) 2/4 64.80±0.08 70.29±0.23 53.34±0.15 59.31±0.49 67.15±0.11 63.01±0.35 ble convolution and RegNet (Radosavovic et al., 2020) with group convolution. Last but not least important, we also investigate the neural architecture searched (NAS) models, MNasNet (Tan et al., 2019). In Table 2, we only quantize weights into low-bit integers and keep activations full precision. We compare with strong baselines including Bias Correction, optimal MSE, AdaRound, AdaQuant, and Bit-split. Note that the ﬁrst and the last layer are kept with 8-bit. While most of the existing methods have good performances in 4-bit quantization, they cannot successfully quantize the model into 2-bit. Our method consistently achieves the lowest accuracy degradation for ResNets (within 5%) and other compact models. We further quantize activations into 4-bit to make the quantized model run on integer-arithmetic hardware platforms. We ﬁnd that 4-bit activation quantization can have a huge impact on RegNet and MobileNet. Nonetheless, our methods produce higher perfor- mance than other state-of-the-arts. To be noted, B RECQ is the ﬁrst to promote the 2W4A accuracy of PTQ to a usable level while all other existing methods crashed. 4.3 C OMPARISON WITH QUANTIZATION -AWARE TRAINING Table 4: Performance as well as training cost comparison with quantization-aware training (QAT). Models Methods Precision Accuracy Model Size Training Data GPU hours ResNet-18 FP: 71.08 ZEROQ (CAI ET AL., 2020)) 4/4 21.20 5.81 MB 0 0.008 BRECQ(OURS) 4/4 69.60 5.81 MB 1024 0.4 BRECQ(W/ DISTILLEDDATA) 4/4 69.32 5.81 MB 0 0.4 PACT (CHOI ET AL., 2018) 4/4 69.2 5.81 MB 1.2 M 100 DSQ (GONG ET AL., 2019) 4/4 69.56 5.81 MB 1.2 M 100 LSQ (ESSER ET AL., 2020) 4/4 71.1 5.81 MB 1.2 M 100 MobileNetV2 FP: 72.49 BRECQ(OURS) 4/4 66.57 2.26 MB 1024 0.8 PACT (CHOI ET AL., 2018) 4/4 61.40 2.26 MB 1.2 M 192 DSQ (GONG ET AL., 2019) 4/4 64.80 2.26 MB 1.2 M 192 BRECQ(OURS) Mixed/8 70.74 1.38 MB 1024 3.2 HAQ (WANG ET AL., 2019) Mixed/8 70.90 1.38 MB 1.2 M 384 7Published as a conference paper at ICLR 2021 Table 5: Objection detection task (MS COCO) comparison onfully quantized post-training models. Activations here are quantized to 8-bit. We report the bounding box mean Average Precision (mAP) metric. Models Backbone Full Prec. Bias Correction* AdaRound* ZeroQ B RECQ(Ours) 32/32 8/8 4/8 4/8 2/8 4MP/8 8/8 4/8 2/8 Faster RCNN (Ren et al., 2015) ResNet-18 34.55 34.30 0.84 33.96 23.01 - 34.53 34.34 31.82 ResNet-50 38.55 38.25 0.25 37.58 19.63 - 38.54 38.29 34.23 MobileNetV2 33.44 33.24 18.39 32.77 16.35 - 33.40 33.18 27.54 RetinaNet (Lin et al., 2017) ResNet-18 33.20 33.00 0.04 32.59 19.93 - 33.14 33.01 31.42 ResNet-50 36.82 36.68 0.07 36.00 19.97 33.7 36.73 36.65 34.75 MobileNetV2 32.63 32.60 18.47 31.89 14.10 - 32.57 32.31 27.59 3.0 3.5 4.0 4.5 5.0 5.5 Model Size (MB) 64 65 66 67 68 69 70 71Test Accuracy66.09 67.99 68.82 69.53 70.13 70.53 ResNet-18 Mixed Unified 2-bit 4-bit ZeroQ 0.8 1.0 1.2 1.4 1.6 Model Size (MB) 56 58 60 62 64 66 68 70 72Test Accuracy 63.73 67.4 68.99 70.28 71.39 MobileNet-V2 1.50 1.75 2.00 2.25 2.50 2.75 3.00 Model Size (MB) 64 66 68 70 72Test Accuracy65.58 68.36 69.62 71.39 71.88 72.68 RegNetX_600MF 30 35 40 45 50 Latency (ms) 64 65 66 67 68 69 70 71Test Accuracy 66.49 68.9 70.25 70.37 70.5 28.5 29.0 29.5 30.0 30.5 31.0 31.5 Latency (ms) 56 58 60 62 64 66 68 70 72Test Accuracy 66.35 67.47 68.63 70.47 71.4 71.54 22 24 26 28 30 Latency (ms) 64 66 68 70 72Test Accuracy 66.61 68.73 69.43 71.3 72.07 72.53 Figure 2: Mixed Precision results. In this section, we compare our algorithm (post-training quantization) with some quantization-aware training methods, including PACT (Choi et al., 2018), DSQ (Gong et al., 2019), LSQ (Esser et al., 2020), and a mixed precision technique HAQ (Wang et al., 2019). Table 4 shows that although BRECQ is a PTQ method with limited available data, it can achieve comparable accuracy results with existing quantization-aware training models. In addition, our method can surpass them in 4- bit MobileNetV2 while using less than one training GPU hours. Our method also has comparable accuracy with HAQ, which is a training-based mixed precision method. Note that our GPU hours include 3 uniﬁed precision training (2-, 4-, 8-bit respectively) and further mixed-precision training only needs to check the lookup table. Instead, HAQ would end-to-end search for each hardware performance threshold from scratch. 4.4 MS COCO To validate the effectiveness of B RECQ on other tasks, we conduct object detection on the two- stage Faster-RCNN (Ren et al., 2015) and the one-stage RetinaNet (Lin et al., 2017). ResNet-18, 50 as well as MobileNetV2 are adopted as backbones for the detection model. Results in Table 5 demonstrate our method nearly does not drop the performance in 4-bit weight quantization and 8- bit activation. In particular, B RECQ only decreases 0.21% mAP performance on 4-bit ResNet-18 backboned Faster RCNN. On 4-bit ResNet-50 backboned RetinaNet, our method is outperforms the mixed precision based ZeroQ model by 3% mAP. Even when the weight bit decreases to 2, the model still achieves near-to-original mAP. 8Published as a conference paper at ICLR 2021 4.5 M IXED PRECISION In this section, we test (1) model-size guaranteed mixed precision and (2) FPGA latency guar- anteed mixed precision 2 to unleash the potential of mixed precision and further push the limit of PTQ. We choose ResNet-18, MobileNetV2, and RegNetX-600MF to validate the efﬁcacy of our algorithm. Note that in this section, we keep activation in 8-bit because we only compare the dis- crepancy between the uniﬁed and mixed precision in weights. We omit 3-bit weight quantization in uniﬁed precision because it is usually unfriendly to the hardware. Latency settings can be found in Appendix B.4.3. From Fig. 2 we ﬁnd that (1) mixed precision consistently outperforms uniﬁed precision, especially when using extremely low-bit, e.g., up to 10% accuracy increase with the same latency as the 2-bit model. (2) mixed precision can produce many bit conﬁgurations that can adapt to plenty of hardware requirements while uniﬁed precision can only have 2 ﬁxed models. 5 R ELATED WORKS Quantization Model quantization can be classiﬁed into two categories: Quantization-aware Train- ing (QAT) and Post-training Quantization (PTQ). Rounding ﬂoating-point numbers to ﬁxed-points numbers will produce 0 gradients almost everywhere. Therefore, most QAT methods employ the Straight-Through Estimator (STE) for gradients approximation. Gong et al. (2019) uses a differen- tiable tanh function to gradually approach the step function. Choi et al. (2018); Esser et al. (2020) introduces parameterized clipping thresholds to learn it by STE. Apart from uniform quantization, some works like Li et al. (2019) argue that non-uniform quantization has better performance than uniform quantization while keeping its efﬁciency. Despite the promising results given by QAT meth- ods, they usually need more than 100 GPU hours to get it. In that case, PTQ plays an important role which is what we focus on in this paper. Generally, most deep learning models can be safely quan- tized to 8-bit without re-training. Data-Free Quantization Nagel et al. (2019) even do layer-wise 8-bit PTQ without any data. However, in 4-bit quantization, most parameter space-based methods cannot obtain good performances. Recently, Nagel et al. (2020) propose to do layer-wise calibration and made huge progress in 4-bit quantization. Our work continues its analysis on Taylor expansion and considers the off-diagonal loss. Another perspective of quantiﬁcation is the precision allocation scheme. Hardware-aware Quantization (HAQ Wang et al. (2019)) leverages reinforcement learning to search the optimal bitwidth conﬁguration. Hessian-aware Weight Quantization (HAWQ) (Dong et al., 2019) utilizes the second-order information to decide the bitwidth. Mixed precision also appears in PTQ, such as the Pareto frontier method in ZeroQ (Cai et al., 2020) and the Integer Programming method in AdaQuant (Hubara et al., 2020). Second-order Analysis and OptimizationThe history of second-order information in perturbation analysis can be traced to the 1990s like Optimal Brain Surgeon (Hassibi & Stork, 1993; Dong et al., 2017). The Hessian matrix is essential for pruning and quantization. As aforementioned, HAWQ uses the largest eigenvalue of Hessian to determine the sensitivity. Hessian matrix is also impor- tant for second-order optimization like Newton’s method as it consists of the curvature informa- tion. However, calculating the real full Hessian is prohibitive on today’s deep learning architectures. Therefore, approximations are made to simplify the calculation and make the storage more ﬂexi- ble, e.g., Gauss-Newton optimization with Kronecker-factored recursive approximation Botev et al. (2017). Hessian-Free optimization (Martens, 2010) avoids the explicit computation of the Hessian matrix by solving the linear system g= Hv. Second-order optimization with FIM is called Natural Gradient Descent (Amari, 1998). K-FAC (Martens & Grosse, 2015) utilizes the layer-diagonal FIM and the approximation of the expected Kronecker product to compute the curvature information. 6 C ONCLUSION In this paper, we propose BRECQ , a post-training quantization framework by analyzing the second- order error. We show that the reconstruction of quantization at the block granularity arrives at a good balance of cross-layer dependency and ﬁrst order approximation, especially in 2-bit weight quantization where no prior works succeed to quantize. B RECQ is compatible with mixed precision and can reduce the search cost. To our best knowledge, B RECQ reaches the highest performance in post-training quantization and is the ﬁrst to be on a par with quantization-aware training using 4-bit. 2We also test mobile CPU latency guaranteed mixed precision, located in Appendix B.3. 9Published as a conference paper at ICLR 2021 ACKNOWLEDGMENT We thank Markus Nagel and anonymous reviewers for their kind help of this work. This project is primarily supported by NSFC 61876032. REFERENCES Shun-Ichi Amari. Natural gradient works efﬁciently in learning. Neural computation, 10(2):251– 276, 1998. Haoli Bai, Jiaxiang Wu, Irwin King, and Michael Lyu. Few shot network compression via cross distillation. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence , volume 34, pp. 3203–3210, 2020. Ron Banner, Yury Nahshan, and Daniel Soudry. Post training 4-bit quantization of convolutional networks for rapid-deployment. In Advances in Neural Information Processing Systems, 2019. Aleksandar Botev, Hippolyt Ritter, and David Barber. Practical gauss-newton optimisation for deep learning. arXiv preprint arXiv:1706.03662, 2017. Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Zeroq: A novel zero shot quantization framework. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13169–13178, 2020. Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srini- vasan, and Kailash Gopalakrishnan. Pact: Parameterized clipping activation for quantized neural networks. arXiv preprint arXiv:1805.06085, 2018. Yoni Choukroun, Eli Kravchik, Fan Yang, and Pavel Kisilev. Low-bit quantization of neural net- works for efﬁcient inference. In 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), pp. 3009–3018. IEEE, 2019. Xin Dong, Shangyu Chen, and Sinno Pan. Learning to prune deep neural networks via layer-wise optimal brain surgeon. In Advances in Neural Information Processing Systems, 2017. Zhen Dong, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Hawq: Hes- sian aware quantization of neural networks with mixed-precision. In Proceedings of the IEEE International Conference on Computer Vision, pp. 293–302, 2019. Steven K. Esser, Jeffrey L. McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dhar- mendra S. Modha. Learned step size quantization. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=rkgO66VKDS. Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu, Jiazhen Lin, Fengwei Yu, and Junjie Yan. Differentiable soft quantization: Bridging full-precision and low-bit neural networks. arXiv preprint arXiv:1908.05033, 2019. Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, and Jian Sun. Single path one-shot neural architecture search with uniform sampling. In European Conference on Computer Vision, pp. 544–560. Springer, 2020. Qingchang Han, Yongmin Hu, Fengwei Yu, Hailong Yang, Bing Liu, Peng Hu, Ruihao Gong, Yanfei Wang, Rui Wang, Zhongzhi Luan, et al. Extremely low-bit convolution optimization for quantized neural network on modern computer architectures. In 49th International Conference on Parallel Processing-ICPP, pp. 1–12, 2020. Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015. Babak Hassibi and David G Stork. Second order derivatives for network pruning: Optimal brain surgeon. In Advances in neural information processing systems, pp. 164–171, 1993. 10Published as a conference paper at ICLR 2021 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog- nition. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 770–778, 2016. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized neural networks: Training neural networks with low precision weights and activations. The Jour- nal of Machine Learning Research, 18(1):6869–6898, 2017. Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. Improving post training neural quantization: Layer-wise calibration and integer programming. arXiv preprint arXiv:2006.10518, 2020. Daniel Jakubovitz, Raja Giryes, and Miguel RD Rodrigues. Generalization error in deep learning. In Compressed Sensing and Its Applications, pp. 153–193. Springer, 2019. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Yann A LeCun, L ´eon Bottou, Genevieve B Orr, and Klaus-Robert M ¨uller. Efﬁcient backprop. In Neural networks: Tricks of the trade, pp. 9–48. Springer, 2012. Yuhang Li, Xin Dong, and Wei Wang. Additive powers-of-two quantization: An efﬁcient non- uniform discretization for neural networks. In International Conference on Learning Representa- tions, 2019. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll ´ar. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision , pp. 2980–2988, 2017. James Martens. Deep learning via hessian-free optimization. 2010. James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate curvature. In International conference on machine learning, pp. 2408–2417, 2015. Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free quantization through weight equalization and bias correction. In Proceedings of the IEEE International Con- ference on Computer Vision, pp. 1325–1334, 2019. Markus Nagel, Rana Ali Amjad, Mart van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training quantization. arXiv preprint arXiv:2004.10568, 2020. Yury Nahshan, Brian Chmiel, Chaim Baskin, Evgenii Zheltonozhskii, Ron Banner, Alex M Bronstein, and Avi Mendelson. Loss aware post-training quantization. arXiv preprint arXiv:1911.07190, 2019. Antonio Polino, Razvan Pascanu, and Dan Alistarh. Model compression via distillation and quanti- zation. arXiv preprint arXiv:1802.05668, 2018. Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll ´ar. Designing network design spaces. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10428–10436, 2020. Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems, pp. 91–99, 2015. Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mo- bilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4510–4520, 2018. 11Published as a conference paper at ICLR 2021 Hardik Sharma, Jongse Park, Naveen Suda, Liangzhen Lai, Benson Chau, Vikas Chandra, and Hadi Esmaeilzadeh. Bit fusion: Bit-level dynamically composable architecture for accelerating deep neural network. In 2018 ACM/IEEE 45th Annual International Symposium on Computer Archi- tecture (ISCA), pp. 764–775. IEEE, 2018. Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2820–2828, 2019. Lucas Theis, Iryna Korshunova, Alykhan Tejani, and Ferenc Husz ´ar. Faster gaze prediction with dense networks and ﬁsher pruning. arXiv preprint arXiv:1801.05787, 2018. Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han. Haq: Hardware-aware automated quan- tization with mixed precision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 8612–8620, 2019. Peisong Wang, Qiang Chen, Xiangyu He, and Jian Cheng. Towards accurate post-training network quantization via bit-split and stitching. In Proc. 37nd Int. Conf. Mach. Learn.(ICML), 2020. Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578, 2016. A M AIN PROOFS A.1 P ROOF OF THEOREM 3.1 Proof. We will prove the theorem using quadratic form. Denote the weight vector shape asθ∈Rd, and the network output vector shape as z(n) ∈Rm. The quadratic form of the ∆θTH(θ)∆θcan be represented by: ∆θTH(θ)∆θ= d∑ i=1 ∆θ2 i (∂2L ∂θ2 i ) + 2 d∑ i<j ∆θi∆θj ∂L ∂θiθj = d∑ i=1 d∑ j=1 ( ∆θi∆θj ∂L ∂θiθj ) , (12) where Lis the cross-entropy loss. Based on Eq. (5), we have ∂2L ∂θiθj = m∑ k,l ∂z(n) k ∂θl ∂2L ∂z(n) k z(n) l ∂z(n) l ∂θj (13) Substituting above equation in Eq. (12), we have ∆θTH(θ)∆θ= d∑ i=1 d∑ j=1 ∆θi∆θj (m∑ k=1 m∑ l=1 ∂z(n) k ∂θi ∂2L ∂z(n) k z(n) l ∂z(n) l ∂θj ) (14a) = d∑ i=1 d∑ j=1 m∑ k=1 m∑ l=1 ( ∆θi∆θj ∂z(n) k ∂θi ∂2L ∂z(n) k z(n) l ∂z(n) l ∂θj ) (14b) = m∑ k=1 m∑ l=1 ( ∂2L ∂z(n) k z(n) l )( d∑ i=1 ∆θi ∂z(n) k ∂θi )  d∑ j=1 ∆θj ∂z(n) k ∂θj   (14c) = (∆θJ [z(n) θ ] )TH(z(n))(∆θJ [z(n) θ ] ), (14d) where we deﬁne the J[x y] is the Jacobian matrix of xw.r.t. y. To this end, we use the ﬁrst-order Taylor expansion as we did in Eq. (8) to approximate the change in network output, i.e., ∆z(n) ≈∆θJ [z(n) θ ] (15) Therefore, the ﬁnal objective is transformed to ∆z(n),TH(z(n))∆z(n). 12Published as a conference paper at ICLR 2021 B E XPERIMENTS B.1 E FFECT OF THE FIRST AND THE LAST LAYER Many papers claim that the ﬁrst and the last layer can have a huge impact on the ﬁnal accuracy. In this section, we investigate this phenomenon as well as the impact of the ﬁrst and the last layer on hardware performances. We test ResNet-18, MobileNetV2 as well as RegNet-600MF. Our observa- tions include: 1. In terms of accuracy, the 4-bit quantization is essentially good, both of these two layers won’t drop too much accuracy (with 0.2%). But in 2-bit quantization, the last fully connected layer is far more important than the ﬁrst layer. We also observe that the ﬁrst layer in MobileNetV2 and RegNet (3×3 kernels, 32 channels) is slightly more sensitive than that in ResNet-18 (7×7 kernel, 64 channels). 2. In terms of model size, the ﬁrst layer merely has a minor impact because the input images only have 3 channels, while the last layer contains many weight parameters and greatly affects the memory footprint. We should point out that just the model size in the ﬁrst layer is low doesn’t mean the memory burden is low, because the input image will cost huge memory space. 3. In terms of latency, the situation depends on the architecture. For example, in ResNet-18 the ﬁrst layer has a huge impact on the latency, while in MobileNetV2 and RegNet-600MF the last layer is more important than the ﬁrst layer. This is because the latency is affected by multiple factors, such as the input size of the featuremap, the FLOPs, and the weight memory size. The arithmetic intensity (OPs/byte) greatly affects the latency. We ﬁnd that the operations with high arithmetic intensity, i.e., shallow layers in the network, generate a less latency gap between different bit- widths. In conclusion, we ﬁnd that keeping the ﬁrst and the last layer 8-bit is unnecessary. Especially in ResNet-18, we ﬁnd that setting all layers to 4-bit results in 53.3 ms latency and is faster than the 59.8 ms in 2-bit quantization (with ﬁrst and last layer 8-bit), but the accuracy is even 4% higher. Such phenomenon indicates the potential power of the mixed precision. B.2 E FFECT OF DATA We evaluated the inﬂuence of the size of calibration dataset and the source of the data on ResNet-18. We test different numbers of input data point and ﬁnd that the improvement in 4-bit quantization is trivial. Yet in 2-bit quantization we can see that the accuracy increases 5% when #data points increase. We also test the distilled data introduced in ZeroQ (Cai et al., 2020). Distilled data is learned from pretrained models’ BN statistics, i.e.,x1 distilled = arg minx∈R ∑n i=1((µi−ˆµi)2 +(ςi− ˆςi)) where µi and ςi is the original mean and variance in BN statistics of the (i)-th layer. We ﬁnd that distilled data performs good in 4-bit quantization but still has a large margin with the original ImageNet dataset in 2-bit quantization. We also ﬁnd the ﬁnal accuracy does not beneﬁt much from the increase of number of distilled data, this might because the distilled data are minimized by a same objective and has low diversity. Table 6: Impact of the ﬁrst and the last layer Models No Quantization Precision 4/8 Precision 2/8 First Last Accuracy Model Size Latency Accuracy Model Size Latency ResNet-18 FP: 71.08 \u0013 \u0013 70.76 5.81 MB 70.72 ms 66.30 3.15 MB 59.84 ms \u0017 \u0013 70.66 5.81 MB 53.76 ms 65.95 3.15 MB 31.20 ms \u0013 \u0017 70.64 5.57 MB 70.08 ms 64.87 2.79 MB 58.72 ms \u0017 \u0017 70.58 5.56 MB 53.28 ms 64.53 2.78 MB 30.88 ms MobileNetV2 FP: 72.49 \u0013 \u0013 71.80 2.26 MB 32.80 ms 59.59 1.74 MB 30.40 ms \u0017 \u0013 71.69 2.26 MB 32.64 ms 59.13 1.74 MB 30.24 ms \u0013 \u0017 71.42 1.65 MB 31.52 ms 56.29 0.83 MB 28.48 ms \u0017 \u0017 71.42 1.65 MB 31.36 ms 55.58 0.82 MB 28.32 ms RegNet-600MF FP: 73.71 \u0013 \u0013 72.98 3.19 MB 31.84 ms 65.66 1.84 MB 23.20 ms \u0017 \u0013 72.89 3.19 MB 31.68 ms 65.83 1.85 MB 22.88 ms \u0013 \u0017 72.69 2.94 MB 31.20 ms 62.93 1.47 MB 22.40 ms \u0017 \u0017 72.73 2.94 MB 31.04 ms 63.08 1.47 MB 22.08 ms 13Published as a conference paper at ICLR 2021 64 128 256 512 1024 # data points 70.0 70.1 70.2 70.3 70.4 70.5 70.6 70.7Accuracy 70.4 70.49 70.55 70.63 70.7 70.1 70.32 70.28 70.34 70.33 ResNet-18 4bit ImageNet data Distilled data 64 128 256 512 1024 # data points 58 60 62 64 66Accuracy 61.11 63.15 64.72 65.65 66.39 58.22 59.17 60.03 60.67 60.94 ResNet-18 2bit ImageNet data Distilled data Figure 3: Effect of #data points and data source. B.3 M OBILE CPU L ATENCY GUARANTEED MIXED PRECISION 550 560 570 580 590 600 Latency (ms) 64 65 66 67 68 69 70 71Test Accuracy 66.65 67.89 69.36 70.04 70.43 ResNet-18 Mixed Unified 2-bit 4-bit 1150 1175 1200 1225 1250 1275 Latency (ms) 71 72 73 74 75 76Test Accuracy 71.92 73.57 74.8 75.71 76.32 ResNet-50 Mixed Unified 2-bit 4-bit Figure 4: Mixed precision results on ResNet-18 and 50. In this section, we test the mobile CPU latency guaranteed mixed precision. The latency lookup table is tested using the technique in Gong et al. (2019). We only validate it on ResNet-18 and ResNet-50 because the current low-bit General Matrix Multiply (GEMM) implementation only supports normal convolution. The results concur with Fig. 2. Below 4-bit, the mixed precision can achieve better task performance than the uniﬁed precision models. For ResNet-50, the improvement is lower than that for ResNet-18 and any other mixed precision models. We think this is because the sensitivity in ResNet-50 is not distinct and therefore the improvement brought by mixed precision is trivial. B.4 I MPLEMENTATION B.4.1 L EARNING STRATEGIES In this work, we mainly focus on developing optimization objective rather than optimization strate- gies. We observe adaptive rounding performs well in post-training quantization. A brief introduction on AdaRound is given below, the detailed algorithm can be found in Nagel et al. (2020). Traditional quantization function is performed by rounding-to-nearest operation: ˆw = s × clip(⌊w/s⌉,n,p ). AdaRound optimizes the rounding policy in post-training quantization. Specif- ically, all weights are initially rounded by ﬂoor operation, and a learnable variable v determines the ﬁnal rounding result to be ﬂooring or ceiling. A sigmoid-like function σ(·) keeps the learnable variable v moving between 0 and 1 and a regularization term assures the σ(v) can converged to either 0 or 1. The formulation is given by ˆw = s×clip ( ⌊w s⌋+ σ(v),n,p ) (16) The minimization problem together with the regularization is given by arg min v E [ ∆z(ℓ),Tdiag(( ∂L ∂z(ℓ) 1 )2,..., ( ∂L ∂z(ℓ) a )2)∆z(ℓ) ] + λ ∑ i ( 1 −|2σ(vi) −1|β) , (17) 14Published as a conference paper at ICLR 2021 where progressively decreasing β in the calibration ensures the σ(v) converged to binary values. The activations cannot be quantized using adaptive rounding because they vary with different input data points. Thus, we can only adjust its quantization step size Esser et al. (2020). Denoting the quadratic loss in above equation as Lq, the gradients of step size is given by ∂Lq ∂s =    ∂Lq ∂ˆx if x >n ∂Lq ∂ˆx (ˆx s −x s ) if 0 ≤x <α 0 if x ≤0 , (18) where all step size in the block will be optimized. B.4.2 G ENETIC ALGORITHM FOR MIXED PRECISION Algorithm 2:Genetic algorithm Input: Random initialized population P0 with population size S; Iteration T, mutation probability p; Hardware performance threshold δ; Hardware measurement function H(·) TopK = ∅ ; for all t= 1,2,...,T -th iteration do Evaluate ﬁtness value (Eq. (11)) for each individual ; Update and sort TopK based on ﬁtness function; repeat New bitwidth conﬁguration by crossover ccross = Crossover(TopK); Pcrossover := Pcrossover + ccross if H(ccross) <δ; until Size of Pcrossover equal to S/2; repeat New bitwidth conﬁguration by mutation cmutate = Mutate(TopK,probability = p); Pmutate := Pmutate + cmutate if H(cmutate) <δ; until Size of Pmutate equal to S/2; Pt = Pcrossover ∪Pmutate; Pmutate = ∅, Pcrossover = ∅; Get the best ﬁtted entry and then do the overall block reconstruction (cf. algorithm 1); return mixed precision model B.4.3 L ATENCY ACQUISITION We test the latency of quantized neural networks on a self-developed simulator of a precision- variable accelerator for NN. The basic architecture of this accelerator is inspired by typical systolic- matrix multiplication. The accelerator supports the per-channel quantization parameter. The preci- sion of each layer of a NN is highly conﬁgurable in this accelerator, supporting 9 types of precision combination: activation: 2-, 4-, 8-bit × weight: 2-, 4-, 8-bit, see Fig. 5a. With the support of scal- able function-unit (Sharma et al., 2018), the peak performance of the accelerator is able to achieve corresponding linear improvement as the precision decreases. For example, the peak performance of this accelerator is 256 GMAC/s in 8-bit ×8-bit precision, and it scales to 512 GMAC/s in 8- bit ×4-bit precision and 4 TMAC/s in 2-bit ×2-bit precision. Although this accelerator provides considerable computation resources especially in low precision, the parallelism of the speciﬁc layer (like depthwise convolution) and the bandwidth of on-chip buffer is limited. Consequently, actual performance may not scale accurately along with the peak performance, and the ﬁnal performance differs according to the size and type of layers. The simulator performs cycle-accurate simulation and evaluation for a given NN executed on the accelerator, so we can get an equivalent evaluation by using this simulator. The simulator is available in the provided source codes. For the acquisition of mobile ARM CPU latency, we adopt the redesigned low-bit GEMM imple- mentation in Han et al. (2020). Fig. 5b shows a brief overview of the low-bit GEMM implementa- tion. Since there is no instruction supporting the bit-width below 8 on ARM architecture, we can 15Published as a conference paper at ICLR 2021 PE Array Input Buffer Weight Buffer Requant/quant 1-D computation Unit Output Buffer Precision -variablePE (a) Accelerator design. A11 A21 A 31 Multiply Replicate Element - wise Multiply Produce Accu - mulate Accumulate A11 A12 A13 A21 A22 A23 A31 A32 A33 A 11 A11 A11 B11 B12 B13 A21 A21 A21 B11 B12 B13 A31 A31 A31 B11 B12 B13 B 11 B 12 B 13 B 21 B 22 B 23 B 31 B 32 B 33 B 11 B 12 B 13 B 11 B 12 B 13 B 11 B 12 B 13 C11 C12 C13 C21 C22 C23 C31 C32 C33 C11 C12 C13 C21 C22 C23 C31 C32 C33 BufferA BufferB BufferC Results MatrixC (b) Low-bit GEMM implementation on ARM CPU. Figure 5: FPGA-based and mobile CPU-based latency acquisition. not get a higher computation efﬁciency for extremely low-bit such as 2-bit and 4-bit. But we can acquire a better memory access efﬁciency. The primary speedup comes from the reduction of data movement. Speciﬁcally, we can conduct more times of addition in the same 8-bit register before we have to move it to a 16-bit register to avoid overﬂow. The lower bit-width is used, the less movement is needed. Together with the optimized data packing and data padding, we can run mixed precision quantization on Raspberry Pi 3B, which has a 1.2 GHz 64-bit quad-core ARM Cortex-A53. Note that this implementation is not optimized for depthwise separable or group convolution, therefore we only verify the latency on ResNets. B.4.4 I MPLEMENTATION DETAILS The ImageNet dataset consists of 1.2M training images and 50K test images. We follows standard pre-process (He et al., 2016) to get 1024 224 ×224 input images as the calibration dataset. We fold the batch normalization layer into convolution and freeze the BN statistics before post-training quan- tization. We use Adam optimizer (Kingma & Ba, 2014) to learn the weight rounding and activation range to reconstruct the block output. Note that some layers are not a component of any block, such as the ﬁrst convolutional layer and the last fully connected layer and the last convolutional layer in the MobileNetV2. These layers use naive layer reconstruction. The batch size of learning is set to 32 and each block will be optimized for 2 ×104 iterations. The learning rate is set to 10−3 during the whole learning process. Other hyper-parameters such as the temperature β are kept the same with Nagel et al. (2020). For activation step size, we also use Adam optimizer and set the learning rate to 4e-5. Note that we do not implement the gradient scale as introduced in the original paper (Esser et al., 2020). After reconstruction, we will store the sensitivity measured on the calibration dataset. Note that we will store intra-block sensitivity in 2-bit quantization. The sensitivity, as well as hard- ware performances for each layer, will be stored in a lookup table. When calculating the ﬁtness value and determining the hardware performances in a genetic algorithm, we will check the lookup table. For genetic algorithm, we set the population size to 50 and evolve 100 iterations to obtain the best individual. The ﬁrst population is initialized by Gaussian distribution and we round the samples to integers in [0, 1, 2], corresponding to bit-width [2, 4, 8]. The mutation probability is set to 0.1. The genetic algorithm usually completes the evolution in only about 3 seconds. For object detection tasks, we use 256 training images taken from the MS COCO dataset for cal- ibration. The image resolution is set to 800 (max size 1333) for ResNet-18 and ResNet-50, while the image resolution for MobileNetV2 is set to 600 (max size 1000). Note that we only apply block reconstruction in the backbone because other parts of the architecture, such as Feature Pyramid Net, do not have the block structure. Therefore a naive layer reconstruction is applied to the rest of the network. Learning hyper-parameters are kept the same with ImageNet experiments. 16",
      "meta_data": {
        "arxiv_id": "2102.05426v2",
        "authors": [
          "Yuhang Li",
          "Ruihao Gong",
          "Xu Tan",
          "Yang Yang",
          "Peng Hu",
          "Qi Zhang",
          "Fengwei Yu",
          "Wei Wang",
          "Shi Gu"
        ],
        "published_date": "2021-02-10T13:46:16Z",
        "pdf_url": "https://arxiv.org/pdf/2102.05426v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces BRECQ, a novel Post-training Quantization (PTQ) framework that pushes the limits of bitwidth down to INT2 for the first time without significant accuracy loss. It achieves 4-bit ResNet and MobileNetV2 performance comparable to Quantization-Aware Training (QAT) with 240x faster production. BRECQ leverages second-order error analysis for neural network building blocks, reconstructs them one-by-one, and incorporates a mixed-precision technique by approximating inter-layer and intra-layer sensitivity using a genetic algorithm.",
        "methodology": "BRECQ analyzes the second-order error based on the Gauss-Newton matrix, transforming it into the network's final output change. To balance cross-layer dependency and generalization error, it adopts a block-wise reconstruction strategy, which empirically outperforms layer-wise, stage-wise, and network-wise approaches. The pre-activation Hessian is approximated using the diagonal Fisher Information Matrix (FIM) to incorporate squared gradient information, giving more attention to outputs with higher absolute gradients. For mixed precision, BRECQ formulates the problem with hardware performance constraints, considers both diagonal and off-diagonal loss (reducing the latter to block-level), and uses a genetic algorithm to search for optimal bitwidth configurations (2, 4, 8-bit), primarily focusing on 2-bit permutations to reduce search space. Adaptive rounding for weights and learned step sizes for activations are employed during calibration.",
        "experimental_setup": "Extensive experiments were conducted on ImageNet for image classification and MS COCO for object detection. Various models were used, including ResNet-18, ResNet-50, MobileNetV2, RegNet-600MF, RegNet-3.2GF, and MNasNet-2.0. The models were evaluated with weight-only quantization (2, 3, 4-bit) and fully quantized models (weights 2, 4-bit; activations 4, 8-bit). A small subset of 1024 images from the training dataset was used for calibration. Performance was compared against strong baselines like Bias Correction, OMSE, AdaRound, AdaQuant, Bit-Split, ACIQ-Mix, ZeroQ, LAPQ, and QAT methods such as PACT, DSQ, LSQ, and HAQ. Ablation studies investigated reconstruction granularity. Latency was acquired using a self-developed simulator for FPGA and redesigned low-bit GEMM implementation on a mobile ARM CPU (Raspberry Pi 3B). Metrics included Top-1 accuracy for classification and mean Average Precision (mAP) for object detection.",
        "limitations": "The theoretical analysis of BRECQ does not provide the optimal configuration for reconstruction granularity; the block-wise choice is primarily supported by experimental evidence. Net-wise and stage-wise optimizations, while considering more dependencies, suffer from bad generalization when using limited calibration data, leading to worse performance. The improvement for ResNet-18 in the ablation study was not significant, potentially due to its smaller block and stage sizes. The assumption that the pretrained model is converged to a minimum, allowing the Hessian to be approximated by the Gauss-Newton matrix, is a foundational constraint. Also, distilled data showed lower diversity and was less effective for 2-bit quantization compared to real ImageNet data, suggesting limitations in its general applicability for extremely low bit-widths.",
        "future_research_directions": "Not mentioned"
      }
    },
    {
      "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache",
      "abstract": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI.",
      "full_text": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Zirui Liu * 1 Jiayi Yuan* 1 Hongye Jin 2 Shaochen (Henry) Zhong 1 Zhaozhuo Xu 3 Vladimir Braverman 1 Beidi Chen 4 Xia Hu 1 Abstract Efficiently serving large language models (LLMs) requires batching of many requests to reduce the cost per request. Yet, with larger batch sizes and longer context lengths, the key-value (KV) cache, which stores attention keys and values to avoid re- computations, significantly increases memory de- mands and becomes the new bottleneck in speed and memory usage. Additionally, the loading of the KV cache causes the computational core to be idle, which limits the inference speed. A straight- forward and effective solution to reduce KV cache size is quantization, which decreases the total bytes taken by KV cache. However, there is a lack of in-depth studies that explore the element dis- tribution of KV cache to understand the hardness and limitation of KV cache quantization. To fill the gap, we conducted a comprehensive study on the element distribution in KV cache of popular LLMs. Our findings indicate that the key cache should be quantized per-channel, i.e., group ele- ments along the channel dimension and quantize them together. In contrast, the value cache should be quantized per-token. From this analysis, we de- veloped a tuning-free 2bit KV cache quantization algorithm named KIVI. With hardware-friendly implementation, KIVI can enable Llama, Falcon, and Mistral models to maintain almost the same quality while using 2.6× less peak memory (in- cluding model weight). This reduction in memory usage enables up to 4× larger batch size, bringing 2.35× ∼3.47× throughput on real LLM infer- ence workload. The source code is available at https://github.com/jy-yuan/KIVI. *Equal contribution . The order of authors is determined by flipping a coin. 1Rice University 2Texas A&M University 3Stevens Institute of Technology 4Carnegie Mellon University. Correspondence to: Zirui Liu <zl105@rice.edu>, Jiayi Yuan <jy101@rice.edu>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). 1. Introduction Large Language Models (LLMs) have demonstrated strong performance across a wide range of tasks (Brown et al., 2020; Taylor et al., 2022; Yuan et al., 2023; Chuang et al., 2024). However, their deployment is very costly, requiring a large number of hardware accelerators such as GPUs. Given these substantial costs, one natural way to reduce the cost per request is to combine a sufficient number of requests together for batch processing. However, in this batch infer- ence scenario, the key-value cache (KV cache), which holds the attention keys and values during generation to prevent re-computations, is becoming the new memory and speed bottleneck. This bottleneck becomes more pronounced with larger batch sizes and longer context lengths. For instance, in 540B PaLM, with a batch size of 512 and a context length of 2048, KV cache alone can take 3TB. This is 3 times the size of the model’s parameters (Pope et al., 2023). Also, the GPU SRAM has to load the whole KV cache from the GPU device memory for every token generated, during which the computational cores are idle. Thus, reducing KV cache size in LLMs while maintaining accuracy is important. Existing works towards this problem can be roughly divided into three categories. First, some work suggests reducing the number of heads in KV cache, such as multi-query attention (Shazeer, 2019) and multi-group attention (Ainslie et al., 2023). However, these methods require either training the model from scratch or fine-tuning the existing model. Second, another research line reduces KV cache size by evicting unimportant tokens (Zhang et al., 2023). Third, some other works try to solve this problem from the system perspective, e.g., offloading KV cache (Sheng et al., 2023) or extending virtual memory and paging techniques into the attention mechanism (Kwon et al., 2023). To reduce the size of KV cache, the most simple and ef- fective way is to reduce the total bytes taken by KV cache, namely, quantization. Unlike the well-studied weight quan- tization (Lin et al., 2023; Xiao et al., 2023a; Zhao et al., 2024), to the best of our knowledge, only a few studies applied the vanilla 4bit round-to-nearest quantization to KV cache (Sheng et al., 2023; Zhang et al., 2023; Zhao et al., 2024) due to the streaming nature of KV cache or other complications. There is a lack of in-depth studies that ex- 1 arXiv:2402.02750v2  [cs.CL]  25 Jul 2024KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache per-token  quantization  per-channel  quantization  Figure 1: Definition of per-token and per-channel quantiza- tion. X ∈ Rlprompt×d is key/value cache where lprompt is the number of tokens and d is the number of channels. zX is the zero-point, sX is the scaling factor. plore the element distribution of KV cache to understand the hardness and limitation of KV cache quantization. To fill the gap, we study the element distribution of KV cache. Our analysis suggests: • For key cache, there are a few fixed channels whose magnitudes are very large, which is consistent with previous finding (Lin et al., 2023; Xiao et al., 2023a). Thus, as shown in Figure 1 right, key cache should be quantized per-channel, i.e., group elements along the channel dimension and quantize them together. In this way, it can confine the error to each individual channel, without impacting the other normal channels. • For value cache, there is no obvious outlier pattern. Al- though value cache has no obvious outlier pattern, we experimentally show that it can only be quantized per- token because it is used to calculate the attention output, which is essentially a value cache mixer. As shown in Figure 1 left, the per-token quantization can confine the error inside each individual token and ensure that the quantization of one token does not adversely impact the others. Based on the above insights, we propose KIVI, a plug-and- play extreme low-bit KV cache quantization method. KIVI quantizes key cache per-channel and quantizes value cache per-token. The per-token value cache quantization aligns well with the streaming nature of auto-regressive inference, allowing newly quantized tensors to be directly appended to the existing quantized value cache by token dimension. However, for per-channel key cache quantization, the quan- tization process spans different tokens, which cannot be directly implemented in this streaming setting. Since the number of tokens in key cache can be arbitrary, our key idea is to split key cache into two parts. The first part is the grouped key cache, which contains several groups of tokens and each group has a certain number of tokens. The second part is the residual key cache, which does not have a suffi- cient number of tokens to form a complete group. Similarly, we split value cache into the grouped and residual parts to maintain the accuracy. We only apply group-wise quanti- zation to the grouped key cache and value cache, while the residual key cache and value cache are kept in full precision. The grouped and residual parts can be combined using tiled matrix multiplication when computing attention scores. Our contributions are summarized as follows: • Extensive analysis regarding the outlier patterns and quantization error of KV cache in commonly- used LLMs. Our observations suggest that key cache should be quantized per-channel and value cache should be quantized per-token. We also explain in depth why these caches require different quantization approaches. • A new plug-and-play 2bit KV cache quantization algorithm without any fine-tuning, KIVI, with hardware-friendly implementation. We conduct an extensive evaluation for KIVI with Llama, Mistral, and Falcon on popular generation tasks. KIVI can efficiently compress KV cache to 2bit and bring 2.6× peak memory usage reduction for Llama-2-7B, with little to no accuracy drop. With our efficient system im- plementation, this memory reduction, in return, enables up to 4× larger batch size and brings2.35× ∼3.47× throughput. 2. Background: Attention Inference-Time Workflow The LLM attention inference-time workflow involves two phases: i) the prefill phase, where the input prompt is used to generate KV cache for each transformer layer of LLMs; and ii) the decoding phase, where the model uses and updates KV cache to generate the next token, one at a time. Prefill Phase. Let X ∈ Rb×lprompt×d be the input tensor, where b is the batch size, lprompt is the length of the input prompt, and d is the model hidden size. For convenience, we ignore the layer index here. The key, value tensors can be computed by XK = XWK, XV = XWV , where WK, WV ∈ Rd×d are the key and value layer weight, respectively. After obtaining XK and XV , they are cached in the memory for the ease of decoding. Decoding Phase. Let t ∈ Rb×1×d be the current input token embedding. Let tK = tWK and tV = tWV be the key and value layer output, respectively. We first update KV cache: XK ← Concat(XK, tK), XV ← Concat(XV , tV ), 2KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache then calculate the attention output as: tQ = tWQ, A = Softmax(tQX⊤ K), tO = AXV , (1) where WQ is the weight matrix of the query layer. For ease of illustration, we ignore the attention output layer and the other parts of the inference workflow. Memory and Speed Analysis. The above process is re- peated until a special token indicating the sentence’s con- clusion is reached. Let lgen be the number of generated tokens. From the above analysis, the shape of KV cache is b ×(lprompt + lgen) ×d. To get a sense of the scale, consider the OPT-175B model with a batch size b 512, a prompt length lprompt 512, and an output length lgen 32. The KV cache requires 1.2TB, which is 3.8 times the model weights (Sheng et al., 2023). Besides the memory, the inference speed is also decided by the KV cache size. The GPU needs to load KV cache from GPU main memory to GPU SRAM once for every token generated during which the computa- tional core of the chip is essentially idle (Pope et al., 2023; Kwon et al., 2023). 3. Methodology In scenarios with long contexts or batched inferences, the memory and speed bottlenecks are storing and loading KV cache. The most simple and effective way to alleviate this problem is to reduce the total bytes occupied by KV cache, specifically, quantization. Following this motivation, we first evaluate the performance of the existing quantization method in Section 3.1. Our observations suggest that key and value cache should be quantized along different dimen- sions. We analyze the rationale behind this observation in Section 3.2. Then based on the analysis, we propose KIVI, a new KV cache quantization method along with its streaming data structure, detailed in Section 3.3. 3.1. Preliminary Study of KV Cache Quantization As we analyzed in Section 2, KV cache functions as a streaming data structure, where the new tensor arrives se- quentially. Thus, optimization-based methods like GPTQ (Frantar et al., 2022) are unsuitable for quantizing KV cache due to the overhead. To the best of our knowledge, the most flexible way for quantizing KV cache is the round- to-nearest quantization. The B−bit integer quantization- dequantization process can be expressed as: Q(X) = ⌊X − zX sX ⌉, X′ = Q(X) · sX + zX, where zX = min X is the zero-point, sX = (max X − min X)/(2B −1) is the scaling factor, and ⌊·⌉ is the round- ing operation. Here we ignore the batch size for ease of Table 1: The results of simulated KV cache group-wise quantization with various configurations. The group size is set as 32. C stands for per-channel quantization and T stands for per-token quantization. Please check the whole evaluation in Table 3. Llama-2-13B CoQA TruthfulQA 16bit 66.37 29.53 4bit (K - T, V - T) 66.48 29.51 2bit (K - T, V - T) 52.93 24.98 2bit (K - C, V - C) 2.88 0.74 2bit (K - T, V - C) 2.80 0.26 2bit (K - C, V - T) 63.53 28.60 understanding. As shown in Figure 1, X is quantized along either the token or channel dimension group-wisely. Considering the streaming nature of KV cache, previous studies often apply per-token quantization to both key and value cache since the newly quantized KV cache can be naively added to the existing quantized one along the token dimension (Sheng et al., 2023). While per-channel quanti- zation is non-trivial, we have designed a padding method to implement per-channel quantization to explore its effect on both key and value cache. Setting. In Table 1, we show the results of fake KV cache group-wise quantization with different configurations on the Llama-2-13B model for the CoQA and TruthfulQA tasks. We use a group size of 32 for all configurations. Here fake quantization means we simulate the quantization process by first quantizing KV cache into lower precision and then dequantizing it in the attention layer. For per-channel quan- tization, if the number of tokens is not divided evenly into groups, we add zero-padding to ensure it can be grouped perfectly. In this way, we ensure that all tokens in KV cache are quantized for a fair comparison. The detailed experi- mental setting can be found in Section 4.1. Specifically, we observe that: OB 1. When using the commonly used per-token quanti- zation to both key and value caches, INT4 precision can maintain accuracy. However, reducing it to INT2 results in a notable accuracy drop. OB 2. When value cache is quantized per-channel, the accuracy significantly worsens regardless of how key cache is quantized. OB 3. When using a lower numerical precision such as INT2, the most accurate approach is to quantize key cache per-channel and value cache per-token. 3KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache 0 50 100 150 200 Token 0 25 50 75 100 125 Channel 0.0 2.5 5.0 7.5 10.0 12.5Absolute Value Llama-2-13B Layer 16 Head 0 Key Cache 0 50 100 150 200 Token 0 25 50 75 100 125 Channel 0.5 1.0 1.5 2.0Absolute Value Llama-2-13B Layer 16 Head 0 Value Cache 0 50 100 150 200 Token 0 25 50 75 100 125 Channel 0.0 2.5 5.0 7.5 10.0 12.5 Llama-2-13B Layer 31 Head 0 Key Cache 0 50 100 150 200 Token 0 25 50 75 100 125 Channel 0.5 1.0 1.5 2.0 2.5 Llama-2-13B Layer 31 Head 0 Value Cache 0 50 100 150 200 Token 0 20 40 60 Channel 0 2 4 6 8 10Absolute Value Falcon-7B Layer 16 Head 0 Key Cache 0 50 100 150 200 Token 0 20 40 60 Channel 0.5 1.0 1.5Absolute Value Falcon-7B Layer 16 Head 0 Value Cache 0 50 100 150 200 Token 0 20 40 60 Channel 0.0 2.5 5.0 7.5 10.0 Falcon-7B Layer 20 Head 0 Key Cache 0 50 100 150 200 Token 0 20 40 60 Channel 0.5 1.0 1.5 Falcon-7B Layer 20 Head 0 Value Cache Figure 2: Magnitude of key and value cache for Llama-2-13B and Falcon-7B. We observe (1) for key cache, there are a few channels whose magnitudes are very large. (2) for value cache, there is no obvious outlier pattern. 3.2. Why Key and Value Cache Should Quantize Along Different Dimensions? In Table 1, we observe that quantizing key cache per-channel and value cache per-token to 2bit results in a very small accu- racy drop. Here we analyze why this configuration delivers better accuracy. In Figure 2 we visualize the original KV cache distribution at different layers. We observe that in key cache, some fixed channels exhibit very large mag- nitudes, whereas in value cache, there is no significant pattern for outliers. Analysis of Key Cache. The above observation for key cache aligns with previous findings that certain fixed columns in activations exhibit larger outliers (Dettmers et al., 2022; Lin et al., 2023). The persistence of outliers within each channel means that per-channel quantization can con- fine the quantization error to each individual channel with- out impacting the other normal channels. Thus, Figure 2 explains why key cache should be quantized per-channel. In Table 2 we show key cache relative reconstruction error ∥XK−X′ K XK ∥F , along with the relative attention score error ∥A−A′ A ∥F where A′ = Softmax(tQX ′⊤ K ). We observe that the per-token quantization can lead to almost 5× larger at- tention score error than per-channel quantization, which is consistent with Figure 2. Table 2: The relative error statistics averaged over all layers and all heads Llama-2-13B K Per-Token K Per-Channel Avg. ∥ XK−X′ K XK ∥F 13.67 4.55 Avg. ∥ A−A′ A ∥F 47.00 9.60 Attention sparsity 84.3% V Per-Token V Per-Channel Avg. ∥ XV −X′ V XV ∥F 4.57 3.73 Avg. ∆ 3.55 49.89 Analysis of Value Cache. Unlike key cache, value cache does not show the channel-wise outlier pattern. Furthermore, Figure 2 alone cannot explain OB2, which indicates value cache should only be quantized per-token. This is because Figure 2 implies that errors should be comparable for both per-token and per-channel quantization, given the absence of a clear pattern. As shown in Equation (1), value cache is used to calculate the attention output tO. Instead of analyzing the quantization error of value cache XV , in Table 2 we analyze the relative error ∆ = ∥AXV −AX′ V AXV ∥F with different quantization configurations. Surprisingly, we observe that the per-token quantization error is almost 15× 4KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Q_MatMul Concat Quantization by Channel Quantization by Token MatMul Prefill Phase Decoding Phase* KVCache * we omit the value cache and  attention output calculation Full Precision Tensor Low Precision Tensor Figure 3: The overview of KIVI algorithm. For ease of illustration, we omit the value cache and attention output parts. The detailed pseudo-code is provided in Algorithm 1. Here “Q_Matmul” is the mix-precision matrix multiplication which fuses the dequantization with matrix multiplication at the tiling level. smaller than per-channel quantization, which explains why OB2 happens. The intuition behind this observation stems from the attention sparsity. Equation (1) can be written as: [AXV ]i∗ = lpromptX j=1 Aij[XV ]j∗, (2) where [XV ]j∗ is the j-th row of XV . From Equation (2), the attention output is the weighted summation of value cache across various tokens, with the weights being the attention scores. Since the attention score is highly sparse (Tian et al., 2023), the output is just the combination of value caches of a few important tokens. The per-token quantization can confine the error to each individual token. Thus, quantizing other tokens does not affect the accuracy of important tokens. Consequently, per-token quantization leads to a much smaller relative error ∆. 3.3. KIVI: Algorithm and System Support Algorithm. As we previously analyzed, key cache should be quantized per-channel and value cache should be quan- tized per-token. Recall that key and value cache of newly generated tokens arrive sequentially. From the implemen- tation perspective, per-token quantization aligns well with streaming settings, allowing newly quantized tensors to be directly appended to the existing quantized value cache by token dimension. However, for per-channel quantiza- tion, the quantization process spans across different tokens, which cannot be directly implemented in the streaming set- ting. As shown in Figure 3, our key idea to solve this prob- lem is to group key cache everyG tokens and quantize them separately. Because the number of tokens in XK can be ar- bitrary, we splitXK into two parts, namely, the grouped part XKg = XK[: l − r] and residual part XKr = XK[l − r :], where l is the number of tokens inside the current key cache XK, r is the number of residual tokens, where l − r can be divisible by G. Since XKg can be evenly divided into (l − r)/G groups, we only store Q(XKg ) with group-wise quantization, while XKr is kept in full precision. During the decoding process, each newly arrived key cache tK is added to XKr and once XKr reaches R tokens, which is a hyperparameter - residual length, we quantize and concatenate it with the previously quantized Q(XKG). Then we reset XKr to an empty tensor. We note that R should be divisible by G. With tiled matrix multiplication, the raw attention logits is then calculated as: Ag = tQQ(X⊤ Kg ), XKr = Concat([XKr , tK]), Ar = tQX⊤ Kr , A = Concat([Ag, Ar]). (3) For value cache, similar to key cache, we also split it into two parts and keep the most recent value cache in full pre- cision, namely, XVg and XVr . Specifically, we maintain a queue and each newly arrived value cache is pushed into the queue. Once the queue reaches the predefined residual length R, the most outdated value cache is poped. Then the poped value cache is quantized per-token and concatenated with the previously quantized value cache along the token dimension. As shown in Figure 3, we also emphasize that during the prefill phase, the exact key and value tensors are passed to the next layers, although only the quantized KV cache is retained in memory. The whole algorithm can be found in Appendix A Algorithm 1. Analysis. In KIVI, the grouped key cacheXKg and value cache XVg is quantized, while the residual key cache XKr and value cache XVr is kept in full precision. By design, there are at most R tokens inside XKr or XVr . In practice, we set R ≤ 128 and the sequence length lprompt + lgen is often much longer than R. Thus the memory overhead from XKr and XVr is negligible when considering the benefit from extreme low-bit quantization, especially for the long context scenarios. Also, since the newly arrived key and 5KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache value tensors are added to XKr and XVr in full precision, KIVI maintains a full precision KV cache sliding window for the local relevant tokens. This window size is expected to be R 2 for key cache, and R for value cache. Later in the experiment section, we show that this full precision sliding window is crucial for obtaining desirable perfor- mance on hard tasks, such as GSM8K. System Support. We provide a hardware-friendly imple- mentation for running KIVI on GPUs. To minimize the overhead, we have fused the dequantization process with matrix multiplication, e.g., Q_MatMul in Figure 3, using CUDA. We also implement the group-wise quantization ker- nel in Triton. Our method is fully compatible with weight- only quantization. 4. Experiments 4.1. Settings Models. We evaluate KIVI using three popular model families: Llama/Llama-2 (Touvron et al., 2023a;b), Fal- con (Penedo et al., 2023) and Mistral (Jiang et al., 2023). Llama and Mistral model is based on multi-head attention, while Falcon is based on multi-query attention (Shazeer, 2019). We use the Hugging Face Transformers codebase and implement the KIVI algorithm upon it. Following previous work (Sheng et al., 2023), the group size G in Algorithm 1 for quantization is set as 32 across all experi- ments, the residual length R for key and value cache is set to 128. Tasks. As we analyzed in Section 2, the KV cache size grows larger with a longer context. Thus, we evaluateKIVI under the normal context length and long context setting, respectively. Specifically, we adopt generation tasks from LM-Eval (Gao et al., 2021) for normal context length eval- uation and LongBench (Bai et al., 2023) for long context evaluation, respectively1. For LM-eval, we adopt CoQA (Exact match accuracy), TruthfulQA (BLEU score), and GSM8K (Exact match accuracy). For LongBench, we chose tasks from four subgroups. Specifically, Qasper (F1 score) is a Single-Document QA task; QMSum (ROUGE score) and MultiNews (ROUGE score) are Summarization tasks; TREC (classification score), TriviaQA (F1 score), and SAM- Sum (ROUGE score) are Few-shot Learning tasks; and LCC (similarity score) and RepoBench-P (similarity score) is Code Completion task. The maximum sequence length in LongBench was set to 8192 for the Mistral model and 4096 for other models. We also consider the needle-in-a-haystack 1The closed-end tasks such as MMLU are not ideal to evaluate KIVI since they only involve one decoding step and directly fetch the output logits, which is not suitable for studying the impact of compressed KV cache. task (NIAH) to evaluate the model’s long context retrieval ability after quantizing KV cache. Detailed NIAH setting can be found in Appendix B. 4.2. Accuracy and Efficiency Analysis 4.2.1. C OMPARISON BETWEEN DIFFERENT QUANTIZATION CONFIGURATIONS We first utilize the fake quantization to demonstrate the effec- tiveness of our asymmetric quantization, namely, quantizing key cache per-channel and value cache per-token. Here fake quantization is exactly the same as in Table 1. The results are shown in Table 3. We observe that “2bit (K per-channel, V per-token)” consistently achieves the best results com- pared to all other configurations. This is consistent with our previous analysis. We also note that for hard generation tasks such as GSM8K, the fake “2bit (K per-channel, V per-token)” quantization results are significantly worse than the full precision counterparts. However, for KIVI in Table 3, we observe that the accuracy drop is only around 2% for GSM8K across different models. As we analyzed in Section 3.3, the difference between fake “2bit (K per-channel, V per-token)” quantization and KIVI is that KIVI maintains a full precision key and value cache sliding window for the local relevant tokens. This sliding window is crucial to maintaining accuracy for hard generation tasks such as mathematical reasoning. 4.2.2. A CCURACY COMPARISON ON GENERATION TASKS LM-Eval Results. We benchmark KIVI in CoQA, Truth- fulQA and GSM8K tasks using the LM-Eval framework. All dataset parameters were set to default. We compare the standard 16bit configuration with our KIVI compression techniques in Llama-2-7B, Llama-2-13B, Falcon-7B and Mistral-7B. As shown in Table 3, we observe that for the Llama and Mistral model, KIVI only has up to 2% accu- racy drop despite the KV cache being stored in 2bit. For instance, in the Llama-2-7B model, the transition from 16bit to 2bit only slightly decreases accuracy. Similar trends are observed in other Llama-family models. Since Falcon-7B adopts multi-query attention and only has one head for KV cache, it is already highly compressed compared to Llama- based models. Thus, in Table 3, 4bit KIVI is needed to maintain the accuracy, while 2bit KIVI may have a large accuracy drop in this case. LongBench Results. The performance of KIVI over vari- ous models in the LongBench dataset is summarised in Table 4. We apply KIVI to Llama2-7B, Llama2-13B, Llama2-7B- Chat, Llama2-13B-Chat, Falcon-7B and Mistral-7B. Table 4 suggests that KIVI is an effective method for KV cache compression with minimal impact on accuracy across var- 6KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  27K tokens (a) Llama-3-8B-Instruct Baseline 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  27K tokens  (b) Llama-3-8B-Instruct + KIVI-2 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  27K tokens  (c) Llama-3-8B-Instruct + KIVI-4 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  30K tokens (d) Mistral-7B-Instruct-v0.2 Baseline 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  30K tokens  (e) Mistral-7B-Instruct-v0.2 + KIVI-2 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  30K tokens  (f) Mistral-7B-Instruct-v0.2 + KIVI-4 0.0 0.2 0.4 0.6 0.8 1.0 Figure 4: Needle-in-a-Haystack results on Llama-3-8B-Instruct and Mistral-7B-Instruct-v0.2. Here we count the number of words instead of tokens to better account for the tokenizer difference. The final token length is noted in the upper right corners of each figure. Detailed setting can be found in Appendix B. ious hard long context generation tasks. We present ad- ditional results using Llama3-8b, Mistral-7B-v0.2, and LongChat-7B-v1.5, which can be found in Appendix D. NIAH Results. From Figure 4, we observe that KIVI can still maintain the retrieval ability of LLMs even with 2bit KV Cache. Detailed NIAH setting can be found in Appendix B. 4.2.3. A BLATION In this section, we benchmark KIVI on GSM8K, one of the hardest generation tasks, to show the effect of hyperpa- rameters group size G and residual length R on the model performance. For full results of KIVI with a residual length of 32, please refer to Appendix C. The effect of group size. We fix the residual length at 128 and vary the group sizes to 32, 64, and 128. From Table 5, we observe that group sizes 32 and 64 yield similar results, whereas the performance significantly decreases when the group size reaches 128. Note the zero-point and the scaling factor mentioned in Section 3.1 are calculated according to this group size; where the choice of group size will greatly impact the KV cache compression effect under a long input. The effect of residual length. We fix the group size at 32 and vary the residual length across 32, 64, 96, and 128. As shown in Table 5, there is no consistent pattern between residual lengths and model accuracy. Namely, while a resid- ual length of 128 achieves good results, 32 and 96 yield similar outcomes, but a residual length of 64 results in the worst performance. We emphasize that while we observe no significance among residual lengths of {32, 96, 128}, hav- ing a reasonably large residual length is important; as it brings much performance boosts on hard tasks like GSM8K, again as shown in Table 5. 4.2.4. E FFICIENCY COMPARISON To evaluate the wall-clock time efficiency ofKIVI, follow- ing vLLM (Kwon et al., 2023), we synthesize workloads based on ShareGPT (sha, 2023), which contain input and output texts of real LLM services. On average, the data set has an input prompt length lprompt of 161 and an output length lgen of 338 (Kwon et al., 2023). We increase the batch 7KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Table 3: Performance comparison between 16bit, 4-bit per- token quantization, four fake 2bit KV cache quantization similar to those in Table 1, KIVI-2 (2bit) / KIVI-4 (4bit) across various models. We emphasize that unlike KIVI, which preserves a small portion of full precision key cache XKr and value cache XVr , all tokens in fake KV cache quantization are quantized for a fair comparison. C stands for per-channel quantization and T stands for per-token quantization. Model CoQA TruthfulQA GSM8K Llama-2-7B 16bit 63.88 30.76 13.50 4bit (K -T, V -T) 64.82 29.85 12.28 2bit (K -C, V -T) 59.08 33.10 5.76 2bit (K -T, V -T) 39.88 18.29 0.83 2bit (K -C, V -C) 3.60 0.27 0.00 2bit (K -T, V -C) 1.30 0.49 0.08 KIVI-4 63.78 30.80 13.80 KIVI-2 63.05 33.95 12.74 Llama-2-13B 16bit 66.37 29.53 22.67 4bit (K -T, V -T) 66.73 29.14 20.92 2bit (K -C, V -T) 63.53 28.60 12.21 2bit (K -T, V -T) 52.93 24.98 4.55 2bit (K -C, V -C) 2.88 0.74 0.00 2bit (K -T, V -C) 2.80 0.26 0.08 KIVI-4 66.38 29.49 23.65 KIVI-2 66.23 29.84 20.77 Falcon-7B 16bit 59.83 23.20 4.55 4bit (K -T, V -T) 58.53 22.94 3.26 2bit (K -C, V -T) 43.93 20.82 1.29 2bit (K -T, V -T) 25.72 0.91 0.53 2bit (K -C, V -C) 41.95 17.11 1.52 2bit (K -T, V -C) 19.53 0.94 0.15 KIVI-4 59.67 22.58 4.47 KIVI-2 57.48 24.98 3.41 Mistral-7B 16bit 67.40 30.45 38.36 4bit (K -T, V -T) 67.80 29.83 36.85 2bit (K -C, V -T) 61.65 29.64 26.46 2bit (K -T, V -T) 54.55 25.86 5.00 2bit (K -C, V -C) 24.40 24.86 2.27 2bit (K -T, V -C) 10.73 19.12 0.99 KIVI-4 66.95 30.49 37.30 KIVI-2 66.35 32.17 36.01 size until out of memory and report the peak memory usage and throughput between KIVI (with residual length 32 and 128) and FP16 baseline for the Llama-2-7B model. The hardware here is a single NVIDIA A100 GPU (80GB). As shown in Figure 5, with similar maximum memory us- age, KIVI enables up to 4× larger batch size and gives 2.35× ∼3.47× larger throughput. This throughput num- ber can grow larger with longer context length and output length. We also note that this speed-up can be greatly increased if we further fuse the KV cache quantization process with previous operations. We leave it as one of future work. Figure 5: Memory usage and throughput comparison be- tween 2bit KIVI and 16bit baseline. KIVI can achieve higher throughput by enabling a larger batch size. 5. Related Work Many machine learning systems and benchmark works con- sider scaling up LLM inference process (Pope et al., 2023; Yuan et al., 2024). Among them, quantization techniques have been widely applied (Frantar et al., 2022; Lin et al., 2023; Kim et al., 2023; Xu et al., 2023). A main branch of LLM quantization is weight-only quantization, which in- volves the quantization of model weights to lower precision. For instance, AWQ (Lin et al., 2023) cleverly quantizes model weights to INT4 and INT3 using an activation-aware manner. GPTQ (Frantar et al., 2022) utilizes approximate second-order information to quantize model weights both accurately and efficiently. SqueezeLLM (Kim et al., 2023) adopts the concept of non-uniform quantization based on sensitivity along with dense-and-sparse decomposition. This line of work is orthogonal to ours, as they can be combined. SmoothQuant (Xiao et al., 2023a) is a post-training quan- tization method that is more closely related to our work. This method uses equivalent transformations to balance the quantization complexity for both activation and weight, making the activation easier to quantize. SmoothQuant can compress KV cache to 8bit with minor performance loss. However, it faces a significant accuracy drop when scaled down to 4bit or less (Zhao et al., 2024). FlexGen (Sheng et al., 2023) adopts 4-bit group-wise quantization for both key and value cache. 8KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Table 4: Performance evaluation of KIVI on various models across a range of benchmarks in LongBench. We highlight the average performance of our method. More similar results on Mistral-7B-v0.2 and LongChat-7b-v1.5 can be found in Table 10 and Table 9 Model Qasper QMSum MultiNews TREC TriviaQA SAMSum LCC RepoBench-P Average Llama2-7B 16bit 9.52 21.28 3.51 66.00 87.72 41.69 66.66 59.82 44.52 KIVI-4 9.28 21.42 3.88 66.00 87.72 41.82 66.80 59.83 44.59 KIVI-2 9.31 20.50 1.14 66.00 87.42 42.71 66.88 60.23 44.27 Llama2-13B 16bit 9.32 21.38 3.71 70.00 87.87 43.55 66.61 56.42 44.85 KIVI-4 9.16 20.86 3.21 69.00 86.97 44.26 65.30 57.08 44.48 KIVI-2 8.58 20.69 6.19 69.50 87.78 44.30 65.08 55.46 44.69 Llama2-7B-Chat 16bit 19.65 20.54 26.36 63.00 84.28 41.12 59.75 52.93 45.95 KIVI-4 19.62 20.70 25.49 63.00 84.13 40.87 59.27 53.56 45.83 KIVI-2 19.32 20.46 25.48 63.00 84.84 40.60 58.71 52.97 45.67 Llama2-13B-Chat 16bit 24.18 20.37 25.69 67.50 86.90 42.18 50.23 50.64 45.96 KIVI-4 23.00 20.36 26.06 67.50 87.20 42.04 52.55 52.77 46.44 KIVI-2 23.59 20.76 25.25 67.5 87.17 41.56 49.93 48.45 45.52 Falcon-7B 16bit 1.48 2.35 11.09 13.00 5.84 2.44 23.86 9.69 8.71 KIVI-4 1.04 2.41 11.98 13.00 5.84 2.36 23.72 9.92 8.78 KIVI-2 1.98 3.61 6.78 10.00 6.24 2.73 22.18 10.12 7.95 Mistral-7B 16bit 8.12 19.98 19.99 67.50 89.80 41.69 66.59 58.99 46.58 KIVI-4 7.89 20.06 20.58 67.50 89.80 41.56 66.45 58.62 46.56 KIVI-2 6.92 19.71 17.92 66.50 89.63 41.66 65.52 58.99 45.85 Table 5: Ablation study of KIVI by changing group size G and residual length R. Model Group Size GSM8K Llama2-13B 32 20.77 64 21.00 128 17.29 Model Residual Length GSM8K Llama2-13B 32 20.62 64 19.86 96 20.55 128 20.77 One essential recipe of KIVI is the per-channel quantiza- tion scheme designed based on the observation made in Section 3.2 and Figure 2. ATOM (Zhao et al., 2024) also indicates that key cache exhibits more outliers compared to the value cache. KIVI provides further extensive analysis and leverages this observation to implement per-channel quantization. A similar observation and approach has been independently discovered and developed in the concurrent work KVQuant (Hooper et al., 2024). vLLM (Kwon et al., 2023) and S3 (Jin et al., 2023) are system-level works, which include memory management through the use of PagedAttention or memory usage pre- diction. They can lower the memory requirements of KV cache and simultaneously increase model throughput. This research direction is orthogonal to our work, since system- level optimizations can also be applied upon our algorithm. Several other works also consider compressing KV cache by evicting tokens. H2O (Zhang et al., 2023) retains only a small portion of tokens that contribute significantly to the attention scores. Similarly, Scissorhands (Liu et al., 2024) exploit the persistence of the importance hypothesis in KV cache sparsification. StreamingLLM (Xiao et al., 2023b) is based on the observation of “attention sink” and maintains only a few initial tokens to preserve performance. Unlike these works, our KIVI retains all input tokens and compresses them into lower precision. This line of work is orthogonal to ours, as they can also be combined together. 6. Conclusion and Future Work In this paper, we systematically analyze KV cache element distribution in popular LLMs. We conclude that key cache should be quantized per-channel and value cache should be quantized per token. Based on these observations, we propose KIVI, a plug-and-play 2bit KV cache quantization algorithm without the need for any tuning. In real LLM workload, KIVI allows up to 4× larger batch sizes and 3.47× throughput. In the future, we will further optimize the implementation to reduce the overhead of quantization process during the prefill and decoding phase. Acknowledgments The authors thank the anonymous reviewers for their help- ful comments. Dr. Beidi Chen is supported by a research gift from Moffett.AI. Dr. Vladimir Braverman is partially 9KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache supported by the Ministry of Trade, Industry and Energy (MOTIE) and Korea Institute for Advancement of Technol- ogy (KIAT) through the International Cooperative R&D pro- gram, the Naval Research (ONR) grant N00014-23-1-2737, and NSF CNS 2333887 award. Dr. Xia Hu is supported by NSF grants NSF IIS-2224843. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. References ShareGPT Team. https://sharegpt.com/, 2023. Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan- guage models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. Yu-Neng Chuang, Songchen Li, Jiayi Yuan, Guanchu Wang, Kwei-Herng Lai, Leisheng Yu, Sirui Ding, Chia-Yuan Chang, Qiaoyu Tan, Daochen Zha, et al. Understand- ing different design choices in training large time series models. arXiv preprint arXiv:2406.14045, 2024. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, An- thony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al. A frame- work for few-shot language model evaluation. Version v0. 0.1. Sept, page 8, 2021. Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W Mahoney, Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cache quantization. arXiv preprint arXiv:2401.18079, 2024. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Yunho Jin, Chun-Feng Wu, David Brooks, and Gu-Yeon Wei. S3: Increasing gpu utilization during genera- tive inference for higher throughput. arXiv preprint arXiv:2306.06000, 2023. Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611–626, 2023. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023. Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and An- shumali Shrivastava. Scissorhands: Exploiting the per- sistence of importance hypothesis for llm kv cache com- pression at test time. Advances in Neural Information Processing Systems, 36, 2024. Amirkeivan Mohtashami and Martin Jaggi. Landmark at- tention: Random-access infinite context length for trans- formers. arXiv preprint arXiv:2305.16300, 2023. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobei- dli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outper- forming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023. 10KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Ja- cob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling trans- former inference. Proceedings of Machine Learning and Systems, 5, 2023. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrit- twieser, et al. Gemini 1.5: Unlocking multimodal un- derstanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150, 2019. Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. Flexgen: High-throughput generative inference of large language models with a single gpu. In International Conference on Machine Learning, pages 31094–31116. PMLR, 2023. Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022. Yuandong Tian, Yiping Wang, Beidi Chen, and Simon Du. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. arXiv preprint arXiv:2305.16380, 2023. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar- tinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and ef- ficient post-training quantization for large language mod- els. In International Conference on Machine Learning, pages 38087–38099. PMLR, 2023a. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023b. Zhaozhuo Xu, Zirui Liu, Beidi Chen, Yuxin Tang, Jue Wang, Kaixiong Zhou, Xia Hu, and Anshumali Shrivastava. Compress, then prompt: Improving accuracy-efficiency trade-off of llm inference with transferable prompt. arXiv preprint arXiv:2305.11186, 2023. Jiayi Yuan, Ruixiang Tang, Xiaoqian Jiang, and Xia Hu. Large language models for healthcare data augmentation: An example on patient-trial matching. In AMIA Annual Symposium Proceedings, volume 2023, page 1324. Amer- ican Medical Informatics Association, 2023. Jiayi Yuan, Hongyi Liu, Shaochen Zhong, Yu-Neng Chuang, Songchen Li, Guanchu Wang, Duy Le, Hongye Jin, Vipin Chaudhary, Zhaozhuo Xu, Zirui Liu, and Xia Hu. Kv cache compression, but what must we give in return? a comprehensive benchmark of long context capable ap- proaches. arXiv preprint arXiv:2407.01527, 2024. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H _2 o: Heavy-hitter oracle for efficient generative inference of large language models. arXiv preprint arXiv:2306.14048, 2023. Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom: Low-bit quanti- zation for efficient and accurate llm serving. Proceedings of Machine Learning and Systems, 6:196–209, 2024. 11KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache A. Detailed Implementations In this section, we present the algorithm for KIVI as discussed in Section 3.3. Specifically, we provide the pseudocode for KIVI when calculating the attention output in the prefill and decoding phases. Algorithm 1: The KIVI Prefill & Decoding Algorithm parameter: group size G, residual length R procedure Prefill: Input: X ∈ Rlprompt×d XK = XWK, XV = XWV XVg = XV [: lprompt − R], XVr = XV [lprompt − R :] Q(XVg ) ← GroupQuant(XVg , dim=token, numGroup=d//G) Q(XKg ), XKr ← KeyQuant(XK) KV cache ← Q(XKg ), XKr , Q(XVg ), XVr return XK, XV end procedure Decoding: Input: KV cache, t ∈ R1×d tQ = tWQ, tK = tWK, tV = tWV Q(XKg ), XKr , Q(XVg ), XVr ← KV cache XKr ← Concat([XKr , tK], dim=token) XVr ← Concat([XVr , tV ], dim=token) if len(XKr ) = R then Q(XKr ), _ ←KeyQuant(XKr ) Q(XKg ) ← Concat([Q(XKg ), Q(XKr )], dim=token) XKr ← empty tensor. end if len(XVr ) > Rthen Q(XV ′r ) ← GroupQuant(XVr [: −R], dim=token, numGroup = d//G) Q(XVg ) ← Concat([Q(XVg ), Q(XV ′r )], dim=token) XVr ← XVr [−R :] end A ← Concat([tQQ(XKg )⊤, tQX⊤ Kr ], dim=token) Ag = Softmax(A)[: −R], Ar = Softmax(A)[−R :] tO ← AgQ(XVg ) + ArXVr KV cache ← Q(XKg ), XKr , Q(XVg ), XVr return tO end function KeyQuant(XK ∈ Rl×d): r = l%R, XKg = XK[: l − r], XKr = XK[l − r :] Q(XKg ) ← GroupQuant(XKg , dim=channel, numGroup=l//G) return Q(XKg ), XKr end 12KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache B. NIAH Setting We largely follows the passkey retrieval prompt template of Mohtashami and Jaggi (2023) but using 7-digit passkey and Paul Graham Essays2 as the background filler, as set forth in Arize-ai and Reid et al. (2024): There is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I will quiz you about the important information there. <prefix filled by Paul Graham Essays> The pass key is <7-DIGIT PASS KEY>. Remember it. <7-DIGIT PASS KEY> is the pass key. <suffix filler> What is the pass key? The pass key is C. More Ablation Results In our efficiency evaluation, we observe that with a residual length of 32, KIVI achieves a significantly higher memory compression rate, which in turn leads to increased throughput. Additionally, our ablation study reveals that changing the residual length from 128 to 32 does not result in a substantial performance gap. We demonstrate KIVI with a residual length of 32 across all benchmark datasets. As shown in Tables 6 and 7, KIVI with a residual length of 32 also delivers performance comparable to that of the 16-bit full model. Table 6: Performance comparison between 16bit, KIVI-2 (2bit) / KIVI-4 (4bit) with residual length 128 and 32 across various models. R32 stands for residual length 32. Model CoQA TruthfulQA GSM8K Llama-2-7B 16bit 63.88 30.76 13.50 KIVI-2 R128 63.05 33.95 12.74 KIVI-2 R32 62.85 33.01 13.57 Llama-2-13B 16bit 66.37 29.53 22.67 KIVI-2 R128 66.23 29.84 20.77 KIVI-2 R32 66.57 29.35 20.62 Falcon-7B 16bit 59.83 23.20 4.55 KIVI-4 R128 59.67 22.58 4.47 KIVI-4 R32 59.73 22.96 3.94 KIVI-2 R128 57.48 24.98 3.41 KIVI-2 R32 57.50 25.70 2.20 Mistral-7B 16bit 67.40 30.45 38.36 KIVI-2 R128 66.35 32.17 36.01 KIVI-2 R32 65.90 31.21 34.34 D. More Experimental Results We present additional results using Llama3-8B, Mistral-7B-v0.2, and LongChat-7B-v1.5 in LongBench, which can be found in Table 8, Table 9 and Table 10, respectively. We also show result of Needle-in-a-Haystack Test in Figure 4. The settings largely follow the format of the original passkey retrieval task (Mohtashami and Jaggi, 2023) while including some modern modifications set forward by Arize-ai and the technical report of Gemini 1.5 (Reid et al., 2024). 2https://paulgraham.com/articles.html 13KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Table 7: Performance evaluation of KIVI with residual length 128 and 32 on various models across a range of benchmarks in LongBench. R32 stands for residual length 32. Model Qasper QMSum MultiNews TREC TriviaQA SAMSum LCC RepoBench-P Average Llama2-7B 16bit 9.52 21.28 3.51 66.00 87.72 41.69 66.66 59.82 44.52 KIVI-2R128 9.31 20.50 1.14 66.00 87.42 42.71 66.88 60.23 44.27 KIVI-2R32 9.26 20.53 0.97 66.00 87.42 42.61 66.22 59.67 44.08 Llama2-13B 16bit 9.32 21.38 3.71 70.00 87.87 43.55 66.61 56.42 44.85 KIVI-2R128 8.58 20.69 6.19 69.50 87.78 44.30 65.08 55.46 44.69 KIVI-2R32 8.38 20.74 7.01 69.50 87.78 44.43 64.89 55.31 44.75 Llama2-7B-Chat 16bit 19.65 20.54 26.36 63.00 84.28 41.12 59.75 52.93 45.95 KIVI-2R128 19.32 20.46 25.48 63.00 84.84 40.60 58.71 52.97 45.67 KIVI-2R32 19.10 20.08 25.33 63.00 85.04 39.80 57.91 52.38 45.33 Llama2-13B-Chat 16bit 24.18 20.37 25.69 67.50 86.90 42.18 50.23 50.64 45.96 KIVI-2R128 23.59 20.76 25.25 67.50 87.17 41.56 49.93 48.45 45.52 KIVI-2R32 23.56 20.90 25.45 67.50 87.42 41.40 48.93 48.81 45.49 Falcon-7B 16bit 1.48 2.35 11.09 13.00 5.84 2.44 23.86 9.69 8.71 KIVI-4R128 1.04 2.41 11.98 13.00 5.84 2.36 23.72 9.92 8.78 KIVI-4R32 1.03 2.45 11.99 13.50 5.84 2.46 23.88 9.95 8.88 KIVI-2R128 1.98 3.61 6.78 10.00 6.24 2.73 22.18 10.12 7.95 KIVI-2R32 2.28 3.23 6.73 10.00 6.31 2.88 22.71 10.45 8.07 Mistral-7B 16bit 8.12 19.98 19.99 67.50 89.80 41.69 66.59 58.99 46.58 KIVI-2R128 6.92 19.71 17.92 66.50 89.63 41.66 65.52 58.99 45.85 KIVI-2R32 6.84 19.81 17.20 66.50 89.63 42.82 65.13 58.06 45.74 Table 8: The results of Llama-3-8B-Instruct with KIVI on LongBench. The model has 8K context length and applies group query attention, which uses 8 heads for KV cache instead of the full 32 heads. We use a 32 group size and 128 residual length for both KIVI-2 and KIVI-4. The baseline is of full precision. NarrativeQA Qasper MultiFieldQA HotpotQA MuSiQue 2WikiMQA GovReport QMSum Baseline 21.71 44.24 44.54 46.82 21.49 36.42 30.03 22.67 w./KIVI-2 21.35 43.17 44.49 46.79 20.56 37.05 29.98 22.07 w./KIVI-4 21.01 44.83 44.60 46.96 21.43 36.48 30.22 22.44 MultiNews LCC RepoBench-P TriviaQA SAMSum TRec PR Avg Baseline 27.79 57.00 51.22 90.23 42.53 74.50 67.00 45.21 w./KIVI-2 27.77 50.84 46.65 90.54 42.26 74.50 67.50 44.37 w./KIVI-4 27.97 57.36 52.03 90.33 42.97 74.50 66.50 45.31 Table 9: The results of Mistral-7B-Instruct-v0.2 with KIVI on LongBench. The model has 32K context length and applies group query attention, which uses 8 heads for KV cache instead of the full 32 heads. We use a 32 group size and 128 residual length for both KIVI-2 and KIVI-4. The baseline is of full precision. NarrativeQA Qasper MultiFieldQA HotpotQA MuSiQue 2WikiMQA GovReport QMSum Baseline 21.02 29.41 47.13 36.53 19.13 21.76 32.59 23.99 w./KIVI-2 20.61 28.73 44.88 35.47 17.95 20.68 32.55 23.65 w./KIVI-4 20.97 29.41 46.52 36.25 19.53 21.66 32.97 24.06 MultiNews LCC RepoBench-P TriviaQA SAMSum TRec PR Avg Baseline 27.09 53.49 51.40 86.23 43.04 71.00 89.33 43.54 w./KIVI-2 26.54 53.03 51.16 86.00 43.34 71.00 80.83 42.43 w./KIVI-4 26.89 53.33 51.41 86.23 43.34 71.00 89.42 43.53 14KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Table 10: The results of LongChat-7B-v1.5-32K with KIVI on LongBench. The model has 32K context length. We use a 32 group size and 128 residual length for both KIVI-2 and KIVI-4. The baseline is of full precision. NarrativeQA Qasper MultiFieldQA HotpotQA MuSiQue 2WikiMQA GovReport QMSum Baseline 20.65 29.42 43.15 33.05 14.66 24.14 30.85 22.84 w./KIVI-2 20.79 28.69 41.02 32.91 13.82 23.00 30.47 22.59 w./KIVI-4 20.49 28.90 43.24 33.07 14.66 24.86 31.40 22.84 MultiNews LCC RepoBench-P TriviaQA SAMSum TRec PR Avg Baseline 26.55 54.83 58.94 83.99 40.75 66.50 30.50 38.72 w./KIVI-2 26.28 54.11 57.62 83.19 41.28 66.50 32.25 38.30 w./KIVI-4 26.52 54.06 58.77 83.88 40.62 67.00 31.50 38.79 15",
      "meta_data": {
        "arxiv_id": "2402.02750v2",
        "doi": "10.13140/RG.2.2.28167.37282",
        "authors": [
          "Zirui Liu",
          "Jiayi Yuan",
          "Hongye Jin",
          "Shaochen Zhong",
          "Zhaozhuo Xu",
          "Vladimir Braverman",
          "Beidi Chen",
          "Xia Hu"
        ],
        "published_date": "2024-02-05T06:06:47Z",
        "pdf_url": "https://arxiv.org/pdf/2402.02750v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper identifies the KV cache as a significant memory and speed bottleneck in LLM inference, especially with larger batch sizes and longer context lengths. It conducts a comprehensive study on KV cache element distribution, finding that key cache should be quantized per-channel due to outlier magnitudes, while value cache should be quantized per-token due to its role in attention output and sparsity. Based on these insights, the authors propose KIVI, a tuning-free asymmetric 2-bit KV cache quantization algorithm with a hardware-friendly implementation. KIVI reduces peak memory usage by 2.6x for Llama, Falcon, and Mistral models with minimal quality loss, enabling up to 4x larger batch sizes and achieving 2.35x to 3.47x throughput on real LLM inference workloads.",
        "methodology": "The methodology involves a preliminary study of KV cache element distribution, leading to the insight that key cache requires per-channel quantization and value cache requires per-token quantization. KIVI implements a 2-bit asymmetric quantization. To accommodate the streaming nature of auto-regressive inference, the KV cache is split into two parts: a 'grouped' part, which is quantized (key cache per-channel, value cache per-token), and a 'residual' part, which retains the most recent tokens in full precision, acting as a sliding window. During decoding, newly arrived tokens are added to the residual part, and once it reaches a certain length, it's quantized and concatenated with the grouped part. The system includes hardware-friendly implementations, fusing dequantization with matrix multiplication using CUDA and implementing group-wise quantization kernels in Triton.",
        "experimental_setup": "KIVI was evaluated using Llama/Llama-2 (7B, 13B), Falcon (7B), and Mistral (7B) models, with additional results for Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.2, and LongChat-7B-v1.5. Key hyperparameters included a group size of 32 and a residual length of 128 (with ablation studies on these parameters). Performance was assessed on normal context length generation tasks from LM-Eval (CoQA, TruthfulQA, GSM8K) and long context tasks from LongBench (Qasper, QMSum, MultiNews, TREC, TriviaQA, SAMSum, LCC, RepoBench-P), with max sequence lengths up to 8192. A Needle-in-a-Haystack (NIAH) task was used to evaluate long context retrieval. Efficiency was measured on a single NVIDIA A100 GPU (80GB) using synthesized workloads based on ShareGPT, comparing peak memory usage and throughput against FP16 baselines.",
        "limitations": "The 2-bit KIVI quantization may lead to a large accuracy drop for models like Falcon-7B (which uses multi-query attention and already has a highly compressed KV cache), necessitating 4-bit quantization to maintain accuracy. The current system implementation indicates that throughput speed-up could be further enhanced by fusing the KV cache quantization process with previous operations. The approach relies on maintaining a full-precision KV cache sliding window (residual part) for local relevant tokens, which is crucial for desirable performance on hard tasks, implying that a purely 2-bit quantization without this window would severely impact accuracy.",
        "future_research_directions": "Future work includes further optimizing the implementation to reduce the overhead of the quantization process during both the prefill and decoding phases. Specifically, the authors mention that the speed-up could be significantly increased by more tightly fusing the KV cache quantization process with previous operations."
      }
    },
    {
      "title": "MiniCache: KV Cache Compression in Depth Dimension for Large Language Models",
      "abstract": "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance.",
      "full_text": "MiniCache: KV Cache Compression in Depth Dimension for Large Language Models Akide Liu1 Jing Liu1 Zizheng Pan1 Yefei He2 Gholamreza Haffari1 Bohan Zhuang1,2† 1ZIP Lab, Monash University, Australia 2ZIP Lab, Zhejiang University, China Abstract A critical approach for efficiently deploying computationally demanding large lan- guage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value states of previously generated tokens, significantly reducing the need for repetitive computations and thereby lowering latency in autoregressive generation. However, the size of the KV cache grows linearly with sequence length, posing challenges for applications requiring long context input and extensive sequence generation. In this paper, we present a simple yet effective approach, called MiniCache, to compress the KV cache across layers from a novel depth perspective, significantly reducing the memory footprint for LLM inference. Our approach is based on the observation that KV cache states exhibit high similarity between the adjacent layers in the middle-to-deep portion of LLMs. To facilitate merging, we propose disentangling the states into the magnitude and direction components, interpolating the directions of the state vectors while preserving their lengths unchanged. Furthermore, we in- troduce a token retention strategy to keep highly distinct state pairs unmerged, thus preserving the information with minimal additional storage overhead. Our Mini- Cache is training-free and general, complementing existing KV cache compression strategies, such as quantization and sparsity. We conduct a comprehensive evalua- tion of MiniCache utilizing various models including LLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks, demonstrating its exceptional performance in achieving superior compression ratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit MiniCache achieves a remarkable compression ratio of up to 5.02×, enhances inference throughput by approximately 5×, and reduces the memory footprint by 41% compared to the FP16 full cache baseline, all while maintaining near-lossless performance. Project is available at https://minicache.vmv.re 1 Introduction Large Language Models (LLMs), exemplified by the GPT series [ 1, 2, 3] and the LLaMA series [4, 5, 6], have emerged as pivotal innovations within the artificial general intelligence, significantly enhancing the capabilities of natural language processing. However, these models are meticulously trained using extensive computational resources [7] and massive datasets [8], which enables them to produce text that effectively mimics human writing styles and conducts complex reasoning analysis, yet raises the challenge of efficient deployment and serving. Within the inference framework of †Corresponding author. Email: bohan.zhuang@gmail.com Preprint. Under review. arXiv:2405.14366v2  [cs.CL]  7 Sep 2024(a)Cross-layer KV cachesimilarity (b)Merged layersvs EMscore on GSM8K Layer𝑙−1 Layer𝑙−2  ... Layer2 Layer1 LMHead Input Layer𝑙−3 Pruning/Quant. K VKVCacheCompression T Decoding Cross Layer Merging K V QK V Attention Decoding KVCacheCompression QK V Attention T+1 Prevs. MiniCache (c)Comparisonbetween MiniCacheand previous methods ...... CosineSimilarity  T T+1 Number of Layers Merged on LLaMA-3-70B Figure 1: Overview of our MiniCache strategy and example results: (a) shows the observation that the KV cache states between two adjacent layers are highly similar, particularly across the middle to deep layers. The x-axis uses index/2 to represent the similarities for each pair of layers. (b) compares the performance of MiniCache, and the mean baseline, which simply averages the KV caches of two layers, using the LLaMA-3-70B model [6] on the GSM8K dataset [10]. MiniCache, which begins merging from the half-layer depth, achieves near-lossless performance. (c) highlights the primary difference between MiniCache and previous approaches. MiniCache investigates the inter-layer redundancy of KV caches along the depth dimension of LLMs, an aspect overlooked by intra-layer-based methods. Here, T refers to the last timestamp of pre-filling, and T + 1 des to the first timestamp of decoding. LLMs, KV caches [9] are crucial for storing pre-computed keys and values, thus avoiding repeated calculations over the preceding context and substantially enhancing LLMs’ deployment efficiency. However, the increasing demand for longer sequence lengths results in voluminous cached states, leading to significant memory consumption during generation. For instance, a 175B GPT-3 model [2], with a batch size of 64 and a sequence length of 4,096 tokens (both prefilled and generated), requires approximately 1,208GB of GPU memory. This requirement is3.45× greater than the memory used to store the model’s weights. In this context, KV cache compression is of paramount importance due to its clear benefits: 1) it largely reduces the memory footprint allowing for faster generation and larger batch serving; 2) it significantly lowers the cost per token, demonstrating substantial commercial benefits. Existing KV cache compression efforts can be roughly categorized into two types, namely quantization and sparsity. The quantization approaches [11, 12] propose storing the KV states in low-bit numerical values. Typically, FlexGen [13] demonstrates that 4-bit KV cache quantization can achieve lossless performance. In contrast, sparsity-driven methods aim to retain only the salient tokens while evicting the rest, either heuristically [14, 15] or adaptively [16]. Some approaches [11] explore the intersection of these two types, by assigning high-bit to salient tokens and extremely low-bit to the rest of the tokens, achieving more aggressive memory gain. Despite these innovations, existing literature merely consider the intra-layer redundancy, while neglecting another important complementary direction – the inter-layer redundancy, as illustrated in the Figure 1(c). Our analysis begins by exploring the redundancy of KV cachesalong the depth dimension, as shown in Figure 1(a). We observe that KV cache states exhibit high similarity between neighbouring layers in 2the middle-to-deep portions of LLMs. This intriguing property suggests that states paired by position between adjacent layers can be accurately merged into a single state space with a strong performance guarantee, as illustrated in Figure 1(b). This approach significantly reduces the memory footprint without the need to retain individual states for each attention layer. Note that these observations are pertinent to dynamic inference strategies such as mixture-of-depths [17] and layer-wise early exiting [18, 19], which optimize computational paths by skipping non-critical layers to enhance training and inference efficiency. Furthermore, layer pruning methods [20] highlight considerable redundancy in deeper layers. However, despite these advancements, the redundancy of KV caches along the depth dimension has largely been overlooked. In this paper, we propose MiniCache, a simple yet effective cross-layer KV cache compression method aimed at advancing the inference efficiency of LLMs. MiniCache consists of two essential components. Firstly, we introduce an accurate cache merging strategy, employing a reparameteri- zation of state vectors that decompose them into the magnitude and direction components, akin to weight normalization [21]. This approach allows for effective interpolation of the directional compo- nent in polar coordinates while preserving the original state norms to retain as much information as possible. This interpolation refers to the cross-layer merging as shown in the Figure 1(c). Secondly, we recognize that a small subset of state pairs, characterized by low similarities but carrying largely distinct semantic meanings, are unsuitable for inter-layer merging. To address this, we propose a token retention strategy to minimize performance degradation, which involves separately retaining these outlier pairs. Our framework is notably memory-efficient, requiring storage for only a single high-dimensional directional component, along with minimal extra memory overhead. The over- head consists of a few unmergeable tokens and their corresponding indexes, as well as token-wise magnitudes to accurately restore the original states. We conduct extensive experiments with representative LLMs, including Mixtral-8x7B [ 22], Phi- 3-Mini [23], and LLaMA-3 [ 6] 8B and 70B, respectively. Our method is benchmarked across a diverse range of question answering and generation datasets [24, 25, 26, 27, 28, 29, 30, 31] using the lm-eval-harness [32]. Additionally, we evaluate our results on LongBench [33] for long-sequence generation. The results demonstrate that MiniCache can reduce the memory footprint required for LLM inference by up to 41%, while simultaneously enhancing throughput by approximately 5× compared to fully cached baseline, clearly surpassing existing methods [11, 12, 14, 15]. Our contributions are summarized as follows: • We introduce MiniCache, a simple yet highly effective framework for KV cache compression. MiniCache pioneers the exploration of KV cache compression along the depth dimension, thereby significantly expanding its capabilities. • We observe a fascinating characteristic of cross-layer KV cache states: high similarity between adjacent layers in the middle to later stages of LLMs. Additionally, we find that not all state pairs are suitable for merging. • We propose an accurate and memory-efficient method for cross-layer cache merging, comprising a reparameterization strategy and a token retention mechanism. Our method complements existing KV cache compression approaches, further enhancing LLM serving efficiency. • Our MiniCache performs favourably against the state-of-the-art methods. Notably, our 4-bit MiniCache achieves a strong compression ratio of up to5.02×, 5× higher inference throughput, and 41% memory reduction compared to the FP16 full cache baseline with near-lossless performance. 2 Related Work Efficient inference for LLMs. Large Language Models (LLMs) are constrained by considerable computational and memory requirements during inference, particularly in resource-constrained environments. To mitigate these challenges, a variety of efficient inference techniques have been developed. For instance, dynamic inference methods [ 18, 34, 35, 36], represented by mixture-of- experts (MoE) [37, 38, 39, 40, 41], adaptively select specific sub-structures of the model during the inference process based on the input data, significantly improving inference efficiency while keeping model capacity. Techniques like Multi-Query Attention [42, 43], Kernel-driven attentions [44, 45, 46, 47], and low-rank attentions [41, 48, 49, 50] approximate the functionality of traditional attention mechanisms but with more efficient implementations. Quantization strategies [51, 52, 53, 54] 3involve converting the model’s weights and activations into a low bit-width format, thereby reducing memory footprint and computational intensity. Sparsification approaches [14, 15, 55, 56] eliminate unnecessary elements in both model weights and token representations, further enhancing efficiency. Some closely related works, such as MoD [17] and LayerSkips [19], considered the dynamic inference nature to ignore unimportant layers according to input. However, these methods require an additional fine-tuning process or carefully designed pre-training stages, which reduces the adaptivity of these methods. MiniCache relies on inter-layer similarity observations to perform cross-layer merging, significantly reducing memory demand. Model merging. Merging compression involves the aggregation of a model’s parameters and activations at various granularities. This process enhances the efficiency of inference in large models and facilitates huge redundancy [57]. Linear Mode Connectivity (LMC) [58] enables the fine-tuning of models from a shared pre-trained base. Commonly, weight averaging [ 59] is employed as an efficient technique to perform merge compression. Notably, Model Soup [60] utilizes linear averaging in this context. Advanced methods like TIES Merging [61], Model Breadcrumbs [62], and DARE [63] further enhance this process by sparsifying and combining model parameters, enabling the merging of models without sacrificing performance capabilities. Additionally, Spherical Linear intERPolation (SLERP) [64] extends beyond simple weight averaging by interpolating between model parameters. The Fisher information matrix [65] and RegMean-based methods [66] further optimize merges to produce ideal weights, minimizing the ℓ2 distance to generation outputs while preserving the privacy of the training data. However, most existing works focus on merging model parameters, with the concept of depth-wise mergeability not being thoroughly explored in prior research. MiniCache focuses on the KV cache token merging in the depth dimensional of LLMs. 3 Motivation In the below, we present our new observations in a novel cross-layer perspective. LLama 2 7BLLama 2 30BLLama 3 8BMixtral 8x7B Models 0 10 20 30 40 50Exact Match (%) Baseline Mean KV (a) Simple average baseline vs. full cache on GSM8K 0 20 40 60 80 100 T okens Index 0.6 0.4 0.2 0.0 0.2 0.4 0.6 Cosine Similarity Layer 16 - 17 Layer 18 - 19 Layer 20 - 21 Layer 22 - 23 Layer 24 - 25 Layer 26 - 27 Layer 28 - 29 Layer 30 - 31 (b) Pairwise similarity in adjacent layers KV cache MathQA OpenBookQA PiQA RTE Winogrande 0.00.20.40.60.8 MiniCache Baseline Mean (c) Benchmark on five QA datasets Figure 2: Overall of our explorations and observations : (a) shows the strong baseline by performing average merging on the KV cache. (b) shows the pairwise similarity of cache states between adjacent layers. (c) compares the MiniCache, simple average, and full cache baseline across five different datasets. 3.1 Cross-Layer Redundancy in KV Cache Prior studies have revealed the ineffectiveness of middle-to-deep layers in LLMs [20]. Thus, layer- wise early exiting in this case can effectively avoid the redundant computation with minor effect on LLM performance [19, 67]. Inspired by this, we explore a layer-wise merging of KV cache in LLMs, starting with a simple baseline by averaging full tokens across adjacent layers. We provide our key observations as follows. Observation 1: KV cache shares a high similarity between adjacent layers. Based on LLaMA-3- 70B [6], we conduct zero-shot inference on the validation sets of three widely recognized benchmarks: COQA [68], GSM8K [10] and TruthfulQA [69]. In general, we find that KV cache in the shallow layers exhibits low similarity, whereas those in the middle-to-deep layers are very similar to one another based on angular distance, as shown in Figure 1(b). Next, we merge the KV cache across 4adjacent layers by conducting five-shot inference with LLaMA-2-7B, LLaMA-2-13B [5], LLaMA- 3-8B [6], and Mixtral-8x7B [22] on GSM8K [10]. Specifically, starting from the middle layer of each model, we merge the KV cache in the adjacent two layers. As shown in Figure 2(a), we observe a favourable performance for different LLMs, which reveals the huge potential for efficiency improvement by sharing the KV cache across adjacent layers during LLM decoding. Observation 2: Not all tokens are equally important to merge, a few distinct pairs require retention. Recent works [15, 16] in KV cache compression have found that keeping a few salient tokens at each layer, which contribute the majority of attention scores, could be sufficient to maintain LLM performance. In our case, we speculate that certain token pairs in adjacent layers also exhibit outlier behaviours, showing strong semantic differences that make them unsuitable for merging. Based on COQA [ 68] and LLaMA-2-7B [ 5], we investigate the similarities at the level of token pairs. As shown in Figure 2(b), we find a significant portion of token pairs share high similarities across adjacent layers. However, we observe a few outlier pairs, such as indices 0 and 15, with large margins of difference. We consider these tokens non-mergeable due to their significant differences. We also show that merging these distinct tokens results in performance degradation, corresponding to γ = 0 row in Table 2. Thus, while cross-layer merging is a promising strategy for reducing memory burdens, it must be implemented with careful consideration of token-level similarities to ensure optimal performance, as shown in Figure 2(c). 4 Method In this section, we introduce our MiniCache, a simple yet effective method aimed at trimming the KV cache redundancy in the depth dimension. This framework exploits the high similarity of KV cache states between adjacent layers and consists of two primary components: a reparameterization-based merging strategy and a token retention mechanism. The merging strategy compresses the KV cache states in adjacent layers to aggregate them into a single shared memory space, beginning from the middle of the model. The token retention mechanism mitigates information lost by retaining the highly distinct state pairs with minimal additional memory cost. With the merged cache, retention tokens, and magnitudes, we can accurately restore the original cache states for token decoding. 4.1 Cross-Layer Compression Our method commences with the identification of an optimal starting layer S. Observations in Section 3.1 indicate that the KV cache from middle-to-deep layers consistently exhibits patterns of high similarity across adjacent layers. Consequently, we select the starting layer from the middle of the LLM, specifically S = L/2. From this layer onward, the KV pairs are assumed to be sufficiently similar across adjacent layers to warrant their consolidation. Central to this approach is a merge function, F, which is designed to integrate the KV caches of consecutive layers into a single, unified cache. We define x as the vectorized cache state of a single token, where the superscript indicates the layer index and the subscripts k and v denote the keys and values, respectively. Specifically, for a pair of key/value tokens at the same position in layers l and l − 1, the merged cache is computed as cl,l−1 k = F(xl k, xl−1 k ), cl,l−1 v = F(xl v, xl−1 v ). (1) This consolidation process effectively eliminates the need to store and process the original memory- intensive keys and values in each layer independently. Instead, it approximates a shared cache across the adjacent layers. 4.2 KV Cache Merging and Restoration Reparameterization-based cache merging. To perform the pairwise merging, one solution is to directly average a pair of KV tokens, analogous to model merging [60, 61]. However, we observe that direct averaging can cause significant information loss. We conjecture that the distance between activations can be larger than that of weights due to the presence of outlier activation channels with extremely large magnitudes in LLMs [70, 71], while weights typically have relatively quite small magnitudes. A potential method to compensate for this information loss is to project from c to xl−1 and xl, then rescale the projected vectors based on their relative magnitudes to exactly restore the 5(a)Cross-layer Compression KV Store𝐶 Keep Rescale Recover (b)Restoration𝑙−1𝑙 𝑙−1 𝑙 Keep × Fetch𝐶\tforlayer 𝑙and𝑙−1KVcachecompressionat layer𝑙 original KV Cache merged KV Cache retentiontokencachemagnitudemergeoperation Figure 3: The illustration of the proposed methodMiniCache. (a) depicts the cross-layer compression process. We fetch the KV caches, from layers l and l − 1, and merge them into shared states via Eq. (3). Additionally, we compute the ℓ2 norm for the caches to obtain their magnitudes. Furthermore, we select unmergable tokens for retention, then store merged cache, retention tokens, and magnitudes at layer l in C. (b) illustrates the restoration process for layers l and l − 1, which includes magnitude rescaling in Eq. (2) and retention token recovery. original states. However, this approach requires extensive additional storage and computations; for example, restoring xl−1 needs both c and xl, which undermines the benefits of cache merging. To efficiently merge token pairs, we draw inspiration from weight normalization [21], which disentangles model parameters into the magnitude and direction components to accelerate the convergence of stochastic gradient descent. Additionally, we take cues from DoRA [72], which employs a similar way to resemble the learning behavior of parameter-efficient fine-tuning compared to full fine-tuning. In our case, the reparameterization can be formulated as follows: ˆxl = el,l−1 · ∥xl∥ ∥el,l−1∥, ˆxl−1 = el,l−1 · ∥xl−1∥ ∥el,l−1∥, (2) where e is the directional vector. This decomposition ensures that el,l−1 ∥el,l−1∥ is a unit vector, and allows the restored states to match the ℓ2 norm of the original states, thereby preserving the cache’s information as much as possible. The restoration is shown as Figure 3(b). For brevity, we omit the subscripts k and v, as keys and values are decomposed in the same way. For estimating the directional component el,l−1, we follow SLERP [64], which adaptively handles the interpolation, which often resembles rotation-like transformations. The choice of SLERP as the merging function is strategic, as it facilitates interpolation along the shortest path on the unit sphere between two high-dimensional vectors, thereby preserving their geometric integrity, which refers to merge operation in Figure 3(a). This is crucial for maintaining the semantic and syntactic properties of the KV caches. The formula for SLERP in our context is: el,l−1 = sin((1 − t)Ωl,l−1) sin(Ωl,l−1) · xl−1 ∥xl−1∥ + sin(tΩl,l−1) sin(Ωl,l−1) · xl ∥xl∥, (3) where Ωl,l−1 = arccos \u0010 xl·xl−1 ∥xl∥∥xl−1∥ \u0011 represents the angle between vectors xl and xl−1, and sin(·) is the sine function. t is an interpolation hyperparameter that adjusts the relative influence of each vector on the resulting direction, tailored to the layer depth and specific characteristics of the KV pairs. Note that when we set t = 0.5, it will become an average merging along the geometry surface, which we consider special cases in Eq. (A). The merged cache for each token pair is then a concatenation of the directional vector, magnitude and Ωl,l−1, denoting as cl,l−1 = [el,l−1, ∥xl−1∥, ∥xl∥, Ωl,l−1], cached components as shown in Figure 3(a). Note that apart from storing the merged directional vector, we only need to store additional token-wise magnitude and angle scalars, which is memory efficient. In this way, we achieve substantial memory efficiencies through reduced redundancy while ensuring the retention of the critical functional characteristics of the original KV pairs across transformer layers. Unmergeable token retention. Highly distinct pairs are sensitive to merging operations, leading us to propose unmergeable token retention, as shown in Figure 3(a). Despite the high similarity between KV cache states across neighbouring layers, a few sensitive distinct pairs remain that are significantly 6difficult to merge and share. Aligned with previous studies, these distinct tokens carry substantial semantic meanings [15, 16]. We observe that merging sensitive tokens, which results in the loss of layer-specific information, can lead to significant performance degradation. Therefore, it is crucial to properly disentangle the shared and unique information between adjacent layers. To address this issue, we designed a token retention strategy to selectively retain tokens that cannot be merged based on their angular distance, defined as: d(xl, xl−1) = 1 π Ω. For the KV caches, the minimum and maximum angular distances are determined to identify the unmergeable tokens. The set of required token indices to keep, I, is obtained by: I = {i | di < dmin + (dmax − dmin) · γ}, (4) where γ is a predefined hyperparameter that controls the retention threshold. The tokens with indices in I are retained and not compressed during the merge, which ensures that performance does not decline by preventing the loss of unmergeable tokens. Next, let X ∈ Rn×h be either the key or value cache at one attention layer, where n denotes the number of tokens and h is the number of hidden dimensions, and E ∈ Rn×h be the shared KV cache states. For each pair of neighbouring two layers, the unmergeable tokens are selected along with the token dimension by Rl = Xl[I], Rl−1 = Xl−1[I], then restoring to our compressed caches by ˆXl[I] = Rl, ˆXl−1[I] = Rl−1, as shown in Figure 3(b). Overall, we share the final cache for the two layers as Cl,l−1 = [El,l−1, Rl, Rl−1, ∥Xl−1∥, ∥Xl∥, I]. This cache includes the shared KV cache states, retention of unmerged tokens, magnitude vectors for each layer, and the token-keeping index, respectively. These additional components are quite lightweight. Thus compared to full-layer caches, our method remains memory-efficient, as discussed in Sec. 4.3. Cache restoration. After obtaining the shared cacheCl,l−1, we further need to approximately restore the original cache states for the current token decoding, as shown in Fig. 3(b). Specifically, to restore Xl, we first rescale the directional shared states with the corresponding magnitude vector along the token dimension, denoted as El,l−1∥Xl∥. Subsequently, we perform retention token recovery by placing the sensitive tokens according to their token indices. 4.3 Efficiency Discussion Compression efficiency. We primarily analyze our memory efficiency in terms of the number of tokens used. Next, let r be the number of layers and and b is the batch size, s and n are input and output sequence length respectively. We consider the FP16 storage for KV cache. The full cache memory usage is given by 4brh(s + n). In our study, we begin merging layers from the middle to the deeper layers, consolidating the KV cache states of every two layers into a single shared state space. As a result, we effectively reduce the GPU memory usage in the decoding inference to 3brh(s + n), demonstrating a significant compression rate. Restoration efficiency. We then analyze the additional memory cost incurred during the restora- tion process, which During the magnitude rescaling phase, we save an additional norm vector for the corresponding layers in the KV cache. It is important to note that the norm vector is in the shape of Rb×s×1, which has a single channel dimension compared to the fully ranked original KV states. Additionally, we suppose that the retention threshold can be set to 0.05. Therefore, we have brh(0.05(s + n)) tokens retained without compression. Finally, our overall memory requirement is given by (3.1h + 2)br(s + n). The detailed derivation is shown in the Appendix E. 5 Experiments We demonstrated that our MiniCache can perform merging compression on the latter half of the layers of LLMs with minimal performance degradation. Implementation details. Our experiments are based on representative model families of LLMs, including a compact LLM Phi-3-Mini [23] and an MoE LLM Mixtral-8x7B [22]. Additionally, we adopt LLaMA-3 [6] 8B and 70B models to explore how our method generalizes to larger LLMs. We sample ten tasks from lm-eval-harness [32], including COPA [24], MathQA [25], OpenBookQA [26], PIQA [27], RTE [28], WinoGrande [29], XSUM [30], and CNN/Daily Mail [31]. We also evaluate long-sequence generation on LongBench [33]. We compare our method with a fully cached baseline, 7and other methods such as round-to-nearest quantization (RTN) [73], SmoothQuant [70] and KIVI [11]. For the proposed MiniCache, we set the interpolation parameter t to 0.6, indicating that the merged results have a smaller rotation angle to the next layer. Furthermore, we set the token retention threshold γ to 0.05, according to the statistics of unmergeable tokens across multiple datasets. In addition to our merging method, we also consider a strong baseline of average merging. For sequential loading of large models, we utilize NVIDIA 4 A100 80GB GPUs, more details refers to Appendix D. Main results. We evaluate MiniCache by merging KV caches across all layers on GSM8K, COQA, and TruthfulQA. The results are shown in Figure 4. In general, we demonstrate the general effective- ness of merging KV caches from middle-to-deep layers across different sized LLMs. Moreover, the proposed MiniCache demonstrates a consistent and significant advantage over the averaging baseline. We also illustrate the performance of merging KV caches across half of the layers with the blue lines, where MiniCache still maintains a robust performance and achieves the best compression ratio. Besides, we find that our method is even more effective for larger LLMs. For instance, based on LLaMA-3-70B, MiniCache shows nearly zero performance drop even with the KV cache in 87.5% of the layers merged on the COQA dataset. This highlights the adaptability and efficiency of our approach in handling large-scale models while ensuring minimal performance degradation. LongBench. We also conduct experiments to evaluate performance and quality in long-sequence gen- eration using the LongBench dataset [33], as shown in Table 1. Our experiments applied MiniCache over several models: LLaMA-2-7B-Chat, LLaMA-2-13B-Chat, Mistral-7B, and Mistral-7B-Instruct. It is important to note that our MiniCache method maintains orthogonality with all existing quan- tization and sparsity (refers to Table A) methods at both the model and token-wise levels. When combined with KIVI-4bit KV cache quantization, our approach achieves a compression rate of5.02×, with minimal impact on accuracy across various challenging long-context generation tasks. The combination of MiniCache and KIVI-4bit KV cache quantization demonstrates significant memory savings without compromising the model’s ability to handle long sequences effectively. This high- Figure 4: Performance comparisons between our proposed MiniCache with the “averaging baseline” and the “unmerged full cache baseline” on multiple datasets with Phi3-Mini, Mixtral-8x7B, LLaMA- 3-8B, and LLaMA-3-70B. More result details are shown in Appendix F. The x-axis indicates the number of layers merged. As more layers are merged, a greater reduction in memory usage is achieved. 8Table 1: Evaluation of different KV cache compression methods on LongBench. MiniCache builds on top of 4-bit KIVI [11] and achieves the best performance with the strongest compression rate. Model Method LCC RepoBench-P PR-en TREC 2wikimqa GovReport MQA-zh AverageCompressionRatio Llama-2-7B-Chat Baseline 58.16 52.19 10.12 64.00 31.12 27.09 10.12 36.41 1xRTN [73] 15.44 8.76 0.79 4.00 0.30 1.93 0.07 4.90 3.21xSmoothQuant [70] 35.31 32.18 0.79 28.75 7.45 11.83 1.68 16.28 2.15xKIVI-2 [11] 49.32 43.71 4.50 63.00 24.07 24.73 10.24 31.51 3.95xMiniCache 58.03 52.01 9.00 64.00 30.58 25.32 10.13 35.44 5.02x Llama-2-13B-Chat Baseline 48.06 50.08 14.25 68.50 13.09 27.76 7.23 32.71 1xRTN [73] 20.89 18.62 0.33 0.00 0.52 1.68 0.16 6.03 3.21xSmoothQuant [70] 32.17 33.86 2.65 48.00 3.53 12.47 0.47 19.16 2.15xKIVI-2 [11] 48.60 48.81 13.50 68.00 14.32 25.70 7.01 32.42 3.95xMiniCache 48.75 48.59 13.00 68.00 14.36 26.57 7.99 32.61 5.02x Mistral-7B Baseline 68.06 60.46 17.71 68.00 10.87 20.09 17.10 37.33 1xRTN [73] 27.98 26.18 3.34 13.00 1.11 2.49 0.45 10.51 3.21xSmoothQuant [70] 40.63 35.14 3.40 30.50 6.03 5.00 4.12 17.55 2.15xKIVI-2 [11] 65.16 58.33 12.43 65.00 11.03 13.22 13.87 33.43 3.95xMiniCache 68.89 60.98 13.92 67.00 10.50 18.06 7.88 35.75 5.02x Mistral-7B-Instruct Baseline 55.51 48.96 60.00 71.00 27.33 32.85 42.74 48.32 1xRTN [73] 32.36 33.23 0.67 1.00 2.25 10.03 2.30 11.55 3.21xSmoothQuant [70] 43.84 38.63 4.79 39.50 10.34 23.61 8.33 24.43 2.15xKIVI-2 [11] 53.13 48.60 47.50 69.00 20.68 29.37 33.88 43.74 3.95xMiniCache 54.79 51.02 64.14 71.00 24.97 31.46 27.54 46.99 5.02x lights the potential of our method to optimize large language models for tasks requiring extensive context, making them more efficient and scalable for real-world applications. Efficiency analysis. To assess the acceleration capabilities of MiniCache, we conduct evaluations based on the methodologies employed in vLLM [74] and KIVI [11]. We generate synthetic workloads derived from ShareGPT, which include real input and output texts from LLM services. The dataset features an average input prompt length of 161 tokens and an output length of 338 tokens. Using the LLaMA-2-7B model on a single 80GB NVIDIA A100 GPU, we benchmark our method in a batch-serving scenario, comparing peak memory usage and throughput among 2-bit KIVI, 4-bit MiniCache, and an FP16 baseline. As illustrated in Figure 5, with a batch size of 128, MiniCache reduces memory usage by 25GB, achieving a 41% memory saving . In terms of throughput, MiniCache outperforms the FP16 baseline by approximately 5×. Additionally, despite utilizing 4-bit quantization, MiniCache benefits from merging and sharing KV caches across adjacent layers, resulting in a 1.29× higher throughput compared to the 2-bit KIVI. These results demonstrate that MiniCache offers a state-of-the-art trade-off between efficiency and performance. 50 100 150 200 250 300 Batch Size 20 30 40 50 60 70 80Peak Memory Usage (GB) Baseline FP16 KIVI 2 MINICache 4 (a) BS. vs. Peak Memory Usage 50 100 150 200 250 300 Batch Size 1000 1500 2000 2500 3000Throughput (tokens/sec)  Baseline FP16 KIVI 2 MINICache 4 (b) BS. vs. Decoding Throughput Figure 5: Memory usage and throughput comparison between our 4-bit MiniCache, 2-bit KIVI, and 16-bit Baseline. MiniCache can achieve higher throughput by enabling a larger batch size while reducing memory footprints via LLaMA-2-7B [5]. 0.3 0.4 0.5 0.6 0.7 Interpolation Parameter t 0.350 0.375 0.400 0.425 0.450 0.475 0.500Exact Match Scores GSM8k 0.0 0.2 0.4 0.6 0.8 1.0 1.2 Normalized Frequency Frequency Figure 6: LLaMA-3-8B [ 6] to experiment on the GSM8K [10]. The right axis is the normalized frequency of the relative magni- tude ratio. Optional t shows a strong correlation with frequency. 6 Ablation Study Table 2: Comparisons of various token retention thresholds γ by LLaMA-2-7B [5] on three benchmarks. γ COQA GSM8K TruthfulQA 0 0.603 0.108 29.813 0.01 0.620 0.126 30.226 0.02 0.630 0.143 33.903 0.05 0.647 0.152 33.213 0.1 0.643 0.152 33.903 1 0.643 0.159 33.743 The effect of interpretation parameter t. We explore the effects of the interpretation parameter t on perfor- mance, particularly in relation to the relative magnitude ratio of adjacent layers, as shown in Figure 6. We maintain all settings constant, starting from layer S = 16 (halfway 9through the layers of LLaMA-3-8B), and vary the interpretation parameter t from 0.3 to 0.7. Our findings reveal several key points. When t = 0.5, the process resembles average merging, which is less effective for cross-layer merging. In contrast, when t = 0.6 is optimal, the merged representation exhibits the most robust performance, while indicating that more information is derived from the second term (xl) of the SLERP. The frequency results also indicate that the high frequencies are clustered around 0.4 and 0.6, corroborating our optimal t. Moreover, there is a strong correlation between the optimalt and the high frequency of the relative magnitude ratio of adjacent layers. This finding provides an opportunity to utilize the relative magnitude ratio to dynamically determine the interpretation parameter t. Dynamic t allows for more flexible weight control in SLERP merging for each layer-wise operation, thereby showing potential for further exploration. The effect of token retention thresholdγ. We investigate the impact of the token retention threshold γ on model performance across the three datasets, as shown in Table 2. A larger t generally means retaining more tokens for improved performance, but this comes at the cost of increased memory demand. The results suggest that setting γ to 0.05 achieves the best balance between performance and efficiency. 7 Conclusion and Future Work This paper presents a pioneering exploration of KV cache compression in the depth dimension, addressing a significant memory bottleneck in LLMs. Our proposed MiniCache offers a simple, effective, and training-free approach to compressing KV caches by leveraging the notable high similarities between KV caches in neighboring layers, starting from the midpoint of LLMs. We have demonstrated that MiniCache can significantly reduce the memory footprint required for LLM inference by up to 41%, while simultaneously enhancing throughput by approximately five times compared to the FP16 baseline. In conclusion, MiniCache significantly advances the field of KV cache compression, offering a state-of-the-art balance between efficiency and performance. Future work will focus on enhancing the compression ratio by cross-multiple-layer merging, developing advanced merging algorithms such as Spherical Cubic Interpolation [ 75], and further optimizing memory usage for large-scale deployments in diverse application scenarios. References [1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., “Language models are few-shot learners,” in NeurIPS, vol. 33, pp. 1877–1901, 2020. [2] OpenAI, J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, and OTHERS, “Gpt-4 technical report,” 2023. [3] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray,et al., “Training language models to follow instructions with human feedback,” in NeurIPS, vol. 35, pp. 27730–27744, 2022. [4] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar,et al., “Llama: Open and efficient foundation language models,” arXiv preprint arXiv:2302.13971, 2023. [5] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale,et al., “Llama 2: Open foundation and fine-tuned chat models,” arXiv preprint arXiv:2307.09288, 2023. [6] “Introducing meta llama 3: The most capable openly available llm to date.”https://ai.meta. com/blog/meta-llama-3/, 2024. Accessed: 2024-05-04. [7] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Rad- ford, J. Wu, and D. Amodei, “Scaling laws for neural language models,” arXiv preprint arXiv:2001.08361, 2020. 10[8] G. Team, R. Anil, S. Borgeaud, Y . Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al., “Gemini: a family of highly capable multimodal models,” arXiv preprint arXiv:2312.11805, 2023. [9] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, J. Heek, K. Xiao, S. Agrawal, and J. Dean, “Efficiently scaling transformer inference,” Proceedings of Machine Learning and Systems, vol. 5, 2023. [10] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al., “Training verifiers to solve math word problems,”arXiv preprint arXiv:2110.14168, 2021. [11] Z. Liu, J. Yuan, H. Jin, S. Zhong, Z. Xu, V . Braverman, B. Chen, and X. Hu, “Kivi: Plug- and-play 2bit kv cache quantization with streaming asymmetric quantization,” arXiv preprint arXiv:2402.02750, 2024. [12] H. Kang, Q. Zhang, S. Kundu, G. Jeong, Z. Liu, T. Krishna, and T. Zhao, “Gear: An efficient kv cache compression recipefor near-lossless generative inference of llm,” arXiv preprint arXiv:2403.05527, 2024. [13] Y . Sheng, L. Zheng, B. Yuan, Z. Li, M. Ryabinin, B. Chen, P. Liang, C. Ré, I. Stoica, and C. Zhang, “Flexgen: High-throughput generative inference of large language models with a single gpu,” in ICML, pp. 31094–31116, PMLR, 2023. [14] Z. Zhang, Y . Sheng, T. Zhou, T. Chen, L. Zheng, R. Cai, Z. Song, Y . Tian, C. Ré, C. Barrett, et al., “H2o: Heavy-hitter oracle for efficient generative inference of large language models,” in NeurIPS, vol. 36, 2024. [15] G. Xiao, Y . Tian, B. Chen, S. Han, and M. Lewis, “Efficient streaming language models with attention sinks,” in ICLR, 2024. [16] S. Ge, Y . Zhang, L. Liu, M. Zhang, J. Han, and J. Gao, “Model tells you what to discard: Adaptive kv cache compression for llms,” ICLR, 2024. [17] D. Raposo, S. Ritter, B. Richards, T. Lillicrap, P. C. Humphreys, and A. Santoro, “Mixture-of- depths: Dynamically allocating compute in transformer-based language models,” arXiv preprint arXiv:2404.02258, 2024. [18] W. Zhou, C. Xu, T. Ge, J. McAuley, K. Xu, and F. Wei, “Bert loses patience: Fast and robust inference with early exit,” in NeurIPS, vol. 33, pp. 18330–18341, 2020. [19] M. Elhoushi, A. Shrivastava, D. Liskovich, B. Hosmer, B. Wasti, L. Lai, A. Mahmoud, B. Acun, S. Agarwal, A. Roman, et al., “Layer skip: Enabling early exit inference and self-speculative decoding,” arXiv preprint arXiv:2404.16710, 2024. [20] A. Gromov, K. Tirumala, H. Shapourian, P. Glorioso, and D. A. Roberts, “The unreasonable ineffectiveness of the deeper layers,”arXiv preprint arXiv:2403.17887, 2024. [21] T. Salimans and D. P. Kingma, “Weight normalization: A simple reparameterization to accelerate training of deep neural networks,” in NeurIPS, vol. 29, 2016. [22] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand, et al., “Mixtral of experts,” arXiv preprint arXiv:2401.04088, 2024. [23] M. Abdin, S. A. Jacobs, A. A. Awan, J. Aneja, A. Awadallah, H. Awadalla, N. Bach, A. Bahree, A. Bakhtiari, H. Behl, et al., “Phi-3 technical report: A highly capable language model locally on your phone,” arXiv preprint arXiv:2404.14219, 2024. [24] M. Roemmele, C. A. Bejan, and A. S. Gordon, “Choice of plausible alternatives: An evaluation of commonsense causal reasoning.,” in AAAI spring symposium: logical formalizations of commonsense reasoning, pp. 90–95, 2011. 11[25] A. Amini, S. Gabriel, S. Lin, R. Koncel-Kedziorski, Y . Choi, and H. Hajishirzi, “Mathqa: Towards interpretable math word problem solving with operation-based formalisms,” inNAACL, pp. 2357–2367, 2019. [26] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal, “Can a suit of armor conduct electricity? a new dataset for open book question answering,” in EMNLP, 2018. [27] Y . Bisk, R. Zellers, R. L. Bras, J. Gao, and Y . Choi, “Piqa: Reasoning about physical common- sense in natural language,” in AAAI, 2020. [28] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman, “Glue: A multi- task benchmark and analysis platform for natural language understanding,” arXiv preprint arXiv:1804.07461, 2018. [29] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y . Choi, “Winogrande: An adversarial winograd schema challenge at scale,” Communications of the ACM, vol. 64, no. 9, pp. 99–106, 2021. [30] S. Narayan, S. B. Cohen, and M. Lapata, “Don’t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization,” arXiv preprint arXiv:1808.08745, 2018. [31] R. Nallapati, B. Zhou, C. Gulcehre, B. Xiang, et al., “Abstractive text summarization using sequence-to-sequence rnns and beyond,” arXiv preprint arXiv:1602.06023, 2016. [32] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, A. Le Noac’h, H. Li, K. McDonell, N. Muennighoff, C. Ociepa, J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron, L. Sutawika, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou, “A framework for few-shot language model evaluation,” 12 2023. [33] Y . Bai, X. Lv, J. Zhang, H. Lyu, J. Tang, Z. Huang, Z. Du, X. Liu, A. Zeng, L. Hou, et al., “Longbench: A bilingual, multitask benchmark for long context understanding,” arXiv preprint arXiv:2308.14508, 2023. [34] L. Del Corro, A. Del Giorno, S. Agarwal, B. Yu, A. Awadallah, and S. Mukherjee, “Skipdecode: Autoregressive skip decoding with batching and caching for efficient llm inference,” arXiv preprint arXiv:2307.02628, 2023. [35] T. Schuster, A. Fisch, J. Gupta, M. Dehghani, D. Bahri, V . Tran, Y . Tay, and D. Metzler, “Confident adaptive language modeling,” in NeurIPS, vol. 35, pp. 17456–17472, 2022. [36] H. Wu and K. Tu, “Layer-condensed kv cache for efficient inference of large language models,” 2024. [37] W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity,” Journal of Machine Learning Research, vol. 23, no. 120, pp. 1–39, 2022. [38] D. Lepikhin, H. Lee, Y . Xu, D. Chen, O. Firat, Y . Huang, M. Krikun, N. Shazeer, and Z. Chen, “Gshard: Scaling giant models with conditional computation and automatic sharding,” arXiv preprint arXiv:2006.16668, 2020. [39] D. Dai, C. Deng, C. Zhao, R. Xu, H. Gao, D. Chen, J. Li, W. Zeng, X. Yu, Y . Wu, et al., “Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models,” arXiv preprint arXiv:2401.06066, 2024. [40] C. Hwang, W. Cui, Y . Xiong, Z. Yang, Z. Liu, H. Hu, Z. Wang, R. Salas, J. Jose, P. Ram,et al., “Tutel: Adaptive mixture-of-experts at scale,” Proceedings of Machine Learning and Systems, vol. 5, 2023. [41] DeepSeek-AI, “Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model,” arXiv preprint arXiv:2405.04434, 2024. [42] J. Ainslie, J. Lee-Thorp, M. de Jong, Y . Zemlyanskiy, F. Lebrón, and S. Sanghai, “Gqa: Training generalized multi-query transformer models from multi-head checkpoints,” arXiv preprint arXiv:2305.13245, 2023. 12[43] N. Shazeer, “Fast transformer decoding: One write-head is all you need,” arXiv preprint arXiv:1911.02150, 2019. [44] K. Choromanski, V . Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, et al., “Rethinking attention with performers,” arXiv preprint arXiv:2009.14794, 2020. [45] H. Peng, N. Pappas, D. Yogatama, R. Schwartz, N. A. Smith, and L. Kong, “Random feature attention,” in ICLR, 2021. [46] T. Dao, D. Fu, S. Ermon, A. Rudra, and C. Ré, “Flashattention: Fast and memory-efficient exact attention with io-awareness,” in NeurIPS, vol. 35, pp. 16344–16359, 2022. [47] T. Dao, “Flashattention-2: Faster attention with better parallelism and work partitioning,” arXiv preprint arXiv:2307.08691, 2023. [48] S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma, “Linformer: Self-attention with linear complexity,”arXiv preprint arXiv:2006.04768, 2020. [49] X. Ma, X. Kong, S. Wang, C. Zhou, J. May, H. Ma, and L. Zettlemoyer, “Luna: Linear unified nested attention,” in NeurIPS, vol. 34, pp. 2441–2453, 2021. [50] J. Lee, Y . Lee, J. Kim, A. Kosiorek, S. Choi, and Y . W. Teh, “Set transformer: A framework for attention-based permutation-invariant neural networks,” in ICML, pp. 3744–3753, PMLR, 2019. [51] J. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and S. Han, “Awq: Activation-aware weight quantization for llm compression and acceleration,” arXiv preprint arXiv:2306.00978, 2023. [52] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “Qlora: Efficient finetuning of quantized llms,” in NeurIPS, vol. 36, 2023. [53] T. Dettmers, M. Lewis, Y . Belkada, and L. Zettlemoyer, “Llm.int8(): 8-bit matrix multiplication for transformers at scale,” in NeurIPS, vol. 35, pp. 30318–30332, 2022. [54] J. Liu, R. Gong, X. Wei, Z. Dong, J. Cai, and B. Zhuang, “Qllm: Accurate and efficient low-bitwidth quantization for large language models,” in ICLR, 2024. [55] M. Zhang, C. Shen, Z. Yang, L. Ou, X. Yu, B. Zhuang, et al., “Loraprune: Pruning meets low-rank parameter-efficient fine-tuning,” inACL findings, 2024. [56] N. Kitaev, Ł. Kaiser, and A. Levskaya, “Reformer: The efficient transformer,”arXiv preprint arXiv:2001.04451, 2020. [57] S. K. Ainsworth, J. Hayase, and S. Srinivasa, “Git re-basin: Merging models modulo permutation symmetries,” in ICLR, 2023. [58] R. Entezari, H. Sedghi, O. Saukh, and B. Neyshabur, “The role of permutation invariance in linear mode connectivity of neural networks,” arXiv preprint arXiv:2110.06296, 2021. [59] T. Garipov, P. Izmailov, D. Podoprikhin, D. P. Vetrov, and A. G. Wilson, “Loss surfaces, mode connectivity, and fast ensembling of dnns,” inNeurIPS, vol. 31, 2018. [60] M. Wortsman, G. Ilharco, S. Y . Gadre, R. Roelofs, R. Gontijo-Lopes, A. S. Morcos, H. Namkoong, A. Farhadi, Y . Carmon, S. Kornblith,et al., “Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time,” in ICML, pp. 23965–23998, PMLR, 2022. [61] P. Yadav, D. Tam, L. Choshen, C. A. Raffel, and M. Bansal, “Ties-merging: Resolving interfer- ence when merging models,” in NeurIPS, vol. 36, 2023. [62] M. Davari and E. Belilovsky, “Model breadcrumbs: Scaling multi-task model merging with sparse masks,” arXiv preprint arXiv:2312.06795, 2023. [63] L. Yu, B. Yu, H. Yu, F. Huang, and Y . Li, “Language models are super mario: Absorbing abilities from homologous models as a free lunch,” arXiv preprint arXiv:2311.03099, 2023. 13[64] K. Shoemake, “Animating rotation with quaternion curves,” inProceedings of the 12th annual conference on Computer graphics and interactive techniques, pp. 245–254, 1985. [65] M. S. Matena and C. A. Raffel, “Merging models with fisher-weighted averaging,” inNeurIPS, vol. 35, pp. 17703–17716, 2022. [66] X. Jin, X. Ren, D. Preotiuc-Pietro, and P. Cheng, “Dataless knowledge fusion by merging weights of language models,” arXiv preprint arXiv:2212.09849, 2022. [67] Y . Chen, X. Pan, Y . Li, B. Ding, and J. Zhou, “Ee-llm: Large-scale training and inference of early-exit large language models with 3d parallelism,” arXiv preprint arXiv:2312.04916, 2023. [68] S. Reddy, D. Chen, and C. D. Manning, “Coqa: A conversational question answering challenge,” Transactions of the Association for Computational Linguistics, vol. 7, pp. 249–266, 2019. [69] S. Lin, J. Hilton, and O. Evans, “Truthfulqa: Measuring how models mimic human falsehoods,” arXiv preprint arXiv:2109.07958, 2021. [70] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han, “Smoothquant: Accurate and efficient post-training quantization for large language models,” in International Conference on Machine Learning, pp. 38087–38099, PMLR, 2023. [71] J. Liu, R. Gong, X. Wei, Z. Dong, J. Cai, and B. Zhuang, “Qllm: Accurate and efficient low-bitwidth quantization for large language models,” in ICLR, 2024. [72] S.-Y . Liu, C.-Y . Wang, H. Yin, P. Molchanov, Y .-C. F. Wang, K.-T. Cheng, and M.-H. Chen, “Dora: Weight-decomposed low-rank adaptation,” arXiv preprint arXiv:2402.09353, 2024. [73] M. Nagel, R. A. Amjad, M. Van Baalen, C. Louizos, and T. Blankevoort, “Up or down? adaptive rounding for post-training quantization,” in International Conference on Machine Learning, pp. 7197–7206, PMLR, 2020. [74] W. Kwon, Z. Li, S. Zhuang, Y . Sheng, L. Zheng, C. H. Yu, J. Gonzalez, H. Zhang, and I. Stoica, “Efficient memory management for large language model serving with pagedattention,” in Proceedings of the 29th Symposium on Operating Systems Principles, pp. 611–626, 2023. [75] D. H. Eberly, “Quaternion algebra and calculus,” 2002. [76] “Stanford crfm.” https://crfm.stanford.edu/2023/10/12/flashdecoding.html, 2024. Accessed: 2024-05-04. [77] Y . Liu, H. Li, K. Du, J. Yao, Y . Cheng, Y . Huang, S. Lu, M. Maire, H. Hoffmann, A. Holtzman, et al. , “Cachegen: Fast context loading for language model applications,” arXiv preprint arXiv:2310.07240, 2023. [78] L. Chen, H. Zhao, T. Liu, S. Bai, J. Lin, C. Zhou, and B. Chang, “An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models,” arXiv preprint arXiv:2403.06764, 2024. [79] S. Wei, T. Ye, S. Zhang, Y . Tang, and J. Liang, “Joint token pruning and squeezing towards more aggressive compression of vision transformers,” in CVPR, pp. 2092–2101, 2023. [80] L. Gao, J. Tow, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, K. McDonell, N. Muennighoff, J. Phang, L. Reynolds, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou, “A framework for few-shot language model evaluation,” 2021. [81] W. Kwon, Z. Li, S. Zhuang, Y . Sheng, L. Zheng, C. H. Yu, J. Gonzalez, H. Zhang, and I. Stoica, “Efficient memory management for large language model serving with pagedattention,” in Proceedings of the 29th Symposium on Operating Systems Principles, pp. 611–626, 2023. 14Appendix A Additional Experiment Results Comparisons with token sparsity methods. We also compare MiniCache with the sparsity-based method H2O [14]. Our method outperforms H2O in most tasks on LongBench. Notably, our approach focuses on reducing inter-layer redundancy, whereas H2O focuses on intra-layer redundancy. Our approaches are orthogonal to sparsity-based methods. Table A: Comparison between MiniCache with token sparsity based method H2O, using mistral-7B- instruct on LongBench dataset. MethodsNrtvQA Qasper MF-en HotpotQA 2WikiMQA Musique GovReport QMSum MultiNews TREC TriviaQA SAMSum PCount PRe Lcc Baseline 26.82 33.06 49.28 42.77 27.33 19.27 32.85 24.25 27.06 71.0 86.23 42.98 2.75 86.98 55.51H2O[14] 22.61 29.06 47.22 36.54 20.6 16.25 30.0 23.8 26.75 70.5 86.16 42.97 3.46 86.38 53.72MiniCache27.0432.5949.38 43.91 24.97 18.3 31.46 23.85 26.64 71.0 86.93 43.6 3.04 79.5654.79 B Additional Related Work Preliminaries of KV cache in LLMs. LLM inference has been significantly improved in recent works [9, 13, 74] by optimizing the KV cache management. Overall, this line of research is typically done in two steps: 1) First, at the prefilling stage, LLM calculates the initial KV caches at each attention layer based on the input sequence and decodes the first token. Second, 2) at the decoding stage, LLM autoregressively predicts the next token, where its KV cache at each attention layer is added to the overall caches. Existing works have compressed KV cache in different aspects (e.g., quantization [11, 12], token pruning [14, 16] ). KV cache compression. In the prior study, various strategies for enhancing efficient transformer architectures are discussed, covering a spectrum of techniques aimed at optimizing performance and managing resource constraints. These methods include attention optimization [ 46, 47, 76], grouping queries [42, 43], sparse KV caching [16, 77, 78], shrinking tokens [15, 79], and improving long-context generation. Significant contributions come from projects such as H2O [ 15], GEAR [15], and KIVI [11]. Additionally, efforts to alleviate KV cache bottlenecks include strategies like multi-query attention [42] and multi-group attention [43], which propose reducing the number of heads in the KV cache. However, these methods often necessitate retraining or fine-tuning models. Other approaches focus on diminishing the size of the KV cache by selectively evicting less important tokens [14] and enhancing the system architecture through technologies like offloading the KV cache [80] or integrating techniques such as virtual memory and paging [81] into the attention mechanism. C Discussions and Limitations Alternative merging function. During our preliminary exploration, we initially considered an alter- native, simpler merge function for cross-layer compression: maximum norm-preserving interpolation. This function is designed to maintain the maximum norm of the vectors involved, ensuring that the most significant features are preserved during the merging process. The maximum norm-preserving interpolation in terms of Fmax can be defined as follows: Fmax(xl, xl−1) = ¯xl,l−1 ∥¯xl,l−1∥ · Max(∥xl∥, ∥xl−1∥). (A) Here ¯xl,l−1 represents the average vector between xl and xl−1. The function Fmax ensures that the merged vector preserves the direction of the average vector while scaling it to the maximum norm of the original KV states. Compared to the SLERP-based merge function, Fmax has less computational overhead and lower memory consumption. However, it is less accurate than SLERP. The choice between using FSLERP or Fmax depends on the specific requirements of the application. In our study, we primarily use SLERP to maximize performance. Societal impact. Our work shows a preliminary exploration of KV cache Compression in the depth dimension, a relatively unexplored yet critically bottlenecked area in large language models 15(LLMs). The proposed MiniCache provides a solution to improve the efficiency of LLM generation and is adaptable to existing intra-layer-based KV cache pruning and quantization technologies. Additionally, we proposed a simple yet effective approach that merges similar KV cache states in a cross-layer manner and effectively restores performance through a novel restoration technique. Our observations indicate that cross-layer similarity and mergeability can be applied in broad-efficient applications for post-training optimization in low-resource scenarios, such as deployment on mobile devices. Moreover, with effective optimization of KV cache memory demand, MiniCache further enhances long-context generation, which is a crucial paradigm for real-world applications, such as understanding concepts in textbooks. We aim for our work to advance the boundaries of two key challenges in the LLM industry and research: batch inference and long-context generation. Furthermore, in addition to the significant contributions of MiniCache for KV cache Compression, several challenges remain that are common to LLMs. Issues such as the truthfulness and security of LLMs are still unresolved. Ensuring the accuracy and reliability of generated content is critical, as LLMs can sometimes produce plausible but incorrect or misleading information. Additionally, safeguarding against security vulnerabilities, such as adversarial attacks or data leakage, is paramount to maintaining the integrity and confidentiality of user interactions. Addressing these challenges requires ongoing research and development to enhance the robustness and trustworthiness of LLMs. This effort must proceed alongside advancements in computational efficiency and performance, as exemplified by innovations like MiniCache. Limitations. The current merging algorithm based on Spherical Linear Interpolation (SLERP) has its limitations. SLERP is suitable only for merging two vectors and uses an interpolation approach that restricts our algorithm from merging multiple layers simultaneously and maximizing the compression ratio in further states. This limitation impacts the overall efficiency of KV cache compression and underscores the need for advanced techniques capable of handling more complex merging scenarios. Future research should focus on developing more sophisticated algorithms that can overcome these constraints, thereby enhancing the compression capabilities and overall performance of LLMs. D Additional Implementation Details Overview of the inference algorithm. The MiniCache inference implementation, as shown in Alg. 1, involves several key steps. When the interface starts, In the prefilling phase, we define the merging starting layer S. Before reaching layer S, the inference uses the original attention and cache logic. From layer S onward, we implement our merging algorithm, which operates in a cross-layer manner and is applied only at odd-numbered layers. During merging, we fetch the KV cache from the previous layer and save the merged shared states into the current layer’s KV cache. To reduce memory usage, we then remove the cache of the previous layer. Additionally, we store the magnitudes vector and retention-sensitive tokens for each layer. During the decoding phase, two scenarios arise after layer S. For even-numbered layers (first round), since the KV cache has been removed during the prefilling phase, we refer to the next layer (l + 1) to fetch the shared KV cache states. We then perform approximated scale restoration and retention token recovery. The new KV states from this phase are stored for use in the next round. In the second round, which involves odd-numbered layers, we use the new KV tokens from both the previous and current layers. After the restoration phase, we perform the merge operations and update the shared KV cache states in the stack. Cross-Layer merging and restoration algorithm. As outlined in Alg. 2, MiniCache algorithm involves several crucial steps to ensure efficient memory usage while maintaining the integrity of the Key-Value (KV) Cache states. Initially, given the KV cacheEl k,v, norm values ∥Xl k,v∥ unmerged tokens Rl k,v, retention indices Ik,v, and the next tokens tl, tl−1, the algorithm proceeds by rescaling the magnitude of the KV pairs. Specifically, ˆXl k and ˆXl v are computed by multiplying the normalized KV pairs El k,v with their respective magnitude norms ∥Xl k,v∥. Following this, the algorithm restores unmerged tokens using the retention indices, updating ˆXl k and ˆXl v accordingly. Next, the new tokens tk and tv are concatenated to the rescaled KV pairs along the token dimension. This augmented KV cache undergoes a softmax attention mechanism where the attention scores A are computed by taking the dot product of the query token tq with the transposed keys ( ˆXl k)⊤. The output token tO is then obtained by multiplying the attention scores A with the values ˆXl v. In cases where the previous token tl−1 exists, the algorithm performs a compression step. It concatenates the existing KV cache 16El k,v with the merged tokens resulting from the current and previous layers, effectively reducing redundancy and optimizing memory. If tl−1 is not available, the KV cache is updated by simply concatenating El k,v with the new tokens tl k,v, deferring the compression until the next iteration. The final output token tO is then returned, concluding the decoding process. In the merging function, the algorithm normalizes the KV pairs from both the current and previous layers. It calculates the angular distance Ω between the normalized vectors, ensuring that the interpolation occurs along the shortest path on the unit sphere. The merged KV cache is then obtained by weighted interpolation of the normalized vectors, preserving the geometric and semantic integrity of the original states. This comprehensive process allows MiniCache to achieve substantial memory efficiencies while maintaining the functional characteristics of the KV pairs across transformer layers. Layer𝑙 QK V Attention Cross-Layer Merging K V KVcachecompressionat layer𝑙 Layer𝑙−1  ... 𝜒! 𝜒!\"# 𝐾,𝑉 C 1 Keep Fetch 2Merge 3 Cache 4Delete Recovery Rescale Retention Recovery 5Fetch Contact New Token QK V Recovery Attention...... 6Fetch Cross Layer Merging Time 7 Prefilling Decoding Figure A: Overall prefilling and decoding logic for MiniCache involves performing cross-layer merging and recovery within our framework. MiniCache execution flow. Figure A delineates the pre-filling and decoding logic for the MiniCache framework, which incorporates cross-layer merging and error suppression to achieve memory effi- ciency and maintain functional integrity. Initially, in Step 1, the KV cache is fetched from the previous layer (Layer L−1) during the pre-filling phase. In Step 2, the fetched KV pairs from the current layer χL are merged with the KV pairs from the preceding layer χL−1, reducing redundancy through a 17merge function. Subsequently, in Step 3, the merged KV pairs are cached for future use, representing a consolidated data set from multiple layers. During the decoding phase, Step 4 involves deleting unnecessary or redundant KV pairs to optimize memory usage. In Step 5, the decoder fetches the required KV pairs from the cache for output generation. Step 6 applies error suppression mechanisms to the fetched KV pairs, including rescaling and retention recovery, to minimize errors introduced during the merging and compression processes. Finally, in Step 7, the cache is updated with the final KV pairs post-error suppression and adjustments, ensuring the cache contains the most accurate and efficient representation of the KV pairs for subsequent layers. This comprehensive approach guarantees substantial memory efficiencies while preserving the critical functional characteristics of the original KV pairs across transformer layers. Algorithm 1: The MiniCache Inference Algorithm 1 procedure MiniCache Inference: Input: Input Tokens: T ∈ Rtinput×d, number of layers L, merging beginning layer S Output: Output Tokens: O ∈ Rtoutput×d 2 for l ← 0 to S − 1 do 3 procedure Standard Prefill: 4 Standard Attention & Standard Cache 5 procedure Standard Decoding: 6 Standard Attention & Standard Cache // Start Merging from layer S 7 for l ← S to L do // Perform Merging in every two layers l%2 == 1 8 if l%2 == 1 and prefilling then 9 procedure MiniCache Prefill: Input: KV cache from Current layer l: Xl k,v, KV cache from Previous layer l − 1: Xl−1 k,v , token retention threshold: γ 10 Delete KV cache of the l − 1-th layer // layer l, l− 1 shares one Cache 11 Standard Attention & Standard Cache // Perform Merging in the second layer 12 if and decoding then 13 if l%2 == 0 then // first round in the cross-layer merging, fetch shared KV cache states from Cl+1 k,v 14 procedure MiniCache Decoding: Input: KV cache: Cl+1 k,v , Norm: ∥Xl k,v∥, Unmerged Tokens: Rl k,v, Retention indices: Ik,v, Next Token: tl 15 else // second round in cross-layer merging, while tl−1 exist 16 procedure MiniCache Decoding: Input: KV cache: Cl k,v, Norm: ∥Xl k,v∥, Unmerged Tokens: Rl k,v, Retention indices: Ik,v, Next Token: tl, tl−1 17 return O 18Algorithm 2: The MiniCache Prefill & Decoding Compression Algorithm Hyperparameter: number of layers L, merging beginning layer S 1 procedure MiniCache Prefill: Input: KV cache from current layer l: Xl k,v ∈ R2×tprompt×d, KV cache from previous layer l − 1: Xl−1 k,v ∈ R2×tprompt×d, token retention threshold: γ 2 El k, ∥Xl k∥, ∥Xl−1 k ∥, Ωk ← Merge(Xl k, Xl−1 k ) ; 3 El v, ∥Xl v∥, ∥Xl−1 v ∥, Ωv ← Merge(Xl v, Xl−1 v ) ; 4 El k,v ∈ Rtprompt×d | {z } compression output , ∥Xl,l−1 k,v ∥ ∈R4×tprompt×1 | {z } norms for rescaling ; 5 d(Xl, Xl−1)k,v = 1 π · Ωk,v // distance metrics 6 Ik,v = {i | di < dmin + (dmax − dmin) · γ} // retention indices 7 Rl,l−1 k ← Xl,l−1 k [Ik], Rl,l−1 v ← Xl,l−1 v [Iv] // unmerged tokens 8 return El k,v, ∥Xl,l−1 k,v ∥, Rl,l−1 k , Rl,l−1 v , Ik,v 9 procedure MiniCache Decoding: Input: KV cache: El k,v ∈ R2×tprompt×d, Norm: ∥Xl k,v∥ ∈R2×tprompt×1, Unmerged Tokens: Rl k,v ∈ R2×γ·tprompt×d, Retention indices: Ik,v ∈ R2×γ·tprompt×1, Next Token: tl ∈ R1×d, tl−1 ∈ R1×d 10 ˆXl k ← El k · ∥Xl k∥ ∥El k∥ ˆXl v ← El v · ∥Xl v∥ ∥Elv∥ // magnitude rescale 11 ˆXl k[Ik] = Rl k ˆXl v[Iv] = Rl v // token restoration 12 ˆXl k ← Concat( ˆXl k, tk, dim=token) ˆXl v ← Concat( ˆXl v, tv, dim=token) A ← Softmax(tq · ( ˆXl k)⊤) tO ← A · ˆXl v if tl−1 exists then 13 KV cache ← Concat(El k,v, Merge(tl k,v, tl−1 k,v ), dim=token) // perform compression 14 else 15 KV cache ← Concat(El k,v, tl k,v, dim=token) // wait for compression 16 return tO 17 function MiniCache Merge(Xl, Xl−1, t): 18 ⃗Xl ← Xl ∥Xl∥ 19 ⃗Xl−1 ← Xl−1 ∥Xl−1∥ 20 Ω ← arccos \u0010 Xl T ·Xl−1 T ∥Xl T ∥∥Xl−1 T ∥ \u0011 21 E ← sin((1−t)Ω) sin(Ω) ⃗X l + sin(tΩ) sin(Ω) ⃗X l−1 22 return E, ∥Xl∥, ∥Xl−1∥, Ω E Detailed Efficiency Derivation In this section, we provide a detailed derivation of the memory efficiency improvements outlined in Section 4.3. First, we consider the original KV cache memory usage, which is given by: 4brh(s + n). Here, r is the number of layers, b is the batch size, h is the hidden size, s is the input sequence length, and n is the output sequence length. To improve efficiency, we begin merging layers starting from the midpoint, S = 1 2 r, by consolidating the KV cache states of every two layers into a single shared state space. The memory usage derivation proceeds as follows: for the unmerged part of the cache (from layer 1 to S): 194brh(s + n) · 1 2 = 2brh(s + n). For the merged part of the cache (from layer S + 1 to r): 4brh(s + n) · 1 2 · 1 2 = brh(s + n). Combining these two parts, the total memory usage is: 2brh(s + n) + brh(s + n) = 3brh(s + n). Next, we consider the additional memory cost incurred during the restoration process. During this phase, we save additional normalized vectors for the layers in the KV cache. These vectors are of shape Rb×s×1, which means they have a single channel dimension compared to the fully ranked original KV states. The additional normalized vectors for layers from S onwards are given by: br(s + n) · 2 = 2br(s + n). We also introduce a retention threshold, which we set to 0.05. This means that 5% of the KV cache tokens are retained without compression: brh(0.05(s + n)). Combining these terms, the total additional memory for the restoration process is: 2br(s + n) + 0.1brh(s + n). Finally, summing the compressed memory usage and the restoration memory cost, the overall memory requirement is: 3brh(s + n) + 2br(s + n) + 0.1brh(s + n). This can be simplified by grouping the common factors: br(s + n) (3h + 2 + 0.1h) . Simplifying the expression inside the parentheses, we get: br(s + n) (3.1h + 2). Therefore, the total memory cost for the KV cache in the MiniCache Framework is: br(s + n)(3.1h + 2). This detailed derivation confirms the efficiency improvements discussed in Section 4.3, highlighting the significant reduction in memory usage achieved through our layer merging and restoration strategies. F Detailed Experiment Results 20Table B: Detailed performance comparison on GSM8K dataset with LLaMA-3-70B. Layer LLaMA-3-70B MiniCache LLaMA-3-70B Mean 0 0.799 0.729 5 0.800 0.640 10 0.792 0.578 15 0.815 0.545 20 0.801 0.560 25 0.812 0.544 30 0.799 0.556 35 0.810 0.557 40 0.790 0.551 45 0.725 0.539 50 0.638 0.541 55 0.638 0.501 60 0.625 0.497 65 0.635 0.511 70 0.623 0.497 75 0.615 0.493 Table C: Detailed performance comparison on COQA dataset with LLaMA-3-70B. Layer LLaMA-3-70B MiniCache LLaMA-3-70B Mean 0 0.705 0.706 4 0.705 0.699 8 0.706 0.696 12 0.704 0.691 16 0.704 0.690 20 0.703 0.690 24 0.701 0.690 28 0.702 0.690 32 0.702 0.688 36 0.703 0.688 40 0.697 0.687 44 0.698 0.685 48 0.699 0.678 52 0.699 0.672 56 0.701 0.668 60 0.704 0.657 64 0.706 0.635 68 0.691 0.611 72 0.689 0.565 76 0.641 0.526 21Table D: Detailed performance comparison on TruthfulQA dataset with LLaMA-3-70B. Layer LLaMA-3-70B MiniCache LLaMA-3-70B Mean 0 22.615 22.130 4 22.512 22.005 8 22.451 21.876 12 22.413 21.303 16 22.387 21.209 20 22.387 20.752 24 22.387 20.657 28 22.276 20.501 32 22.130 20.479 36 22.130 20.335 40 22.073 19.834 44 21.356 17.024 48 21.356 12.440 52 21.333 9.127 56 21.316 3.255 60 21.172 2.349 64 21.153 2.250 68 21.002 1.721 72 20.940 1.119 76 20.683 0.784 Table E: Detailed performance comparison on GSM8K dataset with LLaMA-3-8B. Layer LLaMA-3-8B MiniCache LLaMA-3-8B Mean 0 0.488 0.467 2 0.476 0.369 4 0.489 0.388 6 0.487 0.387 8 0.489 0.359 10 0.479 0.388 12 0.486 0.384 14 0.472 0.368 16 0.477 0.343 18 0.446 0.291 20 0.447 0.271 22 0.433 0.234 24 0.399 0.155 26 0.396 0.140 28 0.395 0.052 30 0.391 0.024 32 0.397 0.025 22Table F: Detailed performance comparison on COQA dataset with LLaMA-3-8B. Layer LLaMA-3-8B MiniCache LLaMA-3-8B Mean 0 0.676 0.676 2 0.676 0.571 4 0.675 0.566 6 0.674 0.564 8 0.674 0.561 10 0.673 0.560 12 0.672 0.560 14 0.670 0.559 16 0.670 0.558 18 0.669 0.555 20 0.669 0.552 22 0.668 0.549 24 0.667 0.543 26 0.667 0.537 28 0.666 0.536 30 0.666 0.531 32 0.665 0.528 Table G: Detailed performance comparison on TruthfulQA dataset with LLaMA-3-8B. Layer LLaMA-3-8B MiniCache LLaMA-3-8B Mean 0 32.520 32.524 2 32.231 28.431 4 31.645 28.197 6 31.485 27.894 8 31.008 27.796 10 30.964 27.704 12 30.798 27.371 14 30.798 27.093 16 30.798 26.643 18 30.798 26.517 20 30.798 26.355 22 30.798 26.011 24 30.798 25.044 26 30.798 15.254 28 30.798 14.791 30 30.765 9.419 32 30.390 6.068 23Table H: Detailed performance comparison on GSM8K dataset with Mixtral-8x7B. Layer Mixtral-8x7B MiniCache Mixtral-8x7B Mean 0 0.589 0.575 2 0.592 0.480 4 0.593 0.491 6 0.591 0.469 8 0.580 0.472 10 0.592 0.492 12 0.582 0.485 14 0.572 0.480 16 0.562 0.462 18 0.522 0.432 20 0.526 0.426 22 0.540 0.416 24 0.519 0.398 26 0.515 0.436 28 0.502 0.401 30 0.515 0.386 32 0.490 0.258 Table I: Detailed performance comparison on COQA dataset with Mixtral-8x7B. Layer Mixtral-8x7B MiniCache Mixtral-8x7B Mean 0 0.672 0.675 2 0.671 0.612 4 0.670 0.601 6 0.672 0.590 8 0.674 0.582 10 0.671 0.571 12 0.674 0.561 14 0.670 0.546 16 0.672 0.544 18 0.672 0.530 20 0.675 0.522 22 0.671 0.512 24 0.660 0.455 26 0.657 0.447 28 0.640 0.440 30 0.634 0.424 32 0.459 0.430 24Table J: Detailed performance comparison on TruthfulQA dataset with Mixtral-8x7B. Layer Mixtral-8x7B MiniCache Mixtral-8x7B Mean 0 21.686 19.465 2 21.385 19.405 4 21.368 19.251 6 21.038 19.094 8 21.038 18.265 10 20.216 17.019 12 20.026 15.902 14 19.723 15.505 16 19.641 15.028 18 19.641 14.723 20 19.546 14.543 22 18.756 14.122 24 18.402 13.834 26 18.366 13.789 28 17.738 12.091 30 16.827 12.008 32 16.635 0.430 Table K: Detailed performance comparison on GSM8K dataset with Phi-3-Mini. Layer Phi-3-Mini MiniCache Phi-3-Mini Mean 0 0.774 0.774 2 0.765 0.667 4 0.757 0.661 6 0.754 0.659 8 0.748 0.657 10 0.750 0.645 12 0.750 0.616 14 0.752 0.575 16 0.739 0.491 18 0.742 0.417 20 0.692 0.272 22 0.685 0.206 24 0.640 0.110 26 0.545 0.061 28 0.500 0.039 30 0.460 0.036 32 0.447 0.028 25Table L: Detailed performance comparison on COQA dataset with Phi-3-Mini. Layer Phi-3-Mini MiniCache Phi-3-Mini Mean 0 0.665 0.665 2 0.662 0.562 4 0.657 0.557 6 0.656 0.556 8 0.656 0.556 10 0.654 0.554 12 0.646 0.546 14 0.648 0.538 16 0.647 0.537 18 0.637 0.527 20 0.627 0.487 22 0.591 0.461 24 0.567 0.437 26 0.548 0.408 28 0.527 0.407 30 0.506 0.406 32 0.503 0.403 Table M: Detailed performance comparison on TruthfulQA dataset with Phi-3-Mini. Layer Phi-3-Mini MiniCache Phi-3-Mini Mean 0 19.686 19.465 2 19.385 19.365 4 19.368 19.221 6 19.100 18.255 8 19.038 17.019 10 19.500 15.912 12 19.216 15.525 14 20.026 15.195 16 19.641 15.058 18 18.756 14.763 20 17.738 14.593 22 19.546 14.182 24 19.723 13.954 26 18.366 13.919 28 18.402 12.231 30 16.827 12.158 32 16.635 10.333 26",
      "meta_data": {
        "arxiv_id": "2405.14366v2",
        "authors": [
          "Akide Liu",
          "Jing Liu",
          "Zizheng Pan",
          "Yefei He",
          "Gholamreza Haffari",
          "Bohan Zhuang"
        ],
        "published_date": "2024-05-23T09:43:52Z",
        "pdf_url": "https://arxiv.org/pdf/2405.14366v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces MiniCache, a novel, training-free, and general approach for KV cache compression in Large Language Models (LLMs) along the depth dimension. It addresses the critical memory bottleneck caused by KV cache growth, especially for long sequence lengths. Key contributions include observing high similarity of KV cache states between adjacent middle-to-deep layers, proposing a reparameterization strategy that disentangles states into magnitude and direction components for effective merging, and introducing a token retention mechanism to preserve critical information from highly distinct state pairs. MiniCache achieves significant memory footprint reduction (up to 41%), enhanced inference throughput (approximately 5x), and a remarkable compression ratio (up to 5.02x) with near-lossless performance, complementing existing compression methods like quantization and sparsity.",
        "methodology": "MiniCache primarily consists of two components: a reparameterization-based cache merging strategy and a token retention mechanism. The merging strategy starts from the middle layer (S = L/2) and integrates KV caches of consecutive layers into a single shared memory space. It reparameterizes state vectors by decomposing them into magnitude (ℓ2 norm) and directional components (unit vectors). Spherical Linear Interpolation (SLERP) is used to effectively interpolate the directional components of adjacent layers. The restoration process uses the merged directional vector and stored magnitudes to reconstruct approximate original states. To mitigate performance degradation, a token retention strategy identifies and retains unmergeable token pairs characterized by low angular similarity (high angular distance) using a predefined threshold (γ). The final compressed cache includes the shared directional vector, token-wise magnitudes, and retained tokens with their indices.",
        "experimental_setup": "The method was extensively evaluated on various LLMs, including LLaMA-2 (7B, 13B), LLaMA-3 (8B, 70B), Phi-3-Mini, Mistral-7B, and Mixtral-8x7B. Benchmarking was performed across diverse question answering and generation datasets from lm-eval-harness (e.g., COPA, MathQA, OpenBookQA, PIQA, RTE, WinoGrande, XSUM, CNN/Daily Mail), and for long-sequence generation using LongBench. Specific evaluations focused on GSM8K, COQA, and TruthfulQA. Comparisons were made against a FP16 full cache baseline and existing methods like round-to-nearest quantization (RTN), SmoothQuant, and KIVI (2-bit and 4-bit). Efficiency analysis, including peak memory usage and throughput, was conducted on synthetic workloads derived from ShareGPT using a LLaMA-2-7B model on a single 80GB NVIDIA A100 GPU. Hyperparameters for MiniCache were set with an interpolation parameter t=0.6 and a token retention threshold γ=0.05. Ablation studies were conducted on these hyperparameters.",
        "limitations": "The current SLERP-based merging algorithm is limited to merging only two vectors at a time, which restricts the potential for simultaneous cross-multiple-layer merging. This limitation impacts the overall efficiency and maximal compression ratio achievable. Additionally, the paper acknowledges that MiniCache does not address broader LLM challenges such as truthfulness and security vulnerabilities.",
        "future_research_directions": "Future work will concentrate on enhancing the compression ratio by exploring cross-multiple-layer merging beyond just two layers. This involves developing more advanced merging algorithms, such as Spherical Cubic Interpolation, to handle more complex scenarios. Further optimization of memory usage for large-scale deployments in diverse application scenarios is also a key direction. Additionally, dynamically determining the interpolation parameter 't' based on the relative magnitude ratio of adjacent layers is identified as a promising area for further exploration."
      }
    },
    {
      "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization",
      "abstract": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model.",
      "full_text": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization Coleman Hooper1 Sehoon Kim1 Hiva Mohammadzadeh1 Michael W. Mahoney1,2,3 Yakun Sophia Shao1 Kurt Keutzer1 Amir Gholami1,2 1University of California, Berkeley 2ICSI 3LBNL {chooper, sehoonkim, hiva, mahoneymw, ysshao, keutzer, amirgh}@berkeley.edu Abstract LLMs are seeing growing use for applications which require large context windows, and with these large context windows KV cache activations surface as the dominant contributor to memory consumption during inference. Quantization is a promising approach for compressing KV cache activations; however, existing solutions fail to represent activations accurately in sub-4-bit precision. Our work, KVQuant, facili- tates low precision KV cache quantization by incorporating several novel methods: (i) Per-Channel Key Quantization, where we adjust the dimension along which we quantize the Key activations to better match the distribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations before the rotary positional em- bedding to mitigate its impact on quantization; (iii) Non-Uniform KV Cache Quan- tization, where we derive per-layer sensitivity-weighted non-uniform datatypes that better represent the distributions; and (iv) Per-Vector Dense-and-Sparse Quanti- zation, where we isolate outliers separately for each vector to minimize skews in quantization ranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral models, we achieve < 0.1 perplexity degradation with 3-bit quantiza- tion on both Wikitext-2 and C4, outperforming existing approaches. Our method enables serving LLaMA-7B with a context length of up to 1 million on a single A100-80GB GPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for KVQuant, showing that we can achieve up to ∼1.7× speedups, compared to baseline fp16 matrix-vector multiplications, for the LLaMA-7B model. Code is available at https://github.com/SqueezeAILab/KVQuant. 1 Introduction Large language models (LLMs) have revolutionized many natural language processing (NLP) tasks. In order to improve the capabilities of LLMs, there is significant interest in increasing the context lengths of LLMs. Longer context lengths enable new applications, including long document summarization, retrieval for answering questions about long documents, extended multi-turn applications [6], and code analysis. To support this pull from applications, there have been significant recent advances in long-context length models in industry [2, 30], as well as in academia [6, 22]. Given the importance of LLM workloads, there is strong motivation to improve their inference efficiency. LLM inference with large context lengths can be incredibly resource-intensive; serving LLMs requires high-end GPUs, and the largest LLMs require costly multi-GPU inference setups. When analyzing the computational nature of generative inference with LLMs, it becomes quickly apparent that, for relatively small batch sizes, the computation is memory bound [ 17]. With the growing divergence between computational speeds and memory speeds, this problem is only going to get worse over time [12]. This makes reducing the memory bottleneck preeminently important. Further analysis shows that the memory bottleneck is strongly related to context size. For short 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2401.18079v6  [cs.LG]  28 May 202516% 84%98% 2% Long Sequence LengthsKV Cache is the bottleneckShort sequence length Weightsare the bottleneck BaselineKVQuant int3 +Grouping 10.87 Pre-RoPEKeyQuantizationNon-UniformQuantization5.755.94 1%Outliers6.23 Per-Channel KeyQuantization7.05 Perplexity on Wikitext2fp16 Baseline Figure 1: Left: Model size versus activation memory size for the LLaMA-7B model with sequence length 512 and 128K. For longer context lengths, the KV cache becomes the dominant memory bottleneck. Memory consumption of model weights and KV cache activations for different LLaMA models with different sequence lengths are provided in Table 7 in Appendix A. Right: Overview of the different components used in KVQuant that result in less than 0.1 perplexity degradation over the fp16 baseline when quantizing the KV cache for the LLaMA-7B model to 3-bit precision. As shown in Table 1, our 3-bit approach results in4.8× reduction in cached activation memory footprint. sequence lengths, the dominant contributor to memory consumption is the weight matrices, and therefore the optimal strategy is to minimize the model size in order to reduce memory consumption as well as bandwidth requirements [ 18, 17]. However, as shown in Figure 1, the main bottleneck for long sequence lengths is the memory requirements for caching Key and Value (KV) activations throughout inference. This challenge is further exacerbated when one considers batched inference. It is therefore crucial to develop methods for compressing the KV cache to enable efficient long- sequence length inference. Existing approaches lead to unacceptable accuracy degradation due to the outlier structures in KV cache activations as well as suboptimal bit allocation with existing uniform and non-uniform approaches. In this work, we perform an extensive analysis of KV cache activations in recent LLMs, revealing patterns which can be exploited to enable ultra-low precision quantization with minimal accuracy loss. In particular, we make the following contributions (summarized in Figure 1): • We find that the Keys exhibit outliers in specific channels before applying RoPE. However, the outlier channel magnitudes become less consistent after applying RoPE, posing a distinct challenge for low precision quantization. We address this by quantizing Keys per-channel before RoPE is applied (see Section 3.1 and Section 3.2). • We find that existing uniform and non-uniform quantization methods result in sub-optimal quanti- zation signpost placement. Instead, we propose a Non-Uniform Quantization (NUQ) method which considers sensitivity and not just magnitude when quantizing activations. We show that one can apply sensitivity-weighted non-uniform quantization offline on a calibration set to derive accurate datatypes for KV cache quantization (see Section 3.3). • Even with the above, we find that outlier values in cached KV activations can significantly de- grade quantization resolution. Unlike for weights, it is non-trivial to extract outlier values from activations, given the dynamic nature of activations. However, we find that we can efficiently and accurately identify and compress outlier values in order to store them compactly in a separate sparse representation. We also find that per-vector outlier detection outperforms per-matrix outlier detection with no additional memory overhead. By removing only 1% of outliers, we can attain under 0.1 perplexity degradation on both Wikitext-2 and C4 for 3-bit KV cache quantization with the LLaMA, Llama-2, Llama-3, and Mistral models, thereby facilitating accurate inference with 4.8× longer context length. • We implement custom CUDA kernels to perform activation quantization efficiently during inference, achieving up to ∼1.7× speedups for Key and Value matrix-vector multiplications for LLaMA-7B at 4-bit precision relative to the fp16 baseline (see Section 3.7 and 4.4). These results demonstrate how our methodology allows for accurate and efficient low-bit KV cache quantization. 22 Background 2.1 LLM Inference When inferring a decoder-only LLM, inference proceeds in two distinct phases. In the prefill phase, the model takes in an input prompt, which it processes in parallel. During the generation phase, the model then generates the output sequence autoregressively, meaning that each token generation is dependent on all previously generated tokens. As such, for small batch sizes, the generation phase of LLM inference is typically memory-bandwidth bound, as the only available parallelism is across different sequences in a given batch. Additionally, during generation, the model needs to store intermediate Key and Value activations at each layer in order to condition generations on previously generated output tokens. Otherwise, we would need to recompute all prior Keys and Values at each timestep, which would be prohibitively expensive. These stored activations are referred to as the Key-Value (KV) cache. Throughout this paper, we will capitalize Key and Value to distinguish when we are referring to the KV cache tensors. Assuming a model with n layers and h attention heads with dimension d that is stored using e bytes per element, the KV cache size for batch size b and sequence length l is 2 · n · h · d · e · b · l, meaning that it grows linearly with both batch size and sequence length. As shown in Table 7, the KV cache becomes the dominant contributor to memory consumption for longer sequence lengths and larger batch sizes. Note that since each sequence in batched inference depends on separate past context, there is no available batch-level parallelism when loading the cached Keys and Values for their respective computations in batched inference. KV cache loading is therefore always memory-bandwidth bound. This motivates pursuing methods to optimally compress the KV cache, even at the expense of a more complex dequantization process. 2.2 LLM Quantization There have been many prior works on LLM quantization. Several have focused on weight-only quantization for LLMs, due to the greater contribution to memory consumption and runtime for small sequence lengths and batch sizes [21, 9, 17]. Prior works have noted the presence of distinct outliers in both weights and activations [7, 9, 17]. One approach that has been developed to address this outlier issue in the context of weight quantization is dense-and-sparse quantization, where each weight matrix is decomposed into a sparse outlier matrix and a dense low-precision matrix [9, 17]. Prior works have also leveraged non-uniform quantization methods to improve accuracy for the same bit precision by allowing for flexible quantization signpost placement [17, 8]. These approaches have either used a fixed non-uniform datatype such as NormalFloat [8], or derived quantization signposts using a sensitivity-weighted k-means approach [17]. Appendix B provides a more detailed overview of related work for outlier-aware LLM quantization and non-uniform LLM quantization. There has also been work on quantizing both weights and activations (including KV cache) [41, 33]. However, there is still a significant perplexity degradation when quantizing KV cache activations to low precision; [34, 44] quantized KV cache activations to 4-bits, but required fine-grained grouping for 4-bit quantization, while still observing some perplexity degradation, and [34] observed that 3-bit KV cache quantization with fine-grained grouping leads to unacceptable accuracy loss. Other works quantized KV cache activations to 4-bits but required retraining to maintain performance [24]. One concurrent work also explores low precision KV cache quantization in order to enable larger batch size inference by reducing the KV cache size [26]. 2.3 KV Cache Compression There have also been several prior works on compressing the KV cache. Some of these methods aim to only store important tokens in the KV cache and to evict less important tokens, thereby maintaining low memory usage [25, 43, 11, 20]. Other methods aim to only retrieve a subset of tokens at each step to achieve memory bandwidth savings [32]. In this work, we explore KV cache quantization as an orthogonal direction for compressing the KV cache in order to enable long context inference. 3Channel 0 10002000 3000 4000 T oken 0 500 1000 1500 2000 Magnitude 0.0 2.5 5.0 7.5 10.0 12.5 15.0 Layer 10 Keys (pre-RoPE) Channel 0 10002000 3000 4000 T oken 0 500 1000 1500 2000 Magnitude 0.0 2.5 5.0 7.5 10.0 12.5 15.0 Layer 10 Keys (post-RoPE) Channel 0 10002000 3000 4000 T oken 0 500 1000 1500 2000 Magnitude 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Layer 10 Values Figure 2: Example distributions of the activation values for Keys pre-RoPE, Keys post-RoPE, and Values for LLaMA-7B on a sample with 2K sequence length from Wikitext-2. We observe several patterns: (i) Keys pre-RoPE exhibit clear outliers in specific channels across different tokens; (ii) after applying RoPE, the distribution becomes less structured and there are less consistent magnitudes for outlier channels (this is expected, as RoPE applies a rotation operation between pairs of channels); and (iii) Values exhibit no fixed outlier pattern with outlier values across channels and tokens. 3 Method 3.1 Per-Channel Key Quantization To inform our approach, we first performed a detailed analysis to understand the KV cache distribu- tions. Figure 2 shows sample distributions for the KV cache activations. We observe that the Key matrices tend to have distinct outlier channels, which have larger average magnitudes than other channels; this corroborates previous observations about outlier channels in LLM activations [7, 41]. The Value matrices exhibit both outlier channels as well as outlier tokens (although these outliers are less extreme than the outlier Key channels). Existing KV cache quantization approaches perform per-token quantization (meaning that the scaling factor and zero-point are shared by elements in the same token) [ 34, 44]. However, due to the differing average magnitudes between channels, the values within a channel are easier to quantize when grouped together than the values across different channels. As such, to better match the distributions, we investigate per-channel KV cache quantization, meaning that the scaling factor and zero-point are shared by elements in the same channel. By sharing the scaling factor and zero-point along the channel dimension, this will naturally group together values with similar magnitudes, thereby mitigating the impacts of outlier channels on other channels when quantizing to low precision. As outlined in Appendix G, we find that per-channel quantization provides significant accuracy benefits for Keys but not for Values. By leveraging per-channel quantization for Keys and per- token quantization for Values, we observe a 3.82 perplexity improvement on Wikitext-2 for 3-bit LLaMA-7B quantization. Note that this can potentially add runtime overhead since the quantization dimension is now misaligned with the reduction dimension for the Keys when performing matrix- vector multiplications. However, we find that we are able to efficiently dequantize Keys and perform the Query-Key matrix-vector multiplication without adding runtime overhead, as shown in Section 4.4. Additionally, as outlined in Section 3.6, per-channel quantization can also be challenging due to the need to recompute scaling factors as tokens are added to the Key cache. We show that we can calibrate offline for scaling factors, thereby avoiding expensive online recomputation. Per-channel Key quantization was also explored in another concurrent work [26], which leveraged similar intuition about grouping together large magnitude values in the same channel to minimize quantization error. Their methodology requires fine-grained grouping for per-channel quantization while maintaining a residual subset of the KV cache in fp16. In our work, we demonstrate that by leveraging offline calibration, we can accurately perform per-channel quantization without grouping. 3.2 Pre-RoPE Key Quantization One issue when quantizing Keys is handling the rotary positional embedding (RoPE), which is applied to Keys and Queries in most public LLMs, including LLaMA and Llama-2 [35]. Given Query and Key vectors Qm = Wq ∗ xm and Kn = Wk ∗ xn at positions m and n in the sequence, RoPE is applied as position-dependent rotations to each of these vectors to obtain ˜Qm = Rd θ,m · Qm and 4˜Kn = Rd θ,n · Kn. This embeds the relative position between a Query and Key vector as an amount of an angle that is a multiple of its position index. When caching Key vectors, we therefore need to either cache ˜Kn, or else we need to cache Kn and apply Rd θ,n on-the-fly during inference. The challenge with caching Key vectors after applying this rotation is that it leads to mixing pairs of channels by different amounts for different positions in the sequence, as shown in Appendix C (since it jointly rotates pairs of channels by different angles depending on the position in the sequence). The post-RoPE activation distribution is also shown in Figure 2, demonstrating how the rotation between pairs of channels leads to less consistent channel magnitudes. This makes it harder to quantize Key activation channels which would typically have consistent large-magnitude values. This motivated our investigation into whether we could perform pre-RoPE Key quantization (meaning that we quantize Kn) and then efficiently apply the positional embeddings on-the-fly after dequantization. The benefits of pre-RoPE Key quantization are highlighted in Appendix H, yielding 0.82 perplexity improvement on Wikitext-2 for 3-bit LLaMA-7B quantization. To be able to quantize Keys pre-RoPE, we develop a fused kernel to efficiently apply RoPE post-dequantization (the details of this approach will be discussed in Section 3.7). 3.3 nuqX: An X-Bit Per-Layer Sensitivity-Weighted Non-Uniform Datatype Uniform quantization is suboptimal for KV cache quantization since the Query and Key activations are non-uniform. Additionally, KV cache loading is memory bandwidth bound, regardless of batch size or sequence length, meaning that the dequantization overhead introduced by non-uniform quantization methods is not problematic (since the added computation does not introduce any additional latency). It is therefore desirable to leverage non-uniform quantization methods for KV cache quantization. In [17], the authors computed non-uniform quantization signposts using a sensitivity-weighted k-means approach. However, this cannot be applied directly to KV cache quantization as the Values are quantized dynamically at runtime, which means that we would need to apply K-means online during inference, and it is also difficult to estimate sensitivity for activations online. We therefore facilitate efficient online non-uniform KV cache quantization by deriving a per-tensor non-uniform datatype offline on calibration data, which is then rescaled per-channel or per-token to accurately represent the key and value distributions. We compute sensitivity-weighted quantization signposts offline on a calibration set prior to inference, while maintaining compatibility with per-vector quantization by separately normalizing each channel to the range [−1, 1] prior to deriving the shared datatype. Using the diagonal Fisher information matrix (derived in Appendix D), along with the quantization error for activation A, we formulate the error minimization objective as follows, where A is flattened to one dimension and where N is the number of elements from all of the samples in our calibration set: Q(A)∗ ≃ arg min Q NX i=1 Fii \u0000 Ai − Q(Ai) \u00012 . (1) We modify the objective in Equation 1 as described in Appendix E in order to apply it using the normalized activation values. We then minimize it offline on a calibration set using a k-means solver in order to obtain the quantization signposts for the non-uniform datatype for each Key or Value layer. Appendix I compares our non-uniform quantization approach with existing uniform and non-uniform quantization baselines [8], demonstrating how our non-uniform approach provides 0.29 perplexity improvement on Wikitext-2 for LLaMA-7B relative to 3-bit uniform methods. Table 16 in Appendix L shows how computing the required Fisher information for the LLaMA-65B model takes only a few minutes, and how using the k-means solver takes only a few minutes per layer (with the computation for each layer being parallelizable). 3.4 Per-Vector Dense-and-Sparse Quantization As shown in Figure 4 in Appendix F, for both Keys and Values, the majority of elements are contained within a small percentage of the dynamic range. This means that by leveraging dense-and-sparse quantization, as demonstrated in [17], in order to isolate a small percentage of numerical outliers, we can restrict the range that we need to represent, thereby allowing us to represent the remaining elements with greater precision. However, when looking at the Key and Value distributions in Figure 2, different channels and tokens have different average magnitudes. Therefore, an element which counts as an outlier in one channel may not be an outlier in another channel (since that channel 5may have a greater average magnitude), making naive application of dense-and-sparse quantization suboptimal. It is therefore crucial to directly target the outlier values that skew the dynamic range at the granularity that we are quantizing in order to address the values that are exaggerating the range along that particular dimension. In this work, we leverage per-vector dense-and-sparse quantization, where we use a different outlier threshold per-vector (either a separate threshold per-channel for per-channel quantization, or a separate threshold per-token for per-token quantization), rather than a single outlier threshold for each layer. Note that computing outlier thresholds for per-vector dense-and-sparse quantization poses potential accuracy and efficiency challenges. However, in Section 3.6, we show that we are able to accurately calibrate for per-channel outlier thresholds offline and efficiently compute per-token outlier thresholds online. After determining the upper and lower outlier thresholds, the remaining numbers in the vector are normalized to the range [−1, 1], and we then minimize Equation 1 (ignoring outliers) in order to obtain the quantization signposts for the non-uniform datatype for the remaining numbers. Appendix J will demonstrate the benefits of removing a small percentage of numerical outliers and keeping them in full precision, as well as the advantages of per-vector dense-and-sparse quantization over using a single global outlier threshold for each layer. As shown in Figure 1, by removing 1% of numerical outliers using per-vector outlier thresholds, we achieve an additional 0.19 perplexity improvement on Wikitext-2 for 3-bit LLaMA-7B quantization, which is within 0.07 perplexity of the fp16 baseline. 3.5 Attention Sink-Aware Quantization Prior work has demonstrated that after the first few layers in LLMs, the model tends to allocate a large attention score to the first token [42]. This occurs even when the initial token is not semantically important. This phenomenon happens because the model tends to use the inital token as a “sink”. In our work, we demonstrate that due to the Attention Sink phenomenon, the model is disproportionately sensitive to quantization error in the first token. By keeping only the first token in fp16, we can attain perplexity benefits, particularly for 2-bit quantization. A similar observation has also been made in another concurrent work [ 23]. Note that when retaining the first token in fp16, we account for this during the calibration process as well, meaning that we ignore the first token when deriving the nuqX datatype and when calibrating the scaling factors and zero points offline for the Keys. As demonstrated in Appendix K, this approach persistently yields performance benefits, particularly with lower bit widths and without dense-and-sparse quantization. 3.6 Offline Calibration versus Online Computation A crucial challenge for activation quantization is that we either need to compute statistics on-the-fly (which is potentially expensive) or else we need to use offline calibration data (which potentially has negative accuracy implications). The challenges with computing scaling factors (and zero-points) online versus offline for both Keys and Values are shown in Figure 5 in Appendix L. In per-channel quantization, it is challenging to update scaling factors online since the scaling factors corresponding to each incoming channel would potentially need to be updated whenever a new token is added to the KV cache. It is therefore desirable to be able to compute statistics offline (i.e., using calibration data before running inference). While this can have negative effects on model accuracy, in Appendix L we show that we can effectively calibrate offline for per-channel quantization, obviating the need for online updates of scaling factors for per-channel quantization. For per-token quantization, it is challenging to calibrate for scaling factors offline due to the presence of outlier Value tokens. It is therefore desirable to be able to compute scaling factors and outlier thresholds online for each incoming token. As shown in Appendix L, we can efficiently compute outlier thresholds online per-token by offloading to the CPU. By leveraging custom quantization function implementations for compressing activations, we are able to perform online per-token Value quantization without compromising on performance. 3.7 Kernel Implementation In order to efficiently perform activation quantization on-the-fly, we leverage dedicated kernel implementations with our 4-bit quantization method for compressing vectors to reduced precision and extracting the sparse outliers, performing matrix-vector multiplications using the compressed 6Table 1: Evaluation of our method for different models using the perplexity (PPL) on Wikitext-2. KVQuant results are using pre-RoPE per-channel quantization for Keys. KV cache sizes are estimated assuming a sequence length of 128K (ignoring context length limits for the models). Note that ATOM and FlexGen use 4-bit quantization with group sizes of 128 and 64 with uniform quantization, respectively, and we extend their methods to 3-bit and 2-bit quantization. We leverage Attention Sink-Aware quantization for all bit widths. We used post-RoPE quantization for all baseline methods since it achieves higher accuracy when quantizing Keys per-token as shown in Appendix P. Table 18 in Appendix O demonstrates a full evaluation on all LLaMA, Llama-2, Llama-3, and Mistral models. Method LLaMA-7B LLaMA-13B LLaMA-30B LLaMA-65B PPL KV Cache (GB) PPL KV Cache (GB) PPL KV Cache (GB) PPL KV Cache (GB) baseline 5.68 64.0 5.09 100.0 4.10 195.0 3.53 320.0 int4 5.98 16.0 5.32 25.0 4.34 48.8 3.73 80.1 nf4 5.87 16.0 5.23 25.0 4.25 48.8 3.63 80.1 ATOM-4bit 5.77 16.6 5.16 26.0 4.16 50.7 3.57 83.1 FlexGen-4bit 5.73 17.3 5.14 27.0 4.14 52.6 3.56 86.3 KVQuant-4bit 5.72 16.0 5.13 25.0 4.13 48.8 3.55 80.0 KVQuant-4bit-1% 5.69 17.3 5.10 27.0 4.11 52.7 3.54 86.5 int3 10.87 12.0 8.69 18.8 6.82 36.6 6.37 60.1 nf3 7.33 12.0 6.21 18.8 5.46 36.6 4.44 60.1 ATOM-3bit 6.17 12.6 5.47 19.7 4.44 38.4 3.78 63.0 FlexGen-3bit 5.93 13.2 5.29 20.6 4.26 40.2 3.66 65.9 KVQuant-3bit 5.87 12.0 5.25 18.8 4.25 36.6 3.63 60.0 KVQuant-3bit-1% 5.75 13.3 5.14 20.8 4.15 40.5 3.57 66.5 int2 11779 8.0 69965 12.5 1470 24.4 7272 40.1 nf2 3210 8.0 5786 12.5 2044 24.4 5367 40.1 ATOM-2bit 37.37 8.6 41.77 13.4 16.49 26.1 13.63 42.8 FlexGen-2bit 11.09 9.1 9.84 14.3 6.60 27.8 5.54 45.6 KVQuant-2bit 7.23 8.0 5.82 12.5 4.87 24.4 4.03 40.0 KVQuant-2bit-1% 6.01 9.3 5.36 14.5 4.35 28.3 3.70 46.5 vectors, and performing sparse matrix-dense vector multiplications using the sparse outliers. We store the quantized Key and Value matrices as 4-bit elements which are used as indices into lookup tables to recover the corresponding fp16 values. We store the sparse outlier matrices in either Compressed- Sparse Row (CSR) or Compressed-Sparse Column (CSC) format (depending on which aligns better with appending new Key and Value tokens). The kernels for the Key matrix-vector operations apply RoPE on-the-fly in order to support pre-RoPE quantization. More kernel implementation details are provided in Appendix R. 4 Results 4.1 Main Evaluation We used the LLaMA-7B/13B/30B/65B, Llama-2-7B/13B/70B, Llama-3-8B/70B, and Mistral-7B models to evaluate our methodology by measuring perplexity on both Wikitext-2 and C4 [36, 37, 1, 16, 27, 31]. Perplexity has been measured using teacher forcing with the output logits of all input tokens. We compared our method against (i) uniform quantization without grouping (intX), (ii) nonuniform quantization using NormalFloat [8] without grouping (nfX), as well as (iii) Atom [44] and FlexGen [34]. Note that Atom and FlexGen use uniform quantization with group sizes of 64 and 128, respectively. All the KVQuant models throughout this experiment section are calibrated using 16 calibration samples of sequence length 2K from the Wikitext-2 training set. See Appendix M for details on our experimental setup, including our methodology for computing KV cache size estimates. Table 1 shows the results for LLaMA models for the Wikitext-2 dataset. We compared our method with per-token quantization with and without grouping. The baseline configurations used by Atom and FlexGen are included for reference [44, 34]. We find that our method consistently outperforms baseline approaches by an especially large margin with 3-bit and 2-bit quantization. Once we incorporate outliers, we further push the performance of low-precision quantization, achieving 4-bit quantization with less than 0.02 perplexity degradation, 3-bit quantization with under 0.1 perplexity degradation, and 2-bit quantization with under 0.5 perplexity degradation on Wikitext-2, relative to the fp16 baseline, across all models (while attaining 3.7 ×, 4.8×, and 6.9 × memory savings, respectively). 73.7x4.8x6.8x 3.7x4.8x6.8x Figure 3: Perplexity results for the LLaMA-2-7B-32K model [5] as well as the Llama-2-70B-32K LongLoRA model [6] on the Wikitext-2 dataset, evaluated using different sequence lengths. Table 2: Passkey retrieval results across different context lengths for the LLaMA-2-7B-32K model (uptrained for long sequence lengths using positional interpolation [ 5]) as well as the Llama-2- 70B-32K LongLoRA model [6]. The values reported are the success rate for retrieving the passkey, computed over 50 samples. We also include comparisons with KIVI for reference, using the 2-bit configuration with group size of 32 and 128-element fp16 residual [ 26]. Average bit widths are estimated for each approach assuming 32K context length. Note that the open-source code for running KIVI with LLaMA does not support grouped-query attention, so we did not include comparisons with KIVI for Llama-2-70B-32K. Model Method 2K 4K 8K 16K 32K Avg. Bit Width LLaMA-2-7B-32K fp16 1 1 1 1 1 16 KIVI-2-gs32-r128 0.76 0.72 0.72 0.68 0.7 3.05 nuq4-1% 1 1 1 1 1 4.33 nuq3-1% 0.98 1 1 1 1 3.33 nuq2-1% 1 1 0.98 1 1 2.33 Llama-2-70B-32K fp16 1 1 1 1 1 16 nuq4-1% 1 1 1 1 1 4.35 nuq3-1% 1 1 1 1 1 3.35 nuq2-1% 0.98 0.98 0.96 1 0.74 2.35 4.2 Long Context Length Evaluation Perplexity Evaluation. We evaluated long context length performance using the LLaMA-2-7B- 32K model (uptrained for long sequence lengths using positional interpolation [ 5]) as well as the Llama-2-70B-32K LongLoRA model [6]. For evaluating performance on longer context lengths, we first evaluated perplexity on Wikitext-2 using larger amounts of input context, as shown in Figure 3 [6, 13]. The results demonstrate how our method maintains accuracy even for longer amounts of input context, thereby enabling efficient and accurate long sequence length inference. Passkey Retrieval Evaluation. We also evaluated the performance of our quantization method on passkey retrieval to assess the model’s ability to use its context. Passkey retrieval involves evaluating the model’s capacity to locate specific information in long texts [19], and this can be used to effectively measure the maximum distance over which a token can attend during the inference stage. We used the passkey evaluation framework from [45] (which is based on the methodology from [28]) to evaluate retrieval performance. The passkey retrieval results are provided in Table 2, demonstrating how our method is able to maintain the retrieval performance for long context length models. We also include comparisons with the passkey retrieval when using KIVI for the LLaMA-2-7B-32K model [26], demonstrating that our approach can attain higher retrieval rate for the same compression level. Our improved performance on retrieval tasks can be attributed to our improved representation of all tokens equally. This approach differs from KIVI, which preserves a local window of residual tokens in fp16. Therefore, while KIVI is effective at representing the tail part of the context, it may provide less benefit for tasks requiring the utilization of the full context window. LongBench Evaluation. Table 3 shows evaluation on LongBench [3] for the LLaMA-2-7B-32K model. LongBench contains a suite of long-context length evaluation benchmarks including QA tasks, summarization, and few-shot learning [ 3]. The max input context length is set at 31500, and results using KIVI are also included for reference [26]. Our results demonstrate that our 3-bit 8Table 3: LongBench evaluation for the LLaMA-2-7B-32K model using KVQuant-3bit-1%. Com- parisons with KIVI are included for reference, using the configuration with group size of 32 and 128-element fp16 residual [26]. Average bit widths are estimated for each approach assuming 12.2K context length, which was the average number of tokens across all tasks. Config Avg. bitNtrvQAQasperMF-enHotpot2WikiMusiqueGovRepQMSumMNewsTRECTriviQASamSumRBenchLCC PsgRetrPsgCntAvg. fp16 Baseline 16 17.96 10.51 33.43 12.55 12.53 6.19 29.65 16.99 22.15 71 87.79 43.97 59.99 62.14 23 1.50 31.96 KIVI-2-gs32-r128 3.17 19.25 10.66 24.78 12.48 11.19 6.38 27.05 16.36 23.37 71 80.80 43.93 57.74 60.61 13.58 1.50 30.04 KVQuant-3bit-1% 3.33 18.87 13.67 30.93 12.07 12.55 6.25 27.10 16.53 16.54 71 87.55 43.95 59.50 61.52 19.5 1.75 31.21 Table 4: RULER evaluation results for the LLaMA-2-7B-32K model with KVQuant quantization methods. We report accuracy across RULER tasks, comparing our KVQuant configurations to baseline and KIVI approaches. A maximum context length of 32K is used for evaluation. Our results show that our method retains baseline accuracy even with aggressive quantization and pruning. Config Avg. bit Niah1 Niah2 Niah3 MKey1MKey2MKey3MValueMQueryVT CWE FWE QA1 QA2 Avg. fp16 Baseline 16 100 99.8 98.6 94 68.2 11 55.95 64.5 37.88 9.64 30.4 31.6 31.6 56.40 KIVI-2-gs32-r128 3.05 76 85.6 59.6 72.6 11.4 0 34.7 46.45 39.6 8.26 30.53 24.8 27.6 39.78 KVQuant-3bit-1% 3.33 99.8 98.8 95.2 92.8 61.6 6.4 47.5 54.45 41.04 8.52 29.33 31.0 31.0 53.65 KVQuant-2bit-1% 2.33 95.4 86.8 49.8 73.6 23.4 0 16.65 22.95 22.52 5.14 24.0 26.4 28.4 36.54 model can attain minimal degradation relative to the fp16 baseline, outperforming KIVI for a similar compression level. RULER Evaluation. Finally in Table 4, we provide evaluation of KVQuant and KIVI on the RULER benchmark suite [15] using LLaMA-2-7B-32K. As can be seen in the table, 3-bit KVQuant achieves 14% better score against KIVI with a similar average bit-width. Furthermore, our 2-bit KVQuant achieves similar accuracy to KIVI with 1.5× smaller bit-width. 4.3 Joint Weight and KV Cache Quantization Table 5 provides results for our KV cache quantization method when the weights are also quantized using the methodology in SqueezeLLM [ 17]. We observe minimal perplexity degradation when leveraging our KV cache quantization approach, even when weights are also quantized to reduced precision. In particular, we observe small 0.02 and 0.1 perplexity degradation of 4-bit and 3-bit weight-only quantization, respectively, when quantizing the KV cache using nuq4-1% for the LLaMA- 7B and LLaMA-13B models. These results demonstrate how our method is compatible with existing weight-only quantization methods. 4.4 Performance Analysis and Memory Savings Table 6 shows kernel benchmarking results using a batch size of 1 for the 4-bit dense-and-sparse compression and matrix-vector kernel implementations. We show results across different sequence lengths to assess the performance of the kernels at different points during generation. We report latency benchmarked on an A6000 GPU. The results show that for the Key and Value multiplications, we can achieve 1.2-1.6× and 1.3-1.7× latency savings, respectively, relative to the baseline. We have integrated these kernels into an end-to-end generation pipeline that is able to compress activations dynamically during inference, thereby achieving significant memory savings and allowing for either larger batch sizes or longer sequence lengths. Appendix A highlights the benefits of KVQuant in supporting longer context lengths through reducing KV cache memory footprint. As shown in Table 8 in Appendix A, our nuq2 method provides 8× KV cache compression and enables serving the quantized LLaMA-7B model with a context length of 1M tokens on a single A100 GPU, as well as enabling serving the LLaMA-7B model with 10M context length on an 8-GPU system. Our results show little degradation compared to baseline fp16 inference 9Table 5: KV cache quantization results when KVQuant is applied in conjunction with the weight quantization methodology in SqueezeLLM [17]. w4-s45 and w3-s45 for weights refer to the 4-bit and 3-bit dense-and-sparse weight quantization approaches in [ 17], respectively. See Appendix M for experimental details. Weights KV Cache LLaMA-7B LLaMA-13B Avg. Bits (KV Cache) fp16 fp16 5.68 5.09 16 w4-s45 fp16 5.77 5.17 16 nuq4-1% 5.79 5.18 4.32-4.33 w3-s45 fp16 6.13 5.45 16 nuq3-1% 6.23 5.52 3.32-3.33 Table 6: Average latency (in microseconds) for the Key and Value nuq4-1% kernels, benchmarked on an A6000 GPU for the LLaMA-2-7B-32K model across different sequence lengths ( l). fp16 matrix-vector multiplication latencies are included for reference, and the fp16 Key multiplication time also includes the time to apply RoPE to the newly appended Key vector. Section 3.7 and Appendix R provide additional details for our kernel implementation, Appendix R describes our benchmarking methodology, and Table 22 provides a detailed breakdown of kernel runtime on an A6000 GPU. Activation Operation l=2K l=4K l=16K Key fp16 Matvec 33.3 59.1 219.4 Key nuq4-1% 25.6 39.9 126.3 Value fp16 Matvec 26.0 50.2 203.7 Value nuq4-1% 22.1 37.9 124.5 while providing significant compression, demonstrating the benefits of our approach for enabling accurate and efficient long sequence length inference. 5 Conclusion As context lengths in LLMs increase, the KV cache activations surface as the dominant contributor to memory consumption. Quantization is a promising approach to reduce the size of KV cache activations, but prior solutions failed to represent activations accurately in ultra-low precisions, such as sub-4-bit. In contrast, we achieve accurate ultra-low precision KV cache quantization. By quantizing Keys per-channel before applying RoPE, we are able to better match the outlier distribution and mitigate the impacts of RoPE on quantization (due to it mixing pairs of channels which may have different average magnitudes). We use non-uniform quantization to better allocate the small number of quantization signposts at low precision. We observe significant accuracy improvements when employing dense-and-sparse quantization, particularly when detecting outliers at the same granularity as we compute quantization scaling factors. Crucially, we demonstrate that we can perform accurate calibration offline for Keys, as well as efficient online scaling factor and outlier threshold computation for Values. By leveraging these methods, we are able to enable accurate low-precision activation quantization, achieving 4.8x compression (nuq3-1% outliers) with only 0.1 perplexity degradation across different LLaMA, Llama-2, Llama-3, and Mistral models. Our methodology therefore supports inferring the LLaMA-7B model with a context length of 10M on an 8-GPU serving system. Through our efficient kernel implementation, we are able to show improved latency relative to the fp16 baseline, demonstrating how our method allows for improved latency in addition to the memory savings. Acknowledgements The authors would like to acknowledge Nicholas Lee for helpful discussions and feedback. We acknowledge gracious support from Intel, Furiosa, Apple, Samsung SAIT, POSCO HOLDINGS, and NVIDIA. We also appreciate the support from Microsoft through their Accelerating Foundation Model Research, including great support from Sean Kuno. Furthermore, we appreciate support from Google Cloud, the Google TRC team, and specifically Jonathan Caton, and Prof. David Patterson. Prof. Keutzer’s lab is sponsored by the Intel corporation, Intel One-API, Intel VLAB team, the Intel 10One-API center of excellence, as well as funding through BDD and BAIR. We appreciate great feedback and support from Ellick Chan, Saurabh Tangri, Andres Rodriguez, and Kittur Ganesh. Sehoon Kim would like to acknowledge the support from the Korea Foundation for Advanced Studies (KFAS). Michael W. Mahoney would also like to acknowledge a J. P. Morgan Chase Faculty Research Award as well as the DOE, NSF, and ONR. Our conclusions do not necessarily reflect the position or the policy of our sponsors, and no official endorsement should be inferred. References [1] AI@Meta. Llama 3 model card. 2024. [2] Anthropic. Introducing claude 2.1, Nov 2023. [3] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. [4] Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding and overcoming the challenges of efficient transformer quantization. arXiv preprint arXiv:2109.12948, 2021. [5] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023. [6] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. arXiv preprint arXiv:2309.12307, 2023. [7] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. [8] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023. [9] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: A sparse-quantized representation for near-lossless llm weight compression. arXiv preprint arXiv:2306.03078, 2023. [10] Goran Flegar and Enrique S Quintana-Ortí. Balanced csr sparse matrix-vector product on graphics processors. In Euro-Par 2017: Parallel Processing: 23rd International Conference on Parallel and Distributed Computing, Santiago de Compostela, Spain, August 28–September 1, 2017, Proceedings 23, pages 697–709. Springer, 2017. [11] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [12] Amir Gholami, Zhewei Yao, Sehoon Kim, Michael Mahoney, and Kurt Keutzer. Ai and memory wall. RiseLab Medium Post, 2021. [13] Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Simple on-the-fly length generalization for large language models. arXiv preprint arXiv:2308.16137, 2023. [14] Jung Hwan Heo, Jeonghoon Kim, Beomseok Kwon, Byeongwook Kim, Se Jung Kwon, and Dongsoo Lee. Rethinking channel dimensions to isolate outliers for low-bit weight quantization of large language models. arXiv preprint arXiv:2309.15531, 2023. [15] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. Ruler: What’s the real context size of your long-context language models? arXiv preprint arXiv:2404.06654, 2024. [16] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [17] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023. 11[18] Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W Mahoney, et al. Full stack optimization of transformer inference: a survey. arXiv preprint arXiv:2302.14017, 2023. [19] Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can context length of open-source llms truly promise? In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023. [20] Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. Snapkv: Llm knows what you are looking for before generation, 2024. [21] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023. [22] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with blockwise ringattention, 2024. [23] Ruikang Liu, Haoli Bai, Haokun Lin, Yuening Li, Han Gao, Zhengzhuo Xu, Lu Hou, Jun Yao, and Chun Yuan. Intactkv: Improving large language model quantization by keeping pivot tokens intact, 2024. [24] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free quantiza- tion aware training for large language models. arXiv preprint arXiv:2305.17888, 2023. [25] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. arXiv preprint arXiv:2305.17118, 2023. [26] Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: Plug-and-play 2bit kv cache quantization with streaming asymmetric quantization. 2023. [27] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016. [28] Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context length for transformers. arXiv preprint arXiv:2305.16300, 2023. [29] Roman Novak, Yasaman Bahri, Daniel A Abolafia, Jeffrey Pennington, and Jascha Sohl- Dickstein. Sensitivity and generalization in neural networks: an empirical study. arXiv preprint arXiv:1802.08760, 2018. [30] OpenAI. New models and developer products announced at devday 2023, Nov 2023. [31] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv e-prints, 2019. [32] Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo Luschi, and Douglas Orr. Sparq attention: Bandwidth-efficient llm inference. arXiv preprint arXiv:2312.04985, 2023. [33] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally calibrated quantiza- tion for large language models. arXiv preprint arXiv:2308.13137, 2023. [34] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. Flexgen: High-throughput generative inference of large language models with a single gpu. In International Conference on Machine Learning, pages 31094–31116. PMLR, 2023. [35] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [36] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 12[37] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [38] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, and Xianglong Liu. Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling. arXiv preprint arXiv:2304.09145, 2023. [39] Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier suppression: Pushing the limit of low-bit transformer language models. Advances in Neural Information Processing Systems, 35:17402–17414, 2022. [40] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the- art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38–45, Online, October 2020. Association for Computational Linguistics. [41] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pages 38087–38099. PMLR, 2023. [42] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks, 2023. [43] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H _2 o: Heavy-hitter oracle for efficient generative inference of large language models. arXiv preprint arXiv:2306.14048, 2023. [44] Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom: Low-bit quantization for efficient and accurate llm serving. arXiv preprint arXiv:2310.19102, 2023. [45] Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. Pose: Efficient context window extension of llms via positional skip-wise training. arXiv preprint arXiv:2309.10400, 2023. 13A Memory Bottlenecks for Long Context Length Inference Table 7 shows the model size and KV cache memory requirements for different LLaMA models with different sequence lengths. For short sequence lengths, the model weights are the primary memory bottleneck. However, for longer sequence lengths and larger batch sizes, the KV cache memory is the main bottleneck. This is particularly pronounced when the weights are already quantized to low precision. In our work, we demonstrate that we can help address the KV cache memory bottleneck through low-precision KV cache quantization. By compressing the KV cache to 2-bit precision, we can enable 1M context length inference with the LLaMA-7B model on a single A100-80GB GPU, and we can also enable 10M context length inference with the LLaMA-7B model on an 8-GPU system. Table 8 shows the KV cache memory requirements for 128K, 1M, and 10M sequence lengths, with the KV cache stored in fp16 as well as 4-bit, 3-bit, and 2-bit precision with KVQuant. As one can see, our method provides 3.7× KV cache compression (nuq4-1%) and enables serving the quantized LLaMA-65B model with a context length of 32K tokens on a single A100-80GB GPU (requiring 30.3GB for the model weights compressed to 4-bit, and 46.5GB for the KV cache when compressed with nuq2-1%), and our nuq2 method enables serving the LLaMA-7B model with a context length of 1M tokens on a single A100 GPU (requiring 64GB for the KV cache). Additionally, when considering an 8-GPU serving system, we enable serving the LLaMA-7B model with 10M context length (with nuq2), or the LLaMA-65B model with 1M context length (with nuq3). Our results show little degradation compared to baseline fp16 inference while providing significant compression, demonstrating the benefits of our approach for enabling accurate and efficient long sequence length inference. Table 7: Model size and activation memory size estimates for different sequence lengths and batch sizes (BS) for different LLaMA models. For long sequence lengths and larger batch sizes, activation memory is the main bottleneck (particularly when weights are already quantized to low precision). By compressing the KV cache to 2-bit precision, we can enable1M context length inference with the LLaMA-7B model on a single A100-80GB GPU, and we can also enable 10M context length inference with the LLaMA-7B model on an 8-GPU system. BS Model Model Size (GB) KV Cache Size w/ Diff. Seq Len (GB) 16 → 2-bit 32K 128K 1M 10M (16 → 2-bit) 1 7B 12.6 → 1.6 16 64 512 4883 → 610 13B 24.1 → 3.0 25 100 800 7629 → 954 30B 60.3 → 7.5 49 195 1560 14877 → 1860 65B 121.1 → 15.1 80 320 2560 24414 → 3052 4 7B 12.6 → 1.6 64 256 2048 19531 → 2441 13B 24.1 → 3.0 100 400 3200 30518 → 3815 30B 60.3 → 7.5 195 780 6240 59509 → 7439 65B 121.1 → 15.1 320 1280 10240 97656 → 12207 B Additional Related Works B.1 Outlier-Aware LLM Quantization LLMs have been known to have distinct outliers both in weights and activations [ 7, 9, 17]. SqueezeLLM and SpQR both decompose the weight matrix into a sparse matrix containing a small portion of outliers and a dense matrix that can be accurately quantized to low precision (referred to as dense-and-sparse or sparse-quantized representation) [9, 17]. LLM.int8() [7] handled particular outlier channels separately in higher precision, and SmoothQuant [41] migrates quantization difficulty due to outlier channels to weights in order to support joint weight-activation quantization. Other works reconsidered the dimension along which we quantize in order to reduce quantization error (or else added per-channel compensation to improve quantization performance) [4, 14, 39, 38]. In this work, we demonstrate that per-channel pre-RoPE Key quantization provides significant accuracy benefits given the outlier structure in Keys, and that dense-and-sparse quantization can be efficiently applied for KV cache quantization. 14Table 8: Activation memory size estimates (GB) for 128K, 1M, and 10M sequence length ( l) for different LLaMA models. By compressing the KV cache to 2-bit precision, we can enable 1M context length inference with the LLaMA-7B model on a single A100-80GB GPU, and we can also enable 10M context length inference with the LLaMA-7B model on an 8-GPU system. Model Method l=128K l=1M l=10M LLaMA-7B fp16 64.0 512.0 4882.8 nuq4 16.0 128.1 1221.9 nuq4-1% 17.3 138.4 1319.6 nuq3 12.0 96.1 916.7 nuq3-1% 13.3 106.4 1014.4 nuq2 8.0 64.1 611.5 nuq2-1% 9.3 74.4 709.2 LLaMA-65B fp16 320.0 2560.0 24414 nuq4 80.0 640.3 6106.5 nuq4-1% 86.5 691.5 6595.0 nuq3 60.0 480.3 4580.6 nuq3-1% 66.5 531.5 5069.1 nuq2 40.0 320.3 3054.7 nuq2-1% 46.5 371.5 3543.3 B.2 Non-uniform LLM Quantization Non-uniform quantization has also been applied in the context of LLMs. Non-uniform quantization allows for more flexible quantization signpost placement relative to uniform quantization methods, enabling improved accuracy for the same bit precision [17, 8]. Building on the observation that model parameters tend to be approximately normally-distributed, prior work has proposed the NormalFloat datatype [8]. SqueezeLLM [ 17] derived per-channel non-uniform quantization signposts using a sensitivity-weighted k-means approach. In this work, we show that we can derive accurate per-layer non-uniform datatypes using a sensitivity-weighted k-means approach with KV cache activations. C RoPE Equation The rotation matrix for RoPE is provided in Equation 2, where c and s are cosine and sine functions, θi = 10000−2(i−1)/d, d is the attention head dimension, and n is the current position in the sequence:   c(nθ1) −s(nθ1) ··· 0 0 s(nθ1) c( nθ1) ··· 0 0 ... ... ... ... ... 0 0 ··· c(nθd/2) −s(nθd/2) 0 0 ··· s(nθd/2) c( nθd/2)   (2) The Query vectors computed at each iteration will have RoPE applied (to obtain ˜Qm = Rd θ,m ∗ Qm). When caching Key vectors, we therefore need to either cache ˜Kn = Rd θ,n ∗ Kn, or else we need to cache Kn and apply Rd θ,n on-the-fly during inference. In order to apply Rd θ,n efficiently on-the-fly, we leverage the element-wise formulation of RoPE rather than the matrix-multiplication formulation from Equation 2. The element-wise formulation for Rd θ,nx is as follows, where ⊙ is the element- wise multiplication operator (note that the formulation that we use matches the implementation in the Transformers library for LLaMA [ 40], and it is a different but equivalent formulation to the element-wise expression in [35]): 15  x1 x2 ... xd 2 xd 2 +1 ... xd−1 xd   ⊙   c(θ1n) c(θ2n) ... c(θd 2 n) c(θ1n) ... c(θd 2 −1n) c(θd 2 n)   +   −xd 2 +1 −xd 2 +2 ... −xd x1 ... xd 2 −1 xd 2   ⊙   s(θ1n) s(θ2n) ... s(θd 2 n) s(θ1n) ... s(θd 2 −1n) s(θd 2 n)   (3) By leveraging this element-wise implementation, we can apply RoPE on-the-fly to the Key activations (after dequantizing the Key activations and before multiplying them with the corresponding elements in the Query vector). D Derivation for Sensitivity Analysis To compute the sensitivity analysis we largely follow the derivation in [29], which was originally provided to compute sensitivity of a NN based classifier but can be extended with minor modifications for quantization. In particular, the sensitivity measure is based on how much the loss output of the model is perturbed. To compute this we denote activations at a layer before quantization as A, after quantization as AQ, quantization perturbation in activation as A − AQ, and the gradient of the Loss function w.r.t. activation as J(A) = ∂L ∂A (A). By making the assumption that the quantization perturbation of different activations follow a Gaussian distribution with zero mean, we can show that the sensitivity of an activation value is proportional to Fii \u0000 A − Q(A) \u00012 as was given in Equation 1: E∆A h |L(A) − L(A + ∆A)|2 i ≈ E∆A h\u0000 J(A)T ∆A \u00012i = E∆A \"\u0010X i Ji∆Ai \u00112 # = E∆A \"X i J2 i ∆A2 i # = X i J2 i E∆A \u0002 ∆A2 i \u0003 . Here note that we first assume a first order Taylor series expansion to approximate the perturbation to the loss, and then use the zero mean assumption of the quantization perturbation to derive the second line. To approximate the E∆A \u0002 ∆A2 i \u0003 we use empirical evaluation of the expectation by sampling multiple different inputs and empirically computing the resulting quantization perturbation which results in Equation 1. E Derivation for Quantization Error In our work, before applying the sensitivity-weighted K-means to derive quantization signposts, we normalize each element Ai in the flattened activation A. This normalization for Ai involves a shift by a zero-point zi followed by rescaling the quantization signposts by a scaling factor si, where si and zi are the scaling factor and zeropoint corresponding to element Ai: Ai,norm = Ai − zi si , (4) where Ai and Ai,norm are element i from activation A before and after normalization, respectively. We then quantize Ai,norm to Q(Ai,norm) with quantization error ∆Ai,norm. After we dequantized, we rescale each element by si and add zi to get the recovered quantized activation value Q(Ai): 16Q(Ai) = si Q(Ai,norm) + zi. (5) As such, if there is quantization error ∆Ai,norm in Ai,norm, this will be scaled by si in terms of the error in Ai, i.e., ∆Ai = si∆Ai,norm. For activation A which is normalized to Anorm (with corresponding scaling factors si for each element Ai), minimizing the sensitivity-weighted quantization error as expressed in Equation 1 gives us the following expression, which we can minimize using the normalized activations across all N elements from the samples in a calibration set: Q(A)∗ ≃ arg min Q NX i=1 Fii \u0000 Ai − Q(Ai) \u00012 (6) = arg min Q NX i=1 Fii \u0010 s2 i \u0000 Ai,norm − Q(Ai,norm) \u00012\u0011 (7) F Key and Value Dynamic Range Figure 4 shows the portion of the elements contained within difference percentages of the dynamic range for both Keys and Values. The majority of values (∼ 99%) are contained in a small portion of the dynamic range, and a small portion of numerical outliers skew the dynamic range that must be represented. This motivates our dense-and-sparse approach which removes numerical outliers and stores them in a separate sparse matrix, thereby restricting the range that needs to be represented in the dense component. 0 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Normalized Magnitude Keys 0 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Normalized Magnitude Values t100 t99.99 t99.9 t99 Figure 4: Distribution of the magnitude of elements of Key (Pre-RoPE) and Value activations for different layers of LLaMA-7B, computed on a single sample with sequence length 2K from the Wikitext-2 dataset. The normalized magnitude is computed by dividing by the largest magnitude value in that layer. As one can see, for both Key and Value activations, the majority of values lie in a small portion of the dynamic range, with a few numerical outliers skewing the dynamic range (and thereby reducing the fidelity when quantizing to low precision). G Per-Channel Key Quantization Ablations As shown in Table 9, per-channel quantization for Keys and per-token quantization for Values outperforms the standard per-token quantization approach for both Keys and Values, yielding an improvement of 3.82 perplexity for the LLaMA-7B model at 3-bit precision. This demonstrates the 17benefits of per-channel Key quantization to mitigate the large outlier channels in Keys. Note that for all experiments using per-channel quantization, we use an fp16 zeropoint rather than a low-precision zeropoint that is rounded to the nearest integer value. We do this since for some of the Key channels, all of the elements are positive or all of the elements are negative, meaning that the zeropoint will fall outside of the range that is representable by a low-precision integer (and rounding it to the nearest low-precision value can degrade performance). Additionally, we observe that per-channel quantization for Values actually performs worse than per-token quantization. We hypothesize that this behavior is because per-channel Value quantization leads to greater error accumulation in particular output values (since the result of the attention scores multiplied by one channel of the Values will be localized to a single value in the output vector), which leads to greater quantization error at later model layers. Another concurrent work, KIVI [26], observes similar behavior for per-channel Value quantization, which they attribute to the fact that per-token Value quantization confines the error to each token. Assuming that the output is a weighted sum of only a few important tokens (as only a few attention scores are large), a perturbation in these tokens can lead to significant degradation. Per-token Value quantization therefore ensures that the quantization of unimportant tokens does not adversely impact the important tokens. Table 9: Ablation Study: Perplexity comparison of per-token and per-channel quantization for KV cache activations for LLaMA-7B. PT refers to per-token quantization, and PC refers to per-channel quantization. Datatype Key Dim. Value Dim. Perplexity KV Cache Size (GB) Seqlen 128K fp16 - - 5.68 64.0 int3 PT PT 10.87 12.0 int3 PC PC 223 12.0 int3 PC PT 7.05 12.0 H Pre-RoPE Key Quantization Ablations As shown in Table 10, pre-RoPE Key quantization achieves higher accuracy than post-RoPE quanti- zation, with an improvement of 0.82 perplexity for 3-bit quantization with the LLaMA-7B model. These results show that the rotary positional embeddings make Key quantization more challenging due to mixing pairs of channels with different magnitudes. Pre-RoPE quantization thereby allows for more accurate quantization at low precision. Table 10: Ablation Study: Perplexity comparison of Pre-RoPE and post-RoPE Key quantization for LLaMA-7B (using per-channel Key quantization and per-token Value quantization). Pre-RoPE quantization leads to significant improvement (see Section 3.2 for more details). Datatype Scheme Perplexity KV Cache Size (GB) Seqlen 128K fp16 - 5.68 64.0 int3 post-RoPE 7.05 12.0 int3 pre-RoPE 6.23 12.0 I Sensitivity-Weighted Non-Uniform Quantization Ablations Table 11 shows perplexity evaluation results across different LLaMA, Llama-2, and Mistral models on Wikitext-2 for different datatypes, including nuq3, nuq3 without using sensitivity-weighting, as well as nuq3 with sensitivity-weighting but without accounting for the per-channel scaling factors when performing k-means. We observe particularly noticeable gains relative to uniform quantization for 3- bit and 2-bit quantization, where the benefits of non-uniform quantization are more pronounced due to the reduced precision. These results demonstrate the benefits of our sensitivity-weighted non-uniform quantization approach relative to NormalFloat quantization [8], as we achieve consistent accuracy improvements of up to 0.33 perplexity across different models. These results also demonstrate the 18necessity of our sensitivity-weighting approach in order to derive performant non-uniform datatypes using a k-means based approach. Additionally, we observe distinct benefits when also accounting for the per-channel scaling factors when performing k-means. Table 11: Ablation Study: Ablation of our sensitivity-weighted non-uniform datatype for different models on Wikitext-2. All experiments use pre-RoPE per-channel quantization for Keys and per-token quantization for Values (meaning that all configurations are the same as in KVQuant, except for the datatype). We compare against both uniform (int3) and non-uniform (nf3) [8] approaches, as well as with using “unweighted” k-means (i.e., not sensitivity-weighted) and “Fisher-weighted k- means” (without accounting for per-channel scaling factors) to compute the non-uniform quantization signposts. Note that there is slight variation in average bitwidth across models due to the differing hidden dimensions. Results report perplexity with 2K/4K/8K sequence length for LLaMA, Llama-2, and Mistral, respectively. Method LLaMA Llama-2 Mistral-7B Avg. Num. Bits7B 13B 30B 65B 7B 13B 70B baseline 5.68 5.09 4.10 3.53 5.12 4.57 3.12 4.76 16 int3 6.23 5.60 5.09 5.18 5.95 5.98 3.26 5.26 3.00-3.02 nf3 6.05 5.42 4.51 3.84 5.55 5.15 3.27 5.13 3.00-3.02 nuq3 (unweighted) 6.84 6.16 5.37 4.57 8.52 7.66 3.67 5.29 3.00-3.02 nuq3 (Fisher-weighted) 6.01 5.34 4.41 3.74 5.49 4.83 3.26 5.03 3.00-3.02 nuq3 (KVQuant) 5.94 5.32 4.34 3.68 5.39 4.82 3.23 4.98 3.00-3.02 J Per-Vector Dense-and-Sparse Quantization Ablations Table 12 shows the performance improvements we observe when isolating a small portion of outliers and storing them in a sparse format. We provide results both with using a single per-matrix outlier threshold, as well as with applying separate outlier thresholds per-vector. In particular, we see greater improvements by employing outlier detection with a different threshold per-channel for Keys and per-token for Values. This provides additional benefits since some values which would be considered outliers for the entire matrix are not actually outliers within a particular channel (so they are not hard to quantize). It is therefore better to directly target the outliers that will skew the quantization range for a particular channel. By removing 1% of outliers using per-vector thresholds, we can achieve an additional 0.19 reduction in perplexity for the LLaMA-7B model at 3 bits, thereby enabling 3-bit quantization with under 0.1 degradation in perplexity. Table 12: Ablation Study: Perplexity comparison of different outlier isolation methods for LLaMA- 7B on Wikitext-2. Per-vector outlier detection allows for significant accuracy improvements relative to per-tensor outlier detection. All experiments use per-token quantization for Values and per-channel quantization for Keys (pre-RoPE). “PV” refers to using per-vector outlier thresholds, and “PM” refers to using a single per-matrix outlier threshold. Datatype % Outliers Outlier Dim. Perplexity KV Cache Size (GB) Seqlen 128K fp16 - - 5.68 64.0 nuq3 - - 5.94 12.0 nuq3 0.1% PM 5.89 12.2 nuq3 0.1% PV 5.82 12.2 nuq3 1% PM 5.85 13.3 nuq3 1% PV 5.75 13.3 K Attention Sink-Aware Quantization Ablations Table 13 provides perplexity with and without Attention Sink-Aware quantization across different LLaMA, Llama-2, and Llama-3 models. For all bit widths (4, 3, and 2-bit) and in both dense- only and dense-and-sparse quantization settings, Attention Sink-Aware quantization consistently yields perplexity improvement. Notably, the perplexity gain is more pronounced at lower bit widths 19and without sparsity. We observe particularly significant improvements for Llama-3 models, with perplexity improvements of 0.25 and 0.13 PPL for nuq2-1% on Llama-3-8B and Llama-3-70B, respectively. Table 13: Ablation Study: Perplexity with and without Attention Sink-Aware quantization on Wikitext-2 with various models. Attention Sink-Aware quantization consistently improves perplexity across different models and bit widths, particularly with lower bit widths and without sparsity. Datatype Attention Sink-Aware LLaMA-7B LLaMA-13B Llama-2-7B Llama-2-13B Llama-3-8B Llama-3-70B fp16 - 5.68 5.09 5.12 4.57 5.54 2.59 nuq4 X 5.72 5.14 5.17 4.62 5.64 2.65 nuq4 O 5.72 5.13 5.16 4.60 5.60 2.62 nuq4-1% X 5.69 5.10 5.13 4.59 5.57 2.60 nuq4-1% O 5.69 5.10 5.13 4.58 5.56 2.60 nuq3 X 5.94 5.32 5.39 4.82 6.10 2.99 nuq3 O 5.87 5.25 5.34 4.71 5.84 2.72 nuq3-1% X 5.75 5.15 5.18 4.62 5.67 2.66 nuq3-1% O 5.75 5.14 5.17 4.61 5.64 2.63 nuq2 X 8.47 7.29 11.20 23.34 16.63 6.79 nuq2 O 7.23 5.82 7.03 9.59 7.04 3.49 nuq2-1% X 6.05 5.39 5.47 4.85 6.29 2.95 nuq2-1% O 6.01 5.36 5.41 4.78 6.04 2.82 L Calibration Ablations Figure 5 outlines the accuracy and efficiency challenges which our work addresses in order to enable accurate and efficient KV cache quantization. We demonstrate that we can circumvent the challenges related to efficiently performing online per-channel scaling factor computation by instead calibrating offline for the scaling factor without hurting accuracy. Additionally, we show that we can perform per-token scaling factor and outlier threshold computation efficiently online during inference, thereby enabling accurate per-token quantization without compromising on efficiency. Per-channel Quantization  dim sequence length KeysNew Key Per-token Quantization  dim sequence length ValuesNew ValueChallenge: need to potentially recompute the scaling factor every time a new key is added if we computeitonline Challenge: Need to compute the scaling factor for each incoming token per-channel scaling factor per-token scaling factor Figure 5: One typically achieves better performance when the scaling factor/zero point are computed online. However, this is quite challenging to do for per-channel quantization, as these factors will not only need to be recomputed for every new Key appended to the Key cache, but also all the prior cached Keys will need to be updated. As such, we use a calibration set to compute per-channel scaling factors offline. A similar challenge exists for per-token quantization, but online calibration for this does not require updating prior cached Values. In Section 3.6 and Appendix L, we discuss how we are able to efficiently compute outlier thresholds / scaling factors for per-token calibration, thereby enabling online computation. Table 14 shows accuracy results when using offline calibration for computing the scaling factors for the Keys. For 3-bit quantization, we observe minor accuracy degradation when not employing outlier extraction methods. However, if we remove a small percentage of outliers, then the accuracy with offline calibration is the same as computing the scaling factors online per-channel during evaluation. This demonstrates that when incorporating outlier extraction methods, we are better able to perform offline calibration due to reduced sensitivity to outliers (either to outliers during calibration that exaggerate the quantization range, or to outliers during evaluation that cannot be represented accurately if there weren’t large outliers observed during calibration). 20Table 15 shows the runtime for the topk operation for the LLaMA-7B model (which is required for computing outlier thresholds online). It compares the runtime of the topk operation with the runtime for the QKV projections, finding that the topk runtime is 45% of the matrix-vector operation runtime for a single projection layer. The topk operation can also be performed efficiently on the CPU, so we can actually run this operation in parallel with the subsequent linear layer matrix-vector operations on the GPU (which is possible by computing the Value projection before the Key and Query projections). This allows us to compress the activations dynamically without added runtime overhead, thereby enabling online scaling factor computation for the Value tensors. Table 16 shows how both Fisher information computation and calibration (including k-means) per- layer take only a few minutes for the LLaMA-65B model on a typical server machine. Even if we perform calibration sequentially for each layer, the entire calibration process would take a maximum of 6 hours for the LLaMA-65B model at 4-bit precision. Table 14: Ablation Study: Model accuracy when using offline calibration for Keys with LLaMA-7B. When incorporating outlier detection, offline calibration for Keys is able to perform comparably with online calibration. All nuq3 experiments use per-token quantization for Values and per-channel quantization for Keys (pre-RoPE), and experiments with outliers use per-vector outlier detection. Datatype % Outliers Perplexity Perplexity (Online for K) (Offline for K) fp16 - 5.68 5.68 nuq3 - 5.91 5.94 nuq3 1% 5.75 5.75 Table 15: topk runtime on a vector of length 4096 for computing outlier thresholds when using 1% sparsity (compared with the runtime for the QKV matrix multiplications, which are 4096×4096 by 4096 matrix-vector multiplications for the LLaMA-7B model). The runtime is reported on a system with an A6000 GPU and an Intel Xeon Gold 6126 CPU. We find that the runtime for the topk operation is only 45% of the runtime of each matvec operation. Additionally, the topk operation can be performed efficiently on the CPU; we can therefore run this operation in parallel with subsequent linear layer operations on the GPU to compress the activations dynamically without added overhead. Operation Device Outlier % Runtime (ms) QKV Projection GPU - 0.172 topk CPU 1% 0.026 topk GPU 1% 0.088 QKV Projection / topk (Fused) GPU / CPU 1% 0.173 Table 16: Runtime for computing Fisher information as well as for calibration (including k-means) with 16 samples for LLaMA-65B quantization. Runtime for computing Fisher information was computed on an 8-GPU A100-80GB system. Runtime for calibration (including k-means) was performed on an Intel Xeon Gold 6442Y CPU, and is shown for a single layer. Note that calibration is independent for each layer, so it can be easily parallelized. Operation Runtime (minutes) Computing Fisher Information 2.8 4-bit Calibration Per-Layer (including k-means) 4.5 3-bit Calibration Per-Layer (including k-means) 2.7 2-bit Calibration Per-Layer (including k-means) 1.9 M Additional Experimental Details For our empirical evaluation, we use 16 calibration samples of sequence length 2K from the Wikitext- 2 training set (as well as the corresponding gradients) to derive the per-channel scaling factors and zero-points, and to derive the non-uniform datatypes for both Keys and Values. While we use KVQuant models that are calibrated on the Wikitext-2 dataset for all experiments, in Appendix L, we 21include an additional experiment that demonstrates the robustness of the calibration process to the choice of data. We measured perplexity on both Wikitext-2 and on C4 using a sequence length equal to the maximum context length of the model (2K for LLaMA, 4K for Llama-2, and 8K for Llama-3 and Mistral- 7B). For generative tasks, when processing the input prompt, the Key/Value matrix multiplications are computed using the fp16 Keys and Values, and then they are separately compressed into low precision. For baseline experiments, we use post-RoPE quantization, both since this is required from an efficiency perspective without a dedicated kernel implementation, and because it provides better accuracy when quantizing Keys per-token as shown in Appendix P. We make several assumptions in order to estimate average bit widths and KV cache sizes for different approaches. We compute these estimates assuming a sequence length of 128K (unless otherwise specified). For integer quantization, we assume a low-precision integer offset and a 16-bit scaling factor, whereas for NormalFloat and NUQ we assume that the zero-point and offset are each 16-bit. For the sparse matrices, 32-bit integers are assumed for the per-token indices (since we need to support long sequence lengths), and the elements and per-element indices are assumed to be 16-bit. This means that for CSR, the rows are assumed to be 32-bit and the columns and values are assumed to be 16-bit, whereas for CSC, the columns are assumed to be 32-bit and the rows and values are assumed to be 16-bit. N Comparison Between Different Datatypes and Sparsity Levels Table 17 shows perplexity across LLaMA, Llama-2, Llama-3, and Mistral models on Wikitext-2 using different datatypes. The results highlight that the NUQ datatype generally outperforms both uniformly quantized INT datatype and non-uniformly quantized NF datatype even after they are incorporated with grouping at the expense of increased bit-widths. This performance can be further improved by extracting 0.1% to 1.0% of outliers. Note that NUQ with a sparsity level 1.0% has a similar memory requirement as INT with a group size of 64, but achieves significant perplexity improvement across all models and bit widths. O Full Perplexity Evaluation Tables 18 and 19 show perplexity evaluation across all LLaMA, Llama-2, Llama-3, and Mistral models on Wikitext-2 and C4, respectively. These results demonstrate the benefits of our approach for KV cache compression across different models and bit widths. 22Table 17: Comparison of our NUQ datatype with and without sparsity against other data types on different models using the perplexity (PPL) measured on Wikitext-2. Non-uniform quantization (“nuq”) results are using pre-RoPE per-channel quantization for Keys. “gs64/128” refers to baseline experiments using grouping with group size 64/128. Note that there is a slight variation in average bitwidth across models due to the differing hidden dimensions. Method LLaMA Llama-2 Llama-3 Mistral-7B Avg. Num. Bits7B 13B 30B 65B 7B 13B 70B 8B 70B baseline 5.68 5.09 4.10 3.53 5.12 4.57 3.12 5.54 2.59 4.76 16 int4 5.98 5.32 4.34 3.73 5.66 5.01 3.31 7.89 14.03 4.97 4.00-4.02 int4-gs128 5.77 5.16 4.16 3.57 5.32 4.71 3.16 5.78 2.73 4.82 4.16 int4-gs64 5.73 5.14 4.14 3.56 5.25 4.66 3.14 5.68 2.66 4.80 4.31 nf4 5.87 5.23 4.25 3.63 5.47 4.90 3.22 6.42 3.86 4.91 4.00-4.03 nf4-gs128 5.77 5.17 4.17 3.58 5.30 4.71 3.16 5.84 2.75 4.83 4.25 nuq4 5.72 5.14 4.14 3.56 5.17 4.62 3.14 5.64 2.65 4.81 4.00-4.02 + 0.1% outliers 5.70 5.12 4.12 3.55 5.15 4.60 3.13 5.63 2.66 4.79 4.04-4.06 + 0.5% outliers 5.70 5.11 4.12 3.54 5.14 4.59 3.13 5.59 2.61 4.78 4.16-4.19 + 1.0% outliers 5.69 5.10 4.11 3.54 5.13 4.59 3.13 5.57 2.60 4.78 4.32-4.35 int3 10.87 8.69 6.82 6.37 22.71 18.26 7.68 125 158 7.64 3.00-3.02 int3-gs128 6.17 5.47 4.44 3.78 6.15 5.34 3.33 7.50 4.11 5.16 3.15 int3-gs64 5.93 5.29 4.26 3.66 5.64 4.98 3.23 6.38 3.23 5.00 3.30 nf3 7.33 6.21 5.46 4.44 9.96 9.50 4.06 61.07 98.64 6.30 3.00-3.03 nf3-gs128 6.26 5.52 4.54 3.83 6.21 5.43 3.38 7.79 4.74 5.23 3.25 nuq3 5.94 5.32 4.34 3.68 5.39 4.82 3.23 6.10 2.99 4.98 3.00-3.02 + 0.1% outliers 5.82 5.22 4.21 3.62 5.27 4.69 3.19 5.96 2.96 4.91 3.04-3.06 + 0.5% outliers 5.76 5.16 4.16 3.59 5.20 4.65 3.16 5.74 2.69 4.84 3.16-3.19 + 1.0% outliers 5.75 5.15 4.15 3.57 5.18 4.62 3.15 5.67 2.66 4.82 3.32-3.35 int2 11779 69965 1470 7272 4708 3943 976 2841 2164 573 2.00-2.02 int2-gs128 37.37 41.77 16.49 13.63 118 93.09 18.31 200 2092 51.96 2.14 int2-gs64 11.09 9.84 6.60 5.54 25.69 26.83 5.93 57.82 43.69 12.47 2.28 nf2 3210 5785 2044 5367 13601 4036 3680 30492 9486 903 2.00-2.03 nf2-gs128 351 141 60.97 31.69 635 642 71.21 1024 3091 253 2.25 nuq2 8.47 7.29 6.08 9.19 11.20 23.34 4.18 16.63 6.79 6.87 2.00-2.02 + 0.1% outliers 6.82 5.72 4.83 4.00 6.38 5.33 3.54 7.97 4.77 5.83 2.04-2.06 + 0.5% outliers 6.24 5.49 4.45 3.80 5.59 4.95 3.33 6.53 3.18 5.28 2.16-2.19 + 1.0% outliers 6.05 5.39 4.41 3.72 5.47 4.85 3.28 6.29 2.95 5.14 2.32-2.35 Table 18: Full evaluation of our method for different models on all LLaMA, Llama-2, Llama-3, and Mistral models using the perplexity on Wikitext-2. KVQuant results are using pre-RoPE per-channel quantization for Keys. Average bit widths assume a sequence length of 128K (ignoring context length limits for the models). Note that ATOM and FlexGen use 4-bit quantization with group sizes of 128 and 64 with uniform quantization, respectively, and we extend their methods to 3-bit and 2-bit quantization. We leverage Attention-Sink Aware quantization for all bit widths. We used post-RoPE quantization for all baseline methods since it achieves higher accuracy when quantizing Keys per-token as shown in Appendix P. Method LLaMA Llama-2 Llama-3 Mistral-7B Avg. Num. Bits7B 13B 30B 65B 7B 13B 70B 8B 70B baseline 5.68 5.09 4.10 3.53 5.12 4.57 3.12 5.54 2.59 4.76 16 int4 5.98 5.32 4.34 3.73 5.66 5.01 3.31 7.89 14.03 4.97 4.00-4.02 nf4 5.87 5.23 4.25 3.63 5.47 4.90 3.22 6.42 3.86 4.91 4.00-4.03 ATOM-4bit 5.77 5.16 4.16 3.57 5.32 4.71 3.16 5.78 2.73 4.82 4.16 Flexgen-4bit 5.73 5.14 4.14 3.56 5.25 4.66 3.14 5.68 2.66 4.80 4.31 KVQuant-4bit 5.72 5.13 4.13 3.55 5.16 4.60 3.13 5.60 2.62 4.80 4.00-4.02 KVQuant-4bit-1% 5.69 5.10 4.11 3.54 5.13 4.58 3.13 5.56 2.60 4.78 4.32-4.35 int3 10.87 8.69 6.82 6.37 22.71 18.26 7.68 125 158 7.64 3.00-3.02 nf3 7.33 6.21 5.46 4.44 9.96 9.50 4.06 61.07 98.64 6.30 3.00-3.03 ATOM-3bit 6.17 5.47 4.44 3.78 6.15 5.34 3.33 7.50 4.11 5.16 3.15 FlexGen-3bit 5.93 5.29 4.26 3.66 5.64 4.98 3.23 6.38 3.23 5.00 3.30 KVQuant-3bit 5.87 5.25 4.25 3.63 5.34 4.71 3.18 5.84 2.72 4.98 3.00-3.02 KVQuant-3bit-1% 5.75 5.14 4.15 3.57 5.17 4.61 3.15 5.64 2.63 4.82 3.32-3.35 int2 11779 69965 1470 7272 4708 3943 976 2841 2164 573 2.00-2.02 nf2 3210 5786 2044 5367 13601 4036 3680 30492 9486 903 2.00-2.03 ATOM-2bit 37.37 41.77 16.49 13.63 118 93.09 18.31 200 2092 51.96 2.14 FlexGen-2bit 11.09 9.84 6.60 5.54 25.69 26.83 5.93 57.82 43.69 12.47 2.28 KVQuant-2bit 7.23 5.82 4.87 4.03 7.03 9.59 3.45 7.04 3.49 6.84 2.00-2.02 KVQuant-2bit-1% 6.01 5.36 4.35 3.70 5.41 4.78 3.26 6.04 2.82 5.14 2.32-2.35 23Table 19: Full evaluation of our method for different models on all LLaMA, Llama-2, Llama-3, and Mistral models using the perplexity on C4. KVQuant results are using pre-RoPE per-channel quantization for Keys. Average bit widths assume a sequence length of 128K (ignoring context length limits for the models). Note that ATOM and FlexGen use 4-bit quantization with group sizes of 128 and 64 with uniform quantization, respectively, and we extend their methods to 3-bit and 2-bit quantization. We leverage Attention-Sink Aware quantization for all bit widths. We used post-RoPE quantization for all baseline methods since it achieves higher accuracy when quantizing Keys per-token as shown in Appendix P. Method LLaMA Llama-2 Llama-3 Mistral-7B Avg. Num. Bits7B 13B 30B 65B 7B 13B 70B 8B 70B baseline 7.08 6.61 5.98 5.62 6.63 6.05 4.97 7.10 5.78 5.71 16 int4 7.40 6.82 6.18 5.75 7.31 6.59 5.12 8.79 14.36 5.91 4.00-4.02 nf4 7.27 6.74 6.10 5.69 7.09 6.45 5.06 7.93 6.58 5.85 4.00-4.03 ATOM-4bit 7.16 6.67 6.02 5.65 6.87 6.20 5.00 7.32 6.03 5.76 4.16 FlexGen-4bit 7.12 6.64 6.00 5.63 6.79 6.15 4.99 7.23 5.84 5.75 4.31 KVQuant-4bit 7.11 6.64 6.00 5.63 6.68 6.08 4.99 7.18 5.80 5.75 4.00-4.02 KVQuant-4bit-1% 7.09 6.62 5.99 5.62 6.64 6.06 4.98 7.12 5.78 5.72 4.32-4.35 int3 12.97 10.95 9.13 8.27 30.14 28.57 16.00 63.75 301 8.84 3.00-3.02 nf3 8.90 7.84 7.43 6.37 14.92 13.75 5.96 68.38 148 7.27 3.00-3.03 ATOM-3bit 7.62 6.93 6.24 5.79 8.00 7.06 5.16 9.00 6.72 6.08 3.15 FlexGen-3bit 7.34 6.78 6.11 5.70 7.29 6.59 5.08 7.92 6.18 5.92 3.30 KVQuant-3bit 7.25 6.72 6.06 5.68 6.89 6.18 5.03 7.43 5.90 5.92 3.00-3.02 KVQuant-3bit-1% 7.13 6.64 6.00 5.63 6.69 6.09 4.99 7.19 5.81 5.76 3.32-3.35 int2 10892 100870 1411 7216 4708 4220 814 2113 1977 477 2.00-2.02 nf2 2850 4680 1617 5190 13081 4176 3217 78331 7616 1102 2.00-2.03 ATOM-2bit 43.49 56.25 21.07 17.05 113 97.04 23.67 135 3734 50.73 2.14 FlexGen-2bit 13.91 13.36 8.49 7.34 35.21 40.40 8.28 50.78 42.27 13.83 2.28 KVQuant-2bit 8.52 7.32 6.67 5.96 9.49 15.36 5.30 10.06 6.45 7.63 2.00-2.02 KVQuant-2bit-1% 7.33 6.78 6.11 5.70 6.96 6.25 5.08 7.60 5.96 6.05 2.32-2.35 24P Post-RoPE Per-Token Quantization Ablation Table 20 shows perplexity evaluation on Wikitext-2 for the LLaMA-7B model with uniform quan- tization, with Keys quantized pre-RoPE and post-RoPE. These results show that post-RoPE Key quantization is superior to pre-RoPE Key quantization when quantizing Keys per-token. This is because when rotating an outlier channel with large average magnitude and another channel with smaller average magnitude together, at some positions in the sequence, part of the magnitude from the outlier channel will be shifted to the smaller channel. This partially mitigates the impact of the outlier channel on skewing the quantization range for some of the tokens in the sequence. As such, for our baseline comparisons, we use post-RoPE per-token Key quantization to serve as a stronger baseline. Table 20: Model accuracy when using pre-RoPE and post-RoPE quantization for LLaMA-7B with per-token Key quantization. Our experiments demonstrate that post-RoPE quantization is superior when using per-token Key quantization. Therefore, we decided to use these results for baseline comparison with per-token quantization. Datatype Perplexity (Pre-RoPE) Perplexity (Post-RoPE) fp16 5.68 5.68 int4 6.02 5.98 int4-gs128 5.76 5.77 int3 14.68 10.87 int3-gs128 6.28 6.17 Q Experiments on Calibration Data Robustness To evaluate the robustness of our quantization method to the choice of calibration datasets, we measure the perplexity on Wikitext-2 and C4 using different quantized models calibrated with Wikitext-2 and C4. As shown in Table 21, the resulting perplexity numbers remain similar even when the models are calibrated with out-of-domain examples (e.g., calibrated with Wikitext-2 and evaluated on C4, and vice versa), demonstrating the robustness of the calibration process to the choice of data. Table 21: Perplexity (PPL) results on Wikitext-2 and C4 using different quantization schemes, calibrated using Wikitext-2 and C4. Datatype Wikitext-2 C4 Calib. with Wikitext-2 Calib. with C4 Calib. with Wikitext-2 Calib. with C4 4-bit, 1% sparsity 5.69 5.70 7.09 7.09 3-bit, 1% sparsity 5.75 5.75 7.13 7.13 2-bit, 1% sparsity 6.05 6.07 7.38 7.38 R Kernel Implementation Details We implemented 4-bit lookup table-based kernels for matrix-vector multiplication between the Key or Value activations (packed as a lookup table (LUT) plus indices into the LUT per-element) and a full-precision activation vector. These kernels load the compressed Key and Value activations and dequantize them only as needed in order to minimize memory bandwidth utilization. All arithmetic is performed in fp16. The lookup table entries are the values of the sensitivity-weighted non-uniform datatype for that particular layer scaled according to the range of activations that need to be represented [8]. When selecting between the Compressed-Sparse Column (CSC format) and the Compressed-Sparse Row (CSR) format for storing the outliers for the Keys and Values, we needed to consider how easy it would be to append new vectors. When using CSC format for the Key matrix, we only need to append a single element to the column vector, as well as one new element to the row and value vectors per nonzero element in that new column. If we used CSR format, we would need to insert the new column and value elements in the middle of the existing column and value vectors, and we would 25need to recompute the elements of the row vector. When using CSR format for the Value matrix, we only need to append a single element to the row vector, as well as one new element to the column and value vectors per nonzero element in that new row. If we used CSC format, we would need to insert the new row and value elements in the middle of the existing row and value vectors, and we would need to recompute the elements of the column vector. We therefore used the CSC format for the Key matrices and the CSR format for the Value matrices. One challenge with efficiently processing the sparse matrix-dense vector operation is that the sparsity distribution may be unbalanced. This poses a challenge for efficiently processing the sparse matrix on a GPU as there can be different numbers of nonzeros to process per thread. We therefore leverage a balanced sparse matrix-dense vector kernel based on [10, 17], which assigns an equal number of nonzeros per thread. This has greater synchronization overhead than assigning a single thread for an entire row or column when processing CSR/CSC matrices, but it leads to a more balanced work assignment between threads. We set the number of threads such that there were 10 nonzero values assigned to each thread. The dense non-uniform kernel and balanced sparse kernels are launched in one call to avoid overhead from summing the output vectors from these separate operations. A second potential challenge is that the quantization dimension for the Keys and Values is not aligned with the reduction dimension for the respective matrix-vector operations. If we used per-channel or per-token lookup tables for dequantization, we would need to load from a different lookup table for each element as we iterate along the reduction dimension. However, since our implementation uses a single per-layer datatype that is rescaled per-vector, we can load a single scaling factor and zero-point for each channel / token and broadcast it across all threads (while doing a table lookup from the shared per-layer datatype, which is duplicated such that each thread can access it efficiently in parallel). The use of a single per-layer datatype that is rescaled per-vector therefore allows for much more efficient GPU kernel implementations in the context of per-channel Key / per-token Value quantization. Table 22 shows a detailed breakdown of kernel runtime, including how much time is spent packing vectors into the compressed format and how much time is spent on the dense and sparse matrix- vector multiplications. For the fp16 baselines, we benchmarked runtime by averaging across 1000 iterations. When benchmarking our dense-and-sparse kernels (both for packing and matrix-vector multiplication), since the runtime of the sparse kernels can vary based on the sparsity pattern, we ran the LLaMA-2-7B-32K model and collected activations at each layer, and we then measured the average runtime across 1000 iterations for each layer in the model. We find that even with 1% sparsity, we can attain significant speedups of up to 1.7× relative to the fp16 matrix-vector multiply kernels, demonstrating how our methodology facilitates efficient inference with a low-precision quantized KV cache. Table 22: Average latency (in microseconds) for the Key and Value dense nuq4 kernels as well as for the sparse kernels (with 1% outliers), benchmarked on an A6000 GPU for the LLaMA-2-7B-32K model. Benchmarking results are reported for different sequence lengths ( l). fp16 matrix-vector multiplication latencies are included for reference, and the Key multiplication time also includes the time to apply RoPE to the newly appended Key vector. We find that our dense-and-sparse approach (even with 1% outliers) provides latency benefits relative to the fp16 baseline, even when accounting for the time to compress activations online. Activation Operation l=2K l=4K l=16K Key fp16 Matvec 33.3 59.1 219.4 Key (nuq4-1%) Packing 4.5 4.5 4.5 Dense Matvec 13.0 24.1 87.6 Sparse Matvec 8.1 11.2 34.2 Total Latency 25.6 39.9 126.3 Value fp16 Matvec 26.0 50.2 203.7 Value (nuq4-1%) Packing 4.1 4.1 4.1 Dense Matvec 10.0 17.8 62.0 Sparse Matvec 7.9 15.9 58.2 Total Latency 22.1 37.9 124.5 26S Limitations While our work enables accurate long-context length inference by reducing the memory requirements, there is significant work required for training long context length models with greater than 100K context length. This work is orthogonal to our efforts, which are constrained to efficient inference with long context length models. Additionally, our latency benchmarking results currently focus on memory-bandwidth bound generation rather than prompt processing (where we need to compress multiple Keys and Values at once). Finally, in the current end-to-end implementation, there are inefficiencies in how memory allocation is handled for updating the sparse matrix (where the data corresponding to the previous tokens have to be copied when concatenating them with the data from the new token). In future work, we plan to optimize this by doing blocked allocation to avoid overheads from reallocating memory. 27",
      "meta_data": {
        "arxiv_id": "2401.18079v6",
        "authors": [
          "Coleman Hooper",
          "Sehoon Kim",
          "Hiva Mohammadzadeh",
          "Michael W. Mahoney",
          "Yakun Sophia Shao",
          "Kurt Keutzer",
          "Amir Gholami"
        ],
        "published_date": "2024-01-31T18:58:14Z",
        "pdf_url": "https://arxiv.org/pdf/2401.18079v6.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This research introduces KVQuant, a novel method for ultra-low precision KV cache quantization to enable long context length LLM inference while maintaining high accuracy. The key contributions include: (i) achieving < 0.1 perplexity degradation with 3-bit quantization on Wikitext-2 and C4 for LLaMA, Llama-2, Llama-3, and Mistral models, outperforming existing approaches; (ii) enabling serving LLaMA-7B with a context length of up to 1 million on a single A100-80GB GPU and up to 10 million on an 8-GPU system; and (iii) developing custom CUDA kernels that achieve up to ~1.7x speedups for matrix-vector multiplications compared to baseline fp16 for LLaMA-7B.",
        "methodology": "KVQuant employs several novel methods for KV cache quantization. It uses (i) Per-Channel Key Quantization, adjusting the dimension along which Keys are quantized to better match their outlier-prone distribution, combined with per-token quantization for Values; (ii) Pre-RoPE Key Quantization, quantizing Key activations before the rotary positional embedding (RoPE) to mitigate its disruptive impact on channel magnitudes; (iii) Non-Uniform KV Cache Quantization (nuqX), where per-layer sensitivity-weighted non-uniform datatypes are derived offline on calibration data using a k-means solver and Fisher information matrix to optimize signpost placement; and (iv) Per-Vector Dense-and-Sparse Quantization, which isolates numerical outliers (e.g., 1%) separately for each vector (per-channel for Keys, per-token for Values) and stores them compactly in a sparse representation, thereby restricting the quantization range for the remaining dense elements. It also includes Attention Sink-Aware Quantization, preserving the first token in fp16 due to its sensitivity. The methodology leverages offline calibration for Key scaling factors and efficient online computation for Value scaling factors and outlier thresholds. Custom CUDA kernels are developed for efficient on-the-fly quantization, dequantization, RoPE application, and sparse matrix-dense vector multiplications.",
        "experimental_setup": "The evaluation was performed on LLaMA-7B/13B/30B/65B, Llama-2-7B/13B/70B, Llama-3-8B/70B, and Mistral-7B models. Perplexity (PPL) was measured on Wikitext-2 and C4 datasets using teacher forcing. Long context length performance was assessed using LLaMA-2-7B-32K and Llama-2-70B-32K LongLoRA models through perplexity on Wikitext-2, passkey retrieval, LongBench tasks (QA, summarization, few-shot learning), and RULER benchmark suite. Calibration for KVQuant models used 16 samples of 2K sequence length from the Wikitext-2 training set. Comparisons were made against uniform quantization (intX), NormalFloat (nfX), ATOM, FlexGen, and KIVI. Kernel benchmarking was conducted on an A6000 GPU for LLaMA-2-7B-32K at varying sequence lengths, measuring latency of Key and Value matrix-vector operations and packing. Memory savings were estimated for KV cache sizes at 128K, 1M, and 10M sequence lengths. Joint weight and KV cache quantization was evaluated using SqueezeLLM for weight quantization.",
        "limitations": "The current work enables efficient inference for long context length models but does not address the significant work required for training LLMs with context lengths greater than 100K. The latency benchmarking results primarily focus on memory-bandwidth bound generation rather than prompt processing, which involves compressing multiple Keys and Values simultaneously. Furthermore, the existing end-to-end implementation has inefficiencies in memory allocation when updating the sparse matrix, as data corresponding to previous tokens must be copied when concatenating with new token data.",
        "future_research_directions": "Future research directions include optimizing the memory allocation for updating the sparse matrix by implementing blocked allocation. This would help avoid overheads associated with reallocating memory in the current end-to-end implementation. While orthogonal to this work, continued advancements in training long context length models with greater than 100K context lengths would further complement the efficient inference capabilities provided by KV cache quantization."
      }
    },
    {
      "title": "ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification",
      "abstract": "KV cache stores key and value states from previous tokens to avoid\nre-computation, yet it demands substantial storage space, especially for long\nsequences. Adaptive KV cache compression seeks to discern the saliency of\ntokens, preserving vital information while aggressively compressing those of\nless importance. However, previous methods of this approach exhibit significant\nperformance degradation at high compression ratios due to inaccuracies in\nidentifying salient tokens. In this paper, we present ZipCache, an accurate and\nefficient KV cache quantization method for LLMs. First, we construct a strong\nbaseline for quantizing KV cache. Through the proposed channel-separable\ntokenwise quantization scheme, the memory overhead of quantization parameters\nare substantially reduced compared to fine-grained groupwise quantization. To\nenhance the compression ratio, we propose normalized attention score as an\neffective metric for identifying salient tokens by considering the lower\ntriangle characteristics of the attention matrix. Moreover, we develop an\nefficient approximation method that decouples the saliency metric from full\nattention scores, enabling compatibility with fast attention implementations\nlike FlashAttention. Extensive experiments demonstrate that ZipCache achieves\nsuperior compression ratios, fast generation speed and minimal performance\nlosses compared with previous KV cache compression methods. For instance, when\nevaluating Mistral-7B model on GSM8k dataset, ZipCache is capable of\ncompressing the KV cache by $4.98\\times$, with only a $0.38\\%$ drop in\naccuracy. In terms of efficiency, ZipCache also showcases a $37.3\\%$ reduction\nin prefill-phase latency, a $56.9\\%$ reduction in decoding-phase latency, and a\n$19.8\\%$ reduction in GPU memory usage when evaluating LLaMA3-8B model with a\ninput length of $4096$.",
      "full_text": "ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification Yefei He1 Luoming Zhang1 Weijia Wu2 Jing Liu3 Hong Zhou1† Bohan Zhuang1,3† 1Zhejiang University, China 2National University of Singapore, Singapore 3ZIP Lab, Monash University, Australia Abstract KV cache stores key and value states from previous tokens to avoid re-computation, yet it demands substantial storage space, especially for long sequences. Adaptive KV cache compression seeks to discern the saliency of tokens, preserving vital information while aggressively compressing those of less importance. However, previous methods of this approach exhibit significant performance degradation at high compression ratios due to inaccuracies in identifying salient tokens. Ad- ditionally, the compression process introduces excessive overhead, substantially increasing memory burdens and the generation latency. In this paper, we present ZipCache, an accurate and efficient KV cache quantization method for large lan- guage models (LLMs). First, we construct a strong baseline for quantizing KV cache. Through the proposed channel-separable tokenwise quantization scheme, the memory overhead of quantization parameters are substantially reduced com- pared to fine-grained groupwise quantization. To enhance the compression ratio, we propose normalized attention score as an effective metric for identifying salient tokens by considering the lower triangle characteristics of the attention matrix. The quantization bit-width for each token is then adaptively assigned based on their saliency. Moreover, we develop an efficient approximation method that decouples the saliency metric from full attention scores, enabling compatibility with fast attention implementations like FlashAttention. Extensive experiments demonstrate that ZipCache achieves superior compression ratios, fast generation speed and min- imal performance losses compared with previous KV cache compression methods. For instance, when evaluating Mistral-7B model on GSM8k dataset, ZipCache is capable of compressing the KV cache by 4.98×, with only a 0.38% drop in accuracy. In terms of efficiency, ZipCache also showcases a 37.3% reduction in prefill-phase latency, a 56.9% reduction in decoding-phase latency, and a 19.8% reduction in GPU memory usage when evaluating LLaMA3-8B model with a input length of 4096. 1 Introduction LLMs with the next-token-prediction scheme have achieved remarkable advancements in various text-related tasks, such as language understanding [13, 34, 10], content creation [1, 5, 36], coding [3, 29, 42] and mathematics [33, 23, 35]. In this generation scheme, the forthcoming token interacts with all previous tokens via the attention mechanism [38], where the query, key and value states will be †Corresponding author. Email: zhouhong_zju@zju.edu.cn, bohan.zhuang@gmail.com Preprint. Under review. arXiv:2405.14256v1  [cs.LG]  23 May 2024calculated for each token. As the past tokens will not be altered, previously computed key and value states can be stored as KV cache to prevent re-computations, significantly improving the generation speed. However, as the batch size and the input context length grows, the stored KV cache emerges as a new memory bottleneck for LLMs. For example, when serving a 175B-parameter LLM [1] with a batch size of 64 and a context length of 4096, the KV cache can occupy 1.2TB of memory space, while the model weights only require 350GB. Meanwhile, the size of KV cache will continue to increase as decoding progresses. Therefore, the compression of KV cache is crucial for the efficient deployment of LLMs. Recent compression methods for KV cache can be broadly categorized into two types. The first type of methods compresses the KV cache uniformly, without considering the significance of individual tokens. To preserve performance, these methods often rely on either high-precision quantization [21] or maintaining recent tokens in full-precision [32], which undoubtedly compromise the compression ratio. Additionally, if salient tokens are not among the most recent ones, such as in information retrieval tasks, it may result in degraded performance. The other type of methods [ 46, 43, 16] compress KV cache adaptively by identifying salient tokens and compresses them separately. This approach aligns with the observation that a minority of tokens contribute the majority of attention scores [41], potentially achieving higher compression ratios than non-adaptive methods. However, current adaptive KV cache compression methods [ 46, 43] use accumulated attention scores as a metric of token saliency, which is insufficient in two aspects. First, accumulated attention scores is inaccurate in identifying important tokens. Due to the presence of attention masks, the attention matrix is a lower triangular matrix. Earlier tokens tend to have larger softmax attention values and more attention scores to be accumulated, as illustrated in Figure 3. Under this metric, the saliency of the most recent tokens can never surpass that of the first token, thereby introducing a bias in determining token saliency. Additionally, to obtain accumulated attention scores, full attention matrices must be explicitly computed and stored, which can be inefficient for serving LLMs. Given an input context length of l, fast attention implementations such as FlashAttention [8, 7] only require O(l) memory by computing attention output in blocks without retaining complete attention matrices. By contrast, storing full attention matrices requires O(l2) memory, and the large number of memory accesses significantly slows down the inference speed, as depicted in Figure 4. Figure 1: Accuracy and efficiency compar- isons across various KV cache compression methods. Data is collected with LLaMA3- 8B model on Line Retrieval dataset. Among these methods, ZipCache achieves the high- est accuracy, generation speed and compres- sion ratio. Details can be found in the sup- plementary material. To address these challenges, we introduce ZipCache, an efficient KV cache compression method that attains ex- ceptionally high compression ratios by accurate salient token identification. Figure 1 presents an overview of latency-accuracy comparisons among ZipCache and diverse KV cache compression methods. We start by designing an efficient quantization baseline for com- pressing the KV cache. To preserve performance, prede- cessor methods [32, 21] employ fine-grained groupwise quantization, which involves independent quantization for a small channel group within each token. However, this method necessitates storing extensive quantization parameters and results in significant memory overhead. By contrast, we introduce a channel-separable quanti- zation scheme that decouples the quantization along channel and token dimensions. This method signifi- cantly reduces the quantization overhead without com- promising performance. To accurately recognize salient tokens, we introduce a new token saliency metric based on normalized attention scores, which alleviates the bias towards earlier tokens that accumulate more val- ues. All tokens, without exception, will be quantized to the target bit-width based on their estimated saliency, boosting the overall compression ratio. Moreover, to ease integration with fast attention implementa- tions, we introduce an efficient approximation of the token saliency metric. This approximation only relies on computing and storing attention scores from a few number of tokens, which we refer to as probe tokens. An effective probe token selection strategy is then introduced to minimize performance loss. As a result, the majority of tokens can benefit from fast attention implementations, significantly enhancing the generation speed. 2In summary, our contributions are as follows: • We establish an efficient channel-separable quantization scheme for KV cache, which significantly reduces the overhead of quantization parameters without compromising performance compared to fine-grained groupwise quantization approach. • We propose an accurate metric for assessing token saliency based on normalized attention scores. All tokens are adaptively quantized according to their assessed saliency, thereby improving the overall compression ratio. • We further develop an efficient approximation method for the token saliency metric that integrates seamlessly with fast attention implementations, enhancing generation speed. • By integrating these three techniques, we present ZipCache, an accurate and efficient framework for KV cache compression. Extensive experiments demonstrate that ZipCache reaches a new state-of-the-art performance for KV cache compression in terms of compression ratio, accuracy and generation efficiency. 2 Related Work 2.1 Model Quantization Quantization is a prevalent technique for compressing deep neural networks by representing model weights and activations with lower numerical bit-widths. This technique can be categorized into two primary approaches based on the necessity of fine-tuning: post-training quantization (PTQ) [26, 17, 14] and quantization-aware training (QAT) [28, 31]. For large language models (LLMs), where fine-tuning can be data- and computation-intensive, PTQ is often the preferred method [40, 11, 45, 27]. In this paper, we also quantize KV cache in a post-training manner. For both approaches, quantization can be implemented at various levels of granularity, including channelwise, tokenwise, and groupwise approach. Typically, a finer quantization granularity involves the independent quantization of smaller parameter groups, which often results in improved performance albeit at the cost of more quantization parameters and increased memory overhead. In the context of LLMs, fine-grained quantization is frequently utilized due to the presence of outliers [22, 45]. However, for KV cache compression, this will greatly reduce the overall compression ratio. Mixed precision quantization [39, 44, 12, 2] allocates varying bit-widths to distinct parts of a model or tensor, enabling a more compact compression. This approach originates from the observation that model components exhibit differing sensitivities to quantization. Consequently, components with low sensitivity can utilize reduced bit-widths without impairing performance. For LLMs, previous studies [46, 43, 30, 18] have shown significant disparities in the importance of tokens, indicating that heavy compression of non-critical tokens has minimal impact on overall performance. This insight highlights the applicability of mixed precision quantization for compressing the KV cache. 2.2 KV Cache Compression While KV cache effectively prevents re-computation and significantly enhances generation speed, its memory footprint is notably substantial with long-context input. To alleviate this, many efforts have been made to reduce the KV cache size. Based on the compression method, these methods can be categorized into two groups: token dropping [46, 16, 30] and KV cache quantization [43, 21, 32]. The former identifies and drops unimportant tokens in the KV cache. For example, H2O [46] only maintain 20% heavy-hitted tokens and 20% recent tokens while evicting the rest. However, discarding tokens permanently erases their information, which proves to be suboptimal for tasks such as retrieval [43]. Conversely, the latter category employs quantization on the cached key and value states, and mixed precision quantization can further be applied once token importance is identified [ 43]. To tackle the outliers present in the KV cache, these methods extract the outlier as full precision [21] or use finer-grained quantization scheme [32], which increases the quantization overhead. In this study, we propose an efficient channel-separable quantization scheme with reduced quantization overhead and strong performance. Additionally, both categories of methods commonly adopt accumulated attention scores as the metric for token importance [46, 43]. However, we observe that this criterion is inaccurate and can result in significant performance deterioration at low bit-widths. In contrast, we achieve superior compression performance by utilizing a more accurate metric for identifying salient tokens. 33 Preliminary 3.1 Attention Block in LLMs Given an input prompt, the generation process of LLMs can be broadly categorized into two distinct phases: the prefill phase, which computes and stores the KV cache for input tokens, and the decoding phase, where new tokens are generated through a next-token-prediction scheme. Given input data X and an attention block with its weight matrices WQ, WK and WV, the prefill phase can be formulated as: Q = XWQ, K = XWK, V = XWV, (1) A = Softmax \u0012QKT √dk \u0013 , O = AV. (2) Here, dk is the dimension of the key, and A refers to the attention scores. K and V will be stored as KV cache. For clarity, we have omitted the output projection. For the decoding phase, given x as the embedding vector of the current token, the query q becomes a vector and the KV cache matrices will be updated as follow: q = xWQ, K = Concat(K, xWK), V = Concat(V, xWV). (3) The attention output are then computed as follows: a = Softmax \u0012qKT √dk \u0013 , o = aV. (4) To ensure clarity and consistency, we introduce notation to define the hyper-parameters used in the paper. Specifically, we denote the batch size as b, the number of attention heads as h, the sequence length as l, and the head dimension as d. 3.2 Model Quantization Uniform quantization is adopted in our study and all experiments. Given a floating-point vector x, it can be uniformly quantized to k-bit as follows: ˆx = QU (x, k) = clip(⌊x s ⌉ + z, 0, 2k − 1) · s. (5) Here, ⌊·⌉ denotes the round operation, s = max(x)−min(x) 2k−1 and z = −⌊min(x) s ⌉ are quantization parameters. It should be noted that the quantization parameters are stored in full-precision, which can lead to significant overhead if the quantization is fine-grained. 4 Method 4.1 A Strong Baseline for KV Cache Quantization Tokenwise quantization, as depicted in Figure 2(b) is prevalent in quantizing large language models (LLMs) due to the distinct representations of individual tokens. However, it has been widely observed, as illustrated in Figure 2(a), that outliers emerge within the channel dimensions of key and value matrices [43, 32], posing challenges for tokenwise quantization. To address this, recent work [32] resorts to groupwise quantization, where outlier channels are processed in distinct groups, as illustrated in Figure 2(c). However, this fine-grained quantization approach introduces excessive memory overhead, thereby significantly impacting the compression ratio. For instance, considering X ∈ Rb×h×l×d as the data to be quantized and a group size of n, tokenwise quantization only results in 2bl quantization parameters, while groupwise quantization would yield 2bhld n quantization parameters. Since these parameters are usually stored in full precision, this overhead would constitute a substantial portion of the storage cost for quantized data. Motivated by depthwise separable convolution [ 19], we introduce an efficient channel-separable tokenwise quantization scheme, which disentangles the channel and token dimensions. As shown in 4𝒔,𝒛∈𝑅𝑙 (b) Tokenwise Quantization (c) Groupwise Quantization (d) Channel-separable  Tokenwise Quantization 𝒄 ∈ 𝑅ℎ𝑑 (a) Visualization of  Key and Value States 𝐗 ∈ 𝑅𝑙∗ℎ𝑑 𝐗 ∈ 𝑅𝑙∗ℎ𝑑 𝐗 ∈ 𝑅𝑙∗ℎ𝑑 𝒔,𝒛∈𝑅𝑙∗ℎ𝑑/𝑛 𝒔,𝒛∈𝑅𝑙 Figure 2: Visualization and different quantization granularities for key and value states. Here, we omit the batch dimension for simplicity. For keys, channel outliers emerge, yet token representations exhibit minimal differences. For values, both channel outliers and distinct token representations exist. Figure 2(d), our approach initiates by normalizing each channel of data X with a scaling factor c. For the i-th channel in X, the normalization process can be formulated as: Xi = Xi ci , where ci = p max(|Xi|). (6) After normalization, each channel is scaled to a closed magnitude, mitigating the influence of outliers during tokenwise quantization. Subsequently, tokenwise quantization can be reliably applied and the scales c are multiplied back to restore the magnitude of each channel. The process of channel-separable tokenwise quantization is summarized in the supplementary material. Within this quantization scheme, the total number of quantization parameters amounts to hd + 2bl, representing a notable reduction compared to groupwise quantization, while effectively balancing the outlier channels and the representation of each token. Table 1: Performance comparisons of different quantization granularities for KV cache. The KV cache is quantized to 4-bit and the compression ratio is calculated with b = 8, hd = l = 4096and n = 32. Data is collected with LLaMA3-8B model on GSM8k dataset. Key Cache Quantization Granularity Value Cache Quantization Granularity Quantization Parameters Compression Ratio Acc.(%) / / 0 1× 55.88 Groupwise Groupwise 4bhld/n 3.2× 54.51 Tokenwise Tokenwise 4bl 3.99× 49.81 Channelwise Tokenwise 2hd+ 2bl 4.00× 52.77 Channelwise Channel-separable Tokenwise 3hd+ 2bl 4.00× 54.74 As referred to Figure 2(a), since the differences in token representations are small in key cache, we employ channelwise quantization for the key cache to further reduce overhead and employ channel- separable tokenwise quantization for the value cache. As depicted in Table 1, this configuration yields superior performance with reduced quantization overhead compared with groupwise quantization, thereby establishing a robust baseline for KV cache quantization. 4.2 Accurate Salient Token Identification Adaptive KV cache compression [46, 43, 16] aims to discern the saliency of each token, keeping the information of salient tokens while evicting or aggressively compressing the rest, to achieve a higher compression ratio. These salient tokens, also referred to as \"Heavy Hitters\" [46], are often identified based on accumulated attention scores. Given attention score matrix A ∈ Rl×l, the saliency of token i is estimated by: pi = lX k=1 Ak,i. (7) 5So   there  are   five   eggs 1.00 0.00 0.00 0.00 0.00 0.33 0.67 0.00 0.00 0.00 0.35 0.30 0.35 0.00 0.00 0.04 0.06 0.14 0.76 0.00 0.02 0.02 0.04 0.06 0.86 egs five   are   there   So : 40% salient tokens 1.74 1.05 0.53 0.82 0.86 0.35 0.26 0.18 0.41 0.86 Question: There are 15 trees in the  grove… Let's think step by step… Question: If there are 3 cars… Let's think step by step… Question: Leah had 32 chocolates… Let's think step by step… … Question: Olivia has $23… Let's think step by step… Question: Janet’s ducks lay 16 eggs per  day…How much in dollars does she  make every day at the farmers' market? (a) (b) (c) 𝒑𝒊 ෥𝒑𝒊 Figure 3: (a) A toy example to illustrate accumulated attention scores and normalized attention scores. Initial tokens have larger attention scores and more values to be accumulated. (b) A sample from GSM8k dataset with chain-of-thoughts (CoT) prompting. (c) The probability of each token being selected as a salient token, measured by both accumulated and normalized attention scores. Tokens correspond to the final question are identified as low saliency by accumulated attention scores. Tokens with large saliency values are then considered salient tokens. However, this approach has inherent limitations due to the lower triangular nature of the attention score matrix, as illustrated in Figure 3(a). There are two primary issues. Firstly, earlier tokens benefit from having more values accumulated since the elements above the diagonal are all zero. For instance, in a sequence of length l, the initial token accumulates l positive values, whereas the final token only accumulates one. Secondly, Softmax function converts real numbers into probabilities, so that the earlier rows of the attention matrix tending to have higher values, as fewer numbers are involved in the Softmax calculation. Consequently, the accumulated attention score of the final token will always be smaller than that of the first, which exceeds 1. To address this, previous works, such as H2O [46], always maintain recent caches in full precision. Nevertheless, this solution is suboptimal since recent tokens are not necessarily the most significant ones. To enhance the evaluation of each token’s saliency, we introduce an accurate token saliency metric based on normalized attention scores ˜pi: ˜pi = Pl k=1 Ak,i nnz(A:,i) (8) Here, nnz(A:,i) denotes the number of non-zero elements in the i-th column of A. As evidenced in Figure 3(a), normalizing the accumulated attention scores mitigates the influence of excessively large values in the initial rows of the attention score matrix, thereby delivering a more precise assessment. To validate the efficacy of our new metric, we input a sample from GSM8k dataset with chain-of- thoughts (CoT) prompting to the LLaMA3-8B model and identify saliency of each token by Eq. 7 and Eq. 8, respectively. As depicted in Figure 3(b) and (c), the salient tokens are at the end of the prompt, which correspond to the question for LLM to answer. However, these tokens are identified as low saliency by accumulated attention scores. Under the KV cache compression framework, these tokens would either be discarded or quantized to extremely low bit-width, resulting in a significant performance deterioration. In contrast, our method accurately identifies the salient tokens. Additional experimental results regarding the accuracy of our method will be detailed in Section 5.2. 4.3 Efficient Approximation of Saliency Metric As analyzed in Section 4.2, adaptive KV cache compression requires the explicit computation of full attention scores, as referred to Figure 4(b), which clashes with fast attention implementations like FlashAttention [8, 7, 9]. As shown in Figure 4(c), FlashAttention computes attention outputs in tiles without storing the intermediate attention scores. To reconcile the efficiency of FlashAttention with the substantial compression offered by adaptive KV caching, we devise an effective approximation for Eq. 8 as a measure of token saliency. Specifically, we sample a small group of tokens, designated 6𝐀 = (b) Standard Attention 𝐀 (c) FlashAttention 𝑸 𝑽 𝑶 𝑽 𝐊𝑻 𝑸 𝐊𝑻 𝑶 Memory=𝑶(𝒍𝟐) More memory access & slower Memory=𝑶(𝒍) Less memory access & faster (a) Efficient Saliency Metric with Probe Tokens Input tokens 𝐀𝑝𝑟𝑜𝑏𝑒 Probe tokens (b) Standard Attention (c) FlashAttention Regular tokens Output : Intermediate attention scores Figure 4: (a): Efficient saliency metric only requires attention scores of probe tokens through standard attention, enabling fast computation for the majority of tokens through FlashAttention. (b): In standard attention, full attention scores are computed before deriving the attention output. (c): FlashAttention avoids large attention matrix memory transfers by partitioning input matrices into blocks for incremental computation. as probe tokens, and compute their attention scores Aprobe as follows: Aprobe = Softmax \u0012QprobeKT √dk \u0013 . (9) By substituting Aprobe into Eq. 8, we can approximate the saliency of all tokens. For the remaining non-probe tokens, their attention scores do not have to be computed explicitly, enabling the integration of fast attention implementations to expedite the generation process, as illustrated in Figure 4(a). Table 2: Performance comparisons of various probe strategies. Data is col- lected from LLaMA3-8B model on GSM8k dataset. We quantize 40% salient tokens to 4-bit and the remaining 60% tokens to 2-bit. The proportion of probe tokens is 10%. Probe Strategy Acc.(%) All tokens 52.54 Random tokens 47.46 Special tokens 46.78 Recent tokens 51.10 Random+recent tokens 52.08 However, the positions of the probe tokens will un- doubtedly affects the accuracy of the approximated token saliency and the selection of probe tokens is under explored. In this study, we suggest four strategies for sampling probe tokens: • Random tokens. The probe tokens are randomly sam- pled from all positions. • Special tokens. The special tokens and punctuation tokens will be treated as probe tokens. • Recent tokens. The most recent tokens are selected as probe tokens. • Random+recent tokens. The probe tokens will be di- vided into two parts, one using recent tokens and the other randomly selecting from the remaining tokens. It should be emphasized that our study diverges from prior research [16] in that, rather than directly choosing special or recent tokens as salient tokens, we opt to sample a subset of tokens as \"probes\" to detect the salient ones. As depicted in Table 2, we present a comprehensive comparison of the performance among four distinct sampling strategies. Among the four strategies examined, a hybrid approach that combines recent tokens with randomly selected tokens emerges as the most effective. Unless otherwise specified, this hybrid strategy with 5% recent tokens and 5% random tokens will be employed in our method. 5 Experiment 5.1 Implementation Details Models and datasets. To validate the efficacy of our proposed method, we conduct experiments with three open-source LLMs: Mistral [ 20], LLaMA2 [37] and LLaMA3. These models are evaluated on three challenging benchmarks: GSM8k [6] for math problem solving, HumanEval [4] for code 7generation, and Line Retrieval [25] for data retrieval. To ensure reproducibility, the reported results are obtained using the Language Model Evaluation Harness [15] and LongEval [24]. Quantization and generation settings. We employ mixed precision quantization for KV cache where salient tokens will be quantized to 4-bit while the remaining will be quantized to 2-bit. For both subsets, we apply channelwise quantization for the key cache and channel-separable tokenwise quantization for the value cache. The proportion of salient tokens will be denoted by \"Saliency Ratio\" in the experimental results. During the decoding process, ZipCache adopts a streaming strategy [21] and repeats the compression process for the KV cache whenever 100 new tokens are generated. 5.2 Comparison with SOTA methods 5.2.1 Evaluation on GSM8k We begin our evaluation on GSM8k dataset with chain-of-thoughts (CoT) prompting, and the results are presented in Table 3. This task requires LLM to solve mathematical problems and return the final answer without multiple options. This task poses considerable challenges and previous KV cache compression methods manifest notable declines in accuracy. For instance, KIVI [32] shows an accuracy drop of 7.89% on LLaMA3-8B model, indicating the suboptimality of preserving recent tokens in full precision instead of identifying salient ones. Moreover, there is a substantial decrease in accuracy, amounting to 20.4%, for MiKV [43] under the high compression ratio. This suggests that accumulated attention scores mistakenly identify salient tokens, resulting in the loss of vital information during compression. By contrast, the proposed normalized attention scores can accurately measure token saliency, leading to a substantial enhancement in accuracy by 18.27% for LLaMA3-8B models in comparison to MiKV . In comparison to GEAR [21], which quantizes the entire KV cache to 4-bit, our approach additionally quantizes 40% tokens to 2-bit with enhanced performance on Mistral-7B model. This underscores the superiority of accurate adaptive compression of KV cache. Table 3: Performance comparisons on GSM8k with CoT prompts. Here, \"H/L\" denotes the bit-width for salient tokens (high-precision) and regular tokens (low-precision), respectively. The compression ratio is calculated with an average input length of l = 840. Model Method Bit-width (H/L) Saliency Ratio Compression Ratio Acc.(%) Mistral-7B FP16 16/16 100% 1 × 41.62 H2O [46] 16/0 40.0% 2.50 × 1.67 GEAR [21] 4/4 100% 3.00 × 39.42 KIVI [32] 16/2 15.2% 3.46 × 39.04 MiKV [43] 4/2 60.0% 4.98 × 36.32 ZipCache 4/2 60.0% 4.98× 41.24 LLaMA2-7B FP16 16/16 100% 1 × 14.18 H2O [46] 16/0 40.0% 2.50 × 13.50 GEAR [21] 4/4 100% 3.00 × 12.96 KIVI [32] 16/2 15.2% 3.46 × 13.19 MiKV [43] 4/2 60.0% 4.98 × 9.02 ZipCache 4/2 60.0% 4.98× 13.50 LLaMA2-13B FP16 16/16 100% 1 × 28.05 H2O [46] 16/0 40.0% 2.50 × 26.00 GEAR [21] 4/4 100% 3.00 × 25.40 KIVI [32] 16/2 15.2% 3.46 × 27.29 MiKV [43] 4/2 60.0% 4.98 × 23.65 ZipCache 4/2 60.0% 4.98× 27.85 LLaMA3-8B FP16 16/16 100% 1 × 55.88 H2O [46] 16/0 40.0% 2.50 × 27.82 GEAR [21] 4/4 100% 3.00 × 49.43 KIVI [32] 16/2 15.2% 3.46 × 47.99 MiKV [43] 4/2 70.0% 4.69 × 35.48 ZipCache 4/2 70.0% 4.69× 53.75 5.2.2 Evaluation on Line Retrival We further evaluate the data retrieval performance of various KV cache compression methods on Line Retrieval [25] dataset, where LLMs are required to retrieve specific content from a record 8of lines using a corresponding line index. The accuracy results under various number of lines are depicted in Figure 5. Notably, all quantization-based compression methods exhibit superior performance compared to the eviction-based approach H2O [ 46]. For eviction-based methods, information is permanently discarded upon eviction, whereas quantization introduces only minor errors while preserving the integrity of the data. Additionally, in comparison to KIVI [32], which always maintains recent caches at full precision, our approach consistently achieves better retrieval accuracy. This can be attributed to the nature of retrieval tasks, where salient tokens may appear at any position within the context, rather than being confined to the most recent caches. Moreover, when compared to MiKV [43], which employs accumulated attention scores as a saliency metric, our method yields a remarkable 42% accuracy improvement when evaluated using 200 lines on the Mistral-7b model. This substantial enhancement once more highlights the effectiveness of normalized attention scores in identifying salient tokens. Additional experimental results on HumanEval [4] can be found in the supplementary material. 50 100 150 200 Number of Lines 0 20 40 60 80 100Accuracy (%) LLaMA2-13B Full Cache ZipCache KIVI-2 MiKV H2O 100 200 300 400 Number of Lines 0 20 40 60 80 100Accuracy (%) LLaMA3-8B Full Cache ZipCache KIVI-2 MiKV H2O 50 100 150 200 Number of Lines 0 20 40 60 80 100Accuracy (%) Mistral-7B Full Cache ZipCache KIVI-2 MiKV H2O Figure 5: Performance comparisons of various KV cache compression methods on Line Retrieval. 5.3 Generation Efficiency In this subsection, we compare the latency and memory consumption of ZipCache and MiKV [43] under various input lengths, as depicted in Figure 6. Data is collected by serving LLaMA3-8B model on a Nvidia A100 GPU. MiKV employs accumulated attention scores to estimate token saliency, necessitating the use of standard attention for both prefill and decoding phases. Conversely, through an efficient approximate saliency metric, ZipCache requires only the calculation of the attention matrix for 10% of the tokens, while the remaining 90% tokens can be computed using either FlashAttention [7] or FlashDecoding [9]. Consequently, ZipCache achieves faster inference speed and lower memory usage, boasting a 37.3% reduction in prefill-phase latency, a 56.9% reduction in decoding-phase latency, and a 19.8% reduction in GPU memory usage when the input length scales to 4096. (a) Prefill phase latency  (b) Decoding phase latency  (c) GPU memory Figure 6: Comparisons of prefill-phase, decoding-phase latency and memory consumption between MiKV and ZipCache. 96 Conclusion and Future Work In this paper, we have proposed ZipCache, an accurate and efficient mixed-precision quantization framework for compressing KV cache. To commence, we introduce a channel-separable quantization scheme for KV cache, effectively reducing the overhead of storing quantization parameters compared to traditional fine-grained quantization schemes without performance degradation. Additionally, we present a novel metric for accurately assessing token saliency based on normalized attention scores. This metric enables adaptive quantization of all tokens according to their saliency, leading to improved compression ratios without sacrificing model performance. Moreover, we introduce an efficient approximation method for the token saliency metric, seamlessly integrating with fast attention implementations such as FlashAttention and FlashDecoding. This enhancement signifi- cantly boosts generation speed and reduces GPU memory requirements. Our extensive experiments have demonstrated that ZipCache achieves state-of-the-art compression performance in terms of compression ratio, accuracy and generation speed. We believe that ZipCache will pave the way for more practical and scalable deployment of LLMs in various real-world applications. Limitations and Broader Impacts. While ZipCache presents promising advancements in KV cache mixed-quantization frameworks for LLMs, the saliency ratio is manually specified before evaluation and cannot be automatically adjusted based on task datasets. Moreover, similar to other generative models, ZipCache can potentially be used to generate malicious content. References [1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. [2] A. Chauhan, U. Tiwari, et al. Post training mixed precision quantization of neural networks using first- order information. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1343–1352, 2023. [3] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y . Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [4] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y . Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [5] W.-L. Chiang, Z. Li, Z. Lin, Y . Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y . Zhuang, J. E. Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, march 2023. URL https://lmsys. org/blog/2023-03-30-vicuna, 3(5), 2023. [6] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [7] T. Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023. [8] T. Dao, D. Y . Fu, S. Ermon, A. Rudra, and C. Ré. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022. [9] T. Dao, D. Haziza, F. Massa, and G. Sizov. Flash-decoding for long-context inference, 2023. [10] J. C. de Winter. Can chatgpt pass high school exams on english language comprehension? International Journal of Artificial Intelligence in Education, pages 1–16, 2023. [11] T. Dettmers, M. Lewis, Y . Belkada, and L. Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems, 35:30318–30332, 2022. [12] Z. Dong, Z. Yao, A. Gholami, M. W. Mahoney, and K. Keutzer. Hawq: Hessian aware quantization of neural networks with mixed-precision. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 293–302, 2019. [13] M. Du, F. He, N. Zou, D. Tao, and X. Hu. Shortcut learning of large language models in natural language understanding. Communications of the ACM, 67(1):110–120, 2023. [14] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. [15] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, A. Le Noac’h, H. Li, K. McDonell, N. Muennighoff, C. Ociepa, J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron, 10L. Sutawika, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou. A framework for few-shot language model evaluation, 12 2023. [16] S. Ge, Y . Zhang, L. Liu, M. Zhang, J. Han, and J. Gao. Model tells you what to discard: Adaptive kv cache compression for llms. In The Twelfth International Conference on Learning Representations, 2024. [17] Y . He, L. Liu, J. Liu, W. Wu, H. Zhou, and B. Zhuang. Ptqd: Accurate post-training quantization for diffusion models. Advances in Neural Information Processing Systems, 36, 2023. [18] L. Hou, R. Y . Pang, T. Zhou, Y . Wu, X. Song, X. Song, and D. Zhou. Token dropping for efficient bert pretraining. arXiv preprint arXiv:2203.13240, 2022. [19] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. [20] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [21] H. Kang, Q. Zhang, S. Kundu, G. Jeong, Z. Liu, T. Krishna, and T. Zhao. Gear: An efficient kv cache compression recipefor near-lossless generative inference of llm. arXiv preprint arXiv:2403.05527, 2024. [22] Y . J. Kim, R. Henry, R. Fahim, and H. H. Awadalla. Finequant: Unlocking efficiency with fine-grained weight-only quantization for llms. arXiv preprint arXiv:2308.09723, 2023. [23] C. Li, W. Wang, J. Hu, Y . Wei, N. Zheng, H. Hu, Z. Zhang, and H. Peng. Common 7b language models already possess strong math capabilities. arXiv preprint arXiv:2403.04706, 2024. [24] D. Li, R. Shao, A. Xie, Y . Sheng, L. Zheng, J. Gonzalez, I. Stoica, X. Ma, , and H. Zhang. How long can open-source llms truly promise on context length?, June 2023. [25] D. Li, R. Shao, A. Xie, Y . Sheng, L. Zheng, J. Gonzalez, I. Stoica, X. Ma, and H. Zhang. How long can context length of open-source llms truly promise? In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023. [26] Y . Li, R. Gong, X. Tan, Y . Yang, P. Hu, Q. Zhang, F. Yu, W. Wang, and S. Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. In International Conference on Learning Representations, 2020. [27] J. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and S. Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023. [28] J. Liu, R. Gong, X. Wei, Z. Dong, J. Cai, and B. Zhuang. Qllm: Accurate and efficient low-bitwidth quanti- zation for large language models. In The Twelfth International Conference on Learning Representations, 2024. [29] J. Liu, C. S. Xia, Y . Wang, and L. Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36, 2024. [30] Z. Liu, A. Desai, F. Liao, W. Wang, V . Xie, Z. Xu, A. Kyrillidis, and A. Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. Advances in Neural Information Processing Systems, 36, 2024. [31] Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y . Mehdad, Y . Shi, R. Krishnamoorthi, and V . Chandra. Llm- qat: Data-free quantization aware training for large language models. arXiv preprint arXiv:2305.17888, 2023. [32] Z. Liu, J. Yuan, H. Jin, S. Zhong, Z. Xu, V . Braverman, B. Chen, and X. Hu. Kivi: A tuning-free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750, 2024. [33] H. Luo, Q. Sun, C. Xu, P. Zhao, J. Lou, C. Tao, X. Geng, Q. Lin, S. Chen, and D. Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct.arXiv preprint arXiv:2308.09583, 2023. [34] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. Improving language understanding by generative pre-training. 2018. [35] M. Tan, L. Wang, L. Jiang, and J. Jiang. Investigating math word problems using pretrained multilingual language models. arXiv preprint arXiv:2105.08928, 2021. [36] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [37] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 11[38] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [39] K. Wang, Z. Liu, Y . Lin, J. Lin, and S. Han. Haq: Hardware-aware automated quantization with mixed precision. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8612–8620, 2019. [40] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pages 38087–38099. PMLR, 2023. [41] G. Xiao, Y . Tian, B. Chen, S. Han, and M. Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2023. [42] F. F. Xu, U. Alon, G. Neubig, and V . J. Hellendoorn. A systematic evaluation of large language models of code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming, pages 1–10, 2022. [43] J. Y . Yang, B. Kim, J. Bae, B. Kwon, G. Park, E. Yang, S. J. Kwon, and D. Lee. No token left behind: Reliable kv cache compression via importance-aware mixed precision quantization. arXiv preprint arXiv:2402.18096, 2024. [44] Z. Yao, Z. Dong, Z. Zheng, A. Gholami, J. Yu, E. Tan, L. Wang, Q. Huang, Y . Wang, M. Mahoney, et al. Hawq-v3: Dyadic neural network quantization. In International Conference on Machine Learning, pages 11875–11886. PMLR, 2021. [45] Z. Yao, R. Yazdani Aminabadi, M. Zhang, X. Wu, C. Li, and Y . He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers.Advances in Neural Information Processing Systems, 35:27168–27183, 2022. [46] Z. Zhang, Y . Sheng, T. Zhou, T. Chen, L. Zheng, R. Cai, Z. Song, Y . Tian, C. Ré, C. Barrett, et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36, 2023. 12Appendix A Calculation of Overhead for Different Quantization Schemes Assuming b = 8, hd = l = 4096, and that the KV cache is quantized to4-bit, we proceed to calculate the actual compression ratio for different quantization granularities. For groupwise quantization with a group size of n = 32, the compression ratio Rgroup is given by: Rgroup = 2 × bhld × 16 2 × bhld × 4 +4bhld n × 16 = 3.200 (A) For tokenwise quantization, the compression ratio Rtoken can be calculated as: Rtoken = 2 × bhld × 16 2 × bhld × 4 + 4× bl × 16 = 3.992 (B) For our proposed quantization baseline, the compression ratio Rbaseline is determined by: Rbaseline = 2 × bhld × 16 2 × bhld × 4 + 3× hd × 16 + 2× bl × 16 = 3.995 (C) B Implementation Details of ZipCache In this section, we provide an overview of the channel-separable tokenwise quantization scheme in Algorithm 1. Additionally, we present the process of ZipCache’s prefill phase as described in Algorithm 2, as well as its decoding phase detailed in Algorithm 3. It is worth mentioning that during both the prefill and decoding phases, rather than calculating attention outputs separately for probe tokens and regular tokens followed by merging, FlashAttention [7] is utilized to compute the attention output for all tokens simultaneously. Additionally, attention scores of probe tokens are calculated. By bypassing the substantial memory accesses associated with matrix splitting and merging, this strategy enhances generation speed. Algorithm 1: Channel-separable Tokenwise Quantization (CSTQuant) procedure CSTQuant: Input: data X ∈ Rl×hd, target bit-width k for i ← 0 to hd do ci = p max(|Xi|) Xi = Xi ci // Normalizing each channel of X ˆX =TokenQuant(X, k) // Do tokenwise quantization for i ← 0 to hd do ˆXi = ˆXi × ci // Rescale each channel of X return ˆX C Additional Experimental Results C.1 Accuracy and Efficiency Comparisons of various KV cache compression methods In this section, we present the accuracy and efficiency comparisons of various KV cache compression methods, as presented in Table A. Data is collected by evaluating LLaMA3-8B model on200-line retrieval task with a Nvidia A100 GPU. We use a batch size of 8 and an average input length of 3072. Among these methods, ZipCache achieves the highest accuracy, compression ratio and generation speed. Specifically, in comparison to MiKV [43], which identifies salient tokens through accumulated attention scores, our method achieves a notable 10.0% accuracy improvement by accurately pinpointing salient tokens and a substantial38.0% decrease in prefill latency by integrating FlashAttention [7]. 13Algorithm 2: ZipCache for Prefill Phase procedure ZipCachePrefill: Input: Query states Q, key states K, value states V, saliency ratio r%, bit-width for salient tokens kh, bit-width for regular tokens kl // Salient Token Identification Select probe tokens and compute their attention scores Aprobe by Eq. 9 Measure the token saliency ˜p with Aprobe by Eq. 8 // Computing Attention Output with FlashAttention O = FlashAttention(Q, K, V) // Compressing KV Cache Partition key states: Ksalient, Kregular = Split(K, ˜p, r%) Partition value states: Vsalient, Vregular = Split(V, ˜p, r%) Ksalient = ChannelQuant(Ksalient, kh), Vsalient = CSTQuant(Vsalient, kh) Kregular = ChannelQuant(Kregular, kl), Vregular = CSTQuant(Vregular, kl) ˆK = Concat(Ksalient, Kregular) ˆV = Concat(Vsalient, Vregular) // Return Attention Output and Compressed KV Cache return O, ( ˆK, ˆV) Algorithm 3: ZipCache for Decoding Phase procedure ZipCacheDecoding: Input: Query vector q, key vector k, value vector v, KV cache ( ˆK, ˆV), saliency ratio r%, bit-width for salient tokens kh, bit-width for regular tokens kl, decoding token index i, probe attention score Aprobe K = Concat(k, ˆK) // Concatenate key cache V = Concat(v, ˆV) // Concatenate value cache o = FlashAttention(q, K, V) // Compute attention output i = i + 1 if i == 100then // Re-compress every 100 tokens Extract K[: −100] and V[: −100] and adaptively compress them with Aprobe Reset i = 0, Aprobe = None else if i >95 or randint(0, 100) < 5 then // probe tokens consists of 5% recent and 5% random tokens. Compute attention scores a of current token by Eq. 4 Aprobe = Concat(a, Aprobe) // Return Attention Output, KV Cache and Attention Scores from Probe Tokens return o, (K, V), Aprobe C.2 Evaluation on HumanEval In this section, we assess the performance of code generation across various KV cache compression methods, as summarized in Table B. Remarkably, ZipCache attains a compression ratio of 4.94× without sacrificing performance when tested with the Mistral-7B model, outperforming predecessor methods. Moreover, when evaluating on LLaMA3-8B model, our approach outperforms KIVI-2 [32] by 7.32% with a significantly higher compression ratio (4.39× vs. 2.55×). It should be noted that the average input length for this task is only119, while KIVI retains the recent 32 tokens in full-precision, thereby considerably diminishing its overall compression ratio. This underscores the advantage of ZipCache over methods that consistently retain information of recent tokens. 14Table A: Accuracy and efficiency comparisons over LLaMA3-8B on the200-line retrieval task. Here, \"H/L\" denotes the bit-width for salient tokens (high-precision) and regular tokens (low-precision), respectively. 0-bit denotes the tokens are evicted. The compression ratio is calculated with an average input length of l = 3072. Method Bit-width (H/L) Saliency Ratio Compression Ratio Acc.(%) Prefill-phase Latency (ms) FP16 16/16 100% 1 × 100 2340.11 H2O [46] 16/0 40.0% 2.50 × 0 4335.01 GEAR [21] 4/4 100% 3.00 × 100 5957.76 KIVI [32] 16/2 8.33% 4.36 × 96 4010.14 MiKV [43] 4/2 80.0% 4.43 × 90 4170.61 ZipCache 4/2 80.0% 4.43× 100 2584.01 Table B: Performance comparisons on HumanEval for code generation. Here, \"H/L\" denotes the bit-width for salient tokens (high-precision) and regular tokens (low-precision), respectively.0-bit denotes the tokens are evicted. The compression ratio is calculated with an average input length of l = 120. Model Method Bit-width (H/L) Saliency Ratio Compression Ratio Acc.(%) Mistral-7B FP16 16/16 100% 1 × 29.27 H2O [46] 16/0 40.0% 2.50 × 14.63 GEAR [21] 4/4 100% 3.00 × 28.05 KIVI [32] 16/2 26.7% 2.55 × 28.05 MiKV [43] 4/2 60.0% 4.94 × 27.44 ZipCache 4/2 60.0% 4.94× 29.27 LLaMA2-7B FP16 16/16 100% 1 × 14.02 H2O [46] 16/0 40.0% 2.50 × 11.59 GEAR [21] 4/4 100% 3.00 × 13.02 KIVI [32] 16/2 26.7% 2.55 × 11.59 MiKV [43] 4/2 80.0% 4.39 × 10.37 ZipCache 4/2 80.0% 4.39× 12.80 LLaMA3-8B FP16 16/16 100% 1 × 33.54 H2O [46] 16/0 40.0% 2.50 × 15.85 GEAR [21] 4/4 100% 3.00 × 28.66 KIVI [32] 16/2 26.7% 2.55 × 25.61 MiKV [43] 4/2 80.0% 4.39 × 29.88 ZipCache 4/2 80.0% 4.39× 32.93 15",
      "meta_data": {
        "arxiv_id": "2405.14256v1",
        "authors": [
          "Yefei He",
          "Luoming Zhang",
          "Weijia Wu",
          "Jing Liu",
          "Hong Zhou",
          "Bohan Zhuang"
        ],
        "published_date": "2024-05-23T07:37:16Z",
        "pdf_url": "https://arxiv.org/pdf/2405.14256v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "ZipCache addresses the memory bottleneck of KV cache in LLMs by proposing an accurate and efficient quantization method. It introduces a channel-separable tokenwise quantization scheme to reduce quantization parameter overhead, develops a novel \"normalized attention scores\" metric for precise salient token identification, and devises an efficient approximation method for the saliency metric compatible with fast attention (e.g., FlashAttention). This approach significantly boosts generation speed and compression ratios with minimal accuracy loss, achieving, for instance, a 4.98x compression on Mistral-7B with only a 0.38% drop in accuracy on the GSM8k dataset.",
        "methodology": "ZipCache employs a channel-separable tokenwise quantization scheme that disentangles channel and token dimensions, normalizing each channel to mitigate outlier influence before applying tokenwise quantization. This reduces quantization parameter overhead. For accurate salient token identification, it utilizes \"normalized attention scores,\" calculated as the sum of attention scores divided by the number of non-zero elements in the attention column, effectively overcoming the bias of traditional accumulated attention scores towards earlier tokens. Based on these scores, tokens are adaptively quantized, with salient tokens typically assigned a higher bit-width (e.g., 4-bit) and regular tokens a lower one (e.g., 2-bit). To ensure compatibility with fast attention implementations like FlashAttention, an efficient approximation method is introduced for saliency estimation. This involves sampling a small group of \"probe tokens\" (using a hybrid strategy of 5% recent and 5% random tokens) and computing their attention scores explicitly to approximate overall token saliency, allowing the majority of tokens to benefit from FlashAttention for faster generation.",
        "experimental_setup": "Experiments were conducted using open-source LLMs: Mistral-7B, LLaMA2-7B, LLaMA2-13B, and LLaMA3-8B. The models were evaluated on three challenging benchmarks: GSM8k for math problem solving (with Chain-of-Thought prompting), HumanEval for code generation, and Line Retrieval for data retrieval. The Language Model Evaluation Harness and LongEval frameworks were used for reporting results. Mixed precision quantization was applied, with salient tokens quantized to 4-bit and regular tokens to 2-bit. Channelwise quantization was used for the key cache, and channel-separable tokenwise quantization for the value cache. A streaming strategy was employed during decoding, repeating the compression process for the KV cache every 100 new tokens. Efficiency metrics, including prefill-phase latency, decoding-phase latency, and GPU memory usage, were measured on an Nvidia A100 GPU. Comparisons were made against various state-of-the-art KV cache compression methods including H2O, GEAR, KIVI, and MiKV, in addition to FP16 baseline.",
        "limitations": "The saliency ratio, which determines the proportion of tokens identified as salient, must be manually specified before evaluation and cannot be automatically adjusted based on specific task datasets. Additionally, similar to other generative models, ZipCache carries the potential risk of being used to generate malicious content.",
        "future_research_directions": "A clear direction for future research is to develop methods for automatically adjusting the saliency ratio, which is currently a manually specified parameter, based on specific task datasets. Enhancing the adaptability and automation of the compression process to varying inputs and tasks would further improve the practical and scalable deployment of LLMs."
      }
    },
    {
      "title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization",
      "abstract": "Efficient deployment of Large Language Models (LLMs) requires batching\nmultiple requests together to improve throughput. As the batch size, context\nlength, or model size increases, the size of the key and value (KV) cache can\nquickly become the main contributor to GPU memory usage and the bottleneck of\ninference latency. Quantization has emerged as an effective technique for KV\ncache compression, but existing methods still fail at very low bit widths. We\nobserve that distinct channels of a key/value activation embedding are highly\ninter-dependent, and the joint entropy of multiple channels grows at a slower\nrate than the sum of their marginal entropies. Based on this insight, we\npropose Coupled Quantization (CQ), which couples multiple key/value channels\ntogether to exploit their inter-dependency and encode the activations in a more\ninformation-efficient manner. Extensive experiments reveal that CQ outperforms\nor is competitive with existing baselines in preserving model quality.\nFurthermore, we demonstrate that CQ can preserve model quality with KV cache\nquantized down to 1-bit.",
      "full_text": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization Tianyi Zhang1, Jonah Yi1, Zhaozhuo Xu2, and Anshumali Shrivastava1,3 1Department of Computer Science, Rice University 2Department of Computer Science, Stevens Institute of Technology 3ThirdAI Corp. {tz21, jwy4, anshumali}@rice.edu, zxu79@stevens.edu May 8, 2024 Abstract Efficient deployment of Large Language Models (LLMs) requires batching multiple requests together to improve throughput. As the batch size, context length, or model size increases, the size of the key and value (KV) cache can quickly become the main contributor to GPU memory usage and the bottleneck of inference latency. Quantization has emerged as an effective technique for KV cache compression, but existing methods still fail at very low bit widths. We observe that distinct channels of a key/value activation embedding are highly inter-dependent, and the joint entropy of multiple channels grows at a slower rate than the sum of their marginal entropies. Based on this insight, we propose Coupled Quantization (CQ), which couples multiple key/value channels together to exploit their inter-dependency and encode the activations in a more information-efficient manner. Extensive experiments reveal that CQ outperforms or is competitive with existing baselines in preserving model quality. Furthermore, we demonstrate that CQ can preserve model quality with KV cache quantized down to 1-bit. 1 Introduction Large Language Models (LLMs) have showcased remarkable generalization abilities across various tasks, including text generation, language translation, and reasoning, without needing specific fine- tuning [25]. These impressive capabilities have empowered LLMs to find applications in numerous domains, such as law [14], education [15], and patient care [35]. However, the high computational demands and the prohibitive deployment costs of LLMs have created significant barriers, hindering their widespread adoption [14, 3]. Particularly, as LLMs move towards larger model size [9] and longer context length [34], they require faster graphics processing units (GPUs), or other special- ized processors, with higher memory capacity for efficient inference. Hence it is crucial to develop approaches for reducing the computational costs and memory requirement of LLMs. To accelerate LLM inference, key and value (KV) caching [20] has been proven to be an effective technique without affecting model quality. In autoregressive decoder-only LLMs, KV caching works through trading off memory for computations: the key and value activations of all previous tokens in the current batch are saved in memory to avoid their recomputation for generating the next token. However, since KV cache scales linearly with the number of tokens and batch size, it can quickly overwhelm the memory capacity of existing GPUs under long context or large batch size 1 arXiv:2405.03917v1  [cs.LG]  7 May 2024settings. In addition, since past key and value activations are not shared between sequences (except for maybe a common prompt), reading the KV cache from GPU memory becomes the primary inference bottleneck as opposed to the computation of the attention scores and value activations of the next token [10]. Thus, it is worthwhile to explore techniques for compressing KV cache for two primary benefits: 1. speeding up LLM inference through reducing the amount of memory reads for KV cache, 2. lowering the GPU memory requirements of inference for a given batch size and context length. Existing approaches typically compress KV cache through token eviction [37, 20] or activation quantization [21, 10]. While they can preserve model quality at moderate compression rates (4× compression or 4 bits per floating-point number), model quality quickly deteriorates at high compression rates (16× compression or 1 bit per floating-point number). In this work, we propose Coupled Quantization (CQ), a novel KV cache quantization method that preserves model quality up to 16× compression or 1 bit per floating-point number. Our approach is motivated by the observation that different channels within the same key/value activation embedding are highly inter-dependent. Thus, it is more information efficient to encode multiple channels of a key/value activation embedding than quantizing each channel independently. Existing KV cache quantization methods employ per-channel or per-token quantization strategies, which fail to exploit the inter-dependence between channels and suffer catastrophic model quality degradation at high compression rates. Our proposed method exploits the mutual dependency be- tween channels by jointly quantizing multiple channels and achieves better preservation of model quality than existing approaches in most cases, especially under low bit width settings. We sum- marize our contributions as follows, 1. We empirically observe the phenomenon that different channels within the same key/value activation embedding in an LLM share a high amount of dependency or mutual information, which is a key insight not leveraged by existing KV cache compression approaches. 2. We propose Coupled Quantization (CQ), a novel KV cache quantization method that takes advantage of the reduced entropy of jointly encoding multiple channels. 3. Through extensive experiments, we demonstrate the effectiveness of CQ against the most competitive existing methods. Furthermore, we showcase the ability of CQ in preserving model quality at an extreme KV cache compression level of 1-bit. 2 Background In this section, we introduce the relevant background and context including the KV caching tech- nique, the von Neumann bottleneck of KV cache, and channel-wise quantization. 2.1 LLM Attention and KV Cache Decoder-only transformer-based LLMs employ masked self-attention [32], in which activations of the current token is only dependent on previous tokens and unaffected by future ones. This prop- erty enables training parallelism for the next-token prediction objective, and gives rise to the KV caching technique for efficient inference decoding. Consider the decoding step for thet-th token in a single head of attention in an LLM. The input embedding of thet-th token (a column vector),et, goes through three distinct transformations to become key, query, and value activation embeddings fK(et), fQ(et), fV (et), where the transformationsfK, fQ, fV are composed of linear projection and positional encoding such as RoPE [29]. The output embedding of attention for thet-th token is 2computed as attention(et) = \u0014 fV (e1) . . . fV (et) \u0015 softmax  \u0014 fK(e1) . . . fK(et) \u0015⊤ fQ(et) ! (1) Thus, computing the output embedding of the current token requires the key and value activation embeddings of all previous tokens,fK(ei) and fV (ei) where i ∈ {1, . . . , t− 1}. These embeddings are cached in memory from previous decoding steps asKV cacheto avoid redundant computations and reduce inference latency. The size of KV cache can be calculated asb × n × l × 2 × h × c floating-point numbers, whereb is the batch size,n is the number of tokens in each sequence,l is the number of layers in the model,2 is for key and value,h is the number of key/value attention heads, and c is the number of channels in a single head of key/value activation embedding. As batch size, context length, or model size increases, the size of KV quickly overwhelms limited GPU memory. 2.2 The von Neumann Bottleneck of KV Cache The attention computation in Equation 1 is primarily bottlenecked by GPU memory bandwidth, known as the von Neumann bottleneck, due to low compute-to-global-memory-access ratio. GPUs are able to perform computations significantly faster than reading from global memory, and previous works have shown that attention computation can be accelerated by avoiding unnecessary global memory reads/writes through kernel fusion [5]. During the decoding phase in LLM attention, com- puting the output for the current token requires fetching the cached key and value embeddings of all previous tokens from global memory, resulting in a low ratio of arithmetic operations to global memory reads [10]. Furthermore, each sequence in a batch retains its own KV cache, leading to poor utilization of the parallel processing capabilities of GPUs. Therefore, KV cache compression algorithms can mitigate the von Neumann bottleneck and improve LLM inference efficiency, even if the approach introduces additional computational overheads in decompression or dequantization. As GPU compute cores are mostly stalled by KV cache memory accesses in attention, quantization approaches can effectively reduce memory reads while introducing negligible latency from dequan- tization computations. 2.3 Channel-wise Quantization Existing KV cache quantization methods [10, 21] employ channel-wise quantization for keys and token-wise quantization for values, based on the observation that certain key channels exhibit outlier patterns in magnitude while value channels do not. Channel-wise and token-wise quantization are similar, except the direction along which the quantization centroids are learned. In non-uniform channel-wise quantization, a set of centroids is learned for each channel. SupposeA is a key or value activation matrix, andAi,∗ denotes the i-th channel ofA. Then, non-uniform b-bit channel-wise quantization aims to learn a set of centroidsC⋆ i ⊂ R for each channeli of A independently through the objective C⋆ i = arg min C⊂R |C|=2b \r\r\rAi,∗ − q(Ai,∗) \r\r\r 2 2 (2) where q quantizes each value inAi,∗ to the nearest centroid inC. 31 2 3 4 5 10 15Key Entropy (bits) Layer 1 1 2 3 4 5 10 Layer 2 1 2 3 4 5 10 Layer 3 1 2 3 4 5 10 Layer 4 1 2 3 4 Num. of Joint Channels 5 10Value Entropy (bits) 1 2 3 4 Num. of Joint Channels 5 10 1 2 3 4 Num. of Joint Channels 2.5 5.0 7.5 10.0 1 2 3 4 Num. of Joint Channels 2.5 5.0 7.5 10.0 Sum of Marginal Entropy Joint Entropy Figure 1: Growth rate of joint entropy versus sum of marginal entropies of the LLaMA-7b key/value activation embeddings on 262k tokens of WikiText-2. Entropy is estimated using Equation 4. The slower growth rate of joint entropy implies that jointly quantizing more channels requires fewer bits than quantizing each channel independently. 3 Methodology In this section, we motivate our proposal using information theory and introduce the Coupled Quantization (CQ) approach for KV cache compression. 3.1 Motivations Our proposed approach is inspired by concepts in information theory [28]. We consider each channel in a key/value activation embedding as a random variableX1, X2, . . .. The amount of information (oruncertainty)inchannel X canbemeasuredby entropy, definedasH(X) =− R X p(x) log2 p(x) dx, where p(·) is the probability density function andX is the support of X. H(X) measures the theoretical number of bits needed for losslessly encoding the channelX, so it can be used to gauge how “quantizable” a channel is: ifH(X1) < H(X2), then channelX1 may be quantized to fewer bits than channelX2 while achieving the same quantization error. Our insight is that different channels from the same key/value activation embedding may be interdependent, which would reduce the number of bits required for jointly encoding multiple channels together compared to encoding them independently. The total amount of information (or uncertainty) in two channelsX1, X2 is measured by joint entropy, defined as H(X1, X2) = − R X1 R X2 p(x1, x2) log2 p(x1, x2) dx2 dx1, wherep(·, ·) is the joint probability density function. The joint entropy of two channels is the difference between the sum of their marginal entropies and their mutual information, i.e.,H(X1, X2) =H(X1) +H(X2) − I(X1, X2), whereI(·, ·) is a non-negative quantity for measuring the mutual dependency of two random variables. Thus, we have H(X1, X2) ≤ H(X1) +H(X2) (3) which implies the number of bits needed for jointly encoding two channels is no more than the total number of bits needed for encoding them independently. Previous works have demonstrated that 41 16 32 LLaMA-7b Key Correlations Layer 1  Layer 2  Layer 13  Layer 14  Layer 28  Layer 29  Layer 31  Layer 32 1 16 32 Channels 1 16 32 LLaMA-7b Value Correlations 1 16 32 Channels 1 16 32 Channels 1 16 32 Channels 1 16 32 Channels 1 16 32 Channels 1 16 32 Channels 1 16 32 Channels 1.0 0.5 0.0 0.5 1.0 Figure 2: Correlation matrices of the first 32 channels of 8 layers of LLaMA-7b key and value activation embeddings on WikiText-2. Channel pairs exhibit high levels of linear dependency, shown by high magnitudes of the correlation coefficients. deep neural networks [11] and attention-based networks [7] tend to produce low-rank embeddings, which suggests that channels of key/value embedding in LLM may exhibit high amount of mutual dependency. It is hence beneficial to measure the difference between the joint entropy of multiple channels and the sum of their marginal entropies in key and value activation embeddings. A significant difference would suggest that encoding these channels together is more information-efficient than encoding them independently. However, it is intractable to derive the exact entropy or joint entropy of channels, since their probability density functions are not known. Therefore, we employ the “binning” trick [17] to estimate entropy. We first observe an empirical distribution of key and value channels by saving the KV cache on a dataset, and partition the support of each channel into equally sized bins. Then, values of each channel are discretized to the index of the bin they fall into. Finally, the joint entropy ofn channels X1, . . . , Xn is estimated with the Riemann sum, H(X1, . . . , Xn) ≈ X x1∈B1 ··· X xn∈Bn bp(x1, . . . , xn) log2 bp(x1, . . . , xn) (4) where Bi is the support of the binned or discretizedXi and bp(·) is the empirical probability density function. Specifically, we divide the channels of key and value embeddings of LLaMA-7b into non- overlapping groups ofc contiguous channels, wherec ∈ {1, 2, 3, 4}, and estimate the joint entropy and the sum of marginal entropies of each group. The support of each channel is partitioned into 16 equally sized bins. Figure 1 shows the mean and standard deviation of the estimated joint entropy and sum of marginal entropies of three layers of LLaMA-7b on 262k tokens of the WikiText-2 dataset, averaged over groups. We only show a maximum group size of 4, since increasing the group size requires saving exponentially more key and value embeddings to avoid empty bins and maintain estimation quality. As shown in Figure 1, the sum of marginal entropies grows at a linear rate while the joint entropy increases slower at a sub-linear rate. This implies that as the number of jointly quantized channels increases, the total amount of information needed for encoding decreases. This phenomenon is the foundation that motivates our proposed approach. In addition to presenting the estimated marginal and joint entropy, we also show the Pearson correlation coefficients between channels of LLaMA-7b key and value activation embeddings on WikiText-2. Correlation coefficient captures the linear dependency between two random variables. Heat maps for the correlation matrices of 8 layers are shown in Figure 2, while the correlation matrices of all layers are presented in Section 6 of the Appendix. The key and value channels 5Channel-wise Quantization Coupled Quantization q0 q1 channel 0 -0.57 0  0.21 1  -0.38 0  -0.13 1  key or value activation embedding channel 1 channel-wise centroids q -0.62 0  0.11 1  -0.38-0.10 0.30 2  -0.38 3  -0.13-0.24 coupled channels 0 & 1 key or value activation embedding multi-channel centroids Figure 3: A comparison of 1-bit channel-wise quantization and our proposed Coupled Quantization (using 2 bits per 2 channels as an example). The quantization results on the first two channels of the first-layer key activation embeddings of LLaMA-7b on the WikiText-2 dataset are shown. Channel-wise quantization is ineffective at capturing the original values at low widths, while CQ leverages the dependency between channels to achieve low quantization errors. exhibit high levels of linear dependency and they are clearly not independently distributed, as shown by high magnitudes of the correlation coefficients. 3.2 Coupled Quantization Motivated by the finding that distinct key/value channels exhibit high dependency, we propose Coupled Quantization (CQ), an information-efficient quantization approach for compressing LLM KV cache. Unlike existing KV cache quantization methods which quantizes channel-wise or token- wise, CQ performs channel-coupled quantization for keys and values. More concretely, channels of a key or value activation embedding are divided into equally sized, non-overlapping groups of contiguous channels. The channels in each group arecoupled, as they are jointly quantized and share a single quantization code. For each group of coupled channels, a distinct set of multi-channel centroids are learned, where each centroid has dimensionality equal to the number of channels in that group. When quantizing a key or value activation embedding, each group of coupled channels are quantized to the nearest centroid in terms of L2 distance. We use the CQ-<c>c<b>b notation to denote the configuration of channel coupling and quantization bit width, where<c> is the number of channels in each group and<b> indicates the number of bits in a quantized code for a group. For example, CQ-4c8b means that every 4 contiguous channels are coupled together and each coupled group shares an 8-bit code, which is equivalent to 2-bit channel-wise quantization in terms of storage overhead of quantized codes. A illustrative comparison of channel-wise quantization and CQ is shown in Figure 3. Although previous works [10, 21] opt to quantize keys channel-wise and values token-wise, we adopt channel-coupled quantization for both keys and values. Similar to existing approaches [10, 21], CQ quantizes keys before RoPE [29] is applied, which increases the quantization difficulty by introducing more outliers in key activations. 3.2.1 Centroid Learning In CQ, the multi-channel centroids for each channel group are learned offline on a calibration dataset by leveraging uniform clustering or second-order-information-informed clustering. Specifically, for uniform centroid learning of the CQ-c cb b configuration, a set of centroidsC⋆ i ⊂ Rc is learned 61c1b 2c2b 4c4b 8c8b CQ Conﬁg. 101 102 103 Perplexity on WikiText-2 2642.72 1614.02 883.60 32.12 620.08 28.39 10.47 8.095.68 LLaMA-7b 1-bit CQ Uniform CQ Fisher-guided CQ FP16 Baseline 1c2b 2c4b 4c8b CQ Conﬁg. 101 102 Perplexity on WikiText-2 204.56 24.46 6.866.37 6.08 5.97 LLaMA-7b 2-bit CQ 1c1b 2c2b 4c4b 8c8b CQ Conﬁg. 750 1000 1250 1500 1750 2000Quant. Error on WikiText-2 LLaMA-7b 1-bit CQ Uniform CQ Key Uniform CQ Value Fisher-guided CQ Key Fisher-guided CQ Value 1c2b 2c4b 4c8b CQ Conﬁg. 600 800 1000Quant. Error on WikiText-2 LLaMA-7b 2-bit CQ Figure 4: Perplexity and key/value quantization errors (averaged over all layers) of LLaMA-7b on WikiText-2. Channels coupling and Fisher-guided centroid learning are effective for improving perplexity. independently for each channel groupi through the objective C⋆ i = arg min C∈Rc |C|=2b \r\r\rAic:(ic+c−1), ∗ − cq \u0000 Aic:(ic+c−1), ∗ \u0001\r\r\r 2 F (5) where Aic:(ic+c−1), ∗ is the sub-matrix ofA containing all coupled channels of thei-th group, and cq quantizes each column vector to the nearest centroid inC in terms of L2 distance. We use the k-means algorithm [22] with k-means++ initialization [1] to optimize the objective. LLMs are more sensitive to the quantization precision of certain weights than others [16]. To better preserve model quality, centroids of CQ should be learned to be biased towards preserving the precision of more important activations. To this end, we leverage an approximation to the Hessian to perform second-order-information-informed centroid learning. More concretely, we use the diagonals of the Fisher information matrixF to identify the more influential key/value activations and guide the centroid learning process. This method was proposed by Li et al. [18] and used by Kim et al. [16] for channel-wise weight quantization, and we extend it to multi-channel CQ. For performing Fisher-guided centroid learning, we first save a key/value activation matrixA and its gradient g(A) = ∂ ∂A L(A) on a calibration dataset, whereL is the training loss function. We approximate the Hessian matrix using the diagonals of the Fisher information matrix, which is the element-wise square of the gradient matrixdiag(F) = g(A) ⊙ g(A). We use the sum of diagonal entries of the Fisher information matrix as a measure of importance for each group of channel-coupled activations, and obtain the centroid setC⋆ i for thei-th channel group using the objective C⋆ i = arg min C⊂Rc |C|=2b X j g \u0000 Aic:(ic+c−1), j \u0001⊤g \u0000 Aic:(ic+c−1), j \u0001\r\r\rAic:(ic+c−1), j− cq(Aic:(ic+c−1), j) \r\r\r 2 F (6) We leverage weighted k-means to optimize the objective. The overhead of the centroid learning process and centroid storage are discussed in Section 4.3. We validate the effectiveness of our proposed channel-coupling and Fisher-guided centroid learn- ing by compressing LLaMA-7b KV cache to 1-bit and 2-bit, and present the perplexity results and quantization errors (∥A − cq(A)∥2 F averaged over layers) on WikiText-2 under different CQ con- figurations in Figure 4. The experimental setup is given in Section 4. As the number of coupled channels increases, perplexity and quantization errors improve significantly, approaching the FP16 7Table 1: Perplexity on WikiText-2 under different KV cache quantization methods at varying bit- width. The results of INT, NF, and KVQuant (excluding 1b and 1b-1%) are from [10]. “NaN” means Not a Number, which is caused by numerical stability issues of quantization. Our method CQ consistently outperforms non-dense-and-sparse quantization methods, and performs better or on par with the dense-and-sparse method KVQuant-<b>b-1%. Bits Per FPN LLaMA-7b LLaMA-13b LLaMA-2-7b LLaMA-2-13b Mistral-7b FP16 16 5.68 5.09 5.12 4.57 4.76 INT4 4.00-4.01 5.98 5.32 5.66 5.01 4.97 INT4-gs128 4.16 5.77 5.16 5.32 4.71 4.82 NF4 4.00-4.01 5.87 5.23 5.47 4.90 4.91 NF4-gs128 4.16 5.77 5.17 5.30 4.71 4.83 KVQuant-4b 4.00-4.02 5.73 5.15 5.18 4.63 4.81 KVQuant-4b-1% 4.32-4.35 5.70 5.11 5.14 4.59 4.78 CQ-2c8b 4.00 5.70 5.11 5.14 4.59 4.79 INT2 2.00-2.01 11779 69965 4708 3942 573 INT2-gs128 2.14 37.37 41.77 117.88 93.09 51.96 NF2 2.00-2.02 3210.5 5785.6 13601 4035.6 902.51 NF2-gs128 2.14 351.23 141.19 634.59 642.44 252.85 KVQuant-2b 2.00-2.02 8.17 7.29 9.75 29.25 7.33 KVQuant-2b-1% 2.32-2.35 6.06 5.40 5.50 4.92 5.16 CQ-4c8b 2.00 5.97 5.32 5.42 4.81 5.11 KVQuant-1b 1.00-1.02 321.58 1617.40 NaN 4709.83 203.73 KVQuant-1b-1% 1.32-1.35 9.93 7.97 9.50 13.76 10.07 CQ-8c8b 1.00 8.09 7.02 7.75 6.55 7.25 CQ-8c10b 1.25 6.78 6.00 6.25 5.47 5.90 baseline performance. Although Fisher-guided centroid learning increases the quantization error, it better preserves the salient activations and achieve lower perplexity. 4 Experiments In this section, we perform extensive experiments to validate the effectiveness of our proposed CQ approach for KV cache compression. We first introduce the experimental setups including hardware, software, metrics, datasets, and baselines used. Then, we present the detailed empirical results and provide discussions. Finally, we perform an ablation study to validate the effectiveness of each component of our proposal. Hardware Testbed Experiments are performed on a Linux server running Ubuntu 20.04, equipped with 2 AMD EPYC 7742 CPUs, 1.5TB RAM, and 4 NVIDIA A100 40GB GPUs. Software ImplementationOur software implementation of CQ is based on PyTorch [24] and the HuggingFace Transformers library [33]. Evaluation Metrics and BenchmarksWe evaluate the quality of 5 popular open-source LLMs on various benchmarks under different KV cache quantization algorithms. The 5 LLMs con- sidered are 1. LLaMA-7b, 2. LLaMA-13b [30], 3. LLaMA-2-7b, 4. LLaMA-2-13b [31], 5. Mistral-7b [13]. We evaluate LLM quality using the perplexity metric on 2 datasets: WikiText-2 [23] and C4 [26], and accuracy on 3 benchmarks in zero-shot setting: WinoGrande [27], PIQA [2], and ARC Challenge [4]. Perplexity is evaluated on the test set of the datasets at the maximum context length 8Table 2: Perplexity on C4 under different KV cache quantization methods at varying bit-width. The results of INT, NF, and KVQuant (excluding 1b and 1b-1%) are from [10]. Our method CQ consistently outperforms non-dense-and-sparse quantization methods, and performs better or on par with the dense-and-sparse method KVQuant-<b>b-1%. Bits Per FPN LLaMA-7b LLaMA-13b LLama-2-7b LLaMA-2-13b Mistral-7b FP16 16 7.08 6.61 6.63 6.05 5.71 INT4 4.00-4.01 7.40 6.82 7.31 6.59 5.91 INT4-gs128 4.16 7.16 6.67 6.87 6.20 5.76 NF4 4.00-4.01 7.27 6.74 7.09 6.45 5.85 NF4-gs128 4.16 7.16 6.66 6.86 6.20 5.77 KVQuant-4b 4.00-4.02 7.13 6.65 6.70 6.11 5.75 KVQuant-4b-1% 4.32-4.35 7.09 6.62 6.65 6.06 5.72 CQ-2c8b 4.00 7.11 6.64 6.67 6.09 5.74 INT2 2.00-2.01 10892 100870 4708 4220 477 INT2-gs128 2.14 43.49 56.25 113.49 97.04 50.73 NF2 2.00-2.02 2850.1 4680.3 13081.2 4175.6 1102.3 NF2-gs128 2.14 248.32 118.18 420.05 499.82 191.73 KVQuant-2b 2.00-2.02 10.28 9.05 15.16 43.77 8.40 KVQuant-2b-1% 2.32-2.35 7.38 6.83 7.06 6.38 6.08 CQ-4c8b 2.00 7.52 6.96 7.23 6.52 6.17 KVQuant-1b 1.00-1.02 168.90 1316.41 362.94 4223.37 127.07 KVQuant-1b-1% 1.32-1.35 11.18 9.56 16.04 22.87 10.53 CQ-8c8b 1.00 12.13 10.53 12.49 10.53 9.89 CQ-8c10b 1.25 9.12 8.23 9.03 8.01 7.46 of the LLM (2048 for LLaMA, 4096 for LLaMA-2, and 8192 for Mistral). Baselines We compare our proposed approach with uncompressed FP16 KV cache and com- petitive KV cache quantization methods, including 1. uniform integer (INT) quantization (without grouping and with a group size of 128), 2. NormalFloat (NF) quantization [6] (without grouping and with a group size of 128), 3. KVQuant [10] (without sparse outliers and with 1% outliers stored in sparse format). KVQuant-<b>b-1% is a dense-and-sparse method that requires an additional sparse matrix multiplication in addition to the dense matrix multiplication during inference, which introduces additional computational overhead. KVQuant and our proposed CQ both require learn- ing centroids on a calibration dataset, and we use the same calibration set of 16 sequences (each with 2048 tokens) of WikiText-2 for both methods. Calibration is performed once on the training set of WikiText-2, while perplexity and accuracy are evaluated on the test sets of different datasets and benchmarks. For each method, we report bits per floating-point number (FPN) to measure the compression rate, which is calculated as the number of bits in the quantized KV cache of each token divided by the number of FPN in the uncompressed KV cache of each token, excluding the constant storage overheads of centroids, scaling factors, and zero points. 4.1 Results The results of perplexity on WikiText-2 are presented in Table 1 and the results on C4 are presented in Table 2. CQ consistently outperforms non-dense-and-sparse quantization methods, especially in low bit-width regions. Despite using lower bit-width, CQ performs better or on par with the dense-and-sparse quantization method KVQuant-<b>b-1%. Dense-and-sparse quantization methods 9Table 3: Accuracy on 3 benchmarks under different KV cache quantization methods at varying bit-width. Bits Per FPN Task LLaMA-7b LLaMA-13b LLaMA-2-7b LLaMA-2-13b Mistral-7b FP16 16 WinoGrande 69.93 72.69 68.90 71.98 73.88 PIQA 78.67 79.16 78.07 79.16 80.58 ARC Challenge 41.72 46.42 43.43 48.29 50.34 KVQuant-4b 4.00-4.02 WinoGrande 69.53 72.61 67.96 71.59 73.88 PIQA 78.62 79.22 77.86 78.94 80.58 ARC Challenge 42.32 45.99 42.75 46.67 49.06 KVQuant-4b-1% 4.32-4.35 WinoGrande 70.72 73.40 68.67 72.30 73.72 PIQA 78.40 79.16 78.07 79.27 80.74 ARC Challenge 41.38 46.76 43.17 47.87 49.91 CQ-2c8b 4.00 WinoGrande 70.40 72.45 68.27 72.53 73.48 PIQA 78.61 79.11 77.91 78.62 80.52 ARC Challenge 41.55 45.99 43.34 47.78 49.15 KVQuant-2b 2.00-2.02 WinoGrande 53.59 59.35 51.70 51.30 63.46 PIQA 72.47 74.81 63.38 65.40 75.46 ARC Challenge 32.00 34.47 22.44 24.66 38.57 KVQuant-2b-1% 2.32-2.35 WinoGrande 68.03 71.43 67.64 70.17 70.80 PIQA 77.69 78.51 76.60 78.51 79.65 ARC Challenge 38.74 45.14 41.47 44.97 47.53 CQ-4c8b 2.00 WinoGrande 67.48 70.72 66.45 69.06 69.38 PIQA 76.11 78.29 76.12 77.42 79.49 ARC Challenge 38.48 44.03 39.93 44.11 45.65 KVQuant-1b 1.00-1.02 WinoGrande 50.51 48.46 50.91 49.41 49.80 PIQA 53.26 53.54 53.37 50.92 54.73 ARC Challenge 21.76 21.33 20.65 21.67 19.88 KVQuant-1b-1% 1.32-1.35 WinoGrande 56.67 61.01 57.77 57.30 58.17 PIQA 71.38 75.46 69.91 70.89 73.83 ARC Challenge 29.69 35.32 31.48 32.59 33.19 CQ-8c8b 1.00 WinoGrande 56.51 61.56 55.01 57.14 58.25 PIQA 71.16 73.99 71.22 73.01 75.24 ARC Challenge 30.20 33.79 30.20 34.30 33.79 CQ-8c10b 1.25 WinoGrande 60.46 65.27 59.19 62.98 63.93 PIQA 73.45 75.90 73.07 74.37 77.31 ARC Challenge 33.28 37.12 34.64 38.74 39.59 introduce additional inference overhead due to the extra sparse matrix multiplications for activation outliers which is inefficient on GPUs, while CQ does not have this limitation. The accuracy results on different benchmarks are shown in Table 3. CQ consistently outperforms the non-dense-and-sparse baseline KVQuant-<b>b at bit-width of 1 and 2, and performs better or on par with the dense-and-sparse baseline KVQuant-<b>b-1%. 4.2 Ablation Study We perform a set of ablation experiments to study the effectiveness of each component of our proposed approach. The results of the ablation experiments are shown in Table 4. We evaluate the perplexity of 2 models Mistral-7b and LLaMA-2-13b on WikiText-2 using CQ at 2 bits per FPN, withvaryingnumberofchannelscoupledandcomparinguniformcentroidlearningandFisher-guided centroid learning. Fisher-guided centroids significantly improve model quality as demonstrated by lower perplexity. With either uniform centroids or Fisher-guided centroids, perplexity improves as the number of coupled channels increases. Hence, our proposed techniques of channel coupling and 10Table 4: Ablation study: perplexity on WikiText-2 using CQ with varying number of coupled channels and fisher-guide centroids. Perplexity consistently improves as the number of coupled channels increases. Mistral-7b LLaMA-2-13b Bits Per FPN 2 2 2 2 2 2 2 2 2 2 2 2 Num. of Channels Coupled 1 2 4 1 2 4 1 2 4 1 2 4 Fisher-guided Centroids ✗ ✗ ✗ ✓ ✓ ✓ ✗ ✗ ✗ ✓ ✓ ✓ Perplexity ↓ 97.76 16.29 5.42 5.34 5.20 5.11 890.42 171.96 6.62 6.06 4.91 4.81 Fisher-guided centroid learning are both effective for maintaining model quality. 4.3 Overhead of Centroid Learning and Storage In this section, we discuss the computational overhead of centroid learning and the memory overhead of centroid storage for CQ. The centroid learning process of CQ consists of many independent k- means runs, which can be time-consuming on CPUs. Hence, we leverage a GPU implementation to accelerate the learning process. In all our experiments, we use k-means++ initialization and run 100 iterations of k-means on a single GPU to obtain the centroids. The memory overhead of storing the centroids can be calculated asl × 2 × h × c × 2b 16-bit floating-point numbers, wherel is the number of layers,2 is for keys and values,h is the number of key/value attention heads,c is the number of channels in a single-head key/value activation embedding, andb is the bit width of quantized codes. The detailed learning and memory overhead for different CQ configurations and models are given in Table 5. CQ can easily scale to large model sizes with low learning and memory overheads. Table 5: Learning and memory overhead of different CQ configurations and models. The number of centroid parameters are shown in millions, and the percentage to the model weights is shown in brackets. Centroid Learning Time Parameter Count in Centroids CQ Config. 2c8b 4c8b 8c8b 2c8b 4c8b 8c8b LLaMA-7b 53 mins 28 mins 14 mins 67.11M (0.996%) 67.11M (0.996%) 67.11M (0.996%) LLaMA-13b 94 mins 44 mins 22 mins 104.86M (0.806%) 104.86M (0.806%) 104.86M (0.806%) LLaMA-2-7b 54 mins 28 mins 14 mins 67.11M (0.996%) 67.11M (0.996%) 67.11M (0.996%) LLaMA-2-13b 83 mins 44 mins 23 mins 104.86M (0.806%) 104.86M (0.806%) 104.86M (0.806%) Mistral-7b 13 mins 7 mins 4 mins 16.78M (0.232%) 16.78M (0.232%) 16.78M (0.232%) 5 Related Works The high memory requirements and computational demands of LLMs pose a great challenge to effi- cient inference. Post-training model weight quantization has been shown to be effective for reducing inference latency and memory requirements. GPTQ [8] scales approximate Hessian-based weight quantization to large-scale models, and AWQ [19] preserves the salient weights in LLMs to achieve better quantized model quality. SqueezeLLM [16] leverages sensitivity-based non-uniform clustering and dense-and-sparse quantization for preserving salient weights and outliers. In addition to weight quantization, KV cache compression approaches are also effective for improving inference efficiency. Scissorhands [20] and H2O [37] achieve KV cache compression while preserving model quality by 11evicting unimportant tokens and only storing pivotal tokens. KIVI [21] quantizes key activations channel-wise and value activations token-wise, and uses a residual to achieve better model quality. KVQuant [10] proposes sensitivity-based quantization and dense-and-sparse quantization for KV cache. FlashAttention [5] improves the inference efficiency of attention-based models on GPUs by fusing kernels to eliminate unnecessary global memory reads/writes, while NoMAD-Attention [36] accelerates attention computations on CPUs by leveraging in-register shuffles. Product Quantiza- tion [12] is an approximate nearest neighbor search method that compresses vectors by decomposing the vector space into a Cartesian product of low-dimensional subspaces, and jointly quantizing the dimensions within each subspace. 6 Conclusion In this work, we propose Coupled Quantization (CQ) for enabling more efficient LLM inference by compressing KV cache, which is the latency and throughput bottleneck in long context or large batch size settings. We observe that distinct channels of key/value activation embeddings exhibit high levels of dependency, which has not been leveraged by existing compression methods. Motivated by this insight, we propose channel coupling for exploiting the inter-channel dependency to achieve more information-efficient encoding of key/value activations. Furthermore, we propose Fisher-guided centroid learning to better preserve salient activations and model quality. Extensive experiments demonstrate that our method mostly outperforms existing methods in terms of model quality under the same quantization bit width. Moreover, CQ can preserve model quality reasonably well with KV cache quantized down to 1-bit. References [1] David Arthur, Sergei Vassilvitskii, et al. k-means++: The advantages of careful seeding. In Soda, volume 7, pages 1027–1035, 2007. [2] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about phys- ical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 7432–7439, 2020. [3] Lingjiao Chen, Matei Zaharia, and James Zou. Frugalgpt: How to use large language models while reducing cost and improving performance.arXiv preprint arXiv:2305.05176, 2023. [4] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [5] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness.Advances in Neural Information Processing Systems, 35:16344–16359, 2022. [6] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient fine- tuning of quantized llms.Advances in Neural Information Processing Systems, 36, 2024. [7] Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: Pure attention loses rank doubly exponentially with depth. InInternational Conference on Machine Learning, pages 2793–2803. PMLR, 2021. 12[8] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post- training quantization for generative pre-trained transformers.arXiv preprint arXiv:2210.17323, 2022. [9] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models.arXiv preprint arXiv:2203.15556, 2022. [10] Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W Mahoney, Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cache quantization.arXiv preprint arXiv:2401.18079, 2024. [11] Minyoung Huh, Hossein Mobahi, Richard Zhang, Brian Cheung, Pulkit Agrawal, and Phillip Isola. The low-rank simplicity bias in deep networks.arXiv preprint arXiv:2103.10427, 2021. [12] Herve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search. IEEE transactions on pattern analysis and machine intelligence, 33(1):117–128, 2010. [13] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chap- lot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b.arXiv preprint arXiv:2310.06825, 2023. [14] Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. Challenges and applications of large language models. arXiv preprint arXiv:2307.10169, 2023. [15] Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke Hüllermeier, et al. Chatgpt for good? on opportunities and challenges of large language models for education.Learning and individual differences, 103:102274, 2023. [16] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023. [17] Alexander Kraskov, Harald Stögbauer, and Peter Grassberger. Estimating mutual information. Physical review E, 69(6):066138, 2004. [18] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. arXiv preprint arXiv:2102.05426, 2021. [19] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023. [20] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time.Advances in Neural Information Process- ing Systems, 36, 2024. 13[21] Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: A tuning-free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750, 2024. [22] Stuart Lloyd. Least squares quantization in pcm.IEEE transactions on information theory, 28(2):129–137, 1982. [23] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. [24] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An impera- tive style, high-performance deep learning library.Advances in neural information processing systems, 32, 2019. [25] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.OpenAI blog, 1(8):9, 2019. [26] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1–67, 2020. [27] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale.Communications of the ACM, 64(9):99–106, 2021. [28] Claude Elwood Shannon. A mathematical theory of communication.The Bell system technical journal, 27(3):379–423, 1948. [29] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding.Neurocomputing, 568:127063, 2024. [30] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models.arXiv preprint arXiv:2302.13971, 2023. [31] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models.arXiv preprint arXiv:2307.09288, 2023. [32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need.Advances in neural information processing systems, 30, 2017. [33] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Transformers: State-of- the-art natural language processing. InProceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, pages 38–45, 2020. [34] Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective long- context scaling of foundation models.arXiv preprint arXiv:2309.16039, 2023. 14[35] Xi Yang, Aokun Chen, Nima PourNejatian, Hoo Chang Shin, Kaleb E Smith, Christopher Parisien, Colin Compas, Cheryl Martin, Anthony B Costa, Mona G Flores, et al. A large language model for electronic health records.NPJ digital medicine, 5(1):194, 2022. [36] Tianyi Zhang, Jonah Wonkyu Yi, Bowen Yao, Zhaozhuo Xu, and Anshumali Shrivastava. Nomad-attention: Efficient llm inference on cpus through multiply-add-free attention.arXiv preprint arXiv:2403.01273, 2024. [37] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H2o: Heavy-hitter oracle for effi- cient generative inference of large language models.Advances in Neural Information Processing Systems, 36, 2024. 15Appendix A Correlation Matrices and Scatter Plots 0 15 31 Layer 1  Layer 2  Layer 3  Layer 4  Layer 5  Layer 6  Layer 7  Layer 8 0 15 31 Layer 9  Layer 10  Layer 11  Layer 12  Layer 13  Layer 14  Layer 15  Layer 16 0 15 31 Layer 17  Layer 18  Layer 19  Layer 20  Layer 21  Layer 22  Layer 23  Layer 24 0 15 31 0 15 31 Layer 25 0 15 31 Layer 26 0 15 31 Layer 27 0 15 31 Layer 28 0 15 31 Layer 29 0 15 31 Layer 30 0 15 31 Layer 31 0 15 31 Layer 32 1.0 0.5 0.0 0.5 1.0 Channel Channel Figure 5: Correlation matrix for the first 32 channels of pre-RoPEkey activation embeddings of all LLaMA-7b layers on WikiText-2. 160 15 31 Layer 1  Layer 2  Layer 3  Layer 4  Layer 5  Layer 6  Layer 7  Layer 8 0 15 31 Layer 9  Layer 10  Layer 11  Layer 12  Layer 13  Layer 14  Layer 15  Layer 16 0 15 31 Layer 17  Layer 18  Layer 19  Layer 20  Layer 21  Layer 22  Layer 23  Layer 24 0 15 31 0 15 31 Layer 25 0 15 31 Layer 26 0 15 31 Layer 27 0 15 31 Layer 28 0 15 31 Layer 29 0 15 31 Layer 30 0 15 31 Layer 31 0 15 31 Layer 32 1.0 0.5 0.0 0.5 1.0 Channel Channel Figure 6: Correlation matrix for the first 32 channels ofvalue activation embeddings of all LLaMA- 7b layers on WikiText-2. Figure 7: 2-dimensional scatter plots of pairs of channels inkey activation embeddings of 4 LLaMA- 7b layers on WikiText-2. 17Figure 8: 2-dimensional scatter plots of pairs of channels in value activation embeddings of 4 LLaMA-7b layers on WikiText-2. 18",
      "meta_data": {
        "arxiv_id": "2405.03917v1",
        "authors": [
          "Tianyi Zhang",
          "Jonah Yi",
          "Zhaozhuo Xu",
          "Anshumali Shrivastava"
        ],
        "published_date": "2024-05-07T00:25:20Z",
        "pdf_url": "https://arxiv.org/pdf/2405.03917v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper identifies the KV cache as a major bottleneck for Large Language Model (LLM) inference, especially at high compression rates where existing quantization methods fail. It observes that distinct channels within key/value activation embeddings are highly inter-dependent, meaning their joint entropy grows slower than the sum of their marginal entropies. Based on this, the authors propose Coupled Quantization (CQ), a novel KV cache quantization method that jointly encodes multiple channels to exploit this inter-dependency for more information-efficient compression. Extensive experiments demonstrate that CQ either outperforms or is competitive with existing baselines in preserving model quality, notably achieving quality preservation down to 1-bit KV cache quantization.",
        "methodology": "Coupled Quantization (CQ) divides channels of key or value activation embeddings into equally sized, non-overlapping groups of contiguous channels. These coupled channels are jointly quantized, sharing a single multi-channel centroid. Centroids are learned offline on a calibration dataset using either uniform clustering (k-means with k-means++ initialization) or Fisher-guided centroid learning. The Fisher-guided approach approximates the Hessian matrix using the element-wise square of the gradient matrix to identify and prioritize more influential key/value activations during the weighted k-means clustering process. Quantization occurs by mapping each group of coupled channels to the nearest centroid in terms of L2 distance. Keys are quantized before Rotary Positional Encoding (RoPE) is applied.",
        "experimental_setup": "Experiments were conducted on a Linux server with 2 AMD EPYC 7742 CPUs, 1.5TB RAM, and 4 NVIDIA A100 40GB GPUs, using PyTorch and the HuggingFace Transformers library. Five popular LLMs were evaluated: LLaMA-7b, LLaMA-13b, LLaMA-2-7b, LLaMA-2-13b, and Mistral-7b. Model quality was assessed using perplexity on WikiText-2 and C4 datasets, and accuracy on zero-shot benchmarks: WinoGrande, PIQA, and ARC Challenge. Perplexity was evaluated at the maximum context length of each LLM (2048 for LLaMA, 4096 for LLaMA-2, 8192 for Mistral). Baselines included uncompressed FP16, uniform integer (INT) quantization (with and without group size 128), NormalFloat (NF) quantization (with and without group size 128), and KVQuant (with and without 1% sparse outliers). A calibration dataset consisting of 16 sequences (2048 tokens each) from the WikiText-2 training set was used for learning centroids for CQ and KVQuant. The compression rate was measured by bits per floating-point number (FPN). Centroid learning involved 100 iterations of k-means with k-means++ initialization on a single GPU.",
        "limitations": "The paper does not explicitly list limitations of the Coupled Quantization (CQ) method itself. It highlights that CQ overcomes the weaknesses of existing methods at very low bit widths. The computational overhead of the centroid learning process is mentioned as potentially time-consuming on CPUs, but this is mitigated by leveraging GPU implementations. The method requires an offline calibration step and storage for centroids, which is an overhead inherent to such quantization approaches.",
        "future_research_directions": "Not mentioned"
      }
    }
  ]
}