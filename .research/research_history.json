{
  "research_topic": "Test-Time Adaptationを収束速度に関して改善したい",
  "queries": [
    "test-time adaptation acceleration",
    "fast test-time adaptation",
    "online adaptation convergence",
    "entropy minimization adaptation",
    "meta-learning test-time adaptation"
  ],
  "research_study_list": [
    {
      "title": "Evaluation of Test-Time Adaptation Under Computational Time Constraints",
      "abstract": "This paper proposes a novel online evaluation protocol for Test Time\nAdaptation (TTA) methods, which penalizes slower methods by providing them with\nfewer samples for adaptation. TTA methods leverage unlabeled data at test time\nto adapt to distribution shifts. Although many effective methods have been\nproposed, their impressive performance usually comes at the cost of\nsignificantly increased computation budgets. Current evaluation protocols\noverlook the effect of this extra computation cost, affecting their real-world\napplicability. To address this issue, we propose a more realistic evaluation\nprotocol for TTA methods, where data is received in an online fashion from a\nconstant-speed data stream, thereby accounting for the method's adaptation\nspeed. We apply our proposed protocol to benchmark several TTA methods on\nmultiple datasets and scenarios. Extensive experiments show that, when\naccounting for inference speed, simple and fast approaches can outperform more\nsophisticated but slower methods. For example, SHOT from 2020, outperforms the\nstate-of-the-art method SAR from 2023 in this setting. Our results reveal the\nimportance of developing practical TTA methods that are both accurate and\nefficient.",
      "full_text": "Evaluation of Test-Time Adaptation Under Computational Time Constraints Motasem Alfarra 1 2 Hani Itani 1 Alejandro Pardo 1 Shyma Alhuwaider 1 Merey Ramazanova 1 Juan C. P´erez 1 Zhipeng Cai 2 Matthias M¨uller 2 Bernard Ghanem 1 Abstract This paper proposes a novel online evaluation protocol for Test Time Adaptation (TTA) meth- ods, which penalizes slower methods by provid- ing them with fewer samples for adaptation. TTA methods leverage unlabeled data at test time to adapt to distribution shifts. Although many effec- tive methods have been proposed, their impressive performance usually comes at the cost of signif- icantly increased computation budgets. Current evaluation protocols overlook the effect of this extra computation cost, affecting their real-world applicability. To address this issue, we propose a more realistic evaluation protocol for TTA meth- ods, where data is received in an online fashion from a constant-speed data stream, thereby ac- counting for the method’s adaptation speed. We apply our proposed protocol to benchmark sev- eral TTA methods on multiple datasets and sce- narios. Extensive experiments show that, when accounting for inference speed, simple and fast approaches can outperform more sophisticated but slower methods. For example, SHOT from 2020, outperforms the state-of-the-art method SAR from 2023 in this setting. Our results re- veal the importance of developing practical TTA methods that are both accurate and efficient1. 1. Introduction In recent years, Deep Neural Networks (DNNs) have demon- strated remarkable success in various tasks (He et al., 2016) thanks to their ability to learn from large datasets (Deng et al., 2009). However, a significant limitation of DNNs is their poor performance when tested on out-of-distribution 1King Abdullah University of Science and Technol- ogy (KAUST), Thuwal, Saudi Arabia 2Intel Labs, Munich, Germany. Correspondence to: Motasem Alfarra <mo- tasem.alfarra@kaust.edu.sa>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). 1Code: github/MotasemAlfarra/Online-Test-Time-Adaptation Current Evaluation Realistic Evaluation40 45 50 55 60 65 70 75Error Rate (%)  AdaBN 17  AdaBN 17  SHOT 20  SHOT 20  TENT 21  TENT 21  SAR 23  SAR 23 Figure 1: The trend of average error rate using offline evaluation vs our proposed online evaluation. In the offline setup, TTA methods demonstrate progress across time with a decreasing average error rate, e.g. from 68.5% using AdaBN to 56.2% using SAR. We propose a realistic evaluation protocol that accounts for the adaptation speed of TTA methods. Under this protocol, fast methods ( e.g. AdaBN) are unaffected, while slower (but more recent and sophisticated) methods (e.g. SAR) are penalized. data, which violates the i.i.d. assumption that the training and testing data are from the same distribution (Hendrycks et al., 2021; Hendrycks & Dietterich, 2019; Kar et al., 2022). Such failure cases are concerning, since distribu- tion shifts are common in real-world applications, e.g., im- age corruptions (Hendrycks & Dietterich, 2019), chang- ing weather conditions (Sakaridis et al., 2021), or security breaches (Goodfellow et al., 2014). Test Time Adaptation (TTA) (Saenko et al., 2010; Sun et al., 2020; Liu et al., 2021) has demonstrated promising results for solving the above problem. TTA leverages the unlabeled data that arrives at test time by adapting the forward pass of pre-trained DNNs according to some proxy task (Liang et al., 2020; Lee et al., 2013). Though recent methods have made significant progress at improving accuracy under dis- tribution shifts (Wang et al., 2020; Niu et al., 2022; Gao et al., 2022), many of them incur high computational over- head. For instance, some methods require self-supervised fine-tuning on the data (Chen et al., 2022), while others perform a diffusion process per input (Gao et al., 2022). The computational overhead of TTA methods decreases 1 arXiv:2304.04795v2  [cs.LG]  23 May 2024Evaluation of Test-Time Adaptation Under Computational Time Constraints their inference speed, which is a critical property in many real-world applications that require the TTA method to pro- duce predictions at the speed of the stream itself. This property, however, is overlooked in the current evaluation protocols for TTA methods. In particular, these protocols assume a setting, which neglects how events constantly un- fold regardless of the model’s speed, causing the model to miss incoming samples when it is busy processing previous ones. For TTA methods that adapt using test data, missing samples has a direct effect on the method’s accuracy, as it will have fewer samples for adaptation. That is, the slower the TTA method, the fewer samples it can leverage for adapt- ing to the distribution shift. Thus, the current protocol for evaluating TTA methods is not suitable for assessing their efficacy in real-world deployment. In this work, we propose a novel realistic evaluation proto- col that factors in inference speed to assess the real-world applicability of TTA methods. Our evaluation protocol is in- spired by Online Learning (Cai et al., 2021; Shalev-Shwartz et al., 2012) and mimics real-world scenarios by exposing all TTA methods to a constant-speed stream of data. In this setting, the performance of slow TTA methods is in- trinsically penalized, as the time spent adapting to a sample may lead to dropped samples that could have been useful for adaptation. Specifically, our protocol dictates that if a method gslow is k times slower than the stream, then it may only use every kth sample for adaptation. In contrast, a method gfast that is as fast as the stream is allowed to adapt to every sample. Figure 1 shows the effect of evaluating several methods under our proposed protocol, where slower methods (e.g., SAR (Niu14 et al., 2023)) are penalized and faster but simpler methods become better alternatives (e.g., SHOT (Liang et al., 2020) and AdaBN (Li et al., 2016)). We apply our proposed evaluation protocol to benchmark several TTA methods on multiple datasets, and provide a fair assessment of their performance subject to the realistic consequences of slower inference speeds. Our experimental results highlight the importance of developing TTA methods that adapt to distribution shifts with minimal impact on inference speed. Our contributions are two-fold: 1. We propose a realistic evaluation protocol for TTA methods that penalizes slower methods by providing them with fewer samples for adaptation. Our approach is effective at assessing TTA methods’ efficacy in sce- narios where data arrives as a constant-speed stream. 2. Following our proposed protocol, we provide a com- prehensive experimental analysis of 15 TTA methods evaluated on 3 large-scale datasets under 3 different evaluation scenarios. These scenarios consider adap- tation to a single domain and continual adaptation to several domains. Our analysis shows that, when in- ference speed is accounted for, simple (but faster) ap- proaches can benefit from adapting to more data, and thus outperform more sophisticated (but slower) meth- ods. Figure 1 demonstrates this for four TTA methods. We hope our evaluation scheme inspires future TTA methods to consider inference speed as a critical di- mension that affects their real-world performance. 2. Related Work Test Time Adaptation. The Test Time Adaptation (TTA) setup relaxes the “i.i.d” assumption between the training and testing distributions (Sun et al., 2020; Boudiaf et al., 2022). This relaxation is usually attained through a lifelong learning scheme on all received unlabeled data (Chen et al., 2022; Gong et al.). Earlier approaches such as TTT (Sun et al., 2020) and TTT++ (Liu et al., 2021), among others (Torralba & Efros, 2011; Tzeng et al., 2017), include a self-supervised loss (Gidaris et al., 2018) during training, which can then provide an error signal during adaptation. Despite their effectiveness, such approaches assume having control over how the model is trained. Fully Test Time Adaptation. Fully TTA methods are a subtype of TTA method that adapts at test time by modify- ing the model’s parameters (Liang et al., 2020; Lee et al., 2013; Mirza et al., 2022b; Mancini et al., 2018; Kojima et al., 2022) or its input (Gao et al., 2022) by using the incoming unlabeled data. Fully TTA methods are practi- cal, as they avoid assumptions on the training phase of a given model (Wang et al., 2020; Gao et al., 2022; Iwasawa & Matsuo, 2021). The first of these approaches adjusts the statistics of the Batch Normalization (BN) layers (Mirza et al., 2022a; Schneider et al., 2020; Li et al., 2016). For example, BN-adaptation (Schneider et al., 2020) leverages the statistics of the source data as a prior and infers the statis- tics for every received sample. On the other hand, AdaBN (Li et al., 2016) discards the statistics of the source domain and uses the statistics computed on the target domain. In line with light TTA methods, LAME (Boudiaf et al., 2022) proposes to only adapt the model’s output by finding the latent assignments that optimize a manifold-regularized like- lihood of the data. In this work, we found that such efficient methods preserve their accuracy under our proposed eval- uation. While fully TTA methods have been studied in the context of adversarial domain shifts (Alfarra et al., 2022; Croce et al., 2022; P´erez et al., 2021), in this work we focus on the context of natural shifts such as realistic image cor- ruptions (Hendrycks & Dietterich, 2019; Kar et al., 2022). Another line of work aims at adapting to distribution shifts by minimizing entropy. For instance, SHOT (Liang et al., 2020) adapts the feature extractor to minimize the entropy of individual predictions; while maximizing the entropy of the predicted classes. TENT (Wang et al., 2020) updates the learnable parameters of the BN layers to minimize the 2Evaluation of Test-Time Adaptation Under Computational Time Constraints Adapted SampleNon-AdaptedSampleTTA method Current evaluation . . . . . . Realistic evaluation . . . . . . Model Figure 2: Inference under the current and realistic evaluation protocols. The current evaluation setting (left) assumes that the incoming batches of stream S can wait until the adaptation process of a TTA method g finishes. This assumption is untenable in a real-time deployment scenario. Our proposed realistic evaluation (right) simulates a more realistic scenario where S reveals data at a constant speed. In this setup, slower TTA methods will adapt to a smaller portion of the stream. The remaining part of the stream will be predicted without adaptation by employing the most recent adapted model. We refer to the most recent adapted model as fθt+1 , with t denoting the time when the last sample was adapted to by g. When g is still adapting to a sample, the incoming sample is fed to fθt+1 to produce predictions. entropy of predictions. EATA (Niu et al., 2022) combines TENT with an active selection of reliable and non-redundant samples from the target domain and an anti-forgetting loss (Kirkpatrick et al., 2017). Further, SAR (Niu14 et al., 2023) equips TENT with an active sampling scheme that filters samples with noisy gradients. Other works use data-augmentation at test time (Ashukha et al., 2020). For example, MEMO (Zhang et al., 2021) adapts model parameters to minimize the entropy over a sample and multiple augmentations of it. CoTTA (Wang et al., 2022) uses augmentations to generate reliable pseudo- labels and then peform distillation. Finally, DDA (Gao et al., 2022) proposes to leverage a diffusion model (Ho et al., 2020) to restore corrupted inputs back to the source data distribution. These methods require multiple forward passes through the network or a diffusion model, leading to slower inference speeds. 3. Methodology In this section, we present our proposed Realistic TTA evalu- ation protocol. We first describe the current TTA evaluation protocol and its limitations Then, we introduce our Realistic TTA evaluation protocol, which addresses the shortcomings of the offline protocol. 3.1. Current Protocol TTA considers the practical setup, in which trained models are deployed in a target domain that exhibits distribution shifts to which they must adapt. Let fθ : X → Ybe a clas- sifier, parameterized by θ, that predicts the label y ∈ Yfor a given input x ∈ X. Before test time, fθ is assumed to have been trained on the dataset Dtrain ⊂ X × Y. At test time, i.e. when executing TTA,fθ is presented with a stream of data S, sampled from X, with potentially multiple distribution shifts w.r.t. Dtrain. Under this setup, a TTA method is a function g(θ, x) that sequentially adapts the model’s param- eters θ and/or the input x to enhance the performance under distributions shifts. Currently, TTA methods are evaluated in an offline setting. Formally, the Current TTA evaluation protocol simulates the interaction between the stream S and the TTA method g, at each time step t ∈ {0, 1, . . . ,∞}, as follows: Curr.1 S reveals a sample xt. Curr.2 g adapts xt to ˆxt, θt to ˆθt, generates prediction ˆyt, and updates parameters θt+1 = αθt + (1 − α)ˆθt.2 Note that all existing TTA methods can be modeled using this framework. For example, TENT (Wang et al., 2020) adapts network parameters to minimize entropy with α = 0, while leaving inputs unchanged, i.e. ˆxt = xt and θt+1 = ˆθt. DDA (Gao et al., 2022) adapts inputs via a diffusion process while preserving network parameters with α = 1, i.e. ˆxt = ˆxt and θt+1 = θt. CoTTA (Wang et al., 2022) applies knowledge distillation, and updates network parameters with an exponential moving average, i.e. setting 0 < α <1. Shortcomings of the Current TTA protocol.In the current protocol, the performance of a TTA method g is measured by comparing the ground truth labels yt with the predic- tions after adaptation ˆyt. An evaluation based only on this measure implicitly assumes that the stream is not constant 2Note that some methods abstain from adapting either xt or θt. 3Evaluation of Test-Time Adaptation Under Computational Time Constraints speed, but rather waits for g to adapt to xt (Curr.2) before revealing the next batch xt+1 (Curr.1). Figure 2 provides an illustration of this situation. This assumption results in the offline protocol favoring slower TTA methods, as the method’s performance is agnostic to its inference speed. However, in practical applications where the test data ar- rives at a constant speed, the offline protocol is not suitable for assessing a method’s performance. Next, we propose a remedy for this shortcoming. 3.2. Realistic Online Evaluation Protocol We propose a realistic evaluation of TTA methods that explicitly considers the relation between the speed of the method and the speed at which the stream reveals new data. This setup is more realistic, as it intrinsically penalizes the performance of slower TTA methods: long times spent in adaptation result in fewer samples to adapt to. A crucial aspect of our realistic TTA protocol is accounting for the implications of simulating a constant speed data stream S. For instance, consider a stream S that reveals data at a constant rate r samples per second. If a method gfast adapts to samples at speed r, then gfast will be able to adapt to every sample. On the other hand, if gslow adapts to samples at a speed r/2, then gslow will skip every other sample. We formalize the notion of the relation between the speed of the stream and the speed of a method g as the “relative adaptation speed of g”. This quantity, denoted by C(g) ∈ N, is simply the integer ratio of the speed of S to the speed of g. For instance, in the previous example, C(gfast) = 1, meaning gfast adjusts as fast as S reveals data, while C(gslow) = 2 , indicating S reveals its second batch while gslow is still adapting to the first one. Without loss of generality, we assume that fθ runs in real- time, i.e. that its speed is equal to r, and thus C(fθ) = 1 . This assumption allows us to suppose that the samples that are not processed by g can be processed by fθ. Under this setup, we define our realistic protocol by introducing the relative adaptation speed C(g) into the offline protocol. In particular, we simulate g’s availability by conditionally performing the adaptation step (Curr.2), depending on C(g). In this manner,g is only permitted to adapt when its previous adaptation step has finished. Formally, the realistic TTA evaluation protocol simulates the interaction between the constant speed stream S and the TTA method g, at each time step t ∈ {0, 1, . . . ,∞}, as follows: RTTA 1 S reveals a sample xt. RTTA 2 If (t mod C(g)) = 0, then g adapts xt to ˆxt, θt to ˆθt, generates a prediction ˆyt, and updates pa- rameters via θt+1 ← αθt + (1 − α)ˆθt. Otherwise, fθt generates a prediction ˆyt. Table 1: Average C(g(xt)). We report the average relative adaptation speed C(g) for 5 TTA methods. The higher C(g) is, the smaller the portion of data to which g adapts is. Method AdaBN TENT TTAC-NQ MEMO DDA C(g) 1 3 12 54 810 Here, “mod” represents the modulo operation. The above protocol assesses the performance of TTA methods by fac- toring in their speed. As such, faster methods are granted more adaptation steps and, conversely, slower methods are granted fewer (see Figure 2). Note that explicitly modeling the relative adaptation speeds allows us to evaluate TTA methods under different adaptation speeds by setting C(g) to arbitrary values. For instance, note that our realistic proto- col recovers the original offline protocol by settingC(g) = 1 for all methods. Next, we explain the calculation of C(g) for our realistic protocol. Online computation of C(g). In practice, estimating the relative adaptation speed C(g) can be a noisy process. The noise in this estimation essentially comes from two factors: hardware and input dependence. Hardware-induced noise applies to all methods, while input dependence applies to methods like ETA (Niu et al., 2022) which, upon receiving an input, may optionally abstain from adapting to it. This noise means that C(g) potentially varies across iterations. Our protocol accounts for this variability by conducting an online computation of C(g) on each revealed input. That is, instead of using a fixed value of C(g) at each itera- tion t, our protocol rather uses C (g(xt)). Formally, if we let R (g(x)) denote the speed at which g processes x, then the relative adaptation speed of g at x is defined as C (g(xt)) = ⌈r/R(g(x))⌉, where the ceiling function ac- counts for the stream’s discrete-time nature. Note that since we assumed C(fθ) = 1, then R (fθ(x)) = r. We report the empirical behavior of this online computation of C (g(xt)) for various TTA methods in Table 1, and leave the rest of the methods and the computation details to the Appendix. Next, we leverage our Realistic TTA protocol to conduct a comprehensive empirical study of several TTA methods. 4. Experiments We follow prior art (Wang et al., 2020; Niu14 et al., 2023; Gao et al., 2022) and focus on the task of image classifica- tion. In all our experiments, we assume that fθ is a ResNet- 50-BN3 (He et al., 2016) trained on ImageNet (Deng et al., 2009) (pretrained weights obtained from torchvision). We further assume that the stream S reveals batches of size 3SAR demonstrated the superiority of using batch independent normalization layers under batch size of 1. We leave this ablation to the Appendix along with experiments on other architectures. 4Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 2: Episodic Error Rate on ImageNet-C. We report the error rate of different TTA methods on ImageNet-C benchmark under both the realistic and the current setup. A lower error rate indicates a better TTA method. The highlighted numbers indicate a better performance per method across setups. Episodic means the model will adapt to one corruption at a time. The model is reset back to the base model when moving to the next corruption. The current setup is merely the reproduction of every method. The first sub-table corresponds to methods that do not incur any or few extra computations, i.e. C(g) = 1. We show that methods generally perform worse in the realistic setup. The more computationally complex the TTA method is, the less data it will adapt to, and the worse is its performance. Noise Blur Weather DigitalMethod Realisticgauss. shot impul.defoc. glass motionzoom snow frost fog brigh. contr. elast. pixel. jpeg Avg. ∆ Source ✓ 97.8 97.1 98.1 82.1 90.2 85.2 77.5 83.1 76.7 75.6 41.1 94.6 83.0 79.4 68.4 82.0 - AdaBN ✓ 84.9 84.3 84.3 85.0 84.7 73.6 61.1 65.8 66.9 52.1 34.8 83.3 56.1 51.1 60.3 68.5 - LAME ✓ 98.3 97.6 98.6 82.4 90.9 86.1 78.1 84.5 77.5 77.3 41.4 94.8 84.8 80.0 68.9 82.7 - BN ✓ 84.6 83.9 83.8 80.1 80.2 71.7 60.4 65.4 65.2 51.6 34.6 76.3 54.4 49.7 59.2 66.7 - ✗ 73.4 70.2 73.0 76.6 75.5 59.8 53.8 54.2 63.4 44.7 35.5 79.3 46.9 43.2 49.7 59.9SHOT ✓ 73.6 69.0 71.1 74.6 74.8 60.0 52.9 54.1 61.3 44.1 34.1 77.8 46.8 43.1 49.2 59.1 (-0.8) ✗ 71.3 69.4 70.2 72.0 72.9 58.7 50.7 52.8 58.8 42.7 32.7 73.3 45.5 41.5 47.7 57.3TENT ✓ 75.7 78.3 75.2 76.3 77.3 64.6 55.6 57.3 61.4 45.9 33.5 77.1 50.1 44.2 51.4 61.6 (+4.3) ✗ 69.5 69.7 69.0 71.2 71.7 58.1 50.5 52.9 57.9 42.7 32.7 62.9 45.5 41.6 47.8 56.2SAR ✓ 79.4 78.5 78.1 79.9 79.3 67.5 56.1 60.5 63.1 47.4 34.0 75.3 51.7 46.6 53.8 63.4 (+7.2) ✗ 78.4 77.8 77.2 80.5 79.1 64.0 53.3 57.8 60.7 44.1 32.9 73.1 48.6 42.3 52.6 61.5CoTTA ✓ 82.9 81.6 81.9 87.4 85.6 75.6 61.1 63.1 64.9 49.9 34.8 91.2 54.0 48.8 56.6 68.0 (+6.5) ✗ 71.3 70.3 70.8 82.1 77.4 63.9 53.9 49.9 55.5 43.9 32.8 81.4 43.7 41.1 46.7 59.0TTAC-NQ ✓ 79.4 75.7 78.9 86.6 86.2 77.1 61.8 58.8 62.4 51.5 34.4 88.5 52.1 49.1 55.5 66.5 (+7.5) ✗ 65.5 62.4 63.5 66.6 67.2 52.0 47.3 48.2 54.1 39.9 32.1 55.0 42.3 39.2 44.8 52.0EATA ✓ 69.3 67.1 69.2 71.1 71.7 57.5 49.9 51.9 57.4 42.4 32.6 60.7 45.1 41.4 47.4 55.6 (+3.6) ✗ 92.5 91.3 91.0 84.0 87.0 79.3 72.4 74.6 71.3 67.9 39.0 89.0 76.2 67.0 62.4 76.3MEMO ✓ 97.7 97.0 98.0 82.1 90.1 85.1 77.4 83.0 76.6 75.4 41.0 94.5 82.9 79.2 68.2 81.9 (+5.6) ✗ 58.6 57.8 59.0 87.0 81.6 76.6 65.9 67.9 66.7 64.0 40.0 92.2 52.2 46.6 49.9 64.4DDA ✓ 97.8 97.0 98.1 82.1 90.2 85.2 77.5 83.1 76.7 75.6 41.1 94.6 83.0 79.4 68.3 82.0 (+17.6) 644, except for MEMO (Zhang et al., 2021), which pre- dicts on single images to incentivize prediction consistency over an input and its augmentations. Regarding datasets, we follow earlier works (Wang et al., 2020; Niu14 et al., 2023; Niu et al., 2022; Gao et al., 2022; Zhang et al., 2021), and thus evaluate on the ImageNet-C dataset (Hendrycks & Dietterich, 2019) with a corruption level of 5 for all 15 corruptions. We further extend our evaluation and consider CIFAR10-C, ImageNet-R (Hendrycks et al., 2021), and the more recent ImageNet-3DCC (Kar et al., 2022), which lever- ages depth estimates to construct more spatially-consistent corruptions. Our experiments compare the performance of the base- line model fθ (without test time adaptation) against 15 state-of-the-art TTA methods published in top-tier venues (e.g., CVPR, NeurIPS, and ICLR) between 2017 and 2023. In particular, we consider: BN (Schneider et al., 2020) and AdaBN (Li et al., 2016), which adjust the statistics of the batch normalization layers; SHOT (Liang et al., 2020) and SHOT-IM (Liang et al., 2020), which fine-tune the feature extractor to maximize mutual information; entropy mini- mization approaches such as TENT (Wang et al., 2020), 4This batch size is recommended by most baselines (Wang et al., 2020; Niu et al., 2022) ETA (Niu et al., 2022) (a more efficient version of TENT), and SAR (Niu14 et al., 2023), which trains the learnable parameters of the batch normalization layers; distillation approaches, such as CoTTA (Wang et al., 2022), Pseudo Labeling (PL) (Lee et al., 2013), and the very recent and efficient LAME (Boudiaf et al., 2022); EATA (Niu et al., 2022) and TTAC (Su et al., 2022) that assume access to the source training data; data-dependent approaches such as MEMO (Zhang et al., 2021) and the diffusion-based method DDA (Gao et al., 2022). For all methods, we use their official implementation with their recommended hyper- parameters. We report our experimental results on a subset of 12 baselines, while leaving ETA, SHOT-IM, and PL to the appendix due to space constraints and their similarity to SHOT and EATA. As mentioned in Section 3.2 , our protocol performs an online computation of the relative adaptation speed of g. In particular, for each batch revealed by the stream, we compute C (g(x)). Then, if C(g(xi)) = k, all the samples {xi+1, xi+2, . . . , xi+k} are processed by fθi without adap- tation. Otherwise, if C(g(xi)) = 1, then these samples are processed by g. For methods that accumulate parameter updates such as TENT (Wang et al., 2020), fθi is the most recent updated model g(fθi−1 ). We report all our main re- sults as the average across three seeds, and leave the detailed 5Evaluation of Test-Time Adaptation Under Computational Time Constraints SHOT TENT TTAC-NQ SAR EATA COTTA brigh.pixel.gauss.motionzoomglassimpul.jpegdefoc.elast.shotfrostsnowfog contr.clean 30 40 50 60 70 80 90 100Error Rate (%) (a) Current Continual TTA. brigh.pixel.gauss.motionzoomglassimpul.jpegdefoc.elast.shotfrostsnowfog contr.clean 30 40 50 60 70 80 90 100Error Rate (%)  (b) Realistic Continual TTA. Figure 3: Continual Error Rate on ImageNet-C. We report the continual error rate of several TTA methods on ImageNet-C benchmark under both realistic and current setups. A lower error rate indicates a better TTA method. Continual evaluation means the corruptions are presented in a sequence without resetting the model in between. We choose the same order as presented along the x-axis; starting with brightness and ending with clean validation set. In the current setup, we observe an increasing trend for SHOT, TENT, and TTAC-NQ. This is hypothesized to be due to overfitting on the early distribution shifts. This behavior is mitigated in the realistic setup due to adapting to fewer batches. EATA and SAR perform equally well in both realistic and current continual setups due to sample rejection. We report the standard deviation across 3 seeds. analysis to the Appendix. Throughout the experiments, we refer to our realistic evaluation protocol as “realistic/on- line”, and refer to the current protocol as “current/offline”. Next, we evaluate all methods on four different scenarios: (i) when domain shifts happen in an episodic manner, (ii) when domain shifts happen continually, i.e. one after the other, (iii) when the stream speed varies, (iii) when domain shifts happen continually with label correlation; practical evaluation (Yuan et al., 2023) ,and (v) when the baseline fθ is unavailable for evaluating the samples skipped by the TTA method g (left for the appendix). 4.1. Episodic Evaluation of TTA First, we consider an episodic evaluation of domain shifts, whereby S contains a single domain (e.g. one corruption) from ImageNet-C. We analyze this simple and most com- mon setup to assess the performance of TTA methods under real-time evaluation. We report the error rates on all corrup- tions in Table 2 and the average error rate across corruptions. We summarize the insights as follows: (i) The performance of TTA methods often degrades significantly under the realistic setup. Most methods induce a significant computational overhead, which prevents them from adapting to every sample from the test stream. For example, the error rate increases by 7.5% for TTAC- NQ and 4.3% for TENT, where C(gTTAC-NQ) = 12 and C(gTENT) = 3 (see Table 1). That is, TENT adapts to one- third of the batches revealed by the stream, while TTAC-NQ adapts to one every twelve batches. (ii) Very efficient methods, withC(g) = 1, such as LAME and BN, do not lose in performance. Evaluating such methods in offline or realistic setups is inconsequential, as their adaptation incurs negligible additional computation (since they adapt during the forward pass (Li et al., 2016; Schneider et al., 2020) or by adjusting the logits (Boudiaf et al., 2022) at a speed that pales in comparison to that of the stream). Interestingly, in our realistic evaluation, the simple BN (published in 2020) with an average error rate of 66.7% outperforms more recent and advanced methods such as SAR (published in 2023) by 1.7%. Furthermore, AdaBN (published in 2017) significantly outperforms the very recent diffusion-based DDA by a notable 13%. (iii) Data-dependent approaches, such as MEMO and DDA, are extremely inefficient. Despite the independence of MEMO and DDA on batch size, they incur a massive computational burden. For instance, C(gMEMO) = 54 and C(gDDA) = 810. Thus, both methods will be busy adapting for considerable portions of the stream, leaving most predic- tions to the non-adapted classifier. This phenomenon is the reason behind the reported performance of these methods being so close to that of fθ (i.e. around 82%). This result calls for future research to focus on increasing the efficiency of data-dependent adaptation methods. (iv) Sample rejection-oriented methods can perform well under the realistic protocol. EATA adapts efficiently due to its fast sample rejection algorithm, which relies solely on 6Evaluation of Test-Time Adaptation Under Computational Time Constraints the forward pass to admit samples for adaptation. EATA’s low error rate of 55.6%, combined with a small performance drop of less than 4%, positions it as the top performer under the realistic evaluation protocol on ImageNet-C. On the other hand, SAR does not benefit from sample rejection. SAR’s performance drop of 7.5% is due to its dependence on gradients for sample rejection, which reduces its speed. (v) SHOT benefits from the realistic protocol. Interest- ingly, we found that SHOT (and SHOT-IM in the Appendix), a fine-tuning-based approach, benefits from our realistic evaluation. In particular, we found that SHOT’s error rate decreases by 2% on fog corruption and by 0.8% on average. This observation could suggest that SHOT could potentially improve performance by disposing of fine-tuning on every batch. It is also worth mentioning that, under our realis- tic evaluation, SHOT (introduced in 2020) outperforms all methods except EATA. (vi) Performance changes are consistent across corrup- tions. Note that all methods that are somewhat efficient can improve the source model across all corruptions, in both the offline and realistic setups. Furthermore, the performance changes when comparing the offline and realistic setups are consistent across all corruptions. This finding suggests that the performance of these methods is independent of the do- main shift being considered. We further test this hypothesis by benchmarking these methods on two other datasets with other types of domain shifts in Section 4.4. 4.2. Continual Evaluation of TTA Next, we analyze the more challenging continual setup, fol- lowing (Wang et al., 2022; Niu et al., 2022). In particular, we construct the stream S by concatenating all corruptions from ImageNet-C. That is, we adapt TTA methods continu- ally on all corruptions followed by the clean validation set, without ever resetting the network weights. We introduce the notion of realistic adaptation to the continual setup to study the effects of a constant stream speed on the bench- mark. We report results in Figure 3 for both the offline and realistic protocols, where the horizontal-axis shows how cor- ruptions are ordered in the stream. We limit the experiments in this section to six TTA methods (SHOT, TENT, TTAC- NQ, COTTA, EATA, and SAR), and leave the remaining details for the Appendix. We observe: (i) Methods that do not perform sample rejection (SHOT, TENT, TTAC) scale poorly in the offline-continual setup. This phenomenon can be attributed to these methods over- fitting to early distributions. However, methods that do perform sample rejection (SAR and EATA) do not overfit as easily to corruptions, and can thus adapt to the rest of the stream. Even worse, such methods tend to even significantly degrade the performance on clean data. 1/16 1/8 1/4 1/2 1 η 52 55 58 61 64 67Error Rate (%) SHOT TENT TTAC-NQ SAR EATA Figure 4: Average Error Rate on ImageNet-C Under Slower Stream Speeds. We report the average error rate for several TTA methods on ImageNet-C under slower stream speeds. In our proposed realistic model evaluation, the stream speed r is normalized by the time needed for a for- ward pass using the base model. We evaluate different TTA methods under a stream with speed ηr with η ∈ (0, 1]. An η = 1/16 means the stream is 16 times slower than the forward pass of the base model. We report the standard deviation across 3 different random seeds. Different TTA methods degrade differently when varying η. (ii) In the realistic-continual setup, methods that do not perform sample rejection benefit from skipping adapta- tion on some batches, and become competitive with the methods that perform sample rejection. That is, while skipping parts of the stream deteriorated the performance of such methods in the episodic evaluation , this skipping actu- ally helped in preventing these methods from over-fitting in the continual setup. 4.3. Stream Speed Analysis In the previous experiments, we normalized the stream speed to be the same as that of fθ’s forward pass. That is, we assumed that the rate r at which S reveals new batches is equal to R (fθ(x)). However, some applications may enjoy a slower stream, giving TTA methods more time to adapt to samples. To explore this scenario, we vary the speed at which the stream reveals new data. In particular, let the new stream rate be η rwith η ∈ (0, 1]. Hence, as η → 0, the stream slows down and allows methods to adapt to all samples. Conversely, as η → 1, the stream speeds up, and at η = 1 we recover our realistic evaluation protocol. We experiment with the stream speed by setting η ∈ {1/16, 1/8, 1/4, 1/2, 1}, and evaluate five representative TTA methods (SHOT, TENT, TTAC-NQ, SAR, and EATA) in the episodic setup . Figure 4 summarizes our results by reporting the average error rate across all corruptions. We next list our observations: (i) The performance of TTA methods varies widely.For 7Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 3: Episodic Error Rate on ImageNet-C with ViT. We report the error rate of three baselines (Source, Tent, SAR) on the 15 different corruptions on ImageNet-C when the backbone is ViT architecture pretrained on ImageNet. We observe that while generally better backbones yield smaller error rate, expensive methods perform worse under our realistic evaluation. The more expensive the method is (e.g. SAR compared to Tent), the more performance reduction it suffers. Noise Blur Weather DigitalMethodRealisticgauss. shot impul. defoc. glass motionzoom snow frost fog brigh. contr. elast. pixel. jpeg Avg. ∆ Source ✓ 90.5 93.3 91.8 71.0 76.6 66.1 72.9 84.1 73.5 52.8 45.3 55.9 69.5 55.5 52.2 70.1 - ✗ 69.9 95.9 68.9 55.8 62.0 52.3 57.9 57.2 53.6 41.8 28.9 40.7 59.1 39.7 42.0 55.0Tent ✓ 80.7 88.9 81.0 63.0 69.5 58.3 64.9 65.8 59.7 47.7 33.2 47.3 64.6 45.1 46.4 61.1 (-6.1) ✗ 55.5 56.9 55.1 47.5 50.4 44.3 48.7 42.4 47.3 33.6 25.4 35.6 44.8 33.5 36.4 43.8SAR ✓ 70.0 72.5 69.4 56.6 63.4 54.0 60.0 56.4 53.5 43.0 30.5 43.3 58.7 41.5 43.8 54.5 (-10.7) example, TTAC-NQ starts degrading faster (at η = 1/16) due to its slow adaptation speed. For other methods, the η at which they degrade varies. For instance, while TENT has a higher error rate than SAR in slow streams (η ≤ 1/8), TENT outperforms SAR in the regime of faster streams η ≤ 1/4. Interestingly, SHOT (Liang et al., 2020) ranks the worst at η ≤ 1/8, then ranks second when η ≥ 1/2, becoming a viable alternative. At last, the order of different methods significantly changes depending on the speed of the stream. For example, SAR changes from being second best at η ≤ 1/8 to third at η = 1/4 and then to fifth ( i.e. second worst) at η ≥ 1/2. (ii) EATA provides a good trade-off between speed and performance. In fact, EATA gives the best overall perfor- mance (lowest error rate) independent of the stream’s speed. This virtue is attributable to EATA’s combination of good performance and adaptation speed based on efficient sample rejection. Results on other datasets are in the Appendix. 4.4. Results on Other Benchmarks and Architectures We extend our evaluation protocol to cover ImageNet- 3DCC (Kar et al., 2022) and ImageNet-R (Hendrycks et al., 2021) datasets and ResNet-18 (results in the ap- pendix) and ViT (Kolesnikov et al., 2021) architectures. ImageNet-R contains rendition versions of ImageNet span- ning 200 classes. ImageNet-3DCC constructs more spatially-consistent corruptions than ImageNet-C by lever- aging depth estimates. For ViT, we conduct episodic evalu- ation on ImageNet-C in a similar setup to Section 4.1 and report the results in Table 3 for the non-adapted model, Tent, and SAR. For ImageNet-R and ImageNet-3DCC, we fix the architecture to ResNet-50 and experiment on the entire datasets and set the severity level to 5 in ImageNet-3DCC. Due to the space constraint, we limit our experiments to the episodic evaluation, and leave other results and analyses to the Appendix. We evaluate the effectiveness of 10 TTA methods in Table 4, where we report the average error rate across all corruptions. We observe that our results are consistent across all con- Table 4: Average Error Rate on ImageNet-R and ImageNet-3DCC. We report the average error rate of dif- ferent TTA methods on ImageNet-R and ImageNet-3DCC under both the realistic and current setups. A lower error rate indicates a better TTA method. The highlighted num- bers indicate a better performance per method across setups. We observe that methods generally perform worse in the more realistic realistic setup. The conclusions are consistent with what we observed on ImageNet-C (Table 2). Method ImageNet-R ImageNet-3DCC Current Realistic ∆ Current Realistic ∆ Source 63.8 63.8 - 73.9 73.9 - AdaBN 60.6 60.6 0 72.1 72.1 0 BN 60.0 60.0 0 70.5 70.5 0 LAME 60.5 60.5 0 72.1 72.1 0 SHOT 70.3 62.6 (+7.7) 69.2 67.0 (+2.2) TENT 58.1 59.1 (-1.0) 64.5 66.8 (-2.3) SAR 57.5 59.6 (-2.1) 63.5 71.4 (-7.9) CoTTA 57.3 61.5 (-4.5) 66.4 75.6 (-9.2) EATA 55.7 57.1 (-1.4) 60.9 63.1 (-2.2) TTAC-NQ 59.2 60.8 (-1.6) 65.7 73.6 (-7.9) sidered datasets and architectures. Similar to our results in Table 2, the more computationally involved SAR de- grades more than Tent when leveraging ViT architecture. Regarding other datasets, we find that simple methods that adapt during the forward pass are unaffected by the realis- tic setup. All the other methods, except SHOT, experience degradation in their results on both datasets. We observe again that, on these two datasets, while SHOT actually ben- efits from the realistic evaluation, EATA remains the best alternative on both ImageNet-R and ImageNet-3DCC. 4.5. Evaluation under Practical TTA Recently, (Yuan et al., 2023) extended the continual test- time adaptation evaluation to include label-imbalances; known as Practical Test-Time Adaptation (PTTA) setup. In this setting, the stream not only reveals a continual se- quence of distribution shifts, but also the revealed batches 8Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 5: Episodic Error Rate on CIFAR10-C under Practical Evaluation (Yuan et al., 2023).We report the error rate of two baselines (Source, RoTTA (Yuan et al., 2023)) on the 15 different corruptions on CIFAR10-C when the backbone is ResNet-18. We observe that under our computational constrained evaluation, the only method tailored to this setting; RoTTA, performs worse than the non-adapted baseline. Noise Blur Weather DigitalMethodRealisticgauss. shot impul. defoc. glass motionzoom snow frost fog brigh. contr. elast. pixel. jpeg Avg. ∆ Source ✓ 72.3 65.7 72.9 46.9 54.3 34.8 42.0 25.1 41.3 26.0 9.3 46.7 26.6 58.5 30.3 43.5 - ✗ 36.9 34.9 45.8 16.6 44.2 19.9 16.53 21.6 22.4 18.8 9.8 20.6 28.4 27.1 34.5 26.5RoTTA ✓ 55.0 54.4 63.2 43.3 62.3 43.7 43.5 44.8 47.7 43.4 35.3 41.8 54.0 47.7 54.6 49.0 (-22.5) have significant label imbalances. To combat this combined challenge, the work of (Yuan et al., 2023) proposed to lever- age a balanced memory bank for adaptation. In this section, we extend our computational constrained evaluation to the PTTA setup and compare RoTTA (Yuan et al., 2023) with a non-adapted model on CIFAR10-C benchmark. Table 5 summarizes the results. We observe that while RoTTA indeed reduces the error rate under the PTTA setup on CIFAR10-C (17% below the non-adapted model), our realistic evaluation uncovers its computational limitation. We found that RoTTA’s error rate increases by over 22% surpassing the error rate of the non-adapted model. Note that RoTTA stores samples from the stream in a memory bank then adapts the model on sampled samples from the memory bank. Thus, the slower the adaptation of RoTTA, the less diverse the samples in the memory bank, hindering its adaptation. 4.6. Effect of Hyper-parameter Tuning The performance of different TTA methods heavily depends on their hyper-parameter settings (Zhao et al., 2023). Here, we assess the impact of our proposed evaluation on TTA methods when tuning their hyperparameters. For that regard, we conduct hyper parameter search for Tent (as a fundamen- tal baseline) and experiment with different learning rates (the only hyper-parameter for Tent). Table 6 summarizes the results under episodic evaluation for 4 different corruptions on ImageNet-C. We observe that while conducting hyper-parameter search indeed improves the performance of TENT, its error rate increases under our realistic evaluation across all hyperparameters. That is, while conducting hyper-parameter search might indeed result in a better performance for TTA methods, the insights obtained through our proposed evaluation scheme remains consistent: more efficient TTA methods will have a smaller performance drop under the realistic evaluation. 5. Conclusions In this work, we find that the performance of Test Time Adaptation (TTA) methods can vary depending on the con- Table 6: Effect of our evaluation under hyperparameter tuning. We report the error rate for Tent under different learning rates under both the current and our proposed real- istic evaluation. While carefully tuning the learning rate for Tent results in a better performance, our realistic evaluation causes a performance drop under all learning rates. lr Realisticgauss. motion fog pixel. Avg. ∆ ✗ 74.1 63.3 44.7 43.5 56.41×10−4 ✓ 79.7 69.0 47.8 46.8 60.8 (-4.4) ✗ 71.1 59.7 43.1 41.9 53.92×10−4 ✓ 77.6 66.1 46.0 45.0 58.7 (-4.7) ✗ 69.6 58.1 42.4 41.1 52.83×10−4 ✓ 74.9 64.0 45.0 44.0 57.0 (-4.2) ✗ 68.8 57.1 42.0 40.8 52.24×10−4 ✓ 73.7 62.3 44.5 43.2 55.9 (-3.7) text in which they are used. In the episodic evaluation, the efficiency of the method is the most important factor, with more efficient methods like AdaBN and BN showing consistent performance, while data-dependent approaches suffer. Sample rejection methods generally perform well, and fine-tuning approaches such as SHOT can even improve when adapting to fewer samples. In the continual evalua- tion, methods that do not perform sample rejection scale poorly in the offline-continual setup but benefit from skip- ping adaptation on some batches in the realistic-continual setup. Furthermore, our stream speed analysis shows that the performance of TTA methods can vary widely at differ- ent speeds. Our findings are consistent across corruptions and multiple datasets. They can help researchers and practi- tioners to better understand the strengths and weaknesses of different TTA methods, and to choose the most appropriate method for their specific use case. Acknowledgements This work was partially done during a research internship of the first author at Intel Labs. This work was supported by the King Abdullah University of Science and Technol- ogy (KAUST) Office of Sponsored Research (OSR) under Award No. OSR-CRG2021-4648. We would like to thank Yasir Ghunaim and Mattia Soldan for the helpful discussion. 9Evaluation of Test-Time Adaptation Under Computational Time Constraints Impact Statement Our work advances Machine Learning by proposing a re- alistic evaluation protocol for Test Time Adaptation meth- ods, prioritizing computational efficiency. This approach promotes the development of AI systems that are both ac- cessible in resource-limited settings and environmentally sustainable, by favoring simpler, faster methods. Such ad- vancements contribute to more inclusive and responsible AI deployment, aligning with ethical goals of broadening access and reducing environmental impacts References Alfarra, M., P´erez, J. C., Thabet, A., Bibi, A., Torr, P. H., and Ghanem, B. Combating adversaries with anti-adversaries. In Proceedings of the AAAI Conference on Artificial In- telligence, volume 36, pp. 5992–6000, 2022. Ashukha, A., Lyzhov, A., Molchanov, D., and Vetrov, D. Pitfalls of in-domain uncertainty estimation and ensem- bling in deep learning. arXiv preprint arXiv:2002.06470, 2020. Boudiaf, M., Mueller, R., Ben Ayed, I., and Bertinetto, L. Parameter-free online test-time adaptation. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8344–8353, 2022. Cai, Z., Sener, O., and Koltun, V . Online continual learning with natural distribution shifts: An empirical study with visual data. In Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision, pp. 8281–8290, 2021. Chen, D., Wang, D., Darrell, T., and Ebrahimi, S. Con- trastive test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 295–305, 2022. Croce, F., Gowal, S., Brunner, T., Shelhamer, E., Hein, M., and Cemgil, T. Evaluating the adversarial robustness of adaptive test-time defenses. In International Conference on Machine Learning, pp. 4421–4435. PMLR, 2022. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009. Gao, J., Zhang, J., Liu, X., Darrell, T., Shelhamer, E., and Wang, D. Back to the source: Diffusion-driven test-time adaptation. arXiv preprint arXiv:2207.03442, 2022. Gidaris, S., Singh, P., and Komodakis, N. Unsupervised rep- resentation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018. Gong, T., Jeong, J., Kim, T., Kim, Y ., Shin, J., and Lee, S.-J. Note: Robust continual test-time adaptation against temporal correlation. In Advances in Neural Information Processing Systems. Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain- ing and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions and pertur- bations. Proceedings of the International Conference on Learning Representations, 2019. Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M., Song, D., Steinhardt, J., and Gilmer, J. The many faces of robustness: A critical analysis of out-of-distribution generalization. ICCV, 2021. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion proba- bilistic models. Advances in Neural Information Process- ing Systems, 33:6840–6851, 2020. Iwasawa, Y . and Matsuo, Y . Test-time classifier adjustment module for model-agnostic domain generalization. Ad- vances in Neural Information Processing Systems , 34: 2427–2440, 2021. Kar, O. F., Yeo, T., Atanov, A., and Zamir, A. 3d common corruptions and data augmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18963–18974, 2022. Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Des- jardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017. Kojima, T., Matsuo, Y ., and Iwasawa, Y . Robustifying vision transformer without retraining from scratch by test- time class-conditional feature alignment. arXiv preprint arXiv:2206.13951, 2022. Kolesnikov, A., Dosovitskiy, A., Weissenborn, D., Heigold, G., Uszkoreit, J., Beyer, L., Minderer, M., Dehghani, M., Houlsby, N., Gelly, S., Unterthiner, T., and Zhai, X. An image is worth 16x16 words: Transformers for image recognition at scale. 2021. 10Evaluation of Test-Time Adaptation Under Computational Time Constraints Lee, D.-H. et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural net- works. In Workshop on challenges in representation learning, ICML, volume 3, pp. 896, 2013. Li, Y ., Wang, N., Shi, J., Liu, J., and Hou, X. Revisit- ing batch normalization for practical domain adaptation. arXiv preprint arXiv:1603.04779, 2016. Liang, J., Hu, D., and Feng, J. Do we really need to access the source data? source hypothesis transfer for unsuper- vised domain adaptation. In International Conference on Machine Learning, pp. 6028–6039. PMLR, 2020. Liu, Y ., Kothari, P., Van Delft, B., Bellot-Gurlet, B., Mordan, T., and Alahi, A. Ttt++: When does self-supervised test-time training fail or thrive? Advances in Neural Information Processing Systems, 34:21808–21820, 2021. Mancini, M., Karaoguz, H., Ricci, E., Jensfelt, P., and Ca- puto, B. Kitting in the wild through online domain adap- tation. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1103–1109. IEEE, 2018. Mirza, M. J., Micorek, J., Possegger, H., and Bischof, H. The norm must go on: dynamic unsupervised do- main adaptation by normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14765–14775, 2022a. Mirza, M. J., Soneira, P. J., Lin, W., Kozinski, M., Possegger, H., and Bischof, H. Actmad: Activation matching to align distributions for test-time-training, 2022b. URL https://arxiv.org/abs/2211.12870. Niu, S., Wu, J., Zhang, Y ., Chen, Y ., Zheng, S., Zhao, P., and Tan, M. Efficient test-time model adaptation with- out forgetting. In International conference on machine learning, pp. 16888–16905. PMLR, 2022. Niu14, S., Wu, J., Zhang, Y ., Wen, Z., Chen, Y ., Zhao, P., and Tan15, M. Towards stable test-time adaptation in dynamic wild world. International Conference on Learning Representations, 2023. P´erez, J. C., Alfarra, M., Jeanneret, G., Rueda, L., Thabet, A., Ghanem, B., and Arbel´aez, P. Enhancing adversarial robustness via test-time transformation ensembling. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 81–91, 2021. Saenko, K., Kulis, B., Fritz, M., and Darrell, T. Adapting visual category models to new domains. In Computer Vision–ECCV 2010: 11th European Conference on Com- puter Vision, Heraklion, Crete, Greece, September 5-11, 2010, Proceedings, Part IV 11 , pp. 213–226. Springer, 2010. Sakaridis, C., Dai, D., and Van Gool, L. Acdc: The ad- verse conditions dataset with correspondences for seman- tic driving scene understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10765–10775, 2021. Schneider, S., Rusak, E., Eck, L., Bringmann, O., Brendel, W., and Bethge, M. Improving robustness against com- mon corruptions by covariate shift adaptation. Advances in Neural Information Processing Systems, 2020. Shalev-Shwartz, S. et al. Online learning and online con- vex optimization. Foundations and Trends® in Machine Learning, 4(2):107–194, 2012. Su, Y ., Xu, X., and Jia, K. Revisiting realistic test-time training: Sequential inference and adaptation by anchored clustering. arXiv preprint arXiv:2206.02721, 2022. Sun, Y ., Wang, X., Liu, Z., Miller, J., Efros, A., and Hardt, M. Test-time training with self-supervision for generaliza- tion under distribution shifts. In International conference on machine learning, pp. 9229–9248. PMLR, 2020. Torralba, A. and Efros, A. A. Unbiased look at dataset bias. In CVPR 2011, pp. 1521–1528. IEEE, 2011. Tzeng, E., Hoffman, J., Saenko, K., and Darrell, T. Adver- sarial discriminative domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7167–7176, 2017. Wang, D., Shelhamer, E., Liu, S., Olshausen, B., and Darrell, T. Tent: Fully test-time adaptation by entropy minimiza- tion. arXiv preprint arXiv:2006.10726, 2020. Wang, Q., Fink, O., Van Gool, L., and Dai, D. Continual test- time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7201–7211, 2022. Yuan, L., Xie, B., and Li, S. Robust test-time adaptation in dynamic scenarios. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15922–15932, 2023. Zhang, M., Levine, S., and Finn, C. Memo: Test time ro- bustness via adaptation and augmentation. arXiv preprint arXiv:2110.09506, 2021. Zhao, H., Liu, Y ., Alahi, A., and Lin, T. On pitfalls of test- time adaptation. International Conference on MAchine Learning, 2023. 11Evaluation of Test-Time Adaptation Under Computational Time Constraints A. Methodology A.1. Online Computation of C(g) Section 3.2 discussed the online evaluation protocol of TTA methods. Here, we give more details on the calcu- lation of C(g), the relative adaptation speed of g, during our online evaluation. First, we set R (g(x)) as the time recording function for g to perform a forward pass for a single batch. To ensure a reliable time calculation, we exe- cute torch.cuda.synchronize() before starting the timer and before ending it. This ensures all GPU operations are finished for the moment time is computed. To alleviate hardware dependence, we also calculate R(fθ(x)) for each evaluation step computing the relative adaptation complex- ity. It is worth mentioning that C(g) for SHOT, EATA, SAR, and COTTA are[3, 3, 8, 103] on average, respectively. B. Experiments B.1. Episodic Evaluation of TTA SHOT, PL, and ETA For completeness, we report the results on 3 baselines: Pseudo Label (Lee et al., 2013), SHOT-IM (Liang et al., 2020), and ETA (Niu et al., 2022) in Table 7. We follow the same setup as in the main paper. Our results are consistent with the findings of Section 4.1 and Table 2. In particular, SHOT-IM improves its perfor- mance under the online evaluation, similar to SHOT. Further, the performance of ETA and PL degrades under the online evaluation due to the additional computational burden. Nev- ertheless, ETA is similar to EATA in providing the best tradeoff between additional computational requirements and performance improvements. SAR with GN We equip our results to include ResNet50 with Group Normalization (GN) layers, following (Niu14 Figure 5: C(g) computation across iterations. We report our online calculations for the relative adaptation speed ofg, C(g), for SAR, SHOT, EATA, and TENT throughout a full evaluation episode. We observe that, overall, C(g) has a stable behavior throughout evaluation iterations. et al., 2023). We report the results in Table 7, where we observe that: (i) Under a relatively large batch size (64), ResNet50 with GN underperforms ResNet50 with Batch Normalization. In fact, the average error rate for SAR in- creases from 56.2% to 65.8%. (ii) The online evaluation penalizes SAR in both architecture choices with a perfor- mance degradation of 3.6% under the GN-based ResNet. Finally, it is worth mentioning that SAR with GN layers attains a similar performance under a batch size of 1. Ablating Batch Sizes In the experiments section, we fixed the batch size to 64 following the recommendations of ear- lier works (Wang et al., 2020; Niu et al., 2022). Here, we investigate the effect of our proposed online evaluation un- der different choices of batch sizes. To that end, we vary the batch size in {1, 16, 32, 128}, and report the results in Figure 6. We draw the following observations: Table 7: Episodic Error Rate on ImageNet-C. We report the error rate of different TTA methods on the ImageNet-C benchmark under both the online and offline setups. A lower error rate indicates a better TTA method. The highlighted numbers indicate a better performance per method across setups. Episodic means the model will adapt to one corruption at a time. The model is reset back to the base model when moving to the next corruption. The offline setup is merely the reproduction of every method. We show that methods generally perform worse in the more realistic online setup. The more computationally complex the TTA method is, the less data it will adapt to, and the worse its performance. SAR-GN represents SAR when deployed on ResNet50 with Group Normalization (GN) layers, following (Niu14 et al., 2023). Noise Blur Weather DigitalMethod Online gauss. shot impul. defoc. glass motionzoom snow frost fog brigh. contr. elast. pixel. jpeg Avg. ∆ ✗ 73.1 69.8 72.0 76.9 75.9 58.5 52.7 53.3 62.2 43.8 34.6 82.6 46.0 42.3 48.9 59.5SHOT-IM ✓ 71.1 68.6 70.7 73.2 73.6 59.1 51.9 52.8 60.5 43.7 33.6 77.3 45.7 42.1 48.6 58.2 (-0.3) ✗ 92.2 92.2 92.8 97.0 89.8 57.7 49.6 50.7 57.1 41.5 32.6 91.1 44.3 40.3 46.6 65.0PL ✓ 90.6 86.3 83.6 93.2 89.7 63.0 51.7 55.0 59.3 43.8 32.9 92.3 47.3 42.4 49.3 65.3 (+0.3) ✗ 64.9 62.7 63.6 66.4 66.3 52.4 47.3 48.2 54.1 40.2 32.2 54.8 42.3 39.2 44.7 52.0ETA ✓ 70.2 67.0 69.6 71.5 71.5 56.9 50.2 51.9 57.0 42.0 32.5 60.5 44.6 40.8 47.1 55.6 (+3.6) ✗ 71.8 69.0 70.3 81.5 81.0 69.6 69.5 57.1 56.6 94.3 29.2 56.0 84.8 51.4 44.7 65.8SAR-GN ✓ 82.0 80.2 82.1 80.2 88.6 78.5 75.1 59.6 53.9 66.9 30.7 63.3 81.3 71.3 47.5 69.4 (+3.6) 12Evaluation of Test-Time Adaptation Under Computational Time Constraints 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) ADABN OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100  BN-ADAPTATION OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 COTTA OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) EATA OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 ETA OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100  LAME OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) PL OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 SAR OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 SHOT OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) SHOTIM OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 TENT OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 TTAC-NQ OFFLINE ONLINE Figure 6: Batch Size Analysis current vs. realistic setups for every method. We assess the performance variation of 12 different TTA methods under varying batch sizes. We experiment with batch sizes in{1, 16, 32, 128}. We do not include the baseline, MEMO, and DDA, since they are data-dependent approaches and are unaffected by batch size. All TTA methods, except LAME, are severely affected by smaller batch sizes. Nonetheless, the realistic evaluation degrades the performance of all methods, except SHOT and SHOT-IM. (i) Online evaluation improves the performance of SHOT and SHOT-IM. This result is consistent with the earlier observations in Table 2. Note that PL shares a similar trend as well. (ii) The performance of TTA methods degrades when switching from offline to online evaluation, regardless of the batch size. This result is highlighted in COTTA, ETA, EATA, SAR, TENT, and TTAC-NQ. (iii) Performance of TTA methods vastly varies when varying the batch size. This result is consistent with earlier findings in the literature (Gao et al., 2022; Niu14 et al., 2023), where most TTA methods fail with small batch sizes. At last, and to ease comparison across methods, we summa- rize all the plots for all methods in Figure 7. Consistency with 3 random seeds. For all of our exper- iments, we run each experiment with 3 random seeds. In most of our results, we found out that the standard deviation of performance across runs is very small. Our results in Figures 3 and 4 demonstrate this variation in the shaded area for 5 different TTA methods. B.2. Continual Evaluation of TTA We further explore another setup for the continual evalua- tion of TTA. In particular, we follow (Wang et al., 2022) in concatenating all corruptions in ImageNet-C with 11 differ- ent orders. We then report the average performance of each method across all runs and corruptions in Table 8. We run each experiment with 3 random seeds, and report our results with standard deviations. For the remaining implementation 13Evaluation of Test-Time Adaptation Under Computational Time Constraints 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) OFFLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 ONLINE ADABN BN-ADAPTATION COTTA EATA ETA LAME PL SAR SHOT SHOTIM TENT TTAC-NQ Figure 7: Summary of batch size analysis: current vs. realistic setups. Left: Current evaluation, i.e.,Section 3.1. Right: Realistic evaluation,i.e.,Section 3.2. While EATA achieves the lowest error rate under batch sizes≥ 32, SHOT becomes a very competitive baseline, outperforming EATA, at a batch size of 128. Table 8: Continual Error Rate on ImageNet-C. We report the average continual error rate for 11 different corruption orders, with 3 different seeds, under both the offline and online setups with a corruption severity level of 5. Continual refers to continually adapting after each corruption without resetting. This metric indicates the model’s capability to learn from previous corruptions. The offline setup refers to the performance of the model in a continual learning scheme, whereas the online setup refers to the performance of the model in a continual learning scheme, under our more realistic online setup. We show that the more complex a method is, the fewer samples it adapts to, achieving better performance in a continual learning scheme. Avg. Error (%) COTTA ETA TENT SAR EATA SHOT TTAC-NQ Offline 65.3 ± 5.9 56 .4 ± 2.3 84 .6 ± 16.0 59 .8 ± 3.0 56 .4 ± 2.3 88 .4 ± 11.4 81 .8 ± 11.4 Online 69.3 ± 2.8 57 .7 ± 2.0 65 .6 ± 5.0 60 .4 ± 1.8 57 .7 ± 1.9 78 .2 ± 7.7 65 .1 ± 3.8 details, we follow our setup in main paper. We observe that, similar to our conclusions in Section 4.2, online eval- uation helps methods that do not perform sample rejection (e.g.,TENT). Nonetheless, both ETA and EATA provide the best trade-off between performance and additional compu- tational burden. B.3. Stream Speed Analysis For completeness, we extend our stream speed analysis in Section 4.3 to cover the ImageNet-3DCC dataset. We preserve our experimental setup by varying the stream speed according to ηr, with η ∈ {1/16, 1/8, 1/4, 1/2, 1. Figure 8 summarizes our results for SHOT, TENT, TTAC-NQ, EATA, and SAR. We observe similar trends to the ones in Figure 4, where the performance of different TTA methods varies widely under different stream speeds. The large relative adaptation speed of TTAC-NQ degrades its performance under even slow streams (e.g.,η = 1/8), while SHOT reduces its error rate under faster streams. Furthermore, EATA is consistently outperforming all other considered approaches under different stream speeds. B.4. Evaluation on Other Benchmarks We report the error rates on all corruptions of ImageNet- 3DCC (Kar et al., 2022), along with the overall average error rate, in Table 9. The conclusions we draw for ImageNet- 3DCC (Kar et al., 2022) are very similar to the ones ob- served on ImageNet-C (Hendrycks & Dietterich, 2019) (in Section 4.1). We observe that efficient methods, with C(g) = 1, such as LAME and BN, maintain performance. Furthermore, the performance of some TTA methods (Wang et al., 2020; Niu14 et al., 2023; Niu et al., 2022; Wang et al., 2022) degrades in the online setup, while others that use pseudo labeling (Lee et al., 2013; Liang et al., 2020) actually improve. This degradation seems to be directly proportional to the amount of data a method misses according to its C(g). 14Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 9: Episodic Error Rate on ImageNet-3DCommonCorruptions. We report the error rate of different TTA methods on ImageNet-3DCC (Kar et al., 2022) benchmark under both the realistic and offline setups. A lower error rate indicates a better TTA method. The highlighted numbers indicate a better performance per method across setups. Episodic means the model will adapt to one corruption at a time. The model is reset back to the base model when moving to the next corruption. The offline setup corresponds to reproducing the reported performance of every method. The first sub-table corresponds to methods that incur none or few additional computations, i.e.,C(g) = 1. We show that methods generally perform worse in the more realistic setup. The more computationally complex the TTA method is, the fewer data it will adapt to, and the worse its performance. Depth of field Noise LightingWeather Video Camera motionMethod RealisticNear focus Far focusColor quant. ISO noise Low lightFlash Fog 3DBit error H.265 ABR H.265 CRFXY-mot. blur Z-mot. blurAvg. ∆ Source ✓ 46.9 55.6 82.5 94.0 71.7 78.7 75.3 88.6 70.6 65.4 82.0 75.3 73.9 -AdaBN ✓ 45.2 55.0 71.8 76.8 64.1 80.8 75.0 91.8 80.9 76.7 79.1 67.5 72.1 -LAME ✓ 45.3 55.0 71.9 76.9 64.1 80.8 75.1 91.8 80.9 76.8 79.2 67.6 72.1 -BN ✓ 43.9 54.3 72.3 76.6 60.9 80.1 72.4 90.9 78.7 73.8 76.9 65.6 70.5 - PL ✗ 39.8 49.8 65.5 72.6 48.9 79.0 66.1 97.5 92.1 86.2 88.7 57.6 70.3(-1.6)✓ 41.0 51.3 66.5 71.5 52.8 77.4 68.1 95.6 86.0 78.7 77.0 59.2 68.7 SHOT ✗ 43.0 53.6 67.1 64.2 51.9 81.1 73.2 97.2 83.5 77.8 77.3 60.1 69.2(-2.2)✓ 41.7 51.4 64.4 63.8 51.6 77.5 71.6 95.1 79.9 74.6 73.7 58.5 67.0 SHOT-IM✗ 42.2 52.7 66.6 63.7 51.0 81.0 72.1 97.0 83.3 77.6 75.6 59.2 68.5(-1.9)✓ 41.2 51.2 64.4 63.3 51.3 77.5 70.9 94.9 79.4 74.1 72.3 58.3 66.6 TENT ✗ 39.9 49.6 62.4 62.2 50.7 75.6 68.5 91.6 75.7 70.2 70.4 57.0 64.5(+2.3)✓ 41.7 51.4 65.5 67.2 54.7 77.4 70.1 90.7 76.8 71.9 74.0 60.8 66.8 SAR ✗ 40.3 50.0 62.0 61.2 50.6 73.8 65.8 90.1 73.9 68.8 69.1 56.8 63.5(+6.9)✓ 44.9 54.7 71.1 75.4 62.6 80.3 73.8 91.7 80.5 76.1 78.6 66.9 71.4 ETA ✗ 38.7 47.9 59.1 56.7 46.8 71.0 62.1 90.6 72.8 67.3 64.7 52.9 60.9(+2.3)✓ 39.7 49.3 61.6 60.7 50.0 73.5 65.2 90.3 74.4 69.1 68.8 55.9 63.2 CoTTA ✗ 40.8 50.9 66.3 68.3 54.6 77.2 68.0 90.2 76.4 71.1 73.1 60.4 66.4(+9.2)✓ 55.4 63.1 74.1 77.0 64.7 83.4 78.1 93.7 84.0 80.3 81.7 71.9 75.6 TTAC-NQ✗ 40.7 50.5 61.0 61.1 51.5 72.8 66.6 93.8 81.1 74.7 75.7 59.1 65.7(+7.9)✓ 49.9 57.0 69.3 72.3 58.9 79.8 76.3 95.8 86.5 83.0 84.6 69.8 73.6 EATA ✗ 38.6 47.8 59.2 56.6 46.9 71.2 62.2 90.9 72.5 67.4 64.6 52.9 60.9(+2.2)✓ 39.8 49.3 61.6 60.5 49.9 73.5 64.8 90.6 73.7 69.1 68.6 55.7 63.1 C. Single Model Evaluation Scheme In Section 3.2, we assume fθt can generate predictions whenever g is occupied with adapting to a batch. This setup assumes the capacity to concurrently deploy two models. However, this assumption might be unfair to methods with C(g) = 1, since it allows expensive methods to skip batches without large penalties. We thus also study the case where only one model can be deployed. Studying this setup requires establishing a policy on how samples missed by the TTA method g are treated. That is, when g is busy adapting, all skipped samples still must be predicted without access to fθt . Depending on the applica- tion, this prediction could leverage prior knowledge about the problem e.g. temporal correlation across samples, or the bias of the distribution. In our setup, we consider the most strict scenario in which, whenever g is busy, a ran- dom classifier generates predictions for the incoming sam- ples. This naive design choice results from our evaluation on ImageNet-based datasets, which contain images whose classes display no bias nor temporal correlation. We conduct episodic evaluation, similar to Section 4.1, on ImageNet-C dataset. We average the error rates per corruption category (e.g. averaging error rates for gaussian, shot, and impulse noises) and present the results of this study in Table 10. We draw the following observation. Single model evaluation strongly favors methods with C(g) = 1. We observe that all models that are slower than the stream are heavily penalized to the point that using the original pre-trained model becomes a better alternative. However, methods that can be as fast as the stream, like AdaBN or BN, become the best alternative due to their speed. This result encourages more research toward devel- oping efficient TTA methods that have negligible additional computational overhead. D. Results on ResNet18 In our experiments in the main paper, we focused on the stan- dard ResNet18-architecture, following the common practice in the literature. Here, and for completeness, we extend our results to cover the smaller and more efficient ResNet18 architecture. Teble 11 summarizes the episodic evaluation of 6 TTA methods on ImageNet-C dataset. Similar to our conclusions in the episodic evaluation section in the main paper, more expensive adaptation methods degrade more under our realistic evaluation scheme. 15Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 10: Per Corruption Category Average Error Rate Using Single Model Evaluation on ImageNet-C. We re- port the average error rate per corruption category of dif- ferent TTA methods under single model realistic evaluation mode on ImageNet-C. Single model mode assumes the de- ployment of a single modelg instead of two under a constant speed stream S. We assume the most extreme scenario, that is if a model g is occupied adapting to a batch, the incoming batch is fed to a random classifier. We observe that the best TTA methods to use in this scenario are AdaBN (Li et al., 2016) and BN (Schneider et al., 2020), which simply adapt the BN statistics. Method Realistic Noise Blur Weather Digital Avg. Source ✓ 97.7 83.8 69.1 81.4 82.0 AdaBN ✓ 84.5 76.1 54.9 62.7 68.5 BN ✓ 84.1 73.1 54.2 59.9 66.7 SHOT ✓ 92.6 91.3 87.0 88.5 89.7 TENT ✓ 91.9 89.4 83.0 85.0 87.0 SAR ✓ 95.6 94.0 90.1 91.3 92.6 EATA ✓ 89.4 87.6 82.0 83.2 85.3 TTAC-NQ ✓ 96.6 96.9 96.3 96.4 96.5 Table 11: Evaluating different TTA methods with ResNet- 18 architecture on ImageNet-C. We report the average error rate across all different types of corruptions (lower is bet- ter). TTA methods generally perform worse in the more realistic setup. The more computationally complex the TTA method is, the less data it will adapt to, and the worse is its performance. Method Basic BN SHOT Tent EATA SAR Current 85.4 70.1 64.4 64.9 59.7 63.8 Realistic 85.4 70.1 64.5 68.3 63.2 69.5 Diff - - 0.1 3.4 3.5 5.7 1/16 1/8 1/4 1/2 1 η 62 64 66 68 70 72 74Error Rate (%) SHOT TENT TTAC-NQ SAR EATA Figure 8: Average Error Rate on ImageNet-3DCC Under Slower Stream Speeds. We report the average error rate for several TTA methods on ImageNet-3DCC under slower stream speeds. In our proposed online model evaluation, the stream speed r is normalized by the time needed for a forward pass using the base model. We evaluate different TTA methods under a stream with speed ηr with η ∈ (0, 1]. An η = 1/16 means the stream is 16 times slower than the forward pass of the base model. We report the standard deviation across 3 random seeds. Different TTA methods degrade differently when varying η. 16",
      "meta_data": {
        "arxiv_id": "2304.04795v2",
        "authors": [
          "Motasem Alfarra",
          "Hani Itani",
          "Alejandro Pardo",
          "Shyma Alhuwaider",
          "Merey Ramazanova",
          "Juan C. Pérez",
          "Zhipeng Cai",
          "Matthias Müller",
          "Bernard Ghanem"
        ],
        "published_date": "2023-04-10T18:01:47Z",
        "pdf_url": "https://arxiv.org/pdf/2304.04795v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper proposes a novel online evaluation protocol for Test Time Adaptation (TTA) methods that addresses the critical oversight of computational cost in existing protocols. It penalizes slower methods by providing them with fewer samples for adaptation, thereby reflecting real-world scenarios where data arrives as a constant-speed stream. The key finding is that, when accounting for inference speed, simple and fast TTA approaches can outperform more sophisticated but slower state-of-the-art methods, highlighting the importance of developing accurate and efficient TTA methods.",
        "methodology": "The core methodology is the proposed Realistic TTA evaluation protocol, which simulates a constant-speed data stream. Unlike current offline protocols that assume the stream waits for TTA adaptation, this protocol intrinsically penalizes slow TTA methods. It introduces a 'relative adaptation speed' C(g), defined as the integer ratio of the stream's speed to the method's speed. If a method g is k times slower than the stream (C(g) = k), it may only use every kth sample for adaptation. During the time a TTA method is adapting, incoming samples are processed by the most recent adapted model (fθt+1) without further adaptation. C(g) is computed online for each input, accounting for hardware and input-dependent variability, using the ceiling function to handle discrete-time streams.",
        "experimental_setup": "Experiments were conducted on image classification tasks using a ResNet-50-BN3 model pre-trained on ImageNet. A batch size of 64 was primarily used, with an ablation study for batch sizes {1, 16, 32, 128}. Datasets included ImageNet-C (with 15 corruptions at level 5), CIFAR10-C, ImageNet-R, and ImageNet-3DCC. The study benchmarked 15 state-of-the-art TTA methods (e.g., BN, AdaBN, SHOT, TENT, SAR, CoTTA, LAME, EATA, TTAC-NQ, MEMO, DDA, PL, SHOT-IM, ETA) published between 2017 and 2023. Evaluations were performed across five scenarios: episodic domain shifts, continual domain shifts, varying stream speeds (ηr where η ∈ {1/16, 1/8, 1/4, 1/2, 1}), continual shifts with label correlation (Practical TTA), and a 'single model' evaluation scheme where no fallback model (fθ) is available for skipped samples. Additionally, evaluations were extended to ViT and ResNet-18 architectures.",
        "limitations": "The current evaluation protocol implicitly assumes that the data stream waits for TTA methods to complete adaptation, favoring slower methods. The proposed realistic protocol addresses this. However, the main realistic protocol assumes the capacity to concurrently deploy two models (the TTA method `g` and the most recent adapted model `fθt+1`), which might be an unfair advantage for expensive methods by allowing them to skip batches without severe penalties. The paper also notes that estimating the relative adaptation speed C(g) can be a noisy process due to hardware and input dependence, though their online computation aims to mitigate this. Data-dependent approaches like MEMO and DDA are found to be extremely inefficient, incurring massive computational burdens. Also, methods like RoTTA, tailored for practical TTA with label imbalances, show significant performance degradation under computational constraints, implying that even specialized methods can be severely limited by speed considerations.",
        "future_research_directions": "Future research should focus on increasing the efficiency of data-dependent adaptation methods, as current ones are extremely inefficient under the proposed realistic evaluation protocol. The proposed evaluation scheme aims to inspire future TTA methods to consider inference speed as a critical dimension affecting their real-world performance. There is also an implicit call for developing efficient TTA methods that have negligible additional computational overhead, as demonstrated by the strong performance of simple, fast methods like AdaBN and BN in the single model evaluation scenario."
      }
    },
    {
      "title": "Leveraging Proxy of Training Data for Test-Time Adaptation"
    },
    {
      "title": "Towards Stable Test-time Adaptation in Dynamic Wild World",
      "abstract": "Test-time adaptation (TTA) has shown to be effective at tackling distribution\nshifts between training and testing data by adapting a given model on test\nsamples. However, the online model updating of TTA may be unstable and this is\noften a key obstacle preventing existing TTA methods from being deployed in the\nreal world. Specifically, TTA may fail to improve or even harm the model\nperformance when test data have: 1) mixed distribution shifts, 2) small batch\nsizes, and 3) online imbalanced label distribution shifts, which are quite\ncommon in practice. In this paper, we investigate the unstable reasons and find\nthat the batch norm layer is a crucial factor hindering TTA stability.\nConversely, TTA can perform more stably with batch-agnostic norm layers, \\ie,\ngroup or layer norm. However, we observe that TTA with group and layer norms\ndoes not always succeed and still suffers many failure cases. By digging into\nthe failure cases, we find that certain noisy test samples with large gradients\nmay disturb the model adaption and result in collapsed trivial solutions, \\ie,\nassigning the same class label for all samples. To address the above collapse\nissue, we propose a sharpness-aware and reliable entropy minimization method,\ncalled SAR, for further stabilizing TTA from two aspects: 1) remove partial\nnoisy samples with large gradients, 2) encourage model weights to go to a flat\nminimum so that the model is robust to the remaining noisy samples. Promising\nresults demonstrate that SAR performs more stably over prior methods and is\ncomputationally efficient under the above wild test scenarios.",
      "meta_data": {
        "arxiv_id": "2302.12400v1",
        "authors": [
          "Shuaicheng Niu",
          "Jiaxiang Wu",
          "Yifan Zhang",
          "Zhiquan Wen",
          "Yaofo Chen",
          "Peilin Zhao",
          "Mingkui Tan"
        ],
        "published_date": "2023-02-24T02:03:41Z",
        "pdf_url": "https://arxiv.org/pdf/2302.12400v1.pdf"
      }
    },
    {
      "title": "Towards Stable Test-time Adaptation in Dynamic Wild World",
      "abstract": "Test-time adaptation (TTA) has shown to be effective at tackling distribution\nshifts between training and testing data by adapting a given model on test\nsamples. However, the online model updating of TTA may be unstable and this is\noften a key obstacle preventing existing TTA methods from being deployed in the\nreal world. Specifically, TTA may fail to improve or even harm the model\nperformance when test data have: 1) mixed distribution shifts, 2) small batch\nsizes, and 3) online imbalanced label distribution shifts, which are quite\ncommon in practice. In this paper, we investigate the unstable reasons and find\nthat the batch norm layer is a crucial factor hindering TTA stability.\nConversely, TTA can perform more stably with batch-agnostic norm layers, \\ie,\ngroup or layer norm. However, we observe that TTA with group and layer norms\ndoes not always succeed and still suffers many failure cases. By digging into\nthe failure cases, we find that certain noisy test samples with large gradients\nmay disturb the model adaption and result in collapsed trivial solutions, \\ie,\nassigning the same class label for all samples. To address the above collapse\nissue, we propose a sharpness-aware and reliable entropy minimization method,\ncalled SAR, for further stabilizing TTA from two aspects: 1) remove partial\nnoisy samples with large gradients, 2) encourage model weights to go to a flat\nminimum so that the model is robust to the remaining noisy samples. Promising\nresults demonstrate that SAR performs more stably over prior methods and is\ncomputationally efficient under the above wild test scenarios.",
      "meta_data": {
        "arxiv_id": "2302.12400v1",
        "authors": [
          "Shuaicheng Niu",
          "Jiaxiang Wu",
          "Yifan Zhang",
          "Zhiquan Wen",
          "Yaofo Chen",
          "Peilin Zhao",
          "Mingkui Tan"
        ],
        "published_date": "2023-02-24T02:03:41Z",
        "pdf_url": "https://arxiv.org/pdf/2302.12400v1.pdf"
      }
    },
    {
      "title": "Robust Test-Time Adaptation in Dynamic Scenarios",
      "abstract": "Test-time adaptation (TTA) intends to adapt the pretrained model to test\ndistributions with only unlabeled test data streams. Most of the previous TTA\nmethods have achieved great success on simple test data streams such as\nindependently sampled data from single or multiple distributions. However,\nthese attempts may fail in dynamic scenarios of real-world applications like\nautonomous driving, where the environments gradually change and the test data\nis sampled correlatively over time. In this work, we explore such practical\ntest data streams to deploy the model on the fly, namely practical test-time\nadaptation (PTTA). To do so, we elaborate a Robust Test-Time Adaptation (RoTTA)\nmethod against the complex data stream in PTTA. More specifically, we present a\nrobust batch normalization scheme to estimate the normalization statistics.\nMeanwhile, a memory bank is utilized to sample category-balanced data with\nconsideration of timeliness and uncertainty. Further, to stabilize the training\nprocedure, we develop a time-aware reweighting strategy with a teacher-student\nmodel. Extensive experiments prove that RoTTA enables continual testtime\nadaptation on the correlatively sampled data streams. Our method is easy to\nimplement, making it a good choice for rapid deployment. The code is publicly\navailable at https://github.com/BIT-DA/RoTTA",
      "full_text": "Robust Test-Time Adaptation in Dynamic Scenarios Longhui Yuan Binhui Xie Shuang Li \f School of Computer Science and Technology, Beijing Institute of Technology {longhuiyuan,binhuixie,shuangli}@bit.edu.cn Abstract Test-time adaptation (TTA) intends to adapt the pre- trained model to test distributions with only unlabeled test data streams. Most of the previous TTA methods have achieved great success on simple test data streams such as independently sampled data from single or multiple distri- butions. However, these attempts may fail in dynamic sce- narios of real-world applications like autonomous driving, where the environments gradually change and the test data is sampled correlatively over time. In this work, we ex- plore such practical test data streams to deploy the model on the fly, namely practical test-time adaptation (PTTA). To do so, we elaborate a Robust Test-Time Adaptation (RoTTA) method against the complex data stream in PTTA. More specifically, we present a robust batch normalization scheme to estimate the normalization statistics. Meanwhile, a memory bank is utilized to sample category-balanced data with consideration of timeliness and uncertainty. Further, to stabilize the training procedure, we develop a time-aware reweighting strategy with a teacher-student model. Exten- sive experiments prove that RoTTA enables continual test- time adaptation on the correlatively sampled data streams. Our method is easy to implement, making it a good choice for rapid deployment. The code is publicly available at https://github.com/BIT-DA/RoTTA 1. Introduction In recent years, many machine learning problems have made considerable headway with the success of deep neu- ral networks [13, 22, 33, 38]. Unfortunately, the perfor- mance of deep models drops significantly when training data and testing data come from different distributions [59], which limits their utility in real-world applications. To re- duce the distribution shift, a handful of works focus on transfer learning field [56], in particular, domain adapta- tion (DA) [17, 42, 45, 48, 69, 72] or domain generalization (DG) [40, 41, 52, 71, 83], in which one or more different but \fCorresponding author Test data stream Continual TTANon-i.i.d.TTAPractical  TTACategoryDistribution Fully TTA Correlation samplingDistributionchanging Figure 1. We consider the practical test-time adaptation (TTA) setup and compare it with related ones. First, Fully TTA [70] adapts models on a fixed test distribution with an independently sampled test stream. Then, on this basis, Continual TTA [73] takes the continually changing distributions into consideration. Next, Non-i.i.d. TTA [19] tries to tackle the correlatively sampled test streams on a single test distribution, where the label distribution among a batch of data deviates from that of the test distribution. To be more practical, Practical TTA strives to connect both worlds: distribution changing and correlation sampling. related labeled datasets (a.k.a. source domain) are collected to help the model generalize well to unlabeled or unseen samples in new datasets (a.k.a. target domain). While both DA and DG have extensively studied the problem of distribution shifts, they typically assume acces- sibility to the raw source data. However, in many practical scenarios like personal consumption records, the raw data should not be publicly available due to data protection reg- ulations. Further, existing methods have to perform heavy backward computation, resulting in unbearable training costs. Test-time adaptation (TTA) [3,11,16,24,26,54,65,81] attempts to address the distribution shift online at test time with only unlabeled test data streams. Unequivocally, TTA has drawn widespread attention in a variety of applications, e.g., 2D/3D visual recognition [2, 29, 49, 65, 82], multi- modality [63, 64] and document understanding [15]. Prior TTA studies [7, 20, 70, 73] mostly concentrate on a simple adaptation scenario, where test samples are inde- pendently sampled from a fixed target domain. To name a few, Sun et al. [65] adapt to online test samples drawn from a constant or smoothly changing distribution with an auxil- iary self-supervised task. Wang et al. [70] adapt to a fixed arXiv:2303.13899v1  [cs.CV]  24 Mar 2023Table 1. Comparison between our proposed practical test-time adaptation (PTTA) and related adaptation settings. Setting Adaptation StageAvailable Data Test Data Stream Train Test Source Target Distribution Sampling Protocol Domain Adaptation ! % ! ! - - Domain Generalization ! % ! % - - Test-Time Training [65] ! ! ! ! stationary independently Fully Test-Time Adaptation [70] % ! % ! stationary independently Continual Test-Time Adaptation [73]% ! % ! continually changing independently Non-i.i.d. Test-Time Adaptation [5, 19]% ! % ! stationary correlatively Practical Test-Time Adaptation (Ours)% ! % ! continually changing correlatively target distribution by performing entropy minimization on- line. However, such an assumption is violated when the test environments change frequently [73]. Later on, Boudiaf et al. [5] and Gonget al. [19] consider the temporal correlation ship within test samples. For example, in autonomous driv- ing, test samples are highly correlated over time as the car will follow more vehicles on the highway or will encounter more pedestrians in the streets. More realistically, the data distribution changes as the surrounding environment alerts in weather, location, or other factors. In a word, distribution change and data correlation occur simultaneously in reality. Confronting continually changing distributions, tradi- tional algorithms like pseudo labeling or entropy minimiza- tion become more unreliable as the error gradients cumu- late. Moreover, the high correlation among test samples re- sults in the erroneous estimation of statistics for batch nor- malization and collapse of the model. Driven by this analy- sis, adapting to such data streams will encounter two major obstacles: 1) incorrect estimation in the batch normaliza- tion statistics leads to erroneous predictions of test samples, consequently resulting in invalid adaptation; 2) the model will easily or quickly overfit to the distribution caused by the correlative sampling. Thus, such dynamic scenarios are pressing for a new TTA paradigm to realize robust adapta- tion. In this work, we launch a more realistic TTA setting, where distribution changing and correlative sampling oc- cur simultaneously at the test phase. We call this Practical Test-Time Adaptation, or briefly,PTTA. To understand more clearly the similarities and differences between PTTA and the previous setups, we visualize them in Figure 1 and sum- marize them in Table 1. To conquer this challenging prob- lem, we propose a Robust Test-Time Adaptation (RoTTA) method, which consists of three parts: 1) robust statistics es- timation, 2) category-balanced sampling considering time- liness and uncertainty and 3) time-aware robust training. More concretely, we first replace the erroneous statistics of the current batch with global ones maintained by the expo- nential moving average. It is a more stable manner to esti- mate the statistics in BatchNorm layers. Then, we simulate a batch of independent-like data in memory with category- balanced sampling while considering the timeliness and un- certainty of the buffered samples. That is, samples that are newer and less uncertain are kept in memory with higher priority. With this batch of category-balanced, timely and confident samples, we can obtain a snapshot of the current distribution. Finally, we introduce a time-aware reweight- ing strategy that considers the timeliness of the samples in the memory bank, with a teacher-student model to perform robust adaptation. With extensive experiments, we demon- strate that RoTTA can robustly adapt in the practical setup, i.e., PTTA. In a nutshell, our contributions can be summarized as: • We propose a new test-time adaptation setup that is more suitable for real-world applications, namely practical test-time adaptation (PTTA). PTTA considers both distribution changing and correlation sampling. • We benchmark the performance of prior methods in PTTA and uncover that they only consider one aspect of the problem, resulting in ineffective adaptation. • We propose a robust test-time adaptation method (RoTTA), which has a more comprehensive considera- tion of PTTA challenges. Ease of implementation and effectiveness make it a practical deployment option. • We extensively demonstrate the practicality of PTTA and the effectiveness of RoTTA on common TTA benchmarks [23], i.e., CIFAR-10-C and CIFAR-100- C and a large-scale DomainNet [58] dataset. RoTTA obtains state-of-the-art results, outperforming the best baseline by a large margin (reducing the averaged classification error by over 5.9%, 5.5% and 2.2% on CIFAR-10-C, CIFAR-100-C and DomainNet, respec- tively). 2. Related Work Domain adaptation (DA) studies the problem of transfer- ring the knowledge learned from a labeled source dataset to an unlabeled target dataset [8, 17, 43, 51, 67, 68]. Represen- tative techniques include latent distribution alignment [48, 77], adversarial training [17, 62], or self-training [75, 85]. The limitation of this setting, however, is that an unlabeled test dataset (target domain) is needed at training time, in addition to a labeled training dataset (source domain). Ac- cordingly, it might fail to handle more practical scenariosFeature 𝐹Robust batch normalization (RBN)Update𝜇௚, 𝜎௚ଶNormalizeFeature𝐹′Update bank with current sample  Training lossℒ௥in Eq. (7) Teacher StudentAdaptation with RBNMemorybankEMA 𝑡A stream of online dataUpdateTest timeCorrelationsamplingStrong & weakaugmentation flowDistributionsCategoryTeacherMajor classhas highest ℋin majorRemoveAddWhen ℋ>ℋSamples to beadded& removed Figure 2. Framework overview. Firstly, we replace the batch normalization layer with RBN which robustly normalizes the feature map. During the inference of the online test stream of PTTA, we utilize the predictions of samples to maintain a memory bank by category- balanced sampling with timeliness and uncertainty. Finally, we use the category-balanced, timely and confident data in the memory bank combined with a robust loss to adapt the model at test time. like test-time adaptation. Our practical test-time adaptation setting can be viewed as performing correlatively sample adaptation on the fly. It is worth noting that standard domain adaptation techniques might collapse when only continual data streams from multiple target domains are accessible. Domain generalization (DG) assumes that multiple source domains are available for model training and tries to learn models that can generalize well to any unseen domains [4, 26,40,41,52,84]. A broad spectrum of methodologies based on data augmentation [78, 84], meta-learning [14, 40], or domain alignment [50,52] has made great progress. In con- trast, this work instead aims to improve the performance of source pre-trained models at the test time by using unla- beled online data streams from multiple continually chang- ing target domains. Continual learning (CL) (also known as incremental learning, life-long learning) addresses the problem of learn- ing a model for many tasks sequentially without forgetting knowledge obtained from the preceding tasks. [1, 6, 31, 37, 60]. CL methods can often be categorized into replay- based [60, 66] and regularization-based [31, 44] methods. Ideas from continual learning are also adopted for continu- ous domain adaptation approaches [34, 74] In our work, we share the same motivation as CL and point out that prac- tical test-time adaptation (PTTA) also suffers catastrophic forgetting (i.e., performance degradation on new test sam- ples due to correlation sampling), which makes test-time adaptation approaches are unstable to deploy. Test-time adaptation (TTA) focus on more challenging settings where only source model and unlabeled target data are available [9, 18, 27, 28, 35, 46, 61]. A similar paradigm is source-free domain adaptation (SFDA) [10, 36, 47, 79], which also requires no access to the training (source) data. To name a few, Liang et al . [45] fit the source hypoth- esis by exploiting the information maximization and self- supervised pseudo-labeling. Kundu et al. [35] formalize a unified solution that explores SFDA without any category- gap knowledge. To fully utilize any arbitrary pre-trained model, Sun et al. [65] propose conducting adaptation on the fly with an auxiliary self-supervised task. Later on, Wanget al. [70] take a source pre-trained model and adapt it to the test data by updating a few trainable parameters in Batch- Norm layers [25] using entropy minimization [21]. While standard TTA has been widely studied in many tasks [2, 20, 63, 64, 70, 82], the fact remains that both dis- tribution changing [73] and data correlation sampling [19] has only been considered in isolation. For example, Gong et al. [19] propose instance-aware batch normalization and prediction-balanced reservoir sampling to address the chal- lenges of correlatively sampled test streams, however, it does not consider unstable adaptation resulting from long- term adaptation on continually changing distributions. On the other hand, Wang et al. [73] assume that the target test data is streamed from a continually changing environment and continually adapt an off-the-shelf source pre-trained model to the current test data. In this work, we launch PTTA, a more practical TTA setting to connect both worlds: distribution changing and correlation sampling. 3. Method 3.1. Problem Definition and Motivation Given a model fθ0 with parameter θ0 pre-trained on source domain DS = {(xS, yS)}, the proposed practical test-time adaptation (PTTA) aims to adapt fθ0 to a stream of online unlabeled samples X0, X1, ...,XT , where Xt is a batch of highly correlated samples from the distribution Ptest that changes with time t continually. More specifi- cally, at test time, with time going on, the test distribution Ptest changes continually as P0, P1, ...,P∞. At time step t, we will receive a batch of unlabeled and correlated samplesmotion distribution changing snow time  Distributions and Labels of PTTA T est Stream uniform 10 1 0.1 0.01 0.001 Dirichlet Parameter  Figure 3. Illustration of the labels and distributions of the test stream of CIFAR10-C under the setup PTTA. And we adopt Dirichlet distribution to simulate the process of correlative sam- pling. It is clear that as the concentration parameter δ decreases, the correlation among sampled data increases, which is reflected in the increasing aggregation of categories. Xt from Ptest. Next, Xt is fed into the model fθt and the model needs to adapt itself to the current test data streams and make predictions fθt (Xt) on the fly. As a matter of fact, this setup is largely driven the prac- tical demands of deploying models in dynamic scenarios. Taking for example the case of autonomous driving men- tioned in § 1, test samples are highly correlated and the data distribution changes continually with the weather or loca- tion. Another example is the situation of intelligent moni- toring, the camera will continuously capture more people at certain times, such as after work, but fewer of them during work time. Meanwhile, the light condition changes con- tinually from day to night. The deployed model should be robustly adapted in such dynamic scenarios. In a word, dis- tribution change and data correlation often happen simul- taneously in the real world. For this reason, existing TTA methods [7,9,19,28,70,73,81] might become unstable when the test stream is sampled from such dynamic scenarios. To obtain the test stream of PTTA, we adopt Dirich- let Distribution with parameter δ to simulate the correla- tion among test samples. We present the test data streams corresponding to different values of δ on the CIFAR10-C dataset in Figure 3. We can observe that the smaller δ is, the higher the correlation will be. For the sake of unity, we set δ = 0.1 as the default for all experiments. In the follow- ing, we present a robust test-time adaptation framework for the practical test-time adaptation setup defined above. An overview of our RoTTA is illustrated in Figure 2. 3.2. Robust Test-Time Adaptation Motivated by the fact that the statistics of current batch data, which are commonly used in previous TTA meth- ods [7, 20, 65, 70, 73], become unreliable when they en- counter correlative test data streams, we first turn to the global robust statistics for normalization. Then, to effec- tively adapt to the current distribution, we maintain a mem- ory bank by category-balanced sampling with considering timeliness and uncertainty, which captures a more stable snapshot of the distribution. Finally, we utilize the teacher- student model and design a timeliness-based reweighting strategy to train the model robustly. Robust batch normalization (RBN). Batch Normaliza- tion (BN) [25] is a widely-used training technique as it can accelerate the training and convergence speed of networks and stabilize the training process by reducing the risk of gradient explosion and vanishing. Given the feature map F ∈ RB×C×H×W as the input for a BN layer when train- ing, the channel-wise mean µ ∈ RC and variance σ2 ∈ RC are calculated as follows: µc = 1 BHW BX b=1 HX h=1 WX w=1 F(b,c,h,w) , (1) σ2 c = 1 BHW BX b=1 HX h=1 WX w=1 (F(b,c,h,w) − µc)2 . (2) Then the feature map is normalized and refined in a channel-wise manner as BN (F(b,c,h,w); µ, σ2) =γc F(b,c,h,w) − µc √σ2c + ϵ + βc , (3) where γ, β∈ RC are learnable parameters in the layer and ϵ > 0 is a constant for numerical stability. Meanwhile, during training, the BN layer maintains a group of global running mean and running variance (µs, σ2 s) for inference. Due to the domain shift at test time, the global statis- tics (µs, σ2 s) normalize test features inaccurately, causing significant performance degradation. To tackle the prob- lem above, some methods [55, 70, 73] use the statistics of the current batch to perform normalization. Unfortunately, when the test samples have a high correlation under PTTA setup, the statistics of the current batch also fail to correctly normalize the feature map, as demonstrated in Figure 4c. Specifically, the performance of BN [53] decreases rapidly as the data correlation increases. Based on the analysis above, we propose a robust batch normalization (RBN) module, which maintains a group of global statistics (µg, σ2 g) to normalize the feature map ro- bustly. Before the whole test-time adaptation, (µg, σ2 g) is initialized as the running mean and variance (µs, σ2 s) of the pre-trained model. When adapting the model, we update the global statistics first by exponential moving average as µg = (1− α)µg + αµ , (4) σ2 g = (1− α)σ2 g + ασ2 , (5) where (µ, σ2) is the statistics of the buffered samples in the memory bank. Then we normalize and affine the feature as Eq. (3) with (µg, σ2 g). When inferring for test samples, we directly utilize (µg, σ2 g) to calculate the output as Eq (3). Al- though simple, RBN is effective enough to tackle the prob- lem of normalization on test streams of PTTA.Category-balanced sampling with timeliness and uncer- tainty (CSTU). In the PTTA setup, the correlation among test samples Xt at time t leads to a deviation between the observed distribution bPtest and the test distribution Ptest. Specifically, the marginal label distribution p(y|t) tends to differ from p(y). Continuously learning with Xt over time t can lead to model adaptation to an unreliable distribution bPtest, resulting in ineffective adaptation and an increased risk of model collapse. To address this issue, we propose a category-balanced memory bank M with a capacity of N, which takes into account the timeliness and uncertainty of samples when up- dating. In particular, we adopt the predictions of test sam- ples as pseudo labels to guide the update ofM. Meanwhile, to guarantee the balance among categories, we distribute the capacity of M equally to each category, and samples of the major categories will be replaced first (refer to lines 5-9 in Algorithm 1). Furthermore, due to the continually changing test distribution, old samples in M are limited in value, and could even impair the ability of the model to adapt to the current distribution. Additionally, samples of high uncer- tainty always produce erroneous gradient information that can hinder model adaptation, as suggested by [55]. With this in mind, we attach each sample in M with a group of heuristics (A, U), where A, initialized as 0 and in- creasing with time t, is the age of the sample, and U the un- certainty calculated as the entropy of the prediction. Next, we combine the timeliness and uncertainty to calculate a heuristic score, i.e., category-balanced sampling with time- liness and uncertainty (CSTU), as follows: H = λt 1 1 + exp(−A/N) + λu U log C , (6) where λt and λu make the trade-off between timeliness and uncertainty, and for simplicity, λt and λu are set to 1.0 for all experiments, andC is the number of categories. We sum- marize our sampling algorithm in Algorithm 1. With CSTU, we can obtain a robust snapshot of the current test distribu- tion Ptest, and effectively adapt the model to it. Robust training with timeliness. Actually, after replacing BN layers with our RBN and obtaining the memory bank selected via CSTU, we can directly adopt the widely used techniques like pseudo labeling or entropy minimization to perform test-time adaptation. However, we notice that too old or unreliable instances still have the opportunity to stay in M since keeping the category balance is assigned the top priority. In addition, too aggressive updates of the model will make the category balance ofM unreliable, resulting in unstable adaptation. Meanwhile, error accumulation caused by the distribution change also makes the aforementioned approaches unworkable. To further reduce the risk of error gradients information from old and unreliable instances and stabilize the adapta- tion, we turn to the robust unsupervised learning method Algorithm 1: CSTU for one test sample. 1 Input: a test sample x and the teacher model fθT . 2 Define: memory bank M and its capacity N, number of classes C, per class occupation O ∈RC, total occupation Ω, classes to pop instance D. 3 Infer as p(y|x) =Softmax(fθT (x)). 4 Calculate the predicted category of x as ˆy = arg maxc p(c|x), the uncertainty as Ux = −PC c=1 p(c|x) log(p(c|x)), the age as Ax = 0, and the heuristic score Hx of x with Eq (6) 5 if Oˆy < N C then 6 if Ω <N: Search range D = ∅. 7 else: Search range D = {j|j = arg maxc Oc} 8 else 9 Search range D = {ˆy} 10 if D is ∅ then 11 Add (x, ˆy, Hx, Ux) into M. 12 else 13 Find the instance (ˆx, yˆx, Aˆx, Uˆx) with the highest value in Eq (6) Hˆx among D. 14 if Hx < Hˆx then 15 Remove (ˆx, yˆx, Aˆx, Uˆx) from M. 16 Add (x, ˆy, Hx, Ux) into M. 17 else 18 Discard x. 19 Increase the age of all instances in M. teacher-student model and propose a timeliness reweight- ing strategy. In addition, for the sake of time efficiency and stability, only affine parameters in RBN are trained during adaptation. At time step t, after inferring for the correlated data Xt with the teacher model fθT t and updating the memory bank M with Xt, we begin updating the student model fθS t and the teacher model fθT t . Firstly, we update parameters of stu- dent model θS t → θS t+1 by minimizing the following loss: Lr = 1 Ω ΩX i=1 L(xM i , Ai; θT t , θS t ) , (7) where Ω = |M| is the total occupation of the memory bank, and xM i and Ai(i = 1, ..., Ω) are instances in the memory bank and their age respectively. Subsequently, the teacher model is updated by exponential moving average as θT t+1 = (1− ν)θT t + νθS t+1 . (8) To calculate the loss value of an instancexM i from the mem- ory bank, the timeliness reweighting term is computed as E(Ai) = exp(−Ai/N) 1 + exp(−Ai/N) , (9)where Ai is the age of xM i , and N is the capacity of the bank. And then we calculate the cross entropy between the soft-max prediction pS(y|x′′ i ) of the strong-augmented view x′′ i from the student model and that pT (y|x′ i) of the weak- augmented view 1 x′ i from the teacher model as follows: ℓ(x′ i, x′′ i ) =−1 C CX c=1 pT (c|x′ i) logpS(c|x′′ i ) . (10) Finally, equipped with Eq. (9) and Eq. (10), the right-hand side of Eq. (7) reduces to L(xM i , Ai; θT t , θS t ) =E(Ai)ℓ(x′ i, x′′ i ) . (11) To sum up, equipped with RBN, CSTU, and robust training with timeliness, our RoTTA is capable of effectively adapt- ing any pre-trained models in dynamic scenarios. 4. Experiments 4.1. Setup Datasets. CIFAR10-C and CIFAR100-C [23] are the com- monly used TTA benchmarks to testify the robustness un- der corruptions. Both of them are obtained by applying 15 kinds of corruption with 5 different degrees of severity on their clean test images of original datasets CIFAR10 and CIFAR100 respectively. CIFAR10/CIFAR100 [32] have 50,000/10,000 training/test images, all of which fall into 10/100 categories. DomainNet [58] is the largest and hard- est dataset to date for domain adaptation and consists of about 0.6 million images with 345 classes. It consists of six different domains including Clipart (clp), Infograph (inf), Painting (pnt), Quickdraw (qdr), Real (rel), and Sketch (skt). We first pre-train a source model on the train set in one of six domains and testify all baseline methods on the test set of the remaining five domains. Implementation details. All experiments are conducted with PyTorch [57] framework. In the case of robustness to corruption, following the previous methods [55, 70, 73], we obtain the pre-trained model from RobustBench bench- mark [12], including the WildResNet-28 [80] for CIFAR10 → CIFAR10-C, and the ResNeXt-29 [76] for CIFAR100 → CIFAR100-C. Then, we change the test corruption at the highest severity 5 one by one to simulate that the test distri- bution continually changes with time in PTTA. And in the case of generalization under the huge domain gap, we train a ResNet-101 [22] by standard classification loss for each domain in DomainNet and adapt them continually to differ- ent domains except the source domain. Meanwhile, we uti- lize the Dirichlet distribution to simulate the correlatively sampled test stream for all datasets. For optimization, we adopt Adam [30] optimizer with learning rate 1.0 × 10−3, 1Weak augmentation is ReSize+CenterCrop. Strong augmentation is a combination nine operations like Clip, ColorJitter, and RandomAffine. β = 0.9. For a fair comparison, we set the batch size for all methods as 64 and the capacity of the memory bank of RoTTA as N = 64. Concerning the hyperparameters, we adopt a unified set of values for RoTTA across all experi- ments including α = 0.05, ν = 0.001, λt = 1.0, λu = 1.0, and δ = 0.1. More details are provided in the appendix. 4.2. Comparisons with the State-of-the-arts Robustness under corruptions. The classification error on CIFAR10→CIFAR10-C and CIFAR100→CIFAR100-C are shown in Table 2 and Table 3 respectively. We change the type of the current corruption at the highest severity 5 as time goes on, and sample data correlatively for infer- ence and adaptation simultaneously. The same test stream is shared across all compared methods. From Table 2 and Table 3, we can see that RoTTA achieves the best performance compared to previous meth- ods. Moreover, RoTTA has a significant performance gain to the second-best method that 5.9% improvement on CIFAR10 →CIFAR10-C and 5.5% improvement on CIFAR100→CIFAR100-C respectively, verifying the effec- tiveness of RoTTA to adapt the model under PTTA. In more detail, we can observe that BN [53], PL [39], TENT [70] and CoTTA [73] negatively adapt the model to the test streams of both datasets compared to Source (−6.5 ∼ −46.4%). This is attributed to the fact that these methods overlook the issues posed by correlation sampling, which can result in highly correlated data within a batch. As a consequence, traditional normalization statistics may be ineffective in appropriately normalizing the feature maps. Equipped with RBN and CSTU, RoTTA no longer suffers from this issue. Meanwhile, in Table 3, if focus on the adaptation procedure, we can see that the performance of PL [39], TENT [70] and NOTE [19] becomes worse and worse, and eventually, the model even collapses (error rate > 97%). This reveals that the impact of error accumula- tion on long-term adaptation can be catastrophic. To tackle this problem, RoTTA turns to robustly adapt the model with timeliness reweighting and confident samples in the mem- ory bank, and superior performance throughout the adapta- tion process demonstrates its effectiveness. In addition, we find that although LAME [5] never tunes the parameters of the model, it is still a competi- tive baseline for example it achieves the second-best result on CIFAR100→CIFAR100-C. However, its performance is very dependent on the performance of the pre-trained model e.g. negligible improvement on difficult corruptions (shot, gaussian, pixelate). On the contrary, our RoTTA is more flexible and achieves better and more robust results. Generalization under domain shift. We also evalu- ate RoTTA under a more challenging dataset DomainNet, where we continually adapt a source pre-trained model to correlatively sampled test streams of the rest domains. AsTable 2. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 34.8 25.1 26.0 65.7 46.9 46.7 42.0 9.3 41.3 26.6 54.3 72.3 58.5 30.3 72.9 43.5BN [53] 73.2 73.4 72.7 77.2 73.7 72.5 72.9 71.0 74.1 77.7 80.0 76.9 75.5 78.3 79.0 75.2PL [39] 73.9 75.0 75.6 81.0 79.9 80.6 82.0 83.2 85.3 87.3 88.3 87.5 87.5 87.5 88.2 82.9TENT [70] 74.3 77.4 80.1 86.2 86.7 87.3 87.9 87.4 88.2 89.0 89.2 89.0 88.3 89.7 89.2 86.0LAME [5] 29.5 19.0 20.3 65.3 42.4 43.4 36.8 5.4 37.2 18.6 51.2 73.2 57.0 22.6 71.3 39.5CoTTA [73]77.1 80.6 83.1 84.4 83.9 84.2 83.1 82.6 84.4 84.2 84.5 84.6 82.7 83.8 84.9 83.2NOTE [19] 18.0 22.1 20.6 35.6 26.9 13.6 26.5 17.3 27.2 37.0 48.3 38.8 42.6 41.9 49.7 31.1 RoTTA 18.1 21.3 18.8 33.6 23.6 16.5 15.1 11.2 21.9 30.7 39.6 26.8 33.7 27.8 39.5 25.2(+5.9) Table 3. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 30.8 39.5 50.3 68.0 29.3 55.1 28.8 29.5 45.8 37.2 54.1 73.0 74.7 41.2 39.4 46.4BN [53] 48.5 54.0 58.9 56.2 46.4 48.0 47.0 45.4 52.9 53.4 57.1 58.2 51.7 57.1 58.8 52.9PL [39] 50.6 62.1 73.9 87.8 90.8 96.0 94.8 96.4 97.4 97.2 97.4 97.4 97.3 97.4 97.4 88.9TENT [70] 53.3 77.6 93.0 96.5 96.7 97.5 97.1 97.5 97.3 97.2 97.1 97.7 97.6 98.0 98.3 92.8LAME [5] 22.4 30.4 43.9 66.3 21.3 51.7 20.6 21.8 39.6 28.0 48.7 72.8 74.6 33.1 32.3 40.5CoTTA [73]49.2 52.7 56.8 53.0 48.7 51.7 49.4 48.7 52.5 52.2 54.3 54.9 49.6 53.4 56.2 52.2NOTE [19] 45.7 53.0 58.2 65.6 54.2 52.0 59.8 63.5 74.8 91.8 98.1 98.3 96.8 97.0 98.2 73.8 RoTTA 31.8 36.7 40.9 42.1 30.0 33.6 27.9 25.4 32.3 34.0 38.8 38.7 31.3 38.0 42.9 35.0(+5.5) Table 4. Average classification error of DomainNet while continually adapting to different domains with correlatively sampled test stream. Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Sourceclp inf pnt qdr rel sktAvg. BN clp inf pnt qdr rel sktAvg. PL clp inf pnt qdr rel sktAvg.TENTclp inf pnt qdr rel sktAvg. clp N/A 83.9 65.4 88.6 48.0 59.1 69.0clp N/A 88.6 70.7 90.5 65.4 67.0 76.5clp N/A 94.5 98.9 99.5 99.7 99.7 98.5clp N/A 87.5 71.9 94.2 96.2 98.9 89.7inf 61.8 N/A 66.9 96.0 50.0 70.6 69.1inf 68.6 N/A 74.2 96.2 69.9 76.8 77.1inf 82.6 N/A 99.2 99.6 99.7 99.3 96.1inf 68.6 N/A 75.0 97.3 95.9 98.7 87.1pnt 56.5 83.7 N/A 94.2 42.6 63.4 68.1pnt 60.8 87.9 N/A 94.3 62.3 68.7 74.8pnt 78.6 99.4 N/A 99.7 99.6 99.7 95.4pnt 61.7 87.1 N/A 96.4 95.3 98.8 87.8qdr 89.2 99.0 98.6 N/A 95.0 92.3 94.8qdr 80.3 97.7 92.6 N/A 88.7 88.1 89.5qdr 81.7 99.5 99.6 N/A 99.7 99.8 96.1qdr 78.9 97.1 91.6 N/A 89.2 88.7 89.1rel 49.4 80.4 51.5 93.4 N/A 63.3 67.6rel 57.9 87.1 63.1 94.3 N/A 70.8 74.6rel 73.5 99.4 99.2 99.6 N/A 99.7 94.3rel 57.8 86.4 68.1 96.9 N/A 96.7 81.2skt 47.5 88.2 62.9 87.1 51.8 N/A 67.5skt 50.4 87.6 64.6 89.6 63.1 N/A 71.1skt 64.8 99.2 99.4 99.7 99.7 N/A 92.6skt 51.9 87.2 69.1 95.3 97.3 N/A 80.1Avg.60.9 87.0 69.1 91.9 57.5 69.7 72.7Avg.63.6 89.8 73.0 93.0 69.9 74.3 77.3Avg.76.2 98.4 99.3 99.6 99.7 99.6 95.5Avg.63.8 89.0 75.1 96.0 94.8 96.4 85.8 Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →LAMEclp inf pnt qdr rel sktAvg.COTTAclp inf pnt qdr rel sktAvg.NOTEclp inf pnt qdr rel sktAvg.RoTTAclp inf pnt qdr rel sktAvg. clp N/A 82.2 64.5 87.7 46.9 58.9 68.0clp N/A 90.6 77.9 89.3 76.3 72.7 81.4clp N/A 89.2 73.0 94.8 98.4 99.4 91.0clp N/A 85.5 62.0 82.0 49.3 59.8 67.7inf 60.1 N/A 65.7 95.4 48.5 69.4 67.8inf 74.5 N/A 82.0 95.7 80.2 81.5 82.8inf 75.4 N/A 78.7 98.7 98.1 99.5 90.1inf 61.8 N/A 63.7 91.5 52.5 67.6 67.4pnt 55.8 81.5 N/A 93.3 41.3 62.1 66.8pnt 66.3 89.8 N/A 93.4 74.0 75.4 79.8pnt 64.7 89.8 N/A 97.8 98.4 99.2 90.0pnt 53.3 84.1 N/A 89.1 47.3 61.4 67.0qdr 88.3 99.1 99.0 N/A 94.9 92.2 94.7qdr 82.3 98.2 94.6 N/A 92.5 90.1 91.5qdr 74.7 97.2 92.2 N/A 93.5 99.6 91.4qdr 77.5 97.0 89.8 N/A 80.3 82.2 85.3rel 48.0 79.3 50.1 91.6 N/A 60.2 65.8rel 64.0 90.3 73.2 93.5 N/A 77.6 79.7rel 61.3 89.2 68.9 98.8 N/A 99.2 83.5rel 49.1 82.3 50.3 88.0 N/A 61.1 66.2skt 45.6 87.1 59.5 83.9 49.9 N/A 65.2skt 56.1 89.2 71.9 89.2 73.5 N/A 76.0skt 55.2 89.7 70.1 96.9 98.3 N/A 82.0skt 42.6 83.7 54.4 80.9 47.5 N/A 61.8Avg.59.6 85.8 67.8 90.4 56.3 68.6 71.4Avg.68.6 91.6 79.9 92.2 79.3 79.5 81.9Avg.66.3 91.0 76.6 97.4 97.3 99.4 88.0Avg.56.8 86.5 64.0 86.3 55.4 66.469.2(+2.2) shown in Table 4, consistent with the previous analysis, most of the methods include BN [53], PL [39], TENT [70], CoTTA [73] and NOTE [19] even perform worse than the Source model ( −4.6 ∼ −22.8%). RoTTA consistently achieves the best performance and has 2.2% gain than the second method LAME [5], demonstrating RoTTA’s effec- tiveness again. 4.3. Ablation Study Effect of each component. To further investigate the effi- cacy of each component, we replace each part with the nor- mally used solutions to obtain three variants: (1) RoTTA w/o RBN, replace RBN with test-time BN in TENT [70]; (2) RoTTA w/o CSTU, directly adapt the model on test stream; (3) RoTTA w/o robust training (RT), directly adapt the model only with entropy minimization. As shown in Table 5, we can observe that significant performance degra- dation occurs for all variants, proving that every part of our proposed method is valid for PTTA. Take one com- ponent for a detailed example, without RBN robustly nor- malizing feature maps, the performance of RoTTA drops 50.2% and 16.3% on CIFAR10-C and CIFAR100-C respec- tively, proving that RBN is robust enough to tackle the prob- lem of normalization of correlatively sampled data streams. CSTU enables RoTTA to adapt to a more stable distribu- tion by maintaining a timely and confident snapshot of the test distribution. Meanwhile, robust training with timeliness greatly reduces the accumulation of errors. Every compo- nent behaves significantly to enable effective adaptation un- der PTTA. Effect of the distribution changing order. To exclude the effect of a fixed order of distribution changing, we con- ducted experiments on ten different sequences of changes on CIFAR10-C and CIFAR100-C with independently andBN PL TENT LAME CoTTA NOTE RoTTA0 10 20 30 40 50 60 70 80Classification error (%) Source CIFAR-10  CIFAR-10-C Independent Correlative (a) CIFAR10-C. BN PL TENT LAME CoTTA NOTE RoTTA0 20 40 60 80Classification error (%) Source CIFAR-100  CIFAR-100-C Independent Correlative (b) CIFAR100-C. uniform 10 1 0.1 0.01 0.001 30 40 50 60 70 80 90 100Classification error (%) Source BN PL TENT LAME CoTTA NOTE RoTTA (c) δ. 16 32 64 128 256 512 40 50 60 70 80 90 100Classification error (%) Source BN PL TENT LAME CoTTA NOTE RoTTA (d) Batch size. Figure 4. (a) & (b) we adapt the model continually to different corruptions of 10 different orders with independently and correlatively sampled test streams on CIFAR10-C and CFAR100-C respectively and report their average classification error. (c) & (d) we verify the effect of δ and batch size to different methods on CIFAR100-C respectively. Table 5. Classification error of different variants of our RoTTA. Variant CIFAR10-C CIFAR100-C Avg. RoTTA w/o RBN 75.4 51.3 63.4 RoTTA w/o CSTU 47.1 46.3 46.7 RoTTA w/o RT 78.2 95.0 81.6 RoTTA 25.2 35.0 30.1 correlatively sampled test streams respectively. As shown in Figure 4a and 4b, no matter what kind of setup, RoTTA can achieve excellent results. The detailed results on the correlatively sampled test streams are shown in Table 6, RoTTA achieves 4.3% and 4.7% progress on CIFAR10- C and CIFAR100-C respectively. This shows that RoTTA can adapt the model robustly and effectively in long-term scenarios where distribution continually changes and test streams are sampled either independently or correlatively, making it a good choice for model deployment. Effect of Dirichlet concentration parameter δ. We vary the value of δ on CIFAR100-C and compare RoTTA with other approaches in Figure 4c. As the value of δ increases, the performance of BN [53], PL [39], TENT [70] and CoTTA [73] drops quickly, because they never consider the increasing correlation among test samples. NOTE [19] is stable to correlatively sampled test streams but does not consider the distribution changing, causing ineffective adaptation. Meanwhile, the higher correlation between test samples will make the propagation of labels more accurate, which is why the result of LAME [5] slightly improves. Fi- nally, excellent and stable results once again prove the sta- bility and effectiveness of RoTTA. Effect of batch size. In real scenarios, considering deploy- ment environments may use different test batch sizes, we conduct experiments with different values of test batch sizes and results are shown in Figure 4d. For a fair comparison, we control the frequency of updating the model of RoTTA so that the number of samples involved in back-propagation is the same. As the batch size increases, we can see that all of the compared methods have a significant improvement except for lame which has a slight decrease. This is be- cause the number of categories in a batch increases with the Table 6. Average classification error of tasks CIFAR10 → CIFAR10-C and CIFAR100 → CIFAR100-C while continually adapting to different corruptions of 10 different orders at the high- est severity 5 with correlatively sampled test stream. Method CIFAR10-C CIFAR100-C Avg. Source 43.5 46.4 46.9 BN [53] 75.2 52.9 64.1 PL [39] 75.2 52.9 60.1 TENT [70] 82.3 93.2 87.8 LAME [5] 39.5 40.6 40.1 NOTE [19] 30.5 76.1 53.3 CoTTA [73] 83.1 52.8 67.9 RoTTA 26.2(+4.3) 35.9(+4.7) 31.1(+9.0) increasing batch size, causing the overall correlation to be- come lower but the propagation of labels to become more difficult. Most significantly, RoTTA achieves the best re- sults across different batch sizes, demonstrating its robust- ness in dynamic scenarios once again. 5. Conclusion This work proposes a more realistic TTA setting where distribution changing and correlative sampling occur si- multaneously at the test phase, namely Practical Test-Time Adaptation (PTTA). To tackle the problems of PTTA, we propose Robust Test-Time Adaptation (RoTTA) method against the complex data stream. More specifically, a group of robust statistics for the normalization of feature maps is estimated by robust batch normalization. Meanwhile, a memory bank is adopted to capture a snapshot of the test distribution by category-balanced sampling with consider- ing timeliness and uncertainty. Further, we develop a time- aware reweighting strategy with a teacher-student model to stabilize the adaptation process. Extensive experiments and ablation studies are conducted to verify the robustness and effectiveness of the proposed method. We believe this work will pave the way for thinking about adapting models into real-world applications by test-time adaptation algorithm. Acknowledgements. This paper was supported by National Key R&D Program of China (No. 2021YFB3301503), and also supported by the National Natural Science Foundation of China under Grant No. 61902028.References [1] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Ben- gio. Gradient based sample selection for online continual learning. In NeurIPS, pages 11816–11825, 2019. 3 [2] Fatemeh Azimi, Sebastian Palacio, Federico Raue, J ¨orn Hees, Luca Bertinetto, and Andreas Dengel. Self-supervised test-time adaptation on video data. In WACV, pages 2603– 2612, 2022. 1, 3 [3] Mathilde Bateson, Herve Lombaert, and Ismail Ben Ayed. Test-time adaptation with shape moments for image segmen- tation. In MICCAI, pages 736–745, 2022. 1 [4] Gilles Blanchard, Gyemin Lee, and Clayton Scott. General- izing from several related classification tasks to a new unla- beled sample. In NeurIPS, pages 2178–2186, 2011. 3 [5] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In CVPR, pages 8344–8353, 2022. 2, 6, 7, 8, 13, 14, 15, 16, 17 [6] Francisco M Castro, Manuel J Mar ´ın-Jim´enez, Nicol´as Guil, Cordelia Schmid, and Karteek Alahari. End-to-end incre- mental learning. In ECCV, pages 233–248, 2018. 3 [7] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In CVPR, pages 295–305, 2022. 1, 4 [8] Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Domain adaptive faster r-cnn for object de- tection in the wild. In CVPR, pages 3339–3348, 2018. 2 [9] Zhixiang Chi, Yang Wang, Yuanhao Yu, and Jin Tang. Test- time fast adaptation for dynamic scene deblurring via meta- auxiliary learning. In CVPR, pages 9137–9146, 2021. 3, 4 [10] Boris Chidlovskii, St ´ephane Clinchant, and Gabriela Csurka. Domain adaptation in the absence of source domain data. In KDD, pages 451–460, 2016. 3 [11] Sungha Choi, Seunghan Yang, Seokeon Choi, and Sun- grack Yun. Improving test-time adaptation via shift-agnostic weight regularization and nearest source prototypes. In ECCV, pages 440–458, 2022. 1 [12] Francesco Croce, Maksym Andriushchenko, Vikash Se- hwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness benchmark. In Neurips, 2021. 6 [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 1 [14] Ying-Jun Du, Jun Xu, Huan Xiong, Qiang Qiu, Xiantong Zhen, Cees G. M. Snoek, and Ling Shao. Learning to learn with variational information bottleneck for domain general- ization. In ECCV, pages 200–216, 2020. 3 [15] Sayna Ebrahimi, Sercan ¨O. Arik, and Tomas Pfister. Test- time adaptation for visual document understanding. CoRR, abs/2206.07240, 2022. 1 [16] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei A Efros. Test-time training with masked autoencoders. In NeurIPS, 2022. 1 [17] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pas- cal Germain, Hugo Larochelle, Franc ¸ois Laviolette, Mario Marchand, and Victor S. Lempitsky. Domain-adversarial training of neural networks. J. Mach. Learn. Res., 17:59:1– 59:35, 2016. 1, 2 [18] Yunhe Gao, Xingjian Shi, Yi Zhu, Hao Wang, Zhiqiang Tang, Xiong Zhou, Mu Li, and Dimitris N. Metaxas. Vi- sual prompt tuning for test-time domain adaptation. CoRR, abs/2210.04831, 2022. 3 [19] Taesik Gong, Jongheon Jeong, Taewon Kim, Yewon Kim, Jinwoo Shin, and Sung-Ju Lee. Robust continual test- time adaptation: Instance-aware BN and prediction-balanced memory. In NeurIPS, 2022. 1, 2, 3, 4, 6, 7, 8, 13, 14, 15, 16, 17 [20] Sachin Goyal, Mingjie Sun, Aditi Raghunathan, and J Zico Kolter. Test time adaptation via conjugate pseudo-labels. In NeurIPS, 2022. 1, 3, 4 [21] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In NeurIPS, pages 529– 536, 2004. 3 [22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770–778, 2016. 1, 6 [23] Dan Hendrycks and Thomas G. Dietterich. Benchmarking neural network robustness to common corruptions and per- turbations. In ICLR, 2019. 2, 6 [24] Hengguan Huang, Xiangming Gu, Hao Wang, Chang Xiao, Hongfu Liu, and Ye Wang. Extrapolative continuous-time bayesian neural network for fast training-free test-time adap- tation. In NeurIPS, 2022. 1 [25] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal co- variate shift. In ICML, pages 448–456, 2015. 3, 4 [26] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier ad- justment module for model-agnostic domain generalization. In NeurIPS, pages 2427–2440, 2021. 1, 3 [27] Vidit Jain and Erik Learned-Miller. Online domain adapta- tion of a pre-trained cascade of classifiers. In CVPR, pages 577–584, 2011. 3 [28] Minguk Jang and Sae-Young Chung. Test-time adaptation via self-training with nearest neighbor information. CoRR, abs/2207.10792, 2022. 3, 4 [29] Junho Kim, Inwoo Hwang, and Young Min Kim. Ev-tta: Test-time adaptation for event-based object recognition. In CVPR, pages 17724–17733, 2022. 1 [30] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 6 [31] James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska- Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Ku- maran, and Raia Hadsell. Overcoming catastrophic forget- ting in neural networks. CoRR, abs/1612.00796, 2016. 3 [32] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 6[33] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural net- works. In NeurIPS, pages 1097–1105, 2012. 1 [34] Ananya Kumar, Tengyu Ma, and Percy Liang. Understand- ing self-training for gradual domain adaptation. In ICML, pages 5468–5479, 2020. 3 [35] Jogendra Nath Kundu, Naveen Venkat, Rahul M. V ., and R. Venkatesh Babu. Universal source-free domain adapta- tion. In CVPR, pages 4543–4552, 2020. 3 [36] Vinod K Kurmi, Venkatesh K Subramanian, and Vinay P Namboodiri. Domain impression: A source data free do- main adaptation method. In WACV, pages 615–625, 2021. 3 [37] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory G. Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying for- getting in classification tasks. IEEE Trans. Pattern Anal. Mach. Intell., 44(7):3366–3385, 2022. 3 [38] Yann LeCun, Yoshua Bengio, and Geoffrey E. Hinton. Deep learning. Nat., 521(7553):436–444, 2015. 1 [39] Dong-Hyun Lee et al. Pseudo-label: The simple and effi- cient semi-supervised learning method for deep neural net- works. In Workshop on challenges in representation learn- ing, ICML, volume 3, page 896, 2013. 6, 7, 8, 12, 14, 15, 16, 17 [40] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Learning to generalize: Meta-learning for do- main generalization. In AAAI, pages 3490–3497, 2018. 1, 3 [41] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C. Kot. Domain generalization with adversarial feature learning. In CVPR, pages 5400–5409, 2018. 1, 3 [42] Shuang Li, Binhui Xie, Qiuxia Lin, Chi Harold Liu, Gao Huang, and Guoren Wang. Generalized domain conditioned adaptation network. IEEE Trans. Pattern Anal. Mach. Intell., 44(8):4093–4109, 2022. 1 [43] Shuang Li, Mixue Xie, Kaixiong Gong, Chi Harold Liu, Yulin Wang, and Wei Li. Transferable semantic augmen- tation for domain adaptation. In CVPR, pages 11516–11525, 2021. 2 [44] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Trans. Pattern Anal. Mach. Intell., 40(12):2935–2947, 2018. 3 [45] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for un- supervised domain adaptation. In ICML, pages 6028–6039, 2020. 1, 3 [46] Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. TTT++: when does self-supervised test-time training fail or thrive? In NeurIPS, pages 21808–21820, 2021. 3 [47] Yuang Liu, Wei Zhang, and Jun Wang. Source-free do- main adaptation for semantic segmentation. In CVPR, pages 1215–1224, 2021. 3 [48] Mingsheng Long, Yue Cao, Zhangjie Cao, Jianmin Wang, and Michael I. Jordan. Transferable representation learning with deep adaptation networks. IEEE Trans. Pattern Anal. Mach. Intell., 41(12):3071–3085, 2019. 1, 2 [49] Wenao Ma, Cheng Chen, Shuang Zheng, Jing Qin, Huimao Zhang, and Qi Dou. Test-time adaptation with calibration of medical image classification nets for label distribution shift. In MICCAI, pages 313–323, 2022. 1 [50] Divyat Mahajan, Shruti Tople, and Amit Sharma. Domain generalization using causal matching. In ICML, pages 7313– 7324, 2021. 3 [51] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds and algorithms. In COLT, 2009. 2 [52] Krikamol Muandet, David Balduzzi, and Bernhard Sch¨olkopf. Domain generalization via invariant fea- ture representation. In ICML, pages 10–18, 2013. 1, 3 [53] Zachary Nado, Shreyas Padhy, D. Sculley, Alexander D’Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robust- ness under covariate shift. CoRR, abs/2006.10963, 2020. 4, 6, 7, 8, 12, 14, 15, 16, 17 [54] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test- time model adaptation without forgetting. In ICML, pages 16888–16905, 2022. 1 [55] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test- time model adaptation without forgetting. In ICML, volume 162, pages 16888–16905, 2022. 4, 5, 6 [56] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Trans. Knowl. Data Eng., 22(10):1345–1359, 2010. 1 [57] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, pages 8024–8035, 2019. 6 [58] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In ICCV, pages 1406–1415, 2019. 2, 6 [59] Joaquin Quinonero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset shift in ma- chine learning. 2008. 1 [60] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H. Lampert. icarl: Incremental classi- fier and representation learning. InCVPR, pages 5533–5542, 2017. 3 [61] Amelie Royer and Christoph H Lampert. Classifier adapta- tion at prediction time. In CVPR, pages 1401–1409, 2015. 3 [62] Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tat- suya Harada. Maximum classifier discrepancy for unsuper- vised domain adaptation. In CVPR, pages 3723–3732, 2018. 2 [63] Inkyu Shin, Yi-Hsuan Tsai, Bingbing Zhuang, Samuel Schulter, Buyu Liu, Sparsh Garg, In So Kweon, and Kuk- Jin Yoon. MM-TTA: multi-modal test-time adaptation for 3d semantic segmentation. In CVPR, pages 16907–16916, 2022. 1, 3[64] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Test- time prompt tuning for zero-shot generalization in vision- language models. In NeurIPS, 2022. 1, 3 [65] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self- supervision for generalization under distribution shifts. In ICML, pages 9229–9248, 2020. 1, 2, 3, 4 [66] Rishabh Tiwari, KrishnaTeja Killamsetty, Rishabh K. Iyer, and Pradeep Shenoy. GCR: gradient coreset based replay buffer selection for continual learning. In CVPR, pages 99– 108, 2022. 3 [67] Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Ki- hyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker. Learning to adapt structured output space for semantic seg- mentation. In CVPR, pages 7472–7481, 2018. 2 [68] Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. In ICCV, pages 4068–4076, 2015. 2 [69] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In CVPR, pages 2962–2971, 2017. 1 [70] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno A. Ol- shausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In ICLR, 2021. 1, 2, 3, 4, 6, 7, 8, 12, 13, 14, 15, 16, 17 [71] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip Yu. Generalizing to unseen domains: A survey on domain generalization. IEEE Trans. Knowl. Data Eng., 2022. 1 [72] Mei Wang and Weihong Deng. Deep visual domain adapta- tion: A survey. Neurocomputing, 312:135–153, 2018. 1 [73] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Con- tinual test-time domain adaptation. In CVPR, pages 7191– 7201, 2022. 1, 2, 3, 4, 6, 7, 8, 13, 14, 15, 16, 17 [74] Markus Wulfmeier, Alex Bewley, and Ingmar Posner. Incre- mental adversarial domain adaptation for continually chang- ing environments. In ICRA, pages 4489–4495, 2018. 3 [75] Binhui Xie, Shuang Li, Mingjia Li, Chi Harold Liu, Gao Huang, and Guoren Wang. Sepico: Semantic-guided pixel contrast for domain adaptive semantic segmentation. IEEE Trans. Pattern Anal. Mach. Intell., pages 1–17, 2023. 2 [76] Saining Xie, Ross Girshick, Piotr Doll ´ar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In CVPR, pages 5987–5995, 2017. 6 [77] Ruijia Xu, Guanbin Li, Jihan Yang, and Liang Lin. Larger norm more transferable: An adaptive feature norm approach for unsupervised domain adaptation. In ICCV, pages 1426– 1435, 2019. 2 [78] Zhenlin Xu, Deyi Liu, Junlin Yang, Colin Raffel, and Marc Niethammer. Robust and generalizable visual representation learning via random convolutions. In ICLR, 2021. 3 [79] Shiqi Yang, Yaxing Wang, Joost van de Weijer, Luis Herranz, and Shangling Jui. Generalized source-free domain adapta- tion. In ICCV, pages 8978–8987, 2021. 3 [80] Sergey Zagoruyko and Nikos Komodakis. Wide residual net- works. In BMVC, 2016. 6 [81] Marvin Mengxin Zhang, Sergey Levine, and Chelsea Finn. MEMO: Test time robustness via adaptation and augmenta- tion. In NeurIPS, 2022. 1, 4 [82] Yizhe Zhang, Shubhankar Borse, Hong Cai, and Fatih Porikli. Auxadapt: Stable and efficient test-time adaptation for temporally consistent video semantic segmentation. In WACV, pages 2633–2642, 2022. 1, 3 [83] Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization: A survey. IEEE Trans. Pattern Anal. Mach. Intell., 2022. 1 [84] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Do- main generalization with mixstyle. In ICLR, 2021. 3 [85] Yang Zou, Zhiding Yu, BVK Vijaya Kumar, and Jinsong Wang. Unsupervised domain adaptation for semantic seg- mentation via class-balanced self-training. In ECCV, pages 289–305, 2018. 26. Appendix 6.1. Discussion Societal impact. RoTTA enables adapting pre-trained models on continually changing distributions with correl- atively sampled test streams without any more raw data or label requirements. Thus, our work may have a positive im- pact on communities to effectively deploy and adapt models in various real-world scenarios, which is economically and environmentally friendly. And since no training data is re- quired, this protects data privacy and has potential commer- cial value. We carry out experiments on benchmark datasets and do not notice any societal issues. It does not involve sensitive attributes. Future work. Our work suggests a few promising direc- tions for future work. Firstly, the proposed RoTTA is a preliminary attempt to perform test-time adaptation for the more realistic test stream under the setup PTTA. One could experiment to improve the algorithm by replacing some parts of RoTTA. More importantly, we hope that with this work, we can open a path to the original goal of test-time adaptation, which is performing test-time adaptation in real- world scenarios. Thus, one could improve PTTA to make it more realistic. Limitations. RoTTA achieves excellent performance on various tasks under the setup PTTA as demonstrated in Sec- tion 4 in the main paper, but we still find some limitations of it. Firstly, the adopted robust batch normalization (RBN) is a naive solution to the normalization of the correlatively sampled batch of data. This requires careful design of the value of α in RBN. Secondly, we observe that during the adaptation procedure of some methods like PL [39] and TENT [70], the model collapse finally. Although we de- sign many strategies to stabilize the adaptation and model collapse never occurs in the experiments of RoTTA, we are still missing a way to recover the model from the collapse state as a remedy. Thirdly, category similarity is only one kind of correlation. Although we conduct experiments on different datasets with Dirichlet distribution to simulate cor- relatively sampled test streams, we still need to validate our approach in some real-world scenarios. 6.2. Sensitivity to different hyper-parameters In this section, we conduct a detailed sensitivity analy- sis of the hyperparameters involved in RoTTA. All experi- ments are conducted on CIFAR100→CIFAR100-C, and the corruptions changes as motion, snow, fog, shot, defocus, contrast, zoom, brightness, frost, elastic, glass, gaussian, pixelate, jpeg, and impulse, and test streams are sampled correlatively with the Dirichlet parameter δ = 0.1. When we investigate the sensitivity to a specific hyperparameter, other hyperparameters are fixed to the default values, i.e., λt = 1.0, λu = 1.0, α = 0.05, and ν = 0.001, for all experiments. Table 7. Classification error with different value of λt/λu. λt/λu 0.0/2.0 0.5/1.5 1.0/1.0 1.5/ 0.5 2.0/ 0.0 CIFAR100-C 57.5 36.9 35.0 35.9 38.9 Trade-off between timeliness and uncertainty. When updating the memory bank, we take the timeliness and uncertainty of samples into account simultaneously, and λt and λu will make a trade-off between them. In Table 7, we show the results of RoTTA with varying λt/λu, i.e., λt/λu ∈ {0.0/2.0, 0.5/1.5, 1.0/1.0, 1.5/0.5, 2.0/0.0}. When we consider both of them, the results are relatively stable (35.0-36.9%). When we only think about one side, the performance drops significantly. For example, when we set λt/λu = 0.0/2.0 which means only considering uncer- tainty, the performance drops 22.5%. That’s because some confident samples get stuck in the memory bank, making it not work the way we design it. Table 8. Classification error with varying α α 0.5 0.1 0.05 0.01 0.005 0.001 CIFAR100-C 39.0 36.0 35.0 36.0 38.1 41.5 Sensitivity to α. We show the results of RoTTA with vary- ing α, i.e., α ∈ {0.5, 0.1, 0.05, 0.01, 0.005, 0.001} in Ta- ble 8. A larger value of α means updating the global statis- tics faster and vice versa. We can see that RoTTA achieves competitive results (35.0 − 36.0%) at appropriate values of α, i.e., α ∈ {0.1, 0.05, 0.01}. Updating too aggressively or too gently can lead to unreliable estimates of statistics. Table 9. Classification error with varying ν ν 0.05 0.01 0.005 0.001 0.0005 0.0001 CIFAR100-C 44.8 39.1 37.1 35.0 37.6 43.6 Sensitivity to ν. We show the results of RoTTA with vary- ing ν, i.e., ν ∈ {0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001} in Table 9. As we can see, the best performance is achieved at ν = 0.001. Updating the teacher model too quickly or too slowly can cause performance degradation. 6.3. Additional experiment details and results 6.3.1 Compared methods BN [53] utilizes statistics of the current batch of data to nor- malize their feature maps without tuning any parameters. PL [39] is based on BN [53], and adopts pseudo labels to train the affine parameters in BN layers.TENT [70] is the first to propose fully test-time adaptation. It adopts test-time batch normalization and utilizes entropy minimization to train the affine parameters of BN layers. We reimplement it following the released code https:// github.com/DequanWang/tent. LAME [5] adapts the output of the pre-trained model by optimizing a group of latent variables without tuning any in- ner parts of the model. We reimplement it following the re- leased code https://github.com/fiveai/LAME. CoTTA [73] considers performing test-time adapta- tion on continually changing distributions and pro- pose augmentation-averaged pseudo-labels and stochastic restoration to address error accumulation and catastrophic forgetting. We reimplement it following the released code https://github.com/qinenergy/cotta. NOTE [19] proposes instance-aware normalization and prediction-balanced reservoir sampling to stable the adapta- tion on temporally correlated test streams. We reimplement it following the released code https://github.com/ TaesikGong/NOTE. 6.3.2 Simulate correlatively sampling As we described in the scenarios of autonomous driving that the car will follow more vehicles on the highway or will en- counter more pedestrians on the sidewalk, so we use the same category to simulate correlation. From a macro point of view, the test distribution Ptest changes continually as P0, P1, ...,P∞. During the period when Ptest = Pt, we adopt Dirichlet distribution to simulate correlatively sam- pled test stream. More specifically, we consider dividing samples of C classes into T slots. Firstly, we utilize Dirich- let distribution with parameter γ to generate the partition criterion q ∈ RC×T . Then for each class c, we split samples into T parts according to qc and assign each part to each slot respectively. Finally, we concatenate all slots to sim- ulate the correlatively sampled test stream for Ptest = Pt. And as Ptest changes, we use the above method again to generate the test stream. 6.3.3 Detailed results of different orders We report the average classification error of ten different distribution changing orders in Table 6 of the main pa- per. And then we present the specific results here, includ- ing Table 10, 11, 12, 13, 14, 15, 16, 17, 18, and 19 for CIFAR10→CIFAR10-C and Table 20, 21, 22, 23, 24, 25, 26, 27, 28, and 29 for CIFAR100 →CIFAR100-C. We can see consistently superior performance of RoTTA. One thing to mention is that on DomainNet we use alphabetical order to determine the order of domain changes.Table 10. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method brightnesspixelategaussianmotionzoom glass impulsejpeg defocuselasticshot frost snow fog contrast Avg. Source 9.3 58.5 72.3 34.8 42.0 54.3 72.9 30.3 46.9 26.6 65.7 41.3 25.1 26.0 46.7 43.5BN [53] 71.1 75.2 76.8 74.2 73.7 80.1 79.3 77.5 73.8 77.7 77.2 73.3 73.8 72.7 71.7 75.2PL [39] 71.7 75.9 80.2 78.4 80.2 85.2 85.3 85.4 85.1 86.7 87.9 87.9 88.1 88.3 87.9 83.6TENT [70] 71.6 75.9 81.3 80.5 82.3 85.6 87.1 87.0 87.1 88.1 88.2 87.8 87.9 88.3 88.2 84.4LAME [5] 5.4 56.8 73.1 29.1 37.0 50.5 71.4 22.3 42.8 18.6 65.5 37.3 18.8 20.4 43.6 39.5CoTTA [73] 75.0 79.8 83.1 83.4 83.2 84.0 84.5 83.2 83.5 83.3 83.6 83.0 83.0 83.4 83.7 82.6NOTE [19] 10.1 29.9 47.1 23.4 28.4 48.4 46.1 41.8 26.9 36.1 37.5 25.0 25.0 23.2 14.2 30.9 RoTTA 10.4 26.6 37.5 23.9 17.0 40.9 39.7 30.1 18.0 29.9 30.1 23.6 21.7 17.6 19.0 25.7(+5.2) Table 11. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method jpeg shot zoom frost contrastfog defocuselasticgaussianbrightnessglass impulsepixelatesnow motion Avg. Source 30.3 65.7 42.0 41.3 46.7 26.0 46.9 26.6 72.3 9.3 54.3 72.9 58.5 25.1 34.8 43.5BN [53] 77.6 75.8 73.4 74.1 73.1 72.5 72.9 77.1 77.2 72.2 79.9 79.9 75.5 74.6 72.9 75.2PL [39] 77.6 77.1 76.6 78.3 77.5 79.8 82.0 84.8 86.1 83.5 87.8 87.1 86.5 85.6 85.7 82.4TENT [70] 78.5 78.2 79.2 81.8 84.8 84.8 86.4 87.3 87.9 86.7 87.3 87.8 87.2 87.5 87.1 84.8LAME [5] 22.5 65.2 37.0 37.1 44.0 20.3 41.7 18.7 72.8 5.2 51.2 71.5 57.0 19.0 29.4 39.5CoTTA [73]78.5 81.0 82.8 84.1 84.9 83.4 83.5 83.5 84.5 83.3 84.7 84.6 83.0 84.4 83.4 83.3NOTE [19]35.4 36.1 22.1 21.3 11.6 24.8 24.5 36.0 37.7 18.4 49.0 47.4 43.9 30.4 29.2 31.2 RoTTA 33.2 33.3 19.8 24.1 24.9 20.5 16.2 31.7 28.4 11.8 43.1 36.9 32.5 20.7 20.6 26.5(+4.7) Table 12. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastdefocusgaussianshot snow frost glass zoom elasticjpeg pixelatebrightnessimpulsemotion fog Avg. Source 46.7 46.9 72.3 65.7 25.1 41.3 54.3 42.0 26.6 30.3 58.5 9.3 72.9 34.8 26.0 43.5BN [53] 72.3 72.6 76.9 77.1 74.8 73.5 80.0 73.2 77.4 78.6 76.4 71.0 79.1 73.9 71.5 75.2PL [39] 72.4 75.3 80.7 82.6 83.3 83.5 86.6 85.7 86.6 88.4 87.5 86.6 88.3 88.2 86.8 84.1TENT [70] 73.5 77.9 85.5 86.9 87.6 87.8 88.3 87.7 88.6 89.2 88.5 88.5 89.3 88.6 88.6 86.4LAME [5] 43.5 42.3 73.1 65.3 19.2 37.3 51.1 36.8 18.5 22.5 56.9 5.5 71.1 29.1 20.5 39.5CoTTA [73]79.4 80.3 83.8 83.9 83.9 83.4 85.0 83.2 85.1 84.3 83.9 83.3 84.7 83.9 82.5 83.4NOTE [19] 9.6 21.8 40.1 31.0 25.5 22.6 44.8 22.8 33.2 39.4 33.2 18.1 50.0 28.3 29.8 30.0 RoTTA 18.4 17.9 38.4 31.9 23.3 19.8 40.7 17.4 31.4 29.8 27.8 11.3 43.8 19.7 18.8 26.0(+4.0) Table 13. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method shot fog glass pixelatesnow elasticbrightnessimpulsedefocusfrost contrastgaussianmotionjpeg zoom Avg. Source 65.7 26.0 54.3 58.5 25.1 26.6 9.3 72.9 46.9 41.3 46.7 72.3 34.8 30.3 42.0 43.5BN [53] 76.4 72.0 80.4 76.2 74.8 77.0 71.1 79.6 73.8 74.4 73.0 77.0 72.5 78.3 72.5 75.3PL [39] 77.0 73.3 82.4 79.8 81.0 82.3 79.5 84.4 82.7 83.5 83.5 85.5 84.8 87.0 84.5 82.1TENT [70]76.9 74.6 82.3 81.7 82.0 84.9 84.8 87.3 86.6 87.3 87.6 89.2 88.3 88.9 87.3 84.6LAME [5] 65.3 20.6 50.9 56.7 19.2 18.8 5.4 71.8 42.8 37.2 43.3 73.2 29.4 22.6 36.9 39.6CoTTA [73]77.4 77.6 83.8 81.9 82.2 82.6 80.4 83.3 82.3 81.5 82.7 82.6 81.1 82.9 81.0 81.6NOTE [19]34.0 20.9 43.1 36.6 24.0 36.4 12.1 48.0 25.9 23.9 13.4 38.1 25.0 43.2 24.2 29.9 RoTTA 35.0 21.1 43.9 29.2 22.1 29.7 10.8 44.6 25.3 22.7 24.6 29.4 26.9 34.4 16.1 27.7(+2.2) Table 14. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method pixelateglass zoomsnow fog impulsebrightnessmotionfrost jpeg gaussianshot contrastdefocus elastic Avg. Source 58.5 54.3 42.0 25.1 26.0 72.9 9.3 34.8 41.3 30.3 72.3 65.7 46.7 46.9 26.6 43.5BN [53] 76.0 79.6 73.3 75.2 72.9 79.8 71.1 73.5 74.1 78.6 77.4 76.1 72.0 73.8 76.4 75.3PL [39] 76.7 81.3 77.4 80.3 81.2 86.3 83.3 85.9 86.2 87.7 88.1 88.4 87.4 87.6 87.7 84.4TENT [70] 76.4 80.2 77.8 81.2 83.0 87.1 85.6 87.2 87.6 88.7 88.6 88.9 88.5 88.6 88.2 85.2LAME [5] 56.9 50.7 37.0 19.0 20.3 71.5 5.4 29.2 37.2 22.5 73.0 65.3 43.8 42.4 18.7 39.5CoTTA [73]77.1 83.6 84.1 84.8 84.4 85.2 84.0 84.3 84.9 84.9 85.0 84.7 85.3 84.4 84.3 84.1NOTE [19] 27.8 52.2 24.5 22.3 21.6 44.5 14.5 21.3 25.9 42.5 38.8 36.0 16.7 28.1 40.6 30.5 RoTTA 25.9 43.3 17.7 22.1 20.2 41.5 12.2 22.9 22.5 31.2 33.8 26.0 31.4 17.7 27.6 26.4(+4.1)Table 15. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 34.8 25.1 26.0 65.7 46.9 46.7 42.0 9.3 41.3 26.6 54.3 72.3 58.5 30.3 72.9 43.5BN [53] 73.2 73.4 72.7 77.2 73.7 72.5 72.9 71.0 74.1 77.7 80.0 76.9 75.5 78.3 79.0 75.2PL [39] 73.9 75.0 75.6 81.0 79.9 80.6 82.0 83.2 85.3 87.3 88.3 87.5 87.5 87.5 88.2 82.9TENT [70] 74.3 77.4 80.1 86.2 86.7 87.3 87.9 87.4 88.2 89.0 89.2 89.0 88.3 89.7 89.2 86.0LAME [5] 29.5 19.0 20.3 65.3 42.4 43.4 36.8 5.4 37.2 18.6 51.2 73.2 57.0 22.6 71.3 39.5CoTTA [73]77.1 80.6 83.1 84.4 83.9 84.2 83.1 82.6 84.4 84.2 84.5 84.6 82.7 83.8 84.9 83.2NOTE [19] 18.0 22.1 20.6 35.6 26.9 13.6 26.5 17.3 27.2 37.0 48.3 38.8 42.6 41.9 49.7 31.1 RoTTA 18.1 21.3 18.8 33.6 23.6 16.5 15.1 11.2 21.9 30.7 39.6 26.8 33.7 27.8 39.5 25.2(+5.9) Table 16. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method frost impulsejpeg contrastzoom glass pixelatesnow defocusmotionbrightnesselasticshot fog gaussian Avg. Source 41.3 72.9 30.3 46.7 42.0 54.3 58.5 25.1 46.9 34.8 9.3 26.6 65.7 26.0 72.3 43.5BN [53] 73.8 79.1 77.9 73.0 73.7 80.1 75.7 74.4 73.7 74.0 71.7 77.0 75.9 72.8 76.2 75.3PL [39] 74.2 80.9 80.4 79.5 81.8 85.9 83.9 85.1 84.7 85.9 85.9 86.7 87.2 87.0 87.8 83.8TENT [70]73.9 80.3 81.8 81.6 83.6 86.3 85.6 85.7 86.4 87.7 87.4 88.8 88.8 88.5 88.4 85.0LAME [5] 37.4 71.8 22.4 43.5 37.0 50.5 57.0 19.0 42.8 29.1 5.4 18.7 65.2 20.4 72.9 39.5CoTTA [73]76.5 82.2 82.8 85.0 82.9 85.0 83.0 82.9 83.5 83.4 82.6 83.7 83.2 83.3 83.6 82.9NOTE [19]21.1 41.4 36.3 10.2 21.7 46.7 37.5 26.4 26.1 21.4 14.3 37.9 38.5 24.4 40.7 29.6 RoTTA 22.2 44.9 35.2 18.8 19.7 41.5 28.5 23.2 21.2 18.6 12.4 30.0 27.4 20.0 31.2 26.3(+3.3) Table 17. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method defocusmotionzoom shot gaussianglass jpeg fog contrastpixelatefrost snow brightnesselastic impulse Avg. Source 46.9 34.8 42.0 65.7 72.3 54.3 30.3 26.0 46.7 58.5 41.3 25.1 9.3 26.6 72.9 43.5BN [53] 72.8 72.7 73.3 77.2 77.3 80.0 77.6 72.6 73.3 76.6 73.8 74.1 70.3 77.5 79.0 75.2PL [39] 73.2 74.6 76.5 81.7 82.8 84.6 85.1 84.6 86.2 86.4 86.1 87.1 86.8 88.4 88.1 83.5TENT [70] 73.7 74.3 77.1 82.5 84.3 86.9 87.4 86.6 88.0 88.5 88.1 88.5 88.4 89.4 88.9 84.8LAME [5] 42.5 29.3 37.0 65.3 73.2 50.5 22.5 20.5 43.5 56.9 37.1 18.9 5.4 18.5 71.3 39.5CoTTA [73]76.3 79.8 82.4 83.3 83.8 84.5 83.1 82.7 84.7 82.9 83.0 83.3 81.4 83.8 83.8 82.6NOTE [19] 18.5 18.8 23.6 36.5 33.7 47.8 38.6 22.8 13.0 40.0 29.2 26.3 17.5 44.0 52.9 30.9 RoTTA 17.0 17.5 16.5 33.8 33.3 42.7 29.4 18.0 19.6 29.5 20.7 22.1 11.5 29.5 38.1 25.3(+5.6) Table 18. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method glass zoom impulsefog snow jpeg gaussianfrost shot brightnesscontrastmotionpixelatedefocus elastic Avg. Source 54.3 42.0 72.9 26.0 25.1 30.3 72.3 41.3 65.7 9.3 46.7 34.8 58.5 46.9 26.6 43.5BN [53] 79.7 72.3 79.8 73.2 74.7 77.7 76.6 73.2 77.1 72.2 73.0 73.3 75.5 73.8 76.4 75.2PL [39] 79.6 73.2 81.3 77.3 79.1 83.0 83.2 83.0 85.5 84.3 87.0 86.9 86.4 86.5 87.6 82.9TENT [70] 79.5 74.1 84.2 82.2 84.5 86.5 86.7 85.9 87.2 86.6 86.8 87.3 86.9 87.4 87.3 84.9LAME [5] 50.8 36.9 71.3 20.6 19.2 22.4 72.5 37.2 65.4 5.2 43.3 29.1 57.0 42.4 18.7 39.5CoTTA [73]81.5 79.4 85.2 84.1 84.5 84.2 84.8 84.0 84.8 83.2 85.2 83.8 83.2 84.6 83.6 83.7NOTE [19]45.0 21.2 42.3 21.0 21.6 38.4 36.4 21.4 33.1 16.7 14.6 25.4 43.5 29.1 38.5 29.9 RoTTA 42.6 17.6 48.1 23.9 21.9 32.6 32.1 20.7 30.2 12.0 21.9 20.0 33.7 16.4 28.1 26.8(+3.1) Table 19. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastgaussiandefocuszoom frost glass jpeg fog pixelateelasticshot impulsesnow motion brightness Avg. Source 46.7 72.3 46.9 42.0 41.3 54.3 30.3 26.0 58.5 26.6 65.7 72.9 25.1 34.8 9.3 43.5BN [53] 72.4 76.2 73.2 73.7 73.6 80.0 77.6 72.6 76.4 77.7 77.2 79.9 73.8 73.9 70.0 75.2PL [39] 73.0 78.2 76.7 79.7 81.6 85.6 86.0 85.3 87.2 88.2 88.3 88.9 88.5 89.2 88.2 84.3TENT [70] 73.6 80.9 83.1 85.6 87.1 88.5 88.8 88.4 89.2 89.3 89.0 89.0 89.3 89.9 89.1 86.7LAME [5] 43.5 73.2 42.3 37.0 37.2 50.5 22.5 20.5 57.0 18.6 65.5 71.5 18.8 29.1 5.6 39.5CoTTA [73]79.5 81.4 83.4 83.6 83.9 85.0 84.0 82.8 84.8 84.8 84.5 84.7 84.1 84.4 82.8 83.6NOTE [19] 9.6 43.6 26.5 24.8 23.9 46.9 38.0 23.4 34.0 41.2 41.5 45.0 27.6 25.8 19.0 31.4 RoTTA 18.4 36.0 21.1 15.6 23.0 41.7 30.8 19.1 34.1 31.1 31.3 39.9 26.0 18.8 12.8 26.6(+4.8)Table 20. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method brightnesspixelategaussianmotionzoom glass impulsejpeg defocuselasticshot frost snow fog contrast Avg. Source 29.5 74.7 73.0 30.8 28.8 54.1 39.4 41.2 29.3 37.2 68.0 45.8 39.5 50.3 55.1 46.4BN [53] 46.5 52.0 58.6 47.4 47.4 57.6 58.2 56.9 47.0 53.4 56.0 52.5 53.1 57.7 49.1 52.9PL [39] 48.5 60.7 77.1 85.9 91.5 95.5 95.8 96.6 96.8 96.9 97.3 97.5 97.6 97.7 97.9 88.9TENT [70] 49.8 69.4 92.2 96.0 96.7 97.3 97.5 97.9 97.5 97.9 98.0 98.2 98.2 98.2 98.2 92.2LAME [5] 21.7 75.1 72.7 22.9 20.6 49.0 32.1 33.3 21.2 28.0 66.8 40.0 30.6 43.9 51.3 40.6CoTTA [73] 46.8 48.4 54.7 48.7 48.6 53.5 55.4 52.8 49.8 51.8 53.5 52.9 54.1 56.7 53.6 52.1NOTE [19] 42.6 53.0 69.9 52.1 53.3 70.4 73.1 76.7 80.8 96.0 97.7 97.1 96.6 97.2 95.8 76.8 RoTTA 28.4 37.3 44.6 31.9 28.3 41.8 43.6 39.9 28.0 35.2 38.2 33.7 33.0 39.5 31.0 35.6(+5.0) Table 21. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method jpeg shot zoom frost contrastfog defocuselasticgaussianbrightnessglass impulsepixelatesnow motion Avg. Source 41.2 68.0 28.8 45.8 55.1 50.3 29.3 37.2 73.0 29.5 54.1 39.4 74.7 39.5 30.8 46.4BN [53] 58.3 56.8 47.8 51.8 48.9 57.3 46.8 53.5 57.8 45.5 57.1 58.5 51.7 53.3 48.8 52.9PL [39] 59.4 66.3 74.9 87.5 94.2 95.5 96.2 97.1 97.4 97.2 97.5 97.7 98.0 98.2 98.2 90.4TENT [70] 62.0 79.3 91.7 95.8 96.9 97.0 97.4 97.7 97.6 97.7 97.9 97.9 98.0 97.9 97.9 93.5LAME [5] 33.6 66.7 21.1 39.9 50.6 43.9 21.0 28.6 72.5 21.6 48.6 32.5 74.5 30.6 22.5 40.6CoTTA [73]54.6 54.1 49.6 52.1 52.7 58.0 50.3 53.3 55.0 49.1 55.4 55.7 51.0 54.6 52.1 53.2NOTE [19]60.4 63.0 49.9 55.7 47.0 65.2 59.4 76.6 90.9 87.2 96.8 97.0 97.3 96.7 96.8 76.0 RoTTA 43.9 45.3 31.0 37.3 35.7 41.2 27.7 34.8 39.7 26.6 39.5 41.9 32.0 33.0 30.5 36.0(+4.6) Table 22. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastdefocusgaussianshot snow frost glass zoom elasticjpeg pixelatebrightnessimpulsemotion fog Avg. Source 55.1 29.3 73.0 68.0 39.5 45.8 54.1 28.8 37.2 41.2 74.7 29.5 39.4 30.8 50.3 46.4BN [53] 49.4 47.2 58.6 56.2 52.7 52.0 57.9 46.1 54.4 57.7 50.5 46.2 58.2 47.6 58.5 52.9PL [39] 54.8 64.2 83.3 92.4 95.5 96.5 96.9 96.4 97.2 97.4 97.8 97.8 97.9 97.7 98.0 90.9TENT [70] 60.2 83.1 95.2 96.5 96.9 97.3 97.0 97.3 97.8 97.8 97.6 97.9 97.8 97.9 98.1 93.9LAME [5] 51.3 21.3 72.7 66.3 30.2 40.0 48.6 20.9 27.7 33.3 75.0 21.5 32.2 22.5 43.8 40.5CoTTA [73]52.1 48.6 55.1 52.7 53.4 51.9 55.9 49.2 53.2 52.8 49.2 49.7 56.2 50.7 58.1 52.6NOTE [19] 39.5 45.9 68.8 61.8 57.4 58.5 71.4 66.5 80.8 90.9 94.2 94.9 97.0 95.5 96.6 74.6 RoTTA 41.7 30.5 44.9 40.5 35.4 34.1 40.5 28.2 34.5 39.5 31.1 26.7 43.3 31.4 38.8 36.1(+4.4) Table 23. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method shot fog glass pixelatesnow elasticbrightnessimpulsedefocusfrost contrastgaussianmotionjpeg zoom Avg. Source 68.0 50.3 54.1 74.7 39.5 37.2 29.5 39.4 29.3 45.8 55.1 73.0 30.8 41.2 28.8 46.4BN [53] 57.5 58.6 58.5 50.5 52.7 53.1 45.9 57.9 47.0 51.5 47.8 58.2 48.2 57.1 47.7 52.8PL [39] 59.5 72.9 85.1 89.6 94.5 96.8 97.1 97.9 97.8 98.0 98.3 98.2 98.0 98.0 98.2 92.0TENT [70]60.3 81.4 95.0 96.6 97.0 97.3 97.3 97.7 97.7 97.7 97.8 97.7 97.6 97.6 97.9 93.8LAME [5] 66.4 43.2 49.0 75.2 30.2 28.5 21.6 32.5 21.2 39.5 52.0 72.8 22.3 33.1 20.5 40.5CoTTA [73]54.5 58.4 55.6 50.0 53.9 53.4 50.3 56.7 51.3 53.2 53.7 56.1 52.0 54.5 51.5 53.7NOTE [19]61.8 60.2 63.4 55.6 59.8 65.9 58.6 75.1 77.8 93.8 94.2 97.0 95.0 95.5 94.4 76.5 RoTTA 45.5 44.5 43.5 35.6 35.1 35.7 26.2 44.0 29.7 34.2 32.0 40.7 31.4 39.4 27.7 36.3(+4.2) Table 24. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method pixelateglass zoomsnow fog impulsebrightnessmotionfrost jpeg gaussianshot contrastdefocus elastic Avg. Source 74.7 54.1 28.8 39.5 50.3 39.4 29.5 30.8 45.8 41.2 73.0 68.0 55.1 29.3 37.2 46.4BN [53] 51.7 58.6 47.8 52.9 57.1 58.2 45.9 47.6 52.9 57.8 57.5 56.7 49.5 46.1 54.0 52.9PL [39] 52.4 68.0 73.4 87.9 93.7 96.1 95.7 96.0 96.5 96.7 97.5 97.7 97.7 97.3 97.7 89.6TENT [70] 53.5 77.8 91.1 96.0 97.0 97.6 97.4 97.6 97.9 98.1 98.1 98.0 98.1 97.9 98.1 92.9LAME [5] 74.8 48.2 21.1 30.6 43.4 32.5 21.6 23.0 39.6 33.3 72.7 66.5 51.5 20.7 27.5 40.5CoTTA [73]49.3 55.1 49.1 52.9 56.8 55.7 49.5 50.0 53.6 53.4 54.9 53.9 53.8 50.1 53.5 52.8NOTE [19] 52.2 64.9 47.5 57.0 61.9 67.3 60.4 67.8 77.4 90.6 97.1 96.8 92.8 95.9 96.6 75.1 RoTTA 36.4 44.4 29.7 36.5 41.0 44.1 26.8 29.5 33.0 40.3 40.3 38.2 33.9 28.5 34.9 35.8(+4.7)Table 25. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 30.8 39.5 50.3 68.0 29.3 55.1 28.8 29.5 45.8 37.2 54.1 73.0 74.7 41.2 39.4 46.4BN [53] 48.5 54.0 58.9 56.2 46.4 48.0 47.0 45.4 52.9 53.4 57.1 58.2 51.7 57.1 58.8 52.9PL [39] 50.6 62.1 73.9 87.8 90.8 96.0 94.8 96.4 97.4 97.2 97.4 97.4 97.3 97.4 97.4 88.9TENT [70] 53.3 77.6 93.0 96.5 96.7 97.5 97.1 97.5 97.3 97.2 97.1 97.7 97.6 98.0 98.3 92.8LAME [5] 22.4 30.4 43.9 66.3 21.3 51.7 20.6 21.8 39.6 28.0 48.7 72.8 74.6 33.1 32.3 40.5CoTTA [73]49.2 52.7 56.8 53.0 48.7 51.7 49.4 48.7 52.5 52.2 54.3 54.9 49.6 53.4 56.2 52.2NOTE [19] 45.7 53.0 58.2 65.6 54.2 52.0 59.8 63.5 74.8 91.8 98.1 98.3 96.8 97.0 98.2 73.8 RoTTA 31.8 36.7 40.9 42.1 30.0 33.6 27.9 25.4 32.3 34.0 38.8 38.7 31.3 38.0 42.9 35.0(+5.5) Table 26. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method frost impulsejpeg contrastzoom glass pixelatesnow defocusmotionbrightnesselasticshot fog gaussian Avg. Source 45.8 39.4 41.2 55.1 28.8 54.1 74.7 39.5 29.3 30.8 29.5 37.2 68.0 50.3 73.0 46.4BN [53] 52.9 58.8 57.6 48.2 47.4 57.6 50.9 52.4 47.0 47.2 45.1 54.0 56.4 57.7 58.2 52.8PL [39] 56.9 73.3 86.7 94.4 95.8 97.3 97.2 97.4 97.6 97.4 97.7 97.6 97.8 98.3 98.1 92.2TENT [70]60.1 84.2 95.7 97.2 97.4 97.9 97.8 98.0 98.1 98.2 98.3 98.4 98.4 98.4 98.4 94.4LAME [5] 39.9 32.4 33.4 51.4 20.6 49.0 74.4 31.3 21.2 22.6 21.9 28.1 66.9 43.9 72.5 40.6CoTTA [73]51.5 55.3 54.3 51.8 49.4 55.3 50.7 54.2 51.4 50.6 49.5 53.6 55.0 57.1 55.8 53.0NOTE [19]51.6 60.9 60.3 45.4 54.3 70.8 68.8 75.0 75.7 87.1 94.7 95.6 96.7 96.4 97.2 75.4 RoTTA 40.0 46.3 42.8 36.4 29.2 42.3 33.2 34.4 28.4 29.2 26.4 34.5 38.5 39.8 39.3 36.0(+4.6) Table 27. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method defocusmotionzoom shot gaussianglass jpeg fog contrastpixelatefrost snow brightnesselastic impulse Avg. Source 29.3 30.8 28.8 68.0 73.0 54.1 41.2 50.3 55.1 74.7 45.8 39.5 29.5 37.2 39.4 46.4BN [53] 47.1 48.6 47.8 56.2 57.6 57.6 57.6 57.5 48.7 50.6 51.8 53.2 46.9 53.5 58.8 52.9PL [39] 48.8 58.7 69.9 88.0 95.1 96.6 96.7 96.9 97.4 97.4 98.2 98.2 98.2 98.3 98.5 89.1TENT [70] 51.0 67.6 85.8 95.9 97.2 97.5 97.2 97.7 98.1 97.9 97.7 97.7 98.0 98.0 98.2 91.7LAME [5] 21.2 22.8 21.1 66.3 72.8 49.0 33.3 44.8 51.7 74.9 39.8 31.2 21.3 27.3 32.3 40.6CoTTA [73]48.4 48.8 48.2 52.9 54.0 53.8 52.7 57.2 52.6 48.6 51.8 53.9 49.4 52.3 56.0 52.0NOTE [19] 45.1 46.7 49.1 67.3 65.5 69.4 75.5 80.3 83.8 96.0 97.6 97.1 96.1 97.9 98.7 77.7 RoTTA 29.6 31.3 28.8 43.9 41.5 41.3 40.9 39.8 32.1 32.6 33.1 33.0 26.5 34.5 42.9 35.4(+5.2) Table 28. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method glass zoom impulsefog snow jpeg gaussianfrost shot brightnesscontrastmotionpixelatedefocus elastic Avg. Source 54.1 28.8 39.4 50.3 39.5 41.2 73.0 45.8 68.0 29.5 55.1 30.8 74.7 29.3 37.2 46.4BN [53] 58.8 47.7 59.2 57.6 52.7 56.9 58.2 52.0 56.7 45.5 47.8 48.2 51.7 46.1 54.0 52.9PL [39] 60.1 59.5 75.1 85.7 91.5 94.6 96.5 97.1 97.4 97.3 98.0 97.7 97.9 97.8 97.7 89.6TENT [70] 61.6 71.5 91.0 95.9 96.6 97.1 96.9 97.3 97.4 97.2 97.9 98.0 98.1 97.9 97.8 92.8LAME [5] 48.6 20.6 32.3 44.4 30.2 33.6 72.4 40.0 66.3 21.6 52.0 22.8 74.6 20.7 27.5 40.5CoTTA [73]56.4 48.9 56.1 57.8 54.1 54.2 56.2 53.6 55.4 50.0 53.6 51.6 51.2 50.7 54.4 53.6NOTE [19]62.5 46.3 61.5 61.1 58.6 68.4 76.1 78.3 92.0 93.4 96.1 95.4 96.2 95.8 96.4 78.5 RoTTA 45.5 30.0 45.9 42.6 35.3 41.8 42.2 34.5 40.2 27.3 31.3 30.2 32.7 28.1 34.9 36.2(+4.3) Table 29. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastgaussiandefocuszoom frost glass jpeg fog pixelateelasticshot impulsesnow motion brightness Avg. Source 55.1 73.0 29.3 28.8 45.8 54.1 41.2 50.3 74.7 37.2 68.0 39.4 39.5 30.8 29.5 46.4BN [53] 49.5 58.8 47.0 46.5 52.2 57.6 57.6 57.6 51.7 53.5 56.0 58.5 53.1 47.6 46.3 52.9PL [39] 53.6 70.4 76.0 85.1 91.2 95.2 96.0 97.0 96.9 97.3 97.3 97.6 97.5 97.6 97.7 89.8TENT [70] 60.2 89.1 95.0 96.2 96.9 97.0 96.5 97.0 97.0 97.2 97.6 97.8 97.5 97.9 97.7 94.0LAME [5] 51.3 72.5 21.5 21.0 39.6 49.0 33.3 44.8 74.8 28.0 66.8 32.5 30.6 22.5 21.4 40.6CoTTA [73]52.3 55.3 49.5 48.1 52.1 54.8 52.7 56.9 50.6 52.6 53.7 55.8 54.6 50.6 50.5 52.7NOTE [19] 39.1 64.7 48.9 50.6 59.1 70.1 71.7 75.0 85.2 95.7 96.9 98.4 96.0 95.9 94.9 76.1 RoTTA 41.4 46.2 30.5 28.5 36.0 40.9 40.5 39.6 33.0 35.0 38.2 43.1 33.9 30.7 27.1 36.3(+4.3)",
      "meta_data": {
        "arxiv_id": "2303.13899v1",
        "authors": [
          "Longhui Yuan",
          "Binhui Xie",
          "Shuang Li"
        ],
        "published_date": "2023-03-24T10:19:14Z",
        "pdf_url": "https://arxiv.org/pdf/2303.13899v1.pdf",
        "github_url": "https://github.com/BIT-DA/RoTTA"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Practical Test-Time Adaptation (PTTA), a new TTA setup that addresses real-world dynamic scenarios by considering both continually changing distributions and correlative sampling in test data streams. It benchmarks prior methods under PTTA, revealing their ineffectiveness due to their focus on only one aspect of the problem. It proposes Robust Test-Time Adaptation (RoTTA), an easy-to-implement and effective method designed to robustly adapt pre-trained models in PTTA settings. RoTTA achieves state-of-the-art results, significantly reducing classification error on CIFAR-10-C, CIFAR-100-C, and DomainNet datasets.",
        "methodology": "The proposed Robust Test-Time Adaptation (RoTTA) method consists of three key components: 1) Robust Batch Normalization (RBN), which replaces erroneous current batch statistics with global statistics maintained by exponential moving average (EMA) to robustly normalize feature maps. 2) Category-Balanced Sampling with Timeliness and Uncertainty (CSTU), which maintains a memory bank of category-balanced samples, prioritizing newer and less uncertain samples based on a heuristic score considering age and prediction entropy. 3) Robust Training with Timeliness, which employs a teacher-student model where the student is updated by minimizing a timeliness-reweighted loss on samples from the memory bank, and the teacher is updated via EMA of the student's parameters. Only affine parameters in RBN are trained for stability. Correlative sampling in test streams is simulated using a Dirichlet distribution.",
        "experimental_setup": "Experiments are conducted on CIFAR-10-C and CIFAR-100-C for robustness to corruptions, and the large-scale DomainNet dataset for generalization under domain shift. Pre-trained models include WildResNet-28 for CIFAR10-C, ResNeXt-29 for CIFAR100-C (from RobustBench), and ResNet-101 for DomainNet. Test streams simulate continually changing distributions (e.g., different corruption types at severity 5, or different domains) and correlative sampling using a Dirichlet distribution (default δ=0.1). Adam optimizer with a learning rate of 1.0 × 10−3 and β=0.9 is used. A batch size of 64 and a memory bank capacity of N=64 are set for all methods. RoTTA's hyperparameters are α=0.05, ν=0.001, λt=1.0, λu=1.0. Performance is validated using average classification error, and extensive ablation studies and sensitivity analyses (on δ, batch size, and RoTTA's hyperparameters) are performed. Baselines include BN, PL, TENT, LAME, CoTTA, and NOTE.",
        "limitations": "The adopted Robust Batch Normalization (RBN) is a relatively naive solution, requiring careful tuning of the `α` value. The method currently lacks a mechanism to explicitly recover the model from a collapsed state, although RoTTA's strategies are designed to prevent such collapses. The simulation of correlative sampling primarily focuses on category similarity using Dirichlet distribution, and further validation in real-world scenarios is needed to confirm its generalizability beyond simulated environments.",
        "future_research_directions": "Future work could focus on improving the RoTTA algorithm by exploring alternative or more sophisticated designs for its individual components. Additionally, there is a direction to enhance the Practical Test-Time Adaptation (PTTA) setup itself to make it even more realistic, bridging the gap towards achieving effective test-time adaptation in true real-world applications.",
        "experimental_code": "class RoTTA(BaseAdapter):\n    def __init__(self, cfg, model, optimizer):\n        super(RoTTA, self).__init__(cfg, model, optimizer)\n        self.mem = memory.CSTU(capacity=self.cfg.ADAPTER.RoTTA.MEMORY_SIZE, num_class=cfg.CORRUPTION.NUM_CLASS, lambda_t=cfg.ADAPTER.RoTTA.LAMBDA_T, lambda_u=cfg.ADAPTER.RoTTA.LAMBDA_U)\n        self.model_ema = self.build_ema(self.model)\n        self.transform = get_tta_transforms(cfg)\n        self.nu = cfg.ADAPTER.RoTTA.NU\n        self.update_frequency = cfg.ADAPTER.RoTTA.UPDATE_FREQUENCY\n        self.current_instance = 0\n\n    @torch.enable_grad()\n    def forward_and_adapt(self, batch_data, model, optimizer):\n        # batch data\n        with torch.no_grad():\n            model.eval()\n            self.model_ema.eval()\n            ema_out = self.model_ema(batch_data)\n            predict = torch.softmax(ema_out, dim=1)\n            pseudo_label = torch.argmax(predict, dim=1)\n            entropy = torch.sum(- predict * torch.log(predict + 1e-6), dim=1)\n\n        # add into memory\n        for i, data in enumerate(batch_data):\n            p_l = pseudo_label[i].item()\n            uncertainty = entropy[i].item()\n            current_instance = (data, p_l, uncertainty)\n            self.mem.add_instance(current_instance)\n            self.current_instance += 1\n\n            if self.current_instance % self.update_frequency == 0:\n                self.update_model(model, optimizer)\n\n        return ema_out\n\n    def update_model(self, model, optimizer):\n        model.train()\n        self.model_ema.train()\n        # get memory data\n        sup_data, ages = self.mem.get_memory()\n        l_sup = None\n        if len(sup_data) > 0:\n            sup_data = torch.stack(sup_data)\n            strong_sup_aug = self.transform(sup_data)\n            ema_sup_out = self.model_ema(sup_data)\n            stu_sup_out = model(strong_sup_aug)\n            instance_weight = timeliness_reweighting(ages)\n            l_sup = (softmax_entropy(stu_sup_out, ema_sup_out) * instance_weight).mean()\n\n        l = l_sup\n        if l is not None:\n            optimizer.zero_grad()\n            l.backward()\n            optimizer.step()\n\n        self.update_ema_variables(self.model_ema, self.model, self.nu)\n\n    @staticmethod\n    def update_ema_variables(ema_model, model, nu):\n        for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n            ema_param.data[:] = (1 - nu) * ema_param[:].data[:] + nu * param[:].data[:]\n        return ema_model\n\n    def configure_model(self, model: nn.Module):\n\n        model.requires_grad_(False)\n        normlayer_names = []\n\n        for name, sub_module in model.named_modules():\n            if isinstance(sub_module, nn.BatchNorm1d) or isinstance(sub_module, nn.BatchNorm2d):\n                normlayer_names.append(name)\n\n        for name in normlayer_names:\n            bn_layer = get_named_submodule(model, name)\n            if isinstance(bn_layer, nn.BatchNorm1d):\n                NewBN = RobustBN1d\n            elif isinstance(bn_layer, nn.BatchNorm2d):\n                NewBN = RobustBN2d\n            else:\n                raise RuntimeError()\n\n            momentum_bn = NewBN(bn_layer,\n                                self.cfg.ADAPTER.RoTTA.ALPHA)\n            momentum_bn.requires_grad_(True)\n            set_named_submodule(model, name, momentum_bn)\n        return model\n\ndef timeliness_reweighting(ages):\n    if isinstance(ages, list):\n        ages = torch.tensor(ages).float().cuda()\n    return torch.exp(-ages) / (1 + torch.exp(-ages))\n\n@torch.jit.script\ndef softmax_entropy(x, x_ema):\n    return -(x_ema.softmax(1) * x.log_softmax(1)).sum(1)\n\nclass MomentumBN(nn.Module):\n    def __init__(self, bn_layer: nn.BatchNorm2d, momentum):\n        super().__init__()\n        self.num_features = bn_layer.num_features\n        self.momentum = momentum\n        if bn_layer.track_running_stats and bn_layer.running_var is not None and bn_layer.running_mean is not None:\n            self.register_buffer(\"source_mean\", deepcopy(bn_layer.running_mean))\n            self.register_buffer(\"source_var\", deepcopy(bn_layer.running_var))\n            self.source_num = bn_layer.num_batches_tracked\n        self.weight = deepcopy(bn_layer.weight)\n        self.bias = deepcopy(bn_layer.bias)\n\n        self.register_buffer(\"target_mean\", torch.zeros_like(self.source_mean))\n        self.register_buffer(\"target_var\", torch.ones_like(self.source_var))\n        self.eps = bn_layer.eps\n\n        self.current_mu = None\n        self.current_sigma = None\n\n    def forward(self, x):\n        raise NotImplementedError\n\n\nclass RobustBN1d(MomentumBN):\n    def forward(self, x):\n        if self.training:\n            b_var, b_mean = torch.var_mean(x, dim=0, unbiased=False, keepdim=False)\n            mean = (1 - self.momentum) * self.source_mean + self.momentum * b_mean\n            var = (1 - self.momentum) * self.source_var + self.momentum * b_var\n            self.source_mean, self.source_var = deepcopy(mean.detach()), deepcopy(var.detach())\n            mean, var = mean.view(1, -1), var.view(1, -1)\n        else:\n            mean, var = self.source_mean.view(1, -1), self.source_var.view(1, -1)\n\n        x = (x - mean) / torch.sqrt(var + self.eps)\n        weight = self.weight.view(1, -1)\n        bias = self.bias.view(1, -1)\n\n        return x * weight + bias\n\n\nclass RobustBN2d(MomentumBN):\n    def forward(self, x):\n        if self.training:\n            b_var, b_mean = torch.var_mean(x, dim=[0, 2, 3], unbiased=False, keepdim=False)\n            mean = (1 - self.momentum) * self.source_mean + self.momentum * b_mean\n            var = (1 - self.momentum) * self.source_var + self.momentum * b_var\n            self.source_mean, self.source_var = deepcopy(mean.detach()), deepcopy(var.detach())\n            mean, var = mean.view(1, -1, 1, 1), var.view(1, -1, 1, 1)\n        else:\n            mean, var = self.source_mean.view(1, -1, 1, 1), self.source_var.view(1, -1, 1, 1)\n\n        x = (x - mean) / torch.sqrt(var + self.eps)\n        weight = self.weight.view(1, -1, 1, 1)\n        bias = self.bias.view(1, -1, 1, 1)\n\n        return x * weight + bias\n\nclass MemoryItem:\n    def __init__(self, data=None, uncertainty=0, age=0):\n        self.data = data\n        self.uncertainty = uncertainty\n        self.age = age\n\n    def increase_age(self):\n        if not self.empty():\n            self.age += 1\n\n    def get_data(self):\n        return self.data, self.uncertainty, self.age\n\n    def empty(self):\n        return self.data == \"empty\"\n\n\nclass CSTU:\n    def __init__(self, capacity, num_class, lambda_t=1.0, lambda_u=1.0):\n        self.capacity = capacity\n        self.num_class = num_class\n        self.per_class = self.capacity / self.num_class\n        self.lambda_t = lambda_t\n        self.lambda_u = lambda_u\n\n        self.data: list[list[MemoryItem]] = [[] for _ in range(self.num_class)]\n\n    def get_occupancy(self):\n        occupancy = 0\n        for data_per_cls in self.data:\n            occupancy += len(data_per_cls)\n        return occupancy\n\n    def per_class_dist(self):\n        per_class_occupied = [0] * self.num_class\n        for cls, class_list in enumerate(self.data):\n            per_class_occupied[cls] = len(class_list)\n\n        return per_class_occupied\n\n    def add_instance(self, instance):\n        assert (len(instance) == 3)\n        x, prediction, uncertainty = instance\n        new_item = MemoryItem(data=x, uncertainty=uncertainty, age=0)\n        new_score = self.heuristic_score(0, uncertainty)\n        if self.remove_instance(prediction, new_score):\n            self.data[prediction].append(new_item)\n        self.add_age()\n\n    def remove_instance(self, cls, score):\n        class_list = self.data[cls]\n        class_occupied = len(class_list)\n        all_occupancy = self.get_occupancy()\n        if class_occupied < self.per_class:\n            if all_occupancy < self.capacity:\n                return True\n            else:\n                majority_classes = self.get_majority_classes()\n                return self.remove_from_classes(majority_classes, score)\n        else:\n            return self.remove_from_classes([cls], score)\n\n    def remove_from_classes(self, classes: list[int], score_base):\n        max_class = None\n        max_index = None\n        max_score = None\n        for cls in classes:\n            for idx, item in enumerate(self.data[cls]):\n                uncertainty = item.uncertainty\n                age = item.age\n                score = self.heuristic_score(age=age, uncertainty=uncertainty)\n                if max_score is None or score >= max_score:\n                    max_score = score\n                    max_index = idx\n                    max_class = cls\n\n        if max_class is not None:\n            if max_score > score_base:\n                self.data[max_class].pop(max_index)\n                return True\n            else:\n                return False\n        else:\n            return True\n\n    def get_majority_classes(self):\n        per_class_dist = self.per_class_dist()\n        max_occupied = max(per_class_dist)\n        classes = []\n        for i, occupied in enumerate(per_class_dist):\n            if occupied == max_occupied:\n                classes.append(i)\n\n        return classes\n\n    def heuristic_score(self, age, uncertainty):\n        return self.lambda_t * 1 / (1 + math.exp(-age / self.capacity)) + self.lambda_u * uncertainty / math.log(self.num_class)\n\n    def add_age(self):\n        for class_list in self.data:\n            for item in class_list:\n                item.increase_age()\n        return\n\n    def get_memory(self):\n        tmp_data = []\n        tmp_age = []\n\n        for class_list in self.data:\n            for item in class_list:\n                tmp_data.append(item.data)\n                tmp_age.append(item.age)\n\n        tmp_age = [x / self.capacity for x in tmp_age]\n\n        return tmp_data, tmp_age\n\nclass LabelDirichletDomainSequence(Sampler):\n    def __init__(self, data_source: List[DatumBase], gamma, batch_size, slots=None):\n\n        self.domain_dict = defaultdict(list)\n        self.classes = set()\n        for i, item in enumerate(data_source):\n            self.domain_dict[item.domain].append(i)\n            self.classes.add(item.label)\n        self.domains = list(self.domain_dict.keys())\n        self.domains.sort()\n\n        self.data_source = data_source\n        self.gamma = gamma\n        self.batch_size = batch_size\n        self.num_class = len(self.classes)\n        if slots is not None:\n            self.num_slots = slots\n        else:\n            self.num_slots = self.num_class if self.num_class <= 100 else 100\n\n    def __len__(self):\n        return len(self.data_source)\n\n    def __iter__(self):\n        final_indices = []\n        for domain in self.domains:\n            indices = np.array(self.domain_dict[domain])\n            labels = np.array([self.data_source[i].label for i in indices])\n\n            class_indices = [np.argwhere(labels == y).flatten() for y in range(self.num_class)]\n            slot_indices = [[] for _ in range(self.num_slots)]\n\n            label_distribution = dirichlet([self.gamma] * self.num_slots, self.num_class)\n\n            for c_ids, partition in zip(class_indices, label_distribution):\n                for s, ids in enumerate(np.split(c_ids, (np.cumsum(partition)[:-1] * len(c_ids)).astype(int))):\n                    slot_indices[s].append(ids)\n\n            for s_ids in slot_indices:\n                permutation = np.random.permutation(range(len(s_ids)))\n                ids = []\n                for i in permutation:\n                    ids.extend(s_ids[i])\n                final_indices.extend(indices[ids])\n\n        return iter(final_indices)",
        "experimental_info": "RoTTA Configuration:\n- Memory Size (`ADAPTER.RoTTA.MEMORY_SIZE`): 64\n- Update Frequency (`ADAPTER.RoTTA.UPDATE_FREQUENCY`): 64\n- EMA Update Rate (`ADAPTER.RoTTA.NU`): 0.001\n- RBN Momentum (`ADAPTER.RoTTA.ALPHA`): 0.05\n- Timeliness Weight (`ADAPTER.RoTTA.LAMBDA_T`): 1.0\n- Uncertainty Weight (`ADAPTER.RoTTA.LAMBDA_U`): 1.0\n\nCorruption/Dataset Settings:\n- Dataset (`CORRUPTION.DATASET`): 'cifar10' (or 'cifar100')\n- Corruption Types (`CORRUPTION.TYPE`): ['gaussian_noise', 'shot_noise', 'impulse_noise', 'defocus_blur', 'glass_blur', 'motion_blur', 'zoom_blur', 'snow', 'frost', 'fog', 'brightness', 'contrast', 'elastic_transform', 'pixelate', 'jpeg_compression']\n- Corruption Severities (`CORRUPTION.SEVERITY`): [5, 4, 3, 2, 1]\n- Number of Examples per corruption (`CORRUPTION.NUM_EX`): 10000\n- Number of Classes (`CORRUPTION.NUM_CLASS`): -1 (determined by dataset)\n\nOptimization Settings:\n- Adaptation Steps per Batch (`OPTIM.STEPS`): 1\n- Learning Rate (`OPTIM.LR`): 1e-3\n- Optimizer Method (`OPTIM.METHOD`): 'Adam' (or 'SGD')\n\nLoader/Sampling Settings:\n- Sampler Type (`LOADER.SAMPLER.TYPE`): 'temporal' (corresponds to LabelDirichletDomainSequence for correlative sampling)\n- Dirichlet Gamma (`LOADER.SAMPLER.GAMMA`): 0.1 (for correlative sampling)\n- Test Batch Size (`TEST.BATCH_SIZE`): 64\n\nData Augmentations (Strong Augmentations for Student Model Training on Memory Bank):\n- Transforms applied by `get_tta_transforms` include:\n    - Clipping values to [0.0, 1.0]\n    - `ColorJitterPro` for brightness (range [0.6, 1.4]), contrast ([0.7, 1.3]), saturation ([0.5, 1.5]), hue ([-0.06, 0.06]), and gamma ([0.7, 1.3])\n    - Padding (half image pixels, edge mode)\n    - Random Affine transformations (degrees [-15, 15], translate (1/16, 1/16), scale (0.9, 1.1))\n    - Gaussian Blur (kernel_size=5, sigma [0.001, 0.5])\n    - Center Crop\n    - Random Horizontal Flip (probability p=0.5)\n    - Gaussian Noise (mean=0, std=0.005)"
      }
    },
    {
      "title": "Test Time Adaptation With Regularized Loss for Weakly Supervised Salient Object Detection"
    },
    {
      "title": "Tent: Fully Test-Time Adaptation by Entropy Minimization",
      "abstract": "A model must adapt itself to generalize to new and different data during\ntesting. In this setting of fully test-time adaptation the model has only the\ntest data and its own parameters. We propose to adapt by test entropy\nminimization (tent): we optimize the model for confidence as measured by the\nentropy of its predictions. Our method estimates normalization statistics and\noptimizes channel-wise affine transformations to update online on each batch.\nTent reduces generalization error for image classification on corrupted\nImageNet and CIFAR-10/100 and reaches a new state-of-the-art error on\nImageNet-C. Tent handles source-free domain adaptation on digit recognition\nfrom SVHN to MNIST/MNIST-M/USPS, on semantic segmentation from GTA to\nCityscapes, and on the VisDA-C benchmark. These results are achieved in one\nepoch of test-time optimization without altering training.",
      "full_text": "Published as a conference paper at ICLR 2021 TENT : F ULLY TEST-TIME ADAPTATION BY ENTROPY MINIMIZATION Dequan Wang1∗, Evan Shelhamer2∗†, Shaoteng Liu1, Bruno Olshausen1, Trevor Darrell1 dqwang@cs.berkeley.edu, shelhamer@google.com UC Berkeley1 Adobe Research2 ABSTRACT A model must adapt itself to generalize to new and different data during testing. In this setting of fully test-time adaptation the model has only the test data and its own parameters. We propose to adapt by test entropy minimization (tent 1): we optimize the model for conﬁdence as measured by the entropy of its predictions. Our method estimates normalization statistics and optimizes channel-wise afﬁne transformations to update online on each batch. Tent reduces generalization error for image classiﬁcation on corrupted ImageNet and CIFAR-10/100 and reaches a new state-of-the-art error on ImageNet-C. Tent handles source-free domain adapta- tion on digit recognition from SVHN to MNIST/MNIST-M/USPS, on semantic segmentation from GTA to Cityscapes, and on the VisDA-C benchmark. These results are achieved in one epoch of test-time optimization without altering training. 1 I NTRODUCTION Deep networks can achieve high accuracy on training and testing data from the same distribution, as evidenced by tremendous benchmark progress (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; He et al., 2016). However, generalization to new and different data is limited (Hendrycks & Dietterich, 2019; Recht et al., 2019; Geirhos et al., 2018). Accuracy suffers when the training (source) data differ from the testing (target) data, a condition known as dataset shift(Quionero-Candela et al., 2009). Models can be sensitive to shifts during testing that were not known during training, whether natural variations or corruptions, such as unexpected weather or sensor degradation. Nevertheless, it can be necessary to deploy a model on different data distributions, so adaptation is needed. During testing, the model must adapt given only its parameters and the target data. Thisfully test-time adaptation setting cannot rely on source data or supervision. Neither is practical when the model ﬁrst encounters new testing data, before it can be collected and annotated, as inference must go on. Real-world usage motivates fully test-time adaptation by data, computation, and task needs: 1. Availability. A model might be distributed without source data for bandwidth, privacy, or proﬁt. 2. Efﬁciency. It might not be computationally practical to (re-)process source data during testing. 3. Accuracy. A model might be too inaccurate without adaptation to serve its purpose. To adapt during testing we minimize the entropy of model predictions. We call this objective the test entropy and name our method tent after it. We choose entropy for its connections to error and shift. Entropy is related to error, as more conﬁdent predictions are all-in-all more correct (Figure 1). Entropy is related to shifts due to corruption, as more corruption results in more entropy, with a strong rank correlation to the loss for image classiﬁcation as the level of corruption increases (Figure 2). To minimize entropy, tent normalizes and transforms inference on target data by estimating statistics and optimizing afﬁne parameters batch-by-batch. This choice of low-dimensional, channel-wise feature modulation is efﬁcient to adapt during testing, even for online updates. Tent does not restrict or alter model training: it is independent of the source data given the model parameters. If the model can be run, it can be adapted. Most importantly, tent effectively reduces not just entropy but error. ∗Equal contribution. †Work done at Adobe Research; the author is now at DeepMind. 1Please see the project page at https://github.com/DequanWang/tent for the code and more. 1 arXiv:2006.10726v3  [cs.LG]  18 Mar 2021Published as a conference paper at ICLR 2021 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Entropy 0 20 40 60 80Error (%) Figure 1: Predictions with lower entropy have lower error rates on corrupted CIFAR-100-C. Certainty can serve as supervision during testing. 0.2 0.3 0.4 0.5 0.6 Entropy 0.2 0.4 0.6 0.8 1.0 1.2Loss = 0.61 original noise blur digital weather  level level Figure 2: More corruption causes more loss and entropy on CIFAR-100-C. Entropy can estimate the degree of shift without training data or labels. Our results evaluate generalization to corruptions for image classiﬁcation, to domain shift for digit recognition, and to simulation-to-real shift for semantic segmentation. For context with more data and optimization, we evaluate methods for robust training, domain adaptation, and self-supervised learning given the labeled source data. Tent can achieve less error given only the target data, and it improves on the state-of-the-art for the ImageNet-C benchmark. Analysis experiments support our entropy objective, check sensitivity to the amount of data and the choice of parameters for adaptation, and back the generality of tent across architectures. Our contributions • We highlight the setting of fully test-time adaptation with only target data and no source data. To emphasize practical adaptation during inference we benchmark with ofﬂine and online updates. • We examine entropy as an adaptation objective and propose tent: a test-time entropy minimization scheme to reduce generalization error by reducing the entropy of model predictions on test data. • For robustness to corruptions, tent reaches 44.0% error on ImageNet-C, better than the state-of- the-art for robust training (50.2%) and the strong baseline of test-time normalization (49.9%). • For domain adaptation, tent is capable of online and source-free adaptation for digit classiﬁcation and semantic segmentation, and can even rival methods that use source data and more optimization. 2 S ETTING : F ULLY TEST-TIME ADAPTATION Adaptation addresses generalization from source to target. A model fθ(x) with parameters θtrained on source data and labels xs,ys may not generalize when tested on shifted target data xt. Table 1 summarizes adaptation settings, their required data, and types of losses. Our fully test-time adaptation setting uniquely requires only the model fθ and unlabeled target data xt for adaptation during inference. Existing adaptation settings extend training given more data and supervision. Transfer learning by ﬁne-tuning (Donahue et al., 2014; Yosinski et al., 2014) needs target labels to (re-)train with a supervised loss L(xt,yt). Without target labels, our setting denies this supervised training. Domain adaptation (DA) (Quionero-Candela et al., 2009; Saenko et al., 2010; Ganin & Lempitsky, 2015; Tzeng et al., 2015) needs both the source and target data to train with a cross-domain loss L(xs,xt). Test-time training (TTT) (Sun et al., 2019b) adapts during testing but ﬁrst alters training to jointly optimize its supervised loss L(xs,ys) and self-supervised loss L(xs). Without source, our setting denies joint training across domains (DA) or losses (TTT). Existing settings have their purposes, but do not cover all practical cases when source, target, or supervision are not simultaneously available. Unexpected target data during testing requires test-time adaptation. TTT and our setting adapt the model by optimizing an unsupervised loss during testing L(xt). During training, TTT jointly optimizes this same loss on source data L(xs) with a supervised loss L(xs,ys), to ensure the parameters θare shared across losses for compatibility with adaptation by L(xt). Fully test-time adaptation is independent of the training data and training loss given the parameters θ. By not changing training, our setting has the potential to require less data and computation for adaptation. 2Published as a conference paper at ICLR 2021 Table 1: Adaptation settings differ by their data and therefore losses during training and testing. Of the source s and target t data xand labels y, our fully test-time setting only needs the target data xt. setting source data target data train loss test loss ﬁne-tuning - xt,yt L(xt,yt) - domain adaptation xs, ys xt L(xs,ys) + L(xs,xt) - test-time training xs, ys xt L(xs,ys) + L(xs) L(xt) fully test-time adaptation - xt - L(xt)     = f (     ; θ)  Loss (   ,      ) θ (a) training <latexit sha1_base64=\"YDIW6Mi/jc4gnv843zXedRTilJU=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49mF654lbdGcgy8XJSgRyNXvmr249ZGnGFTFJjOp6boJ9RjYJJPil1U8MTykZ0wDuWKhpx42ezUyfkxCp9EsbalkIyU39PZDQyZhwFtjOiODSL3lT8z+ukGF74mVBJilyx+aIwlQRjMv2b9IXmDOXYEsq0sLcSNqSaMrTplGwI3uLLy6R1VvVqVc+7qVXql3keRTiCYzgFD86hDtfQgCYwGMAzvMKbI50X5935mLcWnHzmEP7A+fwBBSaOGg==</latexit> x s <latexit sha1_base64=\"1psCF/1OyjuZ4TwBu21voNG0XaI=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8eK1hbaWDbbSbt0swm7GyGE/gQvHhTEq3/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dkorq2vrG+XNytb2zu5edf/gQcepYthisYhVJ6AaBZfYMtwI7CQKaRQIbAfj66nffkKleSzvTZagH9Gh5CFn1FjpLnvU/WrNrbszkGXiFaQGBZr96ldvELM0QmmYoFp3PTcxfk6V4UzgpNJLNSaUjekQu5ZKGqH289mpE3JilQEJY2VLGjJTf0/kNNI6iwLbGVEz0oveVPzP66YmvPRzLpPUoGTzRWEqiInJ9G8y4AqZEZkllClubyVsRBVlxqZTsSF4iy8vk/ZZ3Tuve97tea1xVeRRhiM4hlPw4AIacANNaAGDITzDK7w5wnlx3p2PeWvJKWYO4Q+czx8GrY4b</latexit> y s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"1psCF/1OyjuZ4TwBu21voNG0XaI=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8eK1hbaWDbbSbt0swm7GyGE/gQvHhTEq3/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dkorq2vrG+XNytb2zu5edf/gQcepYthisYhVJ6AaBZfYMtwI7CQKaRQIbAfj66nffkKleSzvTZagH9Gh5CFn1FjpLnvU/WrNrbszkGXiFaQGBZr96ldvELM0QmmYoFp3PTcxfk6V4UzgpNJLNSaUjekQu5ZKGqH289mpE3JilQEJY2VLGjJTf0/kNNI6iwLbGVEz0oveVPzP66YmvPRzLpPUoGTzRWEqiInJ9G8y4AqZEZkllClubyVsRBVlxqZTsSF4iy8vk/ZZ3Tuve97tea1xVeRRhiM4hlPw4AIacANNaAGDITzDK7w5wnlx3p2PeWvJKWYO4Q+czx8GrY4b</latexit> y s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"YDIW6Mi/jc4gnv843zXedRTilJU=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49mF654lbdGcgy8XJSgRyNXvmr249ZGnGFTFJjOp6boJ9RjYJJPil1U8MTykZ0wDuWKhpx42ezUyfkxCp9EsbalkIyU39PZDQyZhwFtjOiODSL3lT8z+ukGF74mVBJilyx+aIwlQRjMv2b9IXmDOXYEsq0sLcSNqSaMrTplGwI3uLLy6R1VvVqVc+7qVXql3keRTiCYzgFD86hDtfQgCYwGMAzvMKbI50X5935mLcWnHzmEP7A+fwBBSaOGg==</latexit> x s (b) fully test-time adaptation θ    = f (    ; θ+Δ)  Entropy (    ) <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t <latexit sha1_base64=\"m/ZzdjACtPk7VVv8qMUMxHkn5f0=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49YK9ccavuDGSZeDmpQI5Gr/zV7ccsjbhCJqkxHc9N0M+oRsEkn5S6qeEJZSM64B1LFY248bPZqRNyYpU+CWNtSyGZqb8nMhoZM44C2xlRHJpFbyr+53VSDC/8TKgkRa7YfFGYSoIxmf5N+kJzhnJsCWVa2FsJG1JNGdp0SjYEb/HlZdI6q3q1qufd1Cr1yzyPIhzBMZyCB+dQh2toQBMYDOAZXuHNkc6L8+58zFsLTj5zCH/gfP4ABquOGw==</latexit> x t <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t <latexit sha1_base64=\"m/ZzdjACtPk7VVv8qMUMxHkn5f0=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49YK9ccavuDGSZeDmpQI5Gr/zV7ccsjbhCJqkxHc9N0M+oRsEkn5S6qeEJZSM64B1LFY248bPZqRNyYpU+CWNtSyGZqb8nMhoZM44C2xlRHJpFbyr+53VSDC/8TKgkRa7YfFGYSoIxmf5N+kJzhnJsCWVa2FsJG1JNGdp0SjYEb/HlZdI6q3q1qufd1Cr1yzyPIhzBMZyCB+dQh2toQBMYDOAZXuHNkc6L8+58zFsLTj5zCH/gfP4ABquOGw==</latexit> x t <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t     = f (     ; θ)  Loss (   ,      ) θ (a) training <latexit sha1_base64=\"YDIW6Mi/jc4gnv843zXedRTilJU=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49mF654lbdGcgy8XJSgRyNXvmr249ZGnGFTFJjOp6boJ9RjYJJPil1U8MTykZ0wDuWKhpx42ezUyfkxCp9EsbalkIyU39PZDQyZhwFtjOiODSL3lT8z+ukGF74mVBJilyx+aIwlQRjMv2b9IXmDOXYEsq0sLcSNqSaMrTplGwI3uLLy6R1VvVqVc+7qVXql3keRTiCYzgFD86hDtfQgCYwGMAzvMKbI50X5935mLcWnHzmEP7A+fwBBSaOGg==</latexit> x s <latexit sha1_base64=\"1psCF/1OyjuZ4TwBu21voNG0XaI=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8eK1hbaWDbbSbt0swm7GyGE/gQvHhTEq3/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dkorq2vrG+XNytb2zu5edf/gQcepYthisYhVJ6AaBZfYMtwI7CQKaRQIbAfj66nffkKleSzvTZagH9Gh5CFn1FjpLnvU/WrNrbszkGXiFaQGBZr96ldvELM0QmmYoFp3PTcxfk6V4UzgpNJLNSaUjekQu5ZKGqH289mpE3JilQEJY2VLGjJTf0/kNNI6iwLbGVEz0oveVPzP66YmvPRzLpPUoGTzRWEqiInJ9G8y4AqZEZkllClubyVsRBVlxqZTsSF4iy8vk/ZZ3Tuve97tea1xVeRRhiM4hlPw4AIacANNaAGDITzDK7w5wnlx3p2PeWvJKWYO4Q+czx8GrY4b</latexit> y s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"1psCF/1OyjuZ4TwBu21voNG0XaI=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8eK1hbaWDbbSbt0swm7GyGE/gQvHhTEq3/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dkorq2vrG+XNytb2zu5edf/gQcepYthisYhVJ6AaBZfYMtwI7CQKaRQIbAfj66nffkKleSzvTZagH9Gh5CFn1FjpLnvU/WrNrbszkGXiFaQGBZr96ldvELM0QmmYoFp3PTcxfk6V4UzgpNJLNSaUjekQu5ZKGqH289mpE3JilQEJY2VLGjJTf0/kNNI6iwLbGVEz0oveVPzP66YmvPRzLpPUoGTzRWEqiInJ9G8y4AqZEZkllClubyVsRBVlxqZTsSF4iy8vk/ZZ3Tuve97tea1xVeRRhiM4hlPw4AIacANNaAGDITzDK7w5wnlx3p2PeWvJKWYO4Q+czx8GrY4b</latexit> y s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"YDIW6Mi/jc4gnv843zXedRTilJU=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49mF654lbdGcgy8XJSgRyNXvmr249ZGnGFTFJjOp6boJ9RjYJJPil1U8MTykZ0wDuWKhpx42ezUyfkxCp9EsbalkIyU39PZDQyZhwFtjOiODSL3lT8z+ukGF74mVBJilyx+aIwlQRjMv2b9IXmDOXYEsq0sLcSNqSaMrTplGwI3uLLy6R1VvVqVc+7qVXql3keRTiCYzgFD86hDtfQgCYwGMAzvMKbI50X5935mLcWnHzmEP7A+fwBBSaOGg==</latexit> x s (b) fully test-time adaptation θ    = f (    ; θ+Δ)  Entropy (    ) <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t <latexit sha1_base64=\"m/ZzdjACtPk7VVv8qMUMxHkn5f0=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49YK9ccavuDGSZeDmpQI5Gr/zV7ccsjbhCJqkxHc9N0M+oRsEkn5S6qeEJZSM64B1LFY248bPZqRNyYpU+CWNtSyGZqb8nMhoZM44C2xlRHJpFbyr+53VSDC/8TKgkRa7YfFGYSoIxmf5N+kJzhnJsCWVa2FsJG1JNGdp0SjYEb/HlZdI6q3q1qufd1Cr1yzyPIhzBMZyCB+dQh2toQBMYDOAZXuHNkc6L8+58zFsLTj5zCH/gfP4ABquOGw==</latexit> x t <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t <latexit sha1_base64=\"m/ZzdjACtPk7VVv8qMUMxHkn5f0=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49YK9ccavuDGSZeDmpQI5Gr/zV7ccsjbhCJqkxHc9N0M+oRsEkn5S6qeEJZSM64B1LFY248bPZqRNyYpU+CWNtSyGZqb8nMhoZM44C2xlRHJpFbyr+53VSDC/8TKgkRa7YfFGYSoIxmf5N+kJzhnJsCWVa2FsJG1JNGdp0SjYEb/HlZdI6q3q1qufd1Cr1yzyPIhzBMZyCB+dQh2toQBMYDOAZXuHNkc6L8+58zFsLTj5zCH/gfP4ABquOGw==</latexit> x t <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t Figure 3: Method overview. Tent does not alter training (a), but minimizes the entropy of predictions during testing (b) over a constrained modulation ∆, given the parameters θand target data xt. 3 M ETHOD : T EST ENTROPY MINIMIZATION VIA FEATURE MODULATION We optimize the model during testing to minimize the entropy of its predictions by modulating its features. We call our method tent for test entropy. Tent requires a compatible model, an objective to minimize (Section 3.1), and parameters to optimize over (Section 3.2) to fully deﬁne the algorithm (Section Section 3.3). Figure 3 outlines our method for fully test-time adaptation. The model to be adapted must be trained for the supervised task, probabilistic, and differentiable. No supervision is provided during testing, so the model must already be trained. Measuring the entropy of predictions requires a distribution over predictions, so the model must be probabilistic. Gradients are required for fast iterative optimization, so the model must be differentiable. Typical deep networks for supervised learning satisfy these model requirements. 3.1 E NTROPY OBJECTIVE Our test-time objective L(xt) is to minimize the entropy H(ˆy) of model predictions ˆy= fθ(xt). In particular, we measure the Shannon entropy (Shannon, 1948), H(ˆy) = −∑ cp(ˆyc) logp(ˆyc) for the probability ˆyc of class c. Note that optimizing a single prediction has a trivial solution: assign all probability to the most probable class. We prevent this by jointly optimizing batched predictions over parameters that are shared across the batch. Entropy is an unsupervised objective because it only depends on predictions and not annotations. However, as a measure of the predictions it is directly related to the supervised task and model. In contrast, proxy tasks for self-supervised learning are not directly related to the supervised task. Proxy tasks derive a self-supervised label y′from the input xt without the task label y. Examples of these proxies include rotation prediction (Gidaris et al., 2018), context prediction (Doersch et al., 2015), and cross-channel auto-encoding (Zhang et al., 2017). Too much progress on a proxy task could interfere with performance on the supervised task, and self-supervised adaptation methods have to limit or mix updates accordingly (Sun et al., 2019b;a). As such, care is needed to choose a proxy compatible with the domain and task, to design the architecture for the proxy model, and to balance optimization between the task and proxy objectives. Our entropy objective does not need such efforts. 3.2 M ODULATION PARAMETERS The model parameters θare a natural choice for test-time optimization, and these are the choice of prior work for train-time entropy minimization (Grandvalet & Bengio, 2005; Dhillon et al., 2020; Carlucci et al., 2017). However, θis the only representation of the training/source data in our setting, and altering θcould cause the model to diverge from its training. Furthermore, f can be nonlinear and θcan be high dimensional, making optimization too sensitive and inefﬁcient for test-time usage. 3Published as a conference paper at ICLR 2021 IN OUT+ <latexit sha1_base64=\"FGMSn1olAms3UkJ+mUM6lRBkJrw=\">AAAB6HicbVDLSgNBEOyNryS+oh69DAZBEMKuKHoMevGYgHlgsoTZSW8yZvbBzKwYlnyBFw+K5OoP+C/e/BqdJB40saChqOqmu8uLBVfatj+tzNLyyupaNpdf39jc2i7s7NZVlEiGNRaJSDY9qlDwEGuaa4HNWCINPIENb3A18Rv3KBWPwhs9jNENaC/kPmdUG6l63CkU7ZI9BVkkzg8plnPx+Pb94avSKXy0uxFLAgw1E1SplmPH2k2p1JwJHOXbicKYsgHtYcvQkAao3HR66IgcGqVL/EiaCjWZqr8nUhooNQw80xlQ3Vfz3kT8z2sl2r9wUx7GicaQzRb5iSA6IpOvSZdLZFoMDaFMcnMrYX0qKdMmm7wJwZl/eZHUT0rOaemsatK4hBmysA8HcAQOnEMZrqECNWCA8AjP8GLdWU/WqzWetWasn5k9+APr7RuTUJCF</latexit> \u0000 <latexit sha1_base64=\"8eHH7cr25vA7s0zJYYCDPQNSaT0=\">AAAB7XicbVDLSgNBEOyNrxhfUY+KDAbBU9gVQb0FvXhMwDwgWcLsZDYZM7OzzMwKYcnRuxcPinj1F/Id3vwGf8LJ46CJBQ1FVTfdXUHMmTau++VklpZXVtey67mNza3tnfzuXk3LRBFaJZJL1QiwppxFtGqY4bQRK4pFwGk96N+M/foDVZrJ6M4MYuoL3I1YyAg2Vqq1ulgI3M4X3KI7AVok3owUSoejyvfj0ajczn+2OpIkgkaGcKx103Nj46dYGUY4HeZaiaYxJn3cpU1LIyyo9tPJtUN0YpUOCqWyFRk0UX9PpFhoPRCB7RTY9PS8Nxb/85qJCS/9lEVxYmhEpovChCMj0fh11GGKEsMHlmCimL0VkR5WmBgbUM6G4M2/vEhqZ0XvvHhVsWlcwxRZOIBjOAUPLqAEt1CGKhC4hyd4gVdHOs/Om/M+bc04s5l9+APn4wd3ypLI</latexit> ⇥ <latexit sha1_base64=\"r9CoIRh1LwyAxszWUWZZpZEIYvU=\">AAAB7XicbVA9TwJBEJ3DL8Av1NLmIjGxIndGoyXRxhIT+YhwIXvLHqzs7V5254yE8B9sLDDG1tL/Yuev0QUsFHzJJC/vzWRmXpgIbtDzPp3M0vLK6lo2l1/f2NzaLuzs1oxKNWVVqoTSjZAYJrhkVeQoWCPRjMShYPWwfznx6/dMG67kDQ4SFsSkK3nEKUEr1VrIY2bahaJX8qZwF4n/Q4rlXDK+fX/4qrQLH62OomnMJFJBjGn6XoLBkGjkVLBRvpUalhDaJ13WtFQSuyQYTq8duYdW6biR0rYkulP198SQxMYM4tB2xgR7Zt6biP95zRSj82DIZZIik3S2KEqFi8qdvO52uGYUxcASQjW3t7q0RzShaAPK2xD8+ZcXSe245J+UTq9tGhcwQxb24QCOwIczKMMVVKAKFO7gEcbw7CjnyXlxXmetGednZg/+wHn7Btf2kwo=</latexit> \u0000 <latexit sha1_base64=\"icKTvSnYuWAwxCN4MXaVcPxJrUE=\">AAAB7HicbVBNS8NAEN34WetX1aMiwSJ4KokI6q3oxWMLpi20oWy2k3bpZhN2J0IJPXr24kERr/6G/g5v/gb/hNuPg7Y+GHi8N8PMvCARXKPjfFlLyyura+u5jfzm1vbObmFvv6bjVDHwWCxi1QioBsEleMhRQCNRQKNAQD3o3479+gMozWN5j4ME/Ih2JQ85o2gkrxUA0nah6JScCexF4s5IsXw0qn4/Ho8q7cJnqxOzNAKJTFCtm66ToJ9RhZwJGOZbqYaEsj7tQtNQSSPQfjY5dmifGqVjh7EyJdGeqL8nMhppPYgC0xlR7Ol5byz+5zVTDK/8jMskRZBsuihMhY2xPf7c7nAFDMXAEMoUN7farEcVZWjyyZsQ3PmXF0ntvORelK6rJo0bMkWOHJITckZccknK5I5UiEcY4eSJvJBXS1rP1pv1Pm1dsmYzB+QPrI8ftLWSVw==</latexit> \u0000 <latexit sha1_base64=\"6pSYsGji0D9Bm0vY9by0e43+pZo=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgxbArAfUW9OIxAfOAZAmzk95kzOzsMjMrhJAv8OJBEa9+kjf/xkmyB00saCiquunuChLBtXHdbye3tr6xuZXfLuzs7u0fFA+PmjpOFcMGi0Ws2gHVKLjEhuFGYDtRSKNAYCsY3c381hMqzWP5YMYJ+hEdSB5yRo2V6he9Ysktu3OQVeJlpAQZar3iV7cfszRCaZigWnc8NzH+hCrDmcBpoZtqTCgb0QF2LJU0Qu1P5odOyZlV+iSMlS1pyFz9PTGhkdbjKLCdETVDvezNxP+8TmrCa3/CZZIalGyxKEwFMTGZfU36XCEzYmwJZYrbWwkbUkWZsdkUbAje8surpHlZ9irlm3qlVL3N4sjDCZzCOXhwBVW4hxo0gAHCM7zCm/PovDjvzseiNedkM8fwB87nD3htjL0=</latexit> ÷ <latexit sha1_base64=\"KLNiQjydwC+UjsLtIanox9T+rq8=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoN6KXjxWsB/QhrLZbNqlu5uwuymU0L/gxYMiXv1D3vw3btoctPXBwOO9GWbmBQln2rjut1Pa2Nza3invVvb2Dw6PqscnHR2nitA2iXmsegHWlDNJ24YZTnuJolgEnHaDyX3ud6dUaRbLJzNLqC/wSLKIEWxyaRCy6bBac+vuAmideAWpQYHWsPo1CGOSCioN4Vjrvucmxs+wMoxwOq8MUk0TTCZ4RPuWSiyo9rPFrXN0YZUQRbGyJQ1aqL8nMiy0nonAdgpsxnrVy8X/vH5qohs/YzJJDZVkuShKOTIxyh9HIVOUGD6zBBPF7K2IjLHCxNh4KjYEb/XlddK5qnuN+u1jo9a8K+IowxmcwyV4cA1NeIAWtIHAGJ7hFd4c4bw4787HsrXkFDOn8AfO5w8aWY5N</latexit> µ <latexit sha1_base64=\"lbHwl5bkUbenc+Yo+u8yNzpxsy0=\">AAAB6nicbVDLSgNBEOyNrxhfUY+KDAbBU9gVQb0FvXhM0DwgWcLsZDYZMjO7zMwKYcnRoxcPinj1I/Id3vwGf8LJ46CJBQ1FVTfdXUHMmTau++VklpZXVtey67mNza3tnfzuXk1HiSK0SiIeqUaANeVM0qphhtNGrCgWAaf1oH8z9usPVGkWyXsziKkvcFeykBFsrHTXEkk7X3CL7gRokXgzUigdjirfj0ejcjv/2epEJBFUGsKx1k3PjY2fYmUY4XSYayWaxpj0cZc2LZVYUO2nk1OH6MQqHRRGypY0aKL+nkix0HogAtspsOnpeW8s/uc1ExNe+imTcWKoJNNFYcKRidD4b9RhihLDB5Zgopi9FZEeVpgYm07OhuDNv7xIamdF77x4VbFpXMMUWTiAYzgFDy6gBLdQhioQ6MITvMCrw51n5815n7ZmnNnMPvyB8/EDTj2RiQ==</latexit> \u0000 <latexit sha1_base64=\"xnrzB72KzfqBMQ17s1zlsxQWR+k=\">AAAB7XicbZDLSgMxFIbP1Fsdb1WXboJFcFVmRFAXYtGNywr2Au1QMmmmjU0yQ5IRytB3cONCETcufBT3bsS3Mb0stPWHwMf/n0POOWHCmTae9+3kFhaXllfyq+7a+sbmVmF7p6bjVBFaJTGPVSPEmnImadUww2kjURSLkNN62L8a5fV7qjSL5a0ZJDQQuCtZxAg21qq1NOsK3C4UvZI3FpoHfwrFiw/3PHn7civtwmerE5NUUGkIx1o3fS8xQYaVYYTTodtKNU0w6eMubVqUWFAdZONph+jAOh0Uxco+adDY/d2RYaH1QIS2UmDT07PZyPwva6YmOg0yJpPUUEkmH0UpRyZGo9VRhylKDB9YwEQxOysiPawwMfZArj2CP7vyPNSOSv5x6ezGK5YvYaI87ME+HIIPJ1CGa6hAFQjcwQM8wbMTO4/Oi/M6Kc05055d+CPn/Qf/xpJs</latexit> <latexit sha1_base64=\"9MzbukliF0G5U4WyINCTJmMNjA8=\">AAACNnicdVBNS8NAFNz4bf2KevSyWAQFLUlR9CiK4EWoYFuhiWWz3dSlu0nYfVFL6K/y4u/w1osHRbz6E9y0PWjVgYVhZh773gSJ4Bocp29NTE5Nz8zOzRcWFpeWV+zVtZqOU0VZlcYiVtcB0UzwiFWBg2DXiWJEBoLVg85p7tfvmNI8jq6gmzBfknbEQ04JGKlpX3gyxZ5gIRCl4nvsSQK3QZCd9RoPTfB3sad5W5Kb8j+h7Xx+D5vszk3Zb9pFp+QMgH8Td0SKaIRK0372WjFNJYuACqJ1w3US8DOigFPBegUv1SwhtEParGFoRCTTfjY4u4e3jNLCYazMiwAP1O8TGZFad2VgkvnCetzLxb+8RgrhkZ/xKEmBRXT4UZgKDDHOO8QtrhgF0TWEUMXNrpjeEkUomKYLpgR3/OTfpFYuuQcl53K/eHwyqmMObaBNtI1cdIiO0TmqoCqi6BH10St6s56sF+vd+hhGJ6zRzDr6AevzC4nRq7w=</latexit> µ  E [ x t ] , \u0000 2  E [( µ \u0000 x t ) 2 ] <latexit sha1_base64=\"5uCFLjsyhVlotMr43Rw1BdZFk0s=\">AAACYXicbZFLS+RAFIUrGZ/tK+Ms3RQ2gqC0iSgzy2bcuHTAVqHTNDfVN21hVRKqbmamCf0nZzcbN/4RKzH4vlBw+O659TiVFEpaCsP/nv9lYXFpeWW1s7a+sbkVfN2+snlpBA5ErnJzk4BFJTMckCSFN4VB0InC6+TurO5f/0ZjZZ5d0qzAkYZpJlMpgBwaB3/jKWgNPFaYEhiT/+EtOeBxAYYkqFgD3UqqzudHL6wxHfI4QXo73YBPh59RbRkH3bAXNsU/iqgVXdbWxTj4F09yUWrMSCiwdhiFBY2qekuhcN6JS4sFiDuY4tDJDDTaUdUkNOd7jkx4mhu3MuINfT1RgbZ2phPnrO9r3/dq+FlvWFL6Y1TJrCgJM/F0UFoqTjmv4+YTaVCQmjkBwkh3Vy5uwYAg9ykdF0L0/skfxdVxLzrthb9Ouv2fbRwrbIftsn0Wse+sz87ZBRswwe69BW/D2/Qe/FU/8LefrL7Xznxjb8rfeQSpH7dZ</latexit> \u0000  \u0000 + @ H / @\u0000 , \u0000  \u0000 + @ H / @\u0000 normalization transformation Figure 4: Tent modulates features during testing by estimating normalization statistics µ,σ and optimizing transformation parameters γ,β. Normalization and transformation apply channel-wise scales and shifts to the features. The statistics and parameters are updated on target data without use of source data. In practice, adapting γ,β is efﬁcient because they make up <1% of model parameters. For stability and efﬁciency, we instead only update feature modulations that are linear (scales and shifts), and low-dimensional (channel-wise). Figure 4 shows the two steps of our modulations: normalization by statistics and transformation by parameters. Normalization centers and standardizes the input xinto ¯x= (x−µ)/σby its mean µand standard deviation σ. Transformation turns ¯xinto the output x′= γ¯x+ βby afﬁne parameters for scale γand shift β. Note that the statistics µ,σ are estimated from the data while the parameters γ,β are optimized by the loss. For implementation, we simply repurpose the normalization layers of the source model. We update their normalization statistics and afﬁne parameters for all layers and channels during testing. 3.3 A LGORITHM Initialization The optimizer collects the afﬁne transformation parameters {γl,k,βl,k}for each normalization layer land channel kin the source model. The remaining parameters θ\\{γl,k,βl,k} are ﬁxed. The normalization statistics {µl,k,σl,k}from the source data are discarded. Iteration Each step updates the normalization statistics and transformation parameters on a batch of data. The normalization statistics are estimated for each layer in turn, during the forward pass. The transformation parameters γ,β are updated by the gradient of the prediction entropy ∇H(ˆy), during the backward pass. Note that the transformation update follows the prediction for the current batch, and so it only affects the next batch (unless forward is repeated). This needs just one gradient per point of additional computation, so we use this scheme by default for efﬁciency. Termination For online adaptation, no termination is necessary, and iteration continues as long as there is test data. For ofﬂine adaptation, the model is ﬁrst updated and then inference is repeated. Adaptation may of course continue by updating for multiple epochs. 4 E XPERIMENTS We evaluate tent for corruption robustness on CIFAR-10/CIFAR-100 and ImageNet, and for domain adaptation on digit adaptation from SVHN to MNIST/MNIST-M/USPS. Our implementation is in PyTorch (Paszke et al., 2019) with the pycls library (Radosavovic et al., 2019). Datasets We run on image classiﬁcation datasets for corruption and domain adaptation conditions. For large-scale experiments we choose ImageNet (Russakovsky et al., 2015), with 1,000 classes, a training set of 1.2 million, and a validation set of 50,000. For experiments at an accessible scale we choose CIFAR-10/CIFAR-100 (Krizhevsky, 2009), with 10/100 classes, a training set of 50,000, and a test set of 10,000. For domain adaptation we choose SVHN (Netzer et al., 2011) as source and MNIST (LeCun et al., 1998)/MNIST-M (Ganin & Lempitsky, 2015)/USPS (Hull, 1994) as targets, with ten classes for the digits 0–9. SVHN has color images of house numbers from street views with a training set of 73,257 and test set of 26,032. MNIST/MNIST-M/USPS have handwritten digits with a training sets of 60,000/60,000/7,291 and test sets of 10,000/10,000/2,007. Models For corruption we use residual networks (He et al., 2016) with 26 layers (R-26) on CIFAR- 10/100 and 50 layers (R-50) on ImageNet. For domain adaptation we use the R-26 architecture. For fair comparison, all methods in each experimental condition share the same architecture. Our networks are equipped with batch normalization (Ioffe & Szegedy, 2015). For the source model without adaptation, the normalization statistics are estimated during training on the source data. For all test-time adaptation methods, we estimate these statistics during testing on the target data, as done in concurrent work on adaptation by normalization (Schneider et al., 2020; Nado et al., 2020). 4Published as a conference paper at ICLR 2021 Table 2: Corruption benchmark on CIFAR-10-C and CIFAR-100-C for the highest severity. Tent has least error, with less optimization than domain adaptation (RG, UDA-SS) and test-time training (TTT), and improves on test-time norm (BN). Method Source Target Error (%) C10-C C100-C Source train 40.8 67.2 RG train train 18.3 38.9 UDA-SS train train 16.7 47.0 TTT train test 17.5 45.0 BN test 17.3 42.6 PL test 15.7 41.2 Tent (ours) test 14.3 37.3 originalgaussshot impulsedefocus glassmotionzoomsnowfrostfog bright contrastelasticpixeljpeg 0 25 50 75Error (%) source 59.5% norm 49.9% tent 44.0% ANT 50.2% Figure 5: Corruption benchmark on ImageNet-C: error for each type averaged over severity levels. Tent improves on the prior state-of-the-art, adver- sarial noise training (Rusak et al., 2020), by fully test-time adaptation without altering training. Optimization We optimize the modulation parameters γ,β following the training hyperparameters for the source model with few changes. On ImageNet we optimize by SGD with momentum; on other datasets we optimize by Adam (Kingma & Ba, 2015). We lower the batch size (BS) to reduce memory usage for inference, then lower the learning rate (LR) by the same factor to compensate (Goyal et al., 2017). On ImageNet, we set BS = 64 and LR = 0.00025, and on other datasets we set BS = 128 and LR = 0.001.We control for ordering by shufﬂing and sharing the order across methods. Baselines We compare to domain adaptation, self-supervision, normalization, and pseudo-labeling: • source applies the trained classiﬁer to the test data without adaptation, • adversarial domain adaptation (RG) reverses the gradients of a domain classiﬁer on source and target to optimize for a domain-invariant representation (Ganin & Lempitsky, 2015), • self-supervised domain adaptation (UDA-SS) jointly trains self-supervised rotation and position tasks on source and target to optimize for a shared representation (Sun et al., 2019a), • test-time training (TTT) jointly trains for supervised and self-supervised tasks on source, then keeps training the self-supervised task on target during testing (Sun et al., 2019b), • test-time normalization (BN) updates batch normalization statistics (Ioffe & Szegedy, 2015) on the target data during testing (Schneider et al., 2020; Nado et al., 2020), • pseudo-labeling (PL) tunes a conﬁdence threshold, assigns predictions over the threshold as labels, and then optimizes the model to these pseudo-labels before testing (Lee, 2013). Only test-time normalization (BN), pseudo-labeling (PL), and tent (ours) are fully test-time adaptation methods. See Section 2 for an explanation and contrast with domain adaptation and test-time training. 4.1 R OBUSTNESS TO CORRUPTIONS To benchmark robustness to corruption, we make use of common image corruptions (see Appendix A for examples). The CIFAR-10/100 and ImageNet datasets are turned into the CIFAR-10/100-C and ImageNet-C corruption benchmarks by duplicating their test/validation sets and applying 15 types of corruptions at ﬁve severity levels (Hendrycks & Dietterich, 2019). Tent improves more with less data and computation.Table 2 reports errors averaged over corrup- tion types at the severest level of corruption. On CIFAR-10/100-C we compare all methods, including those that require joint training across domains or losses, given the convenient sizes of these datasets. Adaptation is ofﬂine for fair comparison with ofﬂine baselines. Tent improves on the fully test-time adaptation baselines (BN, PL) but also the domain adaptation (RG, UDA-SS) and test-time training (TTT) methods that need several epochs of optimization on source and target. Tent consistently improves across corruption types.Figure 5 plots the error for each corruption type averaged over corruption levels on ImageNet-C. We compare the most efﬁcient methods—source, normalization, and tent—given the large scale of the source data (>1 million images) needed by other methods and the 75 target combinations of corruption types and levels. Tent and BN adapt online to rival the efﬁciency of inference without adaptation. Tent reaches the least error for most corruption types without increasing the error on the original data. 5Published as a conference paper at ICLR 2021 Table 3: Digit domain adaptation from SVHN to MNIST/MNIST-M/USPS. Source-free adaptation is not only feasible, but more efﬁcient. Tent always improves on normalization (BN), and in 2/3 cases achieves less error than domain adaptation (RG, UDA-SS) without joint training on source & target. Method Source Target Epochs Error (%) Source + Target MNIST MNIST-M USPS Source train - 18.2 39.7 19.3 RG train train 10 + 10 15.0 33.4 18.9 UDA-SS train train 10 + 10 11.1 22.2 18.4 BN test 0 + 1 15.7 39.7 18.0 Tent (ours) test 0 + 1 10.0 37.0 16.3 Tent (ours) test 0 + 10 8.2 36.8 14.4 Tent reaches a new state-of-the-art without altering training.The state-of-the-art methods for robustness extend training with adversarial noise (ANT) (Rusak et al., 2020) for 50.2% error or mixtures of data augmentations (AugMix) (Hendrycks et al., 2020) for 51.7% error. Combined with stylization from external images (SIN) (Geirhos et al., 2019), ANT+SIN reaches 47.4%. Tent reaches a new state-of-the-art of 44.0% by online adaptation and 42.3% by ofﬂine adaptation. It improves on ANT for all types except noise, on which ANT is trained. This requires just one gradient per test point, without more optimization on the training set (ANT, AugMix) or use of external images (SIN). Among fully test-time adaptation methods, tent reduces the error beyond test-time normalization for 18% relative improvement. In concurrent work, Schneider et al. (2020) report 49.3% error for test-time normalization, for which tent still gives 14% relative improvement. 4.2 S OURCE -FREE DOMAIN ADAPTATION We benchmark digit adaptation (Ganin & Lempitsky, 2015; Tzeng et al., 2015; 2017; Shu et al., 2018) for shifts from SVHN to MNIST/MNIST-M/USPS. Recall that unsupervised domain adaptation makes use the labeled source data and unlabeled target data, while our fully test-time adaptation setting denies use of source data. Adaptation is ofﬂine for fair comparison with ofﬂine baselines. Tent adapts to target without source.Table 3 reports the target errors for domain adaptation and fully test-time adaptation methods. Test-time normalization (BN) marginally improves, while adversarial domain adaptation (RG) and self-supervised domain adaptation (UDA-SS) improve more by joint training on source and target. Tent always has lower error than the source model and BN, and it achieves the lowest error in 2/3 cases, even in just one epoch and without use of source data. While encouraging for fully test-time adaptation, unsupervised domain adaptation remains necessary for the highest accuracy and harder shifts. For SVHN-to-MNIST, DIRT-T (Shu et al., 2018) achieves a remarkable 0.6% error 2. For MNIST-to-SVHN, a difﬁcult shift with source-only error of 71.3%, DIRT-T reaches45.5% and UDA-SS reaches 38.7%. Tent fails on this shift and increases error to 79.8%. In this case success presently requires joint optimization over source and target. Tent needs less computation, but still improves with more.Tent adapts efﬁciently on target data alone with just one gradient per point. RG & UDA-SS also use the source data (SVHN train), which is ∼7×the size of the target data (MNIST test), and optimize for 10 epochs. Tent adapts with ∼80× less computation. With more updates, tent reaches 8.2% error in 10 epochs and 6.5% in 100 epochs. With online updates, tent reaches 12.5% error in one epoch and 8.4% error in 10 epochs. Tent scales to semantic segmentation.To show scalability to large models and inputs, we evaluate semantic segmentation (pixel-wise classiﬁcation) on a domain shift from a simulated source to a real target. The source is GTA (Richter et al., 2017), a video game in an urban environment, and the target is Cityscapes (Cordts et al., 2016), an urban autonomous driving dataset. The model is HRNet-W18, a fully convolutional network (Shelhamer et al., 2017) with high-resolution architecture (Wang et al., 2020). The target intersection-over-union scores (higher is better) are source 28.8%, BN 31.4%, and tent 35.8% with ofﬂine optimization by Adam. For adaptation to a single image, tent reaches 36.4% in 10 iterations with episodic optimization. See the appendix for a qualitative example (Appendix B). 2We exclude DIRT-T from our experiments because of incomparable differences in architecture and model selection. DIRT-T tunes with labeled target data, but we do not. Please refer to Shu et al. (2018) for more detail. 6Published as a conference paper at ICLR 2021 Figure 6: Tent reduces the entropy and loss. We plot changes in entropy∆Hand loss ∆Lfor all of CIFAR-100-C. Change in entropy rank-correlates with change in loss: note the dark diagonal and the rank correlation coefﬁcient of 0.22. (a) Source (b) BN  (c) Tent (d) Oracle  Figure 7: Adapted features on CIFAR-100-C with Gaussian noise (front) and reference features without corruption (back). Corruption shifts fea- tures away from the reference, but BN reduces the shifts. Tent instead shifts features more, and closer to an oracle that optimizes on target labels. Tent scales to the VisDA-C challenge.To show adaptation on a more difﬁcult benchmark, we evaluate on the VisDA-C challenge (Peng et al., 2017). The task is object recognition for 12 classes where the source data is synthesized by rendering 3D models and the target data is collected from real scenes. The validation error for our source model (ResNet-50, pretrained on ImageNet) is 56.1%, while tent reaches 45.6%, and improves to 39.6% by updating all layers except for the ﬁnal classiﬁer as done by Liang et al. (2020). Although ofﬂine source-free adaptation by model adaptation (Li et al., 2020) or SHOT (Liang et al., 2020) can reach lower error with more computation and tuning, tent can adapt online during testing. 4.3 A NALYSIS Tent reduces entropy and error.Figure 6 veriﬁes tent does indeed reduce the entropy and the task loss (softmax cross-entropy). We plot changes in entropy and loss on CIFAR-100-C for all 75 corruption type/level combinations. Both axes are normalized by the maximum entropy of a prediction (log 100) and clipped to ±1. Most points have lower entropy and error after adaptation. Tent needs feature modulation.We ablate the normalization and transformation steps of feature modulation. Not updating normalization increases errors, and can fail to improve over BN and PL. Not updating transformation parameters reduces the method to test-time normalization. Updating only the last layer of the model can improve but then degrades with further optimization. Updating the full model parameters θnever improves over the unadapted source model. Tent generalizes across target data.Adaptation could be limited to the points used for updates. We check that adaptation generalizes across points by adapting on target train and not target test. Test errors drop: CIFAR-100-C error goes from 37.3% to 34.2% and SVHN-to-MNIST error goes from 8.2% to 6.5%. (Train is larger than test; when subsampling to the same size errors differ by <0.1%.) Therefore the adapted modulation is not point speciﬁc but general. Tent modulation differs from normalization.Modulation normalizes and transforms features. We examine the combined effect. Figure 7 contrasts adapted features on corrupted data against reference features on uncorrupted data. We plot features from the source model, normalization, tent, and an oracle that optimizes on the target labels. Normalization makes features more like the reference, but tent does not. Instead, tent makes features more like the oracle. This suggests a different and task-speciﬁc effect. See the appendix for visualizations of more layers (Appendix C). 7Published as a conference paper at ICLR 2021 Tent adapts alternative architectures.Tent is architecture agnostic in principle. To gauge its generality in practice, we evaluate new architectures based on self-attention (SAN) (Zhao et al., 2020) and equilibrium solving (MDEQ) (Bai et al., 2020) for corruption robustness on CIFAR-100-C. Table 4 shows that tent reduces error with the same settings as convolutional residual networks. Table 4: Tent adapts alternative architectures on CIFAR-100-C without tuning. Results are error (%). SAN-10 (pair) SAN-10 (patch) MDEQ (large) Source BN Tent Source BN Tent Source BN Tent 55.3 39.7 36.7 48.0 31.8 29.2 53.3 44.9 41.7 5 R ELATED WORK We relate tent to existing adaptation, entropy minimization, and feature modulation methods. Train-Time AdaptationDomain adaptation jointly optimizes on source and target by cross-domain losses L(xs,xt) to mitigate shift. These losses optimize feature alignment (Gretton et al., 2009; Sun et al., 2017), adversarial invariance (Ganin & Lempitsky, 2015; Tzeng et al., 2017), or shared proxy tasks (Sun et al., 2019a). Transduction (Gammerman et al., 1998; Joachims, 1999; Zhou et al., 2004) jointly optimizes on train and test to better ﬁt speciﬁc test instances. While effective in their settings, neither applies when joint use of source/train and target/test is denied. Tent adapts on target alone. Recent “source-free” methods (Li et al., 2020; Kundu et al., 2020; Liang et al., 2020) also adapt without source data. Li et al. (2020); Kundu et al. (2020) rely on generative modeling and optimize multiple models with multiple losses. Kundu et al. (2020); Liang et al. (2020) also alter training. Tent does not need generative modeling, nor does it alter training, and so it can deployed more generally to adapt online with much more computational efﬁciency. SHOT (Liang et al., 2020) adapts by informa- tion maximization (entropy minimization and diversity regularization), but differs in its other losses and its parameterization. These source-free methods optimize ofﬂine with multiple losses for multiple epochs, which requires more tuning and computation than tent, but may achieve more accuracy with more computation. Tent optimizes online with just one loss and an efﬁcient parameterization of modulation to emphasize fully test-time adaptation during inference. We encourage examination of each of these works on the frontier of adaptation without source data. Chidlovskii et al. (2016) are the ﬁrst to motivate adaptation without source data for legal, commercial, or technical concerns. They adapt predictions by applying denoising auto-encoders while we adapt models by entropy minimization. We share their motivations, but the methods and experiments differ. Test-Time AdaptationTent adapts by test-time optimization and normalization to update the model. Test-time adaptation of predictions, through which harder and uncertain cases are adjusted based on easier and certain cases (Jain & Learned-Miller, 2011), provides inspiration for certainty-based model adaptation schemes like our own. Test-time training (TTT) (Sun et al., 2019b) also optimizes during testing, but differs in its loss and must alter training. TTT relies on a proxy task, such as recognizing rotations of an image, and so its loss depends on the choice of proxy. (Indeed, its authors caution that the proxy must be “both well-deﬁned and non-trivial in the new domain”). TTT alters training to optimize this proxy loss on source before adapting to target. Tent adapts without proxy tasks and without altering training. Normalizing feature statistics is common for domain adaptation (Gretton et al., 2009; Sun et al., 2017). For batch normalization Li et al. (2017); Carlucci et al. (2017) separate source and target statistics during training. Schneider et al. (2020); Nado et al. (2020) estimate target statistics during testing to improve generalization. Tent builds on test-time normalization to further reduce generalization error. Entropy MinimizationEntropy minimization is a key regularizer for domain adaptation (Carlucci et al., 2017; Shu et al., 2018; Saito et al., 2019; Roy et al., 2019), semi-supervised learning (Grandvalet & Bengio, 2005; Lee, 2013; Berthelot et al., 2019), and few-shot learning (Dhillon et al., 2020). Regularizing entropy penalizes decisions at high densities in the data distribution to improve accuracy for distinct classes (Grandvalet & Bengio, 2005). These methods regularize entropy during training in concert with other supervised and unsupervised losses on additional data. Tent is the ﬁrst to minimize 8Published as a conference paper at ICLR 2021 entropy during testing, for adaptation to dataset shifts, without other losses or data. Entropic losses are common; our contribution is to exhibit entropy as the sole lossfor fully test-time adaptation. Feature ModulationModulation makes a model vary with its input. We optimize modulations that are simpler than the full model for stable and efﬁcient adaptation. We modulate channel-wise afﬁne transformations, for their effectiveness in tandem with normalization (Ioffe & Szegedy, 2015; Wu & He, 2018), and for their ﬂexibility in conditioning for different tasks (Perez et al., 2018). These normalization and conditioning methods optimize the modulation during training by a supervised loss, but keep it ﬁxed during testing. We optimize the modulation during testing by an unsupervised loss, so that it can adapt to different target data. 6 D ISCUSSION Tent reduces generalization error on shifted data by test-time entropy minimization. In minimizing entropy, the model adapts itself to feedback from its own predictions. This is truly self-supervised self-improvement. Self-supervision of this sort is totally deﬁned by the supervised task, unlike proxy tasks designed to extract more supervision from the data, and yet it remarkably still reduces error. Nevertheless, errors due to corruption and other shifts remain, and therefore more adaptation is needed. Next steps should pursue test-time adaptation on more and harder types of shift, over more general parameters, and by more effective and efﬁcient losses. Shifts Tent reduces error for a variety of shifts including image corruptions, simple changes in appearance for digits, and simulation-to-real discrepancies. These shifts are popular as standardized benchmarks, but other real-world shifts exist. For instance, the CIFAR 10.1 and ImageNetV2 test sets (Recht et al., 2018; 2019), made by reproducing the dataset collection procedures, entail natural but unknown shifts. Although error is higher on both sets, indicating the presence of shift, tent does not improve generalization. Adversarial shifts (Szegedy et al., 2014) also threaten real-world usage, and attackers keep adapting to defenses. While adversarial training (Madry et al., 2018) makes a difference, test-time adaptation could help counter such test-time attacks. Parameters Tent modulates the model by normalization and transformation, but much of the model stays ﬁxed. Test-time adaptation could update more of the model, but the issue is to identify parameters that are both expressive and reliable, and this may interact with the choice of loss. TTT adapts multiple layers of features shared by supervised and self-supervised models and SHOT adapts all but the last layer(s) of the model. These choices depend on the model architecture, the loss, and tuning. For tent modulation is reliable, but the larger shift on VisDA is better addressed by the SHOT parameterization. Jointly adapting the input could be a more general alternative. If a model can adapt itself on target, then perhaps its input gradients might optimize spatial transformations or image translations to reduce shift without source data. Losses Tent minimizes entropy. For more adaptation, is there an effective loss for general but episodic test-time optimization? Entropy is general across tasks but limited in scope. It needs batches for optimization, and cannot update episodically on one point at a time. TTT can do so, but only with the right proxy task. For less computation, is there an efﬁcient loss for more local optimization? Tent and TTT both require full (re-)computation of the model for updates because they depend on its predictions. If the loss were instead deﬁned on the representation, then updates would require less forward and backward computation. Returning to entropy speciﬁcally, this loss may interact with calibration (Guo et al., 2017), as better uncertainty estimation could drive better adaptation. We hope that the fully test-time adaptation setting can promote new methods for equipping a model to adapt itself, just as tent yields a new model with every update. ACKNOWLEDGMENTS We thank Eric Tzeng for discussions on domain adaptation, Bill Freeman for comments on the experiments, Yu Sun for consultations on test-time training, and Kelsey Allen for feedback on the exposition. We thank the anonymous reviewers of ICLR 2021 for their feedback, which certainly improved the latest adaptation of the paper. 9Published as a conference paper at ICLR 2021 REFERENCES Shaojie Bai, Vladlen Koltun, and J Zico Kolter. Multiscale deep equilibrium models. arXiv preprint arXiv:2006.08656, 2020. David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. In NeurIPS, 2019. Fabio Maria Carlucci, Lorenzo Porzi, Barbara Caputo, Elisa Ricci, and Samuel Rota Bulo. Autodial: Automatic domain alignment layers. In 2017 IEEE International Conference on Computer Vision (ICCV), pp. 5077–5085. IEEE, 2017. Boris Chidlovskii, Stephane Clinchant, and Gabriela Csurka. Domain adaptation in the absence of source domain data. In SIGKDD, pp. 451–460, 2016. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016. Guneet Singh Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano Soatto. A baseline for few-shot image classiﬁcation. In ICLR, 2020. Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context prediction. In ICCV, 2015. J. Donahue, Y . Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In ICML, 2014. A Gammerman, V V ovk, and V Vapnik. Learning by transduction. InUAI, 1998. Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In ICML, 2015. Robert Geirhos, Carlos RM Temme, Jonas Rauber, Heiko H Schütt, Matthias Bethge, and Felix A Wichmann. Generalisation in humans and deep neural networks. In NeurIPS, 2018. Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and Wieland Brendel. Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. In International Conference on Learning Representations, 2019. Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. In ICLR, 2018. Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In NeurIPS, 2005. A. Gretton, AJ. Smola, J. Huang, M. Schmittfull, KM. Borgwardt, and B. Schölkopf. Covariate shift and local learning by distribution matching. In Dataset Shift in Machine Learning, pp. 131–160. MIT Press, Cambridge, MA, USA, 2009. Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In ICML, 2017. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, June 2016. Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In ICLR, 2019. Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. Augmix: A simple data processing method to improve robustness and uncertainty. In ICLR, 2020. Jonathan J. Hull. A database for handwritten text recognition research. TPAMI, 1994. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015. 10Published as a conference paper at ICLR 2021 Vidit Jain and Erik Learned-Miller. Online domain adaptation of a pre-trained cascade of classiﬁers. In CVPR, 2011. Thorsten Joachims. Transductive inference for text classiﬁcation using support vector machines. In ICML, 1999. Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiﬁcation with deep convolutional neural networks. NeurIPS, 25, 2012. Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009. Jogendra Nath Kundu, Naveen Venkat, R Venkatesh Babu, et al. Universal source-free domain adaptation. In CVPR, pp. 4544–4553, 2020. Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. Dong-Hyun Lee. Pseudo-label: The simple and efﬁcient semi-supervised learning method for deep neural networks. In ICML Workshop on challenges in representation learning, 2013. Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, and Si Wu. Model adaptation: Unsupervised domain adaptation without source data. In CVPR, June 2020. Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou. Revisiting batch normalization for practical domain adaptation. In ICLRW, 2017. Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In ICML, 2020. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018. Zachary Nado, Shreyas Padhy, D Sculley, Alexander D’Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robustness under covariate shift. arXiv preprint arXiv:2006.10963, 2020. Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. NeurIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019. Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko. VisDA: The visual domain adaptation challenge. arXiv preprint arXiv:1710.06924, 2017. Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. In AAAI, 2018. Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset shift in machine learning. MIT Press, Cambridge, MA, USA, 2009. Ilija Radosavovic, Justin Johnson, Saining Xie, Wan-Yen Lo, and Piotr Dollár. On network design spaces for visual recognition. In ICCV, 2019. Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do cifar-10 classiﬁers generalize to cifar-10? arXiv preprint arXiv:1806.00451, 2018. Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet classiﬁers generalize to ImageNet? In ICML, 2019. Stephan R Richter, Zeeshan Hayder, and Vladlen Koltun. Playing for benchmarks. In ICCV, 2017. Subhankar Roy, Aliaksandr Siarohin, Enver Sangineto, Samuel Rota Bulo, Nicu Sebe, and Elisa Ricci. Unsuper- vised domain adaptation using feature-whitening and consensus loss. In CVPR, 2019. 11Published as a conference paper at ICLR 2021 Evgenia Rusak, Lukas Schott, Roland S Zimmermann, Julian Bitterwolf, Oliver Bringmann, Matthias Bethge, and Wieland Brendel. A simple way to make neural networks robust against diverse image corruptions. In ECCV, 2020. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. ImageNet large scale visual recognition challenge. IJCV, 2015. Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to new domains. In European conference on computer vision, pp. 213–226. Springer, 2010. Kuniaki Saito, Donghyun Kim, Stan Sclaroff, Trevor Darrell, and Kate Saenko. Semi-supervised domain adaptation via minimax entropy. In ICCV, 2019. Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improv- ing robustness against common corruptions by covariate shift adaptation. arXiv preprint arXiv:2006.16971, 2020. C.E. Shannon. A mathematical theory of communication. Bell system technical journal, 27, 1948. Evan Shelhamer, Jonathan Long, and Trevor Darrell. Fully convolutional networks for semantic segmentation. PAMI, 2017. Rui Shu, Hung H Bui, Hirokazu Narui, and Stefano Ermon. A dirt-t approach to unsupervised domain adaptation. In ICLR, 2018. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015. Baochen Sun, Jiashi Feng, and Kate Saenko. Correlation alignment for unsupervised domain adaptation. In Domain Adaptation in Computer Vision Applications, pp. 153–171. Springer, 2017. Yu Sun, Eric Tzeng, Trevor Darrell, and Alexei A Efros. Unsupervised domain adaptation through self- supervision. arXiv preprint arXiv:1909.11825, 2019a. Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A Efros, and Moritz Hardt. Test-time training for out-of-distribution generalization. arXiv preprint arXiv:1909.13231, 2019b. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. 2014. Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. In ICCV, 2015. Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In CVPR, 2017. Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, et al. Deep high-resolution representation learning for visual recognition. PAMI, 2020. Yuxin Wu and Kaiming He. Group normalization. In ECCV, 2018. Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In NeurIPS, 2014. Richard Zhang, Phillip Isola, and Alexei A Efros. Split-brain autoencoders: Unsupervised learning by cross- channel prediction. In CVPR, 2017. Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition. In CVPR, 2020. Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, and Bernhard Schölkopf. Learning with local and global consistency. NeurIPS, 2004. 12Published as a conference paper at ICLR 2021 APPENDIX This supplement summarizes the image corruptions used in our experiments, highlights a qualitative example of instance-wise adaptation for semantic segmentation, and visualizes feature shifts across more layers. A R OBUSTNESS TO CORRUPTIONS In Section 4.1 we evaluate methods on a common image corruptions benchmark. Table 2 reports errors on the most severe level of corruption, level 5, and Figure 5 reports errors for each corruption type averaged across each of the levels 1–5. We summarize these corruptions types by example in Figure 8. Gaussian Noise  Shot Noise  Impulse Noise  Defocus Blur  Frosted Glass Blur Motion Blur  Zoom Blur  Snow  Frost  Fog Brightness  Contrast  Elastic  Pixelate  JPEG Figure 8: Examples of each corruption type in the image corruptions benchmark. While synthetic, this set of corruptions aims to represent natural factors of variation like noise, blur, weather, and digital imaging effects. This ﬁgure is reproduced from Hendrycks & Dietterich (2019). B S OURCE -FREE ADAPTATION FOR SEMANTIC SEGMENTATION Figure 9 shows a qualitative result on source-free adaptation for semantic segmentation (pixel-wise classiﬁcation) with simulation-to-real (sim-to-real) shift. For this sim-to-real condition, the source data is simulated while the target data is real. Our source data is GTA Richter et al. (2017), a visually-sophisticated video game set in an urban environment, and our target data is Cityscapes Cordts et al. (2016), an urban autonomous driving dataset. The supervised model is HRnet-W18, a fully convolutional network Shelhamer et al. (2017) in the high-resolution network family Wang et al. (2020). For this qualitative example, we run tent on a single image for multiple iterations, because an image is in effect a batch of pixels. This demonstrates adaptation to a target instance, without any further access to the target domain through usage of multiple images from the target distribution. 13Published as a conference paper at ICLR 2021 image label source-only tent, iteration 1 tent, iteration 5 tent, iteration 10 Figure 9: Adaptation for semantic segmentation with simulation-to-real shift from GTA Richter et al. (2017) to Cityscapes Cordts et al. (2016). Tent only uses the target data, and optimizes over a single image as a dataset of pixel-wise predictions. This episodic optimization in effect ﬁts a custom model to each image of the target domain. In only 10 iterations our method suppresses noise (see the completion of the street segment, in purple) and recovers missing classes (see the motorcycle and rider, center). 14Published as a conference paper at ICLR 2021 C F EATURE SHIFTS ACROSS LAYERS AND METHODS (a) Source (b) BN (c) Tent (d) Oracle Layer 2 Layer 5 Layer 8 Layer 11 Layer 14 Layer 18 Layer 20 Layer 23 Layer 26 Figure 10: Adapted features on CIFAR-100-C with Gaussian noise (front) and reference features without corruption (back). Corruption shifts the source features from the reference. BN shifts the features back to be more like the reference. Tent shifts features to be less like the reference, and more like an oracle that optimizes on target labels. 15",
      "meta_data": {
        "arxiv_id": "2006.10726v3",
        "authors": [
          "Dequan Wang",
          "Evan Shelhamer",
          "Shaoteng Liu",
          "Bruno Olshausen",
          "Trevor Darrell"
        ],
        "published_date": "2020-06-18T17:55:28Z",
        "pdf_url": "https://arxiv.org/pdf/2006.10726v3.pdf",
        "github_url": "https://github.com/DequanWang/tent"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces TENT (Test Entropy Minimization), a novel method for fully test-time adaptation where a model adapts to new data during inference without access to source data or supervision. TENT minimizes the entropy of model predictions by modulating features via normalization statistics and affine parameters. This approach significantly reduces generalization error on corrupted ImageNet-C, achieving a new state-of-the-art of 44.0% error, and improves performance on CIFAR-10/100-C, digit recognition domain adaptation (SVHN to MNIST/MNIST-M/USPS), semantic segmentation (GTA to Cityscapes), and the VisDA-C challenge. It demonstrates that adapting models for confidence during testing can be effective and efficient, often outperforming methods requiring source data or altered training.",
        "methodology": "TENT optimizes a pre-trained, probabilistic, and differentiable deep network during testing by minimizing the Shannon entropy (H(ˆy) = −∑ cp(ˆyc) logp(ˆyc)) of its predictions. This is achieved through feature modulation, specifically by updating the normalization statistics (mean µ, standard deviation σ) and optimizing channel-wise affine transformation parameters (scale γ, shift β) within the model's batch normalization layers. Other model parameters remain fixed. In each iteration, normalization statistics are estimated from the current batch during the forward pass, and the transformation parameters are updated via gradient descent on the entropy objective during the backward pass. This process can be applied online, continuing as long as test data is available, or offline for a fixed number of updates. The method does not require altering the original model training or accessing source data.",
        "experimental_setup": "The method was evaluated on: 1. Corruption Robustness: CIFAR-10-C/CIFAR-100-C and ImageNet-C benchmarks, featuring 15 corruption types at five severity levels. Models used were Residual Networks (R-26 for CIFAR, R-50 for ImageNet). 2. Domain Adaptation: Digit recognition from SVHN (source) to MNIST/MNIST-M/USPS (targets) using R-26, semantic segmentation from GTA (simulated source) to Cityscapes (real target) using HRNet-W18, and object recognition on VisDA-C (synthesized to real) using ResNet-50. Optimization employed SGD with momentum for ImageNet and Adam for other datasets, with adjusted batch sizes (64 for ImageNet, 128 for others) and learning rates (0.00025 for ImageNet, 0.001 for others). Baselines included source-only, adversarial domain adaptation (RG), self-supervised domain adaptation (UDA-SS), test-time training (TTT), test-time normalization (BN), and pseudo-labeling (PL). Alternative architectures like SAN and MDEQ were also tested for generality.",
        "limitations": "TENT does not universally improve generalization across all types of dataset shifts, notably showing no improvement on natural unknown shifts like CIFAR 10.1 and ImageNetV2, and even increasing error on a difficult MNIST-to-SVHN domain adaptation task. The entropy objective requires batches for optimization, limiting its applicability for episodic updates on single data points. The current parameterization, which focuses on normalization and affine transformation layers, might be too constrained for larger or more complex shifts, where adapting more model layers (as in SHOT) could yield better results. Additionally, the method requires full forward and backward passes for updates, potentially incurring higher computational costs compared to more localized optimization strategies.",
        "future_research_directions": "Future work should explore test-time adaptation for a wider array of challenging shifts, including natural but unknown variations (e.g., CIFAR 10.1, ImageNetV2) and adversarial attacks. Research could also focus on extending the set of adaptable parameters beyond feature modulations, identifying expressive and reliable subsets of the model to update (e.g., adapting input gradients for spatial or image transformations). Furthermore, developing more effective and computationally efficient loss functions for general and episodic test-time optimization, possibly defined on intermediate representations rather than predictions, is a key direction. Investigating the interplay between entropy minimization and model calibration could also lead to improved uncertainty estimation and adaptation.",
        "experimental_code": "from copy import deepcopy\n\nimport torch\nimport torch.nn as nn\nimport torch.jit\n\n\nclass Tent(nn.Module):\n    \"\"\"Tent adapts a model by entropy minimization during testing.\n\n    Once tented, a model adapts itself by updating on every forward.\n    \"\"\"\n    def __init__(self, model, optimizer, steps=1, episodic=False):\n        super().__init__()\n        self.model = model\n        self.optimizer = optimizer\n        self.steps = steps\n        assert steps > 0, \"tent requires >= 1 step(s) to forward and update\"\n        self.episodic = episodic\n\n        # note: if the model is never reset, like for continual adaptation,\n        # then skipping the state copy would save memory\n        self.model_state, self.optimizer_state = \\\n            copy_model_and_optimizer(self.model, self.optimizer)\n\n    def forward(self, x):\n        if self.episodic:\n            self.reset()\n\n        for _ in range(self.steps):\n            outputs = forward_and_adapt(x, self.model, self.optimizer)\n\n        return outputs\n\n    def reset(self):\n        if self.model_state is None or self.optimizer_state is None:\n            raise Exception(\"cannot reset without saved model/optimizer state\")\n        load_model_and_optimizer(self.model, self.optimizer,\n                                 self.model_state, self.optimizer_state)\n\n\n@torch.jit.script\ndef softmax_entropy(x: torch.Tensor) -> torch.Tensor:\n    \"\"\"Entropy of softmax distribution from logits.\"\"\"\n    return -(x.softmax(1) * x.log_softmax(1)).sum(1)\n\n\n@torch.enable_grad()  # ensure grads in possible no grad context for testing\ndef forward_and_adapt(x, model, optimizer):\n    \"\"\"Forward and adapt model on batch of data.\n\n    Measure entropy of the model prediction, take gradients, and update params.\n    \"\"\"\n    # forward\n    outputs = model(x)\n    # adapt\n    loss = softmax_entropy(outputs).mean(0)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    return outputs\n\n\ndef collect_params(model):\n    \"\"\"Collect the affine scale + shift parameters from batch norms.\n\n    Walk the model's modules and collect all batch normalization parameters.\n    Return the parameters and their names.\n\n    Note: other choices of parameterization are possible!\n    \"\"\"\n    params = []\n    names = []\n    for nm, m in model.named_modules():\n        if isinstance(m, nn.BatchNorm2d):\n            for np, p in m.named_parameters():\n                if np in ['weight', 'bias']:  # weight is scale, bias is shift\n                    params.append(p)\n                    names.append(f\"{nm}.{np}\")\n    return params, names\n\n\ndef copy_model_and_optimizer(model, optimizer):\n    \"\"\"Copy the model and optimizer states for resetting after adaptation.\"\"\"\n    model_state = deepcopy(model.state_dict())\n    optimizer_state = deepcopy(optimizer.state_dict())\n    return model_state, optimizer_state\n\n\ndef load_model_and_optimizer(model, optimizer, model_state, optimizer_state):\n    \"\"\"Restore the model and optimizer states from copies.\"\"\"\n    model.load_state_dict(model_state, strict=True)\n    optimizer.load_state_dict(optimizer_state)\n\n\ndef configure_model(model):\n    \"\"\"Configure model for use with tent.\"\"\"\n    # train mode, because tent optimizes the model to minimize entropy\n    model.train()\n    # disable grad, to (re-)enable only what tent updates\n    model.requires_grad_(False)\n    # configure norm for tent updates: enable grad + force batch statisics\n    for m in model.modules():\n        if isinstance(m, nn.BatchNorm2d):\n            m.requires_grad_(True)\n            # force use of batch stats in train and eval modes\n            m.track_running_stats = False\n            m.running_mean = None\n            m.running_var = None\n    return model\n\n\ndef check_model(model):\n    \"\"\"Check model for compatability with tent.\"\"\"\n    is_training = model.training\n    assert is_training, \"tent needs train mode: call model.train()\"\n    param_grads = [p.requires_grad for p in model.parameters()]\n    has_any_params = any(param_grads)\n    has_all_params = all(param_grads)\n    assert has_any_params, \"tent needs params to update: \" \\\n                           \"check which require grad\"\n    assert not has_all_params, \"tent should not update all params: \" \\\n                               \"check which require grad\"\n    has_bn = any([isinstance(m, nn.BatchNorm2d) for m in model.modules()])\n    assert has_bn, \"tent needs normalization for its optimization\"\n\n# --- Integration code from cifar10c.py ---\nimport torch.optim as optim\nfrom conf import cfg\n\ndef setup_tent(model):\n    \"\"\"Set up tent adaptation.\n\n    Configure the model for training + feature modulation by batch statistics,\n    collect the parameters for feature modulation by gradient optimization,\n    set up the optimizer, and then tent the model.\n    \"\"\"\n    model = configure_model(model)\n    params, param_names = collect_params(model)\n    optimizer = setup_optimizer(params)\n    tent_model = Tent(model, optimizer,\n                           steps=cfg.OPTIM.STEPS,\n                           episodic=cfg.MODEL.EPISODIC)\n    # logger.info(f\"model for adaptation: %s\", model) # Removed logging for brevity\n    # logger.info(f\"params for adaptation: %s\", param_names) # Removed logging for brevity\n    # logger.info(f\"optimizer for adaptation: %s\", optimizer) # Removed logging for brevity\n    return tent_model\n\n\ndef setup_optimizer(params):\n    \"\"\"Set up optimizer for tent adaptation.\n\n    Tent needs an optimizer for test-time entropy minimization.\n    In principle, tent could make use of any gradient optimizer.\n    In practice, we advise choosing Adam or SGD+momentum.\n    For optimization settings, we advise to use the settings from the end of\n    trainig, if known, or start with a low learning rate (like 0.001) if not.\n\n    For best results, try tuning the learning rate and batch size.\n    \"\"\"\n    if cfg.OPTIM.METHOD == 'Adam':\n        return optim.Adam(params,\n                    lr=cfg.OPTIM.LR,\n                    betas=(cfg.OPTIM.BETA, 0.999),\n                    weight_decay=cfg.OPTIM.WD)\n    elif cfg.OPTIM.METHOD == 'SGD':\n        return optim.SGD(params,\n                   lr=cfg.OPTIM.LR,\n                   momentum=cfg.OPTIM.MOMENTUM,\n                   dampening=cfg.OPTIM.DAMPENING,\n                   weight_decay=cfg.OPTIM.WD,\n                   nesterov=cfg.OPTIM.NESTEROV)\n    else:\n        raise NotImplementedError",
        "experimental_info": "The TENT (Test-time Entropy Minimization) method adapts a pre-trained, probabilistic deep network during testing. It minimizes the Shannon entropy of the model's predictions (H(ˆy) = −∑ cp(ˆyc) logp(ˆyc)). This is achieved by modulating features through updating normalization statistics (mean µ, standard deviation σ) and optimizing channel-wise affine transformation parameters (scale γ, shift β) within the model's batch normalization layers.\n\nKey aspects of the TENT method implementation and experimental settings:\n1.  **Objective Function**: The method minimizes `softmax_entropy(outputs).mean(0)`, where `softmax_entropy` is defined as `-(x.softmax(1) * x.log_softmax(1)).sum(1)`. This is the core entropy minimization objective.\n2.  **Adaptable Parameters**: Only the affine scale (`weight`) and shift (`bias`) parameters of `nn.BatchNorm2d` layers are optimized. Other model parameters remain fixed. This is handled by `collect_params` and `configure_model` which sets `requires_grad_(False)` for the entire model then `requires_grad_(True)` specifically for BatchNorm layers, and then `collect_params` filters for 'weight' and 'bias'.\n3.  **Normalization Statistics**: During adaptation, Batch Normalization layers are forced to use batch-wise statistics (i.e., `m.track_running_stats = False`, `m.running_mean = None`, `m.running_var = None`). This means `µ` and `σ` are estimated from the current test batch in each forward pass.\n4.  **Optimization**: The affine transformation parameters are updated via gradient descent on the entropy objective. The `forward_and_adapt` function performs a forward pass, computes the loss, backpropagates, and then calls `optimizer.step()` and `optimizer.zero_grad()`.\n5.  **Online vs. Episodic Adaptation**: The `Tent` class can operate in an 'episodic' mode (`cfg.MODEL.EPISODIC`). If `True`, the model and optimizer states are reset before processing each new batch, meaning adaptation does not persist across batches. If `False` (default), adaptation is 'online' and continues to update the model state, making updates persist.\n6.  **Number of Updates**: The `steps` parameter (`cfg.OPTIM.STEPS`, default 1) in the `Tent` constructor dictates how many optimization steps are performed per input batch during testing.\n7.  **Optimizer Settings**: The `setup_optimizer` function configures the optimizer. It supports 'Adam' (default) or 'SGD' based on `cfg.OPTIM.METHOD`.\n    *   **Adam**: `lr=cfg.OPTIM.LR` (default 1e-3), `betas=(cfg.OPTIM.BETA, 0.999)` (default `BETA` is 0.9), `weight_decay=cfg.OPTIM.WD` (default 0.0).\n    *   **SGD**: `lr=cfg.OPTIM.LR` (default 1e-3), `momentum=cfg.OPTIM.MOMENTUM` (default 0.9), `dampening=cfg.OPTIM.DAMPENING` (default 0.0), `weight_decay=cfg.OPTIM.WD` (default 0.0), `nesterov=cfg.OPTIM.NESTEROV` (default True).\n8.  **Model Configuration**: The `configure_model` function ensures the model is in `train()` mode (for `nn.BatchNorm2d` to use batch statistics and for `requires_grad` to be effective for `nn.Parameter`s), disables gradients for all parameters initially, and then explicitly enables gradients for BatchNorm parameters while forcing them to use batch statistics.\n9.  **Integration**: The `cifar10c.py` script's `setup_tent` function orchestrates the TENT adaptation by loading the base model, configuring it, collecting the adaptable parameters, setting up the optimizer, and instantiating the `Tent` wrapper around the model and optimizer. The `evaluate` function iterates over corruptions and severities, resetting the TENT model (`model.reset()`) if `episodic` mode is enabled."
      }
    },
    {
      "title": "What How and When Should Object Detectors Update in Continually Changing Test Domains?",
      "abstract": "It is a well-known fact that the performance of deep learning models\ndeteriorates when they encounter a distribution shift at test time. Test-time\nadaptation (TTA) algorithms have been proposed to adapt the model online while\ninferring test data. However, existing research predominantly focuses on\nclassification tasks through the optimization of batch normalization layers or\nclassification heads, but this approach limits its applicability to various\nmodel architectures like Transformers and makes it challenging to apply to\nother tasks, such as object detection. In this paper, we propose a novel online\nadaption approach for object detection in continually changing test domains,\nconsidering which part of the model to update, how to update it, and when to\nperform the update. By introducing architecture-agnostic and lightweight\nadaptor modules and only updating these while leaving the pre-trained backbone\nunchanged, we can rapidly adapt to new test domains in an efficient way and\nprevent catastrophic forgetting. Furthermore, we present a practical and\nstraightforward class-wise feature aligning method for object detection to\nresolve domain shifts. Additionally, we enhance efficiency by determining when\nthe model is sufficiently adapted or when additional adaptation is needed due\nto changes in the test distribution. Our approach surpasses baselines on widely\nused benchmarks, achieving improvements of up to 4.9\\%p and 7.9\\%p in mAP for\nCOCO $\\rightarrow$ COCO-corrupted and SHIFT, respectively, while maintaining\nabout 20 FPS or higher.",
      "full_text": "What, How, and When Should Object Detectors Update in Continually Changing Test Domains? Jayeon Yoo1 Dongkwan Lee1 Inseop Chung1 Donghyun Kim2∗ Nojun Kwak1∗ 1Seoul National University 2Korea University 1{jayeon.yoo, biancco, jis3613, nojunk}@snu.ac.kr 2d kim@korea.ac.kr Abstract It is a well-known fact that the performance of deep learning models deteriorates when they encounter a dis- tribution shift at test time. Test-time adaptation (TTA) al- gorithms have been proposed to adapt the model online while inferring test data. However, existing research pre- dominantly focuses on classification tasks through the op- timization of batch normalization layers or classification heads, but this approach limits its applicability to various model architectures like Transformers and makes it chal- lenging to apply to other tasks, such as object detection. In this paper, we propose a novel online adaption approach for object detection in continually changing test domains, considering which part of the model to update, how to up- date it, and when to perform the update. By introducing architecture-agnostic and lightweight adaptor modules and only updating these while leaving the pre-trained backbone unchanged, we can rapidly adapt to new test domains in an efficient way and prevent catastrophic forgetting. Fur- thermore, we present a practical and straightforward class- wise feature aligning method for object detection to resolve domain shifts. Additionally, we enhance efficiency by deter- mining when the model is sufficiently adapted or when ad- ditional adaptation is needed due to changes in the test dis- tribution. Our approach surpasses baselines on widely used benchmarks, achieving improvements of up to 4.9%p and 7.9%p in mAP for COCO → COCO-corrupted and SHIFT, respectively, while maintaining about 20 FPS or higher. 1. Introduction Although deep learning models have demonstrated remark- able success in numerous vision-related tasks, they remain susceptible to domain shifts where the test data distribu- tion differs from that of the training data [3, 25, 40]. In real-world applications, domain shifts frequently occur at test-time due to natural variations, corruptions, changes in weather conditions (e.g., fog, rain) , camera sensor differ- Figure 1. We propose an online adaptation method for object detection in continually changing test domains. Object detectors trained with clean images suffer from performance degradation due to various corruption, such as camera sensor degradation or environmental changes (Direct-Test). Updating full parameters for online adaptation require a large number of test samples and vul- nerable to drastic domain changes (Full-Finetuning), while using only our lightweight adaptor is robust and quickly adapts within a few time steps (Ours). We can further improve efficiency by skip- ping unnecessary adaptation steps (Ours-Skip). ences (e.g., pixelate, defocus blur) , and various other fac- tors. Test-Time Adaptation (TTA) [3, 25, 30, 40, 43, 47] has been proposed to solve the domain shifts in test-time by adapting models to a specific target (test) distribution in an online manner. Furthermore, it is essential to take into account continuously changing test distributions, as the test distribution has the potential to undergo changes and devel- opments as time progresses (i.e., Continual Test-time Adap- tation (CTA)). For instance, autonomous driving systems may experience transitions from clear and sunny conditions to rainy or from daytime to nighttime, which causes contin- ually changing domain shifts [39]. While it is an important research topic, continual test-time adaptation for object de- tection has not been well explored. Recently, several TTA methods [6, 29, 36] tailored for 1 arXiv:2312.08875v1  [cs.CV]  12 Dec 2023object detection have been proposed. ActMAD [29] aligns all the output feature maps ( RC×H×W ) after Batch Nor- malization (BN) layers [14] to adapt the test domain to be similar to that of the training domain. However, this ap- proach requires significant memory during adaptation and does not explicitly consider the objects present in the image. TeST [36] and STFAR [6] adapt to a test domain by utiliz- ing weak and strong augmented test samples with a teacher- student network [37], but they significantly increase infer- ence costs since they require additional forward passes and update steps. Also, these methods update all network pa- rameters, making them highly inefficient in online adapta- tion and vulnerable to losing task-specific knowledge when the test domain experiences continual or drastic changes. In this paper, we aim to develop an efficient continual test-time adaptation (CTA) method for object detection. We investigate the following three key aspects to improve ef- ficiency; what to update: while previous TTA methods for object detection [6, 29, 36] use full fine-tuning, updating all parameters at test time, they are inefficient and prone to losing task-specific knowledge in relatively complex object detection tasks. Updating BN layers, as done in many TTA methods for classification [17, 30, 43, 47], is not as effective for object detection, given its smaller batch size compared to classification and the limitation in applying various back- bones, such as Transformer [26, 41].how to update: several previous TTA methods for object detection [6, 36] adapt the model by using teacher-student networks, resulting in a significant decrease in inference speed, which is detri- mental during test time. While another existing method [29] aligns feature distributions for adaptation, it does not con- sider each object individually, focusing only on image fea- tures, making it less effective for object detection. when to update: most TTA or CTA methods update models using all incoming test samples. However, it is inefficient to update continuously the model if it is already sufficiently adapted when the change of the test domain is not significant. To this end, (1) we propose an efficient continual test- time adaptation method for object detectors to adapt to continually changing test domains through the use of lightweight adaptors which require only 0.54%∼0.89% ad- ditional parameters compared to the full model. It exhibits efficiency in parameters, memory usage, and adaptation time, along with robustness to continuous domain shifts without catastrophic forgetting. Additionally, it demon- strates wide applicability to various backbone types com- pared to BN-based TTA methods [17, 22, 30, 43, 47, 48]. (2) To enhance the adaptation effectiveness in the object detec- tion task, we align the feature distribution of the test domain with that of the training domain at both the image-level and object-level using only the mean and variance of features. For estimating the mean of the test domain features, we employ Exponentially Moving Average (EMA) as we can leverage only the current incoming test samples, not the en- tire test domain data. Due to the unavailability of training data access, we utilize only the mean and variance of the features from a few training samples. (3) We also introduce two novel criteria that do not require additional resources to determine when the model needs adaptation to enhance efficiency in a continually changing test domain environ- ment. As illustrated in Fig. 1, our approach Ours, employ- ing adaptors, tends to adapt much faster to domain changes compared to full parameter updates. This enables efficient TTA by using only a few test samples to update the adaptor and skipping the rest of the updates as shown in Ours-Skip. Our main contributions are summarized as follows: • We introduce an architecture-agnostic lightweight adap- tor, constituting only a maximum of 0.89% of the total model parameters, into the backbone of the object de- tector to adapt the model in a continually changing test domain. This approach ensures efficiency in parameters, memory usage, and adaptation speed, demonstrating the robust preservation of task-specific knowledge owing to its inherent structural characteristics. • We propose a straightforward and effective adaptation loss for CTA in object detection tasks. This is achieved by aligning the distribution of training and test domain fea- tures at both the image and object levels, utilizing only the mean and variance of a few training samples and EMA- updated mean features of the test domain. • We also propose two criteria to determine when the model requires adaptation, enabling dynamic skipping or resum- ing adaptation as needed. This enhancement significantly boosts inference speed by up to about 2 times while main- taining adaptation performance. • Our adaptation method proves effective for diverse types of domain shifts, including weather changes and sensor variations, regardless of whether the domain shift is dras- tic or continuous. In particular, our approach consistently improves the mAP by up to 7.9% in COCO →COCO-C and SHIFT-Discrete/Continuous with higher than 20 FPS. 2. Related Work Test-time adaptation. Recently, there has been a surge of interest in research that adapts models online using unla- beled test samples while simultaneously inferring the test sample to address the domain shift problem, where the test data distribution differs from that of the training data. There are two lines for online adaptation to the test do- main, Test-time Training (TTT) and Test-time Adaptation (TTA). TTT [1, 2, 25, 40] involves modifying the model architecture during training to train it with self-supervised loss, allowing adaptation to the test domain in the test time by applying this self-supervised loss to the unlabeled test samples. On the other hand, TTA aims to adapt the trained model directly to the test domain without specifically tai- 2lored model architectures or losses during training time. NORM [35] and DUA [28] address the domain shifts by adjusting the statistics of batch normalization (BN) layers using the current test samples, without updating other pa- rameters, inspired by [21]. Following this, [22, 30, 43, 48] and [17] update the affine parameters of BN layers using unsupervised loss, entropy minimization loss to enhance the confidence of test data predictions, and feature distribution alignments loss, respectively. Several studies [15, 16] up- date the classifier head using the pseudo-prototypes from the test domain. However, these methods limit their appli- cability to architectures without BN layers or to object de- tection tasks that involve multiple objects in a single im- age. Others [29, 38, 47] update full parameters for online adaptation to the test domain in an online manner, but this approach is inefficient and susceptible to the noisy signal from the unsupervised loss. While existing TTA methods are oriented towards classification tasks, we aim to propose an effective and efficient method for online adaptation in the object detection task. Continual test-time adaptation. Recent studies [31, 44] point out that existing TTA methods have primarily focused on adapting to test domains following an i.i.d assumption and may not perform well when the test data distribution deviates from this assumption. [44] introduces a Contin- ual TTA (CTA) method designed for scenarios where the test domain continuously changes over time. This poses challenges in preventing the model from over-adapting to a particular domain shift and preserving the knowledge of the pre-trained model to avoid catastrophic forgetting. In the field of CTA, the self-training strategy adopting an Exponentially Moving Average (EMA) teacher-student structure is attracting interest as an effective algorithm en- abling robust representation to be learned through self- knowledge distillation. In many studies, the EMA teacher- student structure and catastrophic restoration of source model weights have been proposed as a solution to achieve the goal of CTA [4, 44, 45]. Approaches using source re- play [32], and anti-forgetting regularization [30] have also achieved good performances in robust continuous adapta- tion. Furthermore, there is growing attention on methods that mitigate the computational and memory challenges as- sociated with CTA, such as [12], which relies on updates to batch normalization statistics. Test-time adaptive object detection. Research on TTA for Object Detection (TTAOD) is progressively emerging [6, 29, 36, 42]. Most existing TTAOD methods [6, 36, 42] exploit a teacher-student network to adapt to the test do- main, following the self-training approach commonly em- ployed in Unsupervised Domain Adaptation for object de- tection [7, 18, 19, 34]. However, it is inefficient for TTA due to data augmentation requirements and additional for- ward and backward steps, resulting in slower inference speeds and higher memory usage. Another approach, Act- MAD [29], aligns the distributions of output feature maps after all BN layers along the height, width, and channel axes to adapt to the test domain. However, this location-aware feature alignment is limited to datasets with fixed location priors, such as driving datasets, and is less effective for nat- ural images like COCO. Additionally, CTA for Object De- tection (CTAOD)have not been thoroughly explored. There- fore, there is a need for an effective CTAOD method con- sidering memory and time efficiency. 3. Method To enable the efficient and effective Continual Test-time Adaptation of Object Detectors (CTAOD), we introduce an approach that specifies which part of the model should be updated, describes how to update those using unlabeled test data, and determines whether we perform model updates or not to improve efficiency. 3.1. Preliminary Assume that we have an object detector h ◦ gΘ, here h and g are the RoI head and the backbone, respectively with their parameters being Θ. The training dataset is denoted as Dtrain = {(xi, yi)}N i=1, where xi ∼ Ptrain(x) and yi = ( bboxi, ci), containing information on the bounding box (bbox) and class label ci ∈ C. Consider deploying the detector to the test environments where the test data at pe- riod T is denoted as xT j ∼ PT test(x), PT test ̸= Ptrain and PT test deviates from the i.i.d. assumption. In addition, the domain of PT test continually changes according to T (i.e., PT test ̸= PT−1 test ). Our goal is to adapt the detector h ◦ g to PT test using only test data xT j while making predictions. 3.2. What to update: Adaptation via an adaptor Previous methods [6, 29, 36, 42] adapt the model to the test domain by updating all parameters Θ, leading to in- efficiency at test time and a high risk of losing task knowl- edge from the training data. In contrast, we adapt the model by introducing an adaptor with an extremely small set of parameters and updating only this module while freezing Θ. We introduce a shallow adaptor in parallel for each block, inspired by [5, 13], where transformer-based mod- els are fine-tuned for downstream tasks through parameter- efficient adaptors, as shown in Fig. 2. Each adaptor consists of down-projection layers Wdown ∈ Rd×d r , up-projection layers Wup ∈ R d r ×d and ReLUs, where d denotes the in- put channel dimension and r is the channel reduction ratio set to 32 for all adaptors. We use MLP layers for the Trans- former block (Fig. 2a) and 1×1 convolutional layers for the ResNet block (Fig. 2b) to introduce architecture-agnostic adaptors. The up-projection layer is initialized to 0 values so that the adaptor does not modify the output of the block, 3(a) A block of Transformer  (b) A block of ResNet Figure 2. We attach an adaptor, which is a shallow and low-rank MLP or CNN, to every N block in parallel. We update only these adaptors while other parameters are frozen. Our approach can be applied to diverse architectures including CNNs and Transformers. but as the adaptor is gradually updated, it adjusts the output of the block to adapt to the test domain. Even as the adaptor is updated in the test domain, the original backbone param- eter Θ remains frozen and fully preserved. This structural preservation, as evident in Ours in Fig. 1, enables robust and efficient adaptation to domain changes by maintaining relatively complex task knowledge in object detection and updating very few parameters. 3.3. How to update: EMA feature alignment To adapt the object detector to the test domain, we align the feature distribution of the test domain with that of the training data, inspired by [17, 29, 38]. In contrast to these methods that solely align image feature distribution, we ad- ditionally align object-level features in a class-wise manner, considering class frequency, to enhance its effectiveness for object detection. As the training data is not accessible dur- ing test time, we pre-compute the first and second-order statistics, denoted as µtr = E[Ftr] and Σtr = Var[Ftr], where the operators E and Var represent the mean and vari- ance respectively. The features Ftr = {gΘ(xtr)} are com- puted using only 2,000 training samples, a small subset of the training data. Since a sufficient amount of test domain data is not available at once, and only the current incoming test data, whose features are denoted as Ft te, is accessible at time step t, we estimate the mean of test data features using an exponentially moving average (EMA) as follows: µt te = (1 − α) · µt−1 te + α · E[Ft te], s.t. µ0 te = µtr. (1) Considering the typically small batch size in object detec- tion compared to classification, we approximate the vari- ance of the test features as Σte ≃ Σtr to reduce instability. Image-level feature alignment. We estimate the training and test feature distributions as normal distributions and minimize the KL divergence between them as follows: Limg = DKL(N(µtr, Σtr), N(µt te, Σtr)). (2) Region-level class-wise feature alignment. In object de- tection, we deal with multiple objects within a single image, making it challenging to apply the class-wise feature align- ment proposed in [38], a TTA method for classification. To handle region-level features that correspond to an object, we use ground truth bounding boxes for the training data and utilize the class predictions of RoI pooled features, ft te, for unlabeled test data. In object detection, domain shifts often result in lower recall rates, as a significant number of proposals are predicted as background [20]. To mitigate this issue, we filter out features with background scores exceed- ing a specific threshold. Subsequently, we assign them to the foreground class with the highest probability, as follows: Fk,t te = {ft te|argmax c pfg = k, pbg < 0.5}, where hcls(ft te) = [pfg , pbg] = [p0, ..., pC−1, pbg]. (3) We estimate the class-wise feature distribution of the test domain by exploiting Fk,t te and Eq.1. Furthermore, we in- troduce a weighting scheme for aligning features of less frequently appearing classes, taking into account the severe class imbalance where specific instance ( e.g., person) may appear multiple times within a single image, as follows: Nk,t = Nk,t−1 + ||Fk,t te ||, s.t. Nk,0 = 0 wk,t = log \u0012maxi Ni,t Nk,t \u0013 + 0.01 Lobj = X k wk,t · DKL(N(µk tr, Σk tr), N(µk,t te , Σk tr)). (4) Here, the class-wise mean µk and variance Σk of the train- ing and test data are obtained in the same way as the image- level features. We can effectively adapt the object detector by updating the model to align the feature distribution at both the image and object levels as L = Limg + Lobj. 3.4. When to update: Adaptation on demand As shown in Fig. 1, Ours, which only updates the adaptor proposed in Sec. 3.2, efficiently adapts to changes in the test domain, even with a small subset of early test samples. We leverage its rapid adaptation characteristics to reduce com- putational costs by skipping model updates ( i.e., skipping backward passes) when the model has already sufficiently adapted to the current test domain and resuming model up- dates when confronted with a new test domain. Therefore, we introduce two criteria to determine when to update the model or not as follows: (Criterion 1) When the distribution gap exceeds the in- domain distribution gap. Recall that Limg (Eq. 2) mea- sures the distribution gap between the test and train distri- butions. We assume a model is well-adapted to the current test domain when Limg is closer to the in-domain distri- bution gap. We measure the in-domain distribution gap by 4(a) The ratio of Limg to Din KL (b) The ratio of Limg to Lt ema Figure 3. The test domain undergoes a shift every 4,000 time steps, and each metric reaches its peak at the same intervals. sampling two disjoint subsets, xi and xj, of training fea- tures Ftr from Sec. 3.3 as follows: Din KL = DKL(N(µi tr, Σi tr), N(µj tr, Σj tr)), (5) where µi tr, Σi tr are obtained from xi ∼ Ptrain(x) and µj tr, Σj tr from xj ∼ Ptrain(x). In other words, if Limg is noticeably higher than the in-domain distribution gapDin KL, we consider a model encountering a test domain whose dis- tribution differs from Ptrain(x) and needs to be updated. Based on this, we introduce a new index Limg Din KL . Fig. 3a plots the trend of this index during the model adaptation to a con- tinually changing test domain. It shows that the index has a large value in the early stages of a domain change, decreases rapidly, and then maintains a value close to 1. This index exhibits a similar trend regardless of the backbone type and dataset, as included in the appendix. Therefore, we establish the criterion that model updates are necessary when this in- dex exceeds a certain threshold, τ1, as Limg Din KL > τ1. (Criterion 2 ) When the distribution gap suddenly in- creases. Additionally, we can determine when the test dis- tribution changes and model updates are necessary by ob- serving the trend of the distribution gap ( i.e., Limg). The convergence of Limg indicates that a model is well-adapted to the current test domain. To put it differently, Limg will exhibit a sudden increase when the model encounters a new test domain. We introduce an additional index, denoted as Limg Ltema , representing the ratio of the currentLimg to its expo- nentially moving averageLt ema at time t. We calculate it us- ing the following formula:Lt ema = 0.99·Lt−1 ema+0.01·Limg. Fig. 3b illustrates the trend of the ratio of Limg over the timesteps. It tends to reach a value of 1 as the loss stabilizes at a specific level. Nevertheless, when the model encounters shifts in the test distribution, the ratio experiences a sharp increase, indicating the necessity of a model update when it exceeds a specific threshold, τ2, as Limg Ltema > τ2. If at least one of the two criteria is satisfied, we conclude that the model requires adaptation and proceed to update it. 4. Experiments Sec. 4.1 presents the two object detection benchmark datasets with test distributions that change continuously, ei- ther in a drastic or gradual manner, and our implementation detail is in 4.2. Sec. 4.4 compares our method with other TTA baselines described in Secs. 4.3.. We present detailed ablation studies of our method analyzing the effectiveness and efficiency of our method in terms of what, how, and when to update the models for CTAOD in Sec. 4.5. 4.1. Datasets We experiment with the following three scenarios. COCO → COCO-C simulates continuous and drastic real- istic test domain changes over a long sequence. MS-COCO [23] collects 80 classes of common objects in their natural context with 118k training images and 5k validation images. COCO-C is created by employing 15 types of realistic cor- ruptions [27], such as image distortion and various weather conditions, to simulate test domain changes. In the experi- ments, the model is only trained on the COCO train set and sequentially evaluated on each corruption in the COCO-C validation set during test-time for reproducing continually changing test domains. Finally, the model is evaluated on the original COCO validation set to assess how well it pre- serves knowledge of the original domain (denoted as Org.). SHIFT-(Discrete / Continuous) [39] is a synthetic driving image dataset with 6 classes under different conditions us- ing five weather attributes (clear, cloudy, overcast, fog, rain) and three time-of-day attributes ( daytime, dawn, night ). In SHIFT-Discrete, there are image sets for each attribute, and the model is sequentially evaluated on these attributes, cloudy → overcast → foggy → rainy → dawn → night → clear which contains 2.4k, 1.6k, 2.7k, 3.2k, 1.2k, 1.4k, and 2.8k validation images, respectively. This simulates scenar- ios where the domain undergoes drastic changes. InSHIFT- Continuous, the model is evaluated on four sequences, each consisting of 4k frames, continuously transitioning from clear to foggy (or rainy) and back to clear. 4.2. Implementation Detail We experiment with Faster-RCNN [33] models using ResNet50 [10] and Swin-Tiny [26] as a backbone with FPN [24]. For the COCO → COCO-C adaptation, we em- ploy the publicity available models trained on COCO re- leased in [46] and [26] for ResNet5- and Swin-Tiny-based Faster-RCNN, respectively. For SHIFT experiments, mod- els are trained on the training domain using the detectron2 framework following [33] and [26]. For test-time adapta- tion, we always set the learning to 0.001 for the SGD opti- mizer, and α of Eq. 1 to 0.01, while τ1 and τ2 are set to 1.1 5Table 1. Comparison of mAP, the number of backward and forward passes, and FPS between baselines and our model on COCO→ COCO- C. Our model consistently outperforms baselines on the two different backbones. Furthermore, Ours-Skip with ResNet notably reduces backward passes by as much as 90.5%, leading to a significantly improved frames per second (FPS) rate by up to 109.9%. Noise Blur Weather Digital # step Backbone Method Gau Sht Imp Def Gls Mtn Zm Snw Frs Fog Brt Cnt Els Px Jpg Org. Avg. For. Back. FPS Swin-T [26] Direct-Test 9.7 11.4 10.0 13.4 7.5 12.1 5.2 20.7 24.8 36.1 36.0 12.9 19.1 4.9 15.8 43.0 17.7 80K 0 21.5 ActMAD 10.7 12.0 9.4 12.3 5.7 9.5 4.5 15.3 17.5 27.6 28.2 1.1 16.7 2.6 8.7 36.3 13.9 80K 80K 8.3 Mean-Teacher 10.0 12.1 11.2 12.8 8.1 12.1 4.9 19.6 23.7 34.9 34.0 8.0 18.9 6.1 17.6 41.0 17.2 160K 80K 6.9 Ours 13.6 16.6 16.1 14.0 13.6 14.2 8.3 23.7 27.2 37.4 36.4 27.2 27.2 22.2 22.3 42.3 22.6 80K 80K 9.5 Ours-Skip 13.3 15.3 15.1 14.0 12.8 13.9 6.5 22.0 25.4 35.5 34.9 26.5 25.9 23.4 20.2 41.2 21.6 80K 9.7K 17.7 ResNet50 [10] Direct-Test 9.1 11.0 9.8 12.6 4.5 8.8 4.6 19.1 23.1 38.4 38.0 21.4 15.6 5.3 11.9 44.2 17.3 80K 0 25.8 NORM 9.9 11.9 11.0 12.6 5.2 9.1 5.1 19.4 23.5 38.2 37.6 22.4 17.2 5.7 10.3 43.4 17.5 80K 0 25.8 DUA 9.8 11.7 10.8 12.8 5.2 8.9 5.1 19.3 23.7 38.4 37.8 22.3 17.2 5.4 10.1 44.1 17.1 80K 0 25.8 ActMAD 9.1 9.6 7.0 11.0 3.2 6.1 3.3 12.8 14.0 27.7 27.8 3.9 12.9 2.3 7.2 34.3 10.5 80K 80K 9.6 Mean-Teacher 9.6 12.5 12.0 4.0 2.9 4.8 3.1 16.2 23.5 35.1 34.0 21.8 16.6 8.2 12.7 40.3 14.5 160K 80K 8.1 Ours 12.7 17.8 17.5 12.4 11.5 11.3 6.6 22.8 26.9 38.6 38.5 28.0 25.1 21.2 22.2 41.8 22.2 80K 80K 10.1 Ours-Skip 14.4 17.1 16.0 13.9 11.7 12.2 6.3 22.1 25.5 37.7 37.1 25.5 24.1 23.1 21.1 42.8 21.9 80K 7.6K 21.2 and 1.05, respectively. We use the same hyper-parameters across all backbones and datasets. All experiments are con- ducted with a batch size of 4. 4.3. Baselines Direct-Test evaluates the model trained in the training do- main without adaptation to the test domain. ActMAD [29] is a TTA method aligning the distribution of output features across all BN layers. To apply ActMAD to the Swin Trans- former-based model, we align the output features of the LN layers. We implement Mean-Teacher using a teacher- student network framework to reproduce as close as possi- ble to TeST [36], as its implementation is not publicly avail- able. We follow the FixMatch [37] augmentation method and report results after tuning all hyper-parameters in our scenario. NORM [35] and DUA [28], TTA methods ini- tially designed for classification, are directly applicable to detection tasks by either mixing a certain amount of current batch statistics or updating batch statistics via EMA. How- ever, these are only compatible with architectures contain- ing BN layers. Additional details are provided in Appendix. 4.4. Main Results We compare the performance of each method using mAP and efficiency metrics, including the number of forward and backward passes, as well as FPS during test-time adapta- tion. Results of COCO and SHIFT are in Tab. 1 and 2, re- spectively. COCO → COCO-C. Tab. 1 demonstrates the effective adaptation performance of Ours in the challenging COCO benchmark with 80 classes due to object-level class-wise feature alignment. ActMAD also aligns feature distribution for TTA, but is not effective since it only aligns whole fea- ture maps without considering specific classes in the im- age. NORM and DUA, applicable only to ResNet [10], show minimal performance improvement by adaptation as they are not specifically tailored for object detection and only modify batch statistics across the entire feature map. Ad- ditionally, ActMAD and Mean-Teacher, updating full pa- rameters, gradually lose task knowledge in the continually changing test distributions, resulting in much lower perfor- mance on Org. , the domain identical to the training data, than that of Direct-Test. In contrast, Ours effectively pre- vents catastrophic forgetting by freezing the original param- eters of the models and updating only the adaptor, obtain- ing performance on par with Direct-Test on the Org. do- main and consistently high performance across corrupted domains, with an average mAP improvement of 4.9%p compared to that of Direct-Test. Furthermore, leveraging the rapid adaptation ability of the adaptor,Ours-Skip, which skips unnecessary adaptation, allows using only a maxi- mum of about 12% of the total samples for adaptation with- out significant performance loss. This leads to a substantial improvement in inference speed, more than doubling com- pared to other TTA methods, reaching over 17.7 FPS. SHIFT-Discrete. Ours is also effective in SHIFT, which simulates continuous changes in weather and time in driv- ing scenarios according to the left section of Tab. 2. Espe- cially, Ours shows significant improvements in mAP by 7- 9%p, particularly for the foggy and dawn attributes where Direct-Test obtains lower performance due to severe do- main shift. In contrast, with ActMAD, catastrophic forget- ting takes place when adapting to the cloudy and overcast weather. This is due to the updating of the full parame- ters, despite that Direct-Test already shows proficient per- formance in these conditions. As a result, the performance in the later domains is worse than that of the Direct-Test. DUA, which updates batch statistics using EMA, shows a gradual decrease in performance as the domain contin- uously changes, resulting in much lower performance in the original clear domain ( i.e., clear ). On the other hand, NORM, which utilizes the statistics of the current batch samples, exhibits no catastrophic forgetting and relatively good adaptation, as SHIFT is a relatively easier task com- pared to COCO due to having only 6 classes. Compared to NORM, Ours shows better adaptation performance, and is 6Table 2. Comparison of mAP, the number of backward and forward passes, and FPS between baselines and our model on SHIFT-Discrete and SHIFT-Continuous. Baselines perform effectively in a particular setting but lack generalizability across various settings. Our method consistently achieves results that are either better or on par with the best model in all settings, demonstrating its strong stability. Ours-Skip also effectively reduces the number of backward passes without compromising mAP performance, resulting in a higher FPS. SHIFT-Discrete SHIFT-Continuous mAP # step mAP # Avg. step Backbone Method cloudy overc. fog rain dawn night clear Avg. For. Back. FPS clear↔fog clear ↔rain For. Back. FPS Swin-T [26] Direct-Test 50.0 38.9 23.1 45.1 26.9 39.5 45.9 38.5 15.3K 0 27.5 18.1 21.1 4K 0 28.3 ActMAD 49.8 38.4 21.4 43.1 19.0 32.0 44.8 35.5 15.3K 15.3K 9.3 15.6 16.3 4K 4K 9.8 Mean-Teacher 50.0 39.2 25.7 45.4 26.0 37.5 42.2 38.0 15.3K 15.3K 7.8 20.4 24.3 8K 4K 6.5 Ours 50.3 39.2 32.2 46.7 30.4 39.9 44.3 40.4 15.3K 15.3K 11.2 23.9 22.6 4K 4K 11.6 Ours-Skip 50.3 39.7 29.1 47.1 30.2 41.5 45.9 40.6 15.3K 6.1K 20.0 25.1 23.8 4K 0.83K 19.2 ResNet50 [10] Direct-Test 49.4 37.9 19.7 43.1 20.1 35.3 45.6 35.9 15.3K 0 30.1 12.1 15.4 4K 0 30.0 NORM 49.7 38.6 22.9 44.7 25.1 37.4 45.5 37.7 15.3K 0 30.1 16.9 19.4 4K 0 30.0 DUA 45.2 31.5 27.7 31.9 15.2 18.6 21.1 27.3 15.3K 0 30.1 22.5 22.4 4K 0 30.0 ActMAD 49.2 37.7 18.0 40.6 16.0 32.9 44.3 34.1 15.3K 15.3K 11.3 12.7 16.3 4K 4K 11.2 Mean-Teacher 49.6 38.4 26.8 43.4 26.6 33.1 41.6 37.1 15.3K 15.3K 9.9 16.0 20.8 8K 4K 9.8 Ours 49.7 38.7 27.4 46.3 27.4 37.6 43.8 38.7 15.3K 15.3K 12.9 20.9 21.9 4K 4K 13.9 Ours-Skip 49.7 38.8 26.9 46.2 27.6 38.8 45.0 39.0 15.3K 8.9K 21.5 20.0 22.5 4K 0.75K 21.3 Table 3. Comparison of adaptation performance (mAP), the num- ber of trainable parameters (# Params), and memory usage (Cache) according to which part of the backbone is updated. SD / SC de- notes SHIFT-Discrete/Continuous, respectively. mAP # Params Cache Backbone Trainable Params SD SC Num Ratio Avg. Max Swin-T Full-params 38.4 20.6 27.7M 100% 0.86 11.0 LayerNorm 38.5 20.0 0.03M 0.1% 0.65 7.49 adaptor (Ours) 40.4 23.2 0.15M 0.5% 0.65 6.96 ResNet50 Full-params 37.6 20.4 23.7M 100% 1.65 9.29 BatchNorm 37.9 20.2 0.05M 0.2% 1.47 9.11 adaptor (Ours) 38.7 21.7 0.21M 0.9% 1.48 5.41 also applicable to BN-layer-free Swin Transformers. SHIFT-Continuous. In scenarios where the test domain gradually changes across the entire sequence, Ours also demonstrates effectiveness, improving mAP by up to 7%p, as shown in the right section of Tab. 2. WhileDUA performs well in the clear to foggy transition, it is prone to catas- trophic forgetting in situations where the sequence becomes longer, and the test domain changes more diversely, as seen in the left section. Our strategy for determining when model adaptation is necessary is particularly effective in SHIFT. It improves FPS by about 9, reaching about 20 FPS, while en- hancing mAP. This is likely due to avoiding overfitting that can occur when adapting to all repetitive frames in SHIFT, which consists of continuous frames, leading to improve- ments in both inference speed and adaptation performance. 4.5. Additional Analyses We aim to demonstrate the effectiveness and detailed anal- ysis of our proposed model in terms of 1) which parts of, 2) how, and 3) when the model should be updated. Which part to update? Tab. 3 shows how updating dif- ferent parts of the backbone model affects the performance and the memory usage during continual test-time adapta- Table 4. Ablation on each component of our loss. SHIFT-D / C denotes SHIFT-Discrete / Continuous, respectively. The left and right value in each cell corresponds to the mAP for the Swin-T and ResNet50 backbone, respectively. Limg Lobj COCO SHIFT-D. SHIFT-C. - - 17.7/ 17.3 38.5/ 35.9 19.6/ 13.8 ✔ - 16.7/ 18.1 36.6/ 37.0 19.1/ 16.0 ✔ no class weight 17.8/ 18.9 39.7/ 38.0 25.1/ 23.4 ✔ class weight wk,t 22.6/ 22.2 40.4/ 38.7 23.2/ 21.7 tion. We compare (1) updating full parameters, (2) affine parameters of the normalization layer, and (3) our proposed adaptor for each backbone on the SHIFT dataset. Although our adaptor has fewer parameters, about 0.9% or less of the full parameters, it demonstrates the best adaptation perfor- mance. Updating only the affine parameters of the normal- ization layer, while having fewer parameters, seems less ef- fective for adaptation in object detection compared to clas- sification [30, 43]. Additionally, our adaptor requires only about 60% of the memory compared to updating the full parameters, making it memory-efficient. Ablation study on each component in our loss. Tab. 4 presents the effects of image-level feature alignment,Limg, object-level feature class-wise alignment Lobj, and class frequency weighting wk,t proposed to address class im- balance. Aligning only the image-level feature distribu- tion with Limg (first row) leads to modest adaptation in the ResNet50 backbone, while performance in the Swin- T backbone is even lower than without adaptation. No- tably, aligning object-level features with Lobj leads to a substantial improvement, with the mAP increasing by approximately 10%p compared to the no-adaptation sce- nario. Introducing class-specific frequency-based weighting wk,t, despite a slight performance decrease in the SHIFT- Continuous setting, proves highly effective, particularly in scenarios with significant class imbalance, such as COCO 7(a) Swin Transformer backbone  (b) ResNet50 backbone Figure 4. Comparison of mAP and FPS fromOurs-Skip with vary- ing values of τ1 (♦) and τ2 (▲) against Evenly-Skip (×), adapting every N-th instances, on COCO→COCO-C using both (a) Swin- T and (b) ResNet50. The upward and rightward movement indi- cates a better strategy with higher mAP and faster inference speed, showing that Ours-Skip is consistently better than Evenly-Skip. (a) Accumulated number of backward steps (b) Number of backward steps and mAP of Direct-Test in each domain Figure 5. Analysis of the adaptation of Ours-Skip. with 80 classes, where it enhances the mAP by around 5%p. Trade-off between adaptation performance and effi- ciency according to different skipping strategies. Fig. 4 presents mAP and FPS depending on the values ofτ1 and τ2 in the Sec. 3.4 on COCO → COCO-C, which are used for two criteria to determine when the adaptation is needed. We also show the simple baselineEvenly-Skip, which adapts ev- ery N-th step and skips the rest. In Fig. 4, the blue lines (▲) show the results when τ1 is changing from 1.0 to infinity, where only criterion 2 is used, while τ2 is fixed at 1.05. As τ1 decreases, more adaptation is required, leading to slower FPS but higher mAP. The green lines (♦) show the results of changing τ2, where ‘τ2 = inf’ denotes using only criterion 1, without criterion 2. For all main experiments, we set τ1 and τ2 as 1.1 and 1.05, respectively, considering the balance between mAP and FPS. Additionally, our skipping strategy consistently outperforms Evenly-Skip, achieving higher val- ues in both mAP and FPS. This indicates that our criterion for deciding when to bypass model updates provides an ef- fective balance between accuracy and speed. When do models actually update? We analyze when the model actually skips adaptation and only performs infer- ence or actively utilizes test samples for model adaptation based on the two criteria we propose. This analysis is con- ducted in COCO to COCO-C with 15 corruption domains and 1 original domain. Fig. 5a plots the number of back- ward passes, i.e., the number of batches of test samples used for adaptation, with different values of τ1 for the two backbones. The horizontal and vertical axes represent se- quentially incoming test domains and the cumulative back- ward numbers, respectively. A steep slope in a region in- dicates frequent adaptation, while a gentle slope indicates skipping adaptation, performing only inference. Notably, even without explicit information about when the test do- main changes, the model actively performs adaptation, es- pecially right after the test domain changes. This trend is consistent regardless of changes in τ value or backbone type. Furthermore, it is evident that the number of backward passes is primarily determined by the value ofτ1 rather than the type of backbone, suggesting that a consistent τ1 value can be used irrespective of the backbone. Fig. 5b visually represents the adaptation tendencies by dividing backward steps for each domain in the case of Swin-T backbone with τ1 = 1.1. More clearly, it shows that adaptation occurs ac- tively around the points where each domain changes, and af- terward, adaptation happens intermittently or almost not at all. The light pink bars represent the performance ofDirect- Test, showing that domains with initially high model per- formance tend to have less adaptation, while domains with lower performance initially need more adaptation. In other words, the amount of skipping adaptation is proportional to the amount of the domain shift. Interestingly, the second do- main, ’Shot Noise’, shows almost no adaptation despite the lower performance of the Direct-Test. We conjecture that the preceding domain, ’Gaussian Noise’, shares a similar nature of noise, leading the model to decide that additional adaptation steps may not be necessary. As a result, our skip- ping strategy enables the model to efficiently adapt, consid- ering both the original domain the model is trained on and the previous domain the model has been adapted to. 5. Conclusion We introduce an efficient Continual Test-time Adaptation (CTA) method for object detection in the continually chang- ing domain. Our approach involves 1) lightweight adap- tors, 2) class-wise object-level feature alignment, and 3) skipping unnecessary adaptation. These contributions col- lectively yield a highly efficient and effective adaptation method, showcasing robustness to diverse domain shifts, and achieving notable improvements in mAP performance across various CTA scenarios without serious slowdown in the inference speed. 8What, How, and When Should Object Detectors Update in Continually Changing Test Domains? Supplementary Material 6. Additional Details for Baselines We provide additional implementation details for each base- line model. Our framework incorporates all baseline models using the official code except Mean-Teacher. The results of the experiments are reported based on the optimal hyperpa- rameters that yield the best results in our scenario. ActMAD [29] As ActMAD exclusively conducts experi- ments on the KITTI dataset, where all images have a con- stant height and width (e.g., 370 x 1224), ensuring consis- tent feature map sizes for all samples. ActMAD can easily align them along the spatial axis. However, in the general setting of object detection tasks, such as the COCO bench- mark set, where image sizes and width-to-height ratios vary, aligning feature maps along the spatial axis becomes chal- lenging due to different sizes. To adapt ActMAD to our COCO → COCO-C scenario, we perform center cropping on the feature maps to match the size of training domain fea- ture maps and the current test sample feature maps. We em- ploy a learning rate of 1e-5 for COCO and 1e-4 for SHIFT, respectively. Mean-Teacher As the official code of TeST [36] is not available, we implement the EMA-updated Teacher and Student models following TeST [36], to conduct experi- ments in our scenarios. TeST involves three forward steps for a batch: forwarding weakly augmented samples through the student network, strong augmented samples through the teacher network, and original samples through the teacher network for outputs. However, for a fair comparison, we perform two forward steps, forwarding the original sample through the teacher network and strong augmented sam- ples through the student network, to make predictions be- fore adaptation for every samples. We utilize a learning rate of 1e-5 and set the EMA update rate for the teacher network to 0.999. NORM [35] We set the hyperparameter N that controls the trade-off between training statistics and estimated tar- get statistics as 128. DUA [28] We set the momentum decay as 0.94, minimum momentum constant as 1e-4, and the initial momentum de- cay as 1e-3. 7. The effect of Bottleneck Reduction Ratio in the Adaptor Table 5 shows the results for COCO → COCO-C, SHIFT- Discrete, and SHIFT-Continuous based on the dimension reduction ratio ( r) discussed in Section 3.2, representing Table 5. Comparison of adaptation performance (mAP), the num- ber of trainable parameters (# Params), and memory usage (Cache) according to r of Sec. 3.2, the bottleneck reduction ratio in the adaptor. We set r as 32 for all our experiments in the main paper. SD / SC denotes SHIFT-Discrete / Continuous, respectively. mAP # Params Cache Backbone r COCO SD SC Num Ratio Avg. Max Swin-T 1 22.6 40.0 21.3 4.33M 15.7% 0.75 7.51 2 22.6 40.3 23.2 2.17M 7.85% 0.73 7.27 4 22.6 40.4 23.2 1.09M 3.95% 0.70 7.06 8 22.6 40.4 23.2 0.55M 2.00% 0.69 7.00 16 22.6 40.4 23.2 0.28M 1.02% 0.67 6.98 32 22.6 40.4 23.2 0.15M 0.54% 0.65 6.96 64 22.6 40.4 23.2 0.08M 0.29% 0.65 6.95 ResNet50 1 22.5 38.7 20.8 6.31M 26.7% 1.55 5.89 2 22.4 38.7 20.9 3.16M 13.4% 1.51 5.64 4 22.3 38.6 21.3 1.59M 6.71% 1.49 5.52 8 22.3 38.6 21.4 0.80M 3.39% 1.48 5.46 16 22.2 38.6 21.4 0.41M 1.73% 1.48 5.43 32 22.2 38.7 21.4 0.21M 0.89% 1.48 5.41 64 22.1 38.7 21.3 0.11M 0.48% 1.48 5.40 the ratio of bottleneck size compared to the input size in the adaptor. The adaptation performance remains consistent across different r values. However, in the case of r = 1 in SHIFT experiments, mAP decreases, potentially due to catastrophic forgetting resulting from a large number of adaptable parameters. Since increasing the value of r sig- nificantly reduces the number of learnable parameters and memory usage, we set r to 32 in all other experiments. 8. Results on the KITTI Dataset We conduct additional experiments on the KITTI [8] dataset, the commonly used object detection dataset consist- ing of driving scenes with 8 classes (car, van, truck, person, person sitting, cyclist, tram, misc). To simulate the continu- ally changing domains, we use the following scenario ( Fog → Rain → Snow → Clear) as done in [29]. We use the physics-based rendered dataset [9] forfog and rain and sim- ulate snow using the corruption library from [11]. We use the same split of [29], which divides the 7,441 training sam- ples into 3,740 training and 3,741 test samples. We train the Faster-RCNN using 3,741 training samples representing the Clear attribute with Swin-Transformer and ResNet50 back- bones, and evaluate it sequentially on Fog, Rain, Snow, and Clear test samples. We conduct all experiments with a batch size of 16 on 1 RTX A6000 GPU. Table 6 shows the mAP@50, the num- 1Table 6. Comparison of mAP, the number of backward and forward passes, FPS, and memory usage between baselines and our models on the continually changing KITTI datasets ( Fog → Rain → Snow → Clear). Our models improve mAP@50 by 15.1 and 11.3 for Swin-T and ResNet50 backbone, respectively, compared to Direct-Test while maintaining comparable FPS. All experiments are conducted with a batch size of 16. mAP@50 # For. Steps # Backward Steps FPS Cache Backbone Method Fog Rain Snow Clear Avg. All Fog Rain Snow Clear All Avg. Avg. Max Swin-T Direct-Test 46.9 69.5 28.7 89.6 58.7 936 0 0 0 0 0 24.7 0.4 5.5 ActMAD 53.3 78.1 41.2 90.7 65.8 936 234 234 234 234 936 16.8 0.8 21.9 Mean-Teacher 54.5 80.2 43.2 92.4 67.6 936 234 234 234 234 936 10.0 1.0 22.6 Ours 56.7 82.1 64.6 91.8 73.8 936 234 234 234 234 936 17.1 0.4 11.8 Ours-Skip 57.4 81.5 64.3 91.3 73.6 936 234 65 224 36 559 22.9 0.4 11.8 ResNet50 Direct-Test 33.4 63.5 29.8 88.6 53.8 936 0 0 0 0 0 27.7 0.8 4.3 NORM 38.4 66.4 35.9 87.3 57.0 936 0 0 0 0 0 27.7 0.8 4.3 DUA 34.8 67.7 30.9 89.0 55.6 936 0 0 0 0 0 27.7 0.8 4.3 ActMAD 40.4 66.5 42.7 84.5 58.5 936 234 234 234 234 936 18.5 1.6 22.6 Mean-Teacher 39.6 71.3 43.5 88.2 60.6 936 234 234 234 234 936 11.1 1.8 31.1 Ours 45.6 71.4 52.5 88.3 64.5 936 234 234 234 234 936 18.8 0.8 9.4 Ours-Skip 45.8 71.3 50.9 88.4 64.1 936 234 111 98 45 488 24.5 0.8 9.4 (a) GT bounding boxes. (b) Prediction results of Direct-Test. (c) Prediction results of Ours. Figure 6. Results of COCO images corrupted by Shot-Noise. In the analysis of Sec. 4.5, we conjecture that Ours largely skips adaptation in Shot-Noise domain, despite the low mAP of Direct-Test, because the model has already adapted to a similar domain, Gaussian-Noise. In (c), at the first step before adaptation to the Shot-Noise, our model already predicts ’Oven’ and ’Refrigerator’ which Direct-Test fails to detect. This results in a much faster adaptation, and Ours successfully detects various objects, including rare ones such as ’Fire Hydrants’, in the remaining images of the Shot-Noise domain. ber of forward and backward steps, FPS, and memory usage (Cache). Ours improves the mAP@50 by 15.1 and 10.7 for Swin-T and ResNet50 backbones, respectively, compared to Direct-Test. Compared to ActMAD and Mean-Teacher, our model not only improves the adaptation performance but also reduces memory usage, as we update only an ex- tremely small number of parameters of the adaptor. Further- more, using our skipping criteria of Sec. 3.4 with τ = 1.1 and β = 1.05, we can improve FPS by more than 5.8 with- out sacrificing mAP@50, resulting in much faster inference 2(a) GT bounding boxes. (b) Prediction results of Direct-Test. (c) Prediction results of Ours. Figure 7. Results for COCO images corrupted by Pixelate. In the Pixelate domain, where the model has already experienced various corruptions in a long sequence, Ours initially incorrectly detects objects. In (c), it misidentifies a bed as a couch in the first step. However, it rapidly adapts to the Pixelate domain and effectively detects various objects. Notably, even in cases whereDirect-Testcorrectly identifies objects but with low confidence, Ours detects them with much higher confidence. (a) GT bounding boxes. (b) Prediction results of Direct-Test. (c) Prediction results of Ours. Figure 8. Results for SHIFT-Discrete with continually changing attributes, foggy → rainy → dawn → night. speed compared to other TTA baselines. 39. Qualitative Results Fig. 6 and 7 and Fig. 8 show the qualitative results of Ours and Direct-Test which predict the samples without adapta- tion for COCO → COCO-C and SHIFT, respectively. 9.1. COCO → COCO-C Fig. 6 and 7 compare the prediction results for COCO im- ages corrupted. When the model encounters test images with various corruptions sequentially ( Gaussian-Noise → Shot-Noise → Impulse-Noise → Defocus-Blur → Glass- Blur → Motion-Blur → Zoom-Blur → Snow → Frost → Fog → Brightness → Contrast → Elastic-Transform → Pixelate → JPEG-Compression → Original), Fig. 6 and 7 shows the results when the test images are corrupted by Shot-Noise and Pixelate, respectively. Compared to Direct- Test, our model adapts to the current domain within a few steps, such as 100 iterations, and detects various objects very well in the remaining incoming images. 9.2. SHIFT-Discrete Fig. 8 shows the qualitative results for SHIFT-Discrete. In the SHIFT-Discrete scenario, the model encounters environ- ments sequentially, transitioning from cloudy → overcast → foggy → rainy → dawn → night → clear. Figure. 8 se- lectively shows the foggy → rainy → dawn → night se- quence, where the domain gap from the original clear envi- ronments is relatively large. Compared to Direct-Test, Ours detects various objects such as ’cars’ and ’pedestrians’ re- gardless of distribution changes. References [1] Alexander Bartler, Florian Bender, Felix Wiewel, and Bin Yang. Ttaps: Test-time adaption by aligning prototypes using self-supervision. In 2022 International Joint Conference on Neural Networks (IJCNN), pages 1–8. IEEE, 2022. 2 [2] Alexander Bartler, Andre B ¨uhler, Felix Wiewel, Mario D¨obler, and Bin Yang. Mt3: Meta test-time training for self- supervised test-time adaption. In International Conference on Artificial Intelligence and Statistics , pages 3080–3090. PMLR, 2022. 2 [3] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 8344–8353, 2022. 1 [4] Dhanajit Brahma and Piyush Rai. A probabilistic frame- work for lifelong test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3582–3591, 2023. 3 [5] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yib- ing Song, Jue Wang, and Ping Luo. Adaptformer: Adapting vision transformers for scalable visual recognition.Advances in Neural Information Processing Systems, 35:16664–16678, 2022. 3 [6] Yijin Chen, Xun Xu, Yongyi Su, and Kui Jia. Stfar: Im- proving object detection robustness at test-time by self- training with feature alignment regularization.arXiv preprint arXiv:2303.17937, 2023. 1, 2, 3 [7] Jinhong Deng, Wen Li, Yuhua Chen, and Lixin Duan. Un- biased mean teacher for cross-domain object detection. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 4091–4101, 2021. 3 [8] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The Inter- national Journal of Robotics Research , 32(11):1231–1237, 2013. 1 [9] Shirsendu Sukanta Halder, Jean-Franc ¸ois Lalonde, and Raoul de Charette. Physics-based rendering for improving robustness to rain. In Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision, pages 10203–10212, 2019. 1 [10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. 5, 6, 7 [11] Dan Hendrycks and Thomas Dietterich. Benchmarking neu- ral network robustness to common corruptions and perturba- tions. arXiv preprint arXiv:1903.12261, 2019. 1 [12] Junyuan Hong, Lingjuan Lyu, Jiayu Zhou, and Michael Spranger. Mecta: Memory-economic continual test-time model adaptation. In The Eleventh International Conference on Learning Representations, 2022. 3 [13] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen- Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 3 [14] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal co- variate shift. In International conference on machine learn- ing, pages 448–456. pmlr, 2015. 2 [15] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for modelagnostic domain generaliza- tion. In Advances in Neural Information Processing Systems (NeurIPS), 2021. 3 [16] Minguk Jang, Sae-Young Chung, and Hye Won Chung. Test- time adaptation via self-training with nearest neighbor infor- mation. In International Conference on Learning Represen- tations (ICLR), 2023. 3 [17] Sanghun Jung, Jungsoo Lee, Nanhee Kim, Amirreza Sha- ban, Byron Boots, and Jaegul Choo. Cafa: Class-aware fea- ture alignment for test-time adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vi- sion, 2023. 2, 3, 4 [18] Mehran Khodabandeh, Arash Vahdat, Mani Ranjbar, and William G Macready. A robust learning approach to domain adaptive object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 480– 490, 2019. 3 [19] Seunghyeon Kim, Jaehoon Choi, Taekyung Kim, and Chang- ick Kim. Self-training and adversarial background regular- ization for unsupervised domain adaptive one-stage object 4detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6092–6101, 2019. 3 [20] Xianfeng Li, Weijie Chen, Di Xie, Shicai Yang, Peng Yuan, Shiliang Pu, and Yueting Zhuang. A free lunch for unsuper- vised domain adaptive object detection without source data. In Proceedings of the AAAI Conference on Artificial Intelli- gence, pages 8474–8481, 2021. 4 [21] Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou. Revisiting batch normalization for practical do- main adaptation, 2017. 3 [22] Hyesu Lim, Byeonggeun Kim, Jaegul Choo, and Sungha Choi. Ttn: A domain-shift aware batch normalization in test- time adaptation, 2023. 2, 3 [23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740–755. Springer, 2014. 5 [24] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyra- mid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 2117–2125, 2017. 5 [25] Yuejiang Liu, Parth Kothari, Bastien Van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? Advances in Neural Information Processing Systems , 34: 21808–21820, 2021. 1, 2 [26] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012–10022, 2021. 2, 5, 6, 7 [27] Claudio Michaelis, Benjamin Mitzkus, Robert Geirhos, Evgenia Rusak, Oliver Bringmann, Alexander S Ecker, Matthias Bethge, and Wieland Brendel. Benchmarking ro- bustness in object detection: Autonomous driving when win- ter is coming. arXiv preprint arXiv:1907.07484, 2019. 5 [28] M Jehanzeb Mirza, Jakub Micorek, Horst Possegger, and Horst Bischof. The norm must go on: Dynamic unsuper- vised domain adaptation by normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition, pages 14765–14775, 2022. 3, 6, 1 [29] Muhammad Jehanzeb Mirza, Pol Jan ´e Soneira, Wei Lin, Ma- teusz Kozinski, Horst Possegger, and Horst Bischof. Act- mad: Activation matching to align distributions for test-time- training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 24152– 24161, 2023. 1, 2, 3, 4, 6 [30] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test-time model adaptation without forgetting. In Interna- tional conference on machine learning, pages 16888–16905. PMLR, 2022. 1, 2, 3, 7 [31] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan. Towards stable test-time adaptation in dynamic wild world. arXiv preprint arXiv:2302.12400, 2023. 3 [32] Mario obler, Robert A Marsden, and Bin Yang. Robust mean teacher for continual and gradual test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 7704–7714, 2023. 3 [33] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. 2016. 5 [34] Aruni RoyChowdhury, Prithvijit Chakrabarty, Ashish Singh, SouYoung Jin, Huaizu Jiang, Liangliang Cao, and Erik Learned-Miller. Automatic adaptation of object detectors to new domains using self-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 3 [35] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bring- mann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. Advances in neural information processing sys- tems, 33:11539–11551, 2020. 3, 6, 1 [36] Samarth Sinha, Peter Gehler, Francesco Locatello, and Bernt Schiele. Test: Test-time self-training under distribution shift. In Proceedings of the IEEE/CVF Winter Conference on Ap- plications of Computer Vision, pages 2759–2769, 2023. 1, 2, 3, 6 [37] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. Advances in neural information processing systems, 33:596– 608, 2020. 2, 6 [38] Yongyi Su, Xun Xu, and Kui Jia. Revisiting realistic test- time training: Sequential inference and adaptation by an- chored clustering. Advances in Neural Information Process- ing Systems, 35:17543–17555, 2022. 3, 4 [39] Tao Sun, Mattia Segu, Janis Postels, Yuxuan Wang, Luc Van Gool, Bernt Schiele, Federico Tombari, and Fisher Yu. Shift: a synthetic driving dataset for continuous multi-task domain adaptation. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 21371–21382, 2022. 1, 5 [40] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self- supervision for generalization under distribution shifts. In International conference on machine learning, pages 9229– 9248. PMLR, 2020. 1, 2 [41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 2 [42] Vibashan VS, Poojan Oza, and Vishal M Patel. Towards on- line domain adaptive object detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 478–488, 2023. 3 [43] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Ol- shausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. arXiv preprint arXiv:2006.10726, 2020. 1, 2, 3, 7 5[44] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7201–7211, 2022. 3 [45] Zehao Xiao, Xiantong Zhen, Shengcai Liao, and Cees GM Snoek. Energy-based test sample adaptation for domain gen- eralization. arXiv preprint arXiv:2302.11215, 2023. 3 [46] Mengde Xu, Zheng Zhang, Han Hu, Jianfeng Wang, Lijuan Wang, Fangyun Wei, Xiang Bai, and Zicheng Liu. End- to-end semi-supervised object detection with soft teacher. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. 5 [47] Marvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and augmentation. Advances in Neural Information Processing Systems , 35: 38629–38642, 2022. 1, 2, 3 [48] Bowen Zhao, Chen Chen, and Shu-Tao Xia1. Delta: Degradation-free fully test-time adaptation. In International Conference on Learning Representations (ICLR), 2023. 2, 3 6",
      "meta_data": {
        "arxiv_id": "2312.08875v1",
        "authors": [
          "Jayeon Yoo",
          "Dongkwan Lee",
          "Inseop Chung",
          "Donghyun Kim",
          "Nojun Kwak"
        ],
        "published_date": "2023-12-12T07:13:08Z",
        "pdf_url": "https://arxiv.org/pdf/2312.08875v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the challenge of performance degradation in object detectors when encountering continually changing test domains (Continual Test-time Adaptation, CTA). Existing test-time adaptation (TTA) methods often suffer from inefficiency, architecture specificity, or catastrophic forgetting in object detection tasks. The main contributions include: 1) Introducing architecture-agnostic and lightweight adaptor modules (comprising only 0.54% to 0.89% of total parameters) into the object detector's backbone, which are exclusively updated to ensure efficient adaptation, robustness to domain shifts, and prevention of catastrophic forgetting, while keeping the pre-trained backbone frozen. 2) Proposing a practical and straightforward class-wise feature alignment method for object detection that aligns feature distributions at both image and object levels using mean and variance of training data and Exponentially Moving Average (EMA)-updated test data features, incorporating a weighting scheme to address class imbalance. 3) Developing two novel criteria to dynamically determine when adaptation is necessary or when it can be skipped, significantly enhancing inference efficiency without requiring additional resources. The proposed approach achieves substantial mAP improvements (up to 4.9%p on COCO \n COCO-corrupted and 7.9%p on SHIFT) over baselines, maintaining high inference speeds (over 20 FPS).",
        "methodology": "The proposed method focuses on three key aspects: 'What to update', 'How to update', and 'When to update'. For 'What to update', it introduces lightweight adaptor modules in parallel for each block of the object detector's backbone. These adaptors are architecture-agnostic (MLP layers for Transformer blocks and 1x1 convolutional layers for ResNet blocks) and contain a minimal number of parameters (0.54% to 0.89% of the full model). Only these adaptors are updated, while the pre-trained backbone parameters are frozen, ensuring efficiency and preventing catastrophic forgetting. For 'How to update', an EMA feature alignment strategy is employed. It aligns feature distributions between the test and training domains at both image and object levels. Image-level alignment minimizes the KL divergence between Gaussian distributions of pre-computed training features (mean \n_tr, variance \n_tr, from 2,000 training samples) and EMA-updated test features (mean \n_t_te, approximated variance \n_tr). Object-level alignment extends this to class-wise RoI-pooled features, filtering background scores and applying a class-frequency weighting scheme (\nw_k,t) to handle class imbalance. The total adaptation loss combines image-level (L_img) and object-level (L_obj) losses. For 'When to update', two criteria determine if a model update (backward pass) is needed: 1) If the ratio of the current image-level distribution gap (L_img) to an in-domain distribution gap (D_in_KL) exceeds a threshold \nt_1 (set to 1.1). 2) If the ratio of L_img to its exponentially moving average (L_t_ema) exceeds a threshold \nt_2 (set to 1.05). Adaptation proceeds if either criterion is met, otherwise it's skipped for efficiency.",
        "experimental_setup": "The research primarily uses Faster-RCNN as the object detector, with both ResNet50 and Swin-Tiny backbones, augmented with Feature Pyramid Networks (FPN). Experiments are conducted on three main scenarios: 1) COCO \n COCO-C: The model is trained on the MS-COCO dataset and sequentially evaluated on COCO-C, which comprises 15 types of realistic corruptions (e.g., Gaussian Noise, Pixelate, Fog, Brightness) applied to the COCO validation set. Performance is also assessed on the original COCO validation set ('Org.') to measure knowledge preservation. 2) SHIFT-Discrete: A synthetic driving dataset (6 classes) where the model is sequentially evaluated on distinct environmental attributes (e.g., cloudy, foggy, rainy, night), simulating drastic domain shifts. 3) SHIFT-Continuous: The SHIFT dataset is used to simulate gradual domain changes through sequences of frames continuously transitioning between conditions (e.g., clear to foggy/rainy and back). Supplementary experiments are also performed on the KITTI dataset using a Fog \n Rain \n Snow \n Clear sequence. Baselines include Direct-Test (no adaptation), ActMAD (feature alignment), Mean-Teacher (teacher-student network, inspired by TeST), NORM, and DUA (BN statistics adjustment). Hyperparameters are consistent across experiments: SGD optimizer learning rate of 0.001, EMA decay alpha of 0.01, adaptation thresholds \nt_1 = 1.1 and \nt_2 = 1.05, and a batch size of 4 (16 for KITTI). The adaptor bottleneck reduction ratio \nr is set to 32. Evaluation metrics include mAP, number of forward and backward passes, Frames Per Second (FPS), and memory usage (Cache).",
        "limitations": "The method relies on several pragmatic choices that could present limitations under extreme conditions. It utilizes pre-computed training statistics (mean and variance from a small subset of training data) and approximates test feature variance with training variance ( \n_te \n \n_tr) due to typically small batch sizes in object detection, which might not be robust enough for exceptionally drastic and complex domain shifts. The 'adaptation on demand' strategy, while efficient, depends on manually set thresholds (\nt_1 and \nt_2). Finding universally optimal values for these thresholds is a balancing act between adaptation performance and inference speed, and they may require tuning for different scenarios. Although designed for efficiency, the method still introduces computational overhead (forward and backward passes for adaptors and feature computations) and higher memory usage compared to direct inference without any adaptation or simpler BN-only adaptation methods. Furthermore, the class-specific frequency-based weighting (w_k,t), while generally beneficial, showed a slight performance decrease in some gradual domain shift settings (e.g., SHIFT-Continuous), indicating a minor trade-off.",
        "future_research_directions": "Not explicitly mentioned, but potential future research directions could include: 1) Developing adaptive or learned mechanisms for automatically determining the optimal adaptation thresholds (\nt_1, \nt_2) to eliminate manual tuning and dynamically balance performance and efficiency across diverse continual domain shift scenarios. 2) Exploring more sophisticated and data-efficient approaches for estimating test domain feature statistics, particularly variance, which currently relies on an approximation due to small batch sizes, to enhance robustness for highly divergent distributions. 3) Extending the proposed lightweight adaptor framework and feature alignment strategies to other complex computer vision tasks requiring continual test-time adaptation, such as semantic segmentation, instance segmentation, or video processing. 4) Investigating methods to automatically discover the optimal architecture, placement, and bottleneck ratios (\nr) for the adaptors within various backbone models. 5) Integrating the 'adaptation on demand' concept with other complementary TTA paradigms, such as test-time training with self-supervision, to achieve potentially greater overall robustness and efficiency."
      }
    },
    {
      "title": "DELTA: DEGRADATION-FREE FULLY TEST-TIME ADAPTATION",
      "abstract": "Fully test-time adaptation aims at adapting a pre-trained model to the test\nstream during real-time inference, which is urgently required when the test\ndistribution differs from the training distribution. Several efforts have been\ndevoted to improving adaptation performance. However, we find that two\nunfavorable defects are concealed in the prevalent adaptation methodologies\nlike test-time batch normalization (BN) and self-learning. First, we reveal\nthat the normalization statistics in test-time BN are completely affected by\nthe currently received test samples, resulting in inaccurate estimates. Second,\nwe show that during test-time adaptation, the parameter update is biased\ntowards some dominant classes. In addition to the extensively studied test\nstream with independent and class-balanced samples, we further observe that the\ndefects can be exacerbated in more complicated test environments, such as\n(time) dependent or class-imbalanced data. We observe that previous approaches\nwork well in certain scenarios while show performance degradation in others due\nto their faults. In this paper, we provide a plug-in solution called DELTA for\nDegradation-freE fuLly Test-time Adaptation, which consists of two components:\n(i) Test-time Batch Renormalization (TBR), introduced to improve the estimated\nnormalization statistics. (ii) Dynamic Online re-weighTing (DOT), designed to\naddress the class bias within optimization. We investigate various test-time\nadaptation methods on three commonly used datasets with four scenarios, and a\nnewly introduced real-world dataset. DELTA can help them deal with all\nscenarios simultaneously, leading to SOTA performance.",
      "meta_data": {
        "arxiv_id": "2301.13018v1",
        "authors": [
          "Bowen Zhao",
          "Chen Chen",
          "Shu-Tao Xia"
        ],
        "published_date": "2023-01-30T15:54:00Z",
        "pdf_url": "https://arxiv.org/pdf/2301.13018v1.pdf"
      }
    },
    {
      "title": "Test-Time Training with Self-Supervision for Generalization under Distribution Shifts",
      "abstract": "In this paper, we propose Test-Time Training, a general approach for\nimproving the performance of predictive models when training and test data come\nfrom different distributions. We turn a single unlabeled test sample into a\nself-supervised learning problem, on which we update the model parameters\nbefore making a prediction. This also extends naturally to data in an online\nstream. Our simple approach leads to improvements on diverse image\nclassification benchmarks aimed at evaluating robustness to distribution\nshifts.",
      "full_text": "Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Yu Sun1 Xiaolong Wang1 2 Zhuang Liu1 John Miller1 Alexei A. Efros1 Moritz Hardt1 Abstract In this paper, we propose Test-Time Training, a general approach for improving the performance of predictive models when training and test data come from different distributions. We turn a sin- gle unlabeled test sample into a self-supervised learning problem, on which we update the model parameters before making a prediction. This also extends naturally to data in an online stream. Our simple approach leads to improvements on di- verse image classiﬁcation benchmarks aimed at evaluating robustness to distribution shifts. 1. Introduction Supervised learning remains notoriously weak at generaliza- tion under distribution shifts. Unless training and test data are drawn from the same distribution, even seemingly minor differences turn out to defeat state-of-the-art models (Recht et al., 2018). Adversarial robustness and domain adapta- tion are but a few existing paradigms that try to anticipate differences between the training and test distribution with either topological structure or data from the test distribution available during training. We explore a new take on gener- alization that does not anticipate the distribution shifts, but instead learns from them at test time. We start from a simple observation. The unlabeled test sample xpresented at test time gives us a hint about the distribution from which it was drawn. We propose to take advantage of this hint on the test distribution by allowing the model parameters θto depend on the test sample x, but not its unknown label y. The concept of a variable decision boundary θ(x) is powerful in theory since it breaks away from the limitation of ﬁxed model capacity (see additional discussion in Section A1), but the design of a feedback mechanism from xto θ(x) raises new challenges in practice that we only begin to address here. 1University of California, Berkeley 2University of California, San Diego. Correspondence to: Yu Sun <yusun@berkeley.edu>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). Our proposed test-time training method creates a self- supervised learning problem based on this single test sample x, updating θat test time before making a prediction. Self- supervised learning uses an auxiliary task that automatically creates labels from unlabeled inputs. In our experiments, we use the task of rotating each input image by a multiple of 90 degrees and predicting its angle (Gidaris et al., 2018). This approach can also be easily modiﬁed to work outside the standard supervised learning setting. If several test samples arrive in a batch, we can use the entire batch for test-time training. If samples arrive in an online stream, we obtain further improvements by keeping the state of the parameters. After all, prediction is rarely a single event. The online version can be the natural mode of deployment under the additional assumption that test samples are produced by the same or smoothly changing distribution shifts. We experimentally validate our method in the context of object recognition on several standard benchmarks. These include images with diverse types of corruption at various levels (Hendrycks & Dietterich, 2019), video frames of moving objects (Shankar et al., 2019), and a new test set of unknown shifts collected by (Recht et al., 2018). Our algorithm makes substantial improvements under distribu- tion shifts, while maintaining the same performance on the original distribution. In our experiments, we compare with a strong baseline (labeled joint training) that uses both supervised and self- supervised learning at training-time, but keeps the model ﬁxed at test time. Recent work shows that training-time self- supervision improves robustness (Hendrycks et al., 2019a); our joint training baseline corresponds to an improved imple- mentation of this work. A comprehensive review of related work follows in Section 5. We complement the empirical results with theoretical inves- tigations in Section 4, and establish an intuitive sufﬁcient condition on a convex model of when Test-Time Training helps; this condition, roughly speaking, is to have correlated gradients between the loss functions of the two tasks. Project website: https://test-time-training.github.io/. arXiv:1909.13231v3  [cs.LG]  1 Jul 2020Test-Time Training with Self-Supervision for Generalization under Distribution Shifts 2. Method This section describes the algorithmic details of our method. To set up notation, consider a standard K-layer neural net- work with parameters θk for layer k. The stacked parameter vector θ = ( θ1,...,θ K) speciﬁes the entire model for a classiﬁcation task with loss function lm(x,y; θ) on the test sample (x,y). We call this the main task, as indicated by the subscript of the loss function. We assume to have training data (x1,y1),..., (xn,yn) drawn i.i.d. from a distribution P. Standard empirical risk minimization solves the optimization problem: min θ 1 n n∑ i=1 lm(xi,yi; θ). (1) Our method requires a self-supervised auxiliary task with loss function ls(x). In this paper, we choose the rotation prediction task (Gidaris et al., 2018), which has been demon- strated to be simple and effective at feature learning for convolutional neural networks. The task simply rotates x in the image plane by one of 0, 90, 180 and 270 degrees and have the model predict the angle of rotation as a four- way classiﬁcation problem. Other self-supervised tasks in Section 5 might also be used for our method. The auxiliary task shares some of the model parameters θe = ( θ1,...,θ κ) up to a certain κ ∈ {1,...,K }. We designate those κlayers as a shared feature extractor. The auxiliary task uses its own task-speciﬁc parameters θs = (θ′ κ+1,...,θ ′ K). We call the unshared parameters θs the self-supervised task branch, and θm = (θκ+1,...,θ K) the main task branch . Pictorially, the joint architecture is a Y-structure with a shared bottom and two branches. For our experiments, the self-supervised task branch has the same architecture as the main branch, except for the output dimensionality of the last layer due to the different number of classes in the two tasks. Training is done in the fashion of multi-task learning (Caru- ana, 1997); the model is trained on both tasks on the same data drawn fromP. Losses for both tasks are added together, and gradients are taken for the collection of all parameters. The joint training problem is therefore min θe,θm,θs 1 n n∑ i=1 lm(xi,yi; θm,θe) + ls(xi; θs,θe). (2) Now we describe the standard version of Test-Time Training on a single test sample x. Simply put, Test-Time Training ﬁne-tunes the shared feature extractor θe by minimizing the auxiliary task loss on x. This can be formulated as min θe ls(x; θs,θe). (3) Denote θ∗ e the (approximate) minimizer of Equation 3. The model then makes a prediction using the updated parameters θ(x) = (θ∗ e,θm). Empirically, the difference is negligible between minimizing Equation 3 over θe versus over both θe and θs. Theoretically, the difference exists only when optimization is done with more than one gradient step. Test-Time Training naturally beneﬁts from standard data augmentation techniques. On each test sample x, we per- form the exact same set of random transformations as for data augmentation during training, to form a batch only con- taining these augmented copies of xfor Test-Time Training. Online Test-Time Training. In the standard version of our method, the optimization problem in Equation 3 is al- ways initialized with parameters θ= (θe,θs) obtained by minimizing Equation 2. After making a prediction on x, θ∗ e is discarded. Outside of the standard supervised learning setting, when the test samples arrive online sequentially, the online version solves the same optimization problem as in Equation 3 to update the shared feature extractor θe. How- ever, on test sample xt, θis instead initialized with θ(xt−1) updated on the previous sample xt−1. This allows θ(xt) to take advantage of the distributional information available in x1,...,x t−1 as well as xt. 3. Empirical Results We experiment with both versions of our method (standard and online) on three kinds of benchmarks for distribution shifts, presented here in the order of visually low to high- level. Our code is available at the project website. Network details. Our architecture and hyper-parameters are consistent across all experiments. We use ResNets (He et al., 2016b), which are constructed differently for CIFAR-10 (Krizhevsky & Hinton, 2009) (26-layer) and Ima- geNet (Russakovsky et al., 2015) (18-layer). The CIFAR-10 dataset contains 50K images for training, and 10K images for testing. The ImageNet contains 1.2M images for train- ing and the 50K validation images are used as the test set. ResNets on CIFAR-10 have three groups, each containing convolutional layers with the same number of channels and size of feature maps; our splitting point is the end of the second group. ResNets on ImageNet have four groups; our splitting point is the end of the third group. We use Group Normalization (GN) instead of Batch Nor- malization (BN) in our architecture, since BN has been shown to be ineffective when training with small batches, for which the estimated batch statistics are not accurate (Ioffe & Szegedy, 2015). This technicality hurts Test-Time Training since each batch only contains (augmented) copies of a single image. Different from BN, GN is not dependent on batch size and achieves similar results on our baselines.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40 50Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure 1.Test error (%) on CIFAR-10-C with level 5 corruptions.We compare our approaches, Test-Time Training (TTT) and its online version (TTT-Online), with two baselines: object recognition without self-supervision, and joint training with self-supervision but keeping the model ﬁxed at test time. TTT improves over the baselines and TTT-Online improves even further. We report results with BN in Section A4 of the appendix for completeness. We directly compare our architecture to that of Hendrycks et al. (2018) in subsection A4.5. Optimization details. For joint training (Equation 2), we use stochastic gradient descent with standard hyper- parameters as (Huang et al., 2016; He et al., 2016a). For Test-Time Training (Equation 3), we use stochastic gradient descent with the learning rate set to that of the last epoch during training, which is 0.001 in all our experiments. We set weight decay and momentum to zero during Test-Time Training, inspired by practice in (He et al., 2018; Liu et al., 2018). For the standard version of Test-Time Training, we take ten gradient steps, using batches independently gener- ated by the same image. For online version of Test-Time Training, we take only one gradient step given each new im- age. We use random crop and random horizontal ﬂip for data augmentation. See Section A2 of the appendix for computa- tional aspects of our method. In all the tables and ﬁgures, object recognition task onlyrefers to the plain ResNet model (using GN, unless otherwise speciﬁed); joint training refers to the model jointly trained on both the main task and the self-supervised task, ﬁxed at test time; this has been pro- posed as the method in Hendrycks et al. (2019a); Test-Time Training (TTT) refers to the standard version described sec- tion 2; and online Test-Time Training (TTT-Online)refers to the online version that does not discardθ(xt) for xt arriving sequentially from the same distribution. Performance for TTT-Online is calculated as the average over the entire test set; we always shufﬂe the test set before TTT-Online to avoid ordering artifacts. 3.1. Object Recognition on Corrupted Images Hendrycks & Dietterich (2019) propose to benchmark ro- bustness of object recognition with 15 types of corruptions from four broad categories: noise, blur, weather and digital. Each corruption type comes in ﬁve levels of severity, with level 5 the most severe (details and sample images in the ap- pendix). The corruptions are simulated to mimic real-world corruptions as much as possible on copies of the test set for both CIFAR-10 and ImageNet. The new test sets are named as CIFAR-10-C and ImageNet-C, respectively. In the pro- posed benchmark, training should be done on the original training set, and the diversity of corruption types should make it difﬁcult for any methods to work well across the board if it relies too much on corruption speciﬁc knowledge. For online Test-Time Training, we take the entire test set as a stream of incoming images, and update and test on each image in an online manner as it arrives. CIFAR-10-C. Our results on the level 5 corruptions (most severe) are shown in Figure 1. The results on levels 1-4 are shown in Section A4 in appendix. Across all ﬁve levels and 15 corruption types, both standard and online versions of Test-Time Training improve over the object recognition task only baseline by a large margin. The standard version always improves over joint training, and the online version often improves signiﬁcantly (>10%) over joint training and never hurts by more than 0.2%. Speciﬁcally, TTT-Online contributes >24% on the three noise types and 38% on pix- elation. For a learning problem with the seemingly unstable setup that abuses a single image, this kind of consistency is rather surprising. The baseline ResNet-26 with object recognition task only has error 8.9% on the original test set of CIFAR-10. The joint training baseline actually improves performance on the original to 8.1%. More surprisingly, unlike many other methods that trade off original performance for robustness, Test-Time Training further improves on the original test set by 0.2% consistently over multiple independent trials. This suggests that our method does not choose between speciﬁcity and generality.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 20 40 60Accuracy (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online 0 20000 40000 Number of samples 60 62 64 66 68 70 72 74 76Accuracy (%) Original Sliding window average 0 20000 40000 Number of samples 12 15 18 21 24 27 30 33Accuracy (%) Gaussian Noise Sliding window average 0 20000 40000 Number of samples 16 18 20 22 24 26 28 30 32Accuracy (%) Defocus Blur Sliding window average 0 20000 40000 Number of samples 28 30 32 34 36 38Accuracy (%) Zoom Blur Sliding window average 0 20000 40000 Number of samples 33 36 39 42 45 48 51 54Accuracy (%) Fog Sliding window average 0 20000 40000 Number of samples 30 33 36 39 42 45 48 51Accuracy (%) Elastic Transform Sliding window average Figure 2.Test accuracy (%) on ImageNet-C with level 5 corruptions.Upper panel: Our approaches, TTT and TTT-Online, show signiﬁcant improvements in all corruption types over the two baselines. Lower panel: We show the accuracy of TTT-Online as the average over a sliding window of 100 samples; TTT-Online generalizes better as more samples are evaluated (x-axis), without hurting on the original distribution. We use accuracy instead of error here because the baseline performance is very low for most corruptions. Separate from our method, it is interesting to note that joint training consistently improves over the single-task baseline, as discovered by Hendrycks et al. (2019a). Hendrycks & Dietterich (2019) have also experimented with various other training methods on this benchmark, and point to Adversar- ial Logit Pairing (ALP) (Kannan et al., 2018) as the most effective approach. Results of this additional baseline on all levels of CIFAR-10-C are shown in the appendix, along with its implementation details. While surprisingly robust under some of the most severe corruptions (especially the three noise types), ALP incurs a much larger error (by a factor of two) on the original distribution and some corruptions (e.g. all levels of contrast and fog), and hurts performance signiﬁcantly when the corruptions are not as severe (espe- cially on levels 1-3); this kind of tradeoff is to be expected for methods based on adversarial training. ImageNet-C. Our results on the level 5 corruptions (most severe) are shown in Figure 2. We use accuracy instead of error for this dataset because the baseline performance is very low for most corruptions. The general trend is roughly the same as on CIFAR-10-C. The standard version of TTT always improves over the baseline and joint training, while the online version only hurts on the original by 0.1% over the baseline, but signiﬁcantly improves (by a factor of more than three) on many of the corruption types. In the lower panel of Figure 2, we visualize how the accu- racy (averaged over a sliding window) of the online version changes as more images are tested. Due to space constraints, we show this plot on the original test set, as well as every third corruption type, following the same order as in the original paper. On the original test set, there is no visible trend in performance change after updating on the 50,000 samples. With corruptions, accuracy has already risen sig- niﬁcantly after 10,000 samples, but is still rising towards the end of the 50,000 samples, indicating room for additional improvements if more samples were available. Without seeing a single label, TTT-Online behaves as if we were training on the test set from the appearance of the plots.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg TTT-Online 8.2 25.8 22.6 30.6 14.6 34.4 18.3 17.1 20.0 18.0 16.9 11.2 15.6 21.6 18.1 21.2 UDA-SS 9.0 28.2 26.5 20.8 15.6 43.7 24.5 23.8 25.0 24.9 17.2 12.7 11.6 22.1 20.3 22.6 Table 1.Test error (%) on CIFAR-10-C with level 5 corruption.Comparison between online Test-Time Training (TTT-Online) and unsupervised domain adaptation by self-supervision (UDA-SS) (Sun et al., 2019) with access to the entire (unlabeled) test set during training. We highlight the lower error in bold. We have abbreviated the names of the corruptions, in order: original test set, Gaussian noise, shot noise, impulse noise, defocus blur, glass blue, motion blur, zoom blur, snow, frost, fog, brightness, contrast, elastic transformation, pixelation, and JPEG compression. The reported numbers for TTT-Online are the same as in Figure 1. See complete table in Table A2. 0 2000 4000 6000 8000 Number of samples 12 16 20 24 28 32 36 40 44 48Error (%) Gaussian Noise Joint training TTT TTT-Online UDA-SS 0 2000 4000 6000 8000 Number of samples 9 12 15 18 21 24 27 30 33 36Error (%) Shot Noise Joint training TTT TTT-Online UDA-SS 0 2000 4000 6000 8000 Number of samples 15 20 25 30 35 40 45 50Error (%) Impulse Noise Joint training TTT TTT-Online UDA-SS Figure 3.Test error (%) on CIFAR-10-C, for the three noise types, with gradually changing distribution.The distribution shifts are created by increasing the standard deviation of each noise type from small to large, the further we go on the x-axis. As the samples get noisier, all methods suffer greater errors the more we evaluate into the test set, but online Test-Time Training (TTT-Online) achieves gentler slopes than joint training. For the ﬁrst two noise types, TTT-Online also achieves better results over unsupervised domain adaptation by self-supervision (UDA-SS) (Sun et al., 2019). Comparison with unsupervised domain adaptation. Table 1 empirically compares online Test-Time Training (TTT-Online) with unsupervised domain adaptation through self-supervision (UDA-SS) (Sun et al., 2019), which is sim- ilar to our method in spirit but is designed for the setting of unsupervised domain adaptation (Section 5 provides a sur- vey of other related work in this setting). Given labeled data from the training distribution and unlabeled data from the test distribution, UDA-SS hopes to ﬁnd an invariant repre- sentation that extracts useful features for both distributions by learning to perform a self-supervised task, speciﬁcally rotation prediction, simultaneously on data from both. It then learns a labeling function on top of the invariant rep- resentation using the labeled data. In our experiments, the unlabeled data given to UDA-SS is the entire test set itself without the labels. Because TTT-Online can only learn from the unlabeled test samples that have already been evaluated on, it is given less information than UDA-SS at all times. In this sense, UDA- SS should be regarded as an oracle rather than a baseline. Surprisingly, TTT-Online outperforms UDA-SS on 13 out of the 15 corruptions as well as the original distribution. Our explanation is that UDA-SS has to ﬁnd an invariant representation for both distributions, while TTT-Online only adapts the representation to be good for the current test distribution. That is, TTT-Online has the ﬂexibility to forget the training distribution representation, which is no longer relevant. This suggests that in our setting, forgetting is not harmful and perhaps should even be taken advantage of. Gradually changing distribution shifts.In our previous experiments, we have been evaluating the online version under the assumption that the test inputs xt for t= 1...nare all sampled from the same test distribution Q, which can be different from the training distribution P. This assumption is indeed satisﬁed for i.i.d. samples from a shufﬂed test set. But here we show that this assumption can in fact be relaxed to allow xt ∼Qt, where Qt is close to Qt+1 (in the sense of distributional distance). We call this the assumption of gradually changing distribution shifts. We perform experiments by simulating such distribution shifts on the three noise types of CIFAR-10-C. For each noise type, xt is corrupted with standard deviation σt, and σ1,...,σ n interpolate between the standard deviation of level 1 and level 5. So xt is more severely corrupted as we evaluate further into the test set and t grows larger. As shown in Figure 3, TTT-Online still improves upon joint training (and our standard version) with this relaxed assumption, and even upon UDA-SS for the ﬁrst two noise types.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Accuracy (%) Airplane Bird Car Dog Cat Horse Ship Average Object recognition task only 67.9 35.8 42.6 14.7 52.0 42.0 66.7 41.4 Joint training (Hendrycks et al., 2019a) 70.2 36.7 42.6 15.5 52.0 44.0 66.7 42.4 TTT (standard version) 70.2 39.2 42.6 21.6 54.7 46.0 77.8 45.2 TTT-Online 70.2 39.2 42.6 22.4 54.7 46.0 77.8 45.4 Table 2.Class-wise and average classiﬁcation accuracy (%) on CIFAR classes in VID-Robust, adapted from (Shankar et al., 2019). Test-Time Training (TTT) and online Test-Time Training (TTT-Online) improve over the two baselines on average, and by a large margin on “ship” and “dog” classes where the rotation task is more meaningful than in classes like “airplane” (sample images in Figure A7). 3.2. Object Recognition on Video Frames The Robust ImageNet Video Classiﬁcation (VID-Robust) dataset was developed by Shankar et al. (2019) from the Ima- geNet Video detection dataset (Russakovsky et al., 2015), to demonstrate how deep models for object recognition trained on ImageNet (still images) fail to adapt well to video frames. The VID-Robust dataset contains 1109 sets of video frames in 30 classes; each set is a short video clip of frames that are similar to an anchor frame. Our results are reported on the anchor frames. To map the 1000 ImageNet classes to the 30 VID-Robust classes, we use the max-conversion function in Shankar et al. (2019). Without any modiﬁcations for videos, we apply our method to VID-Robust on top of the same ImageNet model as in the previous subsection. Our classiﬁcation accuracy is reported in Table 3. In addition, we take the seven classes in VID-Robust that overlap with CIFAR-10, and re-scale those video frames to the size of CIFAR-10 images, as a new test set for the model trained on CIFAR-10 in the previous subsection. Again, we apply our method to this dataset without any modiﬁcations. Our results are shown in Table 2, with a breakdown for each class. Noticing that Test-Time Training does not improve on the airplane class, we inspect some airplane samples (Figure A7), and observe black margins on two sides of most images, which provide a trivial hint for rotation prediction. In addition, given an image of airplanes in the sky, it is often impossible even for humans to tell if it is rotated. This shows that our method requires the self-supervised task to be both well deﬁned and non-trivial. 3.3. CIFAR-10.1: Unknown Distribution Shifts CIFAR-10.1 (Recht et al., 2018) is a new test set of size 2000 modeled after CIFAR-10, with the exact same classes and image dimensionality, following the dataset creation process documented by the original CIFAR-10 paper as closely as possible. The purpose is to investigate the distribution shifts present between the two test sets, and the effect on object recognition. All models tested by the authors suffer a large performance drop on CIFAR-10.1 comparing to CIFAR-10, even though there is no human noticeable difference, and Method Accuracy (%) Object recognition task only 62.7 Joint training (Hendrycks et al., 2019a) 63.5 TTT (standard version) 63.8 TTT-Online 64.3 Table 3.Test accuracy (%) on VID-Robust dataset (Shankar et al., 2019). TTT and TTT-Online improve over the baselines. Method Error (%) Object recognition task only 17.4 Joint training (Hendrycks et al., 2019a) 16.7 TTT (standard version) 15.9 Table 4.Test error (%) on CIFAR-10.1 (Recht et al., 2018). TTT is the ﬁrst method to improve the performance of an existing model on this new test set. both have the same human accuracy. This demonstrates how insidious and ubiquitous distribution shifts are, even when researchers strive to minimize them. The distribution shifts from CIFAR-10 to CIFAR-10.1 pose an extremely difﬁcult problem, and no prior work has been able to improve the performance of an existing model on this new test set, probably because: 1) researchers cannot even identify the distribution shifts, let alone describe them mathematically; 2) the samples in CIFAR-10.1 are only revealed at test time; and even if they were revealed during training, the distribution shifts are too subtle, and the sample size is too small, for domain adaptation (Recht et al., 2018). On the original CIFAR-10 test set, the baseline with only object recognition has error 8.9%, and with joint training has 8.1%; comparing to the ﬁrst two rows of Table 4, both suffer the typical performance drop (by a factor of two). TTT yields an improvement of 0.8% (relative improvement of 4.8%) over joint training. We recognize that this improve- ment is small relative to the performance drop, but see it as an encouraging ﬁrst step for this very difﬁcult problem.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts 0 10 20 30 40 50 60 Gradient inner product 0 1 2 3 4 5Improvement (%) Level 5 Level 4 Level 3 Level 2 Level 1 0 10 20 30 40 50 60 Gradient inner product 0 5 10 15 20 25 30 35Improvement (%) Level 5 Level 4 Level 3 Level 2 Level 1 Figure 4.Scatter plot of the inner product between the gradients (on the shared feature extractor θe) of the main task lm and the self- supervised task le, and the improvement in test error (%) from Test-Time Training, for the standard (left) and online (right) version. Each point is the average over a test set, and each scatter plot has 75 test sets, from all 15 types of corruptions over ﬁve levels as described in subsection 3.1. The blue lines and bands are the best linear ﬁts and the 99% conﬁdence intervals. The linear correlation coefﬁcients are 0.93 and 0.89 respectively, indicating strong positive correlation between the two quantities, as suggested by Theorem 1. 4. Theoretical Results This section contains our preliminary study of when and why Test-Time Training is expected to work. For convex models, we prove that positive gradient correlation between the loss functions leads to better performance on the main task after Test-Time Training. Equipped with this insight, we then empirically demonstrate that gradient correlation governs the success of Test-Time Training on the deep learning model discussed in Section 3. Before stating our main theoretical result, we ﬁrst illustrate the general intuition with a toy model. Consider a regression problem where x∈Rd denotes the input, y1 ∈R denotes the label, and the objective is the square loss (ˆy−y1)2/2 for a prediction ˆy. Consider a two layer linear network parametrized by A∈Rh×d and v ∈Rh (where hstands for the hidden dimension). The prediction according to this model is ˆy= v⊤Ax, and the main task loss is lm(x,y1; A,v) = 1 2 ( y1 −v⊤Ax )2 . (4) In addition, consider a self-supervised regression task that also uses the square loss and automatically generates a label ys for x. Let the self-supervised head be parametrized by w∈Rh. Then the self-supervised task loss is ls(x,y2; A,w) = 1 2 ( y2 −w⊤Ax )2 . (5) Now we apply Test-Time Training to update the shared feature extractor Aby one step of gradient descent on ls, which we can compute with y2 known. This gives us A′←A−η ( y2 −w⊤Ax )( −wx⊤) , (6) where A′is the updated matrix and ηis the learning rate. If we set η= η∗where η∗= y1 −v⊤Ax (y2 −w⊤Ax) v⊤wx⊤x, (7) then with some simple algebra, it is easy to see that the main task loss lm(x,y1; A′,v) = 0. Concretely, Test-Time Training drives the main task loss down to zero with a single gradient step for a carefully chosen learning rate. In prac- tice, this learning rate is unknown since it depends on the unknown y1. However, since our model is convex, as long as η∗is positive, it sufﬁces to set η to be a small positive constant (see details in the appendix). If x̸= 0, one sufﬁ- cient condition for η∗to be positive (when neither loss is zero) is to have sign ( y1 −v⊤Ax ) = sign ( y2 −w⊤Ax ) (8) and v⊤w>0 . (9) For our toy model, both parts of the condition above have an intuition interpretation. The ﬁrst part says that the mistakes should be correlated, in the sense that predictions from both tasks are mistaken in the same direction. The second part, v⊤w>0, says that the decision boundaries on the feature space should be correlated. In fact, these two parts hold iff. ⟨∇lm(A),∇ls(A)⟩>0 (see a simple proof of this fact in the appendix). To summarize, if the gradients have positive correlation, Test-Time Training is guaranteed to reduce the main task loss. Our main theoretical result extends this to general smooth and convex loss functions.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Theorem 1. Let lm(x,y; θ) denote the main task loss on test instance x,y with parameters θ, and ls(x; θ) the self- supervised task loss that only depends onx. Assume that for all x,y, lm(x,y; θ) is differentiable, convex andβ-smooth in θ, and both ∥∇lm(x,y; θ)∥,∥∇ls(x,θ)∥≤ Gfor all θ. With a ﬁxed learning rate η= ϵ βG2 , for every x,y such that ⟨∇lm(x,y; θ),∇ls(x; θ)⟩>ϵ, (10) we have lm(x,y; θ) >lm(x,y; θ(x)), (11) where θ(x) = θ−η∇ls(x; θ) i.e. Test-Time Training with one step of gradient descent. The proof uses standard techniques in optimization, and is left for the appendix. Theorem 1 reveals gradient correlation as a determining factor of the success of Test-Time Training in the smooth and convex case. In Figure 4, we empirically show that our insight also holds for non-convex loss func- tions, on the deep learning model and across the diverse set of corruptions considered in Section 3; stronger gradient cor- relation clearly indicates more performance improvement over the baseline. 5. Related Work Learning on test instances. Shocher et al. (2018) pro- vide a key inspiration for our work by showing that image super-resolution could be learned at test time simply by try- ing to upsample a downsampled version of the input image. More recently, Bau et al. (2019) improve photo manipula- tion by adapting a pre-trained GAN to the statistics of the input image. One of the earlier examples of this idea comes from Jain & Learned-Miller (2011), who improve Viola- Jones face detection (Viola et al., 2001) by bootstrapping the more difﬁcult faces in an image from the more easily detected faces in that same image. The online version of our algorithm is inspired by the work of Mullapudi et al. (2018), which makes video segmentation more efﬁcient by using a student model that learns online from a teacher model. The idea of online updates has also been used in Kalal et al. (2011) for tracking and detection. A recent work in echocardiography (Zhu et al., 2019) improves the deep learning model that tracks myocardial motion and cardiac blood ﬂow with sequential updates. Lastly, we share the philosophy of transductive learning (Vapnik, 2013; Gam- merman et al., 1998), but have little in common with their classical algorithms; recent work by Tripuraneni & Mackey (2019) theoretically explores this for linear prediction, in the context of debiasing the LASSO estimator. Self-supervised learning studies how to create labels from the data, by designing various pretext tasks that can learn semantic information without human annotations, such as context prediction (Doersch et al., 2015), solving jig- saw puzzles (Noroozi & Favaro, 2016), colorization (Lars- son et al., 2017; Zhang et al., 2016), noise prediction (Bo- janowski & Joulin, 2017), feature clustering (Caron et al., 2018). Our paper uses rotation prediction (Gidaris et al., 2018). Asano et al. (2019) show that self-supervised learn- ing on only a single image, surprisingly, can produce low- level features that generalize well. Closely related to our work, Hendrycks et al. (2019a) propose that jointly training a main task and a self-supervised task (our joint training baseline in Section 3) can improve robustness on the main task. The same idea is used in few-shot learning (Su et al., 2019), domain generalization (Carlucci et al., 2019), and unsupervised domain adaptation (Sun et al., 2019). Adversarial robustness studies the robust risk RP,∆(θ) = Ex,y∼P maxδ∈∆ l(x + δ,y; θ), where l is some loss function, and ∆ is the set of perturbations; ∆ is often chosen as the Lp ball, for p ∈{1,2,∞}. Many popular algorithms formulate and solve this as a robust optimization problem (Goodfellow et al., 2014; Madry et al., 2017; Sinha et al., 2017; Raghunathan et al., 2018; Wong & Kolter, 2017; Croce et al., 2018), and the most well known technique is adversarial training. Another line of work is based on randomized smoothing (Cohen et al., 2019; Salman et al., 2019), while some other approaches, such as input transformations (Guo et al., 2017; Song et al., 2017), are shown to be less effective (Athalye et al., 2018). There are two main problems with the approaches above. First, all of them can be seen as smoothing the decision boundary. This establishes a theoretical tradeoff between accuracy and robustness (Tsipras et al., 2018; Zhang et al., 2019), which we also observe empirically with our adversarial training baseline in Section 3. Intuitively, the more diverse ∆ is, the less effective this one-boundary-ﬁts-all approach can be for a particular element of ∆. Second, adversarial methods rely heavily on the mathematical structure of ∆, which might not accurately model perturbations in the real world. Therefore, generalization remains hard outside of the ∆ we know in advance or can mathematically model, especially for non-adversarial distribution shifts. Empirically, Kang et al. (2019) shows that robustness for one ∆ might not transfer to another, and training on the L∞ball actually hurts robustness on the L1 ball. Non-adversarial robustness studies the effect of corrup- tions, perturbations, out-of-distribution examples, and real- world distribution shifts (Hendrycks et al., 2019b;a; 2018; Hendrycks & Gimpel, 2016). Geirhos et al. (2018) show that training on images corrupted by Gaussian noise makes deep learning models robust to this particular noise type, but does not improve performance on images corrupted by another noise type e.g. salt-and-pepper noise.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Unsupervised domain adaptation (a.k.a. transfer learn- ing) studies the problem of distribution shifts, when an unlabeled dataset from the test distribution (target domain) is available at training time, in addition to a labeled dataset from the training distribution (source domain) (Chen et al., 2011; Gong et al., 2012; Long et al., 2015; Ganin et al., 2016; Long et al., 2016; Tzeng et al., 2017; Hoffman et al., 2017; Csurka, 2017; Chen et al., 2018). The limitation of the problem setting, however, is that generalization might only be improved for this speciﬁc test distribution, which can be difﬁcult to anticipate in advance. Prior work try to anticipate broader distributions by using multiple and evolv- ing domains (Hoffman et al., 2018; 2012; 2014). Test-Time Training does not anticipate any test distribution, by chang- ing the setting of unsupervised domain adaptation, while taking inspiration from its algorithms. Our paper is a follow- up to Sun et al. (2019), which we explain and empirically compare with in Section 3. Our update rule can be viewed as performing one-sample unsupervised domain adaptation on the ﬂy, with the caveat that standard domain adaptation techniques might become ill-deﬁned when there is only one sample from the target domain. Domain generalization studies the setting where a meta distribution generates multiple environment distributions, some of which are available during training (source), while others are used for testing (target) (Li et al., 2018; Shankar et al., 2018; Muandet et al., 2013; Balaji et al., 2018; Ghifary et al., 2015; Motiian et al., 2017; Li et al., 2017a; Gan et al., 2016). With only a few environments, information on the meta distribution is often too scarce to be helpful, and with many environments, we are back to the i.i.d. setting where each environment can be seen as a sample, and a strong baseline is to simply train on all the environments (Li et al., 2019). The setting of domain generalization is limited by the inherent tradeoff between speciﬁcity and generality of a ﬁxed decision boundary, and the fact that generalization is again elusive outside of the meta distribution i.e. the actual P learned by the algorithm. One (few)-shot learning studies how to learn a new task or a new classiﬁcation category using only one (or a few) sample(s), on top of a general representation that has been learned on diverse samples (Snell et al., 2017; Vinyals et al., 2016; Fei-Fei et al., 2006; Ravi & Larochelle, 2016; Li et al., 2017b; Finn et al., 2017; Gidaris & Komodakis, 2018). Our update rule can be viewed as performing one-shot self- supervised learning and can potentially be improved by progress in one-shot learning. Continual learning (a.k.a. learning without forgetting) studies the setting where a model is made to learn a sequence of tasks, and not forget about the earlier ones while training for the later (Li & Hoiem, 2017; Lopez-Paz & Ranzato, 2017; Kirkpatrick et al., 2017; Santoro et al., 2016). In contrast, with Test-Time Training, we are not concerned about forgetting the past test samples since they have already been evaluated on; and if a past sample comes up by any chance, it would go through Test-Time Training again. In addition, the impact of forgetting the training set is minimal, because both tasks have already been jointly trained. Online learning (a.k.a. online optimization) is a well- studied area of learning theory (Shalev-Shwartz et al., 2012; Hazan et al., 2016). The basic setting repeats the following: receive xt, predict ˆyt, receive yt from a worst-case oracle, and learn. Final performance is evaluated using the regret, which colloquially translates to how much worse the online learning algorithm performs in comparison to the best ﬁxed model in hindsight. In contrast, our setting never reveals any yt during testing even for the online version, so we do not need to invoke the concept of the worst-case oracle or the regret. Also, due to the lack of feedback from the envi- ronment after predicting, our algorithm is motivated to learn (with self-supervision) before predicting ˆyt instead of after. Note that some of the previously covered papers (Hoffman et al., 2014; Jain & Learned-Miller, 2011; Mullapudi et al., 2018) use the term “online learning” outside of the learning theory setting, so the term can be overloaded. 6. Discussion The idea of test-time training also makes sense for other tasks, such as segmentation and detection, and in other ﬁelds, such as speech recognition and natural language process- ing. For machine learning practitioners with prior domain knowledge in their respective ﬁelds, their expertise can be leveraged to design better special-purpose self-supervised tasks for test-time training. Researchers for general-purpose self-supervised tasks can also use test-time training as an evaluation benchmark, in addition to the currently prevalent benchmark of pre-training and ﬁne-tuning. More generally, we hope this paper can encourage re- searchers to abandon the self-imposed constraint of a ﬁxed decision boundary for testing, or even the artiﬁcial division between training and testing altogether. Our work is but a small step toward a new paradigm where much of the learning happens after a model is deployed. Acknowledgements. This work is supported by NSF grant 1764033, DARPA and Berkeley DeepDrive. This paper took a long time to develop, and beneﬁted from con- versations with many of our colleagues, including Ben Recht and his students Ludwig Schmidt, Vaishaal Shanker and Becca Roelofs; Ravi Teja Mullapudi, Achal Dave and Deva Ramanan; and Armin Askari, Allan Jabri, Ashish Kumar, Angjoo Kanazawa and Jitendra Malik.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts References Asano, Y . M., Rupprecht, C., and Vedaldi, A. Surprising effectiveness of few-image unsupervised feature learning. arXiv preprint arXiv:1904.13132, 2019. Athalye, A., Carlini, N., and Wagner, D. Obfuscated gradients give a false sense of security: Circumvent- ing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018. Balaji, Y ., Sankaranarayanan, S., and Chellappa, R. Metareg: Towards domain generalization using meta-regularization. In Advances in Neural Information Processing Systems, pp. 998–1008, 2018. Bau, D., Strobelt, H., Peebles, W., Wulff, J., Zhou, B., Zhu, J.-Y ., and Torralba, A. Semantic photo manipulation with a generative image prior. ACM Transactions on Graphics (TOG), 38(4):59, 2019. Bojanowski, P. and Joulin, A. Unsupervised learning by predicting noise. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 517– 526. JMLR. org, 2017. Carlucci, F. M., D’Innocente, A., Bucci, S., Caputo, B., and Tommasi, T. Domain generalization by solving jigsaw puzzles. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition , pp. 2229–2238, 2019. Caron, M., Bojanowski, P., Joulin, A., and Douze, M. Deep clustering for unsupervised learning of visual features. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 132–149, 2018. Caruana, R. Multitask learning. Machine learning, 28(1): 41–75, 1997. Chen, M., Weinberger, K. Q., and Blitzer, J. Co-training for domain adaptation. In Advances in neural information processing systems, pp. 2456–2464, 2011. Chen, X., Sun, Y ., Athiwaratkun, B., Cardie, C., and Wein- berger, K. Adversarial deep averaging networks for cross- lingual sentiment classiﬁcation. Transactions of the Asso- ciation for Computational Linguistics, 6:557–570, 2018. Cohen, J. M., Rosenfeld, E., and Kolter, J. Z. Certiﬁed adversarial robustness via randomized smoothing. arXiv preprint arXiv:1902.02918, 2019. Croce, F., Andriushchenko, M., and Hein, M. Provable robustness of relu networks via maximization of linear regions. arXiv preprint arXiv:1810.07481, 2018. Csurka, G. Domain adaptation for visual applications: A comprehensive survey. arXiv preprint arXiv:1702.05374, 2017. Ding, G. W., Wang, L., and Jin, X. AdverTorch v0.1: An adversarial robustness toolbox based on pytorch. arXiv preprint arXiv:1902.07623, 2019. Doersch, C., Gupta, A., and Efros, A. A. Unsupervised visual representation learning by context prediction. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1422–1430, 2015. Fei-Fei, L., Fergus, R., and Perona, P. One-shot learning of object categories. IEEE transactions on pattern analysis and machine intelligence, 28(4):594–611, 2006. Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta- learning for fast adaptation of deep networks. In Proceed- ings of the 34th International Conference on Machine Learning-Volume 70, pp. 1126–1135. JMLR. org, 2017. Gammerman, A., V ovk, V ., and Vapnik, V . Learning by transduction. In Proceedings of the Fourteenth conference on Uncertainty in artiﬁcial intelligence , pp. 148–155. Morgan Kaufmann Publishers Inc., 1998. Gan, C., Yang, T., and Gong, B. Learning attributes equals multi-source domain generalization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 87–97, 2016. Ganin, Y ., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M., and Lempitsky, V . Domain-adversarial training of neural networks. The Journal of Machine Learning Research, 17(1):2096–2030, 2016. Geirhos, R., Temme, C. R., Rauber, J., Sch¨utt, H. H., Bethge, M., and Wichmann, F. A. Generalisation in humans and deep neural networks. In Advances in Neural Information Processing Systems, pp. 7538–7550, 2018. Ghifary, M., Bastiaan Kleijn, W., Zhang, M., and Balduzzi, D. Domain generalization for object recognition with multi-task autoencoders. In Proceedings of the IEEE international conference on computer vision, pp. 2551– 2559, 2015. Gidaris, S. and Komodakis, N. Dynamic few-shot visual learning without forgetting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4367–4375, 2018. Gidaris, S., Singh, P., and Komodakis, N. Unsupervised rep- resentation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018. Gong, B., Shi, Y ., Sha, F., and Grauman, K. Geodesic ﬂow kernel for unsupervised domain adaptation. In2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 2066–2073. IEEE, 2012.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain- ing and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. Guo, C., Rana, M., Cisse, M., and van der Maaten, L. Coun- tering adversarial images using input transformations. arXiv preprint arXiv:1711.00117, 2017. Hazan, E. et al. Introduction to online convex optimization. Foundations and Trends® in Optimization, 2(3-4):157– 325, 2016. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016a. He, K., Zhang, X., Ren, S., and Sun, J. Identity mappings in deep residual networks. In European conference on computer vision, pp. 630–645. Springer, 2016b. He, K., Girshick, R., and Doll ´ar, P. Rethinking imagenet pre-training. arXiv preprint arXiv:1811.08883, 2018. Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions and perturba- tions. arXiv preprint arXiv:1903.12261, 2019. Hendrycks, D. and Gimpel, K. A baseline for detecting misclassiﬁed and out-of-distribution examples in neural networks. arXiv preprint arXiv:1610.02136, 2016. Hendrycks, D., Mazeika, M., Wilson, D., and Gimpel, K. Using trusted data to train deep networks on labels cor- rupted by severe noise. InAdvances in neural information processing systems, pp. 10456–10465, 2018. Hendrycks, D., Lee, K., and Mazeika, M. Using pre-training can improve model robustness and uncertainty. arXiv preprint arXiv:1901.09960, 2019a. Hendrycks, D., Mazeika, M., Kadavath, S., and Song, D. Improving model robustness and uncertainty estimates with self-supervised learning. arXiv preprint, 2019b. Hoffman, J., Kulis, B., Darrell, T., and Saenko, K. Discover- ing latent domains for multisource domain adaptation. In European Conference on Computer Vision, pp. 702–715. Springer, 2012. Hoffman, J., Darrell, T., and Saenko, K. Continuous man- ifold based adaptation for evolving visual domains. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 867–874, 2014. Hoffman, J., Tzeng, E., Park, T., Zhu, J.-Y ., Isola, P., Saenko, K., Efros, A. A., and Darrell, T. Cycada: Cycle- consistent adversarial domain adaptation. arXiv preprint arXiv:1711.03213, 2017. Hoffman, J., Mohri, M., and Zhang, N. Algorithms and theory for multiple-source adaptation. In Advances in Neural Information Processing Systems, pp. 8246–8256, 2018. Huang, G., Sun, Y ., Liu, Z., Sedra, D., and Weinberger, K. Q. Deep networks with stochastic depth. In European conference on computer vision, pp. 646–661. Springer, 2016. Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. Jain, V . and Learned-Miller, E. Online domain adaptation of a pre-trained cascade of classiﬁers. In CVPR 2011, pp. 577–584. IEEE, 2011. Kalal, Z., Mikolajczyk, K., and Matas, J. Tracking-learning- detection. IEEE transactions on pattern analysis and machine intelligence, 34(7):1409–1422, 2011. Kang, D., Sun, Y ., Brown, T., Hendrycks, D., and Steinhardt, J. Transfer of adversarial robustness between perturbation types. arXiv preprint arXiv:1905.01034, 2019. Kannan, H., Kurakin, A., and Goodfellow, I. Adversarial logit pairing. arXiv preprint arXiv:1803.06373, 2018. Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Des- jardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017. Krizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009. Larsson, G., Maire, M., and Shakhnarovich, G. Colorization as a proxy task for visual understanding. In CVPR, 2017. Li, D., Yang, Y ., Song, Y .-Z., and Hospedales, T. M. Deeper, broader and artier domain generalization. In Proceed- ings of the IEEE International Conference on Computer Vision, pp. 5542–5550, 2017a. Li, D., Zhang, J., Yang, Y ., Liu, C., Song, Y .-Z., and Hospedales, T. M. Episodic training for domain gen- eralization. arXiv preprint arXiv:1902.00113, 2019. Li, Y ., Tian, X., Gong, M., Liu, Y ., Liu, T., Zhang, K., and Tao, D. Deep domain generalization via conditional invariant adversarial networks. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 624–639, 2018.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Li, Z. and Hoiem, D. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935–2947, 2017. Li, Z., Zhou, F., Chen, F., and Li, H. Meta-sgd: Learning to learn quickly for few-shot learning. arXiv preprint arXiv:1707.09835, 2017b. Liu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T. Re- thinking the value of network pruning. arXiv preprint arXiv:1810.05270, 2018. Long, M., Cao, Y ., Wang, J., and Jordan, M. I. Learn- ing transferable features with deep adaptation networks. arXiv preprint arXiv:1502.02791, 2015. Long, M., Zhu, H., Wang, J., and Jordan, M. I. Unsupervised domain adaptation with residual transfer networks. In Advances in Neural Information Processing Systems, pp. 136–144, 2016. Lopez-Paz, D. and Ranzato, M. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems, pp. 6467–6476, 2017. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 , 2017. Motiian, S., Piccirilli, M., Adjeroh, D. A., and Doretto, G. Uniﬁed deep supervised domain adaptation and gen- eralization. In Proceedings of the IEEE International Conference on Computer Vision, pp. 5715–5725, 2017. Muandet, K., Balduzzi, D., and Sch ¨olkopf, B. Domain generalization via invariant feature representation. In International Conference on Machine Learning, pp. 10– 18, 2013. Mullapudi, R. T., Chen, S., Zhang, K., Ramanan, D., and Fatahalian, K. Online model distillation for efﬁcient video inference. arXiv preprint arXiv:1812.02699, 2018. Noroozi, M. and Favaro, P. Unsupervised learning of visual representations by solving jigsaw puzzles. In European Conference on Computer Vision , pp. 69–84. Springer, 2016. Raghunathan, A., Steinhardt, J., and Liang, P. Certiﬁed defenses against adversarial examples. arXiv preprint arXiv:1801.09344, 2018. Ravi, S. and Larochelle, H. Optimization as a model for few-shot learning. IEEE transactions on pattern analysis and machine intelligence, 2016. Recht, B., Roelofs, R., Schmidt, L., and Shankar, V . Do cifar-10 classiﬁers generalize to cifar-10? arXiv preprint arXiv:1806.00451, 2018. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV) , 115(3):211–252, 2015. doi: 10.1007/s11263-015-0816-y. Salman, H., Yang, G., Li, J., Zhang, P., Zhang, H., Razen- shteyn, I., and Bubeck, S. Provably robust deep learn- ing via adversarially trained smoothed classiﬁers. arXiv preprint arXiv:1906.04584, 2019. Santoro, A., Bartunov, S., Botvinick, M., Wierstra, D., and Lillicrap, T. Meta-learning with memory-augmented neu- ral networks. In International conference on machine learning, pp. 1842–1850, 2016. Shalev-Shwartz, S. et al. Online learning and online con- vex optimization. Foundations and Trends® in Machine Learning, 4(2):107–194, 2012. Shankar, S., Piratla, V ., Chakrabarti, S., Chaudhuri, S., Jyothi, P., and Sarawagi, S. Generalizing across domains via cross-gradient training. arXiv preprint arXiv:1804.10745, 2018. Shankar, V ., Dave, A., Roelofs, R., Ramanan, D., Recht, B., and Schmidt, L. Do image classiﬁers generalize across time? arXiv, 2019. Shocher, A., Cohen, N., and Irani, M. zero-shot super- resolution using deep internal learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3118–3126, 2018. Sinha, A., Namkoong, H., and Duchi, J. Certifying some dis- tributional robustness with principled adversarial training. arXiv preprint arXiv:1710.10571, 2017. Snell, J., Swersky, K., and Zemel, R. Prototypical networks for few-shot learning. In Advances in Neural Information Processing Systems, pp. 4077–4087, 2017. Song, Y ., Kim, T., Nowozin, S., Ermon, S., and Kushman, N. Pixeldefend: Leveraging generative models to understand and defend against adversarial examples. arXiv preprint arXiv:1710.10766, 2017. Su, J.-C., Maji, S., and Hariharan, B. Boosting supervi- sion with self-supervision for few-shot learning. arXiv preprint arXiv:1906.07079, 2019. Sun, Y ., Tzeng, E., Darrell, T., and Efros, A. A. Unsuper- vised domain adaptation through self-supervision. arXiv preprint, 2019.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Tripuraneni, N. and Mackey, L. Debiasing linear prediction. arXiv preprint arXiv:1908.02341, 2019. Tsipras, D., Santurkar, S., Engstrom, L., Turner, A., and Madry, A. Robustness may be at odds with accuracy. arXiv preprint arXiv:1805.12152, 2018. Tzeng, E., Hoffman, J., Saenko, K., and Darrell, T. Adver- sarial discriminative domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7167–7176, 2017. Vapnik, V .The nature of statistical learning theory. Springer science & business media, 2013. Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al. Matching networks for one shot learning. In Advances in neural information processing systems, pp. 3630–3638, 2016. Viola, P., Jones, M., et al. Rapid object detection using a boosted cascade of simple features. CVPR (1), 1(511- 518):3, 2001. Wong, E. and Kolter, J. Z. Provable defenses against adver- sarial examples via the convex outer adversarial polytope. arXiv preprint arXiv:1711.00851, 2017. Zhang, H., Yu, Y ., Jiao, J., Xing, E. P., Ghaoui, L. E., and Jor- dan, M. I. Theoretically principled trade-off between ro- bustness and accuracy. arXiv preprint arXiv:1901.08573, 2019. Zhang, R., Isola, P., and Efros, A. A. Colorful image col- orization. In European conference on computer vision, pp. 649–666. Springer, 2016. Zhu, W., Huang, Y ., Vannan, M. A., Liu, S., Xu, D., Fan, W., Qian, Z., and Xie, X. Neural multi-scale self-supervised registration for echocardiogram dense tracking. arXiv preprint arXiv:1906.07357, 2019.Appendix: Test-Time Training with Self-Supervision for Generalization under Distribution Shifts A1. Informal Discussion on Our Variable Decision Boundary In the introduction, we claim that in traditional supervised learning θgives a ﬁxed decision boundary, while ourθgives a variable decision boundary. Here we informally discuss this claim. Denote the input space Xand output space Y. A decision boundary is simply a mapping f : X →Y. Let Θ be a model class e.g Rd. Now consider a family of parametrized functions gθ : X→Y , where θ∈Θ. In the context of deep learning, gis the neural network architecture and θcontains the parameters. We say that f is a ﬁxed decision boundary w.r.t. g and Θ if there exists θ ∈Θ s.t. f(x) = gθ(x) for every x ∈X , and a variable decision boundary if for every x∈X, there exists θ∈Θ s.t. f(x) = gθ(x). Note how selection of θcan depend on xfor a variable decision boundary, and cannot for a ﬁxed one. It is then trivial to verify that our claim is true under those deﬁnitions. A critical reader might say that with an arbitrarily large model class, can’t every decision boundary be ﬁxed? Yes, but this is not the end of the story. Let d = dim( X) × dim(Y), and consider the enormous model class Θ′= Rd which is capable of representing all possible mappings be- tween Xand Y. Let g′ θ′ simply be the mapping represented by θ′ ∈Θ′. A variable decision boundary w.r.t. g and Θ then indeed must be a ﬁxed decision boundary w.r.t. g′and Θ′, but we would like to note two things. First, without any prior knowledge, generalization in Θ′is impossible with any ﬁnite amount of training data; reasoning about g′and Θ′is most likely not productive from an algorithmic point of view, and the concept of a variable decision boundary is to avoid such reasoning. Second, selecting θbased on xfor a variable decision boundary can be thought of as “training” on all points x ∈Rd; however, “training” only happens when necessary, for the xthat it actually encounters. Altogether, the concept of a variable decision boundary is different from what can be described by traditional learning theory. A formal discussion is beyond the scope of this paper and might be of interest to future work. A2. Computational Aspects of Our Method At test time, our method is 2 × batch size × number of iterations times slower than regular test- ing, which only performs a single forward pass for each sample. As the ﬁrst work on Test-Time Training, this paper is not as concerned about computational efﬁciency as improving robustness, but here we provide two poten- tial solutions that might be useful, but have not been thor- oughly veriﬁed. The ﬁrst is to use the thresholding trick on ls, introduced as a solution for the small batches prob- lem in the method section. For the models considered in our experiments, roughly 80% of the test instances fall below the threshold, so Test-Time Training can only be performed on the other 20% without much effect on per- formance, because those 20% contain most of the sam- ples with wrong predictions. The second is to reduce the number of iterations of test-time updates. For the online version, the number of iterations is al- ready 1, so there is nothing to do. For the standard ver- sion, we have done some preliminary experiments setting number of iterations to 1 (instead of 10) and learn- ing rate to 0.01 (instead of 0.001), and observing results almost as good as the standard hyper-parameter setting. A more in depth discussion on efﬁciency is left for future works, which might, during training, explicitly make the model amenable to fast updates. A3. Proofs Here we prove the theoretical results in the main paper. A3.1. The Toy Problem The following setting applies to the two lemmas; this is simply the setting of our toy problem, reproduced here for ease of reference.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Consider a two layer linear network parametrized by A∈ Rh×d (shared) and v,w ∈Rh (ﬁxed) for the two heads, respectively. Denote x∈Rd the input and y1,y2 ∈R the labels for the two tasks, respectively. For the main task loss lm(A; v) = 1 2 ( y1 −v⊤Ax )2 , (12) and the self-supervised task loss ls(A; w) = 1 2 ( y2 −w⊤Ax )2 , (13) Test-Time Training yields an updated matrix A′←A−η ( y2 −w⊤Ax )( −wx⊤) , (14) where ηis the learning rate. Lemma 1. Following the exposition of the main paper, let η∗= (y1 −v⊤Ax) (y2 −w⊤Ax)v⊤wx⊤x. (15) Assume η∗∈[ϵ,∞) for some ϵ> 0. Then for any η∈(0,ϵ], we are guaranteed an improvement on the main loss i.e. lm(A′) <lm(A). Proof. From the exposition of the main paper, we know that lm(A−η∗∇lsA)) = 0, which can also be derived from simple algebra. Then by convexity, we have lm(A−η∇ls(A)) (16) = lm (( 1 − η η∗ ) A+ η η∗(A−η∗∇ls(A)) ) (17) ≤ ( 1 − η η∗ ) lm(A) + 0 (18) ≤ ( 1 −η ϵ ) lm(A) (19) <lm(A), (20) where the last inequality uses the assumption that lm(A) > 0, which holds because η∗>0. Lemma 2. Deﬁne ⟨U,V⟩= vec (U)⊤vec (V) i.e. the Frobenious inner product, then sign (η∗) = sign (⟨∇lm(A),∇ls(A)⟩) . (21) Proof. By simple algebra, ⟨∇lm(A),∇ls(A)⟩ = ⟨ ( y1 −v⊤Ax )( −vx⊤) , ( y2 −w⊤Ax )( −wx⊤) ⟩ = ( y1 −v⊤Ax )( y2 −w⊤Ax ) Tr ( xv⊤wx⊤) = ( y1 −v⊤Ax )( y2 −w⊤Ax ) v⊤wx⊤x, which has the same sign as η∗. A3.2. Proof of Theorem 1 For any η, by smoothness and convexity, lm(x,y; θ(x)) = lm(x,y; θ−η∇ls(x; θ)) ≤lm(x,y; θ) + η⟨∇lm(x,y; θ),∇ls(x,θ)⟩ + η2β 2 ∥∇ls(x; θ)∥2 . Denote η∗= ⟨∇lm(x,y; θ),∇ls(x,θ)⟩ β∥∇ls(x; θ)∥2 . Then Equation 22 becomes lm(x,y; θ−η∗∇ls(x; θ)) (22) ≤lm(x,y; θ) −⟨∇lm(x,y; θ),∇ls(x,θ)⟩2 2β∥∇ls(x; θ)∥2 . (23) And by our assumptions on the gradient norm and gradient inner product, lm(x,y; θ) −lm(x,y; θ−η∗∇ls(x; θ)) ≥ ϵ2 2βG2 . (24) Because we cannot observe η∗in practice, we instead use a ﬁxed learning rate η = ϵ βG2 , as stated in Theorem 1. Now we argue that this ﬁxed learning rate still improves performance on the main task. By our assumptions, η∗ ≥ ϵ βG2 , so η ∈(0,η∗]. Denote g= ∇ls(x; θ), then by convexity of lm, lm(x,y; θ(x)) = lm(x,y; θ−ηg) (25) = lm ( x,y; ( 1 − η η∗ ) θ+ η η∗(θ−η∗g) ) (26) ≤ ( 1 − η η∗ ) lm(x,y; θ) + η η∗lm(x,y; θ−η∗g) (27) Combining with Equation 24, we have lm(x,y; θ(x)) ≤ ( 1 − η η∗ ) lm(x,y; θ) + η η∗ ( lm(x,y; θ) − ϵ2 2βG2 ) = lm(x,y; θ) − η η∗ ϵ2 2βG2 Since η/η∗>0, we have shown that lm(x,y; θ) −lm(x,y; θ(x)) >0. (28)Test-Time Training with Self-Supervision for Generalization under Distribution Shifts A4. Additional Results on the Common Corruptions Dataset For table aethetics, we use the following abbreviations: B for baseline, JT for joint training, TTT for Test-Time Train- ing standard version, and TTT-Online for online Test-Time Training i.e. the online version. We have abbreviated the names of the corruptions, in order: original test set, Gaussian noise, shot noise, impulse noise, defocus blur, glass blue, motion blur, zoom blur, snow, frost, fog, brightness, contrast, elastic transformation, pixelation, and JPEG compression. A4.1. Results Using Batch Normalization As discussed in the results section, Batch Normalization (BN) is ineffective for small batches, which are the inputs for Test-Time Training (both standard and online version) since there is only one sample available when forming each batch; therefore, our main results are based on a ResNet using Group Normalization (GN). Figure A2 and Table A1 show results of our method on CIFAR-10-C level 5, with a ResNet using Batch Normalization (BN). These results are only meant to be a point of reference for the curious readers. In the early stage of this project, we have experimented with two potential solutions to the small batches problem with BN. The naive solution is to ﬁx the BN layers during Test-Time Training. but this diminishes the performance gains since there are fewer shared parameters. The better solution, adopted for the results below, is hard example mining: instead of updating on all inputs, we only update on inputs that incur large self-supervised task loss ls, where the large improvements might counter the negative effects of inaccurate statistics. Test-Time Training (standard version) is still very effective with BN. In fact, some of the improvements are quite dra- matic, such as on contrast (34%), defocus blue (18%) and Gaussian noise (22% comparing to joint-training, and 16% comparing to the baseline). Performance on the original distribution is still almost the same, and the original error with BN is in fact slightly lower than with GN, and takes half as many epochs to converge. We did not further experiment with BN because of two rea- sons: 1) The online version does not work with BN, because the problem with inaccurate batch statistics is exacerbated when training online for many (e.g. 10000) steps. 2) The baseline error for almost every corruption type is signiﬁ- cantly higher with BN than with GN. Although unrelated to the main idea of our paper, we make the interesting note that GN signiﬁcantly improves model robustness. A4.2. Additional Baseline: Adversarial Logit Pairing As discussed in the results section, Hendrycks & Dietterich (2019) point to Adversarial Logit Pairing (ALP) (Kannan et al., 2018) as an effective method for improving model robustness to corruptions and perturbations, even though it was designed to defend against adversarial attacks. We take ALP as an additional baseline on all benchmarks based on CIFAR-10 (using GN), following the training proce- dure in Kannan et al. (2018) and their recommended hyper- parameters. The implementation of the adversarial attack comes from the codebase of Ding et al. (2019). We did not run ALP on ImageNet because the two papers we reference for this method, Kannan et al. (2018) and Hendrycks & Di- etterich (2019), did not run on ImageNet or make any claim or recommendation. A4.3. Results on CIFAR-10-C and ImageNet-C, Level 5 Table A2 and Table A3 correspond to the bar plots in the results section. Two rows of Table A2 have been presented as Table 1 in the main text. A4.4. Results on CIFAR-10-C, Levels 1-4 The following bar plots and tables are on levels 1-4 of CIFAR-10-C. The original distribution is the same for all levels, so are our results on the original distribution. A4.5. Direct Comparison with Hendrycks et al. (2019a) The following comparison has been requested by an anony- mous reviewer for our ﬁnal version. Our joint training baseline is based on Hendrycks et al. (2019a), but also incor- porates some architectural changes (see below). We found these changes improved the robustness of our method, and felt that it was important to give the baseline the same ben- eﬁt. Note that our joint training baseline overall performs better than Hendrycks: Compare Table S2 to Figure 3 of Hendrycks et al. (2019a) (provided by the authors), our baseline has average error of 22.8% across all corruptions and levels, while their average error is 28.6%. Summary of architectural changes: 1) Group Normalization (GN) instead of Batch Normalization (BN). For complete- ness, the results with BN are provided in Table S1; c.f. GN results in Table S2 which signiﬁcantly improves robustness, with or without self-supervision. 2) We split after the sec- ond residual group, while they split after the third residual group right before the linear layer. This consistently gives about 0.5% - 1% improvement. 3) We use a ResNet-26, while they use a 40-2 Wide ResNet. But our baseline still performs better than their method even though our network is 4x smaller, due to the two tricks above.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Gaussian Noise  Shot Noise  Impulse Noise  Defocus Blur  Frosted Glass Blur Motion Blur  Zoom Blur  Snow  Frost  Fog Brightness  Contrast  Elastic  Pixelate  JPEG Figure A1.Sample images from the Common Corruptions Benchmark, taken from the original paper by Hendrycks & Dietterich (2019). originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 20 40 60Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT Figure A2.Test error (%) on CIFAR-10-C, level 5, ResNet-26 with Batch Normalization. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 7.9 63.9 58.8 64.3 46.3 54.6 41.6 45.9 31.9 44.0 37.5 13.0 69.2 33.8 61.4 31.7 JT 7.5 70.7 65.6 67.2 43.1 55.4 40.9 42.7 30.3 44.5 42.5 12.7 58.6 30.7 62.6 31.9 TTT 7.9 47.9 45.2 54.8 27.6 50.4 31.5 30.9 28.7 34.3 26.9 12.6 35.2 30.6 51.2 31.3 Table A1.Test error (%) on CIFAR-10-C, level 5, ResNet-26 with Batch Normalization. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 50.5 47.2 56.1 23.7 51.7 24.3 26.3 25.6 34.4 28.1 13.5 25.0 27.4 55.8 29.8 JT 8.1 49.4 45.3 53.4 24.2 48.5 24.8 26.4 25.0 32.5 27.5 12.6 25.3 24.0 51.6 28.7 TTT 7.9 45.6 41.8 50.0 21.8 46.1 23.0 23.9 23.9 30.0 25.1 12.2 23.9 22.6 47.2 27.2 TTT-Online 8.2 25.8 22.6 30.6 14.6 34.4 18.3 17.1 20.0 18.0 16.9 11.2 15.6 21.6 18.1 21.2 UDA-SS 9.0 28.2 26.5 20.8 15.6 43.7 24.5 23.8 25.0 24.9 17.2 12.7 11.6 22.1 20.3 22.6 ALP 16.5 22.7 22.9 28.3 25.0 25.6 27.4 23.1 25.2 27.2 64.8 21.7 73.6 23.0 20.2 18.9 Table A2.Test error (%) on CIFAR-10-C, level 5, ResNet-26. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 68.9 1.3 2.0 1.3 7.5 6.6 11.8 16.2 15.7 14.9 15.3 43.9 9.7 16.5 15.3 23.4 JT 69.1 2.1 3.1 2.1 8.7 6.7 12.3 16.0 15.3 15.8 17.0 45.3 11.0 18.4 19.7 22.9 TTT 69.0 3.1 4.5 3.5 10.1 6.8 13.5 18.5 17.1 17.9 20.0 47.0 14.4 20.9 22.8 25.3 TTT-Online 68.8 26.3 28.6 26.9 23.7 6.6 28.7 33.4 35.6 18.7 47.6 58.3 35.3 44.3 47.8 44.3 Table A3.Test accuracy (%) on ImageNet-C, level 5, ResNet-18.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40 50Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A3.Test error (%) on CIFAR-10-C, level 4. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 46.4 39.2 44.8 15.3 52.5 19.1 20.5 21.3 26.9 13.3 10.5 13.7 20.8 35.3 26.9 JT 8.1 45.0 38.3 42.2 16.4 50.2 20.7 20.5 21.1 25.4 14.1 10.0 14.7 19.0 33.2 25.1 TTT 7.9 41.5 35.4 39.8 15.0 47.8 19.1 18.4 20.1 24.0 13.5 10.0 14.1 17.7 29.4 24.5 TTT-Online 8.2 22.9 20.0 23.9 11.2 35.1 15.6 13.8 18.6 15.9 12.3 9.7 11.9 16.7 13.6 19.8 ALP 16.5 21.3 20.5 24.5 20.7 25.9 23.7 21.4 24.2 23.9 42.2 17.5 53.7 22.1 19.1 18.5 Table A4.Test error (%) on CIFAR-10-C, level 4, ResNet-26. originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A4.Test error (%) on CIFAR-10-C, level 3. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 42.2 35.1 30.7 12.2 41.7 18.6 17.5 19.0 25.3 10.8 9.7 11.6 15.3 21.7 24.6 JT 8.1 40.2 34.4 29.9 12.2 37.9 20.8 17.3 18.4 25.0 11.4 9.2 12.0 15.2 20.8 22.8 TTT 7.9 37.2 31.6 28.6 11.5 35.8 19.1 15.8 17.8 23.3 11.0 9.1 11.6 14.3 18.9 22.3 TTT-Online 8.2 21.3 17.7 17.9 9.0 23.4 15.3 12.5 16.4 15.8 10.9 9.0 10.7 12.8 12.2 18.7 ALP 16.5 20.0 19.3 20.5 19.2 21.2 24.0 20.5 20.9 24.2 30.1 16.6 39.6 20.9 17.8 18.0 Table A5.Test error (%) on CIFAR-10-C, level 3, ResNet-26.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A5.Test error (%) on CIFAR-10-C, level 2. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 31.7 22.6 24.3 9.9 42.6 14.9 14.7 21.7 18.4 9.8 9.1 10.0 13.1 17.1 22.4 JT 8.1 31.0 22.6 23.4 9.1 39.2 16.4 14.2 21.2 17.5 9.4 8.3 10.6 12.8 15.9 20.5 TTT 7.9 28.8 20.7 23.0 9.0 36.6 15.4 13.1 20.2 16.9 9.2 8.3 10.2 12.5 14.8 19.7 TTT-Online 8.2 16.8 13.8 15.5 8.5 23.4 13.3 11.5 16.8 12.7 9.4 8.4 9.7 12.4 11.5 17.0 ALP 16.5 18.0 17.2 19.0 17.8 20.7 21.2 19.3 19.0 20.1 22.4 16.3 29.2 20.3 17.4 17.8 Table A6.Test error (%) on CIFAR-10-C, level 2, ResNet-26. originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A6.Test error (%) on CIFAR-10-C, level 1. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 21.7 17.1 17.0 9.0 44.0 12.1 13.9 14.3 13.4 9.2 8.9 9.0 13.2 12.0 17.3 JT 8.1 20.4 16.6 16.9 8.2 40.5 12.2 13.0 13.1 12.3 8.4 8.1 8.5 12.9 11.3 15.9 TTT 7.9 19.1 15.8 16.5 8.0 37.9 11.7 12.2 12.8 11.9 8.2 8.0 8.3 12.6 11.1 15.5 TTT-Online 8.2 13.8 11.9 12.2 8.5 24.4 10.5 11.5 12.4 10.7 8.5 8.3 8.6 12.4 10.7 14.4 ALP 17.0 16.8 17.6 16.8 20.9 18.7 19.0 17.3 17.5 17.4 16.1 18.4 20.4 17.0 17.2 17.5 Table A7.Test error (%) on CIFAR-10-C, level 1, ResNet-26.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Figure A7.Sample Images from the VID-Robust dataset (Shankar et al., 2019) in the results section adapted to CIFAR-10. Each row shows eight sample images from one class. The seven classes shown are, in order: airplane, bird, car, dog, cat, horse, ship.",
      "meta_data": {
        "arxiv_id": "1909.13231v3",
        "authors": [
          "Yu Sun",
          "Xiaolong Wang",
          "Zhuang Liu",
          "John Miller",
          "Alexei A. Efros",
          "Moritz Hardt"
        ],
        "published_date": "2019-09-29T08:09:15Z",
        "pdf_url": "https://arxiv.org/pdf/1909.13231v3.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper proposes Test-Time Training (TTT), a novel approach to improve the generalization of predictive models under distribution shifts. TTT converts a single unlabeled test sample into a self-supervised learning problem, on which model parameters are updated before prediction. This method extends to online data streams (TTT-Online) and is shown to substantially improve performance on diverse image classification benchmarks evaluating robustness to distribution shifts, often while maintaining or improving performance on the original distribution. The work also provides theoretical insights, demonstrating that positive gradient correlation between the main and self-supervised task losses is a sufficient condition for TTT to reduce the main task loss in convex models, a finding empirically extended to deep learning models.",
        "methodology": "The core methodology involves a neural network with a Y-structure, comprising a shared feature extractor (e.g., lower layers of a ResNet) and two task-specific branches: one for the main classification task and another for a self-supervised auxiliary task. The auxiliary task chosen is rotation prediction, where the model predicts the 0, 90, 180, or 270-degree rotation of an input image. Initially, the model undergoes joint training on both tasks using data from the source distribution. At test time, for each unlabeled test sample, the shared feature extractor's parameters are fine-tuned by minimizing the self-supervised auxiliary task loss on that single sample (or an augmented batch derived from it). For the standard TTT, ten gradient steps are taken, while for the online version (TTT-Online), only one gradient step is taken per incoming sample, with parameters initialized from the previous sample's update. Stochastic gradient descent is used for optimization, with specific learning rates and zero weight decay/momentum during TTT. Group Normalization (GN) replaces Batch Normalization (BN) to handle the small batch sizes inherent to test-time updates.",
        "experimental_setup": "The method was evaluated on various image classification benchmarks designed for distribution shifts. Datasets included CIFAR-10 and ImageNet, tested against their corrupted versions: CIFAR-10-C and ImageNet-C, which feature 15 types of corruptions across five severity levels. Further evaluation was conducted on VID-Robust (a video frame dataset adapted from ImageNet Video) and CIFAR-10.1, a new test set designed to expose subtle, unknown distribution shifts. The architecture used was ResNets (26-layer for CIFAR-10, 18-layer for ImageNet). Baselines included models trained solely on the object recognition task, models trained with joint supervised and self-supervised learning (fixed at test time), Unsupervised Domain Adaptation by Self-Supervision (UDA-SS), and Adversarial Logit Pairing (ALP). Performance was measured using test error and accuracy, comparing TTT and TTT-Online against baselines, and observing performance on original distributions, corrupted images, gradually changing shifts, and video frames.",
        "limitations": "The primary limitation is computational efficiency; Test-Time Training is significantly slower (2 × batch size × number of iterations times) than standard inference, which only requires a single forward pass. The effectiveness of the method relies on the self-supervised task being 'well defined and non-trivial'; for instance, it showed no improvement on classes where rotation prediction provided trivial hints (e.g., images with black margins) or was ambiguous even for humans (e.g., airplanes in the sky). The theoretical results proving the benefits of TTT are derived for convex models, with empirical evidence suggesting similar trends for non-convex deep learning, but a formal theoretical extension is not provided. Additionally, Batch Normalization (BN) was found to be ineffective with the small batch sizes used in TTT, necessitating the use of Group Normalization (GN).",
        "future_research_directions": "Future work could focus on improving the computational efficiency of Test-Time Training, possibly by exploring techniques like thresholding the self-supervised loss or reducing the number of test-time update iterations. The approach could be extended to other tasks like segmentation, detection, speech recognition, and natural language processing, leveraging domain expertise to design specialized self-supervised tasks. The paper also suggests using Test-Time Training as a new evaluation benchmark for general-purpose self-supervised tasks. Furthermore, a formal theoretical discussion on the concept of a variable decision boundary in deep learning, which TTT enables, is highlighted as an area of interest. Integration with advancements in one-shot learning could also enhance the self-supervised update rule. More broadly, the authors advocate for a paradigm shift, encouraging researchers to move beyond the traditional fixed decision boundary and the strict separation of training and testing phases, with learning happening post-deployment."
      }
    },
    {
      "title": "Parameter-free, Dynamic, and Strongly-Adaptive Online Learning"
    },
    {
      "title": "Gradient-Variation Online Learning under Generalized Smoothness",
      "abstract": "Gradient-variation online learning aims to achieve regret guarantees that\nscale with variations in the gradients of online functions, which has been\nshown to be crucial for attaining fast convergence in games and robustness in\nstochastic optimization, hence receiving increased attention. Existing results\noften require the smoothness condition by imposing a fixed bound on gradient\nLipschitzness, which may be unrealistic in practice. Recent efforts in neural\nnetwork optimization suggest a generalized smoothness condition, allowing\nsmoothness to correlate with gradient norms. In this paper, we systematically\nstudy gradient-variation online learning under generalized smoothness. We\nextend the classic optimistic mirror descent algorithm to derive\ngradient-variation regret by analyzing stability over the optimization\ntrajectory and exploiting smoothness locally. Then, we explore universal online\nlearning, designing a single algorithm with the optimal gradient-variation\nregrets for convex and strongly convex functions simultaneously, without\nrequiring prior knowledge of curvature. This algorithm adopts a two-layer\nstructure with a meta-algorithm running over a group of base-learners. To\nensure favorable guarantees, we design a new Lipschitz-adaptive meta-algorithm,\ncapable of handling potentially unbounded gradients while ensuring a\nsecond-order bound to effectively ensemble the base-learners. Finally, we\nprovide the applications for fast-rate convergence in games and stochastic\nextended adversarial optimization.",
      "full_text": "arXiv:2408.09074v2  [cs.LG]  3 Nov 2024 Gradient-V ariation Online Learning under Generalized Smoothness Y an-Feng Xie, Peng Zhao, Zhi-Hua Zhou National Key Laboratory for Novel Software T echnology, Nan jing University, China School of Artiﬁcial Intelligence, Nanjing University, Chi na {xieyf, zhaop, zhouzh}@lamda.nju.edu.cn Abstract Gradient-variation online learning aims to achieve regret guarantees that scale with variations in the gradients of online functions, which is crucial for attain- ing fast convergence in games and robustness in stochastic o ptimization, hence receiving increased attention. Existing results often req uire the smoothness con- dition by imposing a ﬁxed bound on gradient Lipschitzness, w hich may be unre- alistic in practice. Recent efforts in neural network optim ization suggest a gen- eralized smoothness condition, allowing smoothness to correlate with gradient norms. In this paper, we systematically study gradient-var iation online learning under generalized smoothness. W e extend the classic optimi stic mirror descent algorithm to derive gradient-variation regret by analyzin g stability over the opti- mization trajectory and exploiting smoothness locally. Th en, we explore universal online learning , designing a single algorithm with the optimal gradient-va riation regrets for convex and strongly convex functions simultane ously, without requir- ing prior knowledge of curvature. This algorithm adopts a tw o-layer structure with a meta-algorithm running over a group of base-learners . T o ensure favor- able guarantees, we design a new Lipschitz-adaptive meta-a lgorithm, capable of handling potentially unbounded gradients while ensuring a second-order bound to effectively ensemble the base-learners. Finally, we provi de the applications for fast-rate convergence in games and stochastic extended adv ersarial optimization. 1 Introduction W e consider online convex optimization (OCO) [ Hazan, 2016; Orabona, 2019], a ﬂexible framework that models the decision-making problem in an online fashio n. At each round t ∈ [T], an online learner is required to submit a decision xt from a convex compact set X ⊆ Rd and the environments reveal a convex function ft : X ↦→R. Then the learner suffers a loss ft(xt) and updates her decision. The standard performance measure is the regret [Zinkevich, 2003] that benchmarks the cumulative loss of the learner against the best decision in hindsight, f ormally deﬁned as RE GT = T∑ t=1 ft(xt) − min x∈X T∑ t=1 ft(x). (1) Regret bounds of O( √ T) and O( 1 λ log T) are established for convex and λ-strongly convex func- tions respectively [ Zinkevich, 2003; Hazan et al. , 2007]. While these results are known to be min- imax optimal [ Abernethy et al. , 2008], in this paper we are more interested in obtaining gradient- variation regret guarantees, which replace the dependence of T in the regret bounds by variations in Correspondence: Peng Zhao <zhaop@lamda.nju.edu.cn> 38th Conference on Neural Information Processing Systems ( NeurIPS 2024).the gradients of online functions [ Chiang et al. , 2012] deﬁned as VT = T∑ t=2 sup x∈X ∥∇ft(x) − ∇ft−1(x)∥2 2. (2) This quantity can be as small as a constant in stable environm ents where online functions remain ﬁxed, and is at most O(T) in the worst case under standard OCO assumptions, safeguard ing min- imax results. Besides this favorable adaptivity, recent st udies have shown close relationships of gradient-variation online learning to various ﬁelds, incl uding fast convergence in games [ Rakhlin and Sridharan , 2013b; Syrgkanis et al. , 2015; Zhang et al. , 2022b] and robust stochastic optimiza- tion [ Sachs et al. , 2022; Chen et al. , 2024], hence receiving increased attention [ Zhao et al. , 2020; Y an et al. , 2023; Tsai et al. , 2023; Ataee T arzanagh et al. , 2024; Zhao et al. , 2024]. In online learning, it is proved that the smoothness assumpt ion is necessary for ﬁrst-order algo- rithms to achieve gradient-variation regret bounds as disc ussed in Remark 1 of Y ang et al. [2014], which is also restated in Proposition 2 in Appendix B. Previous works typically rely on the global L-smoothness condition, imposing a ﬁxed upper bound on the gr adient Lipschitzness, i.e., requiring ∥∇2ft(x)∥2 ≤ Lfor all t∈ [T] and x ∈ X . However, this global assumption restricts the applica- bility of theories to loss functions that are quadratically bounded from above. Furthermore, recent studies in neural network optimization have observed pheno mena where the global smoothness con- dition fails to model optimization dynamics effectively, e specially for important types of neural net- works like LSTM [ Zhang et al. , 2020b] and Transformer [ Crawshaw et al. , 2022]. Therefore, modern optimization has devoted efforts to generalizing the smoot hness condition. For example, Zhang et al. [2020b] introduce (L0,L1)-smoothness, which assumes ∥∇2f(x)∥2 ≤ L0 + L1∥∇f(x)∥2 for an ofﬂine objective function f(·). A notable generalization is the recent proposal of the ℓ-smoothness condition [ Li et al. , 2023], which assumes ∥∇2f(x)∥2 ≤ ℓ(∥∇f(x)∥2) with a link function ℓ(·), signiﬁcantly broadening previous assumptions through the ﬂexibility of ℓ(·). Given this, it is natu- ral to ask how to design online algorithms to exploit generalized smoo thness and obtain favorable gradient-variation regret guarantees. In this paper, we provide a systematic study of gradient-variation online learning under generalized smoothness. W e extend the classic optimistic online mirror descent (optimistic OMD) algorithm [ Chiang et al. , 2012; Rakhlin and Sridharan , 2013a] to derive gradient-variation regret bounds, achieving O(√VT) regret and O(log VT) regret for convex and strongly convex functions under generalized smoothness, respectively. W e emphasize the im portance of stability analysis across the optimization trajectory, which allows generalized smo othness to be effectively exploited locally. Speciﬁcally, optimistic OMD maintains two sequences with s ubmitted decisions {xt}T t=1 and inter- mediate decisions {ˆxt}T t=1. W e need to control algorithmic stability by appropriate st ep size tuning and optimism design, ensuring that xt is sufﬁciently close to ˆxt to exploit local smoothness at ˆxt. Based on this development, we investigate universal online learning [ van Erven and Koolen , 2016; W ang et al. , 2019; Mhammedi et al. , 2019; Zhang et al. , 2022a; Y an et al. , 2023; Y ang et al. , 2024], where the learner aims to design a single algorithm that simu ltaneously attains the optimal regret for both convex and strongly functions without the prior kno wledge of curvature information. For this scenario, a common wisdom is to adopt an online ensemble consisting of a meta-base two-layer structure to handle the environmental uncertainty [ Zhao et al. , 2024], i.e., the unknown curvature of loss functions, where a meta-algorithm is running over a set of base-learners with different conﬁgura- tions. The base-learners are basically the instantiations of the developed variants of optimistic OMD, as mentioned earlier. However, designing the meta-algorit hm is non-trivial with new challenges. The ﬁrst challenge is from the potentially unbounded smooth ness, which might lead to unbounded Lipschitz constants as well. This challenge requires the me ta-algorithm to be Lipschitz-adaptive, adapting to Lipschitzness on the ﬂy. Furthermore, we also ex pect it to provide a second-order regret, technically required when analyzing the ensemble errors, a nd to enable predictions with optimism , thereby producing the gradient variation. The second chall enge is the complexity introduced by the combination procedure inherent in the ensemble method, whi ch further complicates the smoothness estimation, making it difﬁcult to properly tune the meta-al gorithm and exploit smoothness. T o this end, we address both challenges with the function-variation-to-gradient-variation conver- sion and a newly-designed Lipschitz-adaptive meta-algori thm. The conversion technique, drawing inspiration from Bai et al. [2022], decouples the design between the meta and base levels and d e- rives the gradient variation directly from function values , allowing us to avoid the cancellation-based analysis [ Y an et al. , 2023] for utilizing smoothness at the meta level. Nevertheless, this conversion 2requires the meta-algorithm to handle heterogeneous inputs due to certain technical considerations, and we are not aware of available algorithms satisfying all t he requirements, motivating us to de- sign a new algorithm. Based on optimistic Adapt-ML-Prod [ W ei et al. , 2016] and the clipping technique [ Cutkosky, 2019], we present a new Lipschitz-adaptive meta-algorithm with a simpler algorithmic design, which can be of independent interest. W ith this algorithm, we can apply the function-variation-to-gradient-variation conversion t o achieve the optimal results for both convex and strongly convex functions, up to doubly logarithmic fac tors of T, without knowing curvature. Our ﬁndings for gradient-variation online learning are use ful for several important applications, in- cluding fast-convergence online games [ Rakhlin and Sridharan , 2013a; Syrgkanis et al. , 2015] and stochastic extended adversarial online learning [ Sachs et al. , 2022], where we establish new results under the generalized smoothness condition. The rest of paper is organized as follows. Section 2 provides preliminaries and key ideas for exploit- ing the generalized smoothness throughout the trajectory. In Section 3 we study universal online learning and present our key meta-algorithm. Section 4 discusses our applications. Related work is provided in Appendix A. All proofs can be found in the remaining appendices (Append ix B – D). 2 Gradient-V ariation Online Learning under Generalized Sm oothness In this section, we ﬁrst introduce the problem setup, includ ing the formal deﬁnition of generalized smoothness and other assumptions used in the paper. W e then e xtend the optimistic online mirror descent framework to achieve gradient-variation regret bo unds under generalized smoothness. 2.1 Problem Setup: Generalized Smoothness and Assumptions Recent studies [ Zhang et al. , 2020b; Chen et al. , 2023b] extend the global smoothness condition by allowing the smoothness to positively correlate with the gradient norm, where a particular func- tion is required to model this relationship. Zhang et al. [2020b] introduce the (L0,L1)-smoothness condition, where the smoothness is upper bounded by a linear function of the gradient norm, i.e., ∥∇2f(x)∥2 ≤ L0 + L1∥∇f(x)∥2. Li et al. [2023] further generalize this by imposing a weaker assumption on the link function and propose the generalized smoothness deﬁned as follows. Deﬁnition 1 (ℓ-smoothness). A twice-differentiable function f : X ↦→ R is called ℓ-smooth for some non-decreasing continuous link function ℓ : [0 ,+∞) ↦→(0,+∞) if it satisﬁes that ∥∇2f(x)∥2 ≤ ℓ(∥∇f(x)∥2) for any x ∈ X . The mild requirement on the link function ℓ(·) allows for considerable generality. By selecting a linear link function, ℓ-smoothness immediately recovers (L0,L1)-smoothness [ Zhang et al. , 2020b]. Furthermore, it has been shown that ℓ-smoothness can imply a wide class of functions including rational, logarithmic, and self-concordant functions [ Li et al. , 2023]. Based on this generalized smoothness notion, we now provide the formal assumption on t he smoothness of online functions. Assumption 1 (generalized smoothness) . The online function ft : X ↦→R is ℓt-smooth in an open set containing X ⊆ Rd for t∈ [T], and the learner can query ℓt(x) provided any point x ∈ X . W e also require a standard bounded domain assumption in the O CO literature [ Hazan, 2016]. Assumption 2 (bounded domain) . The feasible domain X ⊆ Rd, which contains the origin 0, is non-empty and closed with the diameter bounded by D, i.e., ∥x − y∥2 ≤ Dfor any x,y ∈ X . W e do not assume the prior knowledge of the Lipschitz constan t of online functions. In fact, the unboundedness of smoothness may result in unbounded Lipsch itz constants. If a Lipschitz upper bound were known, the generalized smoothness condition wou ld be trivialized, as it would allow us to directly compute the upper bound of the smoothness cons tant. Furthermore, following the discussion in Jacobsen and Cutkosky [2023, Page 2, second paragraph on the right], we assume that there exist ﬁnite but unknown upper bounds Gand Lfor Lipschitzness and smoothness to ensure the theoretical results are valid. Note that these quantiti es will only appear in the ﬁnal regret bounds, and our algorithms does not use them as the inputs. Throughou t the paper, we use the O(·)-notation to hide the constants and use the ˜O(·)-notation to omit the poly-logarithmic factors in T. 32.2 Algorithmic Framework W e choose optimistic online mirror descent (optimistic OMD ) [ Rakhlin and Sridharan , 2013a] as the algorithmic framework, which provides a uniﬁed view to desi gn and analyze many online algorithms. Compared to classic OMD [ Nemirovskij and Y udin , 1985; Beck and T eboulle , 2003], optimistic OMD predicts with side information, an optimistic vector Mt ∈ Rd. This optimistic vector, also known as optimism, serves as a prediction of the incoming fun ction ft+1(·), leading to tighter regret bounds when accurate. Optimistic OMD updates the decisions in two steps: xt = arg min x∈X {⟨Mt,x⟩+ Bψt (x,ˆxt)} , ˆxt+1 = arg min x∈X {⟨∇ft(xt),x⟩ + Bψt (x,ˆxt)} , (3) where Bψt (x,y) = ψt(x) − ψt(y) − ⟨∇ψt(y),x − y⟩ is the Bregman divergence associated with the regularizer ψt : X ↦→R. Optimistic OMD maintains two sequences: the sequence of su bmitted decisions {xt}T t=1, and the one of intermediate decisions {ˆxt}T t=1. Although a simpliﬁed optimistic OMD with one-step update per round exists [ Joulani et al. , 2020], we will demonstrate later that tun- ing the step size based on intermediate decisions is crucial for adapting to generalized smoothness. 2.3 Gradient-V ariation Regret for Convex and Strongly Conv ex Functions When minimizing the convex or strongly convex functions, we set the regularizer as ψt(x) = 1 2ηt ∥x∥2 2and optimistic OMD updates with the following steps: xt = Π X [ˆxt − ηtMt] , ˆxt+1 = Π X [ˆxt − ηt∇ft(xt)] , (4) where Π X [y] = arg min x∈X ∥x − y∥2 denotes the Euclidean projection operator. Next, we brieﬂy review approaches for obtaining the gradient-variation bo und under global smoothness . This bound typically follows from the regret analysis for optimistic O MD: RE GT ≲ 1 ηT + T∑ t=1 ηt∥∇ft(xt) − Mt∥2 2− T∑ t=1 1 ηt (∥xt − ˆxt∥2 2+ ∥ˆxt − xt−1∥2 2). (5) On the right-hand side, the second term is known as the stabil ity term, while the third one is the negative terms that can be further bounded by O(− ∑ T t=1 1 ηt ∥xt − xt−1∥2 2). Previous studies [ Chi- ang et al. , 2012; Zhao et al. , 2024] for gradient-variation regret under global smoothness of ten set optimism as Mt = ∇ft−1(xt−1), such that, the positive stability term can be upper bounded by ηt∥∇ft(xt) − ∇ft−1(xt)∥2 2+ ηt∥∇ft−1(xt) − ∇ft−1(xt−1)∥2 2, where the ﬁrst part can be directly converted to the desired gradient variation and the second p art will be at most ηtL2∥xt − xt−1∥2 2 under global smoothness. Given the smoothness constant L, tuning the step size as ηt ≤ 1/(4L) ensures O(ηtL2∥xt− xt−1∥2 2− 1 ηt ∥xt− xt−1∥2 2) ≤ 0, thus obtaining the gradient-variation bound. However, under generalized smoothness, we do not have a glob al parameter Lfor setting the step sizes, and the smoothness constants are related to the decis ions. T o follow the previous approach, optimistic OMD would require the smoothness constant before generating xt to tune the step size, ensuring that the negative terms are large enough to cancel ηt∥∇ft−1(xt) − ∇ft−1(xt−1)∥2 2. Nev- ertheless, the smoothness constant between xt and xt−1 can only be evaluated after updating to xt, resulting in a contradiction. Unlike ofﬂine optimization, where the function is ﬁxed and smoothness constants can be shown to decrease along the trajectory [ Li et al. , 2023], in online optimization, the online functions change at each round, preventing the reuse of previous smoothness estimations. T o address this challenge, our key idea is to perform a trajec tory-wise analysis and conﬁgure the algorithm using estimated smoothness so far. The key techni cal lemma is the local smoothness property of ℓt-smooth functions [ Li et al. , 2023], which allows the smoothness constant between two points to be estimated in advance, provided that the two p oints are close enough. Lemma 1 (local smoothness [ Li et al. , 2023, Lemma 3.3]) . Suppose f : X ↦→ R is ℓ-smooth. F or ∀x,y ∈ X such that ∥x − y∥2 ≤ ∥∇f(x)∥2 ℓt(2∥∇f(x)∥2) , ∥∇f(x) − ∇f(y)∥2 ≤ ℓ(2∥∇f(x)∥2) · ∥x − y∥2. Recall that in the update procedures ( 3) of optimistic OMD, the submitted decision xt is updated based on the intermediate decision ˆxt. Therefore, it is convenient to control their distance and then exploit the local smoothness at point ˆxt. Speciﬁcally, we set optimism Mt = ∇ft−1(ˆxt) and the step size ηt ≤ 1/(4ˆLt−1), where ˆLt−1 = ℓt−1(2∥∇ft−1(ˆxt)∥2) denotes the locally estimated 4smoothness and is used to tune the step size. This conﬁgurati on leads to ηt∥∇ft(xt)−∇ft−1(ˆxt)∥2 2 for the second term in Eq. ( 5), which can be further upper bounded as ∥∇ft(xt) − ∇ft−1(ˆxt)∥2 2≤ 2∥∇ft(xt) − ∇ft−1(xt)∥2 2+ 2∥∇ft−1(xt) − ∇ft−1(ˆxt)∥2 2. (6) The ﬁrst part is basically the favorable gradient variation , so it sufﬁces to handle the second part. Performing the stability analysis for OMD and noticing the s tep size setting, it can be veriﬁed that ∥xt − ˆxt∥2 ≤ ηt∥∇ft−1(ˆxt)∥2 ≤ ∥∇ ft−1(ˆxt)∥2/(4ˆLt−1). This satisﬁes the criteria for applying Lemma 1 to the ℓt−1-smooth function ft−1(·), allowing us to upper bound the second term in ( 6) by O(ˆL2 t−1 · ∥xt − ˆxt∥2 2). W e have clipped ηt by 1/(4ˆLt−1), thereby ensuring the negative term is sufﬁcient to cancel out the positive term. Below , we summari ze the result for convex functions. Theorem 1. Under Assumptions 1 - 2 and assuming online functions are convex, we set the optimis m as Mt = ∇ft−1(ˆxt) and f0(·) = 0 , with step sizes as η1 = Dand, for t≥ 2, ηt = min {√ D2 1 + ∑ t−1 s=1∥∇fs(xs) − ∇fs−1(xs)∥2 2 , min s∈[t] 1 4ℓs−1(2∥∇fs−1(ˆxs)∥2) } , (7) optimistic OMD in (4) ensures the following regret bound, RE GT ≤ O ( D √ VT + ˆLmax · D2 ) , where VT = ∑ T t=2 supx∈X ∥∇ft(x) − ∇ft−1(x)∥2 2measures the gradient variations and ˆLmax = maxt∈[T] ˆLt is the maximum smoothness constant over the optimization tr ajectory. This result implies a tighter bound in scenarios where the en vironments change slowly, i.e., VT = O(1). Meanwhile, it safeguards the worst-case optimal result si nce VT ≤ O (T) holds in all cases. When assuming ℓt(·) ≤ L for t ∈ [T], the ℓt-smoothness condition degenerates to the classic global L-smoothness condition, and our result implies an O(√ VT + LD2) bound, which matches the best-known gradient-variation regret bounds with the ﬁ rst-order oracle [ Chiang et al. , 2012; Y an et al. , 2023; Zhao et al. , 2024] even in terms of the dependence on D and L. Compared to ofﬂine optimization, our result depends on ˆLmax, the maximum smoothness constant along the trajectory. This dependence arises from the adversarial nature of onlin e learning, where the loss functions chosen in consecutive rounds may differ signiﬁcantly, maki ng it hopeless to leverage the previous estimates of smoothness to improve the dependence. W e further provide an improved gradient-variation regret b ound for strongly convex functions, with step size tuning based on recent result under global smoothn ess [ Chen et al. , 2024, § 3.4]. Theorem 2. Under Assumptions 1 - 2 and assuming online functions are λ-strongly convex, we set the optimism as Mt = ∇ft−1(ˆxt), f0(·) = 0 , and step sizes as η1 = 2 /λ and, for t ≥ 2, ηt = 2 /(λt+ 16 max s∈[t] ℓs−1(2∥∇fs−1(ˆxs)∥2)), optimistic OMD in (4) ensures the regret bound RE GT ≤ O (1 λ log VT + ˆLmax · D2) , where ˆLmax = max t∈[T] ˆLt. The above theorem requires the knowledge of curvature infor mation λ. In Section 3, we design a universal method to remove this requirement and achieve the optimal gu arantees for convex and strongly convex functions simultaneously without knowing λ. In Appendix B.2, we discuss the challenge to obtain a gradient-variation bound for exp-con cave functions under Assumption 1. 3 Universal Online Learning under Generalized Smoothness Classic online learning algorithms require the curvature information of online functions as algo- rithmic parameters to achieve favorable regret guarantees . However, obtaining these curvature pa- rameters can be difﬁcult in practice. This challenge motiva tes the recent study of universal online learning [ van Erven and Koolen , 2016; Cutkosky and Boahen , 2017; W ang et al. , 2019; Mhammedi et al. , 2019; Zhang et al. , 2022a; Y an et al. , 2023; Y ang et al. , 2024], which aims to design a single al- gorithm that can achieve optimal regrets without knowing th e curvature information. In this section, we study universal online learning with gradient-variatio n regret under generalized smoothness. 53.1 Reviewing Related W ork and T echniques W e review related work on gradient-variation universal onl ine learning under global smoothness [Zhang et al. , 2022a; Y an et al. , 2023]. T o handle the unknown curvature, universal online learni ng algorithms utilize a two-layer structure, consisting of a m eta-algorithm that ensembles a group of base-learners. Each base-learner optimizes functions wit h a speciﬁc convex curvature, while the meta-algorithm is designed to ensure that ensemble errors d o not ruin base-learners’ guarantees. Denoted by N the number of base-learners, the decision xt = ∑ i∈[N] pt,ixt,i submitted by a two- layer structure algorithm comprises two key components: pt ∈ ∆ N, the weights provided by the meta-algorithm, and xt,i, the decision of the i-th base-learner. The analysis of a universal algorithm begins by decomposing the regret into two parts against any b ase-learner. In particular, we choose the base-learner with the best performance (the index i⋆ is unknown) as the benchmark: RE GT = T∑ t=1 ft(xt) − T∑ t=1 ft(xt,i⋆ )    META-R EG + T∑ t=1 ft(xt,i⋆ ) − min x∈X T∑ t=1 ft(x)    BASE -R EG , (8) where the ﬁrst part is the meta-regret, evaluating the meta- algorithm’s performance against the best base-learner, and the second part, known as the base-regret , measures the best learner’s performance. Zhang et al. [2022a] advocate for a simple approach by employing the meta-algor ithms with second- order regret guarantees, which facilitates the analysis at the meta level. In speciﬁc, they use Adapt-ML-Prod [ Gaillard et al. , 2014] as the meta-algorithm, showing that the meta-regret for strongly convex and exp-concave functions are constants by exploiting the negative terms from convexity. Consider λ-strongly convex functions as an example and assume the i⋆-th base-learner ensures the optimal O( 1 λ log VT) base-regret. At the meta level, Zhang et al. [2022a] pass the lin- earized regret rt,i = ⟨∇ft(xt),xt − xt,i⟩ to the meta-algorithm for each base-learner. By strong convexity and the guarantees of Adapt-ML-Prod, the meta-re gret can be bounded by a constant: ME TA-R E G ≤ T∑ t=1 rt,i⋆ − λ 2 T∑ t=1 ∥xt − xt,i⋆ ∥2 2≲    √ T∑ t=1 r2 t,i⋆ − λ 2 T∑ t=1 ∥xt − xt,i⋆ ∥2 2≤ O (1), where the last inequality follows from √ ∑ t⟨∇ft(xt),xt − xt,i⋆ ⟩2 ≤ ˆGmax √ ∑ t∥xt − xt,i⋆ ∥2 2 and is then canceled by the negative terms via the AM-GM inequ ality. By leveraging the negative terms from strong convexity, the meta-regret can be well-bo unded, allowing the overall regret to be dominated by the base-regret, which is then controlled by se lecting appropriate base-algorithms. However, this method is unsuitable for producing the gradie nt-variation bound for convex functions. T o address it, Y an et al. [2023] propose to use a meta-algorithm which ensures an optimisti c and second-order regret bound while provides additional negat ive terms − ∑ t∥pt − pt−1∥2 1. Besides showing that the meta-regret is a constant for strongly conv ex and exp-concave functions following the previous approach, with newly designed optimism, Y an et al. [2023] prove that the meta-regret for convex functions can be roughly bounded by: O (√ VT + T∑ t=1 ∥xt,i⋆ − xt−1,i⋆ ∥2 2+ L2 T∑ t=1 ∥pt − pt−1∥2 1+ L2 T∑ t=1 N∑ i=1 pt,i∥xt,i − xt−1,i∥2 2 ) . The ﬁrst term is the gradient variation, matching the optima l order of convex functions. Y an et al. [2023] demonstrate that the remaining stability terms can be canc eled through the collaboration between the base and meta levels [ Zhao et al. , 2024] with the prior knowledge of the global smooth- ness constant L, thus obtaining the near-optimal gradient-variation boun ds for convex functions as well. Nevertheless, the employed meta-algorithm already h as a two-layer structure, resulting in a three-layer structure for the overall algorithm, which is r elatively complicated. 3.2 Key Challenges and Main Ideas W e aim to design a universal algorithm with the optimal gradi ent-variation bounds under general- ized smoothness, which exhibits two challenges. First, the Lipschitz condition of online functions is unknown to the meta-algorithm, which requires the meta-a lgorithm to be Lipschitz-adaptive, pro- vide a second-order regret , and enable predictions with optimism . Second, the combination of the 6ensemble method further complicates the estimation of smoo thness constants, making it challenging to tune algorithms properly and to cancel stability terms as Y an et al. [2023] did. W e tackle the second challenge by utilizing a function-variation-to-gradient-variation conversion to derive the gradient-variation bounds at the meta level, dra wing inspiration from the development of dynamic regret minimization [ Bai et al. , 2022]. This conversion technique decouples the meta and base levels, allowing us to avoid cancellation-based analy sis. T o illustrate, suppose a meta-algorithm ensuring O( √ ∑ t(ℓt,i⋆ − mt,i⋆ )2) provided optimism mt = ( mt,1,...,m t,N). By setting ℓt,i⋆ = ft(xt,i⋆ ) − ft(xref ) and mt,i⋆ = ft−1(xt,i⋆ ) − ft−1(xref ), where xref is a ﬁxed reference point, the meta regret bound becomes O( √ ∑ t[(ft(xt,i) − ft−1(xt,i)) − (ft(xref ) − ft−1(xref ))]2). By the mean value theorem, [(ft(xt,i) − ft−1(xt,i)) − (ft(xref ) − ft−1(xref ))]2 equals the ﬁrst term below , which can be further upper bounded by the gradient variation : [⟨∇ft(ξt,i) − ∇ft−1(ξt,i),xt,i − xref ⟩]2 ≤ D2 supx∈X ∥∇ft(x) − ∇ft−1(x)∥2 2. This technique brings hope for minimizing the convex functi ons. T o develop a universal method, our ﬁrst attempt is to utilize MsMwC-Master [ Chen et al. , 2021] as the meta-algorithm, which satisﬁes all the three requirements imposed by the ﬁrst challenge. Ne vertheless, the heterogeneous inputs at the meta level present another challenge to this approach . The heterogeneity arises as we pass rt,i = ft(xt) − ft(xt,i) to the meta-algorithm for base-learner responsible for con vex functions, leveraging the conversion technique, while rt,i = ⟨∇ft(xt),xt− xt,i⟩for base-learners minimizing strongly convex functions. It remains unclear how to adapt M sMwC-Master [ Chen et al. , 2021] to our heterogeneous inputs, as MsMwC-Master requires an ℓt as inputs, and the guarantee is for the regret in the form of rt = ⟨pt,ℓt⟩ − ℓt. However, such an ℓt cannot be retrieved from our above design. Fortunately, we observe that the Prod algorit hms [ Cesa-Bianchi et al. , 2007; Gaillard et al. , 2014; W ei et al. , 2016] are friendly to heterogeneous inputs. T echnically, the Pr od algorithms provide the same guarantees as long as ∑ i∈[N] pt,irt,i ≤ 0, thanks to the potential-based analysis. Therefore, aside from the requirements of the ﬁrst challeng e, we expect that the meta-algorithm can be analyzed similarly to the Prod algorithms, motivating us to design a new meta-algorithm. As a byproduct, we present a universal algorithm with a two-lay er structure under global smoothness with the developed techniques. It is more efﬁcient than that by Y an et al. [2023] and attains the optimal gradient-variation guarantees for convex, strong ly convex, and exp-concave functions, at a cost of additional function value queries. W e defer algorit hms and regret bounds to Appendix C.5. 3.3 A New Lipschitz-Adaptive Meta-Algorithm In Algorithm 1, we present our meta-algorithm, which builds on optimistic Adapt-ML-Prod [ W ei et al. , 2016] and incorporates the clipping technique introduced by Cutkosky [2019] and further reﬁned by Chen et al. [2021]. This algorithm, described in the language of Prediction w ith Experts’ Advice (PEA), may be of independent interest beyond adaptin g to the generalized smoothness. Apart for satisfying all expected requirements, this algorithm o ffers a simpler design, which does not need to restart as opposed to Squint+L [ Mhammedi et al. , 2019] and MsMwC-Master [ Chen et al. , 2021]. This efﬁciency improvement is achieved through a novel self -conﬁdent learning rate in Line 6 of Algorithm 1, unlike previous Lipschitz-adaptive algorithms that use a ﬁxed learning rate and thus require restarts. In essence, our approach incorporates th e clipping mechanism by adding B2 t to the denominator and removing the threshold on learning rate s commonly applied in prior Prod al- gorithms [ Gaillard et al. , 2014; W ei et al. , 2016]. This term B2 t acts as a threshold, ensuring that ηt,i|¯rt,i − mt,i| ≤ 1/2, a critical condition in the analysis (typically satisﬁed w hen the Lipschitz constant is provided for prior Prod algorithms). In contras t to previous Prod methods with an ex- plicit clipping operation like min{ηcommon t+1,i ,1/(2Bt)}, where both the commonly applied learning rate ηcommon t+1,i and the threshold 1/(2Bt) are decreasing and thus unstable in our cases, our learning rate remains stable by merging B2 t to the denominator, allowing us to keep ηt,i/ηt+1,iwell-bounded. Theorem 3 summarizes the guarantee of Algorithm 1 and the proof is provided in Appendix C.3. Theorem 3. Setting mt,i = ⟨pt,ℓt−1⟩ − ℓt−1,i in Algorithm 1 ensures that, for any i⋆ ∈ [N],∑ T t=1⟨pt,ℓt⟩ − ∑ T t=1 ℓt,i⋆ is bounded as follows, where BT = max {B0,maxt∈[T]∥rt − mt∥∞}: O (   √ T∑ t=1 (rt,i⋆ − mt,i⋆ )2 · ( log(N) + log( BT + log T) ) + BT ) ≤ ˜O (   √ T∑ t=1 ∥ℓt − ℓt−1∥2 ∞ ) . 7Algorithm 1 Lipschitz Adaptive Optimistic Adapt-ML-Prod Input: prior information of the scale B0, the number of experts N. 1: Initialization: set w1,i = 1 , m1,i = 0 and η1,i = 1 / √ 1 + 4 B2 0 for all i∈ [N]. 2: for t= 1 to T do 3: Update the weight for i∈ [N] ˜wt,i = wt,iexp(ηt,imt,i); 4: Calculate decision pt ∈ ∆ N with pt,i = ηt,i ˜wt,i∑ j∈ [N ] ηt,j ˜wt,j and submit it; 5: Receive rt, update Bt = max {Bt−1,∥rt−mt∥∞}, and build ¯rt,i = mt,i+ Bt− 1 Bt (rt,i−mt,i); 6: Update the learning rate for i∈ [N]: ηt+1,i = √ 1 1+∑ t s=1(¯rs,i−ms,i)2+4B2 t ; 7: Update the weight for i∈ [N]: wt+1,i = ( wt,iexp ( ηt,i¯rt,i − η2 t,i(¯rt,i − mt,i)2))ηt+1,i ηt,i . 8: end for Algorithm 2 Universal Gradient-V ariation Online Learning under Gener alized Smoothness Input: curvature coefﬁcient pool H, number of base-learners N, prior information of the scale B0. 1: Initialization: Send N and B0 to the meta-algorithm, for λ ∈ H , initialize an algorithm in Theorem 2 with it; initialize an algorithm in Theorem 1. 2: for t= 1 to T do 3: Obtain pt from meta-algorithm, xt,i from each base-learner i∈ [N]; 4: Submit xt = ∑ i∈[N] pt,ixt,i; 5: Receive ft(·) and send it to each base-learner for update; 6: For strongly convex functions learners : set rt,i = ⟨∇ft(xt),xt − xt,i⟩; 7: For convex functions learner : set rt,i = ft(xt) − ft(xt,i); 8: Send mt+1,i = ft(xt) − ft(xt,i) to the meta-algorithm for i∈ [N]. 9: end for Our algorithm improves efﬁciency at the cost of an additiona l factor O(√log N). This factor is ignorable for universal online learning since we set N = O(log T), and the factor O(log log T) is often treated as a constant [ Gaillard et al. , 2014; Luo and Schapire , 2015]. Considering other related Lipschitz-adaptive algorithms, Mhammedi et al. [2019] obtain a regret bound of O( √ ∑ t(rt,i⋆ )2 · (log(N) + log log( BTT)) + BT log(N)), which offers better dependence on the dominant term √ ∑ t(rt,i⋆ )2 but it is unclear how to include optimism. Chen et al. [2021] achieve a bound of O( √ ∑ t(ℓt,i⋆ − mt,i⋆ )2 · log(NT) + BT log(NT)) with a two-layer algorithm; however, the O(√log T) term would ruin the desired O(log VT) bound for strongly convex functions. W e remark that the compared methods enjoy other strengths not d iscussed here, such as the ability to compete with an arbitrary competitor x ∈ ∆ N and the versatility to handle various learning scenar- ios, while our method is sufﬁcient for our purpose and the onl y option to tackle all the challenges as we mention in Section 3.2. Lastly, notice that the optimism mt involves the decision pt, which might be improper since pt depends on mt as well. W e refer readers to Appendix C.1 for efﬁciently setting mt through a univariate binary search. W e emphasize that optimism mt in our algorithm is not chosen arbitrarily. In Line 5, we clip the regret with optimism to keep them on the same scale. The perfo rmance is then evaluated based on the clipped regret ¯rt,i. For the analytical purpose, it is essential that ⟨pt,¯rt⟩ ≤ 0, and a sufﬁcient condition for this is ensuring ⟨pt,mt⟩ ≤ 0, which imposes an additional requirement on mt. In Appendix C.2, we discuss how this requirement introduces challenges for exp-concave functions minimization in universal online learning. 3.4 Overall Algorithm and Regret Guarantees The function-variation-to-gradient-variation techniqu e decouples the design of universal methods into the base and meta levels, and we are ready to combine the p roposed components together. W e employ algorithms in Section 2.3 as the base-learners and use Algorithm 1 as the meta-learner, concluding in Algorithm 2. Theorem 4 presents its guarantee with the proof in Appendix C.4. 8Theorem 4. Under Assumptions 1 - 2 and assuming a global lower bound such that ¯ f ≤ ft(x) for any x ∈ X ,t ∈ [T], setting N = ⌈log2 T⌉ + 1 , deﬁning the curvature coefﬁcient pool H = {2i−1/T : i∈ [N − 1]}, and specifying B0, Algorithm 2 simultaneously ensures: RE GT ≤    O(√ VT · log(BT + log T)), (convex), O ( 1 λ log VT + ˆG2 max(log(BT + log T))2/λ ) , (λ-strongly convex ), where λ ∈ [1/T,1], BT = O(max{B0,D maxt∈[T] supx∥∇ft(x) − ∇ ft−1(x)∥2}) and ˆGmax is the Lipschitz constant on the optimization trajectory. Without loss of generality, we assume λ ∈ [1/T,1] for strongly convex functions. If λ <1/T, the optimal O((log VT)/λ) bound would imply linear regret, in which case we would treat them as general convex functions. If λ> 1, our result is slightly worse than the optimal one by a neglig ible constant factor. This simpliﬁcation is also employed by Zhang et al. [2022a]; Y an et al. [2023]. Remark 1. This theorem additionally requires the lower bound of loss f unctions, which is used to perform the binary search when setting the optimism. W e defe r the details of the binary search to Appendix C.1. This assumption is also employed recently in parameter-fr ee optimizations [ Hazan and Kakade , 2019; Attia and Koren , 2024; Khaled and Jin , 2024], and we can simply choose ¯ f = 0 in empirical risk minimization settings [ Hazan and Kakade , 2019]. 4 Applications In this section, we demonstrate the importance of our results by providing two applications (SEA model and online games), where new results can be directly im plied from our ﬁndings. 4.1 Stochastically Extended Adversarial (SEA) Model The stochastically extended adversarial (SEA) model [ Sachs et al. , 2022] interpolates adversarial and stochastic online optimization. It assumes that the env ironments select the loss function ft(·) from a distribution Dt. The adversarial nature is characterized by shifts in distr ibution Dt, and when Dt remains constant, the model captures the environments’ sto chastic behavior. The following quan- tities are introduced to measure the levels of adversarial a nd stochastic behaviors in environments: Σ 2 1:T = E [ T∑ t=2 sup x∈X ∥∇Ft(x) − ∇Ft−1(x)∥2 2 ] ,σ2 1:T = T∑ t=1 sup x∈X E[∥∇ft(x) − ∇Ft(x)∥2 2]. (9) where we denote by Ft(x) = Eft ∼Dt [ft(x)]. In above, Σ 2 1:T represents the adversarial shift of the distribution, and σ2 1:T denotes the stochastic variance. Sachs et al. [2022] prove an O( √ σ2 1:T + √ Σ 2 1:T) regret for convex functions, and a reﬁned regret bound of O((σ2 max + Σ 2 max) log(σ2 1:T + Σ 2 1:T)) for strongly convex functions is obtained by Chen et al. [2023a]; Sachs et al. [2023], where σ2 max = max t∈[T] supx∈X E[∥∇ft(x) − ∇Ft(x)∥2 2] and Σ 2 max = max T t=1 supx∈X ∥∇Ft(x) − ∇ Ft−1(x)∥2 2. Y an et al. [2023] present a universal method which can obtain ˜O( √ σ2 1:T + √ Σ 2 1:T) and O((σ2 max + Σ 2 max) log(σ2 1:T + Σ 2 1:T)) bounds for convex and strongly convex functions. However, these results requ ire the global smoothness assumption. Our result in Section 3 implies a new ﬁnding for the SEA model, relaxing the assumpti on from the global smoothness to the generalized smoothness, while ada pting to unknown curvature, summa- rized in Corollary 1. The proof can be found in Appendix D.1. Corollary 1. Under Assumptions 1 - 2 and assuming a global lower bound for the loss functions such that ¯ f ≤ ft(x) for any x ∈ X ,t ∈ [T], setting N = ⌈log2 T⌉ + 1 , deﬁning the curvature coefﬁcient pool H = {2i−1/T : i∈ [N − 1]}, and specifying B0 with a speciﬁc value, then, under the SEA model, Algorithm 2 simultaneously ensures: E[RE GT] ≤ { O ( ( √ ˜σ2 1:T + √ Σ 2 1:T) · log( ˆGmaxD) ) , (convex), O ( (˜σ2 max + Σ 2 max) log(˜σ2 1:T + Σ 2 1:T) ) , (strongly convex ), where ˆGmax is the maximum empirical Lipschitz constant, ˜σ2 1:T = ∑ T t=1 E[supx∈X ∥∇ft(x) − ∇Ft(x)∥2 2], and ˜σ2 max = E[maxt∈[T] supx∈X ∥∇ft(x) − ∇Ft(x)∥2 2]. 9Note that, in real-world streaming learning applications, this corollary can offer a more generalized depiction of data throughput with limited computing resour ces [ Zhou, 2024; W ang et al. , 2024], given the connection between these scenarios and the SEA mod el [ Chen et al. , 2024, § 5.6]. Our result depends on ˜σ2 1:T, a larger quantity than σ2 1:T but still can track the stochastic variance. This is because our algorithm utilizes the information afterward xt−1 to generate xt. W e refer the interested readers for this subtle issue to the discussion by Chen et al. [2024]. This dependence currently is unknown how to improve even under the global smoothness cond ition since we need to leverage the function variation to produce gradient variation, inevita bly involving the afterward information. 4.2 Fast Rates in Games Our second application explores the min-max game, aiming toachieve an ε-approximate solution to the problem minx∈X maxy∈Y f(x,y) within an O(1/T) fast convergence rate. Here, we assume that f(·,y) is convex for any y ∈ Y , and correspondingly, f(x,·) is concave given any x ∈ X . Additionally, we assume that both X ⊂ Rn and Y ⊂ Rm are bounded convex sets. Pioneering research [ Syrgkanis et al. , 2015] demonstrates that optimistic algorithms can reach a conve rgence rate of O(1/T) by leveraging gradient variation. However, these results a re limited to the global smoothness condition. In this part, we demonstrate that our ﬁndings in Section 2 directly imply a new algorithm that can exploit generalized smoothness. Following the notations of Nemirovski [2004], we deﬁne Z = X ×Y , z = ( x,y) ∈ Z and introduce an operator F : Z → Rn× Rm with F(z) = ( ∇xf(x,y),−∇yf(x,y)). W e extend the concept of ℓ-smoothness to the min-max optimization setting as follows . Deﬁnition 2 (ℓ-smoothness for min-max game) . A differentiable convex-concave function f : X × Y ↦→R is called ℓ-smooth with a non-decreasing link function ℓ: [0 ,+∞) ↦→(0,+∞) if it satisﬁes: for any z1,z2 ∈ Z , if z1,z2 ∈ B ( z, ∥F(z)∥2 ℓ(2∥∇F(z)∥2) ) , then ∥F(z1) − F(z2)∥2 ≤ ℓ(2∥F(z)∥) · ∥z1 − z2∥2, where B(z,r) denotes the Euclidean ball centered at point z with radius r. This deﬁnition is a counterpart to Deﬁnition 1 in the min-max game, but weaker as it does not require the twice-differentiability requirement. The ε-approximate solution (x⋆,y⋆) to the min-max game is formally deﬁned by f(x⋆,y) − ε≤ f(x⋆,y⋆) ≤ f(x,y⋆) + εfor any x ∈ X ,y ∈ Y . T o achieve the fast convergence rate to the solution, indeed our result in Section 2 can be directly applied to the min-max game and obtains the following tailored algorithm f or min-max optimization: zt = Π Z [ˆzt − ηtF(ˆzt)] , ˆzt+1 = Π Z [ˆzt − ηtF(zt)] . (10) In above we set the optimism at the point ˆzt in order to exploit the smoothness locally. W e conclude our result in Corollary 2, with the proof available in Appendix D.2. Corollary 2. Assume that the convex-concave function f(x,y) is ℓ-smooth, and the domain Z is bounded with diameter D. By applying the tuning strategy described in Theorem 1, and deﬁning the ﬁnal approximated solution as ¯zT = 1 T ∑ T t=1 zt, where zt is generated by Eq. ( 10), we achieve an ε-approximate solution with a convergence rate of O(1/T). 5 Conclusion In this paper, we provide a systematic study of gradient-variation online learning under the gener- alized smoothness condition. W e exploit trajectory-wise s moothness to achieve the optimal regret bounds: O(√ VT) for convex functions and O(log VT) for strongly convex functions, respectively. W e further consider more complicated online learning scena rios, motivating us to design a new Lipschitz-adaptive meta-algorithm, which can be of indepe ndent interest. Hinging on this algorithm with the function-variation-to-gradient-variation tech nique, we design a universal algorithm which guarantees the optimal results for convex functions and str ongly convex functions simultaneously without knowing the curvature. In addition, our ﬁndings dir ectly imply new results in stochastic extended adversarial online learning and fast-rate games u nder generalized smoothness. An important future direction for future research is to expl ore whether our method can be further extended to accommodate the one-gradient feedback model, w here the learner receives only the gradient information of the decision submitted in each roun d. Another interesting problem is to exploit the exp-concavity in gradient-variation online le arning under generalized smoothness. 10Acknowledgements This research was supported by National Key R&D Program of Ch ina (2022ZD0114800) and Jiang- suSF (BK20220776). Peng Zhao was supported in part by the Xia omi Foundation. References J. D. Abernethy, P . L. Bartlett, A. Rakhlin, and A. T ewari. Optimal stragies and minimax lower bounds for online convex games. In Proceedings of the 21st Annual Conference on Learning Theory (COLT) , pages 415–424, 2008. D. Ataee T arzanagh, P . Nazari, B. Hou, L. Shen, and L. Balzano . Online bilevel optimization: Regret analysis of online alternating gradient methods. In Proceedings of The 27th International Conference on Artiﬁcial Intelligence and Statistics (AIST ATS), pages 2854–2862, 2024. A. Attia and T . Koren. How free is parameter-free stochastic optimization? In Proceedings of the 41st International Conference on Machine Learning (ICML) , pages 2009–2034, 2024. P . Auer, N. Cesa-Bianchi, and C. Gentile. Adaptive and self- conﬁdent on-line learning algorithms. Journal of Computer and System Sciences , 64(1):48–75, 2002. Y . Bai, Y .-J. Zhang, P . Zhao, M. Sugiyama, and Z.-H. Zhou. Ada pting to online label shift with provable guarantees. In Advances in Neural Information Processing Systems 35 (Neur IPS), pages 29960–29974, 2022. A. Beck and M. T eboulle. Mirror descent and nonlinear projec ted subgradient methods for convex optimization. Operation Research Letter , 31(3):167–175, 2003. N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games . Cambridge University Press, 2006. N. Cesa-Bianchi, Y . Mansour, and G. Stoltz. Improved second -order bounds for prediction with expert advice. Machine Learning , 66(2-3):321–352, 2007. L. Chen, H. Luo, and C. W ei. Impossible tuning made possible: A new expert algorithm and its applications. In Proceedings of the 34th Conference on Learning Theory (COLT ), pages 1216– 1259, 2021. S. Chen, W .-W . Tu, P . Zhao, and L. Zhang. Optimistic online mi rror descent for bridging stochastic and adversarial online convex optimization. In Proceedings of the 40th International Conference on Machine Learning (ICML) , pages 5002–5035, 2023a. S. Chen, Y .-J. Zhang, W .-W . Tu, P . Zhao, and L. Zhang. Optimis tic online mirror descent for bridging stochastic and adversarial online convex optimization. Journal of Machine Learning Research , 25 (178):1 – 62, 2024. Z. Chen, Y . Zhou, Y . Liang, and Z. Lu. Generalized-smooth non convex optimization is as efﬁcient as smooth nonconvex optimization. In Proceedings of the 40th International Conference on Machin e Learning (ICML) , pages 5396–5427, 2023b. C.-K. Chiang, T . Y ang, C.-J. Lee, M. Mahdavi, C.-J. Lu, R. Jin , and S. Zhu. Online optimization with gradual variations. In Proceedings of the 25th Conference On Learning Theory (COLT ), pages 6.1–6.20, 2012. M. Crawshaw , M. Liu, F . Orabona, W . Zhang, and Z. Zhuang. Robu stness to unbounded smoothness of generalized signsgd. In Advances in Neural Information Processing Systems 34 (Neur IPS), volume 35, pages 9955–9968, 2022. A. Cutkosky. Artiﬁcial constraints and hints for unbounded online learning. In Proceedings of the 32nd Annual Conference on Computational Learning Theory (C OLT), pages 874–894, 2019. A. Cutkosky and K. Boahen. Stochastic and adversarial onlin e learning without hyperparameters. In Advances in Neural Information Processing Systems 30 (NIPS ), pages 5059–5067, 2017. J. Duchi, E. Hazan, and Y . Singer. Adaptive subgradient meth ods for online learning and stochastic optimization. Journal of Machine Learning Research , 12(7), 2011. M. Faw , L. Rout, C. Caramanis, and S. Shakkottai. Beyond unif orm smoothness: A stopped analysis of adaptive SGD. In Proceedings of the 36th Conference on Learning Theory (COLT ), pages 89– 160, 2023. 11P . Gaillard, G. Stoltz, and T . van Erven. A second-order boun d with excess losses. In Proceedings of the 27th Annual Conference on Learning Theory (COLT) , pages 176–196, 2014. E. Hazan. Introduction to Online Convex Optimization. F oundations and T rends in Optimization , 2 (3-4):157–325, 2016. E. Hazan and S. Kakade. Revisiting the polyak step size. ArXiv preprint , arXiv:1905.00313, 2019. E. Hazan, A. Agarwal, and S. Kale. Logarithmic regret algori thms for online convex optimization. Machine Learning , 69(2-3):169–192, 2007. A. Jacobsen and A. Cutkosky. Parameter-free mirror descent . In Proceedings of the 35th Annual Conference on Learning Theory (COLT) , pages 4160–4211, 2022. A. Jacobsen and A. Cutkosky. Unconstrained online learning with unbounded losses. In Proceedings of the 40th International Conference on Machine Learning (I CML), pages 14590–14630, 2023. P . Joulani, A. György, and C. Szepesvári. A modular analysis of adaptive (non-)convex optimiza- tion: Optimism, composite objectives, variance reduction , and variational bounds. Theoretical Computer Science , 808:108–138, 2020. A. Khaled and C. Jin. Tuning-free stochastic optimization. In Proceedings of the 41st International Conference on Machine Learning (ICML) , pages 23622–23661, 2024. B. Kulis and P . L. Bartlett. Implicit online learning. In Proceedings of the 27th International Conference on Machine Learning (ICML) , pages 575–582, 2010. H. Li, J. Qian, Y . Tian, A. Rakhlin, and A. Jadbabaie. Convex a nd non-convex optimization under generalized smoothness. In Advances in Neural Information Processing Systems 35 (Neur IPS), 2023. H. Lu, R. M. Freund, and Y . Nesterov. Relatively smooth conve x optimization by ﬁrst-order methods, and applications. SIAM Journal on Optimization , 28(1):333–354, 2018. H. Luo and R. E. Schapire. Achieving all with no parameters: A daNormalHedge. In Proceedings of the 28th Annual Conference on Computational Learning The ory (COLT) , pages 1286–1304, 2015. Z. Mhammedi, W . M. Koolen, and T . van Erven. Lipschitz adapti vity with multiple learning rates in online learning. In Proceedings of the 32nd Annual Conference on Computational Learning Theory (COLT) , pages 2490–2511, 2019. A. Nemirovski. Prox-method with rate of convergence O(1/t) for variational inequalities with lips- chitz continuous monotone operators and smooth convex-con cave saddle point problems. SIAM Journal on Optimization , 15(1):229–251, 2004. A. Nemirovskij and D. B. Y udin. Problem complexity and metho d efﬁciency in optimization. SIAM Review, 27(2):264–265, 1985. Y . Nesterov. Lectures on Convex Optimization , volume 137. Springer, 2018. C. Nicolò and O. Francesco. T emporal variability in implici t online learning. In Advances in Neural Information Processing Systems 33 (NeurIPS) , pages 12377–12387, 2020. F . Orabona. A Modern Introduction to Online Learning. ArXiv preprint , arXiv:1912.13213, 2019. F . Orabona and D. Pál. Scale-free online learning. Theoretical Computer Science , 716:50–69, 2018. A. Rakhlin and K. Sridharan. Online learning with predictab le sequences. In Proceedings of the 26th Conference On Learning Theory (COLT) , pages 993–1019, 2013a. A. Rakhlin and K. Sridharan. Optimization, learning, and ga mes with predictable sequences. In Advances in Neural Information Processing Systems 26 (NIPS ), pages 3066–3074, 2013b. A. Reisizadeh, H. Li, S. Das, and A. Jadbabaie. V ariance-red uced clipping for non-convex optimiza- tion. ArXiv preprint , arXiv:2303.00883, 2023. S. Sachs, H. Hadiji, T . van Erven, and C. Guzmán. Between stoc hastic and adversarial online convex optimization: Improved regret bounds via smoothnes s. In Advances in Neural Information Processing Systems 34 (NeurIPS) , pages 691–702, 2022. S. Sachs, H. Hadiji, T . van Erven, and C. Guzmán. Accelerated rates between stochastic and adver- sarial online convex optimization. ArXiv preprint , arXiv:2303.03272, 2023. 12V . Syrgkanis, A. Agarwal, H. Luo, and R. E. Schapire. Fast con vergence of regularized learning in games. In Advances in Neural Information Processing Systems 28 (NIPS ), pages 2989–2997, 2015. M. T elgarsky. Stochastic linear optimization never overﬁt s with quadratically-bounded losses on general data. In Proceedings of the 35th Annual Conference on Learning Theor y (COLT) , pages 5453–5488, 2022. C. Tsai, Y . Lin, and Y . Li. Data-dependent bounds for online p ortfolio selection without lipschitzness and smoothness. In Advances in Neural Information Processing Systems 36 (Neur IPS), pages 62764–62791, 2023. T . van Erven and W . M. Koolen. MetaGrad: Multiple learning ra tes in online learning. In Advances in Neural Information Processing Systems 29 (NIPS) , pages 3666–3674, 2016. B. W ang, H. Zhang, Z. Ma, and W . Chen. Convergence of adagrad f or non-convex objectives: Sim- ple proofs and relaxed assumptions. In Proceedings of the 36th Annual Conference on Learning Theory (COLT) , pages 161–190, 2023. G. W ang, S. Lu, and L. Zhang. Adaptivity and optimality: A uni versal algorithm for online convex optimization. In Proceedings of the 35th Conference on Uncertainty in Artiﬁc ial Intelligence (UAI), pages 659–668, 2019. J. W ang, M. Y u, P . Zhao, and Z.-H. Zhou. Learning with adaptiv e resource allocation. In Proceedings of the 41st International Conference on Machine Learning (I CML), pages 52099–52116, 2024. C.-Y . W ei, Y .-T . Hong, and C.-J. Lu. Tracking the best expert in non-stationary stochastic envi- ronments. In Advances in Neural Information Processing Systems 29 (NIPS ), pages 3972–3980, 2016. Y .-H. Y an, P . Zhao, and Z.-H. Zhou. Universal online learnin g with gradient variations: A multi- layer online ensemble approach. In Advances in Neural Information Processing Systems 36 (NeurIPS), pages 37682–37715, 2023. T . Y ang, M. Mahdavi, R. Jin, and S. Zhu. Regret bounded by grad ual variation for online convex optimization. Machine Learning , 95(2):183–223, 2014. W . Y ang, Y . W ang, P . Zhao, and L. Zhang. Universal online conv ex optimization with 1 projection per round. ArXiv preprint , arXiv:2405.19705, 2024. B. Zhang, J. Jin, C. Fang, and L. W ang. Improved analysis of cl ipping algorithms for non-convex optimization. In Advances in Neural Information Processing Systems 33 (Neur IPS), pages 15511– 15521, 2020a. J. Zhang, T . He, S. Sra, and A. Jadbabaie. Why gradient clippi ng accelerates training: A theoreti- cal justiﬁcation for adaptivity. In Proceedings of the 8th International Conference on Learnin g Representations (ICLR) , 2020b. L. Zhang, G. W ang, J. Y i, and T . Y ang. A simple yet universal st rategy for online convex optimiza- tion. In Proceedings of the 39th International Conference on Machin e Learning (ICML) , pages 26605–26623, 2022a. M. Zhang, P . Zhao, H. Luo, and Z.-H. Zhou. No-regret learning in time-varying zero-sum games. In Proceedings of the 39th International Conference on Machin e Learning (ICML) , pages 26772– 26808, 2022b. P . Zhao, Y .-J. Zhang, L. Zhang, and Z.-H. Zhou. Dynamic regre t of convex and smooth functions. In Advances in Neural Information Processing Systems 33 (Neur IPS), pages 12510–12520, 2020. P . Zhao, Y .-J. Zhang, L. Zhang, and Z.-H. Zhou. Adaptivity an d non-stationarity: Problem- dependent dynamic regret for online convex optimization. Journal of Machine Learning Research , 25(98):1 – 52, 2024. Z.-H. Zhou. Learnability with time-sharing computational resource concerns. National Science Review, 11(10):nwae204, 2024. M. Zinkevich. Online convex programming and generalized in ﬁnitesimal gradient ascent. In Pro- ceedings of the 20th International Conference on Machine Le arning (ICML) , pages 928–936, 2003. 13A Related W ork In this section, we review the related work in gradient-variation online learning, the generalized smoothness conditions, and the Lipschitz-adaptive algori thms. A.1 Gradient-V ariation Online Learning The gradient-variation quantity deﬁned in Eq. ( 2) is ﬁrstly introduced by Chiang et al. [2012] for global smooth functions. Chiang et al. [2012] obtain O(√VT) and O( d α log VT) regret bounds respectively for general convex and α-exp-concave functions. Zhang et al. [2022a] later achieve O( 1 λ log VT) result for λ-strongly convex functions. Considering the non-stationa ry environ- ments, Zhao et al. [2020] study gradient variation under the dynamic regret, a stren gthened mea- sure that requires the learner to compete with a sequence of t ime-varying comparators. Their work revives the study of gradient-variation online learning, e specially revealing the importance of the stability analysis in the two-layer online ensemble. Their results are further improved in Zhao et al. [2024], where they only require one gradient per round with the sam e optimal guarantees through the collaboration between the meta and the base. For univers al online learning [ van Erven and Koolen, 2016], there are several recent researches trying to derive the g radient-variation bounds in this context [ Zhang et al. , 2022a; Sachs et al. , 2023; Y an et al. , 2023]. Gradient variation demonstrates its importance due to its c lose connection with many online learning problems. Pioneering works [ Rakhlin and Sridharan , 2013a; Syrgkanis et al. , 2015; Zhang et al. , 2022b] demonstrate the importance of gradient variation via a use ful property (Regret bounded by V ariation in Utilities, R VU) to achieve fast-rate converge nces in multi-player games. Recently, Sachs et al. [2022] and Chen et al. [2024] reveal that the gradient variation is also essential in bri dging stochastic and adversarial convex optimization. However, all the mentioned works require the global smoothness assumption. As highlighted in Proposition 2 in Appendix B, the smoothness condition is necessary to obtain the gradie nt-variation bounds for algorithms with ﬁrst-order oracle assumption [ Nesterov, 2018]. Exceptions are that Jacobsen and Cutkosky [2022]; Bai et al. [2022] obtain the gradient-variation bounds without the smoothness assumption, but they require implicit updates [Kulis and Bartlett , 2010; Nicolò and Francesco, 2020] per round, which may be inefﬁcient under online settings. O ur work considers generalized smoothness, and we develop ﬁrst-order methods to achieve gradient-variation bounds. A.2 Generalized Smoothness Generalized smoothness has received increasing attentionin recent years since the analysis under the standard global smoothness is insufﬁcient to depict the dynamics of neural network optimization. Based on the empirical observation for the relationship bet ween the smoothness and the gradients of LSTMs, Zhang et al. [2020b] relax the global smoothness assumption by (L0,L1)-smoothness con- dition, which assumes ∥∇2f(x)∥2 ≤ L0 + L1∥∇f(x)∥2 for ofﬂine objective function f : X ↦→R. With this new smoothness assumption, Zhang et al. [2020b] explain the importance of gradient clip- ping in neural network training. There are a variety of subse quent works developed for different methods and settings [ Zhang et al. , 2020a; Crawshaw et al. , 2022; Reisizadeh et al. , 2023; Faw et al. , 2023; W ang et al. , 2023]. There are studies further generalizing the (L0,L1)-smoothness condi- tion. Chen et al. [2023b] introduce the α-symmetric smoothness condition, which symmetrizes the (L0,L1)-smoothness condition and allows the smoothness to depend p olynomially on the gradient. Notably, Li et al. [2023] propose the ℓ-smoothness, deﬁned as ∥∇2f(x)∥2 ≤ ℓ(∥∇f(x)∥2). This condition does not specify a particular form for the functio n ℓ : R → R, other than some mild assumptions about its properties, thereby allowing great g enerality of this notion. T elgarsky [2022] introduces the concept of (G,L)-quadratically bounded functions, which aims to generalize the Lipschitz condition as ∥∇f(x)∥2 ≤ G+ L∥x − x0∥2 with a reference point x0 ∈ X . Though this notion covers the standard global smoothness c ondition, it lacks a detailed depiction of the relationship between the smoothness and th e gradients, hindering further research into understanding the optimization dynamics such as the ro le of gradient clipping [ Zhang et al. , 2020b]. Jacobsen and Cutkosky [2023] investigate online convex optimization under this constr aint such that the online functions may be unbounded. However, it is unclear how to obtain the gradient- variation regret in their context and using their methods. 14W e also mention another generalization of the standard smoo thness called the relative smooth- ness [Lu et al. , 2018]. A function f : X ↦→ R is L-smooth relative function if f(x) ≤ f(y) + ⟨∇f(y),x − y⟩ + LDh(x,y) for any x,y ∈ int X , where h : X ↦→ R is a reference function with Dh(x,y) denoting the Bregman divergence. However, the global const ant Lis still required to tune the algorithm, and studying this notion is b eyond the scope of our paper. A.3 Lipschitz-Adaptive Algorithms The upper bound of gradients Gand the diameter of the bounded feasible domain D are often re- quired to build up online algorithms. An algorithm that requ ires the diameter D of the bounded feasible domain D but not the upper bound of gradients is known to be Lipschitz- adaptive [ Duchi et al. , 2011; Orabona and Pál , 2018; Cutkosky, 2019; Mhammedi et al. , 2019]. For the general OCO setting, to the best of our knowledge, there are no Lipschitz-adaptive algorithm that ensures a gradient-variation bound. When specialized to the Predic tion with Experts’ Advice (PEA) set- ting [ Cesa-Bianchi and Lugosi , 2006], Chen et al. [2021] design a two-layer algorithm with a restart- ing mechanism that can obtain a gradient-variation bound in PEA setting. In this paper, we develop a new Lipschitz-adaptive algorithm with a lightweight desig n, which guarantees the gradient-variation bound as well and is the only option for our purposes, to the be st of our knowledge. B Omitted Details for Section 2 In this section, we ﬁrst provide a judgement of the necessity of smoothness for ﬁrst-order online algorithms in Appendix B.1. Next, we will provide proofs for the theorems in Section 2.3. In Appendix B.2, we present the omitted discussion for the challenge in desi gning the algorithm for exp-concave functions. W e introduce a key lemma in Appendix B.3, which abstracts the key idea of exploiting the generalized smoothness. Later, the proofs f or Theorem 1 and Theorem 2 are provided in Appendix B.4 and Appendix B.5, respectively. B.1 Necessity of Smoothness W e ﬁrst provide a lower bound on the convergence rate for ﬁrst -order methods with convex functions. Proposition 1 (Theorem 3.2.1 of Nesterov [2018]). There exists a G-Lipschitz and convex function f : Rd ↦→R with ∥x1 − x⋆∥ ≤ Dwhere x⋆ ∈ arg minx∈Rd f(x) such that f(xt) − min x∈Rd f(x) ≥ GD 2(2 + √ t), for any optimization scheme that generates a sequence {xt} satisfying that xt ∈ x1 + Lin {∇f(x1),..., ∇f(xt−1)} , where Lin{a1,..., at} denotes the linear span of vectors a1,..., at. Y ang et al. [2014] prove the necessity of the smoothness to obtain the gradien t-variation bounds with convex functions using the ﬁrst-order online algorithms. W e formally present this idea in below . Proposition 2 (Remark 1 of Y ang et al. [2014]). The smoothness assumption for G-Lipschitz and convex online functions ft : X ↦→ R is necessary for any online algorithms, whose decisions are linear combinations of the queried gradients when no projec tions are made, and ensure: T∑ t=1 ft(xt) − min x∈X T∑ t=1 ft(x) ≤ O ( √ VT) + constant, with only c· T times gradient queries, with c ∈ N being a constant independent of T and environ- mental parameters, such as the Lipschitz constant and the sm oothness constant. Proof. W e prove this proposition by contradiction. Assume that ft is non-smooth. Then consider a special case where the algorithm projects the decisions ont o X = Rd, i.e., no projections are made, 15and all the online functions are equal f1 = · · · = fT = f. Notice that, under such case, the gradient variation is zero and therefore, T∑ t=1 f(xt) − min x∈X T∑ t=1 f(x) ≤ O (1) , which implies ¯xT = 1 T ∑ T t=1 xt approaches an O(1/T) convergence rate. Now it remains to check whether this convergence rate contradicts with Propositio n 1. Denoted by x′ 1,..., x′ cTthe points that the algorithm queries gradients on, because the decisions a re linear combinations of the gradients when no projections are made, then, ¯xT ∈ x1 + Lin {∇f(x′ 1),..., ∇f(x′ cT)} , which indicates that, there exists a method which promises O(c/T) = O(1/T) convergence rate under the protocol considered in Proposition 1, contradicting the lower bound. B.2 Challenge for Exp-Concave Functions in Regret Minimiza tion In Section 2, we design algorithms for convex and strongly convex functi ons respectively. How- ever, the gradient-variation regret for exp-concave funct ions [ Hazan et al. , 2007] under generalized smoothness has not yet been addressed. For online learning w ith global smooth functions, it has been demonstrated that an O(dlog VT) regret is attainable for exp-concave functions [ Chiang et al. , 2012], which is realized by an optimistic variant of the online Newt on step algorithm [ Hazan et al. , 2007] using the last-round gradient as optimism, i.e., Mt = ∇ft−1(xt−1). However, it remains unclear how to obtain a general optimistic bound of order O(dlog DT) with DT = ∑ T t=1∥∇ft(xt) − Mt∥2 2 for arbitrary optimism {Mt}T t=1. Our technique for achieving gradient-variation regret un der gener- alized smoothness relies on a ﬂexible setting for optimism ( which may not be the last-round gradient) and step size tuning (which requires a trajectory-wise stab ility analysis), making it challenging for extension to exp-concave functions. W e leave this as an open question for future research. B.3 Lemma for Regret Minimization Below , we present a lemma that leverages the local smoothness of the optimization trajectory to derive gradient variation within the OMD framework. This le mma can be applied in the analysis of both convex and strongly convex functions. Lemma 2.Under Assumptions 1 and 2, by selecting regularizer as ψt(x) = 1 2ηt ∥x∥2 2, setting step sizes satisfying that ηt+1 ≤ ηt and ηt ≤ 1/(4ˆLt−1), where ˆLt−1 = ℓt−1(2∥∇ft−1(ˆxt)∥2), and by choosing optimism as Mt = ∇ft−1(ˆxt), the OMD in Eq. ( 4) ensures the regret bound: T∑ t=1 ⟨∇ft(xt),xt − x⋆⟩ ≤ T∑ t=1 1 2ηt ( ∥x⋆ − ˆxt∥2 2− ∥x⋆ − ˆxt+1∥2 2 ) + 2 T∑ t=1 ηt∥∇ft(xt) − ∇ft−1(xt)∥2 2− T∑ t=1 1 4ηt ∥xt − ˆxt∥2 2 ≤ D2 2ηT + 2 T∑ t=1 ηt∥∇ft(xt) − ∇ft−1(xt)∥2 2− T∑ t=1 1 4ηt ∥xt − ˆxt∥2 2, (11) where x⋆ ∈ arg minx∈X ∑ T t=1 ft(x) denotes the best decision in hindsight. Proof. Following the standard analysis of OMD (Lemma 3), OMD with the optimism chosen as Mt = ∇ft−1(ˆxt) ensures the following regret bound: T∑ t=1 ⟨∇ft(xt),xt − x⋆⟩ ≤ T∑ t=1 1 2ηt ( ∥x⋆ − ˆxt∥2 2− ∥x⋆ − ˆxt+1∥2 2 )    TERM -A + T∑ t=1 ηt∥∇ft(xt) − ∇ft−1(ˆxt)∥2 2    TERM -B 16− T∑ t=1 1 2ηt ( ∥ˆxt+1 − xt∥2 2+ ∥xt − ˆxt∥2 2 )    TERM -C . The analysis for T E RM -A is straightforward under Assumption 2: TE RM -A ≤ 1 2η1 ∥x⋆ − ˆx1∥2 2+ T∑ t=2 ( 1 2ηt − 1 2ηt−1 )∥x⋆ − ˆxt∥2 2≤ D2 2η1 + T∑ t=2 ( D2 2ηt − D2 2ηt−1 ) = D2 2ηT . Next, we analyze the T E RM -B under the generalized smoothness condition. By the basic calculation, we can decompose the T E RM -B into following two terms: TE RM -B ≤ 2 T∑ t=1 ηt∥∇ft(xt) − ∇ft−1(xt)∥2 2+ 2 T∑ t=2 ηt∥∇ft−1(xt) − ∇ft−1(ˆxt)∥2 2, (12) where the ﬁrst term is the desirable gradient variation and t he second term should be further ana- lyzed under the generalized smoothness. Given the optimism setting and the step size tuning, we demonstrate that xt and ˆxt are sufﬁciently close to apply local smoothness: ∥xt − ˆxt∥2 ≤ ηt∥∇ft−1(ˆxt)∥2 ≤ ∥∇ft−1(ˆxt)∥2 4ˆLt−1 . In above, the ﬁrst inequality is by the the Pythagorean theor em and the second inequality is by the step size tuning. Therefore, we can apply Lemma 1 to bound the gradient deviation in Eq. ( 12) by T∑ t=2 ηt∥∇ft−1(xt) − ∇ft−1(ˆxt)∥2 ≤ T∑ t=2 ηtˆL2 t−1∥xt − ˆxt∥2 2. Finally, combining T E RM -A, T E RM -B and T E RM -C together, we obtain: T∑ t=1 ⟨∇ft(xt),xt − x⋆⟩ ≤ D2 2ηT + 2 T∑ t=1 ηt∥∇ft(xt) − ∇ft−1(xt)∥2 2− T∑ t=1 1 4ηt ∥xt − ˆxt∥2 2 + T∑ t=1 (2ηtˆL2 t−1 − 1 4ηt )∥xt − ˆxt∥2 2 ≤ D2 2ηT + 2 T∑ t=1 ηt∥∇ft(xt) − ∇ft−1(xt)∥2 2− T∑ t=1 1 4ηt ∥xt − ˆxt∥2 2, where the second inequality is by setting ηt ≤ 1/(4ˆLt−1). Hence, we ﬁnish the proof. B.4 Proof of Theorem 1 Proof. The step size tuning in Eq. ( 7) in Theorem 1 satisﬁes the criterions for applying Lemma 2, speciﬁcally that ηt+1 ≤ ηt and ηt ≤ 1/(4ˆLt− 1), where ˆLt−1 = ℓt−1(2∥∇ft−1(ˆxt)∥2). Therefore, by Lemma 2 and the convexity of loss functions, we obtain: T∑ t=1 ft(xt) − T∑ t=1 ft(x⋆) ≤ T∑ t=1 ⟨∇ft(xt),xt − x⋆⟩ ≤ D2 2ηT + 2 T∑ t=1 ηt∥∇ft(xt) − ∇ft−1(xt)∥2 2, where x⋆ ∈ arg minx∈X ∑ T t=1 ft(x). The ﬁrst term can be bounded as follows, D2 2ηT ≤ 2ˆLmaxD2 + D 2    √ 1 + T∑ t=2 ∥∇ft(xt) − ∇ft−1(xt)∥2 2+ D 2 · ∥∇f1(x1)∥2 = O ( ˆLmaxD2 + D √ VT ) , 17where we denote by ˆLmax = max t∈[T] ˆLt. For the second term, we apply the self-conﬁdent tuning lemma (Lemma 5) by choosing f(x) = 1 /√xin the lemma statement: T∑ t=1 ηt∥∇ft(xt) − ∇ft−1(xt)∥2 2≤ T∑ t=1 D∥∇ft(xt) − ∇ft−1(xt)∥2 2 √ 1 + ∑ t−1 s=1∥∇fs(xs) − ∇fs−1(xs)∥2 2 ≤ 2D    √ 1 + T∑ s=1 ∥∇fs(xs) − ∇fs−1(xs)∥2 2+ Dmax t∈[T] ∥∇ft(xt) − ∇ft−1(xt)∥2 2 = O ( D √ VT + ˆG2 maxD ) , where we use ˆGmax to denote the empirically maximum Lipschitz constant. The a dditional term O( ˆG2 maxD) results from the lack of knowledge about ˆGmax. However, we can improve this term to O( ˆGmaxD) by incorporating the clipping technique [ Cutkosky, 2019; Chen et al. , 2021] into OMD framework. In the main text, we avoid introducing this metho d to prevent complicating the approach further. Our goal in the OMD introduction is to illustrate ho w to adapt to generalized smoothness at the base level. The details of this reﬁned approach are provi ded in Appendix B.6. B.5 Proof of Theorem 2 Proof. For λ-strongly convex functions, we tune the step size as ηt = 2 /(λt+ 16 max s∈[t−1] ˆLs), where we denote by ˆLt = ℓt(2∥∇ft(ˆxt+1)∥) the locally estimated smoothness constant. By the property of strongly convex functions and Eq. ( 11) in Lemma 2, we have: T∑ t=1 ft(xt) − T∑ t=1 ft(x⋆) ≤ T∑ t=1 ⟨∇ft(xt),xt − x⋆⟩ − λ 2 ∥x⋆ − xt∥2 2 ≤ T∑ t=1 1 2ηt ( ∥x⋆ − ˆxt∥2 2− ∥x⋆ − ˆxt+1∥2 2 ) − λ 2 ∥x⋆ − xt∥2 2    TERM -A + 2 T∑ t=1 ηt∥∇ft(xt) − ∇ft−1(xt)∥2 2    TERM -B − T∑ t=1 1 4ηt ∥xt − ˆxt∥2 2    TERM -C . Unlike in the case of convex functions, T E RM -A involves negative terms derived from the strong convexity of the loss functions, which require a slightly di fferent analysis: TE RM -A ≤ 1 2η1 ∥x⋆ − ˆx1∥2 2+ T∑ t=2 ( 1 2ηt − 1 2ηt−1 )∥x⋆ − ˆxt∥2 2− λ 2 ∥x⋆ − xt∥2 2 ≤ λD2 4 + T∑ t=2 (λ 4 + 4 max s∈[t] ˆLs − 4 max s∈[t−1] ˆLs)∥x⋆ − ˆxt∥2 2− λ 2 ∥x⋆ − xt∥2 2 ≤ λD2 2 + 4D2 · max s∈[T] ˆLs+ T∑ t=2 λ 4 ∥x⋆ − ˆxt∥2 2− λ 2 ∥x⋆ − xt∥2 2 (bounded domain assumption) ≤ λD2 4 + 4D2 ˆLmax + T∑ t=2 λ 2 ∥xt − ˆxt∥2 2 (∥x⋆ − ˆxt∥2 2≤ 2∥x⋆ − xt∥2 2+ 2∥xt − ˆxt∥2 2) ≤ λD2 4 + 4D2 ˆLmax + T∑ t=2 λ· η2 t 2 ∥∇ft(xt) − ∇ft−1(ˆxt)∥2 2 (Lemma 4) ≤ λD2 4 + 4D2 ˆLmax + T∑ t=1 ηt∥∇ft(xt) − ∇ft−1(ˆxt)∥2 2 18≤ λD2 4 + 4D2 ˆLmax + T∑ t=1 2ηt∥∇ft(xt) − ∇ft−1(xt)∥2 2+ T∑ t=1 2ηt∥∇ft−1(xt) − ∇ft−1(ˆxt)∥2 2, where the last term can be cancelled by T E RM -C, and the third term is in the same order of T E RM -B. Therefore, we only need to focus on T E RM -B. For T E RM -B, we follow the analysis of Chen et al. [2024], who applies a simpler analysis for the self-conﬁdent tuni ng. First, we deﬁne: α= ⌈ T∑ t=1 ∥∇ft(xt) − ∇ft−1(xt)∥2 2 ⌉ . Then by dividing the time horizon into [1,α] and [α+ 1,T], we can upper bound T E RM -B as: TE RM -B ≤ 2 α∑ t=1 ηt∥∇ft(xt) − ∇ft−1(xt)∥2 2+ 2 T∑ t=α+1 ηt∥∇ft(xt) − ∇ft−1(xt)∥2 2 ≤ 4 α∑ t=1 1 λt∥∇ft(xt) − ∇ft−1(xt)∥2 2+ 4 T∑ t=α+1 1 λt∥∇ft(xt) − ∇ft−1(xt)∥2 2 ≤ 4(max s∈[T] ∥∇fs(xs) − ∇fs−1(xs)∥2 2) α∑ t=1 1 λt + 4 T∑ t=α+1 1 λt∥∇ft(xt) − ∇ft−1(xt)∥2 2 (13) ≤ 16 ˆG2 max α∑ t=1 1 λt + 4 λ(α+ 1) T∑ t=α+1 ∥∇ft(xt) − ∇ft−1(xt)∥2 2 ≤ 16 ˆG2 max λ (1 + ln α) + 4 λ ≤ 16 ˆG2 max λ ln ( T∑ t=1 ∥∇ft(xt) − ∇ft−1(xt)∥2 2+ 1 ) + 16 ˆG2 max+ 4 λ , where we use ˆGmax to denote the maximum Lipschitz constant estimated empiric ally. Therefore: T∑ t=1 ft(xt) − T∑ t=1 ft(x⋆) ≤ O (ˆG2 max λ log VT + ˆLmaxD2 + λD2 ) , which ﬁnishes the proof. B.6 OMD Incorporating Clipping T echnique In Appendix B.4, an additional term O( ˆG2 maxD) shows up in the ﬁnal regret bound, which results from the lack of knowledge about ˆGmax. In this subsection, we improve the term O( ˆG2 maxD) to O( ˆGmaxD) by incorporating the clipping technique [ Cutkosky, 2019; Chen et al. , 2021] into the OMD framework. This modiﬁed OMD algorithm is deﬁned as follo ws, xt = Π X [ˆxt − ηt∇ft−1(ˆxt)] , ˆxt+1 = Π X [ˆxt − ηt˜gt] , (14) where ˜gt = ∇ft−1(ˆxt) + Bt− 1 Bt (∇ft(xt) − ∇ ft−1(ˆxt)) is a clipped gradient with the maintained threshold Bt = max {B0,maxs∈[t]∥∇fs(xs) − ∇ fs−1(ˆxs)∥2}. Notice that, xt still updates from ˆxt in the same manner as illustrated in Theorem 1, therefore, we can apply the similar analysis to it when exploiting the smoothness locally. Correspondingl y, we provided the following step size tuning: ηt = min {√ D2 B2 t−1 + ∑ t−1 s=1∥˜gs − ∇fs−1(ˆxs)∥2 2 , min s∈[t] 1 4ˆLs−1 } , (15) where the key modiﬁcation is that we add B2 t−1 in the denominator to facilitate the tuning analysis and we denote by ˆLt = ℓt(2∥∇ft(ˆxt+1)∥). The following theorem presents the guarantee. 19Theorem 5. Under Assumptions 1 - 2, assuming online functions are convex, OMD presented in Eq. (14) with the step sizes in Eq. (15) ensures that: RE GT ≤ 5D 2 √ 2VT + 4 ˆLmaxD2 + 5 ˆGmaxD, where VT = ∑ T t=2 supx∈X ∥∇ft(x)−∇ft−1(x)∥2 2is the gradient variations, ˆLmax = max t∈[T] ˆLt is the maximum smoothness constant over the optimization tr ajectory, and ˆGmax is the maximum empirically estimated Lipschitz constant. Proof.First, we prove that the clipping technique incurs a constan t in the regret bound: T∑ t=1 ⟨∇ft(xt),xt − x⋆⟩ − ⟨˜gt,xt − x⋆⟩ = T∑ t=1 Bt − Bt−1 Bt ⟨∇ft(xt) − ∇ft−1(ˆxt),xt − x⋆⟩ ≤ D T∑ t=1 (Bt − Bt−1) = D(BT − B0) = O( ˆGmaxD). In the following analysis, we will focus on the regret associ ated with the clipped gradient ˜gt. Fol- lowing the standard analysis of OMD (Lemma 3), and with simple calculations, we arrive at: T∑ t=1 ⟨˜gt,xt − x⋆⟩ ≤ D2 2ηT TERM -A + T∑ t=1 ηt∥˜gt − ∇ft−1(ˆxt)∥2 2    TERM -B − T∑ t=1 1 2ηt ∥xt − ˆxt∥2 2    TERM -C . (16) By the step size tuning, T E RM -A can be bounded as: TE RM -A ≤ 2ˆLmaxD2 + D 2    √ B2 T + T∑ s=1 ∥˜gs− ∇fs−1(ˆxs)∥2 2 ≤ 2ˆLmaxD2 + ˆGmaxD+ D 2    √ T∑ s=2 ∥˜gs− ∇fs−1(ˆxs)∥2 2 = 2 ˆLmaxD2 + ˆGmaxD+ D 2    √ T∑ s=2 B2 s−1 B2s ∥∇fs(xs) − ∇fs−1(ˆxs)∥2 2 ≤ 2ˆLmaxD2 + ˆGmaxD+ D 2    √ T∑ s=2 ∥∇fs(xs) − ∇fs−1(ˆxs)∥2 2    TERM -A- VA R . (17) For T E RM -B in ( 16), by the self-conﬁdent tuning lemma (Lemma 6) we obtain, TE RM -B = D T∑ t=1 ∥˜gt − ∇ft−1(ˆxt)∥2 2 √ B2 t−1 + ∑ t−1 s=1∥˜gs− ∇fs−1(ˆxs)∥2 2 ≤ D T∑ t=1 ∥˜gt − ∇ft−1(ˆxt)∥2 2 √ ∑ t s=1∥˜gs− ∇fs−1(ˆxs)∥2 2 ≤ 2D    √ T∑ t=1 ∥˜gs− ∇ft−1(ˆxt)∥2 2≤ 2D    √ T∑ t=2 ∥∇ft(xt) − ∇ft−1(ˆxt)∥2 2    TERM -B-VAR +2 ˆGmaxD. (18) Combine T E RM -A-V A R in ( 17), T E RM -B-V A R in ( 18), and negative term T E RM -C in ( 16): TE RM -A-V A R + TE RM -B-V A R − TE RM -C = 5D 2    √ 2 T∑ t=2 ∥∇ft(xt) − ∇ft−1(xt)∥2 2+ 2 T∑ t=2 ∥∇ft−1(xt) − ∇ft−1(ˆxt)∥2 2− T∑ t=1 1 2ηt ∥xt − ˆxt∥2 2 20≤ 5D 2    √ 2 T∑ t=2 ∥∇ft(xt) − ∇ft−1(xt)∥2 2+ 5D 2    √ 2 T∑ t=2 ∥∇ft−1(xt) − ∇ft−1(ˆxt)∥2 2 − T∑ t=1 1 2ηt ∥xt − ˆxt∥2 2 ≤ 5D 2 √ 2VT + 5D 2    √ 2 T∑ t=2 ˆL2 t−1∥xt − ˆxt∥2 2− T∑ t=2 2 ( max s∈[t−1] ˆLs ) ∥xt − ˆxt∥2 2 ≤ 5D 2 √ 2VT + 25 16 ˆLmaxD2, where in the forth line we apply Lemma 1, and ub the ﬁnal line, we apply the AM-GM inequality, 2 √ ab− a≤ b. Combing each component together, we conclude the proof as: T∑ t=1 ft(x) − T∑ t=1 ft(x⋆) ≤ T∑ t=1 ⟨∇ft(xt),xt − x⋆⟩ ≤ T∑ t=1 ⟨˜gt,xt − x⋆⟩+ 2 ˆGmaxD ≤ 5D 2 √ 2VT + 4 ˆLmaxD2 + 5 ˆGmaxD. C Omitted Details for Section 3 In this section, we present the omitted details for Section 3. Appendix C.1 discusses how set the optimism mt efﬁciently via a binary search. Appendix C.2 provides the omitted discussion for the challenge in universal online learning with exp-concav e functions. Proofs for the theorems in Section 3 are included in Appendix C.3 and Appendix C.4. Appendix C.5 gives a simple universal algorithm under the global smoothness condition, which imp roves the optimality and efﬁciency of the method by Y an et al. [2023], at a cost of additional function value queries. C.1 Discussion on Optimism In this part, we discuss how to set the optimismmt that involves the decision pt. W e present the steps for incorporating the optimism and generating the dec ision pt in Algorithm 1 for reference: ˜wt,i = wt,iexp(ηt,imt,i), p t,i = ηt,i˜wt,i∑ j∈[N] ηt,j ˜wt,j . (19) For more general consideration, we set optimism the as mt,i = f(∑ i∈[N] pt,ixt,i) − h(xt,i) where f : X ↦→ R is a convex and continuous function and xi ∈ X is a decision available before setting the optimism. Notice that ˜wi requires pt to update while pt is produced based on ˜wt,i, resulting in a circular argument. Following W ei et al. [2016], the pt can be solved via the binary search technique. W e deﬁne α = f(∑ i∈[N] pt,ixi), then the weight ˜wt,i is a function of αas ˜wt,i(α) = wt,iexp(ηt,i(α− f(xt,i))). Furthermore, with the update formulation in ( 19), the decision pt,i is a function of αas well, with the formulation pt,i(α) = ηt,i ˜wt,i(α)∑ j∈ [N ] ηt,j ˜wt,j (α) . By introducing a function g(α) = f(∑ i∈[N] pt,i(α)xi), solving the decision pt is equal to solving g(α) = α. Below , we prove the existence of a solution to g(α) = α. Provided the lower bound ¯ f of function f(·), and by the convexity of the function f(·), the searching range of αis restricted to ¯ f ≤ α ≤ maxi∈[N]{f(xi)}, and thus αis bounded. The continuity of the function f(·) implies the continuity of the function g(α) as well. The choice of α = ¯ f results in g(α) − α = f(∑ i∈[N] pt,i(α)xi) − ¯ f ≥ 0, and the choice of α = max i∈[N]{f(xi)} implies g(α) − α ≤ ∑ i∈[N] pt,i(α)f(xi) − maxi∈[N]{f(xi)} ≤ 0, indicating that a solution to g(α) = α exists. By using a binary search within [ ¯ f,maxi∈[N]{f(xi)}], we can approach αwithin an error O(1/T) in O(log T) iterations. 21The above argument requires the lower bound of the function f(·) to determine the searching range over α. When considering a simpler case where f(xi) = ℓi and f(∑ i∈[N] pt,ixi) =∑ i∈[N] pt,iℓi, we can omit the requirement for the lower bound because α falls within [mini∈[N]{ℓi},maxi∈[N]{ℓi}], because of the simpler structure of linear functions. C.2 Challenge for Exp-Concave Functions in Universal Onlin e Learning In Section 3.4, we present Algorithm 2 that achieves gradient-variation bounds for convex and strongly convex functions simultaneously. However, it doe s not guarantee such bounds for exp-concave functions. Besides the challenges discussed i n Section B.2, a new obstacle arises in designing the meta-algorithm. W e require optimism to sat isfy ⟨pt,mt⟩ ≤ 0, since we pass the meta-algorithm with heterogeneous inputs, and the refore we set mt,i = ft−1(xt) − ft−1(xt,i) for all the base-learners. This optimism design is suitable for strongly convex func- tions, as the term √ ∑ t(ft−1(xt) − ft−1(xt,i))2 introduced by optimism can be bounded by ˆGmax √ ∑ t∥xt − xt,i∥2 2, cancelled by the negative term − ∑ tλ∥xt− xt,i∥2 2from strong convexity. However, for exp-concave functions, the negative term −⟨∇ft(xt),xt − xt,i⟩2 from exp-concavity may not be sufﬁcient to cancel √ ∑ t(ft−1(xt) − ft−1(xt,i))2. W e leave this as an open problem for future exploration. C.3 Proof of Theorem 3 In this subsection, we prove a slightly generalized version of Theorem 3, which does not specify optimism and instead imposes conditions only on ¯rt. One can verify that the setting of optimism in Theorem 3 satisﬁes the requirement of Theorem 6. Theorem 6. By setting ¯rt,i such that ∑ N i=1 pt,i¯rt,i ≤ 0, Algorithm 1 ensures that, for any i⋆ ∈ [N], the regret ∑ T t=1⟨pt,ℓt⟩ − ∑ T t=1 ℓt,i⋆ can be bounded by: O (   √ T∑ t=1 (rt,i⋆ − mt,i⋆ )2 · ( log(N) + log( BT + log T) ) + BT ) , where BT = max {B0,maxt∈[T]∥rt − mt∥∞}. Proof. First, we demonstrate that the clipping technique [ Chen et al. , 2021; Cutkosky, 2019] incurs a constant in the regret bound: T∑ t=1 rt,i⋆ − ¯rt,i⋆ = T∑ t=1 rt,i⋆ − mt,i⋆ − Bt−1 Bt (rt,i⋆ − mt,i⋆ ) = T∑ t=1 Bt − Bt−1 Bt (rt,i⋆ − mt,i⋆ ) ≤ T∑ t=1 Bt − Bt−1 Bt |rt,i⋆ − mt,i⋆ | ≤ T∑ t=1 Bt − Bt−1 Bt ∥rt − mt∥∞ ≤ BT − B0. (20) In below , we focus on the analysis associated with clipped re gret ¯rt,i⋆ . Following previous work [ W ei et al. , 2016], we deﬁne Wt = ∑ N i=1 wt,i to represent the summation of weights at time t. The quantity Wt can be realized as the potential to be analyzed. Next, we cons ider to upper bound ln WT+1. By the inequality x≤ xα + (α− 1)/efor x> 0,α ≥ 0, for any i∈ [N], we have: wT+1,i ≤ (wT+1,i) ηT,i ηT +1,i + 1 e ( ηT,i ηT+1,i − 1 ) . (21) Based on the updates in Line 7 of Algorithm 1, we bound the ﬁrst term on the right-hand side as: (wT+1,i) ηT,i ηT +1,i = wT,i exp ( ηT,i¯rT,i − η2 T,i (¯rT,i − mT,i)2 ) = ˜wT,i exp ( ηT,i (¯rT,i − mT,i) − η2 T,i (¯rT,i − mT,i)2 ) 22≤ ˜wT,i (1 + ηT,i (¯rT,i − mT,i)) . (22) The last inequality is by exp(x− x2) ≤ 1 + xfor x≥ − 1/2. This is a crucial condition needed to be veriﬁed for Lipschitz-adaptive meta-algorithm. By the t uning of learning rates, and the clipping technique, we control the range of ηT,i(¯rT,i − mT,i) well: ηT,i|¯rT,i − mT,i| = ηT,i BT−1 BT |rT,i − mT,i| ≤ ηT,iBT−1 ≤ BT−1 2BT−1 = 1 2, which meets the criterion for applying the mentioned inequa lity. By plugging inequality ( 22) into ( 21), we can further analyze the weights for all experts at time T: N∑ i=1 wT+1,i ≤ N∑ i=1 ˜wT,i (1 + ηT,i (¯rT,i − mT,i)) + N∑ i=1 1 e ( ηT,i ηT+1,i − 1 ) = N∑ i=1 ˜wT,i (1 − ηT,imT,i) + N∑ i=1 ηT,i ˜wT,i¯rT,i + N∑ i=1 1 e ( ηT,i ηT+1,i − 1 ) ≤ N∑ i=1 ˜wT,i exp(−ηT,imT,i) + N∑ i=1 ηT,i ˜wT,i¯rT,i + N∑ i=1 1 e ( ηT,i ηT+1,i − 1 ) = N∑ i=1 wT,i + (N∑ j=1 ηT,j ˜wT,j )N∑ i=1 pT,i¯rT,i + N∑ i=1 1 e ( ηT,i ηT+1,i − 1 ) ≤ N∑ i=1 wT,i + N∑ i=1 1 e ( ηT,i ηT+1,i − 1 ) , where we apply 1 − x≤ exp(−x) for any x∈ R, and the last inequality is by the assumption in the theorem statement that ∑ N i=1 pt,i¯rt,i ≤ 0 for any t∈ [T]. Now we are ready to upper bound WT+1 in an inductive style: WT+1 = N∑ i=1 wT+1,i ≤ N∑ i=1 wT,i + N∑ i=1 1 e ( ηT,i ηT+1,i − 1 ) = WT + N∑ i=1 1 e ( ηT,i ηT+1,i − 1 ) ≤ W1 + T∑ t=1 N∑ i=1 1 e ( ηt,i ηt+1,i − 1 ) = N + T∑ t=1 N∑ i=1 1 e ( ηt,i ηt+1,i − 1 ) , (23) where the last inequality is by the induction. It remains to a nalyze the last term, the deviations of the learning rates. W e present the following analysis tailored for the new learning rate setting, ∀i∈ [N]: ηT,i ηT+1,i − 1 =    √ 1 + ∑ T t=1(¯rt,i − mt,i)2 + 4B2 T 1 + ∑ T−1 t=1 (¯rt,i − mt,i)2 + 4B2 T−1 − 1 = √ 1 + 4B2 T − 4B2 T−1 + (¯rT,i − mT,i)2 1 + ∑ T−1 t=1 (¯rt,i − mt,i)2 + 4B2 T−1 − 1 ≤ 1 2 · 4B2 T − 4B2 T−1 + (¯rT,i − mT,i)2 1 + ∑ T−1 t=1 (¯rt,i − mt,i)2 + 4B2 T−1 (√1 + x≤ 1 + 1 2 x) = 1 2 · φT,i 1 + 4 B2 0 + ∑ T−1 t=1 φt,i . (φt,i ≜ 4B2 t − 4B2 t−1 + (¯rt,i − mt,i)2 ≥ 0) By Lemma 5 with the choices of f(x) = 1 /x,a0 = 1 + 4 B2 0 ,at = φt,i in the lemma statement, summing up the preceding inequality from 1 to T results in: T∑ t=1 ( ηt,i ηt+1,i − 1 ) ≤ 2B2 T 1 + 4 B2 0 + 1 2 ln ( 1 + 4 B2 T + T∑ t=1 (¯rt,i − mt,i)2 ) − 1 2 ln(1 + 4 B2 0 ) 23≤ 2B2 T 1 + 4 B2 0 + 1 2 ln (1 + ( T + 4)B2 T 1 + 4 B2 0 ) . (24) Now combining ( 23) and ( 24), we can upper bound ln WT+1 as follows: ln WT+1 ≤ ln ( 1 + B2 T 1 + 4 B2 0 + 1 2eln (1 + TB2 T 1 + 4 B2 0 )) + ln N. (25) In another direction, we lower bound ln WT+1 ≥ ln wT+1,i⋆ with an inductive argument: 1 ηT+1,i⋆ ln wT+1,i⋆ = 1 ηT,i⋆ ( ln wT,i⋆ + ηT,i⋆ ¯rT,i⋆ − η2 T,i⋆ (¯rT,i⋆ − mT,i⋆ )2) = 1 ηT,k ln wT,i⋆ − ηT,i⋆ (¯rT,i⋆ − mT,i⋆ )2 + ¯rT,i⋆ = 1 η1,i⋆ ln w1,i⋆ − T∑ t=1 ηt,i⋆ (¯rt,i⋆ − mt,i⋆ )2 + T∑ t=1 ¯rt,i⋆ = − T∑ t=1 ηt,i⋆ (¯rt,i⋆ − mt,i⋆ )2 + T∑ t=1 ¯rt,i⋆ . (w1,i⋆ = 1) Rearranging the above equality with notice of Eq. ( 20), we have: T∑ t=1 rt,i⋆ ≤ T∑ t=1 ¯rt,i⋆ + BT ≤ T∑ t=1 ηt,i⋆ (¯rt,i⋆ − mt,i⋆ )2 + 1 ηT+1,i⋆ ( ln ( 1 + B2 T 1 + 4 B2 0 + 1 2eln (1 + TB2 T 1 + 4 B2 0 )) + ln N ) + BT, where the second term is in the order of O (   √ T∑ t=1 (rt,i⋆ − mt,i⋆ )2 · (log(BT + log(TBT)) + log N) ) . As for the ﬁrst term, by applying Lemma 6, it can be bounded as follows: T∑ t=1 ηt,i⋆ (¯rt,i⋆ − mt,i⋆ )2 = T∑ t=1 (¯rt,i⋆ − mt,i⋆ )2 √ 1 + 4 B2 t + ∑ t−1 s=1(¯rs,i⋆ − ms,i⋆ )2 ≤ T∑ t=1 (¯rt,i⋆ − mt,i⋆ )2 √ 1 + ∑ t s=1(¯rs,i⋆ − ms,i⋆ )2 ≤ 2    √ 1 + T∑ t=1 (¯rt,i⋆ − mt,i⋆ )2 = 2    √ 1 + T∑ t=1 B2 t−1 B2 t (rt,i⋆ − mt,i⋆ )2 ≤ 2    √ 1 + T∑ t=1 (rt,i⋆ − mt,i⋆ )2. Thus, the proof is complete. C.4 Proof of Theorem 4 Proof. W e ﬁrst decompose the static regret based on the performance of base-learner i⋆ into two parts as presented in Eq. ( 8). The base-regret is guaranteed by the corresponding base-le arner via Theorem 1 and Theorem 2. And we mainly focus on the analysis of the meta-regret by leverag ing Theorem 6. First we are required to verify the condition that ∑ i∈[N] pt,i¯rt,i ≤ 0. Without loss of generality, we assume the 1-st base- learner is for convex functions. Recall that we set rt,i = ⟨∇ft(xt),xt − xt,i⟩ for strongly convex 24functions learners i∈ [2,N], rt,1 = ft(xt) − ft(xt,1) for the convex function learner, and optimism mt,i = ft−1(xt) − ft−1(xt,i) for each base-learner i∈ [N], therefore we have: ∑ i∈[N] pt,i¯rt,i = ( 1 − Bt−1 Bt )( ft−1(xt−1) − ∑ i∈[N] pt,ift−1(xt,i) ) + ( pt,1(ft(xt) − ft(xt,1)) + ∑ i∈[2,N] pt,i⟨∇ft(xt),xt − xt,i⟩ ) ≤ ( 1 − Bt−1 Bt )(∑ i∈[N] pt,ift−1(xt,i) − ∑ i∈[N] pt,ift−1(xt,i) ) + ( pt,1⟨ft(xt),xt − xt,1⟩+ ∑ i∈[2,N] pt,i⟨∇ft(xt),xt − xt,i⟩ ) = 0 . Therefore, Theorem 6 is applicable in analyzing the meta-regret for universal on line learning. In follows, we prove the regret bounds for convex functions and strongly convex functions respectively. Convex functions. By Theorem 1, the base-regret is bounded by O(√VT), as for the meta-regret, by the setting of inputs and optimism, by Theorem 6, it is bounded by ME TA-R E G ≤ O (   √ T∑ t=1 (( ft(xt) − ft(xt,1) ) − ( ft−1(xt) − ft−1(xt,1) ))2 · CT + BT ) = O (   √ T∑ t=1 (( ft(xt) − ft−1(xt) ) − ( ft(xt,1) − ft−1(xt,1) ))2 · CT + BT ) = O (   √ T∑ t=1 ( ⟨∇ft(ξt,1) − ∇ft−1(ξt,1),xt − xt,1⟩ )2 · CT + BT ) ≤ O ( D    √ T∑ t=1 sup x∈X ∥∇ft(x) − ∇ft−1(x)∥2 2· CT + BT ) = O (√ VT · CT + BT ) , where we denote by CT = O(log(BT + log(BTT))) and in the third line we apply the mean value theorem. Combining the base-regret and meta-regret togeth er, we concludes that the static regret bound for convex functions is bounded by O(√VT·log(BT+log(BTT))), where BT = O( ˆGmaxD) with ˆGmax denoting the maximum Lipschitz constant. Strongly convex functions. For λ⋆-strong convex functions with λ⋆ ∈ [1/T,1], by the construc- tion of the curvature coefﬁcient pool H, there exists i⋆ ∈ [2,N] such that λi⋆ ≤ λ⋆ ≤ 2λi⋆ . With this speciﬁc i⋆-th base-learner, the base-regret can be upper bounded by O((log VT)/λ⋆) by Theorem 2, up to a multiplicative constant of 2. The meta-regret can be bounded as follows: ME TA-R E G ≤ T∑ t=1 ⟨∇ft(xt),xt − xt,i⋆ ⟩ − λ⋆ 2 ∥xt − xt,i⋆ ∥2 2 ≤ O (   √ T∑ t=1 (⟨∇ft(xt),xt − xt,i⋆ ⟩ − (ft−1(xt) − ft−1(xt,i⋆ )))2 · CT − λ⋆ 2 ∥xt − xt,i⋆ ∥2 2+ BT ) ≤ O (   √ T∑ t=1 ⟨∇ft(xt),xt − xt,i⋆ ⟩2 + ⟨∇ft−1(ξt,i⋆ ),xt − xt,i⋆ ⟩2 · CT − λ⋆ 2 ∥xt − xt,i⋆ ∥2 2+ BT ) ≤ O ( ˆGmax    √ T∑ t=1 ∥xt − xt,i⋆ ∥2 2· CT − λ⋆ 2 ∥xt − xt,i⋆ ∥2 2+ BT ) 25Algorithm 3 Universal Gradient-V ariation Online Learning under Globa l Smoothness Input: curvature coefﬁcient pools Hsc ,Hexp, number of base-learners N, optimistic Adapt-ML- Prod [ W ei et al. , 2016] as the meta-algorithm, base-algorithms Acvx ,Asc ,Aexp. 1: Initialization: Pass N to the meta-algorithm, initialize a base-learner with Acvx ; for λ ∈ H sc , initialize a base-learner with Asc ; for α∈ H exp, initialize a base-learner with Aexp. 2: for t= 1 to T do 3: Obtain pt from meta-algorithm, xt,i from each base-learner i∈ [N]; 4: Submit xt = ∑ i∈[N] pt,ixt,i; 5: Receive ∇ft(xt) and send it to each base-learner for update; 6: For strongly convex and exp-concave functions learners : set rt,i = ⟨∇ft(xt),xt − xt,i⟩; 7: For convex functions learner : set rt,1 = ft(xt) − ft(xt,1); 8: Send rt to the meta-algorithm; 9: Send mt+1,1 = ft(xt) − ft(xt,1) and mt+1,i = 0 ,i ∈ [2,N] to the meta-algorithm. 10: end for ≤ O (ˆG2 maxC2 T λ⋆ + BT ) = O (ˆG2 max(log(BT + log(BTT)))2 λ⋆ + BT ) , where we use ˆGmax to denote the maximum Lipschitz constant on the optimizatio n trajectory, and BT = O( ˆGmaxD). In the fourth line, we again utilize the mean value theorem. The ﬁfth line follows from Lemma 7, which ensures that: |⟨∇ft−1(ξt,i⋆ ),xt − xt,i⋆ ⟩| ≤ max {|⟨∇ft−1(xt,i⋆ ),xt − xt,i⋆ ⟩|,|⟨∇ft−1(xt),xt − xt,i⋆ ⟩|}, thus, our result depends on the Lipschitz constant on the opt imization trajectory. In the last step, we apply the AM-GM inequality. The above statements show tha t the meta-regret is bounded by constants. With the base-regret guarantee, the proof for st rongly convex functions is complete. C.5 A Simple Universal Algorithm under Global Smoothness As a byproduct, our techniques can be used to design a simpler two-layer universal algorithm that achieves the optimal gradient-variation regret bounds for convex, strongly convex, and exp-concave functions simultaneously, thereby improving upon the resu lts of Y an et al. [2023]. The crux involves leveraging the function-variation-to-gradient-variati on technique for convex functions, and follow- ing the strategy of Zhang et al. [2022a] to demonstrate that the meta-regret is bounded by constant s for strongly convex and exp-concave functions, at a cost of O(log T) times function value queries per round. Given the global smoothness constant and the Lips chitz constant, our algorithm does not need to be Lipschitz-adaptive, thus suitable for more gener al optimism settings. In Algorithm 3, we present this idea. In contrast to Algorithm 2, which is designed under the generalized smoothness, this algorithm in addition can gua rantee gradient-variation bound for exp- concave functions. In below , we present the theoretical gua rantees for Algorithm 3. Corollary 3. Under Assumption 2, and assuming the loss functions are L-smooth and G-Lipschitz, we set N = 2 ⌈log2 T⌉ + 1. The curvature coefﬁcient pools are deﬁned as Hsc = Hexp = {2i−1/T : i ∈ [(N − 1)/2]}. By selecting suitable base-algorithms, for λ,α ∈ [1/T,1], Algorithm 3 ensures the following results simultaneously: RE GT ≤        O( √ VT · (log log T)), (convex), O ( 1 λ log VT + G2 log log T λ ) , (λ-strongly convex ), O ( d α log VT + log log T α ) , (α-exp-concave). Proof. When assuming the Lipschitz constant G, optimistic Adapt-ML-Prod [ W ei et al. , 2016] can ensure the meta-regret bounded by O( √ ∑ t t=1(rt,i − mt,i)2 · log log T), thus the dependence of logarithmic terms is improved compared to Theorem 4. The proofs for convex functions and strongly convex functions are nearly identical to the proofs for Theo rem 4 in Appendix C.4; thus, we omit them here. W e highlight the importance of the function-vari ation-to-gradient-variation technique, bounding the meta-regret of order O(√VT · log log T) without the cancellation-based analysis. 26Next, we show that the meta-regret for α⋆-exp-concave functions is bounded by a constant. By the construction of the curvature pool Hexp , there exists base-learner i⋆ with the input curvature αi⋆ satisfying that αi⋆ ≤ α⋆ ≤ 2αi⋆ . Decompose the regret against this speciﬁc base-learner an d by the deﬁnition of exp-concave functions, we have: ME TA-R E G = T∑ t=1 ft(xt) − ft(xt,i⋆ ) ≤ T∑ t=1 ⟨∇ft(xt),xt − xt,i⋆ ⟩ − α 2 ⟨∇ft(xt),xt − xt,i⋆ ⟩2 ≤ O (   √ T∑ t=1 (rt,i⋆ − mt,i⋆ )2 · log log T − α 2 ⟨∇ft(xt),xt − xt,i⋆ ⟩2 ) = O (   √ T∑ t=1 (⟨∇ft(xt),xt − xt,i⋆ ⟩)2 · log log T − α 2 ⟨∇ft(xt),xt − xt,i⋆ ⟩2 ) ≤ O (log log T α ) , where the second-to-last line follows from the settings tha t rt,i⋆ = ⟨∇ft(xt),xt−xt,i⋆ ⟩and mt,i⋆ = 0, and we apply the AM-GM inequality in the ﬁnal step. Therefor e, by choosing the base-algorithm that ensures the regret bound of O( d α⋆ log VT), we complete the proof for this theorem. D Omitted Details for Section 4 This section provides the omitted proofs for our two applica tions. D.1 Proof of Corollary 1 Proof. W e prove this theorem in a black-box manner, thanks to the gra dient-variation bound we derive. By Theorem 4, for convex functions, Algorithm 2 ensures: RE GT ≤ O (   √ T∑ t=1 sup x∈X ∥∇ft(x) − ∇Ft(x)∥2 2+ T∑ t=2 sup x∈X ∥∇Ft(x) − ∇Ft−1(x)∥2 2 ) , where we decompose ∥∇ft(x) − ∇ft−1(x)∥2 2as follows: O ( ∥∇ft(x) − ∇Ft(x)∥2 2+ ∥∇Ft(x) − ∇Ft−1(x)∥2 2+ ∥∇Ft−1(x) − ∇ft−1(x)∥2 2 ) . (26) Finally, by taking expectation on both sides and leveraging the concavity of the square root, we have: E[RE GT] ≤ O (   √ T∑ t=1 E [ sup x∈X ∥∇ft(x) − ∇Ft(x)∥2 2 ] + T∑ t=2 E [ sup x∈X ∥∇Ft(x) − ∇Ft−1(x)∥2 2 ] ) , which is in order of O (√ Σ 2 I+ √ ˜σ2 I ) . For strongly convex functions, as shown in Eq. ( 13) in the proof of Theorem 2, the multiplicative factor ˆGmax can be replaced by a more reﬁned factor maxt∈[T]∥∇ft(xt) − ∇ ft−1(xt)∥2. Then Algorithm 2 can ensure the following bound for strongly convex function s: RE GT ≤ O ( max t∈[T] ∥∇ft(xt) − ∇ft−1(xt)∥2 2· log ( T∑ t=2 sup x∈X ∥∇ft(x) − ∇ft−1(x)∥2 2 )) . By applying a similar argument as in Eq. ( 26) to decompose the gradient variation, taking the expec- tation on both sides, and leveraging the concavity of the log arithm, we conclude the proof. D.2 Proof of Corollary 2 Proof. The step sizes for optimistic OMD in ( 10) is ηt = min {D, mins∈[t] 1 4ℓs− 1(2∥Fs− 1(ˆzs)∥2) }. By the convexity and the concavity for the objective functio n f(·,·), for any z = ( x,y) ∈ Z , we can linearize the gap for an ε-approximate solution as f(¯xT,y) − f(x,¯yT) ≤ 1 T ∑ T t=1⟨F(zt),zt − z⟩. 27Following the proof of Lemma 2, we can demonstrate that optimistic OMD in ( 10) ensures: T∑ t=1 ⟨F(zt),zt − z⟩ ≤ O ( maxt∈[T]∥zt − z∥2 2 ηT + T∑ t=1 ηt∥F(zt) − F(ˆzt)∥2 2− T∑ t=1 1 ηt ∥zt − ˆzt∥2 2 ) . The ﬁrst term on the right-hand side is in the order of O(ˆLmaxD2) by the step size conﬁgura- tion, where ˆLmax denotes the maximum smoothness constant on the optimizatio n trajectory. By the ℓ-smoothness in Deﬁnition 2, the second term can be bounded as ηt∥F(zt) − F(ˆzt)∥2 2 ≤ O(ηtℓs−1(2∥Fs−1(ˆzs)∥2)2·∥zt−ˆzt∥2 2), which can be further cancelled by the negative terms. There - fore, ∑ T t=1⟨F(zt),zt− z⟩is bounded by a constant, thus, the convergence rate to an ε-approximate solution is O(1/T), implying a fast convergence rate. E Supporting Lemmas E.1 Lemmas for Optimistic OMD W e ﬁrst present the generic lemma for optimistic OMD with dyn amic regret [ Zhao et al. , 2024], which encompasses the standard regret by setting u1 = ...= uT = x⋆ ∈ arg minx∈X ∑ T t=1 ft(x). Lemma 3 (Theorem 1 of Zhao et al. [2024]). Optimistic OMD specialized at Eq. (3) satisﬁes T∑ t=1 ⟨∇ft(xt),xt − ut⟩ ≤ T∑ t=1 ⟨∇ft(xt) − Mt,xt − ˆxt+1⟩+ T∑ t=1 (Dψt (ut,ˆxt) − Dψt (ut,ˆxt+1)) − T∑ t=1 (Dψt (ˆxt+1,xt) + Dψt (xt,ˆxt)) , where u1,..., uT ∈ X are arbitrary comparators in the feasible domain. The next lemma, known as the stability lemma, establishes an upper bound on the proximity between successive decisions in terms of the gradient utilized for u pdates. Lemma 4 (Proposition 7 of Chiang et al. [2012]). Consider the following two updates: (i) x = arg min x∈X {⟨g,x⟩ + Dψ(x,c)}, and (ii) x′ = arg min x∈X {⟨g′,x⟩+ Dψ(x,c)}. When the regularizer ψ : X ↦→ R is λ-strongly convex function with respect to norm ∥·∥, we have λ∥x − x′∥ ≤ ∥ g − g′∥∗. E.2 Self-Conﬁdent T uning Lemmas In this part, we provide some useful lemmas when analyzing the self-conﬁdent tuning strategy. Lemma 5 (Extension of Lemma 14 Gaillard et al. [2014]). Let a0 > 0 and at ∈ [0,B] be real numbers for all t ∈ [T] and let f : (0 ,+∞) ↦→[0,+∞) be a nonincreasing function. Then ∑ T t=1 atf (∑ t−1 s=0 as ) ≤ B· f(a0) + ∫∑ T t=0 at a0 f(u)du. Lemma 6 (Lemma 3.5 of Auer et al. [2002]). Let a1,...,a T and δ be non-negative real numbers. Then ∑ T t=1 at √ δ+∑ t s=1 as ≤ 2 (√ δ+ ∑ T t=1 at − √ δ ) . E.3 T echnical Lemma Lemma 7.Let f : X ↦→ R be a convex, twice differentiable function. Then for any x,y ∈ int X and λ∈ [0,1], we have that: |⟨∇f(λx + (1 − λ)y),x − y⟩| ≤ max {|⟨∇f(x),x − y⟩|,|⟨∇f(y),x − y⟩|}. Proof. Deﬁne a function that φ(t) = f(tx + (1 − t)y). For this univariate convex function, we have φ′(t) = ⟨∇f(tx + (1 − t)y),x − y⟩. By the convexity of φ(t), there is |φ′(t)| ≤ max {|φ′(1)|,|φ′(0)|}, concluding the proof. 28",
      "meta_data": {
        "arxiv_id": "2408.09074v2",
        "authors": [
          "Yan-Feng Xie",
          "Peng Zhao",
          "Zhi-Hua Zhou"
        ],
        "published_date": "2024-08-17T02:22:08Z",
        "pdf_url": "https://arxiv.org/pdf/2408.09074v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper systematically studies gradient-variation online learning under generalized smoothness conditions, addressing the limitation of existing methods that often require a fixed gradient Lipschitzness. The main contributions include extending the optimistic mirror descent (optimistic OMD) algorithm to achieve optimal gradient-variation regret bounds (O(√VT) for convex and O(log VT) for strongly convex functions) under generalized smoothness by leveraging stability analysis over the optimization trajectory and local smoothness. Furthermore, the paper designs a universal online learning algorithm with a two-layer structure that simultaneously achieves optimal gradient-variation regrets for both convex and strongly convex functions without requiring prior knowledge of curvature. This universal algorithm introduces a novel Lipschitz-adaptive meta-algorithm that can handle potentially unbounded gradients and effectively ensemble base-learners. The findings are applied to establish new results for fast-rate convergence in games and stochastic extended adversarial optimization under generalized smoothness.",
        "methodology": "The core methodology involves extending the optimistic online mirror descent (optimistic OMD) framework. For generalized smoothness, the algorithm performs a trajectory-wise analysis, tuning step sizes based on intermediate decisions (ˆxt) and locally estimated smoothness constants ˆLt−1 = ℓt−1(2∥∇ft−1(ˆxt)∥2). Optimism is set as Mt = ∇ft−1(ˆxt). For universal online learning, a two-layer ensemble structure is adopted, with a meta-algorithm overseeing a group of base-learners, each configured for different curvature properties. A key technique is the function-variation-to-gradient-variation conversion, which decouples the meta and base levels by deriving gradient variation directly from function values. To address the challenge of unknown Lipschitz constants and heterogeneous inputs at the meta-level, a new Lipschitz-adaptive meta-algorithm (Algorithm 1) is designed. This meta-algorithm builds on optimistic Adapt-ML-Prod and incorporates a clipping technique and a self-confident learning rate to ensure second-order regret bounds and handle unbounded gradients without requiring restarts. A binary search technique is used to efficiently set the optimism mt.",
        "experimental_setup": "The paper is a theoretical work focused on deriving regret bounds and proving theoretical guarantees for online learning algorithms under generalized smoothness. It does not describe any practical experimental setup involving real-world datasets, simulations, or benchmarks. The theoretical setup assumes online functions are convex or λ-strongly convex and satisfy the ℓ-smoothness condition (generalized smoothness), and the feasible domain X is a bounded convex compact set. For universal learning, a curvature coefficient pool H = {2i−1/T : i∈ [N − 1]} is defined, and the number of base-learners is set to N = ⌈log2 T⌉ + 1. The theoretical results also assume the existence of finite but unknown upper bounds G for Lipschitzness and L for smoothness, which appear only in the final regret bounds, not as algorithmic inputs. A global lower bound ¯f for loss functions is required for the binary search used in setting optimism for the universal algorithm.",
        "limitations": "The current research does not address gradient-variation regret for exp-concave functions under generalized smoothness. Extending the flexible optimism setting and step size tuning to exp-concave functions, and ensuring the meta-algorithm can sufficiently cancel optimism-introduced terms, remain open challenges. The proposed Lipschitz-adaptive meta-algorithm introduces an additional O(√log N) factor in regret bounds (equivalent to O(log log T) for N=O(log T)), which, while often considered negligible, is a slight cost. For the Stochastically Extended Adversarial (SEA) model application, the results depend on ˜σ2 1:T, a larger quantity than σ2 1:T, due to leveraging information after xt−1 to generate xt, and it is currently unknown how to improve this dependence. The universal algorithm requires a global lower bound for loss functions for its binary search mechanism, a common assumption in some parameter-free optimization but still a constraint. The paper does not explore how the proposed methods extend to the one-gradient feedback model.",
        "future_research_directions": "One important future direction is to explore whether the proposed methods can be further extended to accommodate the one-gradient feedback model, where the learner only receives gradient information of the decision submitted in each round. Another interesting problem is to investigate how to exploit exp-concavity in gradient-variation online learning under generalized smoothness, as the current framework does not address this function class."
      }
    },
    {
      "title": "Understanding Adam Optimizer via Online Learning of Updates: Adam is FTRL in Disguise",
      "abstract": "Despite the success of the Adam optimizer in practice, the theoretical\nunderstanding of its algorithmic components still remains limited. In\nparticular, most existing analyses of Adam show the convergence rate that can\nbe simply achieved by non-adative algorithms like SGD. In this work, we provide\na different perspective based on online learning that underscores the\nimportance of Adam's algorithmic components. Inspired by Cutkosky et al.\n(2023), we consider the framework called online learning of updates/increments,\nwhere we choose the updates/increments of an optimizer based on an online\nlearner. With this framework, the design of a good optimizer is reduced to the\ndesign of a good online learner. Our main observation is that Adam corresponds\nto a principled online learning framework called Follow-the-Regularized-Leader\n(FTRL). Building on this observation, we study the benefits of its algorithmic\ncomponents from the online learning perspective.",
      "full_text": "Understanding Adam optimizer via Online Learning of Updates: Adam is FTRL in Disguise Kwangjun Ahn 1 2 Zhiyu Zhang 3 Yunbum Kook4 Yan Dai5 Abstract Despite the success of the Adam optimizer in prac- tice, the theoretical understanding of its algorith- mic components still remains limited. In particu- lar, most existing analyses of Adam show the con- vergence rate that can be simply achieved by non- adative algorithms like SGD. In this work, we pro- vide a different perspective based on online learn- ing that underscores the importance of Adam’s al- gorithmic components. Inspired by Cutkosky et al. (2023), we consider the framework called online learning of updates/increments, where we choose the updates/increments of an optimizer based on an online learner. With this framework, the design of a good optimizer is reduced to the design of a good online learner. Our main observation is that Adam corresponds to a principled online learning framework called Follow-the-Regularized-Leader (FTRL). Building on this observation, we study the benefits of its algorithmic components from the online learning perspective. 1. Introduction Let F : Rd → R be the (training) loss function we want to minimize. In machine learning applications, F is often minimized via an iterative optimization algorithm which starts at some initialization w0 and recursively updates wt+1 = wt + ∆t for t = 0, 1 . . . , (1.1) where ∆t denotes the update/increment 1 chosen by the algorithm at the t-th iteration. Practical optimizers often choose the update ∆t based on the past (stochastic) gradi- ents g1:t = (g1, . . . ,gt) where gt is the stochastic gradi- 1MIT 2Microsoft Research 3Havard University 4Georgia Tech 5Tsinghua University. Correspondence to: Kwangjun Ahn <kjahn@mit.edu>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). 1Sometimes, “update” refers to the iterate wt, but throughout this work, we mean the increment wt+1 − wt. ent of F collected during the t-th iteration. For instance, stochastic gradient descent (SGD) corresponds to choosing ∆t = −αtgt in (1.1) for some learning rate αt > 0. For training deep neural networks, one of the most popular choices is the Adam optimizer (Kingma and Ba, 2014). In particular, several recent works have observed that Adam and its variants are particularly effective for train- ing Transformer-based neural network models (Zhang et al., 2020b; Kunstner et al., 2023; Jiang et al., 2023; Pan and Li, 2023; Ahn et al., 2024). Given some learning rate γt > 0 and discounting factors β1, β2 ∈ (0, 1), Adam chooses ∆t on each coordinate i = 1, 2, . . . , dby combining g1:t as2 ∆t[i] = −γt (1 − β1) Pt s=1 βt−s 1 gs[i]q (1 − β2 2) Pt s=1(βt−s 2 gs[i])2 , where v[i] denotes the i-th coordinate of a vector v. For a streamlined notation, we define the scaled learning rate αt ← γt · (1−β1)/√ 1−β2 2 and consider ∆t[i] = −αt Pt s=1 βt−s 1 gs[i]qPt s=1(βt−s 2 gs[i])2 . (Adam) Compared to SGD, the notable components of Adam is the fact that it aggregates the past gradients g1:t (i.e., momen- tum) with the discounting factors β1, β2. Despite the prevalent application of Adam in deep learning, our theoretical grasp of its mechanics remains incomplete, particularly regarding the roles and significance of its core elements: the momentum and the discounting factors . Most existing theoretical works on Adam and its variants primarily focus on characterizing the convergence rate for convex functions or smooth nonconvex functions (Reddi et al., 2018; Zhou et al., 2019; Chen et al., 2019; Zou et al., 2019; Alacaoglu et al., 2020; Guo et al., 2021; D ´efossez et al., 2022; Zhang et al., 2022; Li et al., 2023; Wang et al., 2023) for which methods like SGD already achieve the min- imax optimal convergence rate. In fact, the latest works in this line (Li et al., 2023; Wang et al., 2023) both mention 2For simplicity, we remove the debiasing step and the appear- ance of ϵ in the denominator used in the original paper. 1 arXiv:2402.01567v2  [cs.LG]  30 May 2024Understanding Adam Optimizer via Online Learning of Updates that their convergence rate of Adam gets worse with momen- tum (Wang et al., 2023, §6) or the rate of Adam is no better than that of SGD (Li et al., 2023, §7). A notable exception is Crawshaw et al. (2022) where they show the benefits of momentum in a variant of Adam, under the generalized smoothness conditions of Zhang et al. (2020a). In this work, we take a different approach to understand Adam from a online learning perspective, as outlined below. 1.1. Our Approach and Main Results Our starting point is the main insight of Cutkosky et al. (2023) that the design of nonconvex optimizers falls under the scope of online linear optimization, an iconic setting in online learning. Specifically, one can regard the selection of the update ∆t based on g1:t as an online prediction procedure. Such a framework will be calledonline learning of updates/increments (OLU). Building on this framework, we then notice that it is im- portant to choose an online learner that performs well in dynamic environments (Cutkosky et al., 2023). Better dy- namic regret leads to better optimization performance (The- orem 2.1), and therefore, the design of good optimizers is reduced to designing good dynamic online learners. Along this line, our results can be summarized as follows: • (Section 2:) Our main observation is that the popular Adam optimizer corresponds to choosing a classical on- line learner called Follow-the-Regularized-Leader (FTRL) (Gordon, 1999; Kalai and Vempala, 2005; Shalev-Shwartz and Singer, 2006; Abernethy et al., 2008; Nesterov, 2009; Hazan and Kale, 2010). Specifically, when using the framework OLU, Adam is recovered by plugging in a discounted instance of FTRL well-suited for dynamic environment, which we call β-FTRL. • (Section 3:) We provide the dynamic regret guarantees of β-FTRL (Theorem 3.1 and Theorem 3.2) through a novel discounted-to-dynamic conversion. It gives us a new perspective on the role of Adam’s algorithmic compo- nents, namely the momentum and the discounting factors. Our results suggest that both components are crucial for designing a good dynamic online learner (see Subsec- tion 3.2). • (Section 4:) We justify the importance of a good dynamic regret, via its implications for optimization. Along the way, we discuss optimization settings for which Adam could be potentially beneficial. 2. Adam is FTRL in Disguise Iterative optimization algorithms are closely connected to adversarial online learning. For example, SGD is often an- alyzed through online gradient descent (OGD), its online learning counterpart. To exploit this connection in (1.1), the traditional approach is using an online learner to di- rectly choose the iterates wt, as demonstrated by Bottou (1998); Cesa-Bianchi et al. (2004); Duchi et al. (2011); Li and Orabona (2019); Ward et al. (2019) and many more. Diverging from this common approach, we consider a new approach due to Cutkosky et al. (2023) that applies the online learner to choose the updates ∆t. 2.1. Choosing Updates/Increments via Online Learning Consider an iconic setting of online learning called online linear optimization (OLO). For the consistency with our optimization algorithm (1.1), we will introduce OLO using slightly nonstandard notations. In each round t, the algo- rithm (or online learner) chooses a point ∆t ∈ Rd, and then receives a linear loss function ℓt(·) = ⟨vt+1, ·⟩ and suffers the loss of ℓt(∆t). In other words, it chooses ∆t based on the previous loss sequence v1:t := (v1, v2, . . . ,vt) and then receives the next loss vt+1. The performance of the on- line learner is measured by the regret against a comparator sequence u0:T−1, defined as RT (u0:T−1) := TX t=1 ⟨vt, ∆t−1 − ut−1⟩. (2.1) To be precise, (2.1) is called the dynamic regret in the lit- erature (Zinkevich, 2003). Another common metric, static regret, is a special case of (2.1) where all ut = u; this is denoted as RT (u). Now given an online learner LEARNER , we consider an optimization algorithm that outputs the ∆t in (1.1) using LEARNER . More formally, ∆t is chosen by LEARNER based on g1:t. (OLU) We call this framework online learning of updates (or on- line learning of increments). This framework was first pro- posed by Cutkosky et al. (2023) (under the name online-to- nonconvex conversion) to design algorithms that find critical points for nonsmooth and nonconvex stochastic optimiza- tion problems. Under OLU, we want LEARNER of choice to be a good online learner for dynamic environments, as summarized in the informal statement below. Theorem 2.1 (Importance of dynamic regret in OLU ; see Theorem 4.1) . In OLU, a better dynamic regret of LEARNER leads to a better optimization guarantee. There- fore, we want LEARNER to have a low dynamic regret. To understand this elegant reduction, let us give examples of how Theorem 4.1 is applied. Recent works (Cutkosky et al., 2023; Zhang and Cutkosky, 2024) choose an online gradient descent (OGD) (Zinkevich, 2003) as LEARNER to design algorithms for finding stationary points for nonconvex and 2Understanding Adam Optimizer via Online Learning of Updates nonsmooth functions. When LEARNER is chosen as OGD, the resulting optimization algorithm under OLU turns out to be SGD with momentum (Zhang and Cutkosky, 2024). However, OGD is known to require a careful tuning of learning rate (Zinkevich, 2003). What if we use an adaptive online learner as LEARNER ? Our main observation is that Adam can be recovered by choosing LEARNER as an adaptive version of Follow-the- Regularized-Leader that is well-suited for dynamic environ- ments, which we gradually elaborate. 2.2. Basics of Follow-the-Regularized-Leader (FTRL) Follow-the-Regularized-Leader (FTRL) is a classical algo- rithmic framework in online learning. Unlike the more intuitive descent-type algorithms, the key idea of FTRL is selecting the decisions by solving a convex optimization problem in each round. Throughout, we focus on the 1D case of OLO (d = 1) since the update of Adam is coordinate- wise. In particular, the overall regret for the d-dimension would be the sum of the regret of each coordinate. The 1D linear loss function is given by ℓt(∆) = vt+1∆ for vt+1 ∈ R. We use the subscript t + 1 to highlight that it is only revealed after deciding ∆t. FTRL relies on a nonnegative convex regularizer Φ, which is set to Φ = 1 2 | · |2 in this work. The algorithm initializes at ∆0 = 0 and in each round outputs ∆t = arg min x h 1 ηt Φ(x) + tX s=1 vsx i = −ηt tX s=1 vs , (FTRL) where the effective step size ηt > 0 is non-increasing in t. The remaining task is to choose good step sizes ηt. One prominent choice is the adaptive step size of the scale- free FTRL algorithm (Orabona and P ´al, 2018, §3) in the style of McMahan and Streeter (2010); Duchi et al. (2011). Scale-free FTRL chooses ηt = α/√Pt s=1 v2s based on a scaling factor α >0, resulting in the update ∆t = −α Pt s=1 vsqPt s=1 v2s . (2.2) Here if the denominator is zero, then we set the output ∆t = 0. The update (2.2) is independent of any constant scaling of loss sequence v1:t, making it a scale-free update. This is beneficial when the magnitude of loss sequence varies across different coordinates. We remark that the analysis of scale-free FTRL is in fact quite subtle, as echoed by McMahan (2017); Orabona and P´al (2018). Using a different proof strategy, we prove a static regret bound (Theorem A.1) of scale-free FTRL that slightly strengthens that of Orabona and P´al (2018). 2.3. Adam Corresponds to Discounted-FTRL Now back to OLU, let us use FTRL to choose the update ∆t. Denoting the coordinate-wise gradients in optimization by g1:t, a na¨ıve approach is to use FTRL directly by setting vt ← gt. Unfortunately, this approach is not a good one because FTRL is designed to achieve low static regret, while OLU requires low dynamic regret. In fact, it is shown by (Jacobsen and Cutkosky, 2022, Theorem 2) that Algorithms of the form (FTRL) are not good dynamic online learners. See also the lower bounds in Theorem 3.3. One could already see intuitively why this is the case: any algorithm in this form does not “forget the past”, as the output ∆t is the (regularized) minimizer of the cumulative loss Lt(x) := Pt s=1 vsx. Therefore, it is only competitive w.r.t. a fixed comparator that minimizes Lt(x), instead of a time-varying comparator sequence. To address this issue, our approach is to “ discount” the losses from the distant past. In particular, we implement this by gradually up-scaling the losses over time. The intuition is that when deciding the output ∆t, the recent losses would have much higher “weights” compared to older ones, which essentially makes the latter negligible. Theorem 2.2 (Informal; see Theorems 3.1 and 3.2) . For some β ∈ (0, 1), the discounted version of scale-free FTRL that internally replaces vt by β−tvt is a good dynamic online learner. Remarkably, plugging this discounted scale-free FTRL into OLU would almost recover Adam. There are just two small issues: in the Adam update, the (scaled) learning rate αt is time-varying, and we need two discounting factors β1 and β2 for the numerator and the denominator separately. It is not hard to fix this last bit, and we end up with an FTRL instance which given the input gt picks vt ← β−t 1 gt , and ηt = αt(β1/β2)t qPt s=1(β−s 2 gs)2 . (2.3) Collecting all the pieces above yields our first main result. Proposition 2.3 (Adam is discounted-FTRL in disguise). For some learning rate αt > 0 and discounting factors β1, β2 ∈ (0, 1], FTRL with (2.3) is equivalent to picking ∆t = −αt Pt s=1 βt−s 1 gsqPt s=1(βt−s 2 gs)2 . Applying it as a coordinate-wiseLEARNER in OLU recovers the Adam optimizer in Adam. Recall that OLU connects the problem of optimization to the well-established problem of dynamic regret minimization. 3Understanding Adam Optimizer via Online Learning of Updates Given Proposition 2.3, we make use of this connection to understand the components of Adam from the dynamic regret perspective. That is the main focus of the next section. Before getting into that, we briefly compare our approach with existing derivations of Adam based on FTRL. 2.4. Comparison with the Previous Approach In fact, Zheng and Kwok (2017) propose a derivation of Adam based on FTRL. However, their approach is quite different than ours, as we detail below. We first briefly summarize the approach of Zheng and Kwok (2017). Their main idea is to consider the “weighted” ver- sion of proximal-FTRL defined as wt = arg min w tX s=1 λs \u0010 ⟨gs, w⟩ + 1 2∥w − ws−1∥2 Qs \u0011 , for some weights {λs} and positive semi-definite matrices {Qs}. Given this, their main observation is that Adam roughly corresponds to this proximal-FTRL with carefully chosen {λs} and {Qs}. Although their motivation to explain Adam with a version of FTRL is similar to ours, we highlight that their approach is different than ours. In fact, our approach overcomes some of the limitations of Zheng and Kwok (2017). • Firstly, their derivation actually needs a heuristic adjust- ment of changing the anchor points of the regularizer from ws−1 to wt−1. A priori, it is not clear why such adjust- ment is needed, and to the best of our knowledge, there is no formal justification given. But with our approach, such an adjustment is naturally derived because under OLU, the online learner chooses the update/increment instead of the iterate. • Secondly, in Zheng and Kwok (2017), in order to recover Adam, they have to choose {ws} and {Qs} carefully, which also lacks justification. One of the main advantages of our approach is the fact that the discounting factors are theoretically justified via the dynamic regret perspective. More specifically, we show that without the discounting factor, FTRL is not a good dynamic learner. 3. Discounted-FTRL as a Dynamic Learner This section provides details on Theorem 2.2, focusing on the special case of Adam where β1, β2 = β for some β ∈ (0, 1], and αt = α for some α >0. From Proposition 2.3 and using the same notation as (2.2), this corresponds to the following coordinate-wise update rule: ∆t = −α Pt s=1 βt−svsqPt s=1(βt−svs)2 , (β-FTRL) and if the denominator is zero, we define ∆t = 0. We call this algorithm β-FTRL. With β = 1 , it exactly recovers scale-free FTRL (2.2) which is shown to be a poor dynamic learner (Jacobsen and Cutkosky, 2022). Therefore, we will focus on β <1 in the dynamic regret analysis. The earlier informal result (Theorem 2.2) is formalized in Theorem 3.1 (for unbounded domain) and Theorem 3.2 (for bounded domain). We provide the simplified versions here, deferring the detailed adaptive version to Theorem B.4. Theorem 3.1 (Dynamic regret of β-FTRL; unbounded domain). For a loss sequence v1:T , consider β-FTRL with β <1 and some constant α >0. Let Mβ := max t∈[1,T] \f\f\fPt s=1 βt−svs \f\f\f qPt s=1(βt−svs)2 . Then, for any comparator sequence u0:T−1 such that |ut| ≤ αMβ for all t, the dynamic regret RT (u0:T−1) is upper bounded by O  \u0000 αM2 β + MβP \u0001 G√1 − β + p 1 − β · αM2 βGT ! . Here, G := maxt∈[1:T] |vt|, and P := PT−1 t=1 |ut − ut−1| is the path length. We sketch the proof of Theorem 3.1 in Subsection 3.3. Theorem 3.1 may not seem straightforward, so let us start with a high level interpretation. First of all, the path lengthP is a standard complexity measure of the comparator u0:T−1 in the literature (Herbster and Warmuth, 2001), which we ex- amine closely in Subsection 3.1. In the context of dynamic online learning, the above bound could be reminiscent of a classical result from (Zinkevich, 2003, Theorem 2): on a do- main of diameter D, the dynamic regret of online gradient descent (OGD) with learning rate η can be bounded as RT (u0:T−1) ≤ O \u0012D2 + DP η + ηG2T \u0013 . (3.1) Intuitively, the choice ofη balances the two conflicting terms on the RHS, and a similar tradeoff remains as a recurring theme in the dynamic online learning literature (Hall and Willett, 2015; Zhang et al., 2018a; Jacobsen and Cutkosky, 2022). In an analogous manner, the discounting factor β in Theorem 3.1 largely serves the similar purpose of balancing conflicting factors. A rigorous discussion is deferred to Subsection 3.2. As a complementary result to Theorem 3.1, we also present a dynamic regret bound for the case of a priori bounded domain, where the outputs of online learner should lie in 4Understanding Adam Optimizer via Online Learning of Updates a bounded domain [−D, D]. In this case, we project the output of β-FTRL to the given domain: ∆t = −clipD  α Pt s=1 βt−svsqPt s=1(βt−svs)2   , (β-FTRLD) where clipD(x) := x min( D |x|, 1). Then, with the same no- tations of G and P as in Theorem 3.1, we get the following result (see Subsection B.5 for details). Theorem 3.2 (Dynamic regret of β-FTRLD; bounded domain). For D >0, consider any comparator sequence u0:T−1 such that |ut| ≤D for all t. Then for any loss sequence v1:T , β-FTRLD with β <1 and α = D has the dynamic regret RT (u0:T−1) upper bounded by O \u0012 DG√1 − β + GP 1 − β + p 1 − βDGT \u0013 . Compared to Theorem 3.1, the main difference is that the regret bound now holds simultaneously for all the loss se- quences v1:T of arbitrary size. The price to pay is the requirement of knowing D, and the multiplying factor on the path length P is slightly worse, i.e., (1 − β)−1/2 → (1 − β)−1. A sneak peek into the details: such a slightly worse factor is due to the projection step breaking the self- bounding property of β-FTRL, which says the discounted gradient sum Pt s=1 βt−svs can be controlled by the maxi- mum update magnitude sup |∆t| times the empirical vari- ance of gradients, i.e., qPt s=1(βt−svs)2. Interested read- ers may compare Subsections B.4 and B.5 for the subtleties. Moving forward, Theorems 3.1 and 3.2 constitute our main results characterizing the dynamic regret of β-FTRL. How- ever, there is still one missing piece. The earlier informal result (Theorem 2.2) states that β-FTRL is a “good” dynamic online learner. However, we have never explained which dynamic regret is good. Actually, the “goodness” criterion in dynamic online learning could be a bit subtle, as the typical sublinear-in-T metric in static online learning becomes vacuous. Next, we briefly provide this important background. 3.1. Basics of Dynamic Online Learning Dynamic online learning is intrinsically challenging. It is well-known that regardless of the algorithm, there exist loss and comparator sequences such that the dynamic regret is at least Ω(T). This is in stark contrast to static regret bounds in OLO, where the standard minimax optimal rate is the sublinear in T, e.g., O \u0000√ T \u0001 . To bypass this issue, the typical approach is throughinstance adaptivity. Each combination of the loss and comparator sequences can be associated to a complexity measure; the larger it is, the harder regret minimization becomes. Al- though it is impossible to guarantee sublinear-in-T regret bounds against the hardest problem instance, one can indeed guarantee a regret bound that depends on such a complex- ity measure. From this perspective, the study of dynamic online learning centers around finding suitable complexity measures and designing adaptive algorithms. Only considering the comparator sequence u0:T−1, the pre- dominant complexity measure is the path length P :=PT−1 t=1 ∥ut − ut−1∥ (Zinkevich, 2003), whose 1D special case is considered in Theorem 3.1 and Theorem 3.2. On a bounded domain with diameter D, the optimal dynamic re- gret bound is O(G √ DP T), which can be achieved through the classical result for OGD (3.1) with the P-dependent learning rate η = G−1p DP/T . On top of that, one could use a model selection approach (Zhang et al., 2018a) to avoid the infeasible oracle tuning (i.e., η depends on the un- known P), at the expense of increased computation. Recent works (Jacobsen and Cutkosky, 2022; Zhang et al., 2023) further extend such results to unbounded domains. The essential “goodness” of this O \u0000 G √ DP T \u0001 bound is due to P ≤ DT . In the worst case the bound is trivially O(DGT ), but if the comparator is easy (i.e., P = O(D)), then it becomes O \u0000 DG √ T \u0001 , recovering the well-known optimal static regret bound. In general, the goodness of a dynamic regret bound is usually measured by the exponents of both P and T (e.g., 1 2 and 1 2 in O \u0000 G √ DP T \u0001 ). Given this background, we now use the dynamic regret results of β-FTRL so far to interpret the role of two key components of Adam, namely the momentum (i.e., aggre- gating past gradients) and the discounting factor β (i.e., exponential moving average). 3.2. Benefits of Momentum and Discounting Factor Recall that a particular strength of the OLU framework is that it establishes one-to-one correspondence between op- timizers and their online learning counterparts. Thus we can compare a variety of optimizers by comparing their corre- sponding online learners, from the perspective of dynamic regret. Notice that β-FTRL from our analysis corresponds to the scaled parameterization of (Adam). Through that, our ultimate goal is to shed light on Adam’s algorithmic components — the momentum and the discounting factor. We first discuss the baseline online learners for this problem: • To understand the momentum, we pick the baselines as a family of “degenerate” online learners that induce non-momentum optimizers, such as SGD and AdaGrad (Duchi et al., 2011). Concretely, for the loss sequence v1:T , this family of LEARNER in OLU has the following generic update rule: for some coordinate-wise learning 5Understanding Adam Optimizer via Online Learning of Updates rate αt[i] > 0 ∀i, it outputs ∆t[i] = −αt[i]vt[i] . (3.2) For example, given a scalar αt > 0, SGD chooses the coordinate-wise learning rate αt[i] independently of the coordinates, i.e., αt[i] = αt , (SGD) while AdaGrad further employs a variance-based precon- ditioning, i.e., αt[i] = αtqPt s=1 vs[i]2 . (AdaGrad) The important observation is that compared to the(FTRL) update rule, the coordinate-wise update ∆t[i] in (3.2) only scales linearly with the most recent observation vt[i], instead of using the entire history v1:t[i]. In other words, from the optimization perspective, this family of algorithms does not make use of the past history of gradi- ents to decide the update direction. • To understand the discounting factor, we pick the baseline as β-FTRL with β = 1 (i.e., no discounting). Alterna- tively, if the domain is bounded, then we use the clipped version of β-FTRLD with β = 1 instead. In other words, such baselines correspond to scale-free FTRL (2.2). In light of Jacobsen and Cutkosky (2022), the case of β = 1 is not a good dynamic online learner, which we discuss more formally below. The following lower bound, inspired by Jacobsen and Cutkosky (2022), shows that the above baselines fail to achieve sublinear dynamic regret for a very benign example of P = O(1). See Subsection C.1 for a proof. Theorem 3.3 (Lower bounds for baselines) . Consider a 2D online linear optimization problem with the bounded domain [−1, 1]2. For any given T, there exist ( i) a loss sequence v1, . . . ,vT ∈ R2 with ∥vt∥ = 1 for all t, and (ii) a comparator sequence u0, . . . ,uT−1 ∈ [−1, 1]2 with the coordinate-wise path length PT−1 t=1 |ut[i] − ut−1[i]| ≤1 for both i = 1, 2, such that the following holds: • For all t, ut−1 ∈ arg minu∈[−1,1]2 ⟨vt, u⟩. • Any “non-momentum” online learner of the form (3.2) has the dynamic regret at least T − 3. • β-FTRLD with β = 1 and D = 1 has the dynamic regret at least (T − 3)/2. The first bullet point says that the constructed comparator se- quence u0:T−1 is the best ones w.r.t.the loss sequence v1:T , therefore the dynamic regret against such u0:T−1 is a good metric to measure the strength of dynamic online learners. Then, the rest of the theorem shows that the two baselines above (corresponding to optimizers without momentum or discounting factor under OLU) cannot guarantee low regret against u0:T−1, thus are not good dynamic online learners. In contrast, Theorem 3.2 shows that β-FTRLD with β <1 is a better dynamic online learner, and we make this very concrete through the following corollary. Again, it suffices to consider the 1D setting. Corollary 3.4. For D >0, consider any comparator se- quence u0:T−1 such that |ut| ≤D for all t. Then, given any constant c >0, β-FTRLD with β = 1 − cT−2/3 > 0 achieves the dynamic regret bound RT (u0:T−1) ≤ O \u0010 DGT 2/3 c 1/2 (1 + c−3/2 P/D) \u0011 , which enjoys a T 2/3 dependency on T. In particular, with the optimal tuning c = Θ \u0000 (P/D) 2/3 \u0001 , the bound becomes O(D 2/3GP 1/3T 2/3). The proof is deferred to Subsection C.2. We emphasize that in the lower bound example of Theorem 3.3, we have P = O(D), so the β <1 case achieves a sublinear dynamic regret bound of O(DGT 2/3). This suggests that in order to design a better dynamic online learner both momentum and discounting factor are necessary. A similar result can be developed for the case of unbounded domain under an assumption regarding the 1D OLO envi- ronment that generates the losses v1:T . Corollary 3.5. Assume the environment is well-behaved in the sense that Mβ ≤ M for all β ∈ (0, 1]. Consider any comparator sequence u0:T−1 such that |ut| ≤αM for all t. Then, given any constant c >0, β-FTRL with parameters α and β = 1 − cT−1 > 0 achieves the dynamic regret bound RT (u0:T−1) ≤ O \u0012 αM2G √ T c 1/2 \u0012 1 + c−1P αM \u0013\u0013 , which enjoys a √ T dependency on T. In particular, with the optimal tuning c = Θ \u0000 P/(αM) \u0001 , the bound becomes O \u0000 α 1/2M 3/2G √ P T \u0001 . In contrast, β-FTRL with the same α but the different β = 1 achieves a dynamic regret bound which is linear inP: RT (u0:T−1) ≤ O \u0010 MGP √ T \u0011 . Essentially, without the discounting factor we can show an O \u0000 P √ T \u0001 dynamic regret bound, but with discounting the bound can be improved to the optimal rate O \u0000√ P T \u0001 , under suitable tuning. This provides another evidence that discounting is helpful for designing a dynamic online learner. We remark that without discounting, the dynamic regret of O \u0000 P √ T \u0001 is unimprovable in light of the lower bound result (Jacobsen and Cutkosky, 2022, Theorem 3). 6Understanding Adam Optimizer via Online Learning of Updates 3.3. Proof Sketch of Theorem 3.1 Finally, we briefly sketch the proof of Theorem 3.1, the dy- namic regret bound of β-FTRL. The proof of Theorem 3.2 mostly follows the same analysis. Our analysis of the dy- namic regret relies on the following “discounted” regret. Definition 3.6 (β-discounted regret). For any discounting factor β ∈ (0, 1], the β-discounted regret is defined as RT;β(u) := TX t=1 βT−tvt(∆t−1 − u) . It is noteworthy that the discounted regret has been consid- ered in the concurrent works (Zhang et al., 2024; Jacobsen and Cutkosky, 2024) to adapt online learners to dynamic environments. Moreover, the discounted regret has found to be useful in designing nonconvex optimization algorithms, as shown in (Zhang and Cutkosky, 2024; Ahn and Cutkosky, 2024). In our work, we propose a generic conversion ap- proach to analyzing the dynamic regret using the discounted regret, called discounted-to-dynamic conversion (Theo- rem B.3), which could be of independent interest. At a high level, our analysis follows the following steps. (Theorem B.2)| {z } Discounted reg. of β-FTRL (Theorem B.3)= = = = = = = = = = =⇒ Discount-to-dynamic (Theorem 3.1)| {z } Dynamic reg. of β-FTRL 1. β-FTRL is the discounted version of scale-free FTRL, and the latter is associated to a static regret bound (Theo- rem A.1). Utilizing this relation, we naturally arrive at a discounted regret bound of β-FTRL (Theorem B.2). In- tuitively, it measures the performance of β-FTRL on an exponentially weighted, “local” look-back window that ends at time T. A particular strength is that the bound is anytime, i.e., it holds for all T simultaneously. 2. Next, consider the dynamic regret over the entire time horizon [1, T]. We can imagine partitioning [1, T] into concatenating subintervals, and on each of them the dy- namic regret can be approximately upper-bounded by suitable aggregations of the above discounted regret bound (modulo certain approximation error) — this is because the discounted regret bound mostly concerns the few recent rounds, so the dynamic regret can be con- trolled by the sum of such local metrics. Formalizing this argument results in the discounted-to-dynamic conver- sion in Theorem B.3: the dynamic regret of an algorithm is expressed using its discounted regret as an equality. Both Theorem 3.1 and 3.2 are obtained by combining these two elements, with slightly different ways of relaxation. 4. Implications for Optimization In this section, we provides the details of Theorem 2.1, which justifies the importance of the dynamic regret guar- antee of LEARNER in OLU, based on its implications for (non-convex) optimization. Recall that in OLU, for the functionF we want to minimize, we choose the update ∆t by the output of LEARNER on the loss sequence g1:t that are stochastic gradients of F. Formally, we assume F : Rd → R is differentiable but not necessarily convex. Following the notations of Cutkosky et al. (2023), given an iterate w and a random variable z, let GRAD (w, z) be the standard stochastic gradient oracle of F at w, satisfying Ez[GRAD (w, z)] = ∇F(w). We first introduce (Cutkosky et al., 2023, Theorem 7) that crucially connects the dynamic regret guarantee to the op- timization guarantee, justifying the importance of the dy- namic regret of LEARNER . Theorem 4.1 (Importance of dynamic regret in OLU). Consider the optimization algorithm (1.1) where • the update ∆t is chosen by LEARNER based on g1:t; • the gradient gt = GRAD (wt−1 + st∆t, zt), where the i.i.d. samples st ∼ Unif([0, 1]) and zt ∼ z. Then, for all T ≥ 0 and any comparator sequence u0:T−1, the iterate wT generated by (1.1) satisfies E[F(wT )]−F(w0) = E \" TX t=1 ⟨gt, ut−1⟩ + RT (u0:T−1) # , where RT (u0:T−1) is the regret bound of LEARNER . The proof is provided in Subsection D.1. The insight is that the nonconvexity can be handled by randomization (through st) at the gradient query, and if F is convex, it suffices to set st = 1 (which is more aligned with practice). Regarding the quantitative result, Theorem 4.1 precisely captures the informal claim from Theorem 2.1: Improving the dynamic regret of LEARNER directly leads to better optimization guarantee. Hence, the effectiveness of β-FTRL as a dynamic online learner (discussed in Section 3) supports the success of its optimizer counterpart, namely the update (Adam). To make this concrete, we revisit the lower bound examples from Theorem 3.3, and discuss the implications. 4.1. Revisiting lower bound example Consider an abstract scenario where we fix the stochastic gradient sequence g1:T to be given the lower bound example 7Understanding Adam Optimizer via Online Learning of Updates of Theorem 3.3. We compare the guarantees in Theorem 4.1, TX t=1 ⟨gt, ut−1⟩ + RT (u0:T−1) . (4.1) Then, the following is a direct corollary of Subsection 3.2. Corollary 4.2. Suppose g1:T and u0:T−1 are chosen as in the 2D example of Theorem 3.3. Then, the following statements hold: • Adam. If LEARNER is β-FTRLD with 1 − β = Θ(T−2/3) and D = 1, then (4.1) = −T + o(T). • No momentum (e.g., SGD/AdaGrad).If LEARNER has the form (3.2), then (4.1) ≥ −3. • No discounting. If LEARNER is β-FTRLD with β = 1 and D = 1, then (4.1) ≥ −1 2 T − 3 2 . Essentially, this result specializes the key insight from Sub- section 3.2, i.e., the benefits of the momentum and the dis- counting factor, to the corresponding optimizers. On the other hand, we acknowledge that the abstract sce- nario of fixing g1:T to be some desired sequence is not en- tirely practical, because (i) they should be stochastic gradi- ents of F and (ii) g1:T depends on the LEARNER of choice. Below, we build on the intuitions of this abstract example and present a concrete classification problem where Adam is more beneficial than the other two baselines. 4.2. Adam Could Be Effective for Sparse and Nonstationary Gradients Inspired by Duchi et al. (2011), we propose a concrete classification scenario for which we see the performance gap described in Corollary 4.2. At a high level, it is designed such that the associated gradient sequence imitates the one from our lower bound construction, Theorem 3.3. Classification of sparse data with small η. Consider the classification of (zi, yi) where zi ∈ Rd is the data vector with its label yi ∈ {±1}. We assume that each data vector zi is sparse. Concretely, we focus on the setting where the dataset consists of coordinate vectors and positive labels,i.e., {(zi, yi)}d i=1 where zi = ei and yi = 1 for i = 1, . . . , d. We consider the following regularized hinge loss ℓ(x) = max(0, 1 − x) + λ|x| for λ <1 , which prevents the classifier from becoming over-confident. See Figure 1 for the landscape of this hinge loss. Then, the training loss is given as F(w) = 1 d dX i=1 ℓ(yi⟨zi, w⟩) , −1 0 1 2 3 40 0.5 1 1.5 2 Figure 1.1D illustration of the regularized hinge loss ℓ(x) = max(0, 1 − x) +λ|x|. We illustrate the case λ = 1/4. Figure 2.Experimental results for the hinge loss classification. (Left) the case of zi = ei. (Right) the case of zi = ciei where ci ∼ Unif[0, 2]. The horizontal dotted line indicates the optimum value of F. All experiments are run for five different random seeds, and we plot the error shades (they are quite small and not conspicuous). and during iterationt, assume the algorithm receives a single data (zi(t), yi(t)), where i(t) is sampled from {1, . . . , d} uniformly at random. For experiments, we initialize at w0 = 0 and use a small learning rate, η = 0 .01, so that each coordinate takes multiple steps to approach the minimum w = 1. Besides, we choose d = 100 and λ = 1/4. Two different settings are considered: 1. Left plot of Figure 2: zi = ei for i = 1, . . . , d. 2. Right plot of Figure 2:zi = ciei, where ci ∼ Unif[0, 2] for i = 1, . . . , d. This setting allows the data vectors to have different magnitudes. From Figure 2, SGD exhibits sluggish progress owing to the sparse nature of stochastic gradients—–that is, only one coordinate is updated at each step. Moreover, setting β = 1 in Adam also results in suboptimal performance once the coordinate-wise iterate wt[i] exceeds 1 (for some coordinate i)—after that, the stochastic gradients point toward other di- rections. In contrast, adopting Adam with β <1 effectively addresses these issues, adeptly managing both the sparsity of updates and the non-stationarity of gradients. Next, we provide a possible qualitative explanation of this gap, from the dynamic regret perspective. Qualitative dynamic regret analysis. Since i(t) is sampled 8Understanding Adam Optimizer via Online Learning of Updates uniformly, it suffices to focus on the first coordinate and consider the 1D setting for simplicity. Then, since learning rate η is chosen small, starting from w0 = 0, the above set- ting could be abstractly thought as generating the following sparse stochastic gradient sequence gt = ( (1 − λ) · I[i(t)=1] if t ≲ τ −λ · I[i(t)=1] if t ≳ τ , (4.2) where τ denotes the first iteration such that wτ > 1. This can be seen as one of the simplest setting of a sparse and non-stationary gradient sequence, mirroring the construc- tion from Theorem 3.3. Now we compare the 1D version of the guarantee (4.1), i.e., the total loss of the LEARNER TX t=1 gtut−1 + RT (u0:T−1) = TX t=1 gt∆t−1 (4.3) for each LEARNER akin to Corollary 4.2: • No momentum. Due to the sparsity in gradient sequence (4.2), having no momentum incurs a large dynamic regret. More specifically, since E|gtgt−1| ≲ 1 d2 , we have E|gt∆t−1| ≲ 1 d2 , for “non-momentum” LEARNER of the form (3.2). There- fore, we have (4.3) = P t gt∆t−1 ≳ − 1 d2 T. • No discounting factor. Due to the non-stationarity in gradient sequence (4.2), having no discounting factor also incurs a large dynamic regret. In particular, β-FTRL with β = 1 generates the update ∆t with the same sign as −P t gt. In a typical run, the sign of −P t gt remains unchanged throughout, but the sign of gt flips once t ≳ τ. Hence, roughly speaking, the update ∆t does not have the “correct” sign (corresponding to the descent direction) after t ≳ τ, leading to (4.3) ≳ −(1 − λ)τ. In contrast, following Corollary 4.2, Adam achieves(4.3) ≤ −(1−λ)τ −λ(T −τ)+ o(T) = −λT −(1−2λ)τ +o(T), improving the above when τ is small. 5. Conclusion and Discussion This work presents a new perspective on the popular Adam optimizer, based on the framework of online learning of updates (OLU) (Cutkosky et al., 2023). Under OLU, our main observation is that Adam corresponds to choosing the dynamic version of FTRL that utilizes the discounting factor. We find this perspective quite advantageous as it gives new insights into the role of Adam’s algorithmic components, such as momentum and the exponential moving average. In fact, our perspective has already inspired a follow-up work by Ahn and Cutkosky (2024), where they show the optimal iteration complexity of Adam for finding stationary points under nonconvex and nonsmooth functions. Their analysis crucially utilizes our perspective that Adam corre- sponds to β-FTRL under OLU. In addition to (Ahn and Cutkosky, 2024), the findings in this work unlock several other important future directions. Below, we list a few of them. • Role of two discounting factors. As an initial effort, this work considers the case of β1 = β2. Given that the default choice in practice is β1 = 0.9 and β2 = 0.999, it would be important to understand the precise effect of choosing β1 < β2. • Other algorithms based on OLU. The framework OLU unlocks a new way to analyze optimization algorithms. As we highlighted in Subsection 3.2, OLU establishes the one-to-one correspondence between other optimizers and their online learning counterparts. The main scope of this work is to provide a better understanding of Adam specifically, and extending our framework to other popu- lar optimizers, such as RMSProp (Tieleman and Hinton, 2012), AdaDelta (Zeiler, 2012), Lion (Chen et al., 2023) etc, is an interesting future direction. We believe that un- derstand them based on our framework would offer new insights for them. • Algorithm design based on OLU. Our current dynamic regret analysis of β-FTRL requires knowledge of the en- vironment. Developing a version of β-FTRL that auto- matically adapts to the environment without prior knowl- edge might lead to more practical algorithms. Moreover, whether one can design practical optimizers based on recent advancements in dynamic online learning (e.g. Ja- cobsen and Cutkosky (2022); Zhang et al. (2023)) would be an important future direction. • Fine-grained analysis of Adam for practical settings. As discussed earlier, Adam has gained significant atten- tion due to its effectiveness in training language models. Recently, Kunstner et al. (2024) investigate key character- istics of the language modeling datasets that might have caused the difficulties in training. In particular, they iden- tify the heavy-tailed imbalance property, where there are a lot more infrequent words/tokens than frequent ones in most language modeling datasets. Further, they demon- strate this property as a main reason why Adam is partic- ularly effective at language modeling tasks (Zhang et al., 2020b). We find their main insights consistent with our claim in Subsection 4.2. The infrequent words in the dataset would likely lead to sparse and non-stationary gradients. Formally investigating this would be also an interesting future direction. 9Understanding Adam Optimizer via Online Learning of Updates Acknowledgements Kwangjun Ahn is indebted to Ashok Cutkosky for several inspiring conversations that led to this project. Kwangjun Ahn was supported by the ONR grant (N00014- 23-1-2299), MIT-IBM Watson, a Vannevar Bush fellowship from Office of the Secretary of Defense, and NSF CAREER award (1846088). Yunbum Kook was supported in part by NSF awards CCF-2007443 and CCF-2134105. Zhiyu Zhang was supported by the funding from Heng Yang. Impact Statement This paper provides a new perspective of understanding the Adam optimizer. This work is theoretical, and we do not see any immediate potential societal consequences. References Jacob D. Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An efficient algorithm for bandit linear optimization. In Conference on Learning Theory (COLT), pages 263–274. Omnipress, 2008. Kwangjun Ahn and Ashok Cutkosky. Adam with model exponential moving average is effective for nonconvex optimization, 2024. Kwangjun Ahn, Xiang Cheng, Minhak Song, Chulhee Yun, Ali Jadbabaie, and Suvrit Sra. Linear attention is (maybe) all you need (to understand Transformer optimization). In International Conference on Learning Representations (ICLR), 2024. Ahmet Alacaoglu, Yura Malitsky, Panayotis Mertikopoulos, and V olkan Cevher. A new regret analysis for Adam- type algorithms. In International Conference on Machine Learning (ICML), pages 202–210. PMLR, 2020. Leon Bottou. Online learning and stochastic approximations. On-Line Learning in Neural Networks, 17(9):142, 1998. Nicol`o Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line learning algo- rithms. Information Theory, IEEE Transactions on, 50 (9):2050–2057, 2004. Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, et al. Symbolic discovery of optimization algorithms. In Advances in Neural Informa- tion Processing Systems (NeurIPS), volume 36, 2023. Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of A class of Adam-type algorithms for non-convex optimization. In International Conference on Learning Representations (ICLR), 2019. Michael Crawshaw, Mingrui Liu, Francesco Orabona, Wei Zhang, and Zhenxun Zhuang. Robustness to unbounded smoothness of generalized SignSGD. In Advances in Neural Information Processing Systems (NeurIPS), vol- ume 35, pages 9955–9968, 2022. Ashok Cutkosky. Artificial constraints and hints for un- bounded online learning. In Conference on Learning Theory (COLT), pages 874–894, 2019. Ashok Cutkosky, Harsh Mehta, and Francesco Orabona. Optimal stochastic non-smooth non-convex optimization through online-to-non-convex conversion. In Interna- tional Conference on Machine Learning (ICML), volume 202, pages 6643–6670. PMLR, 2023. Amit Daniely, Alon Gonen, and Shai Shalev-Shwartz. Strongly adaptive online learning. In International Con- ference on Machine Learning (ICML), pages 1405–1411. PMLR, 2015. Alexandre D´efossez, Leon Bottou, Francis Bach, and Nico- las Usunier. A simple convergence proof of Adam and AdaGrad. Transactions on Machine Learning Research (TMLR), 2022. John Duchi, Elad Hazan, and Yoram Singer. Adaptive sub- gradient methods for online learning and stochastic opti- mization. Journal of Machine Learning Research (JMLR), 12(61):2121–2159, 2011. Geoffrey J. Gordon. Regret bounds for prediction problems. In Conference on Learning Theory (COLT), pages 29–40. ACM, 1999. Zhishuai Guo, Yi Xu, Wotao Yin, Rong Jin, and Tianbao Yang. A novel convergence analysis for algorithms of the Adam family. arXiv preprint arXiv:2112.03459, 2021. Eric C Hall and Rebecca M Willett. Online convex optimiza- tion in dynamic environments. IEEE Journal of Selected Topics in Signal Processing, 9(4):647–662, 2015. Elad Hazan and Satyen Kale. Extracting certainty from un- certainty: Regret bounded by variation in costs. Machine Learning, 80:165–188, 2010. Mark Herbster and Manfred K Warmuth. Tracking the best linear predictor. Journal of Machine Learning Research (JMLR), 1(281-309):10–1162, 2001. Andrew Jacobsen and Ashok Cutkosky. Parameter-free mir- ror descent. In Conference on Learning Theory (COLT), pages 4160–4211. PMLR, 2022. Andrew Jacobsen and Ashok Cutkosky. Online linear re- gression in dynamic environments via discounting. In International Conference on Machine Learning. PMLR, 2024. 10Understanding Adam Optimizer via Online Learning of Updates Kaiqi Jiang, Dhruv Malik, and Yuanzhi Li. How does adap- tive optimization impact local neural network geometry? In Advances in Neural Information Processing Systems (NeurIPS), 2023. Adam Kalai and Santosh Vempala. Efficient algorithms for online decision problems. Journal of Computer and System Sciences (JCSS), 71(3):291–307, 2005. Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2014. Frederik Kunstner, Jacques Chen, Jonathan Wilder Laving- ton, and Mark Schmidt. Noise is not the main factor behind the gap between SGD and Adam on Transformers, but sign descent might be. In International Conference on Learning Representations (ICLR), 2023. Frederik Kunstner, Robin Yadav, Alan Milligan, Mark Schmidt, and Alberto Bietti. Heavy-tailed class imbal- ance and why adam outperforms gradient descent on lan- guage models. arXiv preprint arXiv:2402.19449, 2024. Haochuan Li, Alexander Rakhlin, and Ali Jadbabaie. Con- vergence of Adam under relaxed assumptions. In Advances in Neural Information Processing Systems (NeurIPS), 2023. Xiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with adaptive stepsizes. In International Conference on Artificial Intelligence and Statistics (AISTATS), pages 983–992. PMLR, 2019. H Brendan McMahan. A survey of algorithms and analysis for adaptive online learning. The Journal of Machine Learning Research (JMLR), 18(1):3117–3166, 2017. H. Brendan McMahan and Matthew J. Streeter. Adaptive bound optimization for online convex optimization. In Conference on Learning Theory (COLT), pages 244–256. Omnipress, 2010. Yurii Nesterov. Primal-dual subgradient methods for convex problems. Mathematical Programming, 120(1):221–259, 2009. Francesco Orabona. A modern introduction to online learn- ing. arXiv preprint arXiv:1912.13213, 2019. Francesco Orabona and D´avid P´al. Scale-free online learn- ing. Theoretical Computer Science, 716:50–69, 2018. Yan Pan and Yuanzhi Li. Toward understanding why Adam converges faster than SGD for Transformers. arXiv preprint arXiv:2306.00204, 2023. Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of Adam and beyond. In International Conference on Learning Representations (ICLR), 2018. Shai Shalev-Shwartz and Yoram Singer. Online learn- ing meets optimization in the dual. In Conference on Learning Theory (COLT), volume 4005, pages 423–437. Springer, 2006. Tijmen Tieleman and Geoffrey Hinton. RMSProp: Divide the gradient by a running average of its recent magnitude. COURSERA Neural Networks Mach. Learn, 17, 2012. Tim van Erven. Why FTRL is better than Online Mirror De- scent. Personal Blog Post (Mathematics of Machine learn- ing), 2021. URL https://www.timvanerven. nl/blog/ftrl-vs-omd/#fn:unbounded. Bohan Wang, Jingwen Fu, Huishuai Zhang, Nanning Zheng, and Wei Chen. Closing the gap between the upper bound and lower bound of Adam’s iteration complexity. In Advances in Neural Information Processing Systems (NeurIPS), 2023. Rachel Ward, Xiaoxia Wu, and Leon Bottou. AdaGrad step- sizes: Sharp convergence over nonconvex landscapes. In International Conference on Machine Learning (ICML), pages 6677–6686. PMLR, 2019. Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012. Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates training: A theoretical justification for adaptivity. In International Conference on Learning Representations (ICLR), 2020a. Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank Reddi, Sanjiv Kumar, and Su- vrit Sra. Why are adaptive methods good for attention models? In Advances in Neural Information Process- ing Systems (NeurIPS), volume 33, pages 15383–15393, 2020b. Lijun Zhang, Shiyin Lu, and Zhi-Hua Zhou. Adaptive online learning in dynamic environments. InAdvances in Neural Information Processing Systems (NeurIPS), pages 1330– 1340, 2018a. Lijun Zhang, Tianbao Yang, and Zhi-Hua Zhou. Dynamic regret of strongly adaptive methods. In International Conference on Machine Learning (ICML), pages 5882– 5891. PMLR, 2018b. Qinzi Zhang and Ashok Cutkosky. Random scaling and momentum for non-smooth non-convex optimization. In International Conference on Machine Learning. PMLR, 2024. 11Understanding Adam Optimizer via Online Learning of Updates Yushun Zhang, Congliang Chen, Naichen Shi, Ruoyu Sun, and Zhi-Quan Luo. Adam can converge without any modification on update rules. In Advances in Neural Information Processing Systems (NeurIPS), volume 35, pages 28386–28399, 2022. Zhiyu Zhang, Ashok Cutkosky, and Ioannis Ch Paschalidis. Unconstrained dynamic regret via sparse coding. arXiv preprint arXiv:2301.13349, 2023. Zhiyu Zhang, David Bombara, and Heng Yang. Discounted adaptive online prediction. In International Conference on Machine Learning (ICML). PMLR, 2024. Shuai Zheng and James T Kwok. Follow the moving leader in deep learning. In International Conference on Machine Learning (ICML), pages 4110–4119. PMLR, 2017. Zhiming Zhou, Qingru Zhang, Guansong Lu, Hongwei Wang, Weinan Zhang, and Yong Yu. Adashift: Decorre- lation and convergence of adaptive learning rate methods. In International Conference on Learning Representations (ICLR), 2019. Martin Zinkevich. Online convex programming and gen- eralized infinitesimal gradient ascent. In International Conference on Machine Learning (ICML) , pages 928– 936, 2003. Fangyu Zou, Li Shen, Zequn Jie, Weizhong Zhang, and Wei Liu. A sufficient condition for convergences of Adam and RMSProp. In The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11127– 11135, 2019. 12Understanding Adam Optimizer via Online Learning of Updates A. Analysis of scale-free FTRL Recall from Section 2 that our construction starts with a gradient adaptive FTRL algorithm called scale-free FTRL (Orabona and P´al, 2018). This section presents a self-contained proof of its undiscounted static regret bound. Formally, we consider the 1D OLO problem introduced at the beginning of Section 2. Scale-free FTRL is defined as FTRL with the step size ηt = α/√Pt s=1 v2s, where α >0 is a scaling factor. Equivalently, it has the update rule ∆t = arg min x \" 1 ηt |x|2 + tX s=1 vsx # = −ηt tX s=1 vs = −α Pt s=1 vsqPt s=1 v2s . For well-posedness, if the denominator qPt s=1 v2s = 0, then we set the update to be ∆t = 0. Theorem A.1 (Static regret of scale-free FTRL). For all T > 0, loss sequence v1:T and comparator u ∈ R, scale-free FTRL guarantees the following static regret bound TX t=1 vt(∆t−1 − u) ≤ \u0012u2 2α + √ 2α \u0013vuut TX t=1 v2 t + 2 \u0012 max t∈[1,T] |∆t| \u0013\u0012 max t∈[1,T] |vt| \u0013 . We remark that van Erven (2021) directly applies the clipping technique from (Cutkosky, 2019) to obtain a similar regret bound as Theorem A.1, but in this way the associated algorithm is scale-free FTRL on the “clipped” gradients, rather than scale-FTRL itself. In contrast, we analyze the original scale-free FTRL algorithm for the purpose of explaining Adam (since in practice, Adam does not use the Cutkosky-style clipping on the stochastic gradients). This requires a slightly more involved analysis. Comparison with (Orabona and P´al, 2018). Before proving this theorem, we compare our regret bound with that of scale-free FTRL from (Orabona and P´al, 2018, Theorem 1). Their regret bound in the unconstrained domain setting (which means the domain diameter D defined in their Theorem 1 is infinite) is TX t=1 vt(∆t−1 − u) ≤ O  \u0000 u2 + 1 \u0001 vuut TX t=1 v2 t + √ T max t∈[1,T] |vt|   . Our bound replaces the √ T-factor by the maximum output magnitude (i.e., maxt∈[1,T] |∆t|), and our is better since |∆t| = α \f\f\fPt s=1 vs \f\f\f qPt s=1 v2s ≤ α √ t , which follows from the Cauchy-Schwarz inequality. We need such an improvement because in the discounted setting, the scaled loss sequence will have rapidly growing magnitude, which means this Cauchy-Schwarz step would be quite loose. Our proof makes a nontrivial use of the gradient clipping technique from (Cutkosky, 2019), which is also different from (Orabona and P´al, 2018, Theorem 1) and could be of independent interest. However, we acknowledge that directly modifying the argument of (Orabona and P´al, 2018) might achieve a similar goal. A.1. Proof of Theorem A.1 On the high level, the proof carefully combines the standard FTRL analysis, e.g., (Orabona, 2019, Lemma 7.1), and the gradient clipping technique of (Cutkosky, 2019). Step 1. We start with a preparatory step. Let τ be the index such that vt = 0 for all t ≤ τ, and vτ+1 ̸= 0. Without loss of generality, assume T > τ. Then, TX t=1 vt(∆t−1 − u) = TX t=τ+1 vt(∆t−1 − u) . 13Understanding Adam Optimizer via Online Learning of Updates On the RHS we have ∆τ = 0, and for all t > τ, ∆t is now well-defined by the “nice” gradient adaptive update rule (i.e., the denominator does not cause a problem) ∆t = −ηt tX s=1 vs = −α Pt s=1 vsqPt s=1 v2s . To proceed, for all t > τ, we define Ft(x) = 1 2ηt |x|2 + Pt s=1 vsx, which means that ∆t = arg minx Ft(x). Trivially, at the time index τ, we define Fτ (x) = 0 for all x. Step 2. The main part of the proof starts from the standard FTRL equality (Orabona, 2019, Lemma 7.1), TX t=1 vt(∆t−1 − u) = TX t=1 vt∆t−1 − \u0012 FT (u) − 1 ηT |u|2 \u0013 = 1 ηT |u|2 + T−1X t=τ [Ft(∆t) − Ft+1(∆t+1) + vt+1∆t] + FT (∆T ) − FT (u) ≤ 1 ηT |u|2 + T−1X t=τ [Ft(∆t) − Ft+1(∆t+1) + vt+1∆t] , where the last inequality follows since ∆T = arg minx FT (x). Consider the terms Ft(∆t) − Ft+1(∆t+1) + vt+1∆t in the above sum. Let us define the clipped gradient evt := clip√Pt−1 s=1 v2 s (vt) , where for any D ≥ 0, clipD(x) := x min( D |x|, 1). • For all t > τ, we have Ft(∆t) − Ft+1(∆t+1) + vt+1∆t = Ft(∆t) + vt+1∆t − Ft(∆t+1) − vt+1∆t+1 + 1 2ηt |∆t+1|2 − 1 2ηt+1 |∆t+1|2 ≤ Ft(∆t) + vt+1∆t − Ft(∆t+1) − vt+1∆t+1 ≤ Ft(∆t) + evt+1∆t − Ft(∆t+1) − evt+1∆t+1 + |vt+1 − evt+1|(|∆t| + |∆t+1|) . Following a standard fact of convex functions, e.g., (Orabona, 2019, Lemma 7.8), sinceFt(∆) + evt+1∆ is 1 ηt -strongly convex, it holds that Ft(∆t) + evt+1∆t − Ft(∆t+1) − evt+1∆t+1 ≤ Ft(∆t) + evt+1∆t − min ∆ [Ft(∆) + evt+1∆] ≤ ηt 2 ev2 t+1 . • As for the case of t = τ, since Fτ (∆τ ) = 0 and ∆τ = 0, Fτ (∆τ ) − Fτ+1(∆τ+1) + vτ+1∆τ = −Fτ+1(∆τ+1) = − 1 ητ+1 |∆τ+1|2 − τ+1X s=1 vs∆τ+1 ≤ −vτ+1∆τ+1 ≤ |vτ+1 − evτ+1||∆τ+1| . (evτ+1 = 0) Thus, we obtain the following bound: TX t=1 vt(∆t−1 − u) ≤ u2 2α vuut TX t=1 v2 t + α 2 T−1X t=τ+1 ev2 t+1qPt s=1 v2s + TX t=1 |vt − evt|(|∆t−1| + |∆t|) . 14Understanding Adam Optimizer via Online Learning of Updates Step 3. Finally, consider the two summation terms on the RHS one-by-one. We begin with the first term. ev2 t+1qPt s=1 v2s = √ 2 ev2 t+1q 2 Pt s=1 v2s ≤ √ 2 ev2 t+1q ev2 t+1 + Pt s=1 v2s = 2 √ 2 ev2 t+1 2 q ev2 t+1 + Pt s=1 v2s ≤ 2 √ 2 ev2 t+1q ev2 t+1 + Pt s=1 v2s + qPt s=1 v2s = 2 √ 2   vuutev2 t+1 + tX s=1 v2s − vuut tX s=1 v2s   . Thus, it follows that T−1X t=τ+1 ev2 t+1qPt s=1 v2s ≤ 2 √ 2 T−1X t=τ+1   vuutev2 t+1 + tX s=1 v2s − vuut tX s=1 v2s   ≤ 2 √ 2 T−1X t=τ+1   vuut t+1X s=1 v2s − vuut tX s=1 v2s   ≤ 2 √ 2 vuut TX t=1 v2 t . As for the second summation, we handle it similarly to (Cutkosky, 2019, Theorem 2). Defining Gt = maxs∈[1,t] |vs|, since |∆0| = 0, we have TX t=1 |vt − evt|(|∆t−1| + |∆t|) ≤ 2 \u0012 max t∈[1,T] |∆t| \u0013 TX t=1 |vt − evt| = 2 max \u0012 0, max t∈[1,T] |∆t| \u0013 TX t=1  |vt| − vuut t−1X s=1 v2s   ≤ 2 max \u0012 0, max t∈[1,T] |∆t| \u0013 TX t=1 (Gt − Gt−1) ≤ 2 \u0012 max t∈[1,T] |∆t| \u0013 GT . Combining the two upper bounds above completes the proof. B. Analysis of β-FTRL As discussed in Section 2, Adam corresponds to β-FTRL, the discounted version of scale-free FTRL, through the OLU framework. Thus, quantifying Adam’s performance comes down to analyzing the dynamic regret of β-FTRL. We now present the complete version of Theorem 3.1 (the dynamic regret bound of β-FTRL), fleshing out the proof sketch in Subsection 3.3. A main proof ingredient is our discounted-to-dynamic conversion. As a quick reminder, the formal setting considered is still the 1D OLO problem, where the output of the algorithm is denoted by ∆t ∈ R, and the loss function is denoted by ℓt(x) = vt+1x with vt+1 ∈ R. Step 1: Discounted regret. Our analysis starts with a concept called discounted regret, formalized in Definition 3.6. We recall the definition below for reader’s convenience. Definition B.1 (β-discounted regret). For any discounting factor β ∈ (0, 1], the β-discounted regret is defined as RT;β(u) := TX t=1 βT−tvt(∆t−1 − u) . When β = 1, the β-discounted regret recovers the standard static regret RT (u). The notational difference is simply an extra subscript β in the β-discounted regret, i.e., R·;β. Intuitively, β-FTRL should achieve good β-discounted regret, as long as scale-free FTRL achieves good static regret. This intuition follows from observations that β-FTRL is just scale-free FTRL with the “discounted losses” vt ← β−tvt, and that the β-discounted regret considers the loss sequence β−tvt instead of vt. We formalize this with the proof in Subsection B.1. 15Understanding Adam Optimizer via Online Learning of Updates Theorem B.2 (Discounted regret of β-FTRL). For all T > 0, loss sequence v1:T and comparator u ∈ R, β-FTRL guarantees the β-discounted regret bound RT;β(u) ≤ \u0012u2 2α + √ 2α \u0013vuut TX t=1 (βT−tvt)2 + 2 \u0012 max t∈[1,T] |∆t| \u0013\u0012 max t∈[1,T] |βT−tvt| \u0013 . Step 2: Discounted-to-dynamic conversion. The remaining task is to convert this discounted regret bound to a dynamic regret bound. We accomplish this via a general discounted-to-dynamic conversion, which is of independent interest. The idea is to partition the entire time horizon into subintervals, and then consider static comparators on each of them. To be precise, we consider a partition of [1, T] denoted by SN i=1[ai, bi], where a1 = 1, bi + 1 = ai+1 for all 1 ≤ i < N− 1, and bN = T. Then each partitioned interval [ai, bi] is coupled with an arbitrary fixed comparator ¯ui. We remark that this conversion is independent of the algorithm. For an algorithm A, RA T;β(u) and RA T (u0:T−1) denote its β-discounted regret (Definition 3.6) and its dynamic regret (2.1), respectively. See Subsection B.2 for the proof. Theorem B.3 (Discounted-to-dynamic conversion). Consider an arbitrary 1D OLO algorithm A. For all T >0, loss sequence v1:T and comparator sequence u0:T−1, the dynamic regret of A satisfies RA T (u0:T−1) = βRA T;β(¯uN ) + (1− β) NX i=1 X t∈[ai,bi] RA t;β(¯ui) + β N−1X i=1 \" biX t=1 βbi−tvt ! (¯ui+1 − ¯ui) # + NX i=1 X t∈[ai,bi] vt(¯ui − ut−1) , where SN i=1[ai, bi] is an arbitrary partition of of [1, T], and ¯u1, . . .¯uN ∈ R are also arbitrary. The remarkable aspect of this result is that it is an equality. That is, we do not lose anything through the conversion. Given a discounted regret bound of A, we can make use of this conversion by substituting RA T;β(¯uN ) and RA t;β(¯ui) with their discounted regret bounds, and then taking the infimum on the RHS w.r.t.the partition ∪i∈[N][ai, bi] and the choice of the “approximated comparator sequence” ¯u1, . . . ,¯uN . Step 3: Plugging in β-FTRL. Now we set A in the conversion to β-FTRL. See Subsection B.3 for the proof. Theorem B.4 (Dynamic regret of β-FTRL). Consider β-FTRL with a fixed α >0. Consider any loss sequence v1:T and any comparator sequence u0:T−1 s.t. |ut| ≤U. The dynamic regret (2.1) of the β-FTRL is bounded as RT (u0:T−1) ≤ \u0012U2 2α + √ 2α \u0013\" β q Vβ(v1:T ) + (1− β) TX t=1 q Vβ(v1:t) # + 2β \u0012 max t∈[1,T] |∆t| ·max t∈[1,T] |βT−tvt| \u0013 + 2(1 − β) TX t=1 \u0012 max s∈[1,t] |∆s| ·max s∈[1,t] |βt−svs| \u0013 + VARIATION , where Vβ(v1:t) := Pt s=1(βt−svs)2 is the discounted variance of the losses and VARIATION := inf   β N−1X i=1  biX t=1 βbi−tvt ! (¯ui+1 − ¯ui) + NX i=1 X t∈[ai,bi] vt(¯ui − ut−1)    , and the infimum in VARIATION is taken over all partitions SN i=1[ai, bi] of [1, T] and all choices of {¯ui}i∈N satisfying |¯ui| ≤U. 16Understanding Adam Optimizer via Online Learning of Updates In Theorem B.4, the variation term VARIATION consists of two terms. The first part β N−1X i=1  biX t=1 βbi−tvt ! (¯ui+1 − ¯ui) measures how fast the representative comparators ¯ui’s change across different subintervals, and we hence call it the “inter-partition variation”. The second term NX i=1 X t∈[ai,bi] vt(¯ui − ut−1) measures how different ut’s are from the representative comparators ¯ui’s within each subinterval, and we call it the “intra- partition variation”. A notable strength of the variation term is that it is the infimum over all partitions and ¯ui’s. In other words, the upper bound will automatically adapt to the best choice of partitions and ¯ui’s without knowing them explicitly. For instance, choosing ¯ui = P t∈[ai,bi] vtut−1 P t∈[ai,bi] vt would make the intra-partition variation term zero. Referring to Theorem B.4, we immediately obtain its simplified version in the main text, Theorem 3.1 with the proof in Subsection B.4. For the case of bounded comparators (β-FTRLD), the dynamic regret can be analyzed using almost the same strategy, which leads us to state Theorem 3.2 with the proof in Subsection B.5. B.1. Proof of Theorem B.2 β-FTRL is equivalent to scale-free FTRL with vt ← β−tvt. Therefore, applying Theorem A.1 with vt ← β−tvt leads to TX t=1 β−tvt(∆t−1 − u) ≤ \u0012u2 2α + √ 2α \u0013vuut TX t=1 (β−tvt)2 + 2 \u0012 max t∈[1,T] |∆t| \u0013\u0012 max t∈[1,T] \f\fβ−tvt \f\f \u0013 . Multiplying both sides by βT completes the proof. B.2. Proof of Theorem B.3 Overall, the proof draws inspiration from (Zhang et al., 2018b), where a similar partitioning argument was used to prove a dynamic regret guarantee of a strongly adaptive online learner (Daniely et al., 2015). Throughout the proof, we will omit the superscript A for brevity, since our argument is independent of specific algorithms. We start with a simple fact that connects the dynamic regret to the subinterval static regret. For any partition SN i=1[ai, bi] of [1, T] and any choices of {¯ui}i∈N , RT (u0:T−1) = NX i=1 biX t=ai vt(∆t−1 − ¯ui) + NX i=1 biX t=ai vt(¯ui − ut−1) . (B.1) To handle the static regret on the RHS, we use the following result. Lemma B.5. On any subinterval [a, b] ⊂ [1, T], with any u ∈ R, bX t=a vt(∆t−1 − u) = (1 − β) bX t=a Rt;β(u) + β (Rb;β(u) − Ra−1;β(u)) . Proof. For all t, notice that Rt;β(u) = tX s=1 βt−svs(∆s−1 − u) and Rt−1;β(u) = t−1X s=1 βt−svs(∆s−1 − u) , 17Understanding Adam Optimizer via Online Learning of Updates and thus Rt;β(u) − βRt−1;β(u) = vt(∆t−1 − u) . Summing over t ∈ [a, b], bX t=a vt(∆t−1 − u) = bX t=a Rt;β(u) − β bX t=a Rt−1;β(u) = (1 − β) bX t=a Rt;β(u) − βRa−1;β(u) + βRb;β(u) . Next, applying Lemma B.5 to each [ai, bi] in (B.1) yields: RT (u0:T−1) = (1 − β) NX i=1 X t∈[ai,bi] Rt;β(¯ui) + β NX i=1 [Rbi;β(¯ui) − Rai−1;β(¯ui)] + NX i=1 X t∈[ai,bi] vt(¯ui − ut−1) . Since ai − 1 = bi−1, the second term on the RHS can be rewritten as NX i=1 [Rbi;β(¯ui) − Rai−1;β(¯ui)] = RT;β(¯uN ) + N−1X i=1 [Rbi;β(¯ui) − Rbi;β(¯ui+1)] = RT;β(¯uN ) + N−1X i=1 \" biX t=1 βbi−tvt ! (¯ui+1 − ¯ui) # . Combining everything above completes the proof. B.3. Proof of Theorem B.4 Due to Theorem B.3, for any “approximated comparator sequence” ¯u1, . . . ,¯uN ∈ R with |¯ui| ≤U for i ∈ [N], we have RA T (u0:T−1) = βRA T;β(¯uN ) + (1− β) NX i=1 X t∈[ai,bi] RA t;β(¯ui) + β N−1X i=1 \" biX t=1 βbi−tvt ! (¯ui+1 − ¯ui) # + NX i=1 X t∈[ai,bi] vt(¯ui − ut−1) . Using Theorem B.2 and |¯ui| ≤U, RA T;β(¯uN ) ≤ \u0012¯u2 N 2α + √ 2α \u0013vuut TX t=1 (βT−tvt)2 + 2 \u0012 max t∈[1,T] |∆t| \u0013\u0012 max t∈[1:T] |βT−tvt| \u0013 ≤ \u0012U2 2α + √ 2α \u0013q Vβ(v1:T ) + 2 max t∈[1,T] |∆t| ·max t∈[1:T] |βT−tvt|, and similarly, for any t and ¯ui, RA t;β(¯ui) ≤ \u0012U2 2α + √ 2α \u0013q Vβ(v1:t) + 2 max s∈[1,t] |∆s| ·max s∈[1:t] |βt−svs|. Putting these bounds into the equality and taking the infimum on the RHS (over the partition and the {¯ui}i∈N sequence satisfying |¯ui| ≤U) complete the proof. 18Understanding Adam Optimizer via Online Learning of Updates B.4. Simplification for unbounded domain: Theorem 3.1 Theorem 3.1 follows as a corollary of Theorem B.4 with the partition ST t=1{t}, U = αDβ and ¯ut = ut for all t. With such choices, we have VARIATION ≤ β T−1X t=1  tX s=1 βt−svs ! (ut − ut−1) . Recall that Mβ := maxt∈[1,T] | Pt s=1 βt−svs|√Pt s=1(βt−svs)2 . Hence, it follows that \f\f\f\f\f tX s=1 βt−svs \f\f\f\f\f ≤ Mβ q Vβ(v1:t) . Therefore, the VARIATION term in Theorem B.4 is reduced to VARIATION ≤ βMβ T−1X t=1 q Vβ(v1:t) |ut − ut+1| , and thus the bound becomes (notice that U = αMβ and β <1) RT (u0:T−1) ≤ \u00121 2αM2 β + √ 2α \u0013\"q Vβ(v1:T ) + (1− β) TX t=1 q Vβ(v1:t) # + 2αMβG [1 + (1− β)T] + Mβ T−1X t=1 q Vβ(v1:t) |ut − ut−1| . (B.2) For all t, using β <1 Vβ(v1:t) = tX s=1 (βt−svs)2 ≤ G2 ∞X i=0 β2i = G2 1 − β2 < G2 1 − β . Therefore, RT (u0:T−1) ≤ \u00121 2αM2 β + √ 2α \u0013\u0014 G√1 − β + p 1 − βGT \u0015 + 2αMβG [1 + (1− β)T] + MβGP√1 − β = O \u0012\u0000 α + αM2 β + MβP \u0001 G√1 − β + \u0000 α + αM2 β \u0001p 1 − βGT \u0013 . Hence, we arrive at Theorem 3.1 presented in the main paper. B.5. Simplification for bounded domain Theorem 3.2 For the case of bounded comparators, i.e., |ut| ≤D, we consider β-FTRLD, the D-clipped version of β-FTRL: ∆t = −clipD  α Pt s=1 βt−svsqPt s=1(βt−svs)2   , (β-FTRLD) where clipD(x) = x min( D |x|, 1). With β-FTRLD, since |∆t| ≤D at each step, the following regret bound holds. The proof boils down to verifying that the entire proof strategy of Theorem B.2 goes through even with projection. Theorem B.6 (Discounted regret of β-FTRLD). For allT >0, loss sequence v1:T and comparator |u| ≤D, the discounted regret bound of β-FTRLD is RT;β(u) ≤ \u0012u2 2α + √ 2α \u0013vuut TX t=1 (βT−tvt)2 + 2D \u0012 max t∈[1:T] |βT−tvt| \u0013 . 19Understanding Adam Optimizer via Online Learning of Updates Now based on this discounted regret bound, we prove the claimed dynamic regret bound in Theorem 3.2. Similar to the proof of Theorem 3.1, from Theorem B.4, we choose the partition to be ∪T t=1{t}, and let U = D and ¯ut = ut for all t. With such choices, we have VARIATION ≤ β T−1X t=1 \f\f\f\f\f tX s=1 βt−svs \f\f\f\f\f|ut − ut−1| . Therefore, the upper bound in Theorem B.4 becomes (notice that α = U = D and β <1) RT (u0:T−1) ≤ 2D \"q Vβ(v1:T ) + (1− β) TX t=1 q Vβ(v1:t) # + 2DG [1 + (1− β)T] + β T−1X t=1 \f\f\f\f\f tX s=1 βt−svs \f\f\f\f\f|ut − ut−1| ≤ 4D \"q Vβ(v1:T ) + (1− β) TX t=1 q Vβ(v1:t) # + T−1X t=1 \f\f\f\f\f tX s=1 βt−svs \f\f\f\f\f|ut − ut−1| . Now notice that for all t, since β <1, Vβ(v1:t) = tX s=1 (βt−svs)2 ≤ G2 ∞X i=0 β2i = G2 1 − β2 < G2 1 − β , and \f\f\f\f\f tX s=1 βt−svs \f\f\f\f\f ≤ G ∞X t=0 βt ≤ G 1 − β . Substituting these bounds back to the bound on RT (u0:T−1), we obtain RT (u0:T−1) ≤ 4DG \u0012 1√1 − β + p 1 − β · T \u0013 + GP 1 − β . Therefore, we arrive at Theorem 3.2 presented in the main paper. C. Benefits of momentum and discounting factor This section presents omitted details from Subsection 3.2. The goal is to justify the benefits of Adam’s algorithmic components, namely the momentum and the discounting factor. C.1. Proof of lower bounds (Theorem 3.3) For simplicity, we start by assuming T is a multiple of 4. Consider the following loss sequence: For 1 ≤ t ≤ T/2, vt = ( (1, 0) for t even, (0, 1) for t odd. For T/2 < t≤ T, vt = ( (−1, 0) for t even, (0, −1) for t odd. The comparator sequence u0:T−1 is given as ut = (−1, −1) for 0 ≤ t ≤ T/2 − 1 and ut = (1, 1) for t ≥ T/2. Then, we have PT t=1⟨vt, ut−1⟩ = −T. As for the total loss, • Consider the baseline (3.2). Since vt[i]vt+1[i] = 0 for all t ≥ 1 and i = 1, 2, we have TX t=1 ⟨vt, ∆t−1⟩ = TX t=1 2X i=1 vt[i]∆t−1[i] = − TX t=1 2X i=1 αt−1[i]vt−1[i]vt[i] = 0. 20Understanding Adam Optimizer via Online Learning of Updates • Consider β-FTRLD with β = 1 and D = 1. Recall its coordinate-wise update rule, ∆t[i] = −clip1  α Pt s=1 vs[i]qPt s=1 vs[i]2   . From the loss sequence, it follows that Pt s=1 vs[i] ≥ 0 for all t, and hence, we have −1 ≤ ∆t[i] ≤ 0 for all t ≥ 0 and i = 1, 2. Hence, PT t=1⟨vt, ∆t−1⟩ ≥ −T/2. This completes the proof under the assumption that T is a multiple of 4. For general T, let bT be the largest integer less or equal to T which is a multiple of 4. Then, we define v1: bT and u1: bT−1 as the aforementioned loss and comparator sequences (with T replaced by bT), and this yields lower bounds on PbT t=1⟨vt, ∆t−1 − ut−1⟩. As for the time index satisfying bT < t≤ T, we define vt = (0, 0) and ut−1 = ubT−1. In this way, altogether, PT t=1⟨vt, ∆t−1 − ut−1⟩ = PbT t=1⟨vt, ∆t−1 − ut−1⟩, and the lower bounds for the latter can be applied. C.2. Proof of Corollary 3.4 Using Theorem 3.2 with β = 1 − cT−2/3, RT (u0:T−1) ≲ DG√1 − β + DG 1 − β + p 1 − βDGT (Theorem 3.2) = DGT 1/3 √c + P GT2/3 c + √cDGT 2/3 ≲ DGT 2/3c1/2 \u0010 1 + c−3/2P D \u0011 . With the optimal tuning c = Θ \u0000 (P/D) 2/3 \u0001 , it becomes O(GD 2/3P 1/3T 2/3). C.3. Proof of Corollary 3.5 Consider β < 1 first. Since the environment is well-behaved with constant M, we can invoke Theorem 3.1 with Mβ there replaced by M. Notice that M is independent of β, therefore at the end we may tune β using M. Concretely, using Theorem 3.1 with β = 1 − cT−1, RT (u0:T−1) ≲ \u0000 αM2 + MP \u0001 G√1 − β + p 1 − βαM 2GT (Theorem 3.1) = MG √ T c 1/2 \u0012αM + P c + αM \u0013 ≲ αM2G √ T c 1/2 \u0012 1 + c−1P αM \u0013 . With the optimal tuning c = Θ \u0000 P/(αM) \u0001 , it becomes O(α 1/2M 3/2GP 1/2T 1/2). Next, consider β = 1. We follow the same analysis in Subsection B.4 until (B.2), before plugging in any β. Then, instead of using β <1 there, we plug in β = 1, which yields R[0,T−1](u0:T−1) ≤ \u00121 2αM2 + √ 2α \u0013p V1(v1:T ) + 2αMG + M T−2X t=0 p V1(v1:t+1) |ut − ut+1| ≲ \u0010 αM2 + √ 2α \u0011 G √ T + MG √ T T−2X t=0 |ut − ut+1| ≲ MGP √ T . (T ≫ 1, and P ≫ αM) 21Understanding Adam Optimizer via Online Learning of Updates D. Details on optimization D.1. Proof of Theorem 4.1 Since F is differentiable, the fundamental theorem of calculus implies that for all x, y ∈ Rd, F(y) − F(x) = R1 0⟨∇F(x + t(y − x)), y − x⟩dt. Hence, we have F(wt+1) − F(wt) = Z 1 0 ⟨∇F(wt + s∆t), ∆t⟩ds = Es∼Unif([0,1])⟨∇F(wt + s∆t), ∆t⟩ Now, summing over t and telescoping yield the desired equality. 22",
      "meta_data": {
        "arxiv_id": "2402.01567v2",
        "authors": [
          "Kwangjun Ahn",
          "Zhiyu Zhang",
          "Yunbum Kook",
          "Yan Dai"
        ],
        "published_date": "2024-02-02T17:00:17Z",
        "pdf_url": "https://arxiv.org/pdf/2402.01567v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper provides a new online learning perspective on the Adam optimizer, identifying it as a form of Follow-the-Regularized-Leader (FTRL) specifically, a discounted FTRL (β-FTRL) well-suited for dynamic environments. It offers dynamic regret guarantees for β-FTRL, theoretically justifying the importance of Adam's momentum and discounting factors for achieving better optimization performance. The work introduces a novel \"discounted-to-dynamic conversion\" technique and demonstrates through theoretical analysis and a concrete classification problem that Adam's components are crucial for superior performance under sparse and non-stationary gradient conditions.",
        "methodology": "The core methodology involves the \"online learning of updates/increments\" (OLU) framework, where optimizers are treated as online learners choosing updates (∆t) based on past gradients. Adam is shown to correspond to a discounted instance of scale-free FTRL (β-FTRL), which aggregates past gradients with discounting factors by solving a convex optimization problem in each round. The performance of this online learner is analyzed using dynamic regret bounds, and a general \"discounted-to-dynamic conversion\" technique is proposed. Lower bounds are also established for non-momentum online learners (e.g., SGD, AdaGrad) and FTRL without discounting (β=1) to highlight the necessity of Adam's components.",
        "experimental_setup": "A theoretical 2D online linear optimization problem with a bounded domain ([−1, 1]^2) is constructed to derive lower bounds for baseline algorithms. For practical implications, a concrete classification problem is used, involving regularized hinge loss ℓ(x) = max(0, 1 − x) + λ|x| on sparse data. The dataset consists of coordinate vectors zi=ei (and zi=ciei for varying magnitudes) with positive labels yi=1. Experiments are initialized at w0 = 0, with a small learning rate η = 0.01, dimensionality d = 100, and regularization λ = 1/4. Comparisons are made between Adam (with β<1), SGD, and Adam (with β=1) across five random seeds, with results visualized in Figure 2.",
        "limitations": "The current analysis primarily focuses on the simplified case where both discounting factors in Adam are equal (β1 = β2), not fully exploring the precise effects of their common practical differentiation (β1 < β2). Additionally, the dynamic regret analysis of β-FTRL requires prior knowledge of environment parameters (e.g., path length, domain diameter, maximum gradient) for optimal tuning. The empirical evaluation, while illustrative, relies on an abstract scenario where the stochastic gradient sequence is fixed to match theoretical lower bound constructions, which is acknowledged to be not entirely practical.",
        "future_research_directions": "Future work includes investigating the precise role and benefits of using two distinct discounting factors (β1 vs. β2), especially given the common practice of setting β1 = 0.9 and β2 = 0.999. The OLU framework could be extended to analyze other popular optimizers such as RMSProp, AdaDelta, and Lion. Another direction is to design more practical algorithms by developing adaptive versions of β-FTRL that do not require prior environmental knowledge, possibly leveraging recent advancements in dynamic online learning. A fine-grained analysis of Adam's effectiveness in real-world scenarios, particularly for language models, by formally connecting heavy-tailed class imbalance to sparse and non-stationary gradients, is also suggested."
      }
    },
    {
      "title": "Online PAC-Bayes Learning",
      "abstract": "Most PAC-Bayesian bounds hold in the batch learning setting where data is\ncollected at once, prior to inference or prediction. This somewhat departs from\nmany contemporary learning problems where data streams are collected and the\nalgorithms must dynamically adjust. We prove new PAC-Bayesian bounds in this\nonline learning framework, leveraging an updated definition of regret, and we\nrevisit classical PAC-Bayesian results with a batch-to-online conversion,\nextending their remit to the case of dependent data. Our results hold for\nbounded losses, potentially \\emph{non-convex}, paving the way to promising\ndevelopments in online learning.",
      "full_text": "Online PAC-Bayes Learning Maxime Haddouche Inria and University College London France and UK Benjamin Guedj Inria and University College London France and UK Abstract Most PAC-Bayesian bounds hold in the batch learning setting where data is col- lected at once, prior to inference or prediction. This somewhat departs from many contemporary learning problems where data streams are collected and the algo- rithms must dynamically adjust. We prove new PAC-Bayesian bounds in this online learning framework, leveraging an updated deﬁnition of regret, and we revisit clas- sical PAC-Bayesian results with a batch-to-online conversion, extending their remit to the case of dependent data. Our results hold for bounded losses, potentially non-convex, paving the way to promising developments in online learning. 1 Introduction Batch learning is somewhat the dominant learning paradigm in which we aim to design the best predictor by collecting a training dataset which is then used for inference or prediction. Classical algorithms such as SVMs [see Cristianini et al., 2000, among many others] or feedforward neural networks [Svozil et al., 1997] are popular examples of efﬁcient batch learning. While the mathematics of batch learning constitute a vivid and well understood research ﬁeld, in practice this might not be aligned with the way practitionners collect data, which can be sequential when too much information is available at a given time (e.g. the number of micro-transactions made in ﬁnance on a daily basis). Indeed batch learning is not designed to properly handle dynamic systems. Online learning (OL) [Zinkevich, 2003, Shalev-Shwartz, 2012, Hazan, 2016] ﬁlls this gap by treating data as a continuous stream with a potentially changing learning goal. OL has been studied with convex optimisation tools and the celebrated notion of regret which measures the discrepancy between the cumulative sum of losses for a speciﬁc algorithm at each datum and the optimal strategy. It led to many fruitful results comparing the efﬁciency of prediction for optimisation algorithms such that Online Gradient Descent (OGD), Online Newton Step through static regret [Zinkevich, 2003, Hazan et al., 2007]. OL is ﬂexible enough to incorporate external expert advice onto classical algorithms with the optimistic point of view that such advices are useful for training [Rakhlin and Sridharan, 2013a,b] and then having optimistic regret bounds. Modern extensions also allow to compare to moving strategies through dynamic regret [see e.g. Yang et al., 2016, Zhang et al., 2018, Zhao et al., 2020]. However, this notion of regret has been challenged recently: for instance, Wintenberger [2021] chose to control an expected cumulative loss through PAC inequalities in order to deal with the case of stochastic loss functions. Statements holding with arbitrarily large probability are widely used in learning and especially within the PAC-Bayes theory. Since its emergence in the late 90s, the PAC-Bayes theory (see the seminal works of Shawe-Taylor and Williamson, 1997, McAllester, 1998, 1999 and the recent surveys by Guedj, 2019, Alquier, 2021) has been a powerful tool to obtain generalisation bounds and to derive efﬁcient learning algorithms. Classical PAC-Bayes generalisation bounds help to understand how a learning algorithm may perform on future similar batches of data. More precisely, PAC-Bayes learning exploits the Bayesian paradigm of explaining a learning problem through a meaningful distribution over a space of candidate predictors [see e.g. Maurer, 2004, Catoni, 2007, Tolstikhin and Seldin, 2013, Mhammedi et al., 2019]. An active line of research in PAC-Bayes learning is to 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2206.00024v2  [cs.LG]  13 Oct 2022overcome classical assumptions such that data-free prior, bounded loss, iid data [see Lever et al., 2010, 2013, Alquier and Guedj, 2018, Holland, 2019, Rivasplata et al., 2020, Haddouche et al., 2021, Guedj and Pujol, 2021] while remaining in a batch learning sprit. Finally, a pioneering line of work led by [Seldin et al., 2012a,b] on PAC-Bayes learning for martingales and independently developed by [Gerchinovitz, 2011, Foster et al., 2015, Li et al., 2018] boosted PAC-Bayes learning by providing sparsity regret bound, adaptive regret bounds and online algorithms for clustering. Our contributions. Our goal is to provide a general online framework for PAC-Bayesian learning. Our main contribution (Thm. 2.3 in Sec. 2) is a general bound which is then used to derive several online PAC-Bayesian results (as developed in Secs. 3 and 4). More speciﬁcally, we derive two types of bounds, online PAC-Bayesian training and test bounds. Training bounds exhibit online procedures while the test bound provide efﬁciency guarantees. We propose then several algorithms with their associated training and test bounds as well as a short series of experiments to evaluate the consistency of our online PAC-Bayesian approach. Our efﬁciency criterion is not the classical regret but an expected cumulative loss close to the one of Wintenberger [2021]. More precisely, Sec. 3 propose a stable yet time-consuming Gibbs-based algorithm, while Sec. 4 proposes time efﬁcient yet volatile algorithms. We emphasize that our PAC-Bayesian results only require a bounded loss to hold: no assumption is made on the data distribution, priors can be data-dependent and we do not require any convexity assumption on the loss, as commonly assumed in the OL framework. Outline. Sec. 2 introduces the theoretical framework as well as our main result. Sec. 3 presents an online PAC-Bayesian algorithm and draws links between PAC-Bayes and OL results. Sec. 4 details online PAC-Bayesian disintegrated procedures with reduced computational time and Sec. 5 gathers supporting experiments. We include reminders on OL and PAC-Bayes in Appendixes A.1 and C. Appendix B provide disucssion about our main result. All proofs are deferred to Appendix D. 2 An online PAC-Bayesian bound We establish a novel PAC-Bayesian theorem (which in turn will be particularised in Sec. 3) which overcomes the classical limitation of data-independent prior and iid data. We call our main result an online PAC-Bayesian bound as it allows to consider a sequence of priors which may depend on the past and a sequence of posteriors that can dynamically evolve as well. Indeed, we follow the online learning paradigm which considers a continous stream of data that the algorithm has to process on the ﬂy, adjusting its outputs at each time step w.r.t. the arrival of new data and the past. In the PAC-Bayesian framework, this paradigm translates as follows: from an initial (still data independent) prior Q1 = P and a data sample S = (z1,...,z m), we design a sequence of posterior (Qi)1≤i≤m where Qi = f(Q1,...,Q i−1,zi). Framework. Consider a data space Z (which can be only inputs or pairs of inputs/outputs). We ﬁx an integer m> 0 and our data sample S ∈Zm is drawn from an unknown distribution µ. We do not make any assumption on µ. We set a sequence of priors, starting with P1 = P a data-free distribution and (Pi)i≥2 such that for each i, Pi is Fi−1 measurable where (Fi)i≥0 is an adapted ﬁltration to S. For P,Q ∈M1 (H), the notation P ≪Qindicates that Qis absolutely continuous wrt P (i.e. Q(A) = 0 if P(A) = 0 for measurable A⊂H). We also denote by Qi our sequence of candidate posteriors. There is no restriction on what Qi could be. In what follows we ﬁx a ﬁltration (Fi)i≥0 and we denote by KL the Kullback-Leibler divergence between two distributions. We consider a predictor space H and a loss funtion ℓ: H ×Z →R+ bounded by a real constant K >0. This loss deﬁnes the (potentially moving) learning objective. We denote by M1(H) the set of all probability distributions on H. We now introduce the notion of stochastic kernel [Rivasplata et al., 2020] which formalise properly data-dependent measures within the PAC-Bayes framework. First, for a ﬁxed predictor space H, we set ΣH to be the considered σ-algebra on H. Deﬁnition 2.1 (Stochastic kernels). A stochastic kernel from S = Zm to H is deﬁned as a mapping Q: Zm ×ΣH →[0; 1]where • For any B ∈ΣH, the function S = (z1,...,z m) ↦→Q(S,B) is measurable, • For any S ∈Zm, the function B ↦→Q(S,B) is a probability measure over H. We denote by Stoch(S,H) the set of all stochastic kernels from S to H and for a ﬁxed S, we set QS := Q(S,.) the data-dependent prior associated to the sample Sthrough Q. 2From now, to refer to a distributionQS depending on a dataset S, we introduce a stochastic kernel Q(.,.) such that QS = Q(S,.). Note that this notation is perfectly suited to the case when QS is obtained from an algorithmic procedure A. In this case the stochastic kernel Qof interest is the learning algorithm A. We use this notion to characterise our sequence of priors. Deﬁnition 2.2. We say that a sequence of stochastic kernels (Pi)i=1..m is an online predictive sequence if (i) for all i≥1,S ∈Zm,Pi(S,.) is Fi−1 measurable and (ii) for all i≥2, Pi(S,.) ≫ Pi−1(S,.). Note that (ii) implies that for all i,Pi(S,.) ≫P1(S,.) with P1(S,.) a data-free measure (yet a classical prior in the PAC-Bayesian theory). We can now state our main result. Theorem 2.3. For any distributionµover Zm, any λ> 0 and any online predictive sequence (used as priors) (Pi), for any sequence of stochastic kernels(Qi) we have with probability1−δover the sample S ∼µ, the following, holding for the data-dependent measures Qi,S := Qi(S,.),Pi,S := Pi(S,.) : m∑ i=1 Ehi∼Qi,S [E[ℓ(hi,zi) |Fi−1]] ≤ m∑ i=1 Ehi∼Qi,S [ℓ(hi,zi)]+KL(Qi,S∥Pi,S) λ +λmK2 2 +log(1/δ) λ . Remark 2.4. For the sake of clarity, we assimilate in what follows the stochastic kernels Qi,Pi to the data-dependent distributions Qi(S,.),Pi(S,.). Then, an online predictive sequence is also assimilated to a sequence of data-dependent distributions. Concretely this leads to the switch of notation Qi,S →Qi in Thm. 2.3. The reason of this switch is that, even though stochastic kernel is the right theoretical structure to state our main result, we consider in Secs. 3 and 4 practical algorithmic extensions which focus only on data-dependent distributions, hence the need to alleviate our notations. The proof is deferred to Appendix D.1. See Appendix B for context and discussions. A batch to online conversion. First, we remark that our bound slightly exceeds the OL framework: indeed, it would require our posterior sequence to be an online predictive sequence as well, which is not the case here (for any i, the distribution Qi,S can depend on the whole dataset ). This is a consequence of our proof method (see Appendix D.1), which is classically denoted as a \"batch to online\" conversion (in opposition to the \"online to batch\" procedures as in Dekel and Singer, 2005). In other words, we exploited PAC-Bayesian tools designed for a ﬁxed batch of data to obtain a dynamic result. This is why we refer to our bound as online as it allows to consider sequences of priors and posteriors that can dynamically evolve. Analysis of the different terms in the bound. Our PAC-Bayesian bound formally differs in many points from the classical ones. On the left-hand side of the bound, the sum of the averaged expected loss conditioned to the past appears. Having such a sum of expectations instead of a single one is necessary to assess the quality of all our predictions. Indeed, because data may be dependent, one can not consider a single expectation as in the iid case. We also stress that taking an online predictive sequence as priors leads to control losses conditioned to the past, which differs from classical PAC-Bayes results designed to bound the expected loss. This term, while original in the PAC-Bayesian framework (to the best of our knowledge) recently appeared (in a modiﬁed form) in Wintenberger [2021, Prop 3]. See Appendix B.2 for further disucssions. On the right hand-side of the bound, online counterparts of classical PAC-Bayes terms appear. At time i, the measure Qi (i.e. Qi,S according to Remark 2.4) has a tradeoff to achieve between an overﬁtted prediction of zi (the case Qi = δzi where δis a Dirac measure) and a too weak impact of the new data with regards to our prior knowledge (the case Qi = Pi). The quantity λ >0 can be seen as a regulariser to adjust the relative impact of both terms. Inﬂuence of λ. The quantity λalso plays a crucial role on the bound as it is involved in an explicit tradeoff between the KL terms, the conﬁdence term log(1/δ) and the residual term mK2/2. This idea of seeing λas a trading parameter is not new [Thiemann et al., 2017, Germain et al., 2016]. However, the results from Thiemann et al. [2017] stand w.p. 1 −δfor any λwhile ours and the ones from Germain et al. [2016] hold for any λw.p. 1 −δwhich is weaker and implies to discretise R+ onto a grid to estimate the optimal λ. We now move on to the design of online PAC-Bayesian algorithms. 33 An online PAC-Bayesian procedure OL algorithms (we refer to Hazan, 2016 an introduction to the ﬁeld) are producing sequences of predictors by progressively updating the considered predictor (see Appendix A.1 for an example). Recall that, in the OL framework, an algorithm outputs at timeia predictor which is Fi−1-measurable. Here, our goal is to design an online procedure derived from Thm. 2.3 which outputs an online predictive sequence (which is assimilated, according to Remark 2.4, to a sequence of distributions). Online PAC-Bayesian (OPB) training bound. We state a corollary of our main result which paves the way to an online algorithm. This constructive procedure motivates the nameOnline PAC-Bayesian training bound (OPBT RAIN in short). Corollary 3.1 (OPBT RAIN ). For any distribution µover Zm, any λ> 0 and any online predictive sequences ˆQ,P, the following holds with probability 1 −δover the sample S ∼µ: m∑ i=1 Ehi∼ˆQi+1 [E[ℓ(hi,zi) |Fi−1]] ≤ m∑ i=1 Ehi∼ˆQi+1 [ℓ(hi,zi)] + KL( ˆQi+1∥Pi) λ + λmK2 2 + log(1/δ) λ . Here, λis seen as a scale parameter as precised below. The proof consists in applying Thm. 2.3 with for all i, Qi = ˆQi+1 and Pi. Note that in this case, our posterior sequence is an online predictive sequence in order to ﬁt with the OL framework. Corollary 3.1 suggests to design ˆQas follows, assuming we have drawn a dataset S = {z1,...,z m}, ﬁxed a scale parameter λ> 0 and an online predictive sequence Pi: ˆQ1 = P1, ∀i≥1 ˆQi+1 = argmin Q∈M1(H) Ehi∼Q [ℓ(hi,zi)] + KL(Q∥Pi) λ (1) which leads to the explicit formulation dˆQi+1 dPi (h) = exp (−λℓ(h,zi)) Eh∼Pi [exp (−λℓ(h,zi))]. (2) Thus, the formulation of Eq. (2), which has been highlighted by Catoni [2003, Sec. 5.1] shows that our online procedure produces Gibbs posteriors. So, PAC-Bayesian theory provides sound justiﬁcation for the somewhat intuitive online procedure in Eq. (1): at time i, we adjust our new measure ˆQi+1 by optimising a tradeoff between the impact of the newly arrived data zi and the one of prior knowledge ˆQi. Notice that ˆQis an online predictive sequence: ˆQi is Fi−1-measurable for all ias it depends only on ˆQi−1 and zi−1. Furthermore, one has ˆQi ≫ ˆQi−1 for all ias ˆQi is deﬁned as an argmin and the KL term is ﬁnite if and only it is absolutely continuous w.r.t. ˆQi−1. Remark 3.2. In Corollary 3.1, while the right hand-side is the reason we considered Eq. (1), the left hand side still needs to be analysed. It expresses how the posterior ˆQi+1 (designed from ˆQi,zi) generalises well on average to any new draw of zi. More precisely, this term measures how much the training of ˆQi+1 is overﬁtting on zi. A low value of it ensures our online predictive sequence, which is obtained from a single dataset, is robust to the randomness ofS, hence the interest of optimising the right hand side of the bound. This is a supplementary reason we refer to Corollary 3.1 as an OPBT RAIN bound as it provide robustness guarantees for our training. Online PAC-Bayesian (OPB) test bound.However, Corollary 3.1 does not say if ˆQi+1 will produce good predictors to minimise ℓ(.,zi+1), which is the objective of ˆQi+1 in the OL framework (we only have access to the past to predict the future). We then need to provide anOnline PAC-Bayesian (OPB) test bound (OPBT EST bound) to quantify our prediction’s accuracy. We now derive anOPBT EST bound from Thm. 2.3. 4Corollary 3.3 (OPBT EST ). . For any distribution µover Zm, any λ> 0, and any online predictive sequence ( ˆQi), the following holds with probability 1 −δover the sample S ∼µ: m∑ i=1 Ehi∼ˆQi [E[ℓ(hi,zi) |Fi−1]] ≤ m∑ i=1 Ehi∼ˆQi [ℓ(hi,zi)] + λmK2 2 + log(1/δ) λ . Optimising in λgives λ= √ 2 log(1/δ)/mK2 and ensure that: m∑ i=1 Ehi∼ˆQi [E[ℓ(hi,zi) |Fi−1]] ≤ m∑ i=1 Ehi∼ˆQi [ℓ(hi,zi)] + O (√ log(1/δ)K2m ) . The proof consists in applying Thm. 2.3 with for all i, Qi = ˆQi = Pi. Corollary 3.3 quantiﬁes how efﬁcient will our predictions be. Indeed, the left hand side of this bound relates for all i, how good ˆQi is to predict zi (on average) which is what ˆQi is designed for. Note that here, the involved λcan differ from the scale parameter of Eq. (1), it is now a way to compensate for the tradeoff between the two last terms of the bound. The strength of this bound is that since ˆQis an online predictive sequence, the Kullback-Leibler terms vanished, leaving terms depending only on hyperparameters. Links with previous approaches We now present a speciﬁc case of Corollary 3.1 where we choose as priors the online predictive sequence ˆQ(i.e. in Thm. 2.3, we choose Qi = ˆQi+1,Pi = ˆQi). The reason we focus on this speciﬁc case is that it enables to build strong links between PAC-Bayes and OL. We then adapt our OPBT RAIN bound (Corollary 3.1). The online procedure becomes: ˆQ1 = P, ∀i≥1 ˆQi+1 = argminQEhi∼Q [ℓ(hi,zi)] + KL(Q∥ˆQi) λ , (3) which leads to the explicit formulation dˆQi+1 dˆQi (h) = exp (−λℓ(h,zi)) Eh∼ˆQi [exp (−λℓ(h,zi))]. Links with classical PAC-Bayesian bounds. We denote that the optimal predictor in this case is such that at any time i, dˆQi+1(h) ∝ exp(−λℓ(h,zi))dˆQi(h) hence dˆQm+1(h) ∝ exp (−λ∑m i=1 ℓ(h,zi)) dˆQ1(h). One recognises, up to a multiplicative constant, the optimised predictor of Catoni [2007, Th 1.2.6] which solves argminQEh∼Q [ 1 m ∑m i=1 ℓ(h,zi)] + KL(Q∥ˆQ1) λ , thus one sees that in this case, the output of our online procedure aftermsteps coincides with Catoni’s output. This shows consistency of our general procedure which recovers classical result within an online framework: when too many data are available, treating data sequentially until time mleads to the same Gibbs posterior than if we were treating the whole dataset as a batch. Analogy with Online Gradient Descent (OGD). We propose an analogy between the procedure Eq. (3) and the celebrated OGD algorithm (see Appendix A.1 for a recap). First we remark that our minimisation problem is equivalent to argminQλEhi∼Q [ℓ(hi,zi)] + KL(Q∥ˆQi). Then we assume that for any i, ˆQi = N( ˆmi,Id) with ˆmi ∈Rd and we set Li( ˆmi) = Ehi∼ˆQi [ℓ(hi,zi)] . The minimisation problem becomes: argmin ˆmλLi( ˆm) + 1 2 ∥ˆm−ˆmi∥2. And so using the ﬁrst order Taylor expansion, we use the approximation Li( ˆm) ≈Li( ˆmi) + ⟨ˆm− ˆmi,∇Li( ˆmi)⟩which ﬁnally transform our argmin into the following optimisation process: ˆmi+1 = ˆmi −λ∇Li( ˆmi) which is exactly OGD on the loss sequence Li. We draw an analogy between the scale parameter λand the step size η in OGD. the KL term translates the inﬂuence of the previous point and the expected loss gives the gradient. This analogy has been already exploited in Shalev-Shwartz [2012] where they approximated Ehi∼qµ[ℓ(hi,zi)] := ¯Li(µ) ≈µT∇¯Li(µi) where µis their considered online predictive sequence. Finally, we remark that the optimum rate in Corollary 3.3 is aO(√m) which is comparable to the best rate of Shalev-Shwartz [2012, Eq (2.5)] (see proposition A.2). 5Comparison with previous work. We acknowledge that the procedure of Eq. (3) already appeared in literature. Li et al. [2018, Alg. 1] propose a Gibbs procedure somewhat similar to ours, the main difference being the addition of a surrogate of the true loss at each time step. Within the OL literature, the idea of updating measures online has been recently studied for instance in Chérief-Abdellatif et al. [2019]. More precisely, our procedure is similar to their Streaming Variational Bayes (SVB) algorithm. A slight difference is that they approximated the expected loss similarly to Shalev-Shwartz [2012]. The guarantees Chérief-Abdellatif et al. [2019] provided for SVB hold for Gaussian priors and comes at the cost of additional constraints that do not allow to consider any aggregation strategies contrary to what Corollary 3.1 propose. Their bounds are deterministic and are using tools and assumptions from convex optimisation (such that convex expected losses) while ours are probabilistic and are using measure theory tools which allow to relax these assumptions. Strength of our result. We emphasize two points. First, to the best of our knowledge, Corollary 3.1 is the ﬁrst bound which theoretically suggests Eq. (3) as a learning algorithm. Second, we stress that Eq. (3) is a particular case of Corollary 3.1 and our result can lead to other fruitful routes. For instance, we consider the idea of adding noise to our measures at each time step to avoid overﬁtting (this idea has been used e.g. in Neelakantan et al., 2015 in the context of deep neural networks): if our online predicitve sequence ( ˆQi) can be deﬁned through a sequence of parameter vectors ˆµ, then we can deﬁne Pi by adding a small noise on ˆµi and thus giving more freedom through stochasticity. Thus, we see that our procedure led us to the use of the Gibbs posteriors of Catoni. However, in practice, Gaussian distributions are preferred [e.g. Dziugaite and Roy, 2017, Rivasplata et al., 2019, Perez-Ortiz et al., 2021b,a, Pérez-Ortiz et al., 2021]). That is why we focus next on new online PAC-Bayesian algorithms involving Gaussian distributions. 4 Disintegrated online algorithms for Gaussian distributions. We dig deeper in the ﬁeld of disintegrated PAC-Bayesian bounds, originally explored by Catoni [2007], Blanchard and Fleuret [2007], further studied by Alquier and Biau [2013], Guedj and Alquier [2013] and recently developed by Rivasplata et al. [2020], Viallard et al. [2021] (see Appendix C for a short presentation of the bound we adapted and used). The strength of the disintegrated approach is that we have directly guarantees on the random draw of a single predictor, which avoids to consider expectations over the predictor space. This fact is particularly signiﬁcant in our work as the procedure precised in Eq. (2), require the estimation of an exponential moment to be efﬁcient, which may be costful. We then show that disintegrated PAC-Bayesian bounds can be adapted to the OL framework, and that they have the potential to generate proper online algorithms with weak computational cost and sound efﬁciency guarantees. Online PAC-Bayesian disintegrated (OPBD) training bounds. We present a general form for online PAC-Bayes disintegrated (OPBD) training bounds. The terminology comes from the way we craft those bounds: from PAC-Bayesian disintegrated bounds we use the same tools as in Thm. 2.3 to create the ﬁrst online PAC-Bayesian disintegrated bounds. OPBD training bounds have the following form. For any online predictive sequences ˆQ,P, any λ >0 w.p. 1 −δ over S ∼µand (h1,...,h m) ∼ ˆQ2 ⊗...⊗ˆQm+1: m∑ i=1 E[ℓ(hi,zi) |Fi−1] ≤ m∑ i=1 ℓ(hi,zi) + Ψ(hi, ˆQi+1,Pi) + Φ( m), (4) with Ψ,Φ being real-valued functions. Ψ controls the global behaviour of Qi+1 w.r.t. the Fi−1- measurable prior Pi. If one has no dependency on hi this behaviour is global, otherwise it is local. Note that those functions may depend on λ,δ. However, since they are ﬁxed parameters, we do not make these dependencies explicit. Similarly to Corollary 3.1, this kind of bound allows to derive a learning algorithm (cf Algorithm 1) which outputs an online predicitve sequence ˆQ. Finally we draw (h1,...,h m) ∼ ˆQ2 ⊗...⊗ˆQm+1 (and not ˆQ1 ⊗...⊗ˆQm) since an OPBD bound is designed to justify theoretically an OPBD procedure in the same way Corollary 3.1 allowed to justify Eq. (1). Why focus on Gaussian measures? The reason is that a Gaussian variable h∼N(w,σ2Id) can be written as h= w+ εwith ε∼N(0,σ2Id), and this expression totally deﬁnes h(Id being the identity matrix). 6A general OPBD algorithm for Gaussian measure with ﬁxed variance We use an idea presented in Viallard et al. [2021] which restrict the measure set to Gaussian on Rd with known and ﬁxed covariance matrix σ2Id. Then we present in Algorithm 1 a general algorithm (derived from an OPBD training bound) for Gaussian measures with ﬁxed variance which outputs a sequence of gaussian ˆQi = N( ˆwi,σ2Id) from a prior sequence Pi = N(w0 i,σ2Id) where for each i, w0 i is Fi−1- measurable. Because the variance is ﬁxed, the distribution is uniquely deﬁned by its mean, thus we identify ˆQi and ˆwi, Pi and w0 i. Algorithm 1: A general OPBD algorithm for Gaussian measures with ﬁxed variance. Parameters : Time m, scale parameter λ Initialisation :Variance σ2, Initial mean ˆw1 ∈Rd, epoch m 1 for each iteration iin 1..mdo 2 Observe zi,w0 i and draw εi ∼N(0,σ2Id) 3 Update: ˆwi+1 := argminw∈Rd ℓ(w+ εi,zi) + Ψ(w+ εi,w,w 0 i) 4 end 5 Return ( ˆwi)i=1..m+1 At each time i, Algorithm 1 requires the draw of εi ∼N(0,σ2Id). Doing so, we generated the randomness for our hi (because our bound holds for a single draw of(h1,..,h m) ∼ ˆQ2 ⊗...⊗ˆQm+1), we then write hi = w+ εi and we optimise w.r.t. Ψ to ﬁnd ˆwi+1. Bounds of interest. We present two possible choices of pairs (Ψ,Φ) derived from the disintegrated results presented in Appendix C. Doing so, we explicit two ready-to-use declinations of Algorithm 1. Corollary 4.1. For any distributionµover Zm, any online predictive sequences of Gaussian measures with ﬁxed variance ˆQi = N( ˆwi,σ2Id) and Pi = N(w0 i,σ2Id), any λ >0, w.p. 1 −δover S ∼µ and (hi = ˆwi+1 + εi)i=1..m ∼ ˆQ2 ⊗...⊗ˆQm+1, the bound of Eq. (4) holds for the two following pairs Ψ,Φ: Ψ1(hi, ˆwi+1,w0 i) = 1 λ (||ˆwi+1 + εi −w0 i||2 −||ε||2 2σ2 ) Φ1(m) = λmK2 2 + log(1/δ) λ , (5) Ψ2(hi, ˆwi+1,w0 i)) = 1 λ ||ˆwi+1 −w0 i||2 2σ2 Ψ2(m) = λmK2 + 3 log(1/δ) 2λ . (6) Where the notation 1,2 denote whether the functions have been derived from adapted theorems of Rivasplata et al., 2020, Viallard et al., 2021 recalled in Appendix C We then can use algorithm 1 with Eq. (5), Eq. (6). Proof is deferred to Appendix D.2. Note that in Corollary 4.1, we identiﬁed ˆQi to ˆwi and for the last formula, Ψ has no dependency on hi. Comparison with Eq. (1). The main difference with Eq. (1) provided by the disintegrated framework is that the optimisation route does not include an expected term within the optimisation objective. The main advantage is a weaker computational cost when we restrict to Gaussian distributions. The main weakness is a lack of stability as our algorithm now depends at time ion ℓ(h+ εi,zi) so on εi directly. We denote that Eq. (5) is less stable than Eq. (6) as it involves another dependency on εi through Ψ. The reason is that Rivasplata et al. [2020] proposed a bound involving a disintegrated KL divergence while Viallard et al. [2021] proposed a result involving a Rényi divergence avoiding a dependency on εi. We refer to Appendix C for a detailed statement of those properties. Comparison with van der Hoeven et al. [2018].Theorem 3 of van der Hoeven et al. [2018] recovers OGD from the exponential weights algorithm by taking a sequence of moving distributions being Gaussians with ﬁxed variance which is exactly what we consider here. From these, they retrieve the classical OGD algorithm as well as its classical convergence rate. Let us compare our results with theirs. First, if we ﬁx a single step ηin their bound and assume two traditional assumptions for OGD (a ﬁnite diameter D of the convex set and an uniform bound G on the loss gradients), we recover 7for the OGD (greedy GD in van der Hoeven et al., 2018) a rate of D2 2σ2η + ησ2TG2 2 . This is, up to constants and notation changes, exactly our Ψi (i ∈{1,2}). Also, we notice a difference in the way to use Gaussian distributions: Theorem 3 of van der Hoeven et al. [2018] is based on their Lemma 1 which provides guarantees for the expected regret. This is a clear incentive to consider as predictors the mean of the sucessive Gaussians of interest. On the contrary, Corollary 4.1 involves a supplementary level of randomness by considering predictors hi drawn from our Gaussians. This additional randomness appears in our optimisation process (algorithm 1). Finally, notice that van der Hoeven et al. [2018] based their whole work on the use of a KL divergence while Corollary 4.1 not only exploit a disintegrated KL (Ψ1) but also a Rényi α-divergence (Ψ2). Note that we propose a result only for α = 2 for the sake of space constraints but any other value of αleads to another optimisation objective to explore. OPBD test bounds. Similarly to what we did in Sec. 3, we also provideOPBD test bounds to provide efﬁciency guarantees for online predicitve sequences (e.g. the output of algorithm 1). Our proposed bounds have the following general form. For any online predictive sequence ˆQ, any λ> 0 w.p. 1−δover Sand (h1,...,h m) ∼ ˆQ1 ⊗...⊗ˆQm: m∑ i=1 E[ℓ(hi,zi) |Fi−1] ≤ m∑ i=1 ℓ(hi,zi) + Φ(m), (7) with Φ being a real-valued function(possibly dependent on λ,δ though it is not explicited here). Note that our predictors (h1,...,h m) are now drawn from ˆQ1 ⊗...⊗ˆQm. Thus, the left-hand side of the bound considers a hi drawn from an Fi−1-measurable distribution evaluated on ℓ(.,zi): this is effectively a measure of the prediction performance. We now state a corollary which gives disintegrated guarantees for any online predicitve sequence. Corollary 4.2. For any distributionµover Zm, any λ> 0, and any online predictive sequence ( ˆQi), the following holds with probability 1 −δover the sample S ∼µand the predictors (h1,...,h m) ∼ ˆQ1 ⊗...⊗ˆQm, the bound of Eq. (7) holds with : Φ1(m) = λmK2 2 + log(1/δ) λ , Φ2(m) = 2λmK2 + log(1/δ) λ . Where the notation 1,2 denote whether the functions have been derived from adapted theorems of Rivasplata et al., 2020, Viallard et al., 2021 recalled in Appendix C. The optimisedλgives in both cases a O( √ mlog(1/δ). Proof is deferred to Appendix D.2. 5 Experiments We adapt the experimental framework introduced in Chérief-Abdellatif et al. [2019, Sec.5] to our algorithms (anonymised code available here). We conduct experiments on several real-life datasets, in classiﬁcation and linear regression. Our objective is twofold: check the convergence of our learning methods and compare their efﬁciencies with classical algorithms. We ﬁrst introduce our experimental setup. Algorithms. We consider four online methods of interest: the OPB algorithm of Eq. (3) which update through time a Gibbs posterior. We instantiate it with two different priors ˆQ1: a Gaussian distribution and a Laplace one. We also implement Algorithm 1 with the functions Ψ1,Ψ2 from Corollary 4.1. To assess efﬁciency, we implement the classical OGD (as described in Alg. 1 of Zinkevich, 2003) and the SVB method of Chérief-Abdellatif et al. [2019]. Binary Classiﬁcation. At each round ithe learner receives a data point xi ∈Rd and predicts its label yi ∈{−1,+1}using ⟨xi,hi⟩, with hi = Eh∼ˆQi [h] for OPB methods or hi being drawn under ˆQi for OPBD methods. The adversary reveals the true value yi, then the learner suffers the loss ℓ(hi,zi) = ( 1 −yihT i xi ) + with zi = (xi,yi) and a+ = aif a> 0 and a+ = 0 otherwise. This loss is unbounded but can be thresholded. 8100 200 300 400 500 10 1 100 Loss Classification - Breast Cancer OGD Gibbs Gauss Gibbs Laplace OPBD Riva OPBD Via 100 200 300 400 500 600 700 100 5 × 10 1 6 × 10 1 7 × 10 1 8 × 10 1 9 × 10 1 Loss Classification - PIMA Indians OGD Gibbs Gauss Gibbs Laplace OPBD Riva OPBD Via 200 400 600 800 1000 10 1 100 Loss Regression - Boston Housing OGD SVB Gibbs Gauss Gibbs Laplace OPBD Riva OPBD Via 2500 5000 7500 10000 12500 15000 17500 20000 10 1 100 Loss Regression - California Housing OGD SVB OPBD Riva OPBD Via Figure 1: Averaged cumulative losses for all four considered datasets. ’Gibbs Gauss’ denotes OPB with Gaussian Prior, ’Gibbs Laplace’ denotes OPB with Laplace prior. ’OPBD Riva’ denotes OPBD with Ψ1, ’OPBD Via’ denotes OPBD withΨ2. Linear Regression. At each round i, the learner receives a set of features xi ∈Rd and predicts yi ∈R using ⟨xi,hi⟩with hi = Eh∼ˆQi [h] for SVB and OPB methods or hi being drawn under ˆQi for OPBD methods. Then the adversary reveals the true value yt and the learner suffers the loss ℓ(hi,zi) = ( yi −hT i xi )2 with zi = (xi,yi). This loss is unbounded but can be thresholded. Datasets. We consider four real world dataset: two for classiﬁcation (Breast Cancer and Pima Indians), and two for regression (Boston Housing and California Housing). All datasets except the Pima Indians have been directly extracted from sklearn [Pedregosa et al., 2011]. Breast Cancer dataset [Street et al., 1993] is available here and comes from the UCI ML repository as well as the Boston Housing dataset [Belsley et al., 2005] which can be obtained here. California Housing dataset [Pace and Barry, 1997] comes from the StatLib repository and is available here. Finally, Pima Indians dataset [Smith et al., 1988] has been recovered from this Kaggle repository. Note that we randomly permuted the observations to avoid to learn irrelevant human ordering of data (such that date or label). Parameter settings. We ran our experiments on a 2021 MacBookPro with an M1 chip and 16 Gb RAM. For OGD, the initialisation point is 0Rd and the values of the learning rates are set to η= 1/√m. For SVB, mean is initialised to 0Rd and covariance matrix to Diag(1). Step at time iis ηi = 0.1/ √ i. For both of the OPB algorithms with Gibbs posterior, we chose λ= 1/m. As priors, we took respectively a centered Gaussian vector with the covariance matrix Diag(σ2) (σ = 1.5) and an iid vector following the standard Laplace distribution. For the OPBD algorithm with Ψ1, we chose λ = 10−4/m, the initial mean is 0Rd and our ﬁxed covariance matrix is Diag(σ2) with σ = 3.10−3. For the OPBD algorithm with Ψ1, we chose λ= 2.10−3/m, the initial mean is 0Rd and our covariance matrix is Diag(σ2) with σ= 10−2. The reason of those higher scale parameters and variance is that Ψ from Rivasplata et al. [2020] is more stochastic (yet unstable) than the one Viallard et al. [2021]. Experimental results. For each dataset, we plot the evolution of the average cumulative loss∑ t i=1 ℓ(hi,zi) /tas a function of the step t= 1,...,m , where mis the dataset size and hi is the decision made by the learner hi at step i. The results are gathered in Fig. 1 Empirical ﬁndings. OPB with Gaussian prior (’Gibbs Gauss’) outperforms OGD on all datasets except California Housing (on which this method is not implemented ) while OPB with Laplace prior (’Gibbs Laplace’) always fail w.r.t. OGD. OPB methods fail to compete with SVB on the Boston 9Housing dataset. OPBD methods compete with SVB on regression problems and clearly outperforms OGD on classiﬁcation tasks. OPBD with Ψ2 (labeled as ’OPBD Via’ in Fig. 1) performs better on the California Housing dataset while OPBD with Ψ1 (labeled as ’OPBD Riva’) is more efﬁcient on the Boston Housing dataset. Both methods performs roughly equivalently on classiﬁcation tasks. This brief experimental validation shows the consistency of all our online procedures as we observe a visible decrease of the cumulative losses through time. It particularly shows that OPBD procedures improve on OGD on these dataset. We refer to Appendix E for additional table gathering the error bars of our OPBD methods. Why do we perform better than OGD? As stated in Sec. 4, OGD can be recovered as a Gaussian approximation of the exponential weights algorithm (EWA). Thus, a legitimate question is why do we perform better than OGD as our OPBD methods are also based on a Gaussian surrogate of EWA? van der Hoeven et al. [2018] only used Gaussians distributions with ﬁxed variance as a technical tool when the considered predictors are the Gaussian means. In our work, we exploited a richer characteristic of our distributions in the sense our predictors are points sampled from our Gaussians and not only the means. This also has consequences in our learning algorithm as at time iof our algorithm 1, our optimisation step involves a noise εi ∼N(0,σ2I). Thus, we believe that OPBD methods should perform at least as well as OGD. We write ’at least’ as we think that the higher ﬂexibility due to this additional level of randomness might result in slightly better empirical performances, as seen on the few datasets in Fig. 1. 6 Conclusion We establish links between Online Learning and PAC-Bayes. We show that PAC-bayesian bounds are useful to derive new OL algorithms. We also prove sound theoretical guarantees for such algorithms. We emphasise that all of our results stand for any general bounded loss, especially no convexity assumption is needed. Having no convexity assumption on the loss paves the way to exciting future practical studies, starting with Spiking Neural Network which is investigated in an online fashion (see Lobo et al., 2020 for a recent survey). A follow-up question on the theoretical part is whether we can relax the bounded loss assumption: we leave this for future work. Acknowledgements We thank several anonymous reviewers as well as the Area Chair of the Neurips comitee who provided insigthful comments and suggestions which greatly enhanced the quality of our comparisons with literature and of the discussions around our theorems. References P. Alquier. User-friendly introduction to PAC-Bayes bounds, 2021. URL https://arxiv.org/ abs/2110.11216. P. Alquier and G. Biau. Sparse single-index model. Journal of Machine Learning Research, 14(1), 2013. P. Alquier and B. Guedj. Simpler PAC-Bayesian bounds for hostile data. Machine Learning, 107(5): 887–902, 2018. ISSN 1573-0565. URL http://dx.doi.org/10.1007/s10994-017-5690-0 . P. Alquier, J. Ridgway, and N. Chopin. On the properties of variational approximations of gibbs posteriors. Journal of Machine Learning Research, 17(236):1–41, 2016. URL http://jmlr. org/papers/v17/15-290.html. D. A. Belsley, E. Kuh, and R. E. Welsch. Regression diagnostics: Identifying inﬂuential data and sources of collinearity. John Wiley & Sons, 2005. G. Blanchard and F. Fleuret. Occam’s hammer. In International Conference on Computational Learning Theory, pages 112–126. Springer, 2007. O. Catoni. A PAC-Bayesian approach to adaptive classiﬁcation. preprint, 840, 2003. 10O. Catoni. PAC-Bayesian Supervised Classiﬁcation: The Thermodynamics of Statistical Learning. Institute of Mathematical Statistics Lecture Notes—Monograph Series 56. IMS, Beachwood, OH. MR2483528, 5544465, 2007. B.-E. Chérief-Abdellatif, P. Alquier, and M. E. Khan. A generalization bound for online variational inference. In Asian Conference on Machine Learning, pages 662–677. PMLR, 2019. N. Cristianini, J. Shawe-Taylor, et al. An introduction to support vector machines and other kernel- based learning methods. Cambridge university press, 2000. O. Dekel and Y . Singer. Data-driven online to batch conversions.Advances in Neural Information Processing Systems, 18, 2005. G. K. Dziugaite and D. M. Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. arXiv preprint arXiv:1703.11008, 2017. D. J. Foster, A. Rakhlin, and K. Sridharan. Adaptive Online Learning. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors,Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015. URL https://proceedings.neurips.cc/paper/ 2015/file/19de10adbaa1b2ee13f77f679fa1483a-Paper.pdf. S. Gerchinovitz. Sparsity Regret Bounds for Individual Sequences in Online Linear Regression. In S. M. Kakade and U. von Luxburg, editors, Proceedings of the 24th Annual Conference on Learning Theory, volume 19 of Proceedings of Machine Learning Research , pages 377–396, Budapest, Hungary, 09–11 Jun 2011. PMLR. URL https://proceedings.mlr.press/v19/ gerchinovitz11a.html. P. Germain, F. Bach, A. Lacoste, and S. Lacoste-Julien. PAC-Bayesian theory meets Bayesian inference. Advances in Neural Information Processing Systems, 29, 2016. M. Gil, F. Alajaji, and T. Linder. Rényi divergence measures for commonly used univariate continuous distributions. Information Sciences, 249:124–131, 2013. B. Guedj. A Primer on PAC-Bayesian Learning. In Proceedings of the second congress of the French Mathematical Society, 2019. B. Guedj and P. Alquier. PAC-Bayesian estimation and prediction in sparse additive models.Electron. J. Statist., 7:264–291, 2013. doi: 10.1214/13-EJS771. URL https://doi.org/10.1214/ 13-EJS771. B. Guedj and L. Pujol. Still No Free Lunches: The Price to Pay for Tighter PAC-Bayes Bounds. Entropy, 23(11), 2021. ISSN 1099-4300. doi: 10.3390/e23111529. URL https://www.mdpi. com/1099-4300/23/11/1529. M. Haddouche, B. Guedj, O. Rivasplata, and J. Shawe-Taylor. PAC-Bayes unleashed: generalisation bounds with unbounded losses. Entropy, 23(10):1330, 2021. E. Hazan. Introduction to online convex optimization. Foundations and Trends® in Optimization, 2 (3-4):157–325, 2016. E. Hazan, A. Agarwal, and S. Kale. Logarithmic regret algorithms for online convex optimization. Machine Learning, 69(2):169–192, 2007. M. Holland. PAC-Bayes under potentially heavy tails. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 2715–2724. Curran Associates, Inc., 2019. URL http://papers.nips.cc/ paper/8539-pac-bayes-under-potentially-heavy-tails.pdf . G. Lever, F. Laviolette, and J. Shawe-Taylor. Distribution-Dependent PAC-Bayes Priors. In M. Hutter, F. Stephan, V . V ovk, and T. Zeugmann, editors,Algorithmic Learning Theory, pages 119–133, Berlin, Heidelberg, 2010. Springer Berlin Heidelberg. ISBN 978-3-642-16108-7. 11G. Lever, F. Laviolette, and J. Shawe-Taylor. Tighter PAC-Bayes Bounds through Distribution- Dependent Priors. Theor. Comput. Sci., 473:4–28, Feb. 2013. ISSN 0304-3975. URL https: //doi.org/10.1016/j.tcs.2012.10.013. L. Li, B. Guedj, and S. Loustau. A quasi-Bayesian perspective to online clustering. Electronic Journal of Statistics, 12(2):3071 – 3113, 2018. doi: 10.1214/18-EJS1479. URL https://doi. org/10.1214/18-EJS1479. J. L. Lobo, J. Del Ser, A. Bifet, and N. Kasabov. Spiking neural networks and online learning: An overview and perspectives. Neural Networks, 121:88–100, 2020. A. Maurer. A note on the PAC Bayesian theorem. arXiv preprint cs/0411099, 2004. D. A. McAllester. Some PAC-Bayesian theorems. In Proceedings of the eleventh annual conference on Computational Learning Theory, pages 230–234. ACM, 1998. D. A. McAllester. PAC-Bayesian model averaging. In Proceedings of the twelfth annual conference on Computational Learning Theory, pages 164–170. ACM, 1999. Z. Mhammedi, P. Grünwald, and B. Guedj. PAC-Bayes Un-Expected Bernstein In- equality. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32 , pages 12202–12213. Curran Associates, Inc., 2019. URL http://papers.nips.cc/paper/ 9387-pac-bayes-un-expected-bernstein-inequality.pdf . A. Neelakantan, L. Vilnis, Q. V . Le, I. Sutskever, L. Kaiser, K. Kurach, and J. Martens. Adding gradient noise improves learning for very deep networks. arXiv preprint arXiv:1511.06807, 2015. R. K. Pace and R. Barry. Sparse spatial autoregressions. Statistics & Probability Letters, 33(3): 291–297, 1997. F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten- hofer, R. Weiss, V . Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011. M. Perez-Ortiz, O. Rivasplata, B. Guedj, M. Gleeson, J. Zhang, J. Shawe-Taylor, M. Bober, and J. Kittler. Learning PAC-Bayes priors for probabilistic neural networks. Submitted., 2021a. URL https://arxiv.org/abs/2109.10304. M. Perez-Ortiz, O. Rivasplata, E. Parrado-Hernandez, B. Guedj, and J. Shawe-Taylor. Progress in self-certiﬁed neural networks. In NeurIPS 2021 workshop Bayesian Deep Learning [BDL], 2021b. URL http://bayesiandeeplearning.org/2021/papers/38.pdf. M. Pérez-Ortiz, O. Rivasplata, J. Shawe-Taylor, and C. Szepesvári. Tighter risk certiﬁcates for neural networks. Journal of Machine Learning Research, 22, 2021. A. Rakhlin and K. Sridharan. Online Learning with Predictable Sequences. In S. Shalev-Shwartz and I. Steinwart, editors, Proceedings of the 26th Annual Conference on Learning Theory, volume 30 of Proceedings of Machine Learning Research, pages 993–1019, Princeton, NJ, USA, 12–14 Jun 2013a. PMLR. URL https://proceedings.mlr.press/v30/Rakhlin13.html. S. Rakhlin and K. Sridharan. Optimization, Learning, and Games with Predictable Se- quences. In C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger, editors, Advances in Neural Information Processing Systems , volume 26. Curran As- sociates, Inc., 2013b. URL https://proceedings.neurips.cc/paper/2013/file/ f0dd4a99fba6075a9494772b58f95280-Paper.pdf. O. Rivasplata, V . M. Tankasali, and C. Szepesvári. PAC-Bayes with backprop. arXiv preprint arXiv:1908.07380, 2019. O. Rivasplata, I. Kuzborskij, C. Szepesvári, and J. Shawe-Taylor. PAC-Bayes analysis beyond the usual bounds. Advances in Neural Information Processing Systems, 33:16833–16845, 2020. 12Y . Seldin, N. Cesa-Bianchi, P. Auer, F. Laviolette, and J. Shawe-Taylor. PAC-Bayes-Bernstein Inequality for Martingales and its Application to Multiarmed Bandits. In D. Glowacka, L. Dorard, and J. Shawe-Taylor, editors, Proceedings of the Workshop on On-line Trading of Exploration and Exploitation 2, volume 26 of Proceedings of Machine Learning Research , pages 98–111, Bellevue, Washington, USA, 02 Jul 2012a. PMLR. URL https://proceedings.mlr.press/ v26/seldin12a.html. Y . Seldin, F. Laviolette, N. Cesa-Bianchi, J. Shawe-Taylor, and P. Auer. PAC-Bayesian Inequalities for Martingales. IEEE Transactions on Information Theory , 58(12):7086–7093, 2012b. doi: 10.1109/TIT.2012.2211334. S. Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends® in Machine Learning, 4(2):107–194, 2012. J. Shawe-Taylor and R. C. Williamson. A PAC analysis of a Bayes estimator. In Proceedings of the 10th annual conference on Computational Learning Theory, pages 2–9. ACM, 1997. J. W. Smith, J. E. Everhart, W. Dickson, W. C. Knowler, and R. S. Johannes. Using the adap learning algorithm to forecast the onset of diabetes mellitus. In Proceedings of the annual symposium on computer application in medical care, page 261. American Medical Informatics Association, 1988. W. N. Street, W. H. Wolberg, and O. L. Mangasarian. Nuclear feature extraction for breast tumor diagnosis. In Biomedical image processing and biomedical visualization, volume 1905, pages 861–870. SPIE, 1993. D. Svozil, V . Kvasnicka, and J. Pospichal. Introduction to multi-layer feed-forward neural networks. Chemometrics and intelligent laboratory systems, 39(1):43–62, 1997. N. Thiemann, C. Igel, O. Wintenberger, and Y . Seldin. A strongly quasiconvex PAC-Bayesian bound. In International Conference on Algorithmic Learning Theory, pages 466–492. PMLR, 2017. I. O. Tolstikhin and Y . Seldin. PAC-Bayes-Empirical-Bernstein Inequality. In C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger, editors,Advances in Neural Information Process- ing Systems, volume 26. Curran Associates, Inc., 2013. URL https://proceedings.neurips. cc/paper/2013/file/a97da629b098b75c294dffdc3e463904-Paper.pdf. D. van der Hoeven, T. van Erven, and W. Kotłowski. The many faces of exponential weights in online learning. In S. Bubeck, V . Perchet, and P. Rigollet, editors,Proceedings of the 31st Conference On Learning Theory, volume 75 of Proceedings of Machine Learning Research, pages 2067–2092. PMLR, 06–09 Jul 2018. URL https://proceedings.mlr.press/v75/hoeven18a.html. P. Viallard, P. Germain, A. Habrard, and E. Morvant. A general framework for the derandomization of PAC-Bayesian bounds. 2021. URL https://arxiv.org/abs/2102.08649. O. Wintenberger. Stochastic Online Convex Optimization; Application to probabilistic time series forecasting. arXiv preprint arXiv:2102.00729, 2021. T. Yang, L. Zhang, R. Jin, and J. Yi. Tracking slowly moving clairvoyant: Optimal dynamic regret of online learning with true and noisy gradient. In International Conference on Machine Learning, pages 449–457. PMLR, 2016. L. Zhang, T. Yang, rong jin, and Z.-H. Zhou. Dynamic Regret of Strongly Adaptive Methods. In J. Dy and A. Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 5882–5891. PMLR, 10–15 Jul 2018. URL https://proceedings.mlr.press/v80/zhang18o.html. P. Zhao, Y .-J. Zhang, L. Zhang, and Z.-H. Zhou. Dynamic Regret of Convex and Smooth Functions. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems , volume 33, pages 12510–12520. Cur- ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 939314105ce8701e67489642ef4d49e8-Paper.pdf. M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In Proceedings of the 20th international conference on machine learning (icml-03), pages 928–936, 2003. 13Checklist 1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] From the abstract to the conclu- sion, we explicited our main assumption (bounded loss). (c) Did you discuss any potential negative societal impacts of your work? [N/A] Due to the theoretical nature of our contribution, we do not foresee immediate societal impact. If anything, we certainly hope a better theoretical understanding of online learning algorithms can foster a more responsible use by practitionners. (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [Yes] for all theorems and corollaries (b) Did you include complete proofs of all theoretical results? [Yes] But deferred to Appendix D due to space constraints. 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main ex- perimental results (either in the supplemental material or as a URL)? [Yes] We have included the url in Sec. 5, note that this is an anonymous repository. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] All details are gathered in Sec. 5 so that all readers can replicate our experiments and validate our fundings. (c) Did you report error bars (e.g., with respect to the random seed after running experi- ments multiple times)? [No] We chose to consider the classical online gradient descent as main comparison. (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] We precised the machine we used in the ’Parameters Settings’ paragraph of Sec. 5. 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] We precised the origin of our datasets in the ’Datasets’ paragraph of Sec. 5. (b) Did you mention the license of the assets? [N/A] (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] All the original code is available on the anonymised repository precised in Sec. 5. (d) Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [N/A] (e) Did you discuss whether the data you are using/curating contains personally identiﬁable information or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] 14A Background A.1 Reminder on Online Gradient Descent For the sake of completeness we re-introduce the projected Online Gradient Descent (OGD) on a convex set K. This is a ﬁrst example of online learning philosophy. It may be the algorithm that applies to the most general setting of online convex optimization. This algorithm, which is based on standard gradient descent from ofﬂine optimization, was introduced in its online form by Zinkevich [2003]. In each iteration, the algorithm takes a step from the previous point in the direction of the gradient of the previous cost. This step may result in a point outside of the underlying convex set. In such cases, the algorithm projects the point back to the convex set, i.e. ﬁnds its closest point in the convex set. We precise this algorithm works with the assumptions of a convex setK bounded in diameter by Dand of bounded gradients (by a certain G). We also assume here to have a dataset S = (zt)t=1..T and to be coherent with the online learning philosophy, we assume that for eacht> 0, we possess a loss function ℓt depending on the points (z1,...zt). We present OGD in algorithm 2 Algorithm 2: Projected OGD onto a convex K with ﬁxed step η. Parameters : Epoch T, step-size (η) Initialisation :Convex set K, Initial point θ0 ∈K, T, step sizes (ηt)t 1 for each iteration tin 1..T do 2 Compute f′(θn) 3 Play (observe) θt and compute the cost ft(θt) Update and project ζt = θt−1 −η∇ℓt(θt−1) θt = ΠK(ζt) 4 end 5 Return θT One now deﬁnes the notion of regret which is the classical quantity to evaluate the performance of an online algorithm. Deﬁnition A.1. One deﬁnes the regret of a decision sequence (θt) at time T w.r.t. a pointθas: RegretT(θ) := T∑ t=1 ℓt(θt) − T∑ t=1 ℓt(θ) Now we state a regret bound which can be found in [Shalev-Shwartz, 2012, Eq 2.5] although we slightly modiﬁed the result, which uses additional hypotheses from Hazan [2016]. Proposition A.2. Assume that K has a ﬁxed diameter D and that the gradients of any point is bounded by G. Then for any θ∈K, the regret of projected OGD with ﬁxed stepηsatisﬁes: RegretT(θ) ≤D2 2η + ηTG2 A.2 About PAC-Bayes learning We provide here more information about PAC-Bayes learning. We propose a local framework; then expose what APC-Bayes lerning ais to do and ﬁnally state two celebrated PAC-Bayes theorems. An usual framework We ﬁrst state the following framework: • H is a space of considered predictors • Z is a data space. zcan be an unlabeled data xor a couple (x,y) of a point with its label. We assume that µis a distribution over Z which rules the distribution of our data. • ℓ: H ×Z →R+ is a loss function i.e. the learning objective we want to minimise. 15• S = (z1,...zm) an iid dataset following µ. • The generalisation risk for h∈H: R(h) = Ez∼µ[ℓ(h,z)]. • The empirical risk Rm(h) = 1 m ∑m i=1 ℓ(h,zi). What is PAC-Bayes learning? PAC-Bayes learning is about learning a meaningful data-dependent posterior distribution Qfrom a (classically data-free) prior P without necessarily exploiting the Bayes formula. PAC-Bayes learning controls the expected generalisation error: Eh∼Q[R(h)] := Eh∼QEz∼µ[ℓ(h,z)], which is the averaged error that would make a predictor drawn from our posterior distribution on a new point (usually this objective holds under an iid assumption on our dataset). Two classical theorems. We state below two celebrated PAC-Bayesian results: the McAllester bound enriched with Maurer’s remark as stated in [Guedj, 2019, Thm.1] and Catoni’s bound (Catoni, 2007, Thm 1.2.6). Those theorems both holds with the assumptions of iid data and loss bounded by 1. Note that in Thm. B.1, we also proposed another bound which is a corollary of Catoni’s one. Theorem A.3 (McAllester’ bound). For any prior distribution P, we have with probability 1 −δ over the m-sample S, for any posterior distribution Qsuch that Q≪P: Eh∼Q[R(h)] ≤Eh∼Q[Rm(h)] + √ KL(Q,P) + log(2√m/δ) 2m , where KLis the Kullback-Leibler divergence. Theorem A.4. For any prior distribution P, any λ >0, we have with probability 1 −δover the m-sample S, for any posterior distribution Qsuch that Q≪P: Eh∼Q[R(h)] ≤ 1 −exp { −λEh∼Q[Rm(h)] m −KL(Q||P)−log(δ) m } 1 −exp ( −λ m ) , where KLis the Kullback-Leibler divergence. B Discussion about Thm. 2.3 B.1 Comparison with classical PAC-Bayes The goal of this section is to show how good Thm. 2.3 compared to a naive approach which consists in applying classical PAC-Bayes results sequentially. The interest of this section is twofold: • First, presenting a classical PAC-Bayes result extracted and adapted from Alquier et al. [2016] which is formally close to what we propose. • Second, showing that a naive (yet natural) approach to obtain online PAC-Bayes bound leads to a deteriorated bound. We ﬁrst state our PAC-Bayes bound of interest. Theorem B.1 (Adapted from Alquier et al. [2016], Thm 4.1). Let S = (z1,...,z m) be an iid sample from the same law µ0. For any data-free prior P, for any loss function ℓ bounded by K, any λ> 0,δ ∈]0; 1[, one has with probability 1 −δfor any posterior Q∈M1(H): Eh∼QEz∼µ0 [ℓ(h,z)] ≤ 1 m m∑ i=1 Eh∼Q[ℓ(h,zi)] + KL(Q∥P) + log(1/δ) λ + λK2 2m Remark B.2. Two remarks about this result: • Thm. B.1 is a particular case of the original theorem from Alquier et al. [2016] as we take the case of a bounded loss which implies the subgaussianity of the random variables ℓ(.,zi) and then allows us to recover the factor λK2 m 16• This theorem is derived from Catoni [2007] and constitutes a good basis to compare ourselves with as it similar formally similar. Naive approach A naive way to obtain OPB bounds is to applymtimes Thm. B.1 (one per data) on batches of size 1 and then summing up the associated bounds. Thus one has the beneﬁts of classical PAC-Bayes bound without having no more the need of data-free priors nor the iid assumption. The associated result is stated below: Theorem B.3. For any distributionsµ1,...,µ m over Z (such that zi ∼µi), any λ> 0 and any online predictive sequence (used as priors) (Pi), the following holds with probability 1 −δover the sample S ∼µfor any posterior sequence (Qi) : m∑ i=1 Ehi∼Qi [Ezi∼µi[ℓ(hi,zi)]] ≤ m∑ i=1 Ehi∼Qi [ℓ(hi,zi)] + KL(Qi∥Pi) λ + λmK2 2 + mlog(m/δ) λ . Recall that here again we assimilate the stochastic kernels Qi,Pi to the data-dependent distributions Qi(S,.),Pi(S,.) Proof. First of all, for any i, we apply Thm. B.1 mto the batch {zi}. This allows us to consider Pi as a prior as it does not depend on the current data. We then have, taking δ′= δ/m, for any i∈{1..m} with probability 1 −δ/m: Ehi∼Qi [Ezi∼µi[ℓ(hi,zi)]] ≤Ehi∼Qi [ℓ(hi,zi)] + KL(Qi∥Pi) λ + λK2 2 + log(m/δ) λ Then, taking an union bound on those m events ensure us that with probability 1 −δ, for any i∈{1..m}: Ehi∼Qi [Ezi∼µi[ℓ(hi,zi)]] ≤Ehi∼Qi [ℓ(hi,zi)] + KL(Qi∥Pi) λ + λK2 2 + log(m/δ) λ Finally, summing those minequalities ensure us the ﬁnal result with probability 1 −δ. Comparison between Thm. 2.3 and Thm. B.3 Three points are noticeable between those two theorems: • First of all, the main issue with Thm. B.3 is that has a strongly deteriorated rate of O ( mlog(m/δ) λ ) instead of the rate in O ( log(1/δ) λ ) proposed in Thm. 2.3. More precisely, the problem is that we do not have a sublinear bound: one cannot ensure any learning through time. This point justiﬁes the need of the heavy machinery exploited in Thm. 2.3 proof as it allows a tighter convergence rate. • The second point point lies in the controlled quantity on the left hand-side of the bound. Thm. B.3 controls A := ∑m i=1 Ehi∼Qi [Ezi∼µi[ℓ(hi,zi)]] instead of B :=∑m i=1 Ehi∼Qi [E[ℓ(hi,zi) |Fi−1]]. Ais a less dynamic quantity than Bin the sense that it does not imply any evolution through time, it just considers global expectations. Doing so, Adoes not take into account that at each time step we have acces to all te past data to predict the future, this may explain the deteriorated convergence rate. Thus B, which appears to be a suitable quantity to control to perform online PAC-Bayes (see Appendix B.2 for additional explanations) • Finally, an interesting point is that in Thm. B.3 the bound, while looser, holds unformly for any posterior sequence contrary to Thm. 2.3 which holds only for a speciﬁc posterior sequence. This point will have a consequence for optimisation. We will come back later on this in Appendix B.3. 17B.2 A deeper analysis of Thm. 2.3 This section includes discussion about our proof technique and why all the assumptions made are necessary. We also propose a short discussion about the beneﬁts and limitations of an online PAC-Bayesian framework as well as a deeper reﬂexion about the new term our bound introduce. Why do we need an online predictive sequence as priors? This condition is fully exploited when dealing with the exponential moment ξm in the proof (see Lemma D.2 proof). Indeed, the fact of having Pi being Fi−1-measurable is essential to apply conditional Fubini (Lemma D.3). Note that the condition ∀i,Pi−1 ≪Pi is not necessary as the weaker condition∀i,P1 ≪Pi would sufﬁce here. However, note that when we particularise our theorem, for instance if we choose in Corollary 3.1 Pi = ˆQi, one recovers the condition ˆQi ≪ ˆQi+1 to have ﬁnite KL divergences. Hence the interest of taking directly an online predictive sequence. About the boundedness assumption The only moment where we invoke the boundedness as- sumption is in Lemma D.2’s proof where we apply the conditionnal Hoeffding lemma. This lemma actually translates that the sequence of r.v. (ℓ(.,zi)i=1..m is conditionally subgaussian wrt the past i.e for any i, hi ∈H; λ∈R: E[exp(λ˜ℓi(hi,zi)) |Fi−1] ≤exp (λ2K2 2 ) where ˜ℓi(hi,zi) = E[ℓ(hi,zi) |Fi−1] −ℓ(hi,zi). This condition is the one truly involved in our heavy machinery. However, we chose to restrict ourselves to the stronger assumption of bounded loss function for the sake of clarity. However, an interesting open direction is to ﬁnd whether there exists concrete classes of unbounded losses which may satisfy either conditional subgaussianity or others conditions (such as conditional Bernstein condition for instance). Reﬂections about the left hand side of Thm. 2.3. We study in this paragraph the following term B := m∑ i=1 Ehi∼Qi [E[ℓ(hi,zi) |Fi−1]] has naturally arisen in our work as the right term to compare our empirical loss with to perform the conditional Hoeffding lemma. Taking a broader look, we now interpret this term as the right quantity to control if one wants to perform online PAC-Bayes learning. Indeed this term is a ’best of both world’ quantity bridging PAC-Bayes and online learning: • From the PAC-Bayes point of view one keeps the control on average (cf the conditional expectation in B) on a novel data drawn at each time step. This point is crucial in the PAC-Bayes literature as our posteriors are designed to generalise well to unseen data. • From the Online Learning point of view, one keeps the control of a sequence of points generated from an online algorithm. Because an online learning algorithm generate a prediction for future points while having access to past data, the conditional expectation in Btranslates this. Finally this conditional expectation appears to be a good tradeoff between the classical expectation on data appearing in the PAC-Bayes literature (see e.g. Thm. B.1) and the local control that we have in online learning by only dealing with the performance of a sequence of points generated from a learning algorithm (see e.g. proposition A.2) About the interest of an Online PAC-Bayesian framework The main shift our work does with classical online learning literature is that it does not consider the celebrated regret but instead focuses on B which is a cumulative expected loss conditionned to the past. This shift does not invalidate our work but put some relief to hte guarantees Online PAC-Bayes learning can provide that Online Learning cannot and reversely. 18• Online PAC-Bayes ensure a good potential for generalisation as it deals with the control of conditional expectation. This can be useful if one wants to deal with a periodic process for instance. • Online Learning through the regret compares the studied sequence of predictors (typically generated from an online learning algorithm) and tries to compare it to the best ﬁxed strategy (static regret) or the best dynamic one (dynamic regret). In this way, OL algorithms want to ensure that their predictions are closed from the optimal solution. This point is not guaranteed by our online PAC-Bayesian study. • However the limitations of online learning can arise if the studied problem has a huge variance (for instance micro-transactions in ﬁnance). In this case those algorithms can follow an unpredictable optimisation route while PAC-Bayes still ensure a good performance on average (knowing the past) in this case. • Finally, we want to emphasize that PAC-Bayesian learning circumvent a problem ofmemory- less learning which appears in classical OL algorithms. For instance, the OGD algorthm (see Appendix A.1) uses once a data and do not memorise it for further use. This problem does not happen in Online PAC-Bayes learning. Indeed, we take the example of the procedure Eq. (3) which generates Gibbs posterior which keep in mind the inﬂuence of past data. B.3 Thm. 2.3 and optimisation In this section we discuss about the way Thm 2.2 can be thought in the framework of an optimisation process as we did in Secs. 3 and 4. A signiﬁcant change compared to classical PAC-Bayes Thm. 2.3 holds ’for any posterior se- quence (Qi) the following holds with probability 1 −δover the sample S ∼µ’ while most classical PAC-Bayesian results such that Thm. B.1 holds ’with probability1 −δover the sample S ∼µfor any posterior Q’. This change is signiﬁcant as our theorem does not control simultaneaously all possible sequences of posteriors but only holds for one. Thus, Thm. 2.3 has to be seen as a local or pointwise theorem and not as a global one. In classical PAC-Bayes, this local behavior is a brake on the optimisation process. But as we develop below, it is not the case in our online framework. Thm. 2.3 is compatible with online optimisation We ﬁrst recall that classically, an online algo- rithm like OGD (see Appendix A.1) performs one optimisation step per arriving data. Thus, at time m, such algorithm will perform moptimisation steps and generate mpredictors. Similarly the OPB algorithm of Eq. (1) generates mdistribution in mtime steps. We insist on the fact that, Thm. 2.3 and all its corollaries throughout our paper are valid for a sequence of mposteriors and not only a single one. A key point is that whatever the number mof data, our theoretical guarantee wil still be valid for mposterior distributions with the approximation term log(1/δ) (and not log(m/δ) as an union bound would provide for a classical PAC-Bayes theorem). For this reason, given an online PAC-Bayes algorithm, Thm. 2.3 is suited for optimisation. Indeed, having a bound valid for a sequence of posteriors ensures guarantees for a single run of our OPB algorithm. This point is crucial to bridge a link with online learning as regret bounds (e.g. proposi- tion A.2) also provide guarantees for a single sequence of predictors. In online learning however, those guarantees are mainly deterministic (because based on convex optimisation properties) but not totally: the recent work of Wintenberger [2021] proposed PAC regret bounds for its general Stochastic Online Convex Optimisation framework. An interesting open challenge is to overcome the pointwise behavior of our theorem, for that, we need to rethought [Rivasplata et al., 2020, Thm 2.1] as this basis is pointwise itself. Given we consider a sequence of data-dependent priors one cannot apply the classical change of measure inequality to ensure guarantees holding uniformly on posterior sequences. A crucial point: having an explicit OPB/OPBD algorithm In our previous paragraph we said that our bound were suitable for optimisation given an OPB/OPBD algorithm. We now provide some precision about this point. All the procedures provided in the paper (i.e. Eq. (1), algorithm 1) take into account an update phase implying an argmin. Luckily for our procedures, this argmin is explicit: 19• For the OPB algorithm of Eq. (1), the argmin is solved thanks to the variational formulation of the Gibbs posterior • For OPBD algorithms, given the explicit choices of Ψ given in Corollary 4.1, argmin becomes explicit when one has a derivable loss function. In both cases, this explicit argmin ensure our procedure of interest generates explictly a single posterior per time step: we have a well-deﬁned sequence of mposteriors at time m. Doing so the guarantees of Thm. 2.3 holds for this sequence. C A reminder on PAC-Bayesian disintegrated bounds We present two PAC-Bayesian disintegrated bounds valid with data-dependent priors (i.e. any stochastic kernels). • The ﬁrst one is Th. 1) i) from Rivasplata et al. [2020] which provides a disintegrated version of Thm. 2.3. • The second one is Thm 2. from Viallard et al. [2021] which involves Rényi divergence instead of the classical KL. Note that this bound has originally been stated for data- indepedent prior, which is why we revisit the proof to adapt it to the stochastic kernel framework. Proposition C.1 (Th 1) i) Rivasplata et al. [2020]) . Let P ∈M1(S), Q0 ∈Stoch(Zm,F). Let f : S ×H →R be any measurable function. Then for any Q∈Stoch(Zm,F) and any δ∈(0,1), with probability at least 1 −δover the random draw of S ∼P and h∼QS, we have: f(S,h) ≤log (dQS dQ0 S (h) ) + log(ξm/δ). where ξm := ∫ S ∫ H ef(s,h)Q0 s(dh)P(ds) and dQS dQ0 S is the Radon Nykodym derivative of QS w.r.t.Q0 S. Proposition C.2 (Adapted from Th. 2 of Viallard et al. [2021]) . Let µ ∈ M1(S), Q0 ∈ Stoch(Zm,F). Let α> 1 and f : S ×H →R+ be any measurable function. Then for any Q ∈Stoch(Zm,F) such that for any S ∈Zm,QS >> Q0 S, Q0 S >> QS and any δ∈(0,1), with probability at least 1 −δover the random draw of S ∼µand h∼QS, we have: α α−1 log(f(S,h)) ≤2α−1 α−1 log 2 δ + Dα ( QS∥Q0 S ) + log ( E S′∼µ E h′∼Q0 S′ f(S′,h′) α α−1 ) where Dα(Q∥P) = 1 α−1 log ( E [ Eh∼P ( dQ dP(h) )α]) is the Rényi diverence of order α. Note that Viallard et al. original bound only stand for data-free priors and i.i.d data. However it appears their proof works with any stochastic kernel as prior and any distribution over the dataset. We propose below an adaptation of their proof below to ﬁt with those more general assumptions. C.1 Proof of proposition C.2 Proof. For any sample Sand any stochastic kernel Q, note that f(S,h) is a non-negative random variable. Hence, from Markov’s inequality we have P h∼QS [ f(S,h) ≤2 δ E h′∼QS f(S,h′) ] ≥1 −δ 2 ⇐⇒ E h∼QS 1 [ f(S,h) ≤2 δ E h′∼QS f(S,h′) ] ≥1 −δ 2 Taking the expectation over S ∼µto both sides of the inequality gives E S∼µ E h∼QS 1 [ f(S,h) ≤2 δ E h′∼QS f(S,h′) ] ≥1 −δ 2 ⇐⇒ P S∼µ,h∼QS [ f(S,h) ≤2 δ E h′∼QS f(S,h′) ] ≥1 −δ 2. 20Taking the logarithm to both sides of the equality and multiplying by α α−1 >0, we obtain P S∼µ,h∼QS [ α α−1 log(f(S,h)) ≤ α α−1 log (2 δ E h′∼QS f(S,h′) )] ≥1 −δ 2. We develop the right side of the inequality in the indicator function and make the expectation of the hypothesis over Q0 S our \"prior\" stochadtic kernel appears. Indeed, because for anyS ∈S,QS >>Q0 S and Q0 S >>QS one can write properly dQS dQ0 S and dQ0 S dQS = ( dQS dQ0 S )−1 the Radon-Nykodym derivatives. Thus we have α α−1 log (2 δ E h′∼QS f(S,h′) ) = α α−1 log (2 δ E h′∼QS dQS dQ0 S (h′)dQ0 S dQS (h′)f(S,h′) ) = α α−1 log (2 δ E h′∼Q0 S dQS dQ0 S (h′)f(S,h′) ) . Remark that 1 r + 1 s = 1 with r= αand s= α α−1 . Hence, we can apply Hölder’s inequality: E h′∼Q0 S dQS dQ0 S (h′)f(S,h′) ≤ [ E h′∼Q0 S (dQS dQ0 S (h′) )α]1 α [ E h′∼Q0 S f(S,h′) α α−1 ]α−1 α . Then, by taking the logarithm; adding log (2 δ ) and multiplying by α α−1 > 0 to both sides of the inequality, we obtain α α−1 log (2 δ E h′∼Q0 S dQS dQ0 S (h′)f(S,h′) ) ≤ α α−1 log ( 2 δ [ E h′∼Q0 S (dQS dQ0 S (h′) )α]1 α [ E h′∼Q0 S f(S,h′) α α−1 ]α−1 α ) = 1 α−1 log ( E h′∼Q0 S [dQS dQ0 S (h′) ]α) + α α−1 log 2 δ + log ( E h′∼Q0 S f(S,h′) α α−1 ) = Dα ( QS∥Q0 S ) + α α−1 log 2 δ + log ( E h′∼Q0 S f(S,h′) α α−1 ) From this inequality, we can deduce that P S∼µ,h∼QS [ α α−1 log(f(S,h)) ≤Dα ( QS∥Q0 S ) + α α−1 log 2 δ + log ( E h′∼Q0 S f(S,h′) α α−1 )] ≥1 −δ 2. (8) Note that Eh′∼Q0 S f(S,h′) α α−1 is a non-negative random variable, hence, we apply Markov’s inequal- ity to have P S∼µ [ E h′∼Q0 S f(S,h′) α α−1 ≤2 δ E S′∼µ E h′∼Q0 S f(S′,h′) α α−1 ] ≥1 −δ 2. Since the inequality does not depend on the random variable h∼QS, we have 21P S∼µ [ E h′∼Q0 S f(S,h′) α α−1 ≤2 δ E S′∼µ E h′∼Q0 S f(S′,h′) α α−1 ] = E S∼µ 1 [ E h′∼Q0 S f(S,h′) α α−1 ≤2 δ E S′∼µ E h′∼Q0 S f(S′,h′) α α−1 ] = E S∼µ E h∼QS 1 [ E h′∼Q0 S f(S,h′) α α−1 ≤2 δ E S′∼µ E h′∼Q0 S f(S′,h′) α α−1 ] = P S∼µ,h∼QS [ E h′∼Q0 S f(S,h′) α α−1 ≤2 δ E S′∼µ E h′∼Q0 S f(S′,h′) α α−1 ] . Taking the logarithm to both sides of the inequality and adding α α−1 log 2 δ give us P S∼µ,h∼QS [ E h′∼Q0 S f(S,h′) α α−1 ≤2 δ E S′∼µ E h′∼Q0 S f(S′,h′) α α−1 ] ≥1 −δ 2 ⇐⇒ P S∼µ,h∼QS [ α α−1 log 2 δ + log ( E h′∼Q0 S f(S,h′) α α−1 ) ≤ 2α−1 α−1 log 2 δ + log ( E S′∼µ E h′∼Q0 S f(S′,h′) α α−1 )] ≥1 −δ 2. (9) Combining Equation Eq. (8) and Eq. (9) with a union bound gives us the desired result. D Proofs D.1 Proof of Thm. 2.3 Background We ﬁrst recall [Rivasplata et al., 2020, Thm 2]. Theorem D.1. Let µ∈M1(S), Q0 ∈Stoch(S,F). Let kbe a positive integer, anyA: S×H →Rk a measurable function and F : Rk →R be a convex function . Then for any Q∈Stoch(S,F) and any δ∈(0,1), with probability at least 1 −δover the random draw of S ∼µwe have F(QS[AS]) ≤KL ( QS∥Q0 S ) + log(ξm/δ). where ξm := ∫ S ∫ H ef(s,h)Q0 s(dh)P(ds) and QS[AS] := QS[A(S,.)] = ∫ H A(S,h)QS(dh). Proof of Thm. 2.3. To fully exploit the generality of Thm. D.1, we aim to design a m-tuple of probabilities. Thus, our predictor set of interest is Hm := H⊗m and then, our predictor his a tuple (h1,..,h m) ∈H. Throughout our study, our stochastic kernels Q,Q0 will belong to the speciﬁc class C deﬁned below: C := {Q|∃(Qi)i=1..ms.t. ∀S, Q(S,.) = Q1(S,.) ⊗...⊗Qm(S,.)} (10) Thus our kernels are such that conditionally to a given sample, our predictors (h1,...,h m) are drawn independently. We now apply Thm. D.1. To do so, we consider the following function A: S ×Hm →R2 such that ∀S = (zi)i=1..m,h = (hi)i=1..m ∈S ×Hm: A(S,h) = (m∑ i=1 E[ℓ(hi,zi) |Fi−1], m∑ i=1 ℓ(hi,zi) ) Ais indeed measurable in both of its variables. For a ﬁxed λ >0, we set the function F to be F(x,y) = λ(x−y) . 22The only thing left to set up is our stochastic kernels. To do so, let P = (P1,...Pm) be an online predictive sequence, we then deﬁne Q0 ∈C (deﬁned in Eq. (10)) s.t. for any sample S, Q0 S = P1(S,.)⊗...⊗Pm(S,.). We also ﬁx Q1,...,Q m to be any (posterior) stochastic kernels and similarly we deﬁne the stochastic kernelQ∈C such that for any sampleS, Q(S,.) = Q1(S,.)⊗...⊗Qm(S,.). From now, we ﬁx a dataset Sand, for the sake of clarity, we assimilate in what follows the stochastic kernels Qi,Pi to the data-dependent distributions Qi(S,.),Pi(S,.) (i.e. we drop the dependency in S). Under those choices, one has: QS[AS] = ∫ h∈Hm A(S,h)QS(dh1,...,dh m) = (∫ h∈Hm m∑ i=1 E[ℓ(hi,zi) |Fi−1]QS(dh1,...,dh m), ∫ h∈Hm m∑ i=1 ℓ(hi,zi)QS(dh1,...,dh m) ) Furthermore, Q∈C, thus QS(dh1,...,dh m) = Πm i=1Qi(dhi) so:‘ QS[AS] = (m∑ i=1 Ehi∼Qi[E[ℓ(hi,zi) |Fi−1]] , m∑ i=1 Ehi∼Qi[ℓ(hi,zi)] ) Finally: F(QS[AS]) = λ (m∑ i=1 Ehi∼Qi[E[ℓ(hi,zi) |Fi−1]] − m∑ i=1 Ehi∼Qi[ℓ(hi,zi)] ) Applying Thm. D.1 and re-organising the terms gives us with probability 1 −δ: m∑ i=1 Ehi∼Qi [E[ℓ(hi,zi) |Fi−1]] ≤ m∑ i=1 Ehi∼Qi[ℓ(hi,zi)] + KL(QS∥Q0 S) λ + log(ξm/δ) λ Thus: m∑ i=1 Ehi∼Qi [E[ℓ(hi,zi) |Fi−1]] ≤ m∑ i=1 Ehi∼Qi[ℓ(hi,zi)] + m∑ i=1 KL(Qi∥Pi) λ + log(ξm/δ) λ (11) The last line holding because for a ﬁxed S, QS = Q1 ⊗...⊗Qm and Q0 S = P1 ⊗...⊗Pm. The last term to control is ξm = ES [ Eh1,...,hm∼Q0 S [ exp ( λ m∑ i=1 ˜ℓi(hi,zi) )]] with ˜ℓi(hi,zi) = E[ℓ(hi,zi) |Fi−1] −ℓ(hi,zi). Hence the following lemma. Lemma D.2. One has for any m, ξm ≤exp ( λ2mK2 2 ) with Kbounding ℓ. The proof of this lemma is deferred to Appendix D.1.1 To conclude the proof, we just bound ξm by the result of Lemma D.2 within Eq. (11). D.1.1 Proof of Lemma D.2 Proof of Lemma D.2. We prove our result by recursion: for m= 1, S = z1 and one knows that P1 is F0 measurable yet it does not depend on S. Thus for any h1 ∈H, E[ℓ(h1,z1) |F0] = E[ℓ(h1,z1)]. 23We then has: ξ1 = ESEh1∼P1 [˜ℓ1(h1,z1)] = Eh1∼P1 ES[˜ℓ1(h1,z1)] by Fubini ≤exp λ2K2 2 The last line holding because for any h1 ∈H, ˜ℓ1(h1,z1) is a centered variable belonging in [−K,K] a.s. and so one can apply Hoeffding’s lemma to conclude. Assume the result is true at rank m−1 ≥0. We then has to prove the result at rank m. Our strategy consists in conditioning by Fm−1 within the expectation over S: ξm = ES [ Eh1,...,hm∼Q0 S [ exp ( λ m∑ i=1 ˜ℓi(hi,zi) )]] First, we use that Q0 ∈C, thus Q0 S = P1 ⊗...⊗Pm (i.e. our data are drawn independently for a given S): = ES [ Πm i=1Ehi∼Pi [ exp ( λ˜ℓi(hi,zi) )]] We now condition by Fm−1 and use that Πm−1 i=1 Ehi∼Pi [ exp ( λ˜ℓi(hi,zi) )] is a Fm−1-measurable r.v. ξm = ES [ Πm−1 i=1 Ehi∼Pi [ exp ( λ˜ℓi(hi,zi) )] E [ Ehm∼Pm[exp(λ˜ℓm(hm,zm))] |Fm−1 ]] Now our next step is to use a variant of Fubini valid for Fm−1- measurable measures. Lemma D.3 (Conditional Fubini). Let f : H ×Z →R+. For a sigma-algebra F over Z and a measure P over H such that • P is a F-measurable r.v. • There exists a constant measure (a.s.) P0 such that P >>P0. Then one has almost surely, for any r.v.zover Z: E[Eh∼P[f(h,z)] |F] = Eh∼P [E[f(h,z) |F]] The proof of this lemma lies at the end of this section. We then ﬁx F = Fm−1 and f(h,z) = exp( λ˜ℓi(h,z)). Furthermore, because we assumed the sequence (Pi) to be an online predictive sequence, Pm is Fm−1-measurable and Pm >>P1 with P1 a data-free prior. One then applies Lemma D.3: E [ Ehm∼Pm[exp(λ˜ℓm(hm,zm))] |Fm−1 ] = Ehm∼Pm [ E[exp(λ˜ℓm(hm,zm)) |Fm−1] ] . Yet, injecting this result onto ξm provides: ξm = ES [ Πm−1 i=1 Ehi∼Pi [ exp ( λ˜ℓi(hi,zi) )] Ehm∼Pm [ E[exp(λ˜ℓi(hm,zm)) |Fm−1] ]] The ﬁnal remark is to notice that for any hm ∈H, E[˜ℓm(hm,zm) |Fm−1] = 0 and ˜ℓm(hm,zm) ∈ [−K,K] a.s. then one can apply the conditional Hoeffding’s lemma which ensure us that for any λ> 0: 24E[exp(λ˜ℓm(hm,zm)) |Fm−1] ≤exp (λ2K2 2 ) . One then has ξm ≤exp ( λ2K2 2 ) ξm−1. The recursion assumption concludes the proof. Proof of Lemma D.3. Let Abe a F-measurable event. One wants to show that E[Eh∼P[f(h,z)]1A] = E[Eh∼P [E[f(h,z) |F]] 1A] Where the ﬁrst expectation in each term is taken over z. This will be enough to conclude that E[Eh∼P[f(h,z)] |F] = Eh∼P [E[f(h,z) |F]] thanks to the deﬁnition of conditional expectation. We ﬁrst start by using the fact that P is F- measurable and that P >> P0 with P0 a constant measure. This is enough to obtain that the Radon-Nykodym derivative dP dP0 is a F-measurable function, thus: E[Eh∼P[f(h,z)]1A] = E [ Eh∼P0 [ f(h,z) dP dP0 (h) ] 1A(z) ] = E [ Eh∼P0 [ f(h,z) dP dP0 (h)1A(z) ]] Because f(h,z) dP dP0 (h)1A(z) is a positive function, and that P0 is ﬁxed, one can apply the classical Fubini-Tonelli theorem: = Eh∼P0 [ E [ f(h,z) dP dP0 (h)1A(z) ]] One now conditions by F and use the fact that dP dP0 ,1A are F-measurable: = Eh∼P0 [ E [ E[f(h,z) |F] dP dP0 (h)1A(z) ]] We ﬁnally re-apply Fubini-Tonelli to re-intervert the expectations: = E [ Eh∼P0 [ E[f(h,z) |F] dP dP0 (h)1A(z) ]] = E[Eh∼P [E[f(h,z) |F] 1A(z)]] This ﬁnally proves the announced results, yet concludes the proof. D.2 Proofs of Sec. 4 We prove here Corollary 4.1 and Corollary 4.2. D.2.1 Proof of Corollary 4.1 We ﬁx ˆQ,P to be online predictive sequences (with ˆQ1,P1 being data-free priors). Recall that we assimilated the stochastic kernels ˆQi,Pi to the their associated data-dependent sitribution given a sample S ˆQi(S,.),Pi(S,.). As in Thm. 2.3, our predictor set of interest is Hm := H⊗m and then, our predictor his a tuple (h1,..,h m) ∈H. We consider the stochastic kernel Qbelonging to the class C deﬁned in Eq. (10) such that for any S ∈S,Q(S,.) = ˆQ2 ⊗...⊗ˆQm+1. Similarly one deﬁnes Q0 ∈C such that for any S ∈S,Q0(S,.) = P1 ⊗...⊗Pm 25Proof for (Ψ1,Φ1): For λ> 0, we set our function f to be for any dataset Sand predictor tuple h= (h1,...,h m), f(S,h) = λ (m∑ i=1 E[ℓ(hi,zi) |Fi−1] − m∑ i=1 ℓ(hi,zi) ) We then apply proposition C.1 with the function f, Q,Q0 deﬁned above. One then has by dividing by λwith probability 1 −δover S ∼µand h= (h1,...,h m) ∼ ˆQ2 ⊗...⊗ˆQm+1: m∑ i=1 E[ℓ(hi,zi) |Fi−1] ≤ m∑ i=1 ℓ(hi,zi) + 1 λlog (dQS dQ0 S (hi) ) + 1 λlog(ξm) + log(1/δ) λ And then using the fact that S ∈S,QS = ˆQ2 ⊗...⊗ˆQm+1,Q0 S = P1 ⊗...⊗Pm gives us: m∑ i=1 E[ℓ(hi,zi) |Fi−1] ≤ m∑ i=1 ℓ(hi,zi) + 1 λ m∑ i=1 log ( dˆQi+1 dPi (hi) ) + 1 λlog(ξm) + log(1/δ) λ with ξm = ES [ Eh1,...,hm∼QS [ exp ( λ∑m i=1 ˜ℓi(hi,zi) )]] and for any i, ˜ℓi(hi,zi) = E[ℓ(hi,zi) |Fi−1] −ℓ(hi,zi) Notice that, because P is an online predictive sequence, then one can apply directly Lemma D.2 to conclude that ξm ≤exp ( λ2K2m 2 ) . We also use [Viallard et al., 2021, Lemma 11] which derives the calculation of the disintegrated KL divergence between two Gaussians. One then has for any i, with hi = ˆwi+1 + εi: log ( dˆQi+1 dPi (hi) ) = ||ˆwi+1 + εi −w0 i||2 −||ε||2 2σ2 Combining those facts altogether allows us to conclude. Proof for (Ψ2,Φ2): For λ> 0, we set our function f to be for any dataset Sand predictor tuple (h= h1,...,h m), f(S,h) = exp ( λ (m∑ i=1 E[ℓ(hi,zi) |Fi−1] − m∑ i=1 ℓ(hi,zi) )) We take α= 2 and apply this time proposition C.2. One then has by dividing by 2λwith probability 1 −δover S ∼µand h= (h1,...,h m) ∼ ˆQ2 ⊗...⊗ˆQm+1: m∑ i=1 E[ℓ(hi,zi) |Fi−1] ≤ m∑ i=1 ℓ(hi,zi)+ 3 2λlog 2 δ+D2 ( QS∥Q0 S ) 2λ + 1 2λlog   E S′∼µ E h′∼Q0 S′ f(S′,h′)2    :=ξ′m   We ﬁrst notice that D2 ( QS∥Q0 S ) = ∑m i=1 D2( ˆQi+1∥Pi) as our predictors are drawn independently once Sis given. We also use that for any i, the Rényi divergence with α= 2 between ˆQi+1 and Pi (two multivariate Gaussians with same covariance matrix) is ∥ˆwi+1−w0 i∥2 σ2 (as recalled in Gil et al. [2013]). We then remark that: 26ξ′ m = E S′∼µ E h′∼Q0 S′ exp ( 2λ (m∑ i=1 E[ℓ(h′ i,z′ i) |Fi−1] − m∑ i=1 ℓ(h′ i,z′ i) )) . Thus we recover the exponential moment ξm from the Rivasplata’s case up to a factor 2 within the exponential. We then apply Lemma D.2 with λ′= 2λto obtain that ξ′ m ≤exp ( 2λ2K2m ) . Combining all those facts allows us to conclude. D.2.2 Proof of Corollary 4.2 We apply the exact same proof than Corollary 4.1. The only difference is the way to deﬁne our stochastic kernels. We now take, for a single online predictive sequence ˆQthe following stochastic kernels: We consider the stochastic kernel Qbelonging to the class C deﬁned in Eq. (10) such that for any S ∈S,Q(S,.) = ˆQ1 ⊗...⊗ˆQm and we take Q0 = Q. This fact allows the divergence terms (Rényi or KL depending on which bound we consider) to vanish. The rest of the proof remains unchanged. E Additional experiment In this section we perform error bars for our OPBD methods in order to evaluate their volatility. We ran n= 50 times our algorithms and then show in the table below for each data set the means and the standard deviation of our averaged cumulative losses at regular time steps. We denote for i∈{1,2} ’OPBDΨi’ to indicate that this algorithm is our OPBD method used with thev optimisation objective Ψi. Analysis Those tables shows the robustness of our OPBD methods to their intrinsic randomness: we always have a decreasing mean through time as well as an overall variance reduction. Note that for the most complicated problem (California Housing dataset), the variance is the highest. More precisely, we notice that the standard deviation of OPBD with Ψ1 is always greater than the one of OPBD with Ψ2 which is not a surprise as Ψ1 involves a disintegrated KL divergence whileΨ2 is a proper Rényi divergence. Hence the additional volatility for OPBDwithΨ1. This fact is particurlaly noticeable on the California Housing dataset where both the means and variance of OPBD with Ψ1 increase drastically between t=16000 and t=20000 while the increase is more attenuated for OPBD with Ψ2. This fact is also visible on Fig. 1. 27means OPBD Ψ1 std OPBD Ψ1 means OPBD Ψ2 std OPBD Ψ2 t=200 0.2014 0.0034 0.1993 0.0007 t=400 0.1888 0.0030 0.1861 0.0004 t=600 0.1867 0.0023 0.1839 0.0003 t=800 0.1714 0.0020 0.1686 0.0003 t=1000 0.1760 0.0016 0.1731 0.0003 Table 1: Error bars for the Boston Housing dataset means OPBD Ψ1 std OPBD Ψ1 means OPBD Ψ2 std OPBD Ψ2 t=100 0.1619 0.0063 0.1601 0.0030 t=200 0.1350 0.0057 0.1361 0.0008 t=300 0.1214 0.0044 0.1241 0.0009 t=400 0.1210 0.0043 0.1238 0.0021 t=500 0.1131 0.0037 0.1159 0.0015 Table 2: Error bars for the Breast Cancer dataset means OPBD Ψ1 std OPBD Ψ1 means OPBD Ψ2 std OPBD Ψ2 t=150 0.7102 0.0061 0.7069 0.0007 t=300 0.6455 0.0056 0.6422 0.0007 t=450 0.6134 0.0042 0.6103 0.0007 t=600 0.5860 0.0035 0.5837 0.0008 t=750 0.5685 0.0031 0.5664 0.0008 Table 3: Error bars for the PIMA Indians dataset means OPBD Ψ1 std OPBD Ψ1 means OPBD Ψ2 std OPBD Ψ2 t=4000 0.9320 0.0572 0.8905 0.0003 t=8000 0.6325 0.0335 0.5947 0.0003 t=12000 0.5314 0.0254 0.4954 0.0002 t=16000 0.4967 0.0299 0.4477 0.0004 t=20000 0.5273 0.1056 0.4355 0.0030 Table 4: Error bars for the California Housing dataset 28",
      "meta_data": {
        "arxiv_id": "2206.00024v2",
        "authors": [
          "Maxime Haddouche",
          "Benjamin Guedj"
        ],
        "published_date": "2022-05-31T18:00:09Z",
        "venue": "36th Conference on Neural Information Processing Systems (NeurIPS\n  2022)",
        "pdf_url": "https://arxiv.org/pdf/2206.00024v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the limitation of classical PAC-Bayesian bounds, which typically apply to batch learning, by extending them to the online learning framework. It proves new PAC-Bayesian bounds (Theorem 2.3) that allow for dynamically evolving prior and posterior sequences, relax the iid data assumption, and hold for bounded, potentially non-convex losses. The research introduces two types of bounds: Online PAC-Bayesian (OPB) training bounds (Corollary 3.1) and test bounds (Corollary 3.3), along with corresponding online algorithms. It also develops 'disintegrated' online PAC-Bayesian algorithms for Gaussian distributions (Algorithm 1) with reduced computational costs. The theoretical consistency is shown by recovering classical PAC-Bayes results (Catoni's posterior) and establishing analogies with Online Gradient Descent (OGD) within this new framework. Empirical evaluations demonstrate the consistency and competitive performance of the proposed online PAC-Bayesian procedures against OGD on various datasets.",
        "methodology": "The core methodology involves establishing a novel online PAC-Bayesian bound (Theorem 2.3) using a 'batch-to-online' conversion, which leverages PAC-Bayesian tools designed for fixed batches to obtain dynamic results. This bound uses an updated definition of regret, focusing on an expected cumulative loss conditioned on the past, rather than classical regret. The framework introduces 'stochastic kernels' to formalize data-dependent measures and 'online predictive sequences' for priors. Two main algorithmic procedures are derived: 1. A Gibbs-based OPB algorithm (Eq. 1) optimizing a trade-off between new data impact and prior knowledge, leading to Gibbs posteriors. 2. Disintegrated OPBD algorithms (Algorithm 1) for Gaussian measures with fixed variance, which directly optimize a loss function without an expected term, utilizing disintegrated KL or Rényi divergences (Ψ1, Ψ2 functions from Corollary 4.1). These algorithms leverage conditional Hoeffding's lemma and conditional Fubini's lemma in their proofs to handle dependent data and evolving distributions. The analogy to OGD is drawn by approximating the optimization problem with a first-order Taylor expansion.",
        "experimental_setup": "Experiments were conducted on four real-life datasets: two for binary classification (Breast Cancer, Pima Indians) and two for linear regression (Boston Housing, California Housing). All datasets, except Pima Indians, were extracted from sklearn. Observations were randomly permuted to avoid irrelevant ordering effects. The proposed methods include the OPB algorithm with Gibbs posterior instantiated with Gaussian and Laplace priors ('Gibbs Gauss', 'Gibbs Laplace'), and Algorithm 1 (OPBD) with Ψ1 ('OPBD Riva') and Ψ2 ('OPBD Via'). These were compared against classical Online Gradient Descent (OGD) and Streaming Variational Bayes (SVB). For classification, the loss function used was a thresholded hinge loss: ℓ(hi,zi) = (1 − yi * hi^T * xi)+. For regression, the loss function was squared error: ℓ(hi,zi) = (yi − hi^T * xi)^2. The efficiency criterion was the averaged cumulative loss (sum of losses / time step). Parameters such as learning rates (e.g., η=1/√m for OGD, ηi=0.1/√i for SVB), λ values (e.g., 1/m for OPB Gibbs, 10^-4/m or 2*10^-3/m for OPBD), and variance (σ=1.5 or σ=3*10^-3, σ=10^-2) were specified. Experiments were run on a 2021 MacBook Pro with an M1 chip and 16 Gb RAM. Error bars were reported for OPBD methods based on 50 runs.",
        "limitations": "A primary limitation is the explicit assumption of bounded losses across all theoretical results. While the underlying conditional subgaussianity is the true condition, the paper restricts itself to bounded loss for clarity, leaving extensions to concrete classes of unbounded losses for future work. The main bound (Theorem 2.3) holds for a specific posterior sequence (local/pointwise) rather than uniformly for all possible sequences, which is a significant change compared to classical PAC-Bayes. This prevents uniform guarantees on posterior sequences when using data-dependent priors. The disintegrated online algorithms, while computationally efficient, show a lack of stability as they directly depend on noise (εi) in their optimization step. For instance, Ψ1 is less stable than Ψ2 due to an additional dependency on εi. The online PAC-Bayes framework, by controlling a cumulative expected loss, does not guarantee that predictions are close to the optimal solution, a common objective in traditional online learning (static or dynamic regret). The current experimental evaluation is 'brief' and covers only a few datasets.",
        "future_research_directions": "Future research directions include relaxing the bounded loss assumption to explore concrete classes of unbounded losses that might satisfy conditional subgaussianity or conditional Bernstein conditions. Another challenge is to overcome the 'pointwise' nature of the main theorem to provide guarantees that hold uniformly for any posterior sequence, possibly by re-evaluating the foundational Rivasplata et al. (2020, Thm 2.1) result. The paper suggests extending the methodology to applications in Spiking Neural Networks, given the generality of the bounded loss assumption which makes no convexity requirements. Exploring other values for the Rényi divergence parameter α (beyond α=2) in the disintegrated bounds could lead to new optimization objectives. Integrating noise into measures at each time step to avoid overfitting, for instance by adding small noise to parameter vectors (e.g., in Gaussian means), is also suggested as a fruitful route for future development."
      }
    },
    {
      "title": "Provably Efficient Model-based Policy Adaptation",
      "abstract": "The high sample complexity of reinforcement learning challenges its use in\npractice. A promising approach is to quickly adapt pre-trained policies to new\nenvironments. Existing methods for this policy adaptation problem typically\nrely on domain randomization and meta-learning, by sampling from some\ndistribution of target environments during pre-training, and thus face\ndifficulty on out-of-distribution target environments. We propose new\nmodel-based mechanisms that are able to make online adaptation in unseen target\nenvironments, by combining ideas from no-regret online learning and adaptive\ncontrol. We prove that the approach learns policies in the target environment\nthat can quickly recover trajectories from the source environment, and\nestablish the rate of convergence in general settings. We demonstrate the\nbenefits of our approach for policy adaptation in a diverse set of continuous\ncontrol tasks, achieving the performance of state-of-the-art methods with much\nlower sample complexity.",
      "full_text": "Provably Efﬁcient Model-based Policy Adaptation Yuda Song1 Aditi Mavalankar 1 Wen Sun2 Sicun Gao 1 Abstract The high sample complexity of reinforcement learning challenges its use in practice. A promis- ing approach is to quickly adapt pre-trained poli- cies to new environments. Existing methods for this policy adaptation problem typically rely on domain randomization and meta-learning, by sam- pling from some distribution of target environ- ments during pre-training, and thus face difﬁculty on out-of-distribution target environments. We propose new model-based mechanisms that are able to make online adaptation in unseen target environments, by combining ideas from no-regret online learning and adaptive control. We prove that the approach learns policies in the target en- vironment that can recover trajectories from the source environment, and establish the rate of con- vergence in general settings. We demonstrate the beneﬁts of our approach for policy adaptation in a diverse set of continuous control tasks, achiev- ing the performance of state-of-the-art methods with much lower sample complexity. Our project website, including code, can be found at https: //yudasong.github.io/PADA. 1. Introduction Deep Reinforcement Learning (RL) methods typically re- quire a very large number of interactions with environments, making them difﬁcult to be used on practical systems (Tan et al., 2018). A promising direction is to adapt policies trained in one environment to similar but unseen environ- ments, such as from simulation to real robots. Existing ap- proaches for policy adaptation mostly focus on pre-training the policies to be robust to predeﬁned distributions of dis- turbances in the environment, by increasing the sample di- versity during training (Peng et al., 2018; Tobin et al., 2017; 1Department of Computer Science and Engineering, Univer- sity of California, San Diego, La Jolla, USA 2Microsoft Re- search NYC, New York , USA. Correspondence to: Yuda Song <yus167@ucsd.edu>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 108, 2020. Copyright 2020 by the author(s). Mordatch et al., 2015), or meta-learn policies or models that can be quickly adapted to in-distribution environments (Finn et al., 2017a; Nagabandi et al., 2019a;b; Yu et al., 2018a). A key assumption for these approaches is that the distribu- tion of the target environments is known, and that it can be efﬁciently sampled during training. On out-of-distribution target environments, these methods typically do not deliver good performance, reﬂecting common challenges in gener- alization (Na et al., 2020). If we observe how humans and animals adapt to environment changes, clearly there is an online adaptation process in addition to memorization (Stad- don, 2016). We can quickly learn to walk with a slightly injured leg even if we have not experienced the situation. We draw experiences from normal walking and adapt our actions online, based on how their effects differ from what we are familiar with in normal settings. Indeed, this in- tuition has recently led to practical approaches for policy adaptation. The work in (Christiano et al., 2016) uses a pre- trained policy and model of the training environment, and learns an inverse dynamics model from scratch in the new environment by imitating the behaviors of the pre-trained policy. However, it does not involve mechanisms for ac- tively reducing the divergence between the state trajectories of the two environments, which leads to inefﬁciency and distribution drifting, and does not fully capture the intuition above. The work in (Zhu et al., 2018) uses Generative Ad- versarial Imitation Learning (GAIL) (Ho & Ermon, 2016) to imitate the source trajectories in the new environment, by adding the GAIL discriminator to the reward to reduce divergence, but relies on generic policy optimization meth- ods with high sample complexity. In general, these recent approaches show the feasibility of policy adaptation, but are not designed for optimizing sample efﬁciency. There has been no theoretical analysis of whether policy adaptation methods can converge in general, or their beneﬁts in terms of sample complexity. In this paper, we propose a new model-based approach for the policy adaptation problem that focuses on efﬁciency with theoretical justiﬁcation. In our approach, the agent at- tempts to predict the effects of its actions based on a model of the training environment, and then adapts the actions to minimize the divergence between the state trajectories in the new (target) environment and in the training (source) envi- ronment. This is achieved by iterating between two steps: a arXiv:2006.08051v1  [cs.LG]  14 Jun 2020Provably Efﬁcient Model-based Policy Adaptation modeling step learns the divergence between the source envi- ronment and the target environment, and a planning step that uses the divergence model to plan actions to reduce the di- vergence over time. Under the assumption that the target en- vironment is close to the source environment, the divergence modeling and policy adaption can both be done locally and efﬁciently. We give the ﬁrst theoretical analysis of policy adaptation by establishing the rate of convergence of our approaches under general settings. Our methods combine techniques from model-based RL (Wang et al., 2019) and no- regret online learning (Ross et al., 2011). We demonstrate that the approach is empirically efﬁcient in comparison to the state-of-the-art approaches (Christiano et al., 2016; Zhu et al., 2018). The idea of recovering state trajectories from the source environment in the target environment suggests a strong connection between policy adaptation and imitation learning, such as Learning from Observation (LfO) (Torabi et al., 2018; 2019a; Sun et al., 2019b; Yang et al., 2019). A key difference is that in policy adaptation, the connec- tion between the source and target environments and their difference provide both new challenges and opportunities for more efﬁcient learning. By actively modeling the diver- gence between the source and target environments, the agent can achieve good performance in new environments by only making local changes to the source policies and models. On the other hand, because of the difference in the dynamics and the action spaces, it is not enough to merely imitate the experts (Bain & Sommut, 1999; Ross et al., 2011; Sun et al., 2017). Traditionally, adaptive control theory ( ˚Astr¨om, 1983) studies how to adapt to disturbances by stabilizing the error dynamics. Existing work in adaptive control assumes closed-form dynamics and does not apply to the deep RL setting (Nagabandi et al., 2019a). In comparison to domain randomization and meta-learning approaches, our proposed approach does not require sampling of source environments during pre-training, and makes it possible to adapt in out- of-distribution environments. Note that the two approaches are complementary, and we demonstrate in experiments that our methods can be used in conjunction with domain randomization and meta-learning to achieve the best results. The paper is organized as follows. We review related work in Section 2 and the preliminaries in Section 3. In Section 4, we propose the theoretical version of the adaptation al- gorithm and prove its rate of convergence. In section 5 we describe the practical implementation using deviation mod- els and practical optimization methods. We show detailed comparison with competing approaches in Section 6. 2. Related Work Our work connects to existing works on imitation learning, online adaptation, domain randomization and meta-learning, and model-based reinforcement learning. Imitation Learning. In imitation learning, there is typically no separation between training environments and test en- vironments. Existing imitation learning approaches aim to learn a policy that generates state distributions (Tobin et al., 2017; Torabi et al., 2019b; Sun et al., 2019b; Yang et al., 2019) or state-action distributions (Ho & Ermon, 2016; Fu et al., 2017; Ke et al., 2019; Ghasemipour et al., 2019) that are similar to the ones given by the expert policy. The differ- ence in the policy adaptation setting is that the expert actions do not work in the ﬁrst place in the new environment, and we need to both model the divergence and ﬁnd a new policy for the target environment. In light of this difference, the work (Zhu et al., 2018) considers a setting that is the closest to ours (which we will compare with in the experiments). It uses a state-action-imitation (GAIL(Ho & Ermon, 2016)) approach to learn a policy in the target environment, to gen- erate trajectories that are similar to the trajectories of the expert from the source environments. It also relies on using the true reward signals in the target environment to train the policy besides state imitation. In recent work, (Liu et al., 2020) approaches a similar problem by using Wasserstein distance between the states as the reward. It uses adversarial training and model-free policy optimization methods. Our approach is model-based and relies on reduction to Data Aggregation (Ross et al., 2011) for efﬁciency. The reduc- tion allows us to derive provable convergence guarantee with the rate of convergence. Experimentally we show that our approach is more sample efﬁcient than model-free and minmax-based imitation approaches in general. Online Adaptation. Online adaptation methods transfer policies by learning a mapping between the source and target domains (Daftry et al., 2016; Tzeng et al., 2015). Such methods have achieved success in vision-based robotics but require extra kernel functions or learning feature spaces. In contrast, we focus on control problems where the policy adaptation is completely autonomous. (Christiano et al., 2016) trains an inverse dynamics model (IDM) which can serve as the target policy by inquiring the source policy online. However, the approach does not focus on optimizing sample efﬁciency, which is crucial for the use of policy adaptation. In (Yu et al., 2018b), the agent selects from a range of pre-trained policies online, and does not perform further adaptation, and thus experiences problems similar to domain randomization approaches. Domain Randomization and Meta-Learning. Domain randomization and meta-learning methods are popular ways of transferring pre-trained policies to new environments. These methods rely on the key assumption that the training environments and test environments are sampled from the same predeﬁned distribution. Domain randomization meth- ods train robust agents on diverse samples of target environ- ments (Tobin et al., 2017; Mordatch et al., 2015; Antonova et al., 2017; Chebotar et al., 2019). When the conﬁgurationProvably Efﬁcient Model-based Policy Adaptation (a) (b) (c) t= 0 t= 1 t= 2 t= 3 t= 4 Figure 1.(a) Halfcheetah of mass musing the source policy π(s) in the source environment M(s). (b) Halfcheetah of mass 1.5 ×m using the source policy π(s) in the target environment M(t). (c) Halfcheetah of mass 1.5 ×musing the learned target policy π(t) in M(t) with out method. Using the policy trained in the source environment without adapting it to the target environment yields suboptimal results. The adapted policy π(t) can recover gaits close to the source trajectory. of the target environment lies outside the training distribu- tion, there is no performance guarantee. In meta-learning, such as Model Agnostic Meta-Learning (MAML) (Finn et al., 2017a;b; Nagabandi et al., 2018), meta-learned dynam- ics policies and models can adapt to perturbed environments with notable success including in physical robots. How- ever, similar to domain randomization based approaches, they experience difﬁculties on new environments that are not covered by the training distribution. Our proposed ap- proach focuses on online adaption in unseen environments. It is orthogonal to domain randomization and meta-learning approaches. We show in experiments that these different approaches can be easily combined. Model-based Reinforcement Learning. Model-based re- inforcement learning (MBRL) provides a paradigm that learns the environment dynamics and optimizes the control actions at the same time. Recent work has shown that MBRL has much better sample efﬁciency compared to model-free approaches both theoretically and empirically (Tu & Recht, 2018; Chua et al., 2018; Sun et al., 2019a). Our setting is different from the traditional MBRL setting. We consider test environments that are different from the training envi- ronment, and adapt the policy from the training environment to the test environment. 3. Preliminaries We consider ﬁnite-horizon Markov Decision Processes (MDP) M= ⟨S,A,f,H,R ⟩with the following compo- nents. Sdenotes the state space, and Athe action space. The transition function f: S×S×A → [0,1] deter- mines the probability f(s′|s,a) of transitioning into state s′ from state s after taking action a. The reward func- tion R : S →R is deﬁned only on states. We write πθ to denote a stochastic policy πθ: S×A→ [0,1] param- eterized by θ. Each policy πθ determines a distribution over trajectories {(si,ai,ri)}H i=1 under a ﬁxed dynamics f. The goal of the agent is to maximize the expected cu- mulative reward J(θ) = Eπθ,f [∑H h=1 R(sh) ] over all pos- sible trajectories that can be generated by πθ. Without loss of generality, in the theoretical analysis we always as- sume the normalized total reward is in the [0,1] range, i.e., maxs1,...sH ∑H h=1 R(sh) ∈[0,1]. We write the set {1,2,...,N }as [N], and the uniform distribution over set Aas U(A) throughout the paper. For any two distributions d1 and d2, we use ∥d1 −d2∥to denote the total variation distance between the two distributions. 4. Policy Adaptation with Data Aggregation 4.1. Basic Deﬁnitions In policy adapation, we consider a pair of MDPs and call them a source MDP and a target MDP. We deﬁne the source MDP as M(s) := {S,A(s),f(s),H,R }and the target MDP as M(t) := {S,A(t),f(t),H,R }. Note that the two MDPs share the same state space and reward functions, but can have different action spaces and transition dynamics. Fig. 1 demonstrates the problem of adapting a policy from a source environment to a target environment. Because of the difference in the action space and dynamics, directly using good policies from the source environment (Fig.1(a))Provably Efﬁcient Model-based Policy Adaptation does not work in the target environment (Fig.1(b)). The objective is to adapt the policy from the source to the target environment to achieve good performance (Fig.1(c)). We focus on minimizing the samples needed for adaptation in the target MDP, by leveraging M(s) to quickly learn a policy in M(t). To achieve this, we assume that a pre- trained policy π(s) from M(s) achieves high rewards in M(s). We wish to adapt π(s) to a policy π(t) that works well in M(t). For ease of presentation, we considerπ(s) and π(t) as deterministic throughout the theoretical analysis. Given a policy π, we write E(s) π (·) for the expectation over random outcomes induced by πand M(s). We write d(s) π;h to denote the state distribution induced by πat time step h under M(s), and d(s) π = ∑H h=1 d(s) π;h/Has the average state distribution of π under M(s). We write ρ(s) π to represent the distribution of the state trajectories from π: for τ = {sh}H h=0, ρ(s) π (τ) = ∏H h=1 f(s)(sh|sh−1,π(sh−1)). For the target MDP M(t), we make the same deﬁnitions but drop the superscript (t) for ease of presentation. Namely, Eπ(·) denotes the expectation over the randomness from π and M(t), dπ denotes the induced state distribution of π under M(t), and ρπ denotes the state trajectory distribution. 4.2. Algorithm We now introduce the main algorithm Policy Adaptation with Data Aggregation (PADA). Note that this is the theo- retical version of the algorithm, and the practical implemen- tation will be described in detail in Section 5. To adapt a policy from a source environment to a target environment, PADA learns a model ˆf to approximate the target envi- ronment dynamics f(t). Based on the learned model, the algorithm generates actions that attempt to minimize the di- vergence between the trajectories in the target environment and those in the source environment generated by π(s) at M(s). Namely, the algorithm learns a policy π(t) that repro- duces the behavior ofπ(s) on M(s) in the target MDPM(t). Since the state space Sis often large, learning a model ˆf that can accurately approximate f(t) globally is very costly. Instead, we only aim to iteratively learn a locally accurate model, i.e., a model that is accurate near the states that are generated by π(t). This is the key to efﬁcient adaptation. The detailed algorithm is summarized in Alg. 1. Given a model ˆfe at the e-th iteration, we deﬁne the ideal policy π(t) e π(t) e (s) ≜ argmin a∈A(t) ∥ˆfe(·|s,a) −f(s)(·|s,π(s)(s))∥. (1) The intuition is that, assuming ˆfe is accurate in terms of modelling f(t) at state s, π(t) e (s) aims to pick an action such that the resulting next state distribution under ˆfe is similar to the next state distribution resulting from π(s) under the Algorithm 1 Policy Adaptation with Data Aggregation Require: Source domain policy π(s), source dynamics f(s), model class F 1: Initialize dataset D= ∅ 2: Initialize ˆf1 3: for e = 1, . . . Tdo 4: Deﬁne policy π(t) e as in Eq. 1 5: for n = 1, . . . Ndo 6: Reset M(t) to a random initial state 7: Uniformly sample a time step h∈[H] 8: Execute π(t) e in M(t) for hsteps to get state s 9: Sample exploration action a∼U(A(t)) 10: Take ain M(t) and get next state s′ 11: Add (s,a,s ′) into D(Data Aggregation) 12: end for 13: Update to ˆfe+1 via MLE by Eq. 2 14: end for 15: Output: {π(t) e }T e=1 source dynamics f(s). We then execute π(t) e in the target environment M(t) to generate a batch of data (s,a,s ′). We further aggregate the newly generated data to the dataset D(i.e., data aggregation). We update model to ˆfe+1 via Maximum Likelihood Estimation (MLE) on D: ˆfe+1 = argmax f∈F ∑ s,a,s′∈D log f(s′|s,a). (2) Note that Algorithm 1 relies on two black-box ofﬂine com- putation oracles: (1) a one-step minimization oracle (Eq. 1) and (2) a Maximum Likelihood Estimator (Eq. 2). In Sec- tion 5, we will introduce practical methods to implement these two oracles. We emphasize here that these two ora- cles are ofﬂine computation oracles and the computation itself does not require any fresh samples from the target environment M(t). 4.3. Analysis We now prove the performance guarantee of Alg.1 for policy adaptation and establish its rate of convergence. At a high level, our analysis of Alg. 1 is inspired from the analysis of DAgger (Ross et al., 2011; Ross & Bagnell, 2012) which leverages a reduction to no-regret online learning (Shalev- Shwartz et al., 2012). We will ﬁrst make the connection with the Follow-the-Leader (FTL) algorithm, a classic no-regret online learning algorithm, on a sequence of loss functions. We then show that we can transfer the no-regret property of FTL to performance guarantee on the learned policy π(t). Our analysis uses the FTL regret bound ˜O(1/T) where T is the number of iterations (Shalev-Shwartz et al., 2012). Since our analysis is a reduction to general no-regret online learn- ing, in theory we can also replace FTL by other no-regretProvably Efﬁcient Model-based Policy Adaptation online learning algorithms as well (e.g., Online Gradient De- scent (Zinkevich, 2003) and AdaGrad (Duchi et al., 2011)). Intuitively, for fast policy adaptation to succeed, one should expect that there is similarity between the source environ- ment and the target environment. We formally introduce the following assumption to quantify this. Assumption 4.1 (Adaptability). For any state action pair (s,a) with source action a ∈A(s) , there exists a target action a′∈A(t) in target environment, such that: ∥f(s)(·|s,a) −f(t)(·|s,a′)∥≤ ϵs,a, for some small ϵs,a ∈R+. Remark 4.2. When ϵs,a →0 in the above assumption, the target environment can perfectly recover the dynamics of the source domain at (s,a). However, ϵs,a = 0 does not mean the two transitions are the same, i.e.,f(t)(s,a) = f(s)(s,a). First the two action spaces can be widely different. Secondly, there may exist states s, where one may need to take com- pletely different target actions fromA(t) in order to match the source transition f(s)(·|s,a), i.e., ∃a′∈A(t) such that f(t)(·|s,a′) = f(s)(·|s,a), but a̸= a′. Assumption 4.3 (Realizability). Let the model class Fbe a subset of {f : S×S×A→ [0,1]}. We assume f(t) ∈F. Here we assume that our model class Fis rich enough to include f(t). Note that the assumption on realizability is just for analysis simplicity. Agnostic results can be achieved with more reﬁned analysis similar to (Ross et al., 2011; Ross & Bagnell, 2012). We deﬁne the following loss function: ℓe(f) ≜ Es∼d π(t) e ,a∼U(A(t)) [ DKL ( f(t)(·|s,a),f(·|s,a) )] , for all e ∈ [T]. The loss function ℓe(f) measures the difference between f(t) and f under the state distribution induced by π(t) e under M(t) and the uniform distribution over the action space. This deﬁnition matches the way we collect data inside each episode. We generate (s,a,s ′) triples via sampling sfrom dπ(t) e , afrom U(A(t)), and then s′∼f(t)(·|s,a). At the end of the iteration e, the learner uses FTL to compute ˆfe+1 as: ˆfe+1 = argmin f∈F e∑ i=1 ℓi(f). Using the deﬁnition of KL-divergence, it is straightforward to show that the above optimization is equivalent to the following Maximum Likelihood Estimation: argmax f∈F e∑ i=1 Es∼d π(t) e ,a∼U(A(t)),s′∼f(t)(·|s,a) [log f(s′|s,a)] . At the end of the episode e, the aggregated dataset Dcon- tains triples that are sampled based on the above procedure from the ﬁrst to the e-th episode. With no-regret learning on ˆfe, assumptions 4.1, and 4.3, we can obtain the following main results. We ﬁrst assume that the target environment M(t) has a discrete action space, i.e., A(t) is discrete, and then show that the result can be easily extended to continuous action spaces. Theorem 4.4 (Main Theorem). Assume M(t) has a dis- crete action space A(t) and denote A≜ |A(t)|. Among the sequence of policies computed in Alg. 1, there exists a policy ˆπsuch that: Es∼dˆπ∥f(t)(·|s,ˆπ(s)) −f(s)(·|s,π(s)(s))∥ ≤O ( AT−1/2 + Es∼dˆπ [ ϵs,π(s)(s) ]) , which implies that: ρˆπ −ρ(s) π(s) ≤O ( HAT−1/2 + HEs∼dˆπ [ ϵs,π(s)(s) ]) , where we recall that ρπ stands for the state-trajectory dis- tribution of policy π under M(t) and ρ(s) π(s) stands for the state-trajectory distribution of π(s) under M(s). The full proof is in the Appendix A. The theorem shows that our algorithm can provide a policy in the target en- vironment that induces trajectories close to those induced by the experts in the source environment. For instance, if the target and source MDPs are completely adaptable (i.e., ϵs,a = 0 in Assumption 4.1 for all (s,a)) and the number of iterations approach to inﬁnity, then we can learn a policy ˆπ that generates state trajectories in M(t) that match the state trajectories generated via the source policy π(s) at the source MDP M(s). Remark 4.5. The error Es∼dˆπ [ ϵs,π(s)(s) ] is averaged over the state distribution induced by the learned policy rather than in an ℓ∞form, i.e., maxs,aϵs,a. Although the analysis is done on discrete action space, the algorithm can be naturally applied to compact continuous action space as follows. The proof of the following corollary and its extension to the d-dimensional continuous action spaces are in the Appendix. Corollary 4.6 (Continuous Action Space). Assume A(t) = [0,1], f(t) and functions f ∈F are Lipschitz continuous with (and only with) actions in A(t). Among policies re- turned from Alg. 1, there exists a policy ˆπsuch that: ∥ρˆπ −ρ(s) π(s) ∥≤ O ( HT−1/4 + HEs∼dˆπ [ ϵs,π(s)(s) ]) . Remark 4.7. As we assume the reward function only de- pends on states, ∥ρˆπ −ρ(s) π(s) ∥ ≤δ implies |J(t)(ˆπ) − J(s)(π(s))|≤∥ ρˆπ −ρ(s) π(s) ∥ ( maxs1...sH ∑H h R(sh) ) ≤δProvably Efﬁcient Model-based Policy Adaptation due to the normalization assumption on the rewards. Thus, though our algorithm runs without rewards, when π(s) achieves high reward in the source MDP M(s), the algo- rithm is guaranteed to learn a policy ˆπthat achieves high rewards in the target environmentM(t). 5. Practical Implementation In Algorithm 1 we showed the theoretical version of our approach, which takes an abstract model class Fas input and relies on two ofﬂine computation oracles (Eq. 1 and Eq. 2). We now design the practical implementation by specifying the parameterization of the model class Fand the optimization oracles. Algorithm 2 shows the practical algorithm, and we explain the details in this section. 5.1. Model Parameterization In the continuous control environments, we focus on stochas- tic transitions with Gaussian noise, where f(t)(s,a) = ¯f(t)(s,a) + ϵ, f(s)(s,a) = ¯f(s)(s,a) + ϵ′, with ϵ and ϵ′ from N(0,Σ) and ¯f(t) and ¯f(s) being nonlinear determinis- tic functions. In this case, we consider the following model class with parameterization θ: F= {δθ(s,a) + ˆf(s)(s,π(s)(s)),∀s,a : θ∈Θ}. where ˆf(s) is a pre-trained model of the source dynam- ics f(s) and we assume ˆf(s) well approximates f(s) (and one has full control to the source environment such as the ability to reset). Then for each state s, ˆf(s)(s,π(s)(s)) is a ﬁxed distribution of the next state in the source environ- ment by following the source policy. Deﬁne ∆π(s) (s,a) ≜ ˆf(s)(s,π(s)(s)) −f(t)(s,a), which captures the deviation from taking action ain the target environment to following π(s) in the source environment. So δθ(s,a) is trained to approximate the deviation ∆π(s) (s,a). Note that learning ∆π(s) is just an alternative way to capture the target dynam- ics since we know ˆf(s)(s,π(s)) foresight, thus it should be no harder than learning f(t) directly. 5.2. Model Predictive Control For deterministic transition, Eq 1 reduces to one-step min- imization argmina∈A(t) ∥ˆfe(·|s,a) − ˆf(s)(s,π(s)(s))∥2. Since ˆfe ∈ F, we have ˆfe(s,a) = δθe(s,a) + ˆf(s)(s,π(s)(s)), and the optimization can be further sim- plﬁed to: argmina∈A(t) ∥δθe(s,a)∥2.We use the Cross En- tropy Method (CEM) (Botev et al., 2013) which iteratively repeats: randomly draw N actions, evaluate them in terms of the objective value ∥δθe(s,a)∥2, pick the top Kactions in the increasing order of the objective values, and then reﬁt a new Gaussian distribution using the empirical mean and covariance of the top Kactions. Algorithm 2 Policy Adaptation with Data Aggregation via Deviation Model Require: πs, ˆf(s), deviation model class {δθ : θ ∈Θ}, explore probability ϵ, replay buffer D, learning rate η 1: Randomly initialize divergence model δθ 2: for T Iterations do 3: for nsteps do 4: s←Reset M(t) 5: while current episode does not terminate do 6: With probability ϵ: a∼U(A(t)) 7: Otherwise: a←CEM(A(t),s,δ θ) 8: Execute ain M(t): s′←f(t)(s,a) 9: Update replay buffer: D←D∪{ (s,a,s ′)} 10: s←s′ 11: end while 12: end for 13: Update θwith Eq. 3 14: end for We emphasize here we only need to solve a one-step opti- mization problem without unrolling the system for multiple steps. We write the CEM oracle as CEM(A(t),s,δ θ) which outputs an action afrom A(t) that approximately minimizes ∥δθ(s,a)∥2. Here, CEM(A(t),s,δ θ) : S →A(t) can be considered as a policy that maps state sto a target action a. 5.3. Experience Replay for Model Update Note that Alg. 1 requires to solve a batch optimization prob- lem (MLE in Eq. 2) in every iteration, which could be com- putationally expensive in practice. We use Experience Re- play (Adam et al., 2011; Mnih et al., 2013), which is more suitable to optimize rich non-linear function approximators (δθ is a deep neural network in our experiments). Given the current divergence model δθ and the aggregated dataset D= {s,a,s ′}(aka, replay buffer) with s′ = f(t)(s,a), we randomly sample a mini-batch B ⊂D and perform a stochastic gradient descent step with learning rate η: θ←θ− η |B|∇θ (∑|B| i=1∥ˆf(s)(si,π(s)(si)) +δθ(si,ai) −s′ i∥22 ) . (3) 5.4. Policy Adaptation with Data Aggregation As show in Algorithm 2, we maintain a reply buffer that stores all experiences from the target model M(t) (Line 9) and constantly update the model δθ using mini-batch SGD (Eq. 3). Alg 2 performs local exploration in an ϵ-greedy way. We refer our method as Policy Adaptation with Data Aggregation via Deviation Model (PADA-DM). Remark 5.1. Even being one-step, CEM(A(t),s,δ θ) may be computationally expensive, we could obtain a MPC-free policy (target policy) by training an extra parameterizedProvably Efﬁcient Model-based Policy Adaptation policy to mimic CEM(A(t),s,δ θ) via techniques of Behav- ior Cloning (Bain & Sommut, 1999). When we train this extra parameterized policy, we name the method as PADA- DM with target policyand we will show it does not affect the performance of the overall algorithm during training. However, during test time, such parameterized policy runs faster than CEM and thus is more suitable to be potentially deployed on real-world systems. 6. Experiments In this section we compare our approach with the state- of-the-art methods for policy adaptation (Christiano et al., 2016; Zhu et al., 2018; Schulman et al., 2017; Finn et al., 2017a) and show that we achieve competitive results more efﬁciently. We also test the robustness of the approaches on multi-dimensional perturbations. We then compare to domain randomization and meta-learning approaches and show how they can be combined with our approach. We provide further experiments in Appendix D. Following the same experiment setup as (Christiano et al., 2016), We focus on standard OpenAI Gym (Brockman et al., 2016) and Mujoco (Todorov et al., 2012) control environ- ments such as HalfCheetah, Ant, and Reacher. We perturb the environments by changing their parameters such as mass, gravity, dimensions, motor noise, and friction. More details of task designs are in Appendix B.1. 6.1. Comparison with Existing Approaches We compare our methods (PADA-DM, PADA-DM with target policy) with the following state-of-the-art methods for policy adaptation. The names correspond to the learning curves shown in Figure 2. Christiano et al., 2016: (Christiano et al., 2016) uses a pre-trained policy π(s) and source dynamics f(s), to learn an inverse dynamics model φ: A×S×S→ [0,1], where φ(a|s,s′) is the probability of taking action athat leads to s′from s.1 That is, π(t)(s) = φ(s,f(s)(s,π(s)(s))). Zhu et al., 2018: (Zhu et al., 2018) proposed an approach for training policies in the target domain with a new reward λR(sh) + (1 −λ)Rgail(sh,ah), λ ∈[0,1]. Here Rgail is from the discriminator from GAIL. Note that this baseline has access to true reward signals while ours do not. For additional baselines, we also show the performance of directly running Proximal Policy Optimization ( PPO) (Schulman et al., 2017) in the target environment, as well as directly using the source policy in the perturbed environ- 1In general an inverse model is ill-deﬁned without ﬁrst spec- ifying a policy, i.e., via Bayes rule, φ(a|s,s′) ∝P(a,s,s ′) = f(t)(s′|s,a)π(a|s). Hence one needs to ﬁrst specify πin order to justify the existence of an inverse model φ(a|s,s′). ment without adapation. 7 8 9 10 11 12 13 14 Number of Timesteps (in log) 1000 0 1000 2000 3000 4000 5000 6000 Rewards HalfCheetah 120% Mass PADA-DM PADA-DM w/ target Christiano et al., 2016 PPO Zhu el al., 2018 7 8 9 10 11 12 13 14 Number of Timesteps (in log) 0 500 1000 1500 2000 2500 3000Rewards Ant 50% Mass Figure 3.The long-term learning curves. The x-axis is the number of timesteps in natural logarithm scale. Results. Figure 2 demonstrates the sample efﬁciency of our methods compared to the other methods and baselines. Both PADA-DM and PADA-DM with target policy con- verge within 10k to 50k training samples in the target envi- ronments. In contrast, (Christiano et al., 2016) requires 5 times more samples than our methods on average, and (Zhu et al., 2018) and PPO require about 30 times more. At con- vergence, our methods obtain the highest episodic rewards in 7 out of 8 tasks above among the policy adaptation meth- ods. The baseline performance of PPO is better than the policy adaptation methods in HalfCheetah and Reacher (re- call PPO uses true reward signals), but it takes signiﬁcantly longer as shown in Fig. 2. Note that in the Ant environment, even at convergence our methods outperform PPO as well. The only task where our methods failed to achieve top per- formance is Ant-v2 0.6 std motor noise. In this environment, the action noise causes high divergence between the tar- get and source environments, making it hard to efﬁciently model the domain divergence. All the adaptation methods deliver bad performance in this case, indicating the difﬁculty of the task. We observe that the learning curves of PADA-DM and PADA-DM with target policy are similar across all tasks without sacriﬁcing efﬁciency or performance. The target policy can be directly used without any MPC step. To further illustrate the sample efﬁciency of our method, we compare the long-term learning curves in Fig. 3. We plot the learning curves up to convergence of each method. We further include a long-term version of Fig 2 and the hyperparameters in the Appendix. 6.2. Performance on Multi-Dimensional Perturbations We further evaluate the robustness of our methods by per- turbing multiple dimensions of the target environment (Fig. 4). Note that online adaptation is particularly useful for multiple-dimension perturbations, because they generate an exponentially large space of source environments that are hard to sample ofﬂine. In Fig. 4(b), we show that even when perturbing 15 different degrees of freedom of the tar-Provably Efﬁcient Model-based Policy Adaptation 0 5000 10000 15000 20000 25000 30000 Number of Timesteps 1000 0 1000 2000 3000 4000 Rewards HalfCheetah Mass 150% 0 5000 10000 15000 20000 25000 30000 Number of Timesteps 0 1000 2000 3000 4000Rewards HalfCheetah 50% Gravity 0 5000 10000 15000 20000 25000 30000 Number of Timesteps 500 0 500 1000 1500Rewards HalfCheetah 0.4 std Motor Noise 0 5000 1000015000200002500030000 Number of Timesteps 300 250 200 150 100 50 0 Rewards Reacher-v2 10% first arm length 0 5000 10000 15000 20000 25000 30000 Number of Timesteps 0 500 1000 1500 2000Rewards Ant 50% Mass 0 5000 10000 15000 20000 25000 30000 Number of Timesteps 2000 1000 0 1000 2000 3000 Rewards Ant 200% Gravity 0 5000 10000 15000 20000 25000 30000 Number of Timesteps 1000 500 0 500 1000 Rewards Ant 0.6 std Action Noise 0 5000 1000015000200002500030000 Number of Timesteps 250 200 150 100 50 0 Rewards Reacher-v2 45 degree plane Figure 2.We plot the learning curves across 5 random seeds on a number of tasks. The title of each plot correspounds to the perturbation in the target domain, e.g., HalfCheetah Mass 150% means in mass of the agent in the target domain is 150% of that in the source domain. get environment, our adaptation method can still achieve competitive performance at a much faster rate than all the other methods. We record the details of the conﬁgurations of the target environments in Appendix B.2.7. 0 10000 20000 30000 40000 50000 Number of Timesteps 0 500 1000 1500 2000 2500 3000 3500Rewards Ant-v2 3 DOF PADA-DM PADA-DM w/ target Christiano et al., 2016 PPO Zhu el al., 2018 source policy (a) 0 10000 20000 30000 40000 50000 Number of Timesteps 0 500 1000 1500 2000 2500 3000 3500Rewards Ant-v2 15 DOF (b) Figure 4.Learning curves for: changing 3 (a) and 15 (b) conﬁgura- tions in the target environment. 6.3. Comparison with Domain Randomization and Meta-Learning We now compare with domain randomization and meta- learning approaches and show how they can be combined with our methods. single source domain randomized source domains no adaptation in target domain source policy DR adaptation in target domain PADA-DM PADA-DM w/ DR; MAML Table 1.Relationship of 5 methods in our ablation study. Domain randomization (DR) (Peng et al., 2018): During the training in the source domain, at the beginning of each episode, we randomly sample a new conﬁguration for the source domain within a speciﬁed distribution. The total number of training samples here is the same as that for training the source policy. The policy outputed from DR is used in the target environment without further adaptation. MAML: We adopt the RL version of MAML (Finn et al., 2017a) that meta-learns a model-free policy over a distribu- tion of source environments and performs few-shot adapta- tion on the target environment. We compare the following methods: (1) source policy trained in a ﬁxed source environment, (2) domain random- ization, (3) PADA-DM, (4) PADA-DM with DR (using domain randomization’s output asπ(s) for PADA), and (5) MAML. Table 1 shows how these methods relate to each other. For the ﬁrst four methods, we train the source policy with 2m samples and perform adaptation with 80k samples. For MAML, we use 500 batches of meta-training (400m sam- ples), and adapt 10 times with 80k samples in the target do- main. We perform 100 trajectories across 5 random seeds in the target domain for each method and compare the episodic reward in Figure 5. We ﬁrst observe that when the target domains lie in the distribution of domain randomization (70% −130%), domain randomization outperforms source policy signiﬁcantly, but does not help when the target lies far away from the distribution, which is the most notable short- coming of domain randomization. Note that using domain randomization in conjunction with our adaptation method usually yields the best results. Domain randomization oftenProvably Efﬁcient Model-based Policy Adaptation provides robustness within the environments’ distribution, and online adaptation in real target environment using our approach further ensures robustness to out-of-distribution environments. We also observe that our method provides the most stable performance given the smallest test variances. We include additional experiments and detailed numbers of the performances of all methods (mean and standard deviations) in Appendix D.4. 50% 80% 120% 150% 200% Gravity 0 2000 4000 6000Rewards algorithm MAML DR source policy PADA-DM PADA-DM w/ DR (a) 50% 80% 120% 150% 200% Mass 2000 0 2000 4000 6000 Rewards algorithm MAML DR source policy PADA-DM PADA-DM w/ DR (b) Figure 5.Ablation experiments using domain randomization and meta-learning. (a) Varying gravity. (b) Varying mass. 7. Conclusion We proposed a novel policy adaptation algorithm that com- bines techniques from model-based RL and no-regret online learning. We theoretically proved that our methods gener- ate trajectories in the target environment that converge to those in the source environment. We established the rate of convergence of the algorithms. We have shown that our al- gorithm achieves competitive performance across a diverse set of continuous control tasks with better sample efﬁciency. A natural extension is to use our approach on simulation-to- real problems in combination with domain randomization and meta-learning. As our experiments indicated that the combination of do- main randomization and our online adaptation approach together often yields good results, for future work, we plan to investigate general theoretical framework for combining domain randomization and online adaptive control tech- niques. References Adam, S., Busoniu, L., and Babuska, R. Experience re- play for real-time reinforcement learning control. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 42(2):201–212, 2011. Antonova, R., Cruciani, S., Smith, C., and Kragic, D. Re- inforcement learning for pivoting task. arXiv preprint arXiv:1703.00472, 2017. ˚Astr¨om, K. J. Theory and applications of adaptive controla survey. Automatica, 19(5):471–486, 1983. Bain, M. and Sommut, C. A framework for behavioural claning. Machine intelligence, 15(15):103, 1999. Botev, Z. I., Kroese, D. P., Rubinstein, R. Y ., and LEcuyer, P. The cross-entropy method for optimization. In Handbook of statistics, volume 31, pp. 35–59. Elsevier, 2013. Brockman, G., Cheung, V ., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W. Openai gym. CoRR, abs/1606.01540, 2016. URL http://arxiv. org/abs/1606.01540. Chebotar, Y ., Handa, A., Makoviychuk, V ., Macklin, M., Issac, J., Ratliff, N., and Fox, D. Closing the sim-to- real loop: Adapting simulation randomization with real world experience. In 2019 International Conference on Robotics and Automation (ICRA), pp. 8973–8979. IEEE, 2019. Christiano, P., Shah, Z., Mordatch, I., Schneider, J., Black- well, T., Tobin, J., Abbeel, P., and Zaremba, W. Transfer from simulation to real world through learning deep in- verse dynamics model. arXiv preprint arXiv:1610.03518, 2016. Chua, K., Calandra, R., McAllister, R., and Levine, S. Deep reinforcement learning in a handful of trials using proba- bilistic dynamics models. In Advances in Neural Infor- mation Processing Systems, pp. 4754–4765, 2018. Daftry, S., Bagnell, J. A., and Hebert, M. Learning trans- ferable policies for monocular reactive mav control. In International Symposium on Experimental Robotics, pp. 3–11. Springer, 2016. Duchi, J., Hazan, E., and Singer, Y . Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research , 12(Jul):2121– 2159, 2011. Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta- learning for fast adaptation of deep networks. In Proceed- ings of the 34th International Conference on Machine Learning-Volume 70, pp. 1126–1135. JMLR. org, 2017a. Finn, C., Yu, T., Zhang, T., Abbeel, P., and Levine, S. One- shot visual imitation learning via meta-learning. arXiv preprint arXiv:1709.04905, 2017b. Fu, J., Luo, K., and Levine, S. Learning robust rewards with adversarial inverse reinforcement learning. arXiv preprint arXiv:1710.11248, 2017. Ghasemipour, S. K. S., Zemel, R., and Gu, S. A divergence minimization perspective on imitation learning methods. arXiv preprint arXiv:1911.02256, 2019.Provably Efﬁcient Model-based Policy Adaptation Ho, J. and Ermon, S. Generative adversarial imitation learn- ing. In Advances in neural information processing sys- tems, pp. 4565–4573, 2016. Ke, L., Barnes, M., Sun, W., Lee, G., Choudhury, S., and Srinivasa, S. Imitation learning as f-divergence mini- mization. arXiv preprint arXiv:1905.12888, 2019. Liu, F., Ling, Z., Mu, T., and Su, H. State alignment- based imitation learning. In International Conference on Learning Representations, 2020. URL https:// openreview.net/forum?id=rylrdxHFDr. Mnih, V ., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. Mordatch, I., Lowrey, K., and Todorov, E. Ensemble-cio: Full-body dynamic motion planning that transfers to phys- ical humanoids. In 2015 IEEE/RSJ International Con- ference on Intelligent Robots and Systems (IROS) , pp. 5307–5314. IEEE, 2015. Na, D., Lee, H. B., Lee, H., Kim, S., Park, M., Yang, E., and Hwang, S. J. Learning to balance: Bayesian meta- learning for imbalanced and out-of-distribution tasks. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum? id=rkeZIJBYvr. Nagabandi, A., Clavera, I., Liu, S., Fearing, R. S., Abbeel, P., Levine, S., and Finn, C. Learning to adapt in dynamic, real-world environments through meta-reinforcement learning. arXiv preprint arXiv:1803.11347, 2018. Nagabandi, A., Clavera, I., Liu, S., Fearing, R. S., Abbeel, P., Levine, S., and Finn, C. Learning to adapt in dynamic, real-world environments through meta-reinforcement learning. ICLR, 2019a. Nagabandi, A., Finn, C., and Levine, S. Deep online learn- ing via meta-learning: Continual adaptation for model- based rl. ICLR, 2019b. Nair, V . and Hinton, G. E. Rectiﬁed linear units improve restricted boltzmann machines. InProceedings of the 27th international conference on machine learning (ICML-10), pp. 807–814, 2010. Peng, X. B., Andrychowicz, M., Zaremba, W., and Abbeel, P. Sim-to-real transfer of robotic control with dynam- ics randomization. In 2018 IEEE International Confer- ence on Robotics and Automation (ICRA), pp. 1–8. IEEE, 2018. Ross, S. and Bagnell, J. A. Agnostic system identiﬁcation for model-based reinforcement learning. arXiv preprint arXiv:1203.1007, 2012. Ross, S., Gordon, G., and Bagnell, D. A reduction of imita- tion learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artiﬁcial intelligence and statistics , pp. 627–635, 2011. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Shalev-Shwartz, S. et al. Online learning and online convex optimization. Foundations and Trends R⃝in Machine Learning, 4(2):107–194, 2012. Staddon, J. E. Adaptive behavior and learning. Cambridge University Press, 2016. Sun, W., Venkatraman, A., Gordon, G. J., Boots, B., and Bagnell, J. A. Deeply aggrevated: Differentiable imita- tion learning for sequential prediction. In Proceedings of the 34th International Conference on Machine Learning- Volume 70, pp. 3309–3318. JMLR. org, 2017. Sun, W., Jiang, N., Krishnamurthy, A., Agarwal, A., and Langford, J. Model-based rl in contextual decision pro- cesses: Pac bounds and exponential improvements over model-free approaches. In Conference on Learning The- ory, pp. 2898–2933, 2019a. Sun, W., Vemula, A., Boots, B., and Bagnell, J. A. Provably efﬁcient imitation learning from observation alone. arXiv preprint arXiv:1905.10948, 2019b. Tan, J., Zhang, T., Coumans, E., Iscen, A., Bai, Y ., Hafner, D., Bohez, S., and Vanhoucke, V . Sim-to-real: Learning agile locomotion for quadruped robots. arXiv preprint arXiv:1804.10332, 2018. Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W., and Abbeel, P. Domain randomization for transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 23–30. IEEE, 2017. Todorov, E., Erez, T., and Tassa, Y . Mujoco: A physics engine for model-based control. In IROS 2012, pp. 5026– 5033. IEEE, 2012. Torabi, F., Warnell, G., and Stone, P. Behavioral cloning from observation. In Proceedings of the 27th Interna- tional Joint Conference on Artiﬁcial Intelligence , pp. 4950–4957, 2018. Torabi, F., Warnell, G., and Stone, P. Imitation learning from video by leveraging proprioception. In Proceedings of the 28th International Joint Conference on Artiﬁcial Intelligence, pp. 3585–3591. AAAI Press, 2019a.Provably Efﬁcient Model-based Policy Adaptation Torabi, F., Warnell, G., and Stone, P. Recent advances in imitation learning from observation. In Proceedings of the 28th International Joint Conference on Artiﬁcial Intelligence, pp. 6325–6331. AAAI Press, 2019b. Tu, S. and Recht, B. The gap between model-based and model-free methods on the linear quadratic regulator: An asymptotic viewpoint. arXiv preprint arXiv:1812.03565, 2018. Tzeng, E., Devin, C., Hoffman, J., Finn, C., Abbeel, P., Levine, S., Saenko, K., and Darrell, T. Adapting deep vi- suomotor representations with weak pairwise constraints. arXiv preprint arXiv:1511.07111, 2015. Wang, T., Bao, X., Clavera, I., Hoang, J., Wen, Y ., Langlois, E., Zhang, S., Zhang, G., Abbeel, P., and Ba, J. Bench- marking model-based reinforcement learning. CoRR, abs/1907.02057, 2019. URL http://arxiv.org/ abs/1907.02057. Yang, C., Ma, X., Huang, W., Sun, F., Liu, H., Huang, J., and Gan, C. Imitation learning from observations by minimizing inverse dynamics disagreement. In Advances in Neural Information Processing Systems, pp. 239–249, 2019. Yu, T., Finn, C., Xie, A., Dasari, S., Zhang, T., Abbeel, P., and Levine, S. One-shot imitation from observing hu- mans via domain-adaptive meta-learning. arXiv preprint arXiv:1802.01557, 2018a. Yu, W., Liu, C. K., and Turk, G. Policy transfer with strategy optimization. arXiv preprint arXiv:1810.05751, 2018b. Zhu, Y ., Wang, Z., Merel, J., Rusu, A., Erez, T., Cabi, S., Tunyasuvunakool, S., Kramr, J., Hadsell, R., de Freitas, N., and Heess, N. Reinforcement and imitation learning for diverse visuomotor skills. In Proceedings of Robotics: Science and Systems, Pittsburgh, Pennsylvania, June 2018. doi: 10.15607/RSS.2018.XIV .009. Zinkevich, M. Online convex programming and generalized inﬁnitesimal gradient ascent. In Proceedings of the 20th International Conference on Machine Learning (ICML- 03), pp. 928–936, 2003.Provably Efﬁcient Model-based Policy Adaptation A. Detailed Analysis of Algorithm 1 (Proof of Theorem 4.4) In this section, we provide detailed proof for Theorem 4.4. Consider a Markov Chain p: S→ ∆(S) over horizon H. Denote dp as the induced state distribution under pand ρp as the induced state trajectory distribution under p. Lemma A.1. Consider two Markov Chains pi : S→ ∆(S) with i∈{1,2}. If Es∼dp1 [∥p1(·|s) −p2(·|s)∥] ≤δ, then for trajectory distributions, we have: ∥ρp1 −ρp2 ∥≤ O(Hδ). The above lemma implies that if the Markov chain p2 can predict the Markov chain p1 under the state distribution induced by p1, then we can guarantee that the state-wise trajectory distributions from p1 and p2 are close as well. Proof. Denote ρp,s1,...sh as the trajectory distribution induced by pconditioned on the ﬁrst hmany states are equal to s1,...,s h. Denote p(·|s0) as the initial state distribution for s1 with s0 being a faked state. By deﬁnition, we have: ∥ρp1 −ρp2 ∥= ∑ τ |ρp1 (τ) −ρp2 (τ)| = ∑ s1,...sH ⏐⏐⏐⏐⏐ H∏ h=1 p1(sh|sh−1) − H∏ h=1 p2(sh|sh−1) ⏐⏐⏐⏐⏐ = ∑ s1,...sH ⏐⏐⏐⏐⏐ H∏ h=1 p1(sh|sh−1) −p1(s1|s0) H∏ h=2 p2(sh|sh−1) + p1(s1|s0) H∏ h=2 p2(sh|sh−1) − H∏ h=1 p2(sh|sh−1) ⏐⏐⏐⏐⏐ ≤ ∑ s1 p1(s1|s0) ∑ s2,...,sH ⏐⏐⏐⏐⏐ H∏ h=2 p1(sh|sh−1) − H∏ h=2 p2(sh|sh−1) ⏐⏐⏐⏐⏐+ ∑ s1 |p1(s1|s0) −p2(s1|s0)| ( ∑ s2,...,sH H∏ h=2 p2(sh|sh−1) ) = Es1∼p1 [∥ρp1,s1 −ρp2,s1 ∥] + ∥p1(·|s0) −p2(·|s0)∥ ≤Es1,s2∼p1 [∥ρp1,s1,s2 −ρp2,s1,s2 ∥] + ∥p1(·|s0) −p2(·|s0)∥+ Es1∼dπ1;1 [∥p1(·|s1) −p2(·|s1)∥]. Recursively applying the same operation on ∥ρp1,s1 −ρp2,s1 ∥to time step H, we have: ∥ρp1 −ρp2 ∥≤ H∑ h=1 Esh∼dp1;h [∥p1(·|sh) −p2(·|sh)∥] ≤Hδ, where we recall dπ = ∑H h=1 dπ;h/H by deﬁnition. Extension to continuous state can be achieved by simply replaying summation by integration. The next lemma shows that by leveraging the no-regret property of FTL, we can learn a locally accurate model. Lemma A.2 (Local Accuracy of the Learned Model). Denote the sequence of models learned in Alg. 1 as {ˆf1,..., ˆfT}, there exists a model ˆf ∈{ˆf1,..., ˆfT}such that: Es∼dπˆf [ Ea∼U(A(t)) [ DKL(f(t)(·|s,a), ˆf(·|s,a)) ]] ≤O(1/T), where πf(s) ≜ argmina∈A(t) ∥f(·|s,a) −f(s)(·|s,π(s)(s)) for all s∈S for any f.Provably Efﬁcient Model-based Policy Adaptation Proof. Denote loss function ℓe(f) as: ℓe(f) ≜ Es∼d π(t) e ,a∼U(A(t)) [ Es′∼f(t) s,a [−log(f(s′|s,a))] ] . Since Alg. 1 is equivalent to running FTL on the sequence of strongly convex loss functions {ℓe(f)}T e=1, we have (Shalev- Shwartz et al., 2012): T∑ e=1 ℓe( ˆfe) ≤min f∈F T∑ e=1 ℓe(f) + O(log T) . Add ∑T e=1 Es∼d π(t) e ,a∼U(A(t))[Es′∼f(t) s,a log f(t)(s′|s,a)] on both sides of the above inequality and using the deﬁnition of KL divergence, we have: T∑ e=1 Es∼d π(t) e ,a∼U(A(t)) [ DKL(f(t)(·|s,a), ˆfe(·|s,a)) ] ≤min f∈F T∑ e=1 Es∼d π(t) e ,a∼U(A(t)) [ DKL(f(t)(·|s,a),f(·|s,a)) ] + O(log(T)) = O(log(T)), where the last equality comes from the realizability assumption f(t) ∈F. Using the fact that the minimum among a sequence is less than the average of the sequence, we arrive: min ˆf∈{ˆfe}T e=1 Es∼dπˆf ,a∼U(A(t)) [ DKL(f(t)(·|s,a), ˆf(·|s,a)) ] ≤ ˜O(1/T). The above lemma indicates that as T →∞, we will learn a model ˆf which is close to the target true model f(t) under the state distribution induced by πˆf. But it does not state the difference between the behavior generated by πˆf at the target domain and the behavior generated by π(s) at the source domain. The next lemma uses the deﬁnition πˆf to show that when executing πˆf in the target domain M(t), πˆf can actually generates behavior that is similar to the behavior generated by π(s) in the source domain M(s). Lemma A.3 (The Behavior of πˆf). Denote dπˆf as the state distribution induced by πˆf induced at M(t) (target domain). Es∼dπˆf ∥f(t)(·|s,π ˆf(s)) −f(s)(·|s,π(s)(s))∥≤ O(|A(t)|/ √ T) + Es∼dπˆf [ ϵs,π(s)(s) ] , where we recall the deﬁnition of ϵin Assumption 4.1. Proof. Consider the Markov chain that is deﬁned with respect to f(t) and πˆf, i.e., f(t)(s′|s,π ˆf(s)). Denote the state distri-Provably Efﬁcient Model-based Policy Adaptation bution induced by f(t)(s′|s,π ˆf(s)) at the target domain as dπˆf. Let us bound Es∼dπˆf ∥f(t)(·|s,π ˆf(s)) −f(s)(·|s,π(s)(s))∥. Es∼dπˆf ∥f(t)(·|s,π ˆf(s)) −f(s)(·|s,π(s)(s))∥ ≤Es∼dπˆf ∥f(t)(·|s,π ˆf(s)) −ˆf(·|s,π ˆf(s))∥+ Es∼dπˆf ∥ˆf(·|s,π ˆf(s)) −f(s)(·|s,π(s)(s))∥ ≤ √ Es∼dπˆf DKL(f(t)(·|s,π ˆf(s)), ˆf(·|s,π ˆf(s))) + Es∼dπˆf ∥ˆf(·|s,π ˆf(s)) −f(s)(·|s,π(s)(s))∥ ≤O(|A(t)|/ √ T) + Es∼dπˆf ∥ˆf(·|s,π ˆf(s)) −f(s)(·|s,π(s)(s))∥ ≤O(|A(t)|/ √ T) + Es∼dπˆf ∥ˆf(·|s,πf(t) (s)) −f(s)(·|s,π(s)(s))∥ ≤O(|A(t)|/ √ T) + Es∼dπˆf ∥ˆf(·|s,πf(t) (s)) −f(t)(·|s,πf(t) (s))∥ + Es∼dπˆf ∥f(t)(·|s,πf(t) (s)) −f(s)(·|s,π(s)(s))∥ ≤O(|A(t)|/ √ T) + √ Es∼dπˆf DKL(f(t)(·|s,πf(t) (s)), ˆf(·|s,πf(t) (s))) + Es∼dπˆf [ ϵs,π(s)(s) ] = O(|A(t)|/ √ T) + Es∼dπˆf [ ϵs,π(s)(s) ] , where the ﬁrst inequality uses triangle inequality, the second inequality uses Pinsker’s inequality, the third inequality uses the local accuracy of the learned model ˆf (Lemma A.2), the fourth inequality uses the deﬁnition that πˆf, and the ﬁfth inequality uses triangle inequality again, and the sixth inequality uses Pinsker’s inequality again with the deﬁnition of adaptive ability together with the deﬁnition of πf(t) (s) ≜ argmina∼A(t) ∥f(t)(·|s,a) −f(s)(·|s,π(s)(s))∥. Proof of Theorem 4.4. Use Lemma A.1 and Lemma A.3 and consider f(s)(·|s,π(s)(s)) as a Markov chain, we have that: ∥ρt πˆf −ρs πs∥≤ O(H|A(t)|/ √ T) + Hϵ, where we denote ϵ:= Es∼dπˆf [ ϵs,π(s)(s) ] This concludes the proof of Theorem 4.4. A.1. Extension to Continuous Action Space (Proof of Corollary 4.6) For simplicity, we consider A(t) = [0,1].2 We consider Lipschitz continuous transition dynamics with and only with respect to actions, i.e., ∥f(·|s,a) −f(·|s,a′)∥≤ L|a−a′|, (4) where Lis a Lipschitz constant. We emphasize here that we only assume Lipschitz continuity with respect to action in M(t). Hence this is a much weaker assumption than the common Lipschitz continuity assumption used in RL community, which requires Lipschitz continuity in both action and state spaces. We also assume that our function class Fonly contains function approximators that are Lipschitz continuous with respect to action a(e.g., feedforward fully connected ReLu network is Lipschitz continuous). Proof of Corollary 4.6. For analysis purpose, let us discretize the action space into bins with size δ ∈(0,1). Denote the discrete action set ¯A(t) = {0.5δ,1.5δ,2.5δ,..., 1 −0.5δ}(here we assume 1/δ= N+). Here |¯A(t)|= 1/δ. Now consider the following quantity: Es∼dπˆf ∥f(t)(·|s,ˆa) −ˆf(·|s,ˆa)∥, for any ˆa. Without loss of generality, we assume ˆa∈[0,δ]. Via Pinsker’s inequality and Lemma A.2, we have: Es∼dπˆf Ea∼U([0,1])∥f(t)(·|s,a) −ˆf(·|s,a)∥≤ O(1/ √ T), 2We can always normalize action to [0,1].Provably Efﬁcient Model-based Policy Adaptation which implies that: Es∼dπˆf Ea∼U([0,δ])∥f(t)(·|s,a) −ˆf(·|s,a)∥≤ O(1/(δ √ T)). We proceed as follows: Es∼dπˆf Ea∼U([0,δ])∥f(t)(·|s,a) −ˆf(·|s,a)∥ = Es∼dπˆf Ea∼U([0,δ])∥f(t)(·|s,ˆa+ a−ˆa) −ˆf(·|s,ˆa+ a−ˆa)∥ ≥Es∼dπˆf Ea∼U([0,δ]) ( ∥f(t)(·|s,ˆa) −ˆf(·|s,ˆa)∥−2L|a−ˆa| ) = Es∼dπˆf ∥f(t)(·|s,ˆa) −ˆf(·|s,ˆa)∥−2LEa∼U([0,δ])|a−ˆa| ≥Es∼dπˆf ∥f(t)(·|s,ˆa) −ˆf(·|s,ˆa)∥−2LEa∼U([0,δ])δ = Es∼dπˆf ∥f(t)(·|s,ˆa) −ˆf(·|s,ˆa)∥−2Lδ, where the ﬁrst inequality uses the fact that ˆa ∈[0,δ] and the Lipschitz conditions on both f(t) and ˆf ∈F, the second inequality uses the fact that |a−ˆa|≤ δfor any a∈[0,δ] as ˆa∈[0,δ]. Hence, we have: Es∼dπˆf ∥f(t)(·|s,ˆa) −ˆf(·|s,ˆa)∥≤ 2Lδ+ O(1/(δ √ T)) = O(T−1/4),∀ˆa∈A(t), where in the last step we set δ= Θ(T−1/4). Now, we can simply repeat the process we have for proving Lemma A.3, we will have: Es∼dπˆf ∥f(t)(·|s,π ˆf(s)) −f(s)(·|s,π(s)(s))∥≤ O(T−1/4) + ϵ. Combine with Lemma A.1, we prove the corollary for continuous action setting. For general n-dim action space, our bound will scale in the order O(√nT−1/(2n+2)) + ϵ. The proof of the n-dim result is similar to the proof of the 1-d case and is included below for completeness: Proof for n-dim Action Space. For n-dimensional action space, we have: Es∼dπˆf Ea∼U([0,δ]n)∥f(t)(·|s,a) −ˆf(·|s,a)∥≤ O(1/(δn√ T)). Using the Lipschitz property: Es∼dπˆf Ea∼U([0,δ]n)∥f(t)(·|s,a) −ˆf(·|s,a)∥ = Es∼dπˆf Ea∼U([0,δ]n)∥f(t)(·|s,ˆa+ a−ˆa) −ˆf(·|s,ˆa+ a−ˆa)∥ ≥Es∼dπˆf Ea∼U([0,δ]n) ( ∥f(t)(·|s,ˆa) −ˆf(·|s,ˆa)∥−2L∥a−ˆa∥ ) = Es∼dπˆf ∥f(t)(·|s,ˆa) −ˆf(·|s,ˆa)∥−2LEa∼U([0,δ]n)∥a−ˆa∥ ≥Es∼dπˆf ∥f(t)(·|s,ˆa) −ˆf(·|s,ˆa)∥−2LEa∼U([0,δ]n) √nδ = Es∼dπˆf ∥f(t)(·|s,ˆa) −ˆf(·|s,ˆa)∥−2L√nδ, Combining with above leads to Es∼dπˆf ∥f(t)(·|s,ˆa) −ˆf(·|s,ˆa)∥≤ 2√nLδ+ O(1/(δn√ T)) = O(√nT−1/(2n+2)) where in the last step we set δ= Θ((T n) −1 2(n+1) ). Finally we have: Es∼dπˆf ∥f(t)(·|s,π ˆf(s)) −f(s)(·|s,π(s)(s))∥≤ O(√nT−1/(2n+2)) + ϵ.Provably Efﬁcient Model-based Policy Adaptation B. Detailed description of our experiments B.1. Environment descriptions. For each of the four environments (HalfCheetah, Ant, Reacher) we tested on, we include a detailed numerical description of the environments in table 2. B.2. Descriptions of the perturbations B.2.1. C HANGING GRAVITY . We change the gravity in the whole Mujoco locomotion task by a max range from 50% to 200% of the normal gravity. The normal gravity is set to 0 on xand yaxis and −9.81 on zaxis. B.2.2. C HANGING MASS . We change the mass of the agent in the Mujoco locomotion task by a max range from 50% to 200% of its original mass. Note that most agents are composed of links with independent masses, so besides changing the agent’s mass as a whole, we also design experiments that change the mass of each individual links of the agent respectively. B.2.3. C HANGING PLANE ORIENTATION . In the Reacher task, we tilt the platform of the Reacher so that it forms an angle of 45 degree of the original plane. B.2.4. C HANGING ARM LENGTH . In the Reacher task, we change the ﬁrst link of the Reacher arm (which is composed of two parts) into one tenth of its original length. B.2.5. C HANGING FRICTION . We change the frictional coefﬁcient in the environment on an uniform scale. We found if friction is the only change in the target domain, then the adaptation task is relatively simple, so we incorporate it as one of the changes in the Multi-dimensional perturbation task. B.2.6. M OTOR NOISE . This task tries to mimic the motor malfunction or inaccuracy in the real world. After the agent outputs an action, we add on a noise from a normal distribution with mean 0 and a ﬁxed standard deviation from 0.2 to 1. B.2.7. M ULTIPLE DOF S OF CHANGES . In the 3DOF task, we set the gravity to 0.8, mass to 1.2 and friction coefﬁcient to 0.9. In the 15DOF task, we uniformly sample a coefﬁcient from 0.9 to 1.1 for each of the following conﬁguration: gravity, friction coefﬁcient and the mass of each joint of the agent. We record these changes and apply for all comparing methods. Figure 6.Visual illustration of modiﬁed Reacher environment. left: 45 degrees of tilted plane. right: Reacher with 10% length of its ﬁrst arm.Provably Efﬁcient Model-based Policy Adaptation Environment name Full state space size Model agnostic state size3 Action space size Reward function HalfCheetah 18 1 6 forward reward - 0.1 ×control cost Ant 29 2 8 forward reward - 0.5 ×control cost - 0.0005 ×contact cost + survive reward Reacher 16 4 2 forward distance - control cost Table 2.Description of the OpenAI gym environments. Note that to enforce safety of the agent in the target environment, we make a modiﬁcation on HalfCheetah environment such that the episode will be terminated when the cheetah falls off. B.3. Hyperparameters source PADA-DM PADA-DM with target Christiano et al., 2016 Zhu et al., 2018 PPO # timesteps 2e6 8e4 (5e4,8e4,12e4,15e4) 8e4 (5e4,8e4,1.2e5,1.5e5) 2e5 (1e5,2e5,4e5) 2e6 2e6 learning rate (with linear decay) 7e-4 5e-3 5e-3 5e-3 7e-4 7e-4 soft update rate every 3000 timesteps (3000,5000,10000) explore rate ϵ 0.01 (0.01,0.02) 0.01 (0.01,0.02) reward tradeoff λ 0.5 Table 3.Final hyperparameters we use for our methods and baselines in the experiments. The values in the brackets are the value we considered. C. Implementation details C.1. Pretraining of the source dynamics model In section 5.1, one assumption we make is that we have a pretrained model ˆf(s) that well approximates the source dynamics f(s). In practice, we pretrain the model ˆf(s) with the (s,a,s ′) triplets generated during the training ofπ(s). The model ˆf(s) is a two-layer neural network with hidden size 128 and ReLU nonlinearity, trained with MSE loss since we assume deterministic transitions. Using these existing samples has two major advantages: ﬁrst is that we don’t need further interaction with the source environment and second is that the trained model ˆf(s) especially well approximates the transitions around the actions taken by π(s), which is important to our algorithm. Remark that if we already have the ground truth source dynamics f(s), which is a mild assumption while using a simulator, we can also directly use f(s) to replace ˆf(s). During our experiments, we observe that whether using f(s) or ˆf(s) won’t affect the performance of our method. C.2. Cross Entropy Method Here we provide a pseudocode of the Cross Entropy Method that we used in our method, as in Alg. 3. In our implementation, we use T = 10 iterations, N = 200 actions sampled per iteration, K = 40 elites (elite ratio 0.4) and σ0 = 0.5. We use the output of target policy as the initial mean, and when we don’t have the target policy, we useπ(s)(s) as the initial mean. To avoid invalid actions, for each actionai we sample, we clip the action if certain dimension is out of the bound of A(t) (usually [-1,1] on each dimension). 3This means the number of states that won’t be passed as inputs to models or policies, e.g., the current coordinate or location of the agent.Provably Efﬁcient Model-based Policy Adaptation Algorithm 3 Cross Entropy Method Require: Initial mean µ0, initial standard deviation σ0, action space A(t), current model δθ, current state s, number of iterations T, sample size N, elite size K. 1: Σ0 ←I|A(t)|(σ2 0) 2: for t = 1, . . . Tdo 3: Sample {ai}N i=1 ∼N(µt−1,Σt−1) 4: {ai}N i=1 ←clip(ai,min(A(t)),max(A(t))) 5: Sort {ai}with descending ∥δθ(s,ai)∥2 2 and pick the ﬁrst K actions {aj}K j=1 6: µt ← 1 KΣK j=1aj 7: Σt ← 1 KΣK j=1(aj −µt)(aj −µt)T 8: end for 9: Output: µT D. Supplemental experiments D.1. Accuracy of Deviation Model We evaluate the performance of the deviation model by comparing its output with the actual deviation (i.e.,∆π(s) ) in the target and source environment states. We compare the performance between linear and nonlinear deviation models. We include linear models as they are convenient if we want to use optimal control algorithms. The nonlinear model is the same model we use for our PADA-DM algorithm, which has two 128-neuron layers with ReLU (Nair & Hinton, 2010) nonlinearity. Both of the deviation models are tested on the same initial state distribution after training on 80k samples. In 7(left), we plot the output of the deviation model against the ground truth deviation. We test on 50 trajectories and each data point refers to the average L2 state distance along one trajectory. In 7(right), we plot the ground truth deviation, and the outputs of the linear and nonlinear deviation models over time, on 10 test trajectories. 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 DM output 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00real dynamics divergence linear DM nonlinear DM Figure 7.Comparing the performances of the linear and nonlinear deviation models. left: This plot depicts the correlation between the predicted deviation and the actual deviation. The nonlinear deviation model is more accurate since its slope is closer to 1. right: This plot shows the predicted and actual deviation over the course of 10 trajectories. Here again, the nonlinear model (orange) curve lies very close to the actual deviation curve (green). D.2. Long-term learning curves In this section we show a more comprehensive long-term learning curve in Fig. 8. Each task here corresponds to the task in Fig. 2. Note that here again the x-axis is in natural logarithm scale.Provably Efﬁcient Model-based Policy Adaptation 7 8 9 10 11 12 13 14 Number of Timesteps (in log) 1000 0 1000 2000 3000 4000 5000 Rewards HalfCheetah 150% mass 7 8 9 10 11 12 13 14 Number of Timesteps (in log) 0 1000 2000 3000 4000Rewards HalfCheetah 50% Gravity 7 8 9 10 11 12 13 14 Number of Timesteps (in log) 1000 500 0 500 1000 1500 2000 2500 Rewards HalfCheetah 0.4 std Motor Noise 7 8 9 10 11 12 13 14 Number of Timesteps (in log) 300 250 200 150 100 50 0 Rewards Reacher-v2 10% first arm length 7 8 9 10 11 12 13 14 Number of Timesteps (in log) 0 500 1000 1500 2000 2500 3000Rewards Ant 50% Mass 7 8 9 10 11 12 13 14 Number of Timesteps (in log) 2000 1000 0 1000 2000 3000 Rewards Ant 200% Gravity 7 8 9 10 11 12 13 14 Number of Timesteps (in log) 1500 1000 500 0 500 1000 Rewards Ant 0.6 std Action Noise 7 8 9 10 11 12 13 14 Number of Timesteps (in log) 250 200 150 100 50 0 Rewards Reacher-v2 45 degree plane Figure 8.We plot the learning curves across 5 random seeds of our adaptation method (using PADA-DM and PADA-DM with target), and the baseline methods on a number of tasks including perturbation of the mass, gravity, motor noise for the locomotion environments (HalfCheetah and Ant), and the plane angle and arm lengths for the navigation environment (Reacher). The title of each plot corresponds to the perturbation in the target domain, e.g., HalfCheetah Mass 150% means in mass of the agent in the target domain is 150% of that in the source domain. The shaded area denotes 1 standard deviation range from the mean. D.3. Comparing using true reward To further verify the efﬁciency of our choice of reward (the deviations between two environments), we conduct an additional experiments where we use the ground truth reward for CEM. We ﬁx all the hyperparameters and train an additional model to learn to ground truth reward. To ensure the fairness of the comparison, when we use the ground truth reward, we keep doing 1 step look-ahead during CEM. The results verify that the deviation serves as a better reward for conducting policy adaptation, where the ground truth reward leads to a local minimum that results in suboptimal performance. 0 1 2 3 4 5 6 7 8 Number of Timesteps 1e4 0 1000 2000 3000 4000Rewards HalfCheetah 150% Mass PADA-DM PADA-DM w/ target true reward Figure 9.Comparing learning ground truth reward, learning deviations quickly adapts the policy in the target domain, where using ground truth reward may not necessarily leads to optimal performance in the target domain. D.4. Additional Experiments for Meta-Learning MAML In this section we conduct an additional experiment to Section 6.3. In section 6.3, we use 80k samples of adaptation for our method and MAML to conduct a fair comparison. However, using so many samples for adaptation contradicts MAML’s claim of few-shot adaptation and we also observe that MAMLs test performance does not improve too much as we change the sample size in adaptation phase. Thus here we report the additional experiments to support this claim: we adopt the same experiment setting in Section 6.3, and this time we use 20k samples for MAML during adaptation. The test performanceProvably Efﬁcient Model-based Policy Adaptation is recorded in Fig. 10(a) and Fig. 11(a). Comparing with the original performance (Fig. 10(b) and Fig. 11(b)), the test performance of MAML does not change that much as the number of adaptation samples decreases and our approach still outperforms MAML consistently. In addition, we record the mean and the standard deviation of the test performance of each method to deliver a more direct comparison in Table 4 and Table 5. As we can see, our approach outperforms other baselines most of the time. When the perturbation is small (e.g., the 120% columns in both tables), DR also delivers very strong performances. However when perturbation is large (e.g., 200% columns in both tables), DR fails to adapt completely, which indicates that DR has troubles to adapt to out-of-distribution new environments. 50% 80% 120% 150% 200% Gravity 0 2000 4000 6000Rewards algorithm MAML DR source policy PADA-DM PADA-DM w/ DR (a) 50% 80% 120% 150% 200% Gravity 0 2000 4000 6000Rewards algorithm MAML DR source policy PADA-DM PADA-DM w/ DR (b) Figure 10.Ablation experiments using domain randomization and meta-learning. (a) MAML with 20k adaptation samples. (b) MAML with 80k adaptation samples. The boxplots show the median of the data while more statistics such as mean and standard deviation are shown in the following tables. 50% 80% 120% 150% 200% Mass 2000 0 2000 4000 6000 Rewards algorithm MAML DR source policy PADA-DM PADA-DM w/ DR (a) 50% 80% 120% 150% 200% Mass 2000 0 2000 4000 6000 Rewards algorithm MAML DR source policy PADA-DM PADA-DM w/ DR (b) Figure 11.Ablation experiments using domain randomization and meta-learning. (a) MAML with 20k adaptation samples. (b) MAML with 80k adaptation samples.Provably Efﬁcient Model-based Policy Adaptation Gravity Perturbation 50% 80% 120% 150% 200% Source policy 1549.74 (1090.73) 4444.86 (637.77) 4592.18 (201.30) 2543.55 (916.16) -156.51 (25.93) Domain Randomization 2282.74 (1563.70) 4838.87 (1134.98) 5236.46 (1179.85) 2896.03 (554.00) -43.97 (423.47) PADA-DM 2694.78 (1166.88) 4739.32 (279.06) 4889.02 164.38) 2998.32 (266.75) 1531.23 (400.96) PADA-DM w/ DR 3230.29 (1280.54) 5036.59 (657.98) 4934.04 (720.34) 3200.73 (521.50) 1431.53 (496.11) MAML (20k) 854.24 (692.50) 1810.51 (663.72) 1895.86 (650.76) 1575.06 (653.09) 831.07 (717.78) MAML (80k) 876.37 (711.98) 1778.67 (669.86) 1894.28 (644.55) 1568.60 (646.79) 823.13 (721.15) Table 4.Mean and standard deviation (in the brackets) of the episodic rewards of each method in the target environment with perturbed gravity across 100 trajectories of 5 random seeds (500 trajectories in total). Mass Perturbation 50% 80% 120% 150% 200% Source policy 921.13 (1192.43) 3343.05 (2317.32) 4166.10 (494.94) 2045.26 (665.30) -149.92 (27.28) Domain Randomization 1665.05 (1357.31) 3823.45 (1944.70) 3932.86 (1791.18) 2635.72 (1105.15) 944.50 (1134.32) PADA-DM 3271.52 (752.62) 4914.67 (379.73) 4584.95 375.51) 3557.25 (183.46) 1398.88 (500.09) PADA-DM w/ DR 2673.87 (1009.75) 5348.98 (556.19) 4854.30 (591.50) 3276.70 (874.54) 1616.75 (490.53) MAML (20k) 854.24 (692.50) 1810.51 (663.72) 1895.86 (650.76) 1575.06 (653.09) 831.07 (717.78) MAML (80k) 876.37 (711.98) 1778.67 (669.86) 1894.28 (644.55) 1568.60 (646.79) 823.13 (721.15) Table 5.Mean and standard deviation (in the brackets) of the episodic rewards of each method in the target environment with perturbed mass across 100 trajectories of 5 random seeds (500 trajectories in total).",
      "meta_data": {
        "arxiv_id": "2006.08051v1",
        "authors": [
          "Yuda Song",
          "Aditi Mavalankar",
          "Wen Sun",
          "Sicun Gao"
        ],
        "published_date": "2020-06-14T23:16:20Z",
        "pdf_url": "https://arxiv.org/pdf/2006.08051v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the high sample complexity of reinforcement learning and the limitations of existing policy adaptation methods (domain randomization, meta-learning) when faced with out-of-distribution target environments. The main contributions include proposing new model-based mechanisms for online policy adaptation in unseen target environments, combining no-regret online learning and adaptive control. The approach is theoretically proven to learn policies in the target environment that can recover trajectories from the source environment, with a convergence rate established for general settings. Empirically, it demonstrates state-of-the-art performance with significantly lower sample complexity across diverse continuous control tasks, and is capable of adapting to out-of-distribution environments without requiring pre-training samples from them.",
        "methodology": "The proposed method, Policy Adaptation with Data Aggregation (PADA), is a model-based approach that iteratively learns a locally accurate model of the target environment's dynamics and then plans actions to minimize the divergence between state trajectories in the target environment and those in the source environment. The theoretical Algorithm 1 defines an ideal policy based on model approximation and uses data aggregation to update the model via Maximum Likelihood Estimation (MLE). The practical implementation, PADA-DM, parameterizes the model as a 'deviation model' (a two-layer neural network) that approximates the difference between target dynamics and source dynamics. For planning, it uses the Cross Entropy Method (CEM) for one-step action minimization. Model updates are performed using Stochastic Gradient Descent (SGD) on mini-batches from an experience replay buffer. Exploration is handled in an epsilon-greedy manner. An extension, PADA-DM with target policy, trains an additional parameterized policy via Behavior Cloning to mimic CEM, making it MPC-free for faster execution at test time. The theoretical analysis is inspired by DAgger, leveraging a reduction to no-regret online learning principles like Follow-the-Leader (FTL).",
        "experimental_setup": "The approach was evaluated on standard OpenAI Gym and Mujoco continuous control environments, including HalfCheetah, Ant, and Reacher. Target environments were created by perturbing parameters such as mass, gravity, dimensions (e.g., Reacher arm length), motor noise, and friction, including complex multi-dimensional perturbations (e.g., 3 DOF, 15 DOF changes). Comparisons were made against state-of-the-art policy adaptation methods: Christiano et al., 2016 (learning inverse dynamics model), Zhu et al., 2018 (GAIL with true reward signals), as well as Proximal Policy Optimization (PPO), direct use of the source policy, Domain Randomization (DR), and Model Agnostic Meta-Learning (MAML). Performance was measured by episodic rewards and sample efficiency (number of timesteps). Experiments included learning curves across 5 random seeds, long-term learning curves, and ablation studies combining PADA-DM with DR and MAML. Hyperparameters for all methods were provided, and the source dynamics model was pre-trained using (s,a,s') triplets generated during source policy training.",
        "limitations": "The theoretical analysis relies on certain assumptions, including an 'Adaptability' assumption (Assumption 4.1), which posits that for any state-action pair in the source, there exists a target action allowing the target environment to recover dynamics close to the source, quantified by a small epsilon. Another assumption is 'Realizability' (Assumption 4.3), which states that the true target dynamics belong to the chosen model class (though the paper notes agnostic results are possible with more refined analysis). The Cross Entropy Method (CEM) used in the practical implementation for planning can be computationally expensive, even for one-step optimization, which is addressed by training a separate policy. The method struggled in tasks with high divergence between the target and source environments, such as Ant-v2 with 0.6 standard deviation motor noise. Additionally, the performance guarantee linking policy difference to reward difference assumes the reward function only depends on states.",
        "future_research_directions": "A natural extension of this research is to apply the proposed policy adaptation approach to simulation-to-real problems, potentially in combination with domain randomization and meta-learning. The paper also suggests investigating a more general theoretical framework for effectively combining domain randomization techniques with online adaptive control methods, given that experiments indicated the combination often yielded the best results."
      }
    },
    {
      "title": "The Entropy Enigma: Success and Failure of Entropy Minimization",
      "abstract": "Entropy minimization (EM) is frequently used to increase the accuracy of\nclassification models when they're faced with new data at test time. EM is a\nself-supervised learning method that optimizes classifiers to assign even\nhigher probabilities to their top predicted classes. In this paper, we analyze\nwhy EM works when adapting a model for a few steps and why it eventually fails\nafter adapting for many steps. We show that, at first, EM causes the model to\nembed test images close to training images, thereby increasing model accuracy.\nAfter many steps of optimization, EM makes the model embed test images far away\nfrom the embeddings of training images, which results in a degradation of\naccuracy. Building upon our insights, we present a method for solving a\npractical problem: estimating a model's accuracy on a given arbitrary dataset\nwithout having access to its labels. Our method estimates accuracy by looking\nat how the embeddings of input images change as the model is optimized to\nminimize entropy. Experiments on 23 challenging datasets show that our method\nsets the SoTA with a mean absolute error of $5.75\\%$, an improvement of\n$29.62\\%$ over the previous SoTA on this task. Our code is available at\nhttps://github.com/oripress/EntropyEnigma",
      "full_text": "The Entropy Enigma: Success and Failure of Entropy Minimization Ori Press 1 Ravid Shwartz-Ziv 2 Yann LeCun2 3 Matthias Bethge 1 Abstract Entropy minimization (EM) is frequently used to increase the accuracy of classification models when they’re faced with new data at test time. EM is a self-supervised learning method that op- timizes classifiers to assign even higher proba- bilities to their top predicted classes. In this pa- per, we analyze why EM works when adapting a model for a few steps and why it eventually fails after adapting for many steps. We show that, at first, EM causes the model to embed test im- ages close to training images, thereby increasing model accuracy. After many steps of optimiza- tion, EM makes the model embed test images far away from the embeddings of training images, which results in a degradation of accuracy. Build- ing upon our insights, we present a method for solving a practical problem: estimating a model’s accuracy on a given arbitrary dataset without hav- ing access to its labels. Our method estimates accuracy by looking at how the embeddings of input images change as the model is optimized to minimize entropy. Experiments on 23 challenging datasets show that our method sets the SoTA with a mean absolute error of 5.75%, an improvement of 29.62% over the previous SoTA on this task. Our code is available at: https://github. com/oripress/EntropyEnigma 1. Introduction Practitioners commonly employ model adaptation strategies to enhance classifier performance on real-world data, which often differs significantly from training data. Unsupervised losses play a crucial role in adapting models to images corrupted by noise, such as snow or motion blur, or images from domains not seen in training, such as paintings or computer rendered images. Entropy minimization (EM) is a 1University of T ¨ubingen, T ¨ubingen AI Center, Germany 2New York University 3Meta AI, FAIR. Correspondence to: <ori.press@bethgelab.org>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). Test Time Adaptation (TTA) method that can improve the accuracy of a model on new datasets, without the need for additional labeled training data. EM adapts classifiers by iteratively increasing the probabilities assigned to the most likely classes while diminishing those of the others, and is an integral part of many recent TTA methods (Wang et al., 2020; Mummadi et al., 2021; Rusak et al., 2022b; Goyal et al., 2022; Niu et al., 2022; Cho et al., 2023; Niu et al., 2023; Press et al., 2023; D¨obler et al., 2024; Marsden et al., 2024). In this paper, we analyze EM to understand how it works, when and why it fails, and how to use it to predict model accuracy. The initial intuition behind using entropy minimization, given by Wang et al. (2020) was based on the observation that models tend to be more accurate on images for which they make predictions with higher confidence. The logi- cal extension of this observation was to encourage models to bolster their confidence on such images. However, our analysis reveals this intuition to be only partly true. Remark- ably, even when we construct datasets by excluding samples initially classified correctly — effectively creating datasets with a 100% classification error rate at the start — entropy minimization performance remains largely intact. Our analysis uncovers that during entropy minimization, embeddings of images from the input dataset tend to form distinct clusters. The distances between samples within each cluster diminish, creating more defined groupings, while the centers of these clusters gradually move apart, a phe- nomenon akin to neural collapse (Papyan et al., 2020; Han et al., 2021; Ben-Shaul et al., 2023). At first, embeddings of the input images not only cluster, but also stay close to the embeddings of original training images. Only after nu- merous optimization steps do these embeddings begin to diverge, distancing themselves from the embeddings of the clean training data (Fig. 1). We show this divergence to be intricately tied to a reduction in the model’s accuracy. Drawing from our insights, we present a method designed to estimate the accuracy of a given model on any dataset, with- out labels. This task is notably difficult, because in some cases in-distribution accuracy is tied to out-of-distribution (OOD) accuracy (Miller et al., 2021), while in other cases it is not (Teney et al., 2022). Our approach, termed Weighted Flips (WF), works in conjunction with TTA methods as they 1 arXiv:2405.05012v2  [cs.CV]  12 May 2024The Entropy Enigma: Success and Failure of Entropy Minimization Figure 1.Understanding the successes and failures of EM through clustering embedding dynamics. After a few iterations of adaptation (left), EM improves the accuracy of pretrained classifiers by embedding the input test data near mean embeddings of classes from the training data, marked by stars. Eventually, after many iterations (right), EM fails, because it embeds input test data far from where training data is embedded. We show the t-SNE embeddings of 16-class-Imagenet (Geirhos et al., 2018), throughout adaptation to Gaussian Noise 3 (Hendrycks & Dietterich, 2019). adapt to input data, with minimal added overhead. Using approximations of cluster consistency, WF estimates the ac- curacy of the network by measuring how the predictions of a fixed set of images change: the more they change, the lower the consistency of the clusters and the lower the predicted accuracy. We validate the efficacy of our method across an extensive array of 23 ImageNet-scale (Deng et al., 2009) datasets, encompassing diverse challenges, such as random adversarial noises, hard images, and datasets featuring OOD classes. WF surpasses the prior state-of-the-art methods by a substantial margin of 29.62%, setting a new benchmark in the accuracy estimation domain. 2. The Mystery of Entropy Minimization EM has been validated as effective in semi-supervised settings, with pioneering work by (Grandvalet & Ben- gio, 2004) and subsequent advancements, such as Tent (Wang et al., 2020), which demonstrated EM’s ability to enhance the accuracy of pre-trained classifiers on unlabeled ImageNet-scale (Deng et al., 2009) datasets. EM operates by iteratively optimizing the model to minimize the en- tropy of the output classification probabilities, denoted by H(ˆy) = −P c p(ˆyc) logp(ˆyc), where ˆy is the logits vector and p(ˆyc) is the probability assigned to class c. This ap- proach inherently boosts the likelihood of the most probable classes while diminishing that of the others. Wang et al. (2020) observed a correlation between lower output entropy and accuracy, indicating that images with low entropy out- puts are more likely to be classified correctly. Subsequent studies, including (Niu et al., 2022; Press et al., 2023; Mars- den et al., 2024), have built on this foundation, assigning more weight to lower-entropy samples, and even ignoring high-entropy samples entirely. To assess the influence of correctly classified images on EM’s effectiveness, we tested the effects of omitting im- ages that were initially correctly classified by the model. If such images are pivotal in EM’s ability to enhance classi- fier performance, we expect a notable decline in the EM efficacy. For this purpose, we utilized ImageNet-C (Hendrycks & Dietterich, 2019) Gaussian Noise level 3, dividing it into training and holdout sets. The training set was replicated seven times, systematically omitting images for which the ground truth label lay somewhere in the pre-trained model’s top-k predictions, for ( k ∈ [1, 2, 3, 5, 10, 20, 50]). Con- cretely, for k = 1 , all accurately classified images were excluded, and for k = 2, images whose label ranked within the top two predictions were removed, and so forth. Each altered training set was used to adapt a Tented model. The model’s accuracy was then evaluated on the holdout set, with evaluations every ten iterations, spanning a total of 1,000 iterations. The experiment results (see Figure 2) are revealing, under- scoring the robustness of EM. Notably, EM’s effectiveness endures even when images initially classified correctly are excluded. For instance, removing all initially correctly classified im- ages before adaptation produces an increase in accuracy comparable to not removing any images, with gains of 10.50% and 12.38%, respectively. Even more remarkable is the persistence of this trend: with k = 10, the model still 2The Entropy Enigma: Success and Failure of Entropy Minimization Figure 2.EM remains effective even when initially correctly classified images are excluded. Accuracy gain per iteration on a holdout set, as Tent adapts to its inputs. Surprisingly, the per- formance gain on the holdout set is high, even when we exclude top-k samples from the training set. When top-k = 0, no images are excluded. registers a notable accuracy improvement of 7.88%. This observation is particularly striking given the nature of the excluded images – they are not just numerous, but also rep- resent the highest quality, being those the network is most certain about. Specifically, images excluded atk = 1, which constitute 45% of the dataset, have an average entropy of 1.85, markedly lower than the original dataset’s average entropy of 2.84. Additionally, we also tested the effects of removing images according on their initial entropy level, and found similar results (see Appendix G). These findings intriguingly sug- gest that the model’s accuracy and entropy on individual images may not be as pivotal to EM’s success in enhancing classifier performance as previously thought. It reveals a nuanced dimension of EM’s functionality and hints at the presence of deeper mechanisms, which we will investigate next. 3. Phases of Entropy Minimization: Clustering Dynamics and Embedding Alignment We analyze the evolution of input data embeddings as EM progresses through its iterations. At first, EM causes the model to increase in accuracy, which we refer to as the first phase, followed by a decrease in accuracy, which we refer to as the second phase. The number of EM iterations needed for the model to reach its maximum accuracy (the end of the first phase, and the beginning of the second) is varied and depends on the input data. In the first phase, these embeddings align closely with the embeddings of samples from the original training distribu- tion. However, in the second phase, this alignment starts to deteriorate; the embeddings drift progressively further from the training distribution, disrupting the initial alignment, as conceptually depicted in Figure 3. Figure 3.The two-phase clustring paradigm explains EM be- havior. Intuitive visualization of EM’s phases. In the first phase (success), input test data becomes more clustered, aligning closely with the mean embeddings of corresponding classes from the train- ing data (the colored stars). In the second phase (failure), these clusters diverge from the mean embeddings. To examine the clustering process across the two phases of the EM, we focus on two measures: (1) the quality of the clusters and (2) their alignment with the original training data distribution. For evaluating cluster quality, we ran k-means on the embeddings and computed the Silhouette score (Rousseeuw, 1987), a widely recognized metric for measuring cluster quality. The Silhouette score gauges how closely an embedding corresponds to its own cluster in contrast to neighboring clusters, with a high score indicating distinct and well-separated clusters. To quantify the alignment between clusters and embeddings of the original training distribution, we looked at mean embeddings for the classes in the ImageNet validation set, alongside the centroids of clusters found by k-means. We use the Hungarian method (Kuhn, 1955) to find a matching between mean class embeddings and centroids, which mini- mizes the average distance between each assigned pair of (class embedding, centroid). Henceforth, we refer to this average of distances as “Shift distance”. As ImageNet contains many similar fine-grained classes, we restrict our analyses to the 16 classes outlined in (Geirhos et al., 2018), which represent approximately 20% of the total images. Consequently, we use k = 16 when we cluster the embeddings using k-means. This focused approach allowed for a detailed and controlled examination of clustering be- haviors within the framework of EM. We now examine changes in the Silhouette score and Shift distance as Tent adapts to the input data, over 50,000 it- erations using a ResNet-50 (He et al., 2016). Figure 4 showcases the comparative Silhouette scores and Shift dis- 3The Entropy Enigma: Success and Failure of Entropy Minimization Figure 4.Two-phase behavior during the EM adaption predicts accuracy.Differences in Silhouette score, Shift distance, and accuracy for Tent adaptation. Each point corresponds to a test dataset; each dataset appears twice: once in blue, corresponding to phase 1 (success, ∆ Acc ≥ 0), and once in orange, corresponding to phase 2 (failure, ∆ Acc < 0). Left: In both phases, and across almost all datasets, the Silhouette score of embeddings increases, corresponding to a better-clustered embedding space. Right: In the first phase, input data embeddings are kept close to training image embeddings, while in the second phase, they drift away, exhibiting large Shift distance changes. The datasets used are IN-C, IN-C and IN-3DCC. tances for both phases, incorporating findings from three diverse datasets: IN-C (Hendrycks & Dietterich, 2019), IN- C (Mintun et al., 2021), and IN-3DCC (Kar et al., 2022). Our findings distill into two primary insights: First, a pos- itive change in Silhouette score, indicative of enhanced clustering, is observed in both phases for more than 98% of cases. Notably, during the initial phase, a positive cor- relation exists between changes in Silhouette score and ac- curacy (ρ = 0.70, significant at α = 0.05). Second, Shift distances minimally change (and sometimes diminish, sig- nifying closer proximity to training data embeddings) in the first phase, they notably grow larger in the second phase. During this latter phase, a substantial negative correlation emerges between changes in Shift distance and accuracy (ρ = −0.79, significant at α = 0.05). Synthesizing these results reveals a nuanced picture: EM bolsters accuracy by clustering the embedded data into more concentrated clusters. This strategy remains efficacious as long as these embeddings align closely with the embeddings of the training data. However, as input data embeddings diverge from the training distribution, the classifier’s ac- curacy diminishes. This intricate interplay offers a deeper understanding of EM’s operation and its dependency on the spatial dynamics of data embeddings. We discuss the connection between EM and clustering in more detail in Appendix A. 4. Estimating Dataset Accuracy Leveraging our understanding of EM, we tackle a critical challenge in TTA settings: estimating the accuracy of a classification model on a given dataset. Ideally, one might resort to the metrics used in this paper, namely Silhouette score or Shift distance, for this purpose. However, these metrics encounter practical hurdles: the Silhouette score depends on clustering, which varies across datasets due to differences in class distributions or the total number of classes, and calculating the Shift distance is impossible, as accessing the training data (in order to calculate mean embedding vectors per class) is forbidden in most TTA settings (Wang et al., 2020; Niu et al., 2022; Yuan et al., 2023). 4.1. Label Flipping Due to the difficulties of measuring these scores in prac- tice, we take a different approach. We look at the number of images for which the model’s prediction changes some- where between the initial and the final iteration of the EM (“label flips”). According to our hypothesis, the number of label flips is correlated with the pre-trained model’s ac- curacy on the dataset. Our reasoning is as follows: there exists a tight correlation between accuracy and Silhouette score at iteration 0 — the higher the accuracy, the better clustered the input data, shown in Figure 5. Therefore, we do not expect EM, which works by clustering its inputs, to significantly change an already well-clustered set of embed- dings. It follows that there will likely be only a few label flips. Conversely, given a dataset with a low accuracy, its 4The Entropy Enigma: Success and Failure of Entropy Minimization Figure 5.Label flips are strongly correlated with Silhouette score. Silhouette score at the initial iteration and the total number of label flips at the final iteration are correlated for datasets in IN-C, IN-C, and IN-3DCC. Both metrics are correlated with accuracy, but measuring label flips is easier and more practical. image embeddings will likely be badly clustered initially, which leads EM to change them significantly, resulting in many label flips. We demonstrate the validity of this reasoning by adapt- ing the state-of-the-art TTA method, Rdumb (Press et al., 2023), to IN-C, IN-C, and IN-3DCC. Initially, we used the pre-trained model to classify 1,000 input images and then recorded the total number of label flips after adaptation. The model is adapted for 1,000 iterations because Rdumb resets itself every 1,000 iterations. We find a strong correlation between accuracy and label flips, seen in Figure 5. 4.2. Weighted Flips We now describe the Weighted Flips (WF) method of con- verting the count of label flips into a dataset accuracy es- timate. Instead of just counting the number of flips, we additionally consider the classifier’s initial confidence in its predictions for each image; images initially classified with high confidence that later flip should contribute more significantly than those with lower initial confidence. We then compute the WF as: W F= X i 1{flip}(i) · ci where 1flip (i) is 1 if image i’s label flipped and0 otherwise, and ci is the confidence percentile of imagei. Utilizing pairs of weighted flips and accuracy ((WF, accuracy)k) from IN- Validation and ImageNet-C holdout noises, we interpolate the weighted-flips-to-accuracy function, f (refer to Figure 6). To estimate the accuracy of a model on an unfamiliar dataset, we adapt the model to it using RDumb (for details, see Appendix J), measuring flips on the first 1,000 input images. After adaptation, we count and weigh the flips, estimating the model’s accuracy as f(WF). Importantly, WF is versatile and can work with a range of TTA methods (see Appendix E), and f can be interpolated in a variety of different ways (see Appendix B). In Appendix D.1, D.2, we present ablation studies on the effects of varying end iterations and holdout set sizes on performance. 4.3. Experimental Setting Accuracy estimation methods must yield robust estimates across diverse and challenging datasets to be considered reliable. In our evaluation, we probe the effectiveness of our proposed method using an extensive selection of popular ImageNet-scale classification datasets. This includes all classification datasets from the Shift-Happens benchmark1. Our chosen datasets encompass a wide spectrum, from various types of noise (IN-C, IN-C, IN-3DCC, CCC) and domain shifts (IN-R, IN-V2, IN-D), to adversarial noises (Patch-IN, BG Challenge, IN-Obfuscations), and even im- ages featuring classes not present in ImageNet (NINCO). Several datasets provide multiple splits of a similar nature, the results of which we average, except for ImageNet-D (Rusak et al., 2022a), which encompasses a variety of dis- tinct domains. The CCC dataset (Press et al., 2023) is par- ticularly expansive, containing 27 splits with 7.5M images each; for practicality, we only include the initial 25k images from each split in our analysis. Altogether, our evaluation spans 326 individual dataset splits. We briefly describe the other methods tested alongside ours: • AC (Hendrycks & Gimpel, 2016): Computes the dataset-wide average confidence for the top-predicted class in each image. • DoC (Guillory et al., 2021): Builds upon AC by as- sessing the variance in mean confidence between the validation and OOD sets, demonstrating consistent en- hancements in performance. • ATC (Garg et al., 2022): Estimates accuracy by deter- mining the fraction of unlabeled data samples where the model’s confidence exceeds a learned threshold. • COT (Lu et al., 2023): Estimates accuracy by applying Optimal Transport to quantify the disparity between OOD and in-distribution model outputs. 1https://github.com/ shift-happens-benchmark/icml-2022 5The Entropy Enigma: Success and Failure of Entropy Minimization Figure 6.Fitting and accuracy prediction using the WF method. Left: Fitting f: using the noises in IN-C Holdout and ImageNet- Validation, we fit pairs of (weighted flips, accuracy), shown in red. The black curve shows the function resulting from interpolating the points, f(x) = 0.00036x2 − 0.32x + 75.66. Right: With our weighted-flips-to-accuracy function f, we can estimate the accuracy of a model across the six splits from (Rusak et al., 2022a). We use the same f function and show that it works across different architectures, without refitting. 4.4. Results Looking at Table 1 reveals that our WF method consistently stands out as the best estimator across a broad spectrum of ImageNet-scale datasets. WF sets a new benchmark by achieving an average estimation error of just 5.75%, signifi- cantly outperforming the nearest competitor, COT, reducing the relative error by 29.62%. This exemplary performance of WF is not limited to average cases; even in the most challenging scenarios of worst-case performance, WF main- tains its superiority, cutting the error by 29.74% compared to COT. Furthermore, WF demonstrates remarkable consis- tency as an estimator. In 18 of the 23 datasets evaluated, it either leads the pack or comes a close second. This is in stark contrast to the performance of COT, which, despite being second-best, only achieves top-two rankings in 12 datasets. The persistent effectiveness of WF across diverse conditions underscores its reliability and superiority in ac- curacy estimation. Practicality of WF: Beyond its top-tier performance, WF stands out for its practicality. It operates concurrently with the EM process, requiring only three parameters that define the weighted-flips-to-accuracy function, f. This process adds minimal computational overhead, requiring only 20 additional forward passes for every 1,000 Rdumb iteration steps. Lastly, WF is effective even when only a small num- ber of samples are available, see Appendix C. Versatility across Models and Architectures: To demon- strate the adaptability of the WF method, we tested it across various models and architectures, employing the same weighted-flips-to-accuracy function, f, used in our primary experiments (Table 1). Testing encompassed dif- ferent ResNet variants, including models enhanced with noise augmentation techniques, such as ANT (Rusak et al., 2020), AugMix (Hendrycks et al., 2019), and DeepAugment (Hendrycks et al., 2021a). Additionally, we evaluated a ResNext-101 (Xie et al., 2017), ViTB-16 (Dosovitskiy et al., 2010), and MaxViT-T (Tu et al., 2022). The mean absolute errors between estimated and actual accuracies are reported in Table 2. Remarkably, 5 of the 8 models tested achieved a lower mean absolute error than the baseline model, RN-50, showing that f maintains its efficacy across different model architectures. When f is refitted on the architecture that WF is evaluated on, performance improves (see Appendix F). Robustness to Dataset Choice: In Table 1, we derived the weighted-flips-to-accuracy function f using IN-C holdout and ImageNet validation noises. We further validated the robustness of the WF method by fitting f using a subset of the 23 datasets and then assessing its performance on the remaining datasets. As an added challenge, we excluded datasets used in the original configuration: IN-C Holdout and ImageNet-Validation. For each subset size, we repeated the fitting and evaluation process 50 times. The results, plot- ted in Figure 7, illustrate that the WF method consistently outperforms COT across almost all subset sizes, reinforc- ing its resilience and reliability across a broad spectrum of datasets. 6The Entropy Enigma: Success and Failure of Entropy Minimization Table 1.Mean Absolute Error between estimated accuracy, and true accuracy on a ResNet-50 model, for 4 estimation methods (AC, DoC, ATC, COT) (Hendrycks & Gimpel, 2016; Guillory et al., 2021; Garg et al., 2022; Lu et al., 2023), and ours. Our method (WF) is consistently either best or second best, with the best average and worst-case performance across many different OOD datasets. Best results are in bold; second best are underlined, {.} indicates how many splits are in each dataset, when there are more than 1. Datasets AC DoC ATC COT WF (ours) Noises IN-C {75} (Hendrycks & Dietterich, 2019) 10.06 6.61 7.44 2.23 4.79 IN-C {50} (Mintun et al., 2021) 19.48 15.96 12.16 3.17 7.35 IN-3DCC {60} (Kar et al., 2022) 11.83 3.44 8.15 3.02 3.66 CCC {27} (Press et al., 2023) 15.51 11.95 6.05 2.04 2.80 Domain Shifts Stylized (Geirhos et al., 2019) 31.63 28.08 7.36 12.18 3.81 IN-V2 {3} (Recht et al., 2019) 5.58 2.41 0.45 2.68 4.70 IN-Sketch (Wang et al., 2019) 22.34 18.78 0.15 4.23 1.71 IN-R (Hendrycks et al., 2021a) 23.21 19.65 0.37 2.44 1.88 IN-D (Rusak et al., 2022a) Real 10.56 7.00 1.35 27.54 3.18 Painting 17.40 13.85 3.27 7.49 2.12 Clipart 21.27 17.72 1.62 4.52 3.37 Sketch 24.43 20.87 0.61 0.71 5.44 Infograph 54.12 50.57 36.26 3.44 3.63 Quickdraw 32.67 29.11 4.13 1.60 2.57 Cartoon & Drawing {2} (Salvador & Oberman, 2022) 15.69 12.13 4.42 1.62 13.25 Adversarial Noises BG Challenge {8} (Xiao et al., 2020) 10.54 7.37 4.88 19.68 6.92 IN-A (Hendrycks et al., 2021b) 45.12 41.57 20.51 30.38 21.61 IN-C Patch {75} (Gu et al., 2022) 4.37 0.16 4.42 2.57 1.60 IN-Hard (Taesiri et al., 2023) 29.71 26.15 6.73 15.33 3.64 Patch-IN {10} (Pintor et al., 2023) 8.06 5.11 5.11 10.13 8.87 IN-Obfuscations {3} (Stimberg et al., 2023) 99.90 96.34 99.90 0.12 4.58 OOD/Other ObjectNet (Barbu et al., 2019) 34.59 31.03 9.43 10.40 2.74 NINCO (Bitterwolf et al., 2023) 50.29 46.74 26.97 20.28 18.07 Average 26.02 22.29 11.81 8.17 5.75 Worst Case 99.90 96.34 99.90 30.38 21.61 Average (Worst Case Excluded) 22.66 18.92 7.81 7.16 5.03 5. Related Work To the best of our knowledge, the first time EM was shown to be useful for improving a classifier’s accuracy was in (Grandvalet & Bengio, 2004). They showed how EM can be applied to a logistic regressor, and found it to be ben- eficial in cases where the data was corrupted by outliers. Following this, Lee et al. (2013) proposed pseudo labeling as a means of improving classification accuracy on MNIST. Interestingly, t-SNE is used to show that pseudo labeling works partly by encouraging the model’s embeddings to be better clustered, and away from the decision boundaries of the model. Moreoever, it is stated that pseudo labeling is equivalent to entropy regularization (Grandvalet & Bengio, 2004). Although this might be true in the settings consid- ered then, pseudo labeling was shown to be less effective (and thus not equivalent) on larger-scale datasets, by Tent. Unlike previous work, we demonstrate that EM clusters by measuring the Silhouette score of the clusters themselves, allowing us to empirically evaluate ImageNet scale datasets. Additionally, we show what happens when EM fails, which is not discussed in prior work, with the exception of (Oliver et al., 2018), which shows how EM fails to adapt to a toy “two moons” dataset, because the model increases the mag- nitude of its output logits. This isn’t the case in most TTA settings, as the final layer of the model isn’t trained. Minimizing entropy at test time was popularized by Tent 7The Entropy Enigma: Success and Failure of Entropy Minimization Table 2.Mean Absolute Error between estimated accuracy and true accuracy, across different architectures. Using the same weighted-flips to-accuracy function, f, works across different architectures and models, without need for finetuning. For each model and dataset, the task is to estimate the accuracy of that model on the dataset. Best results are in bold; second best are underlined. AugMix: ♢ ANT: ‡ DeepAugment: ♠ (Hendrycks et al., 2019; Rusak et al., 2020; Hendrycks et al., 2021a) Datasets RN-50 RN-18 RN-34 RN-50 ‡ RN-50 ♢ RN-50 ♢♠ RNXt-101 RNXt-101 ♠ ViT-B/16 MaxViT-T IN-C 4.79 7.21 6.04 5.39 5.02 4.81 5.35 4.12 8.34 6.73 IN-C 7.35 7.90 6.77 6.84 6.60 6.48 5.60 5.63 6.59 4.97 IN-3DCC 3.66 3.58 3.89 3.20 3.07 2.98 7.23 4.37 7.19 6.79 IN-V2 4.70 4.11 3.37 3.67 5.06 5.00 6.47 5.54 4.44 6.08 IN-D Real 3.18 2.83 0.38 2.72 6.59 3.30 4.24 0.61 1.02 3.40 Painting 2.12 5.36 0.78 0.59 7.62 2.51 12.02 1.12 3.02 0.60 Clipart 3.37 1.59 4.42 0.32 6.19 2.19 7.24 0.53 12.82 4.52 Sketch 5.44 1.53 3.93 6.18 9.73 3.60 10.75 1.89 11.04 10.88 Infograph 3.63 1.76 3.67 3.74 6.78 0.28 6.37 2.34 9.27 9.13 Quickdraw 2.57 2.34 2.24 2.20 2.53 1.27 2.27 1.21 2.36 2.31 Average 4.08 3.82 3.55 3.49 5.92 3.10 6.76 2.74 6.61 5.54 Figure 7.WF outperforms other methods across almost all sub- set sizes. Mean Absolute Error of WF when using a weighted-flips- to-accuracy function f to fit on random subsets of the 23 datasets in Table 1. For each point on the x-axis, we sample 50 fitting datasets for WF, and plot the average and the standard deviation of the MAE. For the other methods, we plot average MAE across all datasets . (Wang et al., 2020), which demonstrated the effectiveness of EM on large-scale datasets, such as ImageNet-C. Entropy minimization is ideal for domain adaptation: it can be used on a trained model, without retraining, and doesn’t require balancing a proxy loss with a classification loss, as in (Gidaris et al., 2018; Sun et al., 2020; Gandelsman et al., 2022). Though many prior works use losses that are based on en- tropy (Wang et al., 2020; Rusak et al., 2022b; Goyal et al., 2022; Mummadi et al., 2021; Wang et al., 2022; Niu et al., 2022; Cho et al., 2023; Press et al., 2023; Niu et al., 2023; D¨obler et al., 2024; Marsden et al., 2024), little is known as to why it works. Additionally, entropy minimization, when used in TTA settings, is effective for only a limited num- ber of iterations, before the classifier degrades to chance accuracy, shown in (Press et al., 2023). Interestingly, this degradation of accuracy, named “collapse”, differs from classical definitions of catastrophic forgetting in continual learning (De Lange et al., 2021), in that the task itself does not change. A plethora of methods have been used for adapting a trained classifier to out-of-domain data: from using an auxiliary loss to help learn the test domain (Sun et al., 2019; 2020; Gandelsman et al., 2022) through simply re-estimating the mean and variance statistics (Schneider et al., 2020; Nado et al., 2020) to using image augmentations (Wang et al., 2022; Song et al., 2023; Chakrabarty et al., 2023). However, for their simplicity and success, entropy minimization-based methods are still the most widely used and successful in settings most relevant to this work. Works that follow Tent improve EM by modifying the loss to be more robust to label noise (Rusak et al., 2022b) or smoother (Mummadi et al., 2021), or by adjusting the tem- perature of the output distribution (Goyal et al., 2022). While testing on long sequences of images, both (Wang et al., 2022) and (Niu et al., 2022) show that Tent degrades in accuracy, the more iterations it does. (Press et al., 2023) show that this is in fact true for all TTA methods apart from EATA (Niu et al., 2022), which uses an L2 regularizer to con- strain the adapting model’s weights to be close to those of 8The Entropy Enigma: Success and Failure of Entropy Minimization the pretrained model. (Niu et al., 2023) study the effects of batch size, label shifts and other factors on adaptation; they propose a method to stabilize adaptation. Similarly, (D¨obler et al., 2024) also test entropy minimization-based methods in real-world conditions, and propose a new method based on a diversity and a weighted entropy loss. Entropy has also been used in semi-supervised settings: (Sohn et al., 2020) propose augmentation and an entropy loss to train a classifier when only a few labels are available. Analyzing which labels flip during training has been studied in (Toneva et al., 2018), which explored which samples are forgotten during training. Another work, (Deng et al., 2022) looked at how to reduce the amount of times a label flips dur- ing training. The agreement/disagreement between different models on ID data was shown to be linearly correlated to OOD accuracy and has been recently used to estimate accu- racy in (Miller et al., 2021; Jiang et al., 2021; Baek et al., 2022; Kim et al., 2023). These works are beyond the scope of this work, as they require access to multiple models and ID data, which is disallowed in most TTA settings (Wang et al., 2020; Niu et al., 2022; Yuan et al., 2023). 6. Conclusion While EM is a cornerstone in many TTA methods, the me- chanics of its success have remained enigmatic. This study sheds light on the transformative journey of input data em- beddings under the EM adaption. It reveals a biphasic clus- tering process, where alignment with the training data’s embedding clusters bolsters accuracy, followed by a subse- quent phase where excessive divergence diminishes it. Our work goes beyond deciphering the mystery behind en- tropy minimization; it also utilizes this knowledge to signif- icantly refine the precision of model accuracy predictions in TTA contexts. This dual achievement underscores the potential of deep analytical approaches in enhancing the efficacy and applicability of machine learning models. Acknowledgements We thank Ofir Press for helpful insights and feedback. The authors thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting Ori Press. Matthias Bethge is a member of the Machine Learning Cluster of Excellence, funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Founda- tion) under Germany’s Excellence Strategy – EXC number 2064/1 – Project number 390727645 and acknowledges support by the German Research Foundation (DFG): SFB 1233, Robust Vision: Inference Principles and Neural Mech- anisms, TP 4, Project No: 276693517. This work was supported by the T¨ubingen AI Center. The authors declare no conflicts of interests. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel needs to be highlighted here specifically. References Amini, M.-R. and Gallinari, P. Semi-supervised logistic regression. In ECAI, volume 2, pp. 11, 2002. Baek, C., Jiang, Y ., Raghunathan, A., and Kolter, J. Z. Agreement-on-the-line: Predicting the performance of neural networks under distribution shift.Advances in Neu- ral Information Processing Systems , 35:19274–19289, 2022. Barbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gut- freund, D., Tenenbaum, J., and Katz, B. Objectnet: A large-scale bias-controlled dataset for pushing the lim- its of object recognition models. Advances in neural information processing systems, 32, 2019. Ben-Shaul, I., Shwartz-Ziv, R., Galanti, T., Dekel, S., and LeCun, Y . Reverse engineering self-supervised learning. arXiv preprint arXiv:2305.15614, 2023. Bitterwolf, J., M¨uller, M., and Hein, M. In or out? fixing imagenet out-of-distribution detection evaluation. arXiv preprint arXiv:2306.00826, 2023. Chakrabarty, G., Sreenivas, M., and Biswas, S. Santa: Source anchoring network and target alignment for con- tinual test time adaptation. Transactions on Machine Learning Research, 2023. Cho, Y ., Kim, Y ., and Lee, D. Beyond entropy: Style transfer guided single image continual test-time adaptation. arXiv preprint arXiv:2311.18270, 2023. De Lange, M., Aljundi, R., Masana, M., Parisot, S., Jia, X., Leonardis, A., Slabaugh, G., and Tuytelaars, T. A continual learning survey: Defying forgetting in classifi- cation tasks. IEEE transactions on pattern analysis and machine intelligence, 44(7):3366–3385, 2021. Dempster, A. P., Laird, N. M., and Rubin, D. B. Maximum likelihood from incomplete data via the em algorithm. Journal of the royal statistical society: series B (method- ological), 39(1):1–22, 1977. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009. 9The Entropy Enigma: Success and Failure of Entropy Minimization Deng, X., Xiao, Y ., Long, B., and Zhang, Z. Reducing flip- ping errors in deep neural networks. InProceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 6506–6514, 2022. D¨obler, M., Marencke, F., Marsden, R. A., and Yang, B. Diversity-aware buffer for coping with temporally corre- lated data streams in online test-time adaptation. arXiv preprint arXiv:2401.00989, 2024. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. arxiv 2020. arXiv preprint arXiv:2010.11929, 2010. Gandelsman, Y ., Sun, Y ., Chen, X., and Efros, A. Test-time training with masked autoencoders. Advances in Neural Information Processing Systems, 35:29374–29385, 2022. Garg, S., Balakrishnan, S., Lipton, Z. C., Neyshabur, B., and Sedghi, H. Leveraging unlabeled data to pre- dict out-of-distribution performance. arXiv preprint arXiv:2201.04234, 2022. Geirhos, R., Temme, C. R., Rauber, J., Sch¨utt, H. H., Bethge, M., and Wichmann, F. A. Generalisation in humans and deep neural networks. Advances in neural information processing systems, 31, 2018. Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wich- mann, F. A., and Brendel, W. Imagenet-trained CNNs are biased towards texture; increasing shape bias im- proves accuracy and robustness. In International Confer- ence on Learning Representations, 2019. URL https: //openreview.net/forum?id=Bygh9j09KX. Gidaris, S., Singh, P., and Komodakis, N. Unsupervised rep- resentation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018. Goyal, S., Sun, M., Raghunathan, A., and Kolter, J. Z. Test time adaptation via conjugate pseudo-labels. Advances in Neural Information Processing Systems, 35:6204–6218, 2022. Grandvalet, Y . and Bengio, Y . Semi-supervised learning by entropy minimization. Advances in neural information processing systems, 17, 2004. Gu, J., Tresp, V ., and Qin, Y . Evaluating model robustness to patch perturbations. In ICML 2022 Shift Happens Workshop, 2022. Guillory, D., Shankar, V ., Ebrahimi, S., Darrell, T., and Schmidt, L. Predicting with confidence on unseen distri- butions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1134–1144, 2021. Han, X., Papyan, V ., and Donoho, D. L. Neural collapse under mse loss: Proximity to and dynamics on the central path. arXiv preprint arXiv:2106.02073, 2021. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions and perturba- tions. arXiv preprint arXiv:1903.12261, 2019. Hendrycks, D. and Gimpel, K. A baseline for detecting misclassified and out-of-distribution examples in neural networks. arXiv preprint arXiv:1610.02136, 2016. Hendrycks, D., Mu, N., Cubuk, E. D., Zoph, B., Gilmer, J., and Lakshminarayanan, B. Augmix: A simple data processing method to improve robustness and uncertainty. arXiv preprint arXiv:1912.02781, 2019. Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M., et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 8340–8349, 2021a. Hendrycks, D., Zhao, K., Basart, S., Steinhardt, J., and Song, D. Natural adversarial examples. CVPR, 2021b. Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pp. 448– 456. pmlr, 2015. Jiang, Y ., Nagarajan, V ., Baek, C., and Kolter, J. Z. As- sessing generalization of sgd via disagreement. arXiv preprint arXiv:2106.13799, 2021. Kar, O. F., Yeo, T., Atanov, A., and Zamir, A. 3d common corruptions and data augmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18963–18974, 2022. Kim, E., Sun, M., Raghunathan, A., and Kolter, Z. Reliable test-time adaptation via agreement-on-the-line. arXiv preprint arXiv:2310.04941, 2023. Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009. Kuhn, H. W. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):83– 97, 1955. 10The Entropy Enigma: Success and Failure of Entropy Minimization Kullback, S. and Leibler, R. A. On information and suf- ficiency. The annals of mathematical statistics , 22(1): 79–86, 1951. Lee, D.-H. et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural net- works. In Workshop on challenges in representation learning, ICML, volume 3, pp. 896. Atlanta, 2013. Lu, Y ., Wang, Z., Zhai, R., Kolouri, S., Campbell, J., and Sycara, K. Predicting out-of-distribution er- ror with confidence optimal transport. arXiv preprint arXiv:2302.05018, 2023. Marsden, R. A., D ¨obler, M., and Yang, B. Universal test- time adaptation through weight ensembling, diversity weighting, and prior correction. In Proceedings of the IEEE/CVF Winter Conference on Applications of Com- puter Vision, pp. 2555–2565, 2024. Miller, J. P., Taori, R., Raghunathan, A., Sagawa, S., Koh, P. W., Shankar, V ., Liang, P., Carmon, Y ., and Schmidt, L. Accuracy on the line: on the strong correlation between out-of-distribution and in-distribution generalization. In International Conference on Machine Learning, pp. 7721– 7735. PMLR, 2021. Mintun, E., Kirillov, A., and Xie, S. On interaction between augmentations and corruptions in natural corruption ro- bustness. Advances in Neural Information Processing Systems, 34:3571–3583, 2021. Mummadi, C. K., Hutmacher, R., Rambach, K., Levinkov, E., Brox, T., and Metzen, J. H. Test-time adaptation to distribution shift by confidence maximization and input transformation. arXiv preprint arXiv:2106.14999, 2021. Nado, Z., Padhy, S., Sculley, D., D’Amour, A., Lakshmi- narayanan, B., and Snoek, J. Evaluating prediction-time batch normalization for robustness under covariate shift. arXiv preprint arXiv:2006.10963, 2020. Niu, S., Wu, J., Zhang, Y ., Chen, Y ., Zheng, S., Zhao, P., and Tan, M. Efficient test-time model adaptation with- out forgetting. In International conference on machine learning, pp. 16888–16905. PMLR, 2022. Niu, S., Wu, J., Zhang, Y ., Wen, Z., Chen, Y ., Zhao, P., and Tan, M. Towards stable test-time adaptation in dynamic wild world. arXiv preprint arXiv:2302.12400, 2023. Oliver, A., Odena, A., Raffel, C., Cubuk, E., and Goodfel- low, I. Realistic evaluation of semi-supervised learning algortihms. In International conference on learning rep- resentations, pp. 1–15, 2018. Papyan, V ., Han, X., and Donoho, D. L. Prevalence of neural collapse during the terminal phase of deep learn- ing training. Proceedings of the National Academy of Sciences, 117(40):24652–24663, 2020. Pintor, M., Angioni, D., Sotgiu, A., Demetrio, L., Demontis, A., Biggio, B., and Roli, F. Imagenet-patch: A dataset for benchmarking machine learning robustness against adversarial patches. Pattern Recognition, 134:109064, 2023. Poland, W. B. and Shachter, R. D. Mixtures of gaussians and minimum relative entropy techniques for modeling continuous uncertainties. In Uncertainty in Artificial Intelligence, pp. 183–190. Elsevier, 1993. Press, O., Schneider, S., K ¨ummerer, M., and Bethge, M. Rdumb: A simple approach that questions our progress in continual test-time adaptation. Advances in Neural Information Processing Systems, 36, 2023. Recht, B., Roelofs, R., Schmidt, L., and Shankar, V . Do imagenet classifiers generalize to imagenet? In Interna- tional conference on machine learning, pp. 5389–5400. PMLR, 2019. Rousseeuw, P. J. Silhouettes: a graphical aid to the inter- pretation and validation of cluster analysis. Journal of computational and applied mathematics, 20:53–65, 1987. Rusak, E., Schott, L., Zimmermann, R. S., Bitterwolf, J., Bringmann, O., Bethge, M., and Brendel, W. A simple way to make neural networks robust against diverse im- age corruptions. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part III 16, pp. 53–69. Springer, 2020. Rusak, E., Schneider, S., Gehler, P. V ., Bringmann, O., Bren- del, W., and Bethge, M. Imagenet-d: A new challenging robustness dataset inspired by domain adaptation. In ICML 2022 Shift Happens Workshop, 2022a. Rusak, E., Schneider, S., Pachitariu, G., Eck, L., Gehler, P. V ., Bringmann, O., Brendel, W., and Bethge, M. If your data distribution shifts, use self-learning. Transactions on Machine Learning Research, 2022b. Salvador, T. and Oberman, A. M. Imagenet-cartoon and imagenet-drawing: two domain shift datasets for ima- genet. In ICML 2022 Shift Happens Workshop, 2022. Schneider, S., Rusak, E., Eck, L., Bringmann, O., Bren- del, W., and Bethge, M. Improving robustness against common corruptions by covariate shift adaptation. Ad- vances in neural information processing systems , 33: 11539–11551, 2020. 11The Entropy Enigma: Success and Failure of Entropy Minimization Sohn, K., Berthelot, D., Carlini, N., Zhang, Z., Zhang, H., Raffel, C. A., Cubuk, E. D., Kurakin, A., and Li, C.-L. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. Advances in neural informa- tion processing systems, 33:596–608, 2020. Song, J., Lee, J., Kweon, I. S., and Choi, S. Ecotta: Memory- efficient continual test-time adaptation via self-distilled regularization. In Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition , pp. 11920–11929, 2023. Stimberg, F., Chakrabarti, A., Lu, C.-T., Hazimeh, H., Stretcu, O., Qiao, W., Liu, Y ., Kaya, M., Rashtchian, C., Fuxman, A., et al. Benchmarking robustness to adversar- ial image obfuscations. arXiv preprint arXiv:2301.12993, 2023. Sun, Y ., Tzeng, E., Darrell, T., and Efros, A. A. Unsuper- vised domain adaptation through self-supervision. arXiv preprint arXiv:1909.11825, 2019. Sun, Y ., Wang, X., Liu, Z., Miller, J., Efros, A., and Hardt, M. Test-time training with self-supervision for generaliza- tion under distribution shifts. In International conference on machine learning, pp. 9229–9248. PMLR, 2020. Taesiri, M. R., Nguyen, G., Habchi, S., Bezemer, C.-P., and Nguyen, A. Zoom is what you need: An empirical study of the power of zoom and spatial biases in image classification. arXiv preprint arXiv:2304.05538, 2023. Teney, D., Lin, Y ., Oh, S. J., and Abbasnejad, E. Id and ood performance are sometimes inversely correlated on real-world datasets. arXiv preprint arXiv:2209.00613, 2022. Toneva, M., Sordoni, A., Combes, R. T. d., Trischler, A., Bengio, Y ., and Gordon, G. J. An empirical study of example forgetting during deep neural network learning. arXiv preprint arXiv:1812.05159, 2018. Tu, Z., Talebi, H., Zhang, H., Yang, F., Milanfar, P., Bovik, A., and Li, Y . Maxvit: Multi-axis vision transformer. In European conference on computer vision, pp. 459–479. Springer, 2022. Wang, D., Shelhamer, E., Liu, S., Olshausen, B., and Darrell, T. Tent: Fully test-time adaptation by entropy minimiza- tion. arXiv preprint arXiv:2006.10726, 2020. Wang, H., Ge, S., Lipton, Z., and Xing, E. P. Learning ro- bust global representations by penalizing local predictive power. In Advances in Neural Information Processing Systems, pp. 10506–10518, 2019. Wang, Q., Fink, O., Van Gool, L., and Dai, D. Continual test- time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7201–7211, 2022. Xiao, K., Engstrom, L., Ilyas, A., and Madry, A. Noise or signal: The role of image backgrounds in object recogni- tion. ArXiv preprint arXiv:2006.09994, 2020. Xie, S., Girshick, R., Doll´ar, P., Tu, Z., and He, K. Aggre- gated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vi- sion and pattern recognition, pp. 1492–1500, 2017. Yuan, L., Xie, B., and Li, S. Robust test-time adaptation in dynamic scenarios. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15922–15932, 2023. 12The Entropy Enigma: Success and Failure of Entropy Minimization A. The Relationship between Entropy Minimization and Clustering In this section, we explain the connection between entropy minimization and the Expectation-Maximization algorithm (Dempster et al., 1977) with a mixture of Gaussians and show how the iterative entropy minimization objective leads to a clustering process similar to the Expectation-Maximization algorithm. In the Expectation-Maximization algorithm for clustering, the latent variables represent the cluster assignments, and the algorithm alternates between estimating the cluster assignments (E-step) and updating the cluster parameters (M-step). The convergence of the EM algorithm in this setting has been formally established (Dempster et al., 1977). Poland & Shachter (1993) showed that for a random variableX with a given distribution and the mixture of random variables Y that derive from it, the objective of minimizing the “relative entropy” between X and Y generalizes the objective of the Expectation Maximization algorithm: to maximize the likelihood of the observations x drawn from Y ’s distribution. In our setting, the iterative entropy minimization process corresponds to the Expectation Maximization algorithm, as iterative entropy minimization can also be seen as a form of “self-training” with minimization of the relative entropy (the DKL (Kullback & Leibler, 1951)) of the pseudo-labels (the model’s predictions) (Grandvalet & Bengio, 2004). The forward pass of our training process serves two purposes: (1) it sets the “observations”, which are the model’s predictions, and (2) it acts as the E-step of the algorithm, estimating the distribution given the model parameters (the clustering assignment). The backpropagation step, which updates the model parameters (the cluster parameters), serves as the M-step and maximizes the likelihood under the current pseudo-label estimates (Amini & Gallinari, 2002). It is important to note that in our setting, the entropy minimization procedure involves changing both X and Y in each iteration, which may be different from the original Expectation Maximization algorithm. Using these insights, we can provide a better explanation for the two-phase clustering phenomenon observed in our experiments. In the initial “success” phase, where the change in the embeddings is relatively small during the process, the entropy minimization effectively performs Expectation Maximization unsupervised clustering in the model’s embedding space, guided by the smart initialization provided by the pre-trained model. The E-step estimates the pseudo-labels based on the current embedding structure, while the M-step updates the model to refine the embeddings and increase intra-cluster similarity. This process leads to the formation of well-separated clusters, as reflected by the increasing Silhouette score. However, as the Expectation Maximization algorithm continues over many iterations in the “failure” phase or if there is bad initialization, it starts to overfit the model to the specific characteristics of the ”new” test data. Unlike the regular Expectation Maximization algorithm, in our case, the data distribution (the observations) changes over time, which leads to a drift in the embeddings away from the initialized representations learned from the training data. This overfitting effect, which might even converge to a global minimum, is captured by the increasing Shift distance between the test data embeddings and the training class embeddings. To support this explanation, we also provide visualizations of the prediction space to illustrate the clustering process and the eventual drift from the training embeddings. We used a mixture of Gaussians, and trained a GMM with the Expectation Maximization algorithm using maximum likelihood, where the means are initialized based on random samples. The covariance is used as the identity matrix, with the input samples being trainable and optimizing their location. In Figure 8, each dot represents a sample colored by its original class, where the Xs are the centroids at each iteration. As we can see, with the “smart initialization” of the cluster centers, the points converge to the “right” clusters based on the original cluster centers. However, when we start the cluster centers with some shift, namely there is “wrong” initialization, the clusters start with good clustering but then converge to wrong solutions where they mix points with different classes. 13The Entropy Enigma: Success and Failure of Entropy Minimization Figure 8.Top: With the “smart initialization” of the cluster centers, the points converge to the “right” clusters based on the original cluster centers. Middle: When we start the cluster centers with some shift, namely there is “wrong” initialization, the clusters start with good clustering but then converge to wrong solutions where they mix points with different classes. Bottom: For reference, we also show the regular Expectation Maximization algorithm on the shifted dataset. The X’s represent cluster centroids at each iteration. 14The Entropy Enigma: Success and Failure of Entropy Minimization B. Different Parameterizations of f In this section, we test the different ways of parameterizing the weighted-flips-to-accuracy function, f. Firstly, we look at the effects of not weighing each flip, and then we look at linear and cubic interpolations between flips and accuracy (as opposed to quadratic interpolation, used in the rest of the paper). Our results in Table 3 show that the optimalf is a weighted and interpolated quadratically, with the other variations not far behind. Importantly, all variations off perform better than the second best performing method, COT (Lu et al., 2023). Table 3.Mean Absolute Error between estimated accuracy, and true accuracy on a ResNet-50 model, for weighted and unweighted flips-to-accuracy functions, that are either linear, quadratic, or cubic interpolations of points. Datasets Unweighted Linear Unweighted Quadratic Weighted Linear Weighted Quadratic Weighted Cubic Noises IN-C {75} (Hendrycks & Dietterich, 2019) 4.95 5.04 5.94 4.79 5.23 IN-C {50} (Mintun et al., 2021) 7.19 7.36 7.94 7.35 7.01 IN-3DCC {60} (Kar et al., 2022) 4.10 4.12 4.33 3.66 4.25 CCC {27} (Press et al., 2023) 2.97 3.22 4.8 2.80 4.34 Domain Shifts Stylized (Geirhos et al., 2019) 7.12 7.12 7.12 3.81 7.12 IN-V2 {3} (Recht et al., 2019) 3.55 3.71 5.42 4.70 4.03 IN-Sketch (Wang et al., 2019) 1.11 1.32 2.64 4.23 0.23 IN-R (Hendrycks et al., 2021a) 1.43 1.67 3.01 1.88 0.52 IN-D (Rusak et al., 2022a) Real 3.39 3.16 2.04 3.18 4.70 Painting 2.07 1.94 0.34 2.20 0.85 Clipart 2.78 3.08 5.12 3.37 2.44 Sketch 6.12 6.95 12.89 5.44 12.38 Infograph 7.28 8.76 10.35 3.63 10.35 Quickdraw 0.79 0.79 0.79 2.57 0.79 Cartoon & Drawing {2} (Salvador & Oberman, 2022) 13.60 13.76 14.34 13.25 12.96 Adversarial Noises BG Challenge {8} (Xiao et al., 2020) 7.19 7.36 7.33 6.92 8.50 IN-A (Hendrycks et al., 2021b) 23.70 23.53 20.39 21.61 22.91 IN-C Patch {75} (Gu et al., 2022) 1.95 2.00 2.42 1.60 1.48 IN-Hard (Taesiri et al., 2023) 5.27 4.92 0.72 3.64 3.49 Patch-IN {10} (Pintor et al., 2023) 7.42 7.55 9.02 8.87 7.98 IN-Obfuscations {3} (Stimberg et al., 2023) 0.20 0.10 0.10 4.58 0.10 OOD/Other ObjectNet (Barbu et al., 2019) 6.81 6.81 6.81 2.74 6.81 NINCO (Bitterwolf et al., 2023) 20.20 19.85 14.98 18.07 17.73 Average 6.14 6.27 6.47 5.75 6.36 Worst Case 23.70 23.53 20.39 21.61 22.91 Average (Worst Case Excluded) 5.34 5.48 5.84 5.03 5.60 15The Entropy Enigma: Success and Failure of Entropy Minimization C. WF with Limited Data To further test WF’s ability in a challenging setting, we look at how it performs under memory and data constraints. To this end, we test WF in the following scenarios: (1) WF is only allowed to store 100 samples for calculating flips, and (2) when whole dataset is limited to 100 samples for flip calculation and 1,000 samples for adaptation). We note that previous work assumes the existence of at least 2,000 test samples (Niu et al., 2022). In both cases, we use the original weighted-flips-to-accuracy function, f, by multiplying the the weighted flips calculated on 100 samples by 10, and plugging the output into f. Even with only using 100 samples, WF is able to best the original implementation by a bit. Surprisingly, even with limited data and memory, WF manages to remain competitive with unconstrained methods, and is significantly ahead of COT, when it is constrained in a similar manner. Table 4.WF is effective in memory constrained settings. Without finetuning or refitting f, WF beats the original implementation, when only using 100 samples to calculate weighted flips (WF limited mem). In the limited memory/data setting, WF gets access to only 1000 samples in total, 100 of which are used for flip calculations. In this setting, COT gets access to 1,000 input samples and 1,000 in distribution samples. Best results are in bold; second best are underlined, {.} indicates how many splits are in each dataset, when there are more than 1. Datasets COT original WF original WF limited mem COT limited mem/data WF limited mem/data Noises IN-C {75} (Hendrycks & Dietterich, 2019) 2.23 4.79 7.52 36.67 6.52 IN-C {50} (Mintun et al., 2021) 3.17 7.35 8.34 40.55 4.60 IN-3DCC {60} (Kar et al., 2022) 3.02 3.66 3.97 34.44 4.31 CCC {27} (Press et al., 2023) 2.04 2.80 3.71 26.67 4.92 Domain Shifts Stylized (Geirhos et al., 2019) 12.18 3.81 3.37 38.84 2.50 IN-V2 {3} (Recht et al., 2019) 2.68 4.70 4.00 43.96 3.80 IN-Sketch (Wang et al., 2019) 4.23 1.71 1.68 12.46 3.39 IN-R (Hendrycks et al., 2021a) 2.44 1.88 3.03 14.99 12.03 IN-D (Rusak et al., 2022a) Real 27.54 3.18 1.73 41.52 6.51 Painting 7.49 2.12 0.71 26.21 18.44 Clipart 4.52 3.37 5.91 15.98 8.10 Sketch 0.71 5.44 6.30 12.65 4.50 Infograph 3.44 3.63 1.24 4.57 2.51 Quickdraw 1.60 2.57 2.80 0.06 2.46 Cartoon & Drawing {2} (Salvador & Oberman, 2022) 1.62 13.25 16.48 33.25 13.44 Adversarial Noises BG Challenge {8} (Xiao et al., 2020) 19.68 6.92 5.84 32.84 10.15 IN-A (Hendrycks et al., 2021b) 30.38 21.61 16.75 15.30 29.15 IN-C Patch {75} (Gu et al., 2022) 2.57 1.60 1.98 47.03 1.92 IN-Hard (Taesiri et al., 2023) 15.33 3.64 0.65 5.83 14.73 Patch-IN {10} (Pintor et al., 2023) 10.13 8.87 9.09 49.68 9.81 IN-Obfuscations {3} (Stimberg et al., 2023) 0.12 4.58 4.67 0.09 8.93 OOD/Other ObjectNet (Barbu et al., 2019) 10.40 2.74 0.29 2.44 2.74 NINCO (Bitterwolf et al., 2023) 20.28 18.07 20.24 13.05 35.68 Average 8.17 5.75 5.67 23.87 9.40 Worst Case 30.38 21.61 20.24 49.68 35.68 Average (Worst Case Excluded) 7.16 5.03 5.00 22.70 8.21 16The Entropy Enigma: Success and Failure of Entropy Minimization D. Weighted Flips Ablations D.1. Stopping Iteration Ablations WF measures the amount of weighted flips from iteration 0 to iteration 1,000. This is done because RDumb resets the model to its pretrained state every 1,000 iterations, in order to avoid collapse (Press et al., 2023). Here, we look at how measuring weighted flips before iteration 1,000 affects the performance of WF. Interestingly, using 500 iterations increases performance by a relative 26.89% as opposed to the 1,000 iterations used in the rest of the paper. Stopping Iteration Datasets 1000 500 250 100 50 IN-C 4.79 4.88 4.99 5.48 5.67 IN-C 7.35 7.48 7.84 8.96 10.10 IN-3DCC 3.66 3.32 3.37 3.00 3.06 IN-V2 4.70 4.59 4.93 5.11 5.49 IN-D Real 3.18 0.36 0.96 2.82 5.01 Painting 2.12 1.21 4.02 11.28 14.83 Clipart 3.37 0.31 3.75 9.30 14.85 Sketch 5.44 2.49 0.33 5.61 9.26 Infograph 3.63 3.46 0.09 4.37 7.53 Quickdraw 2.57 1.73 7.55 17.54 25.35 Average 4.08 2.98 3.78 7.35 10.12 Figure 9. Left: Mean Absolute Error between estimated accuracy and true accuracy, when measuring weighted flips between iteration 0 and various stopping iterations. Right: For different stopping iterations, interpolating between the points in the holdout set yields different weighted-flips-to-accuracy functions. D.2. Holdout Set Size Ablations Holdout Set Size Datasets 1000 500 250 100 50 IN-C 4.79 4.77 5.52 6.07 7.62 IN-C 7.35 5.91 6.74 7.06 8.40 IN-3DCC 3.66 5.10 4.34 5.17 5.22 IN-V2 4.70 5.13 5.48 3.47 9.10 IN-D Real 3.18 3.50 1.78 0.57 4.21 Painting 2.12 5.38 1.92 6.93 8.57 Clipart 3.37 6.69 7.96 8.79 7.70 Sketch 5.44 10.23 7.39 4.87 4.10 Infograph 3.63 6.15 0.88 3.05 3.77 Quickdraw 2.57 0.71 9.87 38.86 31.66 Average 4.08 5.36 5.19 8.48 9.04 Figure 10.Left: Mean Absolute Error between estimated accuracy and true accuracy, when measuring weighted flips on sets of images of different sizes. Right: For different holdout set sizes, interpolating between the points in the holdout set yields different weighted-flips-to- accuracy functions. f can only output values that are between 0 and 100. 17The Entropy Enigma: Success and Failure of Entropy Minimization E. WF with other TTA Methods WF esimates the accuracy of a dataset as RDumb (Press et al., 2023) is used to adapt to it. In this section, we show that WF work with a variety of different EM methods. To further showcase the versatility of WF, we do not finetune any method, and use the original weighted-flips-to-accuracy function f, for all experiments in Table 5. Table 5. Mean Absolute Error between estimated accuracy and true accuracy, when adapting to data using a ResNet-50 backbone and different TTA methods: Tent (Wang et al., 2020), RPL (Rusak et al., 2022b), and CPL (Goyal et al., 2022). In all cases, the original weighted-flips-to-accuracy function f is used, highlighting the versatility of WF. Datasets RDumb Tent RPL CPL IN-C 4.79 6.75 6.85 5.14 IN-C 7.35 9.68 7.20 7.44 IN-3DCC 3.66 2.92 3.99 3.72 IN-V2 4.70 3.80 3.82 4.42 IN-D Real 3.18 5.15 5.15 0.31 Painting 2.12 7.59 7.59 0.03 Clipart 3.37 6.98 7.11 2.57 Sketch 5.44 3.30 3.52 3.86 Infograph 3.63 2.29 2.24 3.40 Quickdraw 2.57 2.24 2.24 2.37 Average 4.08 5.07 4.97 3.33 F. Additional Vision Transformer Experiments To further analyze WF and the second best method, COT, we add additionally analysis using a ViT-B/16 model. The task is to estimate the accuracy of a ViT-B/16 on a variety of datasets. We compare between using the original weighted-flips-accuracy function, f, which was interpolated using data from a ResNet-50, and interpolating the function using ViT-B/16 data points. In both cases, the datasets used to interpolate are the same. Additionally, we compare to COT on this task. Table 6. Mean Absolute Error between estimated accuracy and true accuracy, when estimating the accuracy of a ViT-B/16 on different datasets. Datasets WF WF (new f) COT IN-C 8.34 1.64 22.24 IN-C 6.59 1.48 25.37 IN-3DCC 7.19 1.87 18.43 IN-V2 4.44 3.63 21.29 IN-D Real 1.02 7.27 37.21 Painting 3.02 7.06 19.13 Clipart 12.82 0.25 13.58 Sketch 11.04 1.90 5.74 Infograph 9.27 3.34 1.16 Quickdraw 2.36 13.04 0.28 Average 6.61 4.15 16.44 18The Entropy Enigma: Success and Failure of Entropy Minimization G. Omitting Samples by Top-k Accuracy/Entropy Level In addition to removing samples by Top-k accuracy, we also analyze the effects of removing samples according to their initial entropy level. We find that both experiments exhibit similar behaviour: it is possible to remove many Top- k/low entropy samples, without significantly affecting the accuracy gain of Tent (on a holdout set of Gaussian Noise 3). Figure 11.Left: Average entropy across top-k samples for different values of k. The percentages shown are the fraction of images out of the whole dataset. The original dataset, Gaussian Noise 3, has an average entropy of 2.84. Right: The relative size of the datasets, when top-k samples are removed. Figure 12.Left: Accuracy gain per iteration on a holdout set, as Tent adapts to its inputs. Each line corresponds to a different experiment where we remove samples based on their initial entropy level. Similarly to Figure 2, it’s possible to remove low entropy samples while barely hurting performance. When entropy ≤ 0, no images are excluded. Right: The relative size of the datasets and their average entropy, when samples with a entropy level≤ k are removed. 19The Entropy Enigma: Success and Failure of Entropy Minimization H. Silhouette score, Shift distance, and Accuracy Throughout Entropy Minimization In Figure 4, we looked at the changes of Silhouette scores/Shift distances for each phase in EM. Here, we show how these scores, along with accuracy, change in every iteration of Tent. For each one of the datasets analyzed, we group noises based on severity level, and plot their averages and standard deviations, for every iteration. Figure 13.Changes in Silhouette scores, Shift distances, and Accuracies as Tent adapts to its inputs. We group together noises by severity level, and average the data for every iteration. 20The Entropy Enigma: Success and Failure of Entropy Minimization I. WF on CIFAR10/100 WF is not as effecitve on CIFAR10 (Krizhevsky et al., 2009) as it is on ImageNet (Deng et al., 2009) scale datasets. CIFAR10 is an outlier in entropy minimization: for example, Press et al. (2023) showed that Tent doesn’t degrade in accuracy, even after 100 million CIFAR10 images seen. We nonetheless run our method on CIFAR10. On average, we see only 0-5 label flips per dataset on C10-C. This is far from what we see ImageNet-scale datasets we tested. Like in the paper, we interpolate a weighted-flips-to-accuracy function f on the holdout set and get: f(x) = −249.36x2 − 87.39x + 77.01 which has a MAE of 16.64 on the C10 validation set. We repeat this for CIFAR100 and get: f(x) = 0.000322x2 − 0.287x + 99.54 which has a MAE of 9.10 on the C100 validation set. Apart from refitting f, we did not tune any other parameter in these two experiments. J. RDumb WF uses RDumb (Press et al., 2023) to estimate accuracy. We go over the implementation of the method in brief. RDumb is based on ETA (Niu et al., 2022), wherein the model is reset to its pretrained state every 1,000 iterations. Rdumb optimizes the BatchNorm (Ioffe & Szegedy, 2015) parameters, Θ of a given classifier f. The loss optimized is entropy, with two filtration steps: the first, in which samples with high entropy are filtered out, and the second, in which samples that produce logits similar to previous samples are filtered out. For a sample x, the first filtration is given by: Sent(x) = 1 exp[E(x; Θ)− E0] · IE(x;Θ)<E0 (x), with E0 = 0.4 × ln103. The second filtration is given by: Sdiv(x) = I{cos(fo(x),m−1)<ϵ}(x) where cos() is the cosine similarity, and mt is an exponential moving average of the logits of previously seen samples at iteration t: mt = ( y1, if t = 1 αyt + (1 − α)mt−1, if t >1 and yt is the average model prediction on a batch of inputs at step t, and α = 0.9. Put together with entropy minimization, the optimization formula becomes: min ˆΘ −Sent(x) · Sdiv(x) X y∈C fΘ(y|x) logfΘ(y|x) RDumb uses a SGD with a learning rate of 2.5 × 10−4, and a batch size of 64, and is reset to its pre-trained state every 1,000 iterations. 21The Entropy Enigma: Success and Failure of Entropy Minimization K. Software Licenses • ImageNet-C (Hendrycks & Dietterich, 2019)Apache License 2.0 https://github.com/hendrycks/robustness • ImageNet-R (Hendrycks et al., 2021a) MIT License https://github.com/hendrycks/imagenet-r • ImageNet-3D-CC (Kar et al., 2022): CC-BY-NC 4.0 License https://github.com/EPFL-VILAB/3DCommonCorruptions • ImageNet- C (Mintun et al., 2021): MIT License https://github.com/facebookresearch/augmentation-corruption • ImageNet-V2 (Recht et al., 2019): MIT License https://github.com/modestyachts/ImageNetV2 • Backgrounds Challenge (Xiao et al., 2020): https://github.com/MadryLab/backgrounds_challenge • CCC (Press et al., 2023): MIT License https://github.com/oripress/CCC • Stylized ImageNet (Geirhos et al., 2019): MIT License https://github.com/rgeirhos/Stylized-ImageNet • NINCO (Bitterwolf et al., 2023): MIT License https://github.com/j-cb/NINCO • ImageNet-D (Rusak et al., 2022a): Apache License 2.0 https://github.com/bethgelab/robustness • ObjectNet (Barbu et al., 2019): MIT License https://objectnet.dev/ • Shift Happens Benchmark: Apache License 2.0 https://github.com/shift-happens-benchmark/ icml-2022 22",
      "meta_data": {
        "arxiv_id": "2405.05012v2",
        "authors": [
          "Ori Press",
          "Ravid Shwartz-Ziv",
          "Yann LeCun",
          "Matthias Bethge"
        ],
        "published_date": "2024-05-08T12:26:15Z",
        "pdf_url": "https://arxiv.org/pdf/2405.05012v2.pdf",
        "github_url": "https://github.com/hendrycks/robustness"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper analyzes the success and failure mechanisms of Entropy Minimization (EM) in Test Time Adaptation (TTA), revealing a two-phase clustering dynamic where initial alignment with training data embeddings improves accuracy, followed by divergence and accuracy degradation. Building on these insights, it introduces Weighted Flips (WF), a novel, state-of-the-art method for estimating a model's accuracy on unlabeled datasets. WF achieves a mean absolute error of 5.75%, representing a 29.62% improvement over previous state-of-the-art methods.",
        "methodology": "For analyzing EM, the study uses k-means clustering on embeddings and computes the Silhouette score for cluster quality. Alignment with training data is quantified by 'Shift distance', which measures the average distance between k-means centroids and mean class embeddings from the ImageNet validation set, matched using the Hungarian method. The Weighted Flips (WF) method estimates accuracy by measuring the sum of initial confidence percentiles for images whose predicted labels change ('flip') during adaptation using a TTA method (specifically RDumb). This weighted flip count is then converted into an accuracy estimate via a pre-calibrated quadratic function 'f'.",
        "experimental_setup": "The analysis of EM's clustering dynamics was conducted on ImageNet-C (Gaussian Noise level 3), IN-C, and IN-3DCC datasets using a ResNet-50 model. The WF method was extensively evaluated across 23 diverse ImageNet-scale datasets (326 individual splits), including various noise types, domain shifts, adversarial noises, and out-of-distribution (OOD) classes. Performance was measured using Mean Absolute Error and compared against AC, DoC, ATC, and COT methods. WF's versatility was demonstrated across different TTA methods (Tent, RPL, CPL) and model architectures (ResNet variants, ResNext-101, ViT-B-16, MaxViT-T). Ablation studies were performed on stopping iterations for flip calculation, holdout set sizes, and different parameterizations for the 'f' function. Experiments on CIFAR10/100 were also conducted, though with different effectiveness.",
        "limitations": "EM's effectiveness is limited to a few adaptation iterations, after which accuracy degrades due to embedding divergence. Practical application of 'Shift distance' for accuracy estimation is challenging as access to original training data is typically restricted in TTA settings. The Silhouette score's utility is limited by its dependency on dataset-specific class distributions and total number of classes. While robust, WF's optimal performance may require re-fitting the 'f' function for different model architectures. Additionally, WF is less effective on smaller datasets like CIFAR10/100, where EM exhibits different adaptation behaviors.",
        "future_research_directions": "Future research could focus on developing TTA strategies that explicitly prevent the divergence of test data embeddings from training data distributions, thereby maintaining accuracy during prolonged adaptation. There is a need to explore new, practical metrics for accuracy estimation in TTA settings that do not require access to training data or are less sensitive to specific class distributions. Further investigation into why EM behaves differently on datasets of varying scales (e.g., CIFAR vs. ImageNet) could lead to more universally robust TTA and accuracy estimation techniques. Deeper theoretical analyses of EM's relationship with embedding dynamics and neural collapse are also warranted to better understand and control model adaptation processes. Lastly, the principles of the Weighted Flips method could be extended to a broader range of self-supervised learning and adaptation scenarios.",
        "experimental_code": "def dist(sigma, mode='top5'): if mode == 'top5': return np.sum(np.abs(cum_sum_top5[:5] - cum_sum_top5[sigma-1][:5])) elif mode == 'zipf': return np.sum(np.abs(recip - recip[sigma-1])*recip) def ranking_dist(ranks, noise_perturbation=True if 'noise' in args.perturbation else False, mode='top5'): result = 0 step_size = 1 if noise_perturbation else args.difficulty for vid_ranks in ranks: result_for_vid = [] for i in range(step_size): perm1 = vid_ranks[i] perm1_inv = np.argsort(perm1) for rank in vid_ranks[i::step_size][1:]: perm2 = rank result_for_vid.append(dist(perm2[perm1_inv], mode)) if not noise_perturbation: perm1 = perm2 perm1_inv = np.argsort(perm1) result += np.mean(result_for_vid) / len(ranks) return result def flip_prob(predictions, noise_perturbation=True if 'noise' in args.perturbation else False): result = 0 step_size = 1 if noise_perturbation else args.difficulty for vid_preds in predictions: result_for_vid = [] for i in range(step_size): prev_pred = vid_preds[i] for pred in vid_preds[i::step_size][1:]: result_for_vid.append(int(prev_pred != pred)) if not noise_perturbation: prev_pred = pred result += np.mean(result_for_vid) / len(predictions) return result",
        "experimental_info": "The provided repository content does not contain a direct implementation of 'k-means clustering on embeddings and computes the Silhouette score' for cluster quality analysis.For 'Shift distance', the code provides `dist` and `ranking_dist` functions (from ImageNet-P/test.py) which calculate ranking distances based on permutation differences (top5 or zipf distance). However, the method description specifies measuring 'the average distance between k-means centroids and mean class embeddings from the ImageNet validation set, matched using the Hungarian method', which is not directly implemented by the provided `dist` and `ranking_dist` functions.For 'Weighted Flips (WF)', the code includes a `flip_prob` function (from ImageNet-P/test.py) which calculates the probability of predicted labels changing across perturbed images. This partially aligns with the 'flip' aspect of WF. However, the method description explicitly mentions 'sum of initial confidence percentiles', 'adaptation using a TTA method (specifically RDumb)', and conversion into 'an accuracy estimate via a pre-calibrated quadratic function 'f''. These specific components (confidence percentiles, RDumb TTA, and quadratic function for accuracy conversion) are not present in the `flip_prob` implementation."
      }
    },
    {
      "title": "Tent: Fully Test-Time Adaptation by Entropy Minimization",
      "abstract": "A model must adapt itself to generalize to new and different data during\ntesting. In this setting of fully test-time adaptation the model has only the\ntest data and its own parameters. We propose to adapt by test entropy\nminimization (tent): we optimize the model for confidence as measured by the\nentropy of its predictions. Our method estimates normalization statistics and\noptimizes channel-wise affine transformations to update online on each batch.\nTent reduces generalization error for image classification on corrupted\nImageNet and CIFAR-10/100 and reaches a new state-of-the-art error on\nImageNet-C. Tent handles source-free domain adaptation on digit recognition\nfrom SVHN to MNIST/MNIST-M/USPS, on semantic segmentation from GTA to\nCityscapes, and on the VisDA-C benchmark. These results are achieved in one\nepoch of test-time optimization without altering training.",
      "full_text": "Published as a conference paper at ICLR 2021 TENT : F ULLY TEST-TIME ADAPTATION BY ENTROPY MINIMIZATION Dequan Wang1∗, Evan Shelhamer2∗†, Shaoteng Liu1, Bruno Olshausen1, Trevor Darrell1 dqwang@cs.berkeley.edu, shelhamer@google.com UC Berkeley1 Adobe Research2 ABSTRACT A model must adapt itself to generalize to new and different data during testing. In this setting of fully test-time adaptation the model has only the test data and its own parameters. We propose to adapt by test entropy minimization (tent 1): we optimize the model for conﬁdence as measured by the entropy of its predictions. Our method estimates normalization statistics and optimizes channel-wise afﬁne transformations to update online on each batch. Tent reduces generalization error for image classiﬁcation on corrupted ImageNet and CIFAR-10/100 and reaches a new state-of-the-art error on ImageNet-C. Tent handles source-free domain adapta- tion on digit recognition from SVHN to MNIST/MNIST-M/USPS, on semantic segmentation from GTA to Cityscapes, and on the VisDA-C benchmark. These results are achieved in one epoch of test-time optimization without altering training. 1 I NTRODUCTION Deep networks can achieve high accuracy on training and testing data from the same distribution, as evidenced by tremendous benchmark progress (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; He et al., 2016). However, generalization to new and different data is limited (Hendrycks & Dietterich, 2019; Recht et al., 2019; Geirhos et al., 2018). Accuracy suffers when the training (source) data differ from the testing (target) data, a condition known as dataset shift(Quionero-Candela et al., 2009). Models can be sensitive to shifts during testing that were not known during training, whether natural variations or corruptions, such as unexpected weather or sensor degradation. Nevertheless, it can be necessary to deploy a model on different data distributions, so adaptation is needed. During testing, the model must adapt given only its parameters and the target data. Thisfully test-time adaptation setting cannot rely on source data or supervision. Neither is practical when the model ﬁrst encounters new testing data, before it can be collected and annotated, as inference must go on. Real-world usage motivates fully test-time adaptation by data, computation, and task needs: 1. Availability. A model might be distributed without source data for bandwidth, privacy, or proﬁt. 2. Efﬁciency. It might not be computationally practical to (re-)process source data during testing. 3. Accuracy. A model might be too inaccurate without adaptation to serve its purpose. To adapt during testing we minimize the entropy of model predictions. We call this objective the test entropy and name our method tent after it. We choose entropy for its connections to error and shift. Entropy is related to error, as more conﬁdent predictions are all-in-all more correct (Figure 1). Entropy is related to shifts due to corruption, as more corruption results in more entropy, with a strong rank correlation to the loss for image classiﬁcation as the level of corruption increases (Figure 2). To minimize entropy, tent normalizes and transforms inference on target data by estimating statistics and optimizing afﬁne parameters batch-by-batch. This choice of low-dimensional, channel-wise feature modulation is efﬁcient to adapt during testing, even for online updates. Tent does not restrict or alter model training: it is independent of the source data given the model parameters. If the model can be run, it can be adapted. Most importantly, tent effectively reduces not just entropy but error. ∗Equal contribution. †Work done at Adobe Research; the author is now at DeepMind. 1Please see the project page at https://github.com/DequanWang/tent for the code and more. 1 arXiv:2006.10726v3  [cs.LG]  18 Mar 2021Published as a conference paper at ICLR 2021 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Entropy 0 20 40 60 80Error (%) Figure 1: Predictions with lower entropy have lower error rates on corrupted CIFAR-100-C. Certainty can serve as supervision during testing. 0.2 0.3 0.4 0.5 0.6 Entropy 0.2 0.4 0.6 0.8 1.0 1.2Loss = 0.61 original noise blur digital weather  level level Figure 2: More corruption causes more loss and entropy on CIFAR-100-C. Entropy can estimate the degree of shift without training data or labels. Our results evaluate generalization to corruptions for image classiﬁcation, to domain shift for digit recognition, and to simulation-to-real shift for semantic segmentation. For context with more data and optimization, we evaluate methods for robust training, domain adaptation, and self-supervised learning given the labeled source data. Tent can achieve less error given only the target data, and it improves on the state-of-the-art for the ImageNet-C benchmark. Analysis experiments support our entropy objective, check sensitivity to the amount of data and the choice of parameters for adaptation, and back the generality of tent across architectures. Our contributions • We highlight the setting of fully test-time adaptation with only target data and no source data. To emphasize practical adaptation during inference we benchmark with ofﬂine and online updates. • We examine entropy as an adaptation objective and propose tent: a test-time entropy minimization scheme to reduce generalization error by reducing the entropy of model predictions on test data. • For robustness to corruptions, tent reaches 44.0% error on ImageNet-C, better than the state-of- the-art for robust training (50.2%) and the strong baseline of test-time normalization (49.9%). • For domain adaptation, tent is capable of online and source-free adaptation for digit classiﬁcation and semantic segmentation, and can even rival methods that use source data and more optimization. 2 S ETTING : F ULLY TEST-TIME ADAPTATION Adaptation addresses generalization from source to target. A model fθ(x) with parameters θtrained on source data and labels xs,ys may not generalize when tested on shifted target data xt. Table 1 summarizes adaptation settings, their required data, and types of losses. Our fully test-time adaptation setting uniquely requires only the model fθ and unlabeled target data xt for adaptation during inference. Existing adaptation settings extend training given more data and supervision. Transfer learning by ﬁne-tuning (Donahue et al., 2014; Yosinski et al., 2014) needs target labels to (re-)train with a supervised loss L(xt,yt). Without target labels, our setting denies this supervised training. Domain adaptation (DA) (Quionero-Candela et al., 2009; Saenko et al., 2010; Ganin & Lempitsky, 2015; Tzeng et al., 2015) needs both the source and target data to train with a cross-domain loss L(xs,xt). Test-time training (TTT) (Sun et al., 2019b) adapts during testing but ﬁrst alters training to jointly optimize its supervised loss L(xs,ys) and self-supervised loss L(xs). Without source, our setting denies joint training across domains (DA) or losses (TTT). Existing settings have their purposes, but do not cover all practical cases when source, target, or supervision are not simultaneously available. Unexpected target data during testing requires test-time adaptation. TTT and our setting adapt the model by optimizing an unsupervised loss during testing L(xt). During training, TTT jointly optimizes this same loss on source data L(xs) with a supervised loss L(xs,ys), to ensure the parameters θare shared across losses for compatibility with adaptation by L(xt). Fully test-time adaptation is independent of the training data and training loss given the parameters θ. By not changing training, our setting has the potential to require less data and computation for adaptation. 2Published as a conference paper at ICLR 2021 Table 1: Adaptation settings differ by their data and therefore losses during training and testing. Of the source s and target t data xand labels y, our fully test-time setting only needs the target data xt. setting source data target data train loss test loss ﬁne-tuning - xt,yt L(xt,yt) - domain adaptation xs, ys xt L(xs,ys) + L(xs,xt) - test-time training xs, ys xt L(xs,ys) + L(xs) L(xt) fully test-time adaptation - xt - L(xt)     = f (     ; θ)  Loss (   ,      ) θ (a) training <latexit sha1_base64=\"YDIW6Mi/jc4gnv843zXedRTilJU=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49mF654lbdGcgy8XJSgRyNXvmr249ZGnGFTFJjOp6boJ9RjYJJPil1U8MTykZ0wDuWKhpx42ezUyfkxCp9EsbalkIyU39PZDQyZhwFtjOiODSL3lT8z+ukGF74mVBJilyx+aIwlQRjMv2b9IXmDOXYEsq0sLcSNqSaMrTplGwI3uLLy6R1VvVqVc+7qVXql3keRTiCYzgFD86hDtfQgCYwGMAzvMKbI50X5935mLcWnHzmEP7A+fwBBSaOGg==</latexit> x s <latexit sha1_base64=\"1psCF/1OyjuZ4TwBu21voNG0XaI=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8eK1hbaWDbbSbt0swm7GyGE/gQvHhTEq3/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dkorq2vrG+XNytb2zu5edf/gQcepYthisYhVJ6AaBZfYMtwI7CQKaRQIbAfj66nffkKleSzvTZagH9Gh5CFn1FjpLnvU/WrNrbszkGXiFaQGBZr96ldvELM0QmmYoFp3PTcxfk6V4UzgpNJLNSaUjekQu5ZKGqH289mpE3JilQEJY2VLGjJTf0/kNNI6iwLbGVEz0oveVPzP66YmvPRzLpPUoGTzRWEqiInJ9G8y4AqZEZkllClubyVsRBVlxqZTsSF4iy8vk/ZZ3Tuve97tea1xVeRRhiM4hlPw4AIacANNaAGDITzDK7w5wnlx3p2PeWvJKWYO4Q+czx8GrY4b</latexit> y s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"1psCF/1OyjuZ4TwBu21voNG0XaI=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8eK1hbaWDbbSbt0swm7GyGE/gQvHhTEq3/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dkorq2vrG+XNytb2zu5edf/gQcepYthisYhVJ6AaBZfYMtwI7CQKaRQIbAfj66nffkKleSzvTZagH9Gh5CFn1FjpLnvU/WrNrbszkGXiFaQGBZr96ldvELM0QmmYoFp3PTcxfk6V4UzgpNJLNSaUjekQu5ZKGqH289mpE3JilQEJY2VLGjJTf0/kNNI6iwLbGVEz0oveVPzP66YmvPRzLpPUoGTzRWEqiInJ9G8y4AqZEZkllClubyVsRBVlxqZTsSF4iy8vk/ZZ3Tuve97tea1xVeRRhiM4hlPw4AIacANNaAGDITzDK7w5wnlx3p2PeWvJKWYO4Q+czx8GrY4b</latexit> y s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"YDIW6Mi/jc4gnv843zXedRTilJU=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49mF654lbdGcgy8XJSgRyNXvmr249ZGnGFTFJjOp6boJ9RjYJJPil1U8MTykZ0wDuWKhpx42ezUyfkxCp9EsbalkIyU39PZDQyZhwFtjOiODSL3lT8z+ukGF74mVBJilyx+aIwlQRjMv2b9IXmDOXYEsq0sLcSNqSaMrTplGwI3uLLy6R1VvVqVc+7qVXql3keRTiCYzgFD86hDtfQgCYwGMAzvMKbI50X5935mLcWnHzmEP7A+fwBBSaOGg==</latexit> x s (b) fully test-time adaptation θ    = f (    ; θ+Δ)  Entropy (    ) <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t <latexit sha1_base64=\"m/ZzdjACtPk7VVv8qMUMxHkn5f0=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49YK9ccavuDGSZeDmpQI5Gr/zV7ccsjbhCJqkxHc9N0M+oRsEkn5S6qeEJZSM64B1LFY248bPZqRNyYpU+CWNtSyGZqb8nMhoZM44C2xlRHJpFbyr+53VSDC/8TKgkRa7YfFGYSoIxmf5N+kJzhnJsCWVa2FsJG1JNGdp0SjYEb/HlZdI6q3q1qufd1Cr1yzyPIhzBMZyCB+dQh2toQBMYDOAZXuHNkc6L8+58zFsLTj5zCH/gfP4ABquOGw==</latexit> x t <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t <latexit sha1_base64=\"m/ZzdjACtPk7VVv8qMUMxHkn5f0=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49YK9ccavuDGSZeDmpQI5Gr/zV7ccsjbhCJqkxHc9N0M+oRsEkn5S6qeEJZSM64B1LFY248bPZqRNyYpU+CWNtSyGZqb8nMhoZM44C2xlRHJpFbyr+53VSDC/8TKgkRa7YfFGYSoIxmf5N+kJzhnJsCWVa2FsJG1JNGdp0SjYEb/HlZdI6q3q1qufd1Cr1yzyPIhzBMZyCB+dQh2toQBMYDOAZXuHNkc6L8+58zFsLTj5zCH/gfP4ABquOGw==</latexit> x t <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t     = f (     ; θ)  Loss (   ,      ) θ (a) training <latexit sha1_base64=\"YDIW6Mi/jc4gnv843zXedRTilJU=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49mF654lbdGcgy8XJSgRyNXvmr249ZGnGFTFJjOp6boJ9RjYJJPil1U8MTykZ0wDuWKhpx42ezUyfkxCp9EsbalkIyU39PZDQyZhwFtjOiODSL3lT8z+ukGF74mVBJilyx+aIwlQRjMv2b9IXmDOXYEsq0sLcSNqSaMrTplGwI3uLLy6R1VvVqVc+7qVXql3keRTiCYzgFD86hDtfQgCYwGMAzvMKbI50X5935mLcWnHzmEP7A+fwBBSaOGg==</latexit> x s <latexit sha1_base64=\"1psCF/1OyjuZ4TwBu21voNG0XaI=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8eK1hbaWDbbSbt0swm7GyGE/gQvHhTEq3/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dkorq2vrG+XNytb2zu5edf/gQcepYthisYhVJ6AaBZfYMtwI7CQKaRQIbAfj66nffkKleSzvTZagH9Gh5CFn1FjpLnvU/WrNrbszkGXiFaQGBZr96ldvELM0QmmYoFp3PTcxfk6V4UzgpNJLNSaUjekQu5ZKGqH289mpE3JilQEJY2VLGjJTf0/kNNI6iwLbGVEz0oveVPzP66YmvPRzLpPUoGTzRWEqiInJ9G8y4AqZEZkllClubyVsRBVlxqZTsSF4iy8vk/ZZ3Tuve97tea1xVeRRhiM4hlPw4AIacANNaAGDITzDK7w5wnlx3p2PeWvJKWYO4Q+czx8GrY4b</latexit> y s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"1psCF/1OyjuZ4TwBu21voNG0XaI=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8eK1hbaWDbbSbt0swm7GyGE/gQvHhTEq3/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dkorq2vrG+XNytb2zu5edf/gQcepYthisYhVJ6AaBZfYMtwI7CQKaRQIbAfj66nffkKleSzvTZagH9Gh5CFn1FjpLnvU/WrNrbszkGXiFaQGBZr96ldvELM0QmmYoFp3PTcxfk6V4UzgpNJLNSaUjekQu5ZKGqH289mpE3JilQEJY2VLGjJTf0/kNNI6iwLbGVEz0oveVPzP66YmvPRzLpPUoGTzRWEqiInJ9G8y4AqZEZkllClubyVsRBVlxqZTsSF4iy8vk/ZZ3Tuve97tea1xVeRRhiM4hlPw4AIacANNaAGDITzDK7w5wnlx3p2PeWvJKWYO4Q+czx8GrY4b</latexit> y s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"YDIW6Mi/jc4gnv843zXedRTilJU=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49mF654lbdGcgy8XJSgRyNXvmr249ZGnGFTFJjOp6boJ9RjYJJPil1U8MTykZ0wDuWKhpx42ezUyfkxCp9EsbalkIyU39PZDQyZhwFtjOiODSL3lT8z+ukGF74mVBJilyx+aIwlQRjMv2b9IXmDOXYEsq0sLcSNqSaMrTplGwI3uLLy6R1VvVqVc+7qVXql3keRTiCYzgFD86hDtfQgCYwGMAzvMKbI50X5935mLcWnHzmEP7A+fwBBSaOGg==</latexit> x s (b) fully test-time adaptation θ    = f (    ; θ+Δ)  Entropy (    ) <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t <latexit sha1_base64=\"m/ZzdjACtPk7VVv8qMUMxHkn5f0=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49YK9ccavuDGSZeDmpQI5Gr/zV7ccsjbhCJqkxHc9N0M+oRsEkn5S6qeEJZSM64B1LFY248bPZqRNyYpU+CWNtSyGZqb8nMhoZM44C2xlRHJpFbyr+53VSDC/8TKgkRa7YfFGYSoIxmf5N+kJzhnJsCWVa2FsJG1JNGdp0SjYEb/HlZdI6q3q1qufd1Cr1yzyPIhzBMZyCB+dQh2toQBMYDOAZXuHNkc6L8+58zFsLTj5zCH/gfP4ABquOGw==</latexit> x t <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t <latexit sha1_base64=\"m/ZzdjACtPk7VVv8qMUMxHkn5f0=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49YK9ccavuDGSZeDmpQI5Gr/zV7ccsjbhCJqkxHc9N0M+oRsEkn5S6qeEJZSM64B1LFY248bPZqRNyYpU+CWNtSyGZqb8nMhoZM44C2xlRHJpFbyr+53VSDC/8TKgkRa7YfFGYSoIxmf5N+kJzhnJsCWVa2FsJG1JNGdp0SjYEb/HlZdI6q3q1qufd1Cr1yzyPIhzBMZyCB+dQh2toQBMYDOAZXuHNkc6L8+58zFsLTj5zCH/gfP4ABquOGw==</latexit> x t <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t Figure 3: Method overview. Tent does not alter training (a), but minimizes the entropy of predictions during testing (b) over a constrained modulation ∆, given the parameters θand target data xt. 3 M ETHOD : T EST ENTROPY MINIMIZATION VIA FEATURE MODULATION We optimize the model during testing to minimize the entropy of its predictions by modulating its features. We call our method tent for test entropy. Tent requires a compatible model, an objective to minimize (Section 3.1), and parameters to optimize over (Section 3.2) to fully deﬁne the algorithm (Section Section 3.3). Figure 3 outlines our method for fully test-time adaptation. The model to be adapted must be trained for the supervised task, probabilistic, and differentiable. No supervision is provided during testing, so the model must already be trained. Measuring the entropy of predictions requires a distribution over predictions, so the model must be probabilistic. Gradients are required for fast iterative optimization, so the model must be differentiable. Typical deep networks for supervised learning satisfy these model requirements. 3.1 E NTROPY OBJECTIVE Our test-time objective L(xt) is to minimize the entropy H(ˆy) of model predictions ˆy= fθ(xt). In particular, we measure the Shannon entropy (Shannon, 1948), H(ˆy) = −∑ cp(ˆyc) logp(ˆyc) for the probability ˆyc of class c. Note that optimizing a single prediction has a trivial solution: assign all probability to the most probable class. We prevent this by jointly optimizing batched predictions over parameters that are shared across the batch. Entropy is an unsupervised objective because it only depends on predictions and not annotations. However, as a measure of the predictions it is directly related to the supervised task and model. In contrast, proxy tasks for self-supervised learning are not directly related to the supervised task. Proxy tasks derive a self-supervised label y′from the input xt without the task label y. Examples of these proxies include rotation prediction (Gidaris et al., 2018), context prediction (Doersch et al., 2015), and cross-channel auto-encoding (Zhang et al., 2017). Too much progress on a proxy task could interfere with performance on the supervised task, and self-supervised adaptation methods have to limit or mix updates accordingly (Sun et al., 2019b;a). As such, care is needed to choose a proxy compatible with the domain and task, to design the architecture for the proxy model, and to balance optimization between the task and proxy objectives. Our entropy objective does not need such efforts. 3.2 M ODULATION PARAMETERS The model parameters θare a natural choice for test-time optimization, and these are the choice of prior work for train-time entropy minimization (Grandvalet & Bengio, 2005; Dhillon et al., 2020; Carlucci et al., 2017). However, θis the only representation of the training/source data in our setting, and altering θcould cause the model to diverge from its training. Furthermore, f can be nonlinear and θcan be high dimensional, making optimization too sensitive and inefﬁcient for test-time usage. 3Published as a conference paper at ICLR 2021 IN OUT+ <latexit sha1_base64=\"FGMSn1olAms3UkJ+mUM6lRBkJrw=\">AAAB6HicbVDLSgNBEOyNryS+oh69DAZBEMKuKHoMevGYgHlgsoTZSW8yZvbBzKwYlnyBFw+K5OoP+C/e/BqdJB40saChqOqmu8uLBVfatj+tzNLyyupaNpdf39jc2i7s7NZVlEiGNRaJSDY9qlDwEGuaa4HNWCINPIENb3A18Rv3KBWPwhs9jNENaC/kPmdUG6l63CkU7ZI9BVkkzg8plnPx+Pb94avSKXy0uxFLAgw1E1SplmPH2k2p1JwJHOXbicKYsgHtYcvQkAao3HR66IgcGqVL/EiaCjWZqr8nUhooNQw80xlQ3Vfz3kT8z2sl2r9wUx7GicaQzRb5iSA6IpOvSZdLZFoMDaFMcnMrYX0qKdMmm7wJwZl/eZHUT0rOaemsatK4hBmysA8HcAQOnEMZrqECNWCA8AjP8GLdWU/WqzWetWasn5k9+APr7RuTUJCF</latexit> \u0000 <latexit sha1_base64=\"8eHH7cr25vA7s0zJYYCDPQNSaT0=\">AAAB7XicbVDLSgNBEOyNrxhfUY+KDAbBU9gVQb0FvXhMwDwgWcLsZDYZM7OzzMwKYcnRuxcPinj1F/Id3vwGf8LJ46CJBQ1FVTfdXUHMmTau++VklpZXVtey67mNza3tnfzuXk3LRBFaJZJL1QiwppxFtGqY4bQRK4pFwGk96N+M/foDVZrJ6M4MYuoL3I1YyAg2Vqq1ulgI3M4X3KI7AVok3owUSoejyvfj0ajczn+2OpIkgkaGcKx103Nj46dYGUY4HeZaiaYxJn3cpU1LIyyo9tPJtUN0YpUOCqWyFRk0UX9PpFhoPRCB7RTY9PS8Nxb/85qJCS/9lEVxYmhEpovChCMj0fh11GGKEsMHlmCimL0VkR5WmBgbUM6G4M2/vEhqZ0XvvHhVsWlcwxRZOIBjOAUPLqAEt1CGKhC4hyd4gVdHOs/Om/M+bc04s5l9+APn4wd3ypLI</latexit> ⇥ <latexit sha1_base64=\"r9CoIRh1LwyAxszWUWZZpZEIYvU=\">AAAB7XicbVA9TwJBEJ3DL8Av1NLmIjGxIndGoyXRxhIT+YhwIXvLHqzs7V5254yE8B9sLDDG1tL/Yuev0QUsFHzJJC/vzWRmXpgIbtDzPp3M0vLK6lo2l1/f2NzaLuzs1oxKNWVVqoTSjZAYJrhkVeQoWCPRjMShYPWwfznx6/dMG67kDQ4SFsSkK3nEKUEr1VrIY2bahaJX8qZwF4n/Q4rlXDK+fX/4qrQLH62OomnMJFJBjGn6XoLBkGjkVLBRvpUalhDaJ13WtFQSuyQYTq8duYdW6biR0rYkulP198SQxMYM4tB2xgR7Zt6biP95zRSj82DIZZIik3S2KEqFi8qdvO52uGYUxcASQjW3t7q0RzShaAPK2xD8+ZcXSe245J+UTq9tGhcwQxb24QCOwIczKMMVVKAKFO7gEcbw7CjnyXlxXmetGednZg/+wHn7Btf2kwo=</latexit> \u0000 <latexit sha1_base64=\"icKTvSnYuWAwxCN4MXaVcPxJrUE=\">AAAB7HicbVBNS8NAEN34WetX1aMiwSJ4KokI6q3oxWMLpi20oWy2k3bpZhN2J0IJPXr24kERr/6G/g5v/gb/hNuPg7Y+GHi8N8PMvCARXKPjfFlLyyura+u5jfzm1vbObmFvv6bjVDHwWCxi1QioBsEleMhRQCNRQKNAQD3o3479+gMozWN5j4ME/Ih2JQ85o2gkrxUA0nah6JScCexF4s5IsXw0qn4/Ho8q7cJnqxOzNAKJTFCtm66ToJ9RhZwJGOZbqYaEsj7tQtNQSSPQfjY5dmifGqVjh7EyJdGeqL8nMhppPYgC0xlR7Ol5byz+5zVTDK/8jMskRZBsuihMhY2xPf7c7nAFDMXAEMoUN7farEcVZWjyyZsQ3PmXF0ntvORelK6rJo0bMkWOHJITckZccknK5I5UiEcY4eSJvJBXS1rP1pv1Pm1dsmYzB+QPrI8ftLWSVw==</latexit> \u0000 <latexit sha1_base64=\"6pSYsGji0D9Bm0vY9by0e43+pZo=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgxbArAfUW9OIxAfOAZAmzk95kzOzsMjMrhJAv8OJBEa9+kjf/xkmyB00saCiquunuChLBtXHdbye3tr6xuZXfLuzs7u0fFA+PmjpOFcMGi0Ws2gHVKLjEhuFGYDtRSKNAYCsY3c381hMqzWP5YMYJ+hEdSB5yRo2V6he9Ysktu3OQVeJlpAQZar3iV7cfszRCaZigWnc8NzH+hCrDmcBpoZtqTCgb0QF2LJU0Qu1P5odOyZlV+iSMlS1pyFz9PTGhkdbjKLCdETVDvezNxP+8TmrCa3/CZZIalGyxKEwFMTGZfU36XCEzYmwJZYrbWwkbUkWZsdkUbAje8surpHlZ9irlm3qlVL3N4sjDCZzCOXhwBVW4hxo0gAHCM7zCm/PovDjvzseiNedkM8fwB87nD3htjL0=</latexit> ÷ <latexit sha1_base64=\"KLNiQjydwC+UjsLtIanox9T+rq8=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoN6KXjxWsB/QhrLZbNqlu5uwuymU0L/gxYMiXv1D3vw3btoctPXBwOO9GWbmBQln2rjut1Pa2Nza3invVvb2Dw6PqscnHR2nitA2iXmsegHWlDNJ24YZTnuJolgEnHaDyX3ud6dUaRbLJzNLqC/wSLKIEWxyaRCy6bBac+vuAmideAWpQYHWsPo1CGOSCioN4Vjrvucmxs+wMoxwOq8MUk0TTCZ4RPuWSiyo9rPFrXN0YZUQRbGyJQ1aqL8nMiy0nonAdgpsxnrVy8X/vH5qohs/YzJJDZVkuShKOTIxyh9HIVOUGD6zBBPF7K2IjLHCxNh4KjYEb/XlddK5qnuN+u1jo9a8K+IowxmcwyV4cA1NeIAWtIHAGJ7hFd4c4bw4787HsrXkFDOn8AfO5w8aWY5N</latexit> µ <latexit sha1_base64=\"lbHwl5bkUbenc+Yo+u8yNzpxsy0=\">AAAB6nicbVDLSgNBEOyNrxhfUY+KDAbBU9gVQb0FvXhM0DwgWcLsZDYZMjO7zMwKYcnRoxcPinj1I/Id3vwGf8LJ46CJBQ1FVTfdXUHMmTau++VklpZXVtey67mNza3tnfzuXk1HiSK0SiIeqUaANeVM0qphhtNGrCgWAaf1oH8z9usPVGkWyXsziKkvcFeykBFsrHTXEkk7X3CL7gRokXgzUigdjirfj0ejcjv/2epEJBFUGsKx1k3PjY2fYmUY4XSYayWaxpj0cZc2LZVYUO2nk1OH6MQqHRRGypY0aKL+nkix0HogAtspsOnpeW8s/uc1ExNe+imTcWKoJNNFYcKRidD4b9RhihLDB5Zgopi9FZEeVpgYm07OhuDNv7xIamdF77x4VbFpXMMUWTiAYzgFDy6gBLdQhioQ6MITvMCrw51n5815n7ZmnNnMPvyB8/EDTj2RiQ==</latexit> \u0000 <latexit sha1_base64=\"xnrzB72KzfqBMQ17s1zlsxQWR+k=\">AAAB7XicbZDLSgMxFIbP1Fsdb1WXboJFcFVmRFAXYtGNywr2Au1QMmmmjU0yQ5IRytB3cONCETcufBT3bsS3Mb0stPWHwMf/n0POOWHCmTae9+3kFhaXllfyq+7a+sbmVmF7p6bjVBFaJTGPVSPEmnImadUww2kjURSLkNN62L8a5fV7qjSL5a0ZJDQQuCtZxAg21qq1NOsK3C4UvZI3FpoHfwrFiw/3PHn7civtwmerE5NUUGkIx1o3fS8xQYaVYYTTodtKNU0w6eMubVqUWFAdZONph+jAOh0Uxco+adDY/d2RYaH1QIS2UmDT07PZyPwva6YmOg0yJpPUUEkmH0UpRyZGo9VRhylKDB9YwEQxOysiPawwMfZArj2CP7vyPNSOSv5x6ezGK5YvYaI87ME+HIIPJ1CGa6hAFQjcwQM8wbMTO4/Oi/M6Kc05055d+CPn/Qf/xpJs</latexit> <latexit sha1_base64=\"9MzbukliF0G5U4WyINCTJmMNjA8=\">AAACNnicdVBNS8NAFNz4bf2KevSyWAQFLUlR9CiK4EWoYFuhiWWz3dSlu0nYfVFL6K/y4u/w1osHRbz6E9y0PWjVgYVhZh773gSJ4Bocp29NTE5Nz8zOzRcWFpeWV+zVtZqOU0VZlcYiVtcB0UzwiFWBg2DXiWJEBoLVg85p7tfvmNI8jq6gmzBfknbEQ04JGKlpX3gyxZ5gIRCl4nvsSQK3QZCd9RoPTfB3sad5W5Kb8j+h7Xx+D5vszk3Zb9pFp+QMgH8Td0SKaIRK0372WjFNJYuACqJ1w3US8DOigFPBegUv1SwhtEParGFoRCTTfjY4u4e3jNLCYazMiwAP1O8TGZFad2VgkvnCetzLxb+8RgrhkZ/xKEmBRXT4UZgKDDHOO8QtrhgF0TWEUMXNrpjeEkUomKYLpgR3/OTfpFYuuQcl53K/eHwyqmMObaBNtI1cdIiO0TmqoCqi6BH10St6s56sF+vd+hhGJ6zRzDr6AevzC4nRq7w=</latexit> µ  E [ x t ] , \u0000 2  E [( µ \u0000 x t ) 2 ] <latexit sha1_base64=\"5uCFLjsyhVlotMr43Rw1BdZFk0s=\">AAACYXicbZFLS+RAFIUrGZ/tK+Ms3RQ2gqC0iSgzy2bcuHTAVqHTNDfVN21hVRKqbmamCf0nZzcbN/4RKzH4vlBw+O659TiVFEpaCsP/nv9lYXFpeWW1s7a+sbkVfN2+snlpBA5ErnJzk4BFJTMckCSFN4VB0InC6+TurO5f/0ZjZZ5d0qzAkYZpJlMpgBwaB3/jKWgNPFaYEhiT/+EtOeBxAYYkqFgD3UqqzudHL6wxHfI4QXo73YBPh59RbRkH3bAXNsU/iqgVXdbWxTj4F09yUWrMSCiwdhiFBY2qekuhcN6JS4sFiDuY4tDJDDTaUdUkNOd7jkx4mhu3MuINfT1RgbZ2phPnrO9r3/dq+FlvWFL6Y1TJrCgJM/F0UFoqTjmv4+YTaVCQmjkBwkh3Vy5uwYAg9ykdF0L0/skfxdVxLzrthb9Ouv2fbRwrbIftsn0Wse+sz87ZBRswwe69BW/D2/Qe/FU/8LefrL7Xznxjb8rfeQSpH7dZ</latexit> \u0000  \u0000 + @ H / @\u0000 , \u0000  \u0000 + @ H / @\u0000 normalization transformation Figure 4: Tent modulates features during testing by estimating normalization statistics µ,σ and optimizing transformation parameters γ,β. Normalization and transformation apply channel-wise scales and shifts to the features. The statistics and parameters are updated on target data without use of source data. In practice, adapting γ,β is efﬁcient because they make up <1% of model parameters. For stability and efﬁciency, we instead only update feature modulations that are linear (scales and shifts), and low-dimensional (channel-wise). Figure 4 shows the two steps of our modulations: normalization by statistics and transformation by parameters. Normalization centers and standardizes the input xinto ¯x= (x−µ)/σby its mean µand standard deviation σ. Transformation turns ¯xinto the output x′= γ¯x+ βby afﬁne parameters for scale γand shift β. Note that the statistics µ,σ are estimated from the data while the parameters γ,β are optimized by the loss. For implementation, we simply repurpose the normalization layers of the source model. We update their normalization statistics and afﬁne parameters for all layers and channels during testing. 3.3 A LGORITHM Initialization The optimizer collects the afﬁne transformation parameters {γl,k,βl,k}for each normalization layer land channel kin the source model. The remaining parameters θ\\{γl,k,βl,k} are ﬁxed. The normalization statistics {µl,k,σl,k}from the source data are discarded. Iteration Each step updates the normalization statistics and transformation parameters on a batch of data. The normalization statistics are estimated for each layer in turn, during the forward pass. The transformation parameters γ,β are updated by the gradient of the prediction entropy ∇H(ˆy), during the backward pass. Note that the transformation update follows the prediction for the current batch, and so it only affects the next batch (unless forward is repeated). This needs just one gradient per point of additional computation, so we use this scheme by default for efﬁciency. Termination For online adaptation, no termination is necessary, and iteration continues as long as there is test data. For ofﬂine adaptation, the model is ﬁrst updated and then inference is repeated. Adaptation may of course continue by updating for multiple epochs. 4 E XPERIMENTS We evaluate tent for corruption robustness on CIFAR-10/CIFAR-100 and ImageNet, and for domain adaptation on digit adaptation from SVHN to MNIST/MNIST-M/USPS. Our implementation is in PyTorch (Paszke et al., 2019) with the pycls library (Radosavovic et al., 2019). Datasets We run on image classiﬁcation datasets for corruption and domain adaptation conditions. For large-scale experiments we choose ImageNet (Russakovsky et al., 2015), with 1,000 classes, a training set of 1.2 million, and a validation set of 50,000. For experiments at an accessible scale we choose CIFAR-10/CIFAR-100 (Krizhevsky, 2009), with 10/100 classes, a training set of 50,000, and a test set of 10,000. For domain adaptation we choose SVHN (Netzer et al., 2011) as source and MNIST (LeCun et al., 1998)/MNIST-M (Ganin & Lempitsky, 2015)/USPS (Hull, 1994) as targets, with ten classes for the digits 0–9. SVHN has color images of house numbers from street views with a training set of 73,257 and test set of 26,032. MNIST/MNIST-M/USPS have handwritten digits with a training sets of 60,000/60,000/7,291 and test sets of 10,000/10,000/2,007. Models For corruption we use residual networks (He et al., 2016) with 26 layers (R-26) on CIFAR- 10/100 and 50 layers (R-50) on ImageNet. For domain adaptation we use the R-26 architecture. For fair comparison, all methods in each experimental condition share the same architecture. Our networks are equipped with batch normalization (Ioffe & Szegedy, 2015). For the source model without adaptation, the normalization statistics are estimated during training on the source data. For all test-time adaptation methods, we estimate these statistics during testing on the target data, as done in concurrent work on adaptation by normalization (Schneider et al., 2020; Nado et al., 2020). 4Published as a conference paper at ICLR 2021 Table 2: Corruption benchmark on CIFAR-10-C and CIFAR-100-C for the highest severity. Tent has least error, with less optimization than domain adaptation (RG, UDA-SS) and test-time training (TTT), and improves on test-time norm (BN). Method Source Target Error (%) C10-C C100-C Source train 40.8 67.2 RG train train 18.3 38.9 UDA-SS train train 16.7 47.0 TTT train test 17.5 45.0 BN test 17.3 42.6 PL test 15.7 41.2 Tent (ours) test 14.3 37.3 originalgaussshot impulsedefocus glassmotionzoomsnowfrostfog bright contrastelasticpixeljpeg 0 25 50 75Error (%) source 59.5% norm 49.9% tent 44.0% ANT 50.2% Figure 5: Corruption benchmark on ImageNet-C: error for each type averaged over severity levels. Tent improves on the prior state-of-the-art, adver- sarial noise training (Rusak et al., 2020), by fully test-time adaptation without altering training. Optimization We optimize the modulation parameters γ,β following the training hyperparameters for the source model with few changes. On ImageNet we optimize by SGD with momentum; on other datasets we optimize by Adam (Kingma & Ba, 2015). We lower the batch size (BS) to reduce memory usage for inference, then lower the learning rate (LR) by the same factor to compensate (Goyal et al., 2017). On ImageNet, we set BS = 64 and LR = 0.00025, and on other datasets we set BS = 128 and LR = 0.001.We control for ordering by shufﬂing and sharing the order across methods. Baselines We compare to domain adaptation, self-supervision, normalization, and pseudo-labeling: • source applies the trained classiﬁer to the test data without adaptation, • adversarial domain adaptation (RG) reverses the gradients of a domain classiﬁer on source and target to optimize for a domain-invariant representation (Ganin & Lempitsky, 2015), • self-supervised domain adaptation (UDA-SS) jointly trains self-supervised rotation and position tasks on source and target to optimize for a shared representation (Sun et al., 2019a), • test-time training (TTT) jointly trains for supervised and self-supervised tasks on source, then keeps training the self-supervised task on target during testing (Sun et al., 2019b), • test-time normalization (BN) updates batch normalization statistics (Ioffe & Szegedy, 2015) on the target data during testing (Schneider et al., 2020; Nado et al., 2020), • pseudo-labeling (PL) tunes a conﬁdence threshold, assigns predictions over the threshold as labels, and then optimizes the model to these pseudo-labels before testing (Lee, 2013). Only test-time normalization (BN), pseudo-labeling (PL), and tent (ours) are fully test-time adaptation methods. See Section 2 for an explanation and contrast with domain adaptation and test-time training. 4.1 R OBUSTNESS TO CORRUPTIONS To benchmark robustness to corruption, we make use of common image corruptions (see Appendix A for examples). The CIFAR-10/100 and ImageNet datasets are turned into the CIFAR-10/100-C and ImageNet-C corruption benchmarks by duplicating their test/validation sets and applying 15 types of corruptions at ﬁve severity levels (Hendrycks & Dietterich, 2019). Tent improves more with less data and computation.Table 2 reports errors averaged over corrup- tion types at the severest level of corruption. On CIFAR-10/100-C we compare all methods, including those that require joint training across domains or losses, given the convenient sizes of these datasets. Adaptation is ofﬂine for fair comparison with ofﬂine baselines. Tent improves on the fully test-time adaptation baselines (BN, PL) but also the domain adaptation (RG, UDA-SS) and test-time training (TTT) methods that need several epochs of optimization on source and target. Tent consistently improves across corruption types.Figure 5 plots the error for each corruption type averaged over corruption levels on ImageNet-C. We compare the most efﬁcient methods—source, normalization, and tent—given the large scale of the source data (>1 million images) needed by other methods and the 75 target combinations of corruption types and levels. Tent and BN adapt online to rival the efﬁciency of inference without adaptation. Tent reaches the least error for most corruption types without increasing the error on the original data. 5Published as a conference paper at ICLR 2021 Table 3: Digit domain adaptation from SVHN to MNIST/MNIST-M/USPS. Source-free adaptation is not only feasible, but more efﬁcient. Tent always improves on normalization (BN), and in 2/3 cases achieves less error than domain adaptation (RG, UDA-SS) without joint training on source & target. Method Source Target Epochs Error (%) Source + Target MNIST MNIST-M USPS Source train - 18.2 39.7 19.3 RG train train 10 + 10 15.0 33.4 18.9 UDA-SS train train 10 + 10 11.1 22.2 18.4 BN test 0 + 1 15.7 39.7 18.0 Tent (ours) test 0 + 1 10.0 37.0 16.3 Tent (ours) test 0 + 10 8.2 36.8 14.4 Tent reaches a new state-of-the-art without altering training.The state-of-the-art methods for robustness extend training with adversarial noise (ANT) (Rusak et al., 2020) for 50.2% error or mixtures of data augmentations (AugMix) (Hendrycks et al., 2020) for 51.7% error. Combined with stylization from external images (SIN) (Geirhos et al., 2019), ANT+SIN reaches 47.4%. Tent reaches a new state-of-the-art of 44.0% by online adaptation and 42.3% by ofﬂine adaptation. It improves on ANT for all types except noise, on which ANT is trained. This requires just one gradient per test point, without more optimization on the training set (ANT, AugMix) or use of external images (SIN). Among fully test-time adaptation methods, tent reduces the error beyond test-time normalization for 18% relative improvement. In concurrent work, Schneider et al. (2020) report 49.3% error for test-time normalization, for which tent still gives 14% relative improvement. 4.2 S OURCE -FREE DOMAIN ADAPTATION We benchmark digit adaptation (Ganin & Lempitsky, 2015; Tzeng et al., 2015; 2017; Shu et al., 2018) for shifts from SVHN to MNIST/MNIST-M/USPS. Recall that unsupervised domain adaptation makes use the labeled source data and unlabeled target data, while our fully test-time adaptation setting denies use of source data. Adaptation is ofﬂine for fair comparison with ofﬂine baselines. Tent adapts to target without source.Table 3 reports the target errors for domain adaptation and fully test-time adaptation methods. Test-time normalization (BN) marginally improves, while adversarial domain adaptation (RG) and self-supervised domain adaptation (UDA-SS) improve more by joint training on source and target. Tent always has lower error than the source model and BN, and it achieves the lowest error in 2/3 cases, even in just one epoch and without use of source data. While encouraging for fully test-time adaptation, unsupervised domain adaptation remains necessary for the highest accuracy and harder shifts. For SVHN-to-MNIST, DIRT-T (Shu et al., 2018) achieves a remarkable 0.6% error 2. For MNIST-to-SVHN, a difﬁcult shift with source-only error of 71.3%, DIRT-T reaches45.5% and UDA-SS reaches 38.7%. Tent fails on this shift and increases error to 79.8%. In this case success presently requires joint optimization over source and target. Tent needs less computation, but still improves with more.Tent adapts efﬁciently on target data alone with just one gradient per point. RG & UDA-SS also use the source data (SVHN train), which is ∼7×the size of the target data (MNIST test), and optimize for 10 epochs. Tent adapts with ∼80× less computation. With more updates, tent reaches 8.2% error in 10 epochs and 6.5% in 100 epochs. With online updates, tent reaches 12.5% error in one epoch and 8.4% error in 10 epochs. Tent scales to semantic segmentation.To show scalability to large models and inputs, we evaluate semantic segmentation (pixel-wise classiﬁcation) on a domain shift from a simulated source to a real target. The source is GTA (Richter et al., 2017), a video game in an urban environment, and the target is Cityscapes (Cordts et al., 2016), an urban autonomous driving dataset. The model is HRNet-W18, a fully convolutional network (Shelhamer et al., 2017) with high-resolution architecture (Wang et al., 2020). The target intersection-over-union scores (higher is better) are source 28.8%, BN 31.4%, and tent 35.8% with ofﬂine optimization by Adam. For adaptation to a single image, tent reaches 36.4% in 10 iterations with episodic optimization. See the appendix for a qualitative example (Appendix B). 2We exclude DIRT-T from our experiments because of incomparable differences in architecture and model selection. DIRT-T tunes with labeled target data, but we do not. Please refer to Shu et al. (2018) for more detail. 6Published as a conference paper at ICLR 2021 Figure 6: Tent reduces the entropy and loss. We plot changes in entropy∆Hand loss ∆Lfor all of CIFAR-100-C. Change in entropy rank-correlates with change in loss: note the dark diagonal and the rank correlation coefﬁcient of 0.22. (a) Source (b) BN  (c) Tent (d) Oracle  Figure 7: Adapted features on CIFAR-100-C with Gaussian noise (front) and reference features without corruption (back). Corruption shifts fea- tures away from the reference, but BN reduces the shifts. Tent instead shifts features more, and closer to an oracle that optimizes on target labels. Tent scales to the VisDA-C challenge.To show adaptation on a more difﬁcult benchmark, we evaluate on the VisDA-C challenge (Peng et al., 2017). The task is object recognition for 12 classes where the source data is synthesized by rendering 3D models and the target data is collected from real scenes. The validation error for our source model (ResNet-50, pretrained on ImageNet) is 56.1%, while tent reaches 45.6%, and improves to 39.6% by updating all layers except for the ﬁnal classiﬁer as done by Liang et al. (2020). Although ofﬂine source-free adaptation by model adaptation (Li et al., 2020) or SHOT (Liang et al., 2020) can reach lower error with more computation and tuning, tent can adapt online during testing. 4.3 A NALYSIS Tent reduces entropy and error.Figure 6 veriﬁes tent does indeed reduce the entropy and the task loss (softmax cross-entropy). We plot changes in entropy and loss on CIFAR-100-C for all 75 corruption type/level combinations. Both axes are normalized by the maximum entropy of a prediction (log 100) and clipped to ±1. Most points have lower entropy and error after adaptation. Tent needs feature modulation.We ablate the normalization and transformation steps of feature modulation. Not updating normalization increases errors, and can fail to improve over BN and PL. Not updating transformation parameters reduces the method to test-time normalization. Updating only the last layer of the model can improve but then degrades with further optimization. Updating the full model parameters θnever improves over the unadapted source model. Tent generalizes across target data.Adaptation could be limited to the points used for updates. We check that adaptation generalizes across points by adapting on target train and not target test. Test errors drop: CIFAR-100-C error goes from 37.3% to 34.2% and SVHN-to-MNIST error goes from 8.2% to 6.5%. (Train is larger than test; when subsampling to the same size errors differ by <0.1%.) Therefore the adapted modulation is not point speciﬁc but general. Tent modulation differs from normalization.Modulation normalizes and transforms features. We examine the combined effect. Figure 7 contrasts adapted features on corrupted data against reference features on uncorrupted data. We plot features from the source model, normalization, tent, and an oracle that optimizes on the target labels. Normalization makes features more like the reference, but tent does not. Instead, tent makes features more like the oracle. This suggests a different and task-speciﬁc effect. See the appendix for visualizations of more layers (Appendix C). 7Published as a conference paper at ICLR 2021 Tent adapts alternative architectures.Tent is architecture agnostic in principle. To gauge its generality in practice, we evaluate new architectures based on self-attention (SAN) (Zhao et al., 2020) and equilibrium solving (MDEQ) (Bai et al., 2020) for corruption robustness on CIFAR-100-C. Table 4 shows that tent reduces error with the same settings as convolutional residual networks. Table 4: Tent adapts alternative architectures on CIFAR-100-C without tuning. Results are error (%). SAN-10 (pair) SAN-10 (patch) MDEQ (large) Source BN Tent Source BN Tent Source BN Tent 55.3 39.7 36.7 48.0 31.8 29.2 53.3 44.9 41.7 5 R ELATED WORK We relate tent to existing adaptation, entropy minimization, and feature modulation methods. Train-Time AdaptationDomain adaptation jointly optimizes on source and target by cross-domain losses L(xs,xt) to mitigate shift. These losses optimize feature alignment (Gretton et al., 2009; Sun et al., 2017), adversarial invariance (Ganin & Lempitsky, 2015; Tzeng et al., 2017), or shared proxy tasks (Sun et al., 2019a). Transduction (Gammerman et al., 1998; Joachims, 1999; Zhou et al., 2004) jointly optimizes on train and test to better ﬁt speciﬁc test instances. While effective in their settings, neither applies when joint use of source/train and target/test is denied. Tent adapts on target alone. Recent “source-free” methods (Li et al., 2020; Kundu et al., 2020; Liang et al., 2020) also adapt without source data. Li et al. (2020); Kundu et al. (2020) rely on generative modeling and optimize multiple models with multiple losses. Kundu et al. (2020); Liang et al. (2020) also alter training. Tent does not need generative modeling, nor does it alter training, and so it can deployed more generally to adapt online with much more computational efﬁciency. SHOT (Liang et al., 2020) adapts by informa- tion maximization (entropy minimization and diversity regularization), but differs in its other losses and its parameterization. These source-free methods optimize ofﬂine with multiple losses for multiple epochs, which requires more tuning and computation than tent, but may achieve more accuracy with more computation. Tent optimizes online with just one loss and an efﬁcient parameterization of modulation to emphasize fully test-time adaptation during inference. We encourage examination of each of these works on the frontier of adaptation without source data. Chidlovskii et al. (2016) are the ﬁrst to motivate adaptation without source data for legal, commercial, or technical concerns. They adapt predictions by applying denoising auto-encoders while we adapt models by entropy minimization. We share their motivations, but the methods and experiments differ. Test-Time AdaptationTent adapts by test-time optimization and normalization to update the model. Test-time adaptation of predictions, through which harder and uncertain cases are adjusted based on easier and certain cases (Jain & Learned-Miller, 2011), provides inspiration for certainty-based model adaptation schemes like our own. Test-time training (TTT) (Sun et al., 2019b) also optimizes during testing, but differs in its loss and must alter training. TTT relies on a proxy task, such as recognizing rotations of an image, and so its loss depends on the choice of proxy. (Indeed, its authors caution that the proxy must be “both well-deﬁned and non-trivial in the new domain”). TTT alters training to optimize this proxy loss on source before adapting to target. Tent adapts without proxy tasks and without altering training. Normalizing feature statistics is common for domain adaptation (Gretton et al., 2009; Sun et al., 2017). For batch normalization Li et al. (2017); Carlucci et al. (2017) separate source and target statistics during training. Schneider et al. (2020); Nado et al. (2020) estimate target statistics during testing to improve generalization. Tent builds on test-time normalization to further reduce generalization error. Entropy MinimizationEntropy minimization is a key regularizer for domain adaptation (Carlucci et al., 2017; Shu et al., 2018; Saito et al., 2019; Roy et al., 2019), semi-supervised learning (Grandvalet & Bengio, 2005; Lee, 2013; Berthelot et al., 2019), and few-shot learning (Dhillon et al., 2020). Regularizing entropy penalizes decisions at high densities in the data distribution to improve accuracy for distinct classes (Grandvalet & Bengio, 2005). These methods regularize entropy during training in concert with other supervised and unsupervised losses on additional data. Tent is the ﬁrst to minimize 8Published as a conference paper at ICLR 2021 entropy during testing, for adaptation to dataset shifts, without other losses or data. Entropic losses are common; our contribution is to exhibit entropy as the sole lossfor fully test-time adaptation. Feature ModulationModulation makes a model vary with its input. We optimize modulations that are simpler than the full model for stable and efﬁcient adaptation. We modulate channel-wise afﬁne transformations, for their effectiveness in tandem with normalization (Ioffe & Szegedy, 2015; Wu & He, 2018), and for their ﬂexibility in conditioning for different tasks (Perez et al., 2018). These normalization and conditioning methods optimize the modulation during training by a supervised loss, but keep it ﬁxed during testing. We optimize the modulation during testing by an unsupervised loss, so that it can adapt to different target data. 6 D ISCUSSION Tent reduces generalization error on shifted data by test-time entropy minimization. In minimizing entropy, the model adapts itself to feedback from its own predictions. This is truly self-supervised self-improvement. Self-supervision of this sort is totally deﬁned by the supervised task, unlike proxy tasks designed to extract more supervision from the data, and yet it remarkably still reduces error. Nevertheless, errors due to corruption and other shifts remain, and therefore more adaptation is needed. Next steps should pursue test-time adaptation on more and harder types of shift, over more general parameters, and by more effective and efﬁcient losses. Shifts Tent reduces error for a variety of shifts including image corruptions, simple changes in appearance for digits, and simulation-to-real discrepancies. These shifts are popular as standardized benchmarks, but other real-world shifts exist. For instance, the CIFAR 10.1 and ImageNetV2 test sets (Recht et al., 2018; 2019), made by reproducing the dataset collection procedures, entail natural but unknown shifts. Although error is higher on both sets, indicating the presence of shift, tent does not improve generalization. Adversarial shifts (Szegedy et al., 2014) also threaten real-world usage, and attackers keep adapting to defenses. While adversarial training (Madry et al., 2018) makes a difference, test-time adaptation could help counter such test-time attacks. Parameters Tent modulates the model by normalization and transformation, but much of the model stays ﬁxed. Test-time adaptation could update more of the model, but the issue is to identify parameters that are both expressive and reliable, and this may interact with the choice of loss. TTT adapts multiple layers of features shared by supervised and self-supervised models and SHOT adapts all but the last layer(s) of the model. These choices depend on the model architecture, the loss, and tuning. For tent modulation is reliable, but the larger shift on VisDA is better addressed by the SHOT parameterization. Jointly adapting the input could be a more general alternative. If a model can adapt itself on target, then perhaps its input gradients might optimize spatial transformations or image translations to reduce shift without source data. Losses Tent minimizes entropy. For more adaptation, is there an effective loss for general but episodic test-time optimization? Entropy is general across tasks but limited in scope. It needs batches for optimization, and cannot update episodically on one point at a time. TTT can do so, but only with the right proxy task. For less computation, is there an efﬁcient loss for more local optimization? Tent and TTT both require full (re-)computation of the model for updates because they depend on its predictions. If the loss were instead deﬁned on the representation, then updates would require less forward and backward computation. Returning to entropy speciﬁcally, this loss may interact with calibration (Guo et al., 2017), as better uncertainty estimation could drive better adaptation. We hope that the fully test-time adaptation setting can promote new methods for equipping a model to adapt itself, just as tent yields a new model with every update. ACKNOWLEDGMENTS We thank Eric Tzeng for discussions on domain adaptation, Bill Freeman for comments on the experiments, Yu Sun for consultations on test-time training, and Kelsey Allen for feedback on the exposition. We thank the anonymous reviewers of ICLR 2021 for their feedback, which certainly improved the latest adaptation of the paper. 9Published as a conference paper at ICLR 2021 REFERENCES Shaojie Bai, Vladlen Koltun, and J Zico Kolter. Multiscale deep equilibrium models. arXiv preprint arXiv:2006.08656, 2020. David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. In NeurIPS, 2019. Fabio Maria Carlucci, Lorenzo Porzi, Barbara Caputo, Elisa Ricci, and Samuel Rota Bulo. Autodial: Automatic domain alignment layers. In 2017 IEEE International Conference on Computer Vision (ICCV), pp. 5077–5085. IEEE, 2017. Boris Chidlovskii, Stephane Clinchant, and Gabriela Csurka. Domain adaptation in the absence of source domain data. In SIGKDD, pp. 451–460, 2016. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016. Guneet Singh Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano Soatto. A baseline for few-shot image classiﬁcation. In ICLR, 2020. Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context prediction. In ICCV, 2015. J. Donahue, Y . Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In ICML, 2014. A Gammerman, V V ovk, and V Vapnik. Learning by transduction. InUAI, 1998. Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In ICML, 2015. Robert Geirhos, Carlos RM Temme, Jonas Rauber, Heiko H Schütt, Matthias Bethge, and Felix A Wichmann. Generalisation in humans and deep neural networks. In NeurIPS, 2018. Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and Wieland Brendel. Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. In International Conference on Learning Representations, 2019. Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. In ICLR, 2018. Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In NeurIPS, 2005. A. Gretton, AJ. Smola, J. Huang, M. Schmittfull, KM. Borgwardt, and B. Schölkopf. Covariate shift and local learning by distribution matching. In Dataset Shift in Machine Learning, pp. 131–160. MIT Press, Cambridge, MA, USA, 2009. Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In ICML, 2017. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, June 2016. Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In ICLR, 2019. Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. Augmix: A simple data processing method to improve robustness and uncertainty. In ICLR, 2020. Jonathan J. Hull. A database for handwritten text recognition research. TPAMI, 1994. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015. 10Published as a conference paper at ICLR 2021 Vidit Jain and Erik Learned-Miller. Online domain adaptation of a pre-trained cascade of classiﬁers. In CVPR, 2011. Thorsten Joachims. Transductive inference for text classiﬁcation using support vector machines. In ICML, 1999. Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiﬁcation with deep convolutional neural networks. NeurIPS, 25, 2012. Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009. Jogendra Nath Kundu, Naveen Venkat, R Venkatesh Babu, et al. Universal source-free domain adaptation. In CVPR, pp. 4544–4553, 2020. Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. Dong-Hyun Lee. Pseudo-label: The simple and efﬁcient semi-supervised learning method for deep neural networks. In ICML Workshop on challenges in representation learning, 2013. Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, and Si Wu. Model adaptation: Unsupervised domain adaptation without source data. In CVPR, June 2020. Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou. Revisiting batch normalization for practical domain adaptation. In ICLRW, 2017. Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In ICML, 2020. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018. Zachary Nado, Shreyas Padhy, D Sculley, Alexander D’Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robustness under covariate shift. arXiv preprint arXiv:2006.10963, 2020. Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. NeurIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019. Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko. VisDA: The visual domain adaptation challenge. arXiv preprint arXiv:1710.06924, 2017. Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. In AAAI, 2018. Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset shift in machine learning. MIT Press, Cambridge, MA, USA, 2009. Ilija Radosavovic, Justin Johnson, Saining Xie, Wan-Yen Lo, and Piotr Dollár. On network design spaces for visual recognition. In ICCV, 2019. Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do cifar-10 classiﬁers generalize to cifar-10? arXiv preprint arXiv:1806.00451, 2018. Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet classiﬁers generalize to ImageNet? In ICML, 2019. Stephan R Richter, Zeeshan Hayder, and Vladlen Koltun. Playing for benchmarks. In ICCV, 2017. Subhankar Roy, Aliaksandr Siarohin, Enver Sangineto, Samuel Rota Bulo, Nicu Sebe, and Elisa Ricci. Unsuper- vised domain adaptation using feature-whitening and consensus loss. In CVPR, 2019. 11Published as a conference paper at ICLR 2021 Evgenia Rusak, Lukas Schott, Roland S Zimmermann, Julian Bitterwolf, Oliver Bringmann, Matthias Bethge, and Wieland Brendel. A simple way to make neural networks robust against diverse image corruptions. In ECCV, 2020. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. ImageNet large scale visual recognition challenge. IJCV, 2015. Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to new domains. In European conference on computer vision, pp. 213–226. Springer, 2010. Kuniaki Saito, Donghyun Kim, Stan Sclaroff, Trevor Darrell, and Kate Saenko. Semi-supervised domain adaptation via minimax entropy. In ICCV, 2019. Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improv- ing robustness against common corruptions by covariate shift adaptation. arXiv preprint arXiv:2006.16971, 2020. C.E. Shannon. A mathematical theory of communication. Bell system technical journal, 27, 1948. Evan Shelhamer, Jonathan Long, and Trevor Darrell. Fully convolutional networks for semantic segmentation. PAMI, 2017. Rui Shu, Hung H Bui, Hirokazu Narui, and Stefano Ermon. A dirt-t approach to unsupervised domain adaptation. In ICLR, 2018. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015. Baochen Sun, Jiashi Feng, and Kate Saenko. Correlation alignment for unsupervised domain adaptation. In Domain Adaptation in Computer Vision Applications, pp. 153–171. Springer, 2017. Yu Sun, Eric Tzeng, Trevor Darrell, and Alexei A Efros. Unsupervised domain adaptation through self- supervision. arXiv preprint arXiv:1909.11825, 2019a. Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A Efros, and Moritz Hardt. Test-time training for out-of-distribution generalization. arXiv preprint arXiv:1909.13231, 2019b. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. 2014. Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. In ICCV, 2015. Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In CVPR, 2017. Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, et al. Deep high-resolution representation learning for visual recognition. PAMI, 2020. Yuxin Wu and Kaiming He. Group normalization. In ECCV, 2018. Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In NeurIPS, 2014. Richard Zhang, Phillip Isola, and Alexei A Efros. Split-brain autoencoders: Unsupervised learning by cross- channel prediction. In CVPR, 2017. Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition. In CVPR, 2020. Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, and Bernhard Schölkopf. Learning with local and global consistency. NeurIPS, 2004. 12Published as a conference paper at ICLR 2021 APPENDIX This supplement summarizes the image corruptions used in our experiments, highlights a qualitative example of instance-wise adaptation for semantic segmentation, and visualizes feature shifts across more layers. A R OBUSTNESS TO CORRUPTIONS In Section 4.1 we evaluate methods on a common image corruptions benchmark. Table 2 reports errors on the most severe level of corruption, level 5, and Figure 5 reports errors for each corruption type averaged across each of the levels 1–5. We summarize these corruptions types by example in Figure 8. Gaussian Noise  Shot Noise  Impulse Noise  Defocus Blur  Frosted Glass Blur Motion Blur  Zoom Blur  Snow  Frost  Fog Brightness  Contrast  Elastic  Pixelate  JPEG Figure 8: Examples of each corruption type in the image corruptions benchmark. While synthetic, this set of corruptions aims to represent natural factors of variation like noise, blur, weather, and digital imaging effects. This ﬁgure is reproduced from Hendrycks & Dietterich (2019). B S OURCE -FREE ADAPTATION FOR SEMANTIC SEGMENTATION Figure 9 shows a qualitative result on source-free adaptation for semantic segmentation (pixel-wise classiﬁcation) with simulation-to-real (sim-to-real) shift. For this sim-to-real condition, the source data is simulated while the target data is real. Our source data is GTA Richter et al. (2017), a visually-sophisticated video game set in an urban environment, and our target data is Cityscapes Cordts et al. (2016), an urban autonomous driving dataset. The supervised model is HRnet-W18, a fully convolutional network Shelhamer et al. (2017) in the high-resolution network family Wang et al. (2020). For this qualitative example, we run tent on a single image for multiple iterations, because an image is in effect a batch of pixels. This demonstrates adaptation to a target instance, without any further access to the target domain through usage of multiple images from the target distribution. 13Published as a conference paper at ICLR 2021 image label source-only tent, iteration 1 tent, iteration 5 tent, iteration 10 Figure 9: Adaptation for semantic segmentation with simulation-to-real shift from GTA Richter et al. (2017) to Cityscapes Cordts et al. (2016). Tent only uses the target data, and optimizes over a single image as a dataset of pixel-wise predictions. This episodic optimization in effect ﬁts a custom model to each image of the target domain. In only 10 iterations our method suppresses noise (see the completion of the street segment, in purple) and recovers missing classes (see the motorcycle and rider, center). 14Published as a conference paper at ICLR 2021 C F EATURE SHIFTS ACROSS LAYERS AND METHODS (a) Source (b) BN (c) Tent (d) Oracle Layer 2 Layer 5 Layer 8 Layer 11 Layer 14 Layer 18 Layer 20 Layer 23 Layer 26 Figure 10: Adapted features on CIFAR-100-C with Gaussian noise (front) and reference features without corruption (back). Corruption shifts the source features from the reference. BN shifts the features back to be more like the reference. Tent shifts features to be less like the reference, and more like an oracle that optimizes on target labels. 15",
      "meta_data": {
        "arxiv_id": "2006.10726v3",
        "authors": [
          "Dequan Wang",
          "Evan Shelhamer",
          "Shaoteng Liu",
          "Bruno Olshausen",
          "Trevor Darrell"
        ],
        "published_date": "2020-06-18T17:55:28Z",
        "pdf_url": "https://arxiv.org/pdf/2006.10726v3.pdf",
        "github_url": "https://github.com/DequanWang/tent"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Tent (Test-time entropy minimization), a method for fully test-time adaptation where a model adapts using only its parameters and target data, without source data or supervision. It proposes minimizing the entropy of model predictions to increase confidence and reduce generalization error. Tent achieves new state-of-the-art error (44.0%) on ImageNet-C for robustness to corruptions, outperforming previous methods. It also demonstrates effective source-free domain adaptation for digit recognition (SVHN to MNIST/MNIST-M/USPS) and semantic segmentation (GTA to Cityscapes), and on the VisDA-C benchmark. The method performs online updates efficiently by optimizing low-dimensional, channel-wise affine transformations without altering the original model training.",
        "methodology": "The core methodology involves minimizing the Shannon entropy H(ˆy) = −∑ cp(ˆyc) logp(ˆyc) of model predictions ˆy= fθ(xt) on unlabeled target data. This entropy objective acts as an unsupervised signal. The adaptation is achieved by modulating features within the model, specifically by updating normalization statistics (mean µ and standard deviation σ) and optimizing channel-wise affine transformation parameters (scale γ and shift β) of existing normalization layers. All other model parameters are kept fixed. During testing, normalization statistics are estimated on each target data batch during the forward pass, and the affine parameters are updated via gradient descent on the entropy loss during the backward pass. This process requires a compatible model that is probabilistic and differentiable, which typical deep networks satisfy.",
        "experimental_setup": "The method was evaluated on: 1) Corruption robustness benchmarks using CIFAR-10/100-C and ImageNet-C, which are datasets duplicated with 15 types of corruptions at five severity levels. 2) Domain adaptation for digit recognition from SVHN (source) to MNIST/MNIST-M/USPS (targets). 3) Semantic segmentation with a simulation-to-real shift from GTA (simulated source) to Cityscapes (real target), and the VisDA-C challenge for object recognition. Models used include Residual Networks (R-26 for CIFAR/digits, R-50 for ImageNet), HRNet-W18 for semantic segmentation, and ResNet-50 for VisDA-C, all equipped with batch normalization. Optimization utilized SGD with momentum (ImageNet) or Adam (other datasets), with adjusted learning rates and batch sizes to suit test-time inference. Baselines included Source, Adversarial Domain Adaptation (RG), Self-supervised Domain Adaptation (UDA-SS), Test-Time Training (TTT), Test-Time Normalization (BN), and Pseudo-labeling (PL), with a focus on comparing against other fully test-time adaptation methods (BN, PL).",
        "limitations": "Tent does not improve generalization on natural but unknown shifts like CIFAR 10.1 and ImageNetV2. It fails on some difficult domain shifts, such as MNIST-to-SVHN, where methods requiring joint optimization over source and target data are currently more successful. The entropy objective, while general, is limited in scope as it requires batches for optimization and cannot update episodically on single data points. Furthermore, Tent only modulates normalization and transformation parameters, leaving the majority of the model fixed. For larger shifts, a more extensive parameterization (e.g., adapting all but the last layers) might be more effective, as suggested by its performance on VisDA-C compared to other methods.",
        "future_research_directions": "Future research should explore test-time adaptation on a wider range of challenging shifts, including natural shifts (like CIFAR 10.1, ImageNetV2) and adversarial shifts. Investigating more general parameters for adaptation, beyond just normalization and affine transformations, is another direction, carefully balancing expressiveness and reliability. This could involve adapting more layers or even the entire model, considering how this choice interacts with the loss function. The paper also suggests jointly adapting the input, for instance, by optimizing spatial transformations or image translations to reduce shift without source data. Finally, developing more effective and efficient loss functions for general, episodic, or local test-time optimization (perhaps defined on representations rather than predictions) is proposed, along with studying the interaction between entropy loss and model calibration.",
        "experimental_code": "from copy import deepcopy\n\nimport torch\nimport torch.nn as nn\nimport torch.jit\n\n\nclass Tent(nn.Module):\n    \"\"\"Tent adapts a model by entropy minimization during testing.\n\n    Once tented, a model adapts itself by updating on every forward.\n    \"\"\"\n    def __init__(self, model, optimizer, steps=1, episodic=False):\n        super().__init__()\n        self.model = model\n        self.optimizer = optimizer\n        self.steps = steps\n        assert steps > 0, \"tent requires >= 1 step(s) to forward and update\"\n        self.episodic = episodic\n\n        # note: if the model is never reset, like for continual adaptation,\n        # then skipping the state copy would save memory\n        self.model_state, self.optimizer_state = \\\n            copy_model_and_optimizer(self.model, self.optimizer)\n\n    def forward(self, x):\n        if self.episodic:\n            self.reset()\n\n        for _ in range(self.steps):\n            outputs = forward_and_adapt(x, self.model, self.optimizer)\n\n        return outputs\n\n    def reset(self):\n        if self.model_state is None or self.optimizer_state is None:\n            raise Exception(\"cannot reset without saved model/optimizer state\")\n        load_model_and_optimizer(self.model, self.optimizer,\n                                 self.model_state, self.optimizer_state)\n\n\n@torch.jit.script\ndef softmax_entropy(x: torch.Tensor) -> torch.Tensor:\n    \"\"\"Entropy of softmax distribution from logits.\"\"\"\n    return -(x.softmax(1) * x.log_softmax(1)).sum(1)\n\n\n@torch.enable_grad()  # ensure grads in possible no grad context for testing\ndef forward_and_adapt(x, model, optimizer):\n    \"\"\"Forward and adapt model on batch of data.\n\n    Measure entropy of the model prediction, take gradients, and update params.\n    \"\"\"\n    # forward\n    outputs = model(x)\n    # adapt\n    loss = softmax_entropy(outputs).mean(0)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    return outputs\n\n\ndef collect_params(model):\n    \"\"\"Collect the affine scale + shift parameters from batch norms.\n\n    Walk the model's modules and collect all batch normalization parameters.\n    Return the parameters and their names.\n\n    Note: other choices of parameterization are possible!\n    \"\"\"\n    params = []\n    names = []\n    for nm, m in model.named_modules():\n        if isinstance(m, nn.BatchNorm2d):\n            for np, p in m.named_parameters():\n                if np in ['weight', 'bias']:  # weight is scale, bias is shift\n                    params.append(p)\n                    names.append(f\"{nm}.{np}\")\n    return params, names\n\n\ndef copy_model_and_optimizer(model, optimizer):\n    \"\"\"Copy the model and optimizer states for resetting after adaptation.\"\"\"\n    model_state = deepcopy(model.state_dict())\n    optimizer_state = deepcopy(optimizer.state_dict())\n    return model_state, optimizer_state\n\n\ndef load_model_and_optimizer(model, optimizer, model_state, optimizer_state):\n    \"\"\"Restore the model and optimizer states from copies.\"\"\"\n    model.load_state_dict(model_state, strict=True)\n    optimizer.load_state_dict(optimizer_state)\n\n\ndef configure_model(model):\n    \"\"\"Configure model for use with tent.\"\"\"\n    # train mode, because tent optimizes the model to minimize entropy\n    model.train()\n    # disable grad, to (re-)enable only what tent updates\n    model.requires_grad_(False)\n    # configure norm for tent updates: enable grad + force batch statisics\n    for m in model.modules():\n        if isinstance(m, nn.BatchNorm2d):\n            m.requires_grad_(True)\n            # force use of batch stats in train and eval modes\n            m.track_running_stats = False\n            m.running_mean = None\n            m.running_var = None\n    return model\n\n\ndef check_model(model):\n    \"\"\"Check model for compatability with tent.\"\"\"\n    is_training = model.training\n    assert is_training, \"tent needs train mode: call model.train()\"\n    param_grads = [p.requires_grad for p in model.parameters()]\n    has_any_params = any(param_grads)\n    has_all_params = all(param_grads)\n    assert has_any_params, \"tent needs params to update: \" \\\n                           \"check which require grad\"\n    assert not has_all_params, \"tent should not update all params: \" \\\n                               \"check which require grad\"\n    has_bn = any([isinstance(m, nn.BatchNorm2d) for m in model.modules()])\n    assert has_bn, \"tent needs normalization for its optimization\"",
        "experimental_info": "The method, known as TENT (Test-time Entropy Minimization), adapts a model during testing by minimizing the Shannon entropy of its predictions on unlabeled target data. The adaptation mechanism focuses on modulating features within the model by updating normalization statistics and optimizing channel-wise affine transformation parameters (scale 'weight' and shift 'bias') of existing Batch Normalization (nn.BatchNorm2d) layers. All other model parameters are kept fixed by disabling gradients globally and only re-enabling them for the affine parameters of BatchNorm layers. During the forward pass, normalization statistics (mean and standard deviation) are estimated on each target data batch by setting `track_running_stats` to `False` for BatchNorm layers. The affine parameters are then updated via gradient descent on the entropy loss during the backward pass for a specified number of steps per batch. The default optimizer is Adam with a learning rate of 1e-3 and 1 step per batch. By default, the adaptation is online (`episodic=False`), meaning updates persist across batches, but it can be configured to be episodic where the model state is reset for each new batch. The default batch size for testing and adaptation is 128."
      }
    },
    {
      "title": "Diversify, Contextualize, and Adapt: Efficient Entropy Modeling for Neural Image Codec",
      "abstract": "Designing a fast and effective entropy model is challenging but essential for\npractical application of neural codecs. Beyond spatial autoregressive entropy\nmodels, more efficient backward adaptation-based entropy models have been\nrecently developed. They not only reduce decoding time by using smaller number\nof modeling steps but also maintain or even improve rate--distortion\nperformance by leveraging more diverse contexts for backward adaptation.\nDespite their significant progress, we argue that their performance has been\nlimited by the simple adoption of the design convention for forward adaptation:\nusing only a single type of hyper latent representation, which does not provide\nsufficient contextual information, especially in the first modeling step. In\nthis paper, we propose a simple yet effective entropy modeling framework that\nleverages sufficient contexts for forward adaptation without compromising on\nbit-rate. Specifically, we introduce a strategy of diversifying hyper latent\nrepresentations for forward adaptation, i.e., using two additional types of\ncontexts along with the existing single type of context. In addition, we\npresent a method to effectively use the diverse contexts for contextualizing\nthe current elements to be encoded/decoded. By addressing the limitation of the\nprevious approach, our proposed framework leads to significant performance\nimprovements. Experimental results on popular datasets show that our proposed\nframework consistently improves rate--distortion performance across various\nbit-rate regions, e.g., 3.73% BD-rate gain over the state-of-the-art baseline\non the Kodak dataset.",
      "full_text": "Diversify, Contextualize, and Adapt: Efficient Entropy Modeling for Neural Image Codec Jun-Hyuk Kim∗ Seungeon Kim Won-Hee Lee Dokwan Oh Samsung Advanced Institute of Technology {jh131.kim, se2.kim, why_wh.lee, dokwan.oh}@samsung.com Abstract Designing a fast and effective entropy model is challenging but essential for prac- tical application of neural codecs. Beyond spatial autoregressive entropy models, more efficient backward adaptation-based entropy models have been recently de- veloped. They not only reduce decoding time by using smaller number of modeling steps but also maintain or even improve rate–distortion performance by leveraging more diverse contexts for backward adaptation. Despite their significant progress, we argue that their performance has been limited by the simple adoption of the design convention for forward adaptation: using only a single type of hyper latent representation, which does not provide sufficient contextual information, especially in the first modeling step. In this paper, we propose a simple yet effective entropy modeling framework that leverages sufficient contexts for forward adaptation with- out compromising on bit-rate. Specifically, we introduce a strategy of diversifying hyper latent representations for forward adaptation, i.e., using two additional types of contexts along with the existing single type of context. In addition, we present a method to effectively use the diverse contexts for contextualizing the current elements to be encoded/decoded. By addressing the limitation of the previous approach, our proposed framework leads to significant performance improvements. Experimental results on popular datasets show that our proposed framework con- sistently improves rate–distortion performance across various bit-rate regions, e.g., 3.73% BD-rate gain over the state-of-the-art baseline on the Kodak dataset. 1 Introduction Most neural image codecs [ 8, 9, 11, 15, 17, 18] first transform an image into a quantized latent representation. It is then encoded into a bitstream via an entropy coding algorithm, which relies on a learned probability model known as the entropy model. According to the Shannon’s source coding theorem, the minimum expected length of a bitstream is equal to the entropy of the source. Thus, accurately modeling entropy of the quantized latent representation is crucial. Entropy models estimate a joint probability distribution over the elements of the quantized latent representation. Generally, it is assumed that all elements follow conditionally independent probability distributions. To satisfy this, the probability distributions are modeled in context-adaptive manners, which is key to accurate entropy modeling [18]. Recent methods are based on the joint backward and forward adaptation where the probability distributions adapt by leveraging contexts in two different ways: directly using previously encoded/decoded elements (i.e., backward adaptation), and extracting and utilizing an additional hyper latent representation (i.e., forward adaptation). Here, the type of contexts leveraged can be diverse depending on the spatial range they cover. First, each element has dependencies with other elements in the same spatial location along the channel dimension. Since the channel-wise dependencies correspond to the local image area (e.g., a 16 × 16 patch), we denote ∗Corresponding author 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2411.05832v1  [cs.CV]  6 Nov 2024them as the “local” context. Second, dependencies exist among spatially adjacent elements, and we refer to them as the “regional” context. Lastly, long-range spatial dependencies span the entire image area, referred to as the “global” context. For the backward adaptation, the modeling order, i.e., which elements are modeled first, is an important factor, and the key lies in how effectively we can utilize diverse contexts in the modeling process. Early studies employ spatial autoregressive (AR) models that access regional context including the most spatially adjacent elements. However, they suffer from significantly slow decoding times due to the inevitably large number of modeling steps, which is equal to the spatial dimensions [18]. To enhance efficiency in entropy modeling, several attempts reduce the number of modeling steps while leveraging diverse contexts: a 10-step channel-wise AR model [17], a 2-step spatial non-AR model with a checkerboard pattern [8], and a 4-step non-AR model that operates across spatial and channel dimensions using a quadtree partition [14]. Figure 1: DCA diversifies the hyper latent repre- sentations and contextualizes the current elements by leveraging the diverse hyper latent representa- tions along with the previous elements. As a result, the probability distributionsadapt effectively, lead- ing to accurate entropy modeling. Entropy models based on the efficient backward adaptation methods have led to significant im- provements. However, they are still limited in fully leveraging contexts for forward adaptation. Since they use multiple neural layers with down- sampling and upsampling for modeling hyper latent representation, they can only access the regional context. This limits the performance improvement due to the insufficient contexts (Figure 1a). In particular, this limitation is exac- erbated at the first step where only forward adap- tation is utilized due to the absence of previous elements (Figure 6). Therefore, it is necessary to develop effective forward adaptation in synergy with the efficient backward adaptation. In this paper, we propose a simple yet effec- tive entropy modeling framework, called DCA (Diversify, Contextualize, and Adapt), leverag- ing sufficient contexts for forward adaptation without compromising on bit-rate (Figure 1b). Building on the quadtree partition-based back- ward adaptation [ 14], we introduce a strategy of diversification, i.e., extracting local, regional, and global hyper latent representations unlike only a single regional one in the previous ap- proach. Note that simply using more contexts for forward adaptation does not guarantee performance improvements because forward adaptation requires additional bit allocation unlike backward adap- tation. Then, we propose how to effectively utilize the diverse contexts along with the previously modeled elements for contextualizing the current elements to be encoded/decoded. To consider step- wise different situations, e.g., increased number of previous elements over steps, our contextualization method is designed to utilize each hyper latent representation separately in a step-adaptive manner. Additionally, our contextualization method proceeds in the sequence of regional, global, and local hyper latent representations. Similarly to backward adaptation, we empirically observe that modeling order also matters in forward adaptation. Our main contributions are summarized as follows: • We propose a strategy of diversifying contexts for forward adaptation by extracting three different hyper latent representations, i.e., local, regional, and global ones. This strategy can provide sufficient contexts for forward adaptation without compromising on bit-rate. • We introduce how to effectively leverage the diverse contexts, i.e., previously modeled elements and the three hyper latent representations. We empirically show that the modeling order of three types of contexts affects the performance. • Through the diversification and contextualization methods, our DCA effectively adapts, resulting in significant performance improvements. For example, DCA achieves 3.73% BD-rate gain over the state-of-the-art method [14] on the Kodak dataset. 22 Related work Joint backward and forward adaptation. Ballé et al. [3] propose a scale hyperprior for forward adaptation. A hyper latent representation is extracted and utilized for inferring local scale parameters of the parameterized entropy model. Minnen et al. [18] extend the hyperprior model by using an additional mean hyperprior, and introduce joint backward and forward adaptation by combining the extended hyperprior model with a spatial autoregressive (AR) model. A patch matching-based non-local referring model [20] and a multi-head attention-based global hyperprior [11] are proposed to enrich contexts for backward and forward adaptation, respectively. Efficient backward adaptation. To address the slow decoding times of spatial AR-based entropy models, several studies have proposed group-wise backward adaptation methods. They first divide the quantized latent representation into multiple groups and then process them in a group-wise manner, resulting in improved efficiency. He et al.[8] propose dividing the quantized latent representation into two groups using the checkerboard pattern, which is further improved by incorporating Transformer- based modules [21]. While they apply a group-wise modeling in spatial dimension, Minnen and Singh [17] introduce a channel-wise AR model that divides the quantized latent representation into ten groups along channel dimension. Some studies [22, 23] improve this model by applying Swin Transformer [16]. Based on the channel-wise AR model, He et al. [9] optimize the channel division and combine it with the checkerboard-based model. Recently, Li et al. [14] propose a quadtree partition-based backward adaptation that divides the quantized latent representation into four groups considering both channel and spatial dimensions. In this paper, we propose a novel fast and effective entropy model that achieves better rate–distortion performance by diversifying not only the quantized latent representation but also the hyper latent representations for backward and forward adaptation, respectively. 3 Methods We provide an overview of the proposed methods in Figure 2. The analysis transform fa(·) and the synthesis transform fs(·) (the gray blocks in Figure 2) are learned to find an effective mapping between an input image x and a quantized latent representation ˆy, i.e., ˆy = ⌊fa(x)⌉ and ˆx = fs(ˆy), where ⌊·⌉ is a round operation and ˆx is the decoded image. For the analysis and synthesis transforms, we adopt the same model structure as in the ELIC-sm model [9] due to its efficiency. All other components are learned to model a prior probability distribution on the quantized latent representation ˆy, i.e., the entropy model pˆy. The learned entropy model is utilized in the process of entropy coding, for which we employ the asymmetric numeral systems [6]. Here, our goal is to design a fast and effective learned entropy model. Figure 3: Example of the quadtree partition-based backward adaptation for ˆy ∈ R4×4×320. For sim- plicity, channel dimensions are represented via different colors. ˆyi means the elements to be en- coded/decoded at the i-th step. For modeling the current elements ˆyi, all the previous modeled el- ements ˆy<i are used. For example, the elements corresponding to the red arrow leverage diverse contexts including elements across different chan- nels at the same spatial location (local context de- noted as L) and spatially adjacent four elements of the same channel (regional context denoted as R). Quadtree partition. We build our entropy model on the joint forward and backward adapta- tion where the quadtree partition is used, which is formulated as follows [14]: p(ˆy) = p(ˆz)×p(ˆy|ˆz) = p(ˆz)× 4Y i=1 p(ˆyi|ˆy<i, ˆz) where ˆy is the quantized latent representation, ˆz is the quantized regional hyper latent repre- sentation, ˆyi is the elements to be modeled at the i-th step, and ˆy<i is all the previous mod- eled elements before the i-th step. At each step, one-fourth of the total elements are modeled. The method partitions the quantized latent rep- resentation into four groups along the channel dimension, and then divides each group into non-overlapping 2×2 patches along the spatial dimension. The entropy modeling proceeds over 3Figure 2: Overview of the neural image codec with the proposed entropy model, referred to as DCA. DCA can be employed by any analysis and synthesis transforms fa(·) and fs(·). DCA is an adaptive entropy model consisting of two main stages: diversify (Section 3.1) and contextualize (Section 3.2). First, given the latent representation y, DCA extracts diverse hyper latent representations ˆzl, ˆzr, and ˆzg, and then encodes them into the bitstreams using learned factorized entropy models, which are omitted in this figure for simplicity. Second, contextualization proceeds over four steps. By using the three features ϕl, ϕr, and ϕg (from the three hyper latent representations, respectively) and all the previously encoded/decoded elements before the i-th step, i.e., ˆy<i, DCA contextualizes the current elements to be encoded/decoded, i.e., ˆyi, and finally obtains adaptive distribution parameters µi and σi for probability modeling. Using the learned adaptive probability model, the quantized latent representation ˆy are encoded into a bitstream. four steps, with each step modeling different elements as shown in Figure 3. This quadtree partition- based method uses diverse contexts for backward adaptation, capturing dependencies from both spatial and channel dimensions. Motivation. Recent studies to efficient modeling of backward adaptation have made significant advancements in terms of optimizing the rate–distortion–computation trade-off; however, there is still a gap between their assumptions in the probability modeling and actual data, leaving room for further performance enhancement. The assumptions are as follows: 1) All elements of ˆz are independent; 2) All elements of ˆyi are conditionally independent given ˆy<i and ˆz. Here, the more the actual data deviates from the assumptions, the lower the accuracy of the entropy modeling. At the first modeling step, the elements are modeled conditioned only on the quantized hyper latent representation, i.e., p(ˆy1|ˆz), resulting in a hyperprior model known for deviating from to the second assumption [18]. This can be more problematic because the state-of-the-art methods process a relatively large number of elements at the first step in order to complete the overall modeling with a minimal number of steps. We also empirically show that this problem actually occurs in Figure 6. One straightforward solution is to increase the number of steps so that fewer elements are modeled in the first step. However, this leads to slower modeling speeds, which conflict with the goal of our paper, i.e., developing a fast and effective entropy model. Another simple approach is to provide more quantized regional hyper latent representation ˆz when modeling the quantized latent representation ˆy1. However, paradoxically, this approach can introduce another issue due to the first assumption. Since all elements of hyper latent representation are the same type of information (i.e., regional context), there is a relatively high likelihood of dependencies among the elements. Therefore, to meet both assumptions, the newly added hyper latent representation is required to be independent from the existing regional hyper latent representation. This is why our proposed diversification method using 4local, regional, and global hyper latent representations is needed. In this paper, we propose a fast and effective entropy model, called DCA, which consists of three main stages: diversifying the hyper latent representations, contextualizing the elements targeted for probability modeling, and ultimately adapting the probability distribution of the elements to the given contexts. 3.1 Diversify The proposed DCA aims to diversify the information that the hyper latent representations contain. Specifically, given the latent representation y ∈ RH×W×C, where H, W, and C are the height, width, and the number of channels, respectively, DCA extracts three different types of hyper latent representations depending on the range they cover: a local hyper latent representationˆzl ∈ RH×W×Cl, a regional hyper latent representation ˆzr ∈ R H 4 ×W 4 ×Cr , and a global hyper latent representation ˆzg ∈ RN× C N . The whole process is illustrated in the orange blocks of Figure 2. Local context. To model remaining dependencies along channel dimension at each spatial location, which correspond to a 16 × 16 local patch in the image domain, we introduce local hyper analysis and synthesis transforms, la(·) and ls(·), based on Swin Transformer (SwinT) [16]. The local hyper analysis transform la(·) analyzes local information in the latent representation, followed by the quantization operation to obtain a local hyper latent representation ˆzl. The local synthesis transform ls(·) synthesizes the local features ϕl ∈ RH×W×2C for contextualization from the local hyper latent representation ˆzl. Each transform proceeds in the order of a Patch Split block, a SwinT block, and a Patch Merge block. The Patch Split block serves the function of shifting all channel-wise elements at each spatial location to a 2 × 2 spatial resolution, consisting of the depth-to-space, layer normalization, and linear layers in sequence. The SwinT block then captures dependencies between elements within each non-overlapping window of the input, producing an output of the same size as the input. By setting the window size to 2 × 2 in conjunction with the use of the Split block, we enforce the local hyper transforms to focus only on the local image area. The Patch Merge block performs the opposite function of the Patch Split block, containing the layer normalization, linear, and space-to-depth layers in sequence. Regional context. While the receptive field of the local hyper transforms is limited to the local image area (i.e., 16×16 patches), regional hyper analysis transformra(·) and regional hyper synthesis transform rs(·) model remaining dependencies between elements distributed across a relatively wide image area. The regional hyper analysis transform ra(·) analyzes regional information in the latent representation and yields a regional hyper latent representation ˆzr after quantization. From the extracted regional hyper latent representation ˆzr, the regional synthesis transform rs(·) generates the regional features ϕr ∈ RH×W×2C for contextualization. To do this, we stack multiple layers with the downsampling and upsampling operations for the regional hyper analysis and synthesis transforms, respectively. We adopt the same structure as the previous work [22], which is based on SwinT. Specifically, the regional hyper analysis transform ra(·) conducts a Patch Merge block, five SwinT blocks, a Patch Merge block, and a SwinT block. The regional hyper synthesis transform rs(·) is constructed in the opposite order of the hyper analysis transform ra(·), using the Patch Split block instead of the Patch Merge block. Global context. Lastly, to capture remaining dependencies between elements across the whole image area, we construct global hyper analysis and synthesis transforms ga(·) and gs(·) by adopting model structure of the global hyperprior model of Informer [11]. The global hyper analysis transform ga(·) extracts globally abstracted information from the latent representation using a Transformer block with cross-attention and a 1 × 1 convolutional layer. After quantization, it obtains a global hyper latent representation ˆzg. Using a 1 × 1 convolutional layer, the global synthesis transform gs(·) infers the global features ϕg ∈ RN×2C for contextualization. 3.2 Contextualize Diverse contexts, i.e., previously modeled elements (i.e., ˆy<i) and hyper latent representations (i.e., ˆzl, ˆzr, and ˆzg) can be used for adapting probability distributions. Here, an important research question 5emerges: How can we effectively leverage the diverse contexts? First, to consider step-wise varied situations, e.g., increased previously encoded/decoded elements over modeling steps, we propose a step-adaptive utilization of the three hyper latent representations. In other words, instead of applying a combined set of the three hyper latent representations, each hyper latent representation is adaptively leveraged at each step. In addition, we use regional, global, and local information sequentially. We argue that modeling order is crucial for forward adaptation, which is already known to be a key factor for backward adaptation. The green part of Figure 2 illustrates our proposed contextualization model c(·) at the i-th step. First, the previously modeled ˆy<i and the regional feature ϕr are combined based on the same structure as in the previous approach [14], consisting of concatenation, a 1 × 1 convolutional layer, and three DepthConv Blocks. The DepthConv Block employs depth-wise separable convolutional layers for more efficient implementation. Second, the global feature ϕg is combined with the output of the last DepthConv Block using the Transformer block with cross-attention [11]. Finally, we combine the output of the Transformer block with the local feature ϕl using concatenation followed by three 1 × 1 convolutional layers, yielding the distribution parameters µi and σi. To make our contextualization model c(·) more efficient in terms of the number of parameters, all layers share weights across the four steps except for the initial 1 × 1 convolutional layer. Discussion on modeling order. According to the study on theoretical understanding of masked autoencoder via hierarchical latent variable models, the semantic level of the learned representation varies with the masking ratio [ 12]. Specifically, extremely large or small masking ratios lead to low-level detailed information such as texture, while non-extreme masking ratios result in high- level semantic information. Inspired by this, we can infer that the local and global hyper latent representations correspond to relatively low-level information because they are extracted via limited utilization of the latent representation. The receptive field of the local hyper latent representation is limited by 1×1 convolutional layers. While the receptive field of the global hyper latent representation is whole image area, its attention mechanism selectively use the latent representation. Through the same reasoning, we can infer that regional hyper latent representation corresponds to relatively high-level information. Since different type of contexts has different characteristics, we argue that the modeling order is important for effective entropy modeling. In Figure 11, we empirically confirm that modeling higher-level information (i.e., regional context) first and lower-level information (i.e., local and global contexts) later is more effective than the opposite. In addition, the order between global and local is shown to be not influential. 3.3 Adapt We design an adaptive entropy model on the quantized latent representation ˆy where each element is assumed to follow the Gaussian distribution, and each distribution parameters are obtained from the previous diversification and contextualization stages. Following the previous works [3, 18], we formulate our entropy model as follows: pˆy(ˆy) = Y i \u0010 N \u0000 µi, σ2 i \u0001 ∗ U \u0000 −1 2, 1 2 \u0001\u0011 (ˆyi), (1) where µi and σi are the mean and scale of the Gaussian distribution for each element ˆyi, respectively. The transforms and entropy model are jointly trained in an end-to-end manner by minimizing the expected length of the bitstream (rate) and the expected distortion between the original image and the decoded image, d(·, ·). When a learned entropy model precisely matches the actual probability distribution, the entropy coding algorithm achieves the minimum rate. Therefore, we minimize the cross-entropy between the two distributions. We use mean squared error (MSE) for measuring image distortion. The objective function for our method is as follows: L = Ex∼px \u0002 −log2 pˆy(ˆy) − log2 pˆzl(ˆzl) − log2 pˆzr (ˆzr) − log2 pˆzg (ˆzg) + λ · d(x, ˆx) \u0003 , (2) where px is the distribution of the training dataset, the entropy models pˆzl, pˆzr , and pˆzg are the non-parametric fully factorized entropy models [2], and λ is the Lagrange multiplier that determines weighting between rate and distortion. As the value increases, a model is trained in a direction that reduces information loss, and consequently leads to a higher bit-rate. 60.2 0.4 0.6 0.8 Bit-rate (bpp) 28 30 32 34 36 38PSNR (dB) B (AR) + F (R) (Minnen et al., 2018) B (AR) + F (GL) (Kim et al., 2022) B (ChARM) + F (R) (Minnen & Singh, 2020) B (Quadtree) + F (R) (Li et al., 2023) B (Quadtree) + F (RGL) (Ours, DCA) (a) 0.1 0.2 0.3 0.4 0.5 0.6 Bit-rate (bpp) 30 32 34 36 38PSNR (dB) B (AR) + F (R) (Minnen et al., 2018) B (AR) + F (GL) (Kim et al., 2022) B (ChARM) + F (R) (Minnen & Singh, 2020) B (Quadtree) + F (R) (Li et al., 2023) B (Quadtree) + F (RGL) (Ours, DCA) (b) Figure 4: Performance comparison with latest entropy models on the two benchmark datasets: (a) Kodak and (b) Tecnick. For clear comparisons, we denote each method as follows. B and F mean backward and forward adaptation, respectively, and the corresponding methods are written in parentheses. For backward adaptation, AR, ChARM, and Quadtree represent spatial autoregressive model, channel-wise autoregressive model, and qaudtree partition-based model, respectively. For forward adaptation, L, R, G mean local, regional, and global hyper latent representations, respectively. 4 Experiments We use a PyTorch [19] based open-source library and evaluation platform, CompressAI [4], which has been widely used for developing and evaluating neural image codecs. Training. We set our model parameters as follows: C = 320, Cl = 10, Cr = 192, and N = 8. We train our models corresponding six different bit-rates. We use 300,000 images randomly sampled from the OpenImages [13] dataset. We construct a batch size of 16 with 256×256 patches randomly cropped from different training images. All models are trained for 100 epochs using the Adam optimizer. The learning rate is set to 10−4 up to 90 epoch, and then decreases to 10−5. We use PyTorch v1.9.0, CUDA v11.1, CuDNN v8.0.5, and all experiments are conducted using a single NVIDIA A100 GPU. Evaluation. We evaluate our method on the two popular datasets: Kodak [7] and Tecnick [1]. The Kodak dataset consists of 24 images with a resolution of either 768×512 or 512×768 pixels. The Tecnick dataset is composed of 100 images with a resolution of 1200×1200 pixels. We evaluate our method in terms of rate–distortion performance. For this, we calculate the bits per pixel (bpp) after the encoding phase, and measure distortion between the decoded image and the original image using the peak signal-to-noise ratio (PSNR). 4.1 Comparison with state-of-the-art methods We compare the proposed entropy model, DCA, with state-of-the-art entropy models. DCA can be combined with any transforms; in this paper, DCA is implemented with transforms that have the same structure as in ELIC-sm [9]. For the comparison, we further train image compression methods with four different entropy models [11, 14, 17, 18]. For a fair comparison, they are also implemented with transforms that have the same structure as in ELIC-sm [9]. Rate–Distortion. Figures 4a and 4b show the rate–distortion performance on the Kodak and Tecnick datasets, respectively. The proposed DCA consistently achieves the best rate–distortion performance across all bit-rate regions and two benchmark datasets. Specifically, DCA achieves 11.96% average rate savings over VTM-12.1 on the Kodak dataset, while the second (Lie et al. 2023) [14] and third best (Minnen & Singh, 2020) [17] methods obtain 8.55% and 4.86%, respectively. Complexity. We also evaluate DCA in terms of efficiency. To this end, we provide the de- coding time, the number of model parameters, and Bjøntegaard delta rate (BD-rate) [ 5] in Fig- ure 5. Decoding time is measured on the Kodak dataset using a single NVIDIA V100 GPU. 7Table 1: Performance comparison with state-of-the-art entropy models on Kodak. Methods BD-rate ( %) ↓ Decoding time (ms) ↓ # Parameters (M) ↓ Baseline (CVPR’23) [14] 0.00 67.05 32.64 LIC-TCM (CVPR’23) [15] −0.72 139.04 55.19 MLIC++ (ICMLW’23) [10] 5.76 242.61 107.80 DCA (Ours) −3.73 82.05 37.89 0.00 0.05 0.10 0.15 0.20 12 10 8 6 4 2 0 2 BD-rate (%) 53.1 M 32.6 M 37.9 M 8 9 26.0 M 21.9 M B (AR) + F (R) (Minnen et al., 2018) B (AR) + F (GL) (Kim et al., 2022) B (ChARM) + F (R) (Minnen & Singh, 2020) B (Quadtree) + F (R) (Li et al., 2023) B (Quadtree) + F (RGL) (Ours, DCA) 0.0 0.2 0.4 0.6 0.8 1.0 Decoding time (s) 0.0 0.2 0.4 0.6 0.8 1.0 Figure 5: Performance comparison with latest en- tropy models on the Kodak dataset in terms of decoding time, BD-rate, and model size. Decoding time is measured on a NVIDIA V100 GPU. BD- rate means average rate savings over VTM-12.1. The size of the circle is determined proportionally to the number of model parameters, and the spe- cific numbers are written to the left of the circles. BD-rate means the average bit-rate savings com- pared to a baseline while maintaining the same quality of decoded images. We set VTM-12.1 as the baseline, calculate BD-rate for each im- age in the Kodak dataset, and average them. As shown in Figure 5, our DCA achieves better rate–distortion–computation trade-off than the AR models [11, 18] and the ChARM model [17]. Even compared to the quadtree-based entropy model [14], DCA improves performance signif- icantly, i.e., 3.73% BD-rate gain, without com- promising efficiency as much as possible. Using the same structure of transforms, we ad- ditionally compare DCA with two state-of-the- art entropy models (Table 1), which shows that DCA improves performance most efficiently. It is worth noting that performance improvements are significantly difficult to achieve when there are constraints on compute and memory usage, and this is the achievement of our DCA. Figure 6: Illustration of normalized latent representations ¯yi across four steps using both the baseline and proposed DCA. Each sub-figure includes the minimum and maximum values of normalized latent representations. Notably, the baseline exhibits a broader range of values at the first modeling step, resulting in a higher bit-rate. More examples are provided in the appendix. Probability modeling. To further show the role of the proposed DCA, we measure the normalized latent representation for each step, i.e., ¯yi = (yi −µi)/σi. It provides a standardized measure of how far the latent representation deviates from the predicted mean in terms of estimated standard deviations. Smaller values indicate that a learned entropy model estimates the true probability distribution more accurately. In Figure 6, we compare the result with that of the baseline model [14], which does not leverage diverse contexts for forward adaptation. Here, we have an interesting observation: While the existing work shows significantly high values at the first modeling step, DCA demonstrates consistent modeling performance across four steps. At the first step, since only forward adaptation is possible, the previous approach can utilize only a limited amount of context. On the other hand, DCA enables sufficient contextualization through diverse hyper latent representations, addressing the limitation of previous approach, which has difficulty effectively adapting to various situations. 8Figure 7: Qualitative comparison of the decoded images by the proposed DCA and VTM-12.1. 0.30 0.32 0.34 Bit-rate (bpp) 32.2 32.3 32.4PSNR (dB) 0.64 0.66 0.68 0.70 Bit-rate (bpp) 35.825 35.850 35.875 35.900 35.925PSNR (dB) R R + G R + G + L Large R G + L Figure 8: Analysis of forward context diversity. R, G, L denote the regional, global, and local forward contexts, respectively. “Large R” means using larger amount of the regional context. 0.300 0.302 0.304 0.306 Bit-rate (bpp) 32.37 32.38 32.39 32.40PSNR (dB) 0.884 0.886 0.888 0.890 Bit-rate (bpp) 37.75 37.76 37.77 37.78PSNR (dB) B (LR) + F (LRG) B (LRG) + F (LRG) Figure 9: Analysis of backward context diversity. B and F mean backward and forward contexts, respectively. L, R, G denote the local, regional, and global contexts, respectively. Qualitative results. We provide visual results in Figure 7, showing the decoded image from DCA has better visual quality and a higher PSNR value than that of VTM-12.1, under the same bit-rate. 4.2 Model analysis We conduct detailed analyses of DCA. To do this, we train different variants of DCA depending on various aspects for analysis. All results are shown in two different bit-rate regions (Figures 8 to 12). Analysis of diversification. To validate the effectiveness of diversifying contexts for forward adaptation, we compare three different methods depending on the context diversity (Figure 8): one using regional context (“R”), one using both regional and global contexts (“R + G”), and one using regional, global, and local contexts altogether (“R + G + L”). We use two additional models: one using a larger regional context (“Large R”) and the other using both global and local contexts (“G + L”). The comparison among the first three methods show diversifying forward contexts is effective in both bit-rate regions. Through the results showing that the “Large R” method does not contribute to performance improvement, we once again demonstrate the effectiveness of our diversification. The last one is decomposing regional context into global and local ones rather than diversifying, which is equal to simply adopting the forward adaptation of Informer [11]. The result shows the decomposing approach significantly decreases compression efficiency, and diversifying is more effective. In addition, while our focus lies on diversifying forward context, someone might be curious about whether the context utilized for the quadtree-based backward adaptation is diverse enough. To verify this, we conduct experiments additionally extracting global information from the backward context. Figure 9 demonstrates that there is no distinguishable advantage between two: one simply using the quadtree-based method [14] for backward adaptation (“B (LR) + F (LRG)”) and the other using additional global information for backward adaptation (“B (LRG) + F (LRG)”). This implies that backward adaptation is favorable when focusing on local and regional contexts. Analysis of contextualization. To show the effectiveness of our contextualization approach, we first compare two different methods in Figure 10. “R + G (Step-independent)” combines regional and global information in advance and utilizes the combined one regardless of the step. The other adaptively utilizes regional and global information separately for each step, “R + G (Step-adaptive)”. 90.300 0.305 0.310 0.315 Bit-rate (bpp) 32.30 32.32 32.34 32.36 32.38PSNR (dB) 0.640 0.645 0.650 0.655 0.660 Bit-rate (bpp) 35.850 35.875 35.900 35.925 35.950PSNR (dB) R R + G (Step-independent) R + G (Step-adaptive) Figure 10: Analysis of how to contextualize. R and G denote the regional and global contexts, respectively. “Step-independent” first combines R and G and then utilizes them for all steps, while “Step-adaptive” does not pre-combine them and utilizes each separately for all steps. 0.300 0.305 0.310 0.315 Bit-rate (bpp) 32.30 32.32 32.34 32.36 32.38PSNR (dB) 0.640 0.645 0.650 0.655 0.660 Bit-rate (bpp) 35.850 35.875 35.900 35.925 35.950PSNR (dB) R R G L  R L G  G L R  L G R Figure 11: Analysis of contextualization order. R, G, L denote the regional, global, and local for- ward contexts, respectively.→ means the order. For example. the “R→G→L” method utilizes R, G, and L sequentially. As a reference, we use the model utilizing only regional context, i.e., “R”. The result shows that both are effective and our step-adaptive approach is more beneficial for boosting performance. In addition, we analyze the effectiveness of our modeling order for contextualization in Figure 11. Four different ordering methods and one reference method are used for the comparison. Our ordering approach (“R→G→L”) achieves the best rate–distortion performance, and the methods are catego- rized into two groups based on the performance in both bit-rate regions. Models that prioritize the regional context (“R→G→L” and “R→L→G”) show better performance compared to those that do not prioritize it (“G→L→R” and “L→G→R”). We observe that the method using the opposite mod- eling order of the proposed sequence (“L→G→R”) even exhibits a performance decline compared to the baseline (“R”) in a lower bit-rate region. 0.300 0.305 0.310 0.315 Bit-rate (bpp) 32.30 32.32 32.34 32.36 32.38PSNR (dB) 0.640 0.645 0.650 0.655 0.660 Bit-rate (bpp) 35.850 35.875 35.900 35.925 35.950PSNR (dB) B (CNN) + F (CNN) B (CNN) + F (Attention) B (Attention) + F (Attention) Figure 12: Analysis of model architecture for backward and forward adaptation. B and F mean backward and forward adaptation, respectively. Analysis of architecture. Applying attention mechanisms on various tasks is one of the most actively researched topics. We analyze the effect of the architecture combination for forward and backward adaptation in DCA. Figure 12 compares three different methods: one without attention (“B (CNN) + F (CNN)”), another applying attention only to forward adaptation (“B (CNN) + F (Attnen- tion)”), and the last applying attention for both (“B (Attention) + F (Attention)”). The results show that CNN and attention are effective for forward and backward adaptation, respectively. We infer that focusing on local and regional information is preferable in backward adaptation; thus, a CNN with a locality inductive bias may be more effective. 5 Conclusion In this paper, we proposed a fast and effective entropy modeling framework, DCA, which diversifies forward contexts by extracting local, regional, and global information, and contextualizes current elements with the diverse forward and backward contexts. We demonstrated that our DCA improves rate–distortion performance significantly compared to previous approach without compromising efficiency as much as possible. Furthermore, we provided diverse insights into entropy modeling by conducting a comprehensive and in-depth analysis of the design aspects of DCA. Limitation and future works. To address the limitation of the state-of-the-art entropy models, we focused on paving a novel framework with diverse contexts rather than designing neural architec- tures. Therefore, DCA can be limited by the architectural designs that are inspired by the existing works [11, 14, 22]. In the future, we expect that it would be further improved by neural architectures especially designed for the diverse contexts. In addition, it is worth exploring alternative criteria for diversification beyond the spatial range the contexts covers (i.e., local, regional, and global contexts). 10References [1] N. Asuni and A. Giachetti. TESTIMAGES: a large-scale archive for testing visual devices and basic image processing algorithms. In Proceedings of the Smart Tools and Apps for Graphics (STAG), 2014. [2] J. Ballé, V . Laparra, and E. P. Simoncelli. End-to-end optimized image compression. InProceedings of the International Conference on Learning Representations (ICLR), 2017. [3] J. Ballé, D. Minnen, S. Singh, S. J. Hwang, and N. Johnston. Variational image compression with a scale hyperprior. In Proceedings of the International Conference on Learning Representations (ICLR), 2018. [4] J. Bégaint, F. Racapé, S. Feltman, and A. Pushparaja. CompressAI: a PyTorch library and evaluation platform for end-to-end compression research. arXiv preprint arXiv:2011.03029, 2020. [5] G. Bjøntegaard. Calculation of average PSNR differences between RD-curves. VCEG-M33, 2001. [6] J. Duda. Asymmetric numeral systems: entropy coding combining speed of Huffman coding with compres- sion rate of arithmetic coding. arXiv preprint arXiv:1311.2540, 2013. [7] R. Franzen. Kodak lossless true color image suite. http://r0k.us/graphics/kodak/, 1999. [8] D. He, Y . Zheng, B. Sun, Y . Wang, and H. Qin. Checkerboard context model for efficient learned image compression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. [9] D. He, Z. Yang, W. Peng, R. Ma, H. Qin, and Y . Wang. ELIC: Efficient learned image compression with unevenly grouped space-channel contextual adaptive coding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [10] W. Jiang and R. Wang. MLIC++: Linear complexity multi-reference entropy modeling for learned image compression. In Proceedings of the International Conference on Machine Learning (ICML) Workshop, 2023. [11] J.-H. Kim, B. Heo, and J.-S. Lee. Joint global and local hierarchical priors for learned image compression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [12] L. Kong, M. Q. Ma, G. Chen, E. P. Xing, Y . Chi, L.-P. Morency, and K. Zhang. Understanding masked autoencoders via hierarchical latent variable models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [13] I. Krasin, T. Duerig, N. Alldrin, V . Ferrari, S. Abu-El-Haija, A. Kuznetsova, H. Rom, J. Uijlings, S. Popov, A. Veit, S. Belongie, V . Gomes, A. Gupta, C. Sun, G. Chechik, D. Cai, Z. Feng, D. Narayanan, and K. Murphy. OpenImages: A public dataset for large-scale multi-label and multi-class image classification. Dataset available from https://github.com/openimages, 2017. [14] J. Li, B. Li, and Y . Lu. Neural video compression with diverse contexts. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [15] J. Liu, H. Sun, and J. Katto. Learned image compression with mixed Transformer-CNN architectures. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [16] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and B. Guo. Swin Transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. [17] D. Minnen and S. Singh. Channel-wise autoregressive entropy models for learned image compression. In Proceedings of the IEEE International Conference on Image Processing (ICIP), 2020. [18] D. Minnen, J. Ballé, and G. Toderici. Joint autoregressive and hierarchical priors for learned image compression. In Advances in Neural Information Processing Systems (NeurIPS), 2018. [19] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. PyTorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems (NeurIPS), 2019. [20] Y . Qian, Z. Tan, X. Sun, M. Lin, D. Li, Z. Sun, L. Hao, and R. Jin. Learning accurate entropy model with global reference for image compression. In Proceedings of the International Conference on Learning Representations (ICLR), 2021. 11[21] Y . Qian, X. Sun, M. Lin, Z. Tan, and R. Jin. Entroformer: A transformer-based entropy model for learned image compression. In Proceedings of the International Conference on Learning Representations (ICLR), 2022. [22] Y . Zhu, Y . Yang, and T. Cohen. Transformer-based transform coding. InProceedings of the International Conference on Learning Representations (ICLR), 2022. [23] R. Zou, C. Song, and Z. Zhang. The devil is in the details: Window-based attention for image compression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 12A Additional results We provide the normalized latent representations of images from the Kodak dataset in Figure 13. Figure 13: Illustrations of normalized latent representations ¯yi across four steps for Kodak images using both the baseline [14] and proposed DCA. Each sub-figure includes the minimum and maximum values of normalized latent representations. Notably, the baseline exhibits a broader range of values at the first modeling step, resulting in a higher bit-rate. We provide an in-depth runtime analysis by sub-systems in Table 2. Table 2: Runtime (ms) of DCA. Total encoding/decoding time includes the ANS entropy coding. Transforms Entropy model (DCA) Total fa fs la ls ra rs ga gs c Encoding Decoding 3.46 1.54 0.98 0.96 5.32 4.78 0.63 0.08 14.75 117.49 82.05 13",
      "meta_data": {
        "arxiv_id": "2411.05832v1",
        "authors": [
          "Jun-Hyuk Kim",
          "Seungeon Kim",
          "Won-Hee Lee",
          "Dokwan Oh"
        ],
        "published_date": "2024-11-06T04:30:04Z",
        "pdf_url": "https://arxiv.org/pdf/2411.05832v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper proposes DCA (Diversify, Contextualize, and Adapt), an efficient entropy modeling framework for neural image codecs. The main problem addressed is the limitation of previous backward adaptation-based entropy models, which use a single type of hyper latent representation, providing insufficient contextual information, especially in the first modeling step. DCA introduces a strategy of diversifying hyper latent representations for forward adaptation by using three types of contexts: local, regional, and global. It also presents a method to effectively utilize these diverse contexts along with previously modeled elements for contextualizing current elements. This approach significantly improves rate-distortion performance, achieving a 3.73% BD-rate gain over the state-of-the-art baseline on the Kodak dataset without compromising efficiency.",
        "methodology": "DCA builds on joint forward and backward adaptation using quadtree partition. It comprises three main stages: Diversify, Contextualize, and Adapt. Diversify involves extracting three types of hyper latent representations from the latent representation (y): local (ˆzl), regional (ˆzr), and global (ˆzg). Local hyper transforms (la, ls) are based on Swin Transformer (SwinT) with a 2x2 window size. Regional hyper transforms (ra, rs) also use SwinT, stacking multiple layers with downsampling and upsampling. Global hyper transforms (ga, gs) adopt the model structure of Informer's global hyperprior. Contextualize leverages previously modeled elements (ˆy<i) and the three hyper latent representations (ˆzl, ˆzr, ˆzg) in a step-adaptive manner. The modeling order is crucial, empirically shown to be regional (ϕr) first, then global (ϕg), then local (ϕl). This involves combining contexts using concatenation, 1x1 convolutional layers, DepthConv Blocks, and a Transformer block with cross-attention. Layers share weights across the four modeling steps, except for the initial 1x1 convolutional layer. Adapt designs an adaptive entropy model where each element of the quantized latent representation (ˆy) follows a Gaussian distribution. The transforms and entropy model are jointly trained end-to-end by minimizing a loss function combining cross-entropy for bitstream length and mean squared error (MSE) for image distortion, using asymmetric numeral systems for entropy coding.",
        "experimental_setup": "The experiments use a PyTorch-based open-source library and evaluation platform, CompressAI. Model parameters are set as C=320, Cl=10, Cr=192, and N=8. Models are trained for six different bit-rates using 300,000 randomly sampled images from the OpenImages dataset, with a batch size of 16 and 256x256 random patches. Training lasts 100 epochs using the Adam optimizer, with a learning rate of 10^-4 (decreasing to 10^-5 after 90 epochs). All experiments are conducted using a single NVIDIA A100 GPU for training. Evaluation is performed on two popular datasets: Kodak (24 images, 768x512 or 512x768 pixels) and Tecnick (100 images, 1200x1200 pixels). Performance is measured in terms of rate-distortion (bits per pixel (bpp) and peak signal-to-noise ratio (PSNR)), decoding time (on a single NVIDIA V100 GPU), number of model parameters, and Bjøntegaard delta rate (BD-rate) against VTM-12.1 as a baseline. For fair comparison, DCA and state-of-the-art entropy models [11, 14, 17, 18] are implemented with transforms structured like ELIC-sm [9].",
        "limitations": "The proposed DCA framework's architectural designs are inspired by existing works [11, 14, 22]. The paper focused on establishing a novel framework with diverse contexts rather than designing entirely new neural architectures, which may limit its potential performance. The current diversification strategy is primarily based on the spatial range that contexts cover (local, regional, and global).",
        "future_research_directions": "Future research could explore further improvements by designing neural architectures specifically optimized for diverse contexts, rather than relying on existing designs. Additionally, it is worth investigating alternative criteria for diversification beyond the spatial range of contexts (i.e., local, regional, and global contexts) to potentially unlock new performance gains."
      }
    },
    {
      "title": "Entropy Minimization In Emergent Languages",
      "abstract": "There is growing interest in studying the languages that emerge when neural\nagents are jointly trained to solve tasks requiring communication through a\ndiscrete channel. We investigate here the information-theoretic complexity of\nsuch languages, focusing on the basic two-agent, one-exchange setup. We find\nthat, under common training procedures, the emergent languages are subject to\nan entropy minimization pressure that has also been detected in human language,\nwhereby the mutual information between the communicating agent's inputs and the\nmessages is minimized, within the range afforded by the need for successful\ncommunication. That is, emergent languages are (nearly) as simple as the task\nthey are developed for allow them to be. This pressure is amplified as we\nincrease communication channel discreteness. Further, we observe that stronger\ndiscrete-channel-driven entropy minimization leads to representations with\nincreased robustness to overfitting and adversarial attacks. We conclude by\ndiscussing the implications of our findings for the study of natural and\nartificial communication systems.",
      "full_text": "Entropy Minimization In Emergent Languages Eugene Kharitonov 1 Rahma Chaabouni 1 2 Diane Bouchacourt 1 Marco Baroni 1 3 Abstract There is growing interest in studying the lan- guages that emerge when neural agents are jointly trained to solve tasks requiring communication through a discrete channel. We investigate here the information-theoretic complexity of such lan- guages, focusing on the basic two-agent, one- exchange setup. We ﬁnd that, under common training procedures, the emergent languages are subject to an entropy minimization pressure that has also been detected in human language, whereby the mutual information between the com- municating agent’s inputs and the messages is minimized, within the range afforded by the need for successful communication. That is, emer- gent languages are (nearly) as simple as the task they are developed for allow them to be. This pressure is ampliﬁed as we increase communica- tion channel discreteness. Further, we observe that stronger discrete-channel-driven entropy min- imization leads to representations with increased robustness to overﬁtting and adversarial attacks. We conclude by discussing the implications of our ﬁndings for the study of natural and artiﬁcial communication systems. 1. Introduction There has recently been much interest in the analysis of the communication systems arising when deep network agents that interact to accomplish a goal are allowed to exchange language-like discrete messages (Lazaridou et al., 2016; Havrylov & Titov, 2017; Choi et al., 2018; Lazaridou et al., 2018; Li & Bowling, 2019; Chaabouni et al., 2020). Under- standing the emergent protocol is important if we want to eventually develop agents capable of interacting with each other and with us through language (Mikolov et al., 2016; 1Facebook AI Research, Paris, France 2Cognitive Machine Learning (ENS - EHESS - PSL - CNRS - INRIA) 3Catalan Insti- tute for Research and Advanced Studies, Barcelona, Spain. Corre- spondence to: Eugene Kharitonov <kharitonov@fb.com>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). Chevalier-Boisvert et al., 2019). The pursuit might also provide comparative evidence about how core properties of human language have evolved (Kirby, 2002; Hurford, 2014; Harding Graesser et al., 2019). While earlier stud- ies reported ways in which deep agent protocols radically depart from human language (Kottur et al., 2017; Boucha- court & Baroni, 2018; Chaabouni et al., 2019; Lowe et al., 2019), we show here that emergent communication shares an important property of the latter, namely a tendency to- wards entropy minimization. Converging evidence shows that efﬁciency pressures are at work in language and other biological communication systems (Ferrer i Cancho et al., 2013; Gibson et al., 2019). One particular aspect of communicative efﬁciency, robustly observed across many semantic domains, is the tendency to minimize lexicon entropy, to the extent allowed by the coun- teracting need for accuracy (Zaslavsky et al., 2018; 2019). For example, while most languages distinguish grandmoth- ers from grandfathers, few have separate words for mother- and father-side grandmothers, as the latter distinction makes communication only slightly more accurate at the cost of an increase in lexicon complexity (Kemp & Regier, 2012). We show here, in two separate games designed to precisely mea- sure such property, that the protocol evolved by interacting deep agents is subject to the same complexity minimiza- tion pressure. Entropy minimization in natural language has been con- nected to the Information Bottleneck principle (Tishby et al., 1999). In turn, complexity reduction due to the Information Bottleneck provides a beneﬁcial regularization effect on learned representations (Fischer, 2019; Alemi et al., 2016; Achille & Soatto, 2018a;b). It is difﬁcult to experimentally verify the presence of such effect in human language, but we can look for it in our computational simulations. We conﬁrm that, when relaxing channel discreteness, the en- tropy minimization property no longer holds, and the system becomes less robust against overﬁtting and adversarial noise. This in turn raises intriguing questions about the origin of discreteness in human language, that we return to in the conclusion. arXiv:1905.13687v3  [cs.CL]  26 Jun 2020Entropy Minimization In Emergent Languages 2. General framework We establish our results in the context of signaling games (Lewis, 1969), as introduced to the current language emergence literature by Lazaridou et al. (2016) and adopted in several later studies (Havrylov & Titov, 2017; Boucha- court & Baroni, 2018; Lazaridou et al., 2018). There are two agents, Sender and Receiver, provided with individual in- puts at the beginning of each episode. Sender sends a single message to Receiver, and Receiver has to perform an action based on its own input and the received message. Impor- tantly, there is no direct supervision on the message protocol. We consider agents that are deterministic functions of their inputs (after training). As an example, consider the task of communicating a n-bit number, sampled uniformly at random from 0...2n −1. The full number is shown to Sender, and its k (0 ≤k ≤n) least-signiﬁcant bits are also revealed to Receiver. Receiver has to output the full number, based on the message from Sender and its own input. Would Sender transmit the en- tire number through its message? In this case, the protocol would be “complex,” encodingnbits. Alternatively, Sender could only encode the bits that Receiver does not know, and let Receiver ﬁll in the rest by itself. This emergent protocol would be “simple,” encoding only strictly necessary infor- mation. We ﬁnd experimentally that, once the agents are successfully trained to jointly solve the task, the emergent protocol minimizes the entropy of the messages or, equiva- lently in our setup, the mutual information between Sender’s input and messages. In other words, the agents consistently approximate the simplest successful protocol (in the current example, the one transmitting ≈n−kbits). We can connect the entropies of Sender and Receiver in- puts is and ir, messages m, Receiver’s output (the chosen action) o, and ground-truth outputs lby standard inequali- ties (Cover & Thomas, 2012).1 Denoting Sender’s computa- tion as a function S : S(is) =m, and Receiver as function R: R(m,ir) =o, we obtain: H(is) ≥H(S(is)) =H(m) ≥H(m|ir) ≥ ≥H(R(m,ir)|ir) =H(o|ir) ≈H(l|ir), (1) where the last relation stems from the fact that after success- ful training o≈l. Note that, since agents are deterministic after training, H(m) = I(is; m). We can then use these quantities interchangeably. Our empirical measurements indicate that the entropy of the messages min the emergent protocol tends to approach the lower bound: H(m) →H(l|ir), even if the upper bound H(is) is far. that Receiver needs is reduced without chang- ing other parameters, the emergent protocol becomes sim- 1We also use the fact that that H(x) ≥ H(g(x)) for any dis- crete r.v. xand function g. pler (lower entropy). In other words, the emergent protocol adapts to minimize the information that passes through it. Code for our experiments is publicly available at github.com/facebookresearch/EGG/ as a part of the EGG framework (Kharitonov et al., 2019). 3. Methodology 3.1. Games We study two signaling games. In Guess Number, the agents are trained to recover an integer-representing vector with uniform Bernoulli-distributed components. This simple setup gives us full control over the amount of information needed to solve the task. The second game, Image Classi- ﬁcation, employs more naturalistic data, as the agents are jointly trained to classify pairs of MNIST digits (LeCun et al., 1998b). Guess Number We draw an 8-bit integer 0 ≤z ≤255 uniformly at random, by sampling its 8 bits independently from the uniform Bernoulli distribution. All bits are revealed to Sender as an 8-dimensional binary vector is. The last k bits are revealed to Receiver ( 0 ≤k ≤8) as its input ir. Sender outputs a single-symbol message mto Receiver. In turn, Receiver outputs a vector othat recovers all the bits of zand should be equal to is. In this game, Sender has a linear layer that maps the input vector is to a hidden representation of size 10, followed by a leaky ReLU activation. Next is a linear layer followed by a softmax over the vocabulary. Receiver linearly maps both its input ir and the message to 10-dimensional vectors, concatenates them, applies a fully connected layer with output size 20, followed by a leaky ReLU. Finally, another linear layer and a sigmoid nonlinearity are applied. When training with REINFORCE and the Stochastic Computation graph approach (see Sec. 3.2), we increase the hidden layer sizes threefold, as this leads to a more robust convergence. Image Classiﬁcation In this game, the agents are jointly trained to classify 28x56 images of two MNIST digits, stacked side-by-side (more details in Supplementary). Un- like Guess Number, Receiver has no side input. Instead, we control the informational complexity of Receiver’s task by controlling the size of its output space, i.e., the number of labels we assign to the images. To do so, we group all two- digit sequences 00..99 into Nl ∈{2,4,10,20,25,50,100} equally-sized classes. In Sender, input images are embedded by a LeNet-1 in- stance (LeCun et al., 1990) into 400-dimensional vectors. These embedded vectors are passed to a fully connected layer, followed by a softmax selecting a vocabulary sym- bol. Receiver embeds the received messages into 400- dimensional vectors, passed to a fully connected layer withEntropy Minimization In Emergent Languages a softmax activation returning the class probabilities. We report hyperparameter grids in Supplementary. In the following experiments, we ﬁx vocabulary to 1024 symbols (experiments with other vocabulary sizes, multi-symbol mes- sages, and larger architectures are reported in Supplemen- tary). No parts of the agents are pre-trained or shared. The loss being optimized depends on the chosen gradient estima- tion method (see Sec. 3.2). We denote it L(o,l), and it is a function of Receiver’s outputoand the ground-truth output l. When training in Guess Number with REINFORCE, we use a 0/1 loss: the agents get zero loss only when all bits of zare correctly recovered. When training with Gumbel- Softmax relaxation or the Stochastic Computation Graph approach, we use binary cross-entropy (Guess Number) and negative log-likelihood (Image Classiﬁcation). 3.2. Training with discrete channel Training to communicate with discrete messages is non- trivial, as we cannot back-propagate through the messages. Current language emergence work mostly uses Gumbel- Softmax relaxation (e.g., Havrylov & Titov, 2017) or RE- INFORCE (e.g., Lazaridou et al., 2016) to get gradient esti- mates. We also explore the Stochastic Computation Graph optimization approach. We plug the obtained gradient esti- mates into Adam (Kingma & Ba, 2014). Gumbel-Softmax relaxation Samples from the Gumbel- Softmax distribution (a) are reperameterizable, hence allow gradient-based training, and (b) approximate samples from the corresponding Categorical distribution (Maddison et al., 2016; Jang et al., 2016). To get a sample that approximates an n-dimensional Categorical distribution with probabilities pi, we draw ni.i.d. samples gi from Gumbel(0,1) and use them to calculate a vector ywith components: yi = exp[(gi + logpi)/τ]∑ j exp[(gj + logpj)/τ], (2) where τ is the temperature hyperparameter. As τ tends to 0, the samples yget closer to one-hot samples; as τ →+∞, the components yi become uniform. During training, we use these relaxed samples as messages from Sender, making the entire Sender/Receiver setup differentiable. REINFORCE by Williams (1992) is a standard reinforce- ment learning algorithm. In our setup, it estimates the gradi- ent of the expectation of the loss L(o,l) w.r.t. the parameter vector θas follows: Eis,ir Em∼S(is),o∼R(m,ir) [(L(o; l) −b)∇θlog Pθ(m,o)] (3) The expectations are estimated by sampling mfrom Sender and, after that, sampling ofrom Receiver. We use the run- ning mean baseline b(Greensmith et al., 2004; Williams, 1992) as a control variate. We adopt the common trick to add an entropy regularization term (Williams & Peng, 1991; Mnih et al., 2016) that favors higher entropy. We impose entropy regularization on the outputs of the agents with coefﬁcients λs (Sender) and λr (Receiver). Stochastic Computation Graph (SCG) In our setup, the gradient estimate approach of Schulman et al. (2015) re- duces to computing the gradient of the surrogate function: Eis,ir Em∼S(is) [L(o; l) +sg(L(o; l) −b) logPθ(m)] , (4) where sgdenotes stop-gradient operation. We do not sample Receiver actions: Its parameter gradients are obtained with standard backpropagation (ﬁrst term in Eq. 4). Sender’s messages are sampled, and its gradient is calculated akin to REINFORCE (second term in Eq. 4). Again, we apply entropy-favoring regularization on Sender’s output (with coefﬁcient λs) and use the mean baseline. Role of entropy regularization As we mentioned above, when training with REINFORCE and SCG, we include a (standard) entropy regularization term in the loss which explicitly maximizes entropy of Sender’s output. Clearly, this term is at odds with the entropy minimization effect we observe. In our experiments, we found that high values ofλs (the parameter controlling Sender’s entropy regularization) prevent communication success; on the other hand, a small non-zero λs is crucial for successful training. In Sec. 4 we investigate the effect of λs on entropy minimization.2 3.3. Experimental protocol In Guess Number, we use all 28 possible inputs for train- ing, early stopping and analysis. In Image Classiﬁcation, we train on random image pairs from the MNIST training data, and use image pairs from the MNIST held-out set for validation. We select the runs that achieved a high level of performance (training accuracy above 0.99 for Guess Number and validation accuracy above 0.98 for Image Clas- siﬁcation), thus studying typical agent behavior provided they succeeded at the game. At test time, we select the Sender’s message symbol greed- ily, hence the messages are discrete and Sender represents a (deterministic) function S of its input is, m = S(i). Calculating the entropy H(m) of the distribution of dis- crete messages mis straightforward. In Guess Number, we enumerate all 256 possible values of zas inputs, obtain messages and calculate entropy H(m). For Image Classiﬁ- cation, we sample image pairs from the held-out set. The upper bound on H(m) is as follow: Hmax = 8bits (bounded by H(is)) in Guess Number, and Hmax = 10 2The parameter λr, that controls Receiver’s entropy regulariza- tion, does not inﬂuence the observed effect.Entropy Minimization In Emergent Languages bits (bounded by vocabulary size) in Image Classiﬁcation. Its lower bound is equal to Hmin = H(l|ir) = 8−kbits for Guess number. In Image Classiﬁcation, communica- tion can only succeed if H(m) is not less than H(l), i.e., Hmin = H(l) = log2 Nl, with Nl the number of equally- sized classes we split the images into. 4. Experiments 4.1. Entropy minimization Guess Number In Figure 1, the horizontal axes span the number of bits of z that Receiver lacks, 8 −k. The ver- tical axis reports the information content of the protocol, measured by messages entropy H(m). Each integer on the horizontal axis corresponds to a game conﬁguration, and for each such conﬁguration we aggregate multiple (suc- cessful) runs with different hyperparameters and random seeds. Hmin indicates the minimal amount of bits Sender has to send in a particular conﬁguration for the task to be solvable. The upper bound (not shown) is Hmax = 8bits. Across hyperparameters and random seeds, trainings with Gumbel-Softmax and SCG have success rate above 50%. With REINFORCE success rate is approximately 20%. Consider ﬁrst the conﬁgurations where Receiver’s input is insufﬁcient to answer correctly (at least one binary digit hidden, k≤7). From Figure 1a, we observe that the trans- mitted information is strictly monotonically increasing with the number of binary digits hidden from Receiver. Thus, even if Sender sees the very same input in all conﬁgura- tions, a more nuanced protocol is only developed when it is necessary. Moreover, the entropy H(m) (equivalently: the transmitted information) stays close to the lower bound. This entropy minimization property holds for all the consid- ered training approaches across all conﬁgurations. Consider next the conﬁguration where Receiver is getting the whole integer zas its input (k= 8, the leftmost conﬁgu- ration in Figure 1, corresponding to 0 on x axis). Based on the observations above, one would expect that the protocol would approach zero entropy in this case (as no informa- tion needs to be transmitted). However, the measurements indicate that the protocol is encoding considerably more information. It turns out that this information is entirely ignored by Receiver. To demonstrate this, we fed all pos- sible distinct inputs to Sender, retrieved the corresponding messages, and shufﬂed them to destroy any information about the inputs they might carry. The shufﬂed messages were then passed to Receiver alongside its own (un-shufﬂed) inputs. The overall performance was not affected by this manipulation, conﬁrming the hypothesis that Receiver ig- nores the messages. We conclude that in this case there is no entropy minimization pressure on Sender simply be- cause there is no communication. The full experiment is in Supplementary. We further consider the effect of various hyperparameters. In Figure 1b, we split the results obtained with Gumbel- Softmax by relaxation temperature. As discussed in Sec. 3.2, lower temperatures more closely approximate discrete com- munication, hence providing a convenient control of the level of discreteness imposed during training (recall that at test time we enforce full discreteness by selecting the symbol greedily). The ﬁgure shows that lower tempera- tures consistently lead to lower H(m). This implies that, as we increase the “level of discreteness” at training, we get stronger entropy minimization pressure. In Figures 1c & 1d, we report H(m) when training with Stochastic Graph Optimization and REINFORCE across degrees of entropy regularization. We report curves corre- sponding to λs values which converged in more than three conﬁgurations. With REINFORCE, we see a weak tendency for a higher λs to trigger a higher entropy in the protocol. However, message entropy stays generally close to the lower bound even in presence of strong exploration, which favors higher entropy in Sender’s output distribution. Image Classiﬁcation As the models are more complex, we only had consistent success when training with Gumbel- Softmax (success rate is approximately 80%). In Figure 2a we aggregate all successful runs. The information encoded by the protocol grows as Receiver’s output requires more information. However, in all conﬁgurations, the transmit- ted information stays well below the 10-bit upper bound and tends to be close to Hmin. A natural interpretation is that Sender prefers to take charge of image classiﬁcation and directly pass information about the output label, rather than sending along a presumably more information-heavy description of the input. In Figure 2b, we split the runs by temperature. Again, we see that lower temperatures consis- tently lead to stronger entropy minimization pressures. Summarizing, when communicating through a discrete chan- nel, there is consistent pressure for the emergent protocol to encode as little information as necessary. This holds across games, training methods and hyperparameters. When train- ing with Gumbel-Softmax, temperature controls the strength of this pressure, conﬁrming the relation between entropy minimization and discreteness. 4.2. Evolution of message entropy during training To gain further insights into the minimization trend, we studied the evolution of message entropy during training. We observed that the initial entropy of Sender can be both higher and lower than the minimum entropy Hmin required for solving the task. Further, we measured how the en- tropy of the messages changes after each training epoch by applying the same procedure as above, i.e., feeding theEntropy Minimization In Emergent Languages 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits Gumbel-Softmax Stoch. computation REINFORCE Hmin (a) All three training approaches. 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits  = 1.0  = 1.25  = 1.5 Hmin (b) Training with Gumbel-Softmax relaxation. 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits s = 0.01 s = 0.025 s = 0.05 s = 0.075 s = 0.1 Hmin (c) Training with Stochastic Computation Graph. 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits s = 0.01 s = 0.025 s = 0.05 s = 0.075 s = 0.1 s = 0.5 Hmin (d) Training with REINFORCE. Figure 1.Guess Number: entropy of the messages m. Shaded regions represent one standard error of the mean (SEM). entire dataset to Sender and selecting the message symbol greedily. When message entropy starts higher than Hmin, it falls close to it during the training. Similarly, when it starts lower than Hmin, it increases during training. This experiment is reported in Supplementary. Thus, information minimization is not simply due to the difﬁculty of discov- ering a higher-entropy protocol during learning, but also due to the complexity of maintaining mutual coordination between the agents. 4.3. Representation discreteness and robustness The entropy minimization effect indicates that a discrete rep- resentation will only store as much information as necessary to solve the task. This emergent behavior resembles the In- formation Bottleneck principle (Tishby et al., 1999; Achille & Soatto, 2018a). The fact that lower training-time temper- atures in Gumbel-Softmax optimization correlate with both higher discreteness and a tighter bottleneck (see Sec. 3.3) makes us further conjecture that discreteness is causally connected to the emergent bottleneck. The Information Bottleneck principle has also been claimed to govern en- tropy minimization in natural language (Zaslavsky et al., 2018; 2019). Bottleneck effects in neural agents and natural language might be due to the same cause, namely communi- cation discreteness. Further, we hypothesize that the emergent discrete bottle- neck might have useful properties, since existing (continu- ous) architectures that explicitly impose a bottleneck pres- sure are more robust to overﬁtting (Fischer, 2019) and ad- versarial attacks (Alemi et al., 2016; Fischer, 2019). We test whether similar regularization properties also emerge in our computational simulations (without any explicit pressure imposed through the cost function), and whether they are correlated with communication channel discreteness. If this connection exists, it also suggests that discreteness might be “beneﬁcial” to human languages for the same reasons. 4.3.1. R OBUSTNESS TO OVER -FITTING To assess our hypotheses, we consider the Image Classiﬁ- cation game (Nl = 10) in presence of randomly-shufﬂed training labels (the test set is untouched) (Zhang et al., 2016). This task allows us to explore whether the discrete commu- nication bottleneck is associated to robustness to overﬁtting, and whether the latter depends on discreteness level (con- trolled by the temperature τ of Gumbel-Softmax). We use the same architecture as above. The agents are trained withEntropy Minimization In Emergent Languages 1 2 3 4 5 6 log2Nl 1 2 3 4 5 6 7H(m), bits All runs Hmin (a) Successful runs pooled together. 1 2 3 4 5 6 log2Nl 1 2 3 4 5 6 7H(m), bits =0.75 =1.0 =1.5 =2.0 Hmin (b) Successful runs grouped by temperature. Figure 2.Image Classiﬁcation: entropy of the messages min function of log number of target classes, Nl. Shaded regions mark SEM. Gumbel-Softmax relaxation; at test-time the communication is fully discrete. We also consider two baseline architectures without the discrete channel. In Linear, the fully connected output layer of Sender is directly connected to the linear embedding input of Receiver. Softmax (SM) places a softmax activation (with temperature) after Sender’s output layer and passes the result to Receiver. We vary temperature and proportion of training examples with shufﬂed labels. We use temperatures τ = 1.0 and τ = 10.0 (the agents reach a test accuracy of 0.98 when trained with these temperatures on the original training set). SM with τ = 1.0 and τ = 10.0 behave similarly, hence we only report SM with τ = 1.0. Figure 3a shows training accuracy when all labels are shuf- ﬂed. Linear and SM ﬁt the random labels almost perfectly within the ﬁrst 150 epochs. With τ = 10.0, GS achieves 0.8 accuracy within 200 epochs. When GS with τ = 1.0 is considered, the agents only start to improve over random guessing after 150 epochs, and accuracy is well below 0.2 after 200 epochs. As expected, test set performance is at chance level (Figure 3b). In the next experiment, we shufﬂe labels for a randomly selected half of the training instances. Train and test accuracies are shown in Figures 3c and 3d, respectively. All models initially ﬁt the true-label exam- ples (train accuracy ≈0.5, test accuracy ≈0.97). With more training, the baselines and GS with τ = 10.0 start (over)ﬁtting the random labels, too: train accuracy grows, while test accuracy falls. In contrast, GS with τ = 1.0 does not ﬁt random labels, and its test accuracy stays high. Note that SM patterns with Linear and high-temperature GS, showing that the training-time discretization noise in GS is instrumental for robustness to over-ﬁtting. We interpret the results as follows. To fully exploit their joint capacity for “successful” over-ﬁtting, the agents need to coordinate label memorization. This requires passing large amounts of information through the channel. With a low temperature (more closely approximating a discrete channel), this is hard, due to a stronger entropy minimiza- tion pressure. To test the hypothesis, we run an experiment where all labels are shufﬂed and a layer of size 400x400 is either added to Sender (just before the channel) or to Re- ceiver (just after the channel). We predict that, with higherτ (less discrete, less entropy minimization pressure), the train- ing curves will be close, as the extra capacity can be used for memorization equally easy in both cases. With lower τ (more discrete, more pressure), the accuracy curves will be more distant, as the extra capacity can only be successfully exploited for memorization when placed before the channel. Figures 3e & 3f bear out the prediction. 4.3.2. R OBUSTNESS TO ADVERSARIAL EXAMPLES We study next robustness of agents equipped with a relaxed discrete channel against adversarial attacks. We use the same architectures as in the preceding experiment. We train agents with different random seeds and imple- ment white-box attacks on the trained models, varying tem- perature τ and the allowed perturbation norm, ϵ. We use the standard Fast Gradient Sign Method of (Goodfellow et al., 2014). The original image is is perturbed to i∗ s along the direction that maximizes the loss of Receiver’s output o= R(S(is)) w.r.t. the ground-truth class l: i∗ s = clip[is + ϵ·sign[∇is L(o,l)] ,0,1] , (5) where ϵcontrols the L∞norm of the perturbation. Under an attack with a ﬁxed ϵ, a more robust method will have a higher accuracy. To avoid numerical stability issues akin to those reported by (Carlini & Wagner, 2016), all computa- tions are done in 64-bit ﬂoats. We experiment with two approaches of getting gradients forEntropy Minimization In Emergent Languages 0 50 100 150 200 Epochs 0.0 0.2 0.4 0.6 0.8 1.0Train accuracy GS, =1.0 GS, =10.0 Linear SM, =1.0 (a) All train labels are shufﬂed. 0 50 100 150 200 Epochs 0.0 0.2 0.4 0.6 0.8 1.0Test accuracy GS, =1.0 GS, =10.0 Linear SM, =1.0  (b) All train labels are shufﬂed. 0 50 100 150 200 Epochs 0.0 0.2 0.4 0.6 0.8 1.0Train accuracy GS, =1.0 GS, =10.0 Linear SM, =1.0 (c) Half of train labels are shufﬂed. 0 50 100 150 200 Epochs 0.0 0.2 0.4 0.6 0.8 1.0Test accuracy GS, =1.0 GS, =10.0 Linear SM, =1.0  (d) Half of train labels are shufﬂed. 0 200 400 600 800 1000 Epochs 0.0 0.2 0.4 0.6 0.8 1.0Train accuracy =10.0, before channel =10.0, after channel (e) All labels shufﬂed; Additional layer before channel vs. after channel 0 200 400 600 800 1000 Epochs 0.0 0.2 0.4 0.6 0.8 1.0Train accuracy =1.0, before channel =1.0, after channel (f) All labels shufﬂed; Additional layer before channel vs. after chan- nel Figure 3.Learning in presence of random labels. GS (SM) denotes models trained with Gumbel-Softmax (Softmax) channel. Linear are models with the channel removed.Entropy Minimization In Emergent Languages 0.00 0.05 0.10 0.15 0.20 0.25 Perturbation norm,  0.2 0.4 0.6 0.8 1.0Accuracy GS, =0.1 GS, =1.0 GS, =10.0 (a) Robustness vs. temp. τ. 0.00 0.05 0.10 0.15 0.20 0.25 Perturbation norm,  0.0 0.2 0.4 0.6 0.8 1.0Accuracy GS, =0.1 SM, =1.0 Linear (b) Comparison to the baselines. Figure 4.Robustness to adversarial examples: higher accuracy given ﬁxed ϵimplies more robustness. the attack. Under the ﬁrst approach, the gradient ∇is L(o,l) is estimated using the standard Gumbel-Softmax relaxation. It is possible, however, that the randomization that Gumbel- Softmax uses internally reduces the usefulness of gradients used for the attack. Hence we also experiment with a setup that is easier for an adversary: after training (and during the attack), we replace the Gumbel-Softmax by a softmax non-linearity with the same temperature. We found that performance in these two setups is virtually the same, indi- cating that the obtained robustness results are independent from the randomization in the channel. Rather, they are due to emergence of well-separated “categories” during training. As in the preceding experiment, SM behaves similarly with different temperatures (we experimented with τ ∈ {0.1,1.0,10.0}): we only report results with τ = 1.0. Fig- ure 4a shows that, as temperature decreases, the accuracy drop also decreases. The highest robustness is achieved with τ = 0.1. Comparison with the baselines (Figure 4b) conﬁrms that relaxed discrete training with τ = 0.1 im- proves robustness. In sum, increased channel discreteness makes it harder to transmit large amounts of information, and leads to in- creased robustness against over-ﬁtting and adversarial ex- amples. Discreteness brings about a bottleneck that has beneﬁcial properties, which might ultimately provide a mo- tivation for why an emergent communication system should evolve towards discreteness. 5. Related Work We brieﬂy reviewed studies of emergent deep agent commu- nication and entropy minimization in human language in the introduction. We are not aware of earlier work that looks for this property in emergent communication, although Evti- mova et al. (2018) used information theory to study protocol development during learning, and, closer to us, K˚ageb¨ack et al. (2018) studied the effect of explicitly adding a com- plexity minimization term to the cost function of an emer- gent color-naming system. Discrete representations are explored in many places (e.g., van den Oord et al., 2017; Jang et al., 2016; Rolfe, 2016). However, these works focus on ways to learn good discrete representations, rather than analyzing the properties of rep- resentations that are independently emerging on the side. Furthermore, our study extends to agents communicating with variable-length messages, produced and consumed by GRU (Cho et al., 2014) and Transformer (Vaswani et al., 2017) cells (see Supplementary). The sequential setup is speciﬁc to language, clearly distinguished from the settings studied in generic sparse-representation work. Other studies, inspired by the Information Bottleneck prin- ciple, control the complexity of neural representations by regulating their information content (Strouse & Schwab, 2017; Fischer, 2019; Alemi et al., 2016; Achille & Soatto, 2018a;b). While they externally impose the bottleneck, we observe that the latter is an intrinsic feature of learning to communicate through a discrete channel. 6. Discussion Entropy minimization is pervasive in human language, where it constitutes a speciﬁc facet of the more general pressure towards communication efﬁciency. We found that the same property consistently characterizes the protocol emerging in simulations where two neural networks learn to solve a task jointly through a discrete communication code. In a comparative perspective, we hypothesize that entropy minimization is a general property of discrete communica- tion, independent of speciﬁc biological constraints humans are subject to. In particular, our analysis tentatively estab- lishes a link between this property and the inherent difﬁculty of encoding information in discrete form (cf. the effect of adding a layer before or after the communication bottleneck in the over-ﬁtting experiment).Entropy Minimization In Emergent Languages Exploring entropy minimization in computational simula- tions provides a ﬂexibility we lack when studying humans. For example, we uncovered here initial evidence that the communication bottleneck is acting as a good regularizer, making the joint agent system more robust to noise and adversarial examples. This leads to an intriguing conjec- ture on the origin of language. Its discrete nature is often traced back to the fact that it allows us to produce an in- ﬁnite number of expressions by combining a ﬁnite set of primitives (e.g., Berwick & Chomsky, 2016). However, it is far from clear that the need to communicate an inﬁnite number of concepts could have provided the initial pressure to develop a discrete code. More probably, once such code independently emerged, it laid the conditions to develop an inﬁnitely expressive language (Bickerton, 2014; Collier et al., 2014). Our work suggests that, because of its inherent regularizing effect, discrete coding is advantageous already when communication is about a limited number of concepts, providing an alternative explanation for its origin. In the future, we would like to study more continuous seman- tic domains, such as color maps, where perfect accuracy is not easily attainable, nor desirable. Will the networks ﬁnd an accuracy/complexity trade-off similar to those at- tested in human languages? Will other core language prop- erties claimed to be related to this trade-off, such as Zipﬁan frequency distributions (Ferrer i Cancho & D ´ıaz-Guilera, 2007), concurrently emerge? We would also like to compare the performance of human subjects equipped with novel con- tinuous vs. discrete communication protocols, adopting the methods of experimental semiotics (Galantucci, 2009). We expect discrete protocols to be more general and robust. Our results have implications for the efforts to evolve agents interacting with each other and with humans through a dis- crete channel. First, because of entropy minimization, we should not agents to develop a richer protocol than the sim- plest one ensuring accurate communication. For example, Bouchacourt & Baroni (2018) found that agents trained to discriminate pairs of natural images depicting instances of about 500 high-level categories, such as cats and dogs, de- veloped a lexicon that does not denote such categories, but low-level properties of the images themselves. This makes sense from an entropy-minimization perspective, as talking about the 500 high-level categories demands log2 500 bits of information, whereas many low-level strategies (e.g., dis- criminating average pixel intensity in the images) will only require transmitting a few bits. To have agents developing rich linguistic protocols, we must face them with varied challenges that truly demand them. Second, the focus on a discrete protocol is typically moti- vated by the goal to develop machines eventually able to communicate with humans. Indeed, discrete messages are not required in multi-agent scenarios where no human in the loop is foreseen (Sukhbaatar et al., 2016). Our results sug- gest that, long before agents reach the level of complexity necessary to converse with humans, there are independent reasons to encourage discreteness, as it leads to simpler protocols and it provides a source of robustness in a noisy world. An exciting direction for future applied work will be to test the effectiveness of discrete communication as a general form of representation learning. Acknowledgements The authors thank Emmanuel Dupoux for discussions and the anonymous reviewers for their feed- back. References Achille, A. and Soatto, S. Information dropout: Learning optimal representations through noisy computation. IEEE TPAMI, 40(12):2897–2905, 2018a. Achille, A. and Soatto, S. Emergence of invariance and disentanglement in deep representations. The Journal of Machine Learning Research, 19(1):1947–1980, 2018b. Alemi, A. A., Fischer, I., Dillon, J. V ., and Murphy, K. Deep variational information bottleneck. arXiv preprint arXiv:1612.00410, 2016. Berwick, R. and Chomsky, N. Why Only Us: Language and Evolution. MIT Press, Cambridge, MA, 2016. Bickerton, D. More than Nature Needs: Language, Mind, and Evolution. Harvard University Press, Cambridge, MA, 2014. Bouchacourt, D. and Baroni, M. How agents see things: On visual representations in an emergent language game. In EMNLP, 2018. Carlini, N. and Wagner, D. Defensive distillation is not robust to adversarial examples. arXiv preprint arXiv:1607.04311, 2016. Chaabouni, R., Kharitonov, E., Dupoux, E., and Baroni, M. Anti-efﬁcient encoding in emergent communication. In NeurIPS, 2019. Chaabouni, R., Kharitonov, E., Bouchacourt, D., Dupoux, E., and Baroni, M. Compositionality and generalization in emergent languages. In ACL, 2020. Chevalier-Boisvert, M., Bahdanau, D., Lahlou, S., Willems, L., Saharia, C., Nguyen, T. H., and Bengio, Y . BabyAI: A platform to study the sample efﬁciency of grounded language learning. In ICLR, 2019. Cho, K., Van Merri ¨enboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y . Learn- ing phrase representations using rnn encoder-decoderEntropy Minimization In Emergent Languages for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014. Choi, E., Lazaridou, A., and de Freitas, N. Compositional obverter communication learning from raw visual input. arXiv preprint arXiv:1804.02341, 2018. Collier, K., Bickel, B., van Schaik, C., Manser, M., and Townsend, S. Language evolution: Syntax before phonol- ogy? Proceedings of the Royal Society B: Biological Sciences, 281(1788):1–7, 2014. Cover, T. M. and Thomas, J. A. Elements of Information Theory. John Wiley & Sons, 2012. Evtimova, K., Drozdov, A., Kiela, D., and Cho, K. Emergent communication in a multi-modal, multi-step referential game. In ICLR, 2018. Ferrer i Cancho, R. and D´ıaz-Guilera, A. The global minima of the communicative energy of natural communication systems. Journal of Statistical Mechanics: Theory and Experiment, 2007(06):P06009, 2007. Ferrer i Cancho, R., Hern´andez-Fern´andez, A., Lusseau, D., Agoramoorthy, G., Hsu, M., and Semple, S. Compression as a universal principle of animal behavior. Cognitive Science, 37(8):1565–1578, 2013. Fischer, I. The conditional entropy bottleneck, 2019. URL https://openreview.net/forum? id=rkVOXhAqY7. Galantucci, B. Experimental semiotics: A new approach for studying communication as a form of joint action. Topics in Cognitive Science, 1(2):393–410, 2009. Gibson, E., Piantadosi, R. F. S., Dautriche, I., Mahowald, K., Bergen, L., and Levy, R. How efﬁciency shapes human language. Trends in Cognitive Science, 2019. In press. Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain- ing and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. Greensmith, E., Bartlett, P. L., and Baxter, J. Variance reduc- tion techniques for gradient estimates in reinforcement learning. JMLR, 5(Nov):1471–1530, 2004. Harding Graesser, L., Cho, K., and Kiela, D. Emergent lin- guistic phenomena in multi-agent communication games. In EMNLP, 2019. Havrylov, S. and Titov, I. Emergence of language with multi- agent games: Learning to communicate with sequences of symbols. In NIPS, 2017. Hurford, J. The Origins of Language. Oxford University Press, Oxford, UK, 2014. Jang, E., Gu, S., and Poole, B. Categorical repa- rameterization with Gumbel-Softmax. arXiv preprint arXiv:1611.01144, 2016. K˚ageb¨ack, M., Dubhashi, D., and Sayeed, A. DeepColor: Reinforcement learning optimizes information efﬁciency and well-formedness in color name partitioning. In Pro- ceedings of CogSci, pp. 1895–1900, Austin, TX, 2018. Kemp, C. and Regier, T. Kinship categories across lan- guages reﬂect general communicative principles. Science, 336(6084):1049–1054, 2012. Kharitonov, E., Chaabouni, R., Bouchacourt, D., and Ba- roni, M. EGG: a toolkit for research on Emergence of lanGuage in Games. In EMNLP: System Demonstrations, 2019. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Kirby, S. Natural language from artiﬁcial life. Artiﬁcial life, 8(2):185–215, 2002. Kottur, S., Moura, J. M., Lee, S., and Batra, D. Natural lan- guage does not emerge “naturally” in multi-agent dialog. arXiv preprint arXiv:1706.08502, 2017. Lazaridou, A., Peysakhovich, A., and Baroni, M. Multi- agent cooperation and the emergence of (natural) lan- guage. arXiv preprint arXiv:1612.07182, 2016. Lazaridou, A., Hermann, K. M., Tuyls, K., and Clark, S. Emergence of linguistic communication from referential games with symbolic and pixel input. arXiv preprint arXiv:1804.03984, 2018. LeCun, Y ., Boser, B. E., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W. E., and Jackel, L. D. Hand- written digit recognition with a back-propagation network. In NIPS, 1990. LeCun, Y ., Bottou, L., Bengio, Y ., and Haffner, P. Gradient- based learning applied to document recognition. Proceed- ings of the IEEE, 86(11):2278–2324, 1998a. LeCun, Y ., Bottou, L., Bengio, Y ., Haffner, P., et al. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998b. Lewis, D. Convention harvard university press. Cambridge, MA, 1969. Li, F. and Bowling, M. Ease-of-teaching and language structure from emergent communication. In NeurIPS. 2019.Entropy Minimization In Emergent Languages Lowe, R., Foerster, J., Boureau, Y ., Pineau, J., and Dauphin, Y . On the pitfalls of measuring emergent communica- tion. In Proceedings of AAMAS, pp. 693–701, Montreal, Canada, 2019. Maddison, C. J., Mnih, A., and Teh, Y . W. The concrete distribution: A continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016. Mikolov, T., Joulin, A., and Baroni, M. A roadmap towards machine intelligence. In International Conference on In- telligent Text Processing and Computational Linguistics, pp. 29–61. Springer, 2016. Mnih, V ., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., and Kavukcuoglu, K. Asyn- chronous methods for deep reinforcement learning. In ICML, 2016. Rolfe, J. T. Discrete variational autoencoders.arXiv preprint arXiv:1609.02200, 2016. Schulman, J., Heess, N., Weber, T., and Abbeel, P. Gradient estimation using stochastic computation graphs. In NIPS, 2015. Strouse, D. and Schwab, D. J. The deterministic information bottleneck. Neural computation, 29(6):1611–1630, 2017. Sukhbaatar, S., Szlam, A., and Fergus, R. Learning mul- tiagent communication with backpropagation. In NIPS. 2016. Tishby, N., Pereira, F., and Bialek, W. The information bot- tleneck method. In Proceedings of the 37th Annual Aller- ton Conference on Communication, Control and Comput- ing. University of Illinois Press, 1999. van den Oord, A., Vinyals, O., and Kavukcuoglu, K. Neural discrete representation learning. In NIPS, 2017. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., Kaiser, L., and Polosukhin, I. Attention is all you need. In NIPS, 2017. Williams, R. J. Simple statistical gradient-following algo- rithms for connectionist reinforcement learning. Machine learning, 8(3-4):229–256, 1992. Williams, R. J. and Peng, J. Function optimization using connectionist reinforcement learning algorithms. Con- nection Science, 3(3):241–268, 1991. Zaslavsky, N., Kemp, C., Regier, T., and Tishby, N. Ef- ﬁcient compression in color naming and its evolution. Proceedings of the National Academy of Sciences , 115 (31):7937–7942, 2018. Zaslavsky, N., Regier, T., Tishby, N., and Kemp, C. Se- mantic categories of artifacts and animals reﬂect efﬁcient coding. In Proceedings of CogSci, pp. 1254–1260, Mon- treal, Canada, 2019. Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. Understanding deep learning requires rethinking general- ization. arXiv preprint arXiv:1611.03530, 2016. 7. How much does Receiver rely on messages in Guess Number? We supplement the experiments of Section 3 of the main text by studying the degree to which Receiver relies on messages in Guess Number. In particular, we show that when Receiver has the full input (is = ir), it ignores the messages. We measure the degree to which Receiver relies on the messages from Sender by constructing a setup where we break communication, but still let Receiver rely on its own input. More precisely, we ﬁrst enumerate all test inputs for Sender is and Receiver ir. We obtain messages that correspond to Sender’s inputs, and shufﬂe them. Next, we feed the shufﬂed messages alongside Receiver’s own (un- shufﬂed) inputs and compute accuracy, as a measure of Receiver’s dependence on the messages. This procedure preserves the marginal distribution of Sender’s messages, but destroys all the information Sender transmits. Without messages, Receiver, given k input bits, can only reach an accuracy of 28−k. In Figure 5, we report results ag- gregated by training method. Receiver is extremely close to the accuracy’s higher bound in all conﬁgurations. Moreover, when Receiver gets the entire input, the drop in accuracy after shufﬂing is tiny, proving that Receiver’s reliance on the message is minimal in that setting. 8. Inﬂuence of architecture choices 8.1. Does vocabulary size affect the results? We repeat the same experiments as in Section 3 of the main text while varying vocabulary size. Note that, to make Guess Number solvable across each conﬁguration, the vocabulary has to contain at least 256 symbols. Similarly, for Image Classiﬁcation, vocabulary size must be of at least 100. We tried vocabulary sizes of 256, 1024, 4096 for Guess Number, and 512, 1024, 2048 for Image Classiﬁcation. The results are reported in Figures 6 (Guess Number) and 7 (Image Classiﬁcation). We observe that there is little qualitative variation over vocabulary size, hence the conclusions we had in Section 3 of the main paper are robust to variations of this parameter.Entropy Minimization In Emergent Languages 0 2 4 6 8 Binary digits hidden 0.0 0.2 0.4 0.6 0.8 1.0Accuracy Gumbel-Softmax Stoch. computation REINFORCE upper bound Figure 5.Guess Number: Receiver’s dependence on messages, measured as performance drop under message intervention. 8.2. Does Receiver’s capacity affect the results? One potential confounding variable is the capacity of Re- ceiver. Indeed, if Receiver is very simple, then, for the task to be solved, Sender would have to calculate the answer itself and feed it to Receiver. To investigate this, we repeat the Image Classiﬁcation experiment from Section 4.1 of the main paper while controlling the power of Receiver’s ar- chitecture: we put two additional fully-connected 400x400 hidden layers between the input embedding and the output layer, while in Section 4, Receiver had a single hidden layer. In Figure 8, we compare the results obtained with these two variations of Receiver. The reported entropy minimiza- tion effect holds: even in presence of additional layers, the entropy of messages H(m) is far from the upper-bound Hmax = 10 bits and closely follows the lower bound, Hmin = log2 Nl. Thus, again, a more nuanced protocol only appears when it is needed. Finally, we see that results for both architectures are close, although in three out of seven task setups (the number of classes Nl is 2, 10, and 20) a deeper model results in a slightly higher entropy of the protocol, on average. Overall, we conclude that Receiver’s capacity does not play a major role in the entropy mini- mization effect and the latter also takes place with a more powerful Receiver. 8.3. What if communication takes place through sequences of symbols? We also experiment with Guess Number in a setup where the agents communicate via variable-length messages. The general architecture of the agents is same as in Section 3, but we append GRU agents (Cho et al., 2014). Sender GRU is unrolled to generate the message. The message is produced until the GRU outputs a special eos token or until the max- imal length is reached. In the latter case, eos is appended to the message. The produced message is consumed by a Receiver’s GRU unit and the hidden state corresponding to eos is used by Receiver as input to further processing. When Receiver has additional inputs (in the Guess Num- ber game), these inputs are used as initial hidden state of the GRU cell. We use the Stochastic Computation Graph estimator as described in Section 3.2, as it provided fastest convergence. We consider the entire variable-length message as the real- ization of a random variablemwhen calculating the entropy of the messages, H(m). The results are reported in Fig- ure 9, arranged in function of maximal message length and vocabulary size. As before, we aggregate the successful runs according to the entropy regularization coefﬁcient λs applied to Sender’s output layer. From Figure 9 we observe that the results are in line with those obtained in the one-symbol scenario. Entropy mini- mization still holds: a more nuanced (high-entropy) protocol only develops when more digits are hidden from Receiver, which hence requires more information to perform the task. The approximation to the lower bound is however less tight as the overall number of possible messages grows (higher maximum length and/or vocabulary size). There is also a weak tendency for lower λs to encourage a tighter bottle- neck. In preliminary experiments, we have similar results when the variable-length communication is performed via Trans- former cells (Vaswani et al., 2017) instead of GRUs (not reported here). 9. Two-digit MNIST dataset As discussed in Section 3, to ensure high output informa- tional complexity in the Image Classiﬁcation task, we use a two-digit variant of the MNIST dataset (LeCun et al., 1998a). We construct it as follows. When iterating over the original MNIST dataset, we take a batch band (a) select the ﬁrst |b|/2 and last |b|/2 images, refer to them as b1 and b2, respectively; (b) create a new batch where the ith image from b1 is placed to the left of the ith image from b2 and then vice versa. As a result, we obtain a new stream of images, where each MNIST digit is seen twice, on the left and on the right side. Note that not all possible pairwise combinations of the original images are generated (there are 600002 of those in the training set alone) and the exact combinations change across epochs. As labels, we use the depicted two-digit number modulo Nl, where Nl is the re- quired number of classes. All pixels are scaled into [0, 1]. We use this same process to generate training and test sets, based on the training and test images of the original MNIST dataset, respectively.Entropy Minimization In Emergent Languages 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits  = 1.0  = 1.25  = 1.5 Hmin (a) V ocab. size: 256, Gumbel-Softmax 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits  = 1.0  = 1.25  = 1.5 Hmin (b) V ocab. size: 1024, Gumbel-Softmax 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits  = 1.0  = 1.25  = 1.5 Hmin (c) V ocab. size: 4096, Gumbel-Softmax 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits s = 0.01 s = 0.025 s = 0.05 s = 0.075 s = 0.1 Hmin (d) V ocab. size: 256, Stoch. Computation Graph approach 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits s = 0.01 s = 0.025 s = 0.05 s = 0.075 s = 0.1 Hmin (e) V ocab. size: 1024, Stoch. Computation Graph approach 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits s = 0.01 s = 0.025 s = 0.05 s = 0.075 s = 0.1 Hmin (f) V ocab. size: 4096, Stoch. Computation Graph approach 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits s = 0.01 s = 0.025 s = 0.05 s = 0.075 s = 0.1 s = 0.5 Hmin (g) V ocab. size: 256, REINFORCE 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits s = 0.01 s = 0.025 s = 0.05 s = 0.075 s = 0.1 s = 0.5 Hmin (h) V ocab. size: 1024, REINFORCE 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits s = 0.01 s = 0.025 s = 0.05 s = 0.075 s = 0.1 s = 0.5 Hmin (i) V ocab. size: 4096, REINFORCE Figure 6.Guess Number: Entropy of the messages m, depending on vocabulary size, training method, and relaxation temperature τ (when trained with Gumbel-Softmax) or Sender’s entropy regularization coefﬁcientλs. Shaded regions mark standard deviation. 1 2 3 4 5 6 log2Nl 1 2 3 4 5 6 7H(m), bits =0.75 =1.0 =1.5 =2.0 Hmin (a) V ocab. size: 512 1 2 3 4 5 6 log2Nl 1 2 3 4 5 6 7H(m), bits =0.75 =1.0 =1.5 =2.0 Hmin (b) V ocab. size: 1024 1 2 3 4 5 6 log2Nl 1 2 3 4 5 6 7H(m), bits =0.75 =1.0 =1.5 =2.0 Hmin (c) V ocab. size: 2048 Figure 7.Image Classiﬁcation: entropy of the messages H(m) across vocabulary sizes. Successful runs are pooled together. Shaded regions mark standard deviation.Entropy Minimization In Emergent Languages 1 2 3 4 5 6 log2Nl 1 2 3 4 5 6 7H(m), bits One hidden layer Three hidden layers Hmin Figure 8.Image Classiﬁcation: entropy of the messages H(m) across Receiver model sizes. Successful runs are pooled together. Shaded regions mark standard deviation. 10. Hyperparameters In our experiments, we used the following hyperparameter grids. Guess Number (Gumbel-Softmax) V ocab. size: [256, 1024, 4096]; temperature, τ: [0.5, 0.75, 1.0, 1.25, 1.5]; learning rate: [0.001, 0.0001]; max. number of epochs: 250; random seeds: [0, 1, 2, 3]; batch size: 8; early stopping thr.: 0.99; bits shown to Receiver: [0, 1, 2, 3, 4, 5, 6, 7, 8]. Guess Number (REINFORCE) V ocab. size: [256, 1024, 4096]; Sender entropy regularization coef., λs: [0.01, 0.025, 0.05, 0.1, 0.5, 1.0]; Receiver entropy regularization coef., λr: [0.01, 0.1, 0.5, 1.0]; learning rate: [0.0001, 0.001, 0.01]; max. number of epochs: 1000; random seeds: [0, 1, 2, 3]; batch size: 2048; early stopping thr.: 0.99; bits shown to Receiver: [0, 1, 2, 3, 4, 5, 6, 7, 8]. Guess Number (Stochastic Computation Graph ap- proach): V ocab. size: [256, 1024, 4096]; Sender entropy regularization coef., λs: [0.01, 0.05, 0.1, 0.25]; learning rate: [0.0001, 0.001]; max. number of epochs: 1000; ran- dom seeds: [0, 1, 2, 3]; batch size: 2048; early stopping thr.: 0.99; bits shown to Receiver: [0, 1, 2, 3, 4, 5, 6, 7, 8]. Image Classiﬁcation experiments V ocab. size: [512, 1024, 2048]; temperature, τ: [0.5, 0.75, 1.0, 1.5, 2.0]; learning rate: [0.001], max. number of epochs: 100; random seeds: [0, 1, 2]; batch size: 32; early stopping thr.: 0.98; number of classes: [2, 4, 10, 20, 25, 50, 100]. Fitting random labels experiments V ocab. size: 1024; temperature, τ: [1.0, 10.0]; learning rate: 0.0001, max. number of epochs: 200; random seeds: [0, 1, 2, 3, 4]; batch size: 32; early stopping thr.: ∞; prob. of label corruption: [0.0, 0.5, 1.0]. Adversarial attack experiments V ocab. size: 1024; tem- perature, τ: [0.1, 1.0, 10.0]; learning rate: 0.0001, max. number of epochs: 200; random seeds: [0, 1, 2, 3, 4]; batch size: 32; early stopping thr.: 0.98. 11. Evolution of message entropy during training In this Section, we aim to gain additional insight into de- velopment of the communication protocol by measuring its entropy during training. We concentrate on Guess Number and use the same experimental runs summarized in Figure 1 of the main text. For each game conﬁguration (that is, number of bits hidden from Receiver), we randomly select one successful run and plot the evolution of Sender message entropy and accuracy over training epochs.3 We also plot entropy and accuracy curves for a randomly selected failed run, to verify to what extent entropy development depends on task success. We report results for runs where training was performed with Gumbel-Softmax relaxation and with the Stochastic Graph Computation approach in Figures 10 and 11, respectively. The reported entropy and accuracy values are calculated in evaluation mode, where Sender’s output is selected greedily, without sampling. A higher entropy of such deterministic Sender indicates that the latter can encode more information about inputs in its messages. From these results, we ﬁrstly observe that the initial entropy of Sender’s messages (before training) can be both higher than required for communication success (Figures 10a and 11a) and lower (the rest). When it starts higher than needed, it generally falls closer to the minimum level required for the solution. When the initial value is low, it increases during training. The failed runs can have message entropy above (Figures 10a, 10b & 11a) and below (e.g. Figures 10c, 10d & 11d) successful runs, suggesting that there is no systematic relation between degree of entropy and task success. The fact that the entropy can be reduced with no decrease in accuracy or even with accuracy growth (e.g. Figure 10a, red line, epochs 5..30) indicates that the tendency to discover new messages (increasing entropy) is counter-balanced by the complexity of mutual coordination with Receiver when entropy is larger. In our interpretation, it is this interplay that serves as a source of the natural bottleneck. Finally, while in some runs the entropy is effectively in- creased w.r.t. its initialization level, the resulting protocol’s entropy is at, or slightly above the lower bound of what the task allows. In this sense, we argue that the reported effect 3We exclude the conﬁguration in which Receiver sees the entire input, as it is a degenerate case of non-communication, as discussed in Section 4 of the main text.Entropy Minimization In Emergent Languages 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits =0.05 =0.1 =0.01 Hmin (a) Max length: 5, vocabulary size: 16 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits =0.05 =0.1 =0.01 Hmin (b) Max length: 10, vocabulary size: 16 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits =0.05 =0.1 =0.01 Hmin (c) Max length: 5, vocabulary size: 64 0 2 4 6 8 Binary digits hidden 0 2 4 6 8H(m), bits =0.05 =0.1 =0.01 Hmin (d) Max length: 10, vocabulary size: 64 Figure 9.Guess Number: Entropy of the emergent protocol when communication is performed with variable-length messages. Shaded regions mark standard deviation.Entropy Minimization In Emergent Languages can be correctly denoted as a “minimization” result. References Achille, A. and Soatto, S. Information dropout: Learning optimal representations through noisy computation. IEEE TPAMI, 40(12):2897–2905, 2018a. Achille, A. and Soatto, S. Emergence of invariance and disentanglement in deep representations. The Journal of Machine Learning Research, 19(1):1947–1980, 2018b. Alemi, A. A., Fischer, I., Dillon, J. V ., and Murphy, K. Deep variational information bottleneck. arXiv preprint arXiv:1612.00410, 2016. Berwick, R. and Chomsky, N. Why Only Us: Language and Evolution. MIT Press, Cambridge, MA, 2016. Bickerton, D. More than Nature Needs: Language, Mind, and Evolution. Harvard University Press, Cambridge, MA, 2014. Bouchacourt, D. and Baroni, M. How agents see things: On visual representations in an emergent language game. In EMNLP, 2018. Carlini, N. and Wagner, D. Defensive distillation is not robust to adversarial examples. arXiv preprint arXiv:1607.04311, 2016. Chaabouni, R., Kharitonov, E., Dupoux, E., and Baroni, M. Anti-efﬁcient encoding in emergent communication. In NeurIPS, 2019. Chaabouni, R., Kharitonov, E., Bouchacourt, D., Dupoux, E., and Baroni, M. Compositionality and generalization in emergent languages. In ACL, 2020. Chevalier-Boisvert, M., Bahdanau, D., Lahlou, S., Willems, L., Saharia, C., Nguyen, T. H., and Bengio, Y . BabyAI: A platform to study the sample efﬁciency of grounded language learning. In ICLR, 2019. Cho, K., Van Merri ¨enboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y . Learn- ing phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014. Choi, E., Lazaridou, A., and de Freitas, N. Compositional obverter communication learning from raw visual input. arXiv preprint arXiv:1804.02341, 2018. Collier, K., Bickel, B., van Schaik, C., Manser, M., and Townsend, S. Language evolution: Syntax before phonol- ogy? Proceedings of the Royal Society B: Biological Sciences, 281(1788):1–7, 2014. Cover, T. M. and Thomas, J. A. Elements of Information Theory. John Wiley & Sons, 2012. Evtimova, K., Drozdov, A., Kiela, D., and Cho, K. Emergent communication in a multi-modal, multi-step referential game. In ICLR, 2018. Ferrer i Cancho, R. and D´ıaz-Guilera, A. The global minima of the communicative energy of natural communication systems. Journal of Statistical Mechanics: Theory and Experiment, 2007(06):P06009, 2007. Ferrer i Cancho, R., Hern´andez-Fern´andez, A., Lusseau, D., Agoramoorthy, G., Hsu, M., and Semple, S. Compression as a universal principle of animal behavior. Cognitive Science, 37(8):1565–1578, 2013. Fischer, I. The conditional entropy bottleneck, 2019. URL https://openreview.net/forum? id=rkVOXhAqY7. Galantucci, B. Experimental semiotics: A new approach for studying communication as a form of joint action. Topics in Cognitive Science, 1(2):393–410, 2009. Gibson, E., Piantadosi, R. F. S., Dautriche, I., Mahowald, K., Bergen, L., and Levy, R. How efﬁciency shapes human language. Trends in Cognitive Science, 2019. In press. Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain- ing and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. Greensmith, E., Bartlett, P. L., and Baxter, J. Variance reduc- tion techniques for gradient estimates in reinforcement learning. JMLR, 5(Nov):1471–1530, 2004. Harding Graesser, L., Cho, K., and Kiela, D. Emergent lin- guistic phenomena in multi-agent communication games. In EMNLP, 2019. Havrylov, S. and Titov, I. Emergence of language with multi- agent games: Learning to communicate with sequences of symbols. In NIPS, 2017. Hurford, J. The Origins of Language. Oxford University Press, Oxford, UK, 2014. Jang, E., Gu, S., and Poole, B. Categorical repa- rameterization with Gumbel-Softmax. arXiv preprint arXiv:1611.01144, 2016. K˚ageb¨ack, M., Dubhashi, D., and Sayeed, A. DeepColor: Reinforcement learning optimizes information efﬁciency and well-formedness in color name partitioning. In Pro- ceedings of CogSci, pp. 1895–1900, Austin, TX, 2018. Kemp, C. and Regier, T. Kinship categories across lan- guages reﬂect general communicative principles. Science, 336(6084):1049–1054, 2012.Entropy Minimization In Emergent Languages 0 50 100 150 200 250 epoch 0.0 0.5 1.0 1.5 2.0 2.5H(m), bits 0 50 100 150 200 250 epoch 0.0 0.2 0.4 0.6 0.8 1.0Accuracy (a) Binary digits hidden: 2 0 50 100 150 200 250 epoch 0 1 2 3 4 5H(m), bits 0 50 100 150 200 250 epoch 0.0 0.2 0.4 0.6 0.8 1.0Accuracy (b) Binary digits hidden: 4 0 50 100 150 200 250 epoch 0 2 4 6H(m), bits 0 50 100 150 200 250 epoch 0.0 0.2 0.4 0.6 0.8 1.0Accuracy (c) Binary digits hidden: 6 0 50 100 150 200 250 epoch 2 4 6 8H(m), bits 0 50 100 150 200 250 epoch 0.0 0.2 0.4 0.6 0.8 1.0Accuracy (d) Binary digits hidden: 8 Figure 10.Evolution of H(m) over training epochs. Gumbel Softmax-based optimization, Guess Number. For each game conﬁguration, speciﬁed by the number of bits Receiver lacks, we sample one successful (black line) and one failed (red line) training trajectory. The blue line marks Hmin, minimal entropy for a successful solution.Entropy Minimization In Emergent Languages 0 50 100 150 200 250 epoch 0 2 4 6H(m), bits 0 50 100 150 200 250 epoch 0.2 0.4 0.6 0.8 1.0Accuracy (a) Binary digits hidden: 2 0 50 100 150 200 250 epoch 2 4 6H(m), bits 0 50 100 150 200 250 epoch 0.0 0.2 0.4 0.6 0.8 1.0Accuracy (b) Binary digits hidden: 4 0 50 100 150 200 250 epoch 2 3 4 5 6H(m), bits 0 50 100 150 200 250 epoch 0.0 0.2 0.4 0.6 0.8 1.0Accuracy (c) Binary digits hidden: 6 0 50 100 150 200 250 epoch 4 6 8H(m), bits 0 50 100 150 200 250 epoch 0.0 0.2 0.4 0.6 0.8 1.0Accuracy (d) Binary digits hidden: 8 Figure 11.Evolution of H(m) over training epochs. Stochastic Computation Graph-based optimization, Guess Number. For each game conﬁguration, speciﬁed by the number of bits Receiver lacks, we sample one successful (black line) and one failed (red line) training trajectory. The blue line marks Hmin, minimal entropy for a successful solution.Entropy Minimization In Emergent Languages Kharitonov, E., Chaabouni, R., Bouchacourt, D., and Ba- roni, M. EGG: a toolkit for research on Emergence of lanGuage in Games. In EMNLP: System Demonstrations, 2019. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Kirby, S. Natural language from artiﬁcial life. Artiﬁcial life, 8(2):185–215, 2002. Kottur, S., Moura, J. M., Lee, S., and Batra, D. Natural lan- guage does not emerge “naturally” in multi-agent dialog. arXiv preprint arXiv:1706.08502, 2017. Lazaridou, A., Peysakhovich, A., and Baroni, M. Multi- agent cooperation and the emergence of (natural) lan- guage. arXiv preprint arXiv:1612.07182, 2016. Lazaridou, A., Hermann, K. M., Tuyls, K., and Clark, S. Emergence of linguistic communication from referential games with symbolic and pixel input. arXiv preprint arXiv:1804.03984, 2018. LeCun, Y ., Boser, B. E., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W. E., and Jackel, L. D. Hand- written digit recognition with a back-propagation network. In NIPS, 1990. LeCun, Y ., Bottou, L., Bengio, Y ., and Haffner, P. Gradient- based learning applied to document recognition. Proceed- ings of the IEEE, 86(11):2278–2324, 1998a. LeCun, Y ., Bottou, L., Bengio, Y ., Haffner, P., et al. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998b. Lewis, D. Convention harvard university press. Cambridge, MA, 1969. Li, F. and Bowling, M. Ease-of-teaching and language structure from emergent communication. In NeurIPS. 2019. Lowe, R., Foerster, J., Boureau, Y ., Pineau, J., and Dauphin, Y . On the pitfalls of measuring emergent communica- tion. In Proceedings of AAMAS, pp. 693–701, Montreal, Canada, 2019. Maddison, C. J., Mnih, A., and Teh, Y . W. The concrete distribution: A continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016. Mikolov, T., Joulin, A., and Baroni, M. A roadmap towards machine intelligence. In International Conference on In- telligent Text Processing and Computational Linguistics, pp. 29–61. Springer, 2016. Mnih, V ., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., and Kavukcuoglu, K. Asyn- chronous methods for deep reinforcement learning. In ICML, 2016. Rolfe, J. T. Discrete variational autoencoders.arXiv preprint arXiv:1609.02200, 2016. Schulman, J., Heess, N., Weber, T., and Abbeel, P. Gradient estimation using stochastic computation graphs. In NIPS, 2015. Strouse, D. and Schwab, D. J. The deterministic information bottleneck. Neural computation, 29(6):1611–1630, 2017. Sukhbaatar, S., Szlam, A., and Fergus, R. Learning mul- tiagent communication with backpropagation. In NIPS. 2016. Tishby, N., Pereira, F., and Bialek, W. The information bot- tleneck method. In Proceedings of the 37th Annual Aller- ton Conference on Communication, Control and Comput- ing. University of Illinois Press, 1999. van den Oord, A., Vinyals, O., and Kavukcuoglu, K. Neural discrete representation learning. In NIPS, 2017. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., Kaiser, L., and Polosukhin, I. Attention is all you need. In NIPS, 2017. Williams, R. J. Simple statistical gradient-following algo- rithms for connectionist reinforcement learning. Machine learning, 8(3-4):229–256, 1992. Williams, R. J. and Peng, J. Function optimization using connectionist reinforcement learning algorithms. Con- nection Science, 3(3):241–268, 1991. Zaslavsky, N., Kemp, C., Regier, T., and Tishby, N. Ef- ﬁcient compression in color naming and its evolution. Proceedings of the National Academy of Sciences , 115 (31):7937–7942, 2018. Zaslavsky, N., Regier, T., Tishby, N., and Kemp, C. Se- mantic categories of artifacts and animals reﬂect efﬁcient coding. In Proceedings of CogSci, pp. 1254–1260, Mon- treal, Canada, 2019. Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. Understanding deep learning requires rethinking general- ization. arXiv preprint arXiv:1611.03530, 2016.",
      "meta_data": {
        "arxiv_id": "1905.13687v3",
        "authors": [
          "Eugene Kharitonov",
          "Rahma Chaabouni",
          "Diane Bouchacourt",
          "Marco Baroni"
        ],
        "published_date": "2019-05-31T15:54:41Z",
        "pdf_url": "https://arxiv.org/pdf/1905.13687v3.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper investigates the information-theoretic complexity of emergent languages in neural agents trained for communication tasks. It discovers that these emergent languages are subject to an entropy minimization pressure, similar to human language, where the mutual information between the communicating agent's inputs and messages is minimized, while still ensuring successful communication. This pressure is amplified by increased communication channel discreteness. A key finding is that stronger discrete-channel-driven entropy minimization leads to representations with enhanced robustness to overfitting and adversarial attacks. The research suggests that discreteness in communication systems might be 'beneficial' due to its inherent regularizing effect, offering an alternative explanation for the origin of discreteness in human language.",
        "methodology": "The study employs signaling games with a two-agent (Sender, Receiver) one-exchange setup, where agents are deterministic functions of their inputs after training. Two main games are used: 'Guess Number', where agents recover an 8-bit integer with varying information revealed to the Receiver, and 'Image Classification', involving joint classification of paired MNIST digits with informational complexity controlled by the number of output classes. Sender and Receiver architectures include linear layers, Leaky ReLU, Softmax (Sender), and concatenation, fully connected layers, and Sigmoid (Receiver for Guess Number) or LeNet-1 embedding (Sender for Image Classification). Training with discrete messages is handled using Gumbel-Softmax relaxation, REINFORCE, and the Stochastic Computation Graph (SCG) optimization approaches, all utilizing the Adam optimizer. Loss functions vary by method and task (0/1 loss for REINFORCE in Guess Number, binary cross-entropy for Gumbel-Softmax/SCG in Guess Number, negative log-likelihood for Image Classification). Entropy regularization terms are included for REINFORCE and SCG to favor higher entropy in Sender's output.",
        "experimental_setup": "For 'Guess Number', all 256 possible 8-bit integer inputs are used for training, early stopping, and analysis. For 'Image Classification', random image pairs from the MNIST training data are used for training, and held-out MNIST sets for validation. Images are 28x56 pixels, formed by stacking two MNIST digits, and grouped into 2, 4, 10, 20, 25, 50, or 100 equally-sized classes. Experiments primarily fix the vocabulary to 1024 symbols, with other sizes explored in supplementary material, along with variable-length messages using GRU and Transformer cells. Successful runs are defined by a training accuracy above 0.99 for Guess Number and validation accuracy above 0.98 for Image Classification. Message entropy H(m) is the primary metric. Robustness to overfitting is assessed by training models with randomly-shuffled labels in the Image Classification game (Nl=10) and comparing against Linear and Softmax baselines. Robustness to adversarial examples is evaluated using white-box attacks with the Fast Gradient Sign Method (FGSM), varying perturbation norm (ϵ) and Gumbel-Softmax temperature (τ).",
        "limitations": "The emergent protocol exhibits minimal entropy minimization pressure when the Receiver already possesses all necessary information (e.g., k=8 in Guess Number), resulting in Sender transmitting ignored information. High values of Sender's entropy regularization coefficient (λs) can hinder communication success, requiring careful tuning. The main experiments are conducted with single-symbol messages, although variable-length messages are explored in supplementary sections. The study is confined to a basic two-agent, one-exchange communication setup, which might not fully capture the complexities of more elaborate communication systems.",
        "future_research_directions": "Future work could explore continuous semantic domains, such as color maps, where an exact, high-accuracy solution is not always feasible or desired, to investigate if emergent languages develop accuracy/complexity trade-offs similar to human languages. This could also involve examining the concurrent emergence of other core language properties like Zipfian frequency distributions. The authors propose comparing human subjects using continuous vs. discrete communication protocols in experimental semiotics to test the generality and robustness of discrete protocols. Further research is suggested to encourage agents to develop richer linguistic protocols by introducing more complex and varied communication challenges. Lastly, exploring the effectiveness of discrete communication as a general form of representation learning is an exciting direction for applied work."
      }
    },
    {
      "title": "Generalization Guarantees for Sparse Kernel Approximation with Entropic Optimal Features",
      "abstract": "Despite their success, kernel methods suffer from a massive computational\ncost in practice. In this paper, in lieu of commonly used kernel expansion with\nrespect to $N$ inputs, we develop a novel optimal design maximizing the entropy\namong kernel features. This procedure results in a kernel expansion with\nrespect to entropic optimal features (EOF), improving the data representation\ndramatically due to features dissimilarity. Under mild technical assumptions,\nour generalization bound shows that with only $O(N^{\\frac{1}{4}})$ features\n(disregarding logarithmic factors), we can achieve the optimal statistical\naccuracy (i.e., $O(1/\\sqrt{N})$). The salient feature of our design is its\nsparsity that significantly reduces the time and space cost. Our numerical\nexperiments on benchmark datasets verify the superiority of EOF over the\nstate-of-the-art in kernel approximation.",
      "full_text": "Generalization Guarantees for Sparse Kernel Approximation with Entropic Optimal Features Liang Ding, Rui Tuo, Shahin Shahrampour Texas A&M University E-mail: {ldingaa,ruituo,shahin}@tamu.edu Abstract Despite their success, kernel methods suffer from a massive computational cost in practice. In this paper, in lieu of commonly used kernel expansion with respect toNinputs, we develop a novel optimal design maximizing the entropy among kernel features. This procedure results in a kernel expansion with respect to entropic optimal features (EOF), improving the data representation dramatically due to features dissimilarity. Under mild technical assumptions, our generalization bound shows that with only O(N 1 4 ) features (disregarding logarithmic factors), we can achieve the optimal statistical accuracy (i.e., O(1/ √ N)). The salient feature of our design is its sparsity that signiﬁcantly reduces the time and space cost. Our numerical experiments on benchmark datasets verify the superiority of EOF over the state-of-the-art in kernel approximation. 1 Introduction Kernel methods are powerful tools in describing nonlinear data models. However, despite their success in various machine learning tasks, kernel methods always suffer from scalability issues, especially when the learning task involves matrix inversion (e.g., kernel ridge regression). This is simply due to the fact that for a dataset of size N, the inversion step requires O(N3) time cost. To tackle this problem, a great deal of research has been dedicated to the approximation of kernels using low-rank surrogates (1–3). By approximating the kernel, these methods deal with a linear problem, potentially solvable in a linear time with respect to N (see e.g. (4) for linear Support Vector Machines (SVM)). In the approximation of kernel with a ﬁnite number of features, one fundamental question is how to select the features. As an example, in supervised learning, we are interested to identify features that lead to low out-of-sample error. This question has been studied in the context of random features, which 1 arXiv:2002.04195v1  [cs.LG]  11 Feb 2020is an elegant method for kernel approximation ( 3). Most of the works in this area improve the out-of- sample performance by modifying the stochastic oracle from which random features are sampled (5–7). Nevertheless, these methods deal with dense feature matrices (due to randomness) and still require a large number of features to learn the data subspace. Decreasing the number of features directly affects the time and space costs, and to achieve that we must choose features that are as distinct as possible (to better span the space). Focusing on explicit features, we aim to achieve this goal in the current work. 1.1 Our Contributions In this paper, we study low-rank kernel approximation by ﬁnding a set of mutually orthogonal features with nested and compact supports. We ﬁrst theoretically characterize a condition (based on the Sturm-Liouville problem), which allows us to obtain such features. Then, we propose a novel optimal design method that maximizes the metric entropy among those features. The problem is formulated as a combinatorial optimization with a constraint on the number of features used for approximation. The optimization is generally NP-hard but yields closed-form solutions for speciﬁc numbers of features. The algorithm, dubbed entropic optimal features (EOF), can use these features for supervised learning. The construction properties of features (orthogonality, compact support, and nested support) result in a sparse approximation saving dramatically on time and space costs. We establish a generalization bound for EOF that shows with only O(N 1 4 ) features (disregarding logarithmic factors), we can achieve the optimal statistical accuracy (i.e., O(1/ √ N)). Our numerical experiments on benchmark datasets verify the superiority of EOF over the state-of-the-art in kernel approximation. While we postpone the exhaustive literature review to Section 6, none of the previous works has approached the problem from the entropy maximization perspective, which is the unique distinction of the current work. 2 Preliminaries on Kernel Methods Kernel methods map ﬁnite-dimensional data to a potentially inﬁnite dimensional feature space. Any element f in the reproducing kernel Hilbert space (RKHS) of k, denoted by Hk, has the following 2representation: f = ∞∑ i=1 ⟨f,gi⟩kgi, (1) where ⟨·,·⟩k RKHS inner product induced by k and {gi}is any feature set (i.e., orthonormal basis) that spans the space Hk. In general, the kernel trick relies on the observation that the inner product ⟨k(·,x),k(·,x′)⟩k = k(x,x′) with x,x′∈RD (reproducing property), so k(x,x′) is cheap to compute without the need to calculate the inner product. In this case, the feature set selected in equation (1) is {k(·,x) : x ∈RD}and the target function can be written as ∑ i cik(·,xi). Under mild conditions, by the Representer Theorem, it is guaranteed that any solution of the risk minimization problem assumes the form f(·) = ∑N i=1 cik(·,xi), where N is the number of training data points. However, this representation introduces a massive time cost ofO(N3) and a memory cost of O(N2) in the training. Further, the feature space {k(·,x) : x ∈RD}may not cover Hk from an optimal sense. To be more speciﬁc, there might be another set of features {gi}M i=1 with M ≪N such that {k(·,x) : x ∈X}⊂{ gi}M i=1 where X ∈RN×D is the input data. To address the aforementioned problem, (3) propose a random approximation of k(x,x′) k(x,x′) ≈zT(x)z(x′) (2) where z(x) = [ζ1(x),...,ζ M (x)] is a random vector. This decomposes the feature k(·,x) into a linear combination of random low-rank features {ζi}to approximate the original target function ∑N i=1 cik(·,xi) by ∑M i=1 αiζi. This idea resolves the computational issue of the algorithm, but due to random selection of the features, the method does not offer the best candidate features for reconstructing the target function. Furthermore, in supervised learning the goal is to ﬁnd a mapping from inputs to outputs, and an optimal kernel approximation does not necessarily result in an optimal target function. The reason is simply that we require the features that best represent the underlying data model (or target function) rather than the kernel function. 33 Kernel Feature Selection In this paper, we propose an algorithm that uses a sparse representation to attain a high prediction accuracy with a low computational cost. The key ingredient is to ﬁnd an expansion: f = ∞∑ i=1 ⟨f,gi⟩kgi (3) such that features {gi}satisfy the following properties: 1. Compact support: supt [gi] is compact. 2. Nested support: supt [gi] = ⋃ j∈I supt[gj] for some ﬁnite set I. 3. Orthogonality: ⟨gi,gj⟩k = δij where δij denotes the Kronecker delta. Properties 1-2 ensure low time cost for the algorithm by promoting sparsity. To be more speciﬁc, given any ﬁnite set {gi}M i=1 and any data point x, gi(x) = 0 for a large number of gi ∈{gi}M i=1. Property 3 provides a better expansion of Hk. In general, this problem may be intractable; however, we will prove later in Theorem 2 that whenk satisﬁes the following condition, then a feature set {φi}that satisﬁes properties 1-3 does exist: Condition 1. Let kernel kbe of the following product form: k(x,x′) = D∏ d=1 p(min{xd,x′ d})q(max{xd,x′ d}) where pand qare the independent solutions of the Sturm-Liouville problem on the interval [a,b] for any a,b ∈[−∞,∞]: d dxα(x)dy dx + β(x)y= 0, and they satisfy the following boundary conditions: c11p′(a) + c12p(a) = 0 c21q′(b) + c22q(b) = 0 4with cij ≥0 for i,j = 1 ,2 and the operator d dxα(x) d dx + β(x) is an elliptic operator that satisﬁes Lax-Milgram Theorem (see section 6 of (8)). We provide two commonly used kernels that satisfy condition 1: k(x,x′) = e−ω∥x−x′∥1 k(x,x′) = D∏ d=1 [ωmin{xd,x′ d}+ 1]. The ﬁrst one is the Laplace kernel and the second one is the kernel associated to weighted Sobolev space (9). Let zl,i = i2−l for any l,i ∈N. Then, when the dimension D = 1, features associated to Laplace kernel satisfying properties 1-3 are as follows: φl,i(x) =    sinh ω|x−zl,i+1| sinh ω2−l if x∈(zl,i,zl,i+1] sinh ω|x−zl,i−1| sinh ω2−l if x∈[zl,i−1,zl,i] 0 otherwise (4) and features associated to the weighted Sobolev space kernel are as follows: φl,i(x) = max { 0,1 −|x−zl,i| 2−l } where (l,i) is the index of features. We now start from 1-D kernel to construct a feature space that satisﬁes properties 1-3: Theorem 1. Suppose k is a kernel that satisﬁes Condition 1. Let Zl = {zl,i = i2−l : i = 1,2l −1} and let Bl = {i = 1 ,··· ,2l −1 : i is odd}. We then deﬁne the following function on the interval [zl,i−1,zl,i+1] = [(i−1)2−l,(i+ 1)2−l]: φl,i(x) =    q(x)pl,i+1−p(x)ql,i+1 ql,ipl,i+1−pl,iql,i+1 if x∈(zl,i, zl,i+1] p(x)ql,i−1−q(x)pl,i−1 pl,iql,i−1−ql,ipl,i−1 if x∈[zl,i−1, zl,i] 0 otherwise . (5) where pl,i = p(zl,i) = p(i2−l) and ql,i = q(zl,i) = q(i2−l). Then the following feature set is an orthogonal basis of the RKHS of k, Hk, that satisﬁes property 1-3 on the unit interval [0,1]: {φl,i : l∈N,i ∈Bl}. 5The theorem above characterizes the set of features that satisfy Condition 1 when the input is scalar. To extend the idea to D-dimensional space, we only need to take the tensor product form of the 1-dimensional kernel, as described by the consequent theorem: Theorem 2. Suppose kis a kernel that satisﬁes Condition 1. For any l ∈ND, we deﬁne the Cartesian product of sets as follows: Zl = ×D d=1Zld = {zl,i = (zl1,i1 ,··· ,zlD,iD ) : zld,id ∈Zld} Bl = ×D d=1 = {i ∈ND : id ∈Bld}. We then deﬁne the following function on the hypercube×D d=1[zld,id−1,zld,id+1] = ×D d=1[(id −1)2−ld,(id + 1)2−ld]: φl,i(x) = D∏ d=1 φld,id(xd) (6) where the function φld,id is deﬁned in Theorem 1. Then the following feature set is an orthogonal basis of the RKHS of k, Hk, that satisﬁes property 1-3 on the unit cube [0,1]D: {φl,i : l ∈ND,i ∈Bl}. The proof of Theorem 1 is given in the supplementary material. Theorem 2 can be derived from Theorem 1, because the kernel is simply the tensor product of 1-dimensional kernel in Theorem 1. Corollary 3.For any kernelksatisﬁes condition 1 and let φl,i be the function deﬁned in Theorem 2. Then we have the following expansion for k: k(x,x′) = ∑ l∈ND ∑ i∈Bl φl,i(x)φl,i(x′) ⟨φl,i,φl,i⟩k (7) where ⟨·,·⟩k is the inner product induced by k. Proof. We only need to substitute f(·) in equation (3) by k(x,·), then according to the reproducing property of kwe can have the result. 6Corollary 3 is the direct result of Theorem 2. So we can have the following sparse approximation for the value k(x,x′): k(x,x′) ≈zT(x)z(x′) where z(x) = [φl,i(x) ||φl,i||k ] (l,i)∈S for some set S. We will show in section 4.1 that most entries on z(x) are zero. Form this perspective, the expansion (7) is analogous to the random feature (2) except that the above z(x) is nonrandom. We now use the RKHS of the following kernel on[0,1] as an example: k(x,x′) = min{x,x′}[1 −max{x,x′}]. The RKHS associated to kis the ﬁrst order Sobolev space with zero boundary conditions: Hk = { f : ∫ 1 0 [f′(s)]2ds< ∞,f(0) = f(1) = 0 } . In this example, the feature functions given by Theorem 1 coincide with a wavelet basis in Hk. Consider the mother wavelet given by the triangular function: φ(d) = max{0,1 −|d|}. Then for any l∈N, i= 1,··· ,2l −1, direct calculations show that φl,i(x) = φ (x−i2−l 2−l ) . (8) Now it is easy to verify that the features {φl,i : l∈N,i is odd}satisfy the desired properties 1-3: 1. supt [φl,i] = [(i−1)2−l,(i+ 1)2−l]. 2. supt [φl,i] = supt[φl+1,2i−1] ∪supt[φl+1,2i+1]. 3. ∫1 0 φ′ l,iφ′ n,jds= 2l+1δ(l,i),(n,j). 7Figure 1: Top two panels: W2 = {φl,i : l = 2}and W3 = {φl,i : l = 3}; lower two panels: nested structure for the representation of a function f ∈Hk. 0 0.2 0.4 0.6 0.8 10 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 W 2 0 0.2 0.4 0.6 0.8 10 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 W 3 0 0.2 0.4 0.6 0.8 10 0.05 0.1 0.15 0.2 0.25 0.3 , 1,1 , 2,1 , 2,2 0 0.2 0.4 0.6 0.8 10 0.05 0.1 0.15 0.2 0.25 0.3 , 1,1 , 2,1 , 2,2 , 3,1 , 3,2 , 3,3 , 3,4 Figure 2: 2-D tensor product of wavelet features with compact support φ[1,2],[11] and φ[1,2],[13] Figure 1 illustrates the compact and nested supports of these wavelet features. The compact support properties can lead to a signiﬁcant improvement in time cost. Consider the evaluation of f(x) = ∑ |l|≤n αl,iφl,i(x). The compact support property implies that φl,i(x) = 0 for most (l,i)’s, so that the computational cost of evaluating f(x) can be much lower than the total number of features. In Section 4.1, we will leverage this property of the basis functions to propose an efﬁcient algorithm for learning. This goal cannot be achieve when the basis functions are not compactly supported, such as the random features. Figure 2 shows the example of the tensor product of the wavelet feature deﬁned in (8). It is a 2- dimensional extension of the wavelet feature and according to Theorem 2, the features satisfy properties 81-3 in the RKHS induced by the following kernel: k(x,x′) = D∏ d=1 min{xd,x′ d}[1 −max{xd,x′ d}], which is the mixed Sobolev space of ﬁrst order with zero boundary condition on [0,1]D.We refer the reader to (10) for more details on mixed order Sobolev space. In view of Theorem 2, we can now lift a data point fromx ∈RD to a ﬁnite dimensional space spanned by features with compact and nested supports. As a result, the evaluation of x on a large number of features is zero, yielding a sparse and efﬁcient representation. 4 Entropic Optimal Design In the previous section, we provide conditions under which we can ﬁnd features with compact and nested supports. We now present an optimization criterion to select the best ﬁnite set of features with the maximum metric entropy. The intuition behind this choice is that we favor a set of features that are different from each other as much as possible, so that we can reconstruct the underlying model by a moderate amount of features. To formulate the optimization problem, we need to introduce some notation. First we introduce the covering number of an operator between two Banach spaces. Let ε> 0 and A,B be Banach spaces with unit balls BA and BB, respectively. The covering number of an operator T : A →B is deﬁned as N(T,ε) = inf n∈N { n: ∃{bi ∈B}n i=1 s.t. T(BA) ⊆ n⋃ i=1 (bi + εBB) } . The metric entropy of T is then deﬁned as Ent[T,ε] := log N(T,ε). Now, let Hk be the RKHS associated to kernel kwith the inner product ⟨·,·⟩k, and let PS be the projection operator from Hk to the following ﬁnite dimensional subspace FS = {φl,i : (l,i) ∈S}, where φl,i is deﬁned in Theorem 2 and dim(PS) = |S|. Our goal is to ﬁnd the optimal set S∗(with cardinality at most M), whose corresponding feature set maximizes the entropy. This is equivalent to 9solving the following optimization problem: sup S Ent[PS,ε] s.t. |S|≤ M. (9) Following the lines in the proof of Theorem 2 (in the supplementary), we can show that the features in FS are mutually orthogonal with Hilbert norm: ||φl,i||2 Hk =: C−1 l,i , (10) where Cl,i →∞ as |l|→∞ (see lemma 1 in Supplementary Material). We ﬁrst multiply φl,i by C 1 2 l,i to normalize the feature. For any function f ∈Hk, we then have PSf = ∑ (l,i)∈S Cl,i⟨f,φl,i⟩kφl,i. As a result, the entropic optimization problem (9) is equivalent to searching an M-dimensional Euclidean space with the largest unit ball, which can be characterized as follows max S ∑ (l,i)∈S Cl,i s.t.|S|≤ M. This optimization problem is called the Knapsack problem and, in general, is NP-hard (11). However, for some speciﬁc values of M, closed form solutions exist. Consider the Laplace kernel here as an example. For Laplace kernel k(x,x′) = e−ω∥x−y∥1 , from direct calculation, the constant is: Cl,i = D∏ d=1 sinh(ω2−ld). In this case, Cl = Cl,i is independent of i and for any |l|<|l′|, the value Cl >Cl′. Therefore, we can derive that when M = |{l : |l|<n}|for some n, the optimal set S∗ n is S∗ n = {(l,i) : |l|≤ n,i ∈Bl} (11) because for any Cl ∈S∗ n and any Cl′ ̸∈S∗ n, Cl >Cl′. It turns out the set S∗ n is equivalent to the Sparse Grid design (10). 104.1 Algorithm: Entropic Optimal Features With the aforementioned theorems, we can now describe the algorithm to compute the regression function associated to a kernel that satisﬁes Condition 1. Suppose the set S∗ n given by equation (11) is the index set associated to the feature functions that maximizes the entropy optimization problem (9). So given a speciﬁc input x, we aim to compute the vector z(x) = [Cl,iφl,i(x)](l,i)∈S∗n =: [zl,i(x)](l,i)∈S∗n where Cl,i is the coeffecient deﬁned in (10), z(x) is the approximation that satisﬁes k(x,x′) ≈z(x)Tz(x′) in Corollary 3 with φl,i the feature function deﬁned in equation (6). We call z(x) the entropic optimal feature (EOF). According to properties 1-3, the supports of{φl,i : (l,i) ∈S∗ n}are either disjoint or nested. Therefore, only a small amount of entries on z(x) are non-zero. To be more speciﬁc, given any l ∈ND and input x, the supports of {φl,i : i ∈Bl}are disjoint so we can immediately compute the unique non-zero entry zl,i(x). Algorithm 1 shows how to explicitly compute the EOF z(x) at a data point x. Note that ⌈·⌉,⌊·⌋ denote the ceiling and ﬂoor operations, respectively. Algorithm 1Entropic Optimal Features (EOF) Input: point x, S∗ n Initialize z(x) = [zl,i(x)](l,i)∈S∗n = 0 while |l|≤ n+ D−1 do for d= 1 to Ddo id = { ⌈xd 2−ld ⌉if ⌈xd 2−ld ⌉ is odd ⌊xd 2−ld ⌋if ⌊xd 2−ld ⌋ is odd end for zl,i(x) = Cl,iφl,i(x) end while The dimension of the vector z(x) given nlevels is O(2nnD−1) (10). The number of non-zero elements 11for z(x) after running Algorithm 1 is: ∑ |l|≤n+D−1 1 = n+D−1∑ i=D ∑ |l|=i 1 = n+D−1∑ i=D (i−1 D−1 ) = (n+ D−1 D ) = O(nD), which means fraction of non-zeros to the whole vector in z(x) grows with O( n 2n ) as a function of level n. Time Complexity of EOF in Regression:Based on above, if we ﬁx M as the size of z(x), the number of non-zero entries on z(x) is O(logD M). Since we evaluate z(x) for each training data, the feature matrix has O(nlogD M) non-zero elements, resulting in a training cost of O(Nlog2D M), which is smaller than O(NM2) of random features (3), especially when Dis moderate. 5 Generalization Bound In this section, we present the generalization bound for EOF when it is used in supervised learning. Let us deﬁne the approximated target function as ˆf := argmin f∈FM 1 N N∑ j=1 L(yi,f(xi)) + λ∥f∥2 k, given independent and identically distributed samples{(xi,yi)}N i=1, where FM denotes the space spanned by the ﬁrst M EOFs; L is a loss function; and λ is a tuning parameter that may depend on n. We denote by R(f) := Ex,y[L(y,f(x))] the true risk. The goal is to bound the generalization error R( ˆf) − inff∈Hk R(f). We use the following assumptions to establish the bound: Assumption 1. There exists f0 ∈Hk so that inff∈Hk R(f) = R(f0). Assumption 2. The function my(·) := L(y,·) is twice differentiable for all y. Furthermore, my(·) is strongly convex. 12Assumption 3. The density function of input x is uniformly bounded away from inﬁnity. The outputs are uniformly bounded. Assumption 1 allows inﬁmum to be achieved in the RKHS. This is not ensured automatically since we deal with a potentially inﬁnite-dimensional RKHS Hk, that is possibly universal (see Remark 2 of (12)). Assumption 2 is true for common loss functions including least squares for regression (my(y′) = (y−y′)2) and logistic regression for classiﬁcation (my(y′) = log[1 + exp(−yy′)]). The bounded output constraint of Assumption 3 is also common in supervised learning. The generalization bound is given by the following theorem. Theorem 4. Suppose Assumptions 1-3 are fulﬁlled. If the tuning parameter is choosing to have λ ∼ N−1/2, then R( ˆf) −inf f R(f) ≤Op(N−1/2) + CM−2 log4D−4 M, for some C >0. The constants may depend on ∥f0∥k. The theorem above shows that with O(N 1 4 ) EOFs, the optimal statistical accuracy O(1/ √ N) is achieved up to logarithmic factors. Compared to random features for kernel approximation, this result improves the generalization bound. For random features, the number of required features to achieve the optimal rate is O( √ N) in the case of ridge regression (12). 6 Related Literature We provide related works for kernel approximation from different perspectives: Random Features (Randomized Kernel Approximation):Randomized features was introduced as an elegant approach for Monte Carlo approximation of shift-invariant kernels (3), and it was later extended for Quasi Monte Carlo approximation (13). Several methods consider improving the time cost of random features, decreasing it by a linear factor of the input dimension (see e.g., Fast-food (14, 15)). Quadrature- based random features are also shown to boost kernel approximation (16). The generalization properties 13of random features have been studied for ℓ1-regularized risk minimization (17) and ridge regression (12), improving the initial generalization bound of (18). (19) develop orthogonal random features (ORF) to boost the variance of kernel approximation. ORF is shown to provide optimal kernel estimator in terms of mean-squared error ( 20). A number of recent works have considered data-dependent sampling of random features to improve kernel approximation. Examples consist of (21) on compact nonlinear feature maps, (15,22) on approximation of shift-invariant/translation-invariant kernels, and (23) on data-dependent approximation using greedy approaches (e.g., Frank-Wolfe). Furthermore, data-dependent sampling has been used to improve generalization in supervised learning (5, 7) through target kernel alignment. Deterministic Kernel Approximation:The studies on ﬁnding low-rank surrogates for kernels date back two decades (1, 2). As an example, the celebrated Nystr ¨om method (24, 25) samples a subset of training data for approximating a low-rank kernel matrix. The Nystr¨om method has been further improved in (26) and more recently used for approximation of indeﬁnite kernels (27). Explicit feature maps have also proved to provide efﬁcient kernel approximation. The works of (28–30) have proposed low-dimensional Taylor expansions of Gaussian kernel for improving the time cost of learning. (31) further study explicit feature maps for additive homogeneous kernels. Sparse Approximation Using Greedy Methods:Sparse approximation literature has mostly focused on greedy methods. (32) have developed a matching pursuit algorithm where kernels are the dictionary elements. The work of (33) focuses on sparse regression and classiﬁcation models using Mercer kernels, and (34) considers sparse regression with multiple kernels. Classical matching pursuit was developed for regression, but further extensions to logistic regression (35) and smooth loss functions (36) have also been studied. (37) propose a greedy reconstruction technique for regression by empirically ﬁtting squared error residuals. (38) also use greedy methods for sparse approximation using multiple kernels. Our approach is radically different from the prior work in the sense that we characterize a set of features that maximize the entropy. Our feature construction and entropy optimization techniques are novel and have not been explored in the kernel approximation literature. 147 Numerical Experiments Benchmark Algorithm: We now compare EOF with the following random-feature benchmark algo- rithms on several datasets from the UCI Machine Learning Repository: 1) RKS(18) with approximated Laplace kernel feature z(x) = 1√ M [cos(xTγm + bm)]M m=1, where {γm}M m=1 are sampled from a Cauchy distribution multiplied by σ, and {bm}M m=1 are sampled from the uniform distribution on [0,2π]. 2) ORF (19) with approximated Gaussian kernel feature z(x) = 1√ M [cos(xTγm + bm)]M m=1, with [γ1 γ2 ···γm] = σSQ where S is a diagonal matrix, with diagonal entries sampled i.i.d. from the χ-distribution with ddegrees and Q is the orthogonal matrix obtained from the QR decomposition of a matrix G with normally distributed entries. Note that ORF approximates a Gaussian kernel. 3) LKRF(5) with approximated Laplace kernel feature z(x) = 1√ M [cos(xTγm + bm)]M m=1, with ﬁrst a larger number M0 random features are sampled and then re-weighted by solving a kernel alignment optimization. The top M random features would be used in the training. 4) EERF(7), with approximated Laplace kernel feature z(x) = 1√ M [cos(xTγm + bm)]M m=1,where ﬁrst a larger number M0 random features are sampled and then re-weighted according to a score function. The top M random features would appear in the training. Experiment Setup:We also use approximated Laplace kernel feature z(x) = [φl,i(x)](l,i)∈S∗n where φl,i = ∏D d=1 φld,id with φld,id deﬁned as equation (4). To determine the value of σused in RKS, EERF, LKRF and ORF we choose the value of σ−1 for each dataset to be the mean distance of the 50th ℓ2 nearest neighbor (19). We then calculate the corresponding ωfor EOF associated to σ. The number of features in EOF is a function of dimension Dand level n, so it is not possible to calculate them for any M. To resolve this issue, for any given M, we select the set S∗ n deﬁned in (11) that satisﬁes ⏐⏐S∗ n−1 ⏐⏐<M ≤ ⏐⏐S∗ n ⏐⏐ and randomly select M pairs of (l,i) ∈S∗ n to have a random set SM . We then use the following 15Table 1: Input dimension, number of training samples, and number of test samples are denoted by D, Ntrain, and Ntest, respectively D ATA SET TASK D N TRAIN N TEST MNIST C LASSIFICATION 32 20000 10000 E LECTRICAL G RIDS S TABILITY C LASSIFICATION 13 7000 3000 S UPERCONDUCTIVITY R EGRESSION 81 15000 6263 E NERGY E FFICIENCY R EGRESSION 8 512 256 approximated feature: zM (x) := [φl,i(x)](l,i)∈SM . This is equivalent to randomly select M rows from the feature z(x) = [φl,i(x)](l,i)∈S∗n. We let M0 = 10M for LKRF and EERF, then for any M, we compare the performance of different algorithms. Datasets: In Table 1, we report the number of training samples Ntrain and test samples Ntest used for each dataset. For the MNIST data set, we map the original 784−dimensional data to a 32−dimensional space using an auto-encoder. If the training and test samples are not provided separately for a dataset, we split it randomly. We standardize the data as follows: we scale each input to the unit interval [0,1] and the responses in regression to be inside [−1,1]. Comparison: For a ﬁxed number of features, we perform 50 simulation runs for each algorithm on each data set. We then report the average test error (with standard errors) in Fig. 3 where the plot line is the mean error of an algorithm and the error bar reﬂects the standard deviation of the error. Throughout our experiments, we can see that EOF consistently improves the test error compared to other randomized- feature algorithms. This is speciﬁcally visible when the gap between SM and S∗ n becomes very small and, due to the optimality of S∗ n, EOF outperforms any random feature algorithm. In Table 2, we also compare the time complexity and space complexity. We deﬁne the feature matrix F := [z(xi)]N i=1, 16Figure 3: Comparison of the test error of EOF (this work) versus benchmark algorithms inclduing RKS, EERF, LKRF and ORF. 20 30 40 50 60 70 80 Number of Features (M) 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1 Test Error MNIST RKS EERF LKRF ORF EOF(this work) 20 25 30 35 40 45 50 55 60 Number of Features (M) 0 0.02 0.04 0.06 0.08 0.1 Test Error Electrical Grid Stability RKS EERF LKRF ORF EOF(this work) 80 90 100 110 120 130 140 150 160 Number of Features (M) 0.074 0.076 0.078 0.08 0.082 0.084 0.086 0.088 0.09 Test Error Superconductivity RKS EERF LKRF ORF EOF(this work) 10 15 20 25 30 35 40 45 50 55 60 Number of Features (M) 0 0.02 0.04 0.06 0.08 0.1 0.12Test Error Energy Efficiency RKS EERF LKRF ORF EOF(this work) which is an M ×N matrix with M the number of features and N the number of data. Due to the sparse structure of EOF, we can also see that the number of non-zero entries of the F associated to EOF is smaller than other methods. When both the dimension Dand the size of data N are large, the sparsity of EOF becomes more obvious as shown in the case of MNIST. The time cost of running EOF is also quite impressive. It is consistently better than EERF and LKRF and comparable and slightly slower than RKS. In fact, the major time for EOF is spent on feature matrix construction. For random features, due to high efﬁciency of matrix operations in Matlab, feature construction is fast. However, for EOF the feature construction via matrix operations is not possible in an efﬁcient way. We observed that after the feature matrix construction, EOF is the fastest method in training. For example, if we only count the training time (excluding feature construction) as the time cost, in kernel ridge regression on the dataset Superconductivity, the comparison between RKS and EOF is as follows: The run time is obtained on a MacPro with a 4-core, 3.3 GHz Intel Core i5 CPU and 8 GB of RAM (2133Mhz). 17Table 2: Time and Space Complexity Comparison MNIST Method M M0 Ttrain nnz(F) RKS 80 1.64 1.6 × 106 EERF 80 800 4.43 1.6 × 106 LKRF 80 800 3.07 1.6 × 106 ORF 80 1.21 1.6 × 106 EOF 80 2048 2.45 2.5 × 105 Superconductivity Method M M0 Ttrain nnz(F) RKS 160 0.10 2.4 × 106 EERF 160 1600 0.45 2.4 × 106 LKRF 160 1600 0.37 2.4 × 106 ORF 160 0.13 2.4 × 106 EOF 160 161 0.14 1.2 × 106 Electrical Grids Stability Method M M0 Ttrain nnz(F) RKS 60 0.04 4.2 × 105 EERF 60 600 0.14 4.2 × 105 LKRF 60 600 0.13 4.2 × 105 ORF 60 0.06 4.2 × 105 EOF 60 338 0.08 1.3 × 105 Energy Efﬁciency Method M M0 Ttrain nnz(F) RKS 60 0.01 6.1 × 103 EERF 60 600 0.05 6.1 × 103 LKRF 60 600 0.06 6.1 × 103 ORF 60 0.02 6.1 × 103 EOF 60 128 0.03 1.0 × 103 Table 3: Comparison on RKS and EOF in pure training excluding feature construction. M= 80 M= 100 M= 120 M= 140 M= 160 RKS 2×10−3 3×10−3 4×10−3 5×10−3 6×10−3 EOF 2×10−3 2×10−3 2×10−3 2×10−3 2×10−3 8 Conclusion We provide a method to construct a set of mutually orthogonal features (with nested and small supports) and select the best M of them that maximize the entropy of the associated projector. The nested and compact support of feature functions greatly reduces the time and space cost for feature matrix operations. The orthogonality and entropic optimality reduces dramatically the error of approximation. We have provided generalization error bound which indicates that only O(N 1 4 ) features are needed to achieve 18the O(N−1 2 ) optimal accuracy. Future directions include generalizing this method to a broader class of kernels. Supplementary Material A Proof of Theorem 1 The kernel function: k(x,y) = p(min{x,y})q(max{x,y}) is in fact the Green’s function of the Sturm-Liouville operator (39) L:= d dxα(x) d dx + β(x) So the inner product product induced by kis ⟨f,g⟩k = ∫ 1 0 fLgdx For any l∈N and i̸= j, the supports of φl,i and φl,j are [(i−1)2−l,(i+ 1)2−l] and [(j−1)2−l,(j+ 1)2−l]respectively. This two supports are disjoint because both iand j are odd so ⟨φl,i,φl,j⟩k = 0 if i̸= j. For any l,n ∈N and any i,j, the supports supt[φl,i] and supt[φn,j] are either disjoint or nested. If they are disjoint, then ⟨φn,j,φn,j⟩k = 0. If they are nested, , without loss of generality assume l>n and i≤j2l−n, then because both pand qsatisfy: Lp= Lq= 0 so ⟨φl,i,φn,j⟩k = ∫ (i+1)2−l (i−1)2−l φn,jLφl,idx = ∫ (i+1)2−l (i−1)2−l φn,jLp(x)ql,i−1 −q(x)pi,l−1 pl,iql,i−1 −ql,ipl,i−1 dx = 0. 19As a result, we have ⟨φl,i,φn,j⟩k = λl,iδ(l,i),(n,j) where λl,i is a function of land i. B Proof of Theorem 4 We need the following lemmas. Lemma 5. Denote fM = argminf∈FM ∥f0 −f∥k. Then we have R(fM ) −R(f0) ≤CM−2 log4D−4 M∥f0∥2 k, for some constant C. Proof. According to Assumption 2, we can see that R(fM ) −R(f0) = E[m′′ y(u∗)(fM (x) −f0(x))2] In view of Assumption 3, we only need to prove ∥fM −f0∥2 L2 = CM−2 log4D−4 M∥f0∥2 k for any f0 ∈Hk we then can ﬁnish the proof. Let M = |{(l,i) : |l|≤ n,i ∈Bl}|. According to theorem 2, we have the following expansion: ∥fM −f0∥L2 = ∥ ∑ |l|>n ∑ i∈Bl ⟨f0, φl,i ∥φl,i∥k ⟩k φl,i(·) ∥φl,i∥k ∥L2 = ∥ ∑ |l|>n ∑ i∈Bi ∫ Sl,i f0(s)Lφl,i(s)ds φl,i(·) ∥φl,i∥2 k ∥L2 . where Sl,i is the support of φl.i. We let v(·)l := ∑ i∈Bi ∫ Sl,i f0(s)Lφl,i(s)ds φl,i(·) ∥φl,i∥2 k . 20Our ﬁrst goal is to estimate vl. From theorem 2 of ( 40) or direct calculation based on the property of Green’s function, we can see that for anyf ∈Hk: ∫ Sl,i f(s)Lφl,i(s)ds = [ D⨂ d=1 ∆ld,id]f where ∆ld,idf := αld,idf ⏐⏐ xd=zld,id −βld,id−1f ⏐⏐ xd=zld,id−1 −βld,id+1f ⏐⏐ xd=zld,id αl,i = pl,i+1ql,i−1 −pl,i−1ql,i+1 [pl,i+1ql,i −pl,iql,i+1][pl,i1ql,i−1 −pl,i−1ql,i] βl,i = 1 pl,i+1ql,i −pl,iql,i+1 and ⨂denotes the tensor product of the ∆l,i operators. Since bouth q and pare the solution of the SL-equation, therefore, p,q are twice differentiable. We have 1 pl,i+1ql,i −pl,iql,i+1 = 2l [pl,i+1ql,i −pl,iql,i]/2−l −[pl,iql,i+1 −pl,iql,i]/2−l ∼ 2l p′ l,iql,i −pl,iq′ l,i we notice that p′ l,iql,i −pl,iq′ l,i is the Wronskian of the SL-operator, which is bounded away from 0. Therefore, ∆ld,id acting on f has the following approximation: ∆ld,idf ∼ [2f ⏐⏐ xd=zld,id −f ⏐⏐ xd=zld,id−1 −f ⏐⏐ xd=zld,id+1 ] 2−l ≤C max j=1,−1 { |f ⏐⏐ xd=zld,id+j −f ⏐⏐ xd=zld,id | 2−l }. As a result, ⨂D d=1 ∆ld,id acting on f has the following approximation: D⨂ d=1 ∆ld,idf ≤C D∏ d=1 max j=1,−1 { |f ⏐⏐ xd=zld,id+j −f ⏐⏐ xd=zld,id | 2−l }. 21From the same reasoning, we can see that ∥φl,i∥2 k = D∏ d=1 αld,id ∼2|l|. We also Taylor expandφld,id for each 1 ≤d≤Dup to second order and from direct calculation, we can have φld,id(x) ∼max{0,1 −|x−zld,id| 2−ld }+ O(2−ld). This gives us the approximation up to second order: ∥φl,i∥2 L2 = ∫ Sl,i D∏ d=1 φ2 ld,id(sd)ds ∼ ∫ Sl,i D∏ d=1 [max{0,1 −|s−zld,id| 2−ld }]2s = (2 3 )D2−|l|= (1 3 )DV ol(Sl,i). Therefore, we can have the following estimate for vl: ∥vl∥L2 = ∥ ∑ i∈Bi ∫ Sl,i f0(s)Lφl,i(s)ds φl,i(·) ∥φl,i∥2 k ∥L2 ≤ ⏐⏐⏐2−2|l|C ∑ i∈Bi [ D⨂ d=1 ∆ld,idf]2V ol(Sl,i) ⏐⏐⏐ 1 2 ∼2−|l|∥ D∏ d=1 ∂ ∂xd f0∥L2 ∼2−|l|∥f0∥k where the second line is from the fact that supports of {φl,i : i ∈Bl}are disjoint, the third line is from the Riemann integral approximation and the last line is from the energy estimate assumption of SL-operator 22(see, for instance, section 6.2.2 of (8)). Finally, we have: ∥f0 −fM ∥L2 ≤ ∑ |l|>n ∥vl∥L2 ∼∥f0∥k ∑ |l|>n 2−|l| = ∥f0∥k ∑ i>n 2−i ∑ |l|=i 1 = ∥f0∥k ∑ n>i 2−i (i−1 d−1 ) ∼∥f0∥k2−nnD−1 where the identity of the last line can be veriﬁed in (41). From (10) we also have M = O(2nnD−1) we can substitute this identity to the previous equation to have the ﬁnal result. The (ϵ,L∞)-covering number of a function space F, denoted as N(ϵ,F,∥·∥L∞), is deﬁned as the smallest number N0, so that there exist centers f1,...,f N0 , and for each f ∈F, there exists fi so that ∥f −fi∥L∞ <ϵ. Lemma 6. The covering number of the unit ball of Hk, denoted as F:= {f ∈Hk : ∥f∥k ≤1}, is bounded as follows: N(ϵ,F,∥·∥L∞) = O(1 εlogD−1 2 1 ε) Proof. When k(x,y) = e−ω∥x−y∥1 or k(x,y) = ∏D d=1 min{xd,yd}, then Hk is equivalent to the Sobolev space of mixed ﬁrst derivative H1 mix([0,1]D) (41). According to 6.6 of (42), we can immediately derive the result. When kernel kis different than these two, the energy property of an SL-operator requires that ⟨f,f ⟩k = ∫ [0,1]D f(x)[ D∏ d=1 L]f(x)d(x) ≤C ∫ [0,1]D | D∏ d=1 ∂ ∂xd f|2dx 23which means Hk can be embedded on H1 mix. Therefore, the covering number of Hk must be bounded by that of H1 mix. Lemma 7 shows the the function classes associated with the learning problem are Donsker. We refer to (43) for the deﬁnition and properties of Donsker classes. Lemma 7. Let P be the probability measure of (x,y). The space GR is P-Donsker for each R> 0. Proof. In view of Theorem 2.5.6 of (43), it sufﬁces to prove that ∫ ∞ 0 √ log N[](ϵ,GR,∥·∥L2(P))dϵ< ∞, where N[](ϵ,GR,∥·∥ L2(P)) is the covering number with bracketing deﬁned as follows. For function g : RD ×R →R, its L2(P) norm is deﬁned as [E[g(x,y)]2]1/2. Given functions gL,gU such that gL(u,v) ≤gU (u,v) for each (u,v), deﬁne the bracket [gL,gU ] as the set of functions {g : gL(u,v) ≤ g(u,v) ≤gU (u,v)}. The covering number with bracketing N[](ϵ,GR,∥·∥L2(P)) is the smallest number N0 so that there exist brackets [gL,1,gU,1],..., [gL,N0 ,gU,N0 ], such that ∪N0 i=1[gL,i,gU,i] ⊃ GR, and ∥gU,i −gL,i∥L2(P) ≤ϵfor all i. Let FR = {f : ∥f∥k <R}. We start with the centers f1,...,f N0 with N0 = N(ϵ,FR,∥·∥L∞) = N(ϵ/R,F1,∥·∥L∞) so that for each f ∈FR, there exists fi =: ξ(f) such that ∥f−fi∥L∞ <ϵ. To bound the covering number with bracketing, we need to construct the associated brackets. The reproduction property implies that ∥f∥L∞ ≤∥f∥k. Then for any f ∈FR, by mean value theorem, |L(y,f(x)) −L(y,ξ(f)(x))| ≤ sup |u|<R ⏐⏐⏐⏐ ∂L ∂u(y,u) ⏐⏐⏐⏐ϵ=: S(y)ϵ. Now we deﬁne gL,i(u,v) = L(v,fi(u)) −S(v)ϵ and gU,i(u,v) = L(v,fi(u)) + S(v)ϵ. Clearly gL,i ≤gU,i and ∥gU,i −gL,i∥L2(P) = 2ϵ[E[S(y)]2]1/2, 24which is a multiple of ϵaccording to Assumptions 2-3. Besides, (12) implies that for all f such that ∥f −fi∥L∞ <ϵ, L(v,f(u)) ∈[gL,i,gU,i]. So we invoke Lemma 6 to ﬁnd that N[](2ϵ[E[S(y)]2]1/2,∥·∥L2(P)) = O(1 εlogD−1 2 1 ε), which implies the desired result. To bound the generalization error, we observe R( ˆf) −R(f0) = { R( ˆf) − 1 N N∑ i=1 L(yi, ˆf(xi)) } + { 1 N N∑ i=1 L(yi, ˆf(xi)) − 1 N N∑ i=1 L(yi,fM (xi)) } + { 1 N N∑ i=1 L(yi,fM (xi)) −R(fM ) } + {R(fM ) −R(f0)}=: I1 + I2 + I3 + I4. We will bound I1 and I3 with a uniform error bound of empirical processes. For I2, we have I2 ≤λ∥fM ∥2 k −λ∥ˆf∥2 k ≤λ∥f0∥2 K = O(N−1/2)∥f0∥2 k, where the ﬁrst inequality follows from the optimality condition 1 N N∑ i=1 L(yi, ˆf(xi)) + λ∥ˆf∥2 k ≤ 1 N N∑ i=1 L(yi,fM (xi)) + λ∥fM ∥2 k. The term I4 is bounded by Lemma 5. Now we turn to I1 and I3. To show that I1 = Op(N−1/2) and I3 = Op(N−1/2), it sufﬁces to show that the functions L(y, ˆf(x)) and L(y,fM (x)) fall in a Donsker class (43) with probability arbitrarily close to one. For L(y,fM (x)), this is clearly true in view of Lemma 7 and the fact that ∥fM ∥k ≤∥f0∥k. Therefore, I3 = Op(N−1/2) For L(y, ˆf(x)), it sufﬁces to prove that ∥ˆf∥k = Op(1). To show this result, we start with the optimality condition 1 N N∑ i=1 L(yi, ˆf(xi)) + λ∥ˆf∥2 k ≤ 1 N N∑ i=1 L(yi,fM (xi)) + λ∥fM ∥2 k. 25In view of Assumption 2, we can write L(y,f(x)) −L(y,f0(x)) = W ·(f(x) −f0(x)) + m′′ y(u∗)(f(x) −f0(x))2, where W = m′ y(f0(x)), and u∗lies between f(x) and f0(x). Assumptions 2 and 3 implies that the derivative and the expectation are interchangeable, so that 0 = (Emy(f0(X)))′= Em′ y(f0(X)) = EW. We then invoke Assumption 2 to ﬁnd λ∥ˆf∥2 k ≤− 1 N N∑ i=1 Wi( ˆf(xi) −f0(xi)) + { 1 N N∑ i=1 L(yi,fM (xi)) − 1 N N∑ i=1 L(yi,f0(xi)) } − V( ˆf(xi) −f0(xi))2 + λ∥f0∥2 k =: J1 + J2 + J3 + J4, (12) for some V >0 due to the strong convexity of my(·). For the ﬁrst term, we have J1 ≤ (∥ˆf∥k + 1) sup f∈Hk 1 N N∑ i=1 −Wi f(xi) −f0(xi) ∥f∥K + 1 = ( ∥ˆf∥k + 1)Op(N−1/2), where the last step follows from the fact that EWi = 0, Wi is bounded, and Lemma 3.4.3 of (43) and the fact that ∥f−f0∥k/(∥f∥k +1) = O(1). Clearly, we haveJ2 = I3 +Op(N−1/2) = Op(N−1/2) according to the central limit theorem. The third term is clearly non-positive. We also have J4 = Op(N−1/2) by assumption for λ. Now we conclude from (12) that λ∥ˆf∥2 k ≤∥ˆf∥kOp(N−1/2) + Op(N−1/2), which implies ∥ˆf∥k = Op(1). This completes the proof. 26References and Notes 1. A. J. Smola and B. Sch ¨okopf, “Sparse greedy matrix approximation for machine learning,” in Proceedings of the Seventeenth International Conference on Machine Learning, 2000, pp. 911–918. 2. S. Fine and K. Scheinberg, “Efﬁcient SVM training using low-rank kernel representations,”Journal of Machine Learning Research, vol. 2, no. Dec, pp. 243–264, 2001. 3. A. Rahimi and B. Recht, “Random features for large-scale kernel machines,” inAdvances in neural information processing systems, 2008, pp. 1177–1184. 4. T. Joachims, “Training linear SVM’s in linear time,” in Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2006, pp. 217–226. 5. A. Sinha and J. C. Duchi, “Learning kernels with random features,” inAdvances In Neural Information Processing Systems, 2016, pp. 1298–1306. 6. H. Avron, M. Kapralov, C. Musco, C. Musco, A. Velingker, and A. Zandieh, “Random fourier features for kernel ridge regression: Approximation bounds and statistical guarantees,” in Proceedings of the 34th International Conference on Machine Learning-Volume 70, 2017, pp. 253–262. 7. S. Shahrampour, A. Beirami, and V . Tarokh, “On data-dependent random features for improved generalization in supervised learning,” in Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018. 8. L. C. Evans, Partial differential equations. Providence, R.I.: American Mathematical Society, 2010. 9. J. Dick, F. Kuo, and I. Sloan, “High-dimensional integration: The quasi-monte carlo way,” Acta Numerica, vol. 22, 05 2013. 10. H.-J. Bungartz and M. Griebel, “Sparse grids,”In: Acta Numerica. Vol. 13, pp. 147-269, vol. 13, 05 2004. 2711. H. Kellerer, U. Pferschy, and D. Pisinger, Knapsack Problems, 01 2004. 12. A. Rudi and L. Rosasco, “Generalization properties of learning with random features,” inAdvances in Neural Information Processing Systems, 2017, pp. 3218–3228. 13. J. Yang, V . Sindhwani, H. Avron, and M. Mahoney, “Quasi-monte carlo feature maps for shift-invariant kernels,” inInternational Conference on Machine Learning, 2014, pp. 485–493. 14. Q. Le, T. Sarl´os, and A. Smola, “Fastfood-approximating kernel expansions in loglinear time,” in International Conference on Machine Learning, vol. 85, 2013. 15. Z. Yang, A. Wilson, A. Smola, and L. Song, “A la carte–learning fast kernels,” inArtiﬁcial Intelligence and Statistics, 2015, pp. 1098–1106. 16. M. Munkhoeva, Y . Kapushev, E. Burnaev, and I. Oseledets, “Quadrature-based features for kernel approximation,” in Advances in Neural Information Processing Systems, 2018, pp. 9147–9156. 17. I. E.-H. Yen, T.-W. Lin, S.-D. Lin, P. K. Ravikumar, and I. S. Dhillon, “Sparse random feature algorithm as coordinate descent in hilbert space,” in Advances in Neural Information Processing Systems, 2014, pp. 2456–2464. 18. A. Rahimi and B. Recht, “Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning,” in Advances in Neural Information Processing Systems, 2009, pp. 1313– 1320. 19. X. Y . Felix, A. T. Suresh, K. M. Choromanski, D. N. Holtmann-Rice, and S. Kumar, “Orthogonal random features,” in Advances in Neural Information Processing Systems, 2016, pp. 1975–1983. 20. K. Choromanski, M. Rowland, T. Sarl´os, V . Sindhwani, R. Turner, and A. Weller, “The geometry of random features,” in International Conference on Artiﬁcial Intelligence and Statistics, 2018, pp. 1–9. 2821. F. X. Yu, S. Kumar, H. Rowley, and S.-F. Chang, “Compact nonlinear maps and circulant extensions,” arXiv preprint arXiv:1503.03893, 2015. 22. J. B. Oliva, A. Dubey, A. G. Wilson, B. P´oczos, J. Schneider, and E. P. Xing, “Bayesian nonparametric kernel-learning,” inArtiﬁcial Intelligence and Statistics, 2016, pp. 1078–1086. 23. R. Agrawal, T. Campbell, J. Huggins, and T. Broderick, “Data-dependent compression of random features for large-scale kernel approximation,” inThe 22nd International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2019, pp. 1822–1831. 24. C. Williams and M. Seeger, “Using the Nystr¨om method to speed up kernel machines,” inAdvances in Neural Information Processing Systems, 2001. 25. P. Drineas and M. W. Mahoney, “On the Nystr ¨om method for approximating a gram matrix for improved kernel-based learning,”Journal of Machine Learning Research, vol. 6, no. Dec, pp. 2153– 2175, 2005. 26. K. Zhang, I. W. Tsang, and J. T. Kwok, “Improved Nystr ¨om low-rank approximation and error analysis,” in International Conference on Machine Learning. ACM, 2008, pp. 1232–1239. 27. D. Oglic and T. G ¨artner, “Scalable learning in reproducing kernel krein spaces,” in International Conference on Machine Learning, 2019, pp. 4912–4921. 28. C. Yang, R. Duraiswami, and L. Davis, “Efﬁcient kernel machines using the improved fast gauss transform,” inProceedings of the 17th International Conference on Neural Information Processing Systems, 2004, pp. 1561–1568. 29. J.-W. Xu, P. P. Pokharel, K.-H. Jeong, and J. C. Principe, “An explicit construction of a reproducing gaussian kernel Hilbert space,” inIEEE International Conference on Acoustics, Speech and Signal Processing, vol. 5, 2006. 2930. A. Cotter, J. Keshet, and N. Srebro, “Explicit approximations of the gaussian kernel,”arXiv preprint arXiv:1109.4603, 2011. 31. A. Vedaldi and A. Zisserman, “Efﬁcient additive kernels via explicit feature maps,”IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, no. 3, pp. 480–492, 2012. 32. P. Vincent and Y . Bengio, “Kernel matching pursuit,”Machine Learning, vol. 48, no. 1-3, pp. 165–187, 2002. 33. P. B. Nair, A. Choudhury, and A. J. Keane, “Some greedy learning algorithms for sparse regression and classiﬁcation with mercer kernels,” Journal of Machine Learning Research, vol. 3, no. Dec, pp. 781–801, 2002. 34. V . Sindhwani and A. C. Lozano, “Non-parametric group orthogonal matching pursuit for sparse learning with multiple kernels,” in Advances in Neural Information Processing Systems, 2011, pp. 2519–2527. 35. A. Lozano, G. Swirszcz, and N. Abe, “Group orthogonal matching pursuit for logistic regression,” in Artiﬁcial Intelligence and Statistics, 2011, pp. 452–460. 36. F. Locatello, R. Khanna, M. Tschannen, and M. Jaggi, “A uniﬁed optimization view on generalized matching pursuit and frank-wolfe,” inArtiﬁcial Intelligence and Statistics, 2017, pp. 860–868. 37. D. Oglic and T. G¨artner, “Greedy feature construction,” inAdvances in Neural Information Processing Systems, 2016, pp. 3945–3953. 38. S. Shahrampour and V . Tarokh, “Learning bounds for greedy approximation with explicit feature maps from multiple kernels,” in Advances in Neural Information Processing Systems , 2018, pp. 4695–4706. 3039. V . F. Zaitsev and A. D. Polyanin,Handbook of Exact Solutions for Ordinary Differential Equations, 2nd ed. CRC Press, 2002. 40. L. Ding and X. Zhang, “Scalable stochastic kriging with markovian covariances,” 2018. 41. L. Ding, S. Mak, and C.-F. Wu, “Bdrygp: a new gaussian process model for incorporating boundary information,” 08 2019. 42. D. Dung, V . Temlyakov, and T. Ullrich, “Hyperbolic cross approximation,” 01 2016. 43. A. W. van der Vaart and J. A. Wellner,Weak Convergence and Empirical Processes. Springer, 1996. 31",
      "meta_data": {
        "arxiv_id": "2002.04195v1",
        "authors": [
          "Liang Ding",
          "Rui Tuo",
          "Shahin Shahrampour"
        ],
        "published_date": "2020-02-11T04:12:31Z",
        "pdf_url": "https://arxiv.org/pdf/2002.04195v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Entropic Optimal Features (EOF), a novel low-rank kernel approximation method designed to address the high computational cost of kernel methods. EOF constructs a sparse set of mutually orthogonal features with nested and compact supports by maximizing the metric entropy among them. A key theoretical finding is a generalization bound demonstrating that EOF achieves optimal statistical accuracy (O(1/√N)) with only O(N^(1/4)) features, a significant improvement over O(√N) features required by random features. This sparsity dramatically reduces time and space costs, and numerical experiments confirm EOF's superior performance in test error compared to state-of-the-art kernel approximation techniques.",
        "methodology": "The methodology centers on constructing Entropic Optimal Features (EOF). It first characterizes a condition (Condition 1), based on the Sturm-Liouville problem and product-form kernels (e.g., Laplace, weighted Sobolev), under which features with compact, nested, and orthogonal supports can be explicitly derived (Theorems 1 and 2). This construction provides a sparse data representation. An optimization criterion is then proposed to select the best finite set of these features by maximizing the metric entropy of the associated projection operator. This optimization is framed as selecting M features that maximize the sum of their squared Hilbert norms (a Knapsack problem), which has closed-form solutions for specific M values (e.g., relating to Sparse Grid design for Laplace kernels). Algorithm 1 provides a detailed procedure for computing the sparse EOF vectors. The generalization properties of EOF in supervised learning are established through a theoretical bound under mild assumptions.",
        "experimental_setup": "EOF was compared against four benchmark random-feature algorithms: RKS, ORF, LKRF, and EERF. The Laplace kernel was used for RKS, LKRF, EERF, and EOF, while ORF approximated a Gaussian kernel. The kernel width parameter (σ or ω) was determined by the mean distance to the 50th nearest neighbor. For EOF, to ensure a fair comparison with a fixed number of features (M), M features were randomly selected from the larger entropy-optimal set (S*n) for evaluation. Experiments were conducted on four datasets from the UCI Machine Learning Repository: MNIST (classification, 32-dim), Electrical Grids Stability (classification), Superconductivity (regression), and Energy Efficiency (regression). Data inputs were scaled to [0,1], and regression responses to [-1,1]. Performance was assessed over 50 simulation runs, reporting average test error, standard deviation, time complexity (Ttrain), and space complexity (number of non-zero entries in the feature matrix). The hardware used was a MacPro with a 4-core, 3.3 GHz Intel Core i5 CPU and 8 GB of RAM.",
        "limitations": "The proposed method is currently restricted to kernels that satisfy Condition 1, which specifies a product form derivable from solutions to the Sturm-Liouville problem, limiting its general applicability. The feature selection problem, formulated as maximizing metric entropy, is a Knapsack problem, which is generally NP-hard, although closed-form solutions exist for particular configurations of feature numbers. For empirical evaluations requiring an arbitrary fixed number of features (M), the method resorts to randomly selecting features from a larger optimal set, meaning the full entropic optimality is not strictly maintained for all M. Practically, the feature matrix construction phase for EOF was observed to be slower than for random features, despite leading to sparser matrices, due to challenges in leveraging efficient matrix operations.",
        "future_research_directions": "A primary direction for future research is to generalize the Entropic Optimal Features method to encompass a broader class of kernels beyond those currently satisfying Condition 1 (i.e., product form based on Sturm-Liouville solutions)."
      }
    },
    {
      "title": "Test Time Adaptation via Conjugate Pseudo-labels",
      "abstract": "Test-time adaptation (TTA) refers to adapting neural networks to distribution\nshifts, with access to only the unlabeled test samples from the new domain at\ntest-time. Prior TTA methods optimize over unsupervised objectives such as the\nentropy of model predictions in TENT [Wang et al., 2021], but it is unclear\nwhat exactly makes a good TTA loss. In this paper, we start by presenting a\nsurprising phenomenon: if we attempt to meta-learn the best possible TTA loss\nover a wide class of functions, then we recover a function that is remarkably\nsimilar to (a temperature-scaled version of) the softmax-entropy employed by\nTENT. This only holds, however, if the classifier we are adapting is trained\nvia cross-entropy; if trained via squared loss, a different best TTA loss\nemerges. To explain this phenomenon, we analyze TTA through the lens of the\ntraining losses's convex conjugate. We show that under natural conditions, this\n(unsupervised) conjugate function can be viewed as a good local approximation\nto the original supervised loss and indeed, it recovers the best losses found\nby meta-learning. This leads to a generic recipe that can be used to find a\ngood TTA loss for any given supervised training loss function of a general\nclass. Empirically, our approach consistently dominates other baselines over a\nwide range of benchmarks. Our approach is particularly of interest when applied\nto classifiers trained with novel loss functions, e.g., the recently-proposed\nPolyLoss, where it differs substantially from (and outperforms) an\nentropy-based loss. Further, we show that our approach can also be interpreted\nas a kind of self-training using a very specific soft label, which we refer to\nas the conjugate pseudolabel. Overall, our method provides a broad framework\nfor better understanding and improving test-time adaptation. Code is available\nat https://github.com/locuslab/tta_conjugate.",
      "full_text": "Test-Time Adaptation via Conjugate Pseudo-labels Sachin Goyal⋆1 Mingjie Sun⋆1 Aditi Raghunathan1 Zico Kolter1,2 1Carnegie Mellon University, 2Bosch Center for AI {sachingo, mingjies, raditi, zkolter}@cs.cmu.edu Abstract Test-time adaptation (TTA) refers to adapting neural networks to distribution shifts, with access to only the unlabeled test samples from the new domain at test-time. Prior TTA methods optimize over unsupervised objectives such as the entropy of model predictions in TENT [50], but it is unclear what exactly makes a good TTA loss. In this paper, we start by presenting a surprising phenomenon: if we attempt to meta-learn the “best” possible TTA loss over a wide class of functions, then we recover a function that isremarkably similar to (a temperature-scaled version of) the softmax-entropy employed by TENT. This only holds, however, if the classiﬁer we are adapting is trained via cross-entropy loss; if the classiﬁer is trained via squared loss, a different “best” TTA loss emerges. To explain this phenomenon, we analyze test-time adaptation through the lens of the training losses’sconvex conjugate. We show that under natural conditions, this (unsupervised) conjugate function can be viewed as a good local approximation to the original supervised loss and indeed, it recovers the “best” losses found by meta-learning. This leads to a generic recipe that can be used to ﬁnd a good TTA loss for any given supervised training loss function of a general class. Empirically, our approach consistently dominates other TTA alternatives over a wide range of domain adaptation benchmarks. Our approach is particularly of interest when applied to classiﬁers trained with novel loss functions, e.g., the recently-proposed PolyLoss [25] function, where it differs substantially from (and outperforms) an entropy-based loss. Further, we show that our conjugate based approach can also be interpreted as a kind of self-training using a very speciﬁc soft label, which we refer to as the conjugate pseudo-label. Overall, our method provides a broad framework for better understanding and improving test-time adaptation. Code is available at https://github.com/locuslab/ tta_conjugate. 1 Introduction Modern deep networks perform exceeding well on new test inputs that are close to the training distribution. However, this performance dramatically decreases on test inputs drawn from a different distribution. While there is a large body of work on improving the robustness of models, most robust training methods are highly specialized to the setting they cater to. For e.g., they assume pre-speciﬁed perturbations, subpopulations, and spurious correlations, or access to unlabeled data from the target distribution, and most methods offer close to no improvement on general distribution shifts beyond what they were trained for [12, 21]. In practice, it is often cumbersome (or even impossible) to precisely characterize all possible distri- bution shifts a model could encounter and then train accordingly. Instead, a model already trained on some source data must be able to adapt at test-time to new inputs from a different domain. This setting of test-time adaptation (TTA) has gained interest in recent years [ 6, 47, 50, 54]. TTA is typically accomplished by updating the source model parameters via a few steps of optimization on an unsupervised objective involving the new test sample from the target distribution. The choice ⋆ Equal Contribution 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2207.09640v2  [cs.LG]  23 Nov 2022of this unsupervised objective, which we call the TTA loss, dictates the success of the adaptation procedure. [47] uses a self-supervised objective on the test sample, [50] uses the entropy of model predictions, and several follow-ups have proposed variants or alternatives [ 40, 54]. However, it remains unclear as to how to choose or guide the selection of this TTA loss, and thus far the choice of these losses has remained largely heuristic in nature. In this work, we begin by presenting a set of intriguing experiments where we attempt to learn the “best” TTA loss for a given source classiﬁer and distribution shift. We parameterize the TTA loss by another neural network whose parameters are learnt via meta-learning [ 3, 9] where we differentiate through the adaptation process to ﬁnd the TTA loss that achieves the best adaptation on distribution shifts. Surprisingly, we ultimately learn a TTA loss that looksremarkably similar to (a temperature-scaled version of) the softmax-entropy loss, which was already proposed by [50]. Why did we recover the commonly used softmax-entropy loss despite the fact that the procedure is capable of learning a very general class of losses and the meta-learning process could potentially specialize to both the source classiﬁer and the distribution shift of interest? Furthermore, we ﬁnd that this pattern only holds when the loss used to train the source classiﬁer is cross-entropy loss; when a different loss such as squared loss is used instead, the meta-learning procedure recovers a TTA loss that itself looks more like a negative squared error, and is very different from the softmax-entropy loss (Section 3). In order to explain this phenomenon, we propose to consider TTA through the lens of the convex conjugate function. Speciﬁcally, given a hypothesis function h(x) and label y, several common losses (cross-entropy and the squared loss amongst them, but not limited to these) can be written in the form L(h(x),y) = f(h(x)) −yTh(x) for some function f. In these cases, we show that “natural” TTA loss for such classiﬁers is precisely the (negation of) the convex conjugate evaluated at the gradient of h, LTTA(x) = −f∗(∇f(h(x)), where f∗is the convex conjugate of f. This framework not only recovers the results of our meta-learning experiments, but also justiﬁes why some speciﬁc choices of TTA loss in the previous literature work well (e.g., this framework recovers TENT’s choice of softmax-entropy for cross-entropy-trained classiﬁer). Moreover, it also provides a broad framework for what the TTA loss should be when the source model is trained using various different loss functions (for example the recently-proposed PolyLoss [25, 29]) as is becoming increasingly common in machine learning. Further, we show that our proposed conjugate adaptation loss is in fact a kind of self-training with pseudo-labels [42], a classic approach in machine learning. Various formulations of the pseudo-label have been proposed in the literature, and our conjugate analysis provides a general recipe for the “correct” choice of soft pseudo-labels given byˆy(x) = ∇f(h(x)). We thus refer to these as conjugate pseudo-labels (Conjugate PL’s), and believe our work provides a broad framework for understanding adaptation with unlabeled data in general. Finally, we empirically verify the effectiveness of our proposed conjugate adaptation loss across several datasets and training losses, such as cross-entropy and squared loss, along with the recently- proposed PolyLoss [ 25] (which itself has shown higher standard test accuracy on a wide range of vision tasks). Over all models, datasets and training losses, we ﬁnd our proposed conjugate pseudo-labeling consistently outperforms prior TTA losses and improves TTA performance over the current state of the art. 2 Background and preliminaries. Test-time adaptation. We are interested in mapping an input x∈Rd to a label y∈Y. We learn a model hθ : Rd ↦→R|Y|parameterized by θthat maps an input xto predictions hθ(x). We assume access to a trained source model and adapt at test-time over the test input, before making the ﬁnal prediction. This is the standard test-time adaptation (TTA) setting [47, 50]. During TTA, we update the model parameters on an unsupervised objective L(x,hθ). For example, in TENT [50], this loss is the entropy of the softmax-normalized predictions of the model. At each time step of adaptation, we observe a batch of test inputs and we take a gradient step towards optimizing the TTA loss on this test batch. As is standard, we measure the average online performance of models across all steps (number of test batch inputs seen) in the adaptation process. Meta learning the loss function. In order to explore the existence of different TTA losses, we employ the meta-learning procedure where we attempt to learn the TTA loss. We use a similar procedure as prior work on meta-learning loss functions [3, 37] and parameterize the loss function via a neural network mφ : R|Y| ↦→R that takes in the model predictions/logits and outputs a loss value. We want to learn parameter φsuch that when we update θvia the loss function mφ, our ﬁnal 2performance is optimal. In order to do so, let xbe the unlabeled test samples to adapt to, and ybe the corresponding labels. We update θand φalternatively as follows. θt+1 ←θt −α∂mφt(hθt(x)) ∂θt , φt+1 ←φt −β∂L(hθt+1 (x′),y′) ∂φt , (1) where Lis some supervised surrogate loss function such as cross-entropy. Please refer to Appendix A3 for further details regarding meta-learning setup. Note that the meta-learning process above assumes access to labels yof test inputs. In this paper, we do not propose meta-learning the TTA loss as an approach. Rather, we use meta-learning to explore what the “best” TTA losses look like. We discuss our ﬁndings from this exploration in the next section. 3 Test-time Adaptation via Meta-Learnt Losses The objective used in TENT is the softmax-entropy of the model predictions which essentially makes the classiﬁer more conﬁdent in its current predictions. The same can be achieved by various other loss formulations such as those mentioned in [40]. With so many possible choices for the loss function, what should we use for TTA? In this section, we attempt to answer this empirically and present some intriguing observations. (a)  (b) Figure 1: Visualization of meta loss (blue) by varying one input prediction score. (a) For cross-entropy loss trained model, the learnt meta loss can be approximated with a scaled softmax-entropy function (dashed red). (b) When the source model is trained with a squared loss for classiﬁcation, the learnt meta loss (blue) can be ﬁtted closely with a quadratic function (dashed red), shown in Figure 1b. The range (max/min) of the prediction score (logit) in x-axis is chosen to cover the empirical range of the predicted logits. Experiment 1. We learn the TTA loss parameterized by a neural network via meta-learning as described in Section 2. Our source classiﬁer is a ResNet-26 trained on CIFAR-10 and we adapt to distribution shifts in CIFAR-10-C. We use the 4 labeled validation noises in CIFAR-10-C to learn the meta-loss network parameters and we denote the resulting learnt loss function by meta-TTA loss. We then adapt the source classiﬁer to the test set of 15 corruptions by optimizing the meta-TTA loss. Observations. First, we ﬁnd that TTA using meta-TTA loss performs better than TENT (12.35% vs 13.14%), suggesting that there are better TTA losses than previous losses based on softmax-entropy. However, on examining this meta-TTA loss, we ﬁnd a surprising observation. Figure 1a (blue curve) visualizes the learnt meta-loss over model predictions as we vary a single class prediction with the rest ﬁxed. Qualitatively, the learnt meta-loss looks very similar to softmax-entropy in one dimension. In fact, we can ﬁt it closely with a scaled softmax-entropy function (dashed red curve): α·H(softmax(hθ(x)/T)), where αis a magnitude parameter and T is a temperature scaler. We want to test if the meta-loss is basically learning the softmax-entropy function. Hence, we perform test-time adaptation with the ﬁtted softmax-entropy function instead (dashed red curve) and achieve an error of 12.32%, essentially recovering the performance of meta-TTA. 3Despite the ability to represent many different loss functions and potentially specialize to the CIFAR- 10-C setting, the meta-loss procedure gave back the standard entropy objective.Do we always recover a loss that looks like softmax-entropy? Experiment 2. In an attempt to isolate when we get back the entropy objective, we vary several things. We tried different architectures for the source classiﬁer, different lossesLduring the meta- learning process (1) and different training losses for the source classiﬁer. Results. We observed that we consistently recovered the temperature scaled softmax-entropy function in all cases except when we varied the training loss for the source classiﬁer (Appendix A.10). On using the squared loss function [18], a strikingly different meta-TTA loss emerges. Figure 1b (blue curve) shows the learnt meta-loss (13.48% error) for this network. Here again, the meta-TTA loss outperforms entropy (14.57%) but it is not simply due to a scaling factor. The loss now looks like the negative squared error (red curve). Like previously, we tried ﬁtting a quadratic loss directly to the meta loss in Figure 1b, and this time we even slightly outperformed the meta-TTA loss. To summarize, we used a meta-learning procedure to search for the “best” TTA loss, where the loss itself was parameterized by a neural network that could potentially represent arbitrarily complex loss functions. However, we ended up with loss functions displaying remarkable structure: across different architectures and different variants of meta-learning, for a classiﬁer trained with cross-entropy, the meta-TTA loss was temperature scaled softmax-entropy and for a classiﬁer trained with squared loss, the meta-TTA loss was a negative squared loss. This is interesting from both a practical and conceptual standpoint where the “best” TTA loss depends on the loss used to train the source classiﬁer in a clean fashion. We attempt to understand and explain this phenomenon in the next section. 4 Conjugate Pseudo Labels Results in the previous section raise an obvious question: why does softmax-entropy as used in TENT seem to be the “best” possible test time adaptation loss for classiﬁers trained via cross-entropy (at least, best in the sense that meta-learning consistently recovers something which essentially mimics softmax-entropy, even though meta-loss is parameterized by a neural network and hence could learn much more complex functions speciﬁc to the model and the particular shift)? And why, alternatively, does a quadratic TTA loss seem to perform best when the classiﬁer is trained via squared loss? In this section, we offer an explanation of this phenomenon via the construct of the convex conjugate function [1]. As we will see, our method recovers softmax-entropy and quadratic loss as the “natural” objectives for classiﬁers trained via cross-entropy and squared loss respectively. Furthermore, for classiﬁers trained via other loss functions, as is becoming increasingly common in deep learning, our approach naturally suggests corresponding test-time adaptation losses, which we show in the next section to comparatively outperform alternatives. Thus, we argue that our framework overall provides a compelling recipe for specifying the “correct” method for TTA for a large class of possible losses. 4.1 Losses and the convex conjugate We begin by formally considering loss functions between a hypothesis outputhθ(x) (e.g., the logit outputs of a classiﬁer, or the direct prediction of a regressor) and targetythat take the following form L(hθ(x),y) = f(hθ(x)) −yThθ(x) (2) for some function f; when there is no risk of confusion, we will use hin place of hθ(x) for simplicity of notation. While not every loss can be expressed in such a form, this captures a wide variety of common losses (possibly scaled by a constant value). For example, cross-entropy loss corresponds to the choice f(h) = log ∑ iexp(hi) and where y denotes a one-hot encoding of the class label; similarly, squared loss corresponds to the choice f(h) = 1 2 ∥h∥2 2. When training an over-parameterized classiﬁer, we can roughly view the training process as (approxi- mately) attaining the minimum over hypotheses hfor each training example min θ 1 t t∑ i=1 L(hθ(xi),yi) ≈1 t t∑ i=1 min h L(h,yi) (3) 4where t is the number of training samples. However, in the case of losses in the form (2), the minimization over hin this form represents a very speciﬁc and well-known optimization problem: it is known as the convex conjugate [1] of the function f min h L(h,y) = min h {f(h) −yTh}= −f⋆(y) (4) where f⋆ denotes the convex conjugate of f. f⋆ is a convex function in y(and indeed, is convex regardless of whether or not f is convex). Furthermore, for the case that f is convex differentiable, the optimality condition of this minimization problem is given by ∇f(hopt) = y, so we also have that f⋆(y) = f⋆(∇f(hopt)) (5) where hopt refers to the optimal classiﬁer (used interchangeably with hθopt ). Putting this all together, we can state (admittedly, in a rather informal manner) that under the assumption that θopt is chosen so as to approximately minimize the empirical loss on the source data in the over-parameterized setting, we have that for tinputs 1 t t∑ i=1 L(hθopt (xi),yi) ≈1 t t∑ i=1 −f⋆(∇f(hθopt (xi))) (6) i.e., the empirical loss can be approximated by the (negative) conjugate applied to the gradient of the f, at least in a region close to the optimal θopt that minimizes the empirical loss. But the later expression has the notable beneﬁt that it does not require any label yi in order to compute the loss, and thus can be used as a basis for TTA on target domain of the hypothesis function hθopt . Deﬁnition 1 (conjugate adaptation loss) Consider a loss function that takes the form given in 2, used for training a hypothesis hθ in the over-parameterized regime. We deﬁne the conjugate adaptation loss Lconj(hθ(x)) : R|Y|↦→R as follows. Lconj(hθ(x)) = −f⋆(∇f(hθ(x))) = f(hθ(x)) −∇f(hθ(x))⊤hθ(x). (7) 4.2 Recovery of existing test-time adaptation strategies Cross-entropy The interesting aspect to this formalism is that when applied to classiﬁers trained with cross-entropy, it recovers exactly the TENT approach to TTA : minimizing the softmax-entropy of hθ(x). And indeed, this loss was also recovered when using meta-learning to learn the “optimal” test-time adaptation loss. To see this, note that for cross-entropy, we have thatf(h) = log ∑ iexp(hi), giving the optimality condition y= ∇f(hopt) = exp(hopt)∑ iexp(hopt i ) and the conjugate function f⋆(y) = { ∑ iyilog yi if ∑ iyi = 1 ∞ otherwise . (8) In other words, Lconj(hθ(x)) = −f⋆(∇f(hθ(x))) = − ∑ i exp(hi)∑ jexp(hj) log exp(hi)∑ jexp(hj) (9) i.e. softmax-entropy of the model prediction, which is exactly the TTA loss that TENT uses. Squared loss For the squared loss, we have thatf(h) = 1 2 ∥h∥2 2, leading to the optimality condition y = hand conjugate function f⋆(y) = 1 2 ∥y∥2 2. Hence, the adaptation loss in this case would be simply given by Lconj(hθ(x)) = −f⋆(∇f(hθ(x))) = −1 2 ∥h∥2 2 which is also what we observed in the meta-learning experiments discussed in Section 3. 4.3 Conjugate pseudo-labels We now emphasize that by the nature of our approximations, there is an additional simple interpre- tation of the conjugate loss: it is also equal to the original loss (2) applied to the “psuedo-labels” ˜yCPL θ (x) = ∇f(hθ(x)), where CPL refers to conjugate pseudo-labels, i.e., Lconj(hθ(x)) = −f⋆(∇f(hθ(x))) = f(hθ(x)) −∇f(hθ(x))Thθ(x) = L(hθ(x),∇f(hθ(x))). (10) 5This property is known as the Fenchel-Young inequality, that isf(x) + f⋆(u) ≥xTuholding with equality when u = ∇f(x). In other words, our conjugate adaptation loss is precisely equivalent to self-training under the speciﬁc soft pseudo-labels given by ˜yCPL = ∇f(hθ(x)). And indeed, for many cases, this may be a more convenient form to compute than explicitly computing the conjugate function at all. For this reason, we refer to our method as that of conjugate pseudo-labels. In the case of cross-entropy loss, this approach then corresponds exactly to self-training using labels given by the softmax applied to the current hypothesis. We must emphasize, however, that while our conjugate formulation indeed has this “simple” form for the case of cross-entropy loss, the real advantage comes in that it provides the “correct”pseudo-label for use with other losses, which may result in pseudo-labels different from the “common” softmax operation. Example: conjugate pseudo-labels for PolyLoss. PolyLoss [25] is a recently-proposed simple alternative to cross-entropy loss than has been shown to improve performance across a wide variety of compute tasks. This loss is given by the form Lpoly(hθ(x),y) = Lce(hθ(x),y) + ϵ·yT(1 −softmax(hθ(x))) (11) We note that this can be put exactly into our conjugate form (equation 2) by writing the loss in a slightly more involved fashion, which we refer to as the expanded conjugate form Lpoly(hθ(x),y) = f(hθ(x)) −yTg(hθ(x)). (12) where f is the log-sum-exp function as before, and g(h) = h−ϵ(1 −softmax(h)). In order to formally put this into the form of the previous loss function (equation 2), we can simply deﬁne an alternative hypothesis as the function h′ θ(x) = g(hθ(x)), and then deﬁne PolyLoss in the conjugate form as Lpoly(h′ θ(x),y) = f(g−1(h′ θ(x))) −yTh′ θ(x). (13) Typically, however, it is easier to simply operate on the expanded conjugate form, which yields the optimality condition for the pseudo-label ∇f(hopt) = Dg(hopt)˜yCPL θ (x), where D is the Jacobian operator. For the case of PolyLoss, this leads to the conjugate pseudo-label of the following form: ˜yCPL θ (x) = (I+ ϵdiag(z) −ϵzzT)−1z, z ≡softmax(hθ(x)). Test-time adaptation. Finally, we note that the above discussion doesn’t actually address any topics related to test-time adaptation to OOD data, but merely provides a generic characterization of a self- training procedure for generic loss functions of the form(2). However, the application toTTA on OOD data is fairly straightforward: as long as the learnt source parameters θis a reasonable approximation to the true optimal θopt on the shifted domain, self-training with the conjugate pseudo-labels provides a reasonable proxy for ﬁne-tuning the network on the true OOD loss. We emphasize that, common to most approaches for TTA , there are still some amount of design decisions that must be put in place; these are detailed in Section 5.1. In practice, we observe OOD generalization typically beneﬁts (across all baselines) from an additional “temperature” scaling, i.e., applying the TTA loss to hθ(x)/T for some ﬁxed temperature T, although it requires a held-out validation dataset for tuningT. However, we should emphasize that truly unsupervisedTTA would require making an informed guess for the value of these hyper-parameters. The full procedure for test time adaptation via conjugate pseudo-labels is shown in Algorithm 1. Algorithm 1 Conjugate pseudo-labeling (Conjugate PL) Input: Source classiﬁer θ0 trained using loss L(hθ(x),y) = f(hθ(x)) −hθ(x)⊤y. N batches of test data Dtest = [x1,x2,...,x N] Hyperparams: learning rate ηand temperature T. Let ¯hθ(x) def = hθ(x)/T be the temperature scaled predictor. Let ˜yCPL θ (x) denote the conjugate pseudo-label function ˜yCPL θ (x) = ∇(f(¯hθ(x))). for n= 0,1,...N −1 do θn+1 = θn −η∇L ( ¯hθ(xn),˜yCPL θ (xn) ) [Self-training with conjugate pseudo-labels] 65 Experiments In this section, we empirically evaluate the effectiveness and generality of the proposed conjugate pseudo-labeling procedure (Algorithm 1) for test-time adaptation on a variety of datasets. 5.1 Setup Datasets. We evaluate on the three common corruption benchmarks: adapting a classiﬁer trained on CIFAR-10 to CIFAR-10-C, CIFAR-100 to CIFAR-100-C and ImageNet to ImageNet-C [ 15]. Following the previous works [47, 50], we report the error averaged across corruptions at the highest severity for CIFAR-10/100-C and averaged across corruptions and severity level for ImageNet-C. We also evaluate on three domain adaptation datasets: adapting a classiﬁer trained on SVHN to MNIST, an ImageNet classiﬁer to ImageNet-R [16] and adapting from synthetic to real data in VISDA-C [38]. Models and Training losses. Following previous works on TTA[47, 50], we use ResNet-26 [14] as the source classiﬁer architecture for CIFAR-10/100 experiments, ResNet-18 for SVHN to MNIST and a ResNet-50 for ImageNet and source synthetic data on VisDA-C. We consider source classiﬁers trained via the following loss functions: the de-facto cross-entropy, recently proposed polyloss [25] and squared loss [18]. Baselines. Our proposed conjugate pseudo-label is the classic approach of self-training with a speciﬁc form of pseudo-labels. In self-training, we replace the label ywith a pseudo-label ˜y(x) and adapt by optimizing the loss function L(hθ(x),˜y(x)). Note that we could either instantaneously update the pseudo-labels using the current classiﬁer, or generate pseudo-labels once with just the source classiﬁer. Instantaneous updates have been shown to work better for domain adaptation [7, 40], and we perform instantaneous updates for all methods. While we propose using ˜yCPL(x) = ∇f(hθ(x)) (See Section 4.3), we compare to the standard pseudo-labels used in the literature: • (i) the “hard” pseudo-label (hard PL) where ˜y(x) = arg maxi ( hθ(x) ) i is the most likely class as predicted by hθ. As is common in the self-training literature, we perform conﬁdence thresholding. • (ii) The “soft” pseudo-label (soft PL) where ˜y(x) is obtained by applying a softmax function to the model predictions hθ(x). We also compare with the following recently proposed test-time adaptation methods. • Entropy Minimization (ENT) [50] minimizes the entropy of model predictions. • Robust Pseudo-Label [40] where we minimize a robust classiﬁcation loss, Lrpl = q−1(1 −p(i|x)q) where i= argmaxjp(j|x) and q∈[0,1]. • MEMO [54] minimizes entropy of a model’s outputs across different augmentations of a test input. We implement a batch version, where we see multiple test points at once, for fair comparisons. TTA methodology. Following [ 50] and [40], we ﬁne-tune by updating the learnable scale and shift parameters of the batch normalization layers across all adaptation losses. For each batch, batch normalization statistics is also updated, as suggested in [41]. We report performance at the end of one round of test-time adaptation over the entire test set. We tune the learning rate (LR) and temperature (T) on the validation noises in the corruption benchmark by grid-search. LR is selected from {1e−1,1e−2,... 1e−4}and T from {1,2 ... 5}. All the experiments have been performed on A6000 GPU’s. On domain adaptation benchmarks, where there is no held-out target domain, we set T to be 1 and use the LR suggested by [ 6, 50]. We use the same hyperparameter tuning protocol across all methods. We single out temperature as a very important hyperparameter, as we discuss in the results below. 5.2 Results on classiﬁers trained with cross-entropy We study the effectiveness of our proposed conjugate pseudo-labels when the source classiﬁer is trained via cross-entropy loss. In this case, baselines Softmax PL and ENT are the same as Conjugate PL. Thus we omit them in our results. Table 1, reports the performance of various TTA methods. When the source classiﬁer is trained via cross-entropy, our conjugate pseudo-label algorithm exactly corresponds to entropy minimization with an additional temperature scaling. Entropy minimization as 7Dataset Temperature (T) Hard PL Robust PL MEMO Conjugate PL (ENT) CIFAR-10-C \u0017 13.95 (±0.06) 13.97 ( ±0.04) 12.60(±0.04) 13.07 (±0.05) \u0013 13.95 (±0.06) 12.85 ( ±0.04) 12.51(±0.01) 12.51(±0.03) CIFAR-100-C \u0017 45.22 (±0.4) 39.80 ( ±0.18) 38.52(±0.16) 41.15 (±0.25) \u0013 45.22 (±0.4) 36.37 ( ±0.10) 37.38 ( ±0.06) 36.10(±0.07) ImageNet-C \u0017 45.43(±0.05) 45.68 ( ±0.01) 48.91( ±0.03) 45.82(±0.01) \u0013 45.43 (±0.05) 45.61 ( ±0.01) 48.91( ±0.04) 45.36(±0.01) Table 1: Mean errors when adapting to corruptions using a source classiﬁer trained via cross- entropy loss. Here, conjugate pseudo-labeling becomes softmax-entropy minimization. With the right temperature scaling, softmax-entropy minimization matches or outperforms other approaches. Prior reported gains of other methods over softmax-entropy minimization disappear when we use temperature scaling. For additional context, the source classiﬁer errors without adaptation are: CIFAR-10-C (29.54%), CIFAR-100-C (62.26%), ImageNet-C (61.89%) proposed in prior work [50] does not tune the temperature parameter, and some newer objectives such as robust PL or MEMO outperform vanilla entropy minimization. For example, on CIFAR-100-C, vanilla ENT obtaines 41.15% average error, while robust PL improves this to39.80% and MEMO to 38.52%. However, with the right temperature scaling, entropy minimization obtains 36.10% error which outperforms the newer objectives (with and without temperature scaling). A similar observation holds for CIFAR-10-C and ImageNet-C as well. Essentially, the gains over vanilla entropy minimization vanish when we do temperature scaling, and entropy minimization (i.e. conjugate pseudo-labeling corresponding to cross-entropy) turns out to be the best objective after all. 5.3 Results on classiﬁers trained with polyloss and squared loss In the case of cross-entropy, conjugate pseudo-labeling reduces to the familiar notion of entropy minimization. We now explore the performance of our method on different loss functions where the conjugate pseudo-labels differ substantially from entropy minimization (section 4.3). Table 2 presents the results on the corruption benchmarks and Table 3 presents the results on the other domain adaptation datasets for source classiﬁers trained with PolyLoss. Dataset T Hard PL Robust PL ENT MEMO Softmax PL Conjugate PL (Ours) CIFAR-10-C \u0017 13.81(±0.12) 14.23(±0.02) 13.46(±0.06) 13.23(±0.07) 14.64(±0.11) 13.02(±0.09) \u0013 13.81(±0.12) 12.45(±0.05) 12.23(±0.06) 12.33(±0.04) 12.26(±0.04) 12.08(±0.05) CIFAR-100-C\u0017 40.47(±0.05) 42.86(±0.11) 40.12(±0.08) 39.90(±0.05) 41.00(±0.11) 38.17(±0.17) \u0013 40.47(±0.05) 39.80(±0.08) 38.23(±0.05) 39.23(±0.04) 37.04(±0.06) 36.83(±0.08) ImageNet-C \u0017 45.44(±0.21) 46.27(±0.03) 46.10(±0.03) 48.21(±0.05) 44.63(±0.03) 44.01(±0.01) \u0013 45.44(±0.21) 46.27(±0.03) 45.50(±0.02) 48.21(±0.04) 44.45(±0.03) 44.01(±0.01) Table 2: Mean errors when adapting to corruptions using a source classiﬁer trained via recently proposed Poly-1 Loss [ 25]. Conjugate pseudo-labeling consistently outperforms all previous ap- proaches. For additional context, source classiﬁer errors without adaptation : CIFAR-10-C (30.22%), CIFAR-100-C (63.91%) and ImageNet-C (62.18%). First, we note that, across all datasets in Table 2 and Table 3, our conjugate PL approach outperforms all other TTA losses. With polyloss classiﬁers, entropy minimization is no longer the best method—on CIFAR-100-C, entropy minimization achieves38.23% error while our conjugate PL achieves36.83%. We see similar consistent gains on CIFAR-10-C, ImageNet-C, ImageNet-R and VisDA-C. On digit adaptation tasks from SVHN to MNIST/USPS/MNISTM, where there is a larger shift between source and target, the gains are especially pronounced. Figure 2 compares how the task loss (polyloss ϵ= 6) on the test data decreases as we adapt the model through conjugate PL and other baselines. We use CIFAR-10-C as an example. Observe that our proposed conjugate PL indeed reduces the task loss the most among other baselines. 8Dataset Source Error Hard PL Robust PL EntropySoftmax PL Conjugate PL Ours SVHN→MNIST 28.33 20.21 19.73 14.28 16.54 10.73 SVHN→USPS 31.58 23.32 26.12 23.12 24.07 21.62 SVHN→MNISTM61.69 50.73 51.35 49.33 50.47 47.59 ImageNet-R 64.19 58.52 59.46 58.25 56.62 55.63 VisDA-C 58.13 40.43 45.44 44.11 39.63 38.42 Table 3: Target error when adapting models trained via polyloss on source domains across different domain adaptation bench- marks. Conjugate pseudo-labeling offers consistent and substan- tial gains over previous approaches across three datasets. Figure 2: Task Loss (PolyLoss ϵ= 6) evaluated on CIFAR-10-C test data during test-time adaptation. Furthermore, on CIFAR-10-C and ImageNet-C, we ﬁnd that adapting polyloss classiﬁers via conjugate PL improves the performance over all methods applied to cross-entropy trained source classiﬁers. For e.g., on ImageNet-C, the performance improves from 45.34% to 44.01%. However, this is only true when using the proposed conjugate PL. If we just did softmax-entropy minimization (even with temperature scaling), the ﬁnal adapted performance of a polyloss classiﬁer (45.5%) is in fact worse than that of a cross-entropy classiﬁer (45.34%). Our results suggest that as we develop new training losses that improve the source classiﬁers, it is important to adapt via conjugate pseudo-labeling to reap the maximum gains. Similarly, we experiment with the case when the source classiﬁer is trained using squared loss on the CIFAR-10 and CIFAR-100 datasets, and observe consistent gains using the proposed conjugate pseudo-labels over the baselines. For example, on CIFAR-10-C, TTA using conjugate PL gives and error of 12.87%, outperforming baselines like ENT (13.24%) and Softmax PL (31.81%). Table 5 in Appendix A.7 shows the detailed results. Comparing Table 1 and Table 2, we see that the relative ordering between the various baselines differs. This is further evidence that the adaptation loss has to depend on the training loss, and we believe our conjugate pseudo-label approach captures this appropriately by offering consistent gains across the various settings we experimented with. 6 Related Works Test-time adaptation methods. In recent years, the setting of test-time adaptation has gained a lot of interest with a host of different approaches proposed in the literature. One family of TTA approaches update the source classiﬁer by minimizing an unsupervised loss on the target distribution [4, 6, 20, 22, 35, 36, 40, 43, 44, 50, 51, 54]. TENT [ 50] proposes to minimize the entropy of model predictions at test time. Several follow ups like [ 6, 35, 40, 44, 54] propose alternative TTA objectives, e.g. robust pseudo-labelling [40], likelihood ratio loss [35], entropy of marginal probability averaged across augmentations [54] and self-supervised contrastive losses [6, 49]. However, most of these objectives are heuristically designed or chosen. In this paper, we provide a principled approach of designing unsupervised objectives for TTA . Another family of approaches for test-time adaptation such as [ 2, 8, 13, 31, 34, 47] leverage an auxiliary self-supervised task (e.g. rotation prediction [ 47], masked autoencoders [10]) to update model parameters on each test sample. Crucially, these methods require modifying the source model training by augmenting the supervised training objective with an auxiliary self-supervised loss. Hence it cannot be applied to typical standard classiﬁers that are trained by minimizing a supervised loss on the source data. Source-free domain adaptation. A very related setting to test-time adaptation is source-free domain adaptation, where a trained source classiﬁer must be adapted to a target distribution of interest, although the entire target unlabeled data is available at once. SHOT [28] proposes to optimize the source hypothesis (i.e. feature extractor) with a combination of entropy minimization, diversity and self-training on pseudo-labels on the unlabeled target data. [53] promotes feature clustering on features from target distributions. [24, 26] use generative modeling to estimate the underlying source distributions for enforcing feature invariance. Such approaches typically require multiple epochs over the target data and cannot be easily adopted to work in an online fashion. 9Unsupervised domain adaptation. The most canonical setting of domain adaptation involves access to labeled source data and unlabeled target data, all during training. The availability of source and target data during training lends itself to approaches that “align” the source and target representations in some way: [ 32, 33, 45, 48] match distribution statistics, [ 11] uses a discriminator, [ 46] uses self-supervised learning. However, such approaches require access to source data which might not always be feasible due to data privacy and efﬁciency issues. Pseudo-labels and self-training. Self-training is a classic idea for leveraging unlabeled data, devel- oped ﬁrst for the semi-supervised setting. Self-training generates pseudo-labels on the unlabeled data, allowing us to use any “supervised” loss on this pseudo-labeled data. Self-training has shown promising results in various settings like semi-supervised learning [ 19] and improving adversarial robustness [ 5]. Self-training has also been gaining attention in the setting of unsupervised domain adaptation [28, 39], where pseudo-labels generated on the unlabeled data from target domain is used to supervise the adaptation process. [ 7, 23, 52] provide theoretical insights into how self-training with pseudo-labels can help under distribution shift. TENT [50] (i.e entropy minimization) can be viewed as a form of self-training with instantaneous softmax pseudo-labels. Our work provides a general framework for the choice of soft pseudo-labels based on the conjugate analysis of the source training objective. Some prior works like [7, 17, 27, 30, 55, 56] have documented the improvement in performance when using instantaneous pseudo-labels over pre-computed pseudo-labels, and thus lend further support to the beneﬁts of our proposed conjugate pseudo-labeling approach. The ex- periment results presented in this work supporting conjugate pseudo-labels suggest that conjugate pseudo-labels is a promising direction of pseudo-labeling in a broader context. 7 Conclusion, Limitations and Future Directions In this work, we proposed a general test-time adaptation loss, based on the convex conjugate formulation which in turn was motivated by the intriguing meta learning experiments. The fact that meta-learning recovers the proposed loss hints at some kind of optimality of the loss. In Section 4, we prove that for a broad set of loss functions, the proposed (unsupervised) conjugate loss is close to the oracle supervised loss. However, this still does not completely answer what the optimal test-time adaptation loss is and why. The meta-learning framework in this work was constrained to learn functions over the logits of each individual input. It can be expanded to more involved setups, where we consider functions over the intermediate representations too and also consider learning functions over a batch of input while accounting for their interactions. Beyond the choice of the adaptation loss itself, achieving good test-time adaptation generally involves several heuristics like updating only the batch norm parameters [50]. While our work was motivated by the loss function, via the meta-learning experiments, we discovered that temperature scaling is another important hyper-parameter that improves the performance of all previous baselines as well. At a high level, test-time adaptation has to be appropriately regularized to prevent the updates over batches from taking the model too far: updating only a few batch norm parameters is one way to do that, and perhaps temperature scaling provides a similar beneﬁcial regularization effect by making the network predictions on unlabeled inputs less conﬁdent. Understanding the role of these heuristics more concretely is an interesting direction for future work. It also remains an open problem to understand under what sort of real-world distribution shifts would self-training based approaches would help. Finally, it is also worth extending and applying the conjugate pseudo-labeling to other settings like semi-supervised learning. 8 Acknowledgments We thank Shubhang Bhatnagar and Asher Trockman for helping with running the ImageNet experi- ments. We thank Zhili Feng for useful feedback. Sachin Goyal and Mingjie Sun were supported by funding from the Bosch Center for Artiﬁcial Intelligence. Aditi Raghunathan was supported by an Open Philanthropy AI Fellowship. 10References [1] https://en.wikipedia.org/wiki/Convex_conjugate. [2] Pratyay Banerjee, Tejas Gokhale, and Chitta Baral. Self-supervised test-time learning for reading comprehension. In Annual Conference of the North American Chapter of the Association for Computational Linguistics, 2021. [3] Sarah Bechtle, Artem Molchanov, Yevgen Chebotar, Edward Grefenstette, Ludovic Righetti, Gaurav Sukhatme, and Franziska Meier. Meta-learning via learned loss. arXiv preprint arXiv:1906.05374, 2019. [4] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [5] Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and Percy S Liang. Un- labeled data improves adversarial robustness. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips. cc/paper/2019/file/32e0bd1497aa43e02a42f47d9d6515ad-Paper.pdf. [6] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [7] Yining Chen, Colin Wei, Ananya Kumar, and Tengyu Ma. Self-training avoids using spurious features under domain shift. In Advances in Neural Information Processing Systems, 2020. [8] Mohammad Zalbagi Darestani, Jiayu Liu, and Reinhard Heckel. Test-time training can close the natural distribution shift performance gap in deep learning based compressed sensing. In Proceedings of the 39th International Conference on Machine Learning (ICML), 2022. [9] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap- tation of deep networks. In Proceedings of the 34th International Conference on Machine Learning (ICML), 2017. [10] Yossi Gandelsaman, Yu Sun, Xinlei Chen, and Alexei A. Efros. Test-time training with masked autoencoders. In Advances in Neural Information Processing Systems, 2022. [11] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario March, and Victor Lempitsky. Domain-adversarial training of neural networks. Journal of Machine Learning Research, 17(59):1–35, 2016. [12] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. InInternational Conference on Learning Representations, 2021. [13] Nicklas Hansen, Rishabh Jangir, Yu Sun, Guillem Alenya, Pieter Abbeel, Alexei A. Efros, Lerrel Pinto, and Xiaolong Wang. Self-supervised policy adaptation during deployment. In International Conference on Learning Representations, 2021. [14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2016. [15] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In International Conference on Learning Representations, 2019. [16] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. In In IEEE/CVF International Conference on Computer Vision (ICCV), 2021. [17] Yosuke Higuchi, Niko Moritz, Jonathan Le Roux, and Takaaki Hori. Advancing momentum pseudo-labeling with conformer and initialization strategy. In IEEE International Conference on Acoustics, Speech and Signal Processing, 2022. 11[18] Like Hui and Mikhail Belkin. Evaluation of neural architectures trained with square loss vs cross-entropy in classiﬁcation tasks. In International Conference on Learning Representations, 2021. [19] Dong hyun Lee. Pseudo-label: The simple and efﬁcient semi-supervised learning method for deep neural networks. [20] Yusuke Iwasawa and Yutaka Matsuo. Test-time classiﬁer adjustment module for model-agnostic domain generalization. In Advances in Neural Information Processing Systems, 2021. [21] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton A. Earnshaw, Imran S. Haque, Sara Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. Wilds: A benchmark of in-the-wild distribution shifts. In Proceedings of the 38th International Conference on Machine Learning (ICML), 2021. [22] Takeshi Kojima, Yutaka Matsuo, and Yusuke Iwasawa. Robustifying vision transformer without retraining from scratch by test-time class-conditional feature alignment. In International Joint Conference on Artiﬁcial Intelligence, 2022. [23] Ananya Kumar, Tengyu Ma, and Percy Liang. Understanding self-training for gradual domain adaptation. In Proceedings of the 37 th International Conference on Machine Learning (ICML), 2020. [24] Vinod K Kurmi, Venkatesh K Subramanian, and Vinay P Namboodiri. Domain impression: A source data free domain adaptation method. In IEEE Winter Conference on Applications of Computer Vision (WACV), 2021. [25] Zhaoqi Leng, Mingxing Tan, Chenxi Liu, Ekin Dogus Cubuk, Jay Shi, Shuyang Cheng, and Dragomir Anguelov. Polyloss: A polynomial expansion perspective of classiﬁcation loss functions. In International Conference on Learning Representations, 2022. [26] Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, and Si Wu. Model adaptation: Unsuper- vised domain adaptation without source data. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. [27] Xinzhe Li, Qianru Sun, Yaoyao Liu, Qin Zhou, Shibao Zheng, Tat-Seng Chua, and Bernt Schiele. Learning to self-train for semi-supervised few-shot classiﬁcation. In Advances in Neural Information Processing Systems, 2019. [28] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. InProceedings of the 37th International Conference on Machine Learning (ICML), 2020. [29] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object detection. In IEEE/CVF International Conference on Computer Vision (ICCV), 2017. [30] Hong Liu, Jianmin Wang, and Mingsheng Long. Cycle self-training for domain adaptation. In Advances in Neural Information Processing Systems, 2021. [31] Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? In Advances in Neural Information Processing Systems, 2021. [32] Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, and Philip S. Yu. Transfer feature learning with joint distribution adaptation. In IEEE/CVF International Conference on Computer Vision (ICCV), 2013. [33] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I. Jordan. Learning transferable features with deep adaptation networks. In Proceedings of the 32nd International Conference on Machine Learning, 2015. [34] Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen, and Johannes Kopf. Consistent video depth estimation. In SIGGRAPH, 2020. 12[35] Chaithanya Kumar Mummadi, Robin Hutmacher, Kilian Rambach, Evgeny Levinkov, Thomas Brox, and Jan Hendrik Metzen. Test-Time Adaptation to Distribution Shift by Conﬁdence Maximization and Input Transformation. arXiv preprint arXiv: 2106.14999, 2021. [36] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efﬁcient test-time model adaptation without forgetting. In Proceedings of the 39th International Conference on Machine Learning (ICML), 2022. [37] Junhyuk Oh, Matteo Hessel, Wojciech M. Czarnecki, Zhongwen Xu, Hado P van Hasselt, Satinder Singh, and David Silver. Discovering reinforcement learning algorithms. In Advances in Neural Information Processing Systems, 2020. [38] Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko. Visda: The visual domain adaptation challenge, 2017. [39] Viraj Prabhu, Shivam Khare, Deeksha Kartik, and Judy Hoffman. Sentry: Selective entropy optimization via committee consistency for unsupervised domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. [40] Evgenia Rusak, Steffen Schneider, George Pachitariu, Luisa Eck, Peter Vincent Gehler, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. If your data distribution shifts, use self- learning, 2022. URL https://openreview.net/forum?id=1oEvY1a67c1. [41] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. In Advances in Neural Information Processing Systems, 2020. [42] H. Scudder. Probability of error of some adaptive pattern-recognition machines. IEEE Transac- tions on Information Theory, 1965. [43] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Test-time prompt tuning for zero-shot generalization in vision-language models. In Advances in Neural Information Processing Systems, 2022. [44] Prabhu Teja Sivaprasad and François Fleuret. Test time adaptation through perturbation robust- ness. arXiv preprint arXiv: 2110.10232, 2021. [45] Baochen Sun, Jiashi Feng, and Kate Saenko. Correlation alignment for unsupervised domain adaptation. arXiv preprint arXiv: 1612.01939, 2016. [46] Yu Sun, Eric Tzeng, Trevor Darrell, and Alexei A. Efros. Unsupervised domain adaptation through self-supervision. arXiv preprint arXiv:1909.11825, 2019. [47] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A. Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In Proceedings of the 36th International Conference on Machine Learning (ICML), 2019. [48] Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain confusion: Maximizing for domain invariance. arXiv preprint arXiv:1412.3474, 2014. [49] Dequan Wang, Shaoteng Liu, Sayna Ebrahimi, Evan Shelhamer, and Trevor Darrell. On-target adaptation. arXiv preprint arXiv: 2109.01087, 2021. [50] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Representations, 2021. [51] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [52] Sang Michael Xie, Ananya Kumar, Robbie Jones, Fereshte Khani, Tengyu Ma, and Percy Liang. In-n-out: Pre-training and self-training using auxiliary information for out-of-distribution robustness. In International Conference on Learning Representations, 2021. 13[53] Shiqi Yang, Yaxing Wang, Joost van de Weijer, Luis Herranz, and Shangling Jui. Generalized source-free domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. [54] Marvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and augmentation. In Advances in Neural Information Processing Systems, 2022. [55] Yang Zou, Zhiding Yu, B. V . K. Vijaya Kumar, and Jinsong Wang. Domain adaptation for semantic segmentation via class-balanced self-training. European Conference on Computer Vision, 2018. [56] Yang Zou, Zhiding Yu, Xiaofeng Liu, B. V . K. Vijaya Kumar, and Jinsong Wang. Conﬁdence regularized self-training. In IEEE/CVF International Conference on Computer Vision (ICCV), 2019. 14A Appendix A.1 Conjugate Derivations Cross-Entropy Loss : L(h,y) = − c∑ i=1 yilog exp(hi)∑c j=1 exp(hj) = − c∑ i=1 yi ∗hi + log c∑ j=1 exp(hj) = f(h) −y⊤h, (14) where f(h) is log ∑c j=1 exp(hj) and the constraint that ∑c i=1 yi = 1. Now, the conjugate f⋆(y) is given by : f⋆(y) = −min h {f(h) −yTh}= −min h {log c∑ j=1 exp(hj) −yTh} (15) with the constraint ∑c i=1 yi = 1. At the optimality, yi = (∇f(h))i = exp(hi)∑ jexp(hj) (16) Then, f⋆(y) = −log c∑ j=1 exp(hj) + c∑ i=1 hi exp(hi)∑ jexp(hj) = ∑ i exp(hi)∑ jexp(hj) log exp(hi)∑ jexp(hj), (17) if the constraint ∑c i=1 yi = 1 is satisﬁed, otherwise f⋆(y) = ∞by duality. This in turn gives, the conjugate loss for cross-entropy (when the constraint is satisﬁed) : Lconj(h) = −f⋆(y) = −f⋆(∇f(h)) = − ∑ i exp(hi)∑ jexp(hj) log exp(hi)∑ jexp(hj) (18) Squared Loss : L(h,y) = 1 2||h−y||2 2 ≈1 2||h||2 2 −y⊤h [ignoring the constant term] = f(h) −y⊤h, (19) Now, the conjugate f⋆(y) is given by: f⋆(y) = −min h {f(h) −yTh}= −min h {1 2||h||2 2 −yTh} = −1 2||h||2 2 (20) A.2 Experiments on Binary Classiﬁcation with Exponential Loss Here we present the results on a binary classiﬁcation task over a synthetic dataset of 100 dimensional gaussian clusters. 15Dataset Creation For the binary classiﬁcation task, we create a synthetic dataset similar to [23]. Speciﬁcally, let the data X ∼ N(µ,Σ) ∈ R100 and labels Y ∈ {−1,+1}. We sample µ ∼ N(k,I100). For Σ, similar to [ 23], we sample a diagonal matrix D, where each entry is sampled uniformly from a speciﬁed range, and a rotation matrix U from a HAAR distribution, giving Σ = UDUT. For the source data, we sample µ−1 s ,µ+1 s ,Σ−1 s ,Σ+1 s as speciﬁed above with k= 0. Now to create a distribution shifted data of various severity, we sampleµ−1 t ,µ+1 t ,Σ−1 t ,Σ+1 t as speciﬁed above with k= 1, which are then used to sample the shifted data as follows : µ1 λ = λµ1 t + (1 −λ)µ1 s µ−1 λ = λµ−1 t + (1 −λ)µ−1 s Σ1 λ = λΣ1 t + (1 −λ)Σ1 s Σ−1 λ = λΣ−1 t + (1 −λ)Σ−1 s Xλ ∼N(µλ,Σλ) In the following experiments, easy shift refers to λ= 0.6, moderate shift to λ= 0.65 and hard shift to λ= 0.7. Exponential Loss for Binary Classiﬁcation Let zbe the classiﬁcation score hθ(x). For logistic training loss, conjugate adaptation loss would default to entropy with sigmoid probability. Thus, here we experiment with a different but also commonly used surrogate loss to 0/1 loss: exponential loss, which is deﬁned as: Lexp(z,y) = exp(−yz) (21) where y∈{−1,+1}. It can be rewritten in the expanded conjugate form of: Lexp(z,y) = 1 2 · ( ez + e−z) −1 2 ·y· ( ez −e−z) (22) For exponential loss, the conjugate pseudo-label function and the conjugate pseudo-label loss are: yCPL exp (z) = ez −e−z ez + e−z, LCPL exp (z) = 2 ez + e−z (23) The model is adapted on shifted gaussian clusters and we compare the conjugate loss with two baseline approaches: 1) Hard pseudo-labelling exp(−yhard pl ·z); 2) Entropy applied to sigmoid probability P(y= +1) = σ(z). The losses are compared on three degrees of shift (easy, moderate and hard), which is controlled by the drifted distance of Gaussian clusters. The results are shown in Figure 3, where we plot the accuracy curve with respect to adaptation iterations. With easy and moderate shift, conjugate loss (green) generalizes faster to shifted test data; with hard shift, only conjugate loss improves model accuracy on shifted test data while entropy (blue) deteriorates model performance. Figure 3: Test-time adaptation result on synthetic data with three shift levels ranging from easy, moderate and hard (detailed in section A.2). The source model is a linear classiﬁer trained with exponential loss Lexp = e−yhθ(x). Adaptation with the conjugate loss generalizes better compared to baseline losses. 16A.3 Meta Learning Experiment Details In section 3 we talked about learning the meta-loss function parameterized by a neural network mφ : R|Y|↦→R, that takes in the model predictions/logits and outputs a loss value. Here we discuss the architecture chosen and the implementation details. Further, in Appendix A.4 we empirically show that the learnt meta-loss is not affected by the choice of task loss / surrogate loss used in meta learning (Lin Equation 1). Note that the task loss / surrogate loss function is used to update the meta-loss mφ during meta-learning. The surrogate loss is calculated on updated source model’s predictions on labeled samples from test domain. The surrogate loss tries to update the meta-loss in the outer loop such that when meta-loss is later used to update the source model in the inner loop, the source model generalizes better to the test domain. Architecture and Implementation Details Figure 4 gives an overall schema for meta-learning the loss function and algorithm 2 gives the pseudo-code for meta-learning the loss function. Below we describe this in further detail. We use a transformer (denoted by T) with a MLP (denoted by P) over the output of transformer as the architecture for mφ, i.e. mφ(x) = P(T(x)). Speciﬁcally, for a given source trained model hθ and input x∼Dtest : 1. Let hθ(x) ∈R|Y|be the model predictions/logits, where |Y|denotes the number of classes. 2. Let hj θ(x) ∈R,∀j ∈|Y| be the prediction corresponding to class j. 3. The input to transformer is then given by z ∈R|Y|×(1+e), where zj ∈R1+e,∀j ∈|Y| is the concatenation of hj θ(x) and the learnable positional embedding pej ∈Re. 4. The transformer output is given by w= T(z) ∈Rd, where ddenotes the feed-forward dimension of the transformer. 5. The transformer output wis ﬁnally passed through a MLP to get the meta-loss valuemφ(hθ(x)) = P(w) ∈R 6. The source model is updated by optimizing over the meta-loss. θt+1 ←θt −α∂mφt(hθt(x)) ∂θt (24) 7. The updated source model is then used to update the meta-loss by optimizing over some supervised loss function Ltask. φt+1 ←φt −β∂Ltask(hθt+1 (x′),y′) ∂φt , where (x′,y′) ∼Dtest (25) Note that the last step assumes access to labels of test inputs. In this paper, we do not propose meta-learning the TTA loss as an approach. Rather, we use meta-learning to explore what the “best” TTA losses look like. We select the trasformer input embedding dimension (1 + e) from {16,32,64}and transformer feed-forward dimension dfrom {32,64,128}. The number of transformer layers and the hidden layers in MLP are selected from {1,2}. We use Adam optimizer with a learning rate of 1e−3 for learning the meta-loss (i.e. the transformer + MLP). We train the meta-loss for 100 epochs with a batch size of 200. A.4 Effect of Task Loss in Meta Learning In section 3, we show that the meta losses learned on different source classiﬁers differ substantially if the source classiﬁers are trained using different source loss functions. Here we further empirically verify that the learnt meta loss is not affected by the task loss used in meta learning (Lin Equation 1). Thus the learnt meta loss is determined by the source model. In Figure 5, we show the meta loss learnt on a ResNet-26 trained with Cross Entropy loss for two meta task losses: Cross Entropy Figure 5a and Squared Loss Figure 5b. We plot the meta loss as a function over one of its input prediction scores, while keeping other ﬁxed. We can see that the task loss barely affects the learnt meta loss. Similar observations can be made for the classiﬁer trained with squared loss Figure 6. 17Meta-Loss  Backpropogate  Figure 4: Meta-Loss learning procedure : The model predictions hθt(x) are passed through the parameterized loss function mφt, which outputs a loss value. We optimize φ such that when optimizing the source model over the loss mφt(hθt(x)), the updated θt+1 has a better performance on the test domain. To do this, we take one gradient step over the meta-loss to get the update source model parameters θt+1, and then update φby evaluating θt+1 on the labeled validation data using some task loss Ltask. Algorithm 2 Learning the Meta-Loss Input: Source trained classiﬁer hθ0 . Randomly initialized meta-loss mφ0 . Task loss / Surrogate loss Ltask like cross-entropy or squared loss for meta learning N batches of test data Dtest = [(x1,y1),..., (xN,yN)] Hyperparams: learning rates αand β. for epoch= 0,1,2,... do for n= 0,1,...N −1 do θt+1 ←θt −α ∂mφt(hθt(xn)) ∂θt Sample (xr,yr) ∼Dtest. φt+1 ←φt −β∂Ltask(hθt+1 (xr),yr) ∂φt A.5 Test-Time Adaptation Detail For completeness, we also give the test-time adaptation setup in Algorithm 3. A.6 ImageNet results on each severity level In continuation with results shown in Table 2 in Section 5.3, Table 4 shows the mean errors averaged across the 15 corruption types for each of the severity level on ImageNet-C, for a source classiﬁer trained with PolyLoss (ϵ= 8). A.7 Square Loss Trained Source Classiﬁer In Section 5.3, we brieﬂy discussed that similar to the other source training losses like cross-entropy and polyloss, our proposed conjugate loss outperforms the baselines when the source classiﬁer is 18(a)  (b) Figure 5: Visualizations of meta loss by varying one input dimension (prediction score). The source model is a ResNet-26 trained with Cross Entropy. Here we show meta loss trained by two different task losses: Cross Entropy Figure 5a and Squared Loss Figure 5b. (a)  (b) Figure 6: Visualizations of meta loss by varying one input dimension (prediction score). The source model is a ResNet-26 trained with Squared Loss. Here we show meta loss trained by two different task losses: Cross Entropy Figure 6a and Squared Loss Figure 6b. Algorithm 3 Test-Time Adaptation Input: Source classiﬁer θ0 trained using loss L(hθ(x),y), An unsupervised loss function for test-time adaptation Ltta(x), N batches of test data Dtest = [x1,...,x N] Hyperparams: learning rate η. for n= 0,1,...N −1 do θn+1 = θn −η∇Ltta(xn) ˆyn = hθn+1 (xn) [Predictions for the nth batch] 19Corrution Severity Temperature Robust PL Entropy MEMO Softmax PL Conjugate 1 \u0017 34.27 33.17 34.39 32.49 32.26 \u0013 34.27 32.84 34.39 32.70 32.26 2 \u0017 41.25 39.04 40.38 37.78 37.40 \u0013 41.25 38.50 40.38 37.75 37.40 3 \u0017 47.37 44.04 45.67 42.30 41.72 \u0013 47.37 43.33 45.67 42.14 41.72 4 \u0017 56.63 51.88 54.49 49.61 48.84 \u0013 56.63 51.03 54.49 49.39 48.84 5 \u0017 67.11 62.53 66.13 60.94 59.90 \u0013 67.11 61.80 66.13 60.30 59.90 Mean \u0017 49.32 46.13 48.21 44.62 44.02 \u0013 49.32 45.50 48.21 44.45 44.02 Table 4: Mean Errors across the 15 noises for various severity level on the ImageNet-C dataset, with source model trained using Poly-1 Loss. Note that Temperature scaling helped only in the case of Entropy and Softmax PL. trained using a squared loss. Table 5 shows a detailed comparison with the baselines. We note that for the conjugate of squared loss, the temperature scaling can be wrapped into the learning rate as shown in Section 4.2. Further, on the CIFAR-10-C dataset we observe temperature scaling doesn’t help any of the other baselines too, hence we do not include the temperature row in CIFAR-10-C. Dataset Temperature Hard PL Robust PL ENT MEMO Softmax PL Conjugate PL CIFAR-10-C \u0017 13.71 (±0.07) 13.06 (±0.05) 13.24 (±0.02) 13.22 (±0.04) 14.85 (±0.08)12.99(±0.04) CIFAR-100-C \u0017 50.82 (±0.31) 44.53 (±0.13) 43.55 (±0.12) 51.35 (±0.04) 51.99 (±0.03)43.39(±0.11) \u0013 50.82 (±0.31) 43.99 (±0.15)43.21(±0.08) 51.35 (±0.04) 51.99 (±0.03) 43.39 (±0.11) Table 5: Mean Errors on the common corruptions datasets for source classiﬁer trained using squared loss. We note that temperature scaling didn’t help on the CIFAR-10-C dataset. Source Classiﬁer Errors without adaptation : CIFAR-10-C (28.34%), CIFAR-100-C (68.79%) Dataset Temperature (T) Hard PL Robust PL MEMO Conjugate PL (ENT) CIFAR-10-C \u0017 SGD,1e−3, 1 SGD,1 e−3, 1 SGD,1 e−3, 1 SGD, 1e−3, 1 \u0013 SGD,1e−3, 1 SGD,1 e−2, 2 SGD,5 e−3, 3 Adam,1e−3, 2 CIFAR-100-C \u0017 SGD,1e−2, 1 SGD,1 e−2, 1 SGD,5 e−3, 1 SGD, 1e−2, 1 \u0013 SGD,1e−2, 1 SGD,1 e−2, 2 SGD,1 e−2, 2 SGD,1e−2, 2 ImageNet-C \u0017 SGD,1e−2, 1 SGD,2.5 e−3, 1 SGD,1 e−3, 1 SGD,2.5e−3, 1 \u0013 SGD,1e−2, 1 SGD,2.5e−3, 1.5 SGD,1e−3, 1 SGD,2.5e−3, 1.5 Table 6: Hyper-parameters (Optimizer, Learning Rate, Temperature) for the results in Table 1, where we showed the mean errors on the common corruptions dataset for a source classiﬁer trained using cross-entropy loss. A.8 Hyper-Parameters We share the exact hyper-parameters found using gridsearch over the 4 validation noises for the common corruptions dataset. 20Cross Entropy Classiﬁer Experiments In Section 5.2, Table 1 shows the results when adapting a cross entropy trained classiﬁer on various common corruptions dataset. Table 6 gives the optimizer, learning rate and optimal temperature for each of the baseline and our proposed conjugate loss. PolyLoss Classiﬁer Experiments In Section 5.3, Table 2 shows the results when adapting a polyloss trained classiﬁer on various common corruptions dataset. Table 7 gives the optimizer, learning rate and optimal temperature for each of the baseline and our proposed conjugate loss. Dataset T Hard PL Robust PL ENT MEMO Softmax PL Conjugate PL (Ours) CIFAR-10-C\u0017 SGD,1e−3, 1 SGD,1e−3, 1 SGD,1 e−3, 1 SGD,5 e−3, 1 SGD, 1e−3, 1 SGD, 1e−3, 1 \u0013 SGD,1e−3, 1 SGD,1e−2, 3 SGD,1 e−2, 3 SGD,5 e−3, 3 SGD, 1e−3, 2 SGD, 1e−3, 1.5 CIFAR-100-C\u0017 SGD,1e−2, 1 SGD,1e−2, 1 SGD,1 e−2, 1 SGD,1 e−2, 1 SGD, 1e−2, 1 SGD, 1e−2, 1 \u0013 SGD,1e−2, 1 Adam,1e−3, 3 SGD,1 e−2, 2 SGD,1 e−2, 2 SGD, 1e−2, 2.5 SGD, 1e−2, 1.5 ImageNet-C\u0017 SGD,1e−2, 1 SGD,2.5e−3, 1 SGD,2.5e−3, 1 SGD,5e−3, 1 SGD, 2.5e−3, 1 SGD, 2.5e−3, 1 \u0013 SGD,1e−2, 1 SGD,2.5e−3, 1 SGD,2.5e−3, 1.5 SGD,5e−3, 1 SGD, 2.5e−3, 2 SGD, 2.5e−3, 1 Table 7: Hyper-parameters (Optimizer, Learning Rate, Temperature) for the results in Table 2, where we showed the mean errors on the common corruptions dataset for a source classiﬁer trained using poly-loss. Squared Loss Classiﬁer Experiments In Section 5.3, we brieﬂy discussed the results when adapt- ing a squared loss trained classiﬁer on various common corruptions dataset. Table 8 gives the optimizer, learning rate and optimal temperature for each of the baseline and our proposed conjugate loss for the results in Table 5. Digit Adaptation Datasets For the experiments on digits adaptation tasks, we do not have any validation set. Hence, we don’t use temperature scaling here (T = 1) and ﬁx the optimizer and LR as Adam and 1e−2 respectively for all the baselines. A.9 Additional Experiments on Digit Adaptation Datasets Similar to the setting of Table 1, we perform additional experiments on digit adaptation datasets when the source classiﬁer is trained using the cross-entropy loss. Note that when the source classiﬁer is trained using cross-entropy loss, the conjugate loss is equal to the softmax-entropy. In the absence of validation dataset in digit adaptation benchmarks, we used a ﬁxed learning rate of 0.01 for all the baselines, optimizer as Adam and an informed temperature scaling guess of T=2. Table 9 compares softmax-entropy minimization with various baselines. Here, again we observe that on SVHN →MNIST benchmark, without temperature scaling, MEMO (10.67% error) outperforms softmax-entropy (14.41% error). However, similar to the observations in Table 1, with temperature scaling, softmax-entropy minimization (9.26% error) is able to match the performance of MEMO (9.36% error). Further, on the SVHN →USPS benchmark, softmax-entropy (conjugate) and MEMO perform similar even without temperature scaling. A.10 Additional Meta Learning the TTA Loss Experiments In Section 3, we tried to learn a test-time adaptation (TTA) loss via meta-learning for adapting a CIFAR10 trained ResNet26 to distribution shifts on CIFAR10 corruptions. Figure 1 showed that the learnt meta-loss looks like a temperature scaled softmax-entropy. In this section, we show the learnt meta loss across a range of settings as described below : 1. Digit Adaptation: Figure 7a and 7b show the learnt meta-loss when adapting a SVHN trained ResNet26 to MNIST dataset and USPS dataset respectively. We observe that the learnt meta-loss can be well approximated by a temperature scaled softmax-entropy. 2. Various Noise Types: In Figure 8, we show the learnt meta-loss when adapting a ResNet26 trained on CIFAR10 dataset using cross-entropy loss, to various noise types like speckle, gaussian, saturate and spatter. The severity level is kept ﬁxed at the maximum i.e. 5. 21Dataset T Hard PL Robust PL ENT MEMO Softmax PL Conjugate PL (Ours) CIFAR-10-C\u0017 SGD,1e−2, 1 SGD,1 e−2, 1 SGD,1 e−2, 1 SGD,1e−2, 1 SGD,1 e−4, 1 SGD,1e−2, 1 CIFAR-100-C\u0017 Adam,1e−3, 1 Adam,1e−3, 1 Adam,1e−3, 1 Adam,1e−3, 1 Adam, 1e−4, 1 Adam, 1e−3, 1 \u0013 Adam,1e−3, 1 Adam,1e−3, 0.5 Adam,1e−3, 2 Adam,1e−3, 2 Adam, 1e−4, 2.5 Adam, 1e−3, 1 Table 8: Hyper-parameters (Optimizer, Learning Rate, Temperature) for the results in Table 5, where we showed the mean errors on the common corruptions dataset for a source classiﬁer trained using squared loss. Dataset Temperature (T) Hard PL Robust PL MEMO Conjugate PL (ENT) SVHN→MNIST \u0017 21.54 27.44 10.67 14.41 \u0013 21.54 13.26 9.36 9.26 SVHN→USPS \u0017 26.06 26.81 22.72 22.57 \u0013 26.06 22.32 22.42 22.27 Table 9: Mean errors when adapting to digit adaptation benchmarks using a source classiﬁer trained via cross-entropy loss. Here, conjugate pseudo-labeling becomes softmax-entropy minimization. Again we observe that with the right temperature scaling, softmax-entropy minimization matches other approaches. For additional context, the source classiﬁer errors without adaptation are: SVHN →MNIST (34.17%), SVHN →USPS (31.84%). 20  10  0 10 20 prediction score 5 0 5 10loss value meta loss (error 10.44%) softmax entropy (error 14.41) fitted entropy (error 9.26) Meta Loss for SVHN -> MNIST (a) 20  10  0 10 20 prediction score 6 4 2 0 2 4 6 8 loss value meta loss (error 20.13%) softmax entropy (error 22.57) fitted entropy (error 22.22) Meta Loss for SVHN -> USPS adpatation (b) Figure 7: Visualizations of the learnt meta-loss by varying one input dimension (prediction score). The source model is a ResNet-26 trained with cross-entropy on the SVHN dataset. (a) The learnt meta-loss when adapting to the MNIST test dataset. (b) The learnt meta-loss when adapting to the USPS test dataset. 3. Various Severity Levels: In Figure 9, we vary the severity level of the noise, keeping the noise type ﬁxed. 4. Dataset and Architecture: In Figure 10, we compare the learnt meta-loss when adapting to speckle noise, for different source classiﬁer architectures (ResNet26 and ResNet50) and different source training dataset (CIFAR10 and CIFAR100). In all the cases, we again observe that the learnt meta-loss can be well approximated by a temperature scaled softmax-entropy. 5. Squared Loss : Finally, in Figure 11 we show the learnt meta-loss for classiﬁers trained with squared loss function instead of cross-entropy. We observe that in this case, the learnt meta loss mimics a quadratic function as expected from the conjugate formulation. 22For each of the learnt meta losses, we also show the values (α,T,C ) we use to ﬁt the meta loss with softmax entropy function: α·H(softmax(x/T)) −C. Note that although the learnt meta-loss can be approximated by the conjugate, the parameters α,T,C differ across the settings. In the case of classiﬁers trained with squared loss, we ﬁt the meta loss with a quadratic function∑K i=1(A·x2 i + C), where Kis the number of classes and xis the logit vector. Again, we also show the ﬁtted parameter value A,C. The meta loss follows the trend of a quadratic function. The ﬁtted quadratic function performs better or similar as the meta loss, while the parameters of the ﬁtted quadratic function remain different across the meta learning setup (base classiﬁer architectures and noise types). (a)  (b) (c)  (d) Figure 8: Visualization of meta loss (blue) learnt from various noise types in CIFAR-10-C validation set, where base classiﬁers are trained with cross-entropy loss. We show the error of meta loss, softmax entropy and ﬁtted entropy for test-time adaptation on the corresponding noise types. We also show the parameters (α,T,C ) in the ﬁtted entropy. 23(a)  (b) (c)  (d) Figure 9: Visualization of meta loss (blue) learnt on speckle noise with different severity level for CIFAR-10-C, where base classiﬁers are trained with cross-entropy loss. We show the error of meta loss, softmax entropy and ﬁtted entropy for test-time adaptation on the corresponding noise types. We also show the parameters (α,T,C ) in the ﬁtted entropy. 24(a)  (b) (c)  (d) Figure 10: Visualization of meta loss (blue) learnt across datasets (CIFAR-10-C/CIFAR-100-C) and base classiﬁer architectures (ResNet-26/ResNet-50), where base classiﬁers are trained with cross-entropy loss. We show the error of meta loss, softmax entropy and ﬁtted entropy for test-time adaptation on the corresponding noise types. We also show the parameters ( α,T,C ) in the ﬁtted entropy. (a)  (b) Figure 11: Visualization of meta loss (blue), where base classiﬁer is trained with quadratic loss. We show the error of meta loss, softmax entropy and ﬁtted quadratic function for test-time adaptation on the corresponding noise types. We also show the parameters ( A,B,C ) in the ﬁtted quadratic function. 25",
      "meta_data": {
        "arxiv_id": "2207.09640v2",
        "authors": [
          "Sachin Goyal",
          "Mingjie Sun",
          "Aditi Raghunathan",
          "Zico Kolter"
        ],
        "published_date": "2022-07-20T04:02:19Z",
        "pdf_url": "https://arxiv.org/pdf/2207.09640v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper addresses the challenge of selecting an effective unsupervised objective (TTA loss) for Test-Time Adaptation (TTA) of neural networks to distribution shifts using only unlabeled test samples. The authors first demonstrate through meta-learning that the 'best' TTA loss is remarkably similar to temperature-scaled softmax-entropy for cross-entropy trained classifiers, but different for classifiers trained with other losses (e.g., negative squared error for squared loss). To explain this phenomenon, they propose a generic recipe for TTA loss design based on the convex conjugate of the supervised training loss. This conjugate adaptation loss, when applied, recovers both the meta-learned 'best' losses and existing TTA methods like TENT. The method is interpreted as self-training using 'conjugate pseudo-labels' (gradient of the function f from the training loss formulation). Empirically, their approach consistently outperforms other TTA alternatives across various domain adaptation benchmarks, especially for classifiers trained with novel loss functions like PolyLoss.",
        "methodology": "The methodology involves two main parts: an exploratory meta-learning phase and the proposed convex conjugate framework. For exploration, a neural network `mφ` parameterized the TTA loss, learned via meta-learning by differentiating through the adaptation process to find parameters `φ` that optimize performance on distribution shifts. The core proposed method introduces a framework where common losses `L(h(x),y) = f(h(x)) −yTh(x)` are analyzed using their convex conjugate `f⋆`. The conjugate adaptation loss is defined as `Lconj(hθ(x)) = −f⋆(∇f(hθ(x)))`, which simplifies to `f(hθ(x)) −∇f(hθ(x))⊤hθ(x)`. This formulation recovers softmax-entropy for cross-entropy and a negative squared loss for squared loss. The method is interpreted as self-training with 'conjugate pseudo-labels' `˜yCPL θ (x) = ∇f(hθ(x))`. The adaptation process (Algorithm 1) involves iteratively updating model parameters by minimizing `L(¯hθ(x), ˜yCPL θ (x))` where `¯hθ(x)` is a temperature-scaled predictor, and `˜yCPL θ (x)` are the conjugate pseudo-labels derived from the current model predictions. Model adaptation typically involves updating only the learnable scale and shift parameters of batch normalization layers, with batch normalization statistics also updated per batch.",
        "experimental_setup": "The method was evaluated on three common corruption benchmarks: CIFAR-10-C, CIFAR-100-C, and ImageNet-C. It also included three domain adaptation datasets: SVHN to MNIST, ImageNet to ImageNet-R, and synthetic to real data in VISDA-C. Source classifier architectures used were ResNet-26 for CIFAR-10/100, ResNet-18 for SVHN to MNIST, and ResNet-50 for ImageNet and VisDA-C. Source classifiers were trained using cross-entropy, PolyLoss, and squared loss. Baselines included hard pseudo-labeling, soft pseudo-labeling (softmax), Entropy Minimization (ENT), Robust Pseudo-Label, and MEMO (a batch version). Performance was reported as average error across corruptions at the highest severity for CIFAR-10/100-C, and averaged across corruptions and severity for ImageNet-C. Hyperparameters, including learning rate (`{1e−1, 1e−2,... 1e−4}`) and temperature (`{1, 2,... 5}`), were tuned using grid-search on validation noises in corruption benchmarks. For domain adaptation datasets without held-out target domains, a temperature of 1 and previously suggested learning rates were used. Experiments were performed on A6000 GPUs.",
        "limitations": "The research acknowledges that the proposed framework does not fully explain what constitutes an optimal TTA loss. The meta-learning framework used for exploration was constrained to learning functions over individual input logits, suggesting potential for expansion to intermediate representations or interactions within input batches. Good TTA performance still relies on several heuristics, such as updating only batch normalization parameters. The critical role of temperature scaling, which improves all baselines, highlights the challenge that truly unsupervised TTA would require informed guesses for such hyperparameters. Additionally, it remains an open problem to identify the types of real-world distribution shifts where self-training-based approaches would be most beneficial.",
        "future_research_directions": "Future research directions include expanding the meta-learning framework to learn functions over intermediate representations and to account for interactions within input batches. Another area is to gain a more concrete understanding of the role of common heuristics in TTA, such as updating only batch normalization parameters and the effect of temperature scaling. Further research is needed to understand the specific real-world distribution shifts where self-training-based approaches are most effective. Finally, the authors suggest extending and applying the conjugate pseudo-labeling methodology to other machine learning settings, such as semi-supervised learning."
      }
    },
    {
      "title": "Active Test-Time Adaptation: Theoretical Analyses and An Algorithm",
      "abstract": "Test-time adaptation (TTA) addresses distribution shifts for streaming test\ndata in unsupervised settings. Currently, most TTA methods can only deal with\nminor shifts and rely heavily on heuristic and empirical studies.\n  To advance TTA under domain shifts, we propose the novel problem setting of\nactive test-time adaptation (ATTA) that integrates active learning within the\nfully TTA setting.\n  We provide a learning theory analysis, demonstrating that incorporating\nlimited labeled test instances enhances overall performances across test\ndomains with a theoretical guarantee. We also present a sample entropy\nbalancing for implementing ATTA while avoiding catastrophic forgetting (CF). We\nintroduce a simple yet effective ATTA algorithm, known as SimATTA, using\nreal-time sample selection techniques. Extensive experimental results confirm\nconsistency with our theoretical analyses and show that the proposed ATTA\nmethod yields substantial performance improvements over TTA methods while\nmaintaining efficiency and shares similar effectiveness to the more demanding\nactive domain adaptation (ADA) methods. Our code is available at\nhttps://github.com/divelab/ATTA",
      "full_text": "Published as a conference paper at ICLR 2024 ACTIVE TEST-TIME ADAPTATION : T HEORETICAL ANALYSES AND AN ALGORITHM Shurui Gui∗ Texas A&M University College Station, TX 77843 shurui.gui@tamu.edu Xiner Li* Texas A&M University College Station, TX 77843 lxe@tamu.edu Shuiwang Ji Texas A&M University College Station, TX 77843 sji@tamu.edu ABSTRACT Test-time adaptation (TTA) addresses distribution shifts for streaming test data in unsupervised settings. Currently, most TTA methods can only deal with minor shifts and rely heavily on heuristic and empirical studies. To advance TTA under domain shifts, we propose the novel problem setting of active test-time adaptation (ATTA) that integrates active learning within the fully TTA setting. We provide a learning theory analysis, demonstrating that incorporating limited labeled test instances enhances overall performances across test domains with a theoretical guarantee. We also present a sample entropy balancing for implementing ATTA while avoiding catastrophic forgetting (CF). We introduce a simple yet effective ATTA algorithm, known as SimATTA, using real-time sample selection techniques. Extensive experimental results confirm consistency with our theoretical analyses and show that the proposed ATTA method yields substantial performance improvements over TTA methods while maintaining efficiency and shares similar effectiveness to the more demanding active domain adaptation (ADA) methods. Our code is available at https://github.com/divelab/ATTA. 1 I NTRODUCTION Deep learning has achieved remarkable success across various fields, attaining high accuracy in numerous applications (Krizhevsky et al., 2017; Simonyan and Zisserman, 2014). Nonetheless, When training and test data follow distinct distributions, models often experience significant performance degradation during test. This phenomenon, known as the distribution shift or out-of-distribution (OOD) problem, is extensively studied within the context of both domain generalization (DG) (Gulra- jani and Lopez-Paz, 2020; Koh et al., 2021; Gui et al., 2022) and domain adaptation (DA) (Ganin et al., 2016; Sun and Saenko, 2016). While these studies involve intensive training of models with considerable generalization abilities towards target domains, they overlook an important application property; namely, continuous adaptivity to real-time streaming data under privacy, resource, and efficiency constraints. This gap leads to the emergence of test-time adaptation (TTA) tasks, targeting on-the-fly adaptation to continuous new domains during the test phase or application deployment. The study of TTA encompasses two main categories; namely test-time training (TTT) methods (Sun et al., 2020; Liu et al., 2021c) and fully test-time adaptation (FTTA) (Niu et al., 2023; Wang et al., 2021). The TTT pipeline incorporates retraining on the source data, whereas FTTA methods adapt arbitrary pre-trained models to the given test mini-batch by conducting entropy minimization, without access to the source data. Nevertheless, most TTA methods can only handle corrupted distribution shifts (Hendrycks and Dietterich, 2019b) (e.g., Gaussian noise,) and rely heavily on human intuition or empirical studies. To bridge this gap, our paper focuses on tackling significant domain distribution shifts in real time with theoretical insights. We investigate FTTA, which is more general and adaptable than TTT, particularly under data ac- cessibility, privacy, and efficiency constraints. Traditional FTTA aims at adapting a pre-trained model to streaming test-time data from diverse domains under unsupervised settings. However, recent works (Lin et al., 2022; Pearl, 2009) prove that it is theoretically infeasible to achieve OOD generalization without extra information such as environment partitions. Since utilizing environment partitions requires heavy pretraining, contradicting the nature of TTA, we are motivated to incorporate extra information in a different way,i.e., integrating a limited number of labeled test-time samples to alleviate distribution shifts, following the active learning (AL) paradigm (Settles, 2009). To this end, we propose the novel problem setting of active test-time adaptation (ATTA) by incorporating ∗Equal contributions 1 arXiv:2404.05094v1  [cs.LG]  7 Apr 2024Published as a conference paper at ICLR 2024 AL within FTTA. ATTA faces two major challenges; namely, catastrophic forgetting (CF) (Kemker et al., 2018; Li and Hoiem, 2017) and real-time active sample selection. CF problem arises when a model continually trained on a sequence of domains experiences a significant performance drop on previously learned domains, due to the inaccessibility of the source data and previous test data. Real-time active sample selection requires AL algorithms to select informative samples from a small buffer of streaming test data for annotation, without a complete view of the test distribution. In this paper, we first formally define the ATTA setting. We then provide its foundational analysis under the learning theory’s paradigm to guarantee the mitigation of distribution shifts and avoid CF. Aligned with our empirical validations, while the widely used entropy minimization (Wang et al., 2021; Grandvalet and Bengio, 2004) can cause CF, it can conversely become the key to preventing CF problems with our sample selection and balancing techniques. Building on the analyses, we then introduce a simple yet effective ATTA algorithm, SimATTA, incorporating balanced sample selections and incremental clustering. Finally, we conducted a comprehensive experimental study to evaluate the proposed ATTA settings with three different settings in the order of low to high requirement restrictiveness, i.e., TTA, Enhanced TTA, and Active Domain Adaptation (ADA). Intensive experiments indicate that ATTA jointly equips with the efficiency of TTA and the effectiveness of ADA, rendering an uncompromising real-time distribution adaptation direction. Comparison to related studies. Compared to TTA methods, ATTA requires extra active labels, but the failure of TTA methods (Sec. 5.1) and the theoretical proof of Lin et al. (2022); Pearl (2009) justify its necessity and rationality. Compared to active online learning, ATTA focuses on lightweight real-time fine-tuning without round-wise re-trainings as Saran et al. (2023) and emphasizes the importance of CF avoidance instead of resetting models and losing learned distributions. In fact, active online learning is partially similar to our enhanced TTA setting (Sec. 5.2. Compared to ADA methods (Prabhu et al., 2021; Ning et al., 2021), ATTA does not presuppose access to source data, model parameters, or pre-collected target samples. Furthermore, without this information, ATTA can still perform on par with ADA methods (Sec. 5.3). The recent source-free active domain adaptation (SFADA) method SALAD (Kothandaraman et al., 2023) still requires access to model parameter gradients, pre-collected target data, and training of additional networks. Our ATTA, in contrast, with non-regrettable active sample selection on streaming data, is a much lighter and more realistic approach distinct from ADA and SFADA. More related-work discussions are provided in Appx. C. 2 T HE ACTIVE TEST-TIME ADAPTATION FORMULATION TTA methods aim to solve distribution shifts by dynamically optimizing a pre-trained model based on streaming test data. We introduce the novel problem setting of Active Test-Time Adaptation (ATTA), which incorporates active learning during the test phase. In ATTA, the model continuously selects the most informative instances from the test batch to be labeled by an explicit or implicit oracle (e.g., human annotations, self-supervised signals) and subsequently learned by the model, aiming to improve future adaptations. Considering the labeling costs in real-world applications, a “budget” is established for labeled test instances. The model must effectively manage this budget distribution and ensure that the total number of label requests throughout the test phase does not surpass the budget. We now present a formal definition of the ATTA problem. Consider a pre-trained modelf(x; ϕ) with parameters ϕ trained on the source dataset DS = (x, y)|DS|, with each data sample x ∈ Xand a label y ∈ Y. We aim to adapt model parameters θ, initialized as ϕ, to an unlabeled test-time data stream. The streaming test data exhibit distribution shifts from the source data and varies continuously with time, forming multiple domains to which we must continuously adapt. The test phase commences at time step t = 1 and the streaming test data is formulated in batches. The samples are then actively selected, labeled (by the oracle) and collected as Dte(t) = ActAlg(Ute(t)), where ActAlg(·) denotes an active selection/labeling algorithm. The labeled samples Dte(t) are subsequently incorporated into the ATTA training setDtr(t). Finally, we conclude time step t by performing ATTA training, updating model parameters θ(t) using Dtr(t), with θ(t) initialized as the previous final state θ(t − 1). Definition 1 (The ATTA problem). Given a model f(x; θ), with parameters θ, initialized with parameters θ(0) = ϕ obtained by pre-training on source domain data, and streaming test data batches Ute(t) continually changing over time, the ATTA task aims to optimize the model at any time stept (with test phase commencing at t = 1) as θ(t)∗ := argmin θ(t) (E(x,y,t)∈Dtr(t)[ℓCE (f(x; θ(t)), y)] + E(x,t)∈Ute(t)[ℓU (f(x; θ(t)))]), (1) 2Published as a conference paper at ICLR 2024 where Dtr(t) = ( ∅, t = 0 Dtr(t − 1) ∪ Dte(t), t ≥ 1, s.t. |Dtr(t)| ≤ B, (2) Dte(t) = ActAlg(Ute(t)) is actively selected and labeled, ℓCE is the cross entropy loss, ℓU is an unsupervised learning loss, and B is the budget. 3 T HEORETICAL STUDIES In this section, we conduct an in-depth theoretical analysis of TTA based on learning theories. We mainly explore two questions: How can significant distribution shifts be effectively addressed under the TTA setting? How can we simultaneously combat the issue of CF? Sec. 3.1 provides a solution with theoretical guarantees to the first question, namely, active TTA (ATTA), along with the conditions under which distribution shifts can be well addressed. Sec. 3.2 answers the second question with an underexplored technique, i.e., selective entropy minimization, building upon the learning bounds established in Sec. 3.1. We further validate these theoretical findings through experimental analysis. Collectively, we present a theoretically supported ATTA solution that effectively tackles both distribution shift and CF. 3.1 A LLEVIATING DISTRIBUTION SHIFTS THROUGH ACTIVE TEST-TIME ADAPTATION Traditional TTA is performed in unsupervised or self-supervised context. In contrast, ATTA introduces supervision into the adaptation setting. In this subsection, we delve into learning bounds and establish generalization bounds to gauge the efficacy of ATTA in solving distribution shifts. We scrutinize the influence of active learning and evidence that the inclusion of labeled test instances markedly enhances overall performances across incremental test domains. Following Kifer et al. (2004), we examine statistical guarantees for binary classification. A hypothesis is a function h : X → {0, 1}, which can serve as the prediction function within this context. In the ATTA setting, the mapping ofh varies with time as h(x, t). We use H∆H-distance following Ben- David et al. (2010), which essentially provides a measure to quantify the distribution shift between two distributions D1 and D2, and can also be applied between datasets. The probability that an estimated hypothesis h disagrees with the true labeling function g : X → {0, 1} according to distribution D is defined as ϵ(h(t), g) = E(x)∼D[|h(x, t) − g(x)|], which we also refer to as the error or risk ϵ(h(t)). While the source data is inaccessible under ATTA settings, we consider the existence of source dataset DS for accurate theoretical analysis. Thus, we initialize Dtr as Dtr(0) = DS. For every time step t, the test and training data can be expressed asUte(t) and Dtr(t) = DS ∪Dte(1) ∪Dte(2) ∪···∪ Dte(t). Building upon two lemmas (provided in Appx. D), we establish bounds on domain errors under the ATTA setting when minimizing the empirical weighted error using the hypothesish at time t. Theorem 1. Let H be a hypothesis class of VC-dimension d. At time step t, for ATTA data domains DS, Ute(1), ··· , Ute(t), ··· , Si are unlabeled samples of sizem sampled from each of thet+1 domains respectively. The total number of samples in Dtr(t) is N and the ratio of sample numbers in each component is λ = (λ0, ··· , λt). If ˆh(t) ∈ Hminimizes the empirical weighted error ˆϵw(h(t)) with the weight vector w = (w0, ··· , wt) on Dtr(t), and h∗ j (t) = arg minh∈H ϵj(h(t)) is the optimal hypothesis on the jth domain, then for any δ ∈ (0, 1), with probability of at least 1 − δ, we have ϵj(ˆh(t)) ≤ ϵj(h∗ j (t)) + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   + 2C, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. For future test domains j = t + k (k >0), assuming k′ = argmink′∈{0,1,...t} dH∆H(D(k′), Ute(t + k)) and min dH∆H (D(k′), Ute(t + k)) ≤ δD, where 0 ≤ δD ≪ +∞, then ∀δ, with probability of at least 1 − δ, we have ϵt+k(ˆh(t)) ≤ ϵt+k(h∗ t+k(t)) + tX i=0 wi  ˆdH∆H(Si, Sk′ ) + 4 s 2d log(2m) + log 2 δ m + δD + 2γi   + 2C. The adaptation performance on a test domain is majorly bounded by the composition of (labeled) training data, estimated distribution shift, and ideal joint hypothesis performance, which correspond to C, ˆdH∆H(Si, Sj), and γi, respectively. The ideal joint hypothesis error γi gauges the inherent adaptability between domains. Further theoretical analysis are in Appx. D. 3Published as a conference paper at ICLR 2024 Figure 1: (a) Empirical validation of Thm. 1. We train a series of models on N = 2000 samples from the PACS (Li et al., 2017) dataset given differentλ0 and w0 and display the test domain loss of each model. Red points are the test loss minimums given a fixed λ0. The orange line is the reference where w0 = λ0. We observe that w0 with loss minimums are located closed to the orange line but slightly smaller than λ0, which validates our findings in Eq. (4). (b) Empirical analysis with an uncertainty balancing. Given source pre-trained models, we fine-tune the models on 500 samples with different λ0 and w0, and display the combined error surface of test and source error. Although a small λ0 is good for test domain error, it can lead to non-trivial source error exacerbation. Therefore, we can observe that the global loss minimum (green X) locates in a relatively high-λ0 region. If we consider the multiple test data distributions as a single test domain,i.e., St i=1 Ute(i), Thm. 1 can be reduced into bounds for the source domain error ϵS and test domain error ϵT . Given the optimal test/source hypothesis h∗ T (t) = arg minh∈H ϵT (h(t)) and h∗ S(t) = arg minh∈H ϵS(h(t)), we have |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤w0A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (3a) |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤(1 − w0)A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (3b) where the distribution divergence termA = ˆdH∆H(S0, ST )+4 q 2d log(2m)+log 2 δ m +2γ, the empirical gap term B = 2 q d log(2N)−log(δ) 2N , ST is sampled from St i=1 Ute(i), and γ = minh∈H{ϵ0(h(t)) + ϵT (h(t))}. Our learning bounds demonstrates the trade-off between the small amount of budgeted test-time data and the large amount of less relevant source data. Next, we provide an approximation of the condition necessary to achieve optimal adaptation performance, which is calculable from finite samples and can be readily applied in practical ATTA scenarios. Following Eq. (3.a), with approximatelyB = c1 p d/N, the optimal value w∗ 0 to tighten the test error bound is a function of λ0 and A: w∗ 0 = λ0 − s A2N c2 1d − A2Nλ0(1 − λ0), for λ 0 ≥ 1 − d A2N , (4) where c1 is a constant. Note that λ0 ≥ 1 − d A2N should be the satisfied condition in practical ATTA settings, where the budget is not sufficiently big while the source data amount is relatively large. The following theorem offers a direct theoretical guarantee that ATTA reduces the error bound on test domains in comparison to TTA without the integration of active learning. Theorem 2. Let H be a hypothesis class of VC-dimension d. For ATTA data domains DS, Ute(1), Ute(2), ··· , Ute(t), considering the test-time data as a single test domain St i=1 Ute(i), if ˆh(t) ∈ H minimizes the empirical weighted error ˆϵw(h(t)) with the weight vector w on Dtr(t), let the test error be upper-bounded with |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤EBT (w, λ, N, t). Let w′ and λ′ be the weight and sample ratio vectors when no active learning is included, i.e., w′ and λ′ s.t. w′ 0 = λ′ 0 = 1 and w′ i = λ′ i = 0 for i ≥ 1, then for any λ ̸= λ′, there exists w s.t. EBT (w, λ, N, t) < EBT (w′, λ′, N, t). (5) Therefore, the incorporation of labeled test instances in ATTA theoretically enhances the overall performance across test domains, substantiating the significance of the ATTA setting in addressing distribution shifts. All proofs are provided in Appx. E. Finally, we support the theoretical findings with experimental analysis and show the numerical results of applying the principles on real-world datasets, as shown in Fig. 1. For rigorous analysis, note that our theoretical results rest on the underlying condition that N should at least be of the same scale as d, according to the principles of VC-dimension theory. The empirical alignment of our experiments with the theoretical framework can be attributed to the assumption that fine-tuning a model is roughly equivalent to learning a model with a relatively small d. Experiment details and other validations can be found in Appx. H. 4Published as a conference paper at ICLR 2024 3.2 M ITIGATING CATASTROPHIC FORGETTING WITH BALANCED ENTROPY MINIMIZATION Catastrophic forgetting (CF), within the realm of Test-Time Adaptation (TTA), principally manifests as significant declines in overall performance, most notably in the source domain. Despite the lack of well-developed learning theories for analyzing training with series data, empirical studies have convincingly illustrated the crucial role of data sequential arrangement in model learning, thereby accounting for the phenomenon of CF. Traditionally, the mitigation of CF in adaptation tasks involves intricate utilization of source domain data. However, under FTTA settings, access to the source dataset is unavailable, leaving the problem of CF largely unexplored in the data-centric view. Table 1: Correlation analysis of high/low en- tropy samples and domains. We use a source pre-trained model to select samples with low- est/highest entropy, and 1.retrain the model on 2000 samples; 2.fine-tune the model on 300 sam- ples. We report losses on source/test domains for each setting, showing that low-entropy samples form distributions close to the source domain. Sample type Retrain Fine-tune ϵS ϵT ϵS ϵT Low entropy 0.5641 0.8022 0.0619 1.8838 High entropy 2.5117 0.3414 0.8539 0.7725 To overcome this challenge of source dataset ab- sence, we explore the acquisition of “source-like” data. In TTA scenarios, it is generally assumed that the amount of source data is considerably large. We also maintain this assumption in ATTA, practically assuming the volume of source data greatly surpasses the test-time budget. As a re- sult, we can safely assume that the pre-trained model is well-trained on abundant source do- main data DS. Given this adequately trained source model, we can treat it as a “true” source data labeling function f(x; ϕ). The model es- sentially describes a distribution, Dϕ,S(X, Y) = {(x, ˆy) ∈ (X, Y) | ˆy = f(x; ϕ), x∈ DS}. The entropy of the model prediction is defined as H(ˆy) = −P c p(ˆyc) logp(ˆyc), ˆy = f(x; ϕ), where c denotes the class. Lower entropy indicates that the model assigns high probability to one of the classes, suggesting a high level of certainty or confidence in its prediction, which can be interpreted as the sample being well-aligned or fitting closely with the model’s learned distribution. In other words, the model recognizes the sample as being similar to those it was trained on. Thus entropy can be used as an indicator of how closely a sample x aligns with the model distribution Dϕ,S. Since the model distribution is approximately the source distribution, selecting (and labeling) low-entropy samples using f(x; ϕ) essentially provides an estimate of sampling from the source dataset. Therefore, in place of the inaccessible DS, we can feasibly include the source-like dataset into the ATTA training data at each time stept: Dϕ,S(t) = {(x, f(x; ϕ))|x ∈ Ute(t), H(f(x; ϕ)) < el}, (6) where el is the entropy threshold. The assumption that Dϕ,S(t) is an approximation of DS can be empirically validated, as shown by the numerical results on PACS in Tab. 1. In contrast, high-entropy test samples typically deviate more from the source data, from which we select Dte(t) for active labeling. Following the notations in Thm. 1, we are practically minimizing the empirical weighted error of hypothesis h(t) as ˆϵ′ w(h(t)) = tX j=0 wjˆϵj(h(t)) = w0 λ0N X x∈Dϕ,S(t) |h(x, t) − f(x; ϕ)| + tX j=1 wj λjN X x,y∈Dte(j) |h(x, t) − y|. (7) By substituting DS with Dϕ,S(t) in Thm. 1, the bounds of Thm. 1 continue to hold for the test domains. In the corollary below, we bound the source error for practical ATTA at each time stept. Corollary 3. At time step t, for ATTA data domains Dϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), Si are unla- beled samples of size m sampled from each of the t + 1 domains respectively, and SS is unlabeled samples of size m sampled from DS. If ˆh(t) ∈ Hminimizes ˆϵ′ w(h(t)) while other conditions remain identical to Thm. 1, then ϵS(ˆh(t)) ≤ ϵS(h∗ S(t)) + tX i=0 wi  ˆdH∆H(Si, SS) + 4 s 2d log(2m) + log 2 δ m + 2γi   + 2C, with probability at least 1 − δ, where C follows Thm. 1 and γi = minh∈H{ϵi(h(t)) + ϵS(h(t))}. Further analysis and proofs are in Appx. D and E. The following corollary provides direct theoretical support that our strategy conditionally reduces the error bound on the source domain. Corollary 4. At time step t, for ATTA data domains Dϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), suppose that ˆh(t) ∈ Hminimizes ˆϵw′(h(t)) under identical conditions to Thm. 2. Let’s denote the source error upper bound with |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤EBS(w, λ, N, t). Let w′ and λ′ be the weight 5Published as a conference paper at ICLR 2024 <latexit sha1_base64=\"NxhXSyFABPQk4q8627/odirDspg=\">AAAB9XicbVDLSgMxFM34rPVVdekmWARXZab4WhbcuKzYF7S1ZNI7bWgmMyR3lDL0P9y4UMSt/+LOvzHTdqGtBwKHc87l3hw/lsKg6347K6tr6xubua389s7u3n7h4LBhokRzqPNIRrrlMwNSKKijQAmtWAMLfQlNf3ST+c1H0EZEqobjGLohGygRCM7QSg/3mIWFGtAaGOwVim7JnYIuE29OimSOaq/w1elHPAlBIZfMmLbnxthNmUbBJUzyncRAzPiIDaBtqWIhmG46vXpCT63Sp0Gk7VNIp+rviZSFxoxD3yZDhkOz6GXif147weC6mwoVJwiKzxYFiaQY0awC2hcaOMqxJYxrYW+lfMg042iLytsSvMUvL5NGueRdli7uysXK+byOHDkmJ+SMeOSKVMgtqZI64USTZ/JK3pwn58V5dz5m0RVnPnNE/sD5/AFnsJJq</latexit> Streaming Test <latexit sha1_base64=\"a41BOKrutEYSWO9+8CjkPZKHvb8=\">AAAB73icbVBNS8NAEJ3Ur1q/qh69BIvgqSTiR48FLx4r2A9oQ9lsN+3SzSbuToQQ+ie8eFDEq3/Hm//GTZuDtj4YeLw3w8w8PxZco+N8W6W19Y3NrfJ2ZWd3b/+genjU0VGiKGvTSESq5xPNBJesjRwF68WKkdAXrOtPb3O/+8SU5pF8wDRmXkjGkgecEjRSbzAhmKWzyrBac+rOHPYqcQtSgwKtYfVrMIpoEjKJVBCt+64To5cRhZwKNqsMEs1iQqdkzPqGShIy7WXze2f2mVFGdhApUxLtufp7IiOh1mnom86Q4EQve7n4n9dPMGh4GZdxgkzSxaIgETZGdv68PeKKURSpIYQqbm616YQoQtFElIfgLr+8SjoXdfe6fnV/WWs2ijjKcAKncA4u3EAT7qAFbaAg4Ble4c16tF6sd+tj0Vqyiplj+APr8wfpIY/e</latexit> ˆy <latexit sha1_base64=\"SJEOE2ZYxLL1SU/QahOlMH6fop4=\">AAAB8HicbVBNSwMxEM3Wr1q/qh69BItQL2VX/Oix4MVjBbettEvJptk2NMkuyaxQlv4KLx4U8erP8ea/MW33oK0PBh7vzTAzL0wEN+C6305hbX1jc6u4XdrZ3ds/KB8etUycasp8GotYd0JimOCK+cBBsE6iGZGhYO1wfDvz209MGx6rB5gkLJBkqHjEKQErPfr9DNi0Cuf9csWtuXPgVeLlpIJyNPvlr94gpqlkCqggxnQ9N4EgIxo4FWxa6qWGJYSOyZB1LVVEMhNk84On+MwqAxzF2pYCPFd/T2REGjORoe2UBEZm2ZuJ/3ndFKJ6kHGVpMAUXSyKUoEhxrPv8YBrRkFMLCFUc3srpiOiCQWbUcmG4C2/vEpaFzXvunZ1f1lp1PM4iugEnaIq8tANaqA71EQ+okiiZ/SK3hztvDjvzseiteDkM8foD5zPH2KnkB4=</latexit> U te ( t ) <latexit sha1_base64=\"7rdY0fXtveVAqOkqa7z+i6K3Rp0=\">AAAB+XicbVDLSsNAFJ34rPUVdelmsAh1UxLxUXBTcOOygn1AE8pkMmmHTiZh5qZQQv/EjQtF3Pon7vwbp20W2nrgwuGce7n3niAVXIPjfFtr6xubW9ulnfLu3v7BoX103NZJpihr0UQkqhsQzQSXrAUcBOumipE4EKwTjO5nfmfMlOaJfIJJyvyYDCSPOCVgpL5tR1WPhgncYQ+GDMhF3644NWcOvErcglRQgWbf/vLChGYxk0AF0brnOin4OVHAqWDTspdplhI6IgPWM1SSmGk/n18+xedGCXGUKFMS8Fz9PZGTWOtJHJjOmMBQL3sz8T+vl0FU93Mu0wyYpItFUSYwJHgWAw65YhTExBBCFTe3YjokilAwYZVNCO7yy6ukfVlzb2rXj1eVRr2Io4RO0RmqIhfdogZ6QE3UQhSN0TN6RW9Wbr1Y79bHonXNKmZO0B9Ynz9h0pLV</latexit> f ( · ; ✓ ) <latexit sha1_base64=\"ud3dFXm+F2nsLD2/MdusutzkLvU=\">AAAB9HicbVDLSgNBEJyNrxhfUY9eBoPgKeyKr2PAixchgnlAsoTZ2d5kyMzOOjMbDEu+w4sHRbz6Md78GyfJHjSxoKGo6qa7K0g408Z1v53Cyura+kZxs7S1vbO7V94/aGqZKgoNKrlU7YBo4CyGhmGGQztRQETAoRUMb6Z+awRKMxk/mHECviD9mEWMEmMlvysC+ZTdyRD4pNQrV9yqOwNeJl5OKihHvVf+6oaSpgJiQznRuuO5ifEzogyjHCalbqohIXRI+tCxNCYCtJ/Njp7gE6uEOJLKVmzwTP09kRGh9VgEtlMQM9CL3lT8z+ukJrr2MxYnqYGYzhdFKcdG4mkCOGQKqOFjSwhVzN6K6YAoQo3NaRqCt/jyMmmeVb3L6sX9eaV2nsdRREfoGJ0iD12hGrpFddRAFD2iZ/SK3pyR8+K8Ox/z1oKTzxyiP3A+fwCmlpH9</latexit> Model SimATTA <latexit sha1_base64=\"bhVea6W/pzUPuDRNfs2xbDF7qAk=\">AAAB73icbVC7SgNBFL3rM8ZX1NJmMAhWYTf4KgM2FhYRzAOSJcxOZpMhs7PrzF0hhPyEjYUitv6OnX/jbLKFJh4YOJxzD3PvCRIpDLrut7Oyura+sVnYKm7v7O7tlw4OmyZONeMNFstYtwNquBSKN1Cg5O1EcxoFkreC0U3mt564NiJWDzhOuB/RgRKhYBSt1L6jQRYd9Eplt+LOQJaJl5My5Kj3Sl/dfszSiCtkkhrT8dwE/QnVKJjk02I3NTyhbEQHvGOpohE3/mS275ScWqVPwljbp5DM1N+JCY2MGUeBnYwoDs2il4n/eZ0Uw2t/IlSSIlds/lGYSoIxyY4nfaE5Qzm2hDIt7K6EDammDG1FRVuCt3jyMmlWK95l5eK+Wq6d53UU4BhO4Aw8uIIa3EIdGsBAwjO8wpvz6Lw4787HfHTFyTNH8AfO5w/1SI/i</latexit> Labeling <latexit sha1_base64=\"7rdY0fXtveVAqOkqa7z+i6K3Rp0=\">AAAB+XicbVDLSsNAFJ34rPUVdelmsAh1UxLxUXBTcOOygn1AE8pkMmmHTiZh5qZQQv/EjQtF3Pon7vwbp20W2nrgwuGce7n3niAVXIPjfFtr6xubW9ulnfLu3v7BoX103NZJpihr0UQkqhsQzQSXrAUcBOumipE4EKwTjO5nfmfMlOaJfIJJyvyYDCSPOCVgpL5tR1WPhgncYQ+GDMhF3644NWcOvErcglRQgWbf/vLChGYxk0AF0brnOin4OVHAqWDTspdplhI6IgPWM1SSmGk/n18+xedGCXGUKFMS8Fz9PZGTWOtJHJjOmMBQL3sz8T+vl0FU93Mu0wyYpItFUSYwJHgWAw65YhTExBBCFTe3YjokilAwYZVNCO7yy6ukfVlzb2rXj1eVRr2Io4RO0RmqIhfdogZ6QE3UQhSN0TN6RW9Wbr1Y79bHonXNKmZO0B9Ynz9h0pLV</latexit> f ( · ; ✓ ) <latexit sha1_base64=\"DPrA95GNP27SFW5vSoLC/hYa644=\">AAAB9XicbVDLSsNAFJ3UV62vqks3g0Wom5KIj4KbghuXFewDmlgmk0k7dJIJMzdKCf0PNy4Uceu/uPNvnLZZaOuBC4dz7uXee/xEcA22/W0VVlbX1jeKm6Wt7Z3dvfL+QVvLVFHWolJI1fWJZoLHrAUcBOsmipHIF6zjj26mfueRKc1lfA/jhHkRGcQ85JSAkR7CqksDCdfYTYb8tF+u2DV7BrxMnJxUUI5mv/zlBpKmEYuBCqJ1z7ET8DKigFPBJiU31SwhdEQGrGdoTCKmvWx29QSfGCXAoVSmYsAz9fdERiKtx5FvOiMCQ73oTcX/vF4KYd3LeJykwGI6XxSmAoPE0whwwBWjIMaGEKq4uRXTIVGEggmqZEJwFl9eJu2zmnNZu7g7rzTqeRxFdISOURU56Ao10C1qohaiSKFn9IrerCfrxXq3PuatBSufOUR/YH3+AFKlkbs=</latexit> f ( · ; \u0000 ) <latexit sha1_base64=\"DPrA95GNP27SFW5vSoLC/hYa644=\">AAAB9XicbVDLSsNAFJ3UV62vqks3g0Wom5KIj4KbghuXFewDmlgmk0k7dJIJMzdKCf0PNy4Uceu/uPNvnLZZaOuBC4dz7uXee/xEcA22/W0VVlbX1jeKm6Wt7Z3dvfL+QVvLVFHWolJI1fWJZoLHrAUcBOsmipHIF6zjj26mfueRKc1lfA/jhHkRGcQ85JSAkR7CqksDCdfYTYb8tF+u2DV7BrxMnJxUUI5mv/zlBpKmEYuBCqJ1z7ET8DKigFPBJiU31SwhdEQGrGdoTCKmvWx29QSfGCXAoVSmYsAz9fdERiKtx5FvOiMCQ73oTcX/vF4KYd3LeJykwGI6XxSmAoPE0whwwBWjIMaGEKq4uRXTIVGEggmqZEJwFl9eJu2zmnNZu7g7rzTqeRxFdISOURU56Ao10C1qohaiSKFn9IrerCfrxXq3PuatBSufOUR/YH3+AFKlkbs=</latexit> f ( · ; \u0000 ) <latexit sha1_base64=\"ipQ+JKlINPDcPjrbUYUkqyyzp40=\">AAAB+nicbVC7TsMwFHXKq5RXCiOLRYXEQpVUvMZKLIxF0IfURpXj3LRWHSeyHVBV+iksDCDEypew8Te4aQZoOZKlo3Puy8dPOFPacb6twsrq2vpGcbO0tb2zu2eX91sqTiWFJo15LDs+UcCZgKZmmkMnkUAin0PbH13P/PYDSMVica/HCXgRGQgWMkq0kfp2+S6bdNqQoCUxQ4K+XXGqTga8TNycVFCORt/+6gUxTSMQmnKiVNd1Eu1NiNSMcpiWeqmChNARGUDXUEEiUN4kO32Kj40S4DCW5gmNM/V3x4RESo0j31RGRA/VojcT//O6qQ6vvAkTSapB0PmiMOVYx3iWAw6YBKr52BBCJTO3YjokklBt0iqZENzFLy+TVq3qXlTPb2uV+lkeRxEdoiN0glx0ieroBjVQE1H0iJ7RK3qznqwX6936mJcWrLznAP2B9fkDSAyT+w==</latexit> Source-Pretrained <latexit sha1_base64=\"ud3dFXm+F2nsLD2/MdusutzkLvU=\">AAAB9HicbVDLSgNBEJyNrxhfUY9eBoPgKeyKr2PAixchgnlAsoTZ2d5kyMzOOjMbDEu+w4sHRbz6Md78GyfJHjSxoKGo6qa7K0g408Z1v53Cyura+kZxs7S1vbO7V94/aGqZKgoNKrlU7YBo4CyGhmGGQztRQETAoRUMb6Z+awRKMxk/mHECviD9mEWMEmMlvysC+ZTdyRD4pNQrV9yqOwNeJl5OKihHvVf+6oaSpgJiQznRuuO5ifEzogyjHCalbqohIXRI+tCxNCYCtJ/Njp7gE6uEOJLKVmzwTP09kRGh9VgEtlMQM9CL3lT8z+ukJrr2MxYnqYGYzhdFKcdG4mkCOGQKqOFjSwhVzN6K6YAoQo3NaRqCt/jyMmmeVb3L6sX9eaV2nsdRREfoGJ0iD12hGrpFddRAFD2iZ/SK3pyR8+K8Ox/z1oKTzxyiP3A+fwCmlpH9</latexit> Model <latexit sha1_base64=\"5LNAmmVR/AN9Lc2T+FRV/is2yz8=\">AAAB8nicbVDLSgNBEJyNrxhfUY9eBoPgKewGX8eACB48RDAP2CxhdjKbDJmdWWZ6lbDkM7x4UMSrX+PNv3GS7EETCxqKqm66u8JEcAOu++0UVlbX1jeKm6Wt7Z3dvfL+QcuoVFPWpEoo3QmJYYJL1gQOgnUSzUgcCtYOR9dTv/3ItOFKPsA4YUFMBpJHnBKwkn+nnvCNBK2Sca9ccavuDHiZeDmpoByNXvmr21c0jZkEKogxvucmEGREA6eCTUrd1LCE0BEZMN9SSWJmgmx28gSfWKWPI6VtScAz9fdERmJjxnFoO2MCQ7PoTcX/PD+F6CrIuExSYJLOF0WpwKDw9H/c55pREGNLCNXc3orpkGhCwaZUsiF4iy8vk1at6l1Uz+9rlfpZHkcRHaFjdIo8dInq6BY1UBNRpNAzekVvDjgvzrvzMW8tOPnMIfoD5/MHKbiRJQ==</latexit> Low Entropy <latexit sha1_base64=\"vLgKkEyV9E/djVdgAkvKuOUQOTU=\">AAAB7nicbVDLSgMxFL1TX7W+qi7dBIvgqswUX8uCG5cV7QPaoWTSTBuaZEKSEcrQj3DjQhG3fo87/8a0nYW2HrhwOOde7r0nUpwZ6/vfXmFtfWNzq7hd2tnd2z8oHx61TJJqQpsk4YnuRNhQziRtWmY57ShNsYg4bUfj25nffqLasEQ+2omiocBDyWJGsHVS+wELxanplyt+1Z8DrZIgJxXI0eiXv3qDhKSCSks4NqYb+MqGGdaWEU6npV5qqMJkjIe066jEgpowm587RWdOGaA40a6kRXP190SGhTETEblOge3ILHsz8T+vm9r4JsyYVKmlkiwWxSlHNkGz39GAaUosnziCiWbuVkRGWGNiXUIlF0Kw/PIqadWqwVX18r5WqV/kcRThBE7hHAK4hjrcQQOaQGAMz/AKb57yXrx372PRWvDymWP4A+/zB19wj48=</latexit> Samples <latexit sha1_base64=\"wuZucU3JbeEJSquG2WgqGdYMCR8=\">AAAB83icbVDLSgMxFL3js9ZX1aWbYBFclZnia1kQocsK9gHtUDJppg3NJCHJCGXob7hxoYhbf8adf2PazkJbD1w4nHMv994TKc6M9f1vb219Y3Nru7BT3N3bPzgsHR23jEw1oU0iudSdCBvKmaBNyyynHaUpTiJO29H4bua3n6g2TIpHO1E0TPBQsJgRbJ3Uq7PhCN0Lq6Wa9Etlv+LPgVZJkJMy5Gj0S1+9gSRpQoUlHBvTDXxlwwxrywin02IvNVRhMsZD2nVU4ISaMJvfPEXnThmgWGpXwqK5+nsiw4kxkyRynQm2I7PszcT/vG5q49swY0KllgqyWBSnHFmJZgGgAdOUWD5xBBPN3K2IjLDGxLqYii6EYPnlVdKqVoLrytVDtVy7zOMowCmcwQUEcAM1qEMDmkBAwTO8wpuXei/eu/exaF3z8pkT+APv8wfIYpF9</latexit> High Entropy <latexit sha1_base64=\"vLgKkEyV9E/djVdgAkvKuOUQOTU=\">AAAB7nicbVDLSgMxFL1TX7W+qi7dBIvgqswUX8uCG5cV7QPaoWTSTBuaZEKSEcrQj3DjQhG3fo87/8a0nYW2HrhwOOde7r0nUpwZ6/vfXmFtfWNzq7hd2tnd2z8oHx61TJJqQpsk4YnuRNhQziRtWmY57ShNsYg4bUfj25nffqLasEQ+2omiocBDyWJGsHVS+wELxanplyt+1Z8DrZIgJxXI0eiXv3qDhKSCSks4NqYb+MqGGdaWEU6npV5qqMJkjIe066jEgpowm587RWdOGaA40a6kRXP190SGhTETEblOge3ILHsz8T+vm9r4JsyYVKmlkiwWxSlHNkGz39GAaUosnziCiWbuVkRGWGNiXUIlF0Kw/PIqadWqwVX18r5WqV/kcRThBE7hHAK4hjrcQQOaQGAMz/AKb57yXrx372PRWvDymWP4A+/zB19wj48=</latexit> Samples <latexit sha1_base64=\"1BO6D/gzkeZNQ7HNIaph5NqELCI=\">AAAB8nicbVDLSgMxFM3UV62vqks3wSK4KjPF17LgRncV7AOmQ8mkd9rQTDIkGaEM/Qw3LhRx69e482/MtLPQ1gOBwzn3kHtPmHCmjet+O6W19Y3NrfJ2ZWd3b/+genjU0TJVFNpUcql6IdHAmYC2YYZDL1FA4pBDN5zc5n73CZRmUjyaaQJBTEaCRYwSYyX/XlAFMQhD+KBac+vuHHiVeAWpoQKtQfWrP5Q0zdOUE619z01MkBFlGOUwq/RTDQmhEzIC31JBYtBBNl95hs+sMsSRVPYJg+fq70RGYq2ncWgnY2LGetnLxf88PzXRTZAxkaQGBF18FKUcG4nz+/GQKaCGTy0hVDG7K6Zjogg1tqWKLcFbPnmVdBp176p++dCoNS+KOsroBJ2ic+Sha9REd6iF2ogiiZ7RK3pzjPPivDsfi9GSU2SO0R84nz9y2ZFU</latexit> Incremental <latexit sha1_base64=\"Jmobmj50NeE6y3ftB4xt5xZD5Eg=\">AAAB8XicbVDLSgNBEOyNrxhfUY9eBoPgKewGX8dALh4jmAcmS5id9CZDZmeXmVkhLP6FFw+KePVvvPk3TpI9aGJBQ1HVTXdXkAiujet+O4W19Y3NreJ2aWd3b/+gfHjU1nGqGLZYLGLVDahGwSW2DDcCu4lCGgUCO8GkMfM7j6g0j+W9mSboR3QkecgZNVZ6aIhUG1Rcjgblilt15yCrxMtJBXI0B+Wv/jBmaYTSMEG17nluYvyMKsOZwKdSP9WYUDahI+xZKmmE2s/mFz+RM6sMSRgrW9KQufp7IqOR1tMosJ0RNWO97M3E/7xeasIbP+MySQ1KtlgUpoKYmMzeJ0OukBkxtYQyxe2thI2posymoEs2BG/55VXSrlW9q+rlXa1Sv8jjKMIJnMI5eHANdbiFJrSAgYRneIU3RzsvzrvzsWgtOPnMMfyB8/kDzgaQ+A==</latexit> Clustering <latexit sha1_base64=\"c4xrXg0yZYBSSDLHCxlf45OWNzg=\">AAAB7nicbVDLSgNBEOz1GeMr6tHLYBA8hd2Aj2PAi8eI5gHJEmYnnWTIzOwyMyuEJR/hxYMiXv0eb/6Nk2QPmljQUFR1090VJYIb6/vf3tr6xubWdmGnuLu3f3BYOjpumjjVDBssFrFuR9Sg4AoblluB7UQjlZHAVjS+nfmtJ9SGx+rRThIMJR0qPuCMWie1HqhMBJpeqexX/DnIKglyUoYc9V7pq9uPWSpRWSaoMZ3AT2yYUW05EzgtdlODCWVjOsSOo4pKNGE2P3dKzp3SJ4NYu1KWzNXfExmVxkxk5DoltSOz7M3E/7xOagc3YcZVklpUbLFokApiYzL7nfS5RmbFxBHKNHe3EjaimjLrEiq6EILll1dJs1oJriqX99VyrZrHUYBTOIMLCOAaanAHdWgAgzE8wyu8eYn34r17H4vWNS+fOYE/8D5/AF7Wj40=</latexit> Samples <latexit sha1_base64=\"eimCpRgfVxBfxhwCehIJdcsMsvY=\">AAAB8XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5SRMtAGssI5gOTI+xt5pIle3vH7p4QjvwLGwtFbP03dv4bN8kVmvhg4PHeDDPzgkRwbVz32ylsbe/s7hX3SweHR8cn5dOzjo5TxbDNYhGrXkA1Ci6xbbgR2EsU0igQ2A2mzYXffUKleSwfzCxBP6JjyUPOqLHSY1Ok2qDicjwsV9yquwTZJF5OKpCjNSx/DUYxSyOUhgmqdd9zE+NnVBnOBM5Lg1RjQtmUjrFvqaQRaj9bXjwnV1YZkTBWtqQhS/X3REYjrWdRYDsjaiZ63VuI/3n91IS3fsZlkhqUbLUoTAUxMVm8T0ZcITNiZgllittbCZtQRZlNQZdsCN76y5ukU6t69Wr9vlZpuHkcRbiAS7gGD26gAXfQgjYwkPAMr/DmaOfFeXc+Vq0FJ585hz9wPn8AzSSQ9Q==</latexit> Clustering <latexit sha1_base64=\"JgGHFC5oztwX6+XjDtZWQo9C1hA=\">AAAB7nicbVA9TwJBEJ3DL8Qv1NJmIzGxIncUaImxscREwAQuZG8ZYMPe7mV3z4Rc+BE2Fhpj6++x89+4wBUKvmSSl/dmMjMvSgQ31ve/vcLG5tb2TnG3tLd/cHhUPj5pG5Vqhi2mhNKPETUouMSW5VbgY6KRxpHATjS5nfudJ9SGK/lgpwmGMR1JPuSMWid1biQbK2365Ypf9Rcg6yTISQVyNPvlr95AsTRGaZmgxnQDP7FhRrXlTOCs1EsNJpRN6Ai7jkoaowmzxbkzcuGUARkq7UpaslB/T2Q0NmYaR64zpnZsVr25+J/XTe3wOsy4TFKLki0XDVNBrCLz38mAa2RWTB2hTHN3K2FjqimzLqGSCyFYfXmdtGvVoF6t39cqDT+PowhncA6XEMAVNOAOmtACBhN4hld48xLvxXv3PpatBS+fOYU/8D5/AFOaj4U=</latexit> Anchors <latexit sha1_base64=\"eimCpRgfVxBfxhwCehIJdcsMsvY=\">AAAB8XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5SRMtAGssI5gOTI+xt5pIle3vH7p4QjvwLGwtFbP03dv4bN8kVmvhg4PHeDDPzgkRwbVz32ylsbe/s7hX3SweHR8cn5dOzjo5TxbDNYhGrXkA1Ci6xbbgR2EsU0igQ2A2mzYXffUKleSwfzCxBP6JjyUPOqLHSY1Ok2qDicjwsV9yquwTZJF5OKpCjNSx/DUYxSyOUhgmqdd9zE+NnVBnOBM5Lg1RjQtmUjrFvqaQRaj9bXjwnV1YZkTBWtqQhS/X3REYjrWdRYDsjaiZ63VuI/3n91IS3fsZlkhqUbLUoTAUxMVm8T0ZcITNiZgllittbCZtQRZlNQZdsCN76y5ukU6t69Wr9vlZpuHkcRbiAS7gGD26gAXfQgjYwkPAMr/DmaOfFeXc+Vq0FJ585hz9wPn8AzSSQ9Q==</latexit> Clustering <latexit sha1_base64=\"JgGHFC5oztwX6+XjDtZWQo9C1hA=\">AAAB7nicbVA9TwJBEJ3DL8Qv1NJmIzGxIncUaImxscREwAQuZG8ZYMPe7mV3z4Rc+BE2Fhpj6++x89+4wBUKvmSSl/dmMjMvSgQ31ve/vcLG5tb2TnG3tLd/cHhUPj5pG5Vqhi2mhNKPETUouMSW5VbgY6KRxpHATjS5nfudJ9SGK/lgpwmGMR1JPuSMWid1biQbK2365Ypf9Rcg6yTISQVyNPvlr95AsTRGaZmgxnQDP7FhRrXlTOCs1EsNJpRN6Ai7jkoaowmzxbkzcuGUARkq7UpaslB/T2Q0NmYaR64zpnZsVr25+J/XTe3wOsy4TFKLki0XDVNBrCLz38mAa2RWTB2hTHN3K2FjqimzLqGSCyFYfXmdtGvVoF6t39cqDT+PowhncA6XEMAVNOAOmtACBhN4hld48xLvxXv3PpatBS+fOYU/8D5/AFOaj4U=</latexit> Anchors <latexit sha1_base64=\"KzBZ8R84UC9mpPFQBWeRHFxcqjw=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKVI8FLx4rmLbQhrLZbNq1m92wuxFK6H/w4kERr/4fb/4bt20O2vpg4PHeDDPzwpQzbVz32yltbG5t75R3K3v7B4dH1eOTjpaZItQnkkvVC7GmnAnqG2Y47aWK4iTktBtObud+94kqzaR4MNOUBgkeCRYzgo2VOn4aYUOH1ZpbdxdA68QrSA0KtIfVr0EkSZZQYQjHWvc9NzVBjpVhhNNZZZBpmmIywSPat1TghOogX1w7QxdWiVAslS1h0EL9PZHjROtpEtrOBJuxXvXm4n9ePzPxTZAzkWaGCrJcFGccGYnmr6OIKUoMn1qCiWL2VkTGWGFibEAVG4K3+vI66TTqXrPevG/UWldFHGU4g3O4BA+uoQV30AYfCDzCM7zCmyOdF+fd+Vi2lpxi5hT+wPn8AYuwjxQ=</latexit> Update <latexit sha1_base64=\"y2NH6tDs2GygUDqZYglGwvR4SpA=\">AAAB+nicbVBNSwMxEJ2tX7V+bfXoJVgEQSi7PVSPFS8eK9oPaEvJptk2NMkuSVYpa3+KFw+KePWXePPfmLZ70NYHA4/3ZpiZF8ScaeN5305ubX1jcyu/XdjZ3ds/cIuHTR0litAGiXik2gHWlDNJG4YZTtuxolgEnLaC8fXMbz1QpVkk780kpj2Bh5KFjGBjpb5bvMMi5lSjc3QlyShSuu+WvLI3B1olfkZKkKHed7+6g4gkgkpDONa643ux6aVYGUY4nRa6iaYxJmM8pB1LJRZU99L56VN0apUBCiNlSxo0V39PpFhoPRGB7RTYjPSyNxP/8zqJCS97KZNxYqgki0VhwpGJ0CwHNGCKEsMnlmCimL0VkRFWmBibVsGG4C+/vEqalbJfLVdvK6Wal8WRh2M4gTPw4QJqcAN1aACBR3iGV3hznpwX5935WLTmnGzmCP7A+fwBUnKTWg==</latexit> Samples + Anchors <latexit sha1_base64=\"u0BDOcH87PXd3DsT+o414+7cHnI=\">AAAB7XicbZC7SgNBFIbPxluMt6ilIINBsAq7FjGdARvLBMwFkhBmZ2eTMbMzy8ysEJaU9jYWitj6Cql8CDufwZdwcik0+sPAx/+fw5xz/JgzbVz308msrK6tb2Q3c1vbO7t7+f2DhpaJIrROJJeq5WNNORO0bpjhtBUriiOf06Y/vJrmzTuqNJPixoxi2o1wX7CQEWys1eiQQBrdyxfcojsT+gveAgqX75Pa1/3xpNrLf3QCSZKICkM41rrtubHpplgZRjgd5zqJpjEmQ9ynbYsCR1R309m0Y3RqnQCFUtknDJq5PztSHGk9inxbGWEz0MvZ1PwvaycmLHdTJuLEUEHmH4UJR0ai6eooYIoSw0cWMFHMzorIACtMjD1Qzh7BW175LzTOi16pWKq5hUoZ5srCEZzAGXhwARW4hirUgcAtPMATPDvSeXRenNd5acZZ9BzCLzlv33Yvk3g=</latexit> ··· <latexit sha1_base64=\"+7L/8ObZcl+JIZaSFhVO3t+lUUE=\">AAAB7XicbVDLSgNBEOyNrxhf8XHzMhiEeAm7ItFjQA8eI5gHJCHMTmaT0dnZZaZXCEv+wYsHRbz6P978GyebHDSxoKGo6qa7y4+lMOi6305uZXVtfSO/Wdja3tndK+4fNE2UaMYbLJKRbvvUcCkUb6BAydux5jT0JW/5j9dTv/XEtRGRusdxzHshHSoRCEbRSs2bvizjWb9YcituBrJMvDkp1Y6CDPV+8as7iFgScoVMUmM6nhtjL6UaBZN8UugmhseUPdIh71iqaMhNL82unZBTqwxIEGlbCkmm/p5IaWjMOPRtZ0hxZBa9qfif10kwuOqlQsUJcsVmi4JEEozI9HUyEJozlGNLKNPC3krYiGrK0AZUsCF4iy8vk+Z5xatWqnc2jQuYIQ/HcAJl8OASanALdWgAgwd4hld4cyLnxXl3PmatOWc+cwh/4Hz+AFjYkTs=</latexit> D l ( t ) <latexit sha1_base64=\"9C0bB8PYImk9DX0HLfGvGd44PFA=\">AAAB7XicbVDLSgNBEOyNrxhf8XHzMhiEeAm7ItFjQA8eI5gHJCHMTmaT0dnZZaZXCEv+wYsHRbz6P978GyebHDSxoKGo6qa7y4+lMOi6305uZXVtfSO/Wdja3tndK+4fNE2UaMYbLJKRbvvUcCkUb6BAydux5jT0JW/5j9dTv/XEtRGRusdxzHshHSoRCEbRSs2b/qiMZ/1iya24Gcgy8eakVDsKMtT7xa/uIGJJyBUySY3peG6MvZRqFEzySaGbGB5T9kiHvGOpoiE3vTS7dkJOrTIgQaRtKSSZ+nsipaEx49C3nSHFkVn0puJ/XifB4KqXChUnyBWbLQoSSTAi09fJQGjOUI4toUwLeythI6opQxtQwYbgLb68TJrnFa9aqd7ZNC5ghjwcwwmUwYNLqMEt1KEBDB7gGV7hzYmcF+fd+Zi15pz5zCH8gfP5A1K8kTc=</latexit> D h ( t ) <latexit sha1_base64=\"eNrtnhPGeU8n4BRDMStm5cjQ4ts=\">AAAB73icbVBNS8NAEJ34WetX1aOXxSJ4KkmR6rHQi8cK9gPaUDbbTbt0s4m7E6GE/gkvHhTx6t/x5r9x2+agrQ8GHu/NMDMvSKQw6Lrfzsbm1vbObmGvuH9weHRcOjltmzjVjLdYLGPdDajhUijeQoGSdxPNaRRI3gkmjbnfeeLaiFg94DThfkRHSoSCUbRS1zNIGlTKQansVtwFyDrxclKGHM1B6as/jFkacYVMUmN6npugn1GNgkk+K/ZTwxPKJnTEe5YqGnHjZ4t7Z+TSKkMSxtqWQrJQf09kNDJmGgW2M6I4NqveXPzP66UY3vqZUEmKXLHlojCVBGMyf54MheYM5dQSyrSwtxI2ppoytBEVbQje6svrpF2teLVK7b5arl/ncRTgHC7gCjy4gTrcQRNawEDCM7zCm/PovDjvzseydcPJZ87gD5zPH1Naj3k=</latexit> 1st Call <latexit sha1_base64=\"mxsL+XuWb2hqFND+pzTctrB1rcY=\">AAAB73icbVBNS8NAEJ34WetX1aOXxSJ4KkmR6rHQi8cK9gPaUDababt0s4m7G6GE/gkvHhTx6t/x5r9x2+agrQ8GHu/NMDMvSATXxnW/nY3Nre2d3cJecf/g8Oi4dHLa1nGqGLZYLGLVDahGwSW2DDcCu4lCGgUCO8GkMfc7T6g0j+WDmSboR3Qk+ZAzaqzUrcqQNKgQg1LZrbgLkHXi5aQMOZqD0lc/jFkaoTRMUK17npsYP6PKcCZwVuynGhPKJnSEPUsljVD72eLeGbm0SkiGsbIlDVmovycyGmk9jQLbGVEz1qveXPzP66VmeOtnXCapQcmWi4apICYm8+dJyBUyI6aWUKa4vZWwMVWUGRtR0Ybgrb68TtrViler1O6r5fp1HkcBzuECrsCDG6jDHTShBQwEPMMrvDmPzovz7nwsWzecfOYM/sD5/AE0o49l</latexit> 2nd Call <latexit sha1_base64=\"oSA1OFmXXL9y3PJtqoVxTIG9mto=\">AAAB8HicbVA9TwJBEJ3DL8Qv1NJmIzGxIncUaElCY2UwkQ8DF7K3zMGGvb3L7p6REH6FjYXG2Ppz7Pw3LnCFgi+Z5OW9mczMCxLBtXHdbye3sbm1vZPfLeztHxweFY9PWjpOFcMmi0WsOgHVKLjEpuFGYCdRSKNAYDsY1+d++xGV5rG8N5ME/YgOJQ85o8ZKD7f4ZEidCtEvltyyuwBZJ15GSpCh0S9+9QYxSyOUhgmqdddzE+NPqTKcCZwVeqnGhLIxHWLXUkkj1P50cfCMXFhlQMJY2ZKGLNTfE1MaaT2JAtsZUTPSq95c/M/rpia89qdcJqlByZaLwlQQE5P592TAFTIjJpZQpri9lbARVZQZm1HBhuCtvrxOWpWyVy1X7yqlWiWLIw9ncA6X4MEV1OAGGtAEBhE8wyu8Ocp5cd6dj2VrzslmTuEPnM8fSFeQCA==</latexit> Next Call Figure 2: Overview of the SimATTA framework. and sample ratio vectors when Dϕ,S(t) is not included, i.e., w′ and λ′ s.t. w′ 0 = λ′ 0 = 0 . If ˆdH∆H(DS, Dϕ,S(t)) < ˆdH∆H(DS, St i=1 Ute(i)), then for any λ ̸= λ′, there exists w s.t. EBS(w, λ, N, t) < EBS(w′, λ′, N, t). (8) Corollary 4 validates that the selected low-entropy samples can mitigate the CF problem under the assumption that these samples are source-like, which is also empirically validated in Fig. 1. Note that our strategy employs entropy minimization in a selective manner, aiming to solve CF rather than the main adaptation issue. While many FTTA works use entropy minimization to adapt across domains without guarantees, our use is more theoretically-sound. 4 A N ATTA ALGORITHM Building on our theoretical findings, we introduce a simple yet effective ATTA method, known as SimATTA, that innovatively integrates incremental clustering and selective entropy minimization techniques, as illustrated in Fig. 2. We start with an overview of our methodology, including the learning framework and the comprehensive sample selection strategies. We then proceed to discuss the details of the incremental clustering technique designed for real-time sample selections. 4.1 A LGORITHM OVERVIEW Let (x, y) be a labeled sample and f(·; θ) be our neural network, where ˆy = f(x; θ) and θ represents the parameters. We have a model pre-trained on source domains with the pre-trained parameters ϕ. We initialize model parameters as θ(0) = ϕ and aim to adapt the model f(·; θ) in real-time. During the test phase, the model continuously predicts labels for streaming-in test data and concurrently gets fine-tuned. We perform sample selection to enable active learning. As discussed in Sec. 3.2, we empirically consider informative high-entropy samples for addressing distribution shifts and source-like low-entropy samples to mitigate CF. As shown in Alg. 1, at each time step t, we first partition unlabeled test samples Ute(t) into high entropy and low entropy datasets, Uh(t) and Ul(t), using an entropy threshold. The source-pretrained model f(·; ϕ) is frozen to predict pseudo labels for low entropy data. We obtain labeled low-entropy data Dl(t) by labeling Ul(t) with f(·; ϕ) and combining it with Dl(t − 1). In contrast, the selection of high-entropy samples for active labeling is less straightforward. Since the complete test dataset is inaccessible for analyzing the target domain distribution, real-time sample selection is required. We design an incremental clustering sample selection technique to reduce sample redundancy and increase distribution coverage, detailed in Sec. 4.2. The incremental clustering algorithm outputs the labeled test samples Dh(t), also referred to as anchors, given Dh(t −1) and Uh(t). After sample selection, the model undergoes test-time training using the labeled test anchors Dh(t) and pseudo-labeled source-like anchors Dl(t). Following the analyses in Sec. 3.1, the training weights and sample numbers should satisfy w(t) ≈ λ(t) for Dh(t) and Dl(t) for optimal results. The analyses and results in Sec. 3.2 further indicate that balancing the source and target ratio is the key to mitigating CF. However, when source-like samples significantly outnumber test samples, the optimal w(t) for test domains can deviate from λ(t) according to Eq. (4). 4.2 I NCREMENTAL CLUSTERING We propose incremental clustering, a novel continual clustering technique designed to select informa- tive samples in unsupervised settings under the ATTA framework. The primary goal of this strategy is to store representative samples for distributions seen so far. Intuitively, we apply clusters to cover all seen distributions while adding new clusters to cover newly seen distributions. During this process with new clusters added, old clusters may be merged due to the limit of the cluster budget. Since 6Published as a conference paper at ICLR 2024 Algorithm 1 SIMATTA: A SIMPLE ATTA ALGORITHM Require: A fixed source pre-trained model f(·; ϕ) and a real-time adapting model f(·; θ(t)) with θ(0) = ϕ. Streaming test data Ute(t) at time step t. Entropy of predictions H(ˆy) = −P c p(ˆyc) logp(ˆyc). Low entropy and high entropy thresholds el and eh. The number of cluster centroid budget NC (t) at time step t. Centroid increase number k. Learning step size η. 1: for t = 1, . . . , Tdo 2: Model inference on Ute(t) using f(·; θ(t − 1)). 3: Dl(t) ← Dl(t − 1) ∪ {(x, f(x; ϕ))|x ∈ Ute(t), H(f(x; ϕ)) < el} 4: Uh(t) ← {x|x ∈ Ute(t), H(f(x; θ)) > eh} 5: Dh(t) ← Dh(t − 1) ∪ {(x, y)|∀x ∈ IC(Dh(t − 1), Uh(t), NC(t)), y= Oracle(x)} 6: λ(t) ← |Dl(t)|/(|Dl(t)| + |Dh(t)|), |Dh(t)|/(|Dl(t)| + |Dh(t)|) 7: w(t) ← GetW(λ(t)) ▷ Generally, GetW(λ(t)) = λ(t) is a fair choice. 8: θ(t) ← θ(t − 1) 9: for (xl, yl) in Dl and (xh, yh) in Dh do 10: θ(t) ← θ(t) − ηw0∇ℓCE (f(xl; θ(t)), yl) − η(1 − w0)∇ℓCE (f(xh; θ(t)), yh) 11: end for 12: NC (t + 1) ← UpdateCentroidNum(NC (t)) ▷ Naive choice: NC (t + 1) ← NC (t) + k. 13: end for clusters cannot be stored efficiently, we store the representative samples of clusters, named anchors, instead. In this work, we adopt weighted K-means (Krishna and Murty, 1999) as our base clustering method due to its popularity and suitability for new setting explorations. When we apply clustering with new samples, a previously selected anchor should not weigh the same as new samples since the anchor is a representation of a cluster,i.e., a representation of many samples. Instead, the anchor should be considered as a barycenter with a weight of the sum of its cluster’s sample weights. For a newly added cluster, its new anchor has the weight of the whole cluster. For clusters containing multiple old anchors, i.e., old clusters, the increased weights are distributed equally among these anchors. These increased weights are contributed by new samples that are close to these old anchors. Intuitively, this process of clustering is analogous to the process of planet formation. Where there are no planets, new planets (anchors) will be formed by the aggregation of the surrounding material (samples). Where there are planets, the matter is absorbed by the surrounding planets. This example is only for better understanding without specific technical meanings. Specifically, we provide the detailed Alg. 2 for incremental clustering. In each iteration, we apply weighted K-Means for previously selected anchors Danc and the new streaming-in unlabeled data Unew. We first extract all sample features using the model from the previous step f(·; θ(t − 1)), and then cluster these weighted features. The initial weights of the new unlabeled samples are 1, while anchors inherit weights from previous iterations. After clustering, clusters including old anchors are old clusters, while clusters only containing new samples are newly formed ones. For each new cluster, we select the centroid-closest sample as the new anchor to store. As shown in line 10 of Alg. 2, for both old and new clusters, we distribute the sample weights in this cluster as its anchors’ weights. With incremental clustering, although we can control the number of clusters in each iteration, we cannot control the number of new clusters/new anchors. This indirect control makes the increase of new anchors adaptive to the change of distributions, but it also leads to indirect budget control. Therefore, in experimental studies, we set the budget limit, but the actual anchor budget will not reach this limit. The overall extra storage requirement is O(B) since the number of saved unlabeled samples is proportional to the number of saved labeled samples (anchors). 5 E XPERIMENTAL STUDIES In this study, we aim to validate the effectiveness of our proposed method, as well as explore the various facets of the ATTA setting. Specifically, we design experiments around the following research questions: RQ1: Can TTA methods address domain distribution shifts? RQ2: Is ATTA as efficient as TTA? RQ3: How do the components of SimATTA perform? RQ4: Can ATTA perform on par with stronger Active Domain Adaptation (ADA) methods? We compare ATTA with three settings, TTA (Tab. 2), enhanced TTA (Tab. 3 and 5), and ADA (Tab. 4). Datasets. To assess the OOD performance of the TTA methods, we benchmark them using datasets from DomainBed (Gulrajani and Lopez-Paz, 2020) and Hendrycks and Dietterich (2019a). We employ PACS (Li et al., 2017), VLCS (Fang et al., 2013), Office-Home (Venkateswara et al., 2017), and Tiny-ImageNet-C datasets for our evaluations. For each dataset, we designate one domain as 7Published as a conference paper at ICLR 2024 Table 2: TTA comparisons on PACS and VLCS.This table includes the two data stream mentioned in the dataset setup and reports performances in accuracy. Results that outperform all TTA baselines are highlighted in bold font. N/A denotes the adaptations are not applied on the source domain. PACS Domain-wise data stream Post-adaptation Random data stream Post-adaptation P →A→ →C→ →S P A C S →1→ →2→ →3→ →4 P A C S BN w/o adapt 99.70 59.38 28.03 42.91 99.70 59.38 28.03 42.91 43.44 43.44 43.44 43.44 99.70 59.38 28.03 42.91BN w/ adapt 98.74 68.07 64.85 54.57 98.74 68.07 64.85 54.57 62.50 62.50 62.50 62.50 98.74 68.07 64.85 54.57 Tent (steps=1) N/A 67.29 64.59 44.67 97.60 66.85 64.08 42.58 56.35 54.09 51.83 48.58 97.19 63.53 60.75 41.56Tent (steps=10) N/A 67.38 57.85 20.23 62.63 34.52 40.57 13.59 47.36 31.01 22.84 20.33 50.78 23.68 20.95 19.62EATA N/A 67.04 64.72 50.27 98.62 66.50 62.46 48.18 57.31 56.06 58.17 59.78 98.62 69.63 65.70 54.26CoTTA N/A 65.48 62.12 53.17 98.62 65.48 63.10 53.78 56.06 54.33 57.16 57.42 98.62 65.97 62.97 54.62SAR (steps=1) N/A 66.75 63.82 49.58 98.32 66.94 62.93 45.74 56.78 56.35 56.68 56.70 98.44 68.16 64.38 52.53SAR (steps=10) N/A 69.38 68.26 49.02 96.47 62.16 56.19 54.62 53.51 51.15 51.78 45.60 94.13 56.64 56.02 36.37 SimATTA (B ≤300) N/A 76.86 70.90 75.39 98.80 84.47 82.25 81.52 69.47 76.49 82.45 82.22 98.98 84.91 83.92 86.00SimATTA (B ≤500) N/A 77.93 76.02 76.30 98.62 88.33 83.49 83.74 68.46 78.22 80.91 85.49 99.16 86.67 84.77 87.71 VLCS Domain-wise data stream Post-adaptation Random data stream Post-adaptation C →L→ →S→ →V C L S V →1→ →2→ →3→ →4 C L S V BN w/o adapt 100.00 33.55 41.10 49.05 100.00 33.55 41.10 49.05 41.23 41.23 41.23 41.23 100.00 33.55 41.10 49.05BN w/ adapt 85.16 37.31 33.27 52.16 85.16 37.31 33.27 52.16 40.91 40.91 40.91 40.91 85.16 37.31 33.27 52.16 Tent (steps=1) N/A 38.55 34.40 53.88 84.73 43.86 33.61 53.11 44.85 44.29 47.38 44.98 85.30 43.49 37.81 53.35Tent (steps=10) N/A 45.41 31.44 32.32 42.54 37.65 27.79 33.12 46.13 42.31 43.51 39.48 52.01 40.32 33.64 40.37EATA N/A 37.24 33.15 52.58 84.10 37.69 32.39 52.49 43.77 42.48 43.34 41.55 83.32 36.67 31.47 52.55CoTTA N/A 37.39 32.54 52.25 82.12 37.65 33.12 52.90 43.69 42.14 43.21 42.32 81.98 37.99 33.52 53.23SAR (steps=1) N/A 36.18 34.43 52.46 83.96 39.72 36.53 52.37 43.64 43.04 44.20 41.93 85.09 40.70 36.44 53.02SAR (steps=10) N/A 35.32 34.10 51.66 82.12 41.49 33.94 53.08 43.56 42.05 42.53 41.16 85.09 37.58 33.12 52.01 SimATTA (B ≤300) N/A 62.61 65.08 74.38 99.93 69.50 66.67 77.34 62.33 69.33 73.20 71.93 99.93 69.43 72.46 80.39SimATTA (B ≤500) N/A 63.52 68.01 76.13 99.51 70.56 73.10 78.35 62.29 70.45 73.50 72.02 99.43 70.29 72.55 80.18 the source domain and arrange the samples from the other domains to form the test data stream. For DomainBed datasets, we adopt two stream order strategies. The first order uses a domain-wise data stream, i.e., we finish streaming samples from one domain before starting streaming another domain. The second order is random, where we shuffle samples from all target domains and partition them into four splits 1, 2, 3, and 4, as shown in Tab. 2. More dataset details are provided in Appx. G.1. Baselines. For baseline models, we start with the common source-only models, which either utilize pre-calculated batch statistics (BN w/o adapt) or test batch statistics (BN w/ adapt). For comparison with other TTA methods, we consider four state-of-the-art TTA methods: Tent (Wang et al., 2021), EATA (Niu et al., 2022), CoTTA (Wang et al., 2022a), and SAR (Niu et al., 2023). The three of them except Tent provide extra design to avoid CF. To compare with ADA methods, we select algorithms that are partially comparable with our method, i.e., they should be efficient (e.g., uncertainty-based) without the requirements of additional networks. Therefore, we adopt random, entropy (Wang and Shang, 2014), k-means (Krishna and Murty, 1999), and CLUE (Prabhu et al., 2021) for comparisons. Settings. For TTA, we compare with general TTA baselines in streaming adaptation using the two aforementioned data streaming orders, domain-wise and random. We choose P in PACS and C in VLCS as source domains. For domain-wise data stream, we use order A → C → S for PACS and L → S → V for VLCS. We report the real-time adaptation accuracy results for each split of the data stream, as well as the accuracy on each domain after all adaptations through the data stream (under “post-adaptation” columns). Enhanced TTA is built on TTA with access to extra random sample labels. TTA baselines are further fine-tuned with these random samples. To further improve enhanced TTA, we use long-term label storage and larger unlabeled sample pools. To its extreme where the model can access the whole test set samples, the setting becomes similar to ADA, thus we also use ADA methods for comparisons. ADA baselines have access to all samples in the pre-collected target datasets but not source domain data, whereas our method can only access the streaming test data. 5.1 T HE FAILURE OF TEST-TIME ADAPTATION The failure of TTA methods on domain distribution shifts is one of the main motivations of the ATTA setting. As shown in Tab. 2, TTA methods cannot consistently outperform eventhe simplest baseline \"BN w/ adapt\" which uses test time batch statistics to make predictions, evidencing that current TTA methods cannot solve domain distribution shifts (RQ1). Additionally, Tent (step=10) exhibits significant CF issues, where \"step=10\" indicates 10 test-time training updates, i.e., 10 gradient backpropagation iterations. This failure of TTA methods necessitates the position of ATTA. In contrast, SimATTA, with a budget B less than 300, outperforms all TTA methods on both source and target domains by substantial margins. Moreover, compared to the source-only baselines, our method improves the target domain performances significantly with negligible source performance loss, showing that ATTA is a more practically effective setting for real-world distribution shifts. 5.2 E FFICIENCY & ENHANCED TTA SETTING COMPARISONS To validate the efficiency of ATTA and broaden the dataset choice, we conduct this study on Tiny- ImageNet-C which, though does not focus on domain shifts, is much larger than PACS and VLCS. we 8Published as a conference paper at ICLR 2024 Table 3: Comparisons with Enhanced TTA on Tiny-ImageNet-C (severity level 5). Tiny-ImageNet-C Time (sec)Noise Blur Weather Digital Gauss. Shot Impul. Defoc. Glass Motion Zoom Snow Frost Fog Contr. Elastic Pixel JPEG Avg. Tent (step=1) 68.83 9.32 11.97 8.86 10.43 7.00 12.20 14.34 13.58 15.46 13.55 3.99 13.31 17.79 18.61 12.17Tent (step=10) 426.90 0.86 0.63 0.52 0.52 0.55 0.54 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.54EATA 93.14 3.98 3.33 2.18 4.80 2.37 11.02 11.41 14.06 15.26 9.65 1.36 9.88 14.24 12.12 8.26CoTTA 538.78 5.63 7.12 6.31 8.05 5.74 9.68 10.55 11.75 12.00 11.15 4.17 5.35 7.82 8.90 8.16SAR (step=1) 113.76 8.90 3.11 1.67 1.55 1.47 1.35 1.19 1.03 1.04 0.93 0.83 1.00 0.74 0.77 1.83SAR (step=10) 774.11 2.67 3.26 2.38 1.64 1.85 2.49 3.16 3.81 2.72 3.12 0.81 3.47 4.04 1.76 2.66 SimATTA (step=10) 736.289.68 19.40 12.14 30.28 17.03 42.36 43.10 31.96 40.08 29.243.21 34.56 45.24 45.74 28.86 enhance the TTA setting by fine-tuning baselines on randomly selected labeled samples. Specifically, the classifier of ResNet18-BN is pre-adapted to the brightness corruption (source domain) before test-time adapting. SimATTA’s label budget is around 4,000, while all other TTA methods have budget 4,500 for randomly selected labeled samples. The data stream order is shown in Tab. 3. Time is measured across all corrupted images in the Noise and Blur noise types, and the values represent the average time cost for adapting 10,000 images. The results clearly evidence the efficiency of ATTA (RQ2), while substantially outperforming all enhanced TTA baselines. Simply accessing labeled samples cannot benefit TTA methods to match ATTA. With 10 training updates (step=10) for each batch, FTTA methods would suffer from severe CF problem. In contrast, ATTA covers a statistically significant distribution, achieving stronger performances with 10 training updates or even more steps till approximate convergences. In fact, longer training on Tent (step=10) leads to worse results (compared to step=1), which further motivates the design of the ATTA setting. The reason for higher absolute time cost in Tab. 3 is due to differences in training steps. In this experiment, SimATTA has a training step of 10, and similar time cost as SAR per step. Note that if the enhanced TTA setting is further improved to maintain distributions with a balanced CF mitigation strategy and an incremental clustering design, the design approaches ATTA. Specifically, we compare SimATTA with its variants as the ablation study (RQ3) in Appx. I.2. 5.3 C OMPARISONS TO A STRONGER SETTING : ACTIVE DOMAIN ADAPTATION Table 4: Comparisons to ADA baselines. Source domains are denoted as \"(S)\". Results are average accuracies (with standard deviations). PACS P (S) A C S Random (B= 300) 96.21 (0.80) 81.19 (0.48) 80.75 (1.27) 84.34 (0.18)Entropy (B= 300) 96.31 (0.64)88.00 (1.46)82.48 (1.71) 80.55 (1.01)Kmeans (B= 300) 93.71 (1.50) 79.31 (4.01) 79.64 (1.44) 83.92 (0.65)CLUE (B= 300) 96.69 (0.17)83.97 (0.57)84.77 (0.88) 86.91 (0.26) SimATTA (B ≤300) 98.89 (0.09)84.69 (0.22)83.09 (0.83)83.76 (2.24) VLCS C (S) L S V Random (B= 300) 96.21 (1.65) 66.67 (1.70) 70.72 (0.30) 72.14 (1.71)Entropy (B= 300) 97.74 (1.56) 69.29 (2.26)69.25 (4.77) 75.26 (3.07)Kmeans (B= 300) 98.61 (0.27)67.57 (1.64)70.77 (0.01)74.49 (0.97)CLUE (B= 300) 85.70 (10.09) 65.29 (1.49) 69.42 (2.64) 69.09 (6.05) SimATTA (B ≤300) 99.93 (0.00) 69.47 (0.03)69.57 (2.90)78.87 (1.53) In addtion to the above comparisons with (en- hanced) TTA, which necessitate the requirement of extra information in the ATTA setting, we com- pare ATTA with a stronger setting Active Domain Adaptation (ADA) to demonstrate another supe- riority of ATTA, i.e., weaker requirements for comparable performances (RQ4). ADA baselines are able to choose the global best active samples, while ATTA has to choose samples from a small sample buffer (e.g., a size of 100) and discard the rest. Tab. 4 presents the post-adaptation model per- formance results. All ADA results are averaged from 3 random runs, while ATTA results are the post-adaptation performances averaged from the two data stream orders. As can be observed, despite the lack of a pre-collected target dataset, SimATTA produces better or competitive results against ADA methods. Moreover, without source data access, SimATTA’s design for CF allows it to maintain superior source domain performances over ADA methods. Further experimental studies including the Office-Home dataset are provided in Appx. I. In conclusion, the significant improvement compared to weaker settings (TTA, enhanced TTA) and the comparable performance with the stronger setting, ADA, rendering ATTA a setting that is as efficient as TTA and as effective as ADA. This implies its potential is worthy of future explorations. 6 C ONCLUSION AND DISCUSSION There’s no denying that OOD generalization can be extremely challenging without certain information, often relying on various assumptions easily compromised by different circumstances. Thus, it’s prudent to seek methods to achieve significant improvements with minimal cost, e.g., DG methods leveraging environment partitions and ATTA methods using budgeted annotations. As justified in our theoretical and experimental studies, ATTA stands as a robust approach to achieve real-time OOD generalization. Although SimATTA sets a strong baseline for ATTA, there’s considerable scope for further investigation within the ATTA setting. One potential direction involves developing alternatives to prevent CF in ATTA scenarios. While selective entropy minimization on low-entropy samples has prove to be empirically effective, it relies on the quality of the pre-trained model and training on incorrectly predicted low-entropy samples may reinforce the errors. It might not be cost-effective to expend annotation budgets on low-entropy samples, but correcting them could be a viable alternative solution. We anticipate that our work will spur numerous further explorations in this field. 9Published as a conference paper at ICLR 2024 ACKNOWLEDGMENTS This work was supported in part by National Science Foundation grant IIS-2006861 and National Institutes of Health grant U01AG070112. REFERENCES Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, and Mario Marchand. Domain- adversarial neural networks. arXiv preprint arXiv:1412.4446, 2014. Lucas Baier, Tim Schlör, Jakob Schöffer, and Niklas Kühl. Detecting concept drift with neural network model uncertainty. In Hawaii International Conference on System Sciences, 2021. URL https://api.semanticscholar.org/CorpusID:235731947. Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine learning, 79:151–175, 2010. Davide Cacciarelli and Murat Kulahci. A survey on online active learning, 2023. Cheng Chen, Quande Liu, Yueming Jin, Qi Dou, and Pheng-Ann Heng. Source-free domain adaptive fundus image segmentation with denoised pseudo-labeling. In Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part V 24, pages 225–235. Springer, 2021. Li Chen, Tutian Tang, Zhitian Cai, Yang Li, Penghao Wu, Hongyang Li, Jianping Shi, Junchi Yan, and Yu Qiao. Level 2 autonomous driving on a single device: Diving into the devils of openpilot. arXiv preprint arXiv:2206.08176, 2022a. Weijie Chen, Luojun Lin, Shicai Yang, Di Xie, Shiliang Pu, and Yueting Zhuang. Self-supervised noisy label learning for source-free unsupervised domain adaptation. In 2022 IEEE/RSJ In- ternational Conference on Intelligent Robots and Systems (IROS) , pages 10185–10192. IEEE, 2022b. Yining Chen, Colin Wei, Ananya Kumar, and Tengyu Ma. Self-training avoids using spurious features under domain shift. Advances in Neural Information Processing Systems, 33:21061–21071, 2020. David A Cohn, Zoubin Ghahramani, and Michael I Jordan. Active learning with statistical models. Journal of artificial intelligence research, 4:129–145, 1996. Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Aleš Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE transactions on pattern analysis and machine intelligence, 44(7):3366–3385, 2021. Yuhe Ding, Lijun Sheng, Jian Liang, Aihua Zheng, and Ran He. Proxymix: Proxy-based mixup training with label refinery for source-free domain adaptation. arXiv preprint arXiv:2205.14566, 2022. Cian Eastwood, Ian Mason, Christopher KI Williams, and Bernhard Schölkopf. Source-free adaptation to measurement shift via bottom-up feature restoration. arXiv preprint arXiv:2107.05446, 2021. Jiahao Fan, Hangyu Zhu, Xinyu Jiang, Long Meng, Chen Chen, Cong Fu, Huan Yu, Chenyun Dai, and Wei Chen. Unsupervised domain adaptation by statistics alignment for deep sleep staging networks. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 30:205–216, 2022. Chen Fang, Ye Xu, and Daniel N Rockmore. Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. In Proceedings of the IEEE International Conference on Computer Vision, pages 1657–1664, 2013. Yuqi Fang, Pew-Thian Yap, Weili Lin, Hongtu Zhu, and Mingxia Liu. Source-free unsupervised domain adaptation: A survey. arXiv preprint arXiv:2301.00265, 2022. Francois Fleuret et al. Uncertainty reduction for model adaptation in semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9613–9623, 2021. 10Published as a conference paper at ICLR 2024 Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In International conference on machine learning, pages 1180–1189. PMLR, 2015. Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The journal of machine learning research, 17(1):2096–2030, 2016. Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi, Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph Triebel, Peter Jung, Ribana Roscher, et al. A survey of uncertainty in deep neural networks. arXiv preprint arXiv:2107.03342, 2021. Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. Advances in neural information processing systems, 17, 2004. Shurui Gui, Chaoyue Wang, Qihua Chen, and Dacheng Tao. Featureflow: Robust video interpolation via structure-to-texture generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14004–14013, 2020. Shurui Gui, Xiner Li, Limei Wang, and Shuiwang Ji. GOOD: A graph out-of-distribution benchmark. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. URL https://openreview.net/forum?id=8hHg-zs_p-h. Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. arXiv preprint arXiv:2007.01434, 2020. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. March 2019a. doi: 10.48550/ARXIV .1903.12261. Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019b. Steven CH Hoi, Rong Jin, Jianke Zhu, and Michael R Lyu. Semisupervised svm batch mode active learning with applications to image retrieval. ACM Transactions on Information Systems (TOIS), 27(3):1–29, 2009. Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, et al. Planning-oriented autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17853–17862, 2023. Jiaxing Huang, Dayan Guan, Aoran Xiao, and Shijian Lu. Model adaptation: Historical contrastive learning for unsupervised domain adaptation without source data. Advances in Neural Information Processing Systems, 34:3635–3649, 2021. Masato Ishii and Masashi Sugiyama. Source-free domain adaptation via distributional alignment by matching batch normalization statistics. arXiv preprint arXiv:2101.10842, 2021. Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for model-agnostic domain generalization. Advances in Neural Information Processing Systems, 34:2427–2440, 2021. Suyog Dutt Jain and Kristen Grauman. Active image segmentation propagation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2864–2873, 2016. Guoliang Kang, Lu Jiang, Yi Yang, and Alexander G Hauptmann. Contrastive adaptation network for unsupervised domain adaptation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4893–4902, 2019. Ashish Kapoor, Kristen Grauman, Raquel Urtasun, and Trevor Darrell. Active learning with gaussian processes for object categorization. In 2007 IEEE 11th international conference on computer vision, pages 1–8. IEEE, 2007. Neerav Karani, Ertunc Erdil, Krishna Chaitanya, and Ender Konukoglu. Test-time adaptable neural networks for robust medical image segmentation. Medical Image Analysis, 68:101907, 2021. 11Published as a conference paper at ICLR 2024 Ronald Kemker, Marc McClure, Angelina Abitino, Tyler Hayes, and Christopher Kanan. Measuring catastrophic forgetting in neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. Daniel Kifer, Shai Ben-David, and Johannes Gehrke. Detecting change in data streams. In VLDB, volume 4, pages 180–191. Toronto, Canada, 2004. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114 (13):3521–3526, 2017. Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Bal- subramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning, pages 5637–5664. PMLR, 2021. Divya Kothandaraman, Sumit Shekhar, Abhilasha Sancheti, Manoj Ghuhan, Tripti Shukla, and Dinesh Manocha. Salad: Source-free active label-agnostic domain adaptation for classification, segmentation and detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 382–391, 2023. K Krishna and M Narasimha Murty. Genetic k-means algorithm. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 29(3):433–439, 1999. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu- tional neural networks. Communications of the ACM, 60(6):84–90, 2017. David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrap- olation (REx). In International Conference on Machine Learning , pages 5815–5826. PMLR, 2021. Vinod K Kurmi, Venkatesh K Subramanian, and Vinay P Namboodiri. Domain impression: A source data free domain adaptation method. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 615–625, 2021. David D Lewis and Jason Catlett. Heterogeneous uncertainty sampling for supervised learning. In Machine learning proceedings 1994, pages 148–156. Elsevier, 1994. Aodong Li, Alex Boyd, Padhraic Smyth, and Stephan Mandt. Detecting and adapting to irregular distribution shifts in bayesian online learning. Advances in neural information processing systems, 34:6816–6828, 2021a. Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain generalization. In Proceedings of the IEEE international conference on computer vision, pages 5542–5550, 2017. Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, and Si Wu. Model adaptation: Unsupervised domain adaptation without source data. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9641–9650, 2020. Xianfeng Li, Weijie Chen, Di Xie, Shicai Yang, Peng Yuan, Shiliang Pu, and Yueting Zhuang. A free lunch for unsupervised domain adaptive object detection without source data. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 8474–8481, 2021b. Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935–2947, 2017. Jian Liang, Dapeng Hu, Ran He, and Jiashi Feng. Distill and fine-tune: Effective adaptation from a black-box source model. arXiv preprint arXiv:2104.01539, 1(3), 2021. Jian Liang, Dapeng Hu, Jiashi Feng, and Ran He. Dine: Domain adaptation from single and multiple black-box predictors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8003–8013, 2022. 12Published as a conference paper at ICLR 2024 Yong Lin, Shengyu Zhu, Lu Tan, and Peng Cui. Zin: When and how to learn invariance without environment partition? Advances in Neural Information Processing Systems, 35:24529–24542, 2022. Xiaofeng Liu, Fangxu Xing, Chao Yang, Georges El Fakhri, and Jonghye Woo. Adapting off-the- shelf source segmenter for target medical image segmentation. In Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part II 24, pages 549–559. Springer, 2021a. Xinyu Liu and Yixuan Yuan. A source-free domain adaptive polyp detection framework with style diversification flow. IEEE Transactions on Medical Imaging, 41(7):1897–1908, 2022. Yuang Liu, Wei Zhang, Jun Wang, and Jianyong Wang. Data-free knowledge transfer: A survey. arXiv preprint arXiv:2112.15278, 2021b. Yuejiang Liu, Parth Kothari, Bastien Van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? Advances in Neural Information Processing Systems, 34:21808–21820, 2021c. Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with deep adaptation networks. In International conference on machine learning, pages 97–105. PMLR, 2015. David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. Advances in neural information processing systems, 30, 2017. Chaochao Lu, Yuhuai Wu, José Miguel Hernández-Lobato, and Bernhard Schölkopf. Invariant causal representation learning for out-of-distribution generalization. In International Conference on Learning Representations, 2021. Xinhong Ma, Junyu Gao, and Changsheng Xu. Active universal domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8968–8977, 2021. Haitao Mao, Lun Du, Yujia Zheng, Qiang Fu, Zelin Li, Xu Chen, Shi Han, and Dongmei Zhang. Source free unsupervised graph domain adaptation. arXiv preprint arXiv:2112.00955, 2021. Christoforos Mavrogiannis, Francesca Baldini, Allan Wang, Dapeng Zhao, Pete Trautman, Aaron Steinfeld, and Jean Oh. Core challenges of social robot navigation: A survey. ACM Transactions on Human-Robot Interaction, 12(3):1–39, 2023. Zachary Nado, Shreyas Padhy, D Sculley, Alexander D’Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robustness under covariate shift. arXiv preprint arXiv:2006.10963, 2020. Munan Ning, Donghuan Lu, Dong Wei, Cheng Bian, Chenglang Yuan, Shuang Yu, Kai Ma, and Yefeng Zheng. Multi-anchor active domain adaptation for semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9112–9122, 2021. Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test-time model adaptation without forgetting. In International conference on machine learning, pages 16888–16905. PMLR, 2022. Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan. Towards stable test-time adaptation in dynamic wild world. InThe Eleventh International Con- ference on Learning Representations, 2023. URL https://openreview.net/forum?id=g2YraF75Tj. Sinno Jialin Pan, Ivor W Tsang, James T Kwok, and Qiang Yang. Domain adaptation via transfer component analysis. IEEE transactions on neural networks, 22(2):199–210, 2010. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. 13Published as a conference paper at ICLR 2024 Vishal M Patel, Raghuraman Gopalan, Ruonan Li, and Rama Chellappa. Visual domain adaptation: A survey of recent advances. IEEE signal processing magazine, 32(3):53–69, 2015. Judea Pearl. Causality. Cambridge university press, 2009. Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learning in python. the Journal of machine Learning research, 12:2825–2830, 2011. Jonas Peters, Peter Bühlmann, and Nicolai Meinshausen. Causal inference by using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 78(5):947–1012, 2016. Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference: foundations and learning algorithms. The MIT Press, 2017. Viraj Prabhu, Arjun Chandrasekaran, Kate Saenko, and Judy Hoffman. Active domain adaptation via clustering uncertainty-weighted embeddings. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8505–8514, 2021. Elan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. The risks of invariant risk minimization. arXiv preprint arXiv:2010.05761, 2020. Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731, 2019. Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normal- ization help optimization? Advances in neural information processing systems, 31, 2018. Akanksha Saran, Safoora Yousefi, Akshay Krishnamurthy, John Langford, and Jordan T. Ash. Streaming active learning with deep neural networks. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 30005–30021. PMLR, 23–29 Jul 2023. URL https://proceedings.mlr. press/v202/saran23a.html. Harald Schafer, Eder Santana, Andrew Haden, and Riccardo Biasini. A commute in data: The comma2k19 dataset, 2018. Tobias Scheffer, Christian Decomain, and Stefan Wrobel. Active hidden markov models for informa- tion extraction. In Advances in Intelligent Data Analysis: 4th International Conference, IDA 2001 Cascais, Portugal, September 13–15, 2001 Proceedings 4, pages 309–318. Springer, 2001. Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. Advances in Neural Information Processing Systems, 33:11539–11551, 2020. Burr Settles. Active learning literature survey. 2009. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. Jong-Chyi Su, Yi-Hsuan Tsai, Kihyuk Sohn, Buyu Liu, Subhransu Maji, and Manmohan Chandraker. Active adversarial domain adaptation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 739–748, 2020. Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In European conference on computer vision, pages 443–450. Springer, 2016. Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In International conference on machine learning, pages 9229–9248. PMLR, 2020. 14Published as a conference paper at ICLR 2024 Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker. Learning to adapt structured output space for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7472–7481, 2018. Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. In Proceedings of the IEEE international conference on computer vision, pages 4068–4076, 2015. Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7167–7176, 2017. Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5018–5027, 2017. Sudheendra Vijayanarasimhan and Ashish Kapoor. Visual recognition and detection under bounded computational resources. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 1006–1013. IEEE, 2010. Dan Wang and Yi Shang. A new active labeling method for deep learning. In 2014 International joint conference on neural networks (IJCNN), pages 112–119. IEEE, 2014. Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test- time adaptation by entropy minimization. InInternational Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=uXl3bZLkr3c. Mei Wang and Weihong Deng. Deep visual domain adaptation: A survey. Neurocomputing, 312: 135–153, 2018. Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7201–7211, 2022a. Rui Wang, Zuxuan Wu, Zejia Weng, Jingjing Chen, Guo-Jun Qi, and Yu-Gang Jiang. Cross-domain contrastive learning for unsupervised domain adaptation. IEEE Transactions on Multimedia , 2022b. Garrett Wilson and Diane J Cook. A survey of unsupervised deep domain adaptation. ACM Transactions on Intelligent Systems and Technology (TIST), 11(5):1–46, 2020. Binhui Xie, Longhui Yuan, Shuang Li, Chi Harold Liu, Xinjing Cheng, and Guoren Wang. Active learning for domain adaptation: An energy-based approach. InProceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 8708–8716, 2022. Zhao Xu, Kai Yu, V olker Tresp, Xiaowei Xu, and Jizhi Wang. Representative sampling for text classification using support vector machines. In Advances in Information Retrieval: 25th European Conference on IR Research, ECIR 2003, Pisa, Italy, April 14–16, 2003. Proceedings 25, pages 393–407. Springer, 2003. Baoyao Yang, Hao-Wei Yeh, Tatsuya Harada, and Pong C Yuen. Model-induced generalization error bound for information-theoretic representation learning in source-data-free unsupervised domain adaptation. IEEE Transactions on Image Processing, 31:419–432, 2021a. Guanglei Yang, Hao Tang, Zhun Zhong, Mingli Ding, Ling Shao, Nicu Sebe, and Elisa Ricci. Transformer-based source-free domain adaptation. arXiv preprint arXiv:2105.14138, 2021b. Jianfei Yang, Xiangyu Peng, Kai Wang, Zheng Zhu, Jiashi Feng, Lihua Xie, and Yang You. Divide to adapt: Mitigating confirmation bias for domain adaptation of black-box predictors. arXiv preprint arXiv:2205.14467, 2022. H Yao, Yuhong Guo, and Chunsheng Yang. Source-free unsupervised domain adaptation with surrogate data generation. In Proceedings of NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and Applications, 2021. 15Published as a conference paper at ICLR 2024 Hao-Wei Yeh, Baoyao Yang, Pong C Yuen, and Tatsuya Harada. Sofa: Source-data-free feature alignment for unsupervised domain adaptation. InProceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 474–483, 2021. Fuming You, Jingjing Li, and Zhou Zhao. Test-time batch statistics calibration for covariate shift. arXiv preprint arXiv:2110.04065, 2021. Hu Yu, Jie Huang, Yajing Liu, Qi Zhu, Man Zhou, and Feng Zhao. Source-free domain adaptation for real-world image dehazing. In Proceedings of the 30th ACM International Conference on Multimedia, pages 6645–6654, 2022. Haojian Zhang, Yabin Zhang, Kui Jia, and Lei Zhang. Unsupervised domain adaptation of black-box source models. arXiv preprint arXiv:2101.02839, 2021. Marvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and augmentation. Advances in Neural Information Processing Systems, 35:38629–38642, 2022a. Yifan Zhang, Xue Wang, Kexin Jin, Kun Yuan, Zhang Zhang, Liang Wang, Rong Jin, and Tieniu Tan. Adanpc: Exploring non-parametric classifier for test-time adaptation. In International Conference on Machine Learning, pages 41647–41676. PMLR, 2023. Yizhe Zhang, Shubhankar Borse, Hong Cai, and Fatih Porikli. Auxadapt: Stable and efficient test-time adaptation for temporally consistent video semantic segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2339–2348, 2022b. Bowen Zhao, Chen Chen, and Shu-Tao Xia. Delta: degradation-free fully test-time adaptation. arXiv preprint arXiv:2301.13018, 2023a. Hao Zhao, Yuejiang Liu, Alexandre Alahi, and Tao Lin. On pitfalls of test-time adaptation. In International Conference on Machine Learning (ICML), 2023b. Chunting Zhou, Xuezhe Ma, Paul Michel, and Graham Neubig. Examining and combating spurious features under distribution shift. In International Conference on Machine Learning, pages 12857– 12867. PMLR, 2021. 16Published as a conference paper at ICLR 2024 Active Test-Time Adaptation: Foundational Analyses and An Algorithm Supplementary Material A B ROADER IMPACTS The field of domain generalization primarily concentrates on enhancing a model’s generalization abilities by preparing it thoroughly before deployment. However, it is equally important for deep learning applications to have the capacity for real-time adaptation, as no amount of preparation can account for all possible scenarios. Consequently, domain generalization and test-time adaptation are complementary strategies: the former is more weighty and extensive, while the latter is more agile, lightweight and privacy-friendly. This work delves into the development of a real-time model adaptation strategy that can be applied to any pre-trained models, including large language models, to enhance their adaptive capabilities. Our research does not involve any human subjects or dataset releases, nor does it raise any ethical concerns. Since this work does not directly tie to specific applications, we do not foresee any immediate negative societal impacts. Nonetheless, we acknowledge that any technological advancement may carry potential risks, and we encourage the continued assessment of the broader impacts of real-time adaptation methodologies in various contexts. B FAQ & D ISCUSSIONS To facilitate the reviewing process, we summarize the answers to the questions that arose during the discussion of an earlier version of this paper. The major updates of this version are reorganized theoretical studies, incremental clustering details, experimental reorganization, and additional datasets and settings . We include more related field comparisons to distinguish different settings. We also cover the position of this paper in literature and the main claims of this paper. Finally, we will frankly acknowledge the limitations of this paper, explain and justify the scope of coverage, and provide possible future directions. Q1: What is the relationship between the proposed ATTA protocol and stream based active learning (Saran et al., 2023)? A: We would like to discuss the difference between our work and the referenced work. 1. Real-time Training Distinction: Saran et al. (2023) doesn’t operate in real-time capacity. This is evident from their experiments, where their model is trained only after completing a round. In contrast, our work involves training the model post each batch. This positions Saran et al. (2023)’s work as an intrinsic active learning technique, while our approach leans towards TTA methods. 2. Continual Training Nuance: Following the point above, Saran et al. (2023) stands out of the scope of continual training. As they mentioned ‘each time new data are acquired, the ResNet is reset to the ImageNet pre-trained weights before being updated‘, Saran et al. (2023) starts afresh with each iteration and is out of scope for CF discussions. Contrarily, our model is continuously trained on varying distributions, compelling us to address the CF issue while preserving advantages derived from various stored distributions. 3. Comparative Complexity: Given the aforementioned distinctions, it’s evident that our task presents a greater challenge compared to theirs. In addition, we have included comparisons with stronger active learning settings in Sec. 5.3. Q2: What are the insights from the theoretically foundational analysis? A: 1. It sets a well-defined formulation and grounded theoretical framework for the ATTA setting. 2. While entropy minimizations can cause CF, balancing the learning rate and number of high/low entropy samples is conversely the key solution to both distribution shifts and 17Published as a conference paper at ICLR 2024 CF by corresponding benefits. Though adding low-entropy data is intuitive, it is crucial in that this simple operation can make methods either too conservative or too aggressive without the correct balancing conditions. 3. The studies in Sec. 3.1 directly present a feasible and guaranteed solution for imple- menting ATTA to tackle shifts while avoiding CF. The aligned empirical validations of Sec. 3.2 also instruct the implementation of SimATTA. Q3: In test-time adaptation, one important issue is that the number of testing samples in a batch may be small, which means the sample size m will also be very small. May it affect the theorem and make them become very loose? A: We consider this issue jointly from theoretical and empirical validations. 1. It is true that the theoretical bounds can be loose given a small size of m unlabeled test samples. This situation of the error bound is mathematically ascribed to the quotient between the VC-dimension d of the hypothesis class and m. Under the VC-dimension theory, the ResNet18 model we adopt should have d ≫ m. However, practically we perform fine-tuning on pre-trained models instead of training from scratch, which significantly reduces the scale of parameter update. In this case, an assumption can be established that fine-tuning a model is roughly equivalent to learning a model with a relatively small d (Appx. H). This assumption is potentially underpinned by the empirical alignment of our validation experiments with the theoretical framework (Fig. 1). To this end, experiments indicate thatd and m are practically of similar scale for our settings. This prevents our theoretical bounds from being very loose and meaningless in reality. 2. Regarding cases that our assumption does not apply, this issue would appear inevitable, since it is rigorously inherent in the estimation error of our streaming and varying test distributions. The distribution of a test stream can be hardly monitored when only a limited batch is allowed, which we consider as a limitation of TTA settings. Moreover, this issue directly implies the necessity of using a buffer for unlabeled samples. A good practice is to maintain a relatively comparable sample buffer scale. Q4: What distribution shifts can ATTA solve? A: We would like to follow (but not limited to) the work (Zhao et al., 2023b) to discuss the distribution shifts ATTA can solve. 1. As elucidated in Sec. 3.1 and Sec. 5, ATTA can solve domain generalization shifts. Domain generalization shifts include complex shifts on the joint data distribution P(X, Y), given X as the covariates and Y as the label variable. Since P(X, Y) = P(X)P(Y |X), ATTA can handle covariate shift (P(X)), label shift (P(Y )), and conditional shift (P(Y |X)). The shifts on both covariate and conditional distributions can cover the shift on labels, but they (covariate + conditional shifts) are more complicated than pure label shifts, where only the marginal label distribution changes while the conditional distribution remains. Note that the conditional shifts are generally caused by spurious correlations, where the independent causal mechanism assumption (Pearl, 2009) holds or no concept drifts exist. 2. In our framework, the distribution support of X at different time steps can be different, but we don’t cover the situation where the support of Y changes, i.e., class-incremental problems. Q5: It is unclear how many samples are selected in each minibatch of testing samples. How the total budget is distributed across the whole testing data stream? A: The number of selected samples for each minibatch is decided jointly by the incremental clustering and the cluster centroid number NC (t). Intuitively, this sample selection is a dynamic process, with NC (t) restricting the budget and incremental clustering performing sample selection. For each batch, we increase applicable clustering centroids as a maximum limit, while the exact number of the selected samples is given by the incremental clustering by how many clusters are located in the scope of new distributions. e.g., if the incoming batch does not introduce new data distributions, then we select zero samples even with increased NC (t). In contrast, if the incoming batch contains data located in multiple new distributions, the incremental clustering tends to select more samples than the NC (t) limit, thus forcing to merging of multiple previous clusters into one new cluster. 18Published as a conference paper at ICLR 2024 The incremental clustering is detailed in Sec. 4.2, and NC (t) is naively increased by a constant hyper-parameter k. Therefore, the budget is adaptively distributed according to the data streaming distribution with budgets controlled by k, which is also the reason why we compare methods under a budget limit. Q6: Could compared methods have access to a few ground-truth labels as well? Making other algorithms be able to use the same amount of ground-truth labels randomly will produce fairer comparisons. A: 1. The enhanced TTA setting is exactly the setup we provide to produce fairer comparisons. See Tab. 3 and Tab. 5 for comparison results. 2. ATTA also compares to a stronger setting ADA which can access the whole test datasets multiple times. Table 5: The table demonstrates the comparisons on PACS where all enhanced TTA baselines have 300 budgets to randomly select labeled samples. The training steps of these labeled samples are the same as the original TTA method training steps. For accumulated sample selection, please refer to our ablation studies. Method Domain-wise data stream A VG Random data stream A VG P→ →A→ →C→ →S P A C S 1 2 3 4 P A C S Source onlyBN w/o adapt 99.70 59.38 28.03 42.91 99.70 59.38 28.03 42.91 43.44 43.44 43.44 43.44 99.70 59.38 28.03 42.91BN w/ adapt 98.74 68.07 64.85 54.57 98.74 68.07 64.85 54.57 62.50 62.50 62.50 62.50 98.74 68.07 64.85 54.57 TTA Tent (steps=1) N/A 70.07 68.43 64.42 97.72 74.17 72.61 68.92 61.20 62.36 66.59 67.32 98.14 74.37 70.26 66.07Tent (steps=10) N/A 76.27 63.78 49.35 59.46 38.62 48.46 55.03 56.20 53.22 52.55 55.55 58.32 47.56 60.75 58.00EATA N/A 69.53 66.94 61.42 98.56 69.38 66.60 64.83 60.34 59.81 64.38 65.02 98.68 73.78 68.30 59.74CoTTA N/A 66.55 63.14 59.91 90.12 61.67 66.68 67.68 57.26 57.36 63.46 65.64 92.22 71.53 70.44 62.41SAR (steps=1) N/A 66.60 63.78 50.34 98.38 67.87 64.04 49.48 57.21 56.06 56.78 57.14 98.38 68.80 64.59 53.02SAR (steps=10) N/A 69.09 66.55 49.07 96.23 62.50 59.34 46.53 49.76 52.74 48.51 49.06 95.39 57.13 54.61 38.76 Ours (B ≤300) N/A 76.86 70.90 75.39 98.80 84.47 82.25 81.52 69.47 76.49 82.45 82.22 98.98 84.91 83.92 86.00 Q7: What is the position of ATTA? A: Comparisons with different settings are challenging. In this work, the design of our experiments (Sec. 5) is to overcome this challenge by comparing both weaker settings and stronger settings. While the significant performance over weaker settings renders the necessity of extra information, the comparable performance with stronger settings provides the potential to relax restricted requirements. Intuitively, ATTA is the most cost-effective option in the consideration of both efficiency and effectiveness. We further provide the following ATTA summary: ATTA, which incorporates active learning in FTTA, is the light, real-time, source-free, widely applicable setting to achieve high generalization performances for test-time adaptation. 1. Necessity: From the causality perspective, new information is necessary (Lin et al., 2022; Pearl, 2009; Peters et al., 2017) to attain generalizable over distribution shifts which are insurmountable within the current TTA framework. 2. Effectiveness: Compared to FTTA methods, ATTA produces substantially better perfor- mances, on-par with the costly active domain adaptation (ADA) methods as shown in Table 3 in the paper. 3. Efficiency: Relative to ADA methods, ATTA possesses superior efficiency, similar to general FTTA methods, as shown in Tab. 3. 4. Applicability: ATTA is a model-agnostic setting. (1) Compared to domain generalization methods, ATTA do not require re-training and has the potential to apply to any pre-trained models. One interesting future direction is designing ATTA methods for large language models (LLMs), where re-trainings are extremely expensive and source data may be in- accessible. (2) Compared to FTTA methods, ATTA can protect model parameters from corrupting while learning new distributions by fine-tuning pre-trained models, rendering it more feasible and practical. In comparison with existing works, ATTA is motivated to mitigate the limitations of previous settings: 1. FTTA: Limited generalization performance. 19Published as a conference paper at ICLR 2024 2. TTT: Not source-free; limited generalization performance. 3. ADA & domain adaptation/generalization: Expensive re-trainings; limited applicability to pre-trained models. 4. Online active learning: It does not maintain and protect adaptation performances for multiple distributions in one model and does not consider the CF problem. Q8: What is the potential practical utility of ATTA? A: 1. Empirically, our method can generally finish a round of sample selection/training of 100 frames in 5s, i.e., 20 frames per sec, which is more than enough to handle multiple practical situations. Experiments on time complexity are provided in Tab. 3, where SimATTA has comparable time efficiency. 2. As a case analysis, the autopilot system (Hu et al., 2023; Chen et al., 2022a) presents an application scenario requiring high-speed low-latency adaptations, while these adaptations are largely underexplored. When entering an unknown environment, e.g., a construction section, a system of ATTA setting can require the driver to take over the wheel. During the period of manual operation when the driver is handling the wheel, steering signals are generated, and the in-car system quickly adaptations. The system doesn’t need to record 60 frames per second, since only the key steering operations and the corresponding dash cam frames are necessary, which can be handled by ATTA algorithms processing at 20 frames per sec. In this case, the human annotations are necessary and indirect. ATTA makes use of this information and adapts in the short term instead of collecting videos and having a long-round fine-tuning (Schafer et al., 2018). 3. In addition, many scenarios applicable for ATTA are less speed-demanding than the case above. One example is a personalized chatbot that subtly prompts and gathers user labels during user interaction. In a home decoration setting, applications can request that users scan a few crucial areas to ensure effective adaptation. Social robots (Mavrogiannis et al., 2023), e.g., vacuum robots, often require users to label critical obstacles they’ve encountered. 4. Compared with ADA, ATTA stands out as the tailored solution for the above scenarios. It does not require intensive retraining or server-dependent fine-tuning, offering both speed and computational efficiency. Meanwhile, akin to other TTA methods, ATTA also ensures user privacy. While it might marginally exceed the cost of standard TTA methods, the superior generalization ability makes it a compelling choice and justifies the additional expense. Q9: What can be covered by this paper? A: This paper endeavors to establish the foundational framework for a novel setting referred to as ATTA. We target (1) positioning the ATTA setting, (2) solving the two major and basic challenges of ATTA,i.e., the mitigation of distribution shifts and the avoidance of catastrophic forgetting (CF). We achieve the first goal by building the problem formulation and analyses, and further providing extensive qualitative and well-organized experimental comparisons with TTA, enhanced TTA, and ADA settings. These efforts position ATTA as the most cost-effective option between TTA and ADA, where ATTA inherits the efficiency of TTA and the effectiveness of ADA. With our theoretical analyses and the consistent algorithm design, we validate the success of our second goal through significant empirical performances. Q10: What are not covered by this paper? A: Constructing a new setting involves multifaceted complexities. Although there are various potential applications discussed above including scaling this setting up for large models and datasets, we cannot cover them in this single piece of work. There are three main reasons. First, the topics covered by a single paper are limited. Formally establishing ATTA setting and addressing its major challenges of ATTA takes precedence over exploring practical applications. Secondly, given the interrelations between ATTA and other settings, our experimental investigations are predominantly comparative, utilizing the most representative datasets from TTA and domain adaptation to showcase persuasive results. Thirdly, many practical applications necessitate task-specific configurations, rendering them unsuitable for establishing a universal learning setting. While the current focus is on laying down the foundational aspects of ATTA, the exploration of more specialized applications remains a prospective avenue for future work in the ATTA domain. 20Published as a conference paper at ICLR 2024 C R ELATED WORKS The development of deep learning witnesses various applications (He et al., 2016; Gui et al., 2020). To tackle OOD problem, various domain generalization works emerge (Krueger et al., 2021; Sagawa et al., 2019). C.1 U NSUPERVISED DOMAIN ADAPTATION Unsupervised Domain Adaptation (UDA) (Pan et al., 2010; Patel et al., 2015; Wilson and Cook, 2020; Wang and Deng, 2018) aims at mitigating distribution shifts between a source domain and a target domain, given labeled source domain samples and unlabeled target samples. UDA methods generally rely on feature alignment techniques to eliminate distribution shifts by aligning feature distributions between source and target domains. Typical feature alignment techniques include discrepancy minimization (Long et al., 2015; Sun and Saenko, 2016; Kang et al., 2019) and adversarial training (Ganin and Lempitsky, 2015; Tsai et al., 2018; Ajakan et al., 2014; Ganin et al., 2016; Tzeng et al., 2015; 2017). Nevertheless, alignments are normally not guaranteed to be correct, leading to the alignment distortion problem as noted by Ning et al. (2021). Source-free Unsupervised Domain Adaptation (SFUDA) (Fang et al., 2022; Liu et al., 2021b) algorithms aim to adapt a pre-trained model to unlabeled target domain samples without access to source samples. Based on whether the algorithm can access model parameters, these algorithms are categorized into white-box and black-box methods. White-box SFUDA typically considers data recovery (generation) and fine-tuning methods. The former focuses on recovering source- like data (Ding et al., 2022; Yao et al., 2021), e.g., training a Generative Adversarial Network (GAN) (Kurmi et al., 2021; Li et al., 2020), while the latter employs various techniques (Mao et al., 2021), such as knowledge distillation (Chen et al., 2022b; Liu and Yuan, 2022; Yang et al., 2021b; Yu et al., 2022), statistics-based domain alignment (Ishii and Sugiyama, 2021; Liu et al., 2021a; Fan et al., 2022; Eastwood et al., 2021), contrastive learning (Huang et al., 2021; Wang et al., 2022b), and uncertainty-based adaptation (Gawlikowski et al., 2021; Fleuret et al., 2021; Chen et al., 2021; Li et al., 2021b). Black-box SFUDA cannot access model parameters and often relies on self-supervised knowledge distillation (Liang et al., 2022; 2021), pseudo-label denoising (Zhang et al., 2021; Yang et al., 2022), or generative distribution alignment (Yeh et al., 2021; Yang et al., 2021a). C.2 T EST-TIME ADAPTATION Test-time Adaptation (TTA), especially Fully Test-time Adaptation (FTTA) algorithms (Wang et al., 2021; Iwasawa and Matsuo, 2021; Karani et al., 2021; Nado et al., 2020; Schneider et al., 2020; Wang et al., 2022a; Zhao et al., 2023a; Niu et al., 2022; Zhang et al., 2022a; Niu et al., 2023; You et al., 2021; Zhang et al., 2022b), can be considered as realistic and lightweight methods for domain adaptation. Built upon black-box SFUDA, FTTA algorithms eliminate the requirement of a pre-collected target dataset and the corresponding training phase. Instead, they can only access an unlabeled data stream and apply real-time adaptation and training. In addition to FTTA, Test-time Training (TTT) (Sun et al., 2020; Liu et al., 2021c) often relies on appending the original network with a self-supervised task. TTT methods require retraining on the source dataset to transfer information through the self-supervised task. Although they do not access the source dataset during the test-time adaptation phase, TTT algorithms are not off-the-shelf source-free methods. TTA is a promising and critical direction for real-world applications, but current entropy minimization-based methods can be primarily considered as feature calibrations that require high-quality pseudo-labels. This requirement, however, can be easily violated under larger distribution shifts. Current TTA algorithms, inheriting UDA drawbacks, cannot promise good feature calibration results, which can be detrimental in real-world deployments. For instance, entropy minimization on wrongly predicted target domain samples with relatively low entropy can only exacerbate spurious correla- tions (Chen et al., 2020). Without extra information, this problem may be analogous to applying causal inference without intervened distributions, which is intrinsically unsolvable (Peters et al., 2016; Pearl, 2009). This paper aims to mitigate this issue with minimal labeled target domain samples. To minimize the cost, we tailor active learning techniques for TTA settings. It is worth noting that a recent work AdaNPC (Zhang et al., 2023) is essentially a domain gener- alization method with a TTA phase attached, while our ATTA is built based on the FTTA setting. Specifically, Current FTTA methods and our work cannot access the source domain. In contrast, 21Published as a conference paper at ICLR 2024 AdaNPC accesses source data to build its memory bank, circumventing the catastrophic forgetting problem. Furthermore, AdaNPC requires multiple source domains and training before performing TTA. Thus AdaNPC uses additional information on domain labels and retraining resources for its memory bank, undermining the merits of FTTA. Regarding theoretical bounds, their target domain is bounded by source domain error and model estimations (in big-O expression), while we consider active sample learning and time variables for varying test distributions. C.3 C ONTINUAL DOMAIN ADAPTATION Many domain adaptation methods focus on improving target domain performance, neglecting the performance on the source domain, which leads to the CF problem (Kemker et al., 2018; Kirkpatrick et al., 2017; Li and Hoiem, 2017; Lopez-Paz and Ranzato, 2017; De Lange et al., 2021; Wang et al., 2022a; Niu et al., 2022). This issue arises when a neural network, after being trained on a sequence of domains, experiences a significant degradation in its performance on previously learned domains as it continues to learn new domains. Continual learning, also known as lifelong learning, addresses this problem. Recent continual domain adaptation methods have made significant progress by employing gradient regularization, random parameter restoration, buffer sample mixture, and more. Although the CF problem is proposed in the continual learning field, it can occur in any source-free OOD settings since the degradation caused by CF is attributed to the network’s parameters being updated to optimize performance on new domains, which may interfere with the representations learned for previous domains. C.4 A CTIVE DOMAIN ADAPTATION Active Domain Adaptation (ADA) (Prabhu et al., 2021; Ning et al., 2021; Su et al., 2020; Ma et al., 2021; Xie et al., 2022) extends semi-supervised domain adaptation with active learning strate- gies (Cohn et al., 1996; Settles, 2009), aiming to maximize target domain performance with a limited annotation budget. Therefore, the key challenge of active learning algorithms is selecting the most informative unlabeled data in target domains (Kapoor et al., 2007). Sample selection strategies are of- ten based on uncertainty (Lewis and Catlett, 1994; Scheffer et al., 2001), diversity (Jain and Grauman, 2016; Hoi et al., 2009), representativeness (Xu et al., 2003), expected error minimization (Vijaya- narasimhan and Kapoor, 2010), etc. Among these methods, uncertainty and diversity-based methods are simple and computationally efficient, making them the most suitable choices to tailor for TTA settings. Adapting these strategies is non-trivial because, compared to typical active domain adaptation, our proposed Active Test-time Adaptation (ATTA) setting does not provide access to source data, model parameters, or pre-collected target samples. This requirement demands that our active sample selection algorithm select samples for annotation during data streaming. Consequently, this active sampling selection process is non-regrettable, i.e., we can only meet every sample once in a short period. To avoid possible confusion, compared to the recent Source-free Active Domain Adaptation (SFADA) method SALAD (Kothandaraman et al., 2023), we do not require access to model parameter gradients, training additional neural networks, or pre-collected target datasets. Therefore, our ATTA setting is quite different, much lighter, and more realistic than ADA and SFADA. C.5 A CTIVE ONLINE LEARNING The most related branch of active online learning (AOL) (Cacciarelli and Kulahci, 2023) is active online learning on drifting data stream (Zhou et al., 2021; Baier et al., 2021; Li et al., 2021a). Generally, these methods include two components, namely, detection and adaptation. Compared with ATTA, there are several distinctions. First, this line of studies largely focuses on the distribution shift detection problem, while ATTA focuses on multi-domain adaptations. Second, AOL on drifting data stream aims to detect and adapt to one current distribution in the stream, without considering preserving the adaptation abilities of multiple past distributions by maintaining and fine-tuning the original pre-trained models. In contrast, ATTA’s goal is to achieve the OOD generalization optimums adaptable across multiple source and target distributions, leading to the consideration of CF problems. Third, while AOL requires one-by-one data input and discard, ATTA maintains a buffer for incoming data before selection decisions. This is because ATTA targets maintaining the original model without corrupting and replacing it, such that making statistically meaningful and high-quality decisions is 22Published as a conference paper at ICLR 2024 critical for ATTA. In contrast, AOL allows resetting and retraining new models, whose target is more lean to cost saving and one-by-one manner. D F URTHER THEORETICAL STUDIES In this section, we refine the theoretical studies with supplement analysis and further results. We use the H-divergence and H∆H-distance definitions following (Ben-David et al., 2010). Definition 2 (H-divergence). For a function class H and two distributions D1 and D2 over a domain X, the H-divergence between D1 and D2 is defined as dH(D1, D2) = sup h∈H |Px∼D1 [h(x) = 1] − Px∼D2 [h(x) = 1]|. The H∆H-distance is defined base on H-divergence. We use the H∆H-distance definition follow- ing (Ben-David et al., 2010). Definition 3 (H∆H-distance). For two distributions D1 and D2 over a domain X and a hypothesis class H, the H∆H-distance between D1 and D2 w.r.t. H is defined as dH∆H(D1, D2) = sup h,h′∈H Px∼D1 [h(x) ̸= h′(x)] + Px∼D2 [h(x) ̸= h′(x)]. (9) The H∆H-distance essentially provides a measure to quantify the distribution shift between two distributions. It measures the maximum difference of the disagreement between two hypotheses in H for two distributions, providing a metrics to quantify the distribution shift between D1 and D2. H-divergence and H∆H-distance have the advantage that they can be applied between datasets, i.e., estimated from finite samples. Specifically, let S1, S2 be unlabeled samples of size m sampled from D1 and D2; then we have estimated H∆H-distance ˆdH(S1, S2). This estimation can be bounded based on Theorem 3.4 of Kifer et al. (2004), which we state here for completeness. Theorem 5. Let A be a collection of subsets of some domain measure space, and assume that the VC-dimension is some finite d. Let P1 and P2 be probability distributions over that domain and S1, S2 finite samples of sizes m1, m2 drawn i.i.d. according P1, P2 respectively. Then Pm1+m2 [|ϕA(S1, S2) − ϕA(P1, P2)| > ϵ] ≤ (2m)de−m1ϵ2/16 + (2m)de−m2ϵ2/16, (10) where Pm1+m2 is the m1 + m2’th power of P - the probability that P induces over the choice of samples. Theorem 5 bounds the probability for relativized discrepancy, and its applications in below lemmas and Theorem 1 help us bound the quantified distribution shifts between domains. The probability, according to a distribution D, that an estimated hypothesis h disagrees with the true labeling function g : X → {0, 1} is defined as ϵ(h(t), g) = E(x)∼D[|h(x, t) − g(x)|], which we also refer to as the error or risk ϵ(h(t)). While the source domain dataset is inaccessible under ATTA settings, we consider the existence of the source dataset DS for the purpose of accurate theoretical analysis. Thus, we initialize Dtr(0) as DS, i.e., Dtr(0) = DS. For every time step t, the test and training data can be expressed as Ute(t) and Dtr(t) = DS ∪ Dte(1) ∪ Dte(2) ∪ ··· ∪Dte(t). (11) We use N to denote the total number of samples in Dtr(t) and λ = (λ0, λ1, ··· , λt) to represent the ratio of sample numbers in each component subset. In particular, we have |DS| |Dtr(t)| = λ0, |Dte(1)| |Dtr(t)| = λ1, ··· , |Dte(t)| |Dtr(t)| = λt, (12) where Pt i=0 λi = 1. Therefore, at time step t, the model has been trained on labeled data Dtr(t), which contains t + 1 components consisting of a combination of data from the source domain and multiple test-time domains. For each domain the model encounters, DS, Ute(1), Ute(2), ··· , Ute(t), let ϵj(h(t)) denote the error of hypothesis h at time t on the jth domain. Specifically, ϵ0(h(t)) = ϵS(h(t)) represents the error of h(t) on the source data DS, and ϵj(h(t)) for j ≥ 1 denotes the error of h(t) on test data Ute(j). Our optimization minimizes a convex combination of training error over the labeled samples from all domains. Formally, given the vector w = (w0, w1, ··· , wt) of domain error 23Published as a conference paper at ICLR 2024 weights with Pt j=0 wj = 1 and the sample number from each component Nj = λjN, we minimize the empirical weighted error of h(t) as ˆϵw(h(t)) = tX j=0 wjˆϵj(h(t)) = tX j=0 wj Nj X Nj |h(x, t) − g(x)|. (13) Note that w, λ and N are also functions of t, which we omit for simplicity. We now establish two lemmas as the preliminary for Theorem 1. In the following lemma, we bound the difference between the weighted error ϵw(h(t)) and the domain error ϵj(h(t)). Lemma 6. Let H be a hypothesis space of VC-dimension d. At time step t, let the ATTA data domains be DS, Ute(1), Ute(2), ··· , Ute(t), and Si be unlabeled samples of size m sampled from each of the t + 1 domains respectively. Then for any δ ∈ (0, 1), for every h ∈ Hminimizing ϵw(h(t)) on Dtr(t), we have |ϵw(h(t)) − ϵj(h(t))| ≤ tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi  , with probability of at least 1 − δ, where γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. In the following lemma, we provide an upper bound on the difference between the true and empirical weighted errors ϵw(h(t)) and ˆϵw(h(t)). Lemma 7. Let H be a hypothesis class. For Dtr(t) = DS ∪ Dte(1) ∪ ··· ∪Dte(t) at time t, if the total number of samples in Dtr(t) is N, and the ratio of sample numbers in each component is λj, then for any δ ∈ (0, 1) and h ∈ H, with probability of at least 1 − δ, we have P[|ϵw(h(t)) − ˆϵw(h(t))| ≥ϵ] ≤ 2 exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . Thus, as wj deviates from λj, the feasible approximation ˆϵw(h(t)) with a finite number of labeled samples becomes less reliable. The proofs for both lemmas are provided in Appx. E. Building upon the two preceding lemmas, we proceed to derive bounds on the domain errors under the ATTA setting when minimizing the empirical weighted error using the hypothesis h at time t. Lemma 6 bounds the difference between the weighted error ϵw(h(t)) and the domain error ϵj(h(t)), which is majorly influenced by the estimatedH∆H-distance and the quality of discrepancy estimation. During the ATTA process, the streaming test data can form multiple domains and distributions. However, if we consider all data during the test phase as a single test domain,i.e., St i=1 Ute(i), we can simplify Lemma 6 to obtain an upper bound for the test error ϵT as |ϵw(h(t)) − ϵT (h(t))| ≤w0  1 2 ˆdH∆H(S0, ST ) + 2 s 2d log(2m) + log 2 δ m + γ  , (14) where γ = min h∈H{ϵ0(h(t)) + ϵT (h(t))}, and ST is sampled from St i=1 Ute(i). To understand Lamma 7, we need to understand Hoeffding’s Inequality, which we state below as a Proposition for completeness. Proposition 8 (Hoeffding’s Inequality). Let X be a set, D1, . . . , Dt be probability distributions on X, and f1, . . . , ft be real-valued functions on X such that fi : X → [ai, bi] for i = 1, . . . , t. Then for any ϵ >0, P  \f\f\f\f\f 1 t tX i=1 fi(x) − 1 t tX i=1 Ex∼Di[fi(x)] \f\f\f\f\f ≥ ϵ ! ≤ 2 exp   − 2t2ϵ2 Pt i=1(bi − ai)2 ! (15) where E[fi(x)] is the expected value of fi(x). Lamma 7 provides an upper bound on the difference between the true and empirical weighted errors ϵw(h(t)) and ˆϵw(h(t)). Thus, as wj deviates from λj, the feasible approximation ˆϵw(h(t)) with a finite number of labeled samples becomes less reliable. Building upon the two preceding lemmas, we proceed to derive bounds on the domain errors under the ATTA setting when minimizing the empirical weighted error using the hypothesis h at time t. Theorem 1 essentially bounds the performance of ATTA on the source and each test domains. The adaptation performance on a test domain is majorly 24Published as a conference paper at ICLR 2024 bounded by the composition of (labeled) training data, estimated distribution shift, and ideal joint hypothesis performance, which correspond to C, ˆdH∆H(Si, Sj), and γi, respectively. The ideal joint hypothesis error γi gauges the inherent adaptability between domains. If we consider the multiple data distributions during the test phase as a single test domain, i.e., St i=1 Ute(i), Theorem 1 can be reduced into bounds for the source domain error ϵS and test domain error ϵT . With the optimal test/source hypothesis h∗ T (t) = arg min h∈H ϵT (h(t)) and h∗ S(t) = arg minh∈H ϵS(h(t)), |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤w0A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (16a) |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤(1 − w0)A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (16b) where the distribution divergence termA = ˆdH∆H(S0, ST )+4 q 2d log(2m)+log 2 δ m +2γ, the empirical gap term B = 2 q d log(2N)−log(δ) 2N , ST is sampled from St i=1 Ute(i), and γ = minh∈H{ϵ0(h(t)) + ϵT (h(t))}. Our learning bounds demonstrates the trade-off between the small amount of budgeted test-time data and the large amount of less relevant source data. Next, we provide an approximation of the condition necessary to achieve optimal adaptation performance, which is calculable from finite samples and can be readily applied in practical ATTA scenarios. Following Eq. (16.a), with approximately B = c1 p d/N, the optimal value w∗ 0 to tighten the test error bound is a function of λ0 and A: w∗ 0 = λ0 − s A2N c2 1d − A2Nλ0(1 − λ0), for λ 0 ≥ 1 − d A2N , (17) where c1 is a constant. Note that λ0 ≥ 1 − d A2N should be the satisfied condition in practical ATTA settings, where the budget is not sufficiently big while the source data amount is relatively large. When the budget is sufficiently large or the source data amount is not sufficiently large compared to the distribution shift A, the optimal w∗ 0 for the test error bound is w∗ 0 = 0, i.e., using no source data since possible error reduction from the data addition is always less than the error increase caused by large divergence between the source data and the test data. Theorem 2 offers a direct theoretical guarantee that ATTA reduces the error bound on test domains in comparison to TTA without the integration of active learning. Following Theorem 1, when no active learning is included during TTA,i.e., w0 = λ0 = 1, the upper boundw0A+ q w2 0 λ0 + (1−w0)2 1−λ0 B ≥ A+B; when enabling ATTA, withw0 = λ0 ̸= 1, we can easily achieve an upper bound w0A + B < A+ B. Therefore, the incorporation of labeled test instances in ATTA theoretically enhances the overall performance across test domains, substantiating the significance of the ATTA setting in addressing distribution shifts. Entropy quantifies the amount of information contained in a probability distribution. In the context of a classification model, lower entropy indicates that the model assigns high probability to one of the classes, suggesting a high level of certainty or confidence in its prediction. When a model assigns low entropy to a sample, this high confidence can be interpreted as the sample being well-aligned or fitting closely with the model’s learned distribution. In other words, the model “recognizes” the sample as being similar to those it was trained on, hence the high confidence in its prediction. While entropy is not a direct measure of distributional distance, it can be used as an indicator of how closely a sample aligns with the model’s learned distribution. This interpretation is more about model confidence and the implied proximity rather than a strict mathematical measure of distributional distance. The pre-trained model is well-trained on abundant source domain data, and thus the model distribution is approximately the source distribution. Selecting low-entropy samples using essentially provides an estimate of sampling from the source dataset. Thus, Dϕ,S(t), based on well-aligned with the model’s learned distribution is an approximation of DS. When we consider the CF problem and feasibly include the source-like dataset Dϕ,S(t) into the ATTA training data in place of the inaccessible DS in Eq. (11), we can also derive bounds on the domain errors under this practical ATTA setting when minimizing the empirical weighted errorϵ′ w(h(t)) using the hypothesis h at time t, similar to Theorem 1. Let H be a hypothesis class of VC-dimension d. At time step t, for ATTA data domainsDϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), Si are unlabeled samples of size m sampled from each of the t + 1 domains respectively. The total number of samples in Dtr(t) is 25Published as a conference paper at ICLR 2024 N and the ratio of sample numbers in each component is λi. If ˆh(t) ∈ Hminimizes the empirical weighted error ˆϵ′ w(h(t)) with the weight vector w on Dtr(t), and h∗ j (t) = arg minh∈H ϵj(h(t)) is the optimal hypothesis on the jth domain, then for any δ ∈ (0, 1), we have ϵj(ˆh(t)) ≤ ϵj(h∗ j (t)) + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   + 2C with probability of at least 1 − δ, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. Other derived results following Theorem 1 also apply for this practical ATTA setting. Further empirical validations for our theoretical results are provided in Appx. H. E P ROOFS This section presents comprehensive proofs for all the lemmas, theorems, and corollaries mentioned in this paper, along with the derivation of key intermediate results. Lemma 6. Let H be a hypothesis space of VC-dimension d. At time step t, let the ATTA data domains be DS, Ute(1), Ute(2), ··· , Ute(t), and Si be unlabeled samples of size m sampled from each of the t + 1 domains respectively. Then for any δ ∈ (0, 1), for every h ∈ Hminimizing ϵw(h(t)) on Dtr(t), we have |ϵw(h(t)) − ϵj(h(t))| ≤ tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi  , with probability of at least 1 − δ, where γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. Proof. First we prove that given unlabeled samples of size m S1, S2 sampled from two distributions D1 and D2, we have dH∆H(D1, D2) ≤ ˆdH∆H(S1, S2) + 4 s 2d log(2m) + log 2 δ m . (18) We start with Theorem 3.4 of Kifer et al. (2004): Pm1+m2 [|ϕA(S1, S2) − ϕA(P1, P2)| > ϵ] ≤ (2m)de−m1ϵ2/16 + (2m)de−m2ϵ2/16. (19) In Eq. 19, ’d’ is the VC-dimension of a collection of subsets of some domain measure space A, while in our case, d is the VC-dimension of hypothesis space H. Following (Ben-David et al., 2010), the H∆H space is the set of disagreements between every two hypotheses inH, which can be represented as a linear threshold network of depth 2 with 2 hidden units. Therefore, the VC-dimension of H∆H is at most twice the VC-dimension of H, and the VC-dimension of our domain measure space is 2d for Eq. 19 to hold. Given δ ∈ (0, 1), we set the upper bound of the inequality to δ, and solve for ϵ: δ = (2m)2de−m1ϵ2/16 + (2m)2de−m2ϵ2/16. We rewrite the inequality as δ (2m)2d = e−m1ϵ2/16 + e−m2ϵ2/16; taking the logarithm of both sides, we get log δ (2m)2d = −m1 ϵ2 16 + log(1 +e−(m1−m2) ϵ2 16 ). 26Published as a conference paper at ICLR 2024 Assuming m1 = m2 = m and defining a = ϵ2 16 , we have log δ (2m)2d = −ma + log 2; rearranging the equation, we then get ma + log(δ/2) = 2d log(2m). Now, we can solve for a: a = 2d log(2m) + log 2 δ m . Recall that a = ϵ2 16 , so we get: ϵ = 4√a ϵ = 4 s 2d log(2m) + log 2 δ m . With probability of at least 1 − δ, we have |ϕA(S1, S2) − ϕA(P1, P2)| ≤4 s 2d log(2m) + log 2 δ m ; therefore, dH∆H(D1, D2) ≤ ˆdH∆H(S1, S2) + 4 s 2d log(2m) + log 2 δ m . (20) Now we prove Lemma 6. We use the triangle inequality for classification error in the derivation. For the domain error of hypothesis h at time t on the jth domain ϵj(h(t)), given the definition of ϵw(h(t)), |ϵw(h(t)) − ϵj(h(t))| = | tX i=0 wiϵi(h(t)) − ϵj(h(t))| ≤ tX i=0 wi|ϵi(h(t)) − ϵj(h(t))| ≤ tX i=0 wi(|ϵi(h(t)) − ϵi(h(t), h∗ i (t))| + |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))| + |ϵj(h(t), h∗ i (t)) − ϵj(h(t))|) ≤ tX i=0 wi(ϵi(h∗ i (t)) + |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))| + ϵj(h∗ i (t))) ≤ tX i=0 wi(γi + |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))|), where γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. By the definition of H∆H-distance and our proved Eq. 20, |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))| ≤sup h,h′∈H |ϵi(h(t), h′(t)) − ϵj(h(t), h′(t))| = sup h,h′∈H Px∼Di[h(x) ̸= h′(x)] + Px∼Dj [h(x) ̸= h′(x)] = 1 2dH∆H(Di, Dj) ≤ 1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m , 27Published as a conference paper at ICLR 2024 where Di, Dj denote the ith and jth domain. Therefore, |ϵw(h(t)) − ϵj(h(t))| ≤ tX i=0 wi(γi + |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))|) ≤ tX i=0 wi(γi + 1 2dH∆H(Di, Dj)) ≤ tX i=0 wi(γi + 1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m ). Since ϵi(h(t)) − ϵj(h(t)) = 0 when i = j, we derive |ϵw(h(t)) − ϵj(h(t))| ≤ tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi  , with probability of at least 1 − δ, where γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. This completes the proof. Lemma 7. Let H be a hypothesis class. For Dtr(t) = DS ∪ Dte(1) ∪ ··· ∪Dte(t) at time t, if the total number of samples in Dtr(t) is N, and the ratio of sample numbers in each component is λj, then for any δ ∈ (0, 1) and h ∈ H, with probability of at least 1 − δ, we have P[|ϵw(h(t)) − ˆϵw(h(t))| ≥ϵ] ≤ 2 exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . Proof. We apply Hoeffding’s Inequality in our proof: P  \f\f\f\f\f 1 t tX i=1 fi(x) − 1 t tX i=1 Ex∼Di[fi(x)] \f\f\f\f\f ≥ ϵ ! ≤ 2 exp   − 2t2ϵ2 Pt i=1(bi − ai)2 ! . (21) In the jth domain, there are λjN samples. With the true labeling function g(x), for each of the λjN samples x, let there be a real-valued function fi(x) fi(x) = wj λj |h(x, t) − g(x)|, where fi(x) ∈ [0, wj λj ]. Incorporating all the domains, we get ˆϵw(h(t)) = tX j=0 wjˆϵj(h(t)) = tX j=0 wj λjN X λjN |h(x, t) − g(x)| = 1 N tX j=0 λjNX i=1 fi(x), which corresponds to the 1 t Pt i=1 fi(x) part in Hoeffding’s Inequality. Due to the linearity of expectations, we can calculate the sum of expectations as 1 N tX j=0 λjNX i=1 E[fi(x)] = 1 N ( tX j=0 λjN wj λj ϵj(h(t))) = tX j=0 wjϵj(h(t)) = ϵw(h(t)), which corresponds to the 1 t Pt i=1 Ex∼Di[fi(x)] part in Hoeffding’s Inequality. Therefore, we can apply Hoeffding’s Inequality as P[|ϵw(h(t)) − ˆϵw(h(t))| ≥ϵ] ≤ 2 exp   −2N2ϵ2/( NX i=0 range2(fi(x))) ! = 2 exp   −2N2ϵ2/( tX j=0 λjN(wj λj )2) ! = 2 exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . This completes the proof. 28Published as a conference paper at ICLR 2024 Theorem 1. Let H be a hypothesis class of VC-dimension d. At time step t, for ATTA data domains DS, Ute(1), Ute(2), ··· , Ute(t), Si are unlabeled samples of size m sampled from each of the t + 1 domains respectively. The total number of samples in Dtr(t) is N and the ratio of sample numbers in each component is λi. If ˆh(t) ∈ Hminimizes the empirical weighted error ˆϵw(h(t)) with the weight vector w on Dtr(t), and h∗ j (t) = arg minh∈H ϵj(h(t)) is the optimal hypothesis on the jth domain, then for any δ ∈ (0, 1), with probability of at least 1 − δ, we have ϵj(ˆh(t)) ≤ ϵj(h∗ j (t)) + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   + 2C, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. For future test domains j = t + k (k >0), assuming k′ = argmink′∈{0,1,...t} dH∆H(D(k′), Ute(t + k)) and min dH∆H (D(k′), Ute(t + k)) ≤ δD, where 0 ≤ δD ≪ +∞, then ∀δ, with probability of at least 1 − δ, we have ϵt+k(ˆh(t)) ≤ ϵt+k(h∗ t+k(t)) + tX i=0 wi  ˆdH∆H(Si, Sk′ ) + 4 s 2d log(2m) + log 2 δ m + δD + 2γi   + 2C. Proof. First we prove that for any δ ∈ (0, 1) and h ∈ H, with probability of at least 1 − δ, we have |ϵw(h(t)) − ˆϵw(h(t))| ≤ vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 . (22) We apply Theorem 3.2 of Kifer et al. (2004) and Lemma 7, P[|ϵw(h(t)) − ˆϵw(h(t))| ≥ϵ] ≤ (2N)d exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . Given δ ∈ (0, 1), we set the upper bound of the inequality to δ, and solve for ϵ: δ = (2N)d exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . We rewrite the inequality as δ (2N)d = e −2Nϵ2/(Pt j=0 w2 j λj ) , taking the logarithm of both sides, we get log δ (2N)d = −2Nϵ2/( tX j=0 w2 j λj ). Rearranging the equation, we then get ϵ2 = ( tX j=0 w2 j λj )d log(2N) − log(δ) 2N . Therefore, with probability of at least 1 − δ, we have |ϵw(h(t)) − ˆϵw(h(t))| ≤ vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 . (23) 29Published as a conference paper at ICLR 2024 Based on Eq. 23, we now prove Theorem 1. For the empirical domain error of hypothesis h at time t on the jth domain ϵj(ˆh(t)), applying Lemma 6, Eq. 23, and the definition of h∗ j (t), we get ϵj(ˆh(t)) ≤ ϵw(ˆh(t)) + tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(ˆh(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(h∗ j (t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(h∗ j (t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵj(h∗ j (t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   = ϵj(h∗ j (t)) + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   + 2C with probability of at least 1 − δ, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. For future test domains j = t + k where k > 0, we have the assumption that k′ = argmink′∈{0,1,...t} dH∆H(D(k′), Ute(t + k)) and min dH∆H(D(k′), Ute(t + k)) ≤ δD. Here, we slightly abuse the notation D(k′) to represent Ds if k′ = 0 and Ute(k′) if k′ > 0. Then we get ϵt+k(ˆh(t)) ≤ ϵw(ˆh(t)) + tX i=0 wi  1 2 ˆdH∆H(Si, St+k) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(ˆh(t)) + tX i=0 wi  1 2( ˆdH∆H(Si, Sk′ ) + ˆdH∆H(Sk′ , St+k)) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(ˆh(t)) + tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(ˆh(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   30Published as a conference paper at ICLR 2024 ≤ ˆϵw(h∗ t+k(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(h∗ t+k(t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵt+k(h∗ t+k(t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + 2 tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   = ϵt+k(h∗ t+k(t)) + tX i=0 wi  ˆdH∆H(Si, Sk′ ) + 4 s 2d log(2m) + log 2 δ m + δD + 2γi   + 2C. with probability of at least 1−δ, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 , γi = minh∈H{ϵi(h(t))+ ϵt+k(h(t))}, and 0 ≤ δD ≪ +∞. This completes the proof. Theorem 2. Let H be a hypothesis class of VC-dimension d. For ATTA data domains DS, Ute(1), Ute(2), ··· , Ute(t), considering the test-time data as a single test domain St i=1 Ute(i), if ˆh(t) ∈ H minimizes the empirical weighted error ˆϵw(h(t)) with the weight vector w on Dtr(t), let the test error be upper-bounded with |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤EBT (w, λ, N, t). Let w′ and λ′ be the weight and sample ratio vectors when no active learning is included, i.e., w′ and λ′ s.t. w′ 0 = λ′ 0 = 1 and w′ i = λ′ i = 0 for i ≥ 1, then for any λ ̸= λ′, there exists w s.t. EBT (w, λ, N, t) < EBT (w′, λ′, N, t). (24) Proof. From Theorem 1, we can derive the bound for the test error where the test-time data are considered as a single test domain: |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤EBT (w, λ, N, t) = w0( ˆdH∆H(S0, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ) + 2 s w2 0 λ0 + (1 − w0)2 1 − λ0 r d log(2N) − log(δ) 2N ; and we simplify the above equation as |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤w0A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (25) where the distribution divergence termA = ˆdH∆H(S0, ST )+4 q 2d log(2m)+log 2 δ m +2γ, the empirical gap term B = 2 q d log(2N)−log(δ) 2N , ST is sampled from St i=1 Ute(i), and γ = minh∈H{ϵ0(h(t)) + ϵT (h(t))}. Since we have s w2 0 λ0 + (1 − w0)2 1 − λ0 = s (w0 − λ0)2 λ0(1 − λ0) + 1 ≥ 1, (26) 31Published as a conference paper at ICLR 2024 where Formula 26 obtains the minimum value if and only if w0 = λ0; when enabling ATTA with any λ0 ̸= 1, we can get EBT (w, λ, N, t) = w0A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B ≥ w0A + B, (27) where the minimum value EBT (w, λ, N, t)min = w0A + B can be obtained with condition w0 = λ0 ̸= 1. When no active learning is included, i.e., for weight and sample ratio vectors w′ and λ′, w′ 0 = λ′ 0 = 1 and w′ i = λ′ i = 0 for i ≥ 1, we have EBT (w′, λ′, N, t) = w′ 0A + s w′2 0 λ′ 0 + (1 − w′ 0)2 1 − λ′ 0 B = A + B. (28) Since for EBT (w, λ, N, t)min = w0A + B, w0 < 1 and A, B >0 hold, we derive EBT (w, λ, N, t)min = w0A + B < A+ B = EBT (w′, λ′, N, t). (29) This completes the proof. Corollary 3. At time step t, for ATTA data domains Dϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), Si are unla- beled samples of size m sampled from each of the t + 1 domains respectively, and SS is unlabeled samples of size m sampled from DS. If ˆh(t) ∈ Hminimizes ˆϵ′ w(h(t)) while other conditions remain identical to Theorem 1, then ϵS(ˆh(t)) ≤ ϵS(h∗ S(t)) + tX i=0 wi  ˆdH∆H(Si, SS) + 4 s 2d log(2m) + log 2 δ m + 2γi   + 2C, with probability at least 1 − δ, where C follows Theorem 1 and γi = minh∈H{ϵi(h(t)) + ϵS(h(t))}. Proof. For the empirical source error on DS of hypothesis h at time t, similar to Theorem 1, we apply Lemma 6, Eq. 23 to get ϵS(ˆh(t)) ≤ ϵw(ˆh(t)) + tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(ˆh(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(h∗ S(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(h∗ S(t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵS(h∗ S(t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + 2 tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   32Published as a conference paper at ICLR 2024 = ϵS(h∗ S(t)) + tX i=0 wi  ˆdH∆H(Si, SS) + 4 s 2d log(2m) + log 2 δ m + 2γi   + 2C with probability of at least 1 − δ, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵS(h(t))}. This completes the proof. Corollary 4. At time step t, for ATTA data domains Dϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), suppose that ˆh(t) ∈ Hminimizes ˆϵw′(h(t)) under identical conditions to Theorem 2. Let’s denote the source error upper bound with |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤EBS(w, λ, N, t). Let w′ and λ′ be the weight and sample ratio vectors when Dϕ,S(t) is not included, i.e., w′ and λ′ s.t. w′ 0 = λ′ 0 = 0 . If ˆdH∆H(DS, Dϕ,S(t)) < ˆdH∆H(DS, St i=1 Ute(i)), then for any λ ̸= λ′, there exists w s.t. EBS(w, λ, N, t) < EBS(w′, λ′, N, t). (30) Proof. From Theorem 1, considering the test-time data as a single test domain, we can derive the bound for the source error on DS: |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤EBS(w, λ, N, t) = w0( ˆdH∆H(S0, SS) + 4 s 2d log(2m) + log 2 δ m + 2γ) + (1 − w0)( ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′) + 2 s w2 0 λ0 + (1 − w0)2 1 − λ0 r d log(2N) − log(δ) 2N , where ST is sampled fromSt i=1 Ute(i), γ = minh∈H{ϵ0(h(t))+ϵS(h(t))}, and γ′ = minh∈H{ϵT (h(t))+ ϵS(h(t))}. We have s w2 0 λ0 + (1 − w0)2 1 − λ0 = s (w0 − λ0)2 λ0(1 − λ0) + 1 ≥ 1, (31) where the equality and the minimum value are obtained if and only if w0 = λ0. When Dϕ,S(t) is not included,i.e., with the weight and sample ratio vectorsw′ and λ′ s.t. w′ 0 = λ′ 0 = 0, using the empirical gap term B = 2 q d log(2N)−log(δ) 2N , we have EBS(w′, λ′, N, t) = ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′ + s w2 0 λ0 + (1 − w0)2 1 − λ0 B = ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′ + B. When Dϕ,S(t) is included with λ0 ̸= 0, EBS(w, λ, N, t) = w0( ˆdH∆H(S0, SS) + 4 s 2d log(2m) + log 2 δ m + 2γ) + (1 − w0)( ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′) + s w2 0 λ0 + (1 − w0)2 1 − λ0 B ≤ w0( ˆdH∆H(S0, SS) + 4 s 2d log(2m) + log 2 δ m + 2γ) + (1 − w0)( ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′) + B, 33Published as a conference paper at ICLR 2024 Algorithm 2 INCREMENTAL CLUSTERING (IC) Require: Given previously selected anchors, new unlabeled samples, and the cluster budget as Danc, Unew, and NC . Global anchor weights wanc = (wanc 1 , . . . , wanc |Danc|)⊤. 1: For simplicity, we consider anchor weights wanc as a global vector. 2: function IC(Danc, Unew, NC ) 3: wsp ← Concat(wanc, 1⊤ |Unew|) ▷ Assign all new samples with weight 1. 4: Φ ← Extract the features from the penultimate layer of model f on x ∈ Danc ∪ Unew in order. 5: clusters ← Weighted-K-Means(Φ, wsp, NC) 6: new_clusters ← {clusteri | ∀clusteri ∈ clusters, ∀x ∈ Danc, x /∈ clustersi} 7: Xnew_anchors ← {the closest sample x to the centroid of clusteri | ∀clusteri ∈ new_clusters} 8: Xanchors ← {x ∈ Danc} ∪Xnew_anchors 9: wanc ← Concat(wanc, 0⊤ |Xnew_anchors|) ▷ Initialize new anchor weights. 10: for wanc i ∈ wanc, wanc i ← wanc i + # sample of clusterj # anchor in clusterj , wanc i ∈ clusterj ▷ Weight accumulation. 11: Return Xanchors 12: end function where the minimum value can be obtained with condition w0 = λ0 ̸= 0. In practical learning scenarios, we generally assume adaptation tasks are solvable; therefore, there should be a prediction function that performs well on two distinct domains. In this case, γ and γ′ should be relatively small, so we can assume γ ≈ γ′. If ˆdH∆H(S0, SS) < ˆdH∆H(SS, ST ), then we have EBS(w, λ, N, t)min = w0( ˆdH∆H(S0, SS) + 4 s 2d log(2m) + log 2 δ m + 2γ) + (1 − w0)( ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′) + B < ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′ + B = EBS(w′, λ′, N, t). Therefore, we derive EBS(w, λ, N, t)min < EBS(w′, λ′, N, t). (32) This completes the proof. F I NCREMENTAL CLUSTERING F.1 A LGORITHM DETAILS We provide the detailed algorithm for incremental clustering as Alg. 2. F.2 V ISUALIZATION To better illustrate the incremental clustering algorithm, we provide visualization results on PACS to demonstrate the process. As shown in Fig. 3, the initial step of IC is a normal K-Means clustering step, and ten anchors denoted as \"X\" are selected. The weights of all samples in a clusters is aggregated into the corresponding anchor’s weight. Therefore, these ten samples (anchors) are given larger sizes visually (i.e., larger weights) than that of other new test samples in the first IC step (Fig. 4). During the first IC step, several distributions are far away from the existed anchors and form clusters 1,7,9 and 10, which leads to 4 new selected anchors. While the number of cluster centroid is only increased by 1, 4 of the existing anchors are clustered into the same cluster 8 (purple). Thus IC produces 4 new anchors instead of 1. Similarly, in the second IC step (Fig. 5), the new streaming-in test samples introduce a new distribution; IC produces 3 new clusters (4, 8, and 11) and the corresponding number of anchors to cover them. The number of centroid is only increased by 1, which implies that there are two original-cluster-merging events. More IC step visualization results are provided in Fig. 6 and 7. 34Published as a conference paper at ICLR 2024 Figure 3: Initial IC step: normal clustering. Left: Clustering results. Right: Selecting new anchors. Figure 4: The first IC step. Left: Weighted clustering results. Right: Selecting new anchors. Figure 5: The second IC step. Left: Weighted clustering results. Right: Selecting new anchors. 35Published as a conference paper at ICLR 2024 Figure 6: The third IC step. Left: Weighted clustering results. Right: Selecting new anchors. Figure 7: The fourth IC step. Left: Weighted clustering results. Right: Selecting new anchors. 36Published as a conference paper at ICLR 2024 G E XPERIMENT DETAILS In this section, we provide more experimental details including the details of the datasets and training settings. G.1 D ETAILS ABOUT THE DATASETS We adopt datasets PACS, VLCS, and Office-Home from DomainBed (Gulrajani and Lopez-Paz, 2020) with the same domain splits. All available licenses are mentioned below. • PACS (Li et al., 2017) includes four domains: art, cartoons, photos, and sketches. PACS is a 7-class classification dataset with 9,991 images of dimension (3, 224, 224). • VLCS (Fang et al., 2013) contains photographic domains: Caltech101, LabelMe, SUN09, and VOC2007. This dataset includes 10,729 images of dimension (3, 224, 224) with 5 classes. • Office-Home (Venkateswara et al., 2017) is a 65-class dataset, including domains: art, clipart, product, and real. VLCS includes 10,729 images of dimension (3, 224, 244). (License) • Tiny-ImageNet-C is a 200-class dataset, including 15 corrupt types. Tiny-ImageNet-C includes 150,000 images of dimension (3, 224, 244). Since the class number 200 is less than ImageNet (1000), the model’s last layer classifier needs to be adapted. In this work, we use the brightness corruption domain to adapt. In the source pretraining phase, we adopt the most ImageNet-like domain as our source domain. For PACS and Office-Home, we use domains \"photos\" and \"real\" as the source domains, respectively, while for VLCS, Caltech101 is assigned to apply the source pretraining. We freeze the random seeds to generate the sample indices order for the two test data streams, namely, the domain-wise data stream and the random data stream. For PACS, the domain-wise data stream inputs samples from domain art, cartoons, to sketches, while we shuffle all samples from these three domains in the random data stream. For VLCS, we stream the domains in the order: LabelMe, SUN09, and VOC2007, as the domain-wise data stream. For Office-Home, the domain-wise data stream order becomes art, clipart, and product. G.2 T RAINING AND OPTIMIZATION SETTINGS In this section, we extensively discuss the model architectures, optimization settings, and method settings. G.2.1 A RCHITECTURES PACS & VLCS. We adopt ResNet-18 as our model encoder followed by a linear classifier. The initial parameters of ResNet-18 are ImageNet pre-trained weights. In our experiment, we remove the Dropout layer since we empirically found that using the Dropout layer might degrade the optimization process when the sample number is small. The specific implementation of the network is closely aligned with the implementation in DomainBed (Gulrajani and Lopez-Paz, 2020). Office-Home. We employ ResNet-50 as our model encoder for Office-Home. Except for the architecture, the other model settings are aligned with the ResNet-18. Tiny-ImageNet-C ResNet-18 is adapted from ImageNet to Tiny-ImageNet-C by training the last linear layer. G.2.2 T RAINING & OPTIMIZATION In this section, we describe the training configurations for both the source domain pre-training and test-time adaptation procedures. Source domain pre-training. For the PACS and VLCS datasets, models are fine-tuned on the selected source domains for 3,000 iterations. The Adam optimizer is utilized with a learning rate 37Published as a conference paper at ICLR 2024 of 10−4. In contrast, for the Office-Home dataset, the model is fine-tuned for a longer duration of 10,000 iterations with a slightly adjusted learning rate of 5 × 10−5. Test-time adaptation. For test-time adaptation across PACS and VLCS, the pre-trained source model is further fine-tuned using the SGD optimizer with a learning rate of 10−3. While on Office-Home and Tiny-ImageNet-C, a learning rate of 10−4 is adopted. For all TTA baselines, barring specific exceptions, we faithfully adhere to the original implementation settings. A noteworthy exception is the EATA method, which requires a cosine similarity threshold. The default threshold of the original EATA implementation was not suitable for the three datasets used in our study, necessitating an adjustment. We empirically set this threshold to 0.5 for training. Unlike Tent and SAR, which only require the optimization of batch normalization layers (Santurkar et al., 2018), SimATTA allows the training of all parameters in the networks. In experiments, we use a tolerance count (tol) to control the training process. SimATTA will stop updating once the loss does not descrease for more than 5 steps. However, for Tiny-ImageNet-C, SimATTA uses ‘steps=10‘ for time comparisons since other methods apply at most 10 steps. G.2.3 M ETHOD SETTINGS Tent. In our experiments, we apply the official implementation of Tent1. Specifically, we evaluate Tent with 1 test-time training step and 10 steps, respectively. EATA.Our EATA implementation follows its official code2. In our experiments, EATA has 2000 fisher training samples, E0 = 0.4 × log(# class), ϵ <0.5. CoTTA. For CoTTA, we strictly follow all the code and settings from its official implementation3. SAR. With SAR’s official implementation4, we set E0 = 0 .4 × log(# class) and e0 = 0 .1 in our experiments. ADA baselines. For ADA baselines, we follow the architecture of the official implementation of CLUE (Prabhu et al., 2021)5. SimATTA Implementation. Our implementation largely involves straightforward hyperparameter settings. The higher entropy bound eh = 10−2 should exceed the lower entropy bound el, but equal values are acceptable. Empirically, the lower entropy bound el can be set to 10−3 for VLCS and Office-Home, or 10−4 for PACS. The choice of el is largely dependent on the number of source-like samples obtained. A lower el may yield higher-accuracy low-entropy samples, but this could lead to unstable training due to sample scarcity. Though experimentation with different hyperparameters is encouraged, our findings suggest that maintaining a non-trivial number of low-entropy samples and setting an appropriateλ0 are of primary importance. If λ0 < 0.5, CF may ensue, which may negate any potential improvement. Regarding the management of budgets, numerous strategies can be adopted. In our experiments, we utilized a simple hyperparameter k, varying from 1 to 3, to regulate the increasing rate of budget consumption. This strategy is fairly elementary and can be substituted by any adaptive techniques. G.3 S OFTWARE AND HARDWARE We conduct our experiments with PyTorch (Paszke et al., 2019) and scikit-learn (Pedregosa et al., 2011) on Ubuntu 20.04. The Ubuntu server includes 112 Intel(R) Xeon(R) Gold 6258R CPU @2.70GHz, 1.47TB memory, and NVIDIA A100 80GB PCIe graphics cards. The training process costs graphics memory less than 10GB, and it requires CPU computational resources for scikit-learn K-Means clustering calculations. Our implementation also includes a GPU-based PyTorch K-Means method for transferring calculation loads from CPUs to GPUs. However, for consistency, the results of our experiments are obtained with the original scikit-learn K-Means implementation. 1https://github.com/DequanWang/tent 2https://github.com/mr-eggplant/EATA 3https://github.com/qinenergy/cotta 4https://github.com/mr-eggplant/SAR 5https://github.com/virajprabhu/CLUE 38Published as a conference paper at ICLR 2024 Figure 8: Target loss surface on 2000 samples without source pre-training. The red points denote the loss minimum for a fixed λ0. The orange line denote the place where w0 = λ0. Figure 9: Target loss surface on 2000 samples with source pre-training. H E MPIRICAL VALIDATIONS FOR THEORETICAL ANALYSIS In this section, we undertake empirical validation of our learning theory, which encompasses multiple facets awaiting verification. In contemporary computer vision fields, pre-trained models play a pivotal role, and performance would significantly decline without the use of pre-trained features. The learning theory suggests that given the vast VC-dimension of complete ResNets, without substantial data samples, the training error cannot be theoretically tight-bounded. However, we show empirically in the following experiments that fine-tuning pre-trained models is behaviorally akin to training a model with a low VC-dimension. Training on 2000 Samples Without Source Domain Pre-training. For an ImageNet pre-trained ResNet-18 model, we trained it using 2000 samples from the PACS dataset. To ascertain the optimal value w∗ 0 in Equation 4, we trained multiple models for different w0 and λ0 pairings. For each pair, we derived the target domain loss (from art, cartoons, and sketches) post-training and plotted this loss on the z-axis. With w0 and λ0 serving as the xy-axes, we drafted the target domain loss ϵT surface in Figure 8. As the results show, given a λ0, the optimal w∗ 0 typically aligns with the line λ0 = w0, with a slight downward shift, which aligns with Equation 4. 39Published as a conference paper at ICLR 2024 Figure 10: Target loss surface on 500 samples with source pre-training. Figure 11: Source loss surface on 500 samples with source pre-training. 40Published as a conference paper at ICLR 2024 Figure 12: Target and source loss surface on 500 samples with source pre-training. Table 6: TTA comparisons on Office-Home. This table includes the two data stream settings mentioned in the dataset setup and reports performances in accuracy. Results that outperform all TTA baselines are highlighted in bold font. N/A denotes the adaptations are not applied on the source domain. Office-Home Domain-wise data stream Post-adaptation Random data stream Post-adaptation R →A→ →C→ →P R A C P 1 2 3 4 R A C P BN w/o adapt 93.78 42.93 37.62 59.90 93.78 42.93 37.62 59.90 46.82 46.82 46.82 46.82 93.78 42.93 37.62 59.90BN w/ adapt 92.38 49.69 39.43 63.53 92.38 49.69 39.43 63.53 50.88 50.88 50.88 50.88 92.38 49.69 39.43 63.53 Tent (steps=1) N/A 49.61 39.31 63.87 92.47 49.57 39.89 63.89 49.95 50.27 50.23 52.06 92.40 49.24 39.68 63.98Tent (steps=10) N/A 49.61 39.04 61.41 87.08 44.79 38.37 60.49 50.05 49.31 48.74 47.79 85.31 42.85 37.89 58.71EATA N/A 49.65 39.04 63.53 91.60 49.61 38.65 63.48 49.73 50.27 49.45 51.07 91.05 49.11 38.26 62.99CoTTA N/A 49.61 38.76 61.84 87.81 44.95 35.92 59.04 49.84 49.84 48.95 50.43 86.99 43.68 34.73 57.56SAR (steps=1) N/A 49.65 39.24 63.53 92.45 49.73 39.36 63.69 49.84 50.05 49.91 51.67 92.38 49.57 39.50 63.87SAR (steps=10) N/A 49.53 38.81 61.50 88.94 46.15 37.04 59.41 50.09 50.30 49.77 49.22 89.14 46.23 36.31 59.45 SimATTA (B ≤300) N/A 56.20 48.38 71.66 95.75 60.07 52.62 74.70 58.57 60.88 62.91 63.67 95.89 62.01 54.98 74.70SimATTA (B ≤500) N/A 58.71 51.11 74.36 96.03 62.05 57.41 76.98 58.85 62.63 63.41 64.31 95.91 63.78 57.87 77.09 Training on 2000 Samples with Source Domain Pre-training. To further assess the effects of source pre-training, we repeated the same experiment on a source pre-trained ResNet-18. The results are depicted in Figure 9. This experiment provides empirical guidance on selecting w0 in source domain pre-trained situations. The findings suggest that the optimal w∗ 0 non-trivially shifts away from the line λ0 = w0 towards lower-value regions. Considering the source pre-training process as using a greater quantity of source domain samples, it implies that when the number of source samples greatly exceeds target samples, a lower w0 can enhance target domain results. Training on 500 Samples with Source Domain Pre-training. We proceed to fine-tune the source domain pre-trained ResNet-18 using only 500 samples, thereby simulating active TTA settings. We train models with various w0 and λ0 pairings, then graph the target domain losses, source domain losses, and the combined losses. As shown in Figure 10, the target losses still comply with our theoretical deductions where the local minima are close to the line λ0 = w0 and marginally shift towards lower values. Considering the challenge of CF, the source domain results in Figure 11 suggest a reverse trend compared to the target domain, where lower λ0 and w0 values yield superior target domain results but inferior source domain results. Thus, to curb CF, the primary strategy is to maintain a relatively higher λ0. When considering both target and source domains, a balance emerges as depicted in Figure 12. The global minimum is located in the middle region, demonstrating the trade-off between the target domain and source domain performance. I A DDITIONAL EXPERIMENT RESULTS In this section, we provide additional experiment results. The Office-Home results and ablation studies will be presented in a similar way as the main paper. In the full results Sec. I.3, we will post more detailed experimental results with specific budget numbers and intermediate performance during the test-time adaptation. 41Published as a conference paper at ICLR 2024 Table 7: Comparisons to ADA baselines on Office-Home. The source domain is denoted as \"(S)\" in the table. Results are average accuracies with standard deviations). Office-Home R (S) A C P Random (B = 300) 95.04 (0.20) 57.54 (1.16) 53.43 (1.17) 73.46 (0.97) Entropy (B = 300) 94.39 (0.49) 61.21 (0.71) 56.53 (0.71) 72.31 (0.28) Kmeans (B = 300) 95.09 (0.14) 57.37 (0.90) 51.74 (1.34) 71.81 (0.39) CLUE (B = 300) 95.20 (0.23) 60.18 (0.98) 58.05 (0.43) 73.72 (0.70) Ours (B ≤300) 95.82 (0.07) 61.04 (0.97) 53.80 (1.18) 74.70 (0.00) I.1 R ESULTS ON OFFICE -HOME We conduct experiments on Office-Home and get the test-time performances and post-adaptation performances for two data streams. As shown in Tab. 6, SimATTA can outperform all TTA baselines with huge margins. Compared to ADA baselines under the source-free settings, as shown in Tab. 7, SimATTA obtains comparable results. I.2 A BLATION STUDIES Figure 13: Ablation study on PACS and VLCS.\"IC=0\" denotes removing incremental clustering (IC) selection. \"LE=0\" denotes removing the low-entropy (LE) sample training. Domain-wise stream and random stream are applied on first and second rows, respectively. The accuracy values are averaged across all splits/domains. In this section, we explore three variations of our method to examine the individual impacts of its components. The first variant replaces the incremental clustering selection with entropy selection, 42Published as a conference paper at ICLR 2024 where only the samples with the highest entropy are chosen. The second variant eliminates low- entropy sample training. The third variation combines the first and second variants. We perform this ablation study on the PACS and VLCS as outlined in Fig. 13. We denote the use of incremental clustering (IC) and low-entropy training (LE) respectively as IC=1 and LE=1. The experiments essentially reveals the effectiveness of incremental clustering and low-entropy- sample training. As we have detailed in Sec. 3.2, these techniques are designed to to select informative samples, increase distribution coverage, and mitigate catastrophic forgetting. These designs appositely serve the ATTA setting where the oracle has costs and the budget is limited. Therefore, their effectiveness is prominent particularly when the budget is small. As the results show, when the budget B ≤100 or B ≤300, removing the components observably impairs performances. When B gets large, more active samples cover a larger distribution; thus the performance gap from random selection and informative selection gets smaller. In the extreme case where B → ∞, all samples are selected and thus the superiority of our meticulously-designed techniques are not manifested. Specifically, our analysis yields several insights. First, SimATTA (LE=1, IC=1) comprehensively outperforms other variants on both datasets, different streams, and different budgets. Second, variants without low-entropy training (LE=0, IC=0/1) easily fail to produce stable results (e.g., domain-wise stream in VLCS). Third, SimATTA’s performance surpasses this variant on PACS’s domain-wise stream clearly especially when the budgets are low. This indicates these variants fail to retrieve the most informative style shift (PACS’s shifts) samples, which implies the advantage of incremental clustering when the budget is tight. In addition, these results show that IC has its unique advantage on domain-wise streams where distributions change abruptly instead of random streams. Therefore, compared to PACS’s domain- wise stream results, the reason for the smaller performance improvement of SimATTA over the variant (LE=1, IC=0) on VLCS’s domain-wise stream is that images in VLCS are all photos that do not include those severe style shifts in PACS (i.e., art, cartoons, and sketches). That is, when the shift is not severe, we don’t need IC to cover very different distributions, and selecting samples using entropy can produce good results. In brief, IC is extraordinary for severe distribution shifts and quick adaptation. It is worth mentioning that low budget comparison is essential to show the informative sample retrieval ability, since as the budget increases, all AL techniques will tend to perform closely. I.3 C OMPLETE EXPERIMENT RESULTS We provide complete experimental results in this section. As shown in Tab. 8, we present the full results for two data streams. The test-time adaptation accuracies are shown in the \"Current domain\" row, while the \"Budgets\" row denotes the used budget by the end of the domain. The rest four rows denote the four domain test results by the end of the real-time adaptation of the current domain, where the first column results are the test accuracy before the test-time adaptation phase. N/A represents \"do not apply\". Table 8: Tent (steps=1) on PACS. Tent (steps=1) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 67.29 64.59 44.67 56.35 54.09 51.83 48.58 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.68 98.38 97.60 98.56 98.08 97.72 97.19 A 59.38 69.09 68.95 66.85 68.07 67.33 65.58 63.53 C 28.03 64.04 65.19 64.08 64.85 65.19 62.97 60.75 S 42.91 53.65 47.39 42.58 54.57 49.83 44.13 41.56 J C HALLENGES AND PERSPECTIVES Despite advancements, test-time adaptation continues to pose considerable challenges. As previously discussed, without supplementary information and assumptions, the ability to guarantee model generalization capabilities is limited. However, this is not unexpected given that recent progress 43Published as a conference paper at ICLR 2024 Table 9: Tent (steps=10) on PACS. Tent (steps=10) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 67.38 57.85 20.23 47.36 31.01 22.84 20.33 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 95.45 87.43 62.63 93.83 81.32 65.39 50.78 A 59.38 64.94 55.03 34.52 55.32 40.28 28.27 23.68 C 28.03 55.89 56.70 40.57 54.52 39.68 27.22 20.95 S 42.91 36.96 26.27 13.59 32.25 23.16 20.95 19.62 Table 10: EATA on PACS. EATA Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 67.04 64.72 50.27 57.31 56.06 58.17 59.78 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.62 98.50 98.62 98.68 98.62 98.50 98.62 A 59.38 68.90 68.16 66.50 68.65 68.95 69.34 69.63 C 28.03 63.74 65.36 62.46 65.19 66.00 65.57 65.70 S 42.91 54.01 52.89 48.18 55.71 55.64 54.09 54.26 Table 11: CoTTA on PACS. CoTTA Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 65.48 62.12 53.17 56.06 54.33 57.16 57.42 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.68 98.62 98.62 98.62 98.62 98.56 98.62 A 59.38 65.82 65.87 65.48 66.02 65.87 66.31 65.97 C 28.03 62.63 63.05 63.10 63.01 62.88 63.01 62.97 S 42.91 53.88 54.03 53.78 54.67 55.31 55.10 54.62 Table 12: SAR (steps=1) on PACS. SAR (steps=1) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 66.75 63.82 49.58 56.78 56.35 56.68 56.70 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.68 98.50 98.32 98.74 98.56 98.50 98.44 A 59.38 68.02 68.07 66.94 67.87 68.65 68.55 68.16 C 28.03 62.84 64.97 62.93 63.82 64.89 64.46 64.38 S 42.91 53.47 52.07 45.74 54.92 55.46 53.68 52.53 Table 13: SAR (steps=10) on PACS. SAR (steps=10) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 69.38 68.26 49.02 53.51 51.15 51.78 45.60 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.20 95.39 96.47 97.13 97.78 97.72 94.13 A 59.38 72.36 66.60 62.16 62.74 64.94 66.11 56.64 C 28.03 63.44 68.30 56.19 59.77 61.73 62.03 56.02 S 42.91 53.37 44.59 54.62 41.00 49.66 48.79 36.37 44Published as a conference paper at ICLR 2024 Table 14: SimATTA (B ≤300) on PACS. SimATTA (B ≤300) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 76.86 70.90 75.39 69.47 76.49 82.45 82.22 Budgets N/A 75 145 223 66 142 203 267 P 99.70 98.44 98.86 98.80 97.96 98.68 99.04 98.98 A 59.38 80.71 82.32 84.47 73.97 80.52 81.10 84.91 C 28.03 48.12 82.00 82.25 72.35 81.06 83.36 83.92 S 42.91 32.78 56.25 81.52 79.49 83.10 84.78 86.00 Table 15: SimATTA (B ≤500) on PACS. SimATTA (B ≤500) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 77.93 76.02 76.30 68.46 78.22 80.91 85.49 Budgets N/A 121 230 358 102 221 343 425 P 99.70 98.92 98.86 98.62 98.20 99.46 99.10 99.16 A 59.38 87.01 87.60 88.33 73.39 79.20 84.91 86.67 C 28.03 54.78 83.96 83.49 68.43 74.40 84.22 84.77 S 42.91 46.37 63.53 83.74 81.34 81.04 86.66 87.71 Table 16: Tent (steps=1) on VLCS. Tent (steps=1) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 38.55 34.40 53.88 44.85 44.29 47.38 44.98 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 84.81 85.44 84.73 84.95 85.16 85.80 85.30 L 33.55 40.02 43.11 43.86 39.68 41.98 43.11 43.49 S 41.10 33.39 35.41 33.61 36.29 37.90 38.27 37.81 V 49.08 53.20 54.06 53.11 53.76 54.18 53.76 53.35 Table 17: Tent (steps=10) on VLCS. Tent (steps=10) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 45.41 31.44 32.32 46.13 42.31 43.51 39.48 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 73.07 48.34 42.54 74.13 62.19 56.54 52.01 L 33.55 46.61 38.44 37.65 44.88 45.93 43.41 40.32 S 41.10 31.75 28.82 27.79 35.37 36.14 35.28 33.64 V 49.08 48.05 40.14 33.12 50.50 44.49 42.48 40.37 Table 18: EATA on VLCS. EATA Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 37.24 33.15 52.58 43.77 42.48 43.34 41.55 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 85.16 85.02 84.10 84.73 84.52 84.10 83.32 L 33.55 37.16 37.24 37.69 37.09 36.78 36.90 36.67 S 41.10 33.39 33.49 32.39 33.33 32.54 31.84 31.47 V 49.08 51.87 52.16 52.49 52.07 52.43 52.64 52.55 45Published as a conference paper at ICLR 2024 Table 19: CoTTA on VLCS. CoTTA Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 37.39 32.54 52.25 43.69 42.14 43.21 42.32 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 81.55 81.98 82.12 82.61 82.47 82.12 81.98 L 33.55 37.20 37.91 37.65 38.48 38.22 38.40 37.99 S 41.10 30.71 32.78 33.12 34.00 33.70 33.97 33.52 V 49.08 52.01 52.64 52.90 53.64 53.14 53.08 53.23 Table 20: SAR (steps=1) on VLCS. SAR (steps=1) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 36.18 34.43 52.46 43.64 43.04 44.20 41.93 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 84.31 84.17 83.96 85.09 85.23 85.23 85.09 L 33.55 35.62 38.29 39.72 38.55 39.34 40.21 40.70 S 41.10 33.24 36.41 36.53 34.37 35.62 36.29 36.44 V 49.08 51.75 52.61 52.37 52.90 52.75 53.05 53.02 Table 21: SAR (steps=10) on VLCS. SAR (steps=10) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 35.32 34.10 51.66 43.56 42.05 42.53 41.16 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 83.96 83.04 82.12 84.03 84.24 85.23 85.09 L 33.55 34.07 35.92 41.49 39.53 38.37 37.65 37.58 S 41.10 31.93 34.89 33.94 35.19 32.94 33.88 33.12 V 49.08 51.33 51.51 53.08 52.78 52.34 51.78 52.01 Table 22: SimATTA (B ≤300) on VLCS. SimATTA (B ≤300) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 62.61 65.08 74.38 62.33 69.33 73.20 71.93 Budgets N/A 79 175 272 71 135 208 262 C 100.00 99.51 98.52 99.93 99.86 99.79 100.00 99.93 L 33.55 68.11 69.92 69.50 62.61 66.64 68.45 69.43 S 41.10 55.24 68.89 66.67 65.54 69.29 71.79 72.46 V 49.08 66.08 70.94 77.34 73.79 76.87 78.82 80.39 Table 23: SimATTA (B ≤500) on VLCS. SimATTA (B ≤500) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 63.52 68.01 76.13 62.29 70.45 73.50 72.02 Budgets N/A 113 266 446 107 203 283 356 C 100.00 99.29 98.59 99.51 99.93 99.86 99.86 99.43 L 33.55 62.95 70.63 70.56 66.57 67.09 67.24 70.29 S 41.10 51.31 73.83 73.10 65.33 71.79 72.91 72.55 V 49.08 59.36 71.65 78.35 73.58 77.84 80.01 80.18 46Published as a conference paper at ICLR 2024 Table 24: Tent (steps=1) on Office-Home. Tent (steps=1) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.61 39.31 63.87 49.95 50.27 50.23 52.06 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 92.33 92.36 92.47 92.38 92.45 92.45 92.40 A 57.07 49.73 49.73 49.57 49.69 49.73 49.57 49.24 C 44.97 39.27 39.54 39.89 39.45 39.68 39.73 39.68 P 73.15 63.60 63.66 63.89 63.60 63.82 63.93 63.98 Table 25: Tent (steps=10) on Office-Home. Tent (steps=10) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.61 39.04 61.41 50.05 49.31 48.74 47.79 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 91.99 89.14 87.08 92.08 90.80 88.59 85.31 A 57.07 49.94 46.77 44.79 49.44 48.21 45.69 42.85 C 44.97 38.58 39.11 38.37 40.18 40.02 38.63 37.89 P 73.15 63.28 61.03 60.49 64.36 63.64 61.12 58.71 Table 26: EATA on Office-Home. EATA Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.65 39.04 63.53 49.73 50.27 49.45 51.07 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 92.36 92.17 91.60 92.38 92.22 91.71 91.05 A 57.07 49.57 49.53 49.61 49.69 49.40 49.36 49.11 C 44.97 39.08 39.01 38.65 39.27 39.01 38.42 38.26 P 73.15 63.42 63.42 63.48 63.51 63.37 63.33 62.99 Table 27: CoTTA on Office-Home. CoTTA Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.61 38.76 61.84 49.84 49.84 48.95 50.43 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 90.38 88.02 87.81 90.48 89.37 88.00 86.99 A 57.07 48.58 45.53 44.95 47.34 46.35 44.62 43.68 C 44.97 36.66 35.58 35.92 37.55 36.40 35.44 34.73 P 73.15 60.40 57.74 59.04 61.12 59.63 58.35 57.56 Table 28: SAR (steps=1) on Office-Home. SAR (steps=1) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.65 39.24 63.53 49.84 50.05 49.91 51.67 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 92.38 92.31 92.45 92.40 92.36 92.36 92.38 A 57.07 49.65 49.57 49.73 49.69 49.61 49.57 49.57 C 44.97 39.34 39.22 39.36 39.34 39.56 39.47 39.50 P 73.15 63.51 63.51 63.69 63.60 63.71 63.71 63.87 47Published as a conference paper at ICLR 2024 Table 29: SAR (steps=10) on Office-Home. SAR (steps=10) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.53 38.81 61.50 50.09 50.30 49.77 49.22 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 92.20 92.06 88.94 92.40 92.47 91.53 89.14 A 57.07 49.40 49.77 46.15 49.81 50.02 48.91 46.23 C 44.97 39.20 38.63 37.04 39.50 39.29 38.65 36.31 P 73.15 63.53 62.69 59.41 64.18 64.18 62.83 59.45 Table 30: SimATTA (B ≤300) on Office-Home. SimATTA (B ≤300) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 56.20 48.38 71.66 58.57 60.88 62.91 63.67 Budgets N/A 75 187 277 79 147 216 278 R 96.44 95.43 95.43 95.75 95.91 95.96 96.01 95.89 A 57.07 57.56 59.50 60.07 58.34 59.91 61.15 62.01 C 44.97 42.25 52.46 52.62 51.66 52.30 54.75 54.98 P 73.15 68.84 70.13 74.70 72.45 73.10 74.50 74.70 Table 31: SimATTA (B ≤500) on Office-Home. SimATTA (B ≤500) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 58.71 51.11 74.36 58.85 62.63 63.41 64.31 Budgets N/A 107 284 440 126 248 361 467 R 96.44 95.69 95.71 96.03 96.26 96.19 95.87 95.91 A 57.07 61.43 61.43 62.05 58.18 61.15 61.52 63.78 C 44.97 46.41 57.73 57.41 53.17 55.14 56.79 57.87 P 73.15 70.74 71.98 76.98 73.51 74.18 75.78 77.09 48Published as a conference paper at ICLR 2024 in deep learning heavily relies on large-scale data. Consequently, two promising paths emerge: establishing credible assumptions and leveraging additional information. Firstly, developing credible assumptions can lead to comprehensive comparisons across various stud- ies. Given that theoretical guarantees highlight the inherent differences between methods primarily based on the application limits of their assumptions, comparing these assumptions becomes critical. Without such comparative studies, empirical evaluations may lack precise guidance and explanation. Secondly, while we acknowledge the value of real-world data (observations), discussions surrounding the use of extra information remain pertinent. Considerations include the strategies to acquire this supplementary information and the nature of the additional data needed. Despite the myriad of works on domain generalization, domain adaptation, and test-time adaptation, a comprehensive survey or benchmark encapsulating the aforementioned comparisons remains an unmet need. Moreover, potential future directions for out-of-distribution generalization extend beyond domain generalization and test-time adaptation. One promising avenue is bridging the gap between causal inference and deep learning, for instance, through causal representation learning. In conclusion, our hope is that this work not only offers a novel practical setting and algorithm but also illuminates meaningful future directions and research methodologies that can benefit the broader scientific community. 49",
      "meta_data": {
        "arxiv_id": "2404.05094v1",
        "authors": [
          "Shurui Gui",
          "Xiner Li",
          "Shuiwang Ji"
        ],
        "published_date": "2024-04-07T22:31:34Z",
        "pdf_url": "https://arxiv.org/pdf/2404.05094v1.pdf",
        "github_url": "https://github.com/divelab/ATTA"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Active Test-Time Adaptation (ATTA), a novel problem setting that integrates active learning into Fully Test-Time Adaptation (FTTA) to address significant distribution shifts for streaming test data in unsupervised settings. Key contributions include a learning theory analysis demonstrating performance enhancement with limited labeled test instances, a sample entropy balancing technique to avoid catastrophic forgetting (CF), and the proposed simple yet effective algorithm called SimATTA, which uses real-time sample selection. Experimental results show SimATTA substantially outperforms existing TTA methods while maintaining efficiency and achieving comparable effectiveness to more demanding Active Domain Adaptation (ADA) methods.",
        "methodology": "The ATTA framework formally defines how a pre-trained model continuously selects informative instances from test batches for labeling and subsequent learning under a labeling budget. SimATTA, the proposed algorithm, partitions unlabeled test samples into high-entropy (informative, for active labeling) and low-entropy (source-like, for CF mitigation) datasets using an entropy threshold. The source-pretrained model pseudo-labels low-entropy data, which are combined with previously learned low-entropy data. High-entropy samples are selected for active labeling using an incremental clustering technique (weighted K-means) to reduce redundancy and increase distribution coverage by storing representative samples as 'anchors'. The model is then fine-tuned using these labeled high-entropy anchors and pseudo-labeled low-entropy data, with training weights balanced to mitigate CF.",
        "experimental_setup": "The effectiveness of ATTA was validated on several datasets: PACS, VLCS, Office-Home, and Tiny-ImageNet-C. Models were pre-trained on ImageNet-like domains within these datasets (e.g., 'photos' for PACS, 'Caltech101' for VLCS). Two data stream order strategies were used: domain-wise (sequential domains) and random (shuffled target domains). Baselines included common source-only models (BN w/o adapt, BN w/ adapt), state-of-the-art TTA methods (Tent, EATA, CoTTA, SAR), and Active Domain Adaptation (ADA) methods (Random, Entropy, K-means, CLUE) for comparison. Performance was measured by accuracy, and time efficiency was also assessed. Ablation studies explored the impact of incremental clustering and low-entropy sample training.",
        "limitations": "The theoretical bounds for ATTA can be loose when the number of unlabeled test samples (m) in a batch is very small, although fine-tuning pre-trained models is empirically shown to reduce the effective VC-dimension. The selective entropy minimization strategy relies on the quality of the pre-trained model, and training on incorrectly pseudo-labeled low-entropy samples may reinforce errors. It may also not be cost-effective to expend annotation budgets on low-entropy samples. The paper's scope is foundational, thus it does not cover all potential applications, scaling to very large models or datasets, or task-specific configurations in detail. The framework does not cover situations where the support of labels (Y) changes, i.e., class-incremental problems.",
        "future_research_directions": "Future research can explore developing alternative strategies to prevent catastrophic forgetting (CF) in ATTA scenarios, particularly investigating methods beyond selective entropy minimization on low-entropy samples, such as correcting incorrectly predicted low-entropy samples. Another promising direction involves designing and applying ATTA methods for Large Language Models (LLMs), where traditional re-training is computationally expensive and source data may be inaccessible. The authors also suggest bridging the gap between causal inference and deep learning, for example, through causal representation learning, as a broader avenue for out-of-distribution generalization.",
        "experimental_code": "import copy\nimport pathlib\nimport time\nfrom typing import Union\n\nimport numpy as np\n# from sklearnex import patch_sklearn, config_context\n# patch_sklearn()\n\n# from sklearn.cluster import KMeans\n# from ATTA.utils.fast_pytorch_kmeans import KMeans\nfrom sklearn.metrics import pairwise_distances_argmin_min\nfrom typing import Literal\n\nfrom torch import nn\nimport torch\n# import models for resnet18\nfrom munch import Munch\nfrom ATTA import register\nfrom ATTA.utils.config_reader import Conf\nfrom ATTA.data.loaders.fast_data_loader import InfiniteDataLoader, FastDataLoader\nfrom torch.utils.data import TensorDataset\nfrom tqdm import tqdm\nfrom .Base import AlgBase\nimport pandas as pd\nfrom ATTA.definitions import STORAGE_DIR\n\n\n\n@register.alg_register\nclass SimATTA(AlgBase):\n    def __init__(self, config: Conf):\n        super(SimATTA, self).__init__(config)\n\n        self.teacher = copy.deepcopy(self.model.to('cpu'))\n\n        self.model.to(config.device)\n        self.teacher.to(config.device)\n        self.update_teacher(0)  # copy student to teacher\n\n        self.budgets = 0\n        self.anchors = None\n        self.source_anchors = None\n        self.buffer = []\n        self.n_clusters = 10\n        self.nc_increase = self.config.atta.SimATTA.nc_increase\n        self.source_n_clusters = 100\n\n        self.cold_start = self.config.atta.SimATTA.cold_start\n\n        self.consistency_weight = 0\n        self.alpha_teacher = 0\n        self.accumulate_weight = True\n        self.weighted_entropy: Union[Literal['low', 'high', 'both'], None] = 'both'\n        self.aggressive = True\n        self.beta = self.config.atta.SimATTA.beta\n        self.alpha = 0.2\n\n        self.target_cluster = True if self.config.atta.SimATTA.target_cluster else False\n        self.LE = True if self.config.atta.SimATTA.LE else False\n        self.vis_round = 0\n\n\n    def __call__(self, *args, **kwargs):\n        # super(SimATTA, self).__call__()\n        self.continue_result_df = pd.DataFrame(\n            index=['Current domain', 'Budgets', *(i for i in self.config.dataset.test_envs), 'Frame AVG'],\n            columns=[*(i for i in self.config.dataset.test_envs), 'Test AVG'], dtype=float)\n        self.random_result_df = pd.DataFrame(\n            index=['Current step', 'Budgets', *(i for i in self.config.dataset.test_envs), 'Frame AVG'],\n            columns=[*(i for i in range(4)), 'Test AVG'], dtype=float)\n\n        self.enable_bn(self.model)\n        if 'ImageNet' not in self.config.dataset.name:\n            for env_id in self.config.dataset.test_envs:\n                acc = self.test_on_env(env_id)[1]\n                self.continue_result_df.loc[env_id, self.config.dataset.test_envs[0]] = acc\n                self.random_result_df.loc[env_id, self.config.dataset.test_envs[0]] = acc\n\n        for adapt_id in self.config.dataset.test_envs[1:]:\n            self.continue_result_df.loc['Current domain', adapt_id] = self.adapt_on_env(self.fast_loader, adapt_id)\n            self.continue_result_df.loc['Budgets', adapt_id] = self.budgets\n            print(self.budgets)\n            if 'ImageNet' not in self.config.dataset.name:\n                for env_id in self.config.dataset.test_envs:\n                    self.continue_result_df.loc[env_id, adapt_id] = self.test_on_env(env_id)[1]\n\n        self.__init__(self.config)\n        for target_split_id in range(4):\n            self.random_result_df.loc['Current step', target_split_id] = self.adapt_on_env(self.target_loader, target_split_id)\n            self.random_result_df.loc['Budgets', target_split_id] = self.budgets\n            print(self.budgets)\n            if 'ImageNet' not in self.config.dataset.name:\n                for env_id in self.config.dataset.test_envs:\n                    self.random_result_df.loc[env_id, target_split_id] = self.test_on_env(env_id)[1]\n\n        print(f'#IM#\\n{self.continue_result_df.round(4).to_markdown()}\\n'\n              f'{self.random_result_df.round(4).to_markdown()}')\n        # print(self.random_result_df.round(4).to_markdown(), '\\n')\n        self.continue_result_df.round(4).to_csv(f'{self.config.log_file}.csv')\n        self.random_result_df.round(4).to_csv(f'{self.config.log_file}.csv', mode='a')\n\n\n    @torch.no_grad()\n    def val_anchor(self, loader):\n        self.model.eval()\n        val_loss = 0\n        val_acc = 0\n        for data, target in loader:\n            data, target = data.to(self.config.device), target.to(self.config.device)\n            output = self.fc(self.encoder(data))\n            val_loss += self.config.metric.loss_func(output, target, reduction='sum').item()\n            val_acc += self.config.metric.score_func(target, output) * len(data)\n        val_loss /= len(loader.sampler)\n        val_acc /= len(loader.sampler)\n        return val_loss, val_acc\n\n    def update_teacher(self, alpha_teacher):  # , iteration):\n        for t_param, s_param in zip(self.teacher.parameters(), self.model.parameters()):\n            t_param.data[:] = alpha_teacher * t_param[:].data[:] + (1 - alpha_teacher) * s_param[:].data[:]\n        if not self.config.model.freeze_bn:\n            for tm, m in zip(self.teacher.modules(), self.model.modules()):\n                if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n                    tm.running_mean = alpha_teacher * tm.running_mean + (1 - alpha_teacher) * m.running_mean\n                    tm.running_var = alpha_teacher * tm.running_var + (1 - alpha_teacher) * m.running_var\n\n    @torch.enable_grad()\n    def cluster_train(self, target_anchors, source_anchors):\n        self.model.train()\n\n        source_loader = InfiniteDataLoader(TensorDataset(source_anchors.data, source_anchors.target), weights=None,\n                                           batch_size=self.config.train.train_bs,\n                                           num_workers=self.config.num_workers)\n        target_loader = InfiniteDataLoader(TensorDataset(target_anchors.data, target_anchors.target), weights=None,\n                                             batch_size=self.config.train.train_bs, num_workers=self.config.num_workers)\n        alpha = target_anchors.num_elem() / (target_anchors.num_elem() + source_anchors.num_elem())\n        if source_anchors.num_elem() < self.cold_start:\n            alpha = min(0.2, alpha)\n\n        ST_loader = iter(zip(source_loader, target_loader))\n        val_loader = FastDataLoader(TensorDataset(target_anchors.data, target_anchors.target), weights=None,\n                                    batch_size=self.config.train.train_bs, num_workers=self.config.num_workers)\n        optimizer = torch.optim.SGD(self.model.parameters(), lr=self.config.atta.SimATTA.lr, momentum=0.9)\n        # print('Cluster train')\n        delay_break = False\n        loss_window = []\n        tol = 0\n        lowest_loss = float('inf')\n        for i, ((S_data, S_targets), (T_data, T_targets)) in enumerate(ST_loader):\n            S_data, S_targets = S_data.to(self.config.device), S_targets.to(self.config.device)\n            T_data, T_targets = T_data.to(self.config.device), T_targets.to(self.config.device)\n            L_T = self.one_step_train(S_data, S_targets, T_data, T_targets, alpha, optimizer)\n            # self.update_teacher(self.alpha_teacher)\n            if len(loss_window) < self.config.atta.SimATTA.stop_tol:\n                loss_window.append(L_T.item())\n            else:\n                mean_loss = np.mean(loss_window)\n                tol += 1\n                if mean_loss < lowest_loss:\n                    lowest_loss = mean_loss\n                    tol = 0\n                if tol > 5:\n                    break\n                loss_window = []\n            if 'ImageNet' in self.config.dataset.name or 'CIFAR' in self.config.dataset.name:\n                if i > self.config.atta.SimATTA.steps:\n                    break\n\n\n    def one_step_train(self, S_data, S_targets, T_data, T_targets, alpha, optimizer):\n        # print('one step train')\n        L_S = self.config.metric.loss_func(self.model(S_data), S_targets)\n        L_T = self.config.metric.loss_func(self.model(T_data), T_targets)\n        loss = (1 - alpha) * L_S + alpha * L_T\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        return L_T\n\n    def softmax_entropy(self, x: torch.Tensor, y: torch.Tensor = None) -> torch.Tensor:\n        \"\"\"Entropy of softmax distribution from logits.\"\"\"\n        if y is None:\n            if x.shape[1] == 1:\n                x = torch.cat([x, -x], dim=1)\n            return -(x.softmax(1) * x.log_softmax(1)).sum(1)\n        else:\n            return - 0.5 * (x.softmax(1) * y.log_softmax(1)).sum(1) - 0.5 * (y.softmax(1) * x.log_softmax(1)).sum(1)\n\n    def update_anchors(self, anchors, data, target, feats, weight):\n        if anchors is None:\n            anchors = Munch()\n            anchors.data = data\n            anchors.target = target\n            anchors.feats = feats\n            anchors.weight = weight\n            anchors.num_elem = lambda: len(anchors.data)\n        else:\n            anchors.data = torch.cat([anchors.data, data])\n            anchors.target = torch.cat([anchors.target, target])\n            anchors.feats = torch.cat([anchors.feats, feats])\n            anchors.weight = torch.cat([anchors.weight, weight])\n        return anchors\n\n    def update_anchors_feats(self, anchors):\n        # sequential_data = torch.arange(200)[:, None]\n        anchors_loader = FastDataLoader(TensorDataset(anchors.data), weights=None,\n                                        batch_size=32, num_workers=self.config.num_workers, sequential=True)\n\n        anchors.feats = None\n        self.model.eval()\n        for data in anchors_loader:\n            # print(data)\n            data = data[0].to(self.config.device)\n            if anchors.feats is None:\n                anchors.feats = self.model[0](data).cpu().detach()\n            else:\n                anchors.feats = torch.cat([anchors.feats, self.model[0](data).cpu().detach()])\n\n        return anchors\n\n    @torch.no_grad()\n    def adapt_on_env(self, loader, env_id):\n        # beta_func = torch.distributions.beta.Beta(0.8, 0.8)\n        acc = 0\n        for data, target in tqdm(loader[env_id]):\n            data, target = data.to(self.config.device), target.to(self.config.device)\n            outputs, closest, self.anchors = self.sample_select(self.model, data, target, self.anchors, int(self.n_clusters), 1, ent_bound=self.config.atta.SimATTA.eh, incremental_cluster=self.target_cluster)\n            acc += self.config.metric.score_func(target, outputs).item() * data.shape[0]\n            if self.LE:\n                _, _, self.source_anchors = self.sample_select(self.teacher, data, target, self.source_anchors, self.source_n_clusters, 0,\n                                                               use_pseudo_label=True, ent_bound=self.config.atta.SimATTA.el, incremental_cluster=False)\n            else:\n                self.source_anchors = self.update_anchors(None, torch.tensor([]), None, None, None)\n            if not self.target_cluster:\n                self.n_clusters = 0\n            self.source_n_clusters = 100\n\n            self.budgets += len(closest)\n            self.n_clusters += self.nc_increase\n            self.source_n_clusters += 1\n\n            print(self.anchors.num_elem(), self.source_anchors.num_elem())\n            if self.source_anchors.num_elem() > 0:\n                self.cluster_train(self.anchors, self.source_anchors)\n            else:\n                self.cluster_train(self.anchors, self.anchors)\n            self.anchors = self.update_anchors_feats(self.anchors)\n        acc /= len(loader[env_id].sampler)\n        print(f'#IN#Env {env_id} real-time Acc.: {acc:.4f}')\n        return acc\n\n    @torch.no_grad()\n    def sample_select(self, model, data, target, anchors, n_clusters, ent_beta, use_pseudo_label=False, ent_bound=1e-2, incremental_cluster=False):\n        model.eval()\n        feats = model[0](data)\n        outputs = model[1](feats)\n        pseudo_label = outputs.argmax(1).cpu().detach()\n        data = data.cpu().detach()\n        feats = feats.cpu().detach()\n        target = target.cpu().detach()\n        entropy = self.softmax_entropy(outputs).cpu()\n        if not incremental_cluster:\n            entropy = entropy.numpy()\n            if ent_beta == 0:\n                closest = np.argsort(entropy)[: n_clusters]\n                closest = closest[entropy[closest] < ent_bound]\n            elif ent_beta == 1:\n                closest = np.argsort(entropy)[- n_clusters:]\n                closest = closest[entropy[closest] >= ent_bound]\n            else:\n                raise NotImplementedError\n            weights = torch.zeros(len(closest), dtype=torch.float)\n        else:\n            if ent_beta == 0:\n                sample_choice = entropy < ent_bound\n            elif ent_beta == 1:\n                sample_choice = entropy >= ent_bound\n            else:\n                raise NotImplementedError\n\n            data = data[sample_choice]\n            target = target[sample_choice]\n            feats = feats[sample_choice]\n            pseudo_label = pseudo_label[sample_choice]\n\n            if anchors:\n                feats4cluster = torch.cat([anchors.feats, feats])\n                sample_weight = torch.cat([anchors.weight, torch.ones(len(feats), dtype=torch.float)])\n            else:\n                feats4cluster = feats\n                sample_weight = torch.ones(len(feats), dtype=torch.float)\n\n            if self.config.atta.gpu_clustering:\n                from ATTA.utils.fast_pytorch_kmeans import KMeans\n                from joblib import parallel_backend\n                kmeans = KMeans(n_clusters=n_clusters, n_init=10, device=self.config.device).fit(\n                    feats4cluster.to(self.config.device),\n                    sample_weight=sample_weight.to(self.config.device))\n                with parallel_backend('threading', n_jobs=8):\n                    raw_closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, feats4cluster)\n                kmeans_labels = kmeans.labels_\n            # elif self.config.atta.gpu_clustering == 'jax':\n            #     from ott.tools.k_means import k_means as KMeans\n            #     import jax\n            #     import jax.numpy as jnp\n            #     tik = time.time()\n            #     kmeans = KMeans(jnp.array(feats4cluster.numpy()), k=n_clusters, weights=jnp.array(sample_weight.numpy()), n_init=10)\n            #     mit = time.time()\n            #     print(f'#IN#Kmeans time: {mit - tik}')\n            #     @jax.jit\n            #     def jax_pairwise_distances_argmin(c, feats):\n            #         dis = lambda x, y: jnp.sqrt(((x - y) ** 2).sum())\n            #         argmin_dis = lambda x, y: jnp.argmin(jax.vmap(dis, in_axes=(None, 0))(x, y))\n            #         return jax.vmap(argmin_dis, in_axes=(0, None))(c, feats)\n            #     raw_closest = np.array(jax_pairwise_distances_argmin(kmeans.centroids, jnp.array(feats4cluster.numpy())))\n            #     print(f'#IN#Pairwise distance time: {time.time() - mit}')\n            #     kmeans_labels = np.array(kmeans.assignment)\n            else:\n                from joblib import parallel_backend\n                from sklearn.cluster import KMeans\n                with parallel_backend('threading', n_jobs=8):\n                    kmeans = KMeans(n_clusters=n_clusters, n_init=10, algorithm='elkan').fit(feats4cluster,\n                                                                                                  sample_weight=sample_weight)\n                    raw_closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, feats4cluster)\n                kmeans_labels = kmeans.labels_\n\n\n\n            if anchors:\n                num_anchors = anchors.num_elem()\n                prev_anchor_cluster = torch.tensor(kmeans_labels[:num_anchors], dtype=torch.long)\n\n                if self.accumulate_weight:\n                    # previous anchor weight accumulation\n                    # Average the weight of the previous anchor if sharing the same cluster\n                    num_prev_anchors_per_cluster = prev_anchor_cluster.unique(return_counts=True)\n                    num_prev_anchors_per_cluster_dict = torch.zeros(len(raw_closest), dtype=torch.long)\n                    num_prev_anchors_per_cluster_dict[num_prev_anchors_per_cluster[0].long()] = \\\n                    num_prev_anchors_per_cluster[1]\n\n                    num_newsample_per_cluster = torch.tensor(kmeans_labels).unique(return_counts=True)\n                    num_newsample_per_cluster_dict = torch.zeros(len(raw_closest), dtype=torch.long)\n                    num_newsample_per_cluster_dict[num_newsample_per_cluster[0].long()] = num_newsample_per_cluster[1]\n                    assert (num_prev_anchors_per_cluster_dict[prev_anchor_cluster] == 0).sum() == 0\n                    # accumulate the weight of the previous anchor\n                    anchors.weight = anchors.weight + num_newsample_per_cluster_dict[prev_anchor_cluster] / \\\n                                          num_prev_anchors_per_cluster_dict[prev_anchor_cluster].float()\n\n                anchored_cluster_mask = torch.zeros(len(raw_closest), dtype=torch.bool).index_fill_(0,\n                                                                                                    prev_anchor_cluster.unique().long(),\n                                                                                                    True)\n                new_cluster_mask = ~ anchored_cluster_mask\n\n                closest = raw_closest[new_cluster_mask] - num_anchors\n                if (closest < 0).sum() != 0:\n                    # The cluster's closest sample may not belong to the cluster. It makes sense to eliminate them.\n                    print('new_cluster_mask: ', new_cluster_mask)\n                    new_cluster_mask = torch.where(new_cluster_mask)[0]\n                    print('new_cluster_mask: ', new_cluster_mask)\n                    print(closest)\n                    print(closest >= 0)\n                    new_cluster_mask = new_cluster_mask[closest >= 0]\n                    closest = closest[closest >= 0]\n\n\n                weights = torch.tensor(kmeans_labels).unique(return_counts=True)[1][new_cluster_mask]\n            else:\n                num_anchors = 0\n                closest = raw_closest\n                weights = torch.tensor(kmeans_labels).unique(return_counts=True)[1]\n\n        if use_pseudo_label:\n            anchors = self.update_anchors(anchors, data[closest], pseudo_label[closest], feats[closest], weights)\n        else:\n            anchors = self.update_anchors(anchors, data[closest], target[closest], feats[closest], weights)\n\n        return outputs, closest, anchors\n\n    def enable_bn(self, model):\n        if not self.config.model.freeze_bn:\n            for m in model.modules():\n                if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n                    m.momentum = 0.1",
        "experimental_info": "The SimATTA algorithm partitions unlabeled test samples into high-entropy (informative) and low-entropy (source-like) datasets using entropy thresholds (`eh` for high-entropy, `el` for low-entropy). Low-entropy data is pseudo-labeled by a `teacher` model (a deep copy of the student model, not dynamically updated during adaptation in the provided code). High-entropy samples are selected for active labeling using an incremental clustering technique (weighted K-means if `target_cluster` is enabled), storing representative samples as 'anchors'. The number of clusters for high-entropy samples (`n_clusters`) initially set to 10, increases by `nc_increase` each round. For low-entropy samples (`source_n_clusters`), it starts at 100. The model is then fine-tuned using a balanced loss, combining labeled high-entropy anchors and pseudo-labeled low-entropy data. The weighting factor `alpha` is calculated based on the number of elements in each dataset, with a `cold_start` phase influencing its value for initial steps. Fine-tuning uses `torch.optim.SGD` with a learning rate (`lr`) and runs for a specified number of `steps` or until a `stop_tol` criterion is met. GPU-accelerated K-means clustering can be enabled with `gpu_clustering`.\n\nKey hyperparameters and settings:\n- **Entropy Thresholds:** `eh` (high-entropy), `el` (low-entropy, e.g., 1e-3).\n- **Clustering:** Initial `n_clusters` (high-entropy): 10; Initial `source_n_clusters` (low-entropy): 100. `nc_increase` (e.g., 0.25 to 3) defines the increase in clusters per round. `target_cluster` (boolean, 0 or 1) enables incremental weighted K-means for high-entropy samples. `gpu_clustering` (boolean) enables GPU for K-means.\n- **Low-Entropy Handling:** `LE` (boolean, 0 or 1) enables the use of low-entropy samples with pseudo-labels from the `teacher` model.\n- **Fine-tuning:** Optimizer is `torch.optim.SGD` with learning rate `lr` (from `config.atta.SimATTA.lr`). Training `steps` are configured per batch (from `config.atta.SimATTA.steps`). A `stop_tol` (from `config.atta.SimATTA.stop_tol`) is used for early stopping within `cluster_train`.\n- **Weight Balancing:** `alpha` dynamically balances losses from high-entropy and low-entropy data, influenced by `cold_start` (default 100 steps).\n- **Datasets:** Experiments are launched on datasets such as VLCS."
      }
    },
    {
      "title": "EcoTTA: Memory-Efficient Continual Test-Time Adaptation via Self-Distilled Regularization",
      "abstract": "This paper presents a simple yet effective approach that improves continual\ntest-time adaptation (TTA) in a memory-efficient manner. TTA may primarily be\nconducted on edge devices with limited memory, so reducing memory is crucial\nbut has been overlooked in previous TTA studies. In addition, long-term\nadaptation often leads to catastrophic forgetting and error accumulation, which\nhinders applying TTA in real-world deployments. Our approach consists of two\ncomponents to address these issues. First, we present lightweight meta networks\nthat can adapt the frozen original networks to the target domain. This novel\narchitecture minimizes memory consumption by decreasing the size of\nintermediate activations required for backpropagation. Second, our novel\nself-distilled regularization controls the output of the meta networks not to\ndeviate significantly from the output of the frozen original networks, thereby\npreserving well-trained knowledge from the source domain. Without additional\nmemory, this regularization prevents error accumulation and catastrophic\nforgetting, resulting in stable performance even in long-term test-time\nadaptation. We demonstrate that our simple yet effective strategy outperforms\nother state-of-the-art methods on various benchmarks for image classification\nand semantic segmentation tasks. Notably, our proposed method with ResNet-50\nand WideResNet-40 takes 86% and 80% less memory than the recent\nstate-of-the-art method, CoTTA.",
      "full_text": "EcoTTA: Memory-Efficient Continual Test-time Adaptation via Self-distilled Regularization Junha Song1,2* , Jungsoo Lee 1, In So Kweon 2, Sungha Choi 1† 1Qualcomm AI Research‡, 2KAIST Abstract This paper presents a simple yet effective approach that improves continual test-time adaptation (TTA) in a memory- efficient manner. TTA may primarily be conducted on edge devices with limited memory, so reducing memory is cru- cial but has been overlooked in previous TTA studies. In addition, long-term adaptation often leads to catastrophic forgetting and error accumulation, which hinders apply- ing TTA in real-world deployments. Our approach con- sists of two components to address these issues. First, we present lightweight meta networks that can adapt the frozen original networks to the target domain. This novel archi- tecture minimizes memory consumption by decreasing the size of intermediate activations required for backpropaga- tion. Second, our novel self-distilled regularization controls the output of the meta networks not to deviate significantly from the output of the frozen original networks, thereby preserving well-trained knowledge from the source domain. Without additional memory, this regularization prevents er- ror accumulation and catastrophic forgetting, resulting in stable performance even in long-term test-time adaptation. We demonstrate that our simple yet effective strategy out- performs other state-of-the-art methods on various bench- marks for image classification and semantic segmentation tasks. Notably, our proposed method with ResNet-50 and WideResNet-40 takes 86% and 80% less memory than the recent state-of-the-art method, CoTTA. 1. Introduction Despite recent advances in deep learning [15, 24, 23, 22], deep neural networks often suffer from performance degra- dation when the source and target domains differ signifi- cantly [8, 43, 38]. Among several tasks addressing such domain shifts, test-time adaptation (TTA) has recently re- ceived a significant amount of attention due to its practi- cality and wide applicability especially in on-device set- *Work done during an internship at Qualcomm AI Research. †Corresponding author. ‡ Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc. Memory (MB) CIFAR100-C  Error (%) CIFAR10-C  Error (%) ResNet-50 WideResNet-40 CoTTA SWR&NSP TTT++ Con6nual TENT Single domain TENT EATA CoTTA SWR&NSP TTT++ EATA         NOTE Memory (MB)   0 450 900 1350 1800 Param Ac6va6on 86% 72% 0 100 200 300 400 Param Ac6va6on TENT/EATA  CoTTA  Ours 80% 59% (a) (b) Con6nual TENT Single domain TENT             ResNet-50 WideResNet-40 Ours (K=4) Ours (K=5) ���������� ����������� Figure 1. (a) Memory cost comparison between TTA methods. The size of activations, not the parameters, is the primary mem- ory bottleneck during training. (b) CIFAR-C adaptation perfor- mance. We perform the continual online adaptation on CIFAR-C dataset. The x- and y-axis are the average error of all corruptions and the total memory consumption including the parameters and activations, respectively. Our approach, EcoTTA, achieves the best results while consuming the least amount of memory, where K is the model partition factor used in our method. tings [65, 42, 32, 16]. This task focuses on adapting the model to unlabeled online data from the target domain with- out access to the source data. While existing TTA methods show improved TTA per- formances, minimizing the sizes of memory resources have been relatively under-explored, which is crucial considering the applicability of TTA in on-device settings. For example, several studies [66, 42, 9] update entire model parameters 1 arXiv:2303.01904v4  [cs.CV]  23 May 2023TENT𝐷! EATA𝐷! weight regularization: freeze: update: meta networks (Ours) randomrestoration CoTTA Ours (EcoTTA)𝐷!𝐷!transformmovingaverage bnbnbnconv blockmain networksmain networkssource modelmain networksconv blockconv block teacher modelsource model𝐷!:unlabeledonlinetestdata …bn …bn Figure 2. Architecture for test-time adaptation. We illustrate TTA methods: TENT [65], EATA [50], CoTTA [66], and Ours (EcoTTA). TENT and EATA update multiple batch norm layers, in which large activations have to be stored for gradient calculation. In CoTTA, an entire network is trained with additional strategies for continual adaptation that requires a significant amount of both memory and time. In contrast, our approach requires a minimum size of activations by updating onlya few layers. Also, stable long-term adaptation is performed by our proposed regularization, named self-distilled regularization. to achieve large performance improvements, which may be impractical when the available memory sizes are limited. Meanwhile, several TTA approaches update only the batch normalization (BN) parameters [65, 50, 17] to make the optimization efficient and stable However, even updating only BN parameters is not memory efficient enough since the amount of memory required for training models signifi- cantly depends on the size of intermediate activations rather than the learnable parameters [4, 14, 69]. Throughout the paper, activations refer to the intermediate features stored during the forward propagation, which are used for gradi- ent calculations during backpropagation. Fig. 1 (a) demon- strates such an issue. Moreover, a non-trivial number of TTA studies assume a stationary target domain [65, 42, 9, 57], but the target do- main may continuously change in the real world (e.g., con- tinuous changes in weather conditions, illuminations, and location [8] in autonomous driving). Therefore, it is nec- essary to consider long-term TTA in an environment where the target domain constantly varies. However, there exist two challenging issues: 1) catastrophic forgetting [66, 50] and 2) error accumulation. Catastrophic forgetting refers to degraded performance on the source domain due to long- term adaptation to target domains [66, 50]. Such an issue is important since the test samples in the real world may come from diverse domains, including the source and tar- get domains [50]. Also, since target labels are unavailable, TTA relies on noisy unsupervised losses, such as entropy minimization [19], so long-term continual TTA may lead to error accumulation [75, 2]. To address these challenges, we propose memory- Efficient continual Test-Time Adaptation (EcoTTA), a sim- ple yet effective approach for 1) enhancing memory effi- ciency and 2) preventing catastrophic forgetting and error accumulation. First, we present a memory-efficient archi- tecture consisting of frozen original networks and our pro- posed meta networks attached to the original ones. During the test time, we freeze the original networks to discard the intermediate activations that occupy a significant amount of memory. Instead, we only adapt lightweight meta networks to the target domain, composed of only one batch normal- ization and one convolution block. Surprisingly, updating only the meta networks, not the original ones, can result in significant performance improvement as well as consider- able memory savings. Moreover, we propose a self-distilled regularization method to prevent catastrophic forgetting and error accumulation. Our regularization leverages the pre- served source knowledge distilled from the frozen original networks to regularize the meta networks. Specifically, we control the output of the meta networks not to deviate from the one extracted by the original networks significantly. No- tably, our regularization leads to negligible overhead be- cause it requires no extra memory and is performed in par- allel with adaptation loss, such as entropy minimization. Recent TTA studies require access to the source databe- fore model deployments[42, 9, 34, 1, 40, 50]. Similarly, our method uses the source data to warm up the newly attached meta networks for a small number of epochs before model deployment. If the source dataset is publicly available or the owner of the pre-trained model tries to adapt the model to a target domain, access to the source data is feasible [9]. Here, we emphasize that pre-trained original networks are frozen throughout our process, and our method is applicable to any pre-trained model because it is agnostic to the archi- tecture and pre-training method of the original networks. Our paper presents the following contributions: • We present novel meta networks that help the frozen original networks adapt to the target domain. This architecture significantly minimize memory consump- tion up to 86% by reducing the activation sizes of the original networks. • We propose a self-distilled regularization that controls the output of meta networks by leveraging the output of frozen original networks to preserve the source knowl- edge and prevent error accumulation. • We improve both memory efficiency and TTA perfor- mance compared to existing state-of-the-art methods on 1) image classification task ( e.g., CIFAR10/100-C and ImageNet-C) and 2) semantic segmentation task (e.g., Cityscapes with weather corruption) 22. Related Work Mitigating domain shift. One of the fundamental issues of DNNs is the performance degradation due to the domain shift between the train (i.e. source) and test (i.e. target) dis- tributions. Several research fields attempt to address this problem, such as unsupervised domain adaptation [64, 6, 53, 56, 46, 58] and domain generalization [76, 8]. In par- ticular, domain generalization aims to learn invariant rep- resentation so as to cover the possible shifts of test data. They simulate the possible shifts using a single or multiple source dataset [76, 74, 39] or force to minimize the depen- dence on style information [52, 8]. However, it is challeng- ing to handle all potential test shifts using the given source datasets [20]. Thus, instead of enhancing generalization ability during the training time, TTA [65] overcomes the domain shift by directly adapting to the test data. Test-time adaptation. Test-time adaptation allows the model to adapt to the test data ( i.e., target domain) in a source-free and online manner [33, 62, 65]. Existing works improve TTA performance with sophisticated designs of un- supervised loss [48, 72, 42, 9, 45, 57, 5, 16, 1, 3, 12, 59] or enhance the usability of small batch sizes [36, 70, 31, 51, 40] considering streaming test data. They focus on improv- ing the adaptation performance with a stationary target do- main (i.e., single domain TTA setup). In such a setting, the model that finished adaptation to a given target domain is reset to the original model pre-trained with the source do- main in order to adapt to the next target domain. Recently, CoTTA [66] has proposed continual TTA setup to address TTA under a continuously changing target do- main which also involves a long-term adaptation. This setup frequently suffers from error accumulation [75, 2, 63] and catastrophic forgetting [66, 35, 50]. Specifically, perform- ing a long-term adaptation exposes the model to unsuper- vised loss from unlabeled test data for a long time, so er- rors are accumulated significantly. Also, the model focuses on learning new knowledge and forgets about the source knowledge, which becomes problematic when the model needs to correctly classify the test sample as similar to the source distribution. To address such issues, CoTTA [66] randomly restores the updated parameters to the source one, while EATA [50] proposed a weight regularization loss. Efficient on-device learning. Since the edge device is likely to be memory constrained ( e.g., a Raspberry Pi with 512MB and iPhone 13 with 4GB), it is necessary to take account of the memory usage when deploying the models on the device [41]. TinyTL [4], a seminal work in on- device learning, shows that the activation size, not learn- able parameters, bottlenecks the training memory. Follow- ing this, recent on-device learning studies [4, 68, 69] target- ing fine-tuning task attempt to decrease the size of interme- diate activations. In contrast, previous TTA studies [65, 50] have overlooked these facts and instead focused on reduc- ing learnable parameters. This paper, therefore, proposes a method that not only reduces the high activation sizes re- quired for TTA, but also improves adaptation performance. 3. Approach Fig. 3 illustrates our simple yet effective approach which only updates the newly added meta networks on the tar- get domain while regularizing them with the knowledge distilled from the frozen original network. This section describes how such a design promotes memory efficiency and prevents error accumulation and catastrophic forgetting which are frequently observed in long-term adaptation. 3.1. Memory-efficient Architecture Prerequisite. We first formulate the forward and the back- ward propagation. Assume that the ith linear layer in the model consists of weight W and bias b, and the input and output features of this layer are fi and fi+1, respectively. Given that the forward propagation of fi+1 = fiW + b, the backward propagation from the i+1th layer to the ith layer, and the weight gradient are respectively formulated as: ∂L ∂fi = ∂L ∂fi+1 WT , ∂L ∂W = fT i ∂L ∂fi+1 . (1) Eq. (1) means that the learnable layers whose weight W need to be updated must store intermediate activations fi to compute the weight gradient. In contrast, the backward propagation in frozen layers can be accomplished without saving the activations, only requiring its weightW. Further descriptions are provided in Appendix A. TinyTL [4] shows that activations occupy the majority of the memory required for training the model rather than learnable parameters. Due to this fact, updating the entire model (e.g., CoTTA [66]) requires a substantial amount of memory. Also, updating only parameters in batch normal- ization (BN) layers (e.g., TENT [65] and EATA [50]) is not an effective approach enough since they still save the large intermediate activations for multiple BN layers. While pre- vious studies fail to reduce memory by utilizing large ac- tivations, this work proposes a simple yet effective way to reduce a significant amount of memory by discarding them. Before deployment. As illustrated in Fig. 3 (a, b), we first take a pre-trained model using any pre-training method. We divide the encoder of the pre-trained model into K number of parts and attach lightweight meta networks to each part of the original network. The details of how to divide the model into K number of parts are explained in the next sec- tion. One group of meta network composes of one batch normalization layer and one convolution block ( i.e., Conv- BN-Relu). Before the deployment, we pre-train the meta networks on the source dataset Ds for a small number of 3: backpropagation: freeze: update: meta networks (Ours)𝐷!:labeled source data𝐷\":unlabeled online test data Our proposed method (EcoTTA)Anypre-training method(a) partition (K=3)(b) attach and warm up meta networks Any pre-trained model (c) test-time adaptation Deploy=‖𝑥%#-𝑥#‖$ cross entropy 𝐷! K=3 entropy min. input convencoderclassifier 𝑥%#%$ 𝑥# convblockbn 𝑥%# 𝑥%#%$ 𝑥#bn 𝑥%# 𝐷\" self-distilled reg. convblock Figure 3. Overview of our approach. (a) The encoder of the pre-trained model is divided into K parts (i.e., model partition factor K). (b) Before deployment, the meta networks are attached to each part of the original networks and pre-trained with source dataset Ds. (c) After the model is deployed, only the meta networks are updated with unsupervised loss (i.e., entropy minimization) on target data Dt, while the original networks are frozen. To avoid error accumulation and catastrophic forgetting by the long-term adaptation, we regularize the output ˜xk of each group of the meta networks leveraging the output xk of the frozen original network, which preserves the source knowledge. epochs (e.g., 10 epochs for CIFAR dataset) while freezing the original networks. Such a warm-up process is com- pleted before the model deployment, similarly done in sev- eral TTA works [9, 34, 40, 50]. Note that we do not require source dataset Ds during test time. Pre-trained model partition. Previous TTA studies ad- dressing domain shifts [9, 48] indicate that updating shal- low layers is more crucial for improving the adaptation per- formance than updating the deep layers. Inspired by such a finding, given that the encoder of the pre-trained model is split into model partition factor K ( e.g., 4 or 5), we par- tition the shallow parts of the encoder more ( i.e., densely) compared to the deep parts of it. Table 4c shows how per- formance changes as we vary the model partition factor K. After deployment. During the test-time adaptation, we only adapt meta networks to target domains while freezing the original networks. Following EATA [50], we use the entropy minimization H(ˆy) = −P c p(ˆy) logp(ˆy) to the samples achieving entropy less than the pre-defined entropy threshold H0, where ˆy is the prediction output of a test im- age from test dataset Dt and p(·) is the softmax function. Thus, the main task loss for adaptation is defined as Lent = I{H(ˆy)<H0} · H(ˆy), (2) where I{·} is an indicator function. In addition, in order to prevent catastrophic forgetting and error accumulation, we apply our proposed regularization loss Rk, which is de- scribed next in detail. Consequently, the overall loss of our method is formulated as, Ltotal θ = Lent θ + λ KX k Rk θk , (3) where θ and θk denotes parameters of all meta networks and those of k-th group of meta networks, respectively, and λ is used to balance the scale of the two loss functions. Note that our architecture requires less memory than pre- vious works [66, 65] since we use frozen original networks and discard its intermediate activations. To be more spe- cific, our architecture uses 82% and 60% less memory on average than CoTTA and TENT/EATA. 3.2. Self-distilled Regularization The unsupervised loss from unlabeled test data Dt is likely to provide a false signal ( i.e., noise) to the model (ˆy ̸= yt where yt is the ground truth test label). Previ- ous works have verified that long-term adaptation with un- supervised loss causes overfitting due to error accumula- tion [75, 2] and catastrophic forgetting [66, 35]. To prevent the critical issues, we propose a self-distilled regularization utilizing our architecture. As shown in Fig. 3, we regularize the output ˜xk of each k-th group of the meta networks not to deviate from the outputxk of the k-th part of frozen orig- inal networks. Our regularization loss which computes the mean absolute error (i.e., L1 loss) is formulated as follows: Rk θk = ∥˜xk − xk∥1 . (4) Since the original networks are not updated, the output xk,k∼K extracted from them can be considered as contain- ing the knowledge learned from the source domain. Taking advantage of this fact, we let the output of meta networks ˜xk be regularized with knowledge distilled from the origi- nal networks. By preventing the adapted model to not sig- nificantly deviate from the original model, we can prevent 1) catastrophic forgetting by maintaining the source domain knowledge and 2) error accumulation by utilizing the class discriminability of the original model. Remarkably, unlike previous works [66, 50], our regularization does not require saving additional original networks, which accompanies ex- tra memory usage. Moreover, it only needs a negligible 4WideResNet-40 (AugMix) WideResNet-28 ResNet-50Method Avg. err↓ Mem. (MB) Avg. err↓ Mem. (MB) Avg. err↓ Mem. (MB) Source 36.7 11 43.5 58 48.8 91BN Stats Adapt [49] 15.4 11 20.9 58 16.6 91Single do. TENT [65] 12.7 188 19.2 646 15.0 925Continual TENT 13.3 188 20.0 646 15.2 925TTT++ [42] 14.6 391 20.3 1405 16.1 1877SWR&NSP [9] 12.1400 17.2 1551 15.4 1971NOTE [17] 13.4 188 20.2 646 - -EATA [50] 13.0 188 18.6 646 14.2 925CoTTA [66] 14.0 409 17.0 1697 14.4 2066Ours (K=4)12.2 80(80, 58%↓) 16.9404(76, 38%↓) 14.4296(86, 68%↓) Ours (K=5)12.1 92(77, 51%↓) 16.8471(72, 27%↓) 14.1498(76, 46%↓) (a) CIFAR10-C with severity level 5 WideResNet-40 (AugMix) ResNet-50Method Avg. err↓ Mem. (MB) Avg. err↓ Mem. (MB) Source 69.7 11 73.8 91BN Stats Adapt [49] 41.1 11 44.5 91Single do. TENT [65] 36.7 188 40.1 926Continual TENT 38.3 188 45.9 926TTT++ [42] 41.0 391 44.2 1876SWR&NSP [9] 36.6 400 44.1 1970NOTE [17] 42.8 188 - -EATA [50] 37.1 188 39.9 926CoTTA [66] 38.1 409 40.2 2064Ours (K=4)36.4 80(80, 58%↓) 39.5296(86, 68%↓) Ours (K=5)36.3 92(77, 51%↓) 39.3498(76, 46%↓) (b) CIFAR100-C with severity level 5 Table 1. Comparison of error rate ( %) on CIFAR-C. We report an average error of 15 corruptions on continual TTA and a memory requirement including model parameters and activation sizes. The lowest error is in bold, and the second lowest error is underlined. The memory reduction rates compared to CoTTA and TENT are presented sequentially. WideResNet-40 was pre-trained with AugMix [26] that is a data processing to increase the robustness of the model. Source denotes the pre-trained model without adaptation. Single domain (in short, single do.) TENT resets the model when adapting to a new target domain, so the domain labeles are required. ResNet-50 (AugMix) ResNet-50(MB)Total Mem.↓Method Avg. err↓ Avg. err↓ Source 74.36 82.35 91BN Stats Adapt [49] 57.87 72.18 91Continual TENT [65] 56.1 (0.6) 66.2 (1.1) 1486EATA [50] 54.9 (2.3) 63.8 (2.7) 1486CoTTA [66] 54.6(3.9) 62.6(3.1) 3132Ours (K=4) 55.2 (3.0) 64.6 (3.2)438(86, 72%↓) Ours (K=5) 54.4(2.7) 63.4(3.0) 747(75, 51%↓) Table 2. Comparison of error rate ( %) on ImageNet-C with severity level 5. Standard deviation for ten diverse corruption se- quences is denoted by the parentheses values. The total memory refers to the sum of model parameters and activations. Avg. err (%) CIFAR10-C CIFAR100-CMethod Mem. (MB)single do. continual single do. continual BN Stats Adapt [49] 91 16.6 16.6 44.5 44.5TinyTL†[4] 379 15.8 21.9 40.5 77.4RepNet†[69] 508 15.2 20.9 41.5 52.1AuxAdapt†[73] 207 16.0 16.7 44.0 45.8Ours (K=4) 296 14.4 14.4 39.5 39.2 Table 3. Comparison with methods for on-device learning. The backbone is ResNet-50. † denotes our own re-implemented mod- els. single do. indicates the singe domain TTA setup. amount of computational overhead because it is performed in parallel with the entropy minimization loss Lent. 4. Classification Experiments We evaluate our approach to image classification tasks based on the continual test-time adaptation setup with three datasets: CIFAR10-C, CIFAR100-C, and ImageNet-C. Experimental setup. Following CoTTA [66], we conduct most experiments on the continual TTA task, where we continually adapt the deployed model to each corruption type sequentially without resetting the model. This task is more challenging but more realistic than single domain TTA task [65] in which the adapted model is periodically reset to the original pre-trained model after finishing adaptation to each target, so they require additional domain information. Moreover, we evaluate our approach on the long-term TTA setup, which is detailed in Section 4.2. Following the previous TTA studies [65, 66], we eval- uate models with {CIFAR10, CIFAR10-C}, {CIFAR100, CIFAR100-C}, and {ImageNet, ImageNet-C } where the first and the second dataset in each bracket refers to the source and the target domain, respectively. The target do- mains include 15 types of corruptions ( e.g. noise, blur, weather, and digital) with 5 levels of severity, which are widely used in conventional benchmarks [25]. Implementation Details. We evaluate our approach within the frameworks officially provided by previous state-of- the-art methods [66, 50]. For fair comparisons, we use the same pre-trained model, which are WideResNet-28 and WideResNet-40 [71] models from the RobustBench [11], and ResNet-50 [24] model from TTT++ [42, 9]. Before the deployment, we pre-train the meta networks on the source dataset using a cross-entropy loss with SGD optimizer with the learning rate of 5e-2. Since the meta networks contain only a few layers, we pre-train them with a small number of epochs: 10 and 3 epochs for CIFAR and ImageNet, re- spectively. After deployment, similar to EATA [50], we use the same SGD optimizer with the learning rate of 5e-3. In Eq. (2), the entropy threshold H0 is set to 0.4 × ln C where C denotes the number of task classes. The batch size is 64 and 32 for CIFAR and ImageNet, respectively. We set the importance of the regularization λ in Eq. (3) to 0.5 to balance it with the entropy minimization loss. Additional implementation details can be found in Appendix C. Evaluation Metric. For all the experiments, we report error rates calculated during testing and the memory consump- tion, including the model parameter and the activation stor- 5Ours(i) (ii) (iii)(iv)(v)(vi) =‖\t\t\t-\t\t\t‖! convbn convbn bnconv CBAMSE : update conv (a) Visualization of networks variants Avr. errArch CIFAR10-CWRN-28CIFAR10-CWRN-40CIFAR100-CWRN-40(i) 18.1 12.637.2(ii) Ours w\\o BN 18.7 13.7 38.2(iii) Ours w\\o Conv 20.7 14.9 40.1(iv) Conv 60.6 73.3 77.2(v) CBAM [67] 21.4 15.1 40.9(vi) SE [30] 22.3 16.2 40.5Ours 16.812.136.3 (b) Meta network design (K=5) Model #Block Avg. err WRN-28 (12)CIFAR10-C 3,3,3,3 17.34,4,2,2 17.92,2,4,416.9 WRN-40 (18)CIFAR10-C 4,4,5,5 12.86,6,3,3 13.73,3,6,612.2 WRN-40 (18)CIFAR100-C 4,4,5,5 36.96,6,3,3 38.53,3,6,636.4 (c) # of blocks of each partition (K=4) Table 4. Architecture ablation experiments. (a,b) We compare continual TTA performance on several memory-efficient designs. WRN refers to WideResNet [71] backbone. (c) We report the performance based on different designs of partitioning the model. The value next to the backbone’s name denotes the total number of residual blocks of a model. age. We demonstrate the memory efficiency of our work by using the official code provided by TinyTL [4]. 4.1. Comparisons Comparisons with TTA methods. We compare our ap- proach to competing TTA methods on extensive bench- marks and various pre-trained models. The results of CIFAR10/100-C are detailed in Table 1. The model par- tition factor K are set to 4 and 5. Our approach outperforms existing TTA methods with the lowest memory usage in all pre-trained models. Specifically, in WideResNet-40, our method achieves superior performance while requiring 80% and 58% less memory than CoTTA [66] and EATA [50], re- spectively, which are also designed for continual TTA. Ap- proaches targeting single domain TTA [65, 42, 9] show poor performance due to error accumulation and catastrophic for- getting, as observed in CoTTA. The error rates for each cor- ruption type are provided in Appendix F. Table 2 shows the experiment for ImageNet-C. Two ResNet-50 backbones from RobustBench [11] are lever- aged. Following CoTTA, evaluations are conducted on ten diverse corruption-type sequences. We achieve comparable performance to CoTTA while utilizing 86% and 75% less memory with K=4 and 5, respectively. In addition, we ob- serve that our approach shows superior performance when adopting the model pre-trained with strong data augmenta- tion methods (e.g., Augmix [26]). Comparisons with on-device learning methods.We com- pare our approach with methods for memory-efficient on- device learning. TinyTL [4] and RepNet [69] focus on su- pervised on-device learning ( i.e., requiring labeled target data). However, since TTA assumes that we do not have access to the target labels, utilizing such methods to TTA di- rectly is infeasible. Therefore, we experimented by replac- ing supervised loss ( i.e., cross-entropy) with unsupervised loss (i.e., entropy minimization) in TinyTL and RepNet. As shown in Table 3, they suffer from performance degradation in continual TTA, showing inferior performance compared to our proposed approach even in the single domain TTA. Memory (MB) Avg. error (%)  (0.8%)              CIFAR100-C                      WideResNet-40 K=1 K=2 (3.7%) K=3 (4.3%) K=4 (10.8%) K=5 (11.3%) K=6 (12.8%) K=7 (13.3%) ����������� Figure 4. Ablation study of K. We uniformly divide the encoder of the pre-trained model into the model partition factor K. The x- axis indicates the memory size including both model parameter size and activation size while the y-axis indicates the average error rate. The values in parentheses show the rate of increase for the model parameters compared to the original model. Similar to ours, AuxAdapt [73] adds and updates a small network ( i.e., ResNet-18) while freezing the pre-trained model. Unlike our approach, they only modify a prediction output, not intermediate features. While AuxAdapt requires the least memory usage, it fails to improve TTA performan- ce in single domain TTA. Nevertheless, since the original model is frozen, it suffers less from catastrophic forgetting and error accumulation than TinyTL [4] and RepNet [69] in the continual TTA. Through the results, we confirm that our proposed method brings both memory efficiency and a significant performance improvement in both TTA setups. 4.2. Empirical Study Architecture design. An important design of our meta net- works is injecting a single BN layer before the original net- works and utilizing a residual connection with one conv block. Table 4b studies the effectiveness of the proposed design by comparing it with six different variants. From the results, we observe that using only either conv block (ii) or BN (iii) aggravates the performance: error rate increases by 1.4% and 3.8% on CIFAR-100-C with WideResNet-40. In design (i), we enforce both BN parameters and Conv layers in the meta networks to take the output of the origi- nal networks as inputs. Such a design brings performance drop. We speculate that it is because the original network, 6Gaus.ShotImpu.Defo.Glas.Moti.ZoomSnowFros.Fog Brig.Cont.Elas.Pixe.Jpeg 27.5 30.0 32.5 35.0 37.5 40.0 42.5 45.0 25 26 27 28 29 30 31                                Corruption type in 1 round                                Ours (Clean) TENT (Clean) Ours (Corrupt) �������������� (a) Catastrophic forgetting effect Corrup&on Error (%) Round (b) Error accumulation effect Figure 5. Regularization ablation experiments. We conduct experiments with WideResNet-40 on CIFAR100-C. (a) We utilize a test set of the CIFAR-100 dataset to measure clean error after adapting to each corruption. Maintaining clean errors at a stable level indicates that our approach helps the model robust to catastrophic forgetting. (b) We simulate a long-term adaptation scenario by repeating 100 rounds of 15 corruption sequences. In the absence of regularization, error accumulation can lead to overfitting (i.e., the case of the error increases exponentially). However, our approach does not suffer from such an error accumulation. We set K to 5 in the above experiments. Batch size 16 8 4 2 1 Non training Source 69.7 69.7 69.7 69.7 69.7 BN Stats Adapt [49] 41.1 50.2 59.9 81.0 99.1 AdaptBN [55] 39.1 41.2 45.2 49.0 54.0 Training Con. TENT [65] 40.9 47.8 58.6 82.2 99.0 Con. TENT+AdaptBN 38.2 40.2 43.2 47.7 52.2 Ours (K=5) 40.0 45.8 63.4 80.8 99.0 Ours (K=5)+AdaptBN36.9 39.3 42.2 46.5 51.8 Table 5. Experiments with small batch sizes. We evaluate all baselines with WideResNet-40 on CIFAR100-C. Con. TENT is the abbreviation for continual TENT. which is not adapted to the target domain, lacks the ability to extract sufficiently meaningful features from the target image. Also, we observed a significant performance degra- dation after removing the residual connection in design (iv). In addition, since attention mechanisms [67, 30] generally have improved classification accuracy, we study how atten- tion mechanisms can further boost TTA performance of our approach in design (v, vi). The results show that it is diffi- cult for the attention module to train ideally in TTA setup using unsupervised learning, unlike when applying it to su- pervised learning. An ablation study on each element of meta networks can be found in Appendix D. Number of blocks in each partition. ResNet [24] consists of multiple residual blocks (e.g., BasicBlock and Bottleneck in Pytorch [54]). For instance, WideResNet-28 has 12 resid- ual blocks. By varying the number of blocks for each part of the original networks, we analyze TTA performance in Ta- ble 4c. We observe that splitting the shallow parts of the en- coder densely (e.g., 2,2,4,4 blocks, from the shallow to the deep parts sequentially) brings more performance gain than splitting the deep layers densely ( e.g., 4,4,2,2 blocks). We suggest that it is because we modify the lower-level feature more as we split shallow layers densely. Our observation is aligned with the finding of previous TTA works [9, 48], which show that updating the shallow layers more than the deep layers improves TTA performance. Number of model partition K. Fig. 4 shows both memory requirement and adaptation performance according to the model partition factor K. With a small K ( e.g., 1 or 2), the intermediate outputs are barely modified, making it difficult to achieve a reasonable level of performance. We achieve the best TTA performance with K of 4 or 5 as adjusting a greater numver of intermediate features. In the meanwhile, we observe that the average error rate is saturated and re- mains consistent when K is set to large values (e.g. 6,7 or 8) even with the increased amount of activations and learnable parameters. Therefore, we set K to 4 and 5. Catastrophic forgetting. We conduct experiments to con- firm the catastrophic forgetting effect (Fig. 5a). Once fin- ishing adaptation to each corruption, we evaluate the model on clean target data ( i.e., test-set of CIFAR dataset) with- out updating the model. For TENT with no regulariza- tion, the error rates for the clean target data ( i.e., clean er- ror (%)) increase gradually, which can be seen as the phe- nomenon of catastrophic forgetting. In contrast, our ap- proach consistently maintains the error rates for the clean target data, proving that our regularization loss effectively prevents catastrophic forgetting. These results indicate that our method can be reliably utilized in various domains, in- cluding the source and target domains. Error accumulation in long-term adaptation. To evalu- ate the error accumulation effect, we repeat all the corrup- tion sequences for 100 rounds. The results are described in Fig. 5b. For TENT, a gradual increase in error rates is ob- served in later rounds, even with small learning rates. For example, TENT [65] with the learning rate of 1e-5 achieves the error rate of 39.7%, and reached its lowest error rate of 36.5% after 8 rounds. However, it shows increased error rate of 38.6% after 100 rounds due to overfitting. It suggests 7Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − →Round 1 4 7 10 AllMethod Mem. (MB)Brig. Fog Fros. SnowBrig. Fog Fros. SnowBrig. Fog Fros. SnowBrig. Fog Fros. SnowMean Source 280 60.4 54.3 30.0 4.160.4 54.3 30.0 4.160.4 54.3 30.0 4.160.4 54.3 30.0 4.137.2BN Stats Adapt [49]280 69.1 61.0 44.8 39.169.1 61.0 44.8 39.169.1 61.0 44.8 39.169.1 61.0 44.8 39.153.6Continual TENT [65]2721 70.1 62.1 46.1 40.262.2 53.7 44.4 37.950.0 41.5 31.6 26.639.2 32.6 25.3 22.442.9Ours (K=4) 918(66%↓) 70.262.446.341.970.062.846.542.270.062.846.542.170.162.846.642.255.3 Table 6. Semantic segmentation results in continual test-time adaptation tasks. We conduct experiments on Cityscapes [10] with four weather corruptions [25] applied. The four conditions are repeated ten times to simulate continual domain shifts. All results are evaluated based on DeepLabV3Plus-ResNet-50. that without regularization, TTA methods eventually face overfitting in long-term adaptation [75, 2, 35]. Our method in the absence of regularization (λ = 0) also causes overfit- ting. On the other hand, when self-distilled regularization is involved (λ >0), the performance remains consistent even in the long-term adaptation. Small batch size. We examine the scalability of our ap- proach with a TTA method designed for small batches size, named adapting BN statistics (i.e., AdaptBN [55, 72]). When the number of batches is too small, the estimated statistics can be unreliable [55]. Thus, they calibrate the source and target statistics for the normalization of BN lay- ers so as to alleviate the domain shift and preserve the dis- criminative structures. As shown in Table 5, training mod- els with small batch sizes (e.g., 2 or 1) generally increase the error rates. However, such an issue can be addressed by appying AdaptBN to our method. To be more sepcific, we achieve an absolute improvement of 17.9% and 2.2% from Source and AdaptBN, respectively, in the batch size of 1. Number of the source samples for meta networks. Like previous TTA works [9, 42, 34, 40] including EATA [50], our approach requires access to the source data for pre- training our proposed meta networks before model deploy- ment. In order to cope with the situation where we can only make use of a subset of the source dataset, we study the TTA performance of our method according to the number of ac- cessible source samples. The results are specified in Table 7 where we use WideResNet-40. We observe that our method outperforms the baseline model even with small number of training samples ( e.g., 10% or 20%) while showing com- parable performance with excessively small numbers ( e.g. 5%). Note that we still reduce the memory usage of about 51% compared to EATA. 5. Segmentation Experiments We investigate our approach in semantic segmentation. First, we create Cityscapes-C by applying the weather cor- ruptions (brightness, fog, frost, and snow [25]) to the vali- dation set of Cityscapes [10]. Then, to simulate continual distribution shifts, we repeat the four types of Cityscapes-C ten times. In this scenario, we conduct continual TTA using the publicly-available ResNet-50-based DeepLabV3 + [7], which is pre-trained on Cityscapes for domain generaliza- EATA [50] (188MB) # of source samples Target domain Ours(K=5) (92MB) 10k (20%) 5k (10%) 2.5k (5%) CIFAR10-C 13.0 12.1 12.4 12.9 13.1 CIFAR100-C 37.1 36.3 36.4 36.6 37.2 Table 7. Ablation of # of source samples to warm up the meta networks. Before deployment, we pre-trained the meta networks using only a subset of the source dataset (e.g., 20%, 10%, and 5%). The memory usage (MB) of each method is also presented. tion task [8] in semantic segmentation. For TTA, we use the batch size of 2. More details are specified in Appendix C. Results. We report the results based on mean intersection over union (mIoU) in Table 6. It demonstrates that our ap- proach helps to both minimize memory consumption and performs long-term adaptation stably for semantic segmen- tation. Unlike continual TENT, our method avoids catas- trophic forgetting and error accumulation, allowing us to achieve the highest mIoU score while using 66% less mem- ory usage in a continual TTA setup. Additional experiment results can be found in Appendix B. 6. Conclusion This paper proposed a simple yet effective approach that improves continual TTA performance and saves a signifi- cant amount of memory, which can be applied to edge de- vices with limited memory. First, we presented a memory- efficient architecture that consists of original networks and meta networks. This architecture requires much less mem- ory size than the previous TTA methods by decreasing the intermediate activations used for gradient calculations. Sec- ond, in order to preserve the source knowledge and prevent error accumulation during long-term adaptation with noisy unsupervised loss, we proposed self-distilled regularization that controls the output of meta networks not to deviate sig- nificantly from the output of the original networks. With ex- tensive experiments on diverse datasets and backbone net- works, we verified the memory efficiency and TTA perfor- mance of our approach. In this regard, we hope that our efforts will facilitate a variety of studies that make test-time adaptation for edge devices feasible in practice. Acknowledgments. We would like to thank Kyuwoong Hwang, Simyung Chang, and Byeonggeun Kim for their valuable feedback. We are also grateful for the helpful dis- cussions from Qualcomm AI Research teams. 8References [1] Kazuki Adachi, Shin’ya Yamaguchi, and Atsutoshi Kuma- gai. Covariance-aware feature alignment with pre-computed source statistics for test-time adaptation. arXiv preprint arXiv:2204.13263, 2022. 2, 3 [2] Eric Arazo, Diego Ortego, Paul Albert, Noel E O’Connor, and Kevin McGuinness. Pseudo-labeling and confirmation bias in deep semi-supervised learning. In IJCNN, 2020. 2, 3, 4, 8 [3] Kambiz Azarian, Debasmit Das, Hyojin Park, and Fatih Porikli. Test-time adaptation vs. training-time generaliza- tion: A case study in human instance segmentation using keypoints estimation. In WACV Workshops, 2023. 3 [4] Han Cai, Chuang Gan, Ligeng Zhu, and Song Han. Tinytl: Reduce memory, not parameters for efficient on-device learning. In NeurIPS, 2020. 2, 3, 5, 6, 12, 16 [5] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In CVPR, 2022. 3, 13 [6] Lin Chen, Huaian Chen, Zhixiang Wei, Xin Jin, Xiao Tan, Yi Jin, and Enhong Chen. Reusing the task-specific classifier as a discriminator: Discriminator-free adversarial domain adap- tation. In CVPR, 2022. 3 [7] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In ECCV, 2018. 8, 14 [8] Sungha Choi, Sanghun Jung, Huiwon Yun, Joanne T Kim, Seungryong Kim, and Jaegul Choo. Robustnet: Improving domain generalization in urban-scene segmentation via in- stance selective whitening. In CVPR, 2021. 1, 2, 3, 8, 14, 15 [9] Sungha Choi, Seunghan Yang, Seokeon Choi, and Sun- grack Yun. Improving test-time adaptation via shift-agnostic weight regularization and nearest source prototypes. In ECCV, 2022. 1, 2, 3, 4, 5, 6, 7, 8, 13, 15, 16, 17 [10] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016. 8, 14 [11] Francesco Croce, Maksym Andriushchenko, Vikash Se- hwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness benchmark. In NeurIPS Datasets and Benchmarks Track, 2021. 5, 6 [12] Debasmit Das, Shubhankar Borse, Hyojin Park, Kambiz Azarian, Hong Cai, Risheek Garrepalli, and Fatih Porikli. Transadapt: A transformative framework for online test time adaptive semantic segmentation. In ICASSP, 2023. 3 [13] Zhiwei Deng and Olga Russakovsky. Remember the past: Distilling datasets into addressable memories for neural net- works. arXiv preprint arXiv:2206.02916, 2022. 13 [14] Sauptik Dhar, Junyao Guo, Jiayi Liu, Samarth Tripathi, Un- mesh Kurup, and Mohak Shah. A survey of on-device ma- chine learning: An algorithms and learning theory perspec- tive. ACM Transactions on Internet of Things, 2021. 2 [15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, et al. An image is worth 16x16 words: Trans- formers for image recognition at scale. In ICLR, 2021. 1 [16] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei A Efros. Test-time training with masked autoencoders. In NeurIPS, 2022. 1, 3 [17] Taesik Gong, Jongheon Jeong, Taewon Kim, Yewon Kim, Jinwoo Shin, and Sung-Ju Lee. Robust continual test- time adaptation: Instance-aware bn and prediction-balanced memory. In NeurIPS, 2022. 2, 5, 16, 17 [18] Priya Goyal, Piotr Doll ´ar, Ross Girshick, Pieter Noord- huis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large mini- batch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. 16 [19] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In NeurIPS, 2004. 2, 15 [20] Ishaan Gulrajani and David Lopez-Paz. In search of lost do- main generalization. In ICLR, 2021. 3 [21] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In ICML, 2017. 13 [22] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll´ar, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022. 1 [23] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual rep- resentation learning. In CVPR, 2020. 1 [24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 1, 5, 7, 12, 16 [25] Dan Hendrycks and Thomas Dietterich. Benchmarking neu- ral network robustness to common corruptions and perturba- tions. In ICLR, 2019. 5, 8 [26] Dan Hendrycks, Norman Mu, Ekin D. Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. AugMix: A simple data processing method to improve robustness and uncertainty. In ICLR, 2020. 5, 6, 14 [27] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. In NeurIPS, 2014. 13 [28] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In ICML, 2019. 13 [29] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen- Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In ICLR, 2022. 13 [30] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net- works. In CVPR, 2018. 6, 7 [31] Xuefeng Hu, Gokhan Uzunbas, Sirius Chen, Rui Wang, Ashish Shah, Ram Nevatia, and Ser-Nam Lim. Mixnorm: Test-time adaptation through online normalization estima- tion. arXiv preprint arXiv:2110.11478, 2021. 3 9[32] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier ad- justment module for model-agnostic domain generalization. In NeurIPS, 2021. 1 [33] Vidit Jain and Erik Learned-Miller. Online domain adapta- tion of a pre-trained cascade of classifiers. In CVPR, 2011. 3 [34] Sanghun Jung, Jungsoo Lee, Nanhee Kim, and Jaegul Choo. Cafa: Class-aware feature alignment for test-time adaptation. arXiv preprint arXiv:2206.00205, 2022. 2, 4, 8 [35] Tommie Kerssies, Joaquin Vanschoren, and Mert Kılıc ¸kaya. Evaluating continual test-time adaptation for contextual and semantic domain shifts. arXiv preprint arXiv:2208.08767 , 2022. 3, 4, 8 [36] Ansh Khurana, Sujoy Paul, Piyush Rai, Soma Biswas, and Gaurav Aggarwal. Sita: Single image test-time adaptation. arXiv preprint arXiv:2112.02355, 2021. 3 [37] Andreas Krause, Pietro Perona, and Ryan Gomes. Discrim- inative clustering by regularized information maximization. In NeurIPS, 2010. 15 [38] Daiqing Li, Junlin Yang, Karsten Kreis, Antonio Torralba, and Sanja Fidler. Semantic segmentation with generative models: Semi-supervised learning and strong out-of-domain generalization. In CVPR, 2021. 1 [39] Xiaotong Li, Yongxing Dai, Yixiao Ge, Jun Liu, Ying Shan, and Ling-Yu Duan. Uncertainty modeling for out-of- distribution generalization. In ICLR, 2022. 3 [40] Hyesu Lim, Byeonggeun Kim, Jaegul Choo, and Sungha Choi. TTN: A domain-shift aware batch normalization in test-time adaptation. In ICLR, 2023. 2, 3, 4, 8, 13, 16 [41] Ji Lin, Wei-Ming Chen, Yujun Lin, Chuang Gan, Song Han, et al. Mcunet: Tiny deep learning on iot devices. InNeurIPS, 2020. 3 [42] Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? In NeurIPS, 2021. 1, 2, 3, 5, 6, 8, 13, 16, 17 [43] Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Unsupervised domain adaptation with residual trans- fer networks. In NeurIPS, 2016. 1 [44] David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. In NeurIPS, 2017. 13 [45] Robert A Marsden, Mario D ¨obler, and Bin Yang. Gradual test-time adaptation by self-training and style transfer. arXiv preprint arXiv:2208.07736, 2022. 3 [46] Ke Mei, Chuang Zhu, Jiaqi Zou, and Shanghang Zhang. In- stance adaptive self-training for unsupervised domain adap- tation. In ECCV, 2020. 3 [47] Rafael M ¨uller, Simon Kornblith, and Geoffrey E Hinton. When does label smoothing help? In NeurIPS, 2019. 13 [48] Chaithanya Kumar Mummadi, Robin Hutmacher, Kilian Rambach, Evgeny Levinkov, Thomas Brox, and Jan Hendrik Metzen. Test-time adaptation to distribution shift by confi- dence maximization and input transformation.arXiv preprint arXiv:2106.14999, 2021. 3, 4, 7 [49] Zachary Nado, Shreyas Padhy, D Sculley, Alexander D’Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robust- ness under covariate shift. arXiv preprint arXiv:2006.10963, 2020. 5, 7, 8, 16, 17 [50] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test- time model adaptation without forgetting. In ICML, 2022. 2, 3, 4, 5, 6, 8, 12, 13, 14, 15, 16, 17 [51] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan. Towards stable test-time adaptation in dynamic wild world. In ICLR, 2023. 3 [52] Xingang Pan, Ping Luo, Jianping Shi, and Xiaoou Tang. Two at once: Enhancing learning and generalization capacities via ibn-net. In ECCV, 2018. 3 [53] Kwanyong Park, Sanghyun Woo, Inkyu Shin, and In So Kweon. Discover, hallucinate, and adapt: Open compound domain adaptation for semantic segmentation. In NeurIPS, 2020. 3 [54] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, and et al. Lin. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019. 7, 12, 16 [55] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bring- mann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. In NeurIPS, 2020. 7, 8, 16 [56] Inkyu Shin, Dong-Jin Kim, Jae Won Cho, Sanghyun Woo, Kwanyong Park, and In So Kweon. Labor: Labeling only if required for domain adaptive semantic segmentation. In ICCV, 2021. 3 [57] Inkyu Shin, Yi-Hsuan Tsai, Bingbing Zhuang, Samuel Schulter, Buyu Liu, Sparsh Garg, In So Kweon, and Kuk- Jin Yoon. Mm-tta: Multi-modal test-time adaptation for 3d semantic segmentation. In CVPR, 2022. 2, 3 [58] Inkyu Shin, Sanghyun Woo, Fei Pan, and InSo Kweon. Two- phase pseudo label densification for self-training based do- main adaptation. In ECCV, 2020. 3 [59] Junha Song, Kwanyong Park, Inkyu Shin, Sanghyun Woo, Chaoning Zhang, and In So Kweon. Test-time adaptation in the dynamic world with compound domain knowledge man- agement. arXiv preprint arXiv:2212.08356, 2023. 3 [60] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 2014. 13 [61] Qianru Sun, Yaoyao Liu, Tat-Seng Chua, and Bernt Schiele. Meta-transfer learning for few-shot learning. InCVPR, 2019. 13 [62] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A. Efros, and Moritz Hardt. Test-time training with self- supervision for generalization under distribution shifts. In ICML, 2020. 3 [63] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In NeurIPS, 2017. 3 [64] Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Mathieu Cord, and Patrick P´erez. Advent: Adversarial entropy mini- 10mization for domain adaptation in semantic segmentation. In CVPR, 2019. 3 [65] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Ol- shausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In ICLR, 2021. 1, 2, 3, 4, 5, 6, 7, 8, 12, 13, 14, 16, 17 [66] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Con- tinual test-time domain adaptation. In CVPR, 2022. 1, 2, 3, 4, 5, 6, 12, 13, 14, 16, 17 [67] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module. In ECCV, 2018. 6, 7 [68] Li Yang, Adnan Siraj Rakin, and Deliang Fan. Da3: Dy- namic additive attention adaption for memory-efficient on- device multi-domain learning. In CVPR Workshops, 2022. 3 [69] Li Yang, Adnan Siraj Rakin, and Deliang Fan. Rep-net: Efficient on-device learning via feature reprogramming. In CVPR, 2022. 2, 3, 5, 6, 16 [70] Fuming You, Jingjing Li, and Zhou Zhao. Test-time batch statistics calibration for covariate shift. arXiv preprint arXiv:2110.04065, 2021. 3 [71] Sergey Zagoruyko and Nikos Komodakis. Wide residual net- works. In BMVC, 2016. 5, 6, 12 [72] Marvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and augmentation. In NeurIPS, 2021. 3, 8 [73] Yizhe Zhang, Shubhankar Borse, Hong Cai, and Fatih Porikli. Auxadapt: Stable and efficient test-time adaptation for temporally consistent video semantic segmentation. In WACV, 2022. 5, 6, 16 [74] Yabin Zhang, Minghan Li, Ruihuang Li, Kui Jia, and Lei Zhang. Exact feature distribution matching for arbitrary style transfer and domain generalization. In CVPR, 2022. 3 [75] Zixing Zhang, Fabien Ringeval, Bin Dong, Eduardo Coutinho, Erik Marchi, and Bj¨orn Sch¨uller. Enhanced semi- supervised learning for multimodal emotion recognition. In ICASSP, 2016. 2, 3, 4, 8 [76] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Do- main generalization with mixstyle. In ICLR, 2021. 3 11Appendix In this supplementary material, we provide, A. Efficiency for TTA methods B. Discussion and further experiments C. Further implementation details D. Additional ablations E. Baseline details F. Results of all corruptions A. Efficiency for TTA methods Memory efficiency. Existing TTA works [65, 50, 66] up- date model parameters to adapt to the target domain. This process inevitably requires additional memory to store the activations. Fig. 6 describes Eq. (1) of the main paper in more detail. For instance, 1) the backward propagation from the layer (c) to the layer ( b) can be accomplished without saving intermediate activations fi and fi+1, since it only re- quires ∂L ∂fi+1 =∂L ∂LWT i+1 and ∂L ∂fi = ∂L ∂fi+1 WT i =∂L ∂LWT i+1WT i op- erations. 2) During the forward propagation, the learnable layer (a) has to store the intermediate activation fi−1 to cal- culate the weight gradient ∂L ∂Wi−1 =fT i−1 ∂L ∂fi . : freeze: update ℒ𝜕ℒ𝜕ℒ𝜕ℒ𝜕𝑓!\"#𝜕ℒ𝜕𝑓!𝜕ℒ𝜕𝑓!$# 𝒇𝒊$𝟏 𝑓! 𝑓!\"#𝑊!$#,𝑏!$# 𝑊!,𝑏! 𝑊!\"#,𝑏!\"#(𝑎) (𝑏) (𝑐)𝜕ℒ𝜕𝑓!=𝜕ℒ𝜕𝑓!\"#𝑊!' 𝜕ℒ𝜕𝑓!\"#=𝜕ℒ𝜕ℒ𝑊!\"#'𝜕ℒ𝜕𝑓!$#=𝜕ℒ𝜕𝑓!𝑊!$#'𝜕ℒ𝜕𝑊!$#=𝒇𝒊$𝟏𝑻𝜕ℒ𝜕𝑓! 𝑓!\"#=𝑓!𝑊!+𝑏!𝜕ℒ𝜕𝑏!$#=𝜕ℒ𝜕𝑓! Figure 6. Forward and backward propagation. The black and red lines refer to forward and backward propagation, respectively. f and (a, b, c) are the activations and the linear layers, respectively. Computational efficiency. Wall-clock time and floating point operations (FLOPs) are standard measures of com- putational cost. We utilize wall-clock time to compare the computational cost of TTA methods since most libraries computing FLOPs only support inference, not training. Unfortunately, wall-clock time of EATA [50] and our ap- proach can not truly represent its computational efficiency since the current Pytorch version [54] does not support fine-grained implementation [4]. For example, EATA fil- ters samples to improve its computational efficiency. How- ever, its gradient computation is performed on the full mini- batch, so the wall-clock time for backpropagation in EATA is almost the same as that of TENT [65]. In our approach, our implementation follows Algorithm 1 to make each reg- ularization loss Rk θk applied to parameters of k-th group of meta networks θk in Eq. (3). In order to circumvent such an issue, the authors of EATA report the theoretical time, which assumes that PyTorch handles gradient back- propagation at an instance level. Similar to EATA, we also report both theoretical time and wall-clock time in Ta- ble 8. To compute the theoretical time of our approach, we simply subtract the time for re-forward (in Algorithm 1) from wall-clock time. We emphasize that this is mainly an engineering-based issue, and the optimized implementation can further improve computational efficiency. [50]. Using a single NVIDIA 2080Ti GPU, we measure the total time required to adapt to all 15 corruptions, includ- ing the time to load test data and perform TTA. The results in Table 8 show that our proposed method requires neg- ligible overhead compared to CoTTA [66]. For example, CoTTA needs approximately 10 times more training time than Continual TENT [65] with WideResNet-40. Note that meta networks enable our approach to use 80% and 58% less memory than CoTTA and EATA, even with such minor extra operations. B. Discussion and further experiments Comparison on gradually changing setup. In Table 1 and Table 2, we evaluate all methods on the continual TTA task, proposed in CoTTA [66] and EATA [50], where we continually adapt the deployed model to each cor- ruption type sequentially. Additionally, we conduct ex- periments on the gradually changing setup. This grad- WideResNet-40 [71] Avg. err Mem. (MB) Theo. time Wall time Source 69.7 11 - 40s Con. TENT [65] 38.3 188 - 2m 18s CoTTA [66] 38.1 409 - 22m 52s EATA [50] 37.1 188 2m 8s 2m 22s Ours (K=4) 36.4 80(80, 58%↓) 2m 27s 2m 49s Ours (K=5) 36.3 92(77, 51%↓) 2m 31s 2m 52s ResNet-50 [24] Avg. err Mem. (MB) Theo. time Wall time Source 73.8 91 - 1m 8s Con. TENT [65] 45.9 926 - 4m 2s CoTTA [66] 40.2 2064 - 38m 24s EATA [50] 39.9 926 3m 45s 4m 15s Ours (K=4) 39.5 296(86, 68%↓) 4m 16s 4m 41s Ours (K=5) 39.3 498(76, 46%↓) 4m 26s 5m 14s Table 8. Comparison of training time on CIFAR100-C. We re- port both theoretical time (in short, theo. time) and wall-clock time, taking to adapt to all 15 corruption types. Theoretical time is calculated by assuming that the ML frameworks ( e.g., Py- torch [54]) provide fine-grained implementations [4]. Con. TENT refers to continual TENT. 12Algorithm 1: PyTorch-style pseudocode for EcoTTA. # img_t: test image # model: original and meta networks # # ent_min(): Entropy minimization loss # Detach_parts(): Detach the graph connection # between each partition of networks # Attach_parts(): Attach the graph connection # between each partition of networks for img_t in test_loader: # 1. Forward output = model(img_t) # 2. Compute entropy loss loss_ent = ent_min(output) loss_ent.backward() # 3. Re-forward # (This process is not required # in fine-grained ML frameworks.) Detach_parts(model) _ = model(img_t) # 4. Compute regularization loss reg_loss = 0 for k_th_meta in meta_networks: reg_loss += k_th_meta.get_l1_loss() reg_loss.backward() # 5. Update params of meta networks optim.step() optim.zero_grad() Attach_parts(model) ual setup, proposed in CoTTA, represents the sequence by gradually changing severity for the 15 corruption types: . . .2− →1| {z } t-1 and before change − − − − − → type 1− →2− →3− →4− →5− →4− →3− →2− →1| {z } corruption type t, gradually changing severity change − − − − − → type 1− →2. . .| {z } t+1 and on , The results in Table 9 indicate that our approach outper- forms previous TTA methods [65, 50, 66] even with the gradually changing setup. Comparisons with methods for parameter efficient transfer learning. While our framework may be similar to parameter-efficient transfer learning (PETL) [29, 28, 61] in that only partial parameters are updated during training time for PETL or test time for TTA, we utilized meta net- works to minimize intermediate activations, which is crucial for memory-constrained edge devices. We conduct experi- ments by applying a PETL method [28] to the TTA setup. The adapter module is constructed by using 3x3 Conv and ReLU layers as the projection layer and the nonlinearity, re- spectively, and these modules are attached after each resid- ual block of the backbone networks. The Table 10 shows that PETA+SDR needs a 177% increase in memory usage with a 6.1% drop in performance, compared to our method. Comparisons with methods for continual learning. Typ- ical continual learning (CL) and continual TTA assume su- pervised and unsupervised learning, respectively. However, since both are focused on alleviating catastrophic forget- ting, we believe that CL methods can also be applied in continual TTA settings. The methods for addressing catas- trophic forgetting can be divided into regularization- and Method Con. TENT [65] EATA [50] CoTTA [66]Ours (K=4) Avg. err (%) 38.5 31.8 32.5 31.4 Mem. (MB) 188 188 409 80(58, 80%↓) Table 9. Comparision on gradually changing setup. To con- duct experiments, we use WRN-40 backbone on CIFAR100-C. The values in parentheses refer to memory reduction rates com- pared to TENT/EATA and CoTTA, sequentially. Method Con. TENT [65]PETL [28] PETL+SDROurs (K=4) Avg. err (%)38.3 73.3 42.5 36.4Mem. (MB) 188 141 141 80 Table 10. Comparisons with methods for PETL. We com- pare our method with methods [28] for parameter-efficient trans- fer learning (PETL) with WRN-40 on CIFAR100-C. PETL+SDR refers to PETL with our proposed self-distilled regularization. RoundCon. TENTTS DO LS KD Ours (K=4) 1 38.3 37.4 41.0 38.4 39.8 36.410 99.0 96.1 96.3 41.1 40.4 36.3 Table 11. Comparisons with methods for continual learning. We report an average error rate (%) of 15 corruptions using WRN- 40 on CIFAR100-C. In the table, TS: Entropy minimization with temperature scaling [21], DO: Dropout [60], LS: Label smoothing with the pseudo label [47], and KD: Knowledge distillation [27]. replay-based methods. The former can be subdivided into weight regularization ( e.g., CoTTA [66] and EATA [50]) and knowledge distillation [27], while the latter includes GEM [44] and dataset distillation [13]. Suppose dataset dis- tillation is applied to the continual TTA setup; for example, we can periodically replay synthetic samples distilled from the source dataset to prevent the model from forgetting the source knowledge during TTA. Notably, our self-distilled regularization (SDR) is superior to conventional CL meth- ods in terms of the efficiency of TTA in on-device settings. Specifically, unlike previous regularization- or replay-based methods, we do not require storing a copy of the original model or a replay-and-train process. To further compare our SDR with existing regulariza- tion methods, we conduct experiments while keeping our architecture and adaptation loss but replacing SDR with other regularizations, as shown in Table 11. The results demonstrate that our SDR achieves superior performance compared to other regularizations. In addition, Knowledge distillation [27] alleviates the error accumulation effect in long-term adaptation ( e.g., round 10), while showing lim- ited performance for adapting to the target domain. Superiority of our approach compared to existing TTA methods. Our work focuses on proposing an efficient ar- chitecture for continual TTA, which has been overlooked in previous TTA studies [65, 66, 5, 42, 9, 40] by introduc- ing meta networks and self-distilled regularization, rather than adaptation loss such as entropy minimization proposed 13Method Mem. (MB) Round 1 Round 4 Round 7 Round 10 Source 280 37.2 37.2 37.2 37.2Con. TENT 2721 54.6 49.6 37.4 29.9Con. TENT* 2721 56.5 52.7 42.7 36.5CoTTA* 6418 56.7 56.7 56.7 56.7Ours 918(66, 85%↓) 55.2 55.4 55.4 55.4Ours* 918(66, 85%↓) 56.7 56.8 56.9 56.9 Table 12. Further experiments in semantic segmentation. We represent the results based on mean intersection over union (mIoU). * means that the method utilizes the same cross-entropy consistency loss. The values in parentheses refer to memory re- duction rates compared to TENT/EATA and CoTTA, sequentially. #Partitions WRN-28 (12) WRN-40 (18) ResNet-50 (16) K=4 2,2,4,4 3,3,6,6 3,3,5,5 K=5 2,2,2,2,4 3,3,3,3,6 2,2,4,4,4 Table 13. Details of # of blocks of each partition. The list of numbers denotes the number of residual blocks for each part of the original networks, from the shallow to the deep parts sequentially. The values in parentheses are the total number of residual blocks. in TENT [65] and EATA [50]. Thus, our method can be used with various adaptation losses. Moreover, even though our self-distilled regularization can be regarded as a teacher-student distillation from original networks to meta networks, it does not require a large activation size or the storage of an extra source model, unlike CoTTA [66]. In addition to the results in Table 6, we improve the segmentation experiments by comparing our approach with CoTTA [66]. As we aforementioned, our approach has scalability with diverse adaptation loss. Thus, as shown in Table 12, we additionally apply cross-entropy consis- tency loss* with multi-scaling input as proposed in CoTTA, where we use the multi-scale factors of [0.5, 1.0, 1.5, 2.0] and flip. Our method not only achieves comparable per- formance with 85% less memory than CoTTA, but shows consistent performance even for multiple rounds while con- tinual TENT [65] suffers from the error accumulation effect. C. Further implementation details Partition of a pre-trained model. As illustrated in Fig. 3, the given pre-trained model consists of three parts: clas- sifier, encoder, and input conv, where the encoder denotes layer1 to 4 in the case of ResNet. Our method is applied to the encoder and we divide it into K parts. Table 13 describes the details of the number of residual blocks for each part of the encoder. Our method is designed to divide the shallow layers more (i.e., densely) than the deep layers, improving the TTA performance as shown in Table 4c. Convolution layer in meta networks. As the hyperparam- eters of the convolution layer1, we set the bias to false and 1https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html (K=4) Kernel size= 1, padding=0Kernel size=3, padding=1 Arch Avg. err params↑ Mem. Avg. err params↑ Mem. WRN-28 17.2 0.8% 396 16.9 9.5% 404 WRN-40 12.4 0.6% 80 12.2 6.4% 80 ResNet-50 14.4 11.8% 296 14.2 142.2% 394 Table 14. Kernel size in the conv layer.We report the average er- ror rate (%), the increase rate of the model parameters compared to the original model (%), and the total memory consumption (MB) including the model and activation sizes, based on the kernel size of the conv layer in meta networks. (K=4) Transformations Dataset Arch EATA [50]None +Color +Blur +Gray CIFAR10-C WRN-40 13.0 12.5 12.3 12.3 12.2 CIFAR10-C WRN-28 18.6 17.8 17.4 17.2 16.9 CIFAR100-C WRN-4037.1 36.9 36.7 36.6 36.4 Table 15. Ablation of the combination of transformations. To warm up the meta networks, we use the following transformations in Pytorch: ColorJitter (Color), GaussianBlur (Blur), and Ran- domGrayscale (Gray). We report the average error rate (%). the stride to two if the corresponding part of the encoder in- cludes the stride of two; otherwise, one. As shown in the gray area in Table 14, we conduct experiments by modify- ing the kernel size and padding for each architecture. To be more specific, we obtain better performances by setting the kernel size to three with WideResNet (with 10% additional number of model parameters). On the other hand, utilizing the kernel size of three with ResNet leads to significant in- creases in parameters and memory sizes. Thus, we use one and three as the kernel size with ResNet and WideResNet, respectively. Warming up meta networks. Before the model deploy- ment, we warm up meta networks with the source data by applying the following transformations, which prevent the meta networks from being overfitted to the source domain. Regardless of the pre-trained model’s architecture and pre-training method, we use the same transformations to warm up meta networks. Even for WideResNet-40 pre- trained with AugMix [26], a strong data augmentation tech- nique, the following simple transformations are enough to warm up the meta networks. In addition, we provide the ablation of the combination of transformations in Table 15. from t o r c h v i s i o nimport t r a n s f o r m s a s T TRANSFORMS = t o r c h . nn . S e q u e n t i a l ( RandomApply ( T . C o l o r J i t t e r ( 0 . 4 , 0 . 4 , 0 . 4 , 0 . 1 ) , p = 0 . 4 ) RandomApply ( T . G a u s s i a n B l u r ( ( 3 , 3 ) , p = 0 . 2 ) T . RandomGrayscale ( P = 0 . 1 ) ) Semantic segmentation. For semantic segmentation exper- iments, we utilize ResNet-50-based DeepLabV3+ [7] from RobustNet repository2 [8]. We warm up the meta networks on the train set of Cityscapes [10] with SGD optimizer with the learning rate of 5e-2 and the epoch of 5. Image trans- 2https://github.com/shachoi/RobustNet 14: freeze : update ℒ 𝜕ℒ 𝜕ℒ 𝜕ℒ 𝜕𝑓𝑖+1 𝜕ℒ 𝜕𝑓𝑖 𝜕ℒ 𝜕𝑓𝑖−1 𝑓𝑖−1 𝑓𝑖 𝑓𝑖+1 𝑊1,𝑏1 𝑊2,𝑏2 𝑊3,𝑏3 𝐿1 𝐿2 𝐿3 Appendix entropy min. 𝐷𝑡 ෤𝑥𝑘−1 𝑥𝑘 bn ෤𝑥𝑘 relu bn conv Affine tra. Standard. ① ② ③ ④ ⑤ ⑥ � (a) Visualization of meta networks (K=5) CIFAR10-CWRN-28CIFAR10-CWRN-40CIFAR100-CWRN-40Variants1 2 3 4 5 6 7I ✓ ✓ ✓ ✓ ✓19.9 15.4 39.2II ✓ ✓ ✓ ✓ ✓18.6 13.4 38.0III ✓ ✓ ✓ ✓18.7 13.7 38.2IV ✓ ✓ ✓ ✓ ✓18.6 12.4 36.7V ✓ ✓ ✓ ✓ ✓19.8 12.9 37.2VI ✓ ✓ ✓ ✓ 32.3 14.5 51.8VII✓ ✓ ✓ 20.7 14.9 40.1XIII✓ ✓ ✓ ✓ ✓ ✓18.1 12.6 37.2IX ✓ ✓ ✓ ✓60.6 73.3 77.2Ours✓✓✓✓✓ ✓ 16.8 12.1 36.3 (b) Comparison of average error rate (%) on continual TTA setup (K=5) Table 16. Components of meta networks. We conduct an ablation study on components of meta networks ( i.e., 1⃝ ∼7⃝). Here, 1⃝ and 2⃝ refer to affine transformation and standardization in a BN layer after the original networks. 3⃝∼ 5⃝ and 6⃝∼ 7⃝, respectively, indicate modules in a convolution block and two kinds of inputs of it. In table (b), ✓ means applying the component to meta networks. formations follow the implementation details of [8]. After model deployment, we perform TTA using SGD optimizer with the learning rate of 1e-5, the image size of 1600×800, the batch size of 2, and the importance of regularizationλ of 2. The main loss for adaptation is same as Lent in Eq. (2). D. Additional ablations Main task loss for adaptation. To adapt to the target do- main effectively, selecting the main task loss for adaptation is a non-trivial problem. So, we conduct a comparative experiment on three types of adaptation loss: L1) entropy minimization [19], L2) entropy minimization with mean en- tropy maximization [37], and L3) filtering samples using entropy minimization [50]. With a mini-batch of N test im- ages, the three adaptation losses are formulated as follows: L1 = 1 N NX i=1 H(ˆyi), (5) L2 = λm1 1 N NX i=1 H(ˆyi) − λm2 H(y), (6) L3 = 1 N NX i=1 I{H(ˆyi)<H0} · H(ˆyi), (7) where ˆyi is the logits output of i-th test data, y = 1 N PN i=1 p(ˆyi), H(y) = −P C p(y) logp(y), p( ·) is the softmax function, C is the number of classes, and I{·} is an indicator function. λm1 and λm2 indicate the importance of each term in Eq. (6) which are set to 0.2 and 0.25, respec- tively, following SWR&NSP [9]. The entropy thresholdH0 is set to 0.4 × ln C following EATA [50]. The results are described in Table 17. Particularly, apply- ing any of the three losses, our method achieves comparable performance to EATA. Among them, using L3 of Eq. (7) achieves the lowest error rate in most cases. Therefore, we apply L3 to our approach as mentioned in Section 3.1. Components of meta networks. As shown in Table 16, we (K=5) Ours Dataset Arch EATA[50] L1 L2 L3 CIFAR10-C WRN-28 18.6 17.3 16.9 16.9 WRN-40 13.0 12.2 12.3 12.1 Resnet-50 14.2 15.0 14.3 14.1 CIFAR100-C WRN-40 37.1 36.5 36.4 36.3 Resnet-50 39.9 40.7 38.8 39.4 Table 17. Ablation study of main task loss. We compare the average error rate (%) of three types of adaptation losses. (K=5) Ours Dataset Arch MSE loss (Eq. (8))L1 loss (Eq. (4)) CIFAR10-C WRN-28 16.9 16.9 WRN-40 12.3 12.1 Resnet-50 14.1 14.1 CIFAR100-CWRN-40 36.6 36.3 Resnet-50 39.5 39.4 Table 18. Ablation study of loss function of our regularization. We present the average error (%) according to two types of loss functions for self-distilled regularization. conduct an ablation study on each element of our proposed meta networks. We observe that the affine transformation is more critical than standardization in a BN layer after the original networks. Specifically, removing the standardiza- tion (variant II) causes less performance drop than remov- ing the affine transformation (variant I). In addition, using only a conv layer in conv block (variant VI) also cause per- formance degradation, so it is crucial to use the ReLU and BN layers together in the conv block. Loss function choice of our regularization.As mentioned in Section 3.2, self-distilled regularization loss computes the mean absolute error ( i.e., L1 loss) of Eq. (4). This loss regularizes the output ˜xk of each k-th group of the meta networks not to deviate from the outputxk of each k-th part of frozen original networks. The mean squared error ( i.e., MSE loss) also can be used to get a similar effect which is defined as: MSE = (˜xk − xk)2. (8) 15We compare two kinds of loss functions for our regular- ization in Table 18. By observing a marginal performance difference, our method is robust to the loss function choice. Robustness to the importance of regularization λ. We show that our method is robust to the regularization term λ. We conduct experiments using a wide range of λ as shown in Fig. 5 and the following table. Round\\λ 0 0.1 0.5 1 2 5 10 1 36.31 36.30 36.29 36.56 37.20 38.41 39.58 10 55.47 43.83 36.42 36.14 36.48 37.47 38.95 The experiments are performed with WideResNet-40 on CIFAR100-C. When λ is changed from 0.5 to 1, the per- formance difference was only 0.27% in the first round. We also test λ to be extremely large ( e.g., 5, 8, and 10). Since setting λ to 10 may mean that we hardly adapt the meta net- works to the target domain, the error rate (39.58%) with λ of 10 was close to the one (41.1%) of BN Stats Adapt [49]. E. Baseline details E.1. TTA works We refer to the baselines for which the code was of- ficially released: TENT 3, TTT++4, CoTTA5, EATA6, and NOTE7. We did experiments on their code by adding the needed data loader or pre-trained model loader. In this sec- tion, implementation details of the baselines are provided. BN Stats Adapt [49] is one of the non-training TTA ap- proaches. It can be implemented by setting the model to the train mode8 of Pytorch [54] during TTA. TTT+++ [42] was originally implemented as the offline adaptation, i.e., multi-epoch training. So, we modified their setup to continual TTA. We further tuned the learn- ing rate as 0.005 and 0.00025 for adapting to CIFAR10-C and CIFAR100-C, respectively. NOTE [17] proposed the methods named IABN and PBRS with taking account of temporally correlated target data. However, our experiments were conducted with target data that was independent and identically distributed (i.i.d.). Hence, we adapted NOTE-i.i.d ( i.e., NOTE* in their git repository), which is a combination of TENT [65] and IABN without using PBRS. We fine-tuned the α of their main paper ( i.e., self.k in the code 9) to 8 and the learning rate to 1e-5. Others (e.g., TENT [65], SWR&NSP [9], CoTTA [66], and EATA [50]). We utilized the best hyperparameters specified in their paper and code. In the case where the batch size of 3https://github.com/DequanWang/tent 4https://github.com/vita-epfl/ttt-plus-plus 5https://github.com/qinenergy/cotta 6https://github.com/mr-eggplant/EATA 7https://github.com/TaesikGong/NOTE 8pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train 9https://github.com/TaesikGong/NOTE/blob/main/utils/iabn.py their works (e.g., 200 and 256) differs from one for our ex- periments (e.g., 64), we decreased the learning rate linearly based on the batch size [18]. AdaptBN [55]. We set the hyperparameter N of their main paper to 8. When AdaptBN is employed alongside TENT or our approach, we set the learning rate to 1e-5 or 5e-6 [40]. E.2. On-device learning works To unify the backbone network as ResNet-50 [24], we reproduced the following works by referencing their paper and published code: TinyTL 10, Rep-Net11, and AuxAdapt. This section presents additional implementation details for reproducing the above three works. TinyTL [4]. We attach the LiteResidualModules 12 to layer1 to 4 in the case of ResNet-50 13. As the hyperpa- rameters of the LiteResidualModules, the hyperparameter expand is set to 4 while the other hyperparameters follow the default values. Rep-Net [69]. We divide the encoder of ResNet-50 into six parts, as each part of the encoder has 2,2,3,3,3,3 resid- ual blocks (e.g., BasicBlock or Bottleneck in Pytorch) from the shallow to the deep parts sequentially. Then, we connect the ProgramModules14 to each corresponding part of the en- coder. For the ProgramModule, we set the hyperparameter expand to 4 while the rest hyperparameters are used as their default values. We copy the input conv of ResNet-50 and make use of it as the input conv of Rep-Net. AuxAdapt [73]. We use ResNet-18 as the AuxNet. We create pseudo labels by fusing the logits output of ResNet- 50 and ResNet-18, and optimize all parameters of ResNet- 18 using the pseudo labels with cross-entropy loss. Warming up the additional modules. Before model de- ployment, we pre-train the additional modules ( i.e., the LiteResidualModule of TinyTL [4], the ProgramModule of Rep-Net [69], and the AuxNet of AuxAdapt [73]) on the source data using the same strategy warming up the meta networks as mentioned in Section C. F. Results of all corruptions We report the error rates (%) of all corruptions on con- tinual TTA and memory consumption (MB) including the model parameters and activations in Table 19 and Table 20. These tables contain additional details to Table 1. 10https://github.com/mit-han-lab/tinyml/tree/master/tinytl 11https://github.com/ASU-ESIC-FAN-Lab/RepNet 12https://github.com/mit-han-lab/tinyml/blob/master/tinytl/tinytl/model/modules.py 13https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py 14github.com/ASU-ESIC-FAN-Lab/RepNet/blob/master/repnet/model/reprogram.py 16Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − →Arch Method Gaus. Shot Impu. Defo. Glas. Moti. Zoom Snow Fros. Fog Brig. Cont. Elas. Pixe. JpegAvg. err Mem. WRN-40(AugMix) Source 44.3 37.0 44.8 30.6 43.9 32.6 29.4 23.9 30.1 39.7 12.9 66.4 32.7 58.4 23.5 36.7 11tBN [49] 19.5 17.6 23.8 9.6 23.1 11.1 10.3 13.4 14.2 15.0 8.0 13.9 17.3 16.0 18.8 15.4 11Single do. TENT [65]16.4 13.9 19.1 8.3 19.1 9.3 8.6 10.9 11.3 12.0 6.9 11.6 14.6 12.2 15.6 12.7 188TENT continual [65]16.4 12.2 17.1 9.1 18.7 11.4 10.4 12.7 12.4 14.8 10.1 13.0 17.0 13.3 19.0 13.3 188TTT++ [42] 19.1 16.9 22.2 9.3 21.6 10.8 9.8 12.7 13.1 14.3 7.8 13.9 15.9 14.2 17.2 14.6 391SWRNSP [9] 15.9 13.3 18.2 8.4 18.5 9.5 8.6 11.0 10.2 11.7 7.0 8.1 14.6 11.3 15.1 12.1 400NOTE [17] 19.6 16.4 19.9 9.4 20.3 10.3 10.1 11.6 10.6 13.3 7.9 7.7 15.4 12.0 17.3 13.4 188EATA [50] 15.2 13.1 17.5 9.5 19.9 11.6 9.3 11.4 11.5 12.4 7.8 11.1 16.1 12.2 16.1 13.0 188CoTTA [66] 15.6 13.6 17.3 9.8 19.0 11.0 10.2 13.5 12.6 17.4 7.8 17.3 16.2 12.9 16.0 14.0 409Ours (K=4) 16.1 13.2 18.3 8.0 18.3 9.3 8.6 10.5 10.1 12.2 6.8 11.3 14.5 11.0 14.8 12.2 80Ours (K=5) 15.9 12.6 17.2 8.2 18.4 9.3 8.6 10.6 10.4 12.4 6.7 11.7 14.3 11.3 14.9 12.1 92 WRN-28 Source 72.3 65.7 72.9 46.9 54.3 34.8 42.0 25.1 41.3 26.0 9.3 46.7 26.6 58.5 30.3 43.5 58tBN [49] 28.6 26.8 37.0 13.2 35.4 14.4 12.6 18.0 18.2 16.0 8.6 13.3 24.0 20.3 27.8 20.9 58Single do. TENT [65]25.2 23.8 33.5 12.8 32.3 14.1 11.7 16.4 17.0 14.4 8.4 12.2 22.8 18.0 24.8 19.2 646Continual TENT [65]25.2 20.8 29.8 14.4 31.5 15.4 14.2 18.8 17.5 17.3 10.9 14.9 23.6 20.2 25.6 20.0 646TTT++ [42] 27.9 25.8 35.8 13.0 34.3 14.2 12.2 17.4 17.6 15.5 8.6 13.1 23.1 19.6 26.6 20.3 1405SWRNSP [9] 24.6 20.5 29.3 12.4 31.1 13.0 11.3 15.3 14.7 11.7 7.8 9.3 21.5 15.6 20.3 17.2 1551NOTE [17] 30.4 26.7 34.6 13.6 36.3 13.7 13.9 17.2 15.8 15.2 9.1 7.5 24.1 18.4 25.9 20.2 646EATA [50] 23.8 18.8 27.3 13.9 29.7 16.0 13.3 18.0 16.9 15.7 10.5 12.2 22.9 17.1 23.0 18.6 646CoTTA [66] 24.6 21.6 26.5 12.1 28.0 13.0 10.9 15.3 14.6 13.6 8.1 12.2 20.0 14.9 19.5 17.0 1697Ours (K=4) 23.5 19.0 26.6 11.5 30.6 13.1 10.9 15.2 14.5 13.1 7.8 11.4 20.9 15.4 20.8 16.9 404Ours (K=5) 23.8 18.7 25.7 11.5 29.8 13.3 11.3 15.3 15.0 13.0 7.9 11.3 20.2 15.1 20.5 16.8 471 Resnet-50 Source 65.6 60.7 74.4 28.9 79.9 46.0 25.7 35.0 49.4 54.7 13.0 83.2 41.2 46.7 27.7 48.8 91tBN [49] 18.0 17.2 29.3 10.7 27.2 15.5 8.9 16.7 14.6 21.0 9.3 12.7 20.9 12.4 14.8 16.6 91Single do. TENT [65]16.6 15.7 25.7 10.0 24.8 13.8 8.3 14.9 13.8 17.6 8.7 10.0 19.1 11.5 13.8 15.0 925TENT continual [65]16.6 14.4 22.9 10.4 22.6 13.4 10.3 15.8 14.6 18.0 10.5 11.7 18.4 13.1 15.3 15.2 925TTT++ [42] 18.2 16.9 28.7 10.5 26.5 14.5 8.9 16.5 14.5 20.9 9.0 9.0 20.4 12.3 14.7 16.1 1877SWRNSP [9] 17.3 16.1 26.1 10.6 25.6 14.1 8.7 15.6 13.6 18.6 8.8 10.0 19.3 12.0 14.2 15.4 1971EATA [50] 17.2 14.9 23.6 10.2 23.3 13.2 8.5 14.0 12.5 16.6 8.6 9.4 17.2 11.0 12.7 14.2 925CoTTA [66] 16.2 15.0 21.2 10.4 22.8 13.9 8.4 15.1 12.9 19.8 8.6 11.3 17.5 10.5 12.2 14.4 2066Ours (K=4) 16.5 14.5 24.3 9.7 23.7 13.3 8.8 14.7 12.9 17.0 9.1 9.4 17.6 11.4 13.1 14.4 296Ours (K=5) 16.6 14.4 23.6 9.8 23.4 12.7 8.6 14.5 12.6 16.6 8.7 9.0 17.0 11.3 12.6 14.1 498 Table 19. Comparison of error rate (%) on CIFARC10-C with severity level 5. We conduct experiments on continual TTA setup. Avg. err means the average error rate (%) of all 15 corruptions, and Mem. denotes total memory consumption, including model parameter sizes and activations. WRN refers to WideResNet. The implementation details of the baselines are described in Section E.1. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − →Arch Method Gaus. Shot Impu. Defo. Glas. Moti. Zoom Snow Fros. Fog Brig. Cont. Elas. Pixe. JpegAvg. err Mem. WRN-40(AugMix) Source 80.1 77.0 76.4 59.9 77.6 64.2 59.3 64.8 71.3 78.3 48.1 83.4 65.8 80.4 59.2 69.7 11tBN [49] 45.9 45.6 48.2 33.6 47.9 34.5 34.1 40.3 40.4 47.1 31.7 39.7 42.7 39.2 45.6 41.1 11Single do. TENT [65]41.2 40.6 42.2 30.9 43.4 31.8 30.6 35.3 36.2 40.1 28.5 35.5 39.1 33.9 41.7 36.7 188continual TENT [65]41.2 38.2 41.0 32.9 43.9 34.9 33.2 37.7 37.2 41.5 33.2 37.2 41.1 35.9 45.1 38.3 188TTT++ [42] 46.0 45.4 48.2 33.5 47.7 34.4 33.8 39.9 40.2 47.1 31.8 39.7 42.5 38.9 45.5 41.0 391SWRNSP [9] 42.4 40.9 42.7 30.6 43.9 31.7 31.3 36.1 36.2 41.5 28.7 34.1 39.2 33.6 41.3 36.6 400NOTE [17] 50.9 47.4 49.0 37.3 49.6 37.3 37.0 41.3 39.9 47.0 35.2 34.7 45.2 40.9 49.9 42.8 188EATA [50] 41.6 39.9 41.2 31.7 44.0 32.4 31.9 36.2 36.8 39.7 29.1 34.4 39.9 34.2 42.2 37.1 188CoTTA [66] 43.5 41.7 43.7 32.2 43.7 32.8 32.2 38.5 37.6 45.9 29.0 38.1 39.2 33.8 39.4 38.1 409Ours (K=4) 42.7 39.6 42.4 31.4 42.9 31.9 30.8 35.1 34.8 40.7 28.1 35.0 37.5 32.1 40.5 36.4 80Ours (K=5) 41.8 39.0 41.9 31.2 42.7 32.5 31.0 35.0 35.0 39.9 28.8 34.5 37.5 32.8 40.5 36.3 92 Resnet-50 Source 84.7 83.5 93.3 59.6 92.5 71.9 54.8 66.6 77.6 81.8 44.3 91.2 72.2 76.6 56.5 73.8 91tBN [49] 48.1 46.7 60.6 35.1 58.0 41.8 33.2 47.3 43.5 54.9 33.5 35.3 49.8 38.4 40.8 44.5 91Single do. TENT [65]44.1 42.7 53.9 32.6 52.0 37.5 30.5 43.4 40.2 45.7 30.4 31.4 45.1 35.0 37.6 40.1 926continual TENT [65]44.0 40.1 49.9 34.7 50.6 40.0 33.6 47.0 45.7 53.4 42.5 46.2 56.1 51.2 53.3 45.9 926TTT++ [42] 48.1 46.5 60.8 35.1 57.8 41.6 32.9 46.8 43.3 55.0 33.3 34.0 50.0 38.1 40.6 44.2 1876SWRNSP [9] 48.3 46.5 60.5 35.1 57.9 41.7 32.9 47.1 43.5 54.7 33.5 35.1 49.9 38.3 40.7 44.1 1970EATA [50] 44.8 41.9 52.6 33.0 51.1 37.8 30.3 43.0 40.1 45.1 30.1 31.8 45.2 35.2 37.4 39.9 926CoTTA [66] 43.6 42.8 50.4 34.2 51.6 39.2 31.4 43.4 39.6 47.4 31.3 32.2 43.4 35.8 36.7 40.2 2064Ours (K=4) 44.8 40.3 49.2 32.3 50.1 36.3 29.5 41.0 39.9 44.6 31.5 33.7 45.3 36.3 37.7 39.5 296Ours (K=5) 44.9 40.4 48.9 32.7 49.7 36.9 29.3 40.8 39.0 44.4 31.1 33.6 44.0 35.7 37.8 39.3 498 Table 20. Comparison of error rate (%) on CIFARC100-C with severity level 5.We conduct experiments on continual TTA setup. Avg. err means the average error rate (%) of all 15 corruptions, and Mem. denotes total memory consumption, including model parameter sizes and activations. WRN refers to WideResNet. The implementation details of the baselines are described in Section E.1. 17",
      "meta_data": {
        "arxiv_id": "2303.01904v4",
        "authors": [
          "Junha Song",
          "Jungsoo Lee",
          "In So Kweon",
          "Sungha Choi"
        ],
        "published_date": "2023-03-03T13:05:30Z",
        "pdf_url": "https://arxiv.org/pdf/2303.01904v4.pdf",
        "github_url": "https://github.com/qinenergy/cotta"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper introduces EcoTTA, a memory-efficient approach for continual Test-Time Adaptation (TTA) that addresses performance degradation due to domain shifts and limitations of on-device memory. The main contributions are: 1) A novel architecture with lightweight meta networks attached to frozen original networks, significantly reducing memory consumption (up to 86% less than state-of-the-art CoTTA) by minimizing intermediate activation sizes during backpropagation. 2) A self-distilled regularization method that controls meta network outputs to align with the frozen original networks' outputs, effectively preserving source knowledge and preventing catastrophic forgetting and error accumulation in long-term adaptation. 3) Demonstrated superior performance and memory efficiency over existing state-of-the-art methods across various benchmarks for image classification (CIFAR-C, ImageNet-C) and semantic segmentation (Cityscapes with weather corruption) tasks.",
        "methodology": "EcoTTA employs a two-component strategy. First, it uses a memory-efficient architecture consisting of a frozen original network and newly attached lightweight meta networks. The encoder of the pre-trained original model is divided into K parts (e.g., K=4 or 5), with shallow layers partitioned more densely. Each meta network group comprises a batch normalization layer and a convolution block (Conv-BN-ReLU). Before deployment, these meta networks are warmed up on the source dataset for a small number of epochs using cross-entropy loss, while the original networks remain frozen. During test-time adaptation, only the lightweight meta networks are updated, further freezing the original networks to discard memory-intensive intermediate activations. Second, a self-distilled regularization loss, formulated as the Mean Absolute Error (L1 loss) between the output of the meta networks and the corresponding output of the frozen original networks, is applied. This regularization runs in parallel with an unsupervised adaptation loss, typically entropy minimization for samples with entropy below a predefined threshold, to prevent catastrophic forgetting and error accumulation. The overall loss combines these two components using a balancing coefficient lambda.",
        "experimental_setup": "Experiments were conducted on continual TTA and long-term TTA setups. For image classification, CIFAR10-C, CIFAR100-C, and ImageNet-C datasets were used, featuring 15 corruption types with 5 severity levels. Models evaluated included WideResNet-28, WideResNet-40 (pre-trained with AugMix), and ResNet-50. For semantic segmentation, Cityscapes-C (Cityscapes validation set with weather corruptions) was used with a ResNet-50-based DeepLabV3+. Baselines included TENT, EATA, CoTTA, TTT++, SWR&NSP, NOTE, BN Stats Adapt, and on-device learning methods like TinyTL, RepNet, AuxAdapt. The training utilizes SGD optimizer, with learning rates of 5e-2 for warm-up and 5e-3 for TTA. Batch sizes were 64 for CIFAR, 32 for ImageNet, and 2 for segmentation. The entropy threshold H0 was set to 0.4 × ln C, and the regularization weight lambda to 0.5. Performance was measured by error rates (classification), mIoU (segmentation), and total memory consumption (parameters + activations). The number of model partitions K was ablated and set to 4 or 5. Ablations were also performed on meta network design, kernel size, transformations for warm-up, main task loss, regularization loss function, and robustness to lambda.",
        "limitations": "The proposed method requires access to the source dataset for a small number of epochs to warm up the newly attached meta networks before model deployment. While this is acceptable in scenarios where the source data is publicly available or the model owner adapts it, it is still a prerequisite. The paper also notes that the current wall-clock time for computational efficiency, particularly for backpropagation, is not fully optimized due to limitations in current ML frameworks like PyTorch, though theoretical efficiency is higher. Additionally, increasing the model partition factor K beyond empirically optimal values (e.g., 4 or 5) resulted in performance saturation despite increased memory and parameter usage. Attention mechanisms like CBAM and SE, when integrated, did not significantly boost TTA performance in the unsupervised learning setup.",
        "future_research_directions": "Future research could focus on further optimizing the computational efficiency of the framework through fine-grained ML framework implementations. The architecture is adaptable to various adaptation losses, suggesting further exploration with different unsupervised loss functions beyond entropy minimization. The authors also hope their work will facilitate studies making test-time adaptation feasible for a wider range of edge devices and applications. Reducing or eliminating the dependency on source data for the meta network warm-up phase could also be a valuable direction for increasing applicability.",
        "experimental_code": "import torch\nimport torch.nn as nn\nimport torch.jit\nimport torch.optim as optim\nfrom copy import deepcopy\nimport PIL\nimport torchvision.transforms as transforms\nimport torchvision.transforms.functional as F\nfrom numpy import random\n\n# --- Custom Transforms from my_transforms.py ---\nclass GaussianNoise(torch.nn.Module):\n    def __init__(self, mean=0., std=1.):\n        super().__init__()\n        self.std = std\n        self.mean = mean\n\n    def forward(self, img):\n        noise = torch.randn(img.size()) * self.std + self.mean\n        noise = noise.to(img.device)\n        return img + noise\n\nclass Clip(torch.nn.Module):\n    def __init__(self, min_val=0., max_val=1.):\n        super().__init__()\n        self.min_val = min_val\n        self.max_val = max_val\n\n    def forward(self, img):\n        return torch.clip(img, self.min_val, self.max_val)\n\nclass ColorJitterPro(transforms.ColorJitter):\n    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0, gamma=0):\n        super().__init__(brightness, contrast, saturation, hue)\n        self.gamma = self._check_input(gamma, 'gamma')\n\n    def forward(self, img):\n        fn_idx = torch.randperm(5)\n        for fn_id in fn_idx:\n            if fn_id == 0 and self.brightness is not None:\n                brightness = self.brightness\n                brightness_factor = torch.tensor(1.0).uniform_(brightness[0], brightness[1]).item()\n                img = F.adjust_brightness(img, brightness_factor)\n\n            if fn_id == 1 and self.contrast is not None:\n                contrast = self.contrast\n                contrast_factor = torch.tensor(1.0).uniform_(contrast[0], contrast[1]).item()\n                img = F.adjust_contrast(img, contrast_factor)\n\n            if fn_id == 2 and self.saturation is not None:\n                saturation = self.saturation\n                saturation_factor = torch.tensor(1.0).uniform_(saturation[0], saturation[1]).item()\n                img = F.adjust_saturation(img, saturation_factor)\n\n            if fn_id == 3 and self.hue is not None:\n                hue = self.hue\n                hue_factor = torch.tensor(1.0).uniform_(hue[0], hue[1]).item()\n                img = F.adjust_hue(img, hue_factor)\n\n            if fn_id == 4 and self.gamma is not None:\n                gamma = self.gamma\n                gamma_factor = torch.tensor(1.0).uniform_(gamma[0], gamma[1]).item()\n                img = img.clamp(1e-8, 1.0) # to fix Nan values in gradients, which happens when applying gamma after contrast\n                img = F.adjust_gamma(img, gamma_factor)\n        return img\n\n# --- CoTTA Implementation from cotta.py ---\n\ndef get_tta_transforms(gaussian_std: float=0.005, soft=False, clip_inputs=False):\n    img_shape = (32, 32, 3) # Note: For ImageNet, img_shape is (224, 224, 3)\n    n_pixels = img_shape[0]\n\n    clip_min, clip_max = 0.0, 1.0\n\n    p_hflip = 0.5\n\n    tta_transforms = transforms.Compose([\n        Clip(0.0, 1.0), \n        ColorJitterPro(\n            brightness=[0.8, 1.2] if soft else [0.6, 1.4],\n            contrast=[0.85, 1.15] if soft else [0.7, 1.3],\n            saturation=[0.75, 1.25] if soft else [0.5, 1.5],\n            hue=[-0.03, 0.03] if soft else [-0.06, 0.06],\n            gamma=[0.85, 1.15] if soft else [0.7, 1.3]\n        ),\n        transforms.Pad(padding=int(n_pixels / 2), padding_mode='edge'),  \n        transforms.RandomAffine(\n            degrees=[-8, 8] if soft else [-15, 15],\n            translate=(1/16, 1/16),\n            scale=(0.95, 1.05) if soft else (0.9, 1.1),\n            shear=None,\n            resample=PIL.Image.BILINEAR,\n            fillcolor=None\n        ),\n        transforms.GaussianBlur(kernel_size=5, sigma=[0.001, 0.25] if soft else [0.001, 0.5]),\n        transforms.CenterCrop(size=n_pixels),\n        transforms.RandomHorizontalFlip(p=p_hflip),\n        GaussianNoise(0, gaussian_std),\n        Clip(clip_min, clip_max)\n    ])\n    return tta_transforms\n\n\ndef update_ema_variables(ema_model, model, alpha_teacher):\n    for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n        ema_param.data[:] = alpha_teacher * ema_param[:].data[:] + (1 - alpha_teacher) * param[:].data[:]\n    return ema_model\n\n\nclass CoTTA(nn.Module):\n    \"\"\"CoTTA adapts a model by entropy minimization during testing.Once tented, a model adapts itself by updating on every forward.\"\"\"\n    def __init__(self, model, optimizer, steps=1, episodic=False, mt_alpha=0.99, rst_m=0.1, ap=0.9):\n        super().__init__()\n        self.model = model\n        self.optimizer = optimizer\n        self.steps = steps\n        assert steps > 0, \"cotta requires >= 1 step(s) to forward and update\"\n        self.episodic = episodic\n        self.model_state, self.optimizer_state, self.model_ema, self.model_anchor = copy_model_and_optimizer(self.model, self.optimizer)\n        self.transform = get_tta_transforms()    \n        self.mt = mt_alpha\n        self.rst = rst_m\n        self.ap = ap\n\n    def forward(self, x):\n        if self.episodic:\n            self.reset()\n\n        for _ in range(self.steps):\n            outputs = self.forward_and_adapt(x, self.model, self.optimizer)\n\n        return outputs\n\n    def reset(self):\n        if self.model_state is None or self.optimizer_state is None:\n            raise Exception(\"cannot reset without saved model/optimizer state\")\n        load_model_and_optimizer(self.model, self.optimizer,\n                                 self.model_state, self.optimizer_state)\n        self.model_state, self.optimizer_state, self.model_ema, self.model_anchor = copy_model_and_optimizer(self.model, self.optimizer)\n\n    @torch.enable_grad() # ensure grads in possible no grad context for testing\n    def forward_and_adapt(self, x, model, optimizer):\n        outputs = self.model(x)\n        # Teacher Prediction\n        anchor_prob = torch.nn.functional.softmax(self.model_anchor(x), dim=1).max(1)[0]\n        standard_ema = self.model_ema(x)\n        # Augmentation-averaged Prediction\n        N = 32 \n        outputs_emas = []\n        for i in range(N):\n            outputs_  = self.model_ema(self.transform(x)).detach()\n            outputs_emas.append(outputs_)\n        # Threshold choice discussed in supplementary\n        if anchor_prob.mean(0)<self.ap:\n            outputs_ema = torch.stack(outputs_emas).mean(0)\n        else:\n            outputs_ema = standard_ema\n        # Student update\n        loss = (softmax_entropy(outputs, outputs_ema)).mean(0) \n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        # Teacher update\n        self.model_ema = update_ema_variables(ema_model = self.model_ema, model = self.model, alpha_teacher=self.mt)\n        # Stochastic restore\n        if True:\n            for nm, m  in self.model.named_modules():\n                for npp, p in m.named_parameters():\n                    if npp in ['weight', 'bias'] and p.requires_grad:\n                        mask = (torch.rand(p.shape)<self.rst).float().cuda() \n                        with torch.no_grad():\n                            p.data = self.model_state[f\"{nm}.{npp}\"] * mask + p * (1.-mask)\n        return outputs_ema\n\n\n@torch.jit.script\ndef softmax_entropy(x, x_ema):# -> torch.Tensor:\n    \"\"\"Entropy of softmax distribution from logits. (CIFAR implementation)\"\"\"\n    return -(x_ema.softmax(1) * x.log_softmax(1)).sum(1)\n\ndef collect_params(model):\n    \"\"\"Collect all trainable parameters.Walk the model's modules and collect all parameters.Return the parameters and their names.\"\"\"\n    params = []\n    names = []\n    for nm, m in model.named_modules():\n        for np, p in m.named_parameters():\n            if np in ['weight', 'bias'] and p.requires_grad:\n                params.append(p)\n                names.append(f\"{nm}.{np}\")\n    return params, names\n\n\ndef copy_model_and_optimizer(model, optimizer):\n    \"\"\"Copy the model and optimizer states for resetting after adaptation.\"\"\"\n    model_state = deepcopy(model.state_dict())\n    model_anchor = deepcopy(model)\n    optimizer_state = deepcopy(optimizer.state_dict())\n    ema_model = deepcopy(model)\n    for param in ema_model.parameters():\n        param.detach_()\n    return model_state, optimizer_state, ema_model, model_anchor\n\n\ndef load_model_and_optimizer(model, optimizer, model_state, optimizer_state):\n    \"\"\"Restore the model and optimizer states from copies.\"\"\"\n    model.load_state_dict(model_state, strict=True)\n    optimizer.load_state_dict(optimizer_state)\n\n\ndef configure_model(model):\n    \"\"\"Configure model for use with CoTTA.Sets model to train mode, globally disables gradients, then re-enables for all parameters (weights and biases) and forces batch norm to use batch statistics.\"\"\"\n    model.train()\n    model.requires_grad_(False)\n    for m in model.modules():\n        if isinstance(m, nn.BatchNorm2d):\n            m.requires_grad_(True)\n            m.track_running_stats = False\n            m.running_mean = None\n            m.running_var = None\n        else:\n            m.requires_grad_(True)\n    return model\n\n\n# --- Optimizer setup (general for TTA methods) ---\n# The 'cfg' object is assumed to be imported from 'conf.py'\n# and loaded with command-line arguments.\n\ndef setup_optimizer(params, cfg_optim_method, cfg_optim_lr, cfg_optim_beta, cfg_optim_momentum, cfg_optim_dampening, cfg_optim_wd, cfg_optim_nesterov):\n    \"\"\"Set up optimizer for TTA adaptation.\"\"\"\n    if cfg_optim_method == 'Adam':\n        return optim.Adam(params,\n                    lr=cfg_optim_lr,\n                    betas=(cfg_optim_beta, 0.999),\n                    weight_decay=cfg_optim_wd)\n    elif cfg_optim_method == 'SGD':\n        return optim.SGD(params,\n                   lr=cfg_optim_lr,\n                   momentum=cfg_optim_momentum,\n                   dampening=cfg_optim_dampening,\n                   weight_decay=cfg_optim_wd,\n                   nesterov=cfg_optim_nesterov)\n    else:\n        raise NotImplementedError\n\n# --- Example of CoTTA integration (from cifar/cifar100c.py) ---\n\ndef setup_cotta(base_model, cfg_model_episodic, cfg_optim_steps, cfg_optim_mt, cfg_optim_rst, cfg_optim_ap, cfg_optim_method, cfg_optim_lr, cfg_optim_beta, cfg_optim_momentum, cfg_optim_dampening, cfg_optim_wd, cfg_optim_nesterov):\n    \"\"\"Set up CoTTA adaptation for a given base model.\"\"\"\n    model = configure_model(base_model)\n    params, param_names = collect_params(model)\n    optimizer = setup_optimizer(params, cfg_optim_method, cfg_optim_lr, cfg_optim_beta, cfg_optim_momentum, cfg_optim_dampening, cfg_optim_wd, cfg_optim_nesterov)\n    cotta_model = CoTTA(model, optimizer,\n                           steps=cfg_optim_steps,\n                           episodic=cfg_model_episodic, \n                           mt_alpha=cfg_optim_mt, \n                           rst_m=cfg_optim_rst, \n                           ap=cfg_optim_ap)\n    return cotta_model",
        "experimental_info": "EcoTTA (CoTTA) is a test-time adaptation method that uses a two-component strategy. The implementation details extracted from the repository are as follows:1. Model Architecture & Adaptation Scope:- Base Model: Loaded from RobustBench model zoo based on `cfg.MODEL.ARCH` (e.g., 'Standard' WideResNet or ResNet). Preprocessing varies by model/dataset.- Adaptation Scope: Contrary to the method description's 'lightweight meta networks', the code's `configure_model` function globally enables gradients for *all* parameters (weights and biases) of the model by setting `model.requires_grad_(False)` then iterating through all modules and setting `m.requires_grad_(True)`. The `collect_params` function then gathers all these trainable 'weight' and 'bias' parameters for optimization. Thus, the *entire* model is adapted.- Batch Normalization: During adaptation, Batch Normalization layers are explicitly set to `model.train()` mode and forced to use batch-wise statistics (`m.track_running_stats = False`, `m.running_mean = None`, `m.running_var = None`).2. Optimization & Regularization:- Loss Function (Student Update): The core adaptation loss is a form of distillation loss (`softmax_entropy`).    - For CIFAR datasets: `-(x_ema.softmax(1) * x.log_softmax(1)).sum(1)` (KL divergence from teacher's softmax probabilities to student's log-softmax outputs).    - For ImageNet datasets: ` -0.5*(x_ema.softmax(1) * x.log_softmax(1)).sum(1) - 0.5*(x.softmax(1) * x_ema.log_softmax(1)).sum(1)` (symmetric KL divergence). This loss is averaged over the batch.- Teacher Model: An Exponential Moving Average (EMA) of the student model (`self.model_ema`) serves as a teacher. It is updated after each student step with `alpha_teacher = cfg.OPTIM.MT` (default: 0.999).- Stochastic Weight Restoration: After each adaptation step, a 'stochastic restore' mechanism is applied. For each `weight` and `bias` parameter, there is a probability (`cfg.OPTIM.RST`, default: 0.01) that its value is reset to its state before the current test-time adaptation phase (`self.model_state`). This helps prevent catastrophic forgetting.- Augmentation-Averaged Prediction: The teacher's prediction (`outputs_ema`) is conditionally derived. If the mean of the maximum probabilities of the 'anchor model' (original pre-trained model, `self.model_anchor`) output on the input `x` is below a threshold `cfg.OPTIM.AP` (default: 0.92), then `N=32` test-time augmented versions of `x` are passed through the EMA teacher model, and their outputs are averaged to form `outputs_emas`. Otherwise, a single prediction from the EMA teacher on the original `x` is used.- Optimizer: Configurable via `cfg.OPTIM.METHOD` (default: 'Adam', alternatives: 'SGD').    - Adam: `lr=cfg.OPTIM.LR` (default: 1e-3), `betas=(cfg.OPTIM.BETA` (default: 0.9), `0.999`), `weight_decay=cfg.OPTIM.WD` (default: 0.0).    - SGD: `lr=cfg.OPTIM.LR` (default: 1e-3), `momentum=cfg.OPTIM.MOMENTUM` (default: 0.9 for CIFAR, 0.9 for ImageNet), `dampening=cfg.OPTIM.DAMPENING` (default: 0.0), `weight_decay=cfg.OPTIM.WD` (default: 0.0), `nesterov=cfg.OPTIM.NESTEROV` (default: True).- Steps per Batch: `cfg.OPTIM.STEPS` (default: 1) adaptation step per test batch.3. Test-Time Augmentation (TTA):- Transformations (`get_tta_transforms`): A sequence of transformations is applied:    1. Clip inputs to [0.0, 1.0].    2. `ColorJitterPro`: Random adjustments to brightness ([0.8, 1.2] soft / [0.6, 1.4] hard), contrast ([0.85, 1.15] soft / [0.7, 1.3] hard), saturation ([0.75, 1.25] soft / [0.5, 1.5] hard), hue ([-0.03, 0.03] soft / [-0.06, 0.06] hard), and gamma ([0.85, 1.15] soft / [0.7, 1.3] hard). Parameters are chosen randomly from specified ranges.    3. Pad image by `n_pixels / 2` (e.g., 16 for CIFAR, 112 for ImageNet) using `padding_mode='edge'`.    4. `RandomAffine`: Rotations (degrees: [-8, 8] soft / [-15, 15] hard), translations (1/16, 1/16), scaling ([0.95, 1.05] soft / [0.9, 1.1] hard), with `resample=PIL.Image.BILINEAR`.    5. `GaussianBlur`: `kernel_size=5`, `sigma=[0.001, 0.25]` (soft) or `[0.001, 0.5]` (hard).    6. Center crop back to `n_pixels` size.    7. Random horizontal flip (`p=0.5`).    8. `GaussianNoise`: Added with `mean=0` and `std=0.005`.    9. Clip inputs to [0.0, 1.0].4. Evaluation Protocol:- Episodic Adaptation: `cfg.MODEL.EPISODIC` (default: `False`). If `True`, the model state and optimizer state are reset for each forward pass. If `False`, adaptation is continual across batches within the same corruption type and severity, but reset for each new corruption type or severity combination.- Datasets: Evaluation on CIFAR-10-C, CIFAR-100-C, and ImageNet-C.- Corruption Evaluation: The model is evaluated sequentially on each corruption type (`cfg.CORRUPTION.TYPE`) and each severity level (`cfg.CORRUPTION.SEVERITY`).    - CIFAR (Standard): Iterates through severities `[5, 4, 3, 2, 1]` for each corruption type.    - CIFAR (Gradual, `cifar10c_gradual.py`): For the first corruption type, severities are `[5, 4, 3, 2, 1]`. For subsequent corruption types, severities are `[1, 2, 3, 4, 5, 4, 3, 2, 1]` to simulate gradual shifts and then increasing severity.- Number of Samples: `cfg.CORRUPTION.NUM_EX` (e.g., 10000 for CIFAR, 5000 for ImageNet) samples are used per corruption and severity combination.- Batch Size: `cfg.TEST.BATCH_SIZE` (default: 128) for evaluation and adaptation updates."
      }
    },
    {
      "title": "Robust Test-Time Adaptation in Dynamic Scenarios",
      "abstract": "Test-time adaptation (TTA) intends to adapt the pretrained model to test\ndistributions with only unlabeled test data streams. Most of the previous TTA\nmethods have achieved great success on simple test data streams such as\nindependently sampled data from single or multiple distributions. However,\nthese attempts may fail in dynamic scenarios of real-world applications like\nautonomous driving, where the environments gradually change and the test data\nis sampled correlatively over time. In this work, we explore such practical\ntest data streams to deploy the model on the fly, namely practical test-time\nadaptation (PTTA). To do so, we elaborate a Robust Test-Time Adaptation (RoTTA)\nmethod against the complex data stream in PTTA. More specifically, we present a\nrobust batch normalization scheme to estimate the normalization statistics.\nMeanwhile, a memory bank is utilized to sample category-balanced data with\nconsideration of timeliness and uncertainty. Further, to stabilize the training\nprocedure, we develop a time-aware reweighting strategy with a teacher-student\nmodel. Extensive experiments prove that RoTTA enables continual testtime\nadaptation on the correlatively sampled data streams. Our method is easy to\nimplement, making it a good choice for rapid deployment. The code is publicly\navailable at https://github.com/BIT-DA/RoTTA",
      "full_text": "Robust Test-Time Adaptation in Dynamic Scenarios Longhui Yuan Binhui Xie Shuang Li \f School of Computer Science and Technology, Beijing Institute of Technology {longhuiyuan,binhuixie,shuangli}@bit.edu.cn Abstract Test-time adaptation (TTA) intends to adapt the pre- trained model to test distributions with only unlabeled test data streams. Most of the previous TTA methods have achieved great success on simple test data streams such as independently sampled data from single or multiple distri- butions. However, these attempts may fail in dynamic sce- narios of real-world applications like autonomous driving, where the environments gradually change and the test data is sampled correlatively over time. In this work, we ex- plore such practical test data streams to deploy the model on the fly, namely practical test-time adaptation (PTTA). To do so, we elaborate a Robust Test-Time Adaptation (RoTTA) method against the complex data stream in PTTA. More specifically, we present a robust batch normalization scheme to estimate the normalization statistics. Meanwhile, a memory bank is utilized to sample category-balanced data with consideration of timeliness and uncertainty. Further, to stabilize the training procedure, we develop a time-aware reweighting strategy with a teacher-student model. Exten- sive experiments prove that RoTTA enables continual test- time adaptation on the correlatively sampled data streams. Our method is easy to implement, making it a good choice for rapid deployment. The code is publicly available at https://github.com/BIT-DA/RoTTA 1. Introduction In recent years, many machine learning problems have made considerable headway with the success of deep neu- ral networks [13, 22, 33, 38]. Unfortunately, the perfor- mance of deep models drops significantly when training data and testing data come from different distributions [59], which limits their utility in real-world applications. To re- duce the distribution shift, a handful of works focus on transfer learning field [56], in particular, domain adapta- tion (DA) [17, 42, 45, 48, 69, 72] or domain generalization (DG) [40, 41, 52, 71, 83], in which one or more different but \fCorresponding author Test data stream Continual TTANon-i.i.d.TTAPractical  TTACategoryDistribution Fully TTA Correlation samplingDistributionchanging Figure 1. We consider the practical test-time adaptation (TTA) setup and compare it with related ones. First, Fully TTA [70] adapts models on a fixed test distribution with an independently sampled test stream. Then, on this basis, Continual TTA [73] takes the continually changing distributions into consideration. Next, Non-i.i.d. TTA [19] tries to tackle the correlatively sampled test streams on a single test distribution, where the label distribution among a batch of data deviates from that of the test distribution. To be more practical, Practical TTA strives to connect both worlds: distribution changing and correlation sampling. related labeled datasets (a.k.a. source domain) are collected to help the model generalize well to unlabeled or unseen samples in new datasets (a.k.a. target domain). While both DA and DG have extensively studied the problem of distribution shifts, they typically assume acces- sibility to the raw source data. However, in many practical scenarios like personal consumption records, the raw data should not be publicly available due to data protection reg- ulations. Further, existing methods have to perform heavy backward computation, resulting in unbearable training costs. Test-time adaptation (TTA) [3,11,16,24,26,54,65,81] attempts to address the distribution shift online at test time with only unlabeled test data streams. Unequivocally, TTA has drawn widespread attention in a variety of applications, e.g., 2D/3D visual recognition [2, 29, 49, 65, 82], multi- modality [63, 64] and document understanding [15]. Prior TTA studies [7, 20, 70, 73] mostly concentrate on a simple adaptation scenario, where test samples are inde- pendently sampled from a fixed target domain. To name a few, Sun et al. [65] adapt to online test samples drawn from a constant or smoothly changing distribution with an auxil- iary self-supervised task. Wang et al. [70] adapt to a fixed arXiv:2303.13899v1  [cs.CV]  24 Mar 2023Table 1. Comparison between our proposed practical test-time adaptation (PTTA) and related adaptation settings. Setting Adaptation StageAvailable Data Test Data Stream Train Test Source Target Distribution Sampling Protocol Domain Adaptation ! % ! ! - - Domain Generalization ! % ! % - - Test-Time Training [65] ! ! ! ! stationary independently Fully Test-Time Adaptation [70] % ! % ! stationary independently Continual Test-Time Adaptation [73]% ! % ! continually changing independently Non-i.i.d. Test-Time Adaptation [5, 19]% ! % ! stationary correlatively Practical Test-Time Adaptation (Ours)% ! % ! continually changing correlatively target distribution by performing entropy minimization on- line. However, such an assumption is violated when the test environments change frequently [73]. Later on, Boudiaf et al. [5] and Gonget al. [19] consider the temporal correlation ship within test samples. For example, in autonomous driv- ing, test samples are highly correlated over time as the car will follow more vehicles on the highway or will encounter more pedestrians in the streets. More realistically, the data distribution changes as the surrounding environment alerts in weather, location, or other factors. In a word, distribution change and data correlation occur simultaneously in reality. Confronting continually changing distributions, tradi- tional algorithms like pseudo labeling or entropy minimiza- tion become more unreliable as the error gradients cumu- late. Moreover, the high correlation among test samples re- sults in the erroneous estimation of statistics for batch nor- malization and collapse of the model. Driven by this analy- sis, adapting to such data streams will encounter two major obstacles: 1) incorrect estimation in the batch normaliza- tion statistics leads to erroneous predictions of test samples, consequently resulting in invalid adaptation; 2) the model will easily or quickly overfit to the distribution caused by the correlative sampling. Thus, such dynamic scenarios are pressing for a new TTA paradigm to realize robust adapta- tion. In this work, we launch a more realistic TTA setting, where distribution changing and correlative sampling oc- cur simultaneously at the test phase. We call this Practical Test-Time Adaptation, or briefly,PTTA. To understand more clearly the similarities and differences between PTTA and the previous setups, we visualize them in Figure 1 and sum- marize them in Table 1. To conquer this challenging prob- lem, we propose a Robust Test-Time Adaptation (RoTTA) method, which consists of three parts: 1) robust statistics es- timation, 2) category-balanced sampling considering time- liness and uncertainty and 3) time-aware robust training. More concretely, we first replace the erroneous statistics of the current batch with global ones maintained by the expo- nential moving average. It is a more stable manner to esti- mate the statistics in BatchNorm layers. Then, we simulate a batch of independent-like data in memory with category- balanced sampling while considering the timeliness and un- certainty of the buffered samples. That is, samples that are newer and less uncertain are kept in memory with higher priority. With this batch of category-balanced, timely and confident samples, we can obtain a snapshot of the current distribution. Finally, we introduce a time-aware reweight- ing strategy that considers the timeliness of the samples in the memory bank, with a teacher-student model to perform robust adaptation. With extensive experiments, we demon- strate that RoTTA can robustly adapt in the practical setup, i.e., PTTA. In a nutshell, our contributions can be summarized as: • We propose a new test-time adaptation setup that is more suitable for real-world applications, namely practical test-time adaptation (PTTA). PTTA considers both distribution changing and correlation sampling. • We benchmark the performance of prior methods in PTTA and uncover that they only consider one aspect of the problem, resulting in ineffective adaptation. • We propose a robust test-time adaptation method (RoTTA), which has a more comprehensive considera- tion of PTTA challenges. Ease of implementation and effectiveness make it a practical deployment option. • We extensively demonstrate the practicality of PTTA and the effectiveness of RoTTA on common TTA benchmarks [23], i.e., CIFAR-10-C and CIFAR-100- C and a large-scale DomainNet [58] dataset. RoTTA obtains state-of-the-art results, outperforming the best baseline by a large margin (reducing the averaged classification error by over 5.9%, 5.5% and 2.2% on CIFAR-10-C, CIFAR-100-C and DomainNet, respec- tively). 2. Related Work Domain adaptation (DA) studies the problem of transfer- ring the knowledge learned from a labeled source dataset to an unlabeled target dataset [8, 17, 43, 51, 67, 68]. Represen- tative techniques include latent distribution alignment [48, 77], adversarial training [17, 62], or self-training [75, 85]. The limitation of this setting, however, is that an unlabeled test dataset (target domain) is needed at training time, in addition to a labeled training dataset (source domain). Ac- cordingly, it might fail to handle more practical scenariosFeature 𝐹Robust batch normalization (RBN)Update𝜇௚, 𝜎௚ଶNormalizeFeature𝐹′Update bank with current sample  Training lossℒ௥in Eq. (7) Teacher StudentAdaptation with RBNMemorybankEMA 𝑡A stream of online dataUpdateTest timeCorrelationsamplingStrong & weakaugmentation flowDistributionsCategoryTeacherMajor classhas highest ℋin majorRemoveAddWhen ℋ>ℋSamples to beadded& removed Figure 2. Framework overview. Firstly, we replace the batch normalization layer with RBN which robustly normalizes the feature map. During the inference of the online test stream of PTTA, we utilize the predictions of samples to maintain a memory bank by category- balanced sampling with timeliness and uncertainty. Finally, we use the category-balanced, timely and confident data in the memory bank combined with a robust loss to adapt the model at test time. like test-time adaptation. Our practical test-time adaptation setting can be viewed as performing correlatively sample adaptation on the fly. It is worth noting that standard domain adaptation techniques might collapse when only continual data streams from multiple target domains are accessible. Domain generalization (DG) assumes that multiple source domains are available for model training and tries to learn models that can generalize well to any unseen domains [4, 26,40,41,52,84]. A broad spectrum of methodologies based on data augmentation [78, 84], meta-learning [14, 40], or domain alignment [50,52] has made great progress. In con- trast, this work instead aims to improve the performance of source pre-trained models at the test time by using unla- beled online data streams from multiple continually chang- ing target domains. Continual learning (CL) (also known as incremental learning, life-long learning) addresses the problem of learn- ing a model for many tasks sequentially without forgetting knowledge obtained from the preceding tasks. [1, 6, 31, 37, 60]. CL methods can often be categorized into replay- based [60, 66] and regularization-based [31, 44] methods. Ideas from continual learning are also adopted for continu- ous domain adaptation approaches [34, 74] In our work, we share the same motivation as CL and point out that prac- tical test-time adaptation (PTTA) also suffers catastrophic forgetting (i.e., performance degradation on new test sam- ples due to correlation sampling), which makes test-time adaptation approaches are unstable to deploy. Test-time adaptation (TTA) focus on more challenging settings where only source model and unlabeled target data are available [9, 18, 27, 28, 35, 46, 61]. A similar paradigm is source-free domain adaptation (SFDA) [10, 36, 47, 79], which also requires no access to the training (source) data. To name a few, Liang et al . [45] fit the source hypoth- esis by exploiting the information maximization and self- supervised pseudo-labeling. Kundu et al. [35] formalize a unified solution that explores SFDA without any category- gap knowledge. To fully utilize any arbitrary pre-trained model, Sun et al. [65] propose conducting adaptation on the fly with an auxiliary self-supervised task. Later on, Wanget al. [70] take a source pre-trained model and adapt it to the test data by updating a few trainable parameters in Batch- Norm layers [25] using entropy minimization [21]. While standard TTA has been widely studied in many tasks [2, 20, 63, 64, 70, 82], the fact remains that both dis- tribution changing [73] and data correlation sampling [19] has only been considered in isolation. For example, Gong et al. [19] propose instance-aware batch normalization and prediction-balanced reservoir sampling to address the chal- lenges of correlatively sampled test streams, however, it does not consider unstable adaptation resulting from long- term adaptation on continually changing distributions. On the other hand, Wang et al. [73] assume that the target test data is streamed from a continually changing environment and continually adapt an off-the-shelf source pre-trained model to the current test data. In this work, we launch PTTA, a more practical TTA setting to connect both worlds: distribution changing and correlation sampling. 3. Method 3.1. Problem Definition and Motivation Given a model fθ0 with parameter θ0 pre-trained on source domain DS = {(xS, yS)}, the proposed practical test-time adaptation (PTTA) aims to adapt fθ0 to a stream of online unlabeled samples X0, X1, ...,XT , where Xt is a batch of highly correlated samples from the distribution Ptest that changes with time t continually. More specifi- cally, at test time, with time going on, the test distribution Ptest changes continually as P0, P1, ...,P∞. At time step t, we will receive a batch of unlabeled and correlated samplesmotion distribution changing snow time  Distributions and Labels of PTTA T est Stream uniform 10 1 0.1 0.01 0.001 Dirichlet Parameter  Figure 3. Illustration of the labels and distributions of the test stream of CIFAR10-C under the setup PTTA. And we adopt Dirichlet distribution to simulate the process of correlative sam- pling. It is clear that as the concentration parameter δ decreases, the correlation among sampled data increases, which is reflected in the increasing aggregation of categories. Xt from Ptest. Next, Xt is fed into the model fθt and the model needs to adapt itself to the current test data streams and make predictions fθt (Xt) on the fly. As a matter of fact, this setup is largely driven the prac- tical demands of deploying models in dynamic scenarios. Taking for example the case of autonomous driving men- tioned in § 1, test samples are highly correlated and the data distribution changes continually with the weather or loca- tion. Another example is the situation of intelligent moni- toring, the camera will continuously capture more people at certain times, such as after work, but fewer of them during work time. Meanwhile, the light condition changes con- tinually from day to night. The deployed model should be robustly adapted in such dynamic scenarios. In a word, dis- tribution change and data correlation often happen simul- taneously in the real world. For this reason, existing TTA methods [7,9,19,28,70,73,81] might become unstable when the test stream is sampled from such dynamic scenarios. To obtain the test stream of PTTA, we adopt Dirich- let Distribution with parameter δ to simulate the correla- tion among test samples. We present the test data streams corresponding to different values of δ on the CIFAR10-C dataset in Figure 3. We can observe that the smaller δ is, the higher the correlation will be. For the sake of unity, we set δ = 0.1 as the default for all experiments. In the follow- ing, we present a robust test-time adaptation framework for the practical test-time adaptation setup defined above. An overview of our RoTTA is illustrated in Figure 2. 3.2. Robust Test-Time Adaptation Motivated by the fact that the statistics of current batch data, which are commonly used in previous TTA meth- ods [7, 20, 65, 70, 73], become unreliable when they en- counter correlative test data streams, we first turn to the global robust statistics for normalization. Then, to effec- tively adapt to the current distribution, we maintain a mem- ory bank by category-balanced sampling with considering timeliness and uncertainty, which captures a more stable snapshot of the distribution. Finally, we utilize the teacher- student model and design a timeliness-based reweighting strategy to train the model robustly. Robust batch normalization (RBN). Batch Normaliza- tion (BN) [25] is a widely-used training technique as it can accelerate the training and convergence speed of networks and stabilize the training process by reducing the risk of gradient explosion and vanishing. Given the feature map F ∈ RB×C×H×W as the input for a BN layer when train- ing, the channel-wise mean µ ∈ RC and variance σ2 ∈ RC are calculated as follows: µc = 1 BHW BX b=1 HX h=1 WX w=1 F(b,c,h,w) , (1) σ2 c = 1 BHW BX b=1 HX h=1 WX w=1 (F(b,c,h,w) − µc)2 . (2) Then the feature map is normalized and refined in a channel-wise manner as BN (F(b,c,h,w); µ, σ2) =γc F(b,c,h,w) − µc √σ2c + ϵ + βc , (3) where γ, β∈ RC are learnable parameters in the layer and ϵ > 0 is a constant for numerical stability. Meanwhile, during training, the BN layer maintains a group of global running mean and running variance (µs, σ2 s) for inference. Due to the domain shift at test time, the global statis- tics (µs, σ2 s) normalize test features inaccurately, causing significant performance degradation. To tackle the prob- lem above, some methods [55, 70, 73] use the statistics of the current batch to perform normalization. Unfortunately, when the test samples have a high correlation under PTTA setup, the statistics of the current batch also fail to correctly normalize the feature map, as demonstrated in Figure 4c. Specifically, the performance of BN [53] decreases rapidly as the data correlation increases. Based on the analysis above, we propose a robust batch normalization (RBN) module, which maintains a group of global statistics (µg, σ2 g) to normalize the feature map ro- bustly. Before the whole test-time adaptation, (µg, σ2 g) is initialized as the running mean and variance (µs, σ2 s) of the pre-trained model. When adapting the model, we update the global statistics first by exponential moving average as µg = (1− α)µg + αµ , (4) σ2 g = (1− α)σ2 g + ασ2 , (5) where (µ, σ2) is the statistics of the buffered samples in the memory bank. Then we normalize and affine the feature as Eq. (3) with (µg, σ2 g). When inferring for test samples, we directly utilize (µg, σ2 g) to calculate the output as Eq (3). Al- though simple, RBN is effective enough to tackle the prob- lem of normalization on test streams of PTTA.Category-balanced sampling with timeliness and uncer- tainty (CSTU). In the PTTA setup, the correlation among test samples Xt at time t leads to a deviation between the observed distribution bPtest and the test distribution Ptest. Specifically, the marginal label distribution p(y|t) tends to differ from p(y). Continuously learning with Xt over time t can lead to model adaptation to an unreliable distribution bPtest, resulting in ineffective adaptation and an increased risk of model collapse. To address this issue, we propose a category-balanced memory bank M with a capacity of N, which takes into account the timeliness and uncertainty of samples when up- dating. In particular, we adopt the predictions of test sam- ples as pseudo labels to guide the update ofM. Meanwhile, to guarantee the balance among categories, we distribute the capacity of M equally to each category, and samples of the major categories will be replaced first (refer to lines 5-9 in Algorithm 1). Furthermore, due to the continually changing test distribution, old samples in M are limited in value, and could even impair the ability of the model to adapt to the current distribution. Additionally, samples of high uncer- tainty always produce erroneous gradient information that can hinder model adaptation, as suggested by [55]. With this in mind, we attach each sample in M with a group of heuristics (A, U), where A, initialized as 0 and in- creasing with time t, is the age of the sample, and U the un- certainty calculated as the entropy of the prediction. Next, we combine the timeliness and uncertainty to calculate a heuristic score, i.e., category-balanced sampling with time- liness and uncertainty (CSTU), as follows: H = λt 1 1 + exp(−A/N) + λu U log C , (6) where λt and λu make the trade-off between timeliness and uncertainty, and for simplicity, λt and λu are set to 1.0 for all experiments, andC is the number of categories. We sum- marize our sampling algorithm in Algorithm 1. With CSTU, we can obtain a robust snapshot of the current test distribu- tion Ptest, and effectively adapt the model to it. Robust training with timeliness. Actually, after replacing BN layers with our RBN and obtaining the memory bank selected via CSTU, we can directly adopt the widely used techniques like pseudo labeling or entropy minimization to perform test-time adaptation. However, we notice that too old or unreliable instances still have the opportunity to stay in M since keeping the category balance is assigned the top priority. In addition, too aggressive updates of the model will make the category balance ofM unreliable, resulting in unstable adaptation. Meanwhile, error accumulation caused by the distribution change also makes the aforementioned approaches unworkable. To further reduce the risk of error gradients information from old and unreliable instances and stabilize the adapta- tion, we turn to the robust unsupervised learning method Algorithm 1: CSTU for one test sample. 1 Input: a test sample x and the teacher model fθT . 2 Define: memory bank M and its capacity N, number of classes C, per class occupation O ∈RC, total occupation Ω, classes to pop instance D. 3 Infer as p(y|x) =Softmax(fθT (x)). 4 Calculate the predicted category of x as ˆy = arg maxc p(c|x), the uncertainty as Ux = −PC c=1 p(c|x) log(p(c|x)), the age as Ax = 0, and the heuristic score Hx of x with Eq (6) 5 if Oˆy < N C then 6 if Ω <N: Search range D = ∅. 7 else: Search range D = {j|j = arg maxc Oc} 8 else 9 Search range D = {ˆy} 10 if D is ∅ then 11 Add (x, ˆy, Hx, Ux) into M. 12 else 13 Find the instance (ˆx, yˆx, Aˆx, Uˆx) with the highest value in Eq (6) Hˆx among D. 14 if Hx < Hˆx then 15 Remove (ˆx, yˆx, Aˆx, Uˆx) from M. 16 Add (x, ˆy, Hx, Ux) into M. 17 else 18 Discard x. 19 Increase the age of all instances in M. teacher-student model and propose a timeliness reweight- ing strategy. In addition, for the sake of time efficiency and stability, only affine parameters in RBN are trained during adaptation. At time step t, after inferring for the correlated data Xt with the teacher model fθT t and updating the memory bank M with Xt, we begin updating the student model fθS t and the teacher model fθT t . Firstly, we update parameters of stu- dent model θS t → θS t+1 by minimizing the following loss: Lr = 1 Ω ΩX i=1 L(xM i , Ai; θT t , θS t ) , (7) where Ω = |M| is the total occupation of the memory bank, and xM i and Ai(i = 1, ..., Ω) are instances in the memory bank and their age respectively. Subsequently, the teacher model is updated by exponential moving average as θT t+1 = (1− ν)θT t + νθS t+1 . (8) To calculate the loss value of an instancexM i from the mem- ory bank, the timeliness reweighting term is computed as E(Ai) = exp(−Ai/N) 1 + exp(−Ai/N) , (9)where Ai is the age of xM i , and N is the capacity of the bank. And then we calculate the cross entropy between the soft-max prediction pS(y|x′′ i ) of the strong-augmented view x′′ i from the student model and that pT (y|x′ i) of the weak- augmented view 1 x′ i from the teacher model as follows: ℓ(x′ i, x′′ i ) =−1 C CX c=1 pT (c|x′ i) logpS(c|x′′ i ) . (10) Finally, equipped with Eq. (9) and Eq. (10), the right-hand side of Eq. (7) reduces to L(xM i , Ai; θT t , θS t ) =E(Ai)ℓ(x′ i, x′′ i ) . (11) To sum up, equipped with RBN, CSTU, and robust training with timeliness, our RoTTA is capable of effectively adapt- ing any pre-trained models in dynamic scenarios. 4. Experiments 4.1. Setup Datasets. CIFAR10-C and CIFAR100-C [23] are the com- monly used TTA benchmarks to testify the robustness un- der corruptions. Both of them are obtained by applying 15 kinds of corruption with 5 different degrees of severity on their clean test images of original datasets CIFAR10 and CIFAR100 respectively. CIFAR10/CIFAR100 [32] have 50,000/10,000 training/test images, all of which fall into 10/100 categories. DomainNet [58] is the largest and hard- est dataset to date for domain adaptation and consists of about 0.6 million images with 345 classes. It consists of six different domains including Clipart (clp), Infograph (inf), Painting (pnt), Quickdraw (qdr), Real (rel), and Sketch (skt). We first pre-train a source model on the train set in one of six domains and testify all baseline methods on the test set of the remaining five domains. Implementation details. All experiments are conducted with PyTorch [57] framework. In the case of robustness to corruption, following the previous methods [55, 70, 73], we obtain the pre-trained model from RobustBench bench- mark [12], including the WildResNet-28 [80] for CIFAR10 → CIFAR10-C, and the ResNeXt-29 [76] for CIFAR100 → CIFAR100-C. Then, we change the test corruption at the highest severity 5 one by one to simulate that the test distri- bution continually changes with time in PTTA. And in the case of generalization under the huge domain gap, we train a ResNet-101 [22] by standard classification loss for each domain in DomainNet and adapt them continually to differ- ent domains except the source domain. Meanwhile, we uti- lize the Dirichlet distribution to simulate the correlatively sampled test stream for all datasets. For optimization, we adopt Adam [30] optimizer with learning rate 1.0 × 10−3, 1Weak augmentation is ReSize+CenterCrop. Strong augmentation is a combination nine operations like Clip, ColorJitter, and RandomAffine. β = 0.9. For a fair comparison, we set the batch size for all methods as 64 and the capacity of the memory bank of RoTTA as N = 64. Concerning the hyperparameters, we adopt a unified set of values for RoTTA across all experi- ments including α = 0.05, ν = 0.001, λt = 1.0, λu = 1.0, and δ = 0.1. More details are provided in the appendix. 4.2. Comparisons with the State-of-the-arts Robustness under corruptions. The classification error on CIFAR10→CIFAR10-C and CIFAR100→CIFAR100-C are shown in Table 2 and Table 3 respectively. We change the type of the current corruption at the highest severity 5 as time goes on, and sample data correlatively for infer- ence and adaptation simultaneously. The same test stream is shared across all compared methods. From Table 2 and Table 3, we can see that RoTTA achieves the best performance compared to previous meth- ods. Moreover, RoTTA has a significant performance gain to the second-best method that 5.9% improvement on CIFAR10 →CIFAR10-C and 5.5% improvement on CIFAR100→CIFAR100-C respectively, verifying the effec- tiveness of RoTTA to adapt the model under PTTA. In more detail, we can observe that BN [53], PL [39], TENT [70] and CoTTA [73] negatively adapt the model to the test streams of both datasets compared to Source (−6.5 ∼ −46.4%). This is attributed to the fact that these methods overlook the issues posed by correlation sampling, which can result in highly correlated data within a batch. As a consequence, traditional normalization statistics may be ineffective in appropriately normalizing the feature maps. Equipped with RBN and CSTU, RoTTA no longer suffers from this issue. Meanwhile, in Table 3, if focus on the adaptation procedure, we can see that the performance of PL [39], TENT [70] and NOTE [19] becomes worse and worse, and eventually, the model even collapses (error rate > 97%). This reveals that the impact of error accumula- tion on long-term adaptation can be catastrophic. To tackle this problem, RoTTA turns to robustly adapt the model with timeliness reweighting and confident samples in the mem- ory bank, and superior performance throughout the adapta- tion process demonstrates its effectiveness. In addition, we find that although LAME [5] never tunes the parameters of the model, it is still a competi- tive baseline for example it achieves the second-best result on CIFAR100→CIFAR100-C. However, its performance is very dependent on the performance of the pre-trained model e.g. negligible improvement on difficult corruptions (shot, gaussian, pixelate). On the contrary, our RoTTA is more flexible and achieves better and more robust results. Generalization under domain shift. We also evalu- ate RoTTA under a more challenging dataset DomainNet, where we continually adapt a source pre-trained model to correlatively sampled test streams of the rest domains. AsTable 2. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 34.8 25.1 26.0 65.7 46.9 46.7 42.0 9.3 41.3 26.6 54.3 72.3 58.5 30.3 72.9 43.5BN [53] 73.2 73.4 72.7 77.2 73.7 72.5 72.9 71.0 74.1 77.7 80.0 76.9 75.5 78.3 79.0 75.2PL [39] 73.9 75.0 75.6 81.0 79.9 80.6 82.0 83.2 85.3 87.3 88.3 87.5 87.5 87.5 88.2 82.9TENT [70] 74.3 77.4 80.1 86.2 86.7 87.3 87.9 87.4 88.2 89.0 89.2 89.0 88.3 89.7 89.2 86.0LAME [5] 29.5 19.0 20.3 65.3 42.4 43.4 36.8 5.4 37.2 18.6 51.2 73.2 57.0 22.6 71.3 39.5CoTTA [73]77.1 80.6 83.1 84.4 83.9 84.2 83.1 82.6 84.4 84.2 84.5 84.6 82.7 83.8 84.9 83.2NOTE [19] 18.0 22.1 20.6 35.6 26.9 13.6 26.5 17.3 27.2 37.0 48.3 38.8 42.6 41.9 49.7 31.1 RoTTA 18.1 21.3 18.8 33.6 23.6 16.5 15.1 11.2 21.9 30.7 39.6 26.8 33.7 27.8 39.5 25.2(+5.9) Table 3. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 30.8 39.5 50.3 68.0 29.3 55.1 28.8 29.5 45.8 37.2 54.1 73.0 74.7 41.2 39.4 46.4BN [53] 48.5 54.0 58.9 56.2 46.4 48.0 47.0 45.4 52.9 53.4 57.1 58.2 51.7 57.1 58.8 52.9PL [39] 50.6 62.1 73.9 87.8 90.8 96.0 94.8 96.4 97.4 97.2 97.4 97.4 97.3 97.4 97.4 88.9TENT [70] 53.3 77.6 93.0 96.5 96.7 97.5 97.1 97.5 97.3 97.2 97.1 97.7 97.6 98.0 98.3 92.8LAME [5] 22.4 30.4 43.9 66.3 21.3 51.7 20.6 21.8 39.6 28.0 48.7 72.8 74.6 33.1 32.3 40.5CoTTA [73]49.2 52.7 56.8 53.0 48.7 51.7 49.4 48.7 52.5 52.2 54.3 54.9 49.6 53.4 56.2 52.2NOTE [19] 45.7 53.0 58.2 65.6 54.2 52.0 59.8 63.5 74.8 91.8 98.1 98.3 96.8 97.0 98.2 73.8 RoTTA 31.8 36.7 40.9 42.1 30.0 33.6 27.9 25.4 32.3 34.0 38.8 38.7 31.3 38.0 42.9 35.0(+5.5) Table 4. Average classification error of DomainNet while continually adapting to different domains with correlatively sampled test stream. Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Sourceclp inf pnt qdr rel sktAvg. BN clp inf pnt qdr rel sktAvg. PL clp inf pnt qdr rel sktAvg.TENTclp inf pnt qdr rel sktAvg. clp N/A 83.9 65.4 88.6 48.0 59.1 69.0clp N/A 88.6 70.7 90.5 65.4 67.0 76.5clp N/A 94.5 98.9 99.5 99.7 99.7 98.5clp N/A 87.5 71.9 94.2 96.2 98.9 89.7inf 61.8 N/A 66.9 96.0 50.0 70.6 69.1inf 68.6 N/A 74.2 96.2 69.9 76.8 77.1inf 82.6 N/A 99.2 99.6 99.7 99.3 96.1inf 68.6 N/A 75.0 97.3 95.9 98.7 87.1pnt 56.5 83.7 N/A 94.2 42.6 63.4 68.1pnt 60.8 87.9 N/A 94.3 62.3 68.7 74.8pnt 78.6 99.4 N/A 99.7 99.6 99.7 95.4pnt 61.7 87.1 N/A 96.4 95.3 98.8 87.8qdr 89.2 99.0 98.6 N/A 95.0 92.3 94.8qdr 80.3 97.7 92.6 N/A 88.7 88.1 89.5qdr 81.7 99.5 99.6 N/A 99.7 99.8 96.1qdr 78.9 97.1 91.6 N/A 89.2 88.7 89.1rel 49.4 80.4 51.5 93.4 N/A 63.3 67.6rel 57.9 87.1 63.1 94.3 N/A 70.8 74.6rel 73.5 99.4 99.2 99.6 N/A 99.7 94.3rel 57.8 86.4 68.1 96.9 N/A 96.7 81.2skt 47.5 88.2 62.9 87.1 51.8 N/A 67.5skt 50.4 87.6 64.6 89.6 63.1 N/A 71.1skt 64.8 99.2 99.4 99.7 99.7 N/A 92.6skt 51.9 87.2 69.1 95.3 97.3 N/A 80.1Avg.60.9 87.0 69.1 91.9 57.5 69.7 72.7Avg.63.6 89.8 73.0 93.0 69.9 74.3 77.3Avg.76.2 98.4 99.3 99.6 99.7 99.6 95.5Avg.63.8 89.0 75.1 96.0 94.8 96.4 85.8 Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →LAMEclp inf pnt qdr rel sktAvg.COTTAclp inf pnt qdr rel sktAvg.NOTEclp inf pnt qdr rel sktAvg.RoTTAclp inf pnt qdr rel sktAvg. clp N/A 82.2 64.5 87.7 46.9 58.9 68.0clp N/A 90.6 77.9 89.3 76.3 72.7 81.4clp N/A 89.2 73.0 94.8 98.4 99.4 91.0clp N/A 85.5 62.0 82.0 49.3 59.8 67.7inf 60.1 N/A 65.7 95.4 48.5 69.4 67.8inf 74.5 N/A 82.0 95.7 80.2 81.5 82.8inf 75.4 N/A 78.7 98.7 98.1 99.5 90.1inf 61.8 N/A 63.7 91.5 52.5 67.6 67.4pnt 55.8 81.5 N/A 93.3 41.3 62.1 66.8pnt 66.3 89.8 N/A 93.4 74.0 75.4 79.8pnt 64.7 89.8 N/A 97.8 98.4 99.2 90.0pnt 53.3 84.1 N/A 89.1 47.3 61.4 67.0qdr 88.3 99.1 99.0 N/A 94.9 92.2 94.7qdr 82.3 98.2 94.6 N/A 92.5 90.1 91.5qdr 74.7 97.2 92.2 N/A 93.5 99.6 91.4qdr 77.5 97.0 89.8 N/A 80.3 82.2 85.3rel 48.0 79.3 50.1 91.6 N/A 60.2 65.8rel 64.0 90.3 73.2 93.5 N/A 77.6 79.7rel 61.3 89.2 68.9 98.8 N/A 99.2 83.5rel 49.1 82.3 50.3 88.0 N/A 61.1 66.2skt 45.6 87.1 59.5 83.9 49.9 N/A 65.2skt 56.1 89.2 71.9 89.2 73.5 N/A 76.0skt 55.2 89.7 70.1 96.9 98.3 N/A 82.0skt 42.6 83.7 54.4 80.9 47.5 N/A 61.8Avg.59.6 85.8 67.8 90.4 56.3 68.6 71.4Avg.68.6 91.6 79.9 92.2 79.3 79.5 81.9Avg.66.3 91.0 76.6 97.4 97.3 99.4 88.0Avg.56.8 86.5 64.0 86.3 55.4 66.469.2(+2.2) shown in Table 4, consistent with the previous analysis, most of the methods include BN [53], PL [39], TENT [70], CoTTA [73] and NOTE [19] even perform worse than the Source model ( −4.6 ∼ −22.8%). RoTTA consistently achieves the best performance and has 2.2% gain than the second method LAME [5], demonstrating RoTTA’s effec- tiveness again. 4.3. Ablation Study Effect of each component. To further investigate the effi- cacy of each component, we replace each part with the nor- mally used solutions to obtain three variants: (1) RoTTA w/o RBN, replace RBN with test-time BN in TENT [70]; (2) RoTTA w/o CSTU, directly adapt the model on test stream; (3) RoTTA w/o robust training (RT), directly adapt the model only with entropy minimization. As shown in Table 5, we can observe that significant performance degra- dation occurs for all variants, proving that every part of our proposed method is valid for PTTA. Take one com- ponent for a detailed example, without RBN robustly nor- malizing feature maps, the performance of RoTTA drops 50.2% and 16.3% on CIFAR10-C and CIFAR100-C respec- tively, proving that RBN is robust enough to tackle the prob- lem of normalization of correlatively sampled data streams. CSTU enables RoTTA to adapt to a more stable distribu- tion by maintaining a timely and confident snapshot of the test distribution. Meanwhile, robust training with timeliness greatly reduces the accumulation of errors. Every compo- nent behaves significantly to enable effective adaptation un- der PTTA. Effect of the distribution changing order. To exclude the effect of a fixed order of distribution changing, we con- ducted experiments on ten different sequences of changes on CIFAR10-C and CIFAR100-C with independently andBN PL TENT LAME CoTTA NOTE RoTTA0 10 20 30 40 50 60 70 80Classification error (%) Source CIFAR-10  CIFAR-10-C Independent Correlative (a) CIFAR10-C. BN PL TENT LAME CoTTA NOTE RoTTA0 20 40 60 80Classification error (%) Source CIFAR-100  CIFAR-100-C Independent Correlative (b) CIFAR100-C. uniform 10 1 0.1 0.01 0.001 30 40 50 60 70 80 90 100Classification error (%) Source BN PL TENT LAME CoTTA NOTE RoTTA (c) δ. 16 32 64 128 256 512 40 50 60 70 80 90 100Classification error (%) Source BN PL TENT LAME CoTTA NOTE RoTTA (d) Batch size. Figure 4. (a) & (b) we adapt the model continually to different corruptions of 10 different orders with independently and correlatively sampled test streams on CIFAR10-C and CFAR100-C respectively and report their average classification error. (c) & (d) we verify the effect of δ and batch size to different methods on CIFAR100-C respectively. Table 5. Classification error of different variants of our RoTTA. Variant CIFAR10-C CIFAR100-C Avg. RoTTA w/o RBN 75.4 51.3 63.4 RoTTA w/o CSTU 47.1 46.3 46.7 RoTTA w/o RT 78.2 95.0 81.6 RoTTA 25.2 35.0 30.1 correlatively sampled test streams respectively. As shown in Figure 4a and 4b, no matter what kind of setup, RoTTA can achieve excellent results. The detailed results on the correlatively sampled test streams are shown in Table 6, RoTTA achieves 4.3% and 4.7% progress on CIFAR10- C and CIFAR100-C respectively. This shows that RoTTA can adapt the model robustly and effectively in long-term scenarios where distribution continually changes and test streams are sampled either independently or correlatively, making it a good choice for model deployment. Effect of Dirichlet concentration parameter δ. We vary the value of δ on CIFAR100-C and compare RoTTA with other approaches in Figure 4c. As the value of δ increases, the performance of BN [53], PL [39], TENT [70] and CoTTA [73] drops quickly, because they never consider the increasing correlation among test samples. NOTE [19] is stable to correlatively sampled test streams but does not consider the distribution changing, causing ineffective adaptation. Meanwhile, the higher correlation between test samples will make the propagation of labels more accurate, which is why the result of LAME [5] slightly improves. Fi- nally, excellent and stable results once again prove the sta- bility and effectiveness of RoTTA. Effect of batch size. In real scenarios, considering deploy- ment environments may use different test batch sizes, we conduct experiments with different values of test batch sizes and results are shown in Figure 4d. For a fair comparison, we control the frequency of updating the model of RoTTA so that the number of samples involved in back-propagation is the same. As the batch size increases, we can see that all of the compared methods have a significant improvement except for lame which has a slight decrease. This is be- cause the number of categories in a batch increases with the Table 6. Average classification error of tasks CIFAR10 → CIFAR10-C and CIFAR100 → CIFAR100-C while continually adapting to different corruptions of 10 different orders at the high- est severity 5 with correlatively sampled test stream. Method CIFAR10-C CIFAR100-C Avg. Source 43.5 46.4 46.9 BN [53] 75.2 52.9 64.1 PL [39] 75.2 52.9 60.1 TENT [70] 82.3 93.2 87.8 LAME [5] 39.5 40.6 40.1 NOTE [19] 30.5 76.1 53.3 CoTTA [73] 83.1 52.8 67.9 RoTTA 26.2(+4.3) 35.9(+4.7) 31.1(+9.0) increasing batch size, causing the overall correlation to be- come lower but the propagation of labels to become more difficult. Most significantly, RoTTA achieves the best re- sults across different batch sizes, demonstrating its robust- ness in dynamic scenarios once again. 5. Conclusion This work proposes a more realistic TTA setting where distribution changing and correlative sampling occur si- multaneously at the test phase, namely Practical Test-Time Adaptation (PTTA). To tackle the problems of PTTA, we propose Robust Test-Time Adaptation (RoTTA) method against the complex data stream. More specifically, a group of robust statistics for the normalization of feature maps is estimated by robust batch normalization. Meanwhile, a memory bank is adopted to capture a snapshot of the test distribution by category-balanced sampling with consider- ing timeliness and uncertainty. Further, we develop a time- aware reweighting strategy with a teacher-student model to stabilize the adaptation process. Extensive experiments and ablation studies are conducted to verify the robustness and effectiveness of the proposed method. We believe this work will pave the way for thinking about adapting models into real-world applications by test-time adaptation algorithm. Acknowledgements. This paper was supported by National Key R&D Program of China (No. 2021YFB3301503), and also supported by the National Natural Science Foundation of China under Grant No. 61902028.References [1] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Ben- gio. Gradient based sample selection for online continual learning. In NeurIPS, pages 11816–11825, 2019. 3 [2] Fatemeh Azimi, Sebastian Palacio, Federico Raue, J ¨orn Hees, Luca Bertinetto, and Andreas Dengel. Self-supervised test-time adaptation on video data. In WACV, pages 2603– 2612, 2022. 1, 3 [3] Mathilde Bateson, Herve Lombaert, and Ismail Ben Ayed. Test-time adaptation with shape moments for image segmen- tation. In MICCAI, pages 736–745, 2022. 1 [4] Gilles Blanchard, Gyemin Lee, and Clayton Scott. General- izing from several related classification tasks to a new unla- beled sample. In NeurIPS, pages 2178–2186, 2011. 3 [5] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In CVPR, pages 8344–8353, 2022. 2, 6, 7, 8, 13, 14, 15, 16, 17 [6] Francisco M Castro, Manuel J Mar ´ın-Jim´enez, Nicol´as Guil, Cordelia Schmid, and Karteek Alahari. End-to-end incre- mental learning. In ECCV, pages 233–248, 2018. 3 [7] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In CVPR, pages 295–305, 2022. 1, 4 [8] Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Domain adaptive faster r-cnn for object de- tection in the wild. In CVPR, pages 3339–3348, 2018. 2 [9] Zhixiang Chi, Yang Wang, Yuanhao Yu, and Jin Tang. Test- time fast adaptation for dynamic scene deblurring via meta- auxiliary learning. In CVPR, pages 9137–9146, 2021. 3, 4 [10] Boris Chidlovskii, St ´ephane Clinchant, and Gabriela Csurka. Domain adaptation in the absence of source domain data. In KDD, pages 451–460, 2016. 3 [11] Sungha Choi, Seunghan Yang, Seokeon Choi, and Sun- grack Yun. Improving test-time adaptation via shift-agnostic weight regularization and nearest source prototypes. In ECCV, pages 440–458, 2022. 1 [12] Francesco Croce, Maksym Andriushchenko, Vikash Se- hwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness benchmark. In Neurips, 2021. 6 [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 1 [14] Ying-Jun Du, Jun Xu, Huan Xiong, Qiang Qiu, Xiantong Zhen, Cees G. M. Snoek, and Ling Shao. Learning to learn with variational information bottleneck for domain general- ization. In ECCV, pages 200–216, 2020. 3 [15] Sayna Ebrahimi, Sercan ¨O. Arik, and Tomas Pfister. Test- time adaptation for visual document understanding. CoRR, abs/2206.07240, 2022. 1 [16] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei A Efros. Test-time training with masked autoencoders. In NeurIPS, 2022. 1 [17] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pas- cal Germain, Hugo Larochelle, Franc ¸ois Laviolette, Mario Marchand, and Victor S. Lempitsky. Domain-adversarial training of neural networks. J. Mach. Learn. Res., 17:59:1– 59:35, 2016. 1, 2 [18] Yunhe Gao, Xingjian Shi, Yi Zhu, Hao Wang, Zhiqiang Tang, Xiong Zhou, Mu Li, and Dimitris N. Metaxas. Vi- sual prompt tuning for test-time domain adaptation. CoRR, abs/2210.04831, 2022. 3 [19] Taesik Gong, Jongheon Jeong, Taewon Kim, Yewon Kim, Jinwoo Shin, and Sung-Ju Lee. Robust continual test- time adaptation: Instance-aware BN and prediction-balanced memory. In NeurIPS, 2022. 1, 2, 3, 4, 6, 7, 8, 13, 14, 15, 16, 17 [20] Sachin Goyal, Mingjie Sun, Aditi Raghunathan, and J Zico Kolter. Test time adaptation via conjugate pseudo-labels. In NeurIPS, 2022. 1, 3, 4 [21] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In NeurIPS, pages 529– 536, 2004. 3 [22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770–778, 2016. 1, 6 [23] Dan Hendrycks and Thomas G. Dietterich. Benchmarking neural network robustness to common corruptions and per- turbations. In ICLR, 2019. 2, 6 [24] Hengguan Huang, Xiangming Gu, Hao Wang, Chang Xiao, Hongfu Liu, and Ye Wang. Extrapolative continuous-time bayesian neural network for fast training-free test-time adap- tation. In NeurIPS, 2022. 1 [25] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal co- variate shift. In ICML, pages 448–456, 2015. 3, 4 [26] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier ad- justment module for model-agnostic domain generalization. In NeurIPS, pages 2427–2440, 2021. 1, 3 [27] Vidit Jain and Erik Learned-Miller. Online domain adapta- tion of a pre-trained cascade of classifiers. In CVPR, pages 577–584, 2011. 3 [28] Minguk Jang and Sae-Young Chung. Test-time adaptation via self-training with nearest neighbor information. CoRR, abs/2207.10792, 2022. 3, 4 [29] Junho Kim, Inwoo Hwang, and Young Min Kim. Ev-tta: Test-time adaptation for event-based object recognition. In CVPR, pages 17724–17733, 2022. 1 [30] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 6 [31] James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska- Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Ku- maran, and Raia Hadsell. Overcoming catastrophic forget- ting in neural networks. CoRR, abs/1612.00796, 2016. 3 [32] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 6[33] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural net- works. In NeurIPS, pages 1097–1105, 2012. 1 [34] Ananya Kumar, Tengyu Ma, and Percy Liang. Understand- ing self-training for gradual domain adaptation. In ICML, pages 5468–5479, 2020. 3 [35] Jogendra Nath Kundu, Naveen Venkat, Rahul M. V ., and R. Venkatesh Babu. Universal source-free domain adapta- tion. In CVPR, pages 4543–4552, 2020. 3 [36] Vinod K Kurmi, Venkatesh K Subramanian, and Vinay P Namboodiri. Domain impression: A source data free do- main adaptation method. In WACV, pages 615–625, 2021. 3 [37] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory G. Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying for- getting in classification tasks. IEEE Trans. Pattern Anal. Mach. Intell., 44(7):3366–3385, 2022. 3 [38] Yann LeCun, Yoshua Bengio, and Geoffrey E. Hinton. Deep learning. Nat., 521(7553):436–444, 2015. 1 [39] Dong-Hyun Lee et al. Pseudo-label: The simple and effi- cient semi-supervised learning method for deep neural net- works. In Workshop on challenges in representation learn- ing, ICML, volume 3, page 896, 2013. 6, 7, 8, 12, 14, 15, 16, 17 [40] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Learning to generalize: Meta-learning for do- main generalization. In AAAI, pages 3490–3497, 2018. 1, 3 [41] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C. Kot. Domain generalization with adversarial feature learning. In CVPR, pages 5400–5409, 2018. 1, 3 [42] Shuang Li, Binhui Xie, Qiuxia Lin, Chi Harold Liu, Gao Huang, and Guoren Wang. Generalized domain conditioned adaptation network. IEEE Trans. Pattern Anal. Mach. Intell., 44(8):4093–4109, 2022. 1 [43] Shuang Li, Mixue Xie, Kaixiong Gong, Chi Harold Liu, Yulin Wang, and Wei Li. Transferable semantic augmen- tation for domain adaptation. In CVPR, pages 11516–11525, 2021. 2 [44] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Trans. Pattern Anal. Mach. Intell., 40(12):2935–2947, 2018. 3 [45] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for un- supervised domain adaptation. In ICML, pages 6028–6039, 2020. 1, 3 [46] Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. TTT++: when does self-supervised test-time training fail or thrive? In NeurIPS, pages 21808–21820, 2021. 3 [47] Yuang Liu, Wei Zhang, and Jun Wang. Source-free do- main adaptation for semantic segmentation. In CVPR, pages 1215–1224, 2021. 3 [48] Mingsheng Long, Yue Cao, Zhangjie Cao, Jianmin Wang, and Michael I. Jordan. Transferable representation learning with deep adaptation networks. IEEE Trans. Pattern Anal. Mach. Intell., 41(12):3071–3085, 2019. 1, 2 [49] Wenao Ma, Cheng Chen, Shuang Zheng, Jing Qin, Huimao Zhang, and Qi Dou. Test-time adaptation with calibration of medical image classification nets for label distribution shift. In MICCAI, pages 313–323, 2022. 1 [50] Divyat Mahajan, Shruti Tople, and Amit Sharma. Domain generalization using causal matching. In ICML, pages 7313– 7324, 2021. 3 [51] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds and algorithms. In COLT, 2009. 2 [52] Krikamol Muandet, David Balduzzi, and Bernhard Sch¨olkopf. Domain generalization via invariant fea- ture representation. In ICML, pages 10–18, 2013. 1, 3 [53] Zachary Nado, Shreyas Padhy, D. Sculley, Alexander D’Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robust- ness under covariate shift. CoRR, abs/2006.10963, 2020. 4, 6, 7, 8, 12, 14, 15, 16, 17 [54] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test- time model adaptation without forgetting. In ICML, pages 16888–16905, 2022. 1 [55] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test- time model adaptation without forgetting. In ICML, volume 162, pages 16888–16905, 2022. 4, 5, 6 [56] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Trans. Knowl. Data Eng., 22(10):1345–1359, 2010. 1 [57] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, pages 8024–8035, 2019. 6 [58] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In ICCV, pages 1406–1415, 2019. 2, 6 [59] Joaquin Quinonero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset shift in ma- chine learning. 2008. 1 [60] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H. Lampert. icarl: Incremental classi- fier and representation learning. InCVPR, pages 5533–5542, 2017. 3 [61] Amelie Royer and Christoph H Lampert. Classifier adapta- tion at prediction time. In CVPR, pages 1401–1409, 2015. 3 [62] Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tat- suya Harada. Maximum classifier discrepancy for unsuper- vised domain adaptation. In CVPR, pages 3723–3732, 2018. 2 [63] Inkyu Shin, Yi-Hsuan Tsai, Bingbing Zhuang, Samuel Schulter, Buyu Liu, Sparsh Garg, In So Kweon, and Kuk- Jin Yoon. MM-TTA: multi-modal test-time adaptation for 3d semantic segmentation. In CVPR, pages 16907–16916, 2022. 1, 3[64] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Test- time prompt tuning for zero-shot generalization in vision- language models. In NeurIPS, 2022. 1, 3 [65] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self- supervision for generalization under distribution shifts. In ICML, pages 9229–9248, 2020. 1, 2, 3, 4 [66] Rishabh Tiwari, KrishnaTeja Killamsetty, Rishabh K. Iyer, and Pradeep Shenoy. GCR: gradient coreset based replay buffer selection for continual learning. In CVPR, pages 99– 108, 2022. 3 [67] Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Ki- hyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker. Learning to adapt structured output space for semantic seg- mentation. In CVPR, pages 7472–7481, 2018. 2 [68] Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. In ICCV, pages 4068–4076, 2015. 2 [69] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In CVPR, pages 2962–2971, 2017. 1 [70] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno A. Ol- shausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In ICLR, 2021. 1, 2, 3, 4, 6, 7, 8, 12, 13, 14, 15, 16, 17 [71] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip Yu. Generalizing to unseen domains: A survey on domain generalization. IEEE Trans. Knowl. Data Eng., 2022. 1 [72] Mei Wang and Weihong Deng. Deep visual domain adapta- tion: A survey. Neurocomputing, 312:135–153, 2018. 1 [73] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Con- tinual test-time domain adaptation. In CVPR, pages 7191– 7201, 2022. 1, 2, 3, 4, 6, 7, 8, 13, 14, 15, 16, 17 [74] Markus Wulfmeier, Alex Bewley, and Ingmar Posner. Incre- mental adversarial domain adaptation for continually chang- ing environments. In ICRA, pages 4489–4495, 2018. 3 [75] Binhui Xie, Shuang Li, Mingjia Li, Chi Harold Liu, Gao Huang, and Guoren Wang. Sepico: Semantic-guided pixel contrast for domain adaptive semantic segmentation. IEEE Trans. Pattern Anal. Mach. Intell., pages 1–17, 2023. 2 [76] Saining Xie, Ross Girshick, Piotr Doll ´ar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In CVPR, pages 5987–5995, 2017. 6 [77] Ruijia Xu, Guanbin Li, Jihan Yang, and Liang Lin. Larger norm more transferable: An adaptive feature norm approach for unsupervised domain adaptation. In ICCV, pages 1426– 1435, 2019. 2 [78] Zhenlin Xu, Deyi Liu, Junlin Yang, Colin Raffel, and Marc Niethammer. Robust and generalizable visual representation learning via random convolutions. In ICLR, 2021. 3 [79] Shiqi Yang, Yaxing Wang, Joost van de Weijer, Luis Herranz, and Shangling Jui. Generalized source-free domain adapta- tion. In ICCV, pages 8978–8987, 2021. 3 [80] Sergey Zagoruyko and Nikos Komodakis. Wide residual net- works. In BMVC, 2016. 6 [81] Marvin Mengxin Zhang, Sergey Levine, and Chelsea Finn. MEMO: Test time robustness via adaptation and augmenta- tion. In NeurIPS, 2022. 1, 4 [82] Yizhe Zhang, Shubhankar Borse, Hong Cai, and Fatih Porikli. Auxadapt: Stable and efficient test-time adaptation for temporally consistent video semantic segmentation. In WACV, pages 2633–2642, 2022. 1, 3 [83] Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization: A survey. IEEE Trans. Pattern Anal. Mach. Intell., 2022. 1 [84] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Do- main generalization with mixstyle. In ICLR, 2021. 3 [85] Yang Zou, Zhiding Yu, BVK Vijaya Kumar, and Jinsong Wang. Unsupervised domain adaptation for semantic seg- mentation via class-balanced self-training. In ECCV, pages 289–305, 2018. 26. Appendix 6.1. Discussion Societal impact. RoTTA enables adapting pre-trained models on continually changing distributions with correl- atively sampled test streams without any more raw data or label requirements. Thus, our work may have a positive im- pact on communities to effectively deploy and adapt models in various real-world scenarios, which is economically and environmentally friendly. And since no training data is re- quired, this protects data privacy and has potential commer- cial value. We carry out experiments on benchmark datasets and do not notice any societal issues. It does not involve sensitive attributes. Future work. Our work suggests a few promising direc- tions for future work. Firstly, the proposed RoTTA is a preliminary attempt to perform test-time adaptation for the more realistic test stream under the setup PTTA. One could experiment to improve the algorithm by replacing some parts of RoTTA. More importantly, we hope that with this work, we can open a path to the original goal of test-time adaptation, which is performing test-time adaptation in real- world scenarios. Thus, one could improve PTTA to make it more realistic. Limitations. RoTTA achieves excellent performance on various tasks under the setup PTTA as demonstrated in Sec- tion 4 in the main paper, but we still find some limitations of it. Firstly, the adopted robust batch normalization (RBN) is a naive solution to the normalization of the correlatively sampled batch of data. This requires careful design of the value of α in RBN. Secondly, we observe that during the adaptation procedure of some methods like PL [39] and TENT [70], the model collapse finally. Although we de- sign many strategies to stabilize the adaptation and model collapse never occurs in the experiments of RoTTA, we are still missing a way to recover the model from the collapse state as a remedy. Thirdly, category similarity is only one kind of correlation. Although we conduct experiments on different datasets with Dirichlet distribution to simulate cor- relatively sampled test streams, we still need to validate our approach in some real-world scenarios. 6.2. Sensitivity to different hyper-parameters In this section, we conduct a detailed sensitivity analy- sis of the hyperparameters involved in RoTTA. All experi- ments are conducted on CIFAR100→CIFAR100-C, and the corruptions changes as motion, snow, fog, shot, defocus, contrast, zoom, brightness, frost, elastic, glass, gaussian, pixelate, jpeg, and impulse, and test streams are sampled correlatively with the Dirichlet parameter δ = 0.1. When we investigate the sensitivity to a specific hyperparameter, other hyperparameters are fixed to the default values, i.e., λt = 1.0, λu = 1.0, α = 0.05, and ν = 0.001, for all experiments. Table 7. Classification error with different value of λt/λu. λt/λu 0.0/2.0 0.5/1.5 1.0/1.0 1.5/ 0.5 2.0/ 0.0 CIFAR100-C 57.5 36.9 35.0 35.9 38.9 Trade-off between timeliness and uncertainty. When updating the memory bank, we take the timeliness and uncertainty of samples into account simultaneously, and λt and λu will make a trade-off between them. In Table 7, we show the results of RoTTA with varying λt/λu, i.e., λt/λu ∈ {0.0/2.0, 0.5/1.5, 1.0/1.0, 1.5/0.5, 2.0/0.0}. When we consider both of them, the results are relatively stable (35.0-36.9%). When we only think about one side, the performance drops significantly. For example, when we set λt/λu = 0.0/2.0 which means only considering uncer- tainty, the performance drops 22.5%. That’s because some confident samples get stuck in the memory bank, making it not work the way we design it. Table 8. Classification error with varying α α 0.5 0.1 0.05 0.01 0.005 0.001 CIFAR100-C 39.0 36.0 35.0 36.0 38.1 41.5 Sensitivity to α. We show the results of RoTTA with vary- ing α, i.e., α ∈ {0.5, 0.1, 0.05, 0.01, 0.005, 0.001} in Ta- ble 8. A larger value of α means updating the global statis- tics faster and vice versa. We can see that RoTTA achieves competitive results (35.0 − 36.0%) at appropriate values of α, i.e., α ∈ {0.1, 0.05, 0.01}. Updating too aggressively or too gently can lead to unreliable estimates of statistics. Table 9. Classification error with varying ν ν 0.05 0.01 0.005 0.001 0.0005 0.0001 CIFAR100-C 44.8 39.1 37.1 35.0 37.6 43.6 Sensitivity to ν. We show the results of RoTTA with vary- ing ν, i.e., ν ∈ {0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001} in Table 9. As we can see, the best performance is achieved at ν = 0.001. Updating the teacher model too quickly or too slowly can cause performance degradation. 6.3. Additional experiment details and results 6.3.1 Compared methods BN [53] utilizes statistics of the current batch of data to nor- malize their feature maps without tuning any parameters. PL [39] is based on BN [53], and adopts pseudo labels to train the affine parameters in BN layers.TENT [70] is the first to propose fully test-time adaptation. It adopts test-time batch normalization and utilizes entropy minimization to train the affine parameters of BN layers. We reimplement it following the released code https:// github.com/DequanWang/tent. LAME [5] adapts the output of the pre-trained model by optimizing a group of latent variables without tuning any in- ner parts of the model. We reimplement it following the re- leased code https://github.com/fiveai/LAME. CoTTA [73] considers performing test-time adapta- tion on continually changing distributions and pro- pose augmentation-averaged pseudo-labels and stochastic restoration to address error accumulation and catastrophic forgetting. We reimplement it following the released code https://github.com/qinenergy/cotta. NOTE [19] proposes instance-aware normalization and prediction-balanced reservoir sampling to stable the adapta- tion on temporally correlated test streams. We reimplement it following the released code https://github.com/ TaesikGong/NOTE. 6.3.2 Simulate correlatively sampling As we described in the scenarios of autonomous driving that the car will follow more vehicles on the highway or will en- counter more pedestrians on the sidewalk, so we use the same category to simulate correlation. From a macro point of view, the test distribution Ptest changes continually as P0, P1, ...,P∞. During the period when Ptest = Pt, we adopt Dirichlet distribution to simulate correlatively sam- pled test stream. More specifically, we consider dividing samples of C classes into T slots. Firstly, we utilize Dirich- let distribution with parameter γ to generate the partition criterion q ∈ RC×T . Then for each class c, we split samples into T parts according to qc and assign each part to each slot respectively. Finally, we concatenate all slots to sim- ulate the correlatively sampled test stream for Ptest = Pt. And as Ptest changes, we use the above method again to generate the test stream. 6.3.3 Detailed results of different orders We report the average classification error of ten different distribution changing orders in Table 6 of the main pa- per. And then we present the specific results here, includ- ing Table 10, 11, 12, 13, 14, 15, 16, 17, 18, and 19 for CIFAR10→CIFAR10-C and Table 20, 21, 22, 23, 24, 25, 26, 27, 28, and 29 for CIFAR100 →CIFAR100-C. We can see consistently superior performance of RoTTA. One thing to mention is that on DomainNet we use alphabetical order to determine the order of domain changes.Table 10. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method brightnesspixelategaussianmotionzoom glass impulsejpeg defocuselasticshot frost snow fog contrast Avg. Source 9.3 58.5 72.3 34.8 42.0 54.3 72.9 30.3 46.9 26.6 65.7 41.3 25.1 26.0 46.7 43.5BN [53] 71.1 75.2 76.8 74.2 73.7 80.1 79.3 77.5 73.8 77.7 77.2 73.3 73.8 72.7 71.7 75.2PL [39] 71.7 75.9 80.2 78.4 80.2 85.2 85.3 85.4 85.1 86.7 87.9 87.9 88.1 88.3 87.9 83.6TENT [70] 71.6 75.9 81.3 80.5 82.3 85.6 87.1 87.0 87.1 88.1 88.2 87.8 87.9 88.3 88.2 84.4LAME [5] 5.4 56.8 73.1 29.1 37.0 50.5 71.4 22.3 42.8 18.6 65.5 37.3 18.8 20.4 43.6 39.5CoTTA [73] 75.0 79.8 83.1 83.4 83.2 84.0 84.5 83.2 83.5 83.3 83.6 83.0 83.0 83.4 83.7 82.6NOTE [19] 10.1 29.9 47.1 23.4 28.4 48.4 46.1 41.8 26.9 36.1 37.5 25.0 25.0 23.2 14.2 30.9 RoTTA 10.4 26.6 37.5 23.9 17.0 40.9 39.7 30.1 18.0 29.9 30.1 23.6 21.7 17.6 19.0 25.7(+5.2) Table 11. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method jpeg shot zoom frost contrastfog defocuselasticgaussianbrightnessglass impulsepixelatesnow motion Avg. Source 30.3 65.7 42.0 41.3 46.7 26.0 46.9 26.6 72.3 9.3 54.3 72.9 58.5 25.1 34.8 43.5BN [53] 77.6 75.8 73.4 74.1 73.1 72.5 72.9 77.1 77.2 72.2 79.9 79.9 75.5 74.6 72.9 75.2PL [39] 77.6 77.1 76.6 78.3 77.5 79.8 82.0 84.8 86.1 83.5 87.8 87.1 86.5 85.6 85.7 82.4TENT [70] 78.5 78.2 79.2 81.8 84.8 84.8 86.4 87.3 87.9 86.7 87.3 87.8 87.2 87.5 87.1 84.8LAME [5] 22.5 65.2 37.0 37.1 44.0 20.3 41.7 18.7 72.8 5.2 51.2 71.5 57.0 19.0 29.4 39.5CoTTA [73]78.5 81.0 82.8 84.1 84.9 83.4 83.5 83.5 84.5 83.3 84.7 84.6 83.0 84.4 83.4 83.3NOTE [19]35.4 36.1 22.1 21.3 11.6 24.8 24.5 36.0 37.7 18.4 49.0 47.4 43.9 30.4 29.2 31.2 RoTTA 33.2 33.3 19.8 24.1 24.9 20.5 16.2 31.7 28.4 11.8 43.1 36.9 32.5 20.7 20.6 26.5(+4.7) Table 12. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastdefocusgaussianshot snow frost glass zoom elasticjpeg pixelatebrightnessimpulsemotion fog Avg. Source 46.7 46.9 72.3 65.7 25.1 41.3 54.3 42.0 26.6 30.3 58.5 9.3 72.9 34.8 26.0 43.5BN [53] 72.3 72.6 76.9 77.1 74.8 73.5 80.0 73.2 77.4 78.6 76.4 71.0 79.1 73.9 71.5 75.2PL [39] 72.4 75.3 80.7 82.6 83.3 83.5 86.6 85.7 86.6 88.4 87.5 86.6 88.3 88.2 86.8 84.1TENT [70] 73.5 77.9 85.5 86.9 87.6 87.8 88.3 87.7 88.6 89.2 88.5 88.5 89.3 88.6 88.6 86.4LAME [5] 43.5 42.3 73.1 65.3 19.2 37.3 51.1 36.8 18.5 22.5 56.9 5.5 71.1 29.1 20.5 39.5CoTTA [73]79.4 80.3 83.8 83.9 83.9 83.4 85.0 83.2 85.1 84.3 83.9 83.3 84.7 83.9 82.5 83.4NOTE [19] 9.6 21.8 40.1 31.0 25.5 22.6 44.8 22.8 33.2 39.4 33.2 18.1 50.0 28.3 29.8 30.0 RoTTA 18.4 17.9 38.4 31.9 23.3 19.8 40.7 17.4 31.4 29.8 27.8 11.3 43.8 19.7 18.8 26.0(+4.0) Table 13. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method shot fog glass pixelatesnow elasticbrightnessimpulsedefocusfrost contrastgaussianmotionjpeg zoom Avg. Source 65.7 26.0 54.3 58.5 25.1 26.6 9.3 72.9 46.9 41.3 46.7 72.3 34.8 30.3 42.0 43.5BN [53] 76.4 72.0 80.4 76.2 74.8 77.0 71.1 79.6 73.8 74.4 73.0 77.0 72.5 78.3 72.5 75.3PL [39] 77.0 73.3 82.4 79.8 81.0 82.3 79.5 84.4 82.7 83.5 83.5 85.5 84.8 87.0 84.5 82.1TENT [70]76.9 74.6 82.3 81.7 82.0 84.9 84.8 87.3 86.6 87.3 87.6 89.2 88.3 88.9 87.3 84.6LAME [5] 65.3 20.6 50.9 56.7 19.2 18.8 5.4 71.8 42.8 37.2 43.3 73.2 29.4 22.6 36.9 39.6CoTTA [73]77.4 77.6 83.8 81.9 82.2 82.6 80.4 83.3 82.3 81.5 82.7 82.6 81.1 82.9 81.0 81.6NOTE [19]34.0 20.9 43.1 36.6 24.0 36.4 12.1 48.0 25.9 23.9 13.4 38.1 25.0 43.2 24.2 29.9 RoTTA 35.0 21.1 43.9 29.2 22.1 29.7 10.8 44.6 25.3 22.7 24.6 29.4 26.9 34.4 16.1 27.7(+2.2) Table 14. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method pixelateglass zoomsnow fog impulsebrightnessmotionfrost jpeg gaussianshot contrastdefocus elastic Avg. Source 58.5 54.3 42.0 25.1 26.0 72.9 9.3 34.8 41.3 30.3 72.3 65.7 46.7 46.9 26.6 43.5BN [53] 76.0 79.6 73.3 75.2 72.9 79.8 71.1 73.5 74.1 78.6 77.4 76.1 72.0 73.8 76.4 75.3PL [39] 76.7 81.3 77.4 80.3 81.2 86.3 83.3 85.9 86.2 87.7 88.1 88.4 87.4 87.6 87.7 84.4TENT [70] 76.4 80.2 77.8 81.2 83.0 87.1 85.6 87.2 87.6 88.7 88.6 88.9 88.5 88.6 88.2 85.2LAME [5] 56.9 50.7 37.0 19.0 20.3 71.5 5.4 29.2 37.2 22.5 73.0 65.3 43.8 42.4 18.7 39.5CoTTA [73]77.1 83.6 84.1 84.8 84.4 85.2 84.0 84.3 84.9 84.9 85.0 84.7 85.3 84.4 84.3 84.1NOTE [19] 27.8 52.2 24.5 22.3 21.6 44.5 14.5 21.3 25.9 42.5 38.8 36.0 16.7 28.1 40.6 30.5 RoTTA 25.9 43.3 17.7 22.1 20.2 41.5 12.2 22.9 22.5 31.2 33.8 26.0 31.4 17.7 27.6 26.4(+4.1)Table 15. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 34.8 25.1 26.0 65.7 46.9 46.7 42.0 9.3 41.3 26.6 54.3 72.3 58.5 30.3 72.9 43.5BN [53] 73.2 73.4 72.7 77.2 73.7 72.5 72.9 71.0 74.1 77.7 80.0 76.9 75.5 78.3 79.0 75.2PL [39] 73.9 75.0 75.6 81.0 79.9 80.6 82.0 83.2 85.3 87.3 88.3 87.5 87.5 87.5 88.2 82.9TENT [70] 74.3 77.4 80.1 86.2 86.7 87.3 87.9 87.4 88.2 89.0 89.2 89.0 88.3 89.7 89.2 86.0LAME [5] 29.5 19.0 20.3 65.3 42.4 43.4 36.8 5.4 37.2 18.6 51.2 73.2 57.0 22.6 71.3 39.5CoTTA [73]77.1 80.6 83.1 84.4 83.9 84.2 83.1 82.6 84.4 84.2 84.5 84.6 82.7 83.8 84.9 83.2NOTE [19] 18.0 22.1 20.6 35.6 26.9 13.6 26.5 17.3 27.2 37.0 48.3 38.8 42.6 41.9 49.7 31.1 RoTTA 18.1 21.3 18.8 33.6 23.6 16.5 15.1 11.2 21.9 30.7 39.6 26.8 33.7 27.8 39.5 25.2(+5.9) Table 16. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method frost impulsejpeg contrastzoom glass pixelatesnow defocusmotionbrightnesselasticshot fog gaussian Avg. Source 41.3 72.9 30.3 46.7 42.0 54.3 58.5 25.1 46.9 34.8 9.3 26.6 65.7 26.0 72.3 43.5BN [53] 73.8 79.1 77.9 73.0 73.7 80.1 75.7 74.4 73.7 74.0 71.7 77.0 75.9 72.8 76.2 75.3PL [39] 74.2 80.9 80.4 79.5 81.8 85.9 83.9 85.1 84.7 85.9 85.9 86.7 87.2 87.0 87.8 83.8TENT [70]73.9 80.3 81.8 81.6 83.6 86.3 85.6 85.7 86.4 87.7 87.4 88.8 88.8 88.5 88.4 85.0LAME [5] 37.4 71.8 22.4 43.5 37.0 50.5 57.0 19.0 42.8 29.1 5.4 18.7 65.2 20.4 72.9 39.5CoTTA [73]76.5 82.2 82.8 85.0 82.9 85.0 83.0 82.9 83.5 83.4 82.6 83.7 83.2 83.3 83.6 82.9NOTE [19]21.1 41.4 36.3 10.2 21.7 46.7 37.5 26.4 26.1 21.4 14.3 37.9 38.5 24.4 40.7 29.6 RoTTA 22.2 44.9 35.2 18.8 19.7 41.5 28.5 23.2 21.2 18.6 12.4 30.0 27.4 20.0 31.2 26.3(+3.3) Table 17. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method defocusmotionzoom shot gaussianglass jpeg fog contrastpixelatefrost snow brightnesselastic impulse Avg. Source 46.9 34.8 42.0 65.7 72.3 54.3 30.3 26.0 46.7 58.5 41.3 25.1 9.3 26.6 72.9 43.5BN [53] 72.8 72.7 73.3 77.2 77.3 80.0 77.6 72.6 73.3 76.6 73.8 74.1 70.3 77.5 79.0 75.2PL [39] 73.2 74.6 76.5 81.7 82.8 84.6 85.1 84.6 86.2 86.4 86.1 87.1 86.8 88.4 88.1 83.5TENT [70] 73.7 74.3 77.1 82.5 84.3 86.9 87.4 86.6 88.0 88.5 88.1 88.5 88.4 89.4 88.9 84.8LAME [5] 42.5 29.3 37.0 65.3 73.2 50.5 22.5 20.5 43.5 56.9 37.1 18.9 5.4 18.5 71.3 39.5CoTTA [73]76.3 79.8 82.4 83.3 83.8 84.5 83.1 82.7 84.7 82.9 83.0 83.3 81.4 83.8 83.8 82.6NOTE [19] 18.5 18.8 23.6 36.5 33.7 47.8 38.6 22.8 13.0 40.0 29.2 26.3 17.5 44.0 52.9 30.9 RoTTA 17.0 17.5 16.5 33.8 33.3 42.7 29.4 18.0 19.6 29.5 20.7 22.1 11.5 29.5 38.1 25.3(+5.6) Table 18. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method glass zoom impulsefog snow jpeg gaussianfrost shot brightnesscontrastmotionpixelatedefocus elastic Avg. Source 54.3 42.0 72.9 26.0 25.1 30.3 72.3 41.3 65.7 9.3 46.7 34.8 58.5 46.9 26.6 43.5BN [53] 79.7 72.3 79.8 73.2 74.7 77.7 76.6 73.2 77.1 72.2 73.0 73.3 75.5 73.8 76.4 75.2PL [39] 79.6 73.2 81.3 77.3 79.1 83.0 83.2 83.0 85.5 84.3 87.0 86.9 86.4 86.5 87.6 82.9TENT [70] 79.5 74.1 84.2 82.2 84.5 86.5 86.7 85.9 87.2 86.6 86.8 87.3 86.9 87.4 87.3 84.9LAME [5] 50.8 36.9 71.3 20.6 19.2 22.4 72.5 37.2 65.4 5.2 43.3 29.1 57.0 42.4 18.7 39.5CoTTA [73]81.5 79.4 85.2 84.1 84.5 84.2 84.8 84.0 84.8 83.2 85.2 83.8 83.2 84.6 83.6 83.7NOTE [19]45.0 21.2 42.3 21.0 21.6 38.4 36.4 21.4 33.1 16.7 14.6 25.4 43.5 29.1 38.5 29.9 RoTTA 42.6 17.6 48.1 23.9 21.9 32.6 32.1 20.7 30.2 12.0 21.9 20.0 33.7 16.4 28.1 26.8(+3.1) Table 19. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastgaussiandefocuszoom frost glass jpeg fog pixelateelasticshot impulsesnow motion brightness Avg. Source 46.7 72.3 46.9 42.0 41.3 54.3 30.3 26.0 58.5 26.6 65.7 72.9 25.1 34.8 9.3 43.5BN [53] 72.4 76.2 73.2 73.7 73.6 80.0 77.6 72.6 76.4 77.7 77.2 79.9 73.8 73.9 70.0 75.2PL [39] 73.0 78.2 76.7 79.7 81.6 85.6 86.0 85.3 87.2 88.2 88.3 88.9 88.5 89.2 88.2 84.3TENT [70] 73.6 80.9 83.1 85.6 87.1 88.5 88.8 88.4 89.2 89.3 89.0 89.0 89.3 89.9 89.1 86.7LAME [5] 43.5 73.2 42.3 37.0 37.2 50.5 22.5 20.5 57.0 18.6 65.5 71.5 18.8 29.1 5.6 39.5CoTTA [73]79.5 81.4 83.4 83.6 83.9 85.0 84.0 82.8 84.8 84.8 84.5 84.7 84.1 84.4 82.8 83.6NOTE [19] 9.6 43.6 26.5 24.8 23.9 46.9 38.0 23.4 34.0 41.2 41.5 45.0 27.6 25.8 19.0 31.4 RoTTA 18.4 36.0 21.1 15.6 23.0 41.7 30.8 19.1 34.1 31.1 31.3 39.9 26.0 18.8 12.8 26.6(+4.8)Table 20. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method brightnesspixelategaussianmotionzoom glass impulsejpeg defocuselasticshot frost snow fog contrast Avg. Source 29.5 74.7 73.0 30.8 28.8 54.1 39.4 41.2 29.3 37.2 68.0 45.8 39.5 50.3 55.1 46.4BN [53] 46.5 52.0 58.6 47.4 47.4 57.6 58.2 56.9 47.0 53.4 56.0 52.5 53.1 57.7 49.1 52.9PL [39] 48.5 60.7 77.1 85.9 91.5 95.5 95.8 96.6 96.8 96.9 97.3 97.5 97.6 97.7 97.9 88.9TENT [70] 49.8 69.4 92.2 96.0 96.7 97.3 97.5 97.9 97.5 97.9 98.0 98.2 98.2 98.2 98.2 92.2LAME [5] 21.7 75.1 72.7 22.9 20.6 49.0 32.1 33.3 21.2 28.0 66.8 40.0 30.6 43.9 51.3 40.6CoTTA [73] 46.8 48.4 54.7 48.7 48.6 53.5 55.4 52.8 49.8 51.8 53.5 52.9 54.1 56.7 53.6 52.1NOTE [19] 42.6 53.0 69.9 52.1 53.3 70.4 73.1 76.7 80.8 96.0 97.7 97.1 96.6 97.2 95.8 76.8 RoTTA 28.4 37.3 44.6 31.9 28.3 41.8 43.6 39.9 28.0 35.2 38.2 33.7 33.0 39.5 31.0 35.6(+5.0) Table 21. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method jpeg shot zoom frost contrastfog defocuselasticgaussianbrightnessglass impulsepixelatesnow motion Avg. Source 41.2 68.0 28.8 45.8 55.1 50.3 29.3 37.2 73.0 29.5 54.1 39.4 74.7 39.5 30.8 46.4BN [53] 58.3 56.8 47.8 51.8 48.9 57.3 46.8 53.5 57.8 45.5 57.1 58.5 51.7 53.3 48.8 52.9PL [39] 59.4 66.3 74.9 87.5 94.2 95.5 96.2 97.1 97.4 97.2 97.5 97.7 98.0 98.2 98.2 90.4TENT [70] 62.0 79.3 91.7 95.8 96.9 97.0 97.4 97.7 97.6 97.7 97.9 97.9 98.0 97.9 97.9 93.5LAME [5] 33.6 66.7 21.1 39.9 50.6 43.9 21.0 28.6 72.5 21.6 48.6 32.5 74.5 30.6 22.5 40.6CoTTA [73]54.6 54.1 49.6 52.1 52.7 58.0 50.3 53.3 55.0 49.1 55.4 55.7 51.0 54.6 52.1 53.2NOTE [19]60.4 63.0 49.9 55.7 47.0 65.2 59.4 76.6 90.9 87.2 96.8 97.0 97.3 96.7 96.8 76.0 RoTTA 43.9 45.3 31.0 37.3 35.7 41.2 27.7 34.8 39.7 26.6 39.5 41.9 32.0 33.0 30.5 36.0(+4.6) Table 22. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastdefocusgaussianshot snow frost glass zoom elasticjpeg pixelatebrightnessimpulsemotion fog Avg. Source 55.1 29.3 73.0 68.0 39.5 45.8 54.1 28.8 37.2 41.2 74.7 29.5 39.4 30.8 50.3 46.4BN [53] 49.4 47.2 58.6 56.2 52.7 52.0 57.9 46.1 54.4 57.7 50.5 46.2 58.2 47.6 58.5 52.9PL [39] 54.8 64.2 83.3 92.4 95.5 96.5 96.9 96.4 97.2 97.4 97.8 97.8 97.9 97.7 98.0 90.9TENT [70] 60.2 83.1 95.2 96.5 96.9 97.3 97.0 97.3 97.8 97.8 97.6 97.9 97.8 97.9 98.1 93.9LAME [5] 51.3 21.3 72.7 66.3 30.2 40.0 48.6 20.9 27.7 33.3 75.0 21.5 32.2 22.5 43.8 40.5CoTTA [73]52.1 48.6 55.1 52.7 53.4 51.9 55.9 49.2 53.2 52.8 49.2 49.7 56.2 50.7 58.1 52.6NOTE [19] 39.5 45.9 68.8 61.8 57.4 58.5 71.4 66.5 80.8 90.9 94.2 94.9 97.0 95.5 96.6 74.6 RoTTA 41.7 30.5 44.9 40.5 35.4 34.1 40.5 28.2 34.5 39.5 31.1 26.7 43.3 31.4 38.8 36.1(+4.4) Table 23. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method shot fog glass pixelatesnow elasticbrightnessimpulsedefocusfrost contrastgaussianmotionjpeg zoom Avg. Source 68.0 50.3 54.1 74.7 39.5 37.2 29.5 39.4 29.3 45.8 55.1 73.0 30.8 41.2 28.8 46.4BN [53] 57.5 58.6 58.5 50.5 52.7 53.1 45.9 57.9 47.0 51.5 47.8 58.2 48.2 57.1 47.7 52.8PL [39] 59.5 72.9 85.1 89.6 94.5 96.8 97.1 97.9 97.8 98.0 98.3 98.2 98.0 98.0 98.2 92.0TENT [70]60.3 81.4 95.0 96.6 97.0 97.3 97.3 97.7 97.7 97.7 97.8 97.7 97.6 97.6 97.9 93.8LAME [5] 66.4 43.2 49.0 75.2 30.2 28.5 21.6 32.5 21.2 39.5 52.0 72.8 22.3 33.1 20.5 40.5CoTTA [73]54.5 58.4 55.6 50.0 53.9 53.4 50.3 56.7 51.3 53.2 53.7 56.1 52.0 54.5 51.5 53.7NOTE [19]61.8 60.2 63.4 55.6 59.8 65.9 58.6 75.1 77.8 93.8 94.2 97.0 95.0 95.5 94.4 76.5 RoTTA 45.5 44.5 43.5 35.6 35.1 35.7 26.2 44.0 29.7 34.2 32.0 40.7 31.4 39.4 27.7 36.3(+4.2) Table 24. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method pixelateglass zoomsnow fog impulsebrightnessmotionfrost jpeg gaussianshot contrastdefocus elastic Avg. Source 74.7 54.1 28.8 39.5 50.3 39.4 29.5 30.8 45.8 41.2 73.0 68.0 55.1 29.3 37.2 46.4BN [53] 51.7 58.6 47.8 52.9 57.1 58.2 45.9 47.6 52.9 57.8 57.5 56.7 49.5 46.1 54.0 52.9PL [39] 52.4 68.0 73.4 87.9 93.7 96.1 95.7 96.0 96.5 96.7 97.5 97.7 97.7 97.3 97.7 89.6TENT [70] 53.5 77.8 91.1 96.0 97.0 97.6 97.4 97.6 97.9 98.1 98.1 98.0 98.1 97.9 98.1 92.9LAME [5] 74.8 48.2 21.1 30.6 43.4 32.5 21.6 23.0 39.6 33.3 72.7 66.5 51.5 20.7 27.5 40.5CoTTA [73]49.3 55.1 49.1 52.9 56.8 55.7 49.5 50.0 53.6 53.4 54.9 53.9 53.8 50.1 53.5 52.8NOTE [19] 52.2 64.9 47.5 57.0 61.9 67.3 60.4 67.8 77.4 90.6 97.1 96.8 92.8 95.9 96.6 75.1 RoTTA 36.4 44.4 29.7 36.5 41.0 44.1 26.8 29.5 33.0 40.3 40.3 38.2 33.9 28.5 34.9 35.8(+4.7)Table 25. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 30.8 39.5 50.3 68.0 29.3 55.1 28.8 29.5 45.8 37.2 54.1 73.0 74.7 41.2 39.4 46.4BN [53] 48.5 54.0 58.9 56.2 46.4 48.0 47.0 45.4 52.9 53.4 57.1 58.2 51.7 57.1 58.8 52.9PL [39] 50.6 62.1 73.9 87.8 90.8 96.0 94.8 96.4 97.4 97.2 97.4 97.4 97.3 97.4 97.4 88.9TENT [70] 53.3 77.6 93.0 96.5 96.7 97.5 97.1 97.5 97.3 97.2 97.1 97.7 97.6 98.0 98.3 92.8LAME [5] 22.4 30.4 43.9 66.3 21.3 51.7 20.6 21.8 39.6 28.0 48.7 72.8 74.6 33.1 32.3 40.5CoTTA [73]49.2 52.7 56.8 53.0 48.7 51.7 49.4 48.7 52.5 52.2 54.3 54.9 49.6 53.4 56.2 52.2NOTE [19] 45.7 53.0 58.2 65.6 54.2 52.0 59.8 63.5 74.8 91.8 98.1 98.3 96.8 97.0 98.2 73.8 RoTTA 31.8 36.7 40.9 42.1 30.0 33.6 27.9 25.4 32.3 34.0 38.8 38.7 31.3 38.0 42.9 35.0(+5.5) Table 26. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method frost impulsejpeg contrastzoom glass pixelatesnow defocusmotionbrightnesselasticshot fog gaussian Avg. Source 45.8 39.4 41.2 55.1 28.8 54.1 74.7 39.5 29.3 30.8 29.5 37.2 68.0 50.3 73.0 46.4BN [53] 52.9 58.8 57.6 48.2 47.4 57.6 50.9 52.4 47.0 47.2 45.1 54.0 56.4 57.7 58.2 52.8PL [39] 56.9 73.3 86.7 94.4 95.8 97.3 97.2 97.4 97.6 97.4 97.7 97.6 97.8 98.3 98.1 92.2TENT [70]60.1 84.2 95.7 97.2 97.4 97.9 97.8 98.0 98.1 98.2 98.3 98.4 98.4 98.4 98.4 94.4LAME [5] 39.9 32.4 33.4 51.4 20.6 49.0 74.4 31.3 21.2 22.6 21.9 28.1 66.9 43.9 72.5 40.6CoTTA [73]51.5 55.3 54.3 51.8 49.4 55.3 50.7 54.2 51.4 50.6 49.5 53.6 55.0 57.1 55.8 53.0NOTE [19]51.6 60.9 60.3 45.4 54.3 70.8 68.8 75.0 75.7 87.1 94.7 95.6 96.7 96.4 97.2 75.4 RoTTA 40.0 46.3 42.8 36.4 29.2 42.3 33.2 34.4 28.4 29.2 26.4 34.5 38.5 39.8 39.3 36.0(+4.6) Table 27. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method defocusmotionzoom shot gaussianglass jpeg fog contrastpixelatefrost snow brightnesselastic impulse Avg. Source 29.3 30.8 28.8 68.0 73.0 54.1 41.2 50.3 55.1 74.7 45.8 39.5 29.5 37.2 39.4 46.4BN [53] 47.1 48.6 47.8 56.2 57.6 57.6 57.6 57.5 48.7 50.6 51.8 53.2 46.9 53.5 58.8 52.9PL [39] 48.8 58.7 69.9 88.0 95.1 96.6 96.7 96.9 97.4 97.4 98.2 98.2 98.2 98.3 98.5 89.1TENT [70] 51.0 67.6 85.8 95.9 97.2 97.5 97.2 97.7 98.1 97.9 97.7 97.7 98.0 98.0 98.2 91.7LAME [5] 21.2 22.8 21.1 66.3 72.8 49.0 33.3 44.8 51.7 74.9 39.8 31.2 21.3 27.3 32.3 40.6CoTTA [73]48.4 48.8 48.2 52.9 54.0 53.8 52.7 57.2 52.6 48.6 51.8 53.9 49.4 52.3 56.0 52.0NOTE [19] 45.1 46.7 49.1 67.3 65.5 69.4 75.5 80.3 83.8 96.0 97.6 97.1 96.1 97.9 98.7 77.7 RoTTA 29.6 31.3 28.8 43.9 41.5 41.3 40.9 39.8 32.1 32.6 33.1 33.0 26.5 34.5 42.9 35.4(+5.2) Table 28. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method glass zoom impulsefog snow jpeg gaussianfrost shot brightnesscontrastmotionpixelatedefocus elastic Avg. Source 54.1 28.8 39.4 50.3 39.5 41.2 73.0 45.8 68.0 29.5 55.1 30.8 74.7 29.3 37.2 46.4BN [53] 58.8 47.7 59.2 57.6 52.7 56.9 58.2 52.0 56.7 45.5 47.8 48.2 51.7 46.1 54.0 52.9PL [39] 60.1 59.5 75.1 85.7 91.5 94.6 96.5 97.1 97.4 97.3 98.0 97.7 97.9 97.8 97.7 89.6TENT [70] 61.6 71.5 91.0 95.9 96.6 97.1 96.9 97.3 97.4 97.2 97.9 98.0 98.1 97.9 97.8 92.8LAME [5] 48.6 20.6 32.3 44.4 30.2 33.6 72.4 40.0 66.3 21.6 52.0 22.8 74.6 20.7 27.5 40.5CoTTA [73]56.4 48.9 56.1 57.8 54.1 54.2 56.2 53.6 55.4 50.0 53.6 51.6 51.2 50.7 54.4 53.6NOTE [19]62.5 46.3 61.5 61.1 58.6 68.4 76.1 78.3 92.0 93.4 96.1 95.4 96.2 95.8 96.4 78.5 RoTTA 45.5 30.0 45.9 42.6 35.3 41.8 42.2 34.5 40.2 27.3 31.3 30.2 32.7 28.1 34.9 36.2(+4.3) Table 29. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastgaussiandefocuszoom frost glass jpeg fog pixelateelasticshot impulsesnow motion brightness Avg. Source 55.1 73.0 29.3 28.8 45.8 54.1 41.2 50.3 74.7 37.2 68.0 39.4 39.5 30.8 29.5 46.4BN [53] 49.5 58.8 47.0 46.5 52.2 57.6 57.6 57.6 51.7 53.5 56.0 58.5 53.1 47.6 46.3 52.9PL [39] 53.6 70.4 76.0 85.1 91.2 95.2 96.0 97.0 96.9 97.3 97.3 97.6 97.5 97.6 97.7 89.8TENT [70] 60.2 89.1 95.0 96.2 96.9 97.0 96.5 97.0 97.0 97.2 97.6 97.8 97.5 97.9 97.7 94.0LAME [5] 51.3 72.5 21.5 21.0 39.6 49.0 33.3 44.8 74.8 28.0 66.8 32.5 30.6 22.5 21.4 40.6CoTTA [73]52.3 55.3 49.5 48.1 52.1 54.8 52.7 56.9 50.6 52.6 53.7 55.8 54.6 50.6 50.5 52.7NOTE [19] 39.1 64.7 48.9 50.6 59.1 70.1 71.7 75.0 85.2 95.7 96.9 98.4 96.0 95.9 94.9 76.1 RoTTA 41.4 46.2 30.5 28.5 36.0 40.9 40.5 39.6 33.0 35.0 38.2 43.1 33.9 30.7 27.1 36.3(+4.3)",
      "meta_data": {
        "arxiv_id": "2303.13899v1",
        "authors": [
          "Longhui Yuan",
          "Binhui Xie",
          "Shuang Li"
        ],
        "published_date": "2023-03-24T10:19:14Z",
        "pdf_url": "https://arxiv.org/pdf/2303.13899v1.pdf",
        "github_url": "https://github.com/BIT-DA/RoTTA"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Practical Test-Time Adaptation (PTTA), a new and more realistic Test-Time Adaptation (TTA) setup that simultaneously considers continually changing distributions and correlative sampling in test data streams. It proposes Robust Test-Time Adaptation (RoTTA) as a solution to the challenges of PTTA. RoTTA significantly outperforms prior methods, achieving state-of-the-art results on CIFAR-10-C, CIFAR-100-C, and DomainNet, reducing average classification error by over 5.9%, 5.5%, and 2.2% respectively.",
        "methodology": "The RoTTA method consists of three key components: 1) Robust Batch Normalization (RBN), which estimates normalization statistics using global running means and variances updated by an Exponential Moving Average (EMA) from buffered samples, replacing unreliable current batch statistics. 2) Category-balanced sampling with timeliness and uncertainty (CSTU), which maintains a memory bank to capture a stable snapshot of the current distribution. It prioritizes newer and less uncertain samples based on a heuristic score combining age and prediction entropy, ensuring category balance. 3) Robust training with timeliness, which utilizes a teacher-student model for adaptation. The student model's affine parameters in RBN are updated using a timeliness-reweighted cross-entropy loss, while the teacher model is updated via EMA of the student's parameters, stabilizing the adaptation process.",
        "experimental_setup": "Experiments were conducted on CIFAR-10-C and CIFAR-100-C for robustness to corruptions, and DomainNet for generalization under large domain shifts. Continually changing distributions were simulated by sequentially applying different corruption types at the highest severity (5) for CIFAR-C datasets, and by adapting to different target domains for DomainNet. Correlative sampling was simulated using a Dirichlet distribution with parameter δ=0.1. Pre-trained models included WildResNet-28 for CIFAR10-C, ResNeXt-29 for CIFAR100-C (from RobustBench), and ResNet-101 for DomainNet. All experiments used the Adam optimizer (learning rate 1.0e-3, β=0.9), a batch size of 64, and a memory bank capacity of N=64. RoTTA hyperparameters (α=0.05, ν=0.001, λt=1.0, λu=1.0, δ=0.1) were unified across all experiments. Ablation studies and sensitivity analyses on hyperparameters, batch size, and distribution changing order were also performed.",
        "limitations": "The Robust Batch Normalization (RBN) is a relatively naive solution for normalization and requires careful tuning of the `α` parameter. The current method does not include a mechanism to recover the model from a collapsed state, although RoTTA itself did not experience collapse in experiments. The simulation of correlative sampling primarily focuses on category similarity using Dirichlet distribution, and further validation in diverse real-world scenarios is needed.",
        "future_research_directions": "Future work could involve improving the RoTTA algorithm by refining or replacing some of its existing components. A significant direction is to further improve the Practical Test-Time Adaptation (PTTA) setup itself to make it even more realistic and representative of real-world challenges. The authors hope this work encourages further research towards developing TTA algorithms for practical deployment in dynamic real-world applications.",
        "experimental_code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nimport torchvision.transforms.functional as TF\nimport math\nfrom copy import deepcopy\nimport random\nimport numpy as np\nimport PIL\n\n\n# From core/adapter/base_adapter.py\nclass BaseAdapter(nn.Module):\n    def __init__(self, cfg, model, optimizer):\n        super().__init__()\n        self.cfg = cfg\n        self.model = self.configure_model(model)\n        # Simplified: actual BaseAdapter collects params and initializes optimizer\n        self.optimizer = optimizer # Assume optimizer is built elsewhere for brevity\n        self.steps = self.cfg.OPTIM.STEPS\n\n    def forward_and_adapt(self, *args):\n        raise NotImplementedError(\"implement forward_and_adapt by yourself!\")\n\n    def configure_model(self, model):\n        raise NotImplementedError(\"implement configure_model by yourself!\")\n\n    @staticmethod\n    def build_ema(model):\n        ema_model = deepcopy(model)\n        for param in ema_model.parameters():\n            param.detach_()\n        return ema_model\n\n\n@torch.jit.script\ndef softmax_entropy(x, x_ema):\n    return -(x_ema.softmax(1) * x.log_softmax(1)).sum(1)\n\n\n# From core/utils/bn_layers.py\nclass MomentumBN(nn.Module):\n    def __init__(self, bn_layer: nn.BatchNorm2d, momentum):\n        super().__init__()\n        self.num_features = bn_layer.num_features\n        self.momentum = momentum\n        if bn_layer.track_running_stats and bn_layer.running_var is not None and bn_layer.running_mean is not None:\n            self.register_buffer(\"source_mean\", deepcopy(bn_layer.running_mean))\n            self.register_buffer(\"source_var\", deepcopy(bn_layer.running_var))\n            self.source_num = bn_layer.num_batches_tracked\n        self.weight = deepcopy(bn_layer.weight)\n        self.bias = deepcopy(bn_layer.bias)\n\n        self.register_buffer(\"target_mean\", torch.zeros_like(self.source_mean))\n        self.register_buffer(\"target_var\", torch.ones_like(self.source_var))\n        self.eps = bn_layer.eps\n\n        self.current_mu = None\n        self.current_sigma = None\n\n    def forward(self, x):\n        raise NotImplementedError\n\n\nclass RobustBN1d(MomentumBN):\n    def forward(self, x):\n        if self.training:\n            b_var, b_mean = torch.var_mean(x, dim=0, unbiased=False, keepdim=False)  # (C,)\n            mean = (1 - self.momentum) * self.source_mean + self.momentum * b_mean\n            var = (1 - self.momentum) * self.source_var + self.momentum * b_var\n            self.source_mean, self.source_var = deepcopy(mean.detach()), deepcopy(var.detach())\n            mean, var = mean.view(1, -1), var.view(1, -1)\n        else:\n            mean, var = self.source_mean.view(1, -1), self.source_var.view(1, -1)\n\n        x = (x - mean) / torch.sqrt(var + self.eps)\n        weight = self.weight.view(1, -1)\n        bias = self.bias.view(1, -1)\n\n        return x * weight + bias\n\n\nclass RobustBN2d(MomentumBN):\n    def forward(self, x):\n        if self.training:\n            b_var, b_mean = torch.var_mean(x, dim=[0, 2, 3], unbiased=False, keepdim=False)  # (C,)\n            mean = (1 - self.momentum) * self.source_mean + self.momentum * b_mean\n            var = (1 - self.momentum) * self.source_var + self.momentum * b_var\n            self.source_mean, self.source_var = deepcopy(mean.detach()), deepcopy(var.detach())\n            mean, var = mean.view(1, -1, 1, 1), var.view(1, -1, 1, 1)\n        else:\n            mean, var = self.source_mean.view(1, -1, 1, 1), self.source_var.view(1, -1, 1, 1)\n\n        x = (x - mean) / torch.sqrt(var + self.eps)\n        weight = self.weight.view(1, -1, 1, 1)\n        bias = self.bias.view(1, -1, 1, 1)\n\n        return x * weight + bias\n\n\n# From core/utils/memory.py\nclass MemoryItem:\n    def __init__(self, data=None, uncertainty=0, age=0):\n        self.data = data\n        self.uncertainty = uncertainty\n        self.age = age\n\n    def increase_age(self):\n        if not self.empty():\n            self.age += 1\n\n    def get_data(self):\n        return self.data, self.uncertainty, self.age\n\n    def empty(self):\n        return self.data == \"empty\"\n\n\nclass CSTU:\n    def __init__(self, capacity, num_class, lambda_t=1.0, lambda_u=1.0):\n        self.capacity = capacity\n        self.num_class = num_class\n        self.per_class = self.capacity / self.num_class\n        self.lambda_t = lambda_t\n        self.lambda_u = lambda_u\n\n        self.data: list[list[MemoryItem]] = [[] for _ in range(self.num_class)]\n\n    def get_occupancy(self):\n        occupancy = 0\n        for data_per_cls in self.data:\n            occupancy += len(data_per_cls)\n        return occupancy\n\n    def per_class_dist(self):\n        per_class_occupied = [0] * self.num_class\n        for cls, class_list in enumerate(self.data):\n            per_class_occupied[cls] = len(class_list)\n\n        return per_class_occupied\n\n    def add_instance(self, instance):\n        assert (len(instance) == 3)\n        x, prediction, uncertainty = instance\n        new_item = MemoryItem(data=x, uncertainty=uncertainty, age=0)\n        new_score = self.heuristic_score(0, uncertainty)\n        if self.remove_instance(prediction, new_score):\n            self.data[prediction].append(new_item)\n        self.add_age()\n\n    def remove_instance(self, cls, score):\n        class_list = self.data[cls]\n        class_occupied = len(class_list)\n        all_occupancy = self.get_occupancy()\n        if class_occupied < self.per_class:\n            if all_occupancy < self.capacity:\n                return True\n            else:\n                majority_classes = self.get_majority_classes()\n                return self.remove_from_classes(majority_classes, score)\n        else:\n            return self.remove_from_classes([cls], score)\n\n    def remove_from_classes(self, classes: list[int], score_base):\n        max_class = None\n        max_index = None\n        max_score = None\n        for cls in classes:\n            for idx, item in enumerate(self.data[cls]):\n                uncertainty = item.uncertainty\n                age = item.age\n                score = self.heuristic_score(age=age, uncertainty=uncertainty)\n                if max_score is None or score >= max_score:\n                    max_score = score\n                    max_index = idx\n                    max_class = cls\n\n        if max_class is not None:\n            if max_score > score_base:\n                self.data[max_class].pop(max_index)\n                return True\n            else:\n                return False\n        else:\n            return True\n\n    def get_majority_classes(self):\n        per_class_dist = self.per_class_dist()\n        max_occupied = max(per_class_dist)\n        classes = []\n        for i, occupied in enumerate(per_class_dist):\n            if occupied == max_occupied:\n                classes.append(i)\n\n        return classes\n\n    def heuristic_score(self, age, uncertainty):\n        return self.lambda_t * 1 / (1 + math.exp(-age / self.capacity)) + self.lambda_u * uncertainty / math.log(self.num_class)\n\n    def add_age(self):\n        for class_list in self.data:\n            for item in class_list:\n                item.increase_age()\n        return\n\n    def get_memory(self):\n        tmp_data = []\n        tmp_age = []\n\n        for class_list in self.data:\n            for item in class_list:\n                tmp_data.append(item.data)\n                tmp_age.append(item.age)\n\n        tmp_age = [x / self.capacity for x in tmp_age]\n\n        return tmp_data, tmp_age\n\n\n# From core/utils/utils.py\ndef get_named_submodule(model, sub_name: str):\n    names = sub_name.split(\".\")\n    module = model\n    for name in names:\n        module = getattr(module, name)\n    return module\n\ndef set_named_submodule(model, sub_name, value):\n    names = sub_name.split(\".\")\n    module = model\n    for i in range(len(names)):\n        if i != len(names) - 1:\n            module = getattr(module, names[i])\n        else:\n            setattr(module, names[i], value)\n\n\n# From core/utils/custom_transforms.py\ndef get_tta_transforms(cfg, gaussian_std: float=0.005, soft=False):\n    img_shape = (*cfg.INPUT.SIZE, 3)\n    n_pixels = img_shape[0]\n\n    clip_min, clip_max = 0.0, 1.0\n    p_hflip = 0.5\n\n    tta_transforms = transforms.Compose([\n        Clip(0.0, 1.0),\n        ColorJitterPro(\n            brightness=[0.8, 1.2] if soft else [0.6, 1.4],\n            contrast=[0.85, 1.15] if soft else [0.7, 1.3],\n            saturation=[0.75, 1.25] if soft else [0.5, 1.5],\n            hue=[-0.03, 0.03] if soft else [-0.06, 0.06],\n            gamma=[0.85, 1.15] if soft else [0.7, 1.3]\n        ),\n        transforms.Pad(padding=int(n_pixels / 2), padding_mode='edge'),\n        transforms.RandomAffine(\n            degrees=[-8, 8] if soft else [-15, 15],\n            translate=(1/16, 1/16),\n            scale=(0.95, 1.05) if soft else (0.9, 1.1),\n            shear=None,\n            resample=PIL.Image.BILINEAR,\n            fillcolor=None\n        ),\n        transforms.GaussianBlur(kernel_size=5, sigma=[0.001, 0.25] if soft else [0.001, 0.5]),\n        transforms.CenterCrop(size=n_pixels),\n        transforms.RandomHorizontalFlip(p=p_hflip),\n        GaussianNoise(0, gaussian_std),\n        Clip(clip_min, clip_max)\n    ])\n    return tta_transforms\n\n\nclass GaussianNoise(torch.nn.Module):\n    def __init__(self, mean=0., std=1.):\n        super().__init__()\n        self.std = std\n        self.mean = mean\n\n    def forward(self, img):\n        noise = torch.randn(img.size()) * self.std + self.mean\n        noise = noise.to(img.device)\n        return img + noise\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n\n\nclass Clip(torch.nn.Module):\n    def __init__(self, min_val=0., max_val=1.):\n        super().__init__()\n        self.min_val = min_val\n        self.max_val = max_val\n\n    def forward(self, img):\n        return torch.clip(img, self.min_val, self.max_val)\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(min_val={0}, max_val={1})'.format(self.min_val, self.max_val)\n\n\nclass ColorJitterPro(transforms.ColorJitter):\n    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0, gamma=0):\n        super().__init__(brightness, contrast, saturation, hue)\n        self.gamma = self._check_input(gamma, 'gamma')\n\n    @staticmethod\n    @torch.jit.unused\n    def get_params(brightness, contrast, saturation, hue, gamma):\n        transforms_list = []\n        if brightness is not None:\n            brightness_factor = random.uniform(brightness[0], brightness[1])\n            transforms_list.append(lambda img: TF.adjust_brightness(img, brightness_factor))\n        if contrast is not None:\n            contrast_factor = random.uniform(contrast[0], contrast[1])\n            transforms_list.append(lambda img: TF.adjust_contrast(img, contrast_factor))\n        if saturation is not None:\n            saturation_factor = random.uniform(saturation[0], saturation[1])\n            transforms_list.append(lambda img: TF.adjust_saturation(img, saturation_factor))\n        if hue is not None:\n            hue_factor = random.uniform(hue[0], hue[1])\n            transforms_list.append(lambda img: TF.adjust_hue(img, hue_factor))\n        if gamma is not None:\n            gamma_factor = random.uniform(gamma[0], gamma[1])\n            transforms_list.append(lambda img: TF.adjust_gamma(img, gamma_factor))\n        random.shuffle(transforms_list)\n        transform = transforms.Compose(transforms_list)\n        return transform\n\n    def forward(self, img):\n        fn_idx = torch.randperm(5)\n        for fn_id in fn_idx:\n            if fn_id == 0 and self.brightness is not None:\n                brightness = self.brightness\n                brightness_factor = torch.tensor(1.0).uniform_(brightness[0], brightness[1]).item()\n                img = TF.adjust_brightness(img, brightness_factor)\n            if fn_id == 1 and self.contrast is not None:\n                contrast = self.contrast\n                contrast_factor = torch.tensor(1.0).uniform_(contrast[0], contrast[1]).item()\n                img = TF.adjust_contrast(img, contrast_factor)\n            if fn_id == 2 and self.saturation is not None:\n                saturation = self.saturation\n                saturation_factor = torch.tensor(1.0).uniform_(saturation[0], saturation[1]).item()\n                img = TF.adjust_saturation(img, saturation_factor)\n            if fn_id == 3 and self.hue is not None:\n                hue = self.hue\n                hue_factor = torch.tensor(1.0).uniform_(hue[0], hue[1]).item()\n                img = TF.adjust_hue(img, hue_factor)\n            if fn_id == 4 and self.gamma is not None:\n                gamma = self.gamma\n                gamma_factor = torch.tensor(1.0).uniform_(gamma[0], gamma[1]).item()\n                img = img.clamp(1e-8, 1.0)  # to fix Nan values in gradients\n                img = TF.adjust_gamma(img, gamma_factor)\n        return img\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + '('\n        format_string += 'brightness={0}'.format(self.brightness)\n        format_string += ', contrast={0}'.format(self.contrast)\n        format_string += ', saturation={0}'.format(self.saturation)\n        format_string += ', hue={0})'.format(self.hue)\n        format_string += ', gamma={0})'.format(self.gamma)\n        return format_string\n\n\n# From core/adapter/rotta.py\nclass RoTTA(BaseAdapter):\n    def __init__(self, cfg, model, optimizer):\n        super(RoTTA, self).__init__(cfg, model, optimizer)\n        self.mem = CSTU(capacity=self.cfg.ADAPTER.RoTTA.MEMORY_SIZE, num_class=cfg.CORRUPTION.NUM_CLASS, lambda_t=cfg.ADAPTER.RoTTA.LAMBDA_T, lambda_u=cfg.ADAPTER.RoTTA.LAMBDA_U)\n        self.model_ema = self.build_ema(self.model)\n        self.transform = get_tta_transforms(cfg)\n        self.nu = cfg.ADAPTER.RoTTA.NU\n        self.update_frequency = cfg.ADAPTER.RoTTA.UPDATE_FREQUENCY\n        self.current_instance = 0\n\n    @torch.enable_grad()\n    def forward_and_adapt(self, batch_data, model, optimizer):\n        with torch.no_grad():\n            model.eval()\n            self.model_ema.eval()\n            ema_out = self.model_ema(batch_data)\n            predict = torch.softmax(ema_out, dim=1)\n            pseudo_label = torch.argmax(predict, dim=1)\n            entropy = torch.sum(- predict * torch.log(predict + 1e-6), dim=1)\n\n        for i, data in enumerate(batch_data):\n            p_l = pseudo_label[i].item()\n            uncertainty = entropy[i].item()\n            current_instance = (data, p_l, uncertainty)\n            self.mem.add_instance(current_instance)\n            self.current_instance += 1\n\n            if self.current_instance % self.update_frequency == 0:\n                self.update_model(model, optimizer)\n\n        return ema_out\n\n    def update_model(self, model, optimizer):\n        model.train()\n        self.model_ema.train()\n        sup_data, ages = self.mem.get_memory()\n        l_sup = None\n        if len(sup_data) > 0:\n            sup_data = torch.stack(sup_data)\n            strong_sup_aug = self.transform(sup_data)\n            ema_sup_out = self.model_ema(sup_data)\n            stu_sup_out = model(strong_sup_aug)\n            instance_weight = timeliness_reweighting(ages)\n            l_sup = (softmax_entropy(stu_sup_out, ema_sup_out) * instance_weight).mean()\n\n        l = l_sup\n        if l is not None:\n            optimizer.zero_grad()\n            l.backward()\n            optimizer.step()\n\n        self.update_ema_variables(self.model_ema, self.model, self.nu)\n\n    @staticmethod\n    def update_ema_variables(ema_model, model, nu):\n        for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n            ema_param.data[:] = (1 - nu) * ema_param[:].data[:] + nu * param[:].data[:]\n        return ema_model\n\n    def configure_model(self, model: nn.Module):\n        model.requires_grad_(False)\n        normlayer_names = []\n\n        for name, sub_module in model.named_modules():\n            if isinstance(sub_module, nn.BatchNorm1d) or isinstance(sub_module, nn.BatchNorm2d):\n                normlayer_names.append(name)\n\n        for name in normlayer_names:\n            bn_layer = get_named_submodule(model, name)\n            if isinstance(bn_layer, nn.BatchNorm1d):\n                NewBN = RobustBN1d\n            elif isinstance(bn_layer, nn.BatchNorm2d):\n                NewBN = RobustBN2d\n            else:\n                raise RuntimeError()\n\n            momentum_bn = NewBN(bn_layer, self.cfg.ADAPTER.RoTTA.ALPHA)\n            momentum_bn.requires_grad_(True)\n            set_named_submodule(model, name, momentum_bn)\n        return model\n\n\ndef timeliness_reweighting(ages):\n    if isinstance(ages, list):\n        ages = torch.tensor(ages).float().cuda()\n    return torch.exp(-ages) / (1 + torch.exp(-ages))",
        "experimental_info": "RoTTA Specific Parameters:\n  MEMORY_SIZE: 64 (Capacity of the memory bank for CSTU)\n  UPDATE_FREQUENCY: 64 (Frequency of model updates, typically matching memory size)\n  NU: 0.001 (Exponential Moving Average update rate for the teacher model parameters)\n  ALPHA: 0.05 (Momentum for updating Robust Batch Normalization statistics)\n  LAMBDA_T: 1.0 (Weight for timeliness in the CSTU heuristic score)\n  LAMBDA_U: 1.0 (Weight for uncertainty in the CSTU heuristic score)\n\nOptimization Settings:\n  OPTIM.STEPS: 1 (Number of adaptation steps per batch)\n  OPTIM.LR: 1e-3 (Learning rate for the optimizer)\n  OPTIM.METHOD: 'Adam' (Optimizer method)\n  OPTIM.WD: 0.0 (Weight decay)\n\nModel and Data Settings:\n  MODEL.ARCH: 'Standard' (Architecture of the base model)\n  CORRUPTION.DATASET: 'cifar10' (Dataset used for corruption, e.g., cifar10, cifar100)\n  CORRUPTION.NUM_CLASS: -1 (Number of classes, inferred from dataset if -1)\n  TEST.BATCH_SIZE: 64 (Batch size for testing/adaptation)\n  INPUT.SIZE: (32, 32) (Input image size for transformations)\n  INPUT.PIXEL_MEAN: [0.485, 0.456, 0.406] (Mean for image normalization)\n  INPUT.PIXEL_STD: [0.229, 0.224, 0.225] (Standard deviation for image normalization)\n\nData Augmentation for Student (applied via get_tta_transforms, with default 'hard' settings):\n  Clip: (min_val=0.0, max_val=1.0)\n  ColorJitterPro: brightness=[0.6, 1.4], contrast=[0.7, 1.3], saturation=[0.5, 1.5], hue=[-0.06, 0.06], gamma=[0.7, 1.3]\n  Pad: padding=int(n_pixels / 2), padding_mode='edge'\n  RandomAffine: degrees=[-15, 15], translate=(1/16, 1/16), scale=(0.9, 1.1)\n  GaussianBlur: kernel_size=5, sigma=[0.001, 0.5]\n  CenterCrop: size=n_pixels\n  RandomHorizontalFlip: p=0.5\n  GaussianNoise: mean=0, std=0.005"
      }
    },
    {
      "title": "Improved Test-Time Adaptation for Domain Generalization",
      "abstract": "The main challenge in domain generalization (DG) is to handle the\ndistribution shift problem that lies between the training and test data. Recent\nstudies suggest that test-time training (TTT), which adapts the learned model\nwith test data, might be a promising solution to the problem. Generally, a TTT\nstrategy hinges its performance on two main factors: selecting an appropriate\nauxiliary TTT task for updating and identifying reliable parameters to update\nduring the test phase. Both previous arts and our experiments indicate that TTT\nmay not improve but be detrimental to the learned model if those two factors\nare not properly considered. This work addresses those two factors by proposing\nan Improved Test-Time Adaptation (ITTA) method. First, instead of heuristically\ndefining an auxiliary objective, we propose a learnable consistency loss for\nthe TTT task, which contains learnable parameters that can be adjusted toward\nbetter alignment between our TTT task and the main prediction task. Second, we\nintroduce additional adaptive parameters for the trained model, and we suggest\nonly updating the adaptive parameters during the test phase. Through extensive\nexperiments, we show that the proposed two strategies are beneficial for the\nlearned model (see Figure 1), and ITTA could achieve superior performance to\nthe current state-of-the-art methods on several DG benchmarks. Code is\navailable at https://github.com/liangchen527/ITTA.",
      "full_text": "Improved Test-Time Adaptation for Domain Generalization Liang Chen1 Yong Zhang2* Yibing Song3 Ying Shan2 Lingqiao Liu1∗ 1 The University of Adelaide 2 Tencent AI Lab 3 AI3 Institute, Fudan University {liangchen527, zhangyong201303, yibingsong.cv}@gmail.com yingsshan@tencent.com lingqiao.liu@adelaide.edu.au Abstract The main challenge in domain generalization (DG) is to handle the distribution shift problem that lies between the training and test data. Recent studies suggest that test-time training (TTT), which adapts the learned model with test data, might be a promising solution to the problem. Gen- erally, a TTT strategy hinges its performance on two main factors: selecting an appropriate auxiliary TTT task for up- dating and identifying reliable parameters to update during the test phase. Both previous arts and our experiments in- dicate that TTT may not improve but be detrimental to the learned model if those two factors are not properly consid- ered. This work addresses those two factors by proposing an Improved Test-Time Adaptation (ITTA) method. First, in- stead of heuristically defining an auxiliary objective, we pro- pose a learnable consistency loss for the TTT task, which con- tains learnable parameters that can be adjusted toward bet- ter alignment between our TTT task and the main prediction task. Second, we introduce additional adaptive parameters for the trained model, and we suggest only updating the adap- tive parameters during the test phase. Through extensive ex- periments, we show that the proposed two strategies are ben- eficial for the learned model (see Figure 1), and ITTA could achieve superior performance to the current state-of-the-art methods on several DG benchmarks. Code is available at https://github.com/liangchen527/ITTA. 1. Introduction Recent years have witnessed the rapid development of deep learning models, which often assume the training and test data are from the same domain and follow the same distribution. However, this assumption does not always hold in real-world scenarios. Distribution shift among the source and target domains is ubiquitous in related areas [35], such as autonomous driving or object recognition tasks, resulting *Corresponding authors. This work is done when L. Chen is an intern in Tencent AI Lab. 0.5 1.1 0.5 1.2 0.5 0.5 0.5 1.4 0.4 0.4 0.4 0.3 art cartoon photo sketch 79.9 75.4 94.4 75.8 83.3 76.0 94.4 76.7 84.7 78.0 94.5 78.2 Figure 1. Performance improvements from the proposed two strate- gies (i.e. introducing a learnable consistency loss and including additional adaptive parameters to improve TTT) for the baseline model (i.e. ResNet18 [30] with existing augmentation strategy [75]). Experiments are conducted on the PACS dataset [37] with the leave- one-out setting. Following [27], we use 60 sets of random seeds and hyper-parameters for each target domain. The reported average accuracy and error bars verify the effectiveness of our method. in poor performances for delicately designed models and hindering the further application of deep learning techniques. Domain generalization (DG) [2,8,16,23,24,31,38 –40,40, 44, 47, 51, 52, 69], designed to generalize a learned model to unseen target domains, has attracted a great deal of attention in the research community. The problem can be traced back to a decade ago [7], and various approaches have been pro- posed to push the DG boundary ever since. Those efforts in- clude invariant representation learning [28,47,49,58], adver- sarial learning [23,40,44,69], augmentation [9,41,42,66,75], or meta-learning [2, 16, 38, 39]. Despite successes on certain occasions, a recent study [27] shows that, under a rigorous evaluation protocol, most of these arts are inferior to the baseline empirical risk minimization (ERM) method [61]. This finding is not surprising, as most current arts strive to decrease the distribution shift only through the training data while overlooking the contributions from test samples. Recently, the test-time training (TTT) technique [60] has been gaining momentum for easing the distribution shift problem. TTT lies its success in enabling dynamic tuning of the pretrained model with the test samples via an auxil- iary TTT task, which seems to be a promising effort when arXiv:2304.04494v2  [cs.CV]  16 Apr 2023confronting data from different domains. However, TTT is not guaranteed to improve the performance. Previous arts [46, 63] indicate that selecting an appropriate auxiliary TTT task is crucial, and an inappropriate one that does not align with the main loss may deteriorate instead of improv- ing the performance. Meanwhile, it is pointed out in [63] that identifying reliable parameters to update is also essential for generalization, which is in line with our experimental findings in Sec. 5.3. Both of these two tasks are non-trivial, and there are limited efforts made to address them. This paper aims to improve the TTT strategy for better DG. First, different from previous works that empirically define auxiliary objectives and assume they are aligned with the main task, our work does not make such assumptions. Instead, we suggest learning an appropriate auxiliary loss for test-time updating. Specifically, encouraged by recent successes in multi-view consistency learning [13,26,29], we propose to augment the consistency loss by adding learn- able parameters based on the original implementation, where the parameters can be adjusted to assure our TTT task can be more aligned with the main task and are updated by en- forcing the two tasks share the same optimization direction. Second, considering that identifying reliable parameters to update is an everlasting job given the growing size of current deep models, we suggest introducing new adaptive param- eters after each block during the test phase, and we only tune the new parameters by the learned consistency loss while leaving the original parameters unchanged. Through extensive evaluations on the current benchmark [27], we illustrate that the learnable consistency loss performs more effectively than the self-supervised TTT tasks adopted in previous arts [60, 63], and by tuning only the new adaptive parameters, our method is superior to existing strategies that update all the parameters or part of them. This work aims to ease the distribution shift problem by improving TTT, and the main contributions are three-fold: • We introduce a learnable consistency loss for test-time adaptation, which can be enforced to be more aligned with the main loss by tuning its learnable parameters. • We introduce new adaptive parameters for the trained model and only update them during the test phase. • We conduct experiments on various DG benchmarks and illustrate that our ITTA performs competitively against current arts under the rigorous setting [27] for both the multi-source and single-source DG tasks. 2. Related Works 2.1. Domain Generalization. Being able to generalize to new environments while de- ploying is a challenging and practical requirement for cur- rent deep models. Existing DG approaches can be roughly categorized into three types. (1) Invariant representation learning: The pioneering work [5] theoretically proves that if the features remain invariant across different domains, then they are general and transferable to different domains. Guided by this finding, [47] uses maximum mean discrep- ancy (MMD) to align the learned features, and [25] proposes to use a multi-domain reconstruction auto-encoder to obtain invariant features. More recently, [58] suggests maximiz- ing the inner product of gradients from different domains to enforce invariance, and a similar idea is proposed in [52] where these gradients are expected to be similar to their mean values. (2) Optimization algorithms: Among the different optimization techniques adopted in DG, prevail- ing approaches resort to adversarial learning [23, 40, 44, 69] and meta-learning [2, 16, 38, 39]. Adversarial training is often used to enforce the learned features to be agnostic about the domain information. In [23], a domain-adversarial neural network (DANN) is implemented by asking the main- stream feature to maximize the domain classification loss. This idea is also adopted in [44], where adversarial training and an MMD constraint are employed to update an auto- encoder. Meanwhile, the meta-learning technique is used to simulate the distribution shifts between seen and unseen environments [2, 16, 38, 39], and most of these works are developed based on the MAML framework [20]. (3) Aug- mentation: Most augmentation skills applied in the general- ization tasks are operated in the feature level [34, 41, 48, 75] except for [11,66,68] which mix images [68] or its phase [66] to synthesize new data. To enable contrastive learning, we incorporate an existing augmentation strategy [75] in our framework. This method originated from AdaIN [32], which synthesizes new domain information by mixing the statistics of the features. Similar ideas can be found in [42, 48]. 2.2. Test-Time Training and Adaptation Test-Time Training (TTT) is first introduced in [60]. The basic paradigm is to employ a test-time task besides the main task during the training phase and update the pre- trained model using the test data with only the test-time objective before the final prediction step. The idea is empir- ically proved effective [60] and further developed in other related areas [3, 10, 12, 14, 21, 22, 43, 56, 63, 65, 73, 74]. Most current works focus on finding auxiliary tasks for updat- ing during the test phase, and the efforts derive from self- supervion [3, 10, 21, 22, 43, 60], meta-learning [65, 73, 74], information entropy [63], pseudo-labeling [12, 14], to name a few. However, not all empirically selected test-time tasks are effective. A recent study [46] indicates that only when the auxiliary loss aligns with the main loss can TTT improve the trained model. Inspired by that, we propose a learnable consistency loss and enforce alignment between the two ob- jectives. Results show that our strategy can be beneficial for the trained model (see Figure 1).subtract Figure 2. Training process of ITTA. We use x from the source domain as input for the feature extractor fθ(·) to obtain the repre- sentation z and its augmented version z′, where the augmentation skill from [75] is applied. The classifier fϕ(·) and weight subnet- work fw(·) are used to compute the main loss Lmain and learnable consistency loss Lwcont. Please refer to our text for details. Meanwhile, [63] suggests that auxiliary loss is not the only factor that affects the performance. Selecting reliable parameters to update is also crucial within the TTT frame- work. Given the large size of current models, correctly iden- tifying these parameters may require tremendous amounts of effort. To this end, instead of heuristically selecting candi- dates, we propose to include new adaptive parameters for up- dating during the test phase. Experimental results show that the proposed method can obtain comparable performances against existing skills. 3. Methodology In the task of DG, we are often given access to data from S (S ≥ 1) source domains Ds = {D1, D2, ..., DS} and expect a model to make good prediction on unseen target domains Dt = {D1, D2, ..., DT } (T ≥ 1). Our method aims to improve the test-time training (TTT) strategy for better DG. The improvements are two-fold. First, we pro- pose a learnable consistency loss for the TTT task, which could be enforced to align with the main objective by tuning its learnable weights. Second, we suggest including addi- tional adaptive parameters and only updating these adaptive parameters during the test phase. 3.1. A Learnable Consistency Loss for TTT The TTT strategies have shown promising performances when dealing with distribution shift problems [43, 63]. How- ever, their successes are depended on the empirically selected auxiliary TTT tasks, which may deteriorate the performances if chosen improperly. Motivated by the recent successes in multi-view consistency learning [13, 26, 29], we suggest adopting a consistency loss in our TTT task. Note that the naive consistency loss is still not guaranteed to be effective as prior art [46] indicates that only when the auxiliary loss aligns with the main loss, can TTT improves the perfor- mance. To this end, we propose to augment the auxiliary loss with learnable parameters that could be adjusted toward a better alignment between the TTT and main tasks. In our case, we make the adopted consistency loss learnable by introducing a weight subnetwork that allows flexible ways Algorithm 1 Pseudo code of the training phase of ITTA in a PyTorch-like style. # fθ, fϕ, fw: feature extractor, classifier, weight subnetwork # α, 0: weight paramter, all zero tensor # training process for x, yin training loader: # load a minibatch with N samples def forward process(x, y): z, z′ = fθ.forward(x) # computing losses Lmain = CrossEntropyLoss(fϕ.forward(z), y) Lmain+ =CrossEntropyLoss(fϕ.forward(z′), y) Lwcont = MSELoss(fw.forward(z − z′), 0) return Lmain, Lwcont # SGD update: feature extractor and classifier Lmain, Lwcont = forward process(x, y) ([fθ.params, fϕ.params]).zero grad() (Lmain + αLwcont).backward() update( \u0002 fθ.params, fϕ.params \u0003 ) # compute objectives for updating weight subnetwork Lmain, Lwcont = forward process(x, y) Lmain.backward() ˆgmain = fθ.params.grad.clone().normalize() fθ.params.zero grad() Lwcont.backward() ˆgwcont = fθ.params.grad.clone().normalize() # SGD update: weight subnetwork MSELoss(ˆgmain, ˆgwcont).backward() fw.params.zero grad() update(fw.params) to measure the consistency between two views of the same instance. We first introduce the pipeline of our training framework. Given the D dimensional representation z ∈ RD1 and its corresponding augmented version z′ that are obtained from a feature extractor (i.e. {z, z′} = fθ(x), where x is an input image from Ds, and fθ(·) is the feature extractor parame- terized by θ. In our implementation, we use the existing augmentation method [75] to obtain z′ by modifying the intermediate activation in fθ(x). We show in our supplemen- tary material that our framework can also thrive with other augmentation strategies), our learnable consistency loss is given by, Lwcont = ∥fw(z − z′)∥, (1) where ∥ · ∥denotes the L2 norm; fw(·) is the weight sub- network parameterized by w. To make the training process more stable and potentially achieve better performance, we apply a dimension-wise nonlinear function to map each di- mension of z − z′ before calculating the L2 norm. That is, ∀h ∈ RD, fw(h) is implemented by stacking layers of a nonlinear function: ReLU(a ∗ h + b), where a ∈ RD and b ∈ RD are the weight and bias from the nonlinear function, 1We omit the batch dimensions of the variables for simplicity.… … subtract Figure 3. Test adaptation process of ITTA. Different from that in the training stage, we include additional adaptive parameters fΘ after each block of the feature extractor fθ. For each test sample x, the intermediate representations zi and z′i obtained from fi θ are passed to fi Θ before going to the next block fi+1 θ . We use the learnable consistency loss Lwcont as the objective to update fΘ. Please refer to our text for details. and different layers of a, bform the parameter w in fw. In effect, this creates a piecewise-linear mapping function for h: depending on the value of h, the output could be 0, a constant, or a scaling-and-shifted version of h. More studies about the design of fw are provided in our supplementary material. Compared to the naive consistency learning with- out fw, our Lwcont can be more flexible with an adjustable fw, which we show in the following is the key for learning an appropriate loss in the improved TTT framework. Combining Lwcont with the main loss Lmain which applies the cross-entropy loss (CE) for both the origi- nal and augmented inputs ( i.e. Lmain = CE(fϕ(z), y) + CE(fϕ(z′), y), where fϕ is the classifier parameterized by ϕ, and y is the corresponding label), the objective for the feature extractor and classifier can be formulated into, min{θ,ϕ} Lmain + αLwcont, (2) where α is the weight parameter that balances the contri- butions from the two terms. A simple illustration of the workflow is shown in Figure 2. From Eq. (2), the expected gradients for the feature ex- tractor from Lmain and Lwcont can be represented as, \u001a gmain = ∇θ(CE(fϕ(z), y) + CE(fϕ(z′), y)), (3) gwcont = ∇θ∥fw(z − z′)∥. (4) We observe that the direction of gwcont is also determined by the weight subnetwork fw(·), which should be close with gmain to ensure alignment between Lmain and Lwcont [46, 60]. To this end, we propose a straightforward solution by enforcing equality between the normalized versions of gmain and gwcont, and we use this term as the objective for updating fw(·), which gives, min w Lalign, s.t. Lalign = ∥ˆgmain − ˆgwcont∥, (5) where ˆgmain = gmain−Egmain σgmain , and similar for ˆgwcont. In our implementation, we update {θ, ϕ} and w in an alternative manner. Pseudo code of the training process are shown in Algorithm 1. Algorithm 2 Pseudo code of the test phase of ITTA in a PyTorch-like style. # fθ, fϕ: feature extractor, classifier # fw, fΘ: weight subnetwork, additional adaptive blocks # m, 0: total number of blocks in fθ, all zero tensor # test process for x in test loader: # load a test batch def forward process(x): z1, z′1 = f1 Θ.forward((f1 θ .forward(x))) # first blocks for i in range(2, m + 1): # the following m − 1 blocks zi, z′i = fi θ.forward(zi−1), fi θ.forward(z′i−1) zi, z′i = fi Θ.forward(zi), fi Θ.forward(z′i) return zi, z′i # test adaptation phase: SGD update additional adaptive parameters z, z′ = forward process(x) Lwcont = MSELoss(fw.forward(z − z′), 0) fΘ.params.zero grad() Lwcont.backward() update(fΘ.params) # final prediction z, = forward process(x) result = fϕ.forward(z) 3.2. Including Additional Adaptive Parameters Selecting expressive and reliable parameters to update during the test phase is also essential in the TTT frame- work [63]. Some strategies decide to update all the parame- ters from the feature extractor [3, 43], while others use only the parameters from the specific layers for updating [63, 71]. Given the fact that the sizes of current deep models are often very large and still growing, exhaustively trying different combinations among the millions of candidates seems to be an everlasting job. As there are no consensuses on which parameter should be updated, we suggest another easy alter- native in this work. Specifically, assuming there are a total of m blocks in the pretrained feature extractor fθ(·), and the i-th block can be denoted as fi θ(·). Then the intermediate representation zi from fi θ(·) can be formulated as, zi = fi θ(zi−1), s.t. z1 = f1 θ (x). (6) We propose to include additional adaptive blockfΘ that is parameterized by Θ after each block of fθ during the test- time adaptation phase, which reformulates Eq. (6) into, zi = fi Θ(fi θ(zi−1)), s.t. z1 = f1 Θ(f1 θ (x)), (7) where fΘ(·) does not change the dimension and sizes of the intermediate representations. In our work, we use a structure similar to fw to implement fΘ. Note zm is simplified as z in this phase, and the same process is applied for obtaining z′. Then, in the test-time adaptation phase, we suggest only updating the new adaptive parameters via the learned con- sistency loss. The optimization process can be written as,Table 1. Multi sources domain generalization. Experiments are conducted on the DomainBed benchmark [27]. All methods are examined for 60 trials in each unseen domain. Top5 accumulates the number of datasets where a method achieves the top 5 performances. The score here accumulates the numbers of the dataset where a specific art obtains larger accuracy than ERM on account of the variance. Best results are colored as red. Among the 22 methods compared, less than a quarter outperforms ERM in most datasets (Score ≥ 3). PACS VLCS OfficeHome TerraInc DomainNet Avg. Top5↑ Score↑ MMD [40] 81.3 ± 0.8 74.9 ± 0.5 59.9 ± 0.4 42.0 ± 1.0 7.9 ± 6.2 53.2 1 2 RSC [33] 80.5 ± 0.2 75.4 ± 0.3 58.4 ± 0.6 39.4 ± 1.3 27.9 ± 2.0 56.3 0 1 IRM [1] 80.9 ± 0.5 75.1 ± 0.1 58.0 ± 0.1 38.4 ± 0.9 30.4 ± 1.0 56.6 0 1 ARM [72] 80.6 ± 0.5 75.9 ± 0.3 59.6 ± 0.3 37.4 ± 1.9 29.9 ± 0.1 56.7 0 0 DANN [23] 79.2 ± 0.3 76.3 ± 0.2 59.5 ± 0.5 37.9 ± 0.9 31.5 ± 0.1 56.9 1 1 GroupGRO [55] 80.7 ± 0.4 75.4 ± 1.0 60.6 ± 0.3 41.5 ± 2.0 27.5 ± 0.1 57.1 0 1 CDANN [44] 80.3 ± 0.5 76.0 ± 0.5 59.3 ± 0.4 38.6 ± 2.3 31.8 ± 0.2 57.2 0 0 VREx [36] 80.2 ± 0.5 75.3 ± 0.6 59.5 ± 0.1 43.2 ± 0.3 28.1 ± 1.0 57.3 1 1 CAD [53] 81.9 ± 0.3 75.2 ± 0.6 60.5 ± 0.3 40.5 ± 0.4 31.0 ± 0.8 57.8 1 2 CondCAD [53] 80.8 ± 0.5 76.1 ± 0.3 61.0 ± 0.4 39.7 ± 0.4 31.9 ± 0.7 57.9 0 1 MTL [6] 80.1 ± 0.8 75.2 ± 0.3 59.9 ± 0.5 40.4 ± 1.0 35.0 ± 0.0 58.1 0 0 ERM [61] 79.8 ± 0.4 75.8 ± 0.2 60.6 ± 0.2 38.8 ± 1.0 35.3 ± 0.1 58.1 1 - MixStyle [75] 82.6 ± 0.4 75.2 ± 0.7 59.6 ± 0.8 40.9 ± 1.1 33.9 ± 0.1 58.4 1 1 MLDG [38] 81.3 ± 0.2 75.2 ± 0.3 60.9 ± 0.2 40.1 ± 0.9 35.4 ± 0.0 58.6 1 1 Mixup [68] 79.2 ± 0.9 76.2 ± 0.3 61.7 ± 0.5 42.1 ± 0.7 34.0 ± 0.0 58.6 2 2 Fishr [52] 81.3 ± 0.3 76.2 ± 0.3 60.9 ± 0.3 42.6 ± 1.0 34.2 ± 0.3 59.0 2 2 SagNet [48] 81.7 ± 0.6 75.4 ± 0.8 62.5 ± 0.3 40.6 ± 1.5 35.3 ± 0.1 59.1 1 2 SelfReg [34] 81.8 ± 0.3 76.4 ± 0.7 62.4 ± 0.1 41.3 ± 0.3 34.7 ± 0.2 59.3 2 3 Fish [58] 82.0 ± 0.3 76.9 ± 0.2 62.0 ± 0.6 40.2 ± 0.6 35.5 ± 0.0 59.3 3 4 CORAL [59] 81.7 ± 0.0 75.5 ± 0.4 62.4 ± 0.4 41.4 ± 1.8 36.1 ± 0.2 59.4 2 3 SD [51] 81.9 ± 0.3 75.5 ± 0.4 62.9 ± 0.2 42.0 ± 1.0 36.3 ± 0.2 59.7 4 4 Ours 83.8 ± 0.3 76.9 ± 0.6 62.0 ± 0.2 43.2 ± 0.5 34.9 ± 0.1 60.2 4 4 min Θ ∥fw(z − z′)∥, s.t. {z, z′} = fΘ(fθ(x)). (8) Note that different from the training phase, x in this stage is from the target domain Dt, and we use the online setting in [60] for updating. A simple illustration of the test adaptation pipeline is shown in Figure 3. For the final step, we use the original representation ob- tained from the pretrained feature extractor and the adapted adaptive parameters for prediction. Pseudo code of the test stage are shown in Algorithm 2. 4. Experiments 4.1. Settings Datasets. We evalute ITTA on five benchmark datasets: PACS [37] which consists of 9,991 images from 7 cate- gories. This dataset is probably the most widely-used DG benchmark owing to its large distributional shift across 4 do- mains including art painting, cartoon, photo, and sketch; VLCS [18] contains 10,729 images of 5 classes from 4 different datasets (i.e. domains) including PASCAL VOC 2007 [17], LabelMe [54], Caltech [19], and Sun [64] where each dataset is considered a domain in DG;OfficeHome [62] is composed of 15,588 images from 65 classes in office and home environments, and those images can be categorized into 4 domains (i.e. artistic, clipart, product, and real world); TerraInc [4] has 24,788 images from 10 classes. Those images are wild animals taken from 4 different locations (i.e. domains) including L100, L38, L43, and L46; Domain- Net [50] which contains 586,575 images from 345 classes, and the images in it can be depicted in 6 styles (i.e. clipart, infograph, painting, quickdraw, real, and sketch). Implementation details. For all the experiments, we use the ImageNet [15] pretrained ResNet18 [30] backbone that with 4 blocks as the feature extractor fθ, which could en- large the gaps in DG compared to larger models [70]. Corre- spondingly, we also include 4 blocks of additional adaptive parameters (i.e. fΘ), and each block is implemented with 5 layers of learnable parameters with weight initialized as all ones and bias initialized as all zeros. For the weight subnet- work fw, we use 10 layers of learnable parameters with the initialization skill similar to that of fΘ. The classifier fϕ is an MLP layer provided by the Domainbed benchmark [27]. For the weight parameter α in Eq. (2), we set it to be 1 for all experiments (please refer to our supplementary material for analysis). The random seeds, learning rates, batch size, and augmentation skills are all dynamically set for all the compared arts according to [27].Table 2. Single source domain generalization. Experiments are conducted on the PACS dataset [37]. Here A, C, P, and S are the art, cartoon, photo, and sketch domains in PACS. A→C represents models trained on the art domain and tested on the cartoon domain, and similar for others. All methods are examined for 60 trials in each unseen domain. Best results are colored as red. A→C A →P A →S C →A C →P C →S P →A P →C P →S S →A S →C S →P Avg. RSC 66.3 ±1.3 88.2±0.6 57.2±3.1 65.8±1.5 82.4±0.6 68.7±2.5 60.5±2.0 41.3±6.0 53.1±2.8 53.8±1.6 65.9±0.7 48.4±1.9 62.6 Fish 67.1 ±0.5 89.2±1.8 57.0±0.2 66.7±1.0 85.6±0.4 64.5±3.6 55.1±2.1 33.9±2.3 51.2±4.2 59.1±3.2 67.1±0.9 58.4±1.2 62.9 CDANN 66.5±1.7 92.2±0.6 65.0±0.9 70.6±0.1 82.9±1.4 67.7±3.0 60.6±0.3 42.2±6.4 46.9±9.9 51.4±2.3 60.7±1.2 51.9±0.4 63.2 SelfReg 63.9±1.9 90.1±1.0 56.8±2.2 70.2±2.3 85.4±0.3 70.2±2.2 60.9±2.6 38.8±4.0 50.5±3.2 54.5±4.7 66.2±1.2 51.7±4.1 63.3 DANN 67.5 ±1.6 91.2±1.3 67.5±1.3 70.6±1.0 81.4±0.4 66.6±1.1 54.1±2.3 33.5±2.7 52.8±2.3 53.8±1.7 64.4±0.7 58.9±0.8 63.5 CAD 67.1 ±1.5 89.6±0.4 60.2±0.2 67.7±3.1 83.7±1.4 70.2±2.6 60.6±2.6 38.3±3.7 53.8±3.2 50.7±1.6 65.8±1.3 54.4±1.7 63.5 GroupGRO66.5±1.2 90.5±1.5 58.9±2.5 70.8±0.9 85.7±1.2 69.7±1.8 62.3±2.1 41.1±2.7 48.2±4.1 54.8±0.5 65.2±1.6 53.9±1.4 64.0 MTL 67.3 ±1.0 90.1±1.0 58.9±0.7 70.2±1.8 84.2±2.2 71.9±0.7 58.3±2.7 38.5±2.7 52.8±1.5 55.4±3.1 66.1±1.3 55.2±2.6 64.1 IRM 67.5 ±1.8 93.0±0.5 62.9±4.7 67.6±1.3 83.8±0.4 68.9±0.8 63.7±1.8 39.9±3.7 49.0±5.4 54.9±1.4 63.1±2.1 54.9±1.4 64.1 ARM 66.0 ±2.4 91.2±0.7 58.7±6.9 70.6±0.8 84.2±1.0 69.1±0.9 59.2±1.8 42.1±5.6 52.1±3.0 60.0±0.6 62.9±3.3 53.8±2.0 64.2 Mixup 65.5 ±0.8 87.8±0.3 57.2±1.0 71.4±1.1 83.1±1.8 68.0±3.0 59.6±1.7 37.2±2.7 56.5±3.8 55.0±2.2 66.2±1.5 62.7±4.2 64.2 CORAL 66.8±0.5 90.3±0.7 61.5±1.9 67.9±2.1 85.4±0.3 70.4±1.3 55.9±2.9 40.4±4.9 49.8±8.5 55.8±2.1 67.6±0.9 58.9±3.8 64.2 SD 67.1 ±1.3 91.7±1.2 63.7±4.1 70.3±0.9 84.4±0.7 69.4±2.3 57.5±2.5 42.6±0.8 47.7±1.7 55.9±2.4 65.7±0.8 55.8±2.1 64.3 MMD 67.1 ±1.4 88.0±0.8 63.6±1.6 70.0±1.1 83.6±0.2 70.2±1.0 58.8±2.6 40.3±1.0 52.3±2.4 57.4±1.9 68.7±0.9 52.7±3.7 64.4 MLDG 67.3±2.0 90.8±0.5 64.4±0.9 70.8±1.0 84.2±0.3 69.7±1.8 61.6±1.0 41.3±5.1 50.4±0.2 49.9±2.5 66.8±0.4 58.7±3.4 64.7 CondCAD66.9±1.4 92.3±0.7 60.8±4.5 71.0±0.6 84.7±1.1 72.6±0.5 61.2±1.5 40.7±3.6 55.7±1.6 52.3±1.7 64.2±0.4 55.3±1.2 64.8 ERM 67.3 ±0.7 91.7±0.9 60.1±4.7 70.4±0.6 82.3±2.7 68.1±0.9 59.6±1.8 44.7±2.8 56.5±2.7 52.8±2.3 68.1±0.7 58.4±0.9 65.0 VREx 67.1 ±1.5 91.0±1.0 62.6±3.5 71.1±2.4 84.1±0.9 71.7±1.3 62.4±3.1 37.7±3.3 53.6±2.3 60.6±1.6 66.7±0.8 57.5±1.4 65.5 Fishr 67.9 ±1.9 92.7±0.3 62.4±4.7 71.2±0.5 83.4±0.6 70.2±1.1 60.0±2.3 42.7±3.2 57.1±3.9 55.7±3.7 68.4±1.0 62.0±3.1 66.1 SagNet 67.6±1.4 92.3±0.5 59.5±1.7 71.8±0.3 82.8±0.6 69.9±1.8 62.5±2.5 45.2±2.5 64.1±2.0 55.8±1.1 65.7±1.4 55.9±3.5 66.1 MixStyle 68.5±2.0 91.2±1.6 65.1±0.7 73.2±1.3 85.0±0.8 71.7±1.5 63.6±1.7 46.3±1.1 51.6±3.7 54.2±1.5 67.0±3.4 58.3±1.4 66.3 Ours 68.9 ±0.6 92.4±0.1 62.5±0.6 75.3±0.4 85.9±0.3 70.2±1.4 66.5±1.1 52.2±2.7 63.8±1.1 57.6±3.7 68.0±1.3 57.9±2.0 68.4 Training and evaluation details. For all the compared methods, we conduct 60 trials on each source domain, and each with 5,000 iteration steps. During the training stage, we split the examples from training domains to 8:2 (train:val) where the training and validation samples are dynamically selected among different training trials. During test, we select the model that performs the best in the validation samples and test it on the target domains. The strategy is referred to as the “training-domain validate set” model selec- tion method in [27]. For each domain in different datasets, the final performance is the average accuracy from the 60 trials. 4.2. Multi-Source Generalization In these experiments, all five benchmark datasets afore- mentioned are used for evaluation, and the leave-one-out strategy is adopted for training (i.e. with S = |Ds ∪Dt|2 −1, and T = 1). Results are shown in Table 1. We note that ERM method obtains favorable performance against existing arts. In fact, as a strong baseline, ERM is superior to half of the methods in the term of average accuracy, and only 5 arts (i.e. SelfReg [34], Fish [58], CORAL [59], SD [51], and ours) among the compared 22 methods outperforms ERM in most datasets (i.e. with Score ≥ 3). In comparison, the proposed ITTA is more effective than all other models on average. In particular, ITTA achieves the best performances in 3 out of the 5 benchmarks (i.e. PACS, VLCS, and TerraInc datasets) and 4 in the top 5. Note that although our method does not obtain the best performances in the OfficeHome and DomainNet benchmarks, it still outperforms more than half 2We use | · |to denote the number of domains in the environment. of the existing models. The results validate the effectiveness of our method when tested in the multi-source setting. We present results of average accuracy in each domain from different datasets in the supplementary material. Please refer to it for details. 4.3. Single-Source Generalization In these experiments, we adopt the widely-used PACS [37] benchmark for evaluation, and the models are trained on one domain while tested on the remaining three (i.e. with S = 1, and T = 3). Although some approaches, such as MLDG [38] and Fishr [52], may require more than one domain information for their trainings, we can simu- late multi-domain information using only the source domain, and thus the experimental settings are still feasible for them. Compared to the multi-source generalization task, the single- source generalization is considered more difficult due to the limited domain information during the training phase. Evalu- ation results are presented in Table 2. We note that the ERM method outperforms most state-of-the-art models, and only 5 models, including VREx [36], Fishr [52], SagNet [48], MixStyle [75], and the proposed ITTA, can obtain better re- sults than ERM in the term of average accuracy. Meanwhile, our method achieves the best performances when trained in 5 out of the 12 source domain, and it obtains the best perfor- mance on average, leading more than 2% than the second best (i.e. MixStyle [75]) and 3% the ERM method. In line with the findings in [27], we notice that the naive ERM method [61] can indeed perform favorably against most existing models under rigorous evaluation protocol. As a matter of fact, the proposed method is the only one that consistently outperforms ERM in both the multi-sourceTable 3. Evaluations of different TTT-based models in the unseen domain from PACS [37]. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Model Target domain Avg.Art Cartoon Photo Sketch Baseline 79.9 ±0.5 75.4±1.1 94.4±0.5 75.8±1.2 81.4±0.5 TTT [60] 81.5±0.8 77.6±0.6 94.3±0.2 78.4±0.7 83.0±0.2 MT3 [3] 82.0 ±1.0 76.5±1.0 94.1±0.2 77.7±1.3 82.6±0.6 TENT [63] 80.2±0.9 77.2±0.8 94.4±0.2 77.4±0.1 82.3±0.5 Ours 84.7 ±0.4 78.0±0.4 94.5±0.4 78.2±0.3 83.8±0.3 and single-source settings. These results indicate that DG remains challenging for current efforts that aim to ease the distribution shift only through training data, and using the proposed improved TTT strategy may be a promising direc- tion for solving DG. 5. Analysis All experiments in this section are conducted on the widely-used PACS benchmark [37] with the leave-one-out strategy. The experimental settings are the same as that illus- trated in Sec. 4.1. Please refer to our supplementary material for more analysis. 5.1. Compared with Other TTT-Based Models Using test-time adaptation to ease the distribution shift problem has been explored in previous works, such as the original TTT method [60] and MT3 [3]. Their differences lie in that TTT uses a rotation estimation task for the test-time objective, and MT3 adopts a contrastive loss for the task and implements the overall framework using MAML [20]. There is also a recently proposed TENT [63] that aims to minimize the entropy of the final results by tuning the parameters from the batch normalization (BN) layers. To analyze the overall effectiveness of our method, we compare ITTA with these arts using the same baseline (i.e. ResNet18 [30] backbone with the existing augmentation skill [75]). Results are shown in Table 3. We observe that all the com- pared TTT-based methods can improve the baseline model in almost all target domains except for the “Photo” domain, which might be due to the ImageNet pretraining [67]. This phenomenon demonstrates that the TTT strategy may be a promising effort for easing the distribution shift problem. Meanwhile, we observe that the proposed ITTA is superior to all other approaches in most target domains and leads in the term of average accuracy. The main reason is that compared to the empirically designed TTT tasks adopted in previous works, the proposed learnable consistency loss is enforced to be more aligned with the main loss, thus more suitable for the test-time adaptation task [46]. Meanwhile, compared to the strategies that update the original param- eters from the trained model, the adaptation of the newly included parameters is also more effective for the overall (a) Input (b) Ours w/o fw (c) Ours (d) Main Figure 4. Grad-CAM [57] visualizations from different loss terms. We use images with varying class labels from the four target do- mains of PACS [37] as inputs (i.e. art, cartoon, photo, and sketch domains from top to bottom). Ours w/o fw is the naive consis- tency loss with fw disabled in Eq. (1). The proposed learnable consistency loss can align well with the main classification task. TTT framework. In the following, we provide more analysis to support these claims. 5.2. Effectiveness of the Learnable Consistency Loss To examine the effectiveness of our learnable consistency loss, we conduct ablation studies by comparing our method with the following variants. (1) Ours w/o fw: we disable fw when computing the learnable consistency loss in Eq. (1), which uses the naive consistency loss for the auxiliary TTT task. (2) Ours w/ Ent.: after training the model using the baseline settings (i.e. ResNet18 with the augmentation strat- egy [75]), we use the entropy minimization task in [63] for the TTT task. (3) Ours w/ Rot.: we use the rotation estimation task in [60] for the TTT task. To ensure fair com- parisons, we use the same baseline settings and include the same additional adaptive parameters for all the variants. Results are shown in the 4th to 6th rows Table 4. We find that the results from the naive consistency loss ( i.e. Ours w/o fw) are slightly better than that from the other two specially-designed objectives (i.e. Ours w/ Ent. and Ours w/ Rot.) on average. Besides the possibility of deteriorating the performance [46], our results indicate that empirically select- ing a TTT task may also be far from optimal. Meanwhile, we observe that when enabling fw, the proposed learnable consistency loss is superior to that withoutfw in all target do-Table 4. Comparison between different TTT tasks and parameter selecting strategies in the unseen domain from the PACS benchmark [37]. Here the “Ent.”, “Rot.”, and “Lwcont” denotes the entropy minimization task in [63], the rotation estimation task in [60], and the proposed learnable consistency objective, the “All”, “BN”, and “Ada.” are the strategies that update all the parameters, parameters from the batch normalization layer, and the proposed strategy that updates only the new additional adaptive parameters. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Model TTT tasks Param selectings Target domain Avg.Ent. Rot. Lwcont All BN Ada. Art Cartoon Photo Sketch Ours − − ✓ − − ✓ 84.7±0.4 78.0 ±0.4 94.5 ±0.4 78.2 ±0.3 83.8 ±0.3 Ours w/ofw − − − − − ✓ 83.1±0.4 74.6 ±0.6 94.0 ±0.5 78.0 ±0.8 82.5 ±0.1 Ours w/ Ent. ✓ − − − − ✓ 79.9±2.4 77.3 ±0.3 94.8 ±0.8 77.6 ±0.4 82.4 ±0.8 Ours w/ Rot. − ✓ − − − ✓ 81.1±1.0 75.2 ±0.5 94.9 ±0.3 77.3 ±0.6 82.1 ±0.3 Ours w/o TTT − − ✓ − − − 83.3±0.5 76.0 ±0.5 94.4 ±0.5 76.7 ±1.4 82.8 ±0.3 Ours w/ All − − ✓ ✓ − − 83.0±0.7 77.0 ±1.4 94.5 ±0.7 77.4 ±0.9 83.0 ±0.2 Ours w/ BN − − ✓ − ✓ − 81.8±0.5 75.6 ±0.3 94.4 ±0.3 77.9 ±1.1 82.4 ±0.5 mains, and it leads in the term of average accuracy among the variants compared, illustrating its advantage against other adopted TTT tasks. These results are not surprising. By comparing the Grad-CAM [57] visualizations from the main classification task with the learnable and naive consistency losses in Figure 4, we find that the proposed learnable objec- tive can well align with the main loss when fw is enabled as the hot zones activated by these two tasks are similar, which guarantees the improvement for the test-time adapta- tion [46, 60]. Please refer to our supplementary material for more visualizations. 5.3. Effectiveness of the Adaptive Parameters We compare ITTA with three variants to demonstrate the effectiveness of the proposed additional adaptive parameters. (1) Ours w/o TTT: we do not update any parameters during the test phase. This variant is used to verify whether TTT can improve the pretrained model. (2) Ours w/ ALL: similar to the updating strategy in the original TTT method [60], we update all the parameters from the feature extractor during the test phase. (3) Ours w/ BN: following the suggestion from TENT [63], only parameters from the BN layers of the feature extractor are updated. Note the same pretrained model is shared for all variants in these experiments, and the objectives during the test adaptation phase are to minimize the same learned consistency loss. We list the results in the last three rows in Table 4. We observe that when only updating parameters from the BN layers, the performance is inferior to the strategy without test-time adaptation, and updating all the parameters does not ensure improvements in all target domains. The observations are in line with the findings in [63] that selecting reliable parameters to update is essential in the TTT system and may also interact with the choice of the TTT task. In comparison, when including additional adaptive parameters for updating, the pretrained model can be boosted in all environments. The results validate that our adaptive parameters are more effective than that selected with existing strategies [60, 63] when applied with the proposed learnable test-time objective. 5.4. Limitation Although the proposed learned loss can bring satisfaction improvements, we are aware that the lunch is not free. When the weight subnetwork fw is disabled, updating the joint loss in Eq. (2) only costs 1 forward and 1 backward. However, in order to update fw, we have to compute the second-order derivative in Eq. (5), which will require 1 more forward and 3 more backward processes, bringing extra burden to the system. Our future efforts aim to simplify the overall optimization process and reduce the cost for ITTA. 6. Conclusion In this paper, we aim to improve the current TTT strategy for alleviating the distribution shift problem in DG. First, given that the auxiliary TTT task plays a vital role in the over- all framework, and an empirically selecting one that does not align with the main task may potentially deteriorate instead of improving the performance, we propose a learnable con- sistency loss that can be enforced to be more aligned with the main loss by adjusting its learnable parameters. This strategy is ensured to improve the model and shows favorable perfor- mance against some specially-designed objectives. Second, considering that selecting reliable and effective parameters to update during the test phase is also essential while exhaus- tively trying different combinations may require tremendous effort, we propose a new alternative by including new ad- ditional adaptive parameters for adaptation during the test phase. This alternative is shown to outperform some pre- vious parameter selecting strategies via our experimental findings. By conducting extensive experiments under a rig- orous evaluation protocol, we show that our method can achieve superior performance against existing arts in both the multi-source and single-source DG tasks. Acknowledgements. Liang Chen is supported by the ChinaScholarship Council (CSC Student ID 202008440331). References [1] Martin Arjovsky, L´eon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019. 5, 15, 16, 17 [2] Yogesh Balaji, Swami Sankaranarayanan, and Rama Chel- lappa. Metareg: Towards domain generalization using meta- regularization. In NeurIPS, 2018. 1, 2, 14, 15 [3] Alexander Bartler, Andre B¨uhler, Felix Wiewel, Mario D¨obler, and Bin Yang. Mt3: Meta test-time training for self- supervised test-time adaption. In AISTATS, 2022. 2, 4, 7 [4] Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In ECCV, 2018. 5, 17 [5] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for domain adaptation. In NeurIPS, 2006. 2 [6] Gilles Blanchard, Aniket Anand Deshmukh, Urun Dogan, Gyemin Lee, and Clayton Scott. Domain generalization by marginal transfer learning. arXiv preprint arXiv:1711.07910, 2017. 5, 15, 16, 17 [7] Gilles Blanchard, Gyemin Lee, and Clayton Scott. Generaliz- ing from several related classification tasks to a new unlabeled sample. In NeurIPS, 2011. 1 [8] Chaoqi Chen, Jiongcheng Li, Xiaoguang Han, Xiaoqing Liu, and Yizhou Yu. Compound domain generalization via meta- knowledge encoding. In CVPR, 2022. 1 [9] Chaoqi Chen, Luyao Tang, Feng Liu, Gangming Zhao, Yue Huang, and Yizhou Yu. Mix and reason: Reasoning over se- mantic topology with data mixing for domain generalization. In NeurIPS, 2022. 1 [10] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In CVPR, 2022. 2 [11] Liang Chen, Yong Zhang, Yibing Song, Lingqiao Liu, and Jue Wang. Self-supervised learning of adversarial example: Towards good generalizations for deepfake detection. In CVPR, 2022. 2 [12] Liang Chen, Yong Zhang, Yibing Song, Jue Wang, and Lingqiao Liu. Ost: Improving generalization of deepfake detection via one-shot test-time training. In NeurIPS, 2022. 2, 12 [13] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geof- frey Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020. 2, 3 [14] Sungha Choi, Seunghan Yang, Seokeon Choi, and Sungrack Yun. Improving test-time adaptation via shift-agnostic weight regularization and nearest source prototypes. In ECCV, 2022. 2 [15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009. 5 [16] Qi Dou, Daniel Coelho de Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain generalization via model-agnostic learning of semantic features. In NeurIPS, 2019. 1, 2 [17] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. IJCV, 88(2):303–338, 2010. 5 [18] Chen Fang, Ye Xu, and Daniel N Rockmore. Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. In ICCV, 2013. 5, 16 [19] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning gener- ative visual models from few training examples: An incre- mental bayesian approach tested on 101 object categories. In CVPR worksho, 2004. 5 [20] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model- agnostic meta-learning for fast adaptation of deep networks. In ICML, 2017. 2, 7 [21] Francois Fleuret et al. Uncertainty reduction for model adap- tation in semantic segmentation. In CVPR, 2021. 2 [22] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei A Efros. Test-time training with masked autoencoders. In NeurIPS, 2022. 2 [23] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc ¸ois Laviolette, Mario Marc- hand, and Victor Lempitsky. Domain-adversarial training of neural networks. JMLR, 17(1):2096–2030, 2016. 1, 2, 5, 15, 16, 17 [24] Muhammad Ghifary, David Balduzzi, W Bastiaan Kleijn, and Mengjie Zhang. Scatter component analysis: A unified framework for domain adaptation and domain generalization. IEEE TPAMI, 39(7):1414–1430, 2016. 1 [25] Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, and David Balduzzi. Domain generalization for object recognition with multi-task autoencoders. In ICCV, 2015. 2 [26] Jean-Bastien Grill, Florian Strub, Florent Altch ´e, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doer- sch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh- laghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. In NeurIPS, 2020. 2, 3 [27] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In ICLR, 2021. 1, 2, 5, 6, 14, 15, 16, 17 [28] Sivan Harary, Eli Schwartz, Assaf Arbelle, Peter Staar, Shady Abu-Hussein, Elad Amrani, Roei Herzig, Amit Alfassy, Raja Giryes, Hilde Kuehne, et al. Unsupervised domain general- ization by learning a bridge across domains. In CVPR, 2022. 1 [29] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual repre- sentation learning. In CVPR, 2020. 2, 3 [30] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 1, 5, 7, 14 [31] Shoubo Hu, Kun Zhang, Zhitang Chen, and Laiwan Chan. Domain generalization via multidomain discriminant analysis. In UAI, 2020. 1 [32] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In ICCV, 2017. 2 [33] Zeyi Huang, Haohan Wang, Eric P Xing, and Dong Huang. Self-challenging improves cross-domain generalization. In ECCV, 2020. 5, 15, 16, 17[34] Daehee Kim, Youngjun Yoo, Seunghyun Park, Jinkyu Kim, and Jaekoo Lee. Selfreg: Self-supervised contrastive regular- ization for domain generalization. In ICCV, 2021. 2, 5, 6, 15, 16, 17 [35] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribu- tion shifts. In ICML, 2021. 1 [36] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). In ICML, 2021. 5, 6, 15, 16, 17 [37] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain generalization. In ICCV, 2017. 1, 5, 6, 7, 8, 12, 13, 14, 15 [38] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Learning to generalize: Meta-learning for do- main generalization. In AAAI, 2018. 1, 2, 5, 6, 15, 16, 17 [39] Da Li, Jianshu Zhang, Yongxin Yang, Cong Liu, Yi-Zhe Song, and Timothy M Hospedales. Episodic training for domain generalization. In ICCV, 2019. 1, 2 [40] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot. Domain generalization with adversarial feature learning. In CVPR, 2018. 1, 2, 5, 15, 16, 17 [41] Pan Li, Da Li, Wei Li, Shaogang Gong, Yanwei Fu, and Timothy M Hospedales. A simple feature augmentation for domain generalization. In ICCV, 2021. 1, 2, 12, 14 [42] Xiaotong Li, Yongxing Dai, Yixiao Ge, Jun Liu, Ying Shan, and Ling-Yu Duan. Uncertainty modeling for out- of-distribution generalization. In ICLR, 2022. 1, 2 [43] Yizhuo Li, Miao Hao, Zonglin Di, Nitesh Bharadwaj Gun- davarapu, and Xiaolong Wang. Test-time personalization with a transformer for human pose estimation. In NeurIPS, 2021. 2, 3, 4 [44] Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao. Deep domain generaliza- tion via conditional invariant adversarial networks. In ECCV, 2018. 1, 2, 5, 15, 16, 17 [45] Yiying Li, Yongxin Yang, Wei Zhou, and Timothy Hospedales. Feature-critic networks for heterogeneous do- main generalization. In ICML, 2019. 14, 15 [46] Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? In NeurIPS, 2021. 2, 3, 4, 7, 8, 12, 14, 15 [47] Krikamol Muandet, David Balduzzi, and Bernhard Sch¨olkopf. Domain generalization via invariant feature representation. In ICML, 2013. 1, 2 [48] Hyeonseob Nam, HyunJae Lee, Jongchan Park, Wonjun Yoon, and Donggeun Yoo. Reducing domain gap by reducing style bias. In CVPR, 2021. 2, 5, 6, 15, 16, 17 [49] Prashant Pandey, Mrigank Raman, Sumanth Varambally, and Prathosh Ap. Generalization on unseen domains via inference- time label-preserving target projections. In CVPR, 2021. 1 [50] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In ICCV, 2019. 5, 17 [51] Mohammad Pezeshki, Oumar Kaba, Yoshua Bengio, Aaron C Courville, Doina Precup, and Guillaume Lajoie. Gradient star- vation: A learning proclivity in neural networks. In NeurIPS, 2021. 1, 5, 6, 15, 16, 17 [52] Alexandre Rame, Corentin Dancette, and Matthieu Cord. Fishr: Invariant gradient variances for out-of-distribution gen- eralization. In ICML, 2022. 1, 2, 5, 6, 15, 16, 17 [53] Yangjun Ruan, Yann Dubois, and Chris J Maddison. Optimal representations for covariate shift. In ICLR, 2022. 5, 15, 16, 17 [54] Bryan C Russell, Antonio Torralba, Kevin P Murphy, and William T Freeman. Labelme: a database and web-based tool for image annotation. IJCV, 77(1):157–173, 2008. 5 [55] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst- case generalization. In ICLR, 2020. 5, 15, 16, 17 [56] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bring- mann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. In NeurIPS, 2020. 2 [57] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad- cam: Visual explanations from deep networks via gradient- based localization. In ICCV, 2017. 7, 8, 11, 13 [58] Yuge Shi, Jeffrey Seely, Philip HS Torr, N Siddharth, Awni Hannun, Nicolas Usunier, and Gabriel Synnaeve. Gradient matching for domain generalization. In ICLR, 2021. 1, 2, 5, 6, 15, 16, 17 [59] Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In ECCV, 2016. 5, 6, 15, 16, 17 [60] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self- supervision for generalization under distribution shifts. In ICML, 2020. 1, 2, 4, 5, 7, 8, 11, 12, 13 [61] Vladimir Vapnik. The nature of statistical learning theory . Springer science & business media, 1999. 1, 5, 6, 15, 16, 17 [62] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In CVPR, 2017. 5, 16 [63] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Ol- shausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In ICLR, 2021. 2, 3, 4, 7, 8, 11, 12, 13 [64] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recog- nition from abbey to zoo. In CVPR, 2010. 5 [65] Zehao Xiao, Xiantong Zhen, Ling Shao, and Cees GM Snoek. Learning to generalize across domains on single test samples. In ICLR, 2022. 2 [66] Qinwei Xu, Ruipeng Zhang, Ya Zhang, Yanfeng Wang, and Qi Tian. A fourier-based framework for domain generaliza- tion. In CVPR, 2021. 1, 2 [67] Zhenlin Xu, Deyi Liu, Junlin Yang, Colin Raffel, and Marc Niethammer. Robust and generalizable visual representation learning via random convolutions. In ICLR, 2021. 7[68] Shen Yan, Huan Song, Nanxiang Li, Lincan Zou, and Liu Ren. Improve unsupervised domain adaptation with mixup training. arXiv preprint arXiv:2001.00677, 2020. 2, 5, 15, 16, 17 [69] Fu-En Yang, Yuan-Chia Cheng, Zu-Yun Shiau, and Yu- Chiang Frank Wang. Adversarial teacher-student representa- tion learning for domain generalization. In NeurIPS, 2021. 1, 2 [70] Nanyang Ye, Kaican Li, Haoyue Bai, Runpeng Yu, Lanqing Hong, Fengwei Zhou, Zhenguo Li, and Jun Zhu. Ood-bench: Quantifying and understanding two dimensions of out-of- distribution generalization. In CVPR, 2022. 5 [71] Fuming You, Jingjing Li, and Zhou Zhao. Test-time batch statistics calibration for covariate shift. arXiv preprint arXiv:2110.04065, 2021. 4 [72] Marvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine, and Chelsea Finn. Adaptive risk mini- mization: A meta-learning approach for tackling group distri- bution shift. arXiv preprint arXiv:2007.02931, 2020. 5, 15, 16, 17 [73] Marvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine, and Chelsea Finn. Adaptive risk mini- mization: Learning to adapt to domain shift. NeurIPS, 2021. 2 [74] Tao Zhong, Zhixiang Chi, Li Gu, Yang Wang, Yuanhao Yu, and Jin Tang. Meta-dmoe: Adapting to domain shift by meta- distillation from mixture-of-experts. In NeurIPS, 2022. 2 [75] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Do- main generalization with mixstyle. In ICLR, 2021. 1, 2, 3, 5, 6, 7, 12, 15, 16, 17 Appendix In this supplementary material, we provide, 1. Resource usage for ITTA in Section 7. 2. Grad-CAM visualizations of different loss terms in Section 8. 3. Parameter analysis of ITTA in Section 9; 4. Using a different augmentation skill for ITTA in Sec- tion 10. 5. Using different updating steps or a strategy for ITTA during the test phase in Section 11. 6. Using different network structures for the learnable consistency loss and adaptive parameters in Section 12. 7. Comparisons with other related methods in Section 13. 8. Detailed experimental results in the DomainBed bench- mark in Section 14. 7. Resource Usage Comparisons Between ITTA and the Baseline Model Requiring extra resources for our ITTA is a common lim- itation for existing test-time-based arts. To further evaluate our method, in this section, we compare FLOPS, model size, and inference time in Table 5. We compare only with ERM as most existing methods utilize the same network during in- ferences. We note that compare to the baseline model, ITTA requires extra Flops and processing time, this is because the adaptation process uses extra forward and backward steps during the test phase. While the parameters between the two models are similar because the newly included adaptive blocks are much smaller in size compared to the original model. Table 5. Resource comparisons during testing. Here inc. and exc. columns in ITTA indicate to include and exclude the TTA phase. Model Flops (G) Params (M) Time (s) Baseline 1.82 11.18 0.004 ITTA (inc.| exc.) 6.12 | 1.83 14.95 | 14.94 0.021 | 0.005 8. Grad-CAM Visualizations of Different Self- Supervised Objectives In Section 5 of the manuscript, we provide Grad-CAM [57] visualizations of our learnable consistency and the main losses to illustrate their alignment. To further show the differences between several TTT tasks [60, 63], we present more visual examples in this section. Results are shown in Figure 5. We observe that the entropy minimization [63] and rotation estimation [60] objectives do not activate the same regions as the main loss. As shown in the first row, for the class label of giraffe, both the main loss and our learned loss can correctly locate the two giraffes in the image, while the rotation estimation task can only locate one target, the same observation can be found when the learned weightsare disabled in our loss term. Meanwhile, although the two objects can be found for the entropy minimization task, the corresponding hot region does not align with that of the main loss. Similar phenomena can be observed in other samples. These visual examples demonstrate that our learned objective can better align with the main task than the TTT tasks adopted in previous works [60, 63], explaining why using the proposed learnable consistency loss can better improve TTT. 9. Parameter Analysis In this section, we analyze the hyper-parameter used in ITTA. We use the weight parameterα to balance the contri- butions from the main loss and weighted consistency loss (i.e. Lmain + αLwcont in Eq. (2) of our manuscript). To analyze the sensitivity of ITTA regarding different values of α, we conduct ablation studies in the PACS benchmark [37]. Results are listed in Table 6. We observe that the proposed ITTA can obtain favorable performances when α is in the range of 0.1 to 10, and it performs the best on average when setting as 1. We thus fix the parameter as 1 in all experi- ments. 10. A Different Augmentation Skill for ITTA In our manuscript, we use the existing augmentation strat- egy from [75] to obtain the augmented feature. In this sec- tion, we replace this implementation with that from [41] to further verify if our ITTA can still thrive with another aug- mentation skill. Different from [75] that mixes the statics of the feature to synthesize new information, [41] uses an affine transformation to create new features, where the weight for the transformation is sampled from a normal distribution with the mean value of one and standard value of zero, and the bias for the transformation is sampled from a normal distribution with the mean and standard values both zero. Experiments are conducted on the PACS benchmark [37] with the leave-one-out strategy. We compare ITTA with several different variants. (1) Ours w/o fw & TTT: this variant is the baseline model which uses the naive consistency loss for training and does not include TTT during the test phase. (2) Ours w/o fw: we disable the fw in our consistency loss, which uses the naive consistency loss for the test-time updating. (3) Ours w/o TTT: we do not update any parameters during the test phase. This variant is used to verify whether TTT can improve the pretrained model when replacing the augmentation strategy. We also compare these variants with the ERM method to show their effectivenesses. Results are listed in Table 7. We observe that ERM per- forms favorably against the baseline model, indicating that this augmentation strategy may not be beneficial for the training process. Meanwhile, we observe that when fw is disabled, the performances seem to decrease in 3 out of 4 target domains, and the average accuracy is also inferior to the baseline (i.e. Ours w/o fw & TTT). This result is in line with the finding in [46] that an inappropriate TTT task may deteriorate the performance. In comparison, we note that the performances are both improved when fw is enabled (i.e. Ours w/o TTT and Ours), which once again demonstrates that the proposed learnable consistency loss can improve the trained model. Moreover, we can also observe that when combining fw and TTT, our model is superior to other vari- ants and the ERM method. These results demonstrate that the proposed two strategies can improve the current TTT framework despite a less effective augmentation strategy. 11. Different Updating Steps or Strategies for ITTA In the manuscript, we use one TTT step for ITTA before during the testing step. In this section, we conduct experi- ments to evaluate the performances of ITTA with different TTT steps. Experiments are conducted on the PACS bench- mark [37] with the leave-one-out strategy, and each target domain is examined with 60 sets of random seeds and hyper- parameter settings. Results are listed in Table 8. We observe that the average accuracies of using more TTT steps are not improved greatly while the computational times are propor- tional to the TTT steps. To this end, we use one TTT step for ITTA as a compromise between accuracy and efficiency. We use the online setting from TTT [60] for all arts, which assumes test samples arrive sequentially and updates the adaptive blocks based on the states optimized from a previous sample. In this section, we also test ITTA in an episodic manner (i.e. Epi) [12]. Results in Table 8 suggest that while the episodic updating strategy performs slightly worse than the current scheme, and it still outperforms the baseline. 12. Different Network Structures for the Learnable Consistency Loss and Adaptive Parameters In our implementation, we use 10 layers of learnable pa- rameters for fw, and we use 5 layers of learnable parameters for fΘ after each block. In this section, we evaluate our ITTA with different network structures for these two mod- ules. Specifically, we compare the original implementation with the variants that use 1, 5, and 15 layers for fw and 1, 10, and 15 layers for fΘ to evaluate the performances of dif- ferent structures. Similarly, we conduct experiments on the PACS benchmark [37] with the leave-one-out strategy, and each target domain is examined with 60 sets of random seeds and hyper-parameter settings. Evaluation results are listed in Table 9. We observe that their differences in the average accuracy are rather subtle on account of the variances. To(a) Input (b) Entropy (c) Rotation (d) Ours w/o fw (e) Ours (f) Main Figure 5. Grad-CAM [57] visualizations from different loss terms. We use images with varying class labels (i.e. giraffe, elephant, house, and horse from top to bottom) from the four target domains of PACS [37] as inputs (i.e. art, cartoon, photo, and sketch domains from top to bottom). “Entropy” and “Rotation” here denote the entropy minimization and rotation estimation tasks in [63] and [60]. Ours w/o fw is the learnable consistency loss in Eq. (1) in the manuscript (i.e. ∥fw(z − z′)∥) when fw is disabled. The proposed learnable consistency loss can align well with the main classification task. Table 6. Sensitivity analysis of ITTA regarding different values ofα in the unseen domain from PACS [37]. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Values Target domain Avg.Art Cartoon Photo Sketch α = 0.1 83.9 ± 0.7 76.2 ± 1.1 94.8 ± 0.2 78.8 ± 0.8 83.4 ± 0.2 α = 1 (Ours) 84.7 ± 0.4 78.0 ± 0.4 94.5 ± 0.4 78.2 ± 0.3 83.8 ± 0.3 α = 10 83.9 ± 0.5 77.4 ± 0.6 94.2 ± 0.7 77.3 ± 0.8 83.2 ± 0.3 α = 100 81.5 ± 1.2 77.0 ± 0.6 92.6 ± 0.7 78.9 ± 2.1 82.5 ± 0.9 this end, we use the original implementation with 10 layers of learnable parameters for fw and 5 layers of learnable pa- rameters for fΘ, which performs relatively better than other variants. Since the adaptive blocks fΘ are attached after each layer of the network, one may wonder how the varying locations of the adaptive blocks affect the performance of ITTA. To answer this question, we further conduct experiments by adding the adaptive blocks after different layers of the orig- inal network. Denoting as Loc = lan given the n layers in the original network, we note that the model performs less effectively when the adaptive block is placed after the 1st layer of the network, and using all four adaptive blocks (i.e. ours) is more effective than other alternatives. 13. Comparisons with Other Related Methods Apart from the proposed ITTA, some other works also propose to include learnable parameters in their auxiliaryTable 7. Performances of our method with another augmentation strategy from [41] in the unseen domain from PACS [37]. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Model Target domain Avg.Art Cartoon Photo Sketch ERM 78.0 ± 1.3 73.4 ± 0.8 94.1 ± 0.4 73.6 ± 2.2 79.8 ± 0.4 Ours w/o fw & TTT 74.9 ± 0.4 74.1 ± 0.8 90.6 ± 0.3 79.7 ± 0.7 79.8 ± 0.4 Ours w/o fw 77.1 ± 1.0 73.6 ± 1.1 89.9 ± 0.4 78.4 ± 0.8 79.7 ± 0.2 Ours w/o TTT 77.5 ± 0.3 73.2 ± 0.6 92.4 ± 0.4 78.0 ± 1.0 80.3 ± 0.3 Ours (w/ fw & TTT) 79.2 ± 0.8 74.9 ± 1.1 92.2 ± 0.3 76.9 ± 0.7 80.8 ± 0.4 Table 8. Evaluations of ITTA in the unseen domain from PACS [37] with different TTT steps and updating strategies during the testing phase. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. The time consumption (TC) is computed using one image with the size of 224 × 224. Epi. denotes updating ITTA in an episodic manner. Steps Target domain Avg. TCArt Cartoon Photo Sketch 1 step (Ours) 84.7 ± 0.4 78.0 ± 0.4 94.5 ± 0.4 78.2 ± 0.3 83.8 ± 0.3 2.4 ms 2 step 84.2 ± 0.9 77.5 ± 0.6 94.4 ± 0.4 79.1 ± 1.0 83.8 ± 0.1 4.2 ms 3 step 84.5 ± 1.2 77.6 ± 0.6 94.0 ± 0.6 79.3 ± 0.1 83.9 ± 0.3 6.1 ms Epi. 83.6 ± 0.7 77.9 ± 0.5 95.2 ± 0.1 76.6 ± 0.5 83.3 ± 0.4 losses. Examples include MetaReg [2] and Feature-Critic [45] which both suggest using meta-learning to produce more general models. The main difference between these arts and ITTA is that parameters in the auxiliary loss from [2,45] are gradually refined by episode training, and they are updated via a gradient alignment step in ITTA (see Sec. 3.1 in the manuscript), which is much simpler. In this sec- tion, we compare ITTA with these two arts in the PACS dataset [37] using the same settings aforementioned. Be- cause MetaReg [2] does not release codes, we thus directly cite the data from their paper in the comparison. Different from others, the results in [2] are averaged by 5 trials accord- ing to their paper, which is much less than our experimental settings. Meanwhile, we also compare with TTT++ [46] which suggests storing the momentum of the features from the source domain and enforcing the similarity between mo- mentums of features from the source and target domains. We use the same setting in Section 5.1 from the manuscript to evaluate TTT++. Results are listed in Table 10. We observe that our method consistently outperforms that from [2,45,46] for both the cases with and without TTT, indicating that the proposed learnable consistency loss and updating method is not only simpler but also more effective than the losses in [2, 45]. 14. Detailed Results in the DomainBed Bench- mark [27] this section presents the average accuracy in each domain from different datasets. As shown in Table 11, 12, 13, 14, and 15, these results are detailed illustrations of the results in Table 2 in our manuscript. For all the experiments, we use the “training-domain validate set” as the model selection method. A total of 22 methods are examined for 60 trials in each unseen domain, and all methods are trained with the leave-one-out strategy using the ResNet18 [30] backbones.Table 9. Performances of our method with different network structures for the consistency loss (i.e. fw) and adaptive parameters (i.e. fΘ) in the unseen domain from PACS [37]. Here ‘Loc=lan’ locates the adaptive block after the n-th layer of the model (‘la4’ is the last layer). The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Structures Target domain Avg.Art Cartoon Photo Sketch Structures offw 1 layer 83.5 ±1.2 76.0 ±1.0 95.3 ±0.2 78.7 ±1.5 83.4 ±0.4 5 layers 83.7 ±0.6 76.8 ±0.9 94.6 ±0.3 78.8 ±0.3 83.5 ±0.3 10 layers (Ours) 84.7 ±0.4 78.0 ±0.4 94.5 ±0.4 78.2 ±0.3 83.8 ±0.3 15 layers 84.1 ±0.4 75.8 ±0.2 94.3 ±0.3 79.5 ±0.4 83.4 ±0.2 Structures offΘ 1 layer 84.0 ±0.6 77.4 ±0.5 94.4 ±0.5 78.3 ±0.4 83.5 ±0.3 5 layers (Ours) 84.7 ±0.4 78.0 ±0.4 94.5 ±0.4 78.2 ±0.3 83.8 ±0.3 10 layers 84.8 ±0.3 76.0 ±0.6 94.1 ±0.5 78.3 ±0.1 83.3 ±0.3 15 layers 83.9 ±0.8 76.0 ±0.5 93.8 ±0.4 78.7 ±1.4 83.1 ±0.6 Locations offΘ Loc=la1 83.4±0.7 76.8 ±0.3 94.4 ±0.3 77.8 ±0.3 83.1 ±0.3 Loc=la2 83.4±0.6 77.7 ±0.6 94.2 ±0.5 78.0 ±0.5 83.3 ±0.3 Loc=la3 84.0±0.4 77.5 ±0.3 94.4 ±0.1 77.8 ±0.1 83.4 ±0.2 Loc=la4 84.1±0.7 77.8 ±0.5 94.8 ±0.2 76.9 ±1.5 83.4 ±0.4 Table 10. Compare with learnable losses in [2, 45] in the unseen domain from PACS [37]. The reported accuracies ( %) and standard deviations are computed from 60 trials in each target domain except for [2] where the numbers are directly cited from their paper. Model Target domain Avg.Art Cartoon Photo Sketch MetaReg [2] 83.7 ± 0.2 77.2 ± 0.3 95.5 ± 0.2 70.3 ± 0.3 81.7 Feture-Critic [45] 78.4 ± 1.6 75.4 ± 1.2 92.6 ± 0.5 73.3 ± 1.4 80.0 ± 0.3 TTT++ [46] 84.3 ± 0.1 78.4 ± 0.5 93.8 ± 1.3 73.2 ± 3.2 82.4 ± 1.1 Ours w/o TTT 83.3 ± 0.5 76.0 ± 0.5 94.4 ± 0.5 76.7 ± 1.4 82.8 ± 0.3 Ours 84.7 ± 0.4 78.0 ± 0.4 94.5 ± 0.4 78.2 ± 0.3 83.8 ± 0.3 Table 11. Average accuracies on the PACS [37] datasets using the default hyper-parameter settings in DomainBed [27]. art cartoon photo sketch Average ERM [61] 78.0 ± 1.3 73.4 ± 0.8 94.1 ± 0.4 73.6 ± 2.2 79.8 ± 0.4 IRM [1] 76.9 ± 2.6 75.1 ± 0.7 94.3 ± 0.4 77.4 ± 0.4 80.9 ± 0.5 GroupGRO [55] 77.7 ± 2.6 76.4 ± 0.3 94.0 ± 0.3 74.8 ± 1.3 80.7 ± 0.4 Mixup [68] 79.3 ± 1.1 74.2 ± 0.3 94.9 ± 0.3 68.3 ± 2.7 79.2 ± 0.9 MLDG [38] 78.4 ± 0.7 75.1 ± 0.5 94.8 ± 0.4 76.7 ± 0.8 81.3 ± 0.2 CORAL [59] 81.5 ± 0.5 75.4 ± 0.7 95.2 ± 0.5 74.8 ± 0.4 81.7 ± 0.0 MMD [40] 81.3 ± 0.6 75.5 ± 1.0 94.0 ± 0.5 74.3 ± 1.5 81.3 ± 0.8 DANN [23] 79.0 ± 0.6 72.5 ± 0.7 94.4 ± 0.5 70.8 ± 3.0 79.2 ± 0.3 CDANN [44] 80.4 ± 0.8 73.7 ± 0.3 93.1 ± 0.6 74.2 ± 1.7 80.3 ± 0.5 MTL [6] 78.7 ± 0.6 73.4 ± 1.0 94.1 ± 0.6 74.4 ± 3.0 80.1 ± 0.8 SagNet [48] 82.9 ± 0.4 73.2 ± 1.1 94.6 ± 0.5 76.1 ± 1.8 81.7 ± 0.6 ARM [72] 79.4 ± 0.6 75.0 ± 0.7 94.3 ± 0.6 73.8 ± 0.6 80.6 ± 0.5 VREx [36] 74.4 ± 0.7 75.0 ± 0.4 93.3 ± 0.3 78.1 ± 0.9 80.2 ± 0.5 RSC [33] 78.5 ± 1.1 73.3 ± 0.9 93.6 ± 0.6 76.5 ± 1.4 80.5 ± 0.2 SelfReg [34] 82.5 ± 0.8 74.4 ± 1.5 95.4 ± 0.5 74.9 ± 1.3 81.8 ± 0.3 MixStyle [75] 82.6 ± 1.2 76.3 ± 0.4 94.2 ± 0.3 77.5 ± 1.3 82.6 ± 0.4 Fish [58] 80.9 ± 1.0 75.9 ± 0.4 95.0 ± 0.4 76.2 ± 1.0 82.0 ± 0.3 SD [51] 83.2 ± 0.6 74.6 ± 0.3 94.6 ± 0.1 75.1 ± 1.6 81.9 ± 0.3 CAD [53] 83.9 ± 0.8 74.2 ± 0.4 94.6 ± 0.4 75.0 ± 1.2 81.9 ± 0.3 CondCAD [53] 79.7 ± 1.0 74.2 ± 0.9 94.6 ± 0.4 74.8 ± 1.4 80.8 ± 0.5 Fishr [52] 81.2 ± 0.4 75.8 ± 0.8 94.3 ± 0.3 73.8 ± 0.6 81.3 ± 0.3 Ours 84.7 ± 0.4 78.0 ± 0.4 94.5 ± 0.4 78.2 ± 0.3 83.8 ± 0.3Table 12. Average accuracies on the VLCS [18] datasets using the default hyper-parameter settings in DomainBed [27]. Caltech LabelMe Sun VOC Average ERM [61] 97.7 ± 0.3 62.1 ± 0.9 70.3 ± 0.9 73.2 ± 0.7 75.8 ± 0.2 IRM [1] 96.1 ± 0.8 62.5 ± 0.3 69.9 ± 0.7 72.0 ± 1.4 75.1 ± 0.1 GroupGRO [55] 96.7 ± 0.6 61.7 ± 1.5 70.2 ± 1.8 72.9 ± 0.6 75.4 ± 1.0 Mixup [68] 95.6 ± 1.5 62.7 ± 0.4 71.3 ± 0.3 75.4 ± 0.2 76.2 ± 0.3 MLDG [38] 95.8 ± 0.5 63.3 ± 0.8 68.5 ± 0.5 73.1 ± 0.8 75.2 ± 0.3 CORAL [59] 96.5 ± 0.3 62.8 ± 0.1 69.1 ± 0.6 73.8 ± 1.0 75.5 ± 0.4 MMD [40] 96.0 ± 0.8 64.3 ± 0.6 68.5 ± 0.6 70.8 ± 0.1 74.9 ± 0.5 DANN [23] 97.2 ± 0.1 63.3 ± 0.6 70.2 ± 0.9 74.4 ± 0.2 76.3 ± 0.2 CDANN [44] 95.4 ± 1.2 62.6 ± 0.6 69.9 ± 1.3 76.2 ± 0.5 76.0 ± 0.5 MTL [6] 94.4 ± 2.3 65.0 ± 0.6 69.6 ± 0.6 71.7 ± 1.3 75.2 ± 0.3 SagNet [48] 94.9 ± 0.7 61.9 ± 0.7 69.6 ± 1.3 75.2 ± 0.6 75.4 ± 0.8 ARM [72] 96.9 ± 0.5 61.9 ± 0.4 71.6 ± 0.1 73.3 ± 0.4 75.9 ± 0.3 VREx [36] 96.2 ± 0.0 62.5 ± 1.3 69.3 ± 0.9 73.1 ± 1.2 75.3 ± 0.6 RSC [33] 96.2 ± 0.0 63.6 ± 1.3 69.8 ± 1.0 72.0 ± 0.4 75.4 ± 0.3 SelfReg [34] 95.8 ± 0.6 63.4 ± 1.1 71.1 ± 0.6 75.3 ± 0.6 76.4 ± 0.7 MixStyle [75] 97.3 ± 0.3 61.6 ± 0.1 70.4 ± 0.7 71.3 ± 1.9 75.2 ± 0.7 Fish [58] 97.4 ± 0.2 63.4 ± 0.1 71.5 ± 0.4 75.2 ± 0.7 76.9 ± 0.2 SD [51] 96.5 ± 0.4 62.2 ± 0.0 69.7 ± 0.9 73.6 ± 0.4 75.5 ± 0.4 CAD [53] 94.5 ± 0.9 63.5 ± 0.6 70.4 ± 1.2 72.4 ± 1.3 75.2 ± 0.6 CondCAD [53] 96.5 ± 0.8 62.6 ± 0.4 69.1 ± 0.2 76.0 ± 0.2 76.1 ± 0.3 Fishr [52] 97.2 ± 0.6 63.3 ± 0.7 70.4 ± 0.6 74.0 ± 0.8 76.2 ± 0.3 Ours 96.9 ± 1.2 63.7 ± 1.1 72.0 ± 0.3 74.9 ± 0.8 76.9 ± 0.6 Table 13. Average accuracies on the OfficeHome [62] datasets using the default hyper-parameter settings in DomainBed [27]. art clipart product real Average ERM [61] 52.2 ± 0.2 48.7 ± 0.5 69.9 ± 0.5 71.7 ± 0.5 60.6 ± 0.2 IRM [1] 49.7 ± 0.2 46.8 ± 0.5 67.5 ± 0.4 68.1 ± 0.6 58.0 ± 0.1 GroupGRO [55] 52.6 ± 1.1 48.2 ± 0.9 69.9 ± 0.4 71.5 ± 0.8 60.6 ± 0.3 Mixup [68] 54.0 ± 0.7 49.3 ± 0.7 70.7 ± 0.7 72.6 ± 0.3 61.7 ± 0.5 MLDG [38] 53.1 ± 0.3 48.4 ± 0.3 70.5 ± 0.7 71.7 ± 0.4 60.9 ± 0.2 CORAL [59] 55.1 ± 0.7 49.7 ± 0.9 71.8 ± 0.2 73.1 ± 0.5 62.4 ± 0.4 MMD [40] 50.9 ± 1.0 48.7 ± 0.3 69.3 ± 0.7 70.7 ± 1.3 59.9 ± 0.4 DANN [23] 51.8 ± 0.5 47.1 ± 0.1 69.1 ± 0.7 70.2 ± 0.7 59.5 ± 0.5 CDANN [44] 51.4 ± 0.5 46.9 ± 0.6 68.4 ± 0.5 70.4 ± 0.4 59.3 ± 0.4 MTL [6] 51.6 ± 1.5 47.7 ± 0.5 69.1 ± 0.3 71.0 ± 0.6 59.9 ± 0.5 SagNet [48] 55.3 ± 0.4 49.6 ± 0.2 72.1 ± 0.4 73.2 ± 0.4 62.5 ± 0.3 ARM [72] 51.3 ± 0.9 48.5 ± 0.4 68.0 ± 0.3 70.6 ± 0.1 59.6 ± 0.3 VREx [36] 51.1 ± 0.3 47.4 ± 0.6 69.0 ± 0.4 70.5 ± 0.4 59.5 ± 0.1 RSC [33] 49.0 ± 0.1 46.2 ± 1.5 67.8 ± 0.7 70.6 ± 0.3 58.4 ± 0.6 SelfReg [34] 55.1 ± 0.8 49.2 ± 0.6 72.2 ± 0.3 73.0 ± 0.3 62.4 ± 0.1 MixStyle [75] 50.8 ± 0.6 51.4 ± 1.1 67.6 ± 1.3 68.8 ± 0.5 59.6 ± 0.8 Fish [58] 54.6 ± 1.0 49.6 ± 1.0 71.3 ± 0.6 72.4 ± 0.2 62.0 ± 0.6 SD [51] 55.0 ± 0.4 51.3 ± 0.5 72.5 ± 0.2 72.7 ± 0.3 62.9 ± 0.2 CAD [53] 52.1 ± 0.6 48.3 ± 0.5 69.7 ± 0.3 71.9 ± 0.4 60.5 ± 0.3 CondCAD [53] 53.3 ± 0.6 48.4 ± 0.2 69.8 ± 0.9 72.6 ± 0.1 61.0 ± 0.4 Fishr [52] 52.6 ± 0.9 48.6 ± 0.3 69.9 ± 0.6 72.4 ± 0.4 60.9 ± 0.3 Ours 54.4 ± 0.2 52.3 ± 0.8 69.5 ± 0.3 71.7 ± 0.2 62.0 ± 0.2Table 14. Average accuracies on the TerraInc [4] datasets using the default hyper-parameter settings in DomainBed [27]. L100 L38 L43 L46 Average ERM [61] 42.1 ± 2.5 30.1 ± 1.2 48.9 ± 0.6 34.0 ± 1.1 38.8 ± 1.0 IRM [1] 41.8 ± 1.8 29.0 ± 3.6 49.6 ± 2.1 33.1 ± 1.5 38.4 ± 0.9 GroupGRO [55] 45.3 ± 4.6 36.1 ± 4.4 51.0 ± 0.8 33.7 ± 0.9 41.5 ± 2.0 Mixup [68] 49.4 ± 2.0 35.9 ± 1.8 53.0 ± 0.7 30.0 ± 0.9 42.1 ± 0.7 MLDG [38] 39.6 ± 2.3 33.2 ± 2.7 52.4 ± 0.5 35.1 ± 1.5 40.1 ± 0.9 CORAL [59] 46.7 ± 3.2 36.9 ± 4.3 49.5 ± 1.9 32.5 ± 0.7 41.4 ± 1.8 MMD [40] 49.1 ± 1.2 36.4 ± 4.8 50.4 ± 2.1 32.3 ± 1.5 42.0 ± 1.0 DANN [23] 44.3 ± 3.6 28.0 ± 1.5 47.9 ± 1.0 31.3 ± 0.6 37.9 ± 0.9 CDANN [44] 36.9 ± 6.4 32.7 ± 6.2 51.1 ± 1.3 33.5 ± 0.5 38.6 ± 2.3 MTL [6] 45.2 ± 2.6 31.0 ± 1.6 50.6 ± 1.1 34.9 ± 0.4 40.4 ± 1.0 SagNet [48] 36.3 ± 4.7 40.3 ± 2.0 52.5 ± 0.6 33.3 ± 1.3 40.6 ± 1.5 ARM [72] 41.5 ± 4.5 27.7 ± 2.4 50.9 ± 1.0 29.6 ± 1.5 37.4 ± 1.9 VREx [36] 48.0 ± 1.7 41.1 ± 1.5 51.8 ± 1.5 32.0 ± 1.2 43.2 ± 0.3 RSC [33] 42.8 ± 2.4 32.2 ± 3.8 49.6 ± 0.9 32.9 ± 1.2 39.4 ± 1.3 SelfReg [34] 46.1 ± 1.5 34.5 ± 1.6 49.8 ± 0.3 34.7 ± 1.5 41.3 ± 0.3 MixStyle [75] 50.6 ± 1.9 28.0 ± 4.5 52.1 ± 0.7 33.0 ± 0.2 40.9 ± 1.1 Fish [58] 46.3 ± 3.0 29.0 ± 1.1 52.7 ± 1.2 32.8 ± 1.0 40.2 ± 0.6 SD [51] 45.5 ± 1.9 33.2 ± 3.1 52.9 ± 0.7 36.4 ± 0.8 42.0 ± 1.0 CAD [53] 43.1 ± 2.6 31.1 ± 1.9 53.1 ± 1.6 34.7 ± 1.3 40.5 ± 0.4 CondCAD [53] 44.4 ± 2.9 32.9 ± 2.5 50.5 ± 1.3 30.8 ± 0.5 39.7 ± 0.4 Fishr [52] 49.9 ± 3.3 36.6 ± 0.9 49.8 ± 0.2 34.2 ± 1.3 42.6 ± 1.0 Ours 51.7 ± 2.4 37.6 ± 0.6 49.9 ± 0.6 33.6 ± 0.6 43.2 ± 0.5 Table 15. Average accuracies on the DomainNet [50] datasets using the default hyper-parameter settings in DomainBed [27]. clip info paint quick real sketch Average ERM [61] 50.4 ± 0.2 14.0 ± 0.2 40.3 ± 0.5 11.7 ± 0.2 52.0 ± 0.2 43.2 ± 0.3 35.3 ± 0.1 IRM [1] 43.2 ± 0.9 12.6 ± 0.3 35.0 ± 1.4 9.9 ± 0.4 43.4 ± 3.0 38.4 ± 0.4 30.4 ± 1.0 GroupGRO [55] 38.2 ± 0.5 13.0 ± 0.3 28.7 ± 0.3 8.2 ± 0.1 43.4 ± 0.5 33.7 ± 0.0 27.5 ± 0.1 Mixup [68] 48.9 ± 0.3 13.6 ± 0.3 39.5 ± 0.5 10.9 ± 0.4 49.9 ± 0.2 41.2 ± 0.2 34.0 ± 0.0 MLDG [38] 51.1 ± 0.3 14.1 ± 0.3 40.7 ± 0.3 11.7 ± 0.1 52.3 ± 0.3 42.7 ± 0.2 35.4 ± 0.0 CORAL [59] 51.2 ± 0.2 15.4 ± 0.2 42.0 ± 0.2 12.7 ± 0.1 52.0 ± 0.3 43.4 ± 0.0 36.1 ± 0.2 MMD [40] 16.6 ± 13.3 0.3 ± 0.0 12.8 ± 10.4 0.3 ± 0.0 17.1 ± 13.7 0.4 ± 0.0 7.9 ± 6.2 DANN [23] 45.0 ± 0.2 12.8 ± 0.2 36.0 ± 0.2 10.4 ± 0.3 46.7 ± 0.3 38.0 ± 0.3 31.5 ± 0.1 CDANN [44] 45.3 ± 0.2 12.6 ± 0.2 36.6 ± 0.2 10.3 ± 0.4 47.5 ± 0.1 38.9 ± 0.4 31.8 ± 0.2 MTL [6] 50.6 ± 0.2 14.0 ± 0.4 39.6 ± 0.3 12.0 ± 0.3 52.1 ± 0.1 41.5 ± 0.0 35.0 ± 0.0 SagNet [48] 51.0 ± 0.1 14.6 ± 0.1 40.2 ± 0.2 12.1 ± 0.2 51.5 ± 0.3 42.4 ± 0.1 35.3 ± 0.1 ARM [72] 43.0 ± 0.2 11.7 ± 0.2 34.6 ± 0.1 9.8 ± 0.4 43.2 ± 0.3 37.0 ± 0.3 29.9 ± 0.1 VREx [36] 39.2 ± 1.6 11.9 ± 0.4 31.2 ± 1.3 10.2 ± 0.4 41.5 ± 1.8 34.8 ± 0.8 28.1 ± 1.0 RSC [33] 39.5 ± 3.7 11.4 ± 0.8 30.5 ± 3.1 10.2 ± 0.8 41.0 ± 1.4 34.7 ± 2.6 27.9 ± 2.0 SelfReg [34] 47.9 ± 0.3 15.1 ± 0.3 41.2 ± 0.2 11.7 ± 0.3 48.8 ± 0.0 43.8 ± 0.3 34.7 ± 0.2 MixStyle [75] 49.1 ± 0.4 13.4 ± 0.0 39.3 ± 0.0 11.4 ± 0.4 47.7 ± 0.3 42.7 ± 0.1 33.9 ± 0.1 Fish [58] 51.5 ± 0.3 14.5 ± 0.2 40.4 ± 0.3 11.7 ± 0.5 52.6 ± 0.2 42.1 ± 0.1 35.5 ± 0.0 SD [51] 51.3 ± 0.3 15.5 ± 0.1 41.5 ± 0.3 12.6 ± 0.2 52.9 ± 0.2 44.0 ± 0.4 36.3 ± 0.2 CAD [53] 45.4 ± 1.0 12.1 ± 0.5 34.9 ± 1.1 10.2 ± 0.6 45.1 ± 1.6 38.5 ± 0.6 31.0 ± 0.8 CondCAD [53] 46.1 ± 1.0 13.3 ± 0.4 36.1 ± 1.4 10.7 ± 0.2 46.8 ± 1.3 38.7 ± 0.7 31.9 ± 0.7 Fishr [52] 47.8 ± 0.7 14.6 ± 0.2 40.0 ± 0.3 11.9 ± 0.2 49.2 ± 0.7 41.7 ± 0.1 34.2 ± 0.3 Ours 50.7 ± 0.7 13.9 ± 0.4 39.4 ± 0.5 11.9 ± 0.2 50.2 ± 0.3 43.5 ± 0.1 34.9 ± 0.1",
      "meta_data": {
        "arxiv_id": "2304.04494v2",
        "authors": [
          "Liang Chen",
          "Yong Zhang",
          "Yibing Song",
          "Ying Shan",
          "Lingqiao Liu"
        ],
        "published_date": "2023-04-10T10:12:38Z",
        "pdf_url": "https://arxiv.org/pdf/2304.04494v2.pdf",
        "github_url": "https://github.com/liangchen527/ITTA"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the distribution shift problem in Domain Generalization (DG) by improving Test-Time Training (TTT). The main contributions are: 1) Proposing a learnable consistency loss for the TTT task, which includes adjustable parameters to better align the TTT task with the main prediction task. This loss is updated by enforcing shared optimization directions between the two tasks. 2) Introducing additional adaptive parameters to the trained model and suggesting to update only these adaptive parameters during the test phase, leaving original parameters unchanged. Extensive experiments show that these two strategies are beneficial and ITTA achieves superior performance on several DG benchmarks, outperforming state-of-the-art methods in both multi-source and single-source DG tasks.",
        "methodology": "The Improved Test-Time Adaptation (ITTA) method enhances TTT in two ways. First, it introduces a learnable consistency loss (Lwcont) for the TTT task. This loss is defined as the L2 norm of the output of a weight subnetwork (fw) applied to the difference between a feature representation (z) and its augmented version (z'). The fw subnetwork uses stacked layers of ReLU(a * h + b) to provide flexibility. The learnable parameters of fw are updated by enforcing alignment with the main classification loss (Lmain), specifically by minimizing the L2 norm between the normalized gradients of Lmain and Lwcont with respect to the feature extractor's parameters (theta). Second, during the test adaptation phase, ITTA incorporates additional adaptive blocks (fΘ) after each block of the pretrained feature extractor. Only the parameters (Θ) of these newly introduced adaptive blocks are updated using the learned consistency loss, while the original feature extractor parameters remain fixed. The training process involves alternating updates for the feature extractor/classifier and the weight subnetwork, while the test adaptation phase uses an online setting to update the adaptive parameters.",
        "experimental_setup": "ITTA was evaluated on five benchmark datasets: PACS, VLCS, OfficeHome, TerraInc, and DomainNet. The model used an ImageNet-pretrained ResNet18 backbone with 4 blocks as the feature extractor (fθ). The additional adaptive parameters (fΘ) comprised 4 blocks, each with 5 layers of learnable parameters, initialized with all ones for weights and all zeros for biases. The weight subnetwork (fw) consisted of 10 layers with similar initialization. The classifier (fϕ) was an MLP layer provided by the DomainBed benchmark. Experiments were conducted under the rigorous DomainBed evaluation protocol, including 60 trials for each unseen domain, 5,000 iteration steps per trial, and dynamic setting of random seeds, learning rates, batch size, and augmentation skills. The 'training-domain validate set' model selection method was used. Evaluations covered both multi-source (leave-one-out) and single-source (PACS, trained on one domain, tested on three) domain generalization tasks. Grad-CAM visualizations were also used to analyze loss alignment.",
        "limitations": "The primary limitation of ITTA is its increased computational burden. While a naive consistency loss requires only one forward and one backward pass, updating the learnable weight subnetwork (fw) necessitates computing second-order derivatives, which demands one additional forward pass and three additional backward passes. This adds extra overhead to the system compared to simpler TTT strategies.",
        "future_research_directions": "Future research efforts will focus on simplifying the overall optimization process of ITTA and reducing its computational cost to make it more efficient without sacrificing performance.",
        "experimental_code": "class ITTA(Algorithm):\n    \"\"\"\n    Improved Test-Time Adaptation (ITTA)\n    \"\"\"\n\n    def __init__(self, input_shape, num_classes, num_domains, hparams):\n        super().__init__(input_shape, num_classes, num_domains,\n                                  hparams)\n        self.input_shape = input_shape\n        self.num_classes = num_classes\n        self.featurizer = networks.ResNet_ITTA(input_shape, self.hparams)\n        self.classifier = networks.Classifier(\n            self.featurizer.n_outputs,\n            num_classes,\n            self.hparams['nonlinear_classifier'])\n        self.test_mapping = networks.MappingNetwork() #specialized for resnet18\n        self.test_optimizer = torch.optim.Adam(self.test_mapping.parameters(), lr=self.hparams[\"lr\"]*0.1)\n        self.optimizer = torch.optim.Adam([\n            {'params': self.featurizer.parameters()},\n            {'params': self.classifier.parameters()}],\n            lr=self.hparams[\"lr\"],\n            weight_decay=self.hparams['weight_decay']\n        )\n        self.MSEloss = nn.MSELoss()\n        self.adaparams = networks.Adaparams() #specialized for resnet18\n        self.adaparams_optimizer = torch.optim.Adam(self.adaparams.parameters(), lr=self.hparams[\"lr\"]*0.1)\n\n    def _get_grads(self, loss):\n        self.optimizer.zero_grad()\n        loss.backward(inputs=list(self.featurizer.parameters()),\n                          retain_graph=True, create_graph=True)\n        dict = OrderedDict(\n            [\n                (name, weights.grad.clone().view(weights.grad.size(0),-1))\n                for name, weights in self.featurizer.named_parameters()\n            ]\n        )\n\n        return dict\n\n    def update(self, minibatches, unlabeled=None):\n        all_x = torch.cat([x for x,y in minibatches])\n        all_y = torch.cat([y for x,y in minibatches])\n        ############################# this is for network update\n        #############################\n        z_ori, z_aug = self.featurizer(all_x)\n        z_ori, z_aug = self.featurizer.fea2(z_ori, z_aug)\n        z_ori, z_aug = self.featurizer.fea_forward(z_ori), self.featurizer.fea_forward(z_aug)\n        loss_reg = self.MSEloss(self.adaparams(z_aug - z_ori), torch.zeros_like(z_aug))\n        loss_cla = F.cross_entropy(self.classifier(z_ori), all_y) + \\\n                   F.cross_entropy(self.classifier(z_aug), all_y)\n        loss = loss_reg + loss_cla\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        \n        ############################# this is for adaparams update\n        #############################\n        z_ori, z_aug = self.featurizer(all_x)\n        z_ori, z_aug = self.featurizer.fea2(z_ori, z_aug)\n        z_ori, z_aug = self.featurizer.fea_forward(z_ori), self.featurizer.fea_forward(z_aug)\n        loss_reg = self.MSEloss(self.adaparams(z_aug - z_ori), torch.zeros_like(z_aug))\n        loss_cla = F.cross_entropy(self.classifier(z_ori), all_y) + \\\n                   F.cross_entropy(self.classifier(z_aug), all_y)\n        dict_reg = self._get_grads(loss_reg)\n        dict_cla = self._get_grads(loss_cla)\n        penalty = l2_between_dicts(dict_reg, dict_cla, normalize=True) * 0.1\n        self.adaparams_optimizer.zero_grad()\n        penalty.backward(inputs=list(self.adaparams.parameters()))\n        self.adaparams_optimizer.step()\n\n        return {'loss': loss_cla.item(), 'reg': loss_reg.item()}\n\n    def test_adapt(self, x):\n        z_ori, z_aug = self.featurizer(x)\n        z_ori, z_aug = self.test_mapping.fea1(z_ori), self.test_mapping.fea1(z_aug)\n        z_ori, z_aug = self.featurizer.fea2(z_ori, z_aug)\n        z_ori, z_aug = self.test_mapping.fea2(z_ori), self.test_mapping.fea2(z_aug)\n        z_ori, z_aug = self.featurizer.fea3(z_ori), self.featurizer.fea3(z_aug)\n        z_ori, z_aug = self.test_mapping.fea3(z_ori), self.test_mapping.fea3(z_aug)\n        z_ori, z_aug = self.featurizer.fea4(z_ori), self.featurizer.fea4(z_aug)\n        z_ori, z_aug = self.test_mapping.fea4(z_ori), self.test_mapping.fea4(z_aug)\n        z_ori, z_aug = self.featurizer.flat(z_ori), self.featurizer.flat(z_aug)\n        ########## small lr for large datasets\n        loss_reg = self.MSEloss(self.adaparams(z_aug-z_ori), torch.zeros_like(z_ori)) * self.hparams['ada_lr']\n        self.test_optimizer.zero_grad()\n        loss_reg.backward(inputs=list(self.test_mapping.parameters()))\n        self.test_optimizer.step()\n\n    def predict(self, x):\n        z_ori, z_aug = self.featurizer(x)\n        z_ori = self.test_mapping.fea1(z_ori)\n        z_ori, z_aug = self.featurizer.fea2(z_ori,z_aug)\n        z_ori = self.test_mapping.fea2(z_ori)\n        z_ori = self.featurizer.fea3(z_ori)\n        z_ori = self.test_mapping.fea3(z_ori)\n        z_ori = self.featurizer.fea4(z_ori)\n        z_ori = self.test_mapping.fea4(z_ori)\n        z_ori = self.featurizer.flat(z_ori)\n        return self.classifier(z_ori)",
        "experimental_info": "ITTA (Improved Test-Time Adaptation) is implemented as a subclass of `Algorithm` and leverages specialized network components for its two-pronged approach. The core components are:\n\n**Network Architecture:**\n*   `self.featurizer`: An instance of `networks.ResNet_ITTA`, which is a modified ResNet designed to output two feature representations (`z_ori`, `z_aug`) for an input `x`. It incorporates a `mixstyle` augmentation technique (with a 0.5 probability) after the first ResNet block (`layer1`). The `mixstyle` function applies random mixing of mean and standard deviation of features across batch samples.\n*   `self.classifier`: A standard `networks.Classifier` built on top of the featurizer's output.\n*   `self.test_mapping`: An instance of `networks.MappingNetwork`, which represents the *additional adaptive blocks (fΘ)* mentioned in the method. It's designed specifically for ResNet18 (as implied by comments) and contains learnable weights and biases across four feature stages (`fea1`, `fea2`, `fea3`, `fea4`).\n*   `self.adaparams`: An instance of `networks.Adaparams`, which is the *weight subnetwork (fw)*. It's a simple ReLU-stacked MLP (depth 10) that takes the difference between feature representations as input.\n\n**Training (`update` method):**\n1.  **Feature Extraction and Augmentation:** `self.featurizer` processes input `all_x` to produce `z_ori` and `z_aug`. The `mixstyle` augmentation is applied probabilistically within the featurizer.\n2.  **Learnable Consistency Loss (Lwcont):** `loss_reg = self.MSEloss(self.adaparams(z_aug - z_ori), torch.zeros_like(z_aug))` calculates the L2 norm of the output of `adaparams` (fw) applied to the difference between original and augmented features (`z_aug - z_ori`), minimizing it towards zero. This is the `Lwcont` loss.\n3.  **Main Classification Loss (Lmain):** `loss_cla = F.cross_entropy(self.classifier(z_ori), all_y) + F.cross_entropy(self.classifier(z_aug), all_y)` computes cross-entropy for both original and augmented features, summing them up.\n4.  **Feature Extractor/Classifier Update:** The total loss `loss = loss_reg + loss_cla` is used to update the `featurizer` and `classifier` parameters via `self.optimizer`.\n5.  **Weight Subnetwork (`adaparams`) Update:** After updating the main network, `adaparams` is updated. Gradients of `loss_reg` (`dict_reg`) and `loss_cla` (`dict_cla`) with respect to the *featurizer's parameters* are computed. An alignment penalty is calculated: `penalty = l2_between_dicts(dict_reg, dict_cla, normalize=True) * 0.1`. This penalty, representing the L2 norm between normalized gradients, is then used to update the `adaparams` parameters via `self.adaparams_optimizer`. This minimizes the L2 norm between the normalized gradients of Lmain and Lwcont with respect to the feature extractor's parameters (theta).\n\n**Test-Time Adaptation (`test_adapt` method):**\n*   During test adaptation, the method processes input `x` through the featurizer (`self.featurizer(x)`). The resulting features `z_ori` and `z_aug` are passed sequentially through the `self.test_mapping` adaptive blocks (`fea1` to `fea4`) *after* each corresponding block of the original featurizer (implicit, as ResNet_ITTA's fea2-4 are separate calls).\n*   The consistency loss `loss_reg = self.MSEloss(self.adaparams(z_aug-z_ori), torch.zeros_like(z_ori)) * self.hparams['ada_lr']` is computed, and *only* the parameters of `self.test_mapping` are updated using `self.test_optimizer`.\n\n**Inference (`predict` method):**\n*   Similar to `test_adapt`, input `x` passes through `self.featurizer` and then sequentially through the adaptive blocks of `self.test_mapping` along the feature extraction path (`fea1` to `fea4`). Finally, the `classifier` makes the prediction.\n\n**Hyperparameters (`hparams_registry.py`):**\n*   `ada_lr`: `0.1` for `DomainNet`, `1e-6` for other datasets. This controls the learning rate for the test-time adaptation optimizer (`self.test_optimizer`). The comment \"########## small lr for large datasets\" suggests a specific heuristic.\n\n**Evaluation (`domainbed/lib/misc.py`, `accuracy_tsc` function):**\n*   The `accuracy_tsc` function specifically handles the ITTA evaluation. Before calculating accuracy for a batch, it freezes the featurizer and classifier (`network.featurizer.requires_grad_(False)`, `network.classifier.requires_grad_(False)`). Then, for each batch `x`, it performs a single step of `network.test_adapt(x)` to update the adaptive blocks. After this, it evaluates the adapted network (`network.eval()`, `network.predict(x)`) and restores the trainable state of featurizer and classifier. This confirms the online, per-batch adaptation at test time."
      }
    }
  ]
}