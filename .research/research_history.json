{
  "research_topic": "KVキャッシュ量子化 × 精度保持を改善したい（2bit域）",
  "queries": [
    "2-bit KV quantization",
    "accuracy preservation quantization",
    "low-bit attention cache",
    "quantization error reduction",
    "mixed-precision quantization"
  ],
  "research_study_list": [
    {
      "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization",
      "abstract": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model.",
      "full_text": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization Coleman Hooper1 Sehoon Kim1 Hiva Mohammadzadeh1 Michael W. Mahoney1,2,3 Yakun Sophia Shao1 Kurt Keutzer1 Amir Gholami1,2 1University of California, Berkeley 2ICSI 3LBNL {chooper, sehoonkim, hiva, mahoneymw, ysshao, keutzer, amirgh}@berkeley.edu Abstract LLMs are seeing growing use for applications which require large context windows, and with these large context windows KV cache activations surface as the dominant contributor to memory consumption during inference. Quantization is a promising approach for compressing KV cache activations; however, existing solutions fail to represent activations accurately in sub-4-bit precision. Our work, KVQuant, facili- tates low precision KV cache quantization by incorporating several novel methods: (i) Per-Channel Key Quantization, where we adjust the dimension along which we quantize the Key activations to better match the distribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations before the rotary positional em- bedding to mitigate its impact on quantization; (iii) Non-Uniform KV Cache Quan- tization, where we derive per-layer sensitivity-weighted non-uniform datatypes that better represent the distributions; and (iv) Per-Vector Dense-and-Sparse Quanti- zation, where we isolate outliers separately for each vector to minimize skews in quantization ranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral models, we achieve < 0.1 perplexity degradation with 3-bit quantiza- tion on both Wikitext-2 and C4, outperforming existing approaches. Our method enables serving LLaMA-7B with a context length of up to 1 million on a single A100-80GB GPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for KVQuant, showing that we can achieve up to ∼1.7× speedups, compared to baseline fp16 matrix-vector multiplications, for the LLaMA-7B model. Code is available at https://github.com/SqueezeAILab/KVQuant. 1 Introduction Large language models (LLMs) have revolutionized many natural language processing (NLP) tasks. In order to improve the capabilities of LLMs, there is significant interest in increasing the context lengths of LLMs. Longer context lengths enable new applications, including long document summarization, retrieval for answering questions about long documents, extended multi-turn applications [6], and code analysis. To support this pull from applications, there have been significant recent advances in long-context length models in industry [2, 30], as well as in academia [6, 22]. Given the importance of LLM workloads, there is strong motivation to improve their inference efficiency. LLM inference with large context lengths can be incredibly resource-intensive; serving LLMs requires high-end GPUs, and the largest LLMs require costly multi-GPU inference setups. When analyzing the computational nature of generative inference with LLMs, it becomes quickly apparent that, for relatively small batch sizes, the computation is memory bound [ 17]. With the growing divergence between computational speeds and memory speeds, this problem is only going to get worse over time [12]. This makes reducing the memory bottleneck preeminently important. Further analysis shows that the memory bottleneck is strongly related to context size. For short 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2401.18079v6  [cs.LG]  28 May 202516% 84%98% 2% Long Sequence LengthsKV Cache is the bottleneckShort sequence length Weightsare the bottleneck BaselineKVQuant int3 +Grouping 10.87 Pre-RoPEKeyQuantizationNon-UniformQuantization5.755.94 1%Outliers6.23 Per-Channel KeyQuantization7.05 Perplexity on Wikitext2fp16 Baseline Figure 1: Left: Model size versus activation memory size for the LLaMA-7B model with sequence length 512 and 128K. For longer context lengths, the KV cache becomes the dominant memory bottleneck. Memory consumption of model weights and KV cache activations for different LLaMA models with different sequence lengths are provided in Table 7 in Appendix A. Right: Overview of the different components used in KVQuant that result in less than 0.1 perplexity degradation over the fp16 baseline when quantizing the KV cache for the LLaMA-7B model to 3-bit precision. As shown in Table 1, our 3-bit approach results in4.8× reduction in cached activation memory footprint. sequence lengths, the dominant contributor to memory consumption is the weight matrices, and therefore the optimal strategy is to minimize the model size in order to reduce memory consumption as well as bandwidth requirements [ 18, 17]. However, as shown in Figure 1, the main bottleneck for long sequence lengths is the memory requirements for caching Key and Value (KV) activations throughout inference. This challenge is further exacerbated when one considers batched inference. It is therefore crucial to develop methods for compressing the KV cache to enable efficient long- sequence length inference. Existing approaches lead to unacceptable accuracy degradation due to the outlier structures in KV cache activations as well as suboptimal bit allocation with existing uniform and non-uniform approaches. In this work, we perform an extensive analysis of KV cache activations in recent LLMs, revealing patterns which can be exploited to enable ultra-low precision quantization with minimal accuracy loss. In particular, we make the following contributions (summarized in Figure 1): • We find that the Keys exhibit outliers in specific channels before applying RoPE. However, the outlier channel magnitudes become less consistent after applying RoPE, posing a distinct challenge for low precision quantization. We address this by quantizing Keys per-channel before RoPE is applied (see Section 3.1 and Section 3.2). • We find that existing uniform and non-uniform quantization methods result in sub-optimal quanti- zation signpost placement. Instead, we propose a Non-Uniform Quantization (NUQ) method which considers sensitivity and not just magnitude when quantizing activations. We show that one can apply sensitivity-weighted non-uniform quantization offline on a calibration set to derive accurate datatypes for KV cache quantization (see Section 3.3). • Even with the above, we find that outlier values in cached KV activations can significantly de- grade quantization resolution. Unlike for weights, it is non-trivial to extract outlier values from activations, given the dynamic nature of activations. However, we find that we can efficiently and accurately identify and compress outlier values in order to store them compactly in a separate sparse representation. We also find that per-vector outlier detection outperforms per-matrix outlier detection with no additional memory overhead. By removing only 1% of outliers, we can attain under 0.1 perplexity degradation on both Wikitext-2 and C4 for 3-bit KV cache quantization with the LLaMA, Llama-2, Llama-3, and Mistral models, thereby facilitating accurate inference with 4.8× longer context length. • We implement custom CUDA kernels to perform activation quantization efficiently during inference, achieving up to ∼1.7× speedups for Key and Value matrix-vector multiplications for LLaMA-7B at 4-bit precision relative to the fp16 baseline (see Section 3.7 and 4.4). These results demonstrate how our methodology allows for accurate and efficient low-bit KV cache quantization. 22 Background 2.1 LLM Inference When inferring a decoder-only LLM, inference proceeds in two distinct phases. In the prefill phase, the model takes in an input prompt, which it processes in parallel. During the generation phase, the model then generates the output sequence autoregressively, meaning that each token generation is dependent on all previously generated tokens. As such, for small batch sizes, the generation phase of LLM inference is typically memory-bandwidth bound, as the only available parallelism is across different sequences in a given batch. Additionally, during generation, the model needs to store intermediate Key and Value activations at each layer in order to condition generations on previously generated output tokens. Otherwise, we would need to recompute all prior Keys and Values at each timestep, which would be prohibitively expensive. These stored activations are referred to as the Key-Value (KV) cache. Throughout this paper, we will capitalize Key and Value to distinguish when we are referring to the KV cache tensors. Assuming a model with n layers and h attention heads with dimension d that is stored using e bytes per element, the KV cache size for batch size b and sequence length l is 2 · n · h · d · e · b · l, meaning that it grows linearly with both batch size and sequence length. As shown in Table 7, the KV cache becomes the dominant contributor to memory consumption for longer sequence lengths and larger batch sizes. Note that since each sequence in batched inference depends on separate past context, there is no available batch-level parallelism when loading the cached Keys and Values for their respective computations in batched inference. KV cache loading is therefore always memory-bandwidth bound. This motivates pursuing methods to optimally compress the KV cache, even at the expense of a more complex dequantization process. 2.2 LLM Quantization There have been many prior works on LLM quantization. Several have focused on weight-only quantization for LLMs, due to the greater contribution to memory consumption and runtime for small sequence lengths and batch sizes [21, 9, 17]. Prior works have noted the presence of distinct outliers in both weights and activations [7, 9, 17]. One approach that has been developed to address this outlier issue in the context of weight quantization is dense-and-sparse quantization, where each weight matrix is decomposed into a sparse outlier matrix and a dense low-precision matrix [9, 17]. Prior works have also leveraged non-uniform quantization methods to improve accuracy for the same bit precision by allowing for flexible quantization signpost placement [17, 8]. These approaches have either used a fixed non-uniform datatype such as NormalFloat [8], or derived quantization signposts using a sensitivity-weighted k-means approach [17]. Appendix B provides a more detailed overview of related work for outlier-aware LLM quantization and non-uniform LLM quantization. There has also been work on quantizing both weights and activations (including KV cache) [41, 33]. However, there is still a significant perplexity degradation when quantizing KV cache activations to low precision; [34, 44] quantized KV cache activations to 4-bits, but required fine-grained grouping for 4-bit quantization, while still observing some perplexity degradation, and [34] observed that 3-bit KV cache quantization with fine-grained grouping leads to unacceptable accuracy loss. Other works quantized KV cache activations to 4-bits but required retraining to maintain performance [24]. One concurrent work also explores low precision KV cache quantization in order to enable larger batch size inference by reducing the KV cache size [26]. 2.3 KV Cache Compression There have also been several prior works on compressing the KV cache. Some of these methods aim to only store important tokens in the KV cache and to evict less important tokens, thereby maintaining low memory usage [25, 43, 11, 20]. Other methods aim to only retrieve a subset of tokens at each step to achieve memory bandwidth savings [32]. In this work, we explore KV cache quantization as an orthogonal direction for compressing the KV cache in order to enable long context inference. 3Channel 0 10002000 3000 4000 T oken 0 500 1000 1500 2000 Magnitude 0.0 2.5 5.0 7.5 10.0 12.5 15.0 Layer 10 Keys (pre-RoPE) Channel 0 10002000 3000 4000 T oken 0 500 1000 1500 2000 Magnitude 0.0 2.5 5.0 7.5 10.0 12.5 15.0 Layer 10 Keys (post-RoPE) Channel 0 10002000 3000 4000 T oken 0 500 1000 1500 2000 Magnitude 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Layer 10 Values Figure 2: Example distributions of the activation values for Keys pre-RoPE, Keys post-RoPE, and Values for LLaMA-7B on a sample with 2K sequence length from Wikitext-2. We observe several patterns: (i) Keys pre-RoPE exhibit clear outliers in specific channels across different tokens; (ii) after applying RoPE, the distribution becomes less structured and there are less consistent magnitudes for outlier channels (this is expected, as RoPE applies a rotation operation between pairs of channels); and (iii) Values exhibit no fixed outlier pattern with outlier values across channels and tokens. 3 Method 3.1 Per-Channel Key Quantization To inform our approach, we first performed a detailed analysis to understand the KV cache distribu- tions. Figure 2 shows sample distributions for the KV cache activations. We observe that the Key matrices tend to have distinct outlier channels, which have larger average magnitudes than other channels; this corroborates previous observations about outlier channels in LLM activations [7, 41]. The Value matrices exhibit both outlier channels as well as outlier tokens (although these outliers are less extreme than the outlier Key channels). Existing KV cache quantization approaches perform per-token quantization (meaning that the scaling factor and zero-point are shared by elements in the same token) [ 34, 44]. However, due to the differing average magnitudes between channels, the values within a channel are easier to quantize when grouped together than the values across different channels. As such, to better match the distributions, we investigate per-channel KV cache quantization, meaning that the scaling factor and zero-point are shared by elements in the same channel. By sharing the scaling factor and zero-point along the channel dimension, this will naturally group together values with similar magnitudes, thereby mitigating the impacts of outlier channels on other channels when quantizing to low precision. As outlined in Appendix G, we find that per-channel quantization provides significant accuracy benefits for Keys but not for Values. By leveraging per-channel quantization for Keys and per- token quantization for Values, we observe a 3.82 perplexity improvement on Wikitext-2 for 3-bit LLaMA-7B quantization. Note that this can potentially add runtime overhead since the quantization dimension is now misaligned with the reduction dimension for the Keys when performing matrix- vector multiplications. However, we find that we are able to efficiently dequantize Keys and perform the Query-Key matrix-vector multiplication without adding runtime overhead, as shown in Section 4.4. Additionally, as outlined in Section 3.6, per-channel quantization can also be challenging due to the need to recompute scaling factors as tokens are added to the Key cache. We show that we can calibrate offline for scaling factors, thereby avoiding expensive online recomputation. Per-channel Key quantization was also explored in another concurrent work [26], which leveraged similar intuition about grouping together large magnitude values in the same channel to minimize quantization error. Their methodology requires fine-grained grouping for per-channel quantization while maintaining a residual subset of the KV cache in fp16. In our work, we demonstrate that by leveraging offline calibration, we can accurately perform per-channel quantization without grouping. 3.2 Pre-RoPE Key Quantization One issue when quantizing Keys is handling the rotary positional embedding (RoPE), which is applied to Keys and Queries in most public LLMs, including LLaMA and Llama-2 [35]. Given Query and Key vectors Qm = Wq ∗ xm and Kn = Wk ∗ xn at positions m and n in the sequence, RoPE is applied as position-dependent rotations to each of these vectors to obtain ˜Qm = Rd θ,m · Qm and 4˜Kn = Rd θ,n · Kn. This embeds the relative position between a Query and Key vector as an amount of an angle that is a multiple of its position index. When caching Key vectors, we therefore need to either cache ˜Kn, or else we need to cache Kn and apply Rd θ,n on-the-fly during inference. The challenge with caching Key vectors after applying this rotation is that it leads to mixing pairs of channels by different amounts for different positions in the sequence, as shown in Appendix C (since it jointly rotates pairs of channels by different angles depending on the position in the sequence). The post-RoPE activation distribution is also shown in Figure 2, demonstrating how the rotation between pairs of channels leads to less consistent channel magnitudes. This makes it harder to quantize Key activation channels which would typically have consistent large-magnitude values. This motivated our investigation into whether we could perform pre-RoPE Key quantization (meaning that we quantize Kn) and then efficiently apply the positional embeddings on-the-fly after dequantization. The benefits of pre-RoPE Key quantization are highlighted in Appendix H, yielding 0.82 perplexity improvement on Wikitext-2 for 3-bit LLaMA-7B quantization. To be able to quantize Keys pre-RoPE, we develop a fused kernel to efficiently apply RoPE post-dequantization (the details of this approach will be discussed in Section 3.7). 3.3 nuqX: An X-Bit Per-Layer Sensitivity-Weighted Non-Uniform Datatype Uniform quantization is suboptimal for KV cache quantization since the Query and Key activations are non-uniform. Additionally, KV cache loading is memory bandwidth bound, regardless of batch size or sequence length, meaning that the dequantization overhead introduced by non-uniform quantization methods is not problematic (since the added computation does not introduce any additional latency). It is therefore desirable to leverage non-uniform quantization methods for KV cache quantization. In [17], the authors computed non-uniform quantization signposts using a sensitivity-weighted k-means approach. However, this cannot be applied directly to KV cache quantization as the Values are quantized dynamically at runtime, which means that we would need to apply K-means online during inference, and it is also difficult to estimate sensitivity for activations online. We therefore facilitate efficient online non-uniform KV cache quantization by deriving a per-tensor non-uniform datatype offline on calibration data, which is then rescaled per-channel or per-token to accurately represent the key and value distributions. We compute sensitivity-weighted quantization signposts offline on a calibration set prior to inference, while maintaining compatibility with per-vector quantization by separately normalizing each channel to the range [−1, 1] prior to deriving the shared datatype. Using the diagonal Fisher information matrix (derived in Appendix D), along with the quantization error for activation A, we formulate the error minimization objective as follows, where A is flattened to one dimension and where N is the number of elements from all of the samples in our calibration set: Q(A)∗ ≃ arg min Q NX i=1 Fii \u0000 Ai − Q(Ai) \u00012 . (1) We modify the objective in Equation 1 as described in Appendix E in order to apply it using the normalized activation values. We then minimize it offline on a calibration set using a k-means solver in order to obtain the quantization signposts for the non-uniform datatype for each Key or Value layer. Appendix I compares our non-uniform quantization approach with existing uniform and non-uniform quantization baselines [8], demonstrating how our non-uniform approach provides 0.29 perplexity improvement on Wikitext-2 for LLaMA-7B relative to 3-bit uniform methods. Table 16 in Appendix L shows how computing the required Fisher information for the LLaMA-65B model takes only a few minutes, and how using the k-means solver takes only a few minutes per layer (with the computation for each layer being parallelizable). 3.4 Per-Vector Dense-and-Sparse Quantization As shown in Figure 4 in Appendix F, for both Keys and Values, the majority of elements are contained within a small percentage of the dynamic range. This means that by leveraging dense-and-sparse quantization, as demonstrated in [17], in order to isolate a small percentage of numerical outliers, we can restrict the range that we need to represent, thereby allowing us to represent the remaining elements with greater precision. However, when looking at the Key and Value distributions in Figure 2, different channels and tokens have different average magnitudes. Therefore, an element which counts as an outlier in one channel may not be an outlier in another channel (since that channel 5may have a greater average magnitude), making naive application of dense-and-sparse quantization suboptimal. It is therefore crucial to directly target the outlier values that skew the dynamic range at the granularity that we are quantizing in order to address the values that are exaggerating the range along that particular dimension. In this work, we leverage per-vector dense-and-sparse quantization, where we use a different outlier threshold per-vector (either a separate threshold per-channel for per-channel quantization, or a separate threshold per-token for per-token quantization), rather than a single outlier threshold for each layer. Note that computing outlier thresholds for per-vector dense-and-sparse quantization poses potential accuracy and efficiency challenges. However, in Section 3.6, we show that we are able to accurately calibrate for per-channel outlier thresholds offline and efficiently compute per-token outlier thresholds online. After determining the upper and lower outlier thresholds, the remaining numbers in the vector are normalized to the range [−1, 1], and we then minimize Equation 1 (ignoring outliers) in order to obtain the quantization signposts for the non-uniform datatype for the remaining numbers. Appendix J will demonstrate the benefits of removing a small percentage of numerical outliers and keeping them in full precision, as well as the advantages of per-vector dense-and-sparse quantization over using a single global outlier threshold for each layer. As shown in Figure 1, by removing 1% of numerical outliers using per-vector outlier thresholds, we achieve an additional 0.19 perplexity improvement on Wikitext-2 for 3-bit LLaMA-7B quantization, which is within 0.07 perplexity of the fp16 baseline. 3.5 Attention Sink-Aware Quantization Prior work has demonstrated that after the first few layers in LLMs, the model tends to allocate a large attention score to the first token [42]. This occurs even when the initial token is not semantically important. This phenomenon happens because the model tends to use the inital token as a “sink”. In our work, we demonstrate that due to the Attention Sink phenomenon, the model is disproportionately sensitive to quantization error in the first token. By keeping only the first token in fp16, we can attain perplexity benefits, particularly for 2-bit quantization. A similar observation has also been made in another concurrent work [ 23]. Note that when retaining the first token in fp16, we account for this during the calibration process as well, meaning that we ignore the first token when deriving the nuqX datatype and when calibrating the scaling factors and zero points offline for the Keys. As demonstrated in Appendix K, this approach persistently yields performance benefits, particularly with lower bit widths and without dense-and-sparse quantization. 3.6 Offline Calibration versus Online Computation A crucial challenge for activation quantization is that we either need to compute statistics on-the-fly (which is potentially expensive) or else we need to use offline calibration data (which potentially has negative accuracy implications). The challenges with computing scaling factors (and zero-points) online versus offline for both Keys and Values are shown in Figure 5 in Appendix L. In per-channel quantization, it is challenging to update scaling factors online since the scaling factors corresponding to each incoming channel would potentially need to be updated whenever a new token is added to the KV cache. It is therefore desirable to be able to compute statistics offline (i.e., using calibration data before running inference). While this can have negative effects on model accuracy, in Appendix L we show that we can effectively calibrate offline for per-channel quantization, obviating the need for online updates of scaling factors for per-channel quantization. For per-token quantization, it is challenging to calibrate for scaling factors offline due to the presence of outlier Value tokens. It is therefore desirable to be able to compute scaling factors and outlier thresholds online for each incoming token. As shown in Appendix L, we can efficiently compute outlier thresholds online per-token by offloading to the CPU. By leveraging custom quantization function implementations for compressing activations, we are able to perform online per-token Value quantization without compromising on performance. 3.7 Kernel Implementation In order to efficiently perform activation quantization on-the-fly, we leverage dedicated kernel implementations with our 4-bit quantization method for compressing vectors to reduced precision and extracting the sparse outliers, performing matrix-vector multiplications using the compressed 6Table 1: Evaluation of our method for different models using the perplexity (PPL) on Wikitext-2. KVQuant results are using pre-RoPE per-channel quantization for Keys. KV cache sizes are estimated assuming a sequence length of 128K (ignoring context length limits for the models). Note that ATOM and FlexGen use 4-bit quantization with group sizes of 128 and 64 with uniform quantization, respectively, and we extend their methods to 3-bit and 2-bit quantization. We leverage Attention Sink-Aware quantization for all bit widths. We used post-RoPE quantization for all baseline methods since it achieves higher accuracy when quantizing Keys per-token as shown in Appendix P. Table 18 in Appendix O demonstrates a full evaluation on all LLaMA, Llama-2, Llama-3, and Mistral models. Method LLaMA-7B LLaMA-13B LLaMA-30B LLaMA-65B PPL KV Cache (GB) PPL KV Cache (GB) PPL KV Cache (GB) PPL KV Cache (GB) baseline 5.68 64.0 5.09 100.0 4.10 195.0 3.53 320.0 int4 5.98 16.0 5.32 25.0 4.34 48.8 3.73 80.1 nf4 5.87 16.0 5.23 25.0 4.25 48.8 3.63 80.1 ATOM-4bit 5.77 16.6 5.16 26.0 4.16 50.7 3.57 83.1 FlexGen-4bit 5.73 17.3 5.14 27.0 4.14 52.6 3.56 86.3 KVQuant-4bit 5.72 16.0 5.13 25.0 4.13 48.8 3.55 80.0 KVQuant-4bit-1% 5.69 17.3 5.10 27.0 4.11 52.7 3.54 86.5 int3 10.87 12.0 8.69 18.8 6.82 36.6 6.37 60.1 nf3 7.33 12.0 6.21 18.8 5.46 36.6 4.44 60.1 ATOM-3bit 6.17 12.6 5.47 19.7 4.44 38.4 3.78 63.0 FlexGen-3bit 5.93 13.2 5.29 20.6 4.26 40.2 3.66 65.9 KVQuant-3bit 5.87 12.0 5.25 18.8 4.25 36.6 3.63 60.0 KVQuant-3bit-1% 5.75 13.3 5.14 20.8 4.15 40.5 3.57 66.5 int2 11779 8.0 69965 12.5 1470 24.4 7272 40.1 nf2 3210 8.0 5786 12.5 2044 24.4 5367 40.1 ATOM-2bit 37.37 8.6 41.77 13.4 16.49 26.1 13.63 42.8 FlexGen-2bit 11.09 9.1 9.84 14.3 6.60 27.8 5.54 45.6 KVQuant-2bit 7.23 8.0 5.82 12.5 4.87 24.4 4.03 40.0 KVQuant-2bit-1% 6.01 9.3 5.36 14.5 4.35 28.3 3.70 46.5 vectors, and performing sparse matrix-dense vector multiplications using the sparse outliers. We store the quantized Key and Value matrices as 4-bit elements which are used as indices into lookup tables to recover the corresponding fp16 values. We store the sparse outlier matrices in either Compressed- Sparse Row (CSR) or Compressed-Sparse Column (CSC) format (depending on which aligns better with appending new Key and Value tokens). The kernels for the Key matrix-vector operations apply RoPE on-the-fly in order to support pre-RoPE quantization. More kernel implementation details are provided in Appendix R. 4 Results 4.1 Main Evaluation We used the LLaMA-7B/13B/30B/65B, Llama-2-7B/13B/70B, Llama-3-8B/70B, and Mistral-7B models to evaluate our methodology by measuring perplexity on both Wikitext-2 and C4 [36, 37, 1, 16, 27, 31]. Perplexity has been measured using teacher forcing with the output logits of all input tokens. We compared our method against (i) uniform quantization without grouping (intX), (ii) nonuniform quantization using NormalFloat [8] without grouping (nfX), as well as (iii) Atom [44] and FlexGen [34]. Note that Atom and FlexGen use uniform quantization with group sizes of 64 and 128, respectively. All the KVQuant models throughout this experiment section are calibrated using 16 calibration samples of sequence length 2K from the Wikitext-2 training set. See Appendix M for details on our experimental setup, including our methodology for computing KV cache size estimates. Table 1 shows the results for LLaMA models for the Wikitext-2 dataset. We compared our method with per-token quantization with and without grouping. The baseline configurations used by Atom and FlexGen are included for reference [44, 34]. We find that our method consistently outperforms baseline approaches by an especially large margin with 3-bit and 2-bit quantization. Once we incorporate outliers, we further push the performance of low-precision quantization, achieving 4-bit quantization with less than 0.02 perplexity degradation, 3-bit quantization with under 0.1 perplexity degradation, and 2-bit quantization with under 0.5 perplexity degradation on Wikitext-2, relative to the fp16 baseline, across all models (while attaining 3.7 ×, 4.8×, and 6.9 × memory savings, respectively). 73.7x4.8x6.8x 3.7x4.8x6.8x Figure 3: Perplexity results for the LLaMA-2-7B-32K model [5] as well as the Llama-2-70B-32K LongLoRA model [6] on the Wikitext-2 dataset, evaluated using different sequence lengths. Table 2: Passkey retrieval results across different context lengths for the LLaMA-2-7B-32K model (uptrained for long sequence lengths using positional interpolation [ 5]) as well as the Llama-2- 70B-32K LongLoRA model [6]. The values reported are the success rate for retrieving the passkey, computed over 50 samples. We also include comparisons with KIVI for reference, using the 2-bit configuration with group size of 32 and 128-element fp16 residual [ 26]. Average bit widths are estimated for each approach assuming 32K context length. Note that the open-source code for running KIVI with LLaMA does not support grouped-query attention, so we did not include comparisons with KIVI for Llama-2-70B-32K. Model Method 2K 4K 8K 16K 32K Avg. Bit Width LLaMA-2-7B-32K fp16 1 1 1 1 1 16 KIVI-2-gs32-r128 0.76 0.72 0.72 0.68 0.7 3.05 nuq4-1% 1 1 1 1 1 4.33 nuq3-1% 0.98 1 1 1 1 3.33 nuq2-1% 1 1 0.98 1 1 2.33 Llama-2-70B-32K fp16 1 1 1 1 1 16 nuq4-1% 1 1 1 1 1 4.35 nuq3-1% 1 1 1 1 1 3.35 nuq2-1% 0.98 0.98 0.96 1 0.74 2.35 4.2 Long Context Length Evaluation Perplexity Evaluation. We evaluated long context length performance using the LLaMA-2-7B- 32K model (uptrained for long sequence lengths using positional interpolation [ 5]) as well as the Llama-2-70B-32K LongLoRA model [6]. For evaluating performance on longer context lengths, we first evaluated perplexity on Wikitext-2 using larger amounts of input context, as shown in Figure 3 [6, 13]. The results demonstrate how our method maintains accuracy even for longer amounts of input context, thereby enabling efficient and accurate long sequence length inference. Passkey Retrieval Evaluation. We also evaluated the performance of our quantization method on passkey retrieval to assess the model’s ability to use its context. Passkey retrieval involves evaluating the model’s capacity to locate specific information in long texts [19], and this can be used to effectively measure the maximum distance over which a token can attend during the inference stage. We used the passkey evaluation framework from [45] (which is based on the methodology from [28]) to evaluate retrieval performance. The passkey retrieval results are provided in Table 2, demonstrating how our method is able to maintain the retrieval performance for long context length models. We also include comparisons with the passkey retrieval when using KIVI for the LLaMA-2-7B-32K model [26], demonstrating that our approach can attain higher retrieval rate for the same compression level. Our improved performance on retrieval tasks can be attributed to our improved representation of all tokens equally. This approach differs from KIVI, which preserves a local window of residual tokens in fp16. Therefore, while KIVI is effective at representing the tail part of the context, it may provide less benefit for tasks requiring the utilization of the full context window. LongBench Evaluation. Table 3 shows evaluation on LongBench [3] for the LLaMA-2-7B-32K model. LongBench contains a suite of long-context length evaluation benchmarks including QA tasks, summarization, and few-shot learning [ 3]. The max input context length is set at 31500, and results using KIVI are also included for reference [26]. Our results demonstrate that our 3-bit 8Table 3: LongBench evaluation for the LLaMA-2-7B-32K model using KVQuant-3bit-1%. Com- parisons with KIVI are included for reference, using the configuration with group size of 32 and 128-element fp16 residual [26]. Average bit widths are estimated for each approach assuming 12.2K context length, which was the average number of tokens across all tasks. Config Avg. bitNtrvQAQasperMF-enHotpot2WikiMusiqueGovRepQMSumMNewsTRECTriviQASamSumRBenchLCC PsgRetrPsgCntAvg. fp16 Baseline 16 17.96 10.51 33.43 12.55 12.53 6.19 29.65 16.99 22.15 71 87.79 43.97 59.99 62.14 23 1.50 31.96 KIVI-2-gs32-r128 3.17 19.25 10.66 24.78 12.48 11.19 6.38 27.05 16.36 23.37 71 80.80 43.93 57.74 60.61 13.58 1.50 30.04 KVQuant-3bit-1% 3.33 18.87 13.67 30.93 12.07 12.55 6.25 27.10 16.53 16.54 71 87.55 43.95 59.50 61.52 19.5 1.75 31.21 Table 4: RULER evaluation results for the LLaMA-2-7B-32K model with KVQuant quantization methods. We report accuracy across RULER tasks, comparing our KVQuant configurations to baseline and KIVI approaches. A maximum context length of 32K is used for evaluation. Our results show that our method retains baseline accuracy even with aggressive quantization and pruning. Config Avg. bit Niah1 Niah2 Niah3 MKey1MKey2MKey3MValueMQueryVT CWE FWE QA1 QA2 Avg. fp16 Baseline 16 100 99.8 98.6 94 68.2 11 55.95 64.5 37.88 9.64 30.4 31.6 31.6 56.40 KIVI-2-gs32-r128 3.05 76 85.6 59.6 72.6 11.4 0 34.7 46.45 39.6 8.26 30.53 24.8 27.6 39.78 KVQuant-3bit-1% 3.33 99.8 98.8 95.2 92.8 61.6 6.4 47.5 54.45 41.04 8.52 29.33 31.0 31.0 53.65 KVQuant-2bit-1% 2.33 95.4 86.8 49.8 73.6 23.4 0 16.65 22.95 22.52 5.14 24.0 26.4 28.4 36.54 model can attain minimal degradation relative to the fp16 baseline, outperforming KIVI for a similar compression level. RULER Evaluation. Finally in Table 4, we provide evaluation of KVQuant and KIVI on the RULER benchmark suite [15] using LLaMA-2-7B-32K. As can be seen in the table, 3-bit KVQuant achieves 14% better score against KIVI with a similar average bit-width. Furthermore, our 2-bit KVQuant achieves similar accuracy to KIVI with 1.5× smaller bit-width. 4.3 Joint Weight and KV Cache Quantization Table 5 provides results for our KV cache quantization method when the weights are also quantized using the methodology in SqueezeLLM [ 17]. We observe minimal perplexity degradation when leveraging our KV cache quantization approach, even when weights are also quantized to reduced precision. In particular, we observe small 0.02 and 0.1 perplexity degradation of 4-bit and 3-bit weight-only quantization, respectively, when quantizing the KV cache using nuq4-1% for the LLaMA- 7B and LLaMA-13B models. These results demonstrate how our method is compatible with existing weight-only quantization methods. 4.4 Performance Analysis and Memory Savings Table 6 shows kernel benchmarking results using a batch size of 1 for the 4-bit dense-and-sparse compression and matrix-vector kernel implementations. We show results across different sequence lengths to assess the performance of the kernels at different points during generation. We report latency benchmarked on an A6000 GPU. The results show that for the Key and Value multiplications, we can achieve 1.2-1.6× and 1.3-1.7× latency savings, respectively, relative to the baseline. We have integrated these kernels into an end-to-end generation pipeline that is able to compress activations dynamically during inference, thereby achieving significant memory savings and allowing for either larger batch sizes or longer sequence lengths. Appendix A highlights the benefits of KVQuant in supporting longer context lengths through reducing KV cache memory footprint. As shown in Table 8 in Appendix A, our nuq2 method provides 8× KV cache compression and enables serving the quantized LLaMA-7B model with a context length of 1M tokens on a single A100 GPU, as well as enabling serving the LLaMA-7B model with 10M context length on an 8-GPU system. Our results show little degradation compared to baseline fp16 inference 9Table 5: KV cache quantization results when KVQuant is applied in conjunction with the weight quantization methodology in SqueezeLLM [17]. w4-s45 and w3-s45 for weights refer to the 4-bit and 3-bit dense-and-sparse weight quantization approaches in [ 17], respectively. See Appendix M for experimental details. Weights KV Cache LLaMA-7B LLaMA-13B Avg. Bits (KV Cache) fp16 fp16 5.68 5.09 16 w4-s45 fp16 5.77 5.17 16 nuq4-1% 5.79 5.18 4.32-4.33 w3-s45 fp16 6.13 5.45 16 nuq3-1% 6.23 5.52 3.32-3.33 Table 6: Average latency (in microseconds) for the Key and Value nuq4-1% kernels, benchmarked on an A6000 GPU for the LLaMA-2-7B-32K model across different sequence lengths ( l). fp16 matrix-vector multiplication latencies are included for reference, and the fp16 Key multiplication time also includes the time to apply RoPE to the newly appended Key vector. Section 3.7 and Appendix R provide additional details for our kernel implementation, Appendix R describes our benchmarking methodology, and Table 22 provides a detailed breakdown of kernel runtime on an A6000 GPU. Activation Operation l=2K l=4K l=16K Key fp16 Matvec 33.3 59.1 219.4 Key nuq4-1% 25.6 39.9 126.3 Value fp16 Matvec 26.0 50.2 203.7 Value nuq4-1% 22.1 37.9 124.5 while providing significant compression, demonstrating the benefits of our approach for enabling accurate and efficient long sequence length inference. 5 Conclusion As context lengths in LLMs increase, the KV cache activations surface as the dominant contributor to memory consumption. Quantization is a promising approach to reduce the size of KV cache activations, but prior solutions failed to represent activations accurately in ultra-low precisions, such as sub-4-bit. In contrast, we achieve accurate ultra-low precision KV cache quantization. By quantizing Keys per-channel before applying RoPE, we are able to better match the outlier distribution and mitigate the impacts of RoPE on quantization (due to it mixing pairs of channels which may have different average magnitudes). We use non-uniform quantization to better allocate the small number of quantization signposts at low precision. We observe significant accuracy improvements when employing dense-and-sparse quantization, particularly when detecting outliers at the same granularity as we compute quantization scaling factors. Crucially, we demonstrate that we can perform accurate calibration offline for Keys, as well as efficient online scaling factor and outlier threshold computation for Values. By leveraging these methods, we are able to enable accurate low-precision activation quantization, achieving 4.8x compression (nuq3-1% outliers) with only 0.1 perplexity degradation across different LLaMA, Llama-2, Llama-3, and Mistral models. Our methodology therefore supports inferring the LLaMA-7B model with a context length of 10M on an 8-GPU serving system. Through our efficient kernel implementation, we are able to show improved latency relative to the fp16 baseline, demonstrating how our method allows for improved latency in addition to the memory savings. Acknowledgements The authors would like to acknowledge Nicholas Lee for helpful discussions and feedback. We acknowledge gracious support from Intel, Furiosa, Apple, Samsung SAIT, POSCO HOLDINGS, and NVIDIA. We also appreciate the support from Microsoft through their Accelerating Foundation Model Research, including great support from Sean Kuno. Furthermore, we appreciate support from Google Cloud, the Google TRC team, and specifically Jonathan Caton, and Prof. David Patterson. Prof. Keutzer’s lab is sponsored by the Intel corporation, Intel One-API, Intel VLAB team, the Intel 10One-API center of excellence, as well as funding through BDD and BAIR. We appreciate great feedback and support from Ellick Chan, Saurabh Tangri, Andres Rodriguez, and Kittur Ganesh. Sehoon Kim would like to acknowledge the support from the Korea Foundation for Advanced Studies (KFAS). Michael W. Mahoney would also like to acknowledge a J. P. Morgan Chase Faculty Research Award as well as the DOE, NSF, and ONR. Our conclusions do not necessarily reflect the position or the policy of our sponsors, and no official endorsement should be inferred. References [1] AI@Meta. Llama 3 model card. 2024. [2] Anthropic. Introducing claude 2.1, Nov 2023. [3] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. [4] Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding and overcoming the challenges of efficient transformer quantization. arXiv preprint arXiv:2109.12948, 2021. [5] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023. [6] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. arXiv preprint arXiv:2309.12307, 2023. [7] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. [8] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023. [9] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: A sparse-quantized representation for near-lossless llm weight compression. arXiv preprint arXiv:2306.03078, 2023. [10] Goran Flegar and Enrique S Quintana-Ortí. Balanced csr sparse matrix-vector product on graphics processors. In Euro-Par 2017: Parallel Processing: 23rd International Conference on Parallel and Distributed Computing, Santiago de Compostela, Spain, August 28–September 1, 2017, Proceedings 23, pages 697–709. Springer, 2017. [11] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [12] Amir Gholami, Zhewei Yao, Sehoon Kim, Michael Mahoney, and Kurt Keutzer. Ai and memory wall. RiseLab Medium Post, 2021. [13] Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Simple on-the-fly length generalization for large language models. arXiv preprint arXiv:2308.16137, 2023. [14] Jung Hwan Heo, Jeonghoon Kim, Beomseok Kwon, Byeongwook Kim, Se Jung Kwon, and Dongsoo Lee. Rethinking channel dimensions to isolate outliers for low-bit weight quantization of large language models. arXiv preprint arXiv:2309.15531, 2023. [15] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. Ruler: What’s the real context size of your long-context language models? arXiv preprint arXiv:2404.06654, 2024. [16] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [17] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023. 11[18] Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W Mahoney, et al. Full stack optimization of transformer inference: a survey. arXiv preprint arXiv:2302.14017, 2023. [19] Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can context length of open-source llms truly promise? In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023. [20] Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. Snapkv: Llm knows what you are looking for before generation, 2024. [21] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023. [22] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with blockwise ringattention, 2024. [23] Ruikang Liu, Haoli Bai, Haokun Lin, Yuening Li, Han Gao, Zhengzhuo Xu, Lu Hou, Jun Yao, and Chun Yuan. Intactkv: Improving large language model quantization by keeping pivot tokens intact, 2024. [24] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free quantiza- tion aware training for large language models. arXiv preprint arXiv:2305.17888, 2023. [25] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. arXiv preprint arXiv:2305.17118, 2023. [26] Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: Plug-and-play 2bit kv cache quantization with streaming asymmetric quantization. 2023. [27] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016. [28] Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context length for transformers. arXiv preprint arXiv:2305.16300, 2023. [29] Roman Novak, Yasaman Bahri, Daniel A Abolafia, Jeffrey Pennington, and Jascha Sohl- Dickstein. Sensitivity and generalization in neural networks: an empirical study. arXiv preprint arXiv:1802.08760, 2018. [30] OpenAI. New models and developer products announced at devday 2023, Nov 2023. [31] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv e-prints, 2019. [32] Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo Luschi, and Douglas Orr. Sparq attention: Bandwidth-efficient llm inference. arXiv preprint arXiv:2312.04985, 2023. [33] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally calibrated quantiza- tion for large language models. arXiv preprint arXiv:2308.13137, 2023. [34] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. Flexgen: High-throughput generative inference of large language models with a single gpu. In International Conference on Machine Learning, pages 31094–31116. PMLR, 2023. [35] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [36] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 12[37] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [38] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, and Xianglong Liu. Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling. arXiv preprint arXiv:2304.09145, 2023. [39] Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier suppression: Pushing the limit of low-bit transformer language models. Advances in Neural Information Processing Systems, 35:17402–17414, 2022. [40] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the- art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38–45, Online, October 2020. Association for Computational Linguistics. [41] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pages 38087–38099. PMLR, 2023. [42] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks, 2023. [43] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H _2 o: Heavy-hitter oracle for efficient generative inference of large language models. arXiv preprint arXiv:2306.14048, 2023. [44] Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom: Low-bit quantization for efficient and accurate llm serving. arXiv preprint arXiv:2310.19102, 2023. [45] Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. Pose: Efficient context window extension of llms via positional skip-wise training. arXiv preprint arXiv:2309.10400, 2023. 13A Memory Bottlenecks for Long Context Length Inference Table 7 shows the model size and KV cache memory requirements for different LLaMA models with different sequence lengths. For short sequence lengths, the model weights are the primary memory bottleneck. However, for longer sequence lengths and larger batch sizes, the KV cache memory is the main bottleneck. This is particularly pronounced when the weights are already quantized to low precision. In our work, we demonstrate that we can help address the KV cache memory bottleneck through low-precision KV cache quantization. By compressing the KV cache to 2-bit precision, we can enable 1M context length inference with the LLaMA-7B model on a single A100-80GB GPU, and we can also enable 10M context length inference with the LLaMA-7B model on an 8-GPU system. Table 8 shows the KV cache memory requirements for 128K, 1M, and 10M sequence lengths, with the KV cache stored in fp16 as well as 4-bit, 3-bit, and 2-bit precision with KVQuant. As one can see, our method provides 3.7× KV cache compression (nuq4-1%) and enables serving the quantized LLaMA-65B model with a context length of 32K tokens on a single A100-80GB GPU (requiring 30.3GB for the model weights compressed to 4-bit, and 46.5GB for the KV cache when compressed with nuq2-1%), and our nuq2 method enables serving the LLaMA-7B model with a context length of 1M tokens on a single A100 GPU (requiring 64GB for the KV cache). Additionally, when considering an 8-GPU serving system, we enable serving the LLaMA-7B model with 10M context length (with nuq2), or the LLaMA-65B model with 1M context length (with nuq3). Our results show little degradation compared to baseline fp16 inference while providing significant compression, demonstrating the benefits of our approach for enabling accurate and efficient long sequence length inference. Table 7: Model size and activation memory size estimates for different sequence lengths and batch sizes (BS) for different LLaMA models. For long sequence lengths and larger batch sizes, activation memory is the main bottleneck (particularly when weights are already quantized to low precision). By compressing the KV cache to 2-bit precision, we can enable1M context length inference with the LLaMA-7B model on a single A100-80GB GPU, and we can also enable 10M context length inference with the LLaMA-7B model on an 8-GPU system. BS Model Model Size (GB) KV Cache Size w/ Diff. Seq Len (GB) 16 → 2-bit 32K 128K 1M 10M (16 → 2-bit) 1 7B 12.6 → 1.6 16 64 512 4883 → 610 13B 24.1 → 3.0 25 100 800 7629 → 954 30B 60.3 → 7.5 49 195 1560 14877 → 1860 65B 121.1 → 15.1 80 320 2560 24414 → 3052 4 7B 12.6 → 1.6 64 256 2048 19531 → 2441 13B 24.1 → 3.0 100 400 3200 30518 → 3815 30B 60.3 → 7.5 195 780 6240 59509 → 7439 65B 121.1 → 15.1 320 1280 10240 97656 → 12207 B Additional Related Works B.1 Outlier-Aware LLM Quantization LLMs have been known to have distinct outliers both in weights and activations [ 7, 9, 17]. SqueezeLLM and SpQR both decompose the weight matrix into a sparse matrix containing a small portion of outliers and a dense matrix that can be accurately quantized to low precision (referred to as dense-and-sparse or sparse-quantized representation) [9, 17]. LLM.int8() [7] handled particular outlier channels separately in higher precision, and SmoothQuant [41] migrates quantization difficulty due to outlier channels to weights in order to support joint weight-activation quantization. Other works reconsidered the dimension along which we quantize in order to reduce quantization error (or else added per-channel compensation to improve quantization performance) [4, 14, 39, 38]. In this work, we demonstrate that per-channel pre-RoPE Key quantization provides significant accuracy benefits given the outlier structure in Keys, and that dense-and-sparse quantization can be efficiently applied for KV cache quantization. 14Table 8: Activation memory size estimates (GB) for 128K, 1M, and 10M sequence length ( l) for different LLaMA models. By compressing the KV cache to 2-bit precision, we can enable 1M context length inference with the LLaMA-7B model on a single A100-80GB GPU, and we can also enable 10M context length inference with the LLaMA-7B model on an 8-GPU system. Model Method l=128K l=1M l=10M LLaMA-7B fp16 64.0 512.0 4882.8 nuq4 16.0 128.1 1221.9 nuq4-1% 17.3 138.4 1319.6 nuq3 12.0 96.1 916.7 nuq3-1% 13.3 106.4 1014.4 nuq2 8.0 64.1 611.5 nuq2-1% 9.3 74.4 709.2 LLaMA-65B fp16 320.0 2560.0 24414 nuq4 80.0 640.3 6106.5 nuq4-1% 86.5 691.5 6595.0 nuq3 60.0 480.3 4580.6 nuq3-1% 66.5 531.5 5069.1 nuq2 40.0 320.3 3054.7 nuq2-1% 46.5 371.5 3543.3 B.2 Non-uniform LLM Quantization Non-uniform quantization has also been applied in the context of LLMs. Non-uniform quantization allows for more flexible quantization signpost placement relative to uniform quantization methods, enabling improved accuracy for the same bit precision [17, 8]. Building on the observation that model parameters tend to be approximately normally-distributed, prior work has proposed the NormalFloat datatype [8]. SqueezeLLM [ 17] derived per-channel non-uniform quantization signposts using a sensitivity-weighted k-means approach. In this work, we show that we can derive accurate per-layer non-uniform datatypes using a sensitivity-weighted k-means approach with KV cache activations. C RoPE Equation The rotation matrix for RoPE is provided in Equation 2, where c and s are cosine and sine functions, θi = 10000−2(i−1)/d, d is the attention head dimension, and n is the current position in the sequence:   c(nθ1) −s(nθ1) ··· 0 0 s(nθ1) c( nθ1) ··· 0 0 ... ... ... ... ... 0 0 ··· c(nθd/2) −s(nθd/2) 0 0 ··· s(nθd/2) c( nθd/2)   (2) The Query vectors computed at each iteration will have RoPE applied (to obtain ˜Qm = Rd θ,m ∗ Qm). When caching Key vectors, we therefore need to either cache ˜Kn = Rd θ,n ∗ Kn, or else we need to cache Kn and apply Rd θ,n on-the-fly during inference. In order to apply Rd θ,n efficiently on-the-fly, we leverage the element-wise formulation of RoPE rather than the matrix-multiplication formulation from Equation 2. The element-wise formulation for Rd θ,nx is as follows, where ⊙ is the element- wise multiplication operator (note that the formulation that we use matches the implementation in the Transformers library for LLaMA [ 40], and it is a different but equivalent formulation to the element-wise expression in [35]): 15  x1 x2 ... xd 2 xd 2 +1 ... xd−1 xd   ⊙   c(θ1n) c(θ2n) ... c(θd 2 n) c(θ1n) ... c(θd 2 −1n) c(θd 2 n)   +   −xd 2 +1 −xd 2 +2 ... −xd x1 ... xd 2 −1 xd 2   ⊙   s(θ1n) s(θ2n) ... s(θd 2 n) s(θ1n) ... s(θd 2 −1n) s(θd 2 n)   (3) By leveraging this element-wise implementation, we can apply RoPE on-the-fly to the Key activations (after dequantizing the Key activations and before multiplying them with the corresponding elements in the Query vector). D Derivation for Sensitivity Analysis To compute the sensitivity analysis we largely follow the derivation in [29], which was originally provided to compute sensitivity of a NN based classifier but can be extended with minor modifications for quantization. In particular, the sensitivity measure is based on how much the loss output of the model is perturbed. To compute this we denote activations at a layer before quantization as A, after quantization as AQ, quantization perturbation in activation as A − AQ, and the gradient of the Loss function w.r.t. activation as J(A) = ∂L ∂A (A). By making the assumption that the quantization perturbation of different activations follow a Gaussian distribution with zero mean, we can show that the sensitivity of an activation value is proportional to Fii \u0000 A − Q(A) \u00012 as was given in Equation 1: E∆A h |L(A) − L(A + ∆A)|2 i ≈ E∆A h\u0000 J(A)T ∆A \u00012i = E∆A \"\u0010X i Ji∆Ai \u00112 # = E∆A \"X i J2 i ∆A2 i # = X i J2 i E∆A \u0002 ∆A2 i \u0003 . Here note that we first assume a first order Taylor series expansion to approximate the perturbation to the loss, and then use the zero mean assumption of the quantization perturbation to derive the second line. To approximate the E∆A \u0002 ∆A2 i \u0003 we use empirical evaluation of the expectation by sampling multiple different inputs and empirically computing the resulting quantization perturbation which results in Equation 1. E Derivation for Quantization Error In our work, before applying the sensitivity-weighted K-means to derive quantization signposts, we normalize each element Ai in the flattened activation A. This normalization for Ai involves a shift by a zero-point zi followed by rescaling the quantization signposts by a scaling factor si, where si and zi are the scaling factor and zeropoint corresponding to element Ai: Ai,norm = Ai − zi si , (4) where Ai and Ai,norm are element i from activation A before and after normalization, respectively. We then quantize Ai,norm to Q(Ai,norm) with quantization error ∆Ai,norm. After we dequantized, we rescale each element by si and add zi to get the recovered quantized activation value Q(Ai): 16Q(Ai) = si Q(Ai,norm) + zi. (5) As such, if there is quantization error ∆Ai,norm in Ai,norm, this will be scaled by si in terms of the error in Ai, i.e., ∆Ai = si∆Ai,norm. For activation A which is normalized to Anorm (with corresponding scaling factors si for each element Ai), minimizing the sensitivity-weighted quantization error as expressed in Equation 1 gives us the following expression, which we can minimize using the normalized activations across all N elements from the samples in a calibration set: Q(A)∗ ≃ arg min Q NX i=1 Fii \u0000 Ai − Q(Ai) \u00012 (6) = arg min Q NX i=1 Fii \u0010 s2 i \u0000 Ai,norm − Q(Ai,norm) \u00012\u0011 (7) F Key and Value Dynamic Range Figure 4 shows the portion of the elements contained within difference percentages of the dynamic range for both Keys and Values. The majority of values (∼ 99%) are contained in a small portion of the dynamic range, and a small portion of numerical outliers skew the dynamic range that must be represented. This motivates our dense-and-sparse approach which removes numerical outliers and stores them in a separate sparse matrix, thereby restricting the range that needs to be represented in the dense component. 0 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Normalized Magnitude Keys 0 10 20 30 Layer 0.0 0.2 0.4 0.6 0.8 1.0Normalized Magnitude Values t100 t99.99 t99.9 t99 Figure 4: Distribution of the magnitude of elements of Key (Pre-RoPE) and Value activations for different layers of LLaMA-7B, computed on a single sample with sequence length 2K from the Wikitext-2 dataset. The normalized magnitude is computed by dividing by the largest magnitude value in that layer. As one can see, for both Key and Value activations, the majority of values lie in a small portion of the dynamic range, with a few numerical outliers skewing the dynamic range (and thereby reducing the fidelity when quantizing to low precision). G Per-Channel Key Quantization Ablations As shown in Table 9, per-channel quantization for Keys and per-token quantization for Values outperforms the standard per-token quantization approach for both Keys and Values, yielding an improvement of 3.82 perplexity for the LLaMA-7B model at 3-bit precision. This demonstrates the 17benefits of per-channel Key quantization to mitigate the large outlier channels in Keys. Note that for all experiments using per-channel quantization, we use an fp16 zeropoint rather than a low-precision zeropoint that is rounded to the nearest integer value. We do this since for some of the Key channels, all of the elements are positive or all of the elements are negative, meaning that the zeropoint will fall outside of the range that is representable by a low-precision integer (and rounding it to the nearest low-precision value can degrade performance). Additionally, we observe that per-channel quantization for Values actually performs worse than per-token quantization. We hypothesize that this behavior is because per-channel Value quantization leads to greater error accumulation in particular output values (since the result of the attention scores multiplied by one channel of the Values will be localized to a single value in the output vector), which leads to greater quantization error at later model layers. Another concurrent work, KIVI [26], observes similar behavior for per-channel Value quantization, which they attribute to the fact that per-token Value quantization confines the error to each token. Assuming that the output is a weighted sum of only a few important tokens (as only a few attention scores are large), a perturbation in these tokens can lead to significant degradation. Per-token Value quantization therefore ensures that the quantization of unimportant tokens does not adversely impact the important tokens. Table 9: Ablation Study: Perplexity comparison of per-token and per-channel quantization for KV cache activations for LLaMA-7B. PT refers to per-token quantization, and PC refers to per-channel quantization. Datatype Key Dim. Value Dim. Perplexity KV Cache Size (GB) Seqlen 128K fp16 - - 5.68 64.0 int3 PT PT 10.87 12.0 int3 PC PC 223 12.0 int3 PC PT 7.05 12.0 H Pre-RoPE Key Quantization Ablations As shown in Table 10, pre-RoPE Key quantization achieves higher accuracy than post-RoPE quanti- zation, with an improvement of 0.82 perplexity for 3-bit quantization with the LLaMA-7B model. These results show that the rotary positional embeddings make Key quantization more challenging due to mixing pairs of channels with different magnitudes. Pre-RoPE quantization thereby allows for more accurate quantization at low precision. Table 10: Ablation Study: Perplexity comparison of Pre-RoPE and post-RoPE Key quantization for LLaMA-7B (using per-channel Key quantization and per-token Value quantization). Pre-RoPE quantization leads to significant improvement (see Section 3.2 for more details). Datatype Scheme Perplexity KV Cache Size (GB) Seqlen 128K fp16 - 5.68 64.0 int3 post-RoPE 7.05 12.0 int3 pre-RoPE 6.23 12.0 I Sensitivity-Weighted Non-Uniform Quantization Ablations Table 11 shows perplexity evaluation results across different LLaMA, Llama-2, and Mistral models on Wikitext-2 for different datatypes, including nuq3, nuq3 without using sensitivity-weighting, as well as nuq3 with sensitivity-weighting but without accounting for the per-channel scaling factors when performing k-means. We observe particularly noticeable gains relative to uniform quantization for 3- bit and 2-bit quantization, where the benefits of non-uniform quantization are more pronounced due to the reduced precision. These results demonstrate the benefits of our sensitivity-weighted non-uniform quantization approach relative to NormalFloat quantization [8], as we achieve consistent accuracy improvements of up to 0.33 perplexity across different models. These results also demonstrate the 18necessity of our sensitivity-weighting approach in order to derive performant non-uniform datatypes using a k-means based approach. Additionally, we observe distinct benefits when also accounting for the per-channel scaling factors when performing k-means. Table 11: Ablation Study: Ablation of our sensitivity-weighted non-uniform datatype for different models on Wikitext-2. All experiments use pre-RoPE per-channel quantization for Keys and per-token quantization for Values (meaning that all configurations are the same as in KVQuant, except for the datatype). We compare against both uniform (int3) and non-uniform (nf3) [8] approaches, as well as with using “unweighted” k-means (i.e., not sensitivity-weighted) and “Fisher-weighted k- means” (without accounting for per-channel scaling factors) to compute the non-uniform quantization signposts. Note that there is slight variation in average bitwidth across models due to the differing hidden dimensions. Results report perplexity with 2K/4K/8K sequence length for LLaMA, Llama-2, and Mistral, respectively. Method LLaMA Llama-2 Mistral-7B Avg. Num. Bits7B 13B 30B 65B 7B 13B 70B baseline 5.68 5.09 4.10 3.53 5.12 4.57 3.12 4.76 16 int3 6.23 5.60 5.09 5.18 5.95 5.98 3.26 5.26 3.00-3.02 nf3 6.05 5.42 4.51 3.84 5.55 5.15 3.27 5.13 3.00-3.02 nuq3 (unweighted) 6.84 6.16 5.37 4.57 8.52 7.66 3.67 5.29 3.00-3.02 nuq3 (Fisher-weighted) 6.01 5.34 4.41 3.74 5.49 4.83 3.26 5.03 3.00-3.02 nuq3 (KVQuant) 5.94 5.32 4.34 3.68 5.39 4.82 3.23 4.98 3.00-3.02 J Per-Vector Dense-and-Sparse Quantization Ablations Table 12 shows the performance improvements we observe when isolating a small portion of outliers and storing them in a sparse format. We provide results both with using a single per-matrix outlier threshold, as well as with applying separate outlier thresholds per-vector. In particular, we see greater improvements by employing outlier detection with a different threshold per-channel for Keys and per-token for Values. This provides additional benefits since some values which would be considered outliers for the entire matrix are not actually outliers within a particular channel (so they are not hard to quantize). It is therefore better to directly target the outliers that will skew the quantization range for a particular channel. By removing 1% of outliers using per-vector thresholds, we can achieve an additional 0.19 reduction in perplexity for the LLaMA-7B model at 3 bits, thereby enabling 3-bit quantization with under 0.1 degradation in perplexity. Table 12: Ablation Study: Perplexity comparison of different outlier isolation methods for LLaMA- 7B on Wikitext-2. Per-vector outlier detection allows for significant accuracy improvements relative to per-tensor outlier detection. All experiments use per-token quantization for Values and per-channel quantization for Keys (pre-RoPE). “PV” refers to using per-vector outlier thresholds, and “PM” refers to using a single per-matrix outlier threshold. Datatype % Outliers Outlier Dim. Perplexity KV Cache Size (GB) Seqlen 128K fp16 - - 5.68 64.0 nuq3 - - 5.94 12.0 nuq3 0.1% PM 5.89 12.2 nuq3 0.1% PV 5.82 12.2 nuq3 1% PM 5.85 13.3 nuq3 1% PV 5.75 13.3 K Attention Sink-Aware Quantization Ablations Table 13 provides perplexity with and without Attention Sink-Aware quantization across different LLaMA, Llama-2, and Llama-3 models. For all bit widths (4, 3, and 2-bit) and in both dense- only and dense-and-sparse quantization settings, Attention Sink-Aware quantization consistently yields perplexity improvement. Notably, the perplexity gain is more pronounced at lower bit widths 19and without sparsity. We observe particularly significant improvements for Llama-3 models, with perplexity improvements of 0.25 and 0.13 PPL for nuq2-1% on Llama-3-8B and Llama-3-70B, respectively. Table 13: Ablation Study: Perplexity with and without Attention Sink-Aware quantization on Wikitext-2 with various models. Attention Sink-Aware quantization consistently improves perplexity across different models and bit widths, particularly with lower bit widths and without sparsity. Datatype Attention Sink-Aware LLaMA-7B LLaMA-13B Llama-2-7B Llama-2-13B Llama-3-8B Llama-3-70B fp16 - 5.68 5.09 5.12 4.57 5.54 2.59 nuq4 X 5.72 5.14 5.17 4.62 5.64 2.65 nuq4 O 5.72 5.13 5.16 4.60 5.60 2.62 nuq4-1% X 5.69 5.10 5.13 4.59 5.57 2.60 nuq4-1% O 5.69 5.10 5.13 4.58 5.56 2.60 nuq3 X 5.94 5.32 5.39 4.82 6.10 2.99 nuq3 O 5.87 5.25 5.34 4.71 5.84 2.72 nuq3-1% X 5.75 5.15 5.18 4.62 5.67 2.66 nuq3-1% O 5.75 5.14 5.17 4.61 5.64 2.63 nuq2 X 8.47 7.29 11.20 23.34 16.63 6.79 nuq2 O 7.23 5.82 7.03 9.59 7.04 3.49 nuq2-1% X 6.05 5.39 5.47 4.85 6.29 2.95 nuq2-1% O 6.01 5.36 5.41 4.78 6.04 2.82 L Calibration Ablations Figure 5 outlines the accuracy and efficiency challenges which our work addresses in order to enable accurate and efficient KV cache quantization. We demonstrate that we can circumvent the challenges related to efficiently performing online per-channel scaling factor computation by instead calibrating offline for the scaling factor without hurting accuracy. Additionally, we show that we can perform per-token scaling factor and outlier threshold computation efficiently online during inference, thereby enabling accurate per-token quantization without compromising on efficiency. Per-channel Quantization  dim sequence length KeysNew Key Per-token Quantization  dim sequence length ValuesNew ValueChallenge: need to potentially recompute the scaling factor every time a new key is added if we computeitonline Challenge: Need to compute the scaling factor for each incoming token per-channel scaling factor per-token scaling factor Figure 5: One typically achieves better performance when the scaling factor/zero point are computed online. However, this is quite challenging to do for per-channel quantization, as these factors will not only need to be recomputed for every new Key appended to the Key cache, but also all the prior cached Keys will need to be updated. As such, we use a calibration set to compute per-channel scaling factors offline. A similar challenge exists for per-token quantization, but online calibration for this does not require updating prior cached Values. In Section 3.6 and Appendix L, we discuss how we are able to efficiently compute outlier thresholds / scaling factors for per-token calibration, thereby enabling online computation. Table 14 shows accuracy results when using offline calibration for computing the scaling factors for the Keys. For 3-bit quantization, we observe minor accuracy degradation when not employing outlier extraction methods. However, if we remove a small percentage of outliers, then the accuracy with offline calibration is the same as computing the scaling factors online per-channel during evaluation. This demonstrates that when incorporating outlier extraction methods, we are better able to perform offline calibration due to reduced sensitivity to outliers (either to outliers during calibration that exaggerate the quantization range, or to outliers during evaluation that cannot be represented accurately if there weren’t large outliers observed during calibration). 20Table 15 shows the runtime for the topk operation for the LLaMA-7B model (which is required for computing outlier thresholds online). It compares the runtime of the topk operation with the runtime for the QKV projections, finding that the topk runtime is 45% of the matrix-vector operation runtime for a single projection layer. The topk operation can also be performed efficiently on the CPU, so we can actually run this operation in parallel with the subsequent linear layer matrix-vector operations on the GPU (which is possible by computing the Value projection before the Key and Query projections). This allows us to compress the activations dynamically without added runtime overhead, thereby enabling online scaling factor computation for the Value tensors. Table 16 shows how both Fisher information computation and calibration (including k-means) per- layer take only a few minutes for the LLaMA-65B model on a typical server machine. Even if we perform calibration sequentially for each layer, the entire calibration process would take a maximum of 6 hours for the LLaMA-65B model at 4-bit precision. Table 14: Ablation Study: Model accuracy when using offline calibration for Keys with LLaMA-7B. When incorporating outlier detection, offline calibration for Keys is able to perform comparably with online calibration. All nuq3 experiments use per-token quantization for Values and per-channel quantization for Keys (pre-RoPE), and experiments with outliers use per-vector outlier detection. Datatype % Outliers Perplexity Perplexity (Online for K) (Offline for K) fp16 - 5.68 5.68 nuq3 - 5.91 5.94 nuq3 1% 5.75 5.75 Table 15: topk runtime on a vector of length 4096 for computing outlier thresholds when using 1% sparsity (compared with the runtime for the QKV matrix multiplications, which are 4096×4096 by 4096 matrix-vector multiplications for the LLaMA-7B model). The runtime is reported on a system with an A6000 GPU and an Intel Xeon Gold 6126 CPU. We find that the runtime for the topk operation is only 45% of the runtime of each matvec operation. Additionally, the topk operation can be performed efficiently on the CPU; we can therefore run this operation in parallel with subsequent linear layer operations on the GPU to compress the activations dynamically without added overhead. Operation Device Outlier % Runtime (ms) QKV Projection GPU - 0.172 topk CPU 1% 0.026 topk GPU 1% 0.088 QKV Projection / topk (Fused) GPU / CPU 1% 0.173 Table 16: Runtime for computing Fisher information as well as for calibration (including k-means) with 16 samples for LLaMA-65B quantization. Runtime for computing Fisher information was computed on an 8-GPU A100-80GB system. Runtime for calibration (including k-means) was performed on an Intel Xeon Gold 6442Y CPU, and is shown for a single layer. Note that calibration is independent for each layer, so it can be easily parallelized. Operation Runtime (minutes) Computing Fisher Information 2.8 4-bit Calibration Per-Layer (including k-means) 4.5 3-bit Calibration Per-Layer (including k-means) 2.7 2-bit Calibration Per-Layer (including k-means) 1.9 M Additional Experimental Details For our empirical evaluation, we use 16 calibration samples of sequence length 2K from the Wikitext- 2 training set (as well as the corresponding gradients) to derive the per-channel scaling factors and zero-points, and to derive the non-uniform datatypes for both Keys and Values. While we use KVQuant models that are calibrated on the Wikitext-2 dataset for all experiments, in Appendix L, we 21include an additional experiment that demonstrates the robustness of the calibration process to the choice of data. We measured perplexity on both Wikitext-2 and on C4 using a sequence length equal to the maximum context length of the model (2K for LLaMA, 4K for Llama-2, and 8K for Llama-3 and Mistral- 7B). For generative tasks, when processing the input prompt, the Key/Value matrix multiplications are computed using the fp16 Keys and Values, and then they are separately compressed into low precision. For baseline experiments, we use post-RoPE quantization, both since this is required from an efficiency perspective without a dedicated kernel implementation, and because it provides better accuracy when quantizing Keys per-token as shown in Appendix P. We make several assumptions in order to estimate average bit widths and KV cache sizes for different approaches. We compute these estimates assuming a sequence length of 128K (unless otherwise specified). For integer quantization, we assume a low-precision integer offset and a 16-bit scaling factor, whereas for NormalFloat and NUQ we assume that the zero-point and offset are each 16-bit. For the sparse matrices, 32-bit integers are assumed for the per-token indices (since we need to support long sequence lengths), and the elements and per-element indices are assumed to be 16-bit. This means that for CSR, the rows are assumed to be 32-bit and the columns and values are assumed to be 16-bit, whereas for CSC, the columns are assumed to be 32-bit and the rows and values are assumed to be 16-bit. N Comparison Between Different Datatypes and Sparsity Levels Table 17 shows perplexity across LLaMA, Llama-2, Llama-3, and Mistral models on Wikitext-2 using different datatypes. The results highlight that the NUQ datatype generally outperforms both uniformly quantized INT datatype and non-uniformly quantized NF datatype even after they are incorporated with grouping at the expense of increased bit-widths. This performance can be further improved by extracting 0.1% to 1.0% of outliers. Note that NUQ with a sparsity level 1.0% has a similar memory requirement as INT with a group size of 64, but achieves significant perplexity improvement across all models and bit widths. O Full Perplexity Evaluation Tables 18 and 19 show perplexity evaluation across all LLaMA, Llama-2, Llama-3, and Mistral models on Wikitext-2 and C4, respectively. These results demonstrate the benefits of our approach for KV cache compression across different models and bit widths. 22Table 17: Comparison of our NUQ datatype with and without sparsity against other data types on different models using the perplexity (PPL) measured on Wikitext-2. Non-uniform quantization (“nuq”) results are using pre-RoPE per-channel quantization for Keys. “gs64/128” refers to baseline experiments using grouping with group size 64/128. Note that there is a slight variation in average bitwidth across models due to the differing hidden dimensions. Method LLaMA Llama-2 Llama-3 Mistral-7B Avg. Num. Bits7B 13B 30B 65B 7B 13B 70B 8B 70B baseline 5.68 5.09 4.10 3.53 5.12 4.57 3.12 5.54 2.59 4.76 16 int4 5.98 5.32 4.34 3.73 5.66 5.01 3.31 7.89 14.03 4.97 4.00-4.02 int4-gs128 5.77 5.16 4.16 3.57 5.32 4.71 3.16 5.78 2.73 4.82 4.16 int4-gs64 5.73 5.14 4.14 3.56 5.25 4.66 3.14 5.68 2.66 4.80 4.31 nf4 5.87 5.23 4.25 3.63 5.47 4.90 3.22 6.42 3.86 4.91 4.00-4.03 nf4-gs128 5.77 5.17 4.17 3.58 5.30 4.71 3.16 5.84 2.75 4.83 4.25 nuq4 5.72 5.14 4.14 3.56 5.17 4.62 3.14 5.64 2.65 4.81 4.00-4.02 + 0.1% outliers 5.70 5.12 4.12 3.55 5.15 4.60 3.13 5.63 2.66 4.79 4.04-4.06 + 0.5% outliers 5.70 5.11 4.12 3.54 5.14 4.59 3.13 5.59 2.61 4.78 4.16-4.19 + 1.0% outliers 5.69 5.10 4.11 3.54 5.13 4.59 3.13 5.57 2.60 4.78 4.32-4.35 int3 10.87 8.69 6.82 6.37 22.71 18.26 7.68 125 158 7.64 3.00-3.02 int3-gs128 6.17 5.47 4.44 3.78 6.15 5.34 3.33 7.50 4.11 5.16 3.15 int3-gs64 5.93 5.29 4.26 3.66 5.64 4.98 3.23 6.38 3.23 5.00 3.30 nf3 7.33 6.21 5.46 4.44 9.96 9.50 4.06 61.07 98.64 6.30 3.00-3.03 nf3-gs128 6.26 5.52 4.54 3.83 6.21 5.43 3.38 7.79 4.74 5.23 3.25 nuq3 5.94 5.32 4.34 3.68 5.39 4.82 3.23 6.10 2.99 4.98 3.00-3.02 + 0.1% outliers 5.82 5.22 4.21 3.62 5.27 4.69 3.19 5.96 2.96 4.91 3.04-3.06 + 0.5% outliers 5.76 5.16 4.16 3.59 5.20 4.65 3.16 5.74 2.69 4.84 3.16-3.19 + 1.0% outliers 5.75 5.15 4.15 3.57 5.18 4.62 3.15 5.67 2.66 4.82 3.32-3.35 int2 11779 69965 1470 7272 4708 3943 976 2841 2164 573 2.00-2.02 int2-gs128 37.37 41.77 16.49 13.63 118 93.09 18.31 200 2092 51.96 2.14 int2-gs64 11.09 9.84 6.60 5.54 25.69 26.83 5.93 57.82 43.69 12.47 2.28 nf2 3210 5785 2044 5367 13601 4036 3680 30492 9486 903 2.00-2.03 nf2-gs128 351 141 60.97 31.69 635 642 71.21 1024 3091 253 2.25 nuq2 8.47 7.29 6.08 9.19 11.20 23.34 4.18 16.63 6.79 6.87 2.00-2.02 + 0.1% outliers 6.82 5.72 4.83 4.00 6.38 5.33 3.54 7.97 4.77 5.83 2.04-2.06 + 0.5% outliers 6.24 5.49 4.45 3.80 5.59 4.95 3.33 6.53 3.18 5.28 2.16-2.19 + 1.0% outliers 6.05 5.39 4.41 3.72 5.47 4.85 3.28 6.29 2.95 5.14 2.32-2.35 Table 18: Full evaluation of our method for different models on all LLaMA, Llama-2, Llama-3, and Mistral models using the perplexity on Wikitext-2. KVQuant results are using pre-RoPE per-channel quantization for Keys. Average bit widths assume a sequence length of 128K (ignoring context length limits for the models). Note that ATOM and FlexGen use 4-bit quantization with group sizes of 128 and 64 with uniform quantization, respectively, and we extend their methods to 3-bit and 2-bit quantization. We leverage Attention-Sink Aware quantization for all bit widths. We used post-RoPE quantization for all baseline methods since it achieves higher accuracy when quantizing Keys per-token as shown in Appendix P. Method LLaMA Llama-2 Llama-3 Mistral-7B Avg. Num. Bits7B 13B 30B 65B 7B 13B 70B 8B 70B baseline 5.68 5.09 4.10 3.53 5.12 4.57 3.12 5.54 2.59 4.76 16 int4 5.98 5.32 4.34 3.73 5.66 5.01 3.31 7.89 14.03 4.97 4.00-4.02 nf4 5.87 5.23 4.25 3.63 5.47 4.90 3.22 6.42 3.86 4.91 4.00-4.03 ATOM-4bit 5.77 5.16 4.16 3.57 5.32 4.71 3.16 5.78 2.73 4.82 4.16 Flexgen-4bit 5.73 5.14 4.14 3.56 5.25 4.66 3.14 5.68 2.66 4.80 4.31 KVQuant-4bit 5.72 5.13 4.13 3.55 5.16 4.60 3.13 5.60 2.62 4.80 4.00-4.02 KVQuant-4bit-1% 5.69 5.10 4.11 3.54 5.13 4.58 3.13 5.56 2.60 4.78 4.32-4.35 int3 10.87 8.69 6.82 6.37 22.71 18.26 7.68 125 158 7.64 3.00-3.02 nf3 7.33 6.21 5.46 4.44 9.96 9.50 4.06 61.07 98.64 6.30 3.00-3.03 ATOM-3bit 6.17 5.47 4.44 3.78 6.15 5.34 3.33 7.50 4.11 5.16 3.15 FlexGen-3bit 5.93 5.29 4.26 3.66 5.64 4.98 3.23 6.38 3.23 5.00 3.30 KVQuant-3bit 5.87 5.25 4.25 3.63 5.34 4.71 3.18 5.84 2.72 4.98 3.00-3.02 KVQuant-3bit-1% 5.75 5.14 4.15 3.57 5.17 4.61 3.15 5.64 2.63 4.82 3.32-3.35 int2 11779 69965 1470 7272 4708 3943 976 2841 2164 573 2.00-2.02 nf2 3210 5786 2044 5367 13601 4036 3680 30492 9486 903 2.00-2.03 ATOM-2bit 37.37 41.77 16.49 13.63 118 93.09 18.31 200 2092 51.96 2.14 FlexGen-2bit 11.09 9.84 6.60 5.54 25.69 26.83 5.93 57.82 43.69 12.47 2.28 KVQuant-2bit 7.23 5.82 4.87 4.03 7.03 9.59 3.45 7.04 3.49 6.84 2.00-2.02 KVQuant-2bit-1% 6.01 5.36 4.35 3.70 5.41 4.78 3.26 6.04 2.82 5.14 2.32-2.35 23Table 19: Full evaluation of our method for different models on all LLaMA, Llama-2, Llama-3, and Mistral models using the perplexity on C4. KVQuant results are using pre-RoPE per-channel quantization for Keys. Average bit widths assume a sequence length of 128K (ignoring context length limits for the models). Note that ATOM and FlexGen use 4-bit quantization with group sizes of 128 and 64 with uniform quantization, respectively, and we extend their methods to 3-bit and 2-bit quantization. We leverage Attention-Sink Aware quantization for all bit widths. We used post-RoPE quantization for all baseline methods since it achieves higher accuracy when quantizing Keys per-token as shown in Appendix P. Method LLaMA Llama-2 Llama-3 Mistral-7B Avg. Num. Bits7B 13B 30B 65B 7B 13B 70B 8B 70B baseline 7.08 6.61 5.98 5.62 6.63 6.05 4.97 7.10 5.78 5.71 16 int4 7.40 6.82 6.18 5.75 7.31 6.59 5.12 8.79 14.36 5.91 4.00-4.02 nf4 7.27 6.74 6.10 5.69 7.09 6.45 5.06 7.93 6.58 5.85 4.00-4.03 ATOM-4bit 7.16 6.67 6.02 5.65 6.87 6.20 5.00 7.32 6.03 5.76 4.16 FlexGen-4bit 7.12 6.64 6.00 5.63 6.79 6.15 4.99 7.23 5.84 5.75 4.31 KVQuant-4bit 7.11 6.64 6.00 5.63 6.68 6.08 4.99 7.18 5.80 5.75 4.00-4.02 KVQuant-4bit-1% 7.09 6.62 5.99 5.62 6.64 6.06 4.98 7.12 5.78 5.72 4.32-4.35 int3 12.97 10.95 9.13 8.27 30.14 28.57 16.00 63.75 301 8.84 3.00-3.02 nf3 8.90 7.84 7.43 6.37 14.92 13.75 5.96 68.38 148 7.27 3.00-3.03 ATOM-3bit 7.62 6.93 6.24 5.79 8.00 7.06 5.16 9.00 6.72 6.08 3.15 FlexGen-3bit 7.34 6.78 6.11 5.70 7.29 6.59 5.08 7.92 6.18 5.92 3.30 KVQuant-3bit 7.25 6.72 6.06 5.68 6.89 6.18 5.03 7.43 5.90 5.92 3.00-3.02 KVQuant-3bit-1% 7.13 6.64 6.00 5.63 6.69 6.09 4.99 7.19 5.81 5.76 3.32-3.35 int2 10892 100870 1411 7216 4708 4220 814 2113 1977 477 2.00-2.02 nf2 2850 4680 1617 5190 13081 4176 3217 78331 7616 1102 2.00-2.03 ATOM-2bit 43.49 56.25 21.07 17.05 113 97.04 23.67 135 3734 50.73 2.14 FlexGen-2bit 13.91 13.36 8.49 7.34 35.21 40.40 8.28 50.78 42.27 13.83 2.28 KVQuant-2bit 8.52 7.32 6.67 5.96 9.49 15.36 5.30 10.06 6.45 7.63 2.00-2.02 KVQuant-2bit-1% 7.33 6.78 6.11 5.70 6.96 6.25 5.08 7.60 5.96 6.05 2.32-2.35 24P Post-RoPE Per-Token Quantization Ablation Table 20 shows perplexity evaluation on Wikitext-2 for the LLaMA-7B model with uniform quan- tization, with Keys quantized pre-RoPE and post-RoPE. These results show that post-RoPE Key quantization is superior to pre-RoPE Key quantization when quantizing Keys per-token. This is because when rotating an outlier channel with large average magnitude and another channel with smaller average magnitude together, at some positions in the sequence, part of the magnitude from the outlier channel will be shifted to the smaller channel. This partially mitigates the impact of the outlier channel on skewing the quantization range for some of the tokens in the sequence. As such, for our baseline comparisons, we use post-RoPE per-token Key quantization to serve as a stronger baseline. Table 20: Model accuracy when using pre-RoPE and post-RoPE quantization for LLaMA-7B with per-token Key quantization. Our experiments demonstrate that post-RoPE quantization is superior when using per-token Key quantization. Therefore, we decided to use these results for baseline comparison with per-token quantization. Datatype Perplexity (Pre-RoPE) Perplexity (Post-RoPE) fp16 5.68 5.68 int4 6.02 5.98 int4-gs128 5.76 5.77 int3 14.68 10.87 int3-gs128 6.28 6.17 Q Experiments on Calibration Data Robustness To evaluate the robustness of our quantization method to the choice of calibration datasets, we measure the perplexity on Wikitext-2 and C4 using different quantized models calibrated with Wikitext-2 and C4. As shown in Table 21, the resulting perplexity numbers remain similar even when the models are calibrated with out-of-domain examples (e.g., calibrated with Wikitext-2 and evaluated on C4, and vice versa), demonstrating the robustness of the calibration process to the choice of data. Table 21: Perplexity (PPL) results on Wikitext-2 and C4 using different quantization schemes, calibrated using Wikitext-2 and C4. Datatype Wikitext-2 C4 Calib. with Wikitext-2 Calib. with C4 Calib. with Wikitext-2 Calib. with C4 4-bit, 1% sparsity 5.69 5.70 7.09 7.09 3-bit, 1% sparsity 5.75 5.75 7.13 7.13 2-bit, 1% sparsity 6.05 6.07 7.38 7.38 R Kernel Implementation Details We implemented 4-bit lookup table-based kernels for matrix-vector multiplication between the Key or Value activations (packed as a lookup table (LUT) plus indices into the LUT per-element) and a full-precision activation vector. These kernels load the compressed Key and Value activations and dequantize them only as needed in order to minimize memory bandwidth utilization. All arithmetic is performed in fp16. The lookup table entries are the values of the sensitivity-weighted non-uniform datatype for that particular layer scaled according to the range of activations that need to be represented [8]. When selecting between the Compressed-Sparse Column (CSC format) and the Compressed-Sparse Row (CSR) format for storing the outliers for the Keys and Values, we needed to consider how easy it would be to append new vectors. When using CSC format for the Key matrix, we only need to append a single element to the column vector, as well as one new element to the row and value vectors per nonzero element in that new column. If we used CSR format, we would need to insert the new column and value elements in the middle of the existing column and value vectors, and we would 25need to recompute the elements of the row vector. When using CSR format for the Value matrix, we only need to append a single element to the row vector, as well as one new element to the column and value vectors per nonzero element in that new row. If we used CSC format, we would need to insert the new row and value elements in the middle of the existing row and value vectors, and we would need to recompute the elements of the column vector. We therefore used the CSC format for the Key matrices and the CSR format for the Value matrices. One challenge with efficiently processing the sparse matrix-dense vector operation is that the sparsity distribution may be unbalanced. This poses a challenge for efficiently processing the sparse matrix on a GPU as there can be different numbers of nonzeros to process per thread. We therefore leverage a balanced sparse matrix-dense vector kernel based on [10, 17], which assigns an equal number of nonzeros per thread. This has greater synchronization overhead than assigning a single thread for an entire row or column when processing CSR/CSC matrices, but it leads to a more balanced work assignment between threads. We set the number of threads such that there were 10 nonzero values assigned to each thread. The dense non-uniform kernel and balanced sparse kernels are launched in one call to avoid overhead from summing the output vectors from these separate operations. A second potential challenge is that the quantization dimension for the Keys and Values is not aligned with the reduction dimension for the respective matrix-vector operations. If we used per-channel or per-token lookup tables for dequantization, we would need to load from a different lookup table for each element as we iterate along the reduction dimension. However, since our implementation uses a single per-layer datatype that is rescaled per-vector, we can load a single scaling factor and zero-point for each channel / token and broadcast it across all threads (while doing a table lookup from the shared per-layer datatype, which is duplicated such that each thread can access it efficiently in parallel). The use of a single per-layer datatype that is rescaled per-vector therefore allows for much more efficient GPU kernel implementations in the context of per-channel Key / per-token Value quantization. Table 22 shows a detailed breakdown of kernel runtime, including how much time is spent packing vectors into the compressed format and how much time is spent on the dense and sparse matrix- vector multiplications. For the fp16 baselines, we benchmarked runtime by averaging across 1000 iterations. When benchmarking our dense-and-sparse kernels (both for packing and matrix-vector multiplication), since the runtime of the sparse kernels can vary based on the sparsity pattern, we ran the LLaMA-2-7B-32K model and collected activations at each layer, and we then measured the average runtime across 1000 iterations for each layer in the model. We find that even with 1% sparsity, we can attain significant speedups of up to 1.7× relative to the fp16 matrix-vector multiply kernels, demonstrating how our methodology facilitates efficient inference with a low-precision quantized KV cache. Table 22: Average latency (in microseconds) for the Key and Value dense nuq4 kernels as well as for the sparse kernels (with 1% outliers), benchmarked on an A6000 GPU for the LLaMA-2-7B-32K model. Benchmarking results are reported for different sequence lengths ( l). fp16 matrix-vector multiplication latencies are included for reference, and the Key multiplication time also includes the time to apply RoPE to the newly appended Key vector. We find that our dense-and-sparse approach (even with 1% outliers) provides latency benefits relative to the fp16 baseline, even when accounting for the time to compress activations online. Activation Operation l=2K l=4K l=16K Key fp16 Matvec 33.3 59.1 219.4 Key (nuq4-1%) Packing 4.5 4.5 4.5 Dense Matvec 13.0 24.1 87.6 Sparse Matvec 8.1 11.2 34.2 Total Latency 25.6 39.9 126.3 Value fp16 Matvec 26.0 50.2 203.7 Value (nuq4-1%) Packing 4.1 4.1 4.1 Dense Matvec 10.0 17.8 62.0 Sparse Matvec 7.9 15.9 58.2 Total Latency 22.1 37.9 124.5 26S Limitations While our work enables accurate long-context length inference by reducing the memory requirements, there is significant work required for training long context length models with greater than 100K context length. This work is orthogonal to our efforts, which are constrained to efficient inference with long context length models. Additionally, our latency benchmarking results currently focus on memory-bandwidth bound generation rather than prompt processing (where we need to compress multiple Keys and Values at once). Finally, in the current end-to-end implementation, there are inefficiencies in how memory allocation is handled for updating the sparse matrix (where the data corresponding to the previous tokens have to be copied when concatenating them with the data from the new token). In future work, we plan to optimize this by doing blocked allocation to avoid overheads from reallocating memory. 27",
      "meta_data": {
        "arxiv_id": "2401.18079v6",
        "authors": [
          "Coleman Hooper",
          "Sehoon Kim",
          "Hiva Mohammadzadeh",
          "Michael W. Mahoney",
          "Yakun Sophia Shao",
          "Kurt Keutzer",
          "Amir Gholami"
        ],
        "published_date": "2024-01-31T18:58:14Z",
        "pdf_url": "https://arxiv.org/pdf/2401.18079v6.pdf",
        "github_url": "https://github.com/SqueezeAILab/KVQuant"
      },
      "llm_extracted_info": {
        "main_contributions": "KVQuant addresses the dominant memory consumption of KV cache activations during LLM inference with large context windows, which existing solutions struggle to quantize accurately below 4-bit precision. The work introduces four novel methods: (i) Per-Channel Key Quantization, quantizing Keys along the channel dimension before RoPE to better match their distribution and mitigate RoPE's impact; (ii) Non-Uniform KV Cache Quantization, deriving per-layer sensitivity-weighted non-uniform datatypes for improved representation; (iii) Per-Vector Dense-and-Sparse Quantization, isolating and compressing numerical outliers per-vector to minimize quantization range skews; and (iv) Attention Sink-Aware Quantization, preserving the first token in fp16. KVQuant achieves less than 0.1 perplexity degradation with 3-bit quantization on Wikitext-2 and C4 across LLaMA, Llama-2, Llama-3, and Mistral models. This enables serving LLaMA-7B with a context length of up to 1 million on a single A100-80GB GPU and up to 10 million on an 8-GPU system. Custom CUDA kernels provide up to ~1.7x speedups compared to fp16 baseline matrix-vector multiplications for LLaMA-7B.",
        "methodology": "The methodology includes: (i) **Per-Channel Key Quantization** for Keys and per-token for Values, with offline calibration for Key scaling factors. This groups values of similar magnitudes to mitigate outlier channel impact. (ii) **Pre-RoPE Key Quantization** applied before the rotary positional embedding (RoPE) to prevent RoPE from mixing channel magnitudes and making quantization harder. A fused kernel applies RoPE post-dequantization efficiently. (iii) **Non-Uniform Quantization (nuqX)** which computes per-layer sensitivity-weighted non-uniform datatypes offline using a k-means solver on normalized activation values and Fisher information matrix for error minimization. (iv) **Per-Vector Dense-and-Sparse Quantization** that identifies and separates a small percentage (e.g., 1%) of outliers for each vector (per-channel for Keys, per-token for Values) storing them sparsely in full precision, while quantizing the remaining dense elements. Offline calibration is used for per-channel outlier thresholds for Keys, and online computation for per-token outlier thresholds for Values. (v) **Attention Sink-Aware Quantization** keeps the first token in fp16 to counter the model's sensitivity to its quantization error. (vi) **Custom CUDA kernels** are implemented for efficient on-the-fly 4-bit quantization, sparse outlier extraction, and optimized matrix-vector multiplications using a lookup table-based approach and balanced sparse matrix processing.",
        "experimental_setup": "The evaluation was performed on LLaMA-7B/13B/30B/65B, Llama-2-7B/13B/70B, Llama-3-8B/70B, and Mistral-7B models. Perplexity (PPL) was measured using teacher forcing on Wikitext-2 and C4 datasets, using sequence lengths up to the models' maximum context length (2K, 4K, 8K). Long context length evaluations included perplexity on Wikitext-2 for LLaMA-2-7B-32K and Llama-2-70B-32K, Passkey retrieval using a framework from [45] over 50 samples, and evaluations on the LongBench and RULER benchmark suites. Calibration was done using 16 samples of 2K sequence length from the Wikitext-2 training set. Baselines included uniform quantization (intX), NormalFloat (nfX), ATOM, and FlexGen. Kernel benchmarking was conducted on an A6000 GPU for LLaMA-2-7B-32K to assess latency and speedups for Key and Value matrix-vector operations. Memory savings and context length capabilities were estimated for A100-80GB GPUs (single and 8-GPU systems). The method's compatibility with weight-only quantization (SqueezeLLM) was also tested.",
        "limitations": "The work acknowledges that significant effort is still required for training long context length models exceeding 100K tokens, which is orthogonal to their focus on inference efficiency. Current latency benchmarking primarily targets memory-bandwidth bound generation rather than prompt processing, where multiple Keys and Values are compressed simultaneously. Additionally, the existing end-to-end implementation has inefficiencies in memory allocation for updating sparse matrices, requiring copying data for previous tokens when concatenating with new token data.",
        "future_research_directions": "Future work plans to optimize memory allocation for updating sparse matrices by implementing blocked allocation to avoid overheads from reallocating memory. While orthogonal to the current work, the paper implicitly highlights the ongoing need for advancements in training long context length models beyond 100K. Further investigation into prompt processing latency could also be an area for future exploration.",
        "experimental_code": "import numpy as npimport torchimport torch.nn as nnimport mathfrom sklearn.cluster import KMeansimport torchfrom torch.distributions import Normaldef get_outliers(w,channel=-1,outlier_threshold_upper=-1,outlier_threshold_lower=-1,cap_outliers=-1,first_few_fp16=-1):outlier_threshold_upper = outlier_threshold_upper.unsqueeze(channel)outlier_threshold_lower = outlier_threshold_lower.unsqueeze(channel)under_lower = w < outlier_threshold_lowerabove_upper = w > outlier_threshold_upperoutlier_mask = torch.logical_or(under_lower, above_upper)if first_few_fp16 > -1:outlier_mask[:first_few_fp16,:] = Truereturn outlier_maskdef get_outliers_dynamic(w,channel=-1,thresh=0.999,first_few_fp16=-1):t = 1-((1-thresh)/2)w = w.float()outlier_threshold_upper = torch.quantile(w, t, dim=channel)outlier_threshold_lower = torch.quantile(w, 1-t, dim=channel)outlier_threshold_upper = outlier_threshold_upper.unsqueeze(channel)outlier_threshold_lower = outlier_threshold_lower.unsqueeze(channel)under_lower = w <= outlier_threshold_lowerabove_upper = w >= outlier_threshold_upperoutlier_mask = torch.logical_or(under_lower, above_upper)if first_few_fp16 > -1:outlier_mask[:first_few_fp16,:] = Truereturn outlier_maskclass SimQuant:def __init__(self,layer,bits,perchannel=True,qchannel=0,include_rope=False):self.layer = layerself.dev = self.layer.weight.deviceW = layer.weight.data.clone()self.perchannel = perchannelself.qchannel = qchannelself.bits = bitsself.rows = W.shape[0]self.columns = W.shape[1]self.nsamples = 0self.out = Nonedef add_batch(self, inp, out):if len(out.shape) == 2:out = out.unsqueeze(0)tmp = out.shape[0]if isinstance(self.layer, nn.Linear):if len(out.shape) == 3:out = out.reshape((-1, self.rows))self.nsamples += tmpif self.out == None:self.out = out.clone()else:self.out = torch.cat((self.out, out.clone()), dim=0))def quantize(self,include_sparse=False,sparsity_threshold=0.999,nuq=False,fisher=False,norm=False,cap_outliers=False,first_few_fp16=-1):if include_sparse:t = 1-((1-sparsity_threshold)/2)else:t = 1data = self.out.float().cpu().numpy()if self.perchannel:outlier_threshold_upper = np.percentile(data, t*100, axis=self.qchannel)outlier_threshold_lower = np.percentile(data, (1-t)*100, axis=self.qchannel)else:assert(False)data = torch.tensor(data)outlier_threshold_upper = torch.tensor(outlier_threshold_upper).unsqueeze(self.qchannel)outlier_threshold_lower = torch.tensor(outlier_threshold_lower).unsqueeze(self.qchannel)return outlier_threshold_upper, outlier_threshold_lower, centroidsclass QuantLinearSim(nn.Module):def __init__(self,name,bits,quantizer,infeatures,outfeatures,weight,bias,perchannel=True,include_sparse=False,sparsity_threshold=0.999,dynamicquantization=False,nuq=False,nf_nuq=True,norm=False,first_few_fp16=-1,cap_outliers=-1,clamp=False):super().__init__()self.name = nameself.infeatures = infeaturesself.outfeatures = outfeaturesself.bits = bitsself.weight = weight.T.detach().cpu()if bias:self.bias = bias.detach().cpu()else:self.bias = Noneself.perchannel = perchannelself.dynamicquantization = dynamicquantizationif perchannel:self.qchannel = 0else:self.qchannel = -1self.ochannel = self.qchannelself.include_sparse = include_sparseself.sparsity_threshold = sparsity_thresholdself.outlier_threshold_upper = torch.tensor(quantizer[0]).cuda().flatten().half()self.outlier_threshold_lower = torch.tensor(quantizer[1]).cuda().flatten().half()self.nuq = nuqself.nf_nuq = nf_nuqself.first_few_fp16 = first_few_fp16def forward(self, x, other_mat=None):out_shape = x.shape[:-1] + (self.outfeatures, )x = x.reshape(-1,x.shape[-1])self.weight = self.weight.to(x.device)if self.bias is not None:self.bias = self.bias.to(x.device)x = x.half()y = x @ self.weighty = y + self.bias if self.bias is not None else yy = y.float()if self.include_sparse:if self.dynamicquantization:outlier_mask = get_outliers_dynamic(y,channel=self.ochannel,thresh=self.sparsity_threshold,first_few_fp16=self.first_few_fp16)else:self.outlier_threshold_upper = self.outlier_threshold_upper.to(y.device)self.outlier_threshold_lower = self.outlier_threshold_lower.to(y.device)outlier_mask = get_outliers(y,channel=self.ochannel,outlier_threshold_upper=self.outlier_threshold_upper,outlier_threshold_lower=self.outlier_threshold_lower,cap_outliers=self.cap_outliers,first_few_fp16=self.first_few_fp16)else:outlier_mask = None",
        "experimental_info": "Per-channel quantization is applied to Keys (`self_attn.k_proj`) as indicated by `perchannel=True` and `qchannel=0` in `SimQuant` initialization. Per-token quantization is applied to Values (`self_attn.v_proj`) implicitly by setting `qchannel=-1` in `SimQuant` for Value projections, and `perchannel=False` in `make_quant_sim` combined with `self.qchannel=-1` in `QuantLinearSim` to target the last dimension (token dimension). Offline calibration for Key scaling factors is achieved by computing `outlier_threshold_upper` and `outlier_threshold_lower` using `np.percentile` on collected activations for Keys during `SimQuant.quantize` with `dynamicquantization=False` when `make_quant_sim` is called for `k_proj`."
      }
    },
    {
      "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache",
      "abstract": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI.",
      "full_text": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Zirui Liu * 1 Jiayi Yuan* 1 Hongye Jin 2 Shaochen (Henry) Zhong 1 Zhaozhuo Xu 3 Vladimir Braverman 1 Beidi Chen 4 Xia Hu 1 Abstract Efficiently serving large language models (LLMs) requires batching of many requests to reduce the cost per request. Yet, with larger batch sizes and longer context lengths, the key-value (KV) cache, which stores attention keys and values to avoid re- computations, significantly increases memory de- mands and becomes the new bottleneck in speed and memory usage. Additionally, the loading of the KV cache causes the computational core to be idle, which limits the inference speed. A straight- forward and effective solution to reduce KV cache size is quantization, which decreases the total bytes taken by KV cache. However, there is a lack of in-depth studies that explore the element dis- tribution of KV cache to understand the hardness and limitation of KV cache quantization. To fill the gap, we conducted a comprehensive study on the element distribution in KV cache of popular LLMs. Our findings indicate that the key cache should be quantized per-channel, i.e., group ele- ments along the channel dimension and quantize them together. In contrast, the value cache should be quantized per-token. From this analysis, we de- veloped a tuning-free 2bit KV cache quantization algorithm named KIVI. With hardware-friendly implementation, KIVI can enable Llama, Falcon, and Mistral models to maintain almost the same quality while using 2.6× less peak memory (in- cluding model weight). This reduction in memory usage enables up to 4× larger batch size, bringing 2.35× ∼3.47× throughput on real LLM infer- ence workload. The source code is available at https://github.com/jy-yuan/KIVI. *Equal contribution . The order of authors is determined by flipping a coin. 1Rice University 2Texas A&M University 3Stevens Institute of Technology 4Carnegie Mellon University. Correspondence to: Zirui Liu <zl105@rice.edu>, Jiayi Yuan <jy101@rice.edu>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). 1. Introduction Large Language Models (LLMs) have demonstrated strong performance across a wide range of tasks (Brown et al., 2020; Taylor et al., 2022; Yuan et al., 2023; Chuang et al., 2024). However, their deployment is very costly, requiring a large number of hardware accelerators such as GPUs. Given these substantial costs, one natural way to reduce the cost per request is to combine a sufficient number of requests together for batch processing. However, in this batch infer- ence scenario, the key-value cache (KV cache), which holds the attention keys and values during generation to prevent re-computations, is becoming the new memory and speed bottleneck. This bottleneck becomes more pronounced with larger batch sizes and longer context lengths. For instance, in 540B PaLM, with a batch size of 512 and a context length of 2048, KV cache alone can take 3TB. This is 3 times the size of the model’s parameters (Pope et al., 2023). Also, the GPU SRAM has to load the whole KV cache from the GPU device memory for every token generated, during which the computational cores are idle. Thus, reducing KV cache size in LLMs while maintaining accuracy is important. Existing works towards this problem can be roughly divided into three categories. First, some work suggests reducing the number of heads in KV cache, such as multi-query attention (Shazeer, 2019) and multi-group attention (Ainslie et al., 2023). However, these methods require either training the model from scratch or fine-tuning the existing model. Second, another research line reduces KV cache size by evicting unimportant tokens (Zhang et al., 2023). Third, some other works try to solve this problem from the system perspective, e.g., offloading KV cache (Sheng et al., 2023) or extending virtual memory and paging techniques into the attention mechanism (Kwon et al., 2023). To reduce the size of KV cache, the most simple and ef- fective way is to reduce the total bytes taken by KV cache, namely, quantization. Unlike the well-studied weight quan- tization (Lin et al., 2023; Xiao et al., 2023a; Zhao et al., 2024), to the best of our knowledge, only a few studies applied the vanilla 4bit round-to-nearest quantization to KV cache (Sheng et al., 2023; Zhang et al., 2023; Zhao et al., 2024) due to the streaming nature of KV cache or other complications. There is a lack of in-depth studies that ex- 1 arXiv:2402.02750v2  [cs.CL]  25 Jul 2024KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache per-token  quantization  per-channel  quantization  Figure 1: Definition of per-token and per-channel quantiza- tion. X ∈ Rlprompt×d is key/value cache where lprompt is the number of tokens and d is the number of channels. zX is the zero-point, sX is the scaling factor. plore the element distribution of KV cache to understand the hardness and limitation of KV cache quantization. To fill the gap, we study the element distribution of KV cache. Our analysis suggests: • For key cache, there are a few fixed channels whose magnitudes are very large, which is consistent with previous finding (Lin et al., 2023; Xiao et al., 2023a). Thus, as shown in Figure 1 right, key cache should be quantized per-channel, i.e., group elements along the channel dimension and quantize them together. In this way, it can confine the error to each individual channel, without impacting the other normal channels. • For value cache, there is no obvious outlier pattern. Al- though value cache has no obvious outlier pattern, we experimentally show that it can only be quantized per- token because it is used to calculate the attention output, which is essentially a value cache mixer. As shown in Figure 1 left, the per-token quantization can confine the error inside each individual token and ensure that the quantization of one token does not adversely impact the others. Based on the above insights, we propose KIVI, a plug-and- play extreme low-bit KV cache quantization method. KIVI quantizes key cache per-channel and quantizes value cache per-token. The per-token value cache quantization aligns well with the streaming nature of auto-regressive inference, allowing newly quantized tensors to be directly appended to the existing quantized value cache by token dimension. However, for per-channel key cache quantization, the quan- tization process spans different tokens, which cannot be directly implemented in this streaming setting. Since the number of tokens in key cache can be arbitrary, our key idea is to split key cache into two parts. The first part is the grouped key cache, which contains several groups of tokens and each group has a certain number of tokens. The second part is the residual key cache, which does not have a suffi- cient number of tokens to form a complete group. Similarly, we split value cache into the grouped and residual parts to maintain the accuracy. We only apply group-wise quanti- zation to the grouped key cache and value cache, while the residual key cache and value cache are kept in full precision. The grouped and residual parts can be combined using tiled matrix multiplication when computing attention scores. Our contributions are summarized as follows: • Extensive analysis regarding the outlier patterns and quantization error of KV cache in commonly- used LLMs. Our observations suggest that key cache should be quantized per-channel and value cache should be quantized per-token. We also explain in depth why these caches require different quantization approaches. • A new plug-and-play 2bit KV cache quantization algorithm without any fine-tuning, KIVI, with hardware-friendly implementation. We conduct an extensive evaluation for KIVI with Llama, Mistral, and Falcon on popular generation tasks. KIVI can efficiently compress KV cache to 2bit and bring 2.6× peak memory usage reduction for Llama-2-7B, with little to no accuracy drop. With our efficient system im- plementation, this memory reduction, in return, enables up to 4× larger batch size and brings2.35× ∼3.47× throughput. 2. Background: Attention Inference-Time Workflow The LLM attention inference-time workflow involves two phases: i) the prefill phase, where the input prompt is used to generate KV cache for each transformer layer of LLMs; and ii) the decoding phase, where the model uses and updates KV cache to generate the next token, one at a time. Prefill Phase. Let X ∈ Rb×lprompt×d be the input tensor, where b is the batch size, lprompt is the length of the input prompt, and d is the model hidden size. For convenience, we ignore the layer index here. The key, value tensors can be computed by XK = XWK, XV = XWV , where WK, WV ∈ Rd×d are the key and value layer weight, respectively. After obtaining XK and XV , they are cached in the memory for the ease of decoding. Decoding Phase. Let t ∈ Rb×1×d be the current input token embedding. Let tK = tWK and tV = tWV be the key and value layer output, respectively. We first update KV cache: XK ← Concat(XK, tK), XV ← Concat(XV , tV ), 2KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache then calculate the attention output as: tQ = tWQ, A = Softmax(tQX⊤ K), tO = AXV , (1) where WQ is the weight matrix of the query layer. For ease of illustration, we ignore the attention output layer and the other parts of the inference workflow. Memory and Speed Analysis. The above process is re- peated until a special token indicating the sentence’s con- clusion is reached. Let lgen be the number of generated tokens. From the above analysis, the shape of KV cache is b ×(lprompt + lgen) ×d. To get a sense of the scale, consider the OPT-175B model with a batch size b 512, a prompt length lprompt 512, and an output length lgen 32. The KV cache requires 1.2TB, which is 3.8 times the model weights (Sheng et al., 2023). Besides the memory, the inference speed is also decided by the KV cache size. The GPU needs to load KV cache from GPU main memory to GPU SRAM once for every token generated during which the computa- tional core of the chip is essentially idle (Pope et al., 2023; Kwon et al., 2023). 3. Methodology In scenarios with long contexts or batched inferences, the memory and speed bottlenecks are storing and loading KV cache. The most simple and effective way to alleviate this problem is to reduce the total bytes occupied by KV cache, specifically, quantization. Following this motivation, we first evaluate the performance of the existing quantization method in Section 3.1. Our observations suggest that key and value cache should be quantized along different dimen- sions. We analyze the rationale behind this observation in Section 3.2. Then based on the analysis, we propose KIVI, a new KV cache quantization method along with its streaming data structure, detailed in Section 3.3. 3.1. Preliminary Study of KV Cache Quantization As we analyzed in Section 2, KV cache functions as a streaming data structure, where the new tensor arrives se- quentially. Thus, optimization-based methods like GPTQ (Frantar et al., 2022) are unsuitable for quantizing KV cache due to the overhead. To the best of our knowledge, the most flexible way for quantizing KV cache is the round- to-nearest quantization. The B−bit integer quantization- dequantization process can be expressed as: Q(X) = ⌊X − zX sX ⌉, X′ = Q(X) · sX + zX, where zX = min X is the zero-point, sX = (max X − min X)/(2B −1) is the scaling factor, and ⌊·⌉ is the round- ing operation. Here we ignore the batch size for ease of Table 1: The results of simulated KV cache group-wise quantization with various configurations. The group size is set as 32. C stands for per-channel quantization and T stands for per-token quantization. Please check the whole evaluation in Table 3. Llama-2-13B CoQA TruthfulQA 16bit 66.37 29.53 4bit (K - T, V - T) 66.48 29.51 2bit (K - T, V - T) 52.93 24.98 2bit (K - C, V - C) 2.88 0.74 2bit (K - T, V - C) 2.80 0.26 2bit (K - C, V - T) 63.53 28.60 understanding. As shown in Figure 1, X is quantized along either the token or channel dimension group-wisely. Considering the streaming nature of KV cache, previous studies often apply per-token quantization to both key and value cache since the newly quantized KV cache can be naively added to the existing quantized one along the token dimension (Sheng et al., 2023). While per-channel quanti- zation is non-trivial, we have designed a padding method to implement per-channel quantization to explore its effect on both key and value cache. Setting. In Table 1, we show the results of fake KV cache group-wise quantization with different configurations on the Llama-2-13B model for the CoQA and TruthfulQA tasks. We use a group size of 32 for all configurations. Here fake quantization means we simulate the quantization process by first quantizing KV cache into lower precision and then dequantizing it in the attention layer. For per-channel quan- tization, if the number of tokens is not divided evenly into groups, we add zero-padding to ensure it can be grouped perfectly. In this way, we ensure that all tokens in KV cache are quantized for a fair comparison. The detailed experi- mental setting can be found in Section 4.1. Specifically, we observe that: OB 1. When using the commonly used per-token quanti- zation to both key and value caches, INT4 precision can maintain accuracy. However, reducing it to INT2 results in a notable accuracy drop. OB 2. When value cache is quantized per-channel, the accuracy significantly worsens regardless of how key cache is quantized. OB 3. When using a lower numerical precision such as INT2, the most accurate approach is to quantize key cache per-channel and value cache per-token. 3KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache 0 50 100 150 200 Token 0 25 50 75 100 125 Channel 0.0 2.5 5.0 7.5 10.0 12.5Absolute Value Llama-2-13B Layer 16 Head 0 Key Cache 0 50 100 150 200 Token 0 25 50 75 100 125 Channel 0.5 1.0 1.5 2.0Absolute Value Llama-2-13B Layer 16 Head 0 Value Cache 0 50 100 150 200 Token 0 25 50 75 100 125 Channel 0.0 2.5 5.0 7.5 10.0 12.5 Llama-2-13B Layer 31 Head 0 Key Cache 0 50 100 150 200 Token 0 25 50 75 100 125 Channel 0.5 1.0 1.5 2.0 2.5 Llama-2-13B Layer 31 Head 0 Value Cache 0 50 100 150 200 Token 0 20 40 60 Channel 0 2 4 6 8 10Absolute Value Falcon-7B Layer 16 Head 0 Key Cache 0 50 100 150 200 Token 0 20 40 60 Channel 0.5 1.0 1.5Absolute Value Falcon-7B Layer 16 Head 0 Value Cache 0 50 100 150 200 Token 0 20 40 60 Channel 0.0 2.5 5.0 7.5 10.0 Falcon-7B Layer 20 Head 0 Key Cache 0 50 100 150 200 Token 0 20 40 60 Channel 0.5 1.0 1.5 Falcon-7B Layer 20 Head 0 Value Cache Figure 2: Magnitude of key and value cache for Llama-2-13B and Falcon-7B. We observe (1) for key cache, there are a few channels whose magnitudes are very large. (2) for value cache, there is no obvious outlier pattern. 3.2. Why Key and Value Cache Should Quantize Along Different Dimensions? In Table 1, we observe that quantizing key cache per-channel and value cache per-token to 2bit results in a very small accu- racy drop. Here we analyze why this configuration delivers better accuracy. In Figure 2 we visualize the original KV cache distribution at different layers. We observe that in key cache, some fixed channels exhibit very large mag- nitudes, whereas in value cache, there is no significant pattern for outliers. Analysis of Key Cache. The above observation for key cache aligns with previous findings that certain fixed columns in activations exhibit larger outliers (Dettmers et al., 2022; Lin et al., 2023). The persistence of outliers within each channel means that per-channel quantization can con- fine the quantization error to each individual channel with- out impacting the other normal channels. Thus, Figure 2 explains why key cache should be quantized per-channel. In Table 2 we show key cache relative reconstruction error ∥XK−X′ K XK ∥F , along with the relative attention score error ∥A−A′ A ∥F where A′ = Softmax(tQX ′⊤ K ). We observe that the per-token quantization can lead to almost 5× larger at- tention score error than per-channel quantization, which is consistent with Figure 2. Table 2: The relative error statistics averaged over all layers and all heads Llama-2-13B K Per-Token K Per-Channel Avg. ∥ XK−X′ K XK ∥F 13.67 4.55 Avg. ∥ A−A′ A ∥F 47.00 9.60 Attention sparsity 84.3% V Per-Token V Per-Channel Avg. ∥ XV −X′ V XV ∥F 4.57 3.73 Avg. ∆ 3.55 49.89 Analysis of Value Cache. Unlike key cache, value cache does not show the channel-wise outlier pattern. Furthermore, Figure 2 alone cannot explain OB2, which indicates value cache should only be quantized per-token. This is because Figure 2 implies that errors should be comparable for both per-token and per-channel quantization, given the absence of a clear pattern. As shown in Equation (1), value cache is used to calculate the attention output tO. Instead of analyzing the quantization error of value cache XV , in Table 2 we analyze the relative error ∆ = ∥AXV −AX′ V AXV ∥F with different quantization configurations. Surprisingly, we observe that the per-token quantization error is almost 15× 4KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Q_MatMul Concat Quantization by Channel Quantization by Token MatMul Prefill Phase Decoding Phase* KVCache * we omit the value cache and  attention output calculation Full Precision Tensor Low Precision Tensor Figure 3: The overview of KIVI algorithm. For ease of illustration, we omit the value cache and attention output parts. The detailed pseudo-code is provided in Algorithm 1. Here “Q_Matmul” is the mix-precision matrix multiplication which fuses the dequantization with matrix multiplication at the tiling level. smaller than per-channel quantization, which explains why OB2 happens. The intuition behind this observation stems from the attention sparsity. Equation (1) can be written as: [AXV ]i∗ = lpromptX j=1 Aij[XV ]j∗, (2) where [XV ]j∗ is the j-th row of XV . From Equation (2), the attention output is the weighted summation of value cache across various tokens, with the weights being the attention scores. Since the attention score is highly sparse (Tian et al., 2023), the output is just the combination of value caches of a few important tokens. The per-token quantization can confine the error to each individual token. Thus, quantizing other tokens does not affect the accuracy of important tokens. Consequently, per-token quantization leads to a much smaller relative error ∆. 3.3. KIVI: Algorithm and System Support Algorithm. As we previously analyzed, key cache should be quantized per-channel and value cache should be quan- tized per-token. Recall that key and value cache of newly generated tokens arrive sequentially. From the implemen- tation perspective, per-token quantization aligns well with streaming settings, allowing newly quantized tensors to be directly appended to the existing quantized value cache by token dimension. However, for per-channel quantiza- tion, the quantization process spans across different tokens, which cannot be directly implemented in the streaming set- ting. As shown in Figure 3, our key idea to solve this prob- lem is to group key cache everyG tokens and quantize them separately. Because the number of tokens in XK can be ar- bitrary, we splitXK into two parts, namely, the grouped part XKg = XK[: l − r] and residual part XKr = XK[l − r :], where l is the number of tokens inside the current key cache XK, r is the number of residual tokens, where l − r can be divisible by G. Since XKg can be evenly divided into (l − r)/G groups, we only store Q(XKg ) with group-wise quantization, while XKr is kept in full precision. During the decoding process, each newly arrived key cache tK is added to XKr and once XKr reaches R tokens, which is a hyperparameter - residual length, we quantize and concatenate it with the previously quantized Q(XKG). Then we reset XKr to an empty tensor. We note that R should be divisible by G. With tiled matrix multiplication, the raw attention logits is then calculated as: Ag = tQQ(X⊤ Kg ), XKr = Concat([XKr , tK]), Ar = tQX⊤ Kr , A = Concat([Ag, Ar]). (3) For value cache, similar to key cache, we also split it into two parts and keep the most recent value cache in full pre- cision, namely, XVg and XVr . Specifically, we maintain a queue and each newly arrived value cache is pushed into the queue. Once the queue reaches the predefined residual length R, the most outdated value cache is poped. Then the poped value cache is quantized per-token and concatenated with the previously quantized value cache along the token dimension. As shown in Figure 3, we also emphasize that during the prefill phase, the exact key and value tensors are passed to the next layers, although only the quantized KV cache is retained in memory. The whole algorithm can be found in Appendix A Algorithm 1. Analysis. In KIVI, the grouped key cacheXKg and value cache XVg is quantized, while the residual key cache XKr and value cache XVr is kept in full precision. By design, there are at most R tokens inside XKr or XVr . In practice, we set R ≤ 128 and the sequence length lprompt + lgen is often much longer than R. Thus the memory overhead from XKr and XVr is negligible when considering the benefit from extreme low-bit quantization, especially for the long context scenarios. Also, since the newly arrived key and 5KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache value tensors are added to XKr and XVr in full precision, KIVI maintains a full precision KV cache sliding window for the local relevant tokens. This window size is expected to be R 2 for key cache, and R for value cache. Later in the experiment section, we show that this full precision sliding window is crucial for obtaining desirable perfor- mance on hard tasks, such as GSM8K. System Support. We provide a hardware-friendly imple- mentation for running KIVI on GPUs. To minimize the overhead, we have fused the dequantization process with matrix multiplication, e.g., Q_MatMul in Figure 3, using CUDA. We also implement the group-wise quantization ker- nel in Triton. Our method is fully compatible with weight- only quantization. 4. Experiments 4.1. Settings Models. We evaluate KIVI using three popular model families: Llama/Llama-2 (Touvron et al., 2023a;b), Fal- con (Penedo et al., 2023) and Mistral (Jiang et al., 2023). Llama and Mistral model is based on multi-head attention, while Falcon is based on multi-query attention (Shazeer, 2019). We use the Hugging Face Transformers codebase and implement the KIVI algorithm upon it. Following previous work (Sheng et al., 2023), the group size G in Algorithm 1 for quantization is set as 32 across all experi- ments, the residual length R for key and value cache is set to 128. Tasks. As we analyzed in Section 2, the KV cache size grows larger with a longer context. Thus, we evaluateKIVI under the normal context length and long context setting, respectively. Specifically, we adopt generation tasks from LM-Eval (Gao et al., 2021) for normal context length eval- uation and LongBench (Bai et al., 2023) for long context evaluation, respectively1. For LM-eval, we adopt CoQA (Exact match accuracy), TruthfulQA (BLEU score), and GSM8K (Exact match accuracy). For LongBench, we chose tasks from four subgroups. Specifically, Qasper (F1 score) is a Single-Document QA task; QMSum (ROUGE score) and MultiNews (ROUGE score) are Summarization tasks; TREC (classification score), TriviaQA (F1 score), and SAM- Sum (ROUGE score) are Few-shot Learning tasks; and LCC (similarity score) and RepoBench-P (similarity score) is Code Completion task. The maximum sequence length in LongBench was set to 8192 for the Mistral model and 4096 for other models. We also consider the needle-in-a-haystack 1The closed-end tasks such as MMLU are not ideal to evaluate KIVI since they only involve one decoding step and directly fetch the output logits, which is not suitable for studying the impact of compressed KV cache. task (NIAH) to evaluate the model’s long context retrieval ability after quantizing KV cache. Detailed NIAH setting can be found in Appendix B. 4.2. Accuracy and Efficiency Analysis 4.2.1. C OMPARISON BETWEEN DIFFERENT QUANTIZATION CONFIGURATIONS We first utilize the fake quantization to demonstrate the effec- tiveness of our asymmetric quantization, namely, quantizing key cache per-channel and value cache per-token. Here fake quantization is exactly the same as in Table 1. The results are shown in Table 3. We observe that “2bit (K per-channel, V per-token)” consistently achieves the best results com- pared to all other configurations. This is consistent with our previous analysis. We also note that for hard generation tasks such as GSM8K, the fake “2bit (K per-channel, V per-token)” quantization results are significantly worse than the full precision counterparts. However, for KIVI in Table 3, we observe that the accuracy drop is only around 2% for GSM8K across different models. As we analyzed in Section 3.3, the difference between fake “2bit (K per-channel, V per-token)” quantization and KIVI is that KIVI maintains a full precision key and value cache sliding window for the local relevant tokens. This sliding window is crucial to maintaining accuracy for hard generation tasks such as mathematical reasoning. 4.2.2. A CCURACY COMPARISON ON GENERATION TASKS LM-Eval Results. We benchmark KIVI in CoQA, Truth- fulQA and GSM8K tasks using the LM-Eval framework. All dataset parameters were set to default. We compare the standard 16bit configuration with our KIVI compression techniques in Llama-2-7B, Llama-2-13B, Falcon-7B and Mistral-7B. As shown in Table 3, we observe that for the Llama and Mistral model, KIVI only has up to 2% accu- racy drop despite the KV cache being stored in 2bit. For instance, in the Llama-2-7B model, the transition from 16bit to 2bit only slightly decreases accuracy. Similar trends are observed in other Llama-family models. Since Falcon-7B adopts multi-query attention and only has one head for KV cache, it is already highly compressed compared to Llama- based models. Thus, in Table 3, 4bit KIVI is needed to maintain the accuracy, while 2bit KIVI may have a large accuracy drop in this case. LongBench Results. The performance of KIVI over vari- ous models in the LongBench dataset is summarised in Table 4. We apply KIVI to Llama2-7B, Llama2-13B, Llama2-7B- Chat, Llama2-13B-Chat, Falcon-7B and Mistral-7B. Table 4 suggests that KIVI is an effective method for KV cache compression with minimal impact on accuracy across var- 6KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  27K tokens (a) Llama-3-8B-Instruct Baseline 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  27K tokens  (b) Llama-3-8B-Instruct + KIVI-2 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  27K tokens  (c) Llama-3-8B-Instruct + KIVI-4 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  30K tokens (d) Mistral-7B-Instruct-v0.2 Baseline 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  30K tokens  (e) Mistral-7B-Instruct-v0.2 + KIVI-2 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  30K tokens  (f) Mistral-7B-Instruct-v0.2 + KIVI-4 0.0 0.2 0.4 0.6 0.8 1.0 Figure 4: Needle-in-a-Haystack results on Llama-3-8B-Instruct and Mistral-7B-Instruct-v0.2. Here we count the number of words instead of tokens to better account for the tokenizer difference. The final token length is noted in the upper right corners of each figure. Detailed setting can be found in Appendix B. ious hard long context generation tasks. We present ad- ditional results using Llama3-8b, Mistral-7B-v0.2, and LongChat-7B-v1.5, which can be found in Appendix D. NIAH Results. From Figure 4, we observe that KIVI can still maintain the retrieval ability of LLMs even with 2bit KV Cache. Detailed NIAH setting can be found in Appendix B. 4.2.3. A BLATION In this section, we benchmark KIVI on GSM8K, one of the hardest generation tasks, to show the effect of hyperpa- rameters group size G and residual length R on the model performance. For full results of KIVI with a residual length of 32, please refer to Appendix C. The effect of group size. We fix the residual length at 128 and vary the group sizes to 32, 64, and 128. From Table 5, we observe that group sizes 32 and 64 yield similar results, whereas the performance significantly decreases when the group size reaches 128. Note the zero-point and the scaling factor mentioned in Section 3.1 are calculated according to this group size; where the choice of group size will greatly impact the KV cache compression effect under a long input. The effect of residual length. We fix the group size at 32 and vary the residual length across 32, 64, 96, and 128. As shown in Table 5, there is no consistent pattern between residual lengths and model accuracy. Namely, while a resid- ual length of 128 achieves good results, 32 and 96 yield similar outcomes, but a residual length of 64 results in the worst performance. We emphasize that while we observe no significance among residual lengths of {32, 96, 128}, hav- ing a reasonably large residual length is important; as it brings much performance boosts on hard tasks like GSM8K, again as shown in Table 5. 4.2.4. E FFICIENCY COMPARISON To evaluate the wall-clock time efficiency ofKIVI, follow- ing vLLM (Kwon et al., 2023), we synthesize workloads based on ShareGPT (sha, 2023), which contain input and output texts of real LLM services. On average, the data set has an input prompt length lprompt of 161 and an output length lgen of 338 (Kwon et al., 2023). We increase the batch 7KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Table 3: Performance comparison between 16bit, 4-bit per- token quantization, four fake 2bit KV cache quantization similar to those in Table 1, KIVI-2 (2bit) / KIVI-4 (4bit) across various models. We emphasize that unlike KIVI, which preserves a small portion of full precision key cache XKr and value cache XVr , all tokens in fake KV cache quantization are quantized for a fair comparison. C stands for per-channel quantization and T stands for per-token quantization. Model CoQA TruthfulQA GSM8K Llama-2-7B 16bit 63.88 30.76 13.50 4bit (K -T, V -T) 64.82 29.85 12.28 2bit (K -C, V -T) 59.08 33.10 5.76 2bit (K -T, V -T) 39.88 18.29 0.83 2bit (K -C, V -C) 3.60 0.27 0.00 2bit (K -T, V -C) 1.30 0.49 0.08 KIVI-4 63.78 30.80 13.80 KIVI-2 63.05 33.95 12.74 Llama-2-13B 16bit 66.37 29.53 22.67 4bit (K -T, V -T) 66.73 29.14 20.92 2bit (K -C, V -T) 63.53 28.60 12.21 2bit (K -T, V -T) 52.93 24.98 4.55 2bit (K -C, V -C) 2.88 0.74 0.00 2bit (K -T, V -C) 2.80 0.26 0.08 KIVI-4 66.38 29.49 23.65 KIVI-2 66.23 29.84 20.77 Falcon-7B 16bit 59.83 23.20 4.55 4bit (K -T, V -T) 58.53 22.94 3.26 2bit (K -C, V -T) 43.93 20.82 1.29 2bit (K -T, V -T) 25.72 0.91 0.53 2bit (K -C, V -C) 41.95 17.11 1.52 2bit (K -T, V -C) 19.53 0.94 0.15 KIVI-4 59.67 22.58 4.47 KIVI-2 57.48 24.98 3.41 Mistral-7B 16bit 67.40 30.45 38.36 4bit (K -T, V -T) 67.80 29.83 36.85 2bit (K -C, V -T) 61.65 29.64 26.46 2bit (K -T, V -T) 54.55 25.86 5.00 2bit (K -C, V -C) 24.40 24.86 2.27 2bit (K -T, V -C) 10.73 19.12 0.99 KIVI-4 66.95 30.49 37.30 KIVI-2 66.35 32.17 36.01 size until out of memory and report the peak memory usage and throughput between KIVI (with residual length 32 and 128) and FP16 baseline for the Llama-2-7B model. The hardware here is a single NVIDIA A100 GPU (80GB). As shown in Figure 5, with similar maximum memory us- age, KIVI enables up to 4× larger batch size and gives 2.35× ∼3.47× larger throughput. This throughput num- ber can grow larger with longer context length and output length. We also note that this speed-up can be greatly increased if we further fuse the KV cache quantization process with previous operations. We leave it as one of future work. Figure 5: Memory usage and throughput comparison be- tween 2bit KIVI and 16bit baseline. KIVI can achieve higher throughput by enabling a larger batch size. 5. Related Work Many machine learning systems and benchmark works con- sider scaling up LLM inference process (Pope et al., 2023; Yuan et al., 2024). Among them, quantization techniques have been widely applied (Frantar et al., 2022; Lin et al., 2023; Kim et al., 2023; Xu et al., 2023). A main branch of LLM quantization is weight-only quantization, which in- volves the quantization of model weights to lower precision. For instance, AWQ (Lin et al., 2023) cleverly quantizes model weights to INT4 and INT3 using an activation-aware manner. GPTQ (Frantar et al., 2022) utilizes approximate second-order information to quantize model weights both accurately and efficiently. SqueezeLLM (Kim et al., 2023) adopts the concept of non-uniform quantization based on sensitivity along with dense-and-sparse decomposition. This line of work is orthogonal to ours, as they can be combined. SmoothQuant (Xiao et al., 2023a) is a post-training quan- tization method that is more closely related to our work. This method uses equivalent transformations to balance the quantization complexity for both activation and weight, making the activation easier to quantize. SmoothQuant can compress KV cache to 8bit with minor performance loss. However, it faces a significant accuracy drop when scaled down to 4bit or less (Zhao et al., 2024). FlexGen (Sheng et al., 2023) adopts 4-bit group-wise quantization for both key and value cache. 8KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Table 4: Performance evaluation of KIVI on various models across a range of benchmarks in LongBench. We highlight the average performance of our method. More similar results on Mistral-7B-v0.2 and LongChat-7b-v1.5 can be found in Table 10 and Table 9 Model Qasper QMSum MultiNews TREC TriviaQA SAMSum LCC RepoBench-P Average Llama2-7B 16bit 9.52 21.28 3.51 66.00 87.72 41.69 66.66 59.82 44.52 KIVI-4 9.28 21.42 3.88 66.00 87.72 41.82 66.80 59.83 44.59 KIVI-2 9.31 20.50 1.14 66.00 87.42 42.71 66.88 60.23 44.27 Llama2-13B 16bit 9.32 21.38 3.71 70.00 87.87 43.55 66.61 56.42 44.85 KIVI-4 9.16 20.86 3.21 69.00 86.97 44.26 65.30 57.08 44.48 KIVI-2 8.58 20.69 6.19 69.50 87.78 44.30 65.08 55.46 44.69 Llama2-7B-Chat 16bit 19.65 20.54 26.36 63.00 84.28 41.12 59.75 52.93 45.95 KIVI-4 19.62 20.70 25.49 63.00 84.13 40.87 59.27 53.56 45.83 KIVI-2 19.32 20.46 25.48 63.00 84.84 40.60 58.71 52.97 45.67 Llama2-13B-Chat 16bit 24.18 20.37 25.69 67.50 86.90 42.18 50.23 50.64 45.96 KIVI-4 23.00 20.36 26.06 67.50 87.20 42.04 52.55 52.77 46.44 KIVI-2 23.59 20.76 25.25 67.5 87.17 41.56 49.93 48.45 45.52 Falcon-7B 16bit 1.48 2.35 11.09 13.00 5.84 2.44 23.86 9.69 8.71 KIVI-4 1.04 2.41 11.98 13.00 5.84 2.36 23.72 9.92 8.78 KIVI-2 1.98 3.61 6.78 10.00 6.24 2.73 22.18 10.12 7.95 Mistral-7B 16bit 8.12 19.98 19.99 67.50 89.80 41.69 66.59 58.99 46.58 KIVI-4 7.89 20.06 20.58 67.50 89.80 41.56 66.45 58.62 46.56 KIVI-2 6.92 19.71 17.92 66.50 89.63 41.66 65.52 58.99 45.85 Table 5: Ablation study of KIVI by changing group size G and residual length R. Model Group Size GSM8K Llama2-13B 32 20.77 64 21.00 128 17.29 Model Residual Length GSM8K Llama2-13B 32 20.62 64 19.86 96 20.55 128 20.77 One essential recipe of KIVI is the per-channel quantiza- tion scheme designed based on the observation made in Section 3.2 and Figure 2. ATOM (Zhao et al., 2024) also indicates that key cache exhibits more outliers compared to the value cache. KIVI provides further extensive analysis and leverages this observation to implement per-channel quantization. A similar observation and approach has been independently discovered and developed in the concurrent work KVQuant (Hooper et al., 2024). vLLM (Kwon et al., 2023) and S3 (Jin et al., 2023) are system-level works, which include memory management through the use of PagedAttention or memory usage pre- diction. They can lower the memory requirements of KV cache and simultaneously increase model throughput. This research direction is orthogonal to our work, since system- level optimizations can also be applied upon our algorithm. Several other works also consider compressing KV cache by evicting tokens. H2O (Zhang et al., 2023) retains only a small portion of tokens that contribute significantly to the attention scores. Similarly, Scissorhands (Liu et al., 2024) exploit the persistence of the importance hypothesis in KV cache sparsification. StreamingLLM (Xiao et al., 2023b) is based on the observation of “attention sink” and maintains only a few initial tokens to preserve performance. Unlike these works, our KIVI retains all input tokens and compresses them into lower precision. This line of work is orthogonal to ours, as they can also be combined together. 6. Conclusion and Future Work In this paper, we systematically analyze KV cache element distribution in popular LLMs. We conclude that key cache should be quantized per-channel and value cache should be quantized per token. Based on these observations, we propose KIVI, a plug-and-play 2bit KV cache quantization algorithm without the need for any tuning. In real LLM workload, KIVI allows up to 4× larger batch sizes and 3.47× throughput. In the future, we will further optimize the implementation to reduce the overhead of quantization process during the prefill and decoding phase. Acknowledgments The authors thank the anonymous reviewers for their help- ful comments. Dr. Beidi Chen is supported by a research gift from Moffett.AI. Dr. Vladimir Braverman is partially 9KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache supported by the Ministry of Trade, Industry and Energy (MOTIE) and Korea Institute for Advancement of Technol- ogy (KIAT) through the International Cooperative R&D pro- gram, the Naval Research (ONR) grant N00014-23-1-2737, and NSF CNS 2333887 award. Dr. Xia Hu is supported by NSF grants NSF IIS-2224843. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. References ShareGPT Team. https://sharegpt.com/, 2023. Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan- guage models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. Yu-Neng Chuang, Songchen Li, Jiayi Yuan, Guanchu Wang, Kwei-Herng Lai, Leisheng Yu, Sirui Ding, Chia-Yuan Chang, Qiaoyu Tan, Daochen Zha, et al. Understand- ing different design choices in training large time series models. arXiv preprint arXiv:2406.14045, 2024. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, An- thony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al. A frame- work for few-shot language model evaluation. Version v0. 0.1. Sept, page 8, 2021. Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W Mahoney, Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cache quantization. arXiv preprint arXiv:2401.18079, 2024. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Yunho Jin, Chun-Feng Wu, David Brooks, and Gu-Yeon Wei. S3: Increasing gpu utilization during genera- tive inference for higher throughput. arXiv preprint arXiv:2306.06000, 2023. Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611–626, 2023. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023. Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and An- shumali Shrivastava. Scissorhands: Exploiting the per- sistence of importance hypothesis for llm kv cache com- pression at test time. Advances in Neural Information Processing Systems, 36, 2024. Amirkeivan Mohtashami and Martin Jaggi. Landmark at- tention: Random-access infinite context length for trans- formers. arXiv preprint arXiv:2305.16300, 2023. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobei- dli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outper- forming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023. 10KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Ja- cob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling trans- former inference. Proceedings of Machine Learning and Systems, 5, 2023. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrit- twieser, et al. Gemini 1.5: Unlocking multimodal un- derstanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150, 2019. Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. Flexgen: High-throughput generative inference of large language models with a single gpu. In International Conference on Machine Learning, pages 31094–31116. PMLR, 2023. Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022. Yuandong Tian, Yiping Wang, Beidi Chen, and Simon Du. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. arXiv preprint arXiv:2305.16380, 2023. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar- tinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and ef- ficient post-training quantization for large language mod- els. In International Conference on Machine Learning, pages 38087–38099. PMLR, 2023a. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023b. Zhaozhuo Xu, Zirui Liu, Beidi Chen, Yuxin Tang, Jue Wang, Kaixiong Zhou, Xia Hu, and Anshumali Shrivastava. Compress, then prompt: Improving accuracy-efficiency trade-off of llm inference with transferable prompt. arXiv preprint arXiv:2305.11186, 2023. Jiayi Yuan, Ruixiang Tang, Xiaoqian Jiang, and Xia Hu. Large language models for healthcare data augmentation: An example on patient-trial matching. In AMIA Annual Symposium Proceedings, volume 2023, page 1324. Amer- ican Medical Informatics Association, 2023. Jiayi Yuan, Hongyi Liu, Shaochen Zhong, Yu-Neng Chuang, Songchen Li, Guanchu Wang, Duy Le, Hongye Jin, Vipin Chaudhary, Zhaozhuo Xu, Zirui Liu, and Xia Hu. Kv cache compression, but what must we give in return? a comprehensive benchmark of long context capable ap- proaches. arXiv preprint arXiv:2407.01527, 2024. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H _2 o: Heavy-hitter oracle for efficient generative inference of large language models. arXiv preprint arXiv:2306.14048, 2023. Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom: Low-bit quanti- zation for efficient and accurate llm serving. Proceedings of Machine Learning and Systems, 6:196–209, 2024. 11KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache A. Detailed Implementations In this section, we present the algorithm for KIVI as discussed in Section 3.3. Specifically, we provide the pseudocode for KIVI when calculating the attention output in the prefill and decoding phases. Algorithm 1: The KIVI Prefill & Decoding Algorithm parameter: group size G, residual length R procedure Prefill: Input: X ∈ Rlprompt×d XK = XWK, XV = XWV XVg = XV [: lprompt − R], XVr = XV [lprompt − R :] Q(XVg ) ← GroupQuant(XVg , dim=token, numGroup=d//G) Q(XKg ), XKr ← KeyQuant(XK) KV cache ← Q(XKg ), XKr , Q(XVg ), XVr return XK, XV end procedure Decoding: Input: KV cache, t ∈ R1×d tQ = tWQ, tK = tWK, tV = tWV Q(XKg ), XKr , Q(XVg ), XVr ← KV cache XKr ← Concat([XKr , tK], dim=token) XVr ← Concat([XVr , tV ], dim=token) if len(XKr ) = R then Q(XKr ), _ ←KeyQuant(XKr ) Q(XKg ) ← Concat([Q(XKg ), Q(XKr )], dim=token) XKr ← empty tensor. end if len(XVr ) > Rthen Q(XV ′r ) ← GroupQuant(XVr [: −R], dim=token, numGroup = d//G) Q(XVg ) ← Concat([Q(XVg ), Q(XV ′r )], dim=token) XVr ← XVr [−R :] end A ← Concat([tQQ(XKg )⊤, tQX⊤ Kr ], dim=token) Ag = Softmax(A)[: −R], Ar = Softmax(A)[−R :] tO ← AgQ(XVg ) + ArXVr KV cache ← Q(XKg ), XKr , Q(XVg ), XVr return tO end function KeyQuant(XK ∈ Rl×d): r = l%R, XKg = XK[: l − r], XKr = XK[l − r :] Q(XKg ) ← GroupQuant(XKg , dim=channel, numGroup=l//G) return Q(XKg ), XKr end 12KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache B. NIAH Setting We largely follows the passkey retrieval prompt template of Mohtashami and Jaggi (2023) but using 7-digit passkey and Paul Graham Essays2 as the background filler, as set forth in Arize-ai and Reid et al. (2024): There is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I will quiz you about the important information there. <prefix filled by Paul Graham Essays> The pass key is <7-DIGIT PASS KEY>. Remember it. <7-DIGIT PASS KEY> is the pass key. <suffix filler> What is the pass key? The pass key is C. More Ablation Results In our efficiency evaluation, we observe that with a residual length of 32, KIVI achieves a significantly higher memory compression rate, which in turn leads to increased throughput. Additionally, our ablation study reveals that changing the residual length from 128 to 32 does not result in a substantial performance gap. We demonstrate KIVI with a residual length of 32 across all benchmark datasets. As shown in Tables 6 and 7, KIVI with a residual length of 32 also delivers performance comparable to that of the 16-bit full model. Table 6: Performance comparison between 16bit, KIVI-2 (2bit) / KIVI-4 (4bit) with residual length 128 and 32 across various models. R32 stands for residual length 32. Model CoQA TruthfulQA GSM8K Llama-2-7B 16bit 63.88 30.76 13.50 KIVI-2 R128 63.05 33.95 12.74 KIVI-2 R32 62.85 33.01 13.57 Llama-2-13B 16bit 66.37 29.53 22.67 KIVI-2 R128 66.23 29.84 20.77 KIVI-2 R32 66.57 29.35 20.62 Falcon-7B 16bit 59.83 23.20 4.55 KIVI-4 R128 59.67 22.58 4.47 KIVI-4 R32 59.73 22.96 3.94 KIVI-2 R128 57.48 24.98 3.41 KIVI-2 R32 57.50 25.70 2.20 Mistral-7B 16bit 67.40 30.45 38.36 KIVI-2 R128 66.35 32.17 36.01 KIVI-2 R32 65.90 31.21 34.34 D. More Experimental Results We present additional results using Llama3-8B, Mistral-7B-v0.2, and LongChat-7B-v1.5 in LongBench, which can be found in Table 8, Table 9 and Table 10, respectively. We also show result of Needle-in-a-Haystack Test in Figure 4. The settings largely follow the format of the original passkey retrieval task (Mohtashami and Jaggi, 2023) while including some modern modifications set forward by Arize-ai and the technical report of Gemini 1.5 (Reid et al., 2024). 2https://paulgraham.com/articles.html 13KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Table 7: Performance evaluation of KIVI with residual length 128 and 32 on various models across a range of benchmarks in LongBench. R32 stands for residual length 32. Model Qasper QMSum MultiNews TREC TriviaQA SAMSum LCC RepoBench-P Average Llama2-7B 16bit 9.52 21.28 3.51 66.00 87.72 41.69 66.66 59.82 44.52 KIVI-2R128 9.31 20.50 1.14 66.00 87.42 42.71 66.88 60.23 44.27 KIVI-2R32 9.26 20.53 0.97 66.00 87.42 42.61 66.22 59.67 44.08 Llama2-13B 16bit 9.32 21.38 3.71 70.00 87.87 43.55 66.61 56.42 44.85 KIVI-2R128 8.58 20.69 6.19 69.50 87.78 44.30 65.08 55.46 44.69 KIVI-2R32 8.38 20.74 7.01 69.50 87.78 44.43 64.89 55.31 44.75 Llama2-7B-Chat 16bit 19.65 20.54 26.36 63.00 84.28 41.12 59.75 52.93 45.95 KIVI-2R128 19.32 20.46 25.48 63.00 84.84 40.60 58.71 52.97 45.67 KIVI-2R32 19.10 20.08 25.33 63.00 85.04 39.80 57.91 52.38 45.33 Llama2-13B-Chat 16bit 24.18 20.37 25.69 67.50 86.90 42.18 50.23 50.64 45.96 KIVI-2R128 23.59 20.76 25.25 67.50 87.17 41.56 49.93 48.45 45.52 KIVI-2R32 23.56 20.90 25.45 67.50 87.42 41.40 48.93 48.81 45.49 Falcon-7B 16bit 1.48 2.35 11.09 13.00 5.84 2.44 23.86 9.69 8.71 KIVI-4R128 1.04 2.41 11.98 13.00 5.84 2.36 23.72 9.92 8.78 KIVI-4R32 1.03 2.45 11.99 13.50 5.84 2.46 23.88 9.95 8.88 KIVI-2R128 1.98 3.61 6.78 10.00 6.24 2.73 22.18 10.12 7.95 KIVI-2R32 2.28 3.23 6.73 10.00 6.31 2.88 22.71 10.45 8.07 Mistral-7B 16bit 8.12 19.98 19.99 67.50 89.80 41.69 66.59 58.99 46.58 KIVI-2R128 6.92 19.71 17.92 66.50 89.63 41.66 65.52 58.99 45.85 KIVI-2R32 6.84 19.81 17.20 66.50 89.63 42.82 65.13 58.06 45.74 Table 8: The results of Llama-3-8B-Instruct with KIVI on LongBench. The model has 8K context length and applies group query attention, which uses 8 heads for KV cache instead of the full 32 heads. We use a 32 group size and 128 residual length for both KIVI-2 and KIVI-4. The baseline is of full precision. NarrativeQA Qasper MultiFieldQA HotpotQA MuSiQue 2WikiMQA GovReport QMSum Baseline 21.71 44.24 44.54 46.82 21.49 36.42 30.03 22.67 w./KIVI-2 21.35 43.17 44.49 46.79 20.56 37.05 29.98 22.07 w./KIVI-4 21.01 44.83 44.60 46.96 21.43 36.48 30.22 22.44 MultiNews LCC RepoBench-P TriviaQA SAMSum TRec PR Avg Baseline 27.79 57.00 51.22 90.23 42.53 74.50 67.00 45.21 w./KIVI-2 27.77 50.84 46.65 90.54 42.26 74.50 67.50 44.37 w./KIVI-4 27.97 57.36 52.03 90.33 42.97 74.50 66.50 45.31 Table 9: The results of Mistral-7B-Instruct-v0.2 with KIVI on LongBench. The model has 32K context length and applies group query attention, which uses 8 heads for KV cache instead of the full 32 heads. We use a 32 group size and 128 residual length for both KIVI-2 and KIVI-4. The baseline is of full precision. NarrativeQA Qasper MultiFieldQA HotpotQA MuSiQue 2WikiMQA GovReport QMSum Baseline 21.02 29.41 47.13 36.53 19.13 21.76 32.59 23.99 w./KIVI-2 20.61 28.73 44.88 35.47 17.95 20.68 32.55 23.65 w./KIVI-4 20.97 29.41 46.52 36.25 19.53 21.66 32.97 24.06 MultiNews LCC RepoBench-P TriviaQA SAMSum TRec PR Avg Baseline 27.09 53.49 51.40 86.23 43.04 71.00 89.33 43.54 w./KIVI-2 26.54 53.03 51.16 86.00 43.34 71.00 80.83 42.43 w./KIVI-4 26.89 53.33 51.41 86.23 43.34 71.00 89.42 43.53 14KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Table 10: The results of LongChat-7B-v1.5-32K with KIVI on LongBench. The model has 32K context length. We use a 32 group size and 128 residual length for both KIVI-2 and KIVI-4. The baseline is of full precision. NarrativeQA Qasper MultiFieldQA HotpotQA MuSiQue 2WikiMQA GovReport QMSum Baseline 20.65 29.42 43.15 33.05 14.66 24.14 30.85 22.84 w./KIVI-2 20.79 28.69 41.02 32.91 13.82 23.00 30.47 22.59 w./KIVI-4 20.49 28.90 43.24 33.07 14.66 24.86 31.40 22.84 MultiNews LCC RepoBench-P TriviaQA SAMSum TRec PR Avg Baseline 26.55 54.83 58.94 83.99 40.75 66.50 30.50 38.72 w./KIVI-2 26.28 54.11 57.62 83.19 41.28 66.50 32.25 38.30 w./KIVI-4 26.52 54.06 58.77 83.88 40.62 67.00 31.50 38.79 15",
      "meta_data": {
        "arxiv_id": "2402.02750v2",
        "doi": "10.13140/RG.2.2.28167.37282",
        "authors": [
          "Zirui Liu",
          "Jiayi Yuan",
          "Hongye Jin",
          "Shaochen Zhong",
          "Zhaozhuo Xu",
          "Vladimir Braverman",
          "Beidi Chen",
          "Xia Hu"
        ],
        "published_date": "2024-02-05T06:06:47Z",
        "pdf_url": "https://arxiv.org/pdf/2402.02750v2.pdf",
        "github_url": "https://github.com/jy-yuan/KIVI"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper identifies the KV cache as a significant memory and speed bottleneck in large language models (LLMs). It conducts a comprehensive study on KV cache element distribution, revealing that key cache should be quantized per-channel due to large-magnitude outliers, while value cache should be quantized per-token to confine errors effectively due to attention sparsity. Based on these insights, the authors propose KIVI, a tuning-free asymmetric 2-bit KV cache quantization algorithm. KIVI includes a hardware-friendly implementation and maintains almost full quality for Llama, Falcon, and Mistral models. It achieves a 2.6x reduction in peak memory usage, enabling up to 4x larger batch sizes and delivering 2.35x to 3.47x higher throughput on real LLM inference workloads.",
        "methodology": "The methodology involves an initial study of KV cache element distributions, demonstrating the need for different quantization strategies for key and value caches. Key cache is found to have fixed channels with large magnitudes, making per-channel quantization effective in confining errors. Value cache, lacking such outlier patterns, benefits from per-token quantization to localize errors due to the sparse nature of attention scores. The KIVI algorithm implements this asymmetric quantization: key cache is quantized per-channel, and value cache is quantized per-token. To support the streaming nature of auto-regressive inference, KIVI splits the KV cache into two parts: a grouped part, which is quantized, and a residual part (a full-precision sliding window of the most recent R tokens) that remains in full precision to preserve accuracy. System support includes hardware-friendly implementations with fused dequantization and matrix multiplication (Q_MatMul using CUDA) and group-wise quantization kernels implemented in Triton. This design is compatible with weight-only quantization.",
        "experimental_setup": "The evaluation utilized popular LLM families including Llama/Llama-2 (7B, 13B), Falcon-7B, and Mistral-7B, with additional results for Llama3-8B-Instruct, Mistral-7B-Instruct-v0.2, and LongChat-7B-v1.5. Key hyperparameters included a group size (G) of 32 for quantization and a residual length (R) of 128 for the full-precision sliding window, with ablation studies performed for varying G and R values. Performance was assessed across several benchmarks: LM-Eval for normal context lengths (CoQA, TruthfulQA, GSM8K) and LongBench for long context settings (Qasper, QMSum, MultiNews, TREC, TriviaQA, SAMSum, LCC, RepoBench-P). The Needle-in-a-Haystack (NIAH) task was also used to evaluate long context retrieval ability. All experiments were conducted on a single NVIDIA A100 GPU (80GB). Efficiency was measured using peak memory usage and throughput on synthesized ShareGPT workloads.",
        "limitations": "The paper notes that for models like Falcon-7B, which already employ multi-query attention (effectively compressing KV cache), KIVI requires 4-bit quantization to maintain accuracy, as 2-bit quantization can lead to a notable accuracy drop. It is also mentioned that while KIVI achieves significant speed-ups, the throughput could be further increased by more deeply fusing the KV cache quantization process with prior operations, implying current implementation is not maximally optimized. The memory overhead from the full-precision residual KV cache parts is described as negligible for long contexts, but it's not zero, suggesting a potential minor impact in specific, very short context scenarios, though not explicitly detailed as a limitation.",
        "future_research_directions": "Future work will focus on further optimizing the implementation to reduce the overhead associated with the quantization process during both the prefill and decoding phases of LLM inference. This includes exploring more advanced fusion of the KV cache quantization with previous operations to achieve even greater speed-ups.",
        "experimental_code": "import math\nimport warnings\nfrom typing import List, Optional, Tuple\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom quant.new_pack import triton_quantize_and_pack_along_last_dim\nfrom quant.matmul import cuda_bmm_fA_qB_outer\n\nfrom transformers.models.llama.configuration_llama import *\nfrom transformers.models.llama.modeling_llama import *\nfrom transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask\n\n_CONFIG_FOR_DOC = \"LlamaConfig\"\n\n\nclass LlamaAttention_KIVI(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: LlamaConfig):\n        super().__init__()\n        self.config = config\n        self.attention_dropout = config.attention_dropout\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.is_causal = True\n        self.k_bits = config.k_bits\n        self.v_bits = config.v_bits\n        self.group_size = config.group_size\n        self.residual_length = config.residual_length\n        assert getattr(config, \"use_flash\", False), \"currently KIVI is only available for flash-attn. Please add ```config.use_flash = True```\"\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n        self.rotary_emb = LlamaRotaryEmbedding(config=self.config)\n\n\n    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        if \"padding_mask\" in kwargs:\n            warnings.warn(\n                \"Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`\"\n            )\n        bsz, q_len, _ = hidden_states.size()\n\n        if self.config.pretraining_tp > 1:\n            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n            query_slices = self.q_proj.weight.split(\n                (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n            )\n            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n\n            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n            query_states = torch.cat(query_states, dim=-1)\n\n            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n            key_states = torch.cat(key_states, dim=-1)\n\n            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n            value_states = torch.cat(value_states, dim=-1)\n\n        else:\n            query_states = self.q_proj(hidden_states)\n            key_states = self.k_proj(hidden_states)\n            value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        kv_seq_len = key_states.shape[-2]\n        if past_key_value is not None:\n            kv_seq_len += past_key_value[-1]\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n        assert self.num_key_value_groups == 1\n        # [bsz, nh, t, hd]\n        if past_key_value is not None:\n            key_states_quant_trans = past_key_value[0]\n            key_states_full = past_key_value[1]\n            key_scale_trans = past_key_value[2]\n            key_mn_trans = past_key_value[3]\n            value_states_quant = past_key_value[4]\n            value_states_full = past_key_value[5]\n            value_scale = past_key_value[6]\n            value_mn = past_key_value[7]\n\n            if key_states_quant_trans is not None:\n                att_qkquant = cuda_bmm_fA_qB_outer(self.group_size, query_states, key_states_quant_trans, \n                                key_scale_trans, key_mn_trans, self.k_bits)\n            else:\n                att_qkquant = None\n\n            if key_states_full is not None:\n                key_states_full = torch.cat([key_states_full, key_states], dim=2)\n            else:\n                key_states_full = key_states\n            att_qkfull = torch.matmul(query_states, key_states_full.transpose(2, 3))\n            if att_qkquant is not None:\n                attn_weights = torch.cat([att_qkquant, att_qkfull], dim=-1) / math.sqrt(self.head_dim)\n            else:\n                attn_weights = att_qkfull / math.sqrt(self.head_dim)\n\n            if key_states_full.shape[-2] == self.residual_length:\n                assert self.residual_length % self.group_size == 0\n                key_states_quant_trans_new, key_scale_trans_new, key_mn_trans_new = triton_quantize_and_pack_along_last_dim(key_states_full.transpose(2, 3).contiguous(), \n                                                                                                                            self.group_size, \n                                                                                                                            self.k_bits)\n                key_states_full = None\n                if key_states_quant_trans is not None:\n                    key_states_quant_trans = torch.cat([key_states_quant_trans, key_states_quant_trans_new], dim=3)\n                    key_scale_trans = torch.cat([key_scale_trans, key_scale_trans_new], dim=3)\n                    key_mn_trans = torch.cat([key_mn_trans, key_mn_trans_new], dim=3)\n                else:\n                    key_states_quant_trans = key_states_quant_trans_new\n                    key_scale_trans = key_scale_trans_new\n                    key_mn_trans = key_mn_trans_new\n\n            if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n                    f\" {attn_weights.size()}\"\n                )\n\n            if attention_mask is not None:\n                if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                    raise ValueError(\n                        f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                    )\n                attn_weights = attn_weights + attention_mask\n                attn_weights = torch.max(\n                    attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min)\n                )\n\n            # upcast attention to fp32\n            attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n\n            value_states_full = torch.cat([value_states_full, value_states], dim=2)\n            value_full_length = value_states_full.shape[-2]\n            if value_states_quant is None:\n                attn_output = torch.matmul(attn_weights, value_states_full)\n            else:\n                attn_output = cuda_bmm_fA_qB_outer(self.group_size, attn_weights[:, :, :, :-value_full_length], value_states_quant, \n                                                value_scale, value_mn, self.v_bits)\n                attn_output += torch.matmul(attn_weights[:, :, :, -value_full_length:], value_states_full)\n            \n            if value_full_length > self.residual_length:\n                assert value_full_length == self.residual_length + 1\n                value_states_quant_new, scale, mn = triton_quantize_and_pack_along_last_dim(value_states_full[:, :, :1, :].contiguous(), \n                                                                                                self.group_size, \n                                                                                                self.v_bits)\n                value_states_full = value_states_full[:, :, 1:, :].contiguous()\n                if value_states_quant is not None:\n                    value_states_quant = torch.cat([value_states_quant, value_states_quant_new], dim=2)\n                    value_scale = torch.cat([value_scale, scale], dim=2)\n                    value_mn = torch.cat([value_mn, mn], dim=2)\n                else:\n                    value_states_quant = value_states_quant_new\n                    value_scale = scale\n                    value_mn = mn\n\n        else:\n            attn_weights = torch.matmul(query_states, \n                                        key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n            # quantize\n            if key_states.shape[-2] % self.residual_length != 0:\n                if key_states.shape[-2] < self.residual_length:\n                    key_states_quant = None\n                    key_states_full = key_states\n                else:\n                    key_states_quant = key_states[:, :, :-(key_states.shape[-2] % self.residual_length), :].contiguous()\n                    key_states_full = key_states[:, :, -(key_states.shape[-2] % self.residual_length):, :].contiguous()\n            else:\n                key_states_quant = key_states\n                key_states_full = None\n            if key_states_quant is not None:\n                key_states_quant_trans, key_scale_trans, key_mn_trans = triton_quantize_and_pack_along_last_dim(key_states_quant.transpose(2, 3).contiguous(), self.group_size, self.k_bits)\n            else:\n                key_states_quant_trans = None\n                key_scale_trans = None\n                key_mn_trans = None\n            \n            if value_states.shape[-2] <= self.residual_length:\n                value_states_quant = None\n                value_states_full = value_states\n                value_scale = None\n                value_mn = None\n            else:\n                value_states_quant = value_states[:, :, :-self.residual_length, :].contiguous()\n                value_states_full = value_states[:, :, -self.residual_length:, :].contiguous()\n                value_states_quant, value_scale, value_mn = triton_quantize_and_pack_along_last_dim(value_states_quant, \n                                                                                                self.group_size, \n                                                                                                self.v_bits)\n\n            if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n                    f\" {attn_weights.size()}\"\n                )\n\n            if attention_mask is not None:\n                if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                    raise ValueError(\n                        f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                    )\n                attn_weights = attn_weights + attention_mask\n                attn_weights = torch.max(\n                    attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min)\n                )\n\n            # upcast attention to fp32\n            attn_weights = nn.functional.softmax(\n                attn_weights, dim=-1, dtype=torch.float32\n            ).to(query_states.dtype)\n\n            attn_output = torch.matmul(attn_weights, value_states) \n        past_key_value = (key_states_quant_trans, key_states_full, key_scale_trans, key_mn_trans, value_states_quant, value_states_full, value_scale, value_mn, kv_seq_len) if use_cache else None\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        if self.config.pretraining_tp > 1:\n            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n        else:\n            attn_output = self.o_proj(attn_output)\n\n        attn_weights = None\n        return attn_output, attn_weights, past_key_value\n    \nclass LlamaFlashAttention_KIVI(LlamaAttention_KIVI):\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        if \"padding_mask\" in kwargs:\n            warnings.warn(\n                \"Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`\"\n            )\n        bsz, q_len, _ = hidden_states.size()\n\n        if self.config.pretraining_tp > 1:\n            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n            query_slices = self.q_proj.weight.split(\n                (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n            )\n            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n\n            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n            query_states = torch.cat(query_states, dim=-1)\n\n            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n            key_states = torch.cat(key_states, dim=-1)\n\n            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n            value_states = torch.cat(value_states, dim=-1)\n\n        else:\n            query_states = self.q_proj(hidden_states)\n            key_states = self.k_proj(hidden_states)\n            value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        kv_seq_len = key_states.shape[-2]\n        if past_key_value is not None:\n            kv_seq_len += past_key_value[-1]\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n        # assert self.num_key_value_groups == 1\n        # [bsz, nh, t, hd]\n        if past_key_value is not None:\n            key_states_quant_trans = past_key_value[0]\n            key_states_full = past_key_value[1]\n            key_scale_trans = past_key_value[2]\n            key_mn_trans = past_key_value[3]\n            value_states_quant = past_key_value[4]\n            value_states_full = past_key_value[5]\n            value_scale = past_key_value[6]\n            value_mn = past_key_value[7]\n            if key_states_quant_trans is not None:\n                att_qkquant = cuda_bmm_fA_qB_outer(self.group_size, query_states, key_states_quant_trans, \n                                key_scale_trans, key_mn_trans, self.k_bits)\n                # att_qkquant_ref = triton_bmm_fA_qB_outer(self.group_size, query_states, key_states_quant_trans, \n                #                 key_scale_trans, key_mn_trans, self.k_bits)\n                # error = torch.abs(att_qkquant - att_qkquant_ref).float()\n                # rel_error = torch.mean(error / (torch.abs(att_qkquant_ref).float()+1e-5))\n                # print(f\"rel error: {rel_error}\")\n            else:\n                att_qkquant = None\n            if key_states_full is not None:\n                key_states_full = torch.cat([key_states_full, key_states], dim=2)\n            else:\n                key_states_full = key_states\n            att_qkfull = torch.matmul(query_states, repeat_kv(key_states_full, self.num_key_value_groups).transpose(2, 3))\n            if att_qkquant is not None:\n                attn_weights = torch.cat([att_qkquant, att_qkfull], dim=-1) / math.sqrt(self.head_dim)\n            else:\n                attn_weights = att_qkfull / math.sqrt(self.head_dim)\n\n            if key_states_full.shape[-2] == self.residual_length:\n                assert self.residual_length % self.group_size == 0\n                key_states_quant_trans_new, key_scale_trans_new, key_mn_trans_new = triton_quantize_and_pack_along_last_dim(key_states_full.transpose(2, 3).contiguous(), \n                                                                                                                            self.group_size, \n                                                                                                                            self.k_bits)\n                key_states_full = None\n                if key_states_quant_trans is not None:\n                    key_states_quant_trans = torch.cat([key_states_quant_trans, key_states_quant_trans_new], dim=3)\n                    key_scale_trans = torch.cat([key_scale_trans, key_scale_trans_new], dim=3)\n                    key_mn_trans = torch.cat([key_mn_trans, key_mn_trans_new], dim=3)\n                else:\n                    key_states_quant_trans = key_states_quant_trans_new\n                    key_scale_trans = key_scale_trans_new\n                    key_mn_trans = key_mn_trans_new\n\n            if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n                    f\" {attn_weights.size()}\"\n                )\n\n            if attention_mask is not None:\n                if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                    raise ValueError(\n                        f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                    )\n                attn_weights = attn_weights + attention_mask\n                attn_weights = torch.max(\n                    attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min)\n                )\n\n            # upcast attention to fp32\n            attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n\n            value_states_full = torch.cat([value_states_full, value_states], dim=2)\n            value_full_length = value_states_full.shape[-2]\n            if value_states_quant is None:\n                attn_output = torch.matmul(attn_weights, value_states_full)\n            else:\n                attn_output = cuda_bmm_fA_qB_outer(self.group_size, attn_weights[:, :, :, :-value_full_length], value_states_quant, \n                                                value_scale, value_mn, self.v_bits)\n                attn_output += torch.matmul(attn_weights[:, :, :, -value_full_length:], repeat_kv(value_states_full, self.num_key_value_groups))\n            attn_output = attn_output.transpose(1, 2).contiguous()\n            if value_full_length > self.residual_length:\n                assert value_full_length == self.residual_length + 1\n                value_states_quant_new, scale, mn = triton_quantize_and_pack_along_last_dim(value_states_full[:, :, :1, :].contiguous(), \n                                                                                                self.group_size, \n                                                                                                self.v_bits)\n                value_states_full = value_states_full[:, :, 1:, :].contiguous()\n                if value_states_quant is not None:\n                    value_states_quant = torch.cat([value_states_quant, value_states_quant_new], dim=2)\n                    value_scale = torch.cat([value_scale, scale], dim=2)\n                    value_mn = torch.cat([value_mn, mn], dim=2)\n                else:\n                    value_states_quant = value_states_quant_new\n                    value_scale = scale\n                    value_mn = mn\n\n        else:\n            # print(f\"kivi with flash! {self.k_bits}\")\n            input_dtype = query_states.dtype\n            if input_dtype == torch.float32:\n                # Handle the case where the model is quantized\n                if hasattr(self.config, \"_pre_quantization_dtype\"):\n                    target_dtype = self.config._pre_quantization_dtype\n                else:\n                    target_dtype = self.q_proj.weight.dtype\n\n                logger.warning_once(\n                    f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n                    f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n                    f\" {target_dtype}.\"\n                )\n\n                query_states = query_states.to(target_dtype)\n                key_states = key_states.to(target_dtype)\n                value_states = value_states.to(target_dtype)\n            attn_output = self._flash_attention_forward(\n                query_states.transpose(1, 2), key_states.transpose(1, 2), \n                value_states.transpose(1, 2), None, q_len, dropout=0.0\n            )\n            # quantize\n            if key_states.shape[-2] % self.residual_length != 0:\n                if key_states.shape[-2] < self.residual_length:\n                    key_states_quant = None\n                    key_states_full = key_states\n                else:\n                    key_states_quant = key_states[:, :, :-(key_states.shape[-2] % self.residual_length), :].contiguous()\n                    key_states_full = key_states[:, :, -(key_states.shape[-2] % self.residual_length):, :].contiguous()\n            else:\n                key_states_quant = key_states\n                key_states_full = None\n            if key_states_quant is not None:\n                key_states_quant_trans, key_scale_trans, key_mn_trans = triton_quantize_and_pack_along_last_dim(key_states_quant.transpose(2, 3).contiguous(), self.group_size, self.k_bits)\n            else:\n                key_states_quant_trans = None\n                key_scale_trans = None\n                key_mn_trans = None\n            \n            if value_states.shape[-2] <= self.residual_length:\n                value_states_quant = None\n                value_states_full = value_states\n                value_scale = None\n                value_mn = None\n            else:\n                value_states_quant = value_states[:, :, :-self.residual_length, :].contiguous()\n                value_states_full = value_states[:, :, -self.residual_length:, :].contiguous()\n                value_states_quant, value_scale, value_mn = triton_quantize_and_pack_along_last_dim(value_states_quant, \n                                                                                                self.group_size, \n                                                                                                self.v_bits)\n\n        past_key_value = (key_states_quant_trans, key_states_full, key_scale_trans, key_mn_trans, \n                          value_states_quant, value_states_full, value_scale, value_mn, kv_seq_len) if use_cache else None\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        if self.config.pretraining_tp > 1:\n            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n        else:\n            attn_output = self.o_proj(attn_output)\n\n        attn_weights = None\n        return attn_output, attn_weights, past_key_value",
        "experimental_info": "Model architecture: LlamaForCausalLM_KIVI\nKey cache quantization bits (k_bits): 2\nValue cache quantization bits (v_bits): 2\nQuantization group size (group_size): 32\nResidual (full-precision) length (residual_length): 32\nFlash Attention: Enabled (use_flash=True)\nPretrained Model: meta-llama/Llama-3.1-8B-Instruct\nMax new tokens for generation: 96\nInput prompt token count: Variable, derived from `gsm8k` dataset examples.\n\nEvaluation Context (from mem_spd_test.py):\nModel: meta-llama/Llama-2-7b-hf\nKey cache quantization bits (K_BITS): 2\nValue cache quantization bits (V_BITS): 2\nQuantization group size (GROUP_SIZE): 32\nResidual length (RESIDUAL_LENGTH): 128\nBatch size (BATCH_SIZE): 96\nPrompt length (prompt_lenth): 160\nOutput length (output_length): 338\nRepeats for time measurement (num_repeats): 3\nMetrics measured: Used time, Peak memory."
      }
    },
    {
      "title": "FrameQuant: Flexible Low-Bit Quantization for Transformers",
      "abstract": "Transformers are the backbone of powerful foundation models for many Vision\nand Natural Language Processing tasks. But their compute and memory/storage\nfootprint is large, and so, serving such models is expensive often requiring\nhigh-end hardware. To mitigate this difficulty, Post-Training Quantization\nseeks to modify a pre-trained model and quantize it to eight bits or lower,\nsignificantly boosting compute/memory/latency efficiency. Such models have been\nsuccessfully quantized to four bits with some performance loss. In this work,\nwe outline a simple scheme to quantize Transformer-based models to just two\nbits (plus some overhead) with only a small drop in accuracy. Key to our\nformulation is a concept borrowed from Harmonic analysis called Fusion Frames.\nOur main finding is that the quantization must take place not in the original\nweight space, but instead in the Fusion Frame representations. If quantization\nis interpreted as the addition of noise, our casting of the problem allows\ninvoking an extensive body of known consistent recovery and noise robustness\nguarantees. Further, if desired, de-noising filters are known in closed form.\nWe show empirically, via a variety of experiments, that (almost) two-bit\nquantization for Transformer models promises sizable efficiency gains. The code\nis available at https://github.com/vsingh-group/FrameQuant",
      "full_text": "FrameQuant: Flexible Low-Bit Quantization for Transformers Harshavardhan Adepu 1 Zhanpeng Zeng 1 Li Zhang 2 Vikas Singh 1 2 Abstract Transformers are the backbone of powerful foun- dation models for many Vision and Natural Lan- guage Processing tasks. But their compute and memory/storage footprint is large, and so, serv- ing such models is expensive often requiring high-end hardware. To mitigate this difficulty, Post-Training Quantization seeks to modify a pre- trained model and quantize it to eight bits or lower, significantly boosting compute/memory/latency efficiency. Such models have been successfully quantized to four bits with some performance loss. In this work, we outline a simple scheme to quan- tize Transformer-based models to just two bits (plus some overhead) with only a small drop in accuracy. Key to our formulation is a concept borrowed from Harmonic analysis called Fusion Frames. Our main finding is that the quantization must take place not in the original weight space, but instead in the Fusion Frame representations. If quantization is interpreted as the addition of noise, our casting of the problem allows invoking an extensive body of known consistent recovery and noise robustness guarantees. Further, if de- sired, de-noising filters are known in closed form. We show empirically, via a variety of experiments, that (almost) two-bit quantization for Transformer models promises sizable efficiency gains. The code is available at https://github.com/ vsingh-group/FrameQuant 1. Introduction Transformer-based Large Language Models (LLMs) domi- nate the landscape for Natural Language Processing tasks such as language translation and text summarization (Zhang et al., 2023; Touvron et al., 2023; Zhang et al., 2022b). Vision Transformers (VITs) adapt this idea for computer 1University of Wisconsin-Madison 2Google Research. Corre- spondence to: Harshavardhan Adepu <adepu@wisc.edu>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). vision, and achieve state-of-the-art results on image classi- fication (Zhai et al., 2022), object detection (Zhang et al., 2022a), generation (Chang et al., 2022; Hudson & Zitnick, 2021) and segmentation (Cheng et al., 2022; Ranftl et al., 2021). There is general agreement that scale provides re- markable new capabilities. While large models offer strong performance improvements, their deployment as a module within a product creates unique challenges. For example, serving these models on ex- pensive hardware can drastically increase data center costs. Even loading these models on consumer-grade machines is difficult, and the ability to handle heterogeneous resource- constrained devices is almost infeasible. This has led to various efficiency-focused strategies for model compression including but not limited to distillation (Hinton et al., 2015; Zhu et al., 2021), pruning (Chen & Zhao, 2019), sparsity (Yu et al., 2012; Yun et al., 2020) and quantization (Han et al., 2016; Banner et al., 2019). Among these methods, Post-Training Quantization offers unique advantages in that it does not change the model architecture or training scheme. This paper presents a new Post-Training Quantization scheme, FrameQuant, that offers much more flexibility to strike a balance between reducing model size and preserving model quality. Specifically, FrameQuant offers what may be considered equivalent to using a fractional number of bits for quantization, e.g., 2.1 or 2.2 bits: this is valuable because for large Transformer-based models like GPT, model quality deteriorates fast (Frantar et al., 2023) as we reduce bit width in the low-bit quantization regime (e.g., 2-bit). Further, de- pending on the accuracy needs of the downstream task at hand or a desire to control the worst-off error, more flexi- bility offers the user more control. Towards this goal, our main idea is to compute a specific type of redundant/over- complete representation of a pre-trained weight matrix and quantize the matrix in that representation. We will see how robustness to quantization error will follow naturally from our choice of representation. The de-quantization step uses a straightforward scheme to re-construct the full-precision weights. We leverage a mature concept from Harmonic analysis, Fusion Frames, as the foundation for our proposal. Fusion Frames (Donoho et al., 1998; Christensen, 2018) serve an important role in signal processing in analog-to- digital conversion and signal transmission. Frames are guar- 1 arXiv:2403.06082v2  [cs.LG]  31 Jul 2024Flexible Low-Bit Quantization for Transformers anteed to be robust when the Frame coefficients are cor- rupted by additive noise. They are numerically stable, and if additional compute/memory overhead is acceptable, de- noising filters with good theoretical properties or provably optimal recovery schemes are known. To our knowledge, Frame theory for neural network quantization is unexplored. Our key contributions include (a) an approach that offers fractional bit quantization capabilities with theoretical guar- antees. (b) We empirically verify that Transformer-based models can be quantized to two bits (or 2.x bits), on an ex- tensive basket of 15 popular Vision Transformers and Large Language Models from the OPT (Zhang et al., 2022b) as well as Llama2 (Touvron et al., 2023) classes. We achieve consistent improvements over all existing baselines. 1.1. Related Work Given the growth in the scale of foundation models com- mon in our community, model compression is an active topic of research. Distillation (Hinton et al., 2015; Zhu et al., 2021), pruning/shrinking (Chen & Zhao, 2019) and the use of sparsity is quite common (Yu et al., 2012; Yun et al., 2020). There is growing interest (Rokh et al., 2023; Namburi et al., 2023; Gholami et al., 2022) in approaches that perform model compression via quantization either (i) during training or (ii) post-training since minimal changes to the architecture are needed. Quantization during training works well (Gholami et al., 2022; Nagel et al., 2021), but models must be re-trained. Post-training quantization (PTQ) methods (Nagel et al., 2019) simply quantize a pre-trained model on a small calibration set, and involve much less work. These methods are effective for large language models like OPT (Zhang et al., 2022b), BLOOM (Scao et al., 2023) and can reduce the bit-width with only a small degradation in performance. For example, (Nagel et al., 2020) analyzed the effect of data-dependent rounding. A layer-wise proxy loss was studied and AdaRound quantization was proposed to efficiently minimize this loss. The approach in (Frantar & Alistarh, 2022) minimizes the squared error similar to (Nagel et al., 2020), but quantizes each layer individually while adjusting the remaining unquantized weights using the Hessian of the proxy loss term following (Lecun et al., 1989; Hassibi et al., 1993). OPTQ (Frantar et al., 2023)(for- merly GPTQ) extended upon the ideas in OBQ (Frantar & Alistarh, 2022), and offered other adjustments that gives a stable scheme that can compress large language models like OPT-175B or BLOOM-176B to 3 or 4 bits per parameter without a large loss in accuracy. For Vision Transformers, PTQ4ViT (Yuan et al., 2022) quantifies the weights in two stages, and uses a Hessian-guided search for the optimal scale for the weights. In (Liu et al., 2021b), a feature map is used to search for the optimal quantization interval for maintaining similarity between the quantized and original feature maps. The method also chooses different bit widths for each layer. Other strategies proposed for PTQ include (Ding et al., 2022; Li et al., 2023). We note a recent con- current result for two-bit quantization for language models reported in (Chee et al., 2023). Our approaches are based on different starting points: our choice of Frame theory to min- imize quantization error versus the choice in (Chee et al., 2023) of using incoherence as a pre and post-processing step, which is later shown to offer desirable theoretical prop- erties. But fundamentally, both methods work well due to similar underlying principles related to basis expansions (on a space-filling basis). We discuss later how (Chee et al., 2023) can be viewed as a special version of our formulation (but with no redundancy). 2. Finite Frame Theory and Fusion Frames Frames generalize the Orthogonal basis decomposition of a Hilbert space and provide redundant representations. Finite frames find applications in robust signal transmission with quantization and erasures (Goyal et al., 1998; 2001; Casazza & Kovaˇcevi´c, 2003), Coding theory (Strohmer & Heath Jr, 2003), distributed processing (Casazza et al., 2008), Com- pressed Sensing (Boufounos et al., 2009) among others. We start with a brief review of relevant concepts. Readers familiar with these concepts may skim this section. Consider a finite-dimensional Hilbert space H of dimension d. Throughout the paper, we denote this space as Hd. Definition 2.1 (Frames). A family of k vectors ϕ = (φi)k i=1 in Hd is called a frame for Hd if there exist con- stants 0 < A≤ B <∞ such that A||x||2 ≤ kX i=1 |⟨x, φi⟩|2 ≤ B||x||2 (1) for all x ∈ Hd where ⟨·, ·⟩ is the dot-product. The constants A and B are the lower and upper frame bounds. The sandwich expression suggests that x will not be poorly distorted when we calculate its inner products with a frame. When A = B, ϕ is called a A-tight frame. When A = B = 1, we get a Parseval’s frame. Fig. 1 shows examples of Tight Frames for R2 for different k’s. The lower bound is equivalent to asking that ϕ span H. So, for a frame, we always have k ≥ d. If k = 3d, the redundancy is r = 3. Fusion Frames provide a way for fusing “smaller” frames to construct large frames, offering various efficiency and robustness properties (Eldar & Michaeli, 2008). Formally, Definition 2.2 (Fusion Frames). Let (Wi)k i=1 be a family of subspaces in Hd, and let (wi)k i=1 ⊆ R+ be a family of weights. Then, ((Wi, wi))k i=1 is a fusion frame for Hd, if 2Flexible Low-Bit Quantization for Transformers Figure 1.Examples of Tight frames of k = 4, 5, ...,11 in R2 there exists constants 0 < A≤ B <∞ such that A||x||2 ≤ kX i=1 w2 i ||Ui(x)||2 ≤ B||x||2 for all x ∈ Hd where Ui denotes the orthonormal projection onto the sub- space Wi for each i. The constants A and B still denote the lower and upper fusion frame bounds respectively. Similar to the Frames case, the Fusion Frame((Wi, wi))k i=1 is referred to as a tight fusion frame if A = B and as a Parseval fusion frame if A = B = 1. Finally, if wi = 1 for all i, we simply utilize the notation (Wi)k i=1. 2.1. Operators in Fusion Frames Fusion Frame (FF) operators can be formally defined using a Hilbert direct sum. Since we use the operators for model quantization, without loss of generality, we describe them in terms of vectors and matrices, to keep notations simple. Let ((Wi, wi))k i=1 be a Fusion Frame for Hd with orthonormal basis (Pi)k i=1 respectively. The Analysis operator TW takes a signal x ∈ Hd and com- putes its dot product with all the basis (Pi)k i=1. The results represent x w.r.t. the FF as TW : x 7→ (wiPT i (x))k i=1 (2) The Synthesis operator T ∗ W is the adjoint of the analysis operator, and takes a sequence of representation vectors (yi)k i=1 and outputs a signal in Hd: the reconstruction of the original signal from its FF representation is defined as T ∗ W : (yi)k i=1 7→ kX i=1 wiPi(yi) (3) The Fusion Frame operator SW is defined as the compo- sition of these two operators. It first computes the FF rep- resentation of a signal in Hd in different subspaces using the Analysis operator. Then, when needed, we can recon- struct the signal back from these representations using the Synthesis operator. When the Fusion Frame is tight, the reconstruction is exact (Casazza et al., 2011). Formally, SW = T ∗ WTW : x 7→ kX i=1 w2 i Ui(x) (4) Here, Ui = PiPiT is the orthogonal projection onto the subspace Wi. If the Fusion Frame is tight, we have SW = AId where Id is the d × d Identity Matrix. Throughout, we will use Parseval Fusion Frames, where the frame bounds A = B = 1. Fusion Frames offer many other properties but due to space, we will keep the presentation focused. How will Fusion Frames be used? An easy way to see Fusion Frames in practice is to work out a simple example, Example 1. Consider the Euclidean space Hd = R4. Say, an oracle gives us a Fusion Frame where we have k = 3 subspaces, and each subspace is of equal dimension ρ = 2. For notational ease, we represent these subspaces with their Synthesis operator T ∗ W =     0.57 0 .00 0.00 0 .57 0.57 0 .00 0.00 0 .57  ,   0.57 0 .00 0.00 0 .57 −0.28 0 .50 −0.50 −0.28  ,   0.57 0 .00 0.00 0 .57 −0.28 −0.50 0.50 −0.28     We want to compute the FF representation of a signal x = \u0002 −1 −0.5 0 .5 1 \u0003T . To do so, we must apply the Analysis operator TW on x. The Analysis operator is simply based on the individual transposes in T ∗ W defined above. \u0014 0.57 0 .00 0 .57 0 .00 0.00 0 .57 0 .00 0 .57 \u0015 , \u0014 0.57 0 .00 −0.28 −0.50 0.00 0 .57 0 .50 −0.28 \u0015 · · · Applying TW on x, we get the FF representations TW(x) = \u0012\u0014−0.28 0.28 \u0015 , \u0014−1.22 −0.32 \u0015 , \u0014−0.22 −0.82 \u0015\u0013 To get the actual projections ofx onto different subspaces Wi, we multiply these coefficients with the scaled orthonor- mal basis (wiPi)k i=1 of their corresponding subspaces (w2 i Ui(x))3 i=1 =     −0.1667 0.1667 −0.1667 0.1667  ,   −0.7053 −0.1890 0.1890 0.7053  ,   −0.1280 −0.4777 0.4777 0.1280     We can verify by checking the identity SW = Id or check- ing that P3 i=1 w2 i Ui(x) = x (only accurate up to rounding errors) that this Fusion Frame is a Parseval’s frame. Ap- plying the Synthesis operator T ∗ W on the projections above recovers x perfectly. Corrupting FF representations by noise.What happens when the Fusion frame representations are corrupted by noise, say due to erasure or quantization? Because of re- dundancy in the representation of a signal, we expect some immunity to corruptions in the representations due to noise. 3Flexible Low-Bit Quantization for Transformers In the current example, this is indeed the case. If we add noise to TW(x) with an SNR of 10dB and use the noisy coefficients to reconstruct x back, we observe an MSE re- duction of 33% at a redundancy factor of r = 1.5× and 50% MSE reduction r = 2×, consistent with theory (Goyal et al., 1998). Quantizing Transformer layers. Let us consider quan- tizing each layer in a Transformer model as in (Nagel et al., 2020; Frantar & Alistarh, 2022; Frantar et al., 2023; Yuan et al., 2022), e.g., by quantizing individual weights or columns, one by one. First, notice that the quantization error/noise is weight-dependent. Further, the error will also depend on how all other weights are quantized. The only way to guide a quantization scheme is the evaluation of a loss (to be described shortly) on a small calibration dataset D. In this regime, even with strong assumptions on the noise, it is difficult to say much about the quality of the de- quantization. On the other hand, far more is known (Goyal et al., 1998; Waldron, 2019; Casazza & Kutyniok, 2012) about the behavior of quantization of data given in an appro- priate Frame basis (e.g., Fusion Frames), and error bounds on the reconstruction are available. Put simply, quantiza- tion noise in the space of Frame projections incurs far less error in the reconstructions due to the robustness of Frame representations. §3 will leverage this principle. 2.2. Tight Fusion Frames and their construction We first define the type of Fusion Frames we will use and then describe how they can be constructed. Definition 2.3 (Tight Fusion Frames or TFF). For A >0 and with Id giving the d × d Identity matrix, a (k, ρ, d)- TFF is a sequence {Ui}k i=1 of d × d orthogonal projection matrices of rank ρ and scalars {wi}k i=1, wi > 0 such that kX i=1 w2 i Ui = AId. (5) A (k, ρ, d)-TFF is a sequence of k equidimensional sub- spaces of dimension ρ in a d-dimensional space, and Ui is the orthogonal projection matrix onto the ith sub-space. Constructing TFFs. The algorithm in (Casazza et al., 2011) can be used to generate TFFs if we provide the dimension d, the number k of subspaces we need, and the dimension ρ of each of these subspaces. The algorithm has two main steps. First, one generates a Tight Frame of d unit norm vectors for the complex domain Cρ. Then, this Frame is modulated with the square roots of unity to generate the k subspaces for Cd. We use a simple construction described in (Fickus et al., 2023) to extend these Fusion Frames to Rd. Since it can be used as a black-box module, we skip the details and include a brief synopsis in Appendix §J. Remarks. A few properties are useful to note. This Fusion Frame construction is sparse/block diagonal and can be gen- Figure 2.Illustration of standard calculation (on top) versus the corresponding calculations in FF space (bottom) erated one subspace at a time. To generate another Fusion Frame, we can hit it with a random rotation. Depending on the Transformer model at hand, the dimension of the acti- vations of the layer determines d. For a desired redundancy factor (k × ρ ≥ d) in our frames, given d we simply choose a k and ρ such that they are valid (i.e., a TFF exists for the triple (k, ρ, d)) according to (Casazza et al., 2011). If not, we use a slightly lower redundancy factor r knowing that we will always have a trivial solution for k = 1 and ρ = d. 3. Fusion Frames based Quantization We can now leverage the ideas described in the preceding sections for quantizing the parameters of a Transformer model. Consistent with common PTQ approaches (Nagel et al., 2020; Frantar & Alistarh, 2022; Frantar et al., 2023; Yuan et al., 2022), we perform quantization layer-by-layer, minimizing the proxy loss between the quantized and non- quantized output of the layer. What are analogous calculations in FF space? Consider a layer l in a Transformer model, with parameters Θl. Let ˘Aprev be the activation of the already quantized previous layer for the examples in the calibration set D. The (non- quantized) output Zl of layer l is Zl = Θl ˘Aprev (6) Here, Θl maps the input ˘Aprev to the output Zl. To avoid directly quantizing Θl, we want the quantization noise to instead impact the analogous terms in the Fusion Frame representation (but equivalent calculation as (6)). To this end, let us set up some notations. In general, the dimension of Zl and ˘Aprev may not be the same. So, the number of subspaces in their respective Fusion Frames will be differ- ent. Let k, kprev denote the number of subspaces for Zl and ˘Aprev respectively. In other words, Wl = ( Wl i)k i=1 and Wprev = (Wprev i )kprev i=1 . Let the sequence of orthonor- mal basis for the subspaces of Wl and Wprev be given by (Pl i )k i=1 and (Pprev i )kprev i=1 respectively. To reduce notational clutter, we absorb the scalars wi into Pi. To write down the expression in FF space, for simplicity, let us vectorize the 4Flexible Low-Bit Quantization for Transformers set of orthogonal basis above and define Pl = [Pl 1Pl 2 . . . Pl k] and Pprev = [Pprev 1 Pprev 2 . . . Pprev kprev ] Taking the FF representations of the output Zl means PT l Zl = PT l Θl ˘Aprev| {z } =Zl (7) Rearranging brackets, PT l Θl ˘Aprev = PT l Θl(PprevPT prev) ˘Aprev (8) = (PT l ΘlPprev)(PT prev ˘Aprev) (9) In the above expression, the object (PT l ΘlPprev) maps the FF representation of ˘Aprev, i.e., (PT prev ˘Aprev), to the FF representation of (PT l Zl). This operation is completely in the FF representation space as desired. A notation simplification allows us to cross-reference what our FF-space calculations are doing w.r.t. the objective function. Let Cprev = PT prev ˘Aprev and Dl = PT l ΘlPprev. Our objective is to quantize Dl to ˆDl while minimizing the proxy loss in terms of FF representations, L( ˆDl) = ||DlCprev − ˆDlCprev||2 F = tr((Dl − ˆDl)T CT prevCprev(Dl − ˆDl)) = tr((Dl − ˆDl)CprevCT prev(Dl − ˆDl)T ) The term ˜H = CprevCT prev corresponds to the Hessian prominent in most published results on PTQ strategies (Nagel et al., 2020; Frantar & Alistarh, 2022; Frantar et al., 2023; Chee et al., 2023). So, our loss is the same as other approaches, except that we are operating in the FF represen- tation space and enjoy all the associated noise robustness properties. Further, because the loss for quantizing the trans- formed weights Dl is the same as e.g., (Frantar et al., 2023), we can directly use the Hessian-based iterative quantization algorithms in (Frantar & Alistarh, 2022; Frantar et al., 2023) with minimal changes. Finally, following recent results in Post-training Quantization (Nagel et al., 2020; Frantar & Alistarh, 2022; Frantar et al., 2023; Chee et al., 2023) we primarily focus on quantizing the transformed weights (Dl) but include one experiment with a simple activation quan- tization in §F. We note that there are standalone activation quantization strategies for smaller Vision models for up to four bits, see (Ding et al., 2022; Yuan et al., 2022). Details of the quantization procedure.Other than working in the FF space, the quantization itself is almost identical to (Frantar et al., 2023). We use the iterative method from (Frantar et al., 2023) with some modifications to improve the stability of our algorithm. For example, we found that clipping the weights before calling the iterative scheme Figure 3.Inference for a FrameQuant quantized model. from GPTQ reduces the weight range during quantization. This effectively adds more quantization noise to the outlier weights that are too large. Since Fusion Frames spread out the energy uniformly among different subspaces, we observe that there are only a few outliers in the transformed Weight matrices, and hence clipping them boosts performance. We found that simply clipping the weights at 2σ (assuming a Normal distribution), where σ is the standard deviation of Dl, works well in practice. We observe that this change also helps the method in (Chee et al., 2023) (and this modified algorithm is also included in our baselines). Alg. 1 shows the sequence of steps in FrameQuant. Algorithm 1 FrameQuant Require: Weight matrix Θl, previous layer activations ˘Aprev, input and output Fusion Frames Pl, Pprev, block size B 1: Compute Cprev = PT prevAprev, Dl = PT l ΘlPprev 2: Compute σ = std(Dl), µ = mean(Dl) 3: Dl = 2σ clip(Dl, µ− 2σ, µ+ 2σ) 4: ˆDl = quantize(Dl, Cprev, B) // modified GPTQ 5: Store ˆDl // store the quantized matrix ˆDl return Pl ˆDlCprev // return quantized layer activations 3.1. Robustness of Fusion Frames We now state some technical results that apply to both Frames and Fusion Frames. (a) Redundancy related guarantees. During quantization, the Fusion Frame coefficients are corrupted. This can be modeled as an additive noise being added to these coeffi- cients. Assume that the redundancy factor is r >1. Even with classical analysis, the result in (Rozell & Johnson, 2005; Goyal et al., 1998) shows that when using Tight Frames to reconstruct the signal from noisy coefficients, for memoryless quantization, we get an MSE reduction of O(1/r). A rate of O(1/r2) for consistent reconstruction can also be achieved by solving an LP during the dequan- tization step (Goyal et al., 1998). While this may not be preferred in practice, we know that if adopted, this matches 5Flexible Low-Bit Quantization for Transformers the lower bound of (1/r2), see Ch. 2 in (Goyal et al., 1998). (b) Another benefit of Frame representations is that recon- struction can “denoise” using filters available in closed form. For example, with Tight Frames, it is known that the Wiener filter provably minimizes the MSE, see Ch. 13 in (Casazza & Kutyniok, 2012), (Kutyniok et al., 2009). In our exper- iments, we found that even a diagonal approximation of the Wiener filter helps. But our experimental results are reported without utilizing this boost. 3.2. Inference Procedure During inference, the quantized model is loaded into mem- ory. At each layer, the inputs to the layer ( ˘Aprev) are first transformed into their Fusion Frame representations using the analysis operator PT prev. The FF representations are then transformed by the quantized weights (Dl) for this layer into the FF representations of the output. Finally the synthesis operator Pl is used to compute the layer outputs. Figure 3 shows this dequantization process and the bit widths of each of these operations for a single layer in a network. 4. Experiments We performed an extensive set of experiments comparing FrameQuant with several quantization baselines for Vision models and Language models. The goal is to assess (a) per- formance metrics of different methods on benchmark tasks and (b) how close low-bit quantization can approach the full precision performance with a small degree of represen- tation redundancy. We use image classification task (Deng et al., 2009) for Vision models and Perplexity for Language models. We start with an overview of our experimental setup. We present the evaluation results of FrameQuant on 15+ Vision Transformer architectures+configurations for image clas- sification. Next, we conduct an ablation study on image classification task to better understand the behavior of dif- ferent components of FrameQuant. We then present results on Language models such as OPT (Zhang et al., 2022b) and Llama2 (Touvron et al., 2023) by comparing perplexity and accuracy in downstream tasks. The appendix includes many additional experiments. 4.1. Experimental Setup We evaluate our method on the ImageNet-1K classification task. For quantizing the model weights of the pre-trained models obtained from the Huggingface hub (Wightman, 2019), we use 128 images randomly selected images from the training dataset as calibration dataset D. We quantize the parameter matrices of the layers sequentially from shal- low layers to deep layers, similar to (Frantar et al., 2023). After quantizing each layer, we pass the inputs to the layer again and send the output with the quantized weights to the next layer. Finally, we evaluate the quantized models on the ImageNet-1K validation dataset and report the top-1 accu- racy. All our “base” experiments correspond to 2 bits. We note that one of the baselines, PTQ4ViT (Yuan et al., 2022), performs activation quantization together with weight quan- tization, but was not tested in the extreme 2 bit quantiza- tion setting. To ensure fair comparisons to that method, we switch off activation quantization in their method and also add another experiment with 3 bits. For additional ex- periments with activation quantization, Segmentation and Object Detection tasks, we refer the reader to the Appendix sections F, G respectively. 4.2. Results on ImageNet Classification Task We use model architectures (including ViT (Dosovitskiy et al., 2021), DeiT (Touvron et al., 2021), DeiT III (Touvron et al., 2022), and Swin (Liu et al., 2021a)) and model sizes (including small, medium, large, huge) that are available on the Huggingface hub (Wightman, 2019). Our main results for these experiments are shown in Tab. 1–2. Figure 4a shows the performance of the different classes of models on the ImageNet-1K dataset. We observed that clipping the weights at 2σ also helps QuIP (Chee et al., 2023), so we include it as an additional baseline. Even with a redundancy factor of r = 1, FrameQuant achieves better accuracy com- pared to most baselines under consideration. Further, with a redundancy factor of r = 1.1, FrameQuant outperforms all baselines by a good margin and is respectably close to the full precision model, underscoring the robustness of Fusion Frames in the presence of quantization noise. We observe that adding more redundancy to the Frame representations continues to improve the performance of the quantized mod- els, especially when the models are small. See §A for more details. We note that the codebase for PTQ4ViT (Yuan et al., 2022) was not compatible with the Swin-L model, so we could not report their performance for this model. 4.3. Ablation Study In this section, we dissect FrameQuant to understand the contribution of different components of our algorithm. Ta- ble 3 shows the results of this experiment. We use GPTQ (Frantar et al., 2023) as our starting point. With GPTQ (Frantar et al., 2023) alone, the performance drops in the quantized models are significant: as high as 82% for the DeiT III (Touvron et al., 2022) Base model. Simply with the FF representation added (column TFF), we see improve- ments in performance across all models, with a maximum improvement of 56% for DeiT III-H. We note that some of the smaller-sized models are yet to see all the benefits of FF representations. That is because these models have outliers in the weights (much larger than the remaining weights) which results in higher quantization errors. The FF repre- 6Flexible Low-Bit Quantization for Transformers Method #bits ViT DeiT Swin T S S/32 B T S B S B B/384 Full-Precision 32 75.46 81.39 75.99 85.10 72.16 79.85 81.98 82.88 84.67 86.02 PTQ4ViT 2 0.33 0.55 0.71 0.41 1.51 4.47 25.54 12.54 0.15 0.15 GPTQ 2 0.40 0.40 0.39 29.26 1.60 4.23 41.00 43.54 47.38 57.52 QuIP 2 1.42 21.98 19.00 77.54 12.93 51.62 75.51 71.58 74.91 79.85 QuIP (with our 2σ clip) 2 9.10 48.96 41.41 79.54 30.49 65.70 77.69 76.34 79.17 82.40 FrameQuant (r = 1.0) 2 8.92 48.10 41.16 79.53 31.73 66.35 77.62 77.91 80.16 82.44 FrameQuant (r = 1.1) 2.2 25.79 61.51 53.85 80.93 46.48 70.43 78.67 78.77 81.33 83.42 PTQ4ViT 3 18.32 36.18 22.20 21.43 51.73 69.65 75.35 73.01 69.46 70.68 Table 1.ImageNet-1k Top-1 validation accuracy of Tiny to Base sized Vision Transformer-based models when quantized to 2 (or 3) bits by different methods. FrameQuant with a redundancy factor of r = 1already performs better or on par with Quip (Chee et al., 2023). With a slightly higher redundancy factor of r = 1.1, we get the best performance of all the methods under consideration. (a) Validation accuracies for different classes of Transformer models for Vision on ImageNet-1K (b) Weights distribution in a ViT layer and the 2σ thresholds Figure 4.(a) Validation accuracies of Vision Transformers on ImageNet-1K dataset. We can see FrameQuant closing the gap between the full precision model with increasing redundancy. Each dot in the plot corresponds to a model from tables 1-2 combined. (b) shows the distribution of weights in a ViT layer and the 2σ thresholds for clipping. We see that our thresholding keeps most of the mass while removing outliers. Method #bits ViT DeiT III Swin L H L H L Full-Precision 32 85.84 87.59 86.97 87.19 85.95 PTQ4ViT 2 37.05 00.18 2.14 55.57 - GPTQ 2 63.08 42.63 68.43 28.20 71.69 QuIP 2 82.22 84.58 84.76 86.27 83.61 QuIP (our 2σ clip) 2 83.17 85.31 85.48 86.38 84.27 FrameQuant (r = 1.0) 2 83.22 85.49 85.45 86.62 84.25 FrameQuant (r = 1.1) 2.2 83.67 85.99 85.75 86.68 84.42 PTQ4ViT 3 81.26 78.92 83.63 85.39 - Table 2.ImageNet-1K Top-1 validation accuracy of Large and Huge sized Vision Transformer-based models when quantized to 2 (or 3) bits by different methods. sentation yields a nice enough distribution that we can fit a Normal distribution. So, after we clip these weights at the ±2σ level, we see improvements in performance because of the outlier removal. Clipping is most effective once the weights are nicely distributed. A direct application of clip- ping on the weights has limited efficacy and incurs errors for weight configurations that are degenerate/poorly distributed, see D.1 for more details. Finally, we add a redundancy fac- tor of r = 1.1 and the FF representations take advantage of this redundancy: we see the best performance across the board. Impact of Gaussian assumption on the weights distri- bution. Figure 4b shows a representative example of the distribution of weights in a model from the ViT family and why the 2σ clipping seems reasonable for capturing most of the mass. The weights distribution for models from DeiT and Swin Transformer are shown in Figure §13. 4.4. Results on Language Models In this experiment, we evaluate the perplexity of quantized models from the OPT (Zhang et al., 2022b) and Llama2 (Touvron et al., 2023) family on two datasets - WikiText2 (Merity et al., 2017) and C4 (Raffel et al., 2020). Figure 5 shows the perplexity of models from the OPT family as the size is increased. We see that FrameQuant at 1× redun- dancy performs better than all other quantization methods. With a redundancy factor of 1.1×, FrameQuant reduces the performance gap with the full precision models as suggested by the theory. We see similar results for models from the Llama2 family as well. We also finetuned the Llama2-7B model quantized by various methods on diverse downstream 7Flexible Low-Bit Quantization for Transformers GPTQ TFF 2σ clip Redundancy ViT DeiT III Swin r = 1.1 S B H S B H S B L ON OFF OFF OFF 0.4 29 .26 42 .63 0 .45 8 .5 28 .2 43 .54 47 .38 71 .69 ON ON OFF OFF 0.88 59 .87 68 .75 1 .48 29 .92 84 .33 61 .01 60 .21 79 .52 ON ON ON OFF 48.10 79 .53 85 .49 51 .13 77 .99 86 .62 77 .91 80 .16 84 .25 ON ON ON ON 61.51 80.93 85.99 65.33 80.91 86.68 78.77 81.33 84.42 Full Precision 81.39 85 .1 87 .59 83 .06 85 .7 87 .19 82 .79 84 .7 85 .95 Table 3.Incremental impact of various steps in FrameQuant on ImageNet-1k accuracy for different Transformer models in Vision Method #bits acc mm-acc Full-Precision 32 84.19 84 .67 ZeroQuant 4.33 78.69 78 .07 ZeroQuant 3.66 54.91 56 .45 ZeroQuant 2.66 38.00 38 .30 FrameQuant 2.2 80.02 79.37 Table 4.Performance of the BERT model quantized with Zero- Quant and FrameQuant on the MNLI dataset. FrameQuant per- forms better than ZeroQuant even with a lower bit-width than ZeroQuant. Method bits OPT Llama2 125M 1.3B 2.7B 6.7B 7B 70B Full-Precision 16 27.65 14.62 12.47 10.86 5.68 3.32 GPTQ 2 5.7e3 8.9e3 9.1e3 3.1e3 6.4e3 140.5 QuIP 2 913.0 37.59 22.86 15.67 26.02 6.21 FrameQuant 2 345.7 30.54 20.67 15.72 14.85 5.50 FrameQuant 2.2 131.2 22.68 15.86 13.53 8.48 4.67 Table 5.Perplexity (lower is better) of Llama2 and OPT models on WikiText2 dataset when quantized to 2 (or 2.2) bits by different methods. tasks and observed a maximum accuracy boost of 41% by FrameQuant at r = 1.1× compared to vanilla GPTQ. Table 5 summarizes the perplexity of all the models on the Wiki- Text2 (Merity et al., 2017) dataset. Results on downstream tasks/additional datasets is in Appendix §H. 4.5. Comparision with Mixed-precision Quantization A redundancy factor of 1.1 is the same as an average bit- width of 2.2 per weight parameter. Mixed-precision quan- tization methods can achieve fractional bit-widths by us- ing different bit-widths for different weights in the model. We compare FrameQuant with a recent Mixed-precision method, ZeroQuant (Yao et al., 2022). We test FrameQuant with a bit-width of 2 and a redundancy factor of 1.1 relative to ZeroQuant at different fractional bit-widths. As shown in Table 4. FrameQuant performs favorably with ZeroQuant, even at low bit widths. (a) Perplexity on WikiText2  (b) Perplexity on C4 Figure 5.Perplexity of models from OPT family on WikiText2 and C4 datasets. FrameQuant performs better than all other quan- tization methods under consideration. We can also see that the performance gap between the quantized models and the unquan- tized model goes down as the size of the models increases. Llama2 7B Llama2 13B Original model 13G 25G FrameQuant 2.1G 3.6G Table 6.Size of original and quantized model with FrameQuant. 5. Other Practical Considerations 5.1. Storage requirements Weight quantization has a direct improvement on the storage needs of the models. Table 6 shows the sizes of compressed Llama2 models. FrameQuant reduces the size of the models by around 85% on average. 5.2. Inference speeds Since FrameQuant involves additional operations to com- pute and transform the weights from the low-bit Fusion Frame representations to the regular weight space, the raw inference speed is expected to be lower than GPTQ. On the other hand, at 2 bits, the accuracy/perplexity of FrameQuant is much better than GPTQ. So, there is a trade-off. Table 7 shows the inference speeds of the quantized models on a Nvidia A100 GPU. Here, we used the block diagonal struc- ture of Fusion Frames and a Hadamard transform-based fast random projection based on (Dao, 2023; Zeng et al., 2023) for the rotation matrices. This inference speed can be improved by using efficient kernels to load the weights into the GPU and perform the transformations. 8Flexible Low-Bit Quantization for Transformers Method Llama2 7B Llama2 13B GPTQ 1425.07t/s 844.03t/s FrameQuant 974.20t/s 607.01t/s Table 7.Inference speed in tokens/sec (t/s) of quantized models with GPTQ and FrameQuant. 6. Discussion We cover a few additional aspects that were not explicitly discussed thus far. (1) Can we reduce to one bit? We per- formed experiments with redundancy of 1.8× with 1 bit per weight but were unsuccessful. For one bit, once the redundancy has exceeded r = 2, it makes more sense to just use two bits. (2) Can FrameQuant run as QuIP? For each layer, if we choose a Fusion Frame with a redundancy factor r = 1 and the random orthonormal basis Pl, Pprev, we get a setup similar to QuIP (Chee et al., 2023) after removing the 2σ weight clipping. This is also why when QuIP is augmented with our 2σ clipping we see similar results to FrameQuant with 1× redundancy. (3) Additional storage needed?: Since there are efficient deterministic al- gorithms to generate Fusion Frames, during inference, only knowledge of (k, ρ, d) is needed. For rotations, we only need knowledge of the seed. Also, since many layers in a Transformer model have the same shape, these parameters can be shared across layers. Additional details on the stor- age benefits are in §K.1 (4) Why is flexibility useful? If the performance hit at the two-bit level is unacceptable for an application, the only recourse currently is to move up to three bits for existing methods ( 50% increase). However, FrameQuant allows flexibility through the choice of the re- dundancy factor r. (5) Higher bitwidths? The main focus of this work is to evaluate 2-bit quantization of the weights in Vision and Language models and to check the benefits of applying Fusion Frames in terms of flexible bit-widths. Higher bit widths such as 3 or 4-bit quantization have been studied (Frantar et al., 2023; Chee et al., 2023) and also used in practice (Gerganov, 2023). (6) Computational complexity during Inference: The core FF-related compute is similar to alternatives (Chee et al., 2023) with a small overhead related to the number of subspaces k. During inference, we need an additional compute of O(d2(kr + logd)) for transforming the weights from the Fusion Frame representation space to the regular weight space. Any quantization scheme in the low-bit regime will incur a cost of O(d2) to transform the quantized weights by scaling and shifting them. More de- tails are provided in §K.2. (7) Quantization aware training: FrameQuant can be modified to be applicable during QAT although we do not include such experiments here. One option is to use it during fine-tuning where the quantization loss is simulated, which can then be used to regularize the loss to make it more robust to quantization. Fusion Frames can meaningfully inform this bias, via an estimate of the “out of subspace error” to minimize degradation due to quan- tization. (8) Scaling laws vis- `a-vis FrameQuant? During quantization, the number of parameters does not change. Instead, each parameter has a lower degree of freedom since the number of states it can represent is reduced. We can use the (number of parameters × bit-width) as a proxy for the degree of freedom for each (quantized) model. Taking the quantization bit width into account, a line plot of test loss (on the vertical-axis) as a function of (number of parameters × bit-width) on the horizontal axis may have a different slope compared to (Kaplan et al., 2020), Fig. 1. (9) Ratio- nale for clipping: Let u be a vector in p dimensions. Let P be a projection onto a random subspace in p′ dimensions. Projecting u using P gives v as v = Pu. Assume that the entries in u have finite mean and variance and are uncorre- lated. Then each entry of v is effectively a sum of many scaled random variables. The distribution of these entries (sum of scaled variables, suitably standardized) approaches a normal distribution as the dimensionality p grows. Weak dependence or mixing can also be handled. 7. Conclusions This paper describesFrameQuant, a Frames based algorithm for flexible low-bit quantization. Quantization is motivated by the need to efficiently serve Large Language Models on heterogeneous devices and flexibility here means that while we retain the option to go as low as two bits; depend- ing on the needs of the downstream task, the user also has the flexibility to seek models with a net footprint of 2.x bits on average. Across most widely used Vision Trans- former models and Large Language models, we find that effective quantization is possible with only a small loss in performance relative to the full-precision model. Further, flexibility for a minor increase in redundancy is available and uniformly helps close the gap with full precision models. We observe, consistent with the literature, that quantization to low bit width is more favorable for larger models (in terms of a performance hit) than a similar quantization ap- plied to smaller models. While some benefits (e.g., model loading time, loading larger models) are immediate, tighter integration with the hardware can unlock far more efficiency gains. The code is publicly available. Impact Statement This paper introduces a low precision quantization method for inference. The objective is to decrease memory needs and facilitate the implementation of larger models on less powerful devices, thereby reducing costs (economic impact) and the carbon footprint (environmental impact). We have not identified any particular ethical issues that need to be emphasized. 9Flexible Low-Bit Quantization for Transformers Acknowledgments Support through the Google Cloud Platform provided the computational resources for conducting our experiments. The research was also supported in part by a Google gift award to UW-Foundation and funding from the Vilas Board of Trustees. References Banner, R., Nahshan, Y ., and Soudry, D. Post training 4-bit quantization of convolutional networks for rapid- deployment. Advances in Neural Information Processing Systems, 32, 2019. Bisk, Y ., Zellers, R., Gao, J., Choi, Y ., et al. Piqa: Reasoning about physical commonsense in natural language. In Pro- ceedings of the AAAI conference on artificial intelligence, 2020. Boufounos, P., Kutyniok, G., and Rauhut, H. Compressed sensing for fusion frames. Proceedings of SPIE - The International Society for Optical Engineering, 10 2009. doi: 10.1117/12.826327. Casazza, P. G. and Kovaˇcevi´c, J. Equal-norm tight frames with erasures. Advances in Computational Mathematics, 18, 2003. Casazza, P. G. and Kutyniok, G. Finite frames, theory and applications, 2012. URL https: //link.springer.com/book/10.1007/ 978-0-8176-8373-3 . Casazza, P. G., Kutyniok, G., and Li, S. Fusion frames and distributed processing. Applied and computational harmonic analysis, 25(1), 2008. Casazza, P. G., Fickus, M., Mixon, D. G., Wang, Y ., and Zhou, Z. Constructing tight fusion frames. Applied and Computational Harmonic Analysis, 30(2), 2011. ISSN 1063-5203. doi: https://doi.org/10.1016/j.acha.2010.05. 002. URL https://www.sciencedirect.com/ science/article/pii/S1063520310000850. Chang, H., Zhang, H., Jiang, L., Liu, C., and Freeman, W. T. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2022. Chee, J., Cai, Y ., Kuleshov, V ., and Sa, C. D. QuIP: 2-bit quantization of large language models with guarantees. In Thirty-seventh Conference on Neural Information Pro- cessing Systems, 2023. URL https://openreview. net/forum?id=xrk9g5vcXR. Chen, S. and Zhao, Q. Shallowing deep networks: Layer- wise pruning based on feature representations. IEEE Transactions on Pattern Analysis and Machine Intelli- gence, 41(12):3048–3056, 2019. doi: 10.1109/TPAMI. 2018.2874634. Cheng, B., Misra, I., Schwing, A. G., Kirillov, A., and Gird- har, R. Masked-attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022. Christensen, O. An introduction to frames and riesz bases, 2018. URL https://link.springer. com/book/10.1007/978-3-319-25613-9 . Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Dao, T. fast-hadamard-transform, 2023. URL https://github.com/Dao-AILab/ fast-hadamard-transform. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, 2009. Ding, Y ., Qin, H., Yan, Q., Chai, Z., Liu, J., Wei, X., and Liu, X. Towards accurate post-training quantization for vision transformer. In Proceedings of the 30th ACM International Conference on Multimedia, MM ’22, New York, NY , USA, 2022. ISBN 9781450392037. doi: 10. 1145/3503161.3547826. URL https://doi.org/ 10.1145/3503161.3547826. Donoho, D., Vetterli, M., DeV ore, R., and Daubechies, I. Data compression and harmonic analysis. IEEE Transactions on Information Theory, 44(6), 1998. doi: 10.1109/18.720544. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. URL https:// openreview.net/forum?id=YicbFdNTTy. Eldar, Y . C. and Michaeli, T. Beyond bandlimited sampling: Nonlinearities, smoothness and sparsity. ArXiv, abs/0812.3066, 2008. URL https://api. semanticscholar.org/CorpusID:8702589. 10Flexible Low-Bit Quantization for Transformers Fickus, M., Iverson, J. W., Jasper, J., and Mixon, D. G. Harmonic grassmannian codes. Applied and Computational Harmonic Analysis , 65, 2023. ISSN 1063-5203. doi: https://doi.org/10.1016/j.acha.2023.01. 009. URL https://www.sciencedirect.com/ science/article/pii/S1063520323000106. Frantar, E. and Alistarh, D. Optimal brain compression: A framework for accurate post-training quantization and pruning. Advances in Neural Information Processing Systems, 35, 2022. Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. OPTQ: Accurate quantization for generative pre-trained transformers. In The Eleventh International Confer- ence on Learning Representations, 2023. URL https: //openreview.net/forum?id=tcbBPnfwxS. Gao, L., Tow, J., Abbasi, B., et al. A framework for few- shot language model evaluation, 2023. URL https: //zenodo.org/records/10256836. Gerganov, G. llama.cpp, 2023. URL https://github. com/ggerganov/llama.cpp. Gholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M. W., and Keutzer, K. A survey of quantization methods for effi- cient neural network inference. In Low-Power Computer Vision. Chapman and Hall/CRC, 2022. Goyal, V ., Vetterli, M., and Thao, N. Quantized overcom- plete expansions in Rn: analysis, synthesis, and algo- rithms. IEEE Transactions on Information Theory, 44(1), 1998. doi: 10.1109/18.650985. Goyal, V . K., Kova ˇcevi´c, J., and Kelner, J. A. Quan- tized frame expansions with erasures. Applied and Computational Harmonic Analysis, 10(3), 2001. ISSN 1063-5203. doi: https://doi.org/10.1006/acha.2000. 0340. URL https://www.sciencedirect.com/ science/article/pii/S1063520300903403. Han, S., Mao, H., and Dally, W. J. Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding. In Bengio, Y . and Le- Cun, Y . (eds.),4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1510.00149. Hassibi, B., Stork, D. G., and Wolff, G. J. Optimal brain sur- geon and general network pruning. In IEEE international conference on neural networks. IEEE, 1993. Hinton, G., Vinyals, O., and Dean, J. Distilling the knowl- edge in a neural network, 2015. Hudson, D. A. and Zitnick, L. Generative adversarial trans- formers. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learn- ing, volume 139 of Proceedings of Machine Learning Research. PMLR, 18–24 Jul 2021. Kaplan, J., McCandlish, S., Henighan, T., et al. Scaling laws for neural language models, 2020. Kutyniok, G., Pezeshki, A., Calderbank, R., and Liu, T. Robust dimension reduction, fusion frames, and grassmannian packings. Applied and Computational Harmonic Analysis , 26(1):64–76, 2009. ISSN 1063- 5203. doi: https://doi.org/10.1016/j.acha.2008.03. 001. URL https://www.sciencedirect.com/ science/article/pii/S1063520308000249. Le, Q., Sarl´os, T., Smola, A., et al. Fastfood-approximating kernel expansions in loglinear time. In Proceedings of the international conference on machine learning, 2013. Lecun, Y ., Denker, J., and Solla, S. Optimal brain damage. In Advances in Neural Information Processing Systems, volume 2, 01 1989. Li, Z., Xiao, J., Yang, L., and Gu, Q. Repq-vit: Scale repa- rameterization for post-training quantization of vision transformers. In Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision, 2023. Liu, Z., Lin, Y ., Cao, Y ., Hu, H., Wei, Y ., Zhang, Z., Lin, S., and Guo, B. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, 2021a. Liu, Z., Wang, Y ., Han, K., Zhang, W., Ma, S., and Gao, W. Post-training quantization for vision transformer. Ad- vances in Neural Information Processing Systems , 34, 2021b. Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models. In International Conference on Learning Representations, 2017. URL https:// openreview.net/forum?id=Byj72udxe. Nagel, M., Baalen, M. v., Blankevoort, T., and Welling, M. Data-free quantization through weight equalization and bias correction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019. Nagel, M., Amjad, R. A., Van Baalen, M., Louizos, C., and Blankevoort, T. Up or down? Adaptive rounding for post-training quantization. In III, H. D. and Singh, A. (eds.), Proceedings of the 37th International Con- ference on Machine Learning, volume 119 of Proceed- ings of Machine Learning Research. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/ v119/nagel20a.html. 11Flexible Low-Bit Quantization for Transformers Nagel, M., Fournarakis, M., Amjad, R. A., Bondarenko, Y ., van Baalen, M., and Blankevoort, T. A white paper on neural network quantization. ArXiv, abs/2106.08295, 2021. URL https://api.semanticscholar. org/CorpusID:235435934. Namburi, S. S. S., Sreedhar, M., Srinivasan, S., et al. The cost of compression: Investigating the impact of com- pression on parametric knowledge in language models. In Findings of the Association for Computational Lin- guistics: EMNLP 2023. Association for Computational Linguistics, 2023. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., and Liu, P. J. Ex- ploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020. URL http://jmlr. org/papers/v21/20-074.html. Ranftl, R., Bochkovskiy, A., and Koltun, V . Vision trans- formers for dense prediction. In Proceedings of the IEEE/CVF international conference on computer vision, 2021. Rokh, B., Azarpeyvand, A., and Khanteymoori, A. A com- prehensive survey on model quantization for deep neural networks in image classification. ACM Transactions on Intelligent Systems and Technology, 14(6):1–50, Novem- ber 2023. ISSN 2157-6912. doi: 10.1145/3623402. URL http://dx.doi.org/10.1145/3623402. Rozell, C. and Johnson, D. Analysis of noise reduction in re- dundant expansions under distributed processing require- ments. In Proceedings. (ICASSP ’05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005., volume 4, 04 2005. ISBN 0-7803-8874-7. doi: 10.1109/ICASSP.2005.1415976. Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y . Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM , 64(9):99–106, 2021. Scao, T. L., Fan, A., et al. Bloom: A 176b-parameter open- access multilingual language model, 2023. Strohmer, T. and Heath Jr, R. W. Grassmannian frames with applications to coding and communication. Applied and computational harmonic analysis, 14(3), 2003. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jegou, H. Training data-efficient image transform- ers; distillation through attention. In International Con- ference on Machine Learning, volume 139, July 2021. Touvron, H., Cord, M., and J ´egou, H. Deit iii: Revenge of the vit. In European Conference on Computer Vision. Springer, 2022. Touvron, H., Martin, L., Stone, K., et al. Llama 2: Open foundation and fine-tuned chat models, 2023. Waldron, S. F. D. An introduction to finite tight frames, 2019. URL https://link.springer. com/book/10.1007/978-0-8176-4815-2 . Wightman, R. Pytorch image models. https://github. com/rwightman/pytorch-image-models, 2019. Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han, S. SmoothQuant: Accurate and efficient post-training quantization for large language models. InProceedings of the 40th International Conference on Machine Learning, 2023. Yao, Z., Yazdani Aminabadi, R., Zhang, M., et al. Zero- quant: Efficient and affordable post-training quantization for large-scale transformers. In Advances in Neural Infor- mation Processing Systems, 2022. Yu, D., Seide, F., Li, G., and Deng, L. Exploiting sparse- ness in deep neural networks for large vocabulary speech recognition. In 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 4409–4412, 2012. doi: 10.1109/ICASSP.2012.6288897. Yuan, Z., Xue, C., Chen, Y ., Wu, Q., and Sun, G. Ptq4vit: Post-training quantization for vision transformers with twin uniform quantization. In European Conference on Computer Vision, 2022. Yun, C., Chang, Y .-W., Bhojanapalli, S., Rawat, A. S., Reddi, S., and Kumar, S. O (n) connections are expressive enough: Universal approximability of sparse transform- ers. Advances in Neural Information Processing Systems, 33:13783–13794, 2020. Zellers, R., Holtzman, A., Bisk, Y ., Farhadi, A., and Choi, Y . Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Zeng, Z., Davies, M., Pulijala, P., et al. Lookupffn: making transformers compute-lite for cpu inference. In Proceed- ings of the 40th International Conference on Machine Learning, ICML’23. JMLR.org, 2023. Zhai, X., Kolesnikov, A., Houlsby, N., and Beyer, L. Scaling vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. Zhang, B., Haddow, B., and Birch, A. Prompting large language model for machine translation: A case study. In Proceedings of the 40th International Conference on Machine Learning, ICML’23, 2023. 12Flexible Low-Bit Quantization for Transformers Zhang, H., Li, F., Liu, S., Zhang, L., Su, H., Zhu, J., Ni, L. M., and Shum, H.-Y . Dino: Detr with improved denois- ing anchor boxes for end-to-end object detection, 2022a. Zhang, S., Roller, S., Goyal, N., et al. Opt: Open pre-trained transformer language models, 2022b. Zhu, Z., Hong, J., and Zhou, J. Data-free knowledge distil- lation for heterogeneous federated learning. In Interna- tional conference on machine learning, pp. 12878–12889. PMLR, 2021. 13Flexible Low-Bit Quantization for Transformers Appendix In this Appendix, we provide additional details related to the experiments reported in the main paper. This Appendix is organized as follows. In Section A we analyze the impact of redundancy on the performance of the model in terms of classification accuracy on the ImageNet-1K dataset. In Section B, we study this effect on the performance of Vision Transformer models, evaluated using activation maps. Next, in Section C, we study the effect of the size of the calibration data used for quantizing various Vision Models. In Section D, we analyze the choice of the 2σ threshold for clipping the weights during quantization. We provide empirical evidence for different classes of Vision models. We also show that 2σ clipping alone cannot improve quantization performance. On the contrary, it can degrade the performance for weight configurations that are poorly distributed. Section E shows the distribution of weights in the DeiT and Swin Transformer models. In Section F, we present a framework for quantizing activations and show how the FF representation of activations inherently addresses the key pain points described in previous works. We follow this with a simple experiment with activation quantization enabled. In Section G, we provide experiments on Segmentation and Object detection tasks. In Section H, we present more experiments on Language models on different datasets and downstream tasks as mentioned in the main paper. Then, in Section I, we provide an expanded synopsis of the theoretical results that apply to our setting, as briefly described in the main paper. In Section J we provide a brief synopsis of the algorithm used to generate a TFF for the curious reader. Finally in Section K we give a detailed analysis of the storage benefits of FrameQuant and the computational complexity during inference. A. Impact of redundancy in representations We consider the impact of redundancy in our Frame representation moving forward from 2 bits, incrementally increasing redundancy. Table 8 shows the performance of different models at different levels of redundancy. We observe that for large models, the original performance without any redundancy was already high, and adding redundancy did not impact their performance significantly. However, this is not the case for smaller models. Here, we see significant performance improvements (around +21% for the ViT-S model). Redundancy bits ViT DeiT III Swin S B H S B H S B L r = 1.00 (2 .0 bits) 48 .10 79 .53 85 .49 51 .13 77 .99 86 .62 77 .91 80 .16 84 .25 r = 1.05 (2 .1 bits) 56 .19 79 .97 85 .67 58 .74 79 .59 86 .58 78 .47 80 .41 84 .26 r = 1.10 (2 .2 bits) 61 .51 80 .93 85 .99 65 .33 80 .91 86 .68 78 .77 81 .33 84 .42 r = 1.15 (2 .3 bits) 65 .17 81 .27 86 .04 69 .54 81 .69 86 .67 78 .87 81 .88 84 .51 r = 1.20 (2 .4 bits) 66 .53 81 .59 86 .11 71 .07 81 .98 86 .61 79 .56 82 .02 84 .56 r = 1.25 (2 .5 bits) 68 .57 81 .74 86 .06 73 .48 82 .51 86 .55 79 .99 82 .26 84 .51 r = 1.30 (2 .6 bits) 69 .02 81 .77 85 .99 74 .40 82 .54 86 .38 79 .92 82 .39 84 .65 Full Precision - 81.39 85 .1 87 .59 83 .06 85 .7 87 .19 82 .79 84 .7 85 .95 Table 8.Performance of various quantized models on ImageNet-1K classification task as the redundancy in FrameQuant is increased. We see that increasing the redundancy closes the gap between the performance of the quantized model and the Full precision model B. Does redundancy impact attention maps? In the main paper, we discussed how the performance of the models improves as we increase the redundancy in the Fusion Frames during quantization. In this section, we provide additional details on how redundancy affects the attention maps of Vision Transformers from different classes. We will focus mainly on the small and base models where we see significant improvement in the validation accuracy on ImageNet, as we increase the redundancy. Figures 6, 7 and 8 show the attention maps of Vit-S, DeiT III -S, and Deit III - B models respectively. These models show an improvement in the range of 4.55% to 23.27% as we increase the redundancy from r = 1 to r = 1.3. This is reflected in the attention maps as well. We see that as the redundancy is increased, the attention regions concentrate around the objects of interest systematically. This is consistent with the improvement in accuracy and can also be seen in Figure 9. 14Flexible Low-Bit Quantization for Transformers Figure 6.Effect of flexibility/redundancy on activation maps for ViT-S.Figure showing attention maps of ViT-S as the redundancy is increased from r = 1to r = 1.3 in the increments of 0.1 from left to right. The first column shows the image and the ground truth label, and the rest of the columns show the regions that the model is attending to in the final transformer block. We see that as the redundancy is increased, the model gets more focused, with the attention regions concentrating on the objects of interest. #images ViT DeiT III Swin S B H S B H S B L 128 48 .10 79 .53 85 .49 51 .13 77 .99 86 .62 77 .91 80 .16 84 .25 200 51 .48 79 .84 85 .62 53 .74 78 .38 86 .61 77 .66 80 .19 84 .09 256 51 .69 79 .84 85 .74 54 .73 79 .06 86 .47 77 .96 80 .68 84 .31 Table 9.ImageNet-1K Top-1 validation accuracies of models from different classes as the number of calibration images is increased. C. Does the calibration set size matter? In the main paper, we noted that a small calibration set size was sufficient. In this section, we report on experiments varying the number of calibration images and observe the performance of different classes of models on ImageNet-1K. We use a redundancy factor of r = 1 in this experiment. Table 9 shows the validation accuracies for different classes of models as the number of calibration images is increased from 128 to 256. We can see that the performance improvement is seen only in the small-sized models from the ViT and DeiT III classes. So, we will focus on reporting results for these models. Figure 10 shows the accuracies of ViT-S and DeiT III-S models as the number of calibration images is increased from 128 to 512. We 15Flexible Low-Bit Quantization for Transformers Figure 7.Effect of flexibility/redundancy on activation maps for DeiT III-S. Figure showing attention maps of DeiT III -S as the redundancy is increased from r = 1to r = 1.3 in the increments of 0.1 from left to right. can see that there is a small improvement as the number of images is increased from 128 to 200, but the benefits taper off quickly as we increase it further. This shows that if access to the calibration is not limited, a small increase in the number of images used for quantization can benefit the final accuracies of the models, especially for smaller models. D. How does 2σ clipping affect performance? In the main paper, we discussed a simple clipping threshold at the 2σ level. In this section, we analyze the benefits of this choice and its effect on the performance of different classes of models on ImageNet-1K. As in the previous section, we use a redundancy factor of r = 1 for these experiments and focus on the impact of clipping the weights at different levels based on their distribution. Figure 11 shows the accuracies of different classes of models as the threshold for the weights is varied from ±σ to ±3σ. We can see that the performance of all the models peaks in the neighborhood of ±2σ. Clipping at ±σ restricts the range of the weights too aggressively, incurring errors. At ±3σ level, which is close to allowing the entire range, we are stretching the effective scale of the weights to allow all the extreme entries to be represented within the range. This, in turn, increases the width of the quantization levels, which affects the majority of the weights impacting performance. ±2σ seems to be the sweet spot. 16Flexible Low-Bit Quantization for Transformers Figure 8.Effect of flexibility/redundancy on activation maps for DeiT III-B. Figure showing attention maps of DeiT III -B as the redundancy is increased from r = 1to r = 1.3 in the increments of 0.1 from left to right. 1.00 1.05 1.10 1.15 1.20 1.25 1.30 Redundancy (r) 0 20 40 60 80Imagenet 1k Accuracy ViT-S DeiT-S DeiT-B Figure 9.Trend of accuracies in small size models as we increase the redundancy 17Flexible Low-Bit Quantization for Transformers 150 200 250 300 350 400 450 500 # images 0 20 40 60 80ImageNet-1K Accuracy ViT-S DeiT-S Figure 10.Trend of accuracies in small size models as we increase the number of calibration images Model Quantization method WikiText2 C4 Llama2 7B GPTQ without clipping 6.40e3 2.27e3 Llama2 7B GPTQ with clipping 9.45e3 7.40e3 Llama2 7B FrameQuant with clipping 14.85 19.62 Llama2 70B GPTQ without clipping 140.5 68.83 Llama2 70B GPTQ with clipping 2.08e3 1.12e3 Llama2 70B FrameQuant with clipping 5.5 7.85 Table 10.Table showing the impact of clipping on GPTQ. FrameQuant computes the FF representations of the weights that are nicely distributed and can take advantage of clipping to remove outliers. 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 3.00 Clipping Threshold (× ) 0 20 40 60 80Imagenet 1k Accuracy ViT-S DeiT-S Swin-S (a) Accuracies of Small models 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 3.00 Clipping Threshold (× ) 0 20 40 60 80Imagenet 1k Accuracy ViT-B DeiT-B Swin-B (b) Accuracies of Base models 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 3.00 Clipping Threshold (× ) 0 20 40 60 80Imagenet 1k Accuracy ViT-H DeiT-H Swin-L (c) Accuracies of Large models Figure 11.Figure showing the impact of clipping at different thresholds based on σ D.1. Does 2σ clipping alone improve performance? From our alation study 4.3, it might seem that 2σ clipping is doing the heavy lift in improving the performance. However, clipping is most effective once the weights are nicely distributed. A direct application of clipping on the weights has limited efficacy and incurs errors for weight configurations that are degenerate/poorly distributed. Projecting onto a space-filling basis makes clipping effective. To demonstrate this point quantitatively, we run GPTQ on Llama2 models with the 2σ clipping applied directly to the weights. Table 10 shows that the performance degrades when the weights are clipped instead of their Fusion Frame representations as in FrameQuant. E. Distribution of weights in the DeiT and Swin Transformer models This section presents the distribution of the weights in the DeiT and Swin Transformer models. Figure 13 shows the distribution of weights in a linear layer from the DeiT and Swin Transformer families. We can see that the distribution is well behaved and the 2σ threshold captures most of the mass well. 18Flexible Low-Bit Quantization for Transformers (a) Activations of the first block in ViT-M  (b) Activation FF representations of the first block in ViT-M Figure 12.Activations of the first Transformer Block of Vit-M model and their FF representations. We can see the outliers in the activations (shown in red) on the left, while the FF representations are well-behaved. (a) Weight distribution in DeiT  (b) Weight distribution in Swin Transformer Figure 13.Weights distribution in DeiT and Swin Transformer models. F. FrameQuant for Activation Quantization? In the main paper, we restricted the experimental setup of Vision Transformer models to weight quantization for meaningful comparisons to recent PTQ papers. This is because activation quantization in this low-bit regime has not been reported and each baseline will need modifications to report the best possible results. In this section, we provide some details regarding applying FrameQuant for activation Quantization with the caveat that a comprehensive head-to-head comparison to all reported baselines is difficult for the reasons above. Rounding activations to the nearest. For smaller Transformer models, the inference efficiency bottleneck also largely lies in activations. So, we focus on these models to consider activation quantization. We performed activation quantization on ViT-S/B models with a simple rounding to the nearest, and we found that even when the weights are quantized to 2 (or 2.2) bits using FrameQuant, the performance drops are not large. This is promising and shows that FrameQuant is robust in preserving activations even at a 2.x bit level for weights. Table 11 shows the ImageNet-1K accuracy at different bit-widths for weights and activations. Benefits of well-behaved FF representations. Since we operate in the FF representation space, we can first compute the FF representations of the previous layer activations, Cprev = PT prevAprev (10) 19Flexible Low-Bit Quantization for Transformers Method bits ViT-S ViT-B Full Precision W32/A32 81.39 85 .10 FrameQuant W2/A32 48.17 79 .53 FrameQuant W2.2/A32 61.51 80 .93 FrameQuant W2/A8 48.02 79 .51 FrameQuant W2.2/A8 60.96 80 .64 FrameQuant W2/A6 47.41 78 .59 FrameQuant W2.2/A6 58.35 80 .14 Table 11.Performance of quantized ViT-S and ViT-B models on ImageNet-1K validation set. We used FrameQuant to quantize the weights while the activations are rounded to the nearest. and quantize these directly. Also, since activation quantization happens dynamically, during inference time, we keep the activation quantization procedure simple and just use the nearest rounding method. This can be written as: ¯Cprev = ⌊Cprev ∆C ⌉, ∆C = max |Cprev| 2N−1 − 1 (11) where ¯Cprev is in INT8 form and is the quantized version of the FF representations of the activations (Cprev). ⌊·⌉ represents nearest rounding. We can substitute with ⌊·⌋ or ⌈·⌉ to get the floor or the ceil operation. As noted by (Xiao et al., 2023), we also observe that the activations have large outliers in some of the channels whose values are more than 100× larger than the activations of other channels on average and this behavior is consistent across the tokens. This is shown in Figure 12a. So, to quantize the outliers, we need a large-scale factor ∆C, which will quantize all small values to zero. The other option is to use per-channel quantization – where we have different scale factors for different channels. This would solve the outlier problem, but it is not ideal because we cannot use integer kernels for matrix multiplications in the Linear Layers. To use integer arithmetic for the matrix multiplications in the Linear layers, we can only perform per-token quantization for the activations and per-channel quantization for the weights. To solve this problem, (Xiao et al., 2023) shifts the scale from activations to weights that are well-behaved. They dynamically search for different amounts of shifts between the weights and activations using a calibration set and use that during inference. Since we operate in the FF representation space, we observe that after we compute the FF representations of the activations, they are well-behaved. Figure 12b shows the FF representation of activation of the first Transformer block in the ViT-M model. So, we do not need to perform further scaling to reduce the range. This makes FrameQuant to be amenable to activation quantization if necessary in practice. G. Quantizing Segmentation and Object Detection models We used FrameQuant to quantize the Swin backbone for Object Detection and Segmentation Models. We compare our results with RepQ-ViT (Li et al., 2023), one of the state-of-the-art publicly available quantization methods in this regime. Since our primary focus is quantizing the weights of the Transformer, for a fair comparison, we use RepQ-ViT to quantize the rest of the parameters, such as activations and norm layers. From Table 12, we can see that FrameQuant performs similarly to RepQ-ViT, and the main benefits of frameQuant kick in at very low bit widths. H. Additional Experiments on Language models H.1. Evaluation on the C4 dataset This section is a continuation of section 4.4. Here, we present the perplexity of different models from OPT and Llama2 classes on the C4 (Raffel et al., 2020) dataset. Consistent with our previous experiments, we see that FrameQuant with 1× the redundancy performs better than all the methods under consideration. With an additional redundancy of r = 1.1×, FrameQuant closes the gap between the full precision model across all the sizes from different families of Large Language Models. The results are shown in table 13. 20Flexible Low-Bit Quantization for Transformers Method Precision of Swin Backbone Precision of rest of the network MoBY Mask RCNN w. Swin-T (APbox / APmask) MoBY Cascade Mask RCNN with Swin-T (APbox / APmask) Full Precision W32/A32 W32/A32 43.6/39.6 48 .1/41.5 RepQ-ViT W6/A6 W6/A6 42.6/39.0 47 .7/41.3 FrameQuant W6/A6 W6/A6 42.7/39.0 47 .8/41.3 RepQ-ViT W4/A4 W4/A4 34.2/32.3 43 .8/38.6 FrameQuant W4/A4 W4/A4 34.5/32.5 44 .3/39.1 RepQ-ViT W3/A4 W4/A4 27.5/26.4 38 .9/34.8 FrameQuant W3/A4 W4/A4 29.3/27.9 41 .2/36.7 RepQ-ViT W3/A4 W3/A4 16.9/16.9 32 .4/29.2 FrameQuant W3/A4 W3/A4 21.7/21.5 35 .2/31.4 Table 12.Performance of quantized models with Swin-T backbone on the Object Detection and Segmentation tasks. We can see that FrameQuant performs similarly to RepQ-Vit at higher bit widths. The main benefits of Frame representations kick in at very low bit-widths. Method #bits OPT Llama2 125M 350M 1.3B 2.7B 6.7B 7B 70B Full-Precision 16 26.56 22 .58 16 .07 14 .34 12 .71 7.26 5 .71 GPTQ 2 2203.89 5325 .65 4139 .91 4058 .41 528 .41 2265.09 68 .83 QuIP 2 543.63 432 .56 28 .91 21 .49 16 .92 26.61 8 .65 FrameQuant (r = 1.0) 2 226.15 95.38 27.90 20.74 17.28 19.62 7.85 FrameQuant (r = 1.1) 2.2 91.29 47.62 22.39 17.75 15.33 11.23 6.86 Table 13.Perplexity (smaller the better) of Llama2 and OPT models on C4 dataset when quantized to 2 (or 2.2) bits by different methods. Method #bits ARC (challenge) ARC (easy) BoolQ HellaSwag PIQA WinoGrande Full-Precision 16 43.43 76 .35 77 .71 57 .16 78 .07 69 .06 GPTQ 2 22.44 24 .58 41 .19 25 .93 51 .85 50 .43 QuIP 2 22.27 42 .76 50 .31 34 .04 61 .75 52 .64 FrameQuant (r = 1.0) 2 23.98 55.39 63.52 36.76 66.65 55.80 FrameQuant (r = 1.1) 2.2 31.91 65.53 67.95 46.46 73.07 63.61 Table 14.Evaluating Llama2-7B model quantized with different methods on a range of downstream tasks. H.2. Perplexity of Quantized Llama2 7B Figure 14 shows the perplexity of Llama2-7B model quantized by different quantization schemes. We see that FrameQuant with a redundancy of 1x already performs better than all other methods. With increasing redundancy, the performance becomes closer to the Full precision model. H.3. Performance on Downstream tasks In this experiment, we finetune the Llama2-7B model on downstream tasks. We ran experiments on ARC challenge, ARC easy (Clark et al., 2018), BoolQ (Clark et al., 2019), HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020) and WinoGrande (Sakaguchi et al., 2021). We used LM-evaluation harness (Gao et al., 2023) for running our experiments on these diverse tasks. The results are presented in table 14. We can see that again in line with our previous experiments, the LLM quantized with FrameQuant with no redundancy already performs better than all the other methods on the downstream tasks. With added redundancy, this performance goes up across all the tasks under consideration. Based on our previous experiments and as observed in (Chee et al., 2023), we expect the performance gap between the full precision model and the quantized model to go down as the size of the models increases. 21Flexible Low-Bit Quantization for Transformers (a) Perplexity of Llama2-7B model on WikiText2 dataset  (b) Perplexity of Llama2-7B model on C4 dataset Figure 14.Perplexity of Llama2-7B model on WikiText2 and C4 datasets. FrameQuant performs better than all quantization methods tested. With increasing redundancy, we see that the performance of the model also improves as indicated by the theory. I. Robustness guarantees We provide additional details on two specific results (mentioned in the main paper) that apply to our construction. We encourage the interested reader to refer to (Christensen, 2018; Casazza & Kutyniok, 2012) for a more comprehensive treatment of the topic. LMMSE estimation from fusion frame measurements. For a given layer l FrameQuant quantizes the transformed weights matrix Dl which is given by Dl = PT l (ΘlPprev). We can treat ˆDl as a projection of ΘlPprev which is corrupted by noise. During inference, the activations of this layer are given by Zl = Pl ˆDlCprev. But, can we do better? Instead of directly applying the synthesis operator Pl to compute Zl from its FF representations ˆDlCprev, we can design a simple linear filter F that minimizes the MSE in Zl because we are using a quantized ˆDl. The final expression for the computation of the output of the layer will be Zl = F ˆDlCprev. This linear MSE minimizer F is known to be the Wiener Filter and has a closed-form expression with various levels of approximation. The following theorem states that the Wiener filter minimizes MSE when the Fusion Frame is tight. Theorem I.1. (Kutyniok et al., 2009) For the model described above, the MSE in linearly estimating the signal from its noisy projections is minimized when the Fusion Frame is tight Consistent Reconstruction. Assuming the same mode of representing the modified weights Dl as above, during inference, we can get a consistent estimate of the weights ( ˆΘl) from ˆDl if one were to solve a linear program for ˆX \u0014 Pl −Pl \u0015 ˆXl ≤ \u0014∆ 2 + ˆDl ∆ 2 − ˆDl \u0015 , where ∆ is the quantization level. Here, the constraints in the Linear Program make sure that ˆX belongs to the regions where valid unquantized values must lie, thereby removing the out-of-sub-space error (Goyal et al., 1998). We can get the estimated weights from ˆXl as ˆΘl = ˆXlPT prev. Using this consistent reconstruction yields estimates with an MSE which is upper bounded by O(1/r2) (Goyal et al., 1998) J. Synopsis of Construction of Tight Fusion Frames Here, we give a brief synopsis of an algorithm for generating Tight Fusion Frames for the curious reader. (Casazza et al., 2011) was the first to introduce a systematic method for constructing UNTFs (Unit Norm Tight Frames) that play a key role in constructing Tight Fusion Frames. They also characterize the (k, ρ, d) values for which a Tight Fusion Frame exists. 22Flexible Low-Bit Quantization for Transformers Whenever such a TFF exists, we can construct Tight Fusion Frames by using their algorithm. There are two main parts to the algorithm. 1. Play Spectral Tetris to generate a UNTF of d elements in Cρ 2. Modulate this UNTF with complex roots of unity to generate a (k, ρ, d) TFF for Cd So, the first step is to generate a “smaller” frame and in the next step, we modulate the smaller frame to generate a “larger” Tight Fusion Frame. After generating a TFF for Cd we can easily extend it to the Real Field by applying the entrywise map x + iy 7→ \u0014x −y y x \u0015 . We describe the algorithm with the help of an example for the simplicity of explanation. We aim to construct a (5,4,11) TFF. So, k = 5, ρ= 3, d= 11. J.1. Spectral Tetris As the name suggests UNTFs are Tight frames where each frame vector has a unit norm. We construct a 4 × 11 matrix F whose columns are the frame vectors for C4 which satisfies • Columns of unit norm • Orthogonal rows, meaning FF ∗ is diagonal • Rows of constant norm, meaning FF ∗ is a constant multiple of identity matrix with the constant being 11 4 We start with a matrix F =   1 1 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?   This leaves a norm of 11 4 − 2 = 3 4 to be filled in the first row. This can easily be added using a 2 × 2 matrix T(x) where x = 3 4 . T(x) is defined as: T(x) := 1√ 2 \u0014 √x √x√2 − x −√2 − x \u0015 , T (x)T∗(x) = \u0014x 0 0 2 − x \u0015 After inserting T(x), F is now F =   1 1 √ 3√ 8 √ 3√ 8 0 0 0 0 0 0 0 0 0 √ 5√ 8 − √ 5√ 8 ? ? ? ? ? ? ? 0 0 0 0 ? ? ? ? ? ? ? 0 0 0 0 ? ? ? ? ? ? ?   Then we continue adding ones in row two until the norm becomes less than 11 4 . F =   1 1 √ 3√ 8 √ 3√ 8 0 0 0 0 0 0 0 0 0 √ 5√ 8 − √ 5√ 8 1 ? ? ? ? ? ? 0 0 0 0 0 ? ? ? ? ? ? 0 0 0 0 0 ? ? ? ? ? ?   Now we insert T(x) with the remaining norm. We repeat this process until all the rows are filled. The Final F is given by F =   1 1 √ 3√ 8 √ 3√ 8 0 0 0 0 0 0 0 0 0 √ 5√ 8 − √ 5√ 8 1 √ 2√ 8 √ 2√ 8 0 0 0 0 0 0 0 0 0 √ 6√ 8 − √ 6√ 8 1 √ 7√ 8 √ 7√ 8 0 0 0 0 0 0 0 0 0 √ 7√ 8 − √ 7√ 8 1   23Flexible Low-Bit Quantization for Transformers   1 1 √ 3√ 8 √ 3√ 8 0 0 0 0 0 0 0 1 ω √ 3√ 8 ω2 √ 3√ 8 ω3 0 0 0 0 0 0 0 1 ω2 √ 3√ 8 ω4 √ 3√ 8 ω 0 0 0 0 0 0 0 1 ω3 √ 3√ 8 ω √ 3√ 8 ω4 0 0 0 0 0 0 0 1 ω4 √ 3√ 8 ω3 √ 3√ 8 ω2 0 0 0 0 0 0 0 0 0 √ 5√ 8 − √ 3√ 8 1 √ 2√ 8 √ 2√ 8 0 0 0 0 0 0 √ 5√ 8 ω2 − √ 3√ 8 ω3 ω4 √ 2√ 8 √ 2√ 8 ω 0 0 0 0 0 0 √ 5√ 8 ω4 − √ 3√ 8 ω ω 3 √ 2√ 8 √ 2√ 8 ω2 0 0 0 0 0 0 √ 5√ 8 ω − √ 3√ 8 ω4 ω2 √ 2√ 8 √ 2√ 8 ω3 0 0 0 0 0 0 √ 5√ 8 ω3 − √ 3√ 8 ω2 ω √ 2√ 8 √ 2√ 8 ω4 0 0 0 0 0 0 0 0 0 √ 6√ 8 − √ 6√ 8 1 √ 7√ 8 √ 7√ 8 0 0 0 0 0 0 √ 6√ 8 − √ 6√ 8 ω ω 2 √ 7√ 8 ω3 √ 7√ 8 ω4 0 0 0 0 0 0 √ 6√ 8 − √ 6√ 8 ω2 ω4 √ 7√ 8 ω √ 7√ 8 ω3 0 0 0 0 0 0 √ 6√ 8 − √ 6√ 8 ω3 ω √ 7√ 8 ω4 √ 7√ 8 ω2 0 0 0 0 0 0 √ 6√ 8 − √ 6√ 8 ω4 ω3 √ 7√ 8 ω2 √ 7√ 8 ω1 0 0 0 0 0 0 0 0 0 √ 7√ 8 − √ 7√ 8 1 0 0 0 0 0 0 0 0 √ 7√ 8 ω3 − √ 7√ 8 ω4 1 0 0 0 0 0 0 0 0 √ 7√ 8 ω − √ 7√ 8 ω3 1 0 0 0 0 0 0 0 0 √ 7√ 8 ω4 − √ 7√ 8 ω2 1 0 0 0 0 0 0 0 0 √ 7√ 8 ω2 − √ 7√ 8 ω 1   Table 15.(5, 4, 11)-TFF for C11. Here, ω = ei2π/5. Each pair of rows belongs to the same subspace if their indices differ by a multiple of 5 J.2. Modulation In the second step, we modulate the F matrix with complex roots of unity, one subspace at a time. So, for each ki = 0, 1, 2, . . . k− 1, we construct a row vector wki = \u0014\u0010 e i2πki k \u00110 \u0010 e i2πki k \u00111 \u0010 e i2πki k \u00112 . . . \u0010 e i2πki k \u0011d−1\u0015 We multiply each row of F with wki to generate the orthogonal basis for different subspaces indexed by ki. Theorem 14 by Casazza et al. (2011) proves that the Fusion Frames generated by this algorithm are Tight. The Final Fusion Frame vectors are shown in Table 15. K. Storage benefits and Computational complexity during inference K.1. Storage benefits Consider an example where we are quantizing a weight matrix Θl of dimension 1024 × 1024 using FrameQuant with a redundancy factor of r = 1.1×. The size of the original matrix using FP32 is 4MB. After transforming the weights to map within the FF representation space, the transformed weights Dl have dimensions 1126 × 1126, which are quantized and represented using 2 bits. This quantized weight ˆDl has a size of 0.3MB. Along with the quantized weights, we need to store the bias and scale values for each row leading to an additional storage of 1024 FP32 values, which will incur an additional cost of 0.007MB. All this sums up to a storage of 0.307MB from an initial 4MB giving a savings of 13x in the storage requirements. Since we can generate the Fusion Frames on the fly, we just need to store the (k, ρ, d) values, and a seed to 24Flexible Low-Bit Quantization for Transformers generate the random rotation matrix which incurs negligible storage costs. Table 6 shows the sizes of Llama2 models when compressed with FrameQuant. K.2. Computational Complexity during Inference Consider a linear layer in a transformer model with weights Θl of dimensions d × d. Using FrameQuant these weights are transformed to Dl and the quantized weights ˆDl are stored. Let the parameters of the TFF used for quantization be (k, ρ, d). As a recap, k is the number of subspaces, ρ is the dimension of each subspace and d is the dimension of the Hilbert space we are operating in. So, the redundancy in Frame representations is r = kρ d . Let, Tl, Tprev ∈ Rd×kρ be the vectorized Orthonormal basis for the current layer, and the previous layer respectively. During inference, the quantized weights ˆDl are transformed to the weight space as ˆΘl = Pl ˆDlPT prev. Here, Pl = Rl(Tl), Pprev = Rprev(Tprev), where Rl, Rprev ∈ Rd×d denote the rotation matrices for the current and the previous layers respectively. So, the overall operation is ˆΘl = RlTl ˆDlTprevT RT prev. Let us first look at the ˆDlTprevT operation. TprevT is a block diagonal matrix constructed as defined in section 2.2. It has ρ blocks along the diagonal, each with k rows and at most ⌈d ρ ⌉ + 2 columns. The order of the computations required to generate this matrix is O(dk). The computation complexity of ˆDlTprevT is O(d ρ kρdr) = O(d2kr). So, the overall computational complexity for the computation of TprevT and multiplication with ˆDl is O(d2kr). Now, consider the left multiplication with Tl. Tl is again a block diagonal matrix similar to TprevT . But it is multiplying a quantity with dimensions kρ × d. Hence this multiplication has a computational complexity of O(d2k). The worst-case computational complexity of multiplication with the TFF orthonormal basis of current and previous layers is O(d2kr). The final Rl, RT prev are orthogonal rotation matrices which can be efficiently computed in O(d2 log d) time using random projections such as (Le et al., 2013) or any other efficient implementation. Combining all these calculations, the overall computational complexity of transforming the weights during inference is O(d2(kr + logd)). Note that since all of these are matrix operations, they run on GPU in a vectorized manner. Table 7 shows the inference speeds of the quantized models on a Nvidia A100 GPU. 25",
      "meta_data": {
        "arxiv_id": "2403.06082v2",
        "authors": [
          "Harshavardhan Adepu",
          "Zhanpeng Zeng",
          "Li Zhang",
          "Vikas Singh"
        ],
        "published_date": "2024-03-10T04:01:49Z",
        "pdf_url": "https://arxiv.org/pdf/2403.06082v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces FrameQuant, a post-training quantization scheme for Transformer models, enabling quantization to two bits (or 2.x bits via fractional redundancy) with minimal accuracy loss. The key contribution is leveraging Fusion Frames from Harmonic analysis, performing quantization not in the original weight space, but in these redundant representations. This approach provides inherent robustness to quantization noise, offering theoretical guarantees and flexibility in balancing model size and quality. FrameQuant empirically demonstrates consistent performance improvements over existing baselines across 15 popular Vision Transformers and Large Language Models (OPT, Llama2) on tasks like ImageNet classification and perplexity evaluation.",
        "methodology": "FrameQuant employs a layer-by-layer post-training quantization (PTQ) scheme. The core idea is to transform a layer's weight matrix (Θl) into a Fusion Frame (FF) representation (Dl = PTl ΘlPprev) and quantize this transformed matrix (ˆDl) instead of the original weights. This is done by minimizing a proxy loss in the FF representation space, analogous to Hessian-based PTQ methods like GPTQ. A key modification includes clipping the transformed weights Dl at ±2σ before quantization, which improves stability by managing outliers, as FF representations distribute energy uniformly. During inference, layer inputs are first transformed into their FF representations using the analysis operator, processed by the quantized FF weights, and then reconstructed to outputs using the synthesis operator. Fusion Frames are constructed using an algorithm that generates Tight Fusion Frames (TFFs), characterized by a sequence of equidimensional subspaces.",
        "experimental_setup": "FrameQuant was evaluated on a diverse set of Vision Transformer architectures (ViT, DeiT, DeiT III, Swin, ranging from Tiny to Huge sizes) for ImageNet-1K classification, and Large Language Models (OPT and Llama2 families, from 125M to 70B parameters) for perplexity on WikiText2 and C4 datasets, and downstream NLP tasks (ARC, BoolQ, HellaSwag, PIQA, WinoGrande). Experiments also included segmentation and object detection tasks using Swin backbones. Models were quantized primarily to 2 bits, with extensions to 2.x bits using redundancy. A small calibration dataset of 128 ImageNet-1K images was used for PTQ. Performance was compared against baselines including PTQ4ViT, GPTQ, QuIP (with and without 2σ clipping), and ZeroQuant. Inference speeds were measured on an Nvidia A100 GPU.",
        "limitations": "The raw inference speed of FrameQuant is generally lower than methods like GPTQ due to additional computational overhead for transforming weights between Fusion Frame representations and regular weight space, though efficient kernels and specific projection methods are used to mitigate this. Unsuccessful attempts were made to achieve 1-bit per weight quantization. The computational complexity during inference incurs an overhead of O(d^2(kr + logd)). The main experimental focus was on weight quantization, and comprehensive head-to-head comparisons for activation quantization were not performed. The paper also mentions that Quantization-Aware Training (QAT) was not included in the experiments, and the impact on neural language model scaling laws was not fully explored.",
        "future_research_directions": "Future research directions include achieving tighter integration with hardware to unlock greater efficiency gains, developing more efficient kernels for GPU operations to improve inference speeds, and adapting FrameQuant for Quantization-Aware Training (QAT) to further enhance robustness and performance, potentially by leveraging Fusion Frames to inform bias and minimize 'out of subspace error'. Further exploration into the theoretical limits of ultra-low bit quantization within the FrameQuant framework, as well as applying and evaluating known denoising filters (e.g., Wiener filter) during inference, are also potential avenues. Investigating the impact of FrameQuant on neural language model scaling laws is another suggested direction."
      }
    },
    {
      "title": "2DQuant: Low-bit Post-Training Quantization for Image Super-Resolution",
      "abstract": "Low-bit quantization has become widespread for compressing image\nsuper-resolution (SR) models for edge deployment, which allows advanced SR\nmodels to enjoy compact low-bit parameters and efficient integer/bitwise\nconstructions for storage compression and inference acceleration, respectively.\nHowever, it is notorious that low-bit quantization degrades the accuracy of SR\nmodels compared to their full-precision (FP) counterparts. Despite several\nefforts to alleviate the degradation, the transformer-based SR model still\nsuffers severe degradation due to its distinctive activation distribution. In\nthis work, we present a dual-stage low-bit post-training quantization (PTQ)\nmethod for image super-resolution, namely 2DQuant, which achieves efficient and\naccurate SR under low-bit quantization. The proposed method first investigates\nthe weight and activation and finds that the distribution is characterized by\ncoexisting symmetry and asymmetry, long tails. Specifically, we propose\nDistribution-Oriented Bound Initialization (DOBI), using different searching\nstrategies to search a coarse bound for quantizers. To obtain refined quantizer\nparameters, we further propose Distillation Quantization Calibration (DQC),\nwhich employs a distillation approach to make the quantized model learn from\nits FP counterpart. Through extensive experiments on different bits and scaling\nfactors, the performance of DOBI can reach the state-of-the-art (SOTA) while\nafter stage two, our method surpasses existing PTQ in both metrics and visual\neffects. 2DQuant gains an increase in PSNR as high as 4.52dB on Set5 (x2)\ncompared with SOTA when quantized to 2-bit and enjoys a 3.60x compression ratio\nand 5.08x speedup ratio. The code and models will be available at\nhttps://github.com/Kai-Liu001/2DQuant.",
      "full_text": "2DQuant: Low-bit Post-Training Quantization for Image Super-Resolution Kai Liu1, Haotong Qin2, Yong Guo3, Xin Yuan4, Linghe Kong1∗, Guihai Chen1, Yulun Zhang1∗ 1Shanghai Jiao Tong University, 2ETH Zürich, 3Max Planck Institute for Informatics, 4Westlake University Abstract Low-bit quantization has become widespread for compressing image super- resolution (SR) models for edge deployment, which allows advanced SR models to enjoy compact low-bit parameters and efficient integer/bitwise constructions for storage compression and inference acceleration, respectively. However, it is notorious that low-bit quantization degrades the accuracy of SR models compared to their full-precision (FP) counterparts. Despite several efforts to alleviate the degradation, the transformer-based SR model still suffers severe degradation due to its distinctive activation distribution. In this work, we present a dual-stage low- bit post-training quantization (PTQ) method for image super-resolution, namely 2DQuant, which achieves efficient and accurate SR under low-bit quantization. The proposed method first investigates the weight and activation and finds that the distribution is characterized by coexisting symmetry and asymmetry, long tails. Specifically, we propose Distribution-Oriented Bound Initialization (DOBI), using different searching strategies to search a coarse bound for quantizers. To obtain refined quantizer parameters, we further propose Distillation Quantization Calibration (DQC), which employs a distillation approach to make the quantized model learn from its FP counterpart. Through extensive experiments on different bits and scaling factors, the performance of DOBI can reach the state-of-the-art (SOTA) while after stage two, our method surpasses existing PTQ in both met- rics and visual effects. 2DQuant gains an increase in PSNR as high as 4.52dB on Set5 (×2) compared with SOTA when quantized to 2-bit and enjoys a 3.60× compression ratio and 5.08× speedup ratio. The code and models will be available at https://github.com/Kai-Liu001/2DQuant. 1 Introduction As one of the most classical low-level computer vision tasks, image super-resolution (SR) has been widely studied with the significant development of deep neural networks. With the abil- ity to reconstruct high-resolution (HR) image from the corresponding low-resolution (LR) image, SR has been widely used in many real-world scenarios, including medical imaging [ 13, 21, 19], surveillance [44, 37], remote sensing [1], and mobile phone photography. With massive parameters, DNN-based SR models always require expensive storage and computation in the actual application. Some works have been proposed to reduce the demand for computational power of SE models, like lightweight architecture design and compression. One kind of approach investigates lightweight and efficient models as the backbone for image SR. This progression has moved from the earliest convolutional neural network (CNNs) [10, 11, 25, 47] to Transformers [46, 29, 42, 40, 4, 3] and their combinations. The parameter number significantly decreased while maintaining or even enhancing performance. The other kind of approach is compression, which focuses on reducing the parameter (e.g., pruning and distillation) or bit-width (quantization) of existing SR models. Model quantization [7, 9, 20, 28] is a technology that compresses the floating-point parameters of a neural network into lower bit-width. The discretized parameters are homogenized into restricted ∗Corresponding authors: Yulun Zhang, yulun100@gmail.com, Linghe Kong, linghe.kong@sjtu.edu.cn Preprint. Under review. arXiv:2406.06649v1  [eess.IV]  10 Jun 2024candidate values and cause heterogenization between the FP and quantized models, leading to severe performance degradation. Considering the process, quantization approaches can be divided into quantization-aware training (QAT) and post-training quantization (PTQ). QAT simultaneously optimizes the model parameters and the quantizer parameters [6, 16, 26, 48], allowing them to adapt mutually, thereby more effectively alleviating the degradation caused by quantization. However, QAT often suffers from a heavy training cost and a long training time, and the burden is even much heavier than the training process of the FP counterparts, which necessitates a large amount of compatibility and makes it still far from practical in training-resource-limited scenarios. Urban100: img_092 HR Bicubic Percentile [27] DBDC+Pac [39] Ours FP Figure 1: Existing methods suffer from blurring artifacts. Fortunately, post-training quantization emerges as a promising way to quantize models at a low training cost. PTQ fixes the model parameters and only determines the quantizer parameters through search or optimization. Previous researches [39, 26] on PTQ for SR has primarily focused on CNN-based models such as EDSR [30] and SRResNet [24]. However, these quantization methods are not practical for deployment for two reasons. Firstly, these CNN-based models themself require huge space and calculation resources. Their poor starting point makes them inferior to advanced models in terms of parameters and computational cost, even after quantization. As shown in Table 1, the light version of SwinIR needs only 16.2% parameters and 15.9% FLOPs compared with quantized EDSR. But its PSNR metric is close to that of the FP EDSR. While the previous PTQ algorithm, DBDC+Pac, suffers from unacceptable degradation in both visual and metrics. Secondly, most of these methods can not adapt well to Transformer-based models because of the unadaptable changes in weight and activation distributions. As shown in Figure 1, when applied on SwinIR, the existing methods still suffer from distorted artifacts compared with FP or HR. Table 1: Complexity and performance (×4). Model EDSR [30]EDSR (4bit) [39]SwinIR-light [29]DBDC+Pac (4bit) [39]Ours (4bit) Params (MB) 172.36 21.55 3.42 1.17 1.17Ops (G) 823.34 103.05 16.74 4.19 4.19PNSR on Urban100 26.64 25.56 26.47 24.94 25.71 Therefore, we conducted a post-training quantization analysis on super-resolution with a classical Transformer-based model SwinIR [29]. The weight and activation distribution is characterized by coexisting symmetry and asymmetry, long tails. Firstly, if the previous symmetric quantization method is applied for asymmetric distribution, at least half of the candidates are completely ineffective. Besides, the long tail effect causes the vast majority of floating-point numbers to be compressed into one or two candidates, leading to worse parameter homogenization. Furthermore, with such a small number of parameters, SwinIR’s information has been highly compressed, and quantizing the model often results in significant performance degradation. Nevertheless, the excellent performance and extremely low computational requirements of Transformer-based models are precisely what is needed for deployment in real-world scenarios. In this paper, we propose 2DQuant, a two-stage PTQ algorithm for image super-resolution tasks. To enhance the representational capacity in asymmetry scenarios, we employ a quantization method with two bounds. The bounds decide the candidate for numbers out of range and the interval of candidates in range. First, we propose distribution-oriented Bound Initialization(DOBI), a fast MSE-based searching method. It is designed to minimize the value heterogenization between quantized and FP models. Two different MSE [5] search strategies are applied for different distributions to avoid nonsense traversal. This guarantees minimum value shift while maintaining high speed and efficiency in the search process. Second, we propose Distillation Quantization Calibration(DQC), a training- based method. It is designed to adjust each bound to its best position finely. This ensures that the outputs and intermediate feature layers of the quantized model and that of the FP model should remain as consistent as possible. Thereby DQC allows the quantizer parameters to be finely optimized toward the task goal. The contributions of this paper can be summarized as follows: (1) To the best of our knowledge, we are the first to explore PTQ with Transformer-based model in SR thoroughly. We design 2DQuant, a unique and efficient two-stage PTQ method (see Figure 2) for image super-resolution, which utilizes DOBI and DQC to optimize the bound from coarse to fine. (2) In the first stage of post-quantization, we use DOBI to search for quantizer parameters, employing customized search strategies for different distributions to balance speed and accuracy. In the second stage, we design DQC, a more fine-grained optimization-based training strategy, for the quantized model, ensuring it aligns with the FP model on the calibration set. 2FP Model Quant Model Moving Simultaneously Moving Upper Bound Only FP weights and activations Distribution Oriented Bound Initialization Distillation Quantization Calibration Searching  Direction Initial Bound DQC Loss Forward Full-precision Layer Quantized Layer Figure 2: The overall pipeline of our proposed 2DQuant method. The whole pipeline contains two stages, optimizing the clipping bound from coarse to fine. In stage 1, we design DOBI to efficiently obtain the coarse bound. In stage 2, DQC is performed to finetune clipping bounds and guarantee the quantized model learns the full-precision (FP) model’s feature and output information. (3) Our 2DQuant can compress Transformer-based model to 4,3,2 bits with the compression ratio being 3.07×, 3.31×, and 3.60× and speedup ratio being 3.99×, 4.47×, and 5.08×. No additional module is added so 2DQuant enjoys the theoretical upper limit of compression and speedup. (4) Through extensive experiments, our 2DQuant surpasses existing SOTA on all benchmarks. We gain an increase in PSNR by as high as 4.52dB in Set5 ( ×2) when compressed to 2 bits, and our method has a more significant increase when compressed to lower bits. 2 Related work Image super-resolution. Deep CNN networks have shown excellent performance in the field of image super-resolution. The earliest SR-CNN [ 10, 11] method adopted a CNN architecture. It surpassed previous methods in the image super-resolution domain. In 2017, EDSR [ 30] won the NTIRE2017 [38] championship, becoming a representative work of CNNs in the SR by its excellent performance. Thereafter, with the continuous development of Vision Transformers (ViT) [ 12], models based on the ViT architecture have surpassed many CNN networks. These Transformer- based models achieve significant performance improvements and they have fewer parameters and lower computational costs. Many works have modified the ViT architecture, achieving continuous improvements. A notable example is SwinIR [29]. With a simple structure, it outperforms many CNN- based models. However, previous explorations of post-quantization in the super-resolution domain have been limited to CNN-based models. They focus on models like EDSR [30] or SRResNet [24]. It is a far cry from advanced models no matter in parameters, FLOPs, or performance. Currently, there is still a research gap in post-quantization for Transformer architectures. Model quantization. In the field of quantization, quantization methods are mainly divided into PTQ and QAT. QAT is widely accepted due to its minimal performance degradation. PAMS [26] utilizes a trainable truncated parameter to dynamically determine the upper limit of the quantization range. DAQ [17] proposed a channel-wise distribution-aware quantization scheme. CADyQ [16] is proposed as a technique designed for SR networks and optimizes the bit allocation for local regions and layers in the input image. However, QAT usually requires training for as long as or even longer than the original model, which becomes a barrier for real scenarios deployment. Instead of training the model from scratch, existing PTQ methods use the pre-trained models. PTQ algorithms only find the just right clipping bound for quantizers, saving time and costs. DBDC+Pac [ 39] is the first to optimize the post-training quantization for image super-resolution task. It outperforms other existing PTQ algorithms. Whereas, they only focus on EDSR [30] and SRResNet [24]. Their 4-bit quantized version is inferior to advanced models in terms of parameters and computational cost, let alone performance. It reveals a promising result for PTQ applying on SR, but using a more advanced model could bridge the gap between high-performance models and limited calculation resource scenarios. 3 Methodology To simulate the precision loss caused by quantization, we use fake-quantize [ 22],i.e.quantization- dequantization, for activations and weights. and the process can be written as vc = Clip(v, l, u), v r = Round(2N − 1 u − l (vc − l)), v q = u − l 2N − 1vr + l, (1) 3LayerNorm X WQ WK WV Q K V BMM Scale SoftMax BMM Projection LayerNorm FC2 GELU FC1 FP32 INT4 Figure 3: Quantization scheme for SwinIR Transformer blocks. Fake quantization and INT arithmetic are performed in all compute-intensive operators including all linear layers and batch matmul. Lower bits such as 3 or even 2 are also permitted. Dropout of attention and projection is ignored where v denotes the value being fake quantized, which can be weight or activation. l and u are the lower and upper bounds for clipping, respectively. Clip(v, l, u) = max(min(v, u), l), and Round rounds the input value to the nearest integer. vc denotes the value after clipping, and vr denotes the integer representation of v, and vq denotes the value after fake quantization. The Clip and Round operations contribute to reducing the parameters and FLOPs but also introduce quantization errors. Table 2: FLOPs distribution. Module FLOPs (G)Ratio (%) Linear & BMM 14.34 85.66Conv 2.33 13.90Other 0.07 0.44Total 16.74 100.00 Figure 3 shows the basic structure of the Transformer block. We have quantized all the modules with a significant computational load within them, effectively reducing the model’s FLOPs. Table 2 shows the FLOPs needed for each module. The Linear layers and matrix multiplication account for approximately 86% of the compu- tation load, which are all transformed into integer arithmetic. When performing gradient backpropagation, we follow the Straight-Through Estimator [8] (STE) style: ∂vq ∂u = ∂vc ∂u + 1 2n − 1vr − vc − l u − l , ∂vq ∂l = ∂vc ∂l − 1 2n − 1vr + vc − l u − l , (2) where ∂vc ∂u = H(u −v) and vc ∂l = H(l −v), H(·) denotes Heaviside step function [45]. This formula approximates the direction of gradient backpropagation, allowing training-based optimization to proceed. The derivation of the formula can be found in the supplementary material. Figure 2 shows the whole pipeline of 2DQuant, which is a two-stage coarse-to-fine post-training quantization method. The first stage is DOBI, using two strategies to minimize the value shift while the second stage is DQC, optimizing two bound of each quantizer towards the task goal. 3.1 Analysis of data distribution To achieve better quantization results, we need to analyze the distribution of the model’s weights and activations in detail. We notice that the data distribution shows a significantly different pattern from previous explorations, invalidating many of the previous methods. The weights and activations distri- bution of SwinIR is shown in Figure 4. More can be found in supplemental material. Specifically, the weights and activations of SwinIR exhibit noticeable long-tail, coexisting symmetry and asymmetry. Weight. The weights of all linear layers are symmetrically distributed around zero, showing clear symmetry, and are generally similar to a normal distribution. This is attributed to the weight decay applied to weights, which provides quantization-friendly distributions. From the value shift perspective, both symmetric and asymmetric quantization are tolerable. Whereas, from the vantage point of task objectives, asymmetric quantization possesses the potential to offer a markedly enhanced information density, thus elevating the overall precision of the computational processes involved. Activations. As for activations, they exhibit obvious periodicity in different Transformer Blocks. For V or the input of FC1, the obtained activation values are symmetrically distributed around 0. However, for the attention map or the input of FC2 in each Transformer Block, due to the Softmax calculation or the GELU [14] activation function, the minimum value is almost fixed, and the overall distribution is similar to an exponential distribution. Therefore, the data in SwinIR’s weights and activations exhibit two distinctly different distribution characteristics. Setting asymmetric quantization and different search strategies for both can make the search rapid and accurate. 3.2 Distribution-oriented bound initialization Because the data distribution exhibits a significant long-tail effect, we must first clip the range to avoid low effective bits. Common clipping methods include density-based, ratio-based, and MSE-based approaches. The first two require manually specifying the clipping ratio, which significantly affects the clipping outcome and necessitates numerous experiments to determine the optimal ratio. Thus we proposed the Distribution-Oriented Bound Initialization (DOBI) to search the bound for weight and 40.0 0.2 0.4 0.6 0.8 1.0 /uni0000003e/uni00000013/uni00000011/uni00000013/uni00000013/uni0000000f/uni00000014/uni00000011/uni00000013/uni00000013/uni00000040 /uni00000024/uni00000046/uni00000057/uni0000001d/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000011/uni00000013/uni00000011/uni00000045/uni0000004f/uni00000052/uni00000046/uni0000004e/uni00000011/uni00000013/uni00000003/uni00000024/uni00000057/uni00000057/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000052/uni00000051 2  0 2 /uni0000003e/uni00000010/uni00000017/uni00000011/uni00000019/uni00000014/uni0000000f/uni00000016/uni00000011/uni0000001b/uni0000001a/uni00000040 /uni00000024/uni00000046/uni00000057/uni0000001d/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000011/uni00000014/uni00000011/uni00000045/uni0000004f/uni00000052/uni00000046/uni0000004e/uni00000011/uni00000014/uni00000003/uni00000039 5.0  2.5  0.0 2.5 5.0 7.5 /uni0000003e/uni00000010/uni00000018/uni00000011/uni00000019/uni00000014/uni0000000f/uni0000001a/uni00000011/uni00000016/uni00000014/uni00000040 /uni00000024/uni00000046/uni00000057/uni0000001d/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000011/uni00000015/uni00000011/uni00000045/uni0000004f/uni00000052/uni00000046/uni0000004e/uni00000011/uni00000015/uni00000003/uni00000029/uni00000026/uni00000014 0 2 4 6 /uni0000003e/uni00000010/uni00000013/uni00000011/uni00000014/uni0000001a/uni0000000f/uni00000019/uni00000011/uni00000019/uni00000014/uni00000040 /uni00000024/uni00000046/uni00000057/uni0000001d/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000011/uni00000016/uni00000011/uni00000045/uni0000004f/uni00000052/uni00000046/uni0000004e/uni00000011/uni00000016/uni00000003/uni00000029/uni00000026/uni00000015 0.2  0.0 0.2 /uni0000003e/uni00000010/uni00000013/uni00000011/uni00000016/uni00000018/uni0000000f/uni00000013/uni00000011/uni00000015/uni0000001c/uni00000040 /uni0000003a/uni00000048/uni0000004c/uni0000004a/uni0000004b/uni00000057/uni0000001d/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000011/uni00000013/uni00000011/uni00000045/uni0000004f/uni00000052/uni00000046/uni0000004e/uni00000011/uni00000013/uni00000003/uni00000033/uni00000055/uni00000052/uni0000004d/uni00000048/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051 0.50  0.25  0.00 0.25 0.50 /uni0000003e/uni00000010/uni00000013/uni00000011/uni00000019/uni00000013/uni0000000f/uni00000013/uni00000011/uni00000019/uni00000014/uni00000040 /uni0000003a/uni00000048/uni0000004c/uni0000004a/uni0000004b/uni00000057/uni0000001d/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000011/uni00000014/uni00000011/uni00000045/uni0000004f/uni00000052/uni00000046/uni0000004e/uni00000011/uni00000014/uni00000003/uni00000034/uni0000002e/uni00000039 0.75  0.50  0.25  0.00 0.25 0.50 /uni0000003e/uni00000010/uni00000013/uni00000011/uni00000019/uni0000001c/uni0000000f/uni00000013/uni00000011/uni00000019/uni00000019/uni00000040 /uni0000003a/uni00000048/uni0000004c/uni0000004a/uni0000004b/uni00000057/uni0000001d/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000011/uni00000015/uni00000011/uni00000045/uni0000004f/uni00000052/uni00000046/uni0000004e/uni00000011/uni00000015/uni00000003/uni00000029/uni00000026/uni00000014 0.5  0.0 0.5 /uni0000003e/uni00000010/uni00000013/uni00000011/uni0000001b/uni00000015/uni0000000f/uni00000013/uni00000011/uni0000001b/uni00000017/uni00000040 /uni0000003a/uni00000048/uni0000004c/uni0000004a/uni0000004b/uni00000057/uni0000001d/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000011/uni00000013/uni00000011/uni00000045/uni0000004f/uni00000052/uni00000046/uni0000004e/uni00000011/uni00000014/uni00000003/uni00000029/uni00000026/uni00000015 Figure 4: The selected representative distribution of activations (Row 1) and weights (Row 2). The range of data is marked in the figure. All weights obey symmetric distribution. The attention map and the input of FC2 are asymmetric due to softmax function and GELU function. activation, avoiding manually adjusting hyperparameters. The global optimizing goal is as follows {(li, ui)}N i=1 = arg min li,ui NX i=1 ∥vi − vqi∥2 . (3) The collection of all quantizers’ bounds {(li, ui)}N i=1 is the linchpin of quantized model performance as it indicates the candidate value for weights and activations. We note that the data distribution falls into two categories: one resembling a bell-shaped distribution and the other resembling an exponential distribution. For the bell-shaped distribution, we use a symmetric boundary-narrowing search method. Whereas, for the exponential distribution, we fix the lower bound to the minimum value of the data and only traverse the right bound. The specific search method is shown in Algorithm 1. The time complexity of Algorithm 1 is O(MK ), where M is the number of elements in data v and K is the number of search points. The condition v is symmetricalis obtained by observing the visualization of v and the activations are from the statistics on a small calibration set. 3.3 Distillation quantization calibration To further fine-tune the clipping range, we propose distillation quantization calibration (DQC) to transfer the knowledge from the FP model to the quantized model. It leverages the knowl- edge distillation [ 15] where the FP model acts as the teacher while the quantized model is the student. Specifically, for the same input image, the student model needs to continu- ously minimize the discrepancy with the teacher model on the final super-resolution output. Algorithm 1:DOBI pipeline Data: Data to be quantized v, the number of search point K, bit b Result: Clip bound l, u l ← min(v),u ← max(v); min_mse ← +∞; if v is symmetricalthen ∆l ← (max(v) − min(v))/2K; else ∆l ← 0; end ∆u ← (max(v) − min(v))/2K; while i ≤ K do li ← l + i ×∆l, ui ← u + i ×∆u; get vq based on Eq. (1); mse ← ∥v − vq∥2; if mse ≤ min_mse then min_mse ← mse; l_best ← li, u_best ← ui; end end The loss for the final output can be written as LO = 1 COHOWO ∥O − Oq∥1 , (4) where O and Oq are the final outputs of the teacher and student models, CO, HO, and WO represent the number of output channels, height, and width, respec- tively. we adopt the L1 loss for the final output, as it tends to converge more easily compared to the L2 loss [30]. As the quantized model shares the same structure with the FP model and is quantized from the FP model, the student model also need to learn to extract the same feature of the teacher model, which can be written as LF = NX i 1 CiHiWi \r\r\r\r Fi ∥Fi∥2 − Fqi ∥Fqi∥2 \r\r\r\r 2 , (5) where Fi and Fqi are the intermediate features of the teacher and student models respectively andi is the in- dex of the layer. In the field of super-resolution, there is a clear correspondence between the feature maps and the final reconstructed images, making training on feature maps crucial. since the quantized network 5and the full-precision network have identical structures, we do not need to add extra adaptation layers for feature distillation. The final loss function can be written as L = LO + λLF , (6) where λ is the co-efficient ofLF . In the second stage, based on training optimization methods, the gap between the quantized model and the full-precision model will gradually decrease. The performance of the quantized model will progressively improve and eventually converge to the optimal range. 4 Experiments 4.1 Experimental settings Data and evaluation. We use DF2K [38, 31] as the training data, which combines DIV2K [38] and Flickr2K [31], as utilized by most SR models. During training, since we employ a distillation training method, we do not need to use the high-resolution parts of the DF2K images. For validation, we use the Set5 [2] as the validation set. After selecting the best model, we tested it on five commonly used benchmarks in the SR field: Set5 [2], Set14 [43], B100 [34], Urban100 [18], and Manga109 [35]. On the benchmarks, we input low-resolution images into the quantized model to obtain reconstructed images, which we then compared with the high-resolution images to calculate the metrics. We do not use self-ensemble in the test stage as it increases the computational load eightfold, but the improvement in metrics is minimal The evaluation metrics we used are the most common metrics PSNR and SSIM [41], which are calculated on the Y channel (i.e., luminance) of the YCbCr space. Implementation details. We use SwinIR-light [29] as the backbone and provide its structure in the supplementary materials. We conduct comprehensive experiments with scale factors of 2, 3, and 4 and with 2, 3, and 4 bits, where Our hyperparameter settings remain consistent. During DOBI, we use a search step number of K=100, and the statistics of activations are obtained from 32 images in DF2K being randomly cropped to retain only 3 ×64×64. During DQC, we use the Adam [ 23] optimizer with a learning rate of 1 ×10−2, betas set to (0.9, 0.999), and a weight decay of 0. We employ CosineAnnealing [33] as the learning rate scheduler to stabilize the training process. Data augmentation is also performed. We randomly utilize rotation of 90°, 180°, and 270° and horizontal flips to augment the input image. The total iteration for training is 3,000 with batch size of 32. Our code is written with Python and PyTorch [36] and runs on an NVIDIA A800-80G GPU. 4.2 Comparison with state-of-the-art methods The methods we compared include MinMax [ 22], Percentile [ 27], and the current SOTA post- quantization method in the super-resolution field, DBDC+Pac [39]. For a fair comparison, we report the performance of DBDC+Pac [39] on EDSR [30], as the authors performed detailed parameter adjustments and model training on EDSR. We directly used the results reported by the authors, recorded in the table as EDSR †. It should be noted that the EDSR method uses self-ensemble in the final test, which can improve performance to some extent but comes at the cost of 8 times the computational load. Additionally, we applied DBDC+Pac [39] to SwinIR-light [29], using the same hyperparameters as those set by the authors for EDSR, recorded in the table as DBDC+Pac [39]. The following are the quantitative and qualitative results of the comparison. Quantitative results. Table 3 shows the extensive results of comparing different quantization methods with bit depths of 2, 3, and 4, as well as different scaling factors of ×2, ×3, and ×4. 0 20 40 60 80 100 120 140 0.00 0.05 0.10 0.15 0.20 0.25 0.30 /uni00000034/uni00000058/uni00000044/uni00000051/uni00000057/uni0000004c/uni0000005d/uni00000048/uni00000055/uni00000003/uni0000004f/uni00000052/uni0000005a/uni00000048/uni00000055/uni00000003/uni00000045/uni00000052/uni00000058/uni00000051/uni00000047/uni00000003/uni00000053/uni00000048/uni00000055/uni00000046/uni00000048/uni00000051/uni00000057/uni0000004c/uni0000004f/uni00000048/uni00000011 DOBI DOBI+DQC 0 20 40 60 80 100 120 140 0.70 0.75 0.80 0.85 0.90 0.95 1.00 /uni00000034/uni00000058/uni00000044/uni00000051/uni00000057/uni0000004c/uni0000005d/uni00000048/uni00000055/uni00000003/uni00000058/uni00000053/uni00000053/uni00000048/uni00000055/uni00000003/uni00000045/uni00000052/uni00000058/uni00000051/uni00000047/uni00000003/uni00000053/uni00000048/uni00000055/uni00000046/uni00000048/uni00000051/uni00000057/uni0000004c/uni0000004f/uni00000048/uni00000011 DOBI DOBI+DQC Figure 5: The bound percentile of DOBI and DQC. DBDC+Pac [39] performs poorly mainly because 1. The DBDC process requires manually specifying the clipping ratio, which signif- icantly affects performance. 2. DBDC does not prune weights, and the learning rate in the Pac process is too low, causing slow con- vergence of weight quantizer parameters. However, both adverse factors are eliminated in our 2DQuant algorithm. When using only DOBI algorithm, our performance has already reached a level comparable to that of DBDC+Pac algorithms. Upon applying DQC, our performance experienced a remarkable and discernible enhancement, elevating it to new heights. In the case of ×2, 4-bit on Set5 and Urban100, DOBI has an improvement of 1.11dB and 0.39 dB compared to EDSR, while 6Table 3: Quantitative comparison with SOTA methods. EDSR† means applying DBDC+Pac [39] on CNN-based backbone EDSR [31]. Its results are cited from the paper [39]. Set5 (×2) Set14 (×2) B100 (×2) Urban100 (×2) Manga109 (×2)Method Bit PSNR↑ SSIM↑ PSNR↑ SSIM↑ PSNR↑ SSIM↑ PSNR↑ SSIM↑ PSNR↑ SSIM↑ SwinIR-light [29]32 38.15 0.9611 33.86 0.9206 32.31 0.9012 32.76 0.9340 39.11 0.9781 Bicubic 32 32.25 0.9118 29.25 0.8406 28.68 0.8104 25.96 0.8088 29.17 0.9128 MinMax [22] 4 34.39 0.9202 30.55 0.8512 29.72 0.8409 28.40 0.8520 33.70 0.9411 Percentile [27] 4 37.37 0.9568 32.96 0.9113 31.61 0.8917 31.17 0.9180 37.19 0.9714 EDSR†[30, 39] 4 36.33 0.9420 32.75 0.9040 31.48 0.8840 30.90 0.9130 N/A N/A DBDC+Pac [39]4 37.18 0.9550 32.86 0.9106 31.56 0.8908 30.66 0.9110 36.76 0.9692 DOBI (Ours) 4 37.44 0.9568 33.15 0.9132 31.75 0.8937 31.29 0.9193 37.93 0.9743 2DQuant (Ours)4 37.87 0.9594 33.41 0.9161 32.02 0.8971 31.84 0.9251 38.31 0.9761 MinMax [22] 3 28.19 0.6961 26.40 0.6478 25.83 0.6225 25.19 0.6773 28.97 0.7740 Percentile [27] 3 34.37 0.9170 31.04 0.8646 29.82 0.8339 28.25 0.8417 33.43 0.9214 DBDC+Pac [39]3 35.07 0.9350 31.52 0.8873 30.47 0.8665 28.44 0.8709 34.01 0.9487 DOBI (Ours) 3 36.37 0.9496 32.33 0.9041 31.12 0.8836 29.65 0.8967 36.18 0.9661 2DQuant (Ours)3 37.32 0.9567 32.85 0.9106 31.60 0.8911 30.45 0.9086 37.24 0.9722 MinMax [22] 2 33.88 0.9185 30.81 0.8748 29.99 0.8535 27.48 0.8501 31.86 0.9306 Percentile [27] 2 30.82 0.8016 28.80 0.7616 27.95 0.7232 26.30 0.7378 30.37 0.8351 DBDC+Pac [39]2 34.55 0.9386 31.12 0.8912 30.27 0.8706 27.63 0.8649 32.15 0.9467 DOBI (Ours) 2 35.25 0.9361 31.72 0.8917 30.62 0.8699 28.52 0.8727 34.65 0.9529 2DQuant (Ours)2 36.00 0.9497 31.98 0.9012 30.91 0.8810 28.62 0.8819 34.40 0.9602 Set5 (×3) Set14 (×3) B100 (×3) Urban100 (×3) Manga109 (×3)Method Bit PSNR↑ SSIM↑ PSNR↑ SSIM↑ PSNR↑ SSIM↑ PSNR↑ SSIM↑ PSNR↑ SSIM↑ SwinIR-light [29]32 34.63 0.9290 30.54 0.8464 29.20 0.8082 28.66 0.8624 33.99 0.9478 Bicubic 32 29.54 0.8516 27.04 0.7551 26.78 0.7187 24.00 0.7144 26.16 0.8384 MinMax [22] 4 31.66 0.8784 28.17 0.7641 27.19 0.7257 25.60 0.7485 29.98 0.8854 Percentile [27] 4 33.34 0.9137 29.61 0.8275 28.49 0.7899 27.06 0.8242 32.10 0.9303 DBDC+Pac [39]4 33.42 0.9143 29.69 0.8261 28.51 0.7869 27.05 0.8217 31.89 0.9274 DOBI (Ours) 4 33.78 0.9200 29.87 0.8338 28.72 0.7970 27.53 0.8391 32.57 0.9367 2DQuant (Ours)4 34.06 0.9231 30.12 0.8374 28.89 0.7988 27.69 0.8405 32.88 0.9389 MinMax [22] 3 26.01 0.6260 23.41 0.4944 22.46 0.4182 21.70 0.4730 24.68 0.6224 Percentile [27] 3 30.91 0.8426 28.02 0.7545 27.23 0.7183 25.32 0.7349 29.43 0.8537 DBDC+Pac [39]3 30.91 0.8445 28.02 0.7538 26.99 0.6937 25.10 0.7122 28.84 0.8403 DOBI (Ours) 3 32.85 0.9075 29.33 0.8200 28.27 0.7820 26.36 0.8036 31.14 0.9178 2DQuant (Ours)3 33.24 0.9135 29.56 0.8255 28.50 0.7873 26.65 0.8116 31.46 0.9235 MinMax [22] 2 26.05 0.5827 24.74 0.5302 24.42 0.4973 22.87 0.5155 24.66 0.5652 Percentile [27] 2 25.30 0.5677 23.60 0.4890 23.77 0.4751 22.33 0.4965 24.65 0.5882 DBDC+Pac [39]2 29.96 0.8254 27.53 0.7507 27.05 0.7136 24.57 0.7117 27.23 0.8213 DOBI (Ours) 2 30.54 0.8321 27.74 0.7312 26.69 0.6643 24.80 0.6797 28.18 0.7993 2DQuant (Ours)2 31.62 0.8887 28.54 0.8038 27.85 0.7679 25.30 0.7685 28.46 0.8814 Set5 (×4) Set14 (×4) B100 (×4) Urban100 (×4) Manga109 (×4)Method Bit PSNR↑ SSIM↑ PSNR↑ SSIM↑ PSNR↑ SSIM↑ PSNR↑ SSIM↑ PSNR↑ SSIM↑ SwinIR-light [29]32 32.45 0.8976 28.77 0.7858 27.69 0.7406 26.48 0.7980 30.92 0.9150 Bicubic 32 27.56 0.7896 25.51 0.6820 25.54 0.6466 22.68 0.6352 24.19 0.7670 MinMax [22] 4 28.63 0.7891 25.73 0.6657 25.10 0.6061 23.07 0.6216 26.97 0.8104 Percentile [27] 4 30.64 0.8679 27.61 0.7563 26.96 0.7151 24.96 0.7479 28.78 0.8803 EDSR†[30, 39] 4 31.20 0.8670 27.98 0.7600 27.09 0.7140 25.56 0.7640 N/A N/A DBDC+Pac [39]4 30.74 0.8609 27.66 0.7526 26.97 0.7104 24.94 0.7369 28.52 0.8697 DOBI (Ours) 4 31.10 0.8770 28.03 0.7672 27.18 0.7237 25.43 0.7631 29.31 0.8916 2DQuant (Ours)4 31.77 0.8867 28.30 0.7733 27.37 0.7278 25.71 0.7712 29.71 0.8972 MinMax [22] 3 19.41 0.3385 18.35 0.2549 18.79 0.2434 17.88 0.2825 19.13 0.3097 Percentile [27] 3 27.55 0.7270 25.15 0.6043 24.45 0.5333 22.80 0.5833 26.15 0.7569 DBDC+Pac [39]3 27.91 0.7250 25.86 0.6451 25.65 0.6239 23.45 0.6249 26.03 0.7321 DOBI (Ours) 3 29.59 0.8237 26.87 0.7156 26.24 0.6735 24.17 0.6880 27.62 0.8349 2DQuant (Ours)3 30.90 0.8704 27.75 0.7571 26.99 0.7126 24.85 0.7355 28.21 0.8683 MinMax [22] 2 23.96 0.4950 22.92 0.4407 22.70 0.3943 21.16 0.4053 22.94 0.5178 Percentile [27] 2 23.03 0.4772 22.12 0.4059 21.83 0.3816 20.45 0.3951 20.88 0.3948 DBDC+Pac [39]2 25.01 0.5554 23.82 0.4995 23.64 0.4544 21.84 0.4631 23.63 0.5854 DOBI (Ours) 2 28.82 0.7699 26.46 0.6804 25.97 0.6319 23.67 0.6407 26.32 0.7718 2DQuant (Ours)2 29.53 0.8372 26.86 0.7322 26.46 0.6927 23.84 0.6912 26.07 0.8163 2DQuant has an improvement of 0.69 dB and 1.18 dB compared to the SOTA method. All these results indicate that our two-stage PTQ method can effectively mitigate the degradation caused by quantization and ensure the quality of the reconstructed images. Figure 5 shows the bound percentile of DOBI searching and DQC. Overall, the bound of DQC is tighter as the values around the zero point enjoy greater importance. Besides, the shallow layers’ 7Urban100: img_004 ( ×4) HR Bicubic MinMax [22] Percentile [27] DBDC+Pac [39] DOBI (Ours) 2DQuant (Ours) FP Urban100: img_046 ( ×4) HR Bicubic MinMax [22] Percentile [27] DBDC+Pac [39] DOBI (Ours) 2DQuant (Ours) FP Urban100: img_023 ( ×4) HR Bicubic MinMax [22] Percentile [27] DBDC+Pac [39] DOBI (Ours) 2DQuant (Ours) FP Figure 6: Visual comparison for image SR (×4) in some challenging cases. Learning rate PSNR↑ SSIM↑ 10−1 37.82 0.9594 10−2 37.87 0.9594 10−3 37.78 0.9592 10−4 37.74 0.9587 (a) Learning rate Batch size PSNR↑ SSIM↑ 4 37.82 0.9594 8 37.83 0.9594 16 37.84 0.9593 32 37.87 0.9594 (b) Batch size DOBI DQC PSNR↑ SSIM↑ 34.39 0.9202 ✓ 37.44 0.9568 ✓ 37.32 0.9563 ✓ ✓ 37.87 0.9594 (c) DOBI and DQC Table 4: Ablation studies. The models are trained on DIV2K and Flickr2K, and tested on Set5 (×2). bounds vary more significantly due to the elevated significance of these layers within the neural network. Detailedly, the bound for the second MLP fully connected layer’s weight in Layer 0 Block 1 only remains 46% data in its range. It has the second-highest lower bound percentile and the smallest upper bound percentile among the network. Its percentiles are 0.2401 and 0.7035 respectively while its bound values are -0.062 and 0.047 and its distribution is visualized in Figure 4. In conclusion, only through task-oriented optimization of each bound at a fine-grained level can redundant information be maximally excluded and useful information be maximally retained. Qualitative results. We show the visual comparison results for×4 in Figure 6. Since quantized models are derived from full-precision models with information loss, their global performance will rarely exceed that of full-precision models. As seen in the three images for Minmax, after quantization, if no clipping is performed, the long tail effect will lead to a large number of useless bits, resulting in a significant amount of noise and repeated distorted patterns in the reconstructed images. In these challenging cases, our training method allows the model to retain edge information of objects better, preventing blurring and distorted effects. For example, in img_046 and img_023, we have the highest similarity to the full-precision model, while other methods show varying degrees of edge diffusion, significantly affecting image quality. Compared to the DBDC+Pac method, our DOBI and DQC allow for better representation of edge and texture information in the images and effectively avoid distortions and misalignments in the graphics. The visual results demonstrate that our proposed DQC is essential for improving performance in both metric and visual comparisons. 4.3 Ablation study Learning rate and batchsize. We first study the performance variations of the model under different hyperparameters. From Tables 4a and 4b, it can be seen that our DQC enables the model to 8converge within a range of outstanding performance for most learning rates and batch sizes. Due to the non-smooth impact of quantization parameters on the model, the quantized model is more prone to local optima compared to the full-precision model, resulting in a noticeable performance drop when the learning rate is too low. Additionally, as shown in Table 4b, the larger the batch size, the better the model’s performance, and the smoother the convergence process. However, even with a smaller batch size, we can still achieve a performance of 37.82dB on Set5, indicating that our two-stage method has good robustness to different hyperparameters. DOBI and DQC. Moreover, we also study the impact of different stages on performance, with the results shown in Table 4c, from which we can draw the following conclusions:Firstly, the goal of DOBI is to minimize the value shift for weights and activations. Although it is not the task goal, it can still enjoy significant enhancement due to better bit representational ability. Secondly, DQC alone cannot achieve the optimization effect of DOBI. This is because the impact of quantizer parameters on model performance is oscillatory, and training alone is prone to converge to local optima. In contrast, search-based methods can naturally avoid local optima. So it’s necessary to use results from the search-based method to initialize training-based method in PTQ. Thirdly, when DOBI and DQC are combined, namely our 2DQuant, the 4-bit quantized model has only a 0.28dB decrease on Set5 compared to the FP model, which maximally mitigates the accuracy loss caused by quantization. 5 Discussion Why our results surpass FP outcomes.While our method’s performance metrics do not yet fully match those of full-precision models, visual results reveal a compelling advantage. As observed in image img_092 of Figure 1 of Urban100, our approach correctly identifies the direction of the stripes in the image. Whereas the full-precision model erroneously selects the wrong direction. This discrepancy arises because the lower-resolution image, affected by aliasing, creates an illusion of slanted stripes, misleading the FP model’s reconstruction. This phenomenon demonstrates that our PTQ algorithm allows more accurate restored results in certain localized and challenging tasks without being misled. More examples are in the supplementary materials. It suggests that full-precision models contain not only redundant knowledge but also incorrect information. The latter is hard to get rid of by training the FP model. Our quantization method can effectively reduce model parameters and computational demands while eliminating erroneous information, achieving multiple benefits simultaneously. This also suggests that the FP model doesn’t represent the pinnacle of what a quantized model can achieve. Limitations. Despite achieving excellent results, this study still has some limitations. During the DOBI process, the data distribution of activations and weights is required to approximate a bell curve or exponential distribution; otherwise, the DOBI method cannot find the most suitable positions. Additionally, increasing the number of search points for a single tensor in MSE does not necessarily guarantee better performance. However, the second-stage training can somewhat alleviate this issue. Moreover, our method requires a calibration set; without which, the first-stage DOBI and the second-stage DQC cannot be carried out at all. Societal impacts. Our super-resolution quantization method effectively saves computational re- sources, facilitating the deployment of super-resolution models at the cutting edge 6 Conclusion This paper studies the post-training quantization in the field of image super-resolution. We first conducted a detailed analysis of the data distribution of Transformer-based model in SR. These data exhibit a clear long-tail effect and symmetry and asymmetry coexisting effect. We designed 2DQuant, a dual-stage PTQ algorithms. In the first stage DOBI, we designed two different search strategies for the two different distributions. In the second stage DQC, we designed a distillation- based training method that let the quantized model learn from the FP model, minimizing the accuracy loss caused by quantization. Our 2DQuant can compress Transformer-based model to 4,3,2 bits with the compression ratio being 3.07×, 3.31×, and 3.60× and speedup ratio being 3.99×, 4.47×, and 5.08×. No additional module is added so 2DQuant enjoys the theoretical upper limit of compression and speedup. Extensive experiments demonstrate that 2DQuant surpasses all existing PTQ methods in the field of SR and even surpasses the FP model in some challenging cases. In the future, recognizing the significant impact of the model on performance, we will conduct PTQ research on more advanced super-resolution models and attempt to deploy quantized super-resolution algorithms to actual photography tasks, providing a more detailed evaluation of the performance of PTQ algorithms. 9References [1] Wele Gedara Chaminda Bandara and Vishal M. Patel. Hypertransformer: A textural and spectral feature fusion transformer for pansharpening. In CVPR, 2022. 1 [2] Marco Bevilacqua, Aline Roumy, Christine Guillemot, and Marie Line Alberi-Morel. Low-complexity single-image super-resolution based on nonnegative neighbor embedding. In BMVC, 2012. 6 [3] Zheng Chen, Yulun Zhang, Jinjin Gu, Linghe Kong, Xiaokang Yang, and Fisher Yu. Dual aggregation transformer for image super-resolution. In CVPR, 2023. 1 [4] Zheng Chen, Yulun Zhang, Jinjin Gu, Yongbing Zhang, Linghe Kong, and Xin Yuan. Cross aggregation transformer for image restoration. In NeurIPS, 2022. 1 [5] Jungwook Choi, Pierce I-Jen Chuang, Zhuo Wang, Swagath Venkataramani, Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan. Bridging the accuracy gap for 2-bit quantized neural networks (qnn). arXiv, 2018. 2 [6] Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan. Pact: Parameterized clipping activation for quantized neural networks. arXiv preprint arXiv:1805.06085, 2018. 2 [7] Yoni Choukroun, Eli Kravchik, Fan Yang, and Pavel Kisilev. Low-bit quantization of neural networks for efficient inference. In ICCVW, 2019. 1 [8] Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1. arXiv preprint arXiv:1602.02830, 2016. 4, 17 [9] Yifu Ding, Haotong Qin, Qinghua Yan, Zhenhua Chai, Junjie Liu, Xiaolin Wei, and Xianglong Liu. Towards accurate post-training quantization for vision transformer. In ACM MM, 2022. 1 [10] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Learning a deep convolutional network for image super-resolution. In ECCV, 2014. 1, 3 [11] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Image super-resolution using deep convolutional networks. TPAMI, 2016. 1, 3 [12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv, 2020. 3 [13] Hayit Greenspan. Super-resolution in medical imaging. The Computer Journal, 2008. 1 [14] Dan Hendrycks and Kevin Gimpel. Bridging nonlinearities and stochastic regularizers with gaussian error linear units. CoRR, 2016. 4 [15] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. In NeurIPS Workshop, 2014. 5 [16] Cheeun Hong, Sungyong Baik, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee. Cadyq: Content-aware dynamic quantization for image super-resolution. In ECCV, 2022. 2, 3 [17] Cheeun Hong, Heewon Kim, Sungyong Baik, Junghun Oh, and Kyoung Mu Lee. Daq: Channel-wise distribution-aware quantization for deep image super-resolution networks. In WACV, 2022. 3 [18] Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja. Single image super-resolution from transformed self-exemplars. In CVPR, 2015. 6 [19] Yawen Huang, Ling Shao, and Alejandro F Frangi. Simultaneous super-resolution and cross-modality synthesis of 3d medical images using weakly-supervised joint convolutional sparse coding. In CVPR, 2017. 1 [20] Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. Accurate post training quantization with small calibration sets. In ICML, 2021. 1 [21] Jithin Saji Isaac and Ramesh Kulkarni. Super resolution techniques for medical image processing. In ICTSD, 2015. 1 [22] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer- arithmetic-only inference. In CVPR, 2018. 3, 6, 7, 8, 18 [23] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 6 [24] Christian Ledig, Lucas Theis, Ferenc Huszár, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, and Wenzhe Shi. Photo-realistic single image super-resolution using a generative adversarial network. In CVPR, 2017. 2, 3 10[25] Christian Ledig, Lucas Theis, Ferenc Huszár, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew P Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image super-resolution using a generative adversarial network. In CVPR, 2017. 1 [26] Huixia Li, Chenqian Yan, Shaohui Lin, Xiawu Zheng, Baochang Zhang, Fan Yang, and Rongrong Ji. Pams: Quantized super-resolution via parameterized max scale. In ECCV, 2020. 2, 3 [27] Rundong Li, Yan Wang, Feng Liang, Hongwei Qin, Junjie Yan, and Rui Fan. Fully quantized network for object detection. In CVPR, 2019. 2, 6, 7, 8, 18 [28] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. In ICLR, 2021. 1 [29] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir: Image restoration using swin transformer. In ICCVW, 2021. 1, 2, 3, 6, 7 [30] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee. Enhanced deep residual networks for single image super-resolution. In CVPRW, 2017. 2, 3, 5, 6, 7 [31] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee. Enhanced deep residual networks for single image super-resolution. In CVPRW, 2017. 6, 7 [32] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021. 13 [33] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv, 2016. 6 [34] David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In ICCV, 2001. 6 [35] Yusuke Matsui, Kota Ito, Yuji Aramaki, Azuma Fujimoto, Toru Ogawa, Toshihiko Yamasaki, and Kiyoharu Aizawa. Sketch-based manga retrieval using manga109 dataset. Multimedia Tools and Applications, 2017. 6 [36] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. NeurIPS, 2019. 6 [37] Pejman Rasti, Tõnis Uiboupin, Sergio Escalera, and Gholamreza Anbarjafari. Convolutional neural network super resolution for face recognition in surveillance monitoring. In AMDO, 2016. 1 [38] Radu Timofte, Eirikur Agustsson, Luc Van Gool, Ming-Hsuan Yang, Lei Zhang, Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, Kyoung Mu Lee, et al. Ntire 2017 challenge on single image super-resolution: Methods and results. In CVPRW, 2017. 3, 6 [39] Zhijun Tu, Jie Hu, Hanting Chen, and Yunhe Wang. Toward accurate post-training quantization for image super resolution. In CVPR, 2023. 2, 3, 6, 7, 8, 18 [40] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li. Uformer: A general u-shaped transformer for image restoration. In CVPR, 2022. 1 [41] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. TIP, 2004. 6 [42] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Restormer: Efficient transformer for high-resolution image restoration. In CVPR, 2022. 1 [43] Roman Zeyde, Michael Elad, and Matan Protter. On single image scale-up using sparse-representations. In Proc. 7th Int. Conf. Curves Surf., 2010. 6 [44] Liangpei Zhang, Hongyan Zhang, Huanfeng Shen, and Pingxiang Li. A super-resolution reconstruction algorithm for surveillance images. Elsevier Signal Processing, 2010. 1 [45] Weihong Zhang and Ying Zhou. Chapter 2 - level-set functions and parametric functions. In The Feature- Driven Method for Structural Optimization. Elsevier. 4 [46] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu. Image super-resolution using very deep residual channel attention networks. In ECCV, 2018. 1 [47] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and Yun Fu. Residual dense network for image super-resolution. In CVPR, 2018. 1 [48] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016. 2 11Appendix / Supplemental material A Detailed structure of SwinIR SwinIR comprises three core modules: shallow feature extraction, deep feature extraction, and high-quality (HQ) image reconstruction. Shallow and deep feature extraction.Given a low-quality (LQ) input ILQ ∈ RH×W×Cin (where H, W, and Cin represent the image height, width, and input channel number, respectively), a 3 × 3 convolutional layer HSF(·) is employed to extract shallow features F0 ∈ RH×W×C as follows: F0 = HSF(ILQ), (7) where C denotes the number of feature channels. Subsequently, deep features FDF ∈ RH×W×C are extracted from F0 as: FDF = HDF(F0), (8) where HDF(·) represents the deep feature extraction module, comprising K residual Swin Transformer blocks (RSTB) and a 3 × 3 convolutional layer. Specifically, intermediate features F1, F2, . . . , FK and the output deep feature FDF are sequentially extracted as follows: Fi = HRSTBi(Fi−1), i = 1, 2, . . . , K, FDF = HCONV(FK), (9) where HRSTBi(·) denotes the i-th RSTB, and HCONV is the concluding convolutional layer. Incorporating a convolutional layer at the end of feature extraction introduces the inductive bias of the convolution operation into the Transformer-based network, laying a robust foundation for subsequent aggregation of shallow and deep features. Image reconstruction. In the context of image super-resolution (SR), the high-quality image IRHQ is reconstructed by combining shallow and deep features as follows: IRHQ = HREC(F0 + FDF), (10) where HREC(·) is the reconstruction module’s function. The reconstruction module is implemented using a sub-pixel convolution layer to upsample the feature.Additionally, residual learning is utilized to reconstruct the residual between the LQ and HQ images instead of the HQ image itself, formulated as: IRHQ = HSwinIR(ILQ) +ILQ, (11) where HSwinIR(·) represents the SwinIR function. A.1 Residual Swin Transformer block The residual Swin Transformer block (RSTB) is a residual block incorporating Swin Transformer layers (STL) and convolutional layers. Given the input featureFi,0 of the i-th RSTB, intermediate featuresFi,1, Fi,2, . . . , Fi,L are first extracted by L Swin Transformer layers as follows: Fi,j = HSTLi,j (Fi,j−1), j = 1, 2, . . . , L, (12) where HSTLi,j (·) is the j-th Swin Transformer layer in the i-th RSTB. A convolutional layer is added before the residual connection, and the output of RSTB is formulated as: Fi,out = HCONVi(Fi,L) +Fi,0, (13) where HCONVi(·) is the convolutional layer in the i-th RSTB. Swin Transformer layer.Given an input of size H × W × C, the Swin Transformer first reshapes the input into a HW M2 × M2 × C feature by partitioning the input into non-overlapping M × M local windows, where HW M2 is the total number of windows. It then computes the standard self-attention for each window (i.e., local attention). For a local window feature X ∈ RM2×C, the query, key, and value matrices Q, K, and V are computed as follows: Q = XPQ, K = XPK, V = XPV , (14) where PQ, PK, and PV are projection matrices shared across different windows. Typically, Q, K, V∈ RM2×d. The attention matrix is then computed via the self-attention mechanism within a local window as follows: Attention(Q, K, V) =SoftMax(QKT / √ d + B)V, (15) where B is the learnable relative positional encoding. In practice, the attention function is performed h times in parallel, and the results are concatenated for multi-head self-attention (MSA). 12Next, a multi-layer perceptron (MLP) with two fully-connected layers and GELU non-linearity between them is used for further feature transformations. The LayerNorm (LN) layer is added before both MSA and MLP, with residual connections employed for both modules. The entire process is formulated as: X = MSA(LN(X)) +X, X = MLP(LN(X)) +X. (16) However, when the partition is fixed across different layers, there are no connections between local windows. Thus, regular and shifted window partitioning are used alternately to enable cross-window connections [32], with shifted window partitioning involving shifting the feature by (⌊M 2 ⌋, ⌊M 2 ⌋) pixels before partitioning. A.2 Our settings We use the SwinIR light version provided by the original authors. The light version has only 4 RSTBs in the body part while for each RSTB, there are only 6 STLs. For each STL’s MSA, the number of heads is 6, the embedding dimension is 60, the window size is 8, and the MLP ratio is 2. B Detailed distribution of weights and activations In code implementation, the RSTB is called layers while the STL is called blocks. We visualize all layers’ distribution of the pre-trained SwinIR light model’s weights in Figure 7. Bias is ignored as it is not quantized. Also, we visualize the distribution of activations from 32 image patches with a size of 3×64×64 in Figure 8, Figure 9, Figure 10, and Figure 11. We can safely ignore the detailed value of each axis but just care about the shape of distributions. 13Figure 7: Visualization of SwinIR weights. 14Figure 8: Visualization of SwinIR first layer activation. Figure 9: Visualization of SwinIR second layer activation. 15Figure 10: Visualization of SwinIR third layer activation. Figure 11: Visualization of SwinIR fourth layer activation. 16C The derivation of the backward gradient propagation formula In this section, we provide the derivation of our backpropagation formula.We follow the STE [8] style to process the round term, which is ∂Round (x) ∂x = 1 (17) As for the clip function, we take a similar approach, which is ∂Clip(x, l, u) ∂x = ( 1 if l ≤ x ≤ u 0 if x < lor x > u ∂Clip(x, l, u) ∂l = ( 1 if x < l 0 if x ≥ l ∂Clip(x, l, u) ∂u = ( 1 if x > u 0 if x ≤ u (18) With Eqs. (1), (17), and (18), we first derive ∂vq ∂u ∂vq ∂u = ∂ ∂u ( u − l 2N − 1vr + l) = 1 2N − 1vr + u − l 2N − 1 ∂vr ∂u = 1 2N − 1vr + u − l 2N − 1(− 2N − 1 (u − l)2(vc − l) + 2N − 1 u − l ∂vc ∂u ) = ∂vc ∂u + 1 2n − 1vr − vc − l u − l (19) ∂vq ∂l can be derived roughly the same, which can be written as ∂vq ∂l = ∂ ∂l ( u − l 2N − 1vr + l) = − 1 2N − 1vr + u − l 2N − 1 ∂vr ∂u + 1 = − 1 2N − 1vr + u − l 2N − 1( 2N − 1 (u − l)2(vc − l) + 2N − 1 u − l ( ∂vc ∂u − 1)) + 1 = ∂vc ∂u − 1 2n − 1vr + vc − l u − l (20) D More visual examples We provide more visual illustrations to demonstrate the superiority of our method, as shown in Figure 12. In img_016, our method does not distort straight lines. In img_040, our method does not introduce noise to the camera and does not alter the shape at the camera lens. In img_072, we once again outperform the full-precision model by not adding vertical stripes to the curtains. In img_096, we ensure the shape of each window to the greatest extent. These images prove that we can surpass the current SOTA methods in visual effects and avoid misleading results in some tricky cases, generating correct results. 17Urban100: img_016 ( ×4) HR Bicubic MinMax [22] Percentile [27] DBDC+Pac [39] DOBI (Ours) 2DQuant (Ours) FP Urban100: img_040 ( ×4) HR Bicubic MinMax [22] Percentile [27] DBDC+Pac [39] DOBI (Ours) 2DQuant (Ours) FP Urban100: img_072 ( ×4) HR Bicubic MinMax [22] Percentile [27] DBDC+Pac [39] DOBI (Ours) 2DQuant (Ours) FP Urban100: img_096 ( ×4) HR Bicubic MinMax [22] Percentile [27] DBDC+Pac [39] DOBI (Ours) 2DQuant (Ours) FP Figure 12: Visual comparison for image SR (×4) in some challenging cases. 18",
      "meta_data": {
        "arxiv_id": "2406.06649v1",
        "authors": [
          "Kai Liu",
          "Haotong Qin",
          "Yong Guo",
          "Xin Yuan",
          "Linghe Kong",
          "Guihai Chen",
          "Yulun Zhang"
        ],
        "published_date": "2024-06-10T06:06:11Z",
        "pdf_url": "https://arxiv.org/pdf/2406.06649v1.pdf",
        "github_url": "https://github.com/Kai-Liu001/2DQuant"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces 2DQuant, a dual-stage post-training quantization (PTQ) method for low-bit image super-resolution (SR), specifically designed for Transformer-based models. It addresses the severe accuracy degradation of existing PTQ methods on Transformer SR models due to their distinctive activation distributions. 2DQuant comprises Distribution-Oriented Bound Initialization (DOBI) for coarse bound search based on identified symmetric and asymmetric long-tail distributions, and Distillation Quantization Calibration (DQC) for fine-grained bound optimization using knowledge distillation. The method achieves state-of-the-art performance, with PSNR increases as high as 4.52dB on Set5 (×2) at 2-bit quantization, alongside significant compression (3.60×) and speedup (5.08×) ratios. It is the first thorough exploration of PTQ for Transformer-based SR models and demonstrates improved visual quality, even surpassing full-precision models in some challenging cases by eliminating erroneous information.",
        "methodology": "2DQuant employs a two-stage coarse-to-fine PTQ approach. It simulates precision loss using fake-quantization for weights and activations and applies integer arithmetic to compute-intensive operators (linear layers and batch matrix multiplications) within Transformer blocks, which account for ~86% of FLOPs. Gradient backpropagation follows the Straight-Through Estimator (STE) style. The first stage, Distribution-Oriented Bound Initialization (DOBI), analyzes SwinIR's weight and activation distributions, identifying coexisting symmetry (bell-shaped for weights) and asymmetry with long tails (exponential-like for activations). DOBI then uses customized MSE-based search strategies: a symmetric boundary-narrowing search for bell-shaped distributions and a fixed lower bound (to minimum value) with right bound traversal for exponential distributions to find coarse clipping bounds. The second stage, Distillation Quantization Calibration (DQC), refines these bounds using knowledge distillation. A full-precision (FP) model acts as the teacher, guiding the quantized student model to minimize discrepancies in both the final super-resolution output (L1 loss) and intermediate feature layers (L2 loss). The total loss function is a weighted sum of output and feature losses (L = LO + λLF).",
        "experimental_setup": "The research uses SwinIR-light as the backbone model. Training data consists of DF2K, which combines DIV2K and Flickr2K. Validation is performed on Set5. Performance is evaluated on five common SR benchmarks: Set5, Set14, B100, Urban100, and Manga109. Evaluation metrics are PSNR and SSIM, calculated on the Y channel of the YCbCr space, without self-ensemble during testing. Experiments cover quantization bit depths of 2, 3, and 4 bits, and scaling factors of ×2, ×3, and ×4. For DOBI, a search step number of K=100 is used, with activation statistics derived from 32 randomly cropped 3×64×64 images from DF2K. DQC uses the Adam optimizer with a learning rate of 1×10−2, betas (0.9, 0.999), and no weight decay. A CosineAnnealing scheduler stabilizes training. Data augmentation includes random 90°, 180°, 270° rotations and horizontal flips. Training runs for 3,000 iterations with a batch size of 32 on an NVIDIA A800-80G GPU. Comparisons are made against MinMax, Percentile, and the SOTA PTQ method DBDC+Pac on both EDSR and SwinIR-light.",
        "limitations": "The DOBI process relies on specific assumptions about data distribution, requiring activations and weights to approximate either a bell curve or an exponential distribution. If these assumptions are not met, DOBI may not effectively identify the optimal clipping bounds. Additionally, increasing the number of search points in DOBI does not inherently guarantee improved performance for a single tensor, though the second DQC stage can partially mitigate this. A critical constraint for the entire 2DQuant method is the absolute necessity of a calibration set; without it, neither the DOBI nor the DQC stages can be executed.",
        "future_research_directions": "Future work will involve extending the post-training quantization (PTQ) research to more advanced super-resolution models beyond SwinIR-light. There are plans to deploy these quantized super-resolution algorithms in real-world photography tasks. This deployment will include a more detailed and practical evaluation of the performance of PTQ algorithms in actual application scenarios.",
        "experimental_code": "def DOBI(input:torch.Tensor, bit:int, one_direction = False, num:int=100):\n    min_value = torch.min(input)\n    max_value = torch.max(input)\n    \n    diff = (max_value - min_value) / (2 * num)\n    \n    history_min = float('inf')\n    input = input.cuda()\n    \n    if one_direction:\n        diff = (max_value - min_value) / num\n        for i in range(num):\n            lb = min_value\n            ub = max_value - diff * i\n            cur_value = cal_mse(input, lb, ub, bit)\n            if cur_value < history_min:\n                best_lb = lb\n                best_ub = ub\n                history_min = cur_value\n    else:\n        diff = (max_value - min_value) / (2 * num)\n        for i in range(num):\n            lb = min_value + diff * i\n            ub = max_value - diff * i\n            cur_value = cal_mse(input, lb, ub, bit)\n            if cur_value < history_min:\n                best_lb = lb\n                best_ub = ub\n                history_min = cur_value\n    global calibrated_num\n    global total_num\n    \n    calibrated_num += 1\n    print(f'calibration:{calibrated_num}/{total_num}')\n    \n    return float(best_lb), float(best_ub)\n\nclass Differentiable_Round(Function):\n    @staticmethod\n    def forward(ctx: _ContextMethodMixin, x: Tensor):\n        return x.round()\n\n    @staticmethod\n    def backward(ctx: _ContextMethodMixin, grad_outputs):\n        return grad_outputs\n\n\nclass Differentiable_Clip(Function):\n    @staticmethod\n    def forward(\n        ctx: _ContextMethodMixin,\n        input: Tensor,\n        min_val: Tensor,\n        max_val: Tensor,\n    ) -> Any:\n        ctx.save_for_backward(input, min_val, max_val)\n        output = input.clamp(min_val.item(), max_val.item())\n        return output\n\n    @staticmethod\n    def backward(ctx: _ContextMethodMixin, grad_outputs: Tensor) -> Any:\n        input, min_val, max_val = ctx.saved_tensors\n\n        grad_input = grad_outputs.clone()\n        grad_input[(input < min_val) | (input > max_val)] = 0\n        \n        grad_min = grad_outputs.clone()\n        grad_min[input > min_val] = 0\n        grad_min = grad_min.sum().view(1)\n\n        grad_max = grad_outputs.clone()\n        grad_max[input < max_val] = 0\n        grad_max = grad_max.sum().view(1)\n        return grad_input, grad_min, grad_max\n\n\nclass FakeQuantizerBase(Module):\n    def __init__(self, int_quant: bool = True, bit:int=4) -> None:\n        super().__init__()\n        self.lower_bound = Parameter(\n            torch.randn((1,), dtype=torch.float32),\n        )\n        self.upper_bound = Parameter(\n            torch.randn((1,), dtype=torch.float32),\n        )\n        self.n_bit = Parameter(\n            torch.randn((1,), dtype=torch.float32),\n        )\n        self.set_n_bit_manually(bit)\n        \n        self.bit2bound = {}\n        self.use_bit2bound = False\n        self.size_of_input = None\n        \n        self.int_quant = int_quant\n\n        self.clip = Differentiable_Clip.apply\n        self.round = Differentiable_Round.apply\n        \n        self.calibrated = False\n        self.one_direction_search = False\n        \n        global total_num \n        total_num += 1\n        \n\n    def set_int_quant(self, enable: bool):\n        self.int_quant = enable\n\n    def set_require_grad(self, enable_lb: bool, enable_up: bool, enable_nbit: bool):\n        self.lower_bound.requires_grad = enable_lb\n        self.upper_bound.requires_grad = enable_up\n\n    def set_params_manually(self, lb: Tensor, ub: Tensor, n_bit: Tensor):\n        device = self.lower_bound.device\n        self.lower_bound.data = FloatTensor([lb]).data.clone().to(device)\n        self.upper_bound.data = FloatTensor([ub]).data.clone().to(device)\n        \n    def set_params_lb_manually(self, lb: Tensor):\n        device = self.lower_bound.device\n        self.lower_bound.data = FloatTensor([lb]).data.clone().to(device)\n    \n    def set_params_ub_manually(self, ub: Tensor):\n        device = self.upper_bound.device\n        self.upper_bound.data = FloatTensor([ub]).data.clone().to(device)\n\n    def set_n_bit_manually(self, n_bit):\n        device = self.n_bit.device\n        self.n_bit.data = FloatTensor([n_bit]).data.clone().to(device)\n\n\nclass FakeQuantizerWeight(FakeQuantizerBase):\n    def __init__(self,bit=4) -> None:\n        super(FakeQuantizerWeight, self).__init__(bit=bit)\n\n    def forward(self, x:torch.Tensor):\n        if not self.calibrated:\n            lb, rb = DOBI(x, bit=self.n_bit, one_direction=self.one_direction_search)\n            self.set_params_lb_manually(lb)\n            self.set_params_ub_manually(rb)\n            self.calibrated = True\n            return x\n        if self.size_of_input is None:\n            self.size_of_input = x.numel()\n        \n        n_bits = self.n_bit if not self.int_quant else self.round(self.n_bit)\n        \n        if self.use_bit2bound:\n            try:\n                lb, ub = self.bit2bound[int(n_bits.item())]\n                self.set_params_lb_manually(lb)\n                self.set_params_ub_manually(ub)\n            except Exception as e:\n                print(f'use bit 2 bound.{int(n_bits.item())} not found.')\n\n        s = (self.upper_bound - self.lower_bound) / (torch.pow(2, n_bits) - 1)\n\n        c = self.clip(x, self.lower_bound, self.upper_bound)\n\n        r = self.round((c - self.lower_bound) / s)\n\n        return s * r + self.lower_bound\n\n\nclass FakeQuantizerAct(FakeQuantizerBase):\n    def __init__(self,bit=4) -> None:\n        super(FakeQuantizerAct, self).__init__(bit=bit)\n\n        self.running_stat = False \n        self.first_iter = False \n        self.dynamic = False\n        self.beta = 0.995\n        self.identity = False\n\n    def forward(self, x):\n        if not self.calibrated:\n            lb, rb = DOBI(x, bit=self.n_bit, one_direction=self.one_direction_search)\n            self.set_params_lb_manually(lb)\n            self.set_params_ub_manually(rb)\n            self.calibrated = True\n            return x\n\n        if self.size_of_input is None:\n            self.size_of_input = x.numel()  \n\n        if self.identity:\n            return x\n        \n        if self.dynamic:\n            n_bits = self.n_bit if not self.int_quant else self.round(self.n_bit)\n\n            lb = torch.min(x).detach()\n            ub = torch.max(x).detach()\n            n_bits = n_bits.detach()\n\n            s = (ub - lb) / (torch.pow(2, n_bits) - 1)\n\n            c = self.clip(x, lb, ub)\n\n            r = self.round((c - lb) / s)\n\n            return s * r + lb\n\n        if self.running_stat:\n            if self.first_iter:\n                self.lower_bound.data = torch.min(x).detach().clone()\n                self.upper_bound.data = torch.max(x).detach().clone()\n                self.first_iter = False\n            else:\n                self.lower_bound =  self.beta * self.lower_bound + (1-self.beta) *  torch.min(x)\n                self.upper_bound =  self.beta * self.upper_bound + (1-self.beta) *  torch.max(x)\n            return x\n\n        n_bits = self.n_bit if not self.int_quant else self.round(self.n_bit)\n        if self.use_bit2bound:\n            try:\n                lb, ub = self.bit2bound[int(n_bits.item())]\n                self.set_params_lb_manually(lb)\n                self.set_params_ub_manually(ub)\n            except Exception as e:\n                print(f'use bit 2 bound.{int(n_bits.item())} not found.')\n            \n        s = (self.upper_bound - self.lower_bound) / (torch.pow(2, n_bits) - 1)\n\n        c = self.clip(x, self.lower_bound, self.upper_bound)\n\n        r = self.round((c - self.lower_bound) / s)\n\n        return s * r + self.lower_bound\n\nclass QuantBase(Module):\n    def __init__(self,config):\n        super().__init__()\n        self.quant = True\n        self.bit = config['bit']\n        self.weight_quantizer = FakeQuantizerWeight(self.bit)\n        self.act_quantizer = FakeQuantizerAct(self.bit)\n\n    def get_weight_quantizer(self):\n        return self.weight_quantizer\n\n    def get_act_quantizer(self):\n        return self.act_quantizer\n\n    def set_quant_flag(self, enable: bool):\n        self.quant = enable\n\n    def set_require_grad(self, enable: bool):\n        self.weight_quantizer.set_require_grad(enable,enable, enable)\n        self.act_quantizer.set_require_grad(enable,enable, enable)\n\n    def set_weight_bias_grad(self, enable: bool):\n        self.weight.requires_grad = enable\n        if self.bias:\n            self.bias.requires_grad = enable\n\n    def get_quant_weight_bias(self):\n        quant_weight = self.weight_quantizer(self.weight)\n\n        return (quant_weight, self.bias)\n\n\nclass QuantLinear(QuantBase):\n    def __init__(self,config):\n        super().__init__(config)\n    \n    def load_values(self, value):\n        min_value, max_value = value\n        self.act_quantizer.set_params_lb_manually(min_value)\n        self.act_quantizer.set_params_ub_manually(max_value)\n\n    def set_param(self, linear: Linear):\n        self.in_feature = linear.in_features\n        self.out_feature = linear.out_features\n\n        self.weight = Parameter(linear.weight.data.clone())\n\n        if linear.bias is not None:\n            self.bias = Parameter(linear.bias.data.clone())\n        else:\n            self.bias = linear.bias\n\n    def forward(self, x):\n        if not self.quant:\n            return F.linear(x, self.weight, self.bias)\n\n        quant_act = self.act_quantizer(x)\n        quant_weight = self.weight_quantizer(self.weight)\n\n        return F.linear(quant_act, quant_weight, self.bias)\n\nclass QuantLinearQKV(Module):\n    def __init__(self,config):\n        super().__init__()\n        self.q = QuantLinear(config)\n        self.k = QuantLinear(config)\n        self.v = QuantLinear(config)\n    \n    def load_values(self, value):\n        min_value, max_value = value\n        self.q.act_quantizer.set_params_lb_manually(min_value)\n        self.q.act_quantizer.set_params_ub_manually(max_value)\n        self.k.act_quantizer.set_params_lb_manually(min_value)\n        self.k.act_quantizer.set_params_ub_manually(max_value)\n        self.v.act_quantizer.set_params_lb_manually(min_value)\n        self.v.act_quantizer.set_params_ub_manually(max_value)\n        \n    def set_quant_flag(self, enable: bool):\n        self.q.set_quant_flag(enable)\n        self.k.set_quant_flag(enable)\n        self.v.set_quant_flag(enable)\n        \n    def set_require_grad(self, enable: bool):\n        self.q.set_require_grad(enable)\n        self.k.set_require_grad(enable)\n        self.v.set_require_grad(enable)\n\n\n    def set_weight_bias_grad(self, enable: bool):\n        self.q.set_weight_bias_grad(enable)\n        self.k.set_weight_bias_grad(enable)\n        self.v.set_weight_bias_grad(enable)\n    \n\n    def get_quant_weight_bias(self):\n        w_q,b_q = self.q.get_quant_weight_bias()\n        w_k,b_k = self.k.get_quant_weight_bias()\n        w_v,b_v = self.v.get_quant_weight_bias()\n        \n        quant_weight = torch.cat([w_q, w_k, w_v],dim=0)\n        if b_q is not None:\n            bias = torch.cat([b_q,b_k,b_v])\n\n        return (quant_weight, bias)\n\n    def set_param(self, linear: Linear):\n        self.in_feature = linear.in_features \n        self.out_feature = linear.out_features // 3\n        \n        linear_q = Linear(self.in_feature, self.out_feature,bias=linear.bias is not None)\n        linear_k = Linear(self.in_feature, self.out_feature,bias=linear.bias is not None)\n        linear_v = Linear(self.in_feature, self.out_feature,bias=linear.bias is not None)\n        \n        \n        linear_q.weight.data,linear_k.weight.data,linear_v.weight.data = linear.weight.data.clone().reshape(3,self.in_feature,self.out_feature)\n        \n        \n        if linear.bias is not None:\n            linear_q.bias.data, linear_k.bias.data, linear_v.bias.data = linear.bias.data.clone().reshape(3,self.out_feature)\n    \n        self.q.set_param(linear_q)\n        self.k.set_param(linear_k)\n        self.v.set_param(linear_v)\n\n\n    def forward(self, x):\n        q = self.q(x)\n        k = self.k(x)\n        v = self.v(x)\n        return torch.cat([q,k,v], dim=-1)\n\n\nclass QuantConv2d(QuantBase):\n    def __init__(self,config):\n        super().__init__(config)\n\n    def set_param(self, conv: Conv2d):\n        self.in_channels = conv.in_channels\n        self.out_channels = conv.out_channels\n        self.conv_kwargs = {\n            \"stride\": conv.stride,\n            \"padding\": conv.padding,\n            \"dilation\": conv.dilation,\n            \"groups\": conv.groups,\n        }\n        self.weight = Parameter(conv.weight.data.clone())\n        if conv.bias is not None:\n            self.bias = Parameter(conv.bias.data.clone())\n        else:\n            self.bias = conv.bias\n\n    def forward(self, x):\n        if not self.quant:\n            return F.conv2d(x, self.weight, self.bias, **self.conv_kwargs)\n\n        quant_act = self.act_quantizer(x)\n        quant_weight = self.weight_quantizer(self.weight)\n\n        return F.conv2d(quant_act, quant_weight, self.bias, **self.conv_kwargs)",
        "experimental_info": "This code implements the 2DQuant method, a two-stage coarse-to-fine Post-Training Quantization (PTQ) approach, specifically targeting the SwinIR model architecture.\n\n**Key Experimental Settings and Components:**\n\n1.  **Bit-width**: The `bit` parameter in `QuantBase`, `FakeQuantizerWeight`, and `FakeQuantizerAct` allows for configurable bit precision, typically 4-bit, which is passed via a `config` dictionary during module replacement.\n\n2.  **Model Quantization**: The `TDQuantModel` class:\n    *   Initializes both a full-precision (FP) model (`self.net_F`) and a quantized student model (`self.net_Q`) using the `SwinIR` architecture.\n    *   The `build_quantized_network` method systematically replaces standard `torch.nn.Linear` layers (specifically for QKV projections, attention projection, and MLP feed-forward networks within `SwinTransformerBlock` modules) with `QuantLinear` or `QuantLinearQKV`. If `quant_conv` is enabled in the options, `torch.nn.Conv2d` layers within `RSTB` are replaced with `QuantConv2d`.\n    *   `FakeQuantizerAct` modules are inserted for activation quantization within these replaced linear layers.\n\n3.  **Two-Stage PTQ Approach:**\n    *   **Stage 1: Distribution-Oriented Bound Initialization (DOBI)**:\n        *   This stage is triggered by `self.calibration()` in `TDQuantModel`'s `__init__`, which passes calibration data through `self.net_Q` with `torch.no_grad()`. \n        *   Inside the `FakeQuantizerWeight` and `FakeQuantizerAct` forward passes, if `self.calibrated` is `False`, the `DOBI` function is called.\n        *   The `DOBI` function calculates the Mean Squared Error (MSE) between original and quantized values (`cal_mse`) over a range of possible clipping bounds. It then selects the bounds (`best_lb`, `best_ub`) that minimize this MSE.\n        *   It supports a `one_direction_search` for distributions with long tails (e.g., activations) and a symmetric search for bell-shaped distributions (e.g., weights), as specified in the `DOBI` function signature and `FakeQuantizerBase`.\n    *   **Stage 2: Distillation Quantization Calibration (DQC)**:\n        *   This stage is set up by `init_training_settings` and executed in `optimize_parameters` within `TDQuantModel`.\n        *   **Teacher-Student Setup**: The FP `net_F` acts as the teacher, and the quantized `net_Q` is the student.\n        *   **Loss Functions**: Two main losses are used:\n            *   `cri_pix` (L1 loss on final output): Measures the discrepancy between the output of `net_Q` and `net_F` (`|O_F-O_Q|`). Configured by `train_opt['pixel_opt']`.\n            *   `feature_loss` (L2 loss on intermediate feature layers): Measures the discrepancy between normalized intermediate features of `net_Q` and `net_F` (`|F_F-F_Q|`). The `build_hooks_on_Q_and_F` method registers forward hooks to capture these features from both models. Configured by `train_opt['feature_loss']`.\n        *   **Optimization**: The total loss is a weighted sum of `l_pix` and `l_feature`. Only the bounds and other quantization parameters (if `optim_bound` is enabled) are optimized using an Adam optimizer, while other model weights are typically frozen.\n\n4.  **Straight-Through Estimator (STE)**: The `Differentiable_Round` and `Differentiable_Clip` classes explicitly implement the STE for `round()` and `clamp()` operations, allowing gradients to pass through these non-differentiable operations during DQC.\n\n5.  **Calibration Data**: The `basicsr/getcalidata.py` script's `train_pipeline` loads a batch of training data and saves it as `keydata/cali_data_x3.pth`, which is then loaded by `TDQuantModel` as `cali_data` for the DOBI stage."
      }
    },
    {
      "title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization",
      "abstract": "Efficient deployment of Large Language Models (LLMs) requires batching\nmultiple requests together to improve throughput. As the batch size, context\nlength, or model size increases, the size of the key and value (KV) cache can\nquickly become the main contributor to GPU memory usage and the bottleneck of\ninference latency. Quantization has emerged as an effective technique for KV\ncache compression, but existing methods still fail at very low bit widths. We\nobserve that distinct channels of a key/value activation embedding are highly\ninter-dependent, and the joint entropy of multiple channels grows at a slower\nrate than the sum of their marginal entropies. Based on this insight, we\npropose Coupled Quantization (CQ), which couples multiple key/value channels\ntogether to exploit their inter-dependency and encode the activations in a more\ninformation-efficient manner. Extensive experiments reveal that CQ outperforms\nor is competitive with existing baselines in preserving model quality.\nFurthermore, we demonstrate that CQ can preserve model quality with KV cache\nquantized down to 1-bit.",
      "full_text": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization Tianyi Zhang1, Jonah Yi1, Zhaozhuo Xu2, and Anshumali Shrivastava1,3 1Department of Computer Science, Rice University 2Department of Computer Science, Stevens Institute of Technology 3ThirdAI Corp. {tz21, jwy4, anshumali}@rice.edu, zxu79@stevens.edu May 8, 2024 Abstract Efficient deployment of Large Language Models (LLMs) requires batching multiple requests together to improve throughput. As the batch size, context length, or model size increases, the size of the key and value (KV) cache can quickly become the main contributor to GPU memory usage and the bottleneck of inference latency. Quantization has emerged as an effective technique for KV cache compression, but existing methods still fail at very low bit widths. We observe that distinct channels of a key/value activation embedding are highly inter-dependent, and the joint entropy of multiple channels grows at a slower rate than the sum of their marginal entropies. Based on this insight, we propose Coupled Quantization (CQ), which couples multiple key/value channels together to exploit their inter-dependency and encode the activations in a more information-efficient manner. Extensive experiments reveal that CQ outperforms or is competitive with existing baselines in preserving model quality. Furthermore, we demonstrate that CQ can preserve model quality with KV cache quantized down to 1-bit. 1 Introduction Large Language Models (LLMs) have showcased remarkable generalization abilities across various tasks, including text generation, language translation, and reasoning, without needing specific fine- tuning [25]. These impressive capabilities have empowered LLMs to find applications in numerous domains, such as law [14], education [15], and patient care [35]. However, the high computational demands and the prohibitive deployment costs of LLMs have created significant barriers, hindering their widespread adoption [14, 3]. Particularly, as LLMs move towards larger model size [9] and longer context length [34], they require faster graphics processing units (GPUs), or other special- ized processors, with higher memory capacity for efficient inference. Hence it is crucial to develop approaches for reducing the computational costs and memory requirement of LLMs. To accelerate LLM inference, key and value (KV) caching [20] has been proven to be an effective technique without affecting model quality. In autoregressive decoder-only LLMs, KV caching works through trading off memory for computations: the key and value activations of all previous tokens in the current batch are saved in memory to avoid their recomputation for generating the next token. However, since KV cache scales linearly with the number of tokens and batch size, it can quickly overwhelm the memory capacity of existing GPUs under long context or large batch size 1 arXiv:2405.03917v1  [cs.LG]  7 May 2024settings. In addition, since past key and value activations are not shared between sequences (except for maybe a common prompt), reading the KV cache from GPU memory becomes the primary inference bottleneck as opposed to the computation of the attention scores and value activations of the next token [10]. Thus, it is worthwhile to explore techniques for compressing KV cache for two primary benefits: 1. speeding up LLM inference through reducing the amount of memory reads for KV cache, 2. lowering the GPU memory requirements of inference for a given batch size and context length. Existing approaches typically compress KV cache through token eviction [37, 20] or activation quantization [21, 10]. While they can preserve model quality at moderate compression rates (4× compression or 4 bits per floating-point number), model quality quickly deteriorates at high compression rates (16× compression or 1 bit per floating-point number). In this work, we propose Coupled Quantization (CQ), a novel KV cache quantization method that preserves model quality up to 16× compression or 1 bit per floating-point number. Our approach is motivated by the observation that different channels within the same key/value activation embedding are highly inter-dependent. Thus, it is more information efficient to encode multiple channels of a key/value activation embedding than quantizing each channel independently. Existing KV cache quantization methods employ per-channel or per-token quantization strategies, which fail to exploit the inter-dependence between channels and suffer catastrophic model quality degradation at high compression rates. Our proposed method exploits the mutual dependency be- tween channels by jointly quantizing multiple channels and achieves better preservation of model quality than existing approaches in most cases, especially under low bit width settings. We sum- marize our contributions as follows, 1. We empirically observe the phenomenon that different channels within the same key/value activation embedding in an LLM share a high amount of dependency or mutual information, which is a key insight not leveraged by existing KV cache compression approaches. 2. We propose Coupled Quantization (CQ), a novel KV cache quantization method that takes advantage of the reduced entropy of jointly encoding multiple channels. 3. Through extensive experiments, we demonstrate the effectiveness of CQ against the most competitive existing methods. Furthermore, we showcase the ability of CQ in preserving model quality at an extreme KV cache compression level of 1-bit. 2 Background In this section, we introduce the relevant background and context including the KV caching tech- nique, the von Neumann bottleneck of KV cache, and channel-wise quantization. 2.1 LLM Attention and KV Cache Decoder-only transformer-based LLMs employ masked self-attention [32], in which activations of the current token is only dependent on previous tokens and unaffected by future ones. This prop- erty enables training parallelism for the next-token prediction objective, and gives rise to the KV caching technique for efficient inference decoding. Consider the decoding step for thet-th token in a single head of attention in an LLM. The input embedding of thet-th token (a column vector),et, goes through three distinct transformations to become key, query, and value activation embeddings fK(et), fQ(et), fV (et), where the transformationsfK, fQ, fV are composed of linear projection and positional encoding such as RoPE [29]. The output embedding of attention for thet-th token is 2computed as attention(et) = \u0014 fV (e1) . . . fV (et) \u0015 softmax  \u0014 fK(e1) . . . fK(et) \u0015⊤ fQ(et) ! (1) Thus, computing the output embedding of the current token requires the key and value activation embeddings of all previous tokens,fK(ei) and fV (ei) where i ∈ {1, . . . , t− 1}. These embeddings are cached in memory from previous decoding steps asKV cacheto avoid redundant computations and reduce inference latency. The size of KV cache can be calculated asb × n × l × 2 × h × c floating-point numbers, whereb is the batch size,n is the number of tokens in each sequence,l is the number of layers in the model,2 is for key and value,h is the number of key/value attention heads, and c is the number of channels in a single head of key/value activation embedding. As batch size, context length, or model size increases, the size of KV quickly overwhelms limited GPU memory. 2.2 The von Neumann Bottleneck of KV Cache The attention computation in Equation 1 is primarily bottlenecked by GPU memory bandwidth, known as the von Neumann bottleneck, due to low compute-to-global-memory-access ratio. GPUs are able to perform computations significantly faster than reading from global memory, and previous works have shown that attention computation can be accelerated by avoiding unnecessary global memory reads/writes through kernel fusion [5]. During the decoding phase in LLM attention, com- puting the output for the current token requires fetching the cached key and value embeddings of all previous tokens from global memory, resulting in a low ratio of arithmetic operations to global memory reads [10]. Furthermore, each sequence in a batch retains its own KV cache, leading to poor utilization of the parallel processing capabilities of GPUs. Therefore, KV cache compression algorithms can mitigate the von Neumann bottleneck and improve LLM inference efficiency, even if the approach introduces additional computational overheads in decompression or dequantization. As GPU compute cores are mostly stalled by KV cache memory accesses in attention, quantization approaches can effectively reduce memory reads while introducing negligible latency from dequan- tization computations. 2.3 Channel-wise Quantization Existing KV cache quantization methods [10, 21] employ channel-wise quantization for keys and token-wise quantization for values, based on the observation that certain key channels exhibit outlier patterns in magnitude while value channels do not. Channel-wise and token-wise quantization are similar, except the direction along which the quantization centroids are learned. In non-uniform channel-wise quantization, a set of centroids is learned for each channel. SupposeA is a key or value activation matrix, andAi,∗ denotes the i-th channel ofA. Then, non-uniform b-bit channel-wise quantization aims to learn a set of centroidsC⋆ i ⊂ R for each channeli of A independently through the objective C⋆ i = arg min C⊂R |C|=2b \r\r\rAi,∗ − q(Ai,∗) \r\r\r 2 2 (2) where q quantizes each value inAi,∗ to the nearest centroid inC. 31 2 3 4 5 10 15Key Entropy (bits) Layer 1 1 2 3 4 5 10 Layer 2 1 2 3 4 5 10 Layer 3 1 2 3 4 5 10 Layer 4 1 2 3 4 Num. of Joint Channels 5 10Value Entropy (bits) 1 2 3 4 Num. of Joint Channels 5 10 1 2 3 4 Num. of Joint Channels 2.5 5.0 7.5 10.0 1 2 3 4 Num. of Joint Channels 2.5 5.0 7.5 10.0 Sum of Marginal Entropy Joint Entropy Figure 1: Growth rate of joint entropy versus sum of marginal entropies of the LLaMA-7b key/value activation embeddings on 262k tokens of WikiText-2. Entropy is estimated using Equation 4. The slower growth rate of joint entropy implies that jointly quantizing more channels requires fewer bits than quantizing each channel independently. 3 Methodology In this section, we motivate our proposal using information theory and introduce the Coupled Quantization (CQ) approach for KV cache compression. 3.1 Motivations Our proposed approach is inspired by concepts in information theory [28]. We consider each channel in a key/value activation embedding as a random variableX1, X2, . . .. The amount of information (oruncertainty)inchannel X canbemeasuredby entropy, definedasH(X) =− R X p(x) log2 p(x) dx, where p(·) is the probability density function andX is the support of X. H(X) measures the theoretical number of bits needed for losslessly encoding the channelX, so it can be used to gauge how “quantizable” a channel is: ifH(X1) < H(X2), then channelX1 may be quantized to fewer bits than channelX2 while achieving the same quantization error. Our insight is that different channels from the same key/value activation embedding may be interdependent, which would reduce the number of bits required for jointly encoding multiple channels together compared to encoding them independently. The total amount of information (or uncertainty) in two channelsX1, X2 is measured by joint entropy, defined as H(X1, X2) = − R X1 R X2 p(x1, x2) log2 p(x1, x2) dx2 dx1, wherep(·, ·) is the joint probability density function. The joint entropy of two channels is the difference between the sum of their marginal entropies and their mutual information, i.e.,H(X1, X2) =H(X1) +H(X2) − I(X1, X2), whereI(·, ·) is a non-negative quantity for measuring the mutual dependency of two random variables. Thus, we have H(X1, X2) ≤ H(X1) +H(X2) (3) which implies the number of bits needed for jointly encoding two channels is no more than the total number of bits needed for encoding them independently. Previous works have demonstrated that 41 16 32 LLaMA-7b Key Correlations Layer 1  Layer 2  Layer 13  Layer 14  Layer 28  Layer 29  Layer 31  Layer 32 1 16 32 Channels 1 16 32 LLaMA-7b Value Correlations 1 16 32 Channels 1 16 32 Channels 1 16 32 Channels 1 16 32 Channels 1 16 32 Channels 1 16 32 Channels 1 16 32 Channels 1.0 0.5 0.0 0.5 1.0 Figure 2: Correlation matrices of the first 32 channels of 8 layers of LLaMA-7b key and value activation embeddings on WikiText-2. Channel pairs exhibit high levels of linear dependency, shown by high magnitudes of the correlation coefficients. deep neural networks [11] and attention-based networks [7] tend to produce low-rank embeddings, which suggests that channels of key/value embedding in LLM may exhibit high amount of mutual dependency. It is hence beneficial to measure the difference between the joint entropy of multiple channels and the sum of their marginal entropies in key and value activation embeddings. A significant difference would suggest that encoding these channels together is more information-efficient than encoding them independently. However, it is intractable to derive the exact entropy or joint entropy of channels, since their probability density functions are not known. Therefore, we employ the “binning” trick [17] to estimate entropy. We first observe an empirical distribution of key and value channels by saving the KV cache on a dataset, and partition the support of each channel into equally sized bins. Then, values of each channel are discretized to the index of the bin they fall into. Finally, the joint entropy ofn channels X1, . . . , Xn is estimated with the Riemann sum, H(X1, . . . , Xn) ≈ X x1∈B1 ··· X xn∈Bn bp(x1, . . . , xn) log2 bp(x1, . . . , xn) (4) where Bi is the support of the binned or discretizedXi and bp(·) is the empirical probability density function. Specifically, we divide the channels of key and value embeddings of LLaMA-7b into non- overlapping groups ofc contiguous channels, wherec ∈ {1, 2, 3, 4}, and estimate the joint entropy and the sum of marginal entropies of each group. The support of each channel is partitioned into 16 equally sized bins. Figure 1 shows the mean and standard deviation of the estimated joint entropy and sum of marginal entropies of three layers of LLaMA-7b on 262k tokens of the WikiText-2 dataset, averaged over groups. We only show a maximum group size of 4, since increasing the group size requires saving exponentially more key and value embeddings to avoid empty bins and maintain estimation quality. As shown in Figure 1, the sum of marginal entropies grows at a linear rate while the joint entropy increases slower at a sub-linear rate. This implies that as the number of jointly quantized channels increases, the total amount of information needed for encoding decreases. This phenomenon is the foundation that motivates our proposed approach. In addition to presenting the estimated marginal and joint entropy, we also show the Pearson correlation coefficients between channels of LLaMA-7b key and value activation embeddings on WikiText-2. Correlation coefficient captures the linear dependency between two random variables. Heat maps for the correlation matrices of 8 layers are shown in Figure 2, while the correlation matrices of all layers are presented in Section 6 of the Appendix. The key and value channels 5Channel-wise Quantization Coupled Quantization q0 q1 channel 0 -0.57 0  0.21 1  -0.38 0  -0.13 1  key or value activation embedding channel 1 channel-wise centroids q -0.62 0  0.11 1  -0.38-0.10 0.30 2  -0.38 3  -0.13-0.24 coupled channels 0 & 1 key or value activation embedding multi-channel centroids Figure 3: A comparison of 1-bit channel-wise quantization and our proposed Coupled Quantization (using 2 bits per 2 channels as an example). The quantization results on the first two channels of the first-layer key activation embeddings of LLaMA-7b on the WikiText-2 dataset are shown. Channel-wise quantization is ineffective at capturing the original values at low widths, while CQ leverages the dependency between channels to achieve low quantization errors. exhibit high levels of linear dependency and they are clearly not independently distributed, as shown by high magnitudes of the correlation coefficients. 3.2 Coupled Quantization Motivated by the finding that distinct key/value channels exhibit high dependency, we propose Coupled Quantization (CQ), an information-efficient quantization approach for compressing LLM KV cache. Unlike existing KV cache quantization methods which quantizes channel-wise or token- wise, CQ performs channel-coupled quantization for keys and values. More concretely, channels of a key or value activation embedding are divided into equally sized, non-overlapping groups of contiguous channels. The channels in each group arecoupled, as they are jointly quantized and share a single quantization code. For each group of coupled channels, a distinct set of multi-channel centroids are learned, where each centroid has dimensionality equal to the number of channels in that group. When quantizing a key or value activation embedding, each group of coupled channels are quantized to the nearest centroid in terms of L2 distance. We use the CQ-<c>c<b>b notation to denote the configuration of channel coupling and quantization bit width, where<c> is the number of channels in each group and<b> indicates the number of bits in a quantized code for a group. For example, CQ-4c8b means that every 4 contiguous channels are coupled together and each coupled group shares an 8-bit code, which is equivalent to 2-bit channel-wise quantization in terms of storage overhead of quantized codes. A illustrative comparison of channel-wise quantization and CQ is shown in Figure 3. Although previous works [10, 21] opt to quantize keys channel-wise and values token-wise, we adopt channel-coupled quantization for both keys and values. Similar to existing approaches [10, 21], CQ quantizes keys before RoPE [29] is applied, which increases the quantization difficulty by introducing more outliers in key activations. 3.2.1 Centroid Learning In CQ, the multi-channel centroids for each channel group are learned offline on a calibration dataset by leveraging uniform clustering or second-order-information-informed clustering. Specifically, for uniform centroid learning of the CQ-c cb b configuration, a set of centroidsC⋆ i ⊂ Rc is learned 61c1b 2c2b 4c4b 8c8b CQ Conﬁg. 101 102 103 Perplexity on WikiText-2 2642.72 1614.02 883.60 32.12 620.08 28.39 10.47 8.095.68 LLaMA-7b 1-bit CQ Uniform CQ Fisher-guided CQ FP16 Baseline 1c2b 2c4b 4c8b CQ Conﬁg. 101 102 Perplexity on WikiText-2 204.56 24.46 6.866.37 6.08 5.97 LLaMA-7b 2-bit CQ 1c1b 2c2b 4c4b 8c8b CQ Conﬁg. 750 1000 1250 1500 1750 2000Quant. Error on WikiText-2 LLaMA-7b 1-bit CQ Uniform CQ Key Uniform CQ Value Fisher-guided CQ Key Fisher-guided CQ Value 1c2b 2c4b 4c8b CQ Conﬁg. 600 800 1000Quant. Error on WikiText-2 LLaMA-7b 2-bit CQ Figure 4: Perplexity and key/value quantization errors (averaged over all layers) of LLaMA-7b on WikiText-2. Channels coupling and Fisher-guided centroid learning are effective for improving perplexity. independently for each channel groupi through the objective C⋆ i = arg min C∈Rc |C|=2b \r\r\rAic:(ic+c−1), ∗ − cq \u0000 Aic:(ic+c−1), ∗ \u0001\r\r\r 2 F (5) where Aic:(ic+c−1), ∗ is the sub-matrix ofA containing all coupled channels of thei-th group, and cq quantizes each column vector to the nearest centroid inC in terms of L2 distance. We use the k-means algorithm [22] with k-means++ initialization [1] to optimize the objective. LLMs are more sensitive to the quantization precision of certain weights than others [16]. To better preserve model quality, centroids of CQ should be learned to be biased towards preserving the precision of more important activations. To this end, we leverage an approximation to the Hessian to perform second-order-information-informed centroid learning. More concretely, we use the diagonals of the Fisher information matrixF to identify the more influential key/value activations and guide the centroid learning process. This method was proposed by Li et al. [18] and used by Kim et al. [16] for channel-wise weight quantization, and we extend it to multi-channel CQ. For performing Fisher-guided centroid learning, we first save a key/value activation matrixA and its gradient g(A) = ∂ ∂A L(A) on a calibration dataset, whereL is the training loss function. We approximate the Hessian matrix using the diagonals of the Fisher information matrix, which is the element-wise square of the gradient matrixdiag(F) = g(A) ⊙ g(A). We use the sum of diagonal entries of the Fisher information matrix as a measure of importance for each group of channel-coupled activations, and obtain the centroid setC⋆ i for thei-th channel group using the objective C⋆ i = arg min C⊂Rc |C|=2b X j g \u0000 Aic:(ic+c−1), j \u0001⊤g \u0000 Aic:(ic+c−1), j \u0001\r\r\rAic:(ic+c−1), j− cq(Aic:(ic+c−1), j) \r\r\r 2 F (6) We leverage weighted k-means to optimize the objective. The overhead of the centroid learning process and centroid storage are discussed in Section 4.3. We validate the effectiveness of our proposed channel-coupling and Fisher-guided centroid learn- ing by compressing LLaMA-7b KV cache to 1-bit and 2-bit, and present the perplexity results and quantization errors (∥A − cq(A)∥2 F averaged over layers) on WikiText-2 under different CQ con- figurations in Figure 4. The experimental setup is given in Section 4. As the number of coupled channels increases, perplexity and quantization errors improve significantly, approaching the FP16 7Table 1: Perplexity on WikiText-2 under different KV cache quantization methods at varying bit- width. The results of INT, NF, and KVQuant (excluding 1b and 1b-1%) are from [10]. “NaN” means Not a Number, which is caused by numerical stability issues of quantization. Our method CQ consistently outperforms non-dense-and-sparse quantization methods, and performs better or on par with the dense-and-sparse method KVQuant-<b>b-1%. Bits Per FPN LLaMA-7b LLaMA-13b LLaMA-2-7b LLaMA-2-13b Mistral-7b FP16 16 5.68 5.09 5.12 4.57 4.76 INT4 4.00-4.01 5.98 5.32 5.66 5.01 4.97 INT4-gs128 4.16 5.77 5.16 5.32 4.71 4.82 NF4 4.00-4.01 5.87 5.23 5.47 4.90 4.91 NF4-gs128 4.16 5.77 5.17 5.30 4.71 4.83 KVQuant-4b 4.00-4.02 5.73 5.15 5.18 4.63 4.81 KVQuant-4b-1% 4.32-4.35 5.70 5.11 5.14 4.59 4.78 CQ-2c8b 4.00 5.70 5.11 5.14 4.59 4.79 INT2 2.00-2.01 11779 69965 4708 3942 573 INT2-gs128 2.14 37.37 41.77 117.88 93.09 51.96 NF2 2.00-2.02 3210.5 5785.6 13601 4035.6 902.51 NF2-gs128 2.14 351.23 141.19 634.59 642.44 252.85 KVQuant-2b 2.00-2.02 8.17 7.29 9.75 29.25 7.33 KVQuant-2b-1% 2.32-2.35 6.06 5.40 5.50 4.92 5.16 CQ-4c8b 2.00 5.97 5.32 5.42 4.81 5.11 KVQuant-1b 1.00-1.02 321.58 1617.40 NaN 4709.83 203.73 KVQuant-1b-1% 1.32-1.35 9.93 7.97 9.50 13.76 10.07 CQ-8c8b 1.00 8.09 7.02 7.75 6.55 7.25 CQ-8c10b 1.25 6.78 6.00 6.25 5.47 5.90 baseline performance. Although Fisher-guided centroid learning increases the quantization error, it better preserves the salient activations and achieve lower perplexity. 4 Experiments In this section, we perform extensive experiments to validate the effectiveness of our proposed CQ approach for KV cache compression. We first introduce the experimental setups including hardware, software, metrics, datasets, and baselines used. Then, we present the detailed empirical results and provide discussions. Finally, we perform an ablation study to validate the effectiveness of each component of our proposal. Hardware Testbed Experiments are performed on a Linux server running Ubuntu 20.04, equipped with 2 AMD EPYC 7742 CPUs, 1.5TB RAM, and 4 NVIDIA A100 40GB GPUs. Software ImplementationOur software implementation of CQ is based on PyTorch [24] and the HuggingFace Transformers library [33]. Evaluation Metrics and BenchmarksWe evaluate the quality of 5 popular open-source LLMs on various benchmarks under different KV cache quantization algorithms. The 5 LLMs con- sidered are 1. LLaMA-7b, 2. LLaMA-13b [30], 3. LLaMA-2-7b, 4. LLaMA-2-13b [31], 5. Mistral-7b [13]. We evaluate LLM quality using the perplexity metric on 2 datasets: WikiText-2 [23] and C4 [26], and accuracy on 3 benchmarks in zero-shot setting: WinoGrande [27], PIQA [2], and ARC Challenge [4]. Perplexity is evaluated on the test set of the datasets at the maximum context length 8Table 2: Perplexity on C4 under different KV cache quantization methods at varying bit-width. The results of INT, NF, and KVQuant (excluding 1b and 1b-1%) are from [10]. Our method CQ consistently outperforms non-dense-and-sparse quantization methods, and performs better or on par with the dense-and-sparse method KVQuant-<b>b-1%. Bits Per FPN LLaMA-7b LLaMA-13b LLama-2-7b LLaMA-2-13b Mistral-7b FP16 16 7.08 6.61 6.63 6.05 5.71 INT4 4.00-4.01 7.40 6.82 7.31 6.59 5.91 INT4-gs128 4.16 7.16 6.67 6.87 6.20 5.76 NF4 4.00-4.01 7.27 6.74 7.09 6.45 5.85 NF4-gs128 4.16 7.16 6.66 6.86 6.20 5.77 KVQuant-4b 4.00-4.02 7.13 6.65 6.70 6.11 5.75 KVQuant-4b-1% 4.32-4.35 7.09 6.62 6.65 6.06 5.72 CQ-2c8b 4.00 7.11 6.64 6.67 6.09 5.74 INT2 2.00-2.01 10892 100870 4708 4220 477 INT2-gs128 2.14 43.49 56.25 113.49 97.04 50.73 NF2 2.00-2.02 2850.1 4680.3 13081.2 4175.6 1102.3 NF2-gs128 2.14 248.32 118.18 420.05 499.82 191.73 KVQuant-2b 2.00-2.02 10.28 9.05 15.16 43.77 8.40 KVQuant-2b-1% 2.32-2.35 7.38 6.83 7.06 6.38 6.08 CQ-4c8b 2.00 7.52 6.96 7.23 6.52 6.17 KVQuant-1b 1.00-1.02 168.90 1316.41 362.94 4223.37 127.07 KVQuant-1b-1% 1.32-1.35 11.18 9.56 16.04 22.87 10.53 CQ-8c8b 1.00 12.13 10.53 12.49 10.53 9.89 CQ-8c10b 1.25 9.12 8.23 9.03 8.01 7.46 of the LLM (2048 for LLaMA, 4096 for LLaMA-2, and 8192 for Mistral). Baselines We compare our proposed approach with uncompressed FP16 KV cache and com- petitive KV cache quantization methods, including 1. uniform integer (INT) quantization (without grouping and with a group size of 128), 2. NormalFloat (NF) quantization [6] (without grouping and with a group size of 128), 3. KVQuant [10] (without sparse outliers and with 1% outliers stored in sparse format). KVQuant-<b>b-1% is a dense-and-sparse method that requires an additional sparse matrix multiplication in addition to the dense matrix multiplication during inference, which introduces additional computational overhead. KVQuant and our proposed CQ both require learn- ing centroids on a calibration dataset, and we use the same calibration set of 16 sequences (each with 2048 tokens) of WikiText-2 for both methods. Calibration is performed once on the training set of WikiText-2, while perplexity and accuracy are evaluated on the test sets of different datasets and benchmarks. For each method, we report bits per floating-point number (FPN) to measure the compression rate, which is calculated as the number of bits in the quantized KV cache of each token divided by the number of FPN in the uncompressed KV cache of each token, excluding the constant storage overheads of centroids, scaling factors, and zero points. 4.1 Results The results of perplexity on WikiText-2 are presented in Table 1 and the results on C4 are presented in Table 2. CQ consistently outperforms non-dense-and-sparse quantization methods, especially in low bit-width regions. Despite using lower bit-width, CQ performs better or on par with the dense-and-sparse quantization method KVQuant-<b>b-1%. Dense-and-sparse quantization methods 9Table 3: Accuracy on 3 benchmarks under different KV cache quantization methods at varying bit-width. Bits Per FPN Task LLaMA-7b LLaMA-13b LLaMA-2-7b LLaMA-2-13b Mistral-7b FP16 16 WinoGrande 69.93 72.69 68.90 71.98 73.88 PIQA 78.67 79.16 78.07 79.16 80.58 ARC Challenge 41.72 46.42 43.43 48.29 50.34 KVQuant-4b 4.00-4.02 WinoGrande 69.53 72.61 67.96 71.59 73.88 PIQA 78.62 79.22 77.86 78.94 80.58 ARC Challenge 42.32 45.99 42.75 46.67 49.06 KVQuant-4b-1% 4.32-4.35 WinoGrande 70.72 73.40 68.67 72.30 73.72 PIQA 78.40 79.16 78.07 79.27 80.74 ARC Challenge 41.38 46.76 43.17 47.87 49.91 CQ-2c8b 4.00 WinoGrande 70.40 72.45 68.27 72.53 73.48 PIQA 78.61 79.11 77.91 78.62 80.52 ARC Challenge 41.55 45.99 43.34 47.78 49.15 KVQuant-2b 2.00-2.02 WinoGrande 53.59 59.35 51.70 51.30 63.46 PIQA 72.47 74.81 63.38 65.40 75.46 ARC Challenge 32.00 34.47 22.44 24.66 38.57 KVQuant-2b-1% 2.32-2.35 WinoGrande 68.03 71.43 67.64 70.17 70.80 PIQA 77.69 78.51 76.60 78.51 79.65 ARC Challenge 38.74 45.14 41.47 44.97 47.53 CQ-4c8b 2.00 WinoGrande 67.48 70.72 66.45 69.06 69.38 PIQA 76.11 78.29 76.12 77.42 79.49 ARC Challenge 38.48 44.03 39.93 44.11 45.65 KVQuant-1b 1.00-1.02 WinoGrande 50.51 48.46 50.91 49.41 49.80 PIQA 53.26 53.54 53.37 50.92 54.73 ARC Challenge 21.76 21.33 20.65 21.67 19.88 KVQuant-1b-1% 1.32-1.35 WinoGrande 56.67 61.01 57.77 57.30 58.17 PIQA 71.38 75.46 69.91 70.89 73.83 ARC Challenge 29.69 35.32 31.48 32.59 33.19 CQ-8c8b 1.00 WinoGrande 56.51 61.56 55.01 57.14 58.25 PIQA 71.16 73.99 71.22 73.01 75.24 ARC Challenge 30.20 33.79 30.20 34.30 33.79 CQ-8c10b 1.25 WinoGrande 60.46 65.27 59.19 62.98 63.93 PIQA 73.45 75.90 73.07 74.37 77.31 ARC Challenge 33.28 37.12 34.64 38.74 39.59 introduce additional inference overhead due to the extra sparse matrix multiplications for activation outliers which is inefficient on GPUs, while CQ does not have this limitation. The accuracy results on different benchmarks are shown in Table 3. CQ consistently outperforms the non-dense-and-sparse baseline KVQuant-<b>b at bit-width of 1 and 2, and performs better or on par with the dense-and-sparse baseline KVQuant-<b>b-1%. 4.2 Ablation Study We perform a set of ablation experiments to study the effectiveness of each component of our proposed approach. The results of the ablation experiments are shown in Table 4. We evaluate the perplexity of 2 models Mistral-7b and LLaMA-2-13b on WikiText-2 using CQ at 2 bits per FPN, withvaryingnumberofchannelscoupledandcomparinguniformcentroidlearningandFisher-guided centroid learning. Fisher-guided centroids significantly improve model quality as demonstrated by lower perplexity. With either uniform centroids or Fisher-guided centroids, perplexity improves as the number of coupled channels increases. Hence, our proposed techniques of channel coupling and 10Table 4: Ablation study: perplexity on WikiText-2 using CQ with varying number of coupled channels and fisher-guide centroids. Perplexity consistently improves as the number of coupled channels increases. Mistral-7b LLaMA-2-13b Bits Per FPN 2 2 2 2 2 2 2 2 2 2 2 2 Num. of Channels Coupled 1 2 4 1 2 4 1 2 4 1 2 4 Fisher-guided Centroids ✗ ✗ ✗ ✓ ✓ ✓ ✗ ✗ ✗ ✓ ✓ ✓ Perplexity ↓ 97.76 16.29 5.42 5.34 5.20 5.11 890.42 171.96 6.62 6.06 4.91 4.81 Fisher-guided centroid learning are both effective for maintaining model quality. 4.3 Overhead of Centroid Learning and Storage In this section, we discuss the computational overhead of centroid learning and the memory overhead of centroid storage for CQ. The centroid learning process of CQ consists of many independent k- means runs, which can be time-consuming on CPUs. Hence, we leverage a GPU implementation to accelerate the learning process. In all our experiments, we use k-means++ initialization and run 100 iterations of k-means on a single GPU to obtain the centroids. The memory overhead of storing the centroids can be calculated asl × 2 × h × c × 2b 16-bit floating-point numbers, wherel is the number of layers,2 is for keys and values,h is the number of key/value attention heads,c is the number of channels in a single-head key/value activation embedding, andb is the bit width of quantized codes. The detailed learning and memory overhead for different CQ configurations and models are given in Table 5. CQ can easily scale to large model sizes with low learning and memory overheads. Table 5: Learning and memory overhead of different CQ configurations and models. The number of centroid parameters are shown in millions, and the percentage to the model weights is shown in brackets. Centroid Learning Time Parameter Count in Centroids CQ Config. 2c8b 4c8b 8c8b 2c8b 4c8b 8c8b LLaMA-7b 53 mins 28 mins 14 mins 67.11M (0.996%) 67.11M (0.996%) 67.11M (0.996%) LLaMA-13b 94 mins 44 mins 22 mins 104.86M (0.806%) 104.86M (0.806%) 104.86M (0.806%) LLaMA-2-7b 54 mins 28 mins 14 mins 67.11M (0.996%) 67.11M (0.996%) 67.11M (0.996%) LLaMA-2-13b 83 mins 44 mins 23 mins 104.86M (0.806%) 104.86M (0.806%) 104.86M (0.806%) Mistral-7b 13 mins 7 mins 4 mins 16.78M (0.232%) 16.78M (0.232%) 16.78M (0.232%) 5 Related Works The high memory requirements and computational demands of LLMs pose a great challenge to effi- cient inference. Post-training model weight quantization has been shown to be effective for reducing inference latency and memory requirements. GPTQ [8] scales approximate Hessian-based weight quantization to large-scale models, and AWQ [19] preserves the salient weights in LLMs to achieve better quantized model quality. SqueezeLLM [16] leverages sensitivity-based non-uniform clustering and dense-and-sparse quantization for preserving salient weights and outliers. In addition to weight quantization, KV cache compression approaches are also effective for improving inference efficiency. Scissorhands [20] and H2O [37] achieve KV cache compression while preserving model quality by 11evicting unimportant tokens and only storing pivotal tokens. KIVI [21] quantizes key activations channel-wise and value activations token-wise, and uses a residual to achieve better model quality. KVQuant [10] proposes sensitivity-based quantization and dense-and-sparse quantization for KV cache. FlashAttention [5] improves the inference efficiency of attention-based models on GPUs by fusing kernels to eliminate unnecessary global memory reads/writes, while NoMAD-Attention [36] accelerates attention computations on CPUs by leveraging in-register shuffles. Product Quantiza- tion [12] is an approximate nearest neighbor search method that compresses vectors by decomposing the vector space into a Cartesian product of low-dimensional subspaces, and jointly quantizing the dimensions within each subspace. 6 Conclusion In this work, we propose Coupled Quantization (CQ) for enabling more efficient LLM inference by compressing KV cache, which is the latency and throughput bottleneck in long context or large batch size settings. We observe that distinct channels of key/value activation embeddings exhibit high levels of dependency, which has not been leveraged by existing compression methods. Motivated by this insight, we propose channel coupling for exploiting the inter-channel dependency to achieve more information-efficient encoding of key/value activations. Furthermore, we propose Fisher-guided centroid learning to better preserve salient activations and model quality. Extensive experiments demonstrate that our method mostly outperforms existing methods in terms of model quality under the same quantization bit width. Moreover, CQ can preserve model quality reasonably well with KV cache quantized down to 1-bit. References [1] David Arthur, Sergei Vassilvitskii, et al. k-means++: The advantages of careful seeding. In Soda, volume 7, pages 1027–1035, 2007. [2] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about phys- ical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 7432–7439, 2020. [3] Lingjiao Chen, Matei Zaharia, and James Zou. Frugalgpt: How to use large language models while reducing cost and improving performance.arXiv preprint arXiv:2305.05176, 2023. [4] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [5] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness.Advances in Neural Information Processing Systems, 35:16344–16359, 2022. [6] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient fine- tuning of quantized llms.Advances in Neural Information Processing Systems, 36, 2024. [7] Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: Pure attention loses rank doubly exponentially with depth. InInternational Conference on Machine Learning, pages 2793–2803. PMLR, 2021. 12[8] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post- training quantization for generative pre-trained transformers.arXiv preprint arXiv:2210.17323, 2022. [9] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models.arXiv preprint arXiv:2203.15556, 2022. [10] Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W Mahoney, Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cache quantization.arXiv preprint arXiv:2401.18079, 2024. [11] Minyoung Huh, Hossein Mobahi, Richard Zhang, Brian Cheung, Pulkit Agrawal, and Phillip Isola. The low-rank simplicity bias in deep networks.arXiv preprint arXiv:2103.10427, 2021. [12] Herve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search. IEEE transactions on pattern analysis and machine intelligence, 33(1):117–128, 2010. [13] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chap- lot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b.arXiv preprint arXiv:2310.06825, 2023. [14] Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. Challenges and applications of large language models. arXiv preprint arXiv:2307.10169, 2023. [15] Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke Hüllermeier, et al. Chatgpt for good? on opportunities and challenges of large language models for education.Learning and individual differences, 103:102274, 2023. [16] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023. [17] Alexander Kraskov, Harald Stögbauer, and Peter Grassberger. Estimating mutual information. Physical review E, 69(6):066138, 2004. [18] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. arXiv preprint arXiv:2102.05426, 2021. [19] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023. [20] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time.Advances in Neural Information Process- ing Systems, 36, 2024. 13[21] Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: A tuning-free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750, 2024. [22] Stuart Lloyd. Least squares quantization in pcm.IEEE transactions on information theory, 28(2):129–137, 1982. [23] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. [24] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An impera- tive style, high-performance deep learning library.Advances in neural information processing systems, 32, 2019. [25] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.OpenAI blog, 1(8):9, 2019. [26] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1–67, 2020. [27] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale.Communications of the ACM, 64(9):99–106, 2021. [28] Claude Elwood Shannon. A mathematical theory of communication.The Bell system technical journal, 27(3):379–423, 1948. [29] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding.Neurocomputing, 568:127063, 2024. [30] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models.arXiv preprint arXiv:2302.13971, 2023. [31] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models.arXiv preprint arXiv:2307.09288, 2023. [32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need.Advances in neural information processing systems, 30, 2017. [33] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Transformers: State-of- the-art natural language processing. InProceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, pages 38–45, 2020. [34] Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective long- context scaling of foundation models.arXiv preprint arXiv:2309.16039, 2023. 14[35] Xi Yang, Aokun Chen, Nima PourNejatian, Hoo Chang Shin, Kaleb E Smith, Christopher Parisien, Colin Compas, Cheryl Martin, Anthony B Costa, Mona G Flores, et al. A large language model for electronic health records.NPJ digital medicine, 5(1):194, 2022. [36] Tianyi Zhang, Jonah Wonkyu Yi, Bowen Yao, Zhaozhuo Xu, and Anshumali Shrivastava. Nomad-attention: Efficient llm inference on cpus through multiply-add-free attention.arXiv preprint arXiv:2403.01273, 2024. [37] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H2o: Heavy-hitter oracle for effi- cient generative inference of large language models.Advances in Neural Information Processing Systems, 36, 2024. 15Appendix A Correlation Matrices and Scatter Plots 0 15 31 Layer 1  Layer 2  Layer 3  Layer 4  Layer 5  Layer 6  Layer 7  Layer 8 0 15 31 Layer 9  Layer 10  Layer 11  Layer 12  Layer 13  Layer 14  Layer 15  Layer 16 0 15 31 Layer 17  Layer 18  Layer 19  Layer 20  Layer 21  Layer 22  Layer 23  Layer 24 0 15 31 0 15 31 Layer 25 0 15 31 Layer 26 0 15 31 Layer 27 0 15 31 Layer 28 0 15 31 Layer 29 0 15 31 Layer 30 0 15 31 Layer 31 0 15 31 Layer 32 1.0 0.5 0.0 0.5 1.0 Channel Channel Figure 5: Correlation matrix for the first 32 channels of pre-RoPEkey activation embeddings of all LLaMA-7b layers on WikiText-2. 160 15 31 Layer 1  Layer 2  Layer 3  Layer 4  Layer 5  Layer 6  Layer 7  Layer 8 0 15 31 Layer 9  Layer 10  Layer 11  Layer 12  Layer 13  Layer 14  Layer 15  Layer 16 0 15 31 Layer 17  Layer 18  Layer 19  Layer 20  Layer 21  Layer 22  Layer 23  Layer 24 0 15 31 0 15 31 Layer 25 0 15 31 Layer 26 0 15 31 Layer 27 0 15 31 Layer 28 0 15 31 Layer 29 0 15 31 Layer 30 0 15 31 Layer 31 0 15 31 Layer 32 1.0 0.5 0.0 0.5 1.0 Channel Channel Figure 6: Correlation matrix for the first 32 channels ofvalue activation embeddings of all LLaMA- 7b layers on WikiText-2. Figure 7: 2-dimensional scatter plots of pairs of channels inkey activation embeddings of 4 LLaMA- 7b layers on WikiText-2. 17Figure 8: 2-dimensional scatter plots of pairs of channels in value activation embeddings of 4 LLaMA-7b layers on WikiText-2. 18",
      "meta_data": {
        "arxiv_id": "2405.03917v1",
        "authors": [
          "Tianyi Zhang",
          "Jonah Yi",
          "Zhaozhuo Xu",
          "Anshumali Shrivastava"
        ],
        "published_date": "2024-05-07T00:25:20Z",
        "pdf_url": "https://arxiv.org/pdf/2405.03917v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the challenge of the KV cache becoming a major bottleneck for GPU memory usage and inference latency in LLMs, especially with larger batch sizes, context lengths, or model sizes. Existing KV cache quantization methods fail at very low bit widths. The main contributions are: 1. Empirical observation that distinct channels of key/value activation embeddings are highly inter-dependent, a phenomenon not exploited by previous methods. 2. Proposal of Coupled Quantization (CQ), a novel KV cache quantization method that leverages this inter-dependency by jointly encoding multiple channels, leading to more information-efficient compression. 3. Extensive experimental demonstration that CQ outperforms or is competitive with existing baselines, preserving model quality even at an extreme 1-bit KV cache compression level.",
        "methodology": "Coupled Quantization (CQ) is motivated by information theory, observing that the joint entropy of multiple channels grows slower than the sum of their marginal entropies, implying inter-channel dependency. CQ performs channel-coupled quantization by dividing key or value activation embedding channels into equally sized, non-overlapping groups of contiguous channels. Channels within each group are jointly quantized, sharing a single multi-channel code. Quantization maps each channel group to the nearest centroid using L2 distance. Centroids are learned offline on a calibration dataset using either: 1. Uniform clustering (k-means algorithm with k-means++ initialization) or 2. Fisher-guided centroid learning, which approximates the Hessian using diagonals of the Fisher information matrix to bias centroid learning towards preserving important activations, optimized using weighted k-means. CQ quantizes keys before RoPE application.",
        "experimental_setup": "Experiments were conducted on a Linux server with 2 AMD EPYC 7742 CPUs, 1.5TB RAM, and 4 NVIDIA A100 40GB GPUs. The software implementation used PyTorch and the HuggingFace Transformers library. Evaluation involved 5 LLMs: LLaMA-7b, LLaMA-13b, LLaMA-2-7b, LLaMA-2-13b, and Mistral-7b. Model quality was assessed using perplexity on WikiText-2 and C4 datasets, and accuracy on zero-shot benchmarks: WinoGrande, PIQA, and ARC Challenge. Perplexity was evaluated at the maximum context length of each LLM (2048 for LLaMA, 4096 for LLaMA-2, 8192 for Mistral). Baselines included uncompressed FP16 KV cache, uniform integer (INT) quantization (with and without group size 128), NormalFloat (NF) quantization (with and without group size 128), and KVQuant (both without sparse outliers and with 1% outliers stored sparsely). Calibration for CQ and KVQuant used 16 sequences (each with 2048 tokens) from the WikiText-2 training set. Centroid learning was accelerated on a single GPU, running 100 k-means iterations with k-means++ initialization. Compression rate was measured by bits per floating-point number (FPN).",
        "limitations": "The centroid learning process for Coupled Quantization can be time-consuming on CPUs, requiring GPU implementation to accelerate. Additionally, the empirical estimation of joint entropy for larger groups of channels becomes intractable, as it demands saving exponentially more key and value embeddings to maintain estimation quality and avoid empty bins. While the memory overhead for centroid storage is mentioned, it is presented as low and scalable.",
        "future_research_directions": "Not mentioned"
      }
    },
    {
      "title": "Towards Cheaper Inference in Deep Networks with Lower Bit-Width Accumulators",
      "abstract": "The majority of the research on the quantization of Deep Neural Networks\n(DNNs) is focused on reducing the precision of tensors visible by high-level\nframeworks (e.g., weights, activations, and gradients). However, current\nhardware still relies on high-accuracy core operations. Most significant is the\noperation of accumulating products. This high-precision accumulation operation\nis gradually becoming the main computational bottleneck. This is because, so\nfar, the usage of low-precision accumulators led to a significant degradation\nin performance. In this work, we present a simple method to train and fine-tune\nhigh-end DNNs, to allow, for the first time, utilization of cheaper, $12$-bits\naccumulators, with no significant degradation in accuracy. Lastly, we show that\nas we decrease the accumulation precision further, using fine-grained gradient\napproximations can improve the DNN accuracy.",
      "full_text": "TOWARDS CHEAPER INFERENCE IN DEEP NETWORKS WITH LOWER BIT-WIDTH ACCUMULATORS Yaniv Blumenfeld Technion, Israel yanivblm6@gmail.com Itay Hubara Technion, Israel itayhubara@gmail.com Daniel Soudry Technion, Israel daniel.soudry@gmail.com ABSTRACT The majority of the research on the quantization of Deep Neural Networks (DNNs) is focused on reducing the precision of tensors visible by high-level frameworks (e.g., weights, activations, and gradients). However, current hardware still relies on high-accuracy core operations. Most significant is the operation of accumulating products. This high-precision accumulation operation is gradually becoming the main computational bottleneck. This is because, so far, the usage of low-precision accumulators led to a significant degradation in performance. In this work, we present a simple method to train and fine-tune high-end DNNs, to allow, for the first time, utilization of cheaper, 12-bits accumulators, with no significant degradation in accuracy. Lastly, we show that as we decrease the accumulation precision further, using fine-grained gradient approximations can improve the DNN accuracy. 1 I NTRODUCTION Deep Neural Networks (DNNs) quantization (Hubara et al., 2017; Sun et al., 2020; Banner et al., 2018; Nagel et al., 2022; Chmiel et al., 2021) have been generally successful at improving the efficiency of neural networks’ computation without harming the accuracy of the network Liang et al. (2021). The suggested methods aim to reduce the cost of the Multiply-And-Accumulate (MAC) operations for both training and inference. To this end, they quantize the weights, activations, and gradients. For applications utilizing such quantization methods, the cost of multiplications, commonly considered to be the computational bottleneck, can be substantially reduced. However, the accumulation of computed products is still performed with high-precision data types. Consequently, the cost of the accumulation, as a component of MAC operations, becomes increasingly dominant in performance breakdowns (Sakr et al., 2019; Ni et al., 2020; Chmiel et al., 2021). For example, when the weights and activations are in the common FP8 format, van Baalen et al. (2023) showed the accumulation becomes a computational bottleneck. For example, they conducted experiments to estimate the raw gate count for various FP8 implementations (a first-order approx- imation for power and area) and observed a 2× reduction in gate count when employing FP16 accumulators instead of FP32. Similarly, Ni et al. (2020) reported analogous findings for INT8, demonstrating that an 8-bit×8-bit multiplier consumes a comparable amount of power and silicon area to a 32-bit accumulator. In this study, we focus on reducing the numerical precision of the accumulation operation in DNNs. Building our solution on top of the emerging FP8 format, which has gained prominence for both training and inference on the most prevalent hardware Andersch et al. (2022), we aim to optimize such DNNs, to enable inference on hardware with Low Bit-width Accumulators (LBAs). Our main contributions are: • We propose a simple scheme for fine-tuning models with 12-bit accumulators for a variety of tasks, and show this method can already achieve strong performance. For example, we show for the first time that 12-bits accumulators can be used in ResNets on ImageNet, with no significant degradation in accuracy. • We examine more fine-grained approaches, in which, for the first time, we backpropagate through the entire accumulation-computation graph. Though much more expensive during training, such fine-grained backpropagation can be used to significantly improve the accuracy of DNNs with LBAs at lower bit-widths. 1 arXiv:2401.14110v1  [cs.LG]  25 Jan 20242 P RELIMINARIES : Q UANTIZED NEURAL NETWORKS 2.1 Q UANTIZED WEIGHTS AND ACTIVATIONS The quantization of neural networks is, by now, a standard practice for achieving efficient neural networks. Unlike traditional scientific computation, that often (Bailey, 2005) requires high-precision floating point arithmetic (e.g., FP64) to achieve accurate results, it was observed (Gupta et al., 2015) that deep neural networks can maintain high accuracy when the weights and activations in the network are represented in low bit representation. As a result, training deep neural networks (DNNs) using half-precision (FP16) arithmetic became the default setup for modern Deep Learning applications (Brown et al., 2020). Lower precision representation (INT8, FP8, INT4, FP4, and Binary) (Sun et al., 2019; 2020; Courbariaux et al., 2016) is also used for a variety of deep learning applications, for either training or inference, albeit using them is more experimental and may result in lower model performance, depending on the specific application. Quantization of Weights and Activations (W/A) has two main benefits. • Lower memory footprint: By reducing the number of bits used for representation of each numerical value, W/A quantization can significantly reduce the memory required for storing and using a neural network. Consequently, W/A quantization enables storing larger models (with more parameters and activations) on DL accelerators with finite storage and improves the computation efficiency of smaller models by mitigating memory bottlenecks. • Reduced complexity of multiplication operation: Neural networks commonly compute mul- tiplications of weight and activation pairs. When both weight and activation are represented at a lower precision, it is possible to perform the multiplication operation with cheaper hardware (smaller area, less energy). This allows us to do more multiplication operations per second, provided that the hardware was designed to support these lower-precision operations. Numerical values are typically represented using either fixed, or floating point format. Methods for quantization of DNNs can be divided accordingly. 2.2 F IXED POINT QUANTIZATION Given a full-precision value x, a fixed number of bits B, and an integer b (exponent-bias), we define the fixed-point quantization of x as: Rmin ≡ −2B−b−1 Rmax ≡ 2−b \u0000 2B−1 − 1 \u0001 QFIXED B,b (x) ≡    Rmin x ≤ Rmin Rmax x ≥ Rmax 2−bRound \u0000 x · 2b\u0001 else (1) As we can see from Eq. (1), the process of fixed-point quantization involves two explicit changes to the value of x. First, we round x · 2b to an integer value. The rounding operation can be done using a variety of operations (such as Floor, Ceil, Nearest-Neighbour, or Stochastic Rounding (Wang et al., 2018)), but will result in a loss of information either way, with a rounding error that decreases as we increase the parameter b: ∆ ∼ 2−b. If the value of x is sufficiently small |x| < 2−b, the quantization noise will exceed the represented value (∆ > |x|) and we have no way to accurately represent the value of x. We will refer to this event as underflow. Second, we have a limited range for representation, that increases with the number of bits B and decreases with b. We refer to the event when x is outside the range (Rmin, Rmax) as overflow, noting that the quantization error in this case is unbounded. Integer quantization is a specific case of fixed-point quantization, where the exponent biasb is set to 0. While we defined the exponent-bias b to be an integer, it is important to note that non-integer values could have worked mathematically just as well to define valid quantization operations. The main benefit of choosing b to be an integer is the efficiency of computing power-of-two multiplications in hardware. The main advantage of fixed point quantization comes from its relative simplicity. Integer multiplica- tion (and addition) are generally considered to be cheaper on hardware when compared with floating point operations. 22.3 F LOATING POINT QUANTIZATION Given a full-precision scalar value x, number of mantissa bits M, number of exponent bits E, and an integer b (exponent-bias), we define the floating point (Dekker, 1971) quantization M/E: s ≡ 1 2 (1 − sign(x)) , e ≡ ⌊log2(|x|)⌋ m = 2−M Round \u0000 2M (|x|2−e − 1) \u0001 ROF ≡ 22E−b−1 \u0000 2 − 2−M \u0001 , R UF = 2−b QFLOAT M,E,b (x) ≡ (−1)s    ROF |x| ≥ROF 0 |x| < RUF 2e (m + 1) else (2) Note that 1 ≤ |x|2−e < 2, due to the definition of e, which helps make sense of the quantization operation in Eq. (2). The total number of bits used for this representation is B = M + E + 1: 1 sign bit (s), M mantissa bits (m) and E exponent bits (e). As we can see, floating point representation can cover a larger range of values when compared with a fixed point representation that uses the same amount of bits and exponent bias, (ROF/UF depends on 22±E while Rmax, Rmin depends on 2B), reducing the occurrence of overflow and underflow events. Unlike fixed-point representation, which had a fixed bound for quantization error (∆ ∼ 2−b) within the represented range, the quantization error for floating point representation varies, depending on the magnitude of x: ∆ ∼ 2e−M . As a direct result, floating point’s arithmetic also adds additional complexity, in the form of swamping Higham (1993). When performing an addition over two floating points values ¯z = z1 +(FP) z2 ≡ QFLOAT M,E,b (z1 + z2), it is possible that the precision of ¯z will not be sufficient for full-representation of its summands, causing the least significant bits to be swamped out — resulting in a ‘noisy‘ addition operation. In the extreme case, denoted as Full-Swamping, if |z1| > 2M+1|z2|, z2 is swamped out entirely, so ¯z = z1 despite z2 being non-zero. In contrast, fixed-point addition will always be exact, as long as the sum remains within the representation range (no overflow). 2.4 L OW BIT-WIDTH ACCUMULATORS When performing a general matrix multiplication (GEMM) operation, (e.g. matrix-multiplication, or convolution), each individual scalar computed during the operation can be expressed as the sum of product pairs y = N−1X i=0 xiwi. (3) Here, y is a scalar component of the output tensor of the GEMM operation, N is the accumulations size (i.e., the number of summands used per scalar output), while {xi}N−1 i=0 and {wi}N−1 i=0 are two series of scalar inputs used for the calculation of y. The values in both series originate from the input tensors, but the exact mapping, from tensors to series, will depend on the performed operation (see Appendix A for more details). Due to the common structure of the multiply-accumulate operation, hardware implementations of GEMM operation often rely on the fundamental Fused Multiply-Add (FMA) operation, defined as FMA(x, w, s) ≡ x · w + s, with x, w, sbeing scalars. Our goal in this work will be to decrease the cost of the FMA component. Previous discussed methods, such as W/A quantization, have been helpful in reducing the cost of the multiplication of FMA. In contrast, the accumulation component of FMA has been studied to a much lesser extent. In (Wang et al., 2018), the authors show that training a neural network with FP16 accumulators can result in noisy training, with a modest loss of accuracy. To mitigate this, the paper recommends chunk-based accumulation and floating-point stochastic rounding. Chunk-based accumulation changes the order of accumulation, while stochastic rounding is a method where a small, random noise is added to the result of high-precision summation, before the result is cast to a low-precision representation. While successful at closing the gap (e.g., for ResNet18 on ImageNet), both methods may prove difficult to implement on modern hardware. Specifically, the order of accumulation on DL accelerators will usually depend on their block architecture and is not easily configured. Moreover, stochastic rounding requires an implicit addition operation, which is projected to increase the cost of hardware addition, negating the benefit of using LBAs. Sakr et al. (2019) examined the effect of low precision accumulators on training through the accu- mulation variance statistic, which they theoretically derive, given several statistical assumptions on the distribution of the summands. In Ni et al. (2020), the authors propose WrapNet, where the 3additions are performed with 8 and 12 integer accumulators with wrap-around. WrapNet is shown to perform complex inference tasks (e.g. ImageNet classification) with extreme quantization (e.g., 7 bits activations, 2 bit weights, and 12 bits accumulators), but it does suffer a noticeable accuracy degradation in this setup, for tasks such as ImageNet classification. Although mostly experimental, FP16 accumulation was integrated in the design of several commercial products (Agrawal et al., 2021), including the tensor cores in the Hopper architecture (NVIDIA) Andersch et al. (2022). 3 F INE -TUNING NEURAL NETWORKS WITH LOW-BIT ACCUMULATORS One key difference between W/A quantization and quantization of the accumulators is that accumula- tion is an internal FMA operation, which is not generally visible to the software user. To simulate the effect of quantized FMA component, we implement the GEMM operations (convolution/ matrix multiply) in CUDA, where the FMA operation is replaced with our custom FMAq operation: FMAq(x, w, s) ≡ Qacc (Qprod (x · w) + s) , (4) as illustrated in Fig. 1. In all experiments, we use a constant chunk size of 16, based on the sizes exposed to the user of NVIDIA’s tensor cores. It is important to highlight that the product and W i X i  Multiply  Q W Q A  S i - 1  Q p r o d  Add S i Q a c c  FMA  w 0 x 0  FMA  w 1 x 1  FMA  w n - 1 x n - 1  FMA  w n x n  FMA  w n + 1 x n + 1  FMA FMA  w 2 n - 1 x 2 n - 1  Add  w N - n x N - n  FMA FMA FMA  w N - 1 x N - 1 w N - n + 1 x N - n + 1  Q acc  Add  Q acc  Figure 1: Left: an illustration of quantized FMA component, as simulated in our work. Unlike the W/A quantization operations (QW (w), QA(x)) that can be efficiently performed in software, Qprod and Qacc are explicitly internal hardware operations, intended to simulate the logic of a cheaper hardware component. Right: Illustration of chunk-based accumulation, with chunk base of n. Chunk- based accumulation is useful for reducing error caused by swamping, but the chunk size is not easily configured and will usually depend on the architecture design of the systolic array. accumulator quantization functions (Qprod and Qacc) are intended to simulate the hardware, rather than suggest an implementation for it. Breaking down the FMA to components in hardware would, in practice, undermine its efficiency — as it will no longer be ‘fused’. Taking this into account, Qprod and Qacc must remain simple and computationally efficient. For example, ‘round to nearest’ and stochastic rounding methods, which are taken for granted for W/A quantization, will not be available to us during inference, as their hardware implementation would still perform addition internally with a higher number of bits. Our quantization will instead rely on the simple ‘floor’ operation, implemented in software via bit-mask. For Hardware analysis of our implementation, see Appendix E. As discussed in section 2.3, floating point quantization can be broken down into 3-distinct events: underflow, overflow and swamping. Eventually, our low-precision model will have to handle all three events. We will, however, start by examining their individual properties, as displayed in Tab. 1. Our main insight from Tab. 1, is that underflow events are expected to have the least significant effect over the network output (They have the lowest absolute error, since the default value for the exponent bias b, as used by the common FP32/FP16 definitions, is b = 2E−1.). In Fig. 2, we evaluate the correctness of this claim, and show that the wide-scope loss-landscape of an LBA ResNet is barely affected when we ignore UF events. And yet, the large relative error induced during underflow 4Event Condition Key Parameters Absolute Error (bound): ∆ = |Q(x) − x| Relative Error: ∆ |x| Overflow (OF) |x| ≳ 22E−b E, −b ∞ (0%, ∞) Underflow (UF) |x| < 2−b E, b 2−b 100% Swamping No OF/UF M 2⌊log2(|x|)⌋−M \u0002 2−M−1, 2−M \u0003 Table 1: Properties of each type of floating-point quantization event. (small elements are effectively replaced with zero), will cause significant optimization errors for gradient-based methods: During fine-tuning, we can expect the magnitude of the weight updates to be proportional to the magnitude of the corresponding weights, causing the underflow region to be particular hard region to ‘escape’ from. Values that are stuck at underflow are effectively excluded from the training since the forced value of zero prevents them from learning meaningful correlations. (see Appendix F for more details.) 0.2 0.4 0.6 0.8 1.0 0.2 0.4 0.6 0.8 1.0 2 4 6 8 10 2 4 6 8 (a) Baseline LBA 0.2 0.4 0.6 0.8 1.0 0.2 0.4 0.6 0.8 1.0 2 4 6 8 10 2 4 6 8 (b) Excluding UF 0.2 0.4 0.6 0.8 1.0 0.2 0.4 0.6 0.8 1.0 2 4 6 8 10 12 2 4 6 8 10 (c) Excluding Swamping Figure 2: Wide scope loss landscapes Li et al. (2018) of an LBA resnet50, using pre-trained ResNet50 weights (CIFAR10, FP32). Here, we compare the qualitative effect of different components in floating points quantization over the network output: In (a), we use a complete implementation of FP quantization during convolution accumulation, with 7 Mantissa and 4 Exponent bits. In (b), we repeat the previous experiment but ignore underflow events during quantization. For comparison, in (c), we repeat the original experiment, but add 16 additional bits to the mantissa, greatly diminishing the effect of swamping, without affecting the role of underflow. All landscapes appear similar, but while the effect of excluding swamping events (c) is visible, the loss landscapes of networks with (a) and without (b) underflow are hardly distinguishable. Therefore, we propose the following method: Starting off with the weights of a pre-trained net- work (trained in full-precision), we will design a network that utilizes quantized FMA for forward- propagation, excluding underflow events, and perform a standard gradient-based optimization (i.e. Stochastic gradient decent, while keeping the backward implementation of each operation as it was with full-precision FMAs). Once we converge to some accuracy value, we will enable the underflow implementation and proceed with further fine-tuning. As seen in Tab. 1, the exponent bias (b) can be configured to control underflow and overflow events, with a clear trade-off between the former and the latter. Previous works Kuzmin et al. (2022) have made the insight, that the default value b = 2E−1 is not always suitable for neural networks. For our purposes, we note that the different quantization functions Qprod and Qacc as seen in Fig. 1, are likely to require different ranges for representation: Assuming the product terms ui = wixi are i.i.d, the accumulator’s value will follow the central limit theorem, and is therefore more likely to reach overflow, resulting unbounded quantization noise. To try and avoid this scenario, our setup will give a smaller exponent bias to the accumulator. In our experiments, we use a relative factor based on the chunk-size, so that bacc = bprod − 1 2 log2 (Chunk-Size). Following the same reasoning, one may suggest that the exponent bias should depend on the sequence number in which the FMA is applied within every GEMM operation. Nevertheless, for the context of this work, we will treat all FMA units as homogeneous, with the same exponent bias. 53.1 E XPERIMENTS : I MAGE CLASSIFICATION For our first set of experiments, we aim to check the effect low-bit accumulators have on residual neural networks He et al. (2016). For each experiment, we use the standard ResNet architecture and replace each GEMM operation used during forward-propagation (convolution and matrix multiplica- tion) with our custom implementation, as described in section 3. With no adjustments, the effect of these changes on the network accuracy (zero-shot), can be severe, as we show in Appendix B. For Qprod, Qacc, we used the same amount of mantissa and exponent bits, M = 7, E= 4, a setup we will denote as M7E4. For overflow, we used the exponent biases: bacc = 10, bprod = 12, but disabled underflow events for the first part of the experiment. After loading the networks with pre-trained weights, we proceed to train the network for 5 epochs, using Adam optimizer with a learning rate of η0 = 10 −6, and a cosine scheduler, so that η5 = 10 −8). Then, we enable underflow events and run a fine-tuning again for a single epoch, using a reduced learning rate of ηUF = 10−7. To evaluate the benefit of the two-staged fine-tuning, we also ran the same experiment with a single stage, where underflow is enabled for 10 epochs. The baseline numbers were obtained by repeating the fine-tuning process in a non-LBA setup, which resulted in an improvement of up to 0.65% over the zero-shot accuracy. Our full setup and implementation are detailed in Appendix C. The results of this experiment are presented in Tab. 2. Model Baseline 1-stage no UF* no UF → with UF ResNet18 70.23% 69.94% 70.01% 70.06% ResNet34 73.87% 73.64% 73.61% 73.45% ResNet50 76.80% 74.70% 76.60% 76.40% Table 2: Top-1 Accuracy results: Fine-tuning ResNets with low-bit accumulators for ImageNet classification. *Intermediate Stage: Both training and evaluation are done without underflow. For LBA ResNets with full-precision W/A, our results indicate that the models we suggest can train surprisingly well even without a dedicated fine-tuning regime. The dual-stage approach (Training without UF first and enabling it later) only shows clear benefit, so far, in the case of the larger, ResNet50 model. That being said, scaling the method for larger models is important, and tasks will only become more difficult from now on. In order for a model with low-bit accumulators to be commercially viable, it is vital to show that quantized accumulation still works when the weights and activations are quantized. Therefore, our next set of experiments will test the feasibility of LBA ResNets in this setting. For weights and activations, we will use 8-bit floating point representation (Wang et al., 2018). Following the results presented in Kuzmin et al. (2022), we use M4E3 representation with flex-bias for both weights and activations, implemented using the qtorch library Zhang et al. (2019). For our flex-bias implementation, we evaluate the maximal exponent for each tensor during forward propagation, and use the maximal integer exponent bias that is sufficient to prevent overflows (single value per tensor). The results of fine-tuning LBA ResNets in this setup can be seen in Tab. 3, as well as a comparison of our results with previous works that also used lower-bit accumulators. We note that a direct comparison between the methods based on final accuracy alone will not be valid: the method presented in Wang et al. (2018) is intended for quantized training, and includes several more quantized components, as well as several methods that are projected to reduce hardware efficiency. Meanwhile, Ni et al. (2020) proposes the cheapest implementation (Fewer bits for Weights and activations, Integer quantization), sacrificing model accuracy for hardware efficiency. Nevertheless, when aiming for cheaper inference, our LBA models were the only models to achieve accuracy on par with non-LBA models, while providing a cheaper alternative compared to models with standard accumulation. 3.2 E XPERIMENTS : L ANGUAGE MODELS To assess the capability of LBA language models, our next set of experiments will focus on the common Bert (Devlin et al., 2018) architecture, and the SQUAD (Question-Answering) task. In 6Model Data Type Weights Activations Accumulator Top-1 Accuracy ResNet18 Baseline FP 32 32 32 70.23% Baseline (FP8) FP 8 8 32 69.90% Wang et al. (2018) FP 8 8 16 66.95% Ni et al. (2020) INT 7 2 12 63.84% Ours (1-stage) FP 8 8 12 69.54% Ours (dual-stage) FP 8 8 12 69.70% ResNet34 Baseline FP 32 32 32 73.87% Baseline (FP8) FP 8 8 32 73.49% Ours (1-stage) FP 8 8 12 73.18% Ours (dual-stage) FP 8 8 12 73.42% ResNet50 Baseline FP 32 32 32 76.80% Baseline (FP8) FP 8 8 32 76.25% Wang et al. (2018) FP 8 8 16 71.72% Ours (1-stage) FP 8 8 12 74.15% Ours (dual-stage) FP 8 8 12 76.22% Table 3: Top-1 Accuracy results: Fine-tuning ResNets with low-bit accumulators and FP8 weights and activations for ImageNet classification. Results are compared with similar models utilizing LBAs in the literature. this case, fine-tuning a pre-trained model is already the standard. In contrast to our experience with residual networks, breaking down the fine-tuning process into separate phases was not, in general, beneficial for the accuracy of the larger models. The exponent biases we used for the different LBA models also had to be changed, to avoid overflow events. In table4, we compare the results of fine-tuning LBA Bert models with the results of fine-tuning non-LBA models, as described in C.2. While LBA Bert-small has a small (∆f1 = 0.37%) performance degradation compared with the non-LBA model, the gap is closed completely for the Bert ( ∆f1 = −0.09%) and Bert-Large (∆f1 = −0.26%). Baseline LBA (M7E4) bacc,bprod=7,9 LBA (M7E4) bacc,bprod=8,10 Model Exact (%) f1 (%) Exact (%) f1 (%) Exact (%) f1 (%) Bert-Small 71.32 80.96 70.88 80.24 71.35 80.59 Bert-Base 79.84 87.53 79.60 87.62 79.80 87.52 Bert-Large 83.22 90.40 82.97 89.97 83.25 90.66 Table 4: SQUAD v1 fine-tuning for LBA-Bert models. Inspired by our LBA-Bert model results (which were favorable toward larger models), we tested our LBA-aware fine-tuning method on the LLama-v2-7B model (Touvron et al., 2023). We used the same settings and scripts as QLoRA paper (Dettmers et al., 2023), which uses frozen 4-bit weights with an additional trainable low-rank matrix in BF16. To measure performance on a range of language understanding tasks, we used the MMLU (Massively Multitask Language Understanding) benchmark Hendrycks et al. (2020), a multiple-choice benchmark covering 57 tasks. The fine-tuning was done over the Open Assistant (OASSA1) dataset Köpf et al. (2023) using official training scripts found in the QLoRA code (i.e., llama2_guanaco_7b). We report 5-shot test accuracy in tabel 5. Model Baseline M10E5 M6E5 M7E4* LLamma v2 (OASSA1) 45.3 45.4 44.3 45.1 Table 5: MMLU 5-shot test accuracy with and without LBA, for QLORA+ LLama v2 (7B parameters). * For runs with 4 exponent bits, we used dynamic (per-layer) exponent-bias. 74 BELOW 12 BITS : F INE -GRAINED GRADIENT FOR LOW BIT ACCUMULATORS Thus far, we have shown that 12 bits are sufficient for inference in a variety of deep neural networks. However, the simple methods described in section 3 are not sufficient for training neural networks with lower amounts of accumulation bits. For example, a shallow fully-connected DNN trained over MNIST, will fail when using a M4E3 accumulator, even when excluding underflow events. The cause of the failure is known as it is similar to quantization failures in other areas of deep neural network: Quantization changes the output of the network during forward pass, and when the change is significant enough, it is no longer feasible to rely on the gradients of non-quantized operations for optimization. Of course, we cannot use the “real” gradients with respect to quantized operation, since they are zero almost everywhere. The common solution to this problem, with relation to the quantization of weights and activations, is to replace the derivative of the quantized function with a Straight-Through-Estimator (STE). In our case, we would like to use the STE with respect to the derivatives of the quantizers Qacc and Qprod inside the FMAq operation from Eq. (4). So far in this work, we used the naive “identity STE” (Bengio et al., 2013) which makes the replacement “ d dx Q(x)” = 1 (we will use the quotation marks to denote an STE replacement of a derivative). However, the more common STE for quantization zeros out gradients outside of the representation range (Hubara et al., 2017). For the quantizers in Eq. (1) and Eq. (2), we get: “ d dxQFIXED B,b (x)” = 1(Rmin < x < Rmax) ; “ d dxQFLOAT M,E,b (x)” = 1(|x| < ROF), (5) where we defined 1(·) as the indicator function which is equal 1 if its input is ‘true’ and zero otherwise. Many alternative forms of STEs exist and have been studied in the context of W/A quantization. The implementation of STEs for LBA networks, on the other hand, has several additional difficulties. The first, most immediate problem, is that the values of the inputs of the quantization functions within the FMAq (Qacc and Qprod) are not exposed to the software or stored in memory during forward propagation. Saving these internal values is generally not feasible, since the quantization operation occurs in each FMAq, and the number of FMAqs in DNNs typically exceeds the size of weights and activations by many orders of magnitude. However, if the hardware operation is deterministic and well-known, we found we are still able to use software for re-computation of the GEMM operation, to retrieve the required values during backpropagation (1 bit per operation). Such a re-computation operation is expensive (training time is doubled, at the very least), and so far feasible only in fully connected and attention layers (not convolutional layers). To the best our knowledge, this is the first time backpropagation is used on the full computational graph of the summation operation. Another possible problem for using standard STEs for the accumulation process stems from the recursive nature of the summation operation. The STE in equation Eq. (5) sets the corresponding gradient of any overflowing value to zero. As explained in Appendix D, if this STE is used for the accumulator’s quantization function, each overflow event will eliminate the gradients of all previously accumulated product pairs. Lastly, another possible problem is that, for floating point summation, other events besides overflow can potentially be important when estimating the gradient. Motivated by the last two potential problems, in appendix D, we propose, describe, and justify the practicality of several alternative methods for estimating the gradients of FMAq(x, w, s). The different methods use different types of STE: OF passes zero on overflow of Qacc (using Eq. (5), while DIFF passes zero on overflow, underflow, and full-swamping events of the FMAq. We also distinguish between a method where we apply identity STE with respect to the partial sum s, and the non-identity STE over the product-pair (x, w) (a.k.a Immediate), to the standard method, where the STE is applied with respect to all inputs (x, w, s) (a.k.a Recursive). For example, defining z ≡ FMAq(x, w, s) = Qacc (Qprod (x · w) + s) and ϵ1, ϵ2 as some small constants, we get: Immediate / DIFF: “dz ds” = 1 ; 1 w “ dz dx” = 1 x“ dz dw ” = 1 \u0012 |z − s| |xw| + ϵ1 > ϵ2 \u0013 , (6) Recursive / OF: “dz ds” = 1 w “ dz dx” = 1 x“ dz dw ” = 1(|Qprod (xw) + s| < ROF) ) (7) In Tab. 6, we compare the accuracy achieved using the proposed STE variants over the MNIST dataset. We see that such fine-grained gradient methods can indeed enable high accuracy in models with only 8-bit accumulators. 8STE Underflow Accuracy (Top-1, %) STE Underflow Accuracy (Top-1, %) Baseline - 98.65 Immediate / OF Yes 98.47 Identity Yes 18.28 Immediate / DIFF Yes 11.35 Identity No 18.28 Immediate / DIFF No 97.67 +Identity* Yes 42.28 Recursive / OF Yes 98.47 Table 6: Training a fully-connected NN with 8-bit (M4E3) accumulators for MNIST classification. The reported accuracy matches the final accuracy of the experiment. The model’s loss does not converge when using naive (Identity) STE for accumulation. Full details in Appendix C.3. *The mantissa for the accumulator was extended by 2 additional bits in this run. As we saw in the case of residual neural networks (Tab. 2 and 3) with 1-stage training, successful implementation of LBA is not guaranteed to scale to larger models. To evaluate the quality of our estimated gradients, we would like to compare the optimization of the different approaches. To that end, we train a small LBA transformer from scratch for masked language modeling, over a modest-sized dataset (200K rows), for 15 epochs. In Tab. 7, we compare different STE variants for a variety of very-low precision accumulators. Accumulator Identity (%) Recursive / OF (%) Immediate / OF (%) Immediate / DIFF (%) FP32 51.31 - - - M3E3 20.86 19.20 14.80 24.60 M4E3 13.88 39.57 37.23 41.94 M5E3 9.47 45.28 44.76 50.12 M6E3 14.71 46.17 46.13 50.03 M3E4 15.2 15.15 15.43 25.53 M4E4 42.93 42.81 42.81 41.50 M5E4 47.87 48.76 48.76 47.93 Table 7: Accuracy of LBA transformer for the task of Masked Language Modelling (200K rows), when using different STEs for the accumulator operation. Full details of the experiments are available in Appendix C.4. Based on our results for training masked language models, using fine-grained STEs becomes crucial when the number of accumulation bits is decreased below M = 4 or E = 4 (hence, this includes all possible FP8 formats). While successful at improving the optimization, none of the STEs we have tried were successful at closing the gap with the baseline completely, when extreme accumulator quantization was applied. Out of the three proposed STEs, we recommend Immediate/ DIFF STE, which generally achieved better accuracy in the areas where naive, identity STE was insufficient, despite its higher cost. The Immediate/ DIFF STE may also prove more suitable in cases where the exact behavior of the FMAq is unknown (i.e., ’black-box’) since its definition is agnostic to the FMAq internals. 5 D ISCUSSION The quantization of the accumulator in deep neural networks is a hard but necessary task in the effort to improve neural networks’ efficiency, reduce cost, and cut down carbon footprint. Despite the many difficulties involving the training, the implementation, and the theoretical analysis of networks with low-bit-accumulators, our results show that LBA networks are surprisingly easy to fine-tune. By applying simple optimization methods over pre-trained networks, we show it is possible to adjust the models for inference with cheaper hardware, that utilizes12 bits accumulators. When the accumulators bit width is further reduced we alleviate the accuracy degradation by using fine-grained approaches for estimating the gradient. 9ACKNOWLEDGMENTS The research of DS was Funded by the European Union (ERC, A-B-C-Deep, 101039436). Views and opinions expressed are however those of the author only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency (ERCEA). Neither the European Union nor the granting authority can be held responsible for them. DS also acknowledges the support of the Schmidt Career Advancement Chair in AI. REFERENCES Ankur Agrawal, Sae Kyu Lee, Joel Silberman, Matthew Ziegler, Mingu Kang, Swagath Venkatara- mani, Nianzheng Cao, Bruce Fleischer, Michael Guillorn, Matthew Cohen, et al. 9.1 a 7nm 4-core ai chip with 25.6 tflops hybrid fp8 training, 102.4 tops int4 inference and workload-aware throttling. In 2021 IEEE International Solid-State Circuits Conference (ISSCC), volume 64, pp. 144–146. IEEE, 2021. (Cited on 4) Michael Andersch, Greg Palmer, Ronny Krashinsky, Nick Stam, Vishal Mehta, Gonzalo Brito, and Sridhar Ramaswamy. Nvidia hopper architecture in-depth, Apr 2022. URL https: //developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/ . (Cited on 1, 4) David H Bailey. High-precision floating-point arithmetic in scientific computation. Computing in science & engineering, 7(3):54–61, 2005. (Cited on 2) Ron Banner, Itay Hubara, Elad Hoffer, and Daniel Soudry. Scalable methods for 8-bit training of neural networks. In Advances in Neural Information Processing Systems, pp. 5145–5153, 2018. (Cited on 1, 17) Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation, 2013. (Cited on 8) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. (Cited on 2) Brian Chmiel, Ron Banner, Elad Hoffer, Hilla Ben Yaacov, and Daniel Soudry. Logarithmic unbiased quantization: Practical 4-bit training in deep learning. arXiv preprint arXiv:2112.10769, 2021. (Cited on 1) Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks. Advances in Neural Information Processing Systems, 2016. (Cited on 2) Theodorus Jozef Dekker. A floating-point technique for extending the available precision.Numerische Mathematik, 18(3):224–242, 1971. (Cited on 3) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023. (Cited on 7) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. (Cited on 6) Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited numerical precision. In International Conference on Machine Learning, pp. 1737–1746, 2015. (Cited on 2) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. (Cited on 6) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. (Cited on 7) 10Nicholas J Higham. The accuracy of floating point summation.SIAM Journal on Scientific Computing, 14(4):783–799, 1993. (Cited on 3) Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized neural networks: Training neural networks with low precision weights and activations.The Journal of Machine Learning Research, 18(1):6869–6898, 2017. (Cited on 1, 8) Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richárd Nagyfi, et al. Openassistant conversations–democratizing large language model alignment. arXiv preprint arXiv:2304.07327, 2023. (Cited on 7) Andrey Kuzmin, Mart Van Baalen, Yuwei Ren, Markus Nagel, Jorn Peters, and Tijmen Blankevoort. Fp8 quantization: The power of the exponent. Advances in Neural Information Processing Systems, 35:14651–14662, 2022. (Cited on 5, 6) Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. In Neural Information Processing Systems, 2018. (Cited on 5) Tailin Liang, John Glossner, Lei Wang, Shaobo Shi, and Xiaotong Zhang. Pruning and quantization for deep neural network acceleration: A survey. Neurocomputing, 461:370–403, 2021. (Cited on 1) Markus Nagel, Marios Fournarakis, Yelysei Bondarenko, and Tijmen Blankevoort. Overcoming oscillations in quantization-aware training. arXiv preprint arXiv:2203.11086, 2022. (Cited on 1) Renkun Ni, Hong-min Chu, Oscar Castañeda, Ping-yeh Chiang, Christoph Studer, and Tom Gold- stein. Wrapnet: Neural net inference with ultra-low-resolution arithmetic. arXiv preprint arXiv:2007.13242, 2020. (Cited on 1, 3, 6, 7) Charbel Sakr, Naigang Wang, Chia-Yu Chen, Jungwook Choi, Ankur Agrawal, Naresh Shanbhag, and Kailash Gopalakrishnan. Accumulation bit-width scaling for ultra-low precision training of deep networks. arXiv preprint arXiv:1901.06588, 2019. (Cited on 1, 3) Xiao Sun, Jungwook Choi, Chia-Yu Chen, Naigang Wang, Swagath Venkataramani, Xiaodong Cui, Wei Zhang, Kailash Gopalakrishnan, et al. Hybrid 8-bit floating point (hfp8) training and inference for deep neural networks. 2019. (Cited on 2, 17) Xiao Sun, Naigang Wang, Chia-Yu Chen, Jiamin Ni, Ankur Agrawal, Xiaodong Cui, Swagath Venkataramani, Kaoutar El Maghraoui, Vijayalakshmi Viji Srinivasan, and Kailash Gopalakrishnan. Ultra-low precision 4-bit training of deep neural networks. Advances in Neural Information Processing Systems, 33:1796–1807, 2020. (Cited on 1, 2) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. (Cited on 7) Mart van Baalen, Andrey Kuzmin, Suparna S Nair, Yuwei Ren, Eric Mahurin, Chirag Patel, Sundar Subramanian, Sanghyuk Lee, Markus Nagel, Joseph Soriaga, et al. Fp8 versus int8 for efficient deep learning inference. arXiv preprint arXiv:2303.17951, 2023. (Cited on 1, 16) Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan. Training deep neural networks with 8-bit floating point numbers. In Advances in neural information processing systems, pp. 7675–7684, 2018. (Cited on 2, 3, 6, 7) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Huggingface’s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019. (Cited on 13, 14) Tianyi Zhang, Zhiqiu Lin, Guandao Yang, and Christopher De Sa. Qpytorch: A low-precision arithmetic simulation framework, 2019. (Cited on 6, 13) 11A G ENERAL MATRIX MULTIPLICATION : E XAMPLE In section 2.4, we defined the FMA operation, and presented Eq. (3) as a general formula for all GEMM operation. It is worth taking a moment to illustrate the connection between the known tensor operations and the formula. For example, let us look at the simple case of matrix-multiplication (Y = XW T , X∈ Rd0×d1 , W∈ Rd2×d1 ). Here, if we wish to calculate the scalar y = Ykl, we can use the mapping: xi = Xki, wi = Wli. In this case, all values of X and W were used exactly once in the calculation of Y . This is not always the case, however. In batch matrix multiplication, values ofW will be used multiple times, paired with values of X of different batch dimension. In convolution, the same values of W will be used to calculate every scalar in the same output channel, and the neuron in the input channel may be used to calculate a multiple values in multiple output channel. This level of repetition will be, in part, what prevents us from using fine-grained STE methods on convolutional neural networks, in Sec. 4. B E FFECT OF QUANTIZED FMA ON ZERO -SHOT ACCURACY To give the reader a sense of the effect of low bit accumulators on deep neural networks, we include Tab. 8, where we measure the zero-shot accuracy of different ResNet architectures, pretrained with full-precision, after replacing all FMA components with FMAq (as described in C). Mantissa Effect Model Baseline M10E5 M9E5 M8E5 M7E5 M6E5 ResNet18 69.75 69.50 68.95 66.70 57.09 20.49 ResNet34 73.31 73.17 72.68 70.46 60.07 17.19 ResNet50 76.12 75.95 75.57 73.70 64.94 19.48 Exponent Bias Effect (M7E4) Model b = 8 b = 9 b = 10 b = 11 b = 12 bacc, bprod = 10, 12 ResNet18 55.68 60.64 60.00 58.84 56.96 60.14 ResNet34 50.80 63.30 63.88 62.46 59.90 63.65 ResNet50 26.41 64.25 68.69 67.57 66.12 68.49 Table 8: Zeroshot Accuracies for LBA-ResNets, with weights of pre-trained, full precision ResNets [%] The accuracies presented in Tab. 8 illustrates well why M7E4 quantization was chosen: Increasing the mantissa below M = 7 bits would result a much lower zero-shot accuracy, too far for proper fine-tuning. Likewise, reducing the number of bits to E = 4 already resulted lower accuracy due to overflow and underflow events, as indicated by the effect of the exponent bias. For example, the default exponent bias for E = 4 is b = 8, and using it for the accumulator in Resnet50 results in a significant degradation in accuracy. A small increase to b = 9, increases both underflow and overflow thresholds by a factor of 2 and is sufficient for increasing the accuracy by almost 40%. C E XPERIMENTS IMPLEMENTATION DETAILS C.1 I MAGE NET Each of the ImageNet experiments were performed on a single server, containing 8 NVIDIA GPUs (RTX 2080 Ti, RTX A6000). We used a total mini-batch size of 256, equally divided across the 8 workers. For the training datasets, we used the standard RandomResizedCrop and RandomHorizon- talFlip augmentations only. With no quantization, our model architecture was identical to the standard torchvision architecture, for all ImageNet models, while our custom GEMM kernels were used to override all forward GEMM operations (convolutions and matrix multiplications). For optimization, we used the Adam optimizer, with the hyperparameters β = (0.9, 0.999), ϵ= 10−8, λ= 10−4. Dropout was not used. As mentioned in the main text, we used cosine scheduling, the parameters of which depend on the phase in which it was used. We used 10 epochs in the 1-stage compared 12with 5 epochs for the dual-stage to support our claims that the gaps between the methods (where they exist) are not simply a result of better hyperparameters. The epoch count was initially chosen due to time-constraints, and was kept since the benefit of running more epochs was small. For W/A quantization, we used the qtorch (Zhang et al., 2019) library, which provides reliable quantization functions. Weights quantization was applied during every optimization step, while the activations were quantized using dedicated modules, preceding all convolutions, except the first one, and the downsample convolutions. The input of the final fully-connected layer was not quantized as well, in accordance with prior works. The quantization function we applied used stochastic-rounding (which is not considered expensive in this case, as we are not implementing the FMAq internals and the number of quantized components is significantly lower). No other components (e.g. Gradients or Momentum) were quantized since our solution is only aimed at inference. In addition to hyperparameters used in this experiment, we have also ran a set of experiments using fixed-learning rates (no cosine annealing). In the other set, we tested a few initial learning rate values (1E-7, 3E-8, 1E-8) for few epochs, and used enough epochs to reach convergence (for training accuracy/loss). The results in this regime were slightly better than the results published in the paper: For 8bit quantized ResNets with 4ME3, we achieved 69.6% for Resnet18, 73.48% for ResNet34 and 76.35% for ResNet50. However, this required more epochs and finer-tuned hyperparameters (different models used different learning rates). In the paper, we used the regime with cosine annealing since it was more robust to hyperparameter changes. C.2 SQUAD For the SQUAD fine-tuning experiment, we use 8 NVIDIA GPUs (RTX 2080 Ti, RTX A6000). We used the SQUAD training script of the transformers library (Wolf et al., 2019), while using our custom, LBA-model. In our LBA model, all fully connected layers and matrix multiplication operations were modified to use LBA GEMM operations during forward propagation, with the exception of the final fully connected layer (qa-outputs). For pre-trained models, we used either bert-base-uncased for Bert or prajjwal1/bert-small for Bert-small. For optimization, the Adam optimizer, with 1000 warmup steps to a learning rate of 3 · 10−5, from which we applied a cosine annealing scheduler. The batch size was configured to be 8. Our run was set for 20 epochs, but we applied early stopping once the model performance reached its peak (usually after 3 − 5 epochs). C.3 MNIST For each experiment with the MNIST setting, we used a single RTX 2080 Ti GPU with a mini- batch size of 16. Our neural network consisted of 4 fully connected layers (with LBA), and ReLU activations, with all hidden layers being1024 neurons wide. Outside of the accumulator, all data types were with full precision. Dropout wasn’t used (although it was shown to benefit the results slightly), and no data augmentation wasn’t used during training. For optimization, we used Adam optimizer, with an initial learning rate of10−3, with the hyper-parameters: β = (0.9, 0.999), ϵ= 10−8, λ= 0.0, and StepLR scheduler (γ = 0.95). We used 100 epochs per experiment, which was usually much more than needed for convergence or divergence. To test the STE, we replaced the default linear operations with our custom implementation, this time also implementing a custom backward operation. During backpropagation, we used a new, cuda kernel that imitated the (deterministic) operation of the original GEMM operation (using the available weights and activations), but outputted a binary tensor, that indicated the value of all STEs involved in the operation (the type of STE was configurable). For recursive implementation, we modified the tensor ad-hoc to account for the recursive nature of the STE (although less efficient than the optimal implementation). After running the kernel, we used the output to adjust the computation of the weights/ neural gradients as described in section D. In this experiment, we used a fixed exponent bias of 5, which was shown to perform the best among all values in its vicinity. C.4 M ASKED LANGUAGE MODELLING Each of the Masked Language Modelling (MLM) experiments was performed on a single server, containing 8 NVIDIA GPUs (RTX 2080 Ti, RTX A6000, or A100). Our tests were run over theoscar: unshuffled-original-af dataset, with a single tokenizer we trained over the same dataset (vocabulary 13size of 1000). The dataset was chosen due to its moderate size (200K rows), being difficult enough to show gaps in convergence while allowing us to perform meaningful optimizations with simulation kernels in moderate time. For the transformer, we used the Bert architecture, with the hidden size of 512, 2 hidden layers, 4 attention heads, and maximum position embedding of 1024 (All other parameters were according to transformers library defaults). We used the available Huggingface infrastructure (Wolf et al., 2019) to train/ evaluate the model, with Adam optimizer, an initial learning rate for 10−3, a drop-on-plateau scheduler (evaluating every 250 step, γ = 0.1), and a global mini- batch size of 64. In practice, the drop-on-plateau scheduling was only applied to ‘failed’ runs, to give them another shot for optimization, with no success (They did not converge, even well passed the 15 specified epochs). When the number of exponent bits was set to E = 3, we used a fixed exponent bias of b = 6 for the product and accumulator. D G RADIENT ESTIMATION FOR LBAS Following the general equation for GEMM operation (Eq. (3)), the operation can be expressed, using the recursive expression: S0 = 0; Si+1 = FMA (xi, wi, Si) ; y = SN−1 (8) In this example, we add the values of the product to the intermediate accumulator sum, ( S), in a sequential manner. Different orderings of the FMA operation are possible and can have an effect on the output (i.e., floating point addition is not commutative as ‘ideal’ addition, due to swamping). Let us write the recursive expression in Eq. (8) explicitly Sq i ≡ FMAq (xi−1, wi−1, FMAq (xi−2, wi−2, FMAq (...FMAq(x0, w0, 0)))) ; yq = Sq N−1. (9) Our goal in this section is to find a good estimate for the derivative for ∂yq ∂xi and ∂yq ∂wi . D.1 R ECURSIVE STE S The first, and most obvious method to estimate the derivative is by using the common STE (Eq. (5)). Per our definition, FMAq contains two quantization functions. Our main concern, however, is for the post-accumulator quantization, Qacc, and our first attempt will be to quantize it directly using a general STE function (STE : R3 → R). By doing so, we get the gradients: “ dyq dSq i ” = 1 wi “dyq dxi ” = 1 xi “ dyq dwi ” = dyq dSq i+1 STE (xi, wi, Sq i ) , (10) which, when expanded upon, will give us: “ dyq dSq i ” = NY k=i+1 STE (xk, wk, Sq k) . (11) Eq. (11) reveals an additional, possible issue for using standard STEs for the accumulation process. Usually, when applied over an overflowed activation neuron, the STE in equation Eq. (5) will set the corresponding neural gradient to zero. If the same STE is used for the accumulator’s quantization function, as per Eq. (11), each overflow event will eliminate the neural gradients of all previously accumulated product-pairs. Still, this approach may help us calculate a more accurate gradient, provided that conditions in which the STE returns 0 are not commonly met. We will denote the approach in Eq. (11) as Recursive. To perform the recursive correction to the gradient, all we really need to know is the last index (if any), in which the accumulator was overflown. While this may be more complex in cases where the values are added in non-sequential ordering, this is still a feasible calculation to perform, with modest-sized output. Computationally, calculating the underflow indexes is no more difficult as a task than computing the original GEMM operation, and this calculation (as well the calculation that will be presented in the following section), can be done during backpropagation, to avoid using essential memory for a long term. 14D.2 I MMEDIATE STE S To avoid setting too many gradients to zero, we suggest an alternative approach. First, we will re-write Eq. (9) as: Sq i = α0x0w0 + α1x1w1 + α2x2jw2 + ... + αi−1xi−1wi−1 , (12) where αi ≡ FMAq (xi, wi, Sq i ) − Sq i xiwi . (13) If xi = 0 or wi = 0 , we will define αi = 0 for simplicity. Recall Sq i here is the value of the “quantized\" accumulator in step i (Eq. (9)). From its definition, αi is the correction we make to the product xi · wi, to account for the FMA quantization error. Our choice to express the correction this way is based on the assumption that for most steps, |Sq i | ≫ |xi · wi|. This is true, because Sq i is, approximately, the accumulated sum of many such products. During a floating point addition, the bits of the lesser value will be the first to be swamped out, and thus we entangle the quantization error with this component. Moving on to the gradients, we are interested in the gradients of the operation inputs, dyq dxi and dyq dwi . We can use the chain rule to get: dyq dxi = wiαi + N−1X k=i+1 dαk dxi xkwk. (14) The exact expression we got for the gradient remains complex, sincedαk dxi ̸= 0, for k > i. Nevertheless, moving forward, we will take the approximation that ∀k, dαk dxi = 0. i.e., we neglect the cumulative effect that any individual scalar value has on the quantization correction we make in the following steps of the GEMM operation. We then get: dyq dxi = wiαi, dyq dwi = xiαi (15) Eq. (15) suggests a correction we can make to the standard backpropagation operation, which we will denote as an Immediate STE. However, to make any use of this correction, we must first have the values αi. In terms of computation, calculating α is quite similar to performing the original GEMM operation. In terms of memory, however, αi scales with the number of overall FMA operations. This is feasible in the case of fully connected operations, but not for GEMM operations that include a large amount of shared weights. To make sure the evaluation of αi by itself does not overburden the memory, it is possible to quantize the values of αi. By doing so, we get the equation: 1 wi “dyq dxi ” = 1 xi “ dyq dwi ” = QFIXED B,0 \u0012FMAq (xi, wi, Sq i ) − Sq i xiwi + ϵ1 \u0013 . (16) where ϵ1 is a small constant, added with flexible sign to prevent invalid denominator. In our experi- ments, we have observed that the quantization of αi does not harm the quality of the optimization process, and proceeded to binarize the value. The result is that we ended up suggesting an alternative STE to the one presented in Eq. (5), which is designed to address overflow only. We denote the new STE as DIFF: STEDIFF (xi, wi, Sq i ) = ( 1 |FMAq(xi,wi,Sq i )−Sq i | |xiwi|+ϵ1 > ϵ2 0 Else STEOF (xi, wi, Sq i ) = \u001a1 |Qprod(xiwi) + Sq i | < ROF 0 Else (17) The DIFF STE is similar to the common Overflow STE, but is tuned to detect cases of full-swamping and cases of product underflow in addition to cases of overflow. In our experiments, we tested the immediate approach with both STEs. One unique advantage of the DIFF STE, is that is agnostic to the specific implementation of the FMAq component. Therefore, the DIFF STE remains relevant in 15the general cases where the FMAq operation has an unspecified, black-box behavior, as common for hardware modules. Our derivation in this section was done in respect to the sequential ordering of FMAq operations, which is not commonplace in hardware accelerators that try to achieve large degree of parallelism. A more typical case, presumably common for systolic-array-like accelerators, is the chunk-based accumulation, where the accumulation is performed in two hierarchies, as seen in Fig. 1. In our experiments, all simulated GEMM operations and gradient estimation used a chunk size of 16, which means that an operation with an accumulation width of N is initiated with N 16 parallel operations (i.e., the first hierarchy), before aggregating the results (i.e., the second hierarchy). For example, in the case of recursive STE, every detection of OF or DIFF during re-computation will result in a ‘0’ in all preceding operations, just as we saw for sequential accumulation. The only difference for parallel accumulation is that the hierarchy tree can expand in two directions (Like 1 (right), with all the arrows reversed). E H ARDWARE ANALYSIS In this section, we try to give an estimate for the effect of incorporation of LBA models on hardware cost (area/ power), by estimating the number of gates needed to implement qLBA with different levels of quantization. Following an existing design of FMAq component (van Baalen et al. (2023),figure 2b), we adjusted the design for the case of FMAq with m/e quantization of weights and activations and M/E quantization of intermediate values (product, accumulator), and suggested the following gate counts, as seen in table 9. The gate counts are all based on the gate count assumptions listed in [van Baalen et al. (2023), appendix B], and common block designs. FMA Components breakdown Gate Count Exponent Adder (e − 1) · CFA + CHA Exponent Differ (min(E, e+ 1) − 1) · CFA + CHA · (1 + |e + 1 − E|) Exponent Max E · CMUX Mantissa MUL (m + 3)2 · CAND + (m + 2)2 · CFA + (m + 2) · CHA Sort Exponent (M + 1) · CMUX 1st Shift (M + 1 >> k→ F) (F − 1) · log2(kmax) · CMUX Mantissa Adder (F, F→ F) (M) · CFA + CHA Leading Zero Detector F(CAND + COR) + log2(kmax)2COR 2nd Shift (F >> k→ M + 1) (M + 1) · log2(kmax) · CMUX − kmax · (CFA − CAND) Exponent Rebase (E − 1) · CFA + CHA Final Incrementor (M + 1)CHA Table 9: FMA components gate-count breakdown. For the gate count, we used CAND = COR = 1 for the standard gates AND2/OR2, CMUX = 3 for MUX2, and CHA = 3, CFA = 7 for half and full adder. We do not include Flip-Flops in our gate count. For the value of F (Canvas bits, after shifting to fixed-point representation), we used 2M +1, the maximum bit width in which two 2’s complementary values with M + 1 bits can interact during addition. For kmax (the maximum shift distance), we used min(log2(F), E), as the magnitude of the shift is bounded by both the number of exponent bits and the size of the canvas F. Weights/Activations bits FMAq Bits Canvas Gates m e M E F log2(kmax) Count Ratio [%] 4 3 23 8 47 6 2208 100 4 3 10 5 21 5 1082 49 4 3 7 4 15 4 808 37 Table 10: Gate estimation for Quantized FMA We summarize our numerical results in table 10. Our results show that for 8bit activations and weights (at FP format M4E3), as we used in the paper, any half-precision FMAq that follows our quantization 16scheme is expected to reduce the number of gates by about 50% from the gate count of full-precision accumulators. Reducing the accumulator to M7E4, as was done in the paper, will cut the number of gates by an additional 25%, compared to half precision. We conclude our 12bit accumulators will reduce the gate count by 63% compared to 32 bit FP accumulation. We note that the 16-bit accumulation gate count in our analysis is not directly applicable to previous works that used16-bits accumulators– This is because in Sun et al. (2019), only the output of the accumulator was quantized with no explicit quantization of the internals. Presumably, the majority of the gain there was achieved by the reduction of communication bandwidth between FMAq components, which does not affect the gate count in this analysis. F W HY IS IT HARD TO TRAIN WITH UNDERFLOW ? Our claim that underflow cause unique problems during SGD optimization is based, first and foremost, on empirical observations (see: Tab. 2, Tab. 3). In addition, we suggest several explanations to why this problem may occur when underflow is introduced during training, despite it having a small effect on the loss landscape. Consider a neural network, where a specific scalar weight w is connected to its scalar activation x as input, and their product is z = xw. Suppose w is small enough so that z consistently results in product underflow, i.e. Qprod(z) = 0. In this case, during forward propagation, the value of x has little to no direct effect on the output neuron to which w is connected. Therefore, it is reasonable to assume that the computed neural gradient g ≡ dL dz (where L is the loss) will be uncorrelated with x. Consequently, the gradient update of the weight w will be ∆w ∝ gx, with the expected value E[∆w] ∝ E[gx] = E[g]E[x]. Based on previous quantization literature Banner et al. (2018), we have approximately E[g] = 0, and so E[∆w] = 0. Therefore, any sufficiently small weight w will become “stuck\", so that its z cannot escape underflow for a long time. The issue is excavated by the ratio between updates magnitude, and the magnitude a weight has to be updated to surpass the underflow threshold. In a fully-trained model, the gradients are expected to be dL dW ≃ 0. When transitioning to an LBA-model, we make sure to avoid significant changes to the loss landscape (as indicated by the zero-shot accuracy). As a result, we can expect the relative change in gradient to remain small, |∆w| = |η dL dw | ∼ |w|. (Otherwise, the loss landscape would change rapidly during SGD, and we can no longer consider the process as fine-tuning). When dealing with quantized values, it is always possible that a gradient step will be too small to change the value. (This is the main motivation behind stochastic rounding, which is not suitable for our case). For example, for floating point quantization without underflow/overflow, the gradient step must be approximately |∆w| = |η dL dw | ≥2−M |w| for the quantized value of w to ‘jump‘ quantization level. In this case, |∆w| ∼ |w| means that the ability of all weights to change during fine-tuning only depends on M, and the learning rate. In the case of underflow, however, values must surpass an absolute threshold (2−b), for the gradient step to have any effect. Consequently, under previous assumptions, any small enough value subjected to floating point quantization is expected to receive updates which are too small to result a state-change. This is what we referred to when mentioning values being ’stuck’ and ’escaping’. In LBA networks, the quantization is performed over intermediate values (products and partial accumulation values). These value do not get the explicit updates, but they will still get implicit updates by passing their respective neural gradient backwards. 17",
      "meta_data": {
        "arxiv_id": "2401.14110v1",
        "authors": [
          "Yaniv Blumenfeld",
          "Itay Hubara",
          "Daniel Soudry"
        ],
        "published_date": "2024-01-25T11:46:01Z",
        "pdf_url": "https://arxiv.org/pdf/2401.14110v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The primary research problem addressed is the computational bottleneck caused by high-precision accumulation operations in Deep Neural Networks (DNNs) even when weights and activations are quantized. Existing low-precision accumulators typically lead to significant performance degradation. The paper presents a simple method to train and fine-tune high-end DNNs, enabling, for the first time, the utilization of cheaper, 12-bit accumulators with no significant degradation in accuracy. Furthermore, for accumulation precisions lower than 12 bits, the authors show that using fine-grained gradient approximations can significantly improve DNN accuracy, specifically by backpropagating through the entire accumulation-computation graph.",
        "methodology": "The core methodology involves simulating quantized Fused Multiply-Add (FMA) operations, referred to as FMAq, in CUDA, where `FMAq(x, w, s) ≡ Qacc (Qprod (x · w) + s)`. The `Qprod` and `Qacc` quantization functions simulate hardware operations using a simple 'floor' operation (bit-mask) for efficiency. For 12-bit accumulators, a two-stage fine-tuning scheme is proposed: initially fine-tuning a pre-trained full-precision model with quantized FMA (M7E4, bacc=10, bprod=12) while excluding underflow events, followed by a second stage where underflow is enabled for further fine-tuning with a reduced learning rate. The exponent bias for the accumulator is dynamically adjusted (`bacc = bprod − 1/2 log2 (Chunk-Size)`) to avoid overflow. For bit-widths below 12 bits, fine-grained gradient approximation methods are introduced to address quantization failures. This involves re-computing the GEMM operation during backpropagation to retrieve internal values for applying Straight-Through-Estimators (STEs) directly to the `Qacc` and `Qprod` quantizers. Two main STE variants are explored: 'Recursive / OF', which zeros gradients on Qacc overflow, and 'Immediate / DIFF', which applies identity STE to the partial sum and a non-identity STE to the product pair, tuned to detect overflow, underflow, and full-swamping.",
        "experimental_setup": "Experiments were conducted on various DNN architectures and tasks. For Image Classification, ResNet18, ResNet34, and ResNet50 models were fine-tuned for ImageNet, using M7E4 12-bit accumulators, with some experiments also incorporating FP8 (M4E3 flex-bias) quantization for weights and activations. Training utilized 8 NVIDIA GPUs (RTX 2080 Ti, RTX A6000), Adam optimizer (initial learning rate 10^-6, cosine scheduler), and a mini-batch size of 256. For Language Models, Bert (Small, Base, Large) was fine-tuned for the SQUAD Question-Answering task, and LLama-v2-7B was fine-tuned using QLoRA for the MMLU benchmark, both using M7E4 accumulators and varying exponent biases. Further experiments on fine-grained gradient methods used a shallow fully-connected DNN on MNIST (with 8-bit M4E3 accumulators) and a small LBA transformer for Masked Language Modelling (MLM) on a 200K row dataset, testing various very-low precision accumulators (M3E3, M4E3, M5E3, M6E3, M3E4, M4E4, M5E4) with different STE variants. The ImageNet experiments used a two-stage fine-tuning approach (5 epochs without underflow, then 1 epoch with underflow). Validation metrics included Top-1 accuracy, F1 score, Exact match, and 5-shot test accuracy.",
        "limitations": "The hardware implementation of certain techniques, such as chunk-based accumulation and stochastic rounding, from prior works, is noted as challenging on modern hardware due to difficulties in configuring accumulation order and increased hardware cost. The proposed fine-grained gradient methods suffer from significant computational expense, doubling training time at least, and are currently only feasible for fully connected and attention layers, not convolutional layers, due to memory and re-computation costs. While successful at improving optimization for lower bit-widths, the fine-grained STEs did not entirely close the accuracy gap with full-precision baselines when extreme accumulator quantization was applied. The 'Recursive / OF' STE variant can lead to the undesirable effect of eliminating gradients of previously accumulated product-pairs if an overflow event occurs. The 'Immediate / DIFF' STE makes an approximation by neglecting the cumulative effect of individual scalar values on quantization correction in subsequent steps.",
        "future_research_directions": "The paper implies future research should focus on scaling the proposed methods to larger models and more complex tasks, as this is crucial for commercial viability. Further investigation into more effective fine-grained Straight-Through-Estimators (STEs) is needed, particularly for extreme accumulator quantization (below M=4 or E=4), to fully close the accuracy gap with full-precision baselines. Exploring the applicability and improving the efficiency of fine-grained gradient methods for convolutional neural networks is a clear area for extension. Additionally, research into STEs that are agnostic to the internal workings of the FMAq component (like the Immediate/DIFF STE) could be beneficial for scenarios involving 'black-box' hardware modules."
      }
    },
    {
      "title": "Searching for Low-Bit Weights in Quantized Neural Networks",
      "abstract": "Quantized neural networks with low-bit weights and activations are attractive\nfor developing AI accelerators. However, the quantization functions used in\nmost conventional quantization methods are non-differentiable, which increases\nthe optimization difficulty of quantized networks. Compared with full-precision\nparameters (i.e., 32-bit floating numbers), low-bit values are selected from a\nmuch smaller set. For example, there are only 16 possibilities in 4-bit space.\nThus, we present to regard the discrete weights in an arbitrary quantized\nneural network as searchable variables, and utilize a differential method to\nsearch them accurately. In particular, each weight is represented as a\nprobability distribution over the discrete value set. The probabilities are\noptimized during training and the values with the highest probability are\nselected to establish the desired quantized network. Experimental results on\nbenchmarks demonstrate that the proposed method is able to produce quantized\nneural networks with higher performance over the state-of-the-art methods on\nboth image classification and super-resolution tasks.",
      "full_text": "Searching for Low-Bit Weights in Quantized Neural Networks Zhaohui Yang1,2, Yunhe Wang2, Kai Han2, Chunjing Xu2, Chao Xu1, Dacheng Tao3∗, Chang Xu3 1 Key Lab of Machine Perception (MOE), Dept. of Machine Intelligence, Peking University. 2 Noah’s Ark Lab, Huawei Technologies. 3 School of Computer Science, Faculty of Engineering, University of Sydney. zhaohuiyang@pku.edu.cn; {yunhe.wang,kai.han,xuchunjing}@huawei.com xuchao@cis.pku.edu.cn; {dacheng.tao, c.xu}@sydney.edu.au Abstract Quantized neural networks with low-bit weights and activations are attractive for developing AI accelerators. However, the quantization functions used in most conventional quantization methods are non-differentiable, which increases the optimization difﬁculty of quantized networks. Compared with full-precision param- eters (i.e., 32-bit ﬂoating numbers), low-bit values are selected from a much smaller set. For example, there are only 16 possibilities in 4-bit space. Thus, we present to regard the discrete weights in an arbitrary quantized neural network as searchable variables, and utilize a differential method to search them accurately. In particular, each weight is represented as a probability distribution over the discrete value set. The probabilities are optimized during training and the values with the highest prob- ability are selected to establish the desired quantized network. Experimental results on benchmarks demonstrate that the proposed method is able to produce quantized neural networks with higher performance over the state-of-the-art methods on both image classiﬁcation and super-resolution tasks. 1 Introduction The huge success of deep learning is well demonstrated in considerable computer vision tasks, including image recognition [ 20], object detection [ 11, 40], visual segmentation [ 18], and image processing [26]. On the other side, these deep neural architectures are often oversized for accuracy reason. A great number of network compression and acceleration methods have been proposed to eliminate the redundancy and explore the efﬁciency in neural networks , including pruning [16, 48], low-bit quantization [6, 39, 56], weight decompostion [53, 31], neural architecture search [34, 49] and efﬁcient block design [22, 41, 37]. Among these algorithms, quantization is very particular which represents parameters in deep neural networks as low-bit values. Since the low costs of quantized networks on both memory usage and computation, they can be easily deployed on mobile devices with speciﬁc hardware design. For example, compared with conventional 32-bit networks, binary neural networks (BNNs) can directly obtain a 32×compression ratio, and an extreme computational complexity reduction by executing bit-wise operations (e.g., 57×speed-up ratio in XNORNet [39]). However, the performance of the low-bit neural networks is usually worse than that of full-precision baselines, due to the optimization difﬁculty raised by the low-bit quantization functions. ∗Corresponding Author. Preprint. Under review. arXiv:2009.08695v1  [cs.CV]  18 Sep 2020To reduce the accuracy drop of quantized neural networks, some methods have been proposed in recent years. BiRealNet [35] inserts more shortcuts to help optimization. Structured Binary [59] and ABCNet [32] explore some sophisticated binary blocks and achieve comparable performance to that of full-precision networks, etc. Admittedly, the aforementioned methods have made great efforts to improve the performance of the quantized neural networks. However, the accuracy gap between the full-precision network and its quantized version is still very huge, especially the binarized model. For example, the state-of- the-art accuracy of binarized ResNet-18 is about 10% lower than that of the full-precision baseline. A common problem in existing quantization methods is the estimated gradients for quantization functions, using either STE [6, 39] or self-designed gradient computation manner [34]. The estimated gradients may provide inaccurate optimization direction and consequently lead to worse performance. Therefore, an effective approach for learning quantized neural networks without estimated gradients is urgently required. In this paper, we present a novel weight searching method for training the quantized deep neural network without gradient estimation. Since there are only a few values of low-bit weights (e.g., +1 and -1 in binary networks), we develop a weight searching algorithm to avoid the non-differentiable problem of quantization functions. All low-bit values of an arbitrary weight in the given network are preserved with different probabilities. These probabilities will be optimized during the training phase. To further eliminate the performance gap after searching, we explore the temperature factor and a state batch normalization for seeking consistency in both training and testing. The effectiveness of the proposed differentiable optimization method is then veriﬁed on image classiﬁcation and super-resolution tasks, in terms of accuracy and PSNR values. 2 Related Works To reduce memory usage and computational complexity of deep neural networks, a series of methods are explored, including efﬁcient block design, network pruning, weight decomposition, and network quantization. MobileNet [22, 41] and ShufﬂeNet [37] design novel blocks and construct the networks with high efﬁciency. Weight pruning methods such as DeepCompression [ 16] propose to remove redundant neurons in pre-trained models, while requires speciﬁc hardware for acceleration. Structured pruning methods [21, 48, 51] discard a group of weights (e.g., an entire ﬁlter) in pre-trained networks so that the compressed model can be directly accelerated in any off-the-shelf platforms. Low-rank decomposition methods [ 8, 25, 53, 50] explore the relationship between weights and ﬁlters and explore more compact representations. Gradient-based neural architecture search methods [34, 47] use the continuous relaxation strategy to search discrete operations for each layer. All the candidate operations are mixed with a learnable distribution during searching and the operation with the highest probability is selected as the ﬁnal operation. Quantization methods [ 29, 1, 38, 12, 43, 9, 46, 55] represent weights or activations in a given neural architecture as low-bit values. The model size and computational complexity of the quantized network are much smaller than those of its original version. Compared with previous model compression methods, network quantization does not change the architecture and the calculation process of original ones, which is supported by some mainstream platforms. To maintain the high performance of quantized neural networks, considerable approaches have been explored for training low-bit weights and activations. For instance, Fixed point quantization [ 30] reduces about 20% complexity without loss of accuracy on CIFAR-10 dataset. BinaryConnect [6] represents weights by binary values {−1,+1}, TTQ [ 57] uses the ternary weights {−1,0,+1}. Activations in these two methods are still full-precision values. DoReFa [56] quantizes both weights and activations to low-bit values. Among all the quantization methods, in extreme cases, if activations and weights are both 1 bit, the binary neural network not only occupies less storage space but also utilizes low-level bit operations xnor and bitcount to accelerate inference. ABCNet [32], CBCN [33], BENN [58] and SBNN [59] explored how to modify the BNN to match the performance with full- precision networks. In [ 60], a clearly different viewpoint is investigated into gradient descent, which provides the ﬁrst attempt for the ﬁeld and is worth further exploration. The hidden relationship between different variables is carefully studied, which will beneﬁt many applications. 2Nevertheless, most of the quantization functions in existing methods are non-differentiable, which increases the optimization difﬁculty. Thus, this paper aims to explore a more effective algorithm for quantizing deep neural networks. 3 Methods In this section, we ﬁrst brieﬂy introduce the optimization approach in the conventional weight quantization methods and the existing problems. Then, a low-bit weight search approach is developed for accurately learning quantized deep neural networks. The strategies, including gradually decreasing temperature and state batch normalization, are proposed to eliminate the quantization gap. 3.1 Optimization Difﬁculty in Network Quantization Representing massive weights and activations in deep neural networks using low-bit values is effective for reducing memory usage and computational complexity. However, it is very hard to learn these low-bit weights in practice. For the case of q-bit quantization, the coordinate axis is divided into m = 2q intervals, and each interval corresponds to one discrete value in setV = {v1,v2,··· ,vm},v1 <··· <vm. Denoting the thresholdsas T = {t1,...,t m−1},v1 <t1 ≤v2 ...v m−1 <tm−1 ≤vm, the quantization function Q(x) can be deﬁned as, Q(x) =    v1 if x<t 1, v2 if t1 ≤x<t 2, ... vm if tm−1 ≤x, (1) where x ∈R is the original full-precision values. Given full-precision latent convolutional ﬁlters Wlat q , the weights are quantized as Wq = Q(Wlat q ), which are used to calculate the output features (i.e., activations). In the training phase, the gradients ∇Wq w.r.t the quantized ﬁlters Wq can be calculated with standard back-propagation algorithm. However, the gradients∇Wlatq is hard to obtain, because the derivative ofQis zero almost everywhere and inﬁnite at zero point. The Straight Through Estimator (STE) [6] is a widely used strategy to estimate the gradient of Wlat q , i.e., ∇Wlatq = ∇Wq (2) where the approximate gradients ∇Wlatq are use to update weights Wlat q , ˆWlatq = Wlat q −η·∇Wlatq ·σ(∇Wlatq ), (3) where ηis the learning rate and σ(·) is the gradient clip function. Although Eq. 2 can provide the approximate gradients of Wlat q , the estimation error cannot be ignored if we aim to further improve the performance of quantized networks. Considering that there are a number of layers and learnable parameters in modern neural networks, the optimization difﬁculty for learning accurate quantized models is still very large. 3.2 Low-bit Weight Searching The parameters in a quantized neural network can be divided into two parts: the non-quantized parameters Wf (e.g., in batch normalization, fully connected layer), and the quantized convolutional parameters Wq. The target of training the network is to ﬁnd W∗ f,W∗ q that minimizes W∗ f,W∗ q = arg min Wf ∈R,Wq∈V L(Wf,Wq), (4) where the Wf and Wq belong to different domains, which are real number R and discrete set V, respectively. To optimize modern neural networks, the stochastic gradient descent (SGD) strategy is usually utilized. However, the discrete variables Wq are not feasible to be optimized by SGD. The previously described solution (Eq. 2- 3) by introducing the latent variables can be viewed as a heuristic solution 3that enables updating all the parameters in an end-to-end manner. However, the estimated and inaccurate gradients limit the upper bound of the performance. In contrast, inspired by gradient-based neural architecture search methods [34, 49, 17, 52], we introduce the continuous relaxation strategy to search discrete weights. Consider optimizing an n-dimensional discrete variable W of size (d1,...,d n), where each element w in W is chosen from the m discrete values V = {v1,v2,··· ,vm}. A new auxiliary tensor A ∈Rm×d1×···×dn is created to learn the distribution of W, and the probability over mdiscrete variables is computed according to the following formula, Pi = expAi/τ ∑ jexpAj/τ, i∈{1,··· ,m}, (5) where Pi is probability that the elements in W belong to the i-th discrete value vi, and τ is the temperature controlling the entropy of this system. The expectation of the quantized values for W can be obtained: Wc = ∑ i Pi ·vi, (6) where the continuous state tensor Wc is calculated according to the probability over all the discrete values. Wc is used for convolution during training, and the process of Eq. 6 is differentiable and friendly for end-to-end training. Here we optimize the auxiliary tensor Awhose gradients can be accurately calculated so that we avoid the gradient estimation in previous works. In the inference stage, the weights are quantized by selecting the discrete value with the maximum probability for each position. In formal, the quantized state weights are Wq = ∑ i Ii ·vi, (7) where I = onehot(arg maxi(Pi)), i∈{1,··· ,m}indicates the one-hot vectors with the maximum probability. For a convolution layer in neural networks, a four-dimensional weight tensorW of size M ×N × DK ×DK is to be learned, where M,N , and DK are output channels, input channels, and kernel size, respectively. By using the proposed method to search forq-bit quantized weight W, the problem can be transformed into a special case of dimension n= 4and discrete values m= 2q (Eq. 1). The tensor A∈Rm×M×N×DK×DK is constructed to learn the distribution over the discrete value set V according to Eq. 5. By using the continuous relaxation, the learning of Eq. 4 could be transformed to learn the tensor A, W∗ f,A∗= arg min Wf ∈R,A∈R L(Wf,A), (8) where the entire network is differentiable and can be optimized end-to-end with accurate gradients. 3.3 Optimization Details We use the full-precision tensor Wc for training and quantized tensor Wq for inference. Although the continuous relaxation method solves the problem of inaccurate gradient during training, there is still a quantization gap after converting Wc to Wq. This will lead to the mismatch between the Wq and other parameters trained according to Wc, such as the statistics in the batch normalization layers. 3.3.1 Reducing the Quantization Gap In the proposed low-bit weight searching scheme, the continuous Wc for training and the discrete Wq for inference are not exactly the same. The quantization gap is introduced to measure the quantization error in the process of transforming the softmax distribution to the one-hot vector (Eq. 6- 7), Wgap = Wq −Wc. (9) Fortunately, As stated in the temperature limit Theorem 1, the quantization gap Wgap could be an inﬁnitesimal quantity if the temperature τ is small enough. Theorem 1 ( Temperature Limit Theorem ) Assuming a ∈Rm is a vector in tensor A. If the temperature τ is gradually decreasing to zero, then the quantization gap is an inﬁnitesimal quantity. 4Algorithm 1 Training algorithm of SLB Input: The network Nconstructed by convolution and state batch normalization, training iterations I, dataset D, initialize temperature τs, end temperature τe and temperature decay scheduler T. 1: for iter iin 1,...,I do 2: Update temperature parameter τ according to the temperature decay scheduler T. 3: Get minibatch data X and target Y from dataset D, calculate continuous weights Wc and quantized weights Wq of Naccording to the current temperature τ. 4: The continuous state weights Wc (Eq. 6) is computed and prediction P = N(X). The continuous state statistics in SBN are also updated. 5: Compute the discrete weights Wq (Eq. 7) and the discrete state statistics in SBN layer are updated (Eq. 13). 6: The loss is calculated according to the prediction P and target Y, and the back-propogated gradients are used to update auxiliary matrix A. 7: end for 8: Record quantized tensors Wq and the discrete state statistics in SBN. Output: A trained quantized neural network N∗. proof 1 The distribution pis computed as, pi = expai/τ ∑ jexpaj/τ = 1∑ jexp(aj−ai)/τ, (10) by gradually decreasing τ to zero, the index of the maximum value is k= arg maxipi, the quantiza- tion gap wgap is computed as, lim τ→0 pk = 1, lim τ→0 pi,i̸=k = 0 lim τ→0 wgap = vk − ∑ i pi ·vi = (1−pk) ·vk + ∑ i,i̸=k pi ·vi = 0. (11) We propose the gradually decreasing temperature strategy during training so that limτ→0 Wc = Wq. In particular, at the beginning of optimization, the temperature factor τ is high, and the distribution P is relatively smooth. With the change of temperature τ, the distribution P becomes sharper. In this way, the quantization gap will gradually decrease as τ changes. 3.3.2 State Batch Normalization With the gradually decreasing temperature strategy, the continuous Wc will converge to the discrete Wq. Nevertheless, one problem that cannot be ignored is that the temperature cannot be decreased to a minimal value, i.e., 0. If a neuron w’s probability of choosing its value to be a discrete value is greater than a relatively high threshold (e.g., max(p) >0.999), we may infer that the discrete value will not change as we continue to decrease the temperature and make the distribution sharper. Under this assumption, the quantized weights are well optimized and the only difference between training and inference is the statistics difference for batch normalization layers. Thus we proceed to develop a corresponding state batch normalization that acts as a bridge between a sharp softmax distribution and a one-hot distribution. Batch Normalization (BN) [24] is a widely-used module for increasing the training stability of deep neural networks. The conventional BN for a given input feature ycan be written as, ˆyi = 1 σi (yi −µi), (12) where iis the index of channel and µi,σi are the mean and standard deviation values for the i-th channel, respectively. In the proposed method, Wc is used for convolution during training, so the normalization is formulated as, yc = x⊗Wc, ˆyc,i = 1 σc,i (yc,i −µc,i), (13) 5where xis the input data, ⊗is the convolution operation and yc is the output. µc,i and σc,i are the mean value and standard deviation of the i-th channel in yc. Therefore, the statistics of the quantized weights Wq does not match with the statistics σc and µc of BN , which results in a precision decline in inference. To address this problem, we proposed the State Batch Normalization (SBN). To be speciﬁc, the SBN calculates two groups of statistics during training, one is for yc and the other is for yq where yq is the convolution output using quantized weights Wq. The normalization process of yq is yq = x⊗Wq, ˆyq,i = 1 σq,i (yq,i −µq,i), (14) where µq,i and σq,i are the mean and standard deviation of the i-th channel in yq. Both ˆyc and ˆyq have a mean of zero and a standard deviation of one. We make they share the same group of afﬁne coefﬁcients, zc,i = γi ˆyc,i + βi, zq,i = γi ˆyq,i + βi (15) In this case, the proposed SBN eliminates the small quantization gap in the statistics for batch normalization layers. 3.3.3 Overall Algorithm We view the training for quantized networks as a search problem, and utilize a differentiable method for learning the distribution of quantized weights. The strategies including gradually decreasing temperature and state batch normalization are proposed to eliminate the quantization gap. The overall training algorithm of searching for low-bit weights (SLB) is summarized in Algorithm 1. 4 Experiments 0 100 200 300 400 500 Epochs 40 50 60 70 80 Accuracy (%) exp cos linear Figure 1: The diagram of dif- ferent temperature scheduler. We examine our proposed method on quantized neural networks with different bitwidths. Following common practice in most works, we use the CIFAR-10 [28] and large scale ILSVRC2012 [7] recognition datasets to demonstrate the effectiveness of our method. For all the quantized neural networks, following previous methods [35, 54, 56], all convolution layers except for the ﬁrst one are quantized. The low-bit set V is constructed by uniform distribution from −1 to 1. 4.1 Experiments on CIFAR-10 In this section, we ﬁrst examine different temperature decay schedulers and then compare the results with other methods. We train the network for 500 epochs in total and decay the learning rate by a factor of 10 at 350, 440, and 475 epochs. The matrix Ais initialized by kaiming initialization [19]. The quantization function for the activations is the same as DoReFa [56]. We test several different decay schedulers for temperature changing, includingsin, linear and exp, which are widely used in adjusting the learning rate [ 36, 22]. For convenience, we used T in this section to represent 1 τ in Eq. 5. The temperature T is used to control the entropy of the system. Given the start temperature factor Ts, the end temperature Te and the total number of training iterations I. At iteration i, the temperature Tt in different schedulers is calculated as follow, Tt,linear = Ts + i I ×(Te −Ts), Tt,sin = Ts + sin( iπ 2I) ×(Te −Ts), Tt,exp = Ts ×(Te Ts ) i I . In Figure 1, we comprehensively compare different schedulers on binary ResNet20. We setTs = 0.01 and Te = 10. For sin and linear schedulers, the accuracies converge rapidly. However, some weights are not adequately trained, which results in relatively low accuracies. For the exp scheduler, the accuracies are much higher which indicates the weights converge to better local minima. In the following experiments, we adopt the exp scheduler for temperature adjusting. 6Table 1: Results of ResNet20 on CIFAR-10. Methods Bit-width Acc. (W/A) (%) FP32 [20] 32/32 92.1 DoReFa [56, 13] 1/1 79.3 DSQ [12] 1/1 84.1 SQ [13] 1/1 84.1 SLB 1/1 85.5 LQ-Net [54] 1/2 88.4 SLB 1/2 89.5 SLB 1/4 90.3 SLB 1/8 90.5 LQ-Net [54] 1/32 90.1 DSQ [12] 1/32 90.2 SLB 1/32 90.6 LQ-Net [54] 2/2 90.2 SLB 2/2 90.6 SLB 2/4 91.3 SLB 2/8 91.7 SLB 2/32 92.0 SLB 4/4 91.6 SLB 4/8 91.8 SLB 4/32 92.1 Table 2: Results of VGG-Small on CIFAR-10. Methods Bit-width Acc. (W/A) (%) FP32 [44] 32/32 94.1 BNN [23] 1/1 89.9 XNORNet [39] 1/1 89.8 DoReFa [56, 13] 1/1 90.2 SQ [13] 1/1 91.7 SLB 1/1 92.0 LQ-Net [54] 1/2 93.4 SLB 1/2 93.4 SLB 1/4 93.5 SLB 1/8 93.8 LQ-Net [54] 1/32 93.5 SLB 1/32 93.8 LQ-Net [54] 2/2 93.5 SLB 2/2 93.5 SLB 2/4 93.9 SLB 2/8 94.0 SLB 2/32 94.0 SLB 4/4 93.8 SLB 4/8 94.0 SLB 4/32 94.1 We compare our results with other SOTA quantization methods including BNN, XNORNet, DoReFa, DSQ, SQ, and LQ-Net, on two different network architectures, i.e., VGG-Small [ 44] and ResNet20 [20]. As shown in Table 1 and Table 2, our method outperforms other methods and achieves state-of-the-art results on different bit-widths. 4.2 Experiments on ILSVRC2012 In the ILSVRC2012 classiﬁcation experiments, we use ResNet18 [ 20, 35] as our backbone and compare the results with other state-of-the-art methods. The learning rate starts from 1e-3, weight decay is set to 0, and Adam optimizer is used to update parameters. Table 3 presents the results of our method and a number of other quantization methods. The binary neural network is the most potential method because the xnor and bitcount operations are relatively efﬁcient. Our proposed method on the binary case achieves state-of-the-art accuracy with a Top-1 accuracy of 61.3% . When using more bits on the activations, the Top-1 accuracies gradually increase. Our method consistently outperforms other methods by a signiﬁcant margin. By adding the bitwidth of weights, the proposed SLB also achieves state-of-the-art accuracies on different settings. 4.2.1 The Impact on State Batch Normalization We further verify the importance of state batch normalization by removing it in binary ResNet18. In particular, the discrete weights and discrete state statistics are not calculated during training, and the continuous state statistics are used after training. Because of the quantization gap, the SLB (w/o SBN) in Table 3 decreases the Top-1 accuracy by 0.3%. This indicates the essentiality to maintain two groups of statistics, which are estimated by the continuous state outputs and discrete state outputs, respectively. 4.3 Experiment on Super Resolution To verify the generalization of the proposed method, we apply SLB to the image super-resolution task. The typical model VDSR [27] is selected as the baseline model. Following the original paper, 7Table 3: Overall comparison of quantized ResNet18 on ILSVRC2012 large scale classiﬁcation dataset. ’W/A’ denotes the bitwidth of weights and activations, respectively. Methods Bit-width Top-1 Top-5 (W/A) (%) (%) FP32 [20] 32/32 69.3 89.2 BNN [23] 1/1 42.2 67.1 ABCNet [32] 1/1 42.7 67.6 XNORNet [39] 1/1 51.2 73.2 BiRealNet [35] 1/1 56.4 79.5 PCNN [14] 1/1 57.3 80.0 SQ [13] 1/1 53.6 75.3 ResNetE [2] 1/1 58.1 80.6 BONN [15] 1/1 59.3 81.6 SLB 1/1 61.3 83.1 SLB (w/o SBN) 1/1 61.0 82.9 DoReFa [56] 1/2 53.4 - LQ-Net [54] 1/2 62.6 84.3 HWGQ [4] 1/2 59.6 82.2 TBN [45] 1/2 55.6 79.0 HWGQ [4] 1/2 59.6 82.2 SLB 1/2 64.8 85.5 DoReFa [56] 1/4 59.2 - SLB 1/4 66.0 86.4 SYQ [10] 1/8 62.9 84.6 SLB 1/8 66.2 86.5 Methods Bit-width Top-1 Top-5 (W/A) (%) (%) FP32 [20] 32/32 69.3 89.2 BWN [39] 1/32 60.8 83.0 DSQ [12] 1/32 63.7 - SQ [13] 1/32 66.5 87.3 SLB 1/32 67.1 87.2 PACT [5] 2/2 64.4 - LQ-Net [54] 2/2 64.9 85.9 DSQ [12] 2/2 65.2 - SLB 2/2 66.1 86.3 SLB 2/4 67.5 87.4 SYQ [10] 2/8 67.7 87.8 SLB 2/8 68.2 87.7 TTQ [57] 2/32 66.6 87.2 LQ-Net [54] 2/32 68.0 88.0 SLB 2/32 68.4 88.1 Input Groundtruth (PSNR) FP32 (37.68) DoReFa (36.34) SLB (36.84) (a) Super-resolution results with scale factor ×2. Input Groundtruth (PSNR) FP32 (33.87) DoReFa (32.29) SLB (33.01) (b) Super-resolution results with scale factor ×3. Figure 2: Super-resolution results on Set5 dataset with different scale factors. we use the 291 images as in [ 42] for training and test on Set5 dataset [3]. Each image in the training data is split into patches and augmented with rotation or ﬂip. The baseline model and the binarized models using DoReFa and SLB algorithms are trained with the same setting as in [27]. The models are compared at different scale factors, i.e.×2 and ×3. From the results in Figure 2, we can see that SLB achieves much higher PSNR than DoReFa at both ×2 and ×3 scale factors. In Figure 2(a), the eyebrow in DoReFa’s result is somewhat red while the result of SLB is normal and closer to the raw image. In Figure 2(b), the texture on the wing in DoReFa’s result is blurry and hard to see clearly. SLB could well recover the texture and performs close to the full-precision model. 85 Conclusions In this paper, we use the continuous relaxation strategy which addresses the gradient mismatch problem. To learn the discrete convolution kernels, an auxiliary probability matrix is constructed to learn the distribution for each weight from soft to hard, and the gradients are calculated to update the distribution. The state batch normalization is also proposed to minimize the gap between continuous state outputs and discrete state outputs. Our SLB can be applied to optimize quantized networks on a number of bitwidth cases and achieved state-of-the-art performance, which demonstrates the effectiveness of our method. References [1] Thalaiyasingam Ajanthan, Puneet Dokania, Richard Hartley, and Philip Torr. Proximal mean- ﬁeld for neural network quantization. In ddV, 2019. [2] Joseph Bethge, Haojin Yang, Marvin Bornstein, and Christoph Meinel. Back to simplicity: How to train accurate bnns from scratch? arXiv, 2019. [3] Marco Bevilacqua, Aline Roumy, Christine Guillemot, and Marie Line Alberi-Morel. Low- complexity single-image super-resolution based on nonnegative neighbor embedding. BMVC, 2012. [4] Zhaowei Cai, Xiaodong He, Jian Sun, and Nuno Vasconcelos. Deep learning with low precision by half-wave gaussian quantization. CVPR, 2017. [5] Jungwook Choi. Pact: Parameterized clipping activation for quantized neural networks. arXiv, 2018. [6] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: training deep neural networks with binary weights during propagations. NeurIPS, 2015. [7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. CVPR, 2009. [8] Emily Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Exploiting linear structure within convolutional networks for efﬁcient evaluation. arXiv, 2014. [9] Ruizhou Ding, Ting-Wu Chin, Zeye Liu, and Diana Marculescu. Regularizing activation distribution for training binarized deep networks. CVPR, 2019. [10] Julian Faraone, Nicholas Fraser, Michaela Blott, and Philip H.W. Leong. Syq: Learning symmetric quantization for efﬁcient deep neural networks. CVPR, 2018. [11] Ross Girshick. Fast r-cnn. ICCV, 2015. [12] Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu, Jiazhen Lin, Fengwei Yu, and Junjie Yan. Differentiable soft quantization: Bridging full-precision and low-bit neural networks. CVPR, 2019. [13] Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu, Jiazhen Lin, Fengwei Yu, and Junjie Yan. Differentiable soft quantization: Bridging full-precision and low-bit neural networks. ICCV, 2019. [14] Jiaxin Gu, Ce Li, Baochang Zhang, Jungong Han, Xianbin Cao, Jianzhuang Liu, and David Doermann. Projection convolutional neural networks for 1-bit cnns via discrete back propagation. arXiv, 2018. [15] Jiaxin Gu, Junhe Zhao, Xiaolong Jiang, Baochang Zhang, Liu Jianzhuang, Guodong Guo, and Rongrong Ji. Bayesian optimized 1-bit cnns. ICCV, 2019. [16] Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. ICLR, 2016. [17] Chaoyang He, Haishan Ye, Li Shen, and Tong Zhang. Milenas: Efﬁcient neural architecture search via mixed-level reformulation. arXiv, 2020. [18] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. ICCV, 2017. [19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation. ICCV, 2015. 9[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CVPR, 2016. [21] Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural networks. ICCV, 2017. [22] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolutional neural networks for mobile vision applications. arXiv, 2017. [23] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Bina- rized neural networks. NeurIPS, 2016. [24] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. ICML, 2015. [25] Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks with low rank expansions. BMVC, 2014. [26] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. ECCV, 2016. [27] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate image super-resolution using very deep convolutional networks. CVPR, 2016. [28] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, 2009. [29] Zefan Li, Bingbing Ni, Wenjun Zhang, Xiaokang Yang, and Wen Gao. Performance guaranteed network acceleration via high-order residual quantization. ICCV, 2017. [30] Darryl D. Lin, Sachin S. Talathi, and V . Sreekanth Annapureddy. Fixed point quantization of deep convolutional networks. ICML, 2016. [31] Shaohui Lin, Rongrong Ji, Chao Chen, Dacheng Tao, and Jiebo Luo. Holistic cnn compression via low-rank decomposition with knowledge transfer. TPAMI, 2018. [32] Xiaofan Lin, Cong Zhao, and Wei Pan. Towards accurate binary convolutional neural network. NeurIPS, 2017. [33] Chunlei Liu, Wenrui Ding, Xin Xia, Baochang Zhang, Jiaxin Gu, Jianzhuang Liu, Rongrong Ji, and David Doermann. Circulant binary convolutional networks: Enhancing the performance of 1-bit dcnns with circulant back propagation. CVPR, 2019. [34] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. In ICLR, 2019. [35] Zechun Liu, Baoyuan Wu, Wenhan Luo, Xiaodong Yang, Weiwei Liu, and Kwang-Ting Cheng. Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm. ECCV, 2018. [36] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. ICLR, 2017. [37] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufﬂenet v2: Practical guidelines for efﬁcient cnn architecture design. ECCV, 2018. [38] Hai Phan, Dang Huynh, Yihui He, Marios Savvides, and Zhiqiang Shen. Mobinet: A mobile binary network for image classiﬁcation. arXiv, 2019. [39] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classiﬁcation using binary convolutional neural networks. ECCV, 2016. [40] Joseph Redmon, Santosh Kumar Divvala, Ross B. Girshick, and Ali Farhadi. You only look once: Uniﬁed, real-time object detection. CVPR, 2016. [41] Mark B. Sandler, Andrew G. Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. CVPR, 2018. [42] Samuel Schulter, Christian Leistner, and Horst Bischof. Fast and accurate image upscaling with super-resolution forests. CVPR, 2015. [43] Mingzhu Shen, Xianglong Liu, Kai Han, Ruihao Gong, Yunhe Wang, and Chang Xu. Balanced binary neural networks with gated residual. arXiv, 2019. 10[44] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. ICLR, 2015. [45] Diwen Wan, Fumin Shen, Li Liu, Fan Zhu, Jie Qin, Ling Shao, and Heng Tao Shen. Tbn: Convolutional neural network with ternary inputs and binary weights. ECCV, 2018. [46] Ziwei Wang, Jiwen Lu, Chenxin Tao, Jie Zhou, and Qi Tian. Learning channel-wise interactions for binary convolutional neural networks. CVPR, 2019. [47] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, and Kurt Keutzer. Fbnet: Hardware-aware efﬁcient convnet design via differentiable neural architecture search. CVPR, 2019. [48] Zuxuan Wu, Tushar Nagarajan, Abhishek Kumar, Steven Rennie, Larry S. Davis, Kristen Grauman, and Rogério Schmidt Feris. Blockdrop: Dynamic inference paths in residual networks. CVPR, 2018. [49] Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin. Snas: stochastic neural architecture search. In ICLR, 2019. [50] Zhaohui Yang, Yunhe Wang, Chuanjian Liu, Hanting Chen, Chunjing Xu, Boxin Shi, Chao Xu, and Chang Xu. Legonet: Efﬁcient convolutional neural networks with lego ﬁlters. ICML, 2019. [51] Jiahui Yu and Thomas Huang. Autoslim: Towards one-shot architecture search for channel numbers. arXiv, 2019. [52] Jiahui Yu, Pengchong Jin, Hanxiao Liu, Gabriel Bender, Pieter-Jan Kindermans, Mingxing Tan, Thomas S. Huang, Xiaodan Song, Ruoming Pang, and Quoc Le. Bignas: Scaling up neural architecture search with big single-stage models. arXiv, 2020. [53] Xiyu Yu, Tongliang Liu, Xinchao Wang, and Dacheng Tao. On compressing deep models by low rank and sparse decomposition. CVPR, 2017. [54] Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua. Lq-nets: Learned quantiza- tion for highly accurate and compact deep neural networks. ECCV, 2018. [55] Feng Zheng, Cheng Deng, and Heng Huang. Binarized neural networks for resource-efﬁcient hashing with minimizing quantization loss. IJCAI, 2019. [56] Shuchang Zhou, Zekun Ni, Xinyu Zhou, He Wen, Yuxin Wu, and Yuheng Zou. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv, 2016. [57] Chenzhuo Zhu, Song Han, Huizi Mao, and William J. Dally. Trained ternary quantization. ICLR, 2017. [58] Shilin Zhu, Xin Dong, and Hao Su. Binary ensemble neural network: More bits per network or more networks per bit? CVPR, 2018. [59] Bohan Zhuang, Chunhua Shen, Mingkui Tan, Lingqiao Liu, and Ian D. Reid. Structured binary neural networks for accurate image classiﬁcation and semantic segmentation. CVPR, 2018. [60] Li’an Zhuo, Baochang Zhang, Linlin Yang, Hanlin Chen, Qixiang Ye, David Doermann, Rongrong Ji, and Guodong Guo. Cogradient descent for bilinear optimization. CVPR, 2020. 11",
      "meta_data": {
        "arxiv_id": "2009.08695v1",
        "authors": [
          "Zhaohui Yang",
          "Yunhe Wang",
          "Kai Han",
          "Chunjing Xu",
          "Chao Xu",
          "Dacheng Tao",
          "Chang Xu"
        ],
        "published_date": "2020-09-18T09:13:26Z",
        "pdf_url": "https://arxiv.org/pdf/2009.08695v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the challenge of non-differentiable quantization functions in low-bit quantized neural networks, which leads to optimization difficulties and an accuracy gap compared to full-precision models. It proposes a novel differentiable weight searching method, called Searching for Low-Bit Weights (SLB), where discrete weights are treated as searchable variables represented by a probability distribution over a discrete value set. The probabilities are optimized during training to accurately select low-bit values. Additionally, the method introduces a gradually decreasing temperature factor and State Batch Normalization (SBN) to minimize the quantization gap between continuous training weights and discrete inference weights. The proposed method achieves state-of-the-art performance on image classification and super-resolution tasks across various bit-widths.",
        "methodology": "The core methodology involves a continuous relaxation strategy inspired by gradient-based neural architecture search. Each low-bit weight is represented as a probability distribution over its discrete value set, which is learned via an auxiliary tensor `A`. During training, a continuous state tensor `Wc` (the expectation of quantized values) is used for convolution, ensuring differentiability and accurate gradient calculation for `A`. For inference, the quantized state weights `Wq` are obtained by selecting the discrete value with the maximum probability. To address the 'quantization gap' between `Wc` and `Wq`, two optimization details are introduced: 1) A gradually decreasing temperature factor `τ` (using an exponential scheduler) during training to make the probability distribution sharper, ensuring `Wc` converges towards `Wq`. 2) State Batch Normalization (SBN) which calculates and maintains two sets of statistics during training: one for continuous state outputs (`yc` from `Wc`) and another for discrete state outputs (`yq` from `Wq`). These statistics share the same affine coefficients to bridge the mismatch between continuous and discrete weight statistics, especially when the temperature cannot be reduced to zero.",
        "experimental_setup": "The proposed SLB method was evaluated on image classification and super-resolution tasks. For image classification, experiments were conducted on the CIFAR-10 and large-scale ILSVRC2012 (ImageNet) datasets using ResNet20, VGG-Small, and ResNet18 architectures. All convolution layers except the first were quantized, and the low-bit set `V` was constructed by uniform distribution from -1 to 1. On CIFAR-10, networks were trained for 500 epochs with learning rate decay and `A` initialized by kaiming initialization. Different temperature decay schedulers (sin, linear, exp) were tested, with `exp` showing superior performance (Ts=0.01, Te=10). On ILSVRC2012, ResNet18 was trained with a learning rate of 1e-3, weight decay of 0, and Adam optimizer. For super-resolution, the VDSR model was used as a baseline, trained on 291 images (augmented with rotation/flip) and tested on the Set5 dataset, evaluating with scale factors ×2 and ×3. The performance was measured using accuracy (Top-1, Top-5) for classification and PSNR values for super-resolution. Results were compared against state-of-the-art quantization methods like BNN, XNORNet, DoReFa, DSQ, SQ, and LQ-Net.",
        "limitations": "The paper implies a limitation that the temperature factor `τ` cannot be practically decreased to an absolute minimal value (zero) during training, which would ideally eliminate the quantization gap between continuous (`Wc`) and discrete (`Wq`) weights. This practical constraint necessitates the introduction of State Batch Normalization (SBN) to further minimize the remaining quantization gap in batch normalization statistics.",
        "future_research_directions": "Not mentioned"
      }
    },
    {
      "title": "PowerQuant: Automorphism Search for Non-Uniform Quantization",
      "abstract": "Deep neural networks (DNNs) are nowadays ubiquitous in many domains such as\ncomputer vision. However, due to their high latency, the deployment of DNNs\nhinges on the development of compression techniques such as quantization which\nconsists in lowering the number of bits used to encode the weights and\nactivations. Growing concerns for privacy and security have motivated the\ndevelopment of data-free techniques, at the expanse of accuracy. In this paper,\nwe identity the uniformity of the quantization operator as a limitation of\nexisting approaches, and propose a data-free non-uniform method. More\nspecifically, we argue that to be readily usable without dedicated hardware and\nimplementation, non-uniform quantization shall not change the nature of the\nmathematical operations performed by the DNN. This leads to search among the\ncontinuous automorphisms of $(\\mathbb{R}_+^*,\\times)$, which boils down to the\npower functions defined by their exponent. To find this parameter, we propose\nto optimize the reconstruction error of each layer: in particular, we show that\nthis procedure is locally convex and admits a unique solution. At inference\ntime, we show that our approach, dubbed PowerQuant, only require simple\nmodifications in the quantized DNN activation functions. As such, with only\nnegligible overhead, it significantly outperforms existing methods in a variety\nof configurations.",
      "full_text": "Arxiv version POWER QUANT : A UTOMORPHISM SEARCH FOR NON- UNIFORM QUANTIZATION Edouard Yvinec1,2 , Arnaud Dapogny2 , Matthieu Cord1 , Kevin Bailly1,2 Sorbonne Universit´e1, CNRS, ISIR, f-75005, 4 Place Jussieu 75005 Paris, France Datakalab2, 114 boulevard Malesherbes, 75017 Paris, France ey@datakalab.com ABSTRACT Deep neural networks (DNNs) are nowadays ubiquitous in many domains such as computer vision. However, due to their high latency, the deployment of DNNs hinges on the development of compression techniques such as quantization which consists in lowering the number of bits used to encode the weights and acti- vations. Growing concerns for privacy and security have motivated the devel- opment of data-free techniques, at the expanse of accuracy. In this paper, we identity the uniformity of the quantization operator as a limitation of existing ap- proaches, and propose a data-free non-uniform method. More speciﬁcally, we argue that to be readily usable without dedicated hardware and implementation, non-uniform quantization shall not change the nature of the mathematical oper- ations performed by the DNN. This leads to search among the continuous auto- morphisms of (R∗ +,×), which boils down to the power functions deﬁned by their exponent. To ﬁnd this parameter, we propose to optimize the reconstruction error of each layer: in particular, we show that this procedure is locally convex and admits a unique solution. At inference time, we show that our approach, dubbed PowerQuant, only require simple modiﬁcations in the quantized DNN activation functions. As such, with only negligible overhead, it signiﬁcantly outperforms existing methods in a variety of conﬁgurations. 1 I NTRODUCTION Deep neural networks (DNNs) tremendously improved algorithmic solutions for a wide range of tasks. In particular, in computer vision, these achievements come at a consequent price, as DNNs deployment bares a great energetic price. Consequently, the generalization of their usage hinges on the development of compression strategies. Quantization is one of the most promising such technique, that consists in reducing the number of bits needed to encode the DNN weights and/or activations, thus limiting the cost of data processing on a computing device. Existing DNN quantization techniques, for computer vision tasks, are numerous and can be distin- guished by their constraints. One such constraint is data usage, as introduced in Nagel et al. (2019), and is based upon the importance of data privacy and security concerns. Data-free approaches such as Banner et al. (2019); Cai et al. (2020); Choukroun et al. (2019); Fang et al. (2020); Garg et al. (2021); Zhao et al. (2019); Nagel et al. (2019), exploit heuristics and weight properties in order to perform the most efﬁcient weight quantization without having access to the training data. As com- pared to data-driven methods, the aforementioned techniques are more convenient to use but usually come with higher accuracy loss at equivalent compression rates. Data-driven methods performance offer an upper bound on what can be expected from data-free approaches and in this work, we aim at further narrowing the gap between these methods. To achieve this goal, we propose to leverage a second aspect of quantization: uniformity. For sim- plicity reasons, most quantization techniques such as Nagel et al. (2019); Zhao et al. (2019); Cong et al. (2022) perform uniform quantization, i.e. they consist in mapping ﬂoating point values to an evenly spread, discrete space. However, non-uniform quantization can theoretically provide a closer ﬁt to the network weight distributions, thus better preserving the network accuracy. Previous work on non-uniform quantization either focused on the search of binary codes (Banner et al., 2019; 1 arXiv:2301.09858v1  [cs.CV]  24 Jan 2023Arxiv version W8/A8 W6/A6 W4/A4 PowerQuant SPIQ SQuant OCS DFQ 80 70 60 50 40 ImageNet top1 Accuracy Figure 1: Comparison of the proposed method to other data-free quantization schemes on DenseNet 121 pre- trained on ImageNet. The proposed method (right bin in blue) drastically improves upon the existing data-free methods especially in the challenging W4/A4 quantization. Hubara et al., 2016; Jeon et al., 2020; Wu et al., 2016; Zhang et al., 2018) or leverage logarithmic distribution (Miyashita et al., 2016; Zhou et al., 2017). However, these approaches map ﬂoating point multiplications operations to other operations that are hard to leverage on current hardware (e.g. bit-shift) as opposed to uniform quantization which maps ﬂoating point multiplications to in- teger multiplications (Gholami et al., 2021; Zhou et al., 2016). To circumvent this limitation and reach a tighter ﬁt between the quantized and original weight distributions, in this work, we propose to search for the best possible quantization operator that preserves the nature of the mathematical operations. We show that this search boils down to the space deﬁned by the continuous automor- phisms of (R∗ +,×), which is limited to power functions deﬁned by their exponent. We optimize the value of this parameter by minimizing the error introduced by quantization. This allows us to reach superior accuracy, as illustrated in Fig 1. To sum it up, our contributions are: • We search for the best quantization operator that do not change the nature of the mathemat- ical operations performed by the DNN, i.e. the automorphisms of (R∗ +,×). We show that this search can be narrowed down to ﬁnding the best exponent for power functions. • We ﬁnd the optimal exponent parameter to more closely ﬁt the original weight distribution compared with existing (e.g. uniform and logarithmic) baselines. To do so, we propose to optimize the quantization reconstruction error. We show that this problem is locally convex and admits a unique solution. • In practice, we show that the proposed approach, dubbed PowerQuant, only requires simple modiﬁcations in the quantized DNN activation functions. Furthermore, we demonstrate through extensive experimentation that our method achieves outstanding results on various and challenging benchmarks with only negligible computational overhead. 2 R ELATED WORK 2.1 Q UANTIZATION In this section, we provide a background on the current state of DNNs quantization. Notice that while certain approaches are geared towards memory footprint reduction (e.g. without quantizing inputs and activations) (Chen et al., 2015; Gong et al., 2014; Han et al., 2016; Zhou et al., 2017), in what follows, we essentially focus on methods that aim at reducing the inference time. In particular, motivated by the growing concerns for privacy and security, data-free quantization methods (Banner et al., 2019; Cai et al., 2020; Choukroun et al., 2019; Fang et al., 2020; Garg et al., 2021; Zhao et al., 2019; Nagel et al., 2019; Cong et al., 2022) are emerging and have signiﬁcantly improved over the recent years. The ﬁrst breakthrough in data-free quantization (Nagel et al., 2019) was based on two mathematical ingenuities. First, they exploited the mathematical properties of piece-wise afﬁne activation func- 2Arxiv version tions (such as e.g. ReLU based DNNs) in order to balance the per-channel weight distributions by iteratively applying scaling factors to consecutive layers. Second, they proposed a bias correction scheme that consists in updating the bias terms of the layers with the difference between the ex- pected quantized prediction and the original predictions. They achieved near full-precision accuracy in int8 quantization. Since this seminal work, two main categories of data-free quantization methods have emerged. First, data-generation based methods, such as Cai et al. (2020); Garg et al. (2021), that used samples generated by Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) as samples to ﬁne-tune the quantized model through knowledge distillation (Hinton et al., 2014). Nevertheless, these methods are time-consuming and require signiﬁcantly more computational re- sources. Other methods, such as Banner et al. (2019); Choukroun et al. (2019); Fang et al. (2020); Zhao et al. (2019); Nagel et al. (2019); Cong et al. (2022), focus on improving the quantization operator but usually achieve lower accuracy. One limitation of these approaches is that they are essentially restricted to uniform quantization, while considering non-uniform mappings between the ﬂoating point and low-bit representation might be key to superior performance. 2.2 N ON-UNIFORM QUANTIZATION Indeed, in uniform settings, continuous variables are mapped to an equally-spaced grid in the orig- inal, ﬂoating point space. Such mapping introduces an error: however, applying such uniform mapping to an a priori non-uniform weight distribution is likely to be suboptimal in the general case. To circumvent this limitation, non-uniform quantization has been introduced (Banner et al., 2019; Hubara et al., 2016; Jeon et al., 2020; Wu et al., 2016; Zhang et al., 2018; Miyashita et al., 2016; Zhou et al., 2017) and (Zhang et al., 2021a; Li et al., 2019). We distinguish two categories of non-uniform quantization approaches. First, methods that introduce a code-base and require very sophisticated implementations for actual inference beneﬁts such as Banner et al. (2019); Hubara et al. (2016); Jeon et al. (2020); Wu et al. (2016); Zhang et al. (2018). Second, methods that simply modify the quantization operator such as Miyashita et al. (2016); Zhou et al. (2017). In particular, (Zhang et al., 2021a) propose a log-quantization technique. Similarly, Li et al. (Li et al., 2019) use log quantization with basis 2. In both cases, in practice, such logarithmic quantization scheme changes the nature of the mathematical operations involved, with multiplications being replaced by bit shifts. Nevertheless, one limitation of this approach is that because the very nature of the mathematical operations is intrinsically altered, in practice, it is hard to leverage without dedicated hardware and implementation. Instead of transforming ﬂoating point multiplications in integer mul- tiplications, they change ﬂoating point multiplications into bit-shifts or even look up tables (LUTs). Some of these operations are very speciﬁc to some hardware (e.g. LUTs are thought for FPGAs) and may not be well supported on most hardware. Conversely, in this work, we propose a non-uniform quantization scheme that preserves the nature of the mathematical operations by mapping ﬂoating point multiplications to standard integer multiplications. As a result, our approach boils down to simple modiﬁcations of the computations in the quantized DNN, hence allowing higher accuracies than uniform quantization methods while leading to straightforward, ready-to-use inference speed gains. Below we describe the methodology behind the proposed approach. 3 M ETHODOLOGY Let F be a trained feed forward neural network with L layers, each comprising a weight tensor Wl. Let Qbe a quantization operator such that the quantized weights Q(Wl) are represented on b bits. The most popular such operator is the uniform one. We argue that, despite its simplicity, the choice of such a uniform operator is responsible for a signiﬁcant part of the quantization error. In fact, the weights Wl most often follow a bell-shaped distribution for which uniform quantization is ill-suited: intuitively, in such a case, we would want to quantize more precisely the small weights on the peak of the distribution. For this reason, the most popular non-uniform quantization scheme is logarithmic quantization, outputting superior performance. Practically speaking, however, it consists in replacing the quantized multiplications by bit-shift operations. As a result, these methods have limited adaptability as the increment speed is hardware dependent. To adress this problem, we look for the non-uniform quantization operators that preserve the nature of matrix multiplications. Formally, taking aside the rounding operation in quantization, we want to 3Arxiv version square root distribution uniform distribution square distribution a=0.5 a=1 a=2 Figure 2: Inﬂuence of the power parameter a on the quantized distribution for weights distributed following a Gaussian prior. In such a case, the reconstruction error is typically minimized for a <1. deﬁne the space Qof functions Qsuch that ∀Q∈Q,∃Q−1 ∈Q s.t. ∀x,y Q −1(Q(x) ∗Q(y)) =x×y (1) where ∗is the intern composition law of the quantized space and ×is the standard multiplication, and Q, Q−1 are the quantization and de-quantization operators, respectively. In the case of uniform quantization and our work ∗will be the multiplication while in other non-uniform works it often corresponds to other operations that are harder to leverage, e.g. bit-shift. Recall that, for now, we omit the rounding operator. The proposed PowerQuant method consists in the search for the best suited operator Qfor a given trained neural network and input statistics. 3.1 A UTOMORPHISMS OF (R∗ +,×) Let Qbe a transformation from R+ to R+. In this case, ∗, the intern composition law in quantized space in (1), simply denote the scalar multiplication operator ×and (1) becomes Q(x) ×Q(y) = Q(x×y) ∀x,y ∈R2 +. In order to deﬁne a de-quantization operation, we need Q−1 to be deﬁned i.e. Qis bijective. Thus, by deﬁnition, Qis a group automorphism of (R∗ +,×). Thus, quantization operators that preserve the nature of multiplications are restricted to automorphisms of (R∗ +,×). The following lemma further restricts the search to power functions. Lemma 1.The set of continuous automorphisms of(R∗ +,×) is deﬁned by the set of power functions Q= {Q: x↦→xa|a∈R}. A proof of this result can be found in Appendix A. For the sake of clarity, we will now include the rounding operation in the quantization operators. Q= { Qa : W ↦→ ⌊ (2b−1 −1)sign(W) ×|W|a max |W|a ⌋⏐⏐⏐a∈R } (2) where W is a tensor and all the operations are performed element-wise. As functions of W, the quantization operators deﬁned in equation 2 are (signed) power functions. Fig 2 illustrates the effect of the power parameter a on quantization (vertical bars). Uniform quantization and a = 1 are equivalent and correspond to a quantization invariant to the weight distribution. For a <1, the quantization is more ﬁne-grained on weight values with low absolute value and coarser on high absolute values. Conversely, fora> 1, the quantization becomes more ﬁne-grained on high absolute values. We now deﬁne the search protocol in the proposed search space Q. 3.2 A UTOMORPHISM SEARCH AS A MINIMIZATION PROBLEM We propose to use the error introduced by quantization on the weights as a proxy on the distance between the quantized and the original model. Reconstruction Error Minimization: The operator Qa is not a bijection. Thus, quantization introduces a reconstruction error summed over all the layers of the network, and deﬁned as follows: ϵ(F,a) = L∑ l=1 Wl −Q−1 a (Qa(Wl))  p (3) 4Arxiv version where ∥·∥p denotes the Lp vector norm (in practice p= 2, see appendix B) and the de-quantization operator Q−1 a is deﬁned as: Q−1 a (W) =sign(W) × ⏐⏐⏐⏐W ×max |W| 2b−1 −1 ⏐⏐⏐⏐ 1 a (4) In practice, the problem of ﬁnding the best exponent a∗= argminaϵ(F,a) in (3) is a locally convex optimization problem (Appendix C.1) which has a unique minimum (see Appendix C.2). We ﬁnd the optimal value for ausing the Nelder–Mead method (Nelder & Mead, 1965) which solves problems for which derivatives may not be known or, in our case, are almost-surely zero (due to the rounding operation). In practice, more recent solvers are not required in order to reach the optimal solution (see Appendix D). Lastly, we discuss the limitations of the proposed metric in Appendix H. 3.3 F USED DE-QUANTIZATION AND ACTIVATION FUNCTION Based on equation 2, the quantization process of the weights necessitates the storage and multipli- cation of W along with a signs tensor, which is memory and computationally intensive. For the weights, however, this can be computed once during the quantization process, inducing no overhead during inference. As for activations, we do not have to store the sign of ReLU activations as they are always positive. In this case, the power function has to be computed at inference time (see al- gorithm 2). However, it can be efﬁciently computed Kim et al. (2021), using Newton’s method to approximate continuous functions in integer-only arithmetic. This method is very efﬁcient in prac- tice as it converges in 2 steps for low bit representations (four steps for int32). Thus, PowerQuant leads to signiﬁcant accuracy gains with limited computational overhead. Conversely, for non-ReLU feed forward networks such as EfﬁcientNets (SiLU) or Image Transformers (GeLU), activations are signed. This can be tackled using asymmetric quantization which consists in the use of a zero-point. In general, asymmetric quantization allows one to have a better coverage of the quantized values support. In our case, we use asymmetric quantization to work with positive values only. Formally, for both SiLU and GeLU, the activations are analytically bounded below by CSiLU = 0.27846 and CGeLU = 0.169971 respectively. Consequently, assuming a layer with SiLU activation with input x and weights W, we have: Q−1 a (Qa(x+ CSiLU)Qa(W)) ≈((x+ CSiLU)aWa) 1 a = xW + CSiLUW (5) The bias term CSiLUW induces a very slight computation overhead which is standard in asymmetric quantization. We provide a detailed empirical evaluation of this cost in Appendix G. Using the adequate value for the bias corrector, we can generalize equation 5 to any activation functionσ. The quantization process and inference with the quantized DNN are summarized in Algorithm 1 and 2. The proposed representation is fully compatible with integer multiplication as deﬁned in Jacob et al. (2018), thus it is fully compatible with integer only inference (see appendix F for more details). Algorithm 1Weight Quantization Algorithm Require: trained neural network F with Llayers to quantize, number of bits b a←solver(min{error(F,a)}) ⊿in practice we use the Nelder–Mead method for l∈{1,...,L }do Wsign ←sign(Wl) ⊿save the sign of the scalar values in W Wl ←Wsign ×|Wl|a ⊿power transformation s←max |Wl| 2b−1−1 ⊿get quantization scale Q: Wl ↦→ ⌊Wl s ⌉ and Q−1 : W ↦→Wsign ×|W ×s| 1 a ⊿qdeﬁne Qand Q−1 end for 4 E XPERIMENTS In this section, we empirically validate our method. First, we discuss the optimization of the expo- nent parameter aof PowerQuant using the reconstruction error, showing its interest as a proxy for the quantized model accuracy from an experimental standpoint. We show that the proposed approach preserves this reconstruction error signiﬁcantly better, allowing a closer ﬁt to the original weight dis- tribution through non-uniform quantization. Second, we show through a variety of benchmarks that 5Arxiv version Algorithm 2Simulated Inference Algorithm Require: trained neural network F quantized with Llayers, input X and exponent a∗ for l∈{1,...,L }do X ←Xa∗ ⊿X is assumed positive (see equation (5)) XQ ←⌊XsX⌉ ⊿where sX is a scale in the input range O←Fl(XQ) ⊿O contains the quantized output of the layer X ← ( σ(O) sXsW )1 a∗ ⊿where σis the activation function and sW the weight scale end for return X the proposed approach signiﬁcantly outperforms state-of-the-art data-free methods, thanks to more efﬁcient power function quantization with optimized exponent. Third, we show that the proposed approach comes at a negligible cost in term of inference speed. 4.1 D ATASETS AND IMPLEMENTATION DETAILS We validate the proposed PowerQuant method on ImageNet classiﬁcation (Deng et al., 2009) (≈ 1.2M images train/50k test). In our experiments we used pre-trained MobileNets (Sandler et al., 2018), ResNets (He et al., 2016), EfﬁcientNets (Tan & Le, 2019) and DenseNets (Huang et al., 2017). We used Tensorﬂow implementations of the baseline models from ofﬁcial repositories, achieving standard baseline accuracies. The quantization process was done using Numpy library. Activations are quantized as unsigned integers and weights are quantized using a symmetric repre- sentation. We fold batch-normalization layers as in Yvinec et al. (2022a). We performed ablation study using the uniform quantization operator over weight values from Kr- ishnamoorthi (2018) and logarithmic quantization from Miyashita et al. (2016). For our compari- son with state-of-the-art approaches in data-free quantization, we implemented the more complex quantization operator from SQuant (Cong et al., 2022). To compare with strong baselines, we also implement bias correction (Nagel et al., 2019) (which measures the expected difference between the outputs of the original and quantized models and updates the biases terms to compensate for this difference) as well as input weight quantization (Nagel et al., 2019). 4.2 E XPONENT PARAMETER FITTING Fig 3 illustrates the evolution of both the accuracy of the whole DNN and the reconstruction error summed over all the layers of the network, as functions of the exponent parameter a. Our target is the highest accuracy with respect to the value of a: however, in a data-free context, we only have access to the reconstruction error. Nevertheless, as shown on Fig 3, these metrics are strongly anti- correlated. Furthermore, while the reconstruction curve is not convex it behaves well for simplex based optimization method such as the Nelder-Mead method. This is due to two properties: locally convex (Appendix C.1) and has a unique minimum (Appendix C.2). Empirically, optimal values a∗ for the exponent parameter are centered on 0.55, which approxi- mately corresponds to the ﬁrst distribution in Fig 2. Still, as shown on Table 1 we observe some variations on the best value for awhich motivates the optimization of afor each network and bit- width. Furthermore, our results provide a novel insight on the difference between pruning and quantization. In the pruning literature (Han et al., 2015; Frankle & Carbin, 2018; Molchanov et al., 2019), the baseline method consists in setting the smallest scalar weight values to zero and keeping unchanged the highest non-zero values, assuming that small weights contribute less to the network prediction. In a similar vein, logarithmic or power quantization with a > 1 roughly quantizes (almost zeroing it out) small scalar values to better preserve the precision on larger values. In prac- tice, in our case, lower reconstruction errors, and better accuracies, are achieved by setting a <1: this suggests that the assumption behind pruning can’t be straightforwardly applied to quantization, where in fact we argue that ﬁnely quantizing smaller weights is paramount to preserve the patterns learned at each layer, and the representation power of the whole network. 6Arxiv version Figure 3: Accuracy/reconstruction error relationship for ResNet and DenseNet quantized in W4/A4. Table 1: Comparison between logarithmic, uniform and the proposed quantization scheme on ResNet 50 trained for ImageNet classiﬁcation task. We report for different quantization conﬁguration (weights noted W and activations noted A) both the top1 accuracy and the reconstruction error (equation 3). Architecture Method W-bit A-bit a∗ Accuracy Reconstruction Error ResNet 50 Baseline 32 32 - 76.15 - uniform 8 8 1 76.15 1.1 ×10−4 logarithmic 8 8 - 76.12 2.0 ×10−4 PowerQuant 8 8 0.55 76.15 1.0 ×10−4 uniform 4 4 1 54.68 3.5 ×10−3 logarithmic 4 4 - 57.07 2.1 ×10−3 PowerQuant 4 4 0.55 70.29 1.9 ×10−3 Another approach that puts more emphasis on the nuances between low valued weights is logarith- mic based non-uniform quantization. In Table 1 and Appendix E, we compare the proposed power method to both uniform and logarithmic approaches. By deﬁnition, the proposed power method nec- essarily outperforms the uniform method in every scenario as uniform quantization is included in the search space. For instance, in int4, the proposed method improves the accuracy by 13.22 points on ResNet 50. This improvement can also be attributed to a better input quantization of each layer, especially on ResNet 50 where the gap in the reconstruction error (over the weights) is smaller. 4.3 C OMPARISON WITH DATA-FREE QUANTIZATION METHODS In table 2, we report the performance of several data-free quantization approaches on ResNet 50. Although no real training data is involved in these methods, some approaches such as ZeroQ (Cai et al., 2020), DSG (Zhang et al., 2021b) or GDFQ (Xu et al., 2020) rely on data generation (DG) in order to calibrate parameters of the method or to apply ﬁne-tuning to preserve the accuracy through quantization. As shown in table 2, in the W8/A8 setup, the proposed PowerQuant method outper- forms other data-free solutions, fully preserving the accuracy of the ﬂoating point model. The gap is even wider on the more challenging low bit quantization W4/A4 setup, where the PowerQuant im- proves the accuracy by1.93 points over SQuant (Cong et al., 2022) and by14.88 points over GDFQ. This shows the effectiveness of the method on ResNet 50. We provide more results on DenseNet (Huang et al., 2017), MobileNet (Sandler et al., 2018), Efﬁcient Net (Tan & Le, 2019) in Appendix J. These results demonstrate the versatility of the method on both large and very compact convnets. In summary, the proposed PowerQuant vastly outperforms other data-free quantization schemes. Last but not least, when compared to recent QAT methods such as OCTA V Sakr et al. (2022), PowerQuant achieves competitive results on both ResNets and MobileNets using either both static or dynamic quantization. This is remarkable since PowerQuant does not involve any ﬁne-tuning of the network. We provide more details on this benchamrk in Appendix I. In what follows, we evaluate PowerQuant on recent transformer architectures for both image and language applications. 7Arxiv version Table 2: Comparison between state-of-the-art post training quantization techniques on ResNet 50 on ImageNet. We distinguish methods relying on data (synthetic or real) or not. In addition to being fully data-free, our approach signiﬁcantly outperforms existing methods. Architecture Method Data W-bit A-bit Accuracy gap ResNet 50 Baseline - 32 32 76.15 - DFQ Nagel et al. (2019) No 8 8 75.45 -0.70 ZeroQ Cai et al. (2020) Synthetic 8 8 75.89 -0.26 DSG Zhang et al. (2021b) Synthetic 8 8 75.87 -0.28 GDFQ Xu et al. (2020) Synthetic 8 8 75.71 -0.44 SQuant Cong et al. (2022) No 8 8 76.04 -0.11 PowerQuant No 8 8 76.15 0.00 DFQ Nagel et al. (2019) No 4 4 0.10 -76.05 ZeroQ Cai et al. (2020) Synthetic 4 4 7.75 -68.40 DSG Zhang et al. (2021b) Synthetic 4 4 23.10 -53.05 GDFQ Xu et al. (2020) Synthetic 4 4 55.65 -20.50 SQuant Cong et al. (2022) No 4 4 68.60 -7.55 PowerQuant No 4 4 70.53 -5.62 Table 3: Comparison of data-free quantization methods on ViT and DeiT trained on ImageNet. model method W / A accuracy ViT baseline -/- 78.05% DFQ (ICCV 2019) 8/8 70.33% SQuant (ICLR 2022) 8/8 68.85% PSAQ (arxiv 2022) 8/8 37.36% PowerQuant 8/8 77.46% DFQ (ICCV 2019) 4/8 66.63% SQuant (ICLR 2022) 4/8 64.62% PSAQ (arxiv 2022) 4/8 25.34% PowerQuant 4/8 75.24% (a) Evaluation for ViT Base model method W / A accuracy DeiT T baseline -/- 72.21% DFQ (ICCV 2019) 8/8 71.32% SQuant (ICLR 2022) 8/8 71.11% PSAQ (arxiv 2022) 8/8 71.56% PowerQuant 8/8 72.23% DFQ (ICCV 2019) 4/8 67.71% SQuant (ICLR 2022) 4/8 67.58% PSAQ (arxiv 2022) 4/8 65.57% PowerQuant 4/8 69.77% (b) Evaluation for DeiT Tiny model method W / A accuracy DeiT S baseline -/- 79.85% DFQ (ICCV 2019) 8/8 78.76% SQuant (ICLR 2022) 8/8 78.94% PSAQ (arxiv 2022) 8/8 76.92% PowerQuant 8/8 79.33% DFQ (ICCV 2019) 4/8 76.75% SQuant (ICLR 2022) 4/8 76.61% PSAQ (arxiv 2022) 4/8 73.23% PowerQuant 4/8 78.16% (c) Evaluation for DeiT Small model method W / A accuracy DeiT B baseline -/- 81.85% DFQ (ICCV 2019) 8/8 80.72% SQuant (ICLR 2022) 8/8 80.60% PSAQ (arxiv 2022) 8/8 79.10% PowerQuant 8/8 81.26% DFQ (ICCV 2019) 4/8 79.41% SQuant (ICLR 2022) 4/8 79.21% PSAQ (arxiv 2022) 4/8 77.05% PowerQuant 4/8 80.67% (d) Evaluation for DeiT Base 4.4 E VALUATION ON TRANSFORMER ARCHITECTURES In Table 3, we quantized the weight tensors of a ViT Dosovitskiy et al. (2021) with85M parameters and baseline accuracy≈78 as well as DeiT T,S and B Touvron et al. (2021) with baseline accuracies 72.2, 79.9 and 81.8 and ≈5M, ≈22M, ≈87M parameters respectively. Similarly to ConvNets, the image transformer is better quantized using PowerQuant rather than standard uniform quantization schemes such as DFQ. Furthermore, more complex and recent data-free quantization schemes such as SQuant, tend to under-perform on the novel Transformer architectures as compared to ConvNets. This is not the case for PowerQuant which maintains its very high performance even in low bit representations. This is best illustrated on ViT where PowerQuant W4/A8 out performs both DFQ and SQuant even when they are allowed 8 bits for the weights (W8/A8) by a whopping 4.91 points. The proposed PowerQuant even outperforms methods dedicated to transformer quantization such as PSAQ Li et al. (2022) on every image transformer tested. 8Arxiv version Table 4: Complementary Benchmarks on the GLUE task quantized in W4/A8. We consider the BERT trans- former architecture. We provide the original performance (from the article) of BERT on GLUE as well as our reproduced results (baseline). task original baseline uniform log SQuant PowerQuant CoLA 49.23 47.90 45.60 45.67 46.88 47.11 SST-2 91.97 92.32 91.81 91.53 91.09 92.23 MRPC 89.47/85.29 89.32/85.41 88.24/84.49 86.54/82.69 88.78/85.24 89.26/85.34 STS-B 83.95/83.70 84.01/83.87 83.89/83.85 84.01/83.81 83.80/83.65 84.01/83.87 QQP 88.40/84.31 90.77/84.65 89.56/83.65 90.30/84.04 90.34/84.32 90.61/84.45 MNLI 80.61/81.08 80.54/80.71 78.96/79.13 78.96/79.71 78.35/79.56 79.02/80.28 QNLI 87.46 91.47 89.36 89.52 90.08 90.23 RTE 61.73 61.82 60.96 60.46 60.21 61.45 WNLI 45.07 43.76 39.06 42.19 42.56 42.72 Table 5: ACE cost of the overhead computations introduced by PowerQuant. Architecture overhead cost accuracy in W6/A6 ResNet 50 0.63% 75.07 DenseNet 121 0.97% 72.71 MobileNet V2 0.57% 52.20 EfﬁcientNet B0 0.80% 58.24 We further compare the proposed power quantization, in W4/A8, on natural language processing (NLP) tasks and report results in Table 4. We evaluate a BERT model (Devlin et al., 2018) on GLUE (Wang et al., 2018) and report both the original (reference) and our reproduced (baseline) results. We compare the three quantization processes: uniform, logarithmic and PowerQuant. Similarly to com- puter vision tasks, the power quantization outperforms the other methods in every instances which further conﬁrms its ability to generalize well to transformers and NLP tasks. In what follows, we show experimentally that our approach induces very negligible overhead at inference time, making this accuracy enhancement virtually free from a computational standpoint. 4.5 I NFERENCE COST AND PROCESSING TIME The ACE metrics was recently introduced in Zhang et al. (2022) to provide a hardware-agnostic measurement of the overhead computation cost in quantized neural networks. In Table 5, we evaluate the cost in the inference graph due to the change in the activation function. We observe very similar results to Table 17. The proposed changes are negligible in terms of computational cost on all tested networks. Furthermore, DenseNet has the highest cost due to its very dense connectivity. On the other hand, using this metric it seems that the overhead cost due to the zero-point technique from section 3.3 for EfﬁcientNet has no signiﬁcant impact as compared to MobileNet and ResNet. In addition, we provide a more detailed discussion on the inference and processing cost of PowerQuant on speciﬁc hardware using dedicated tools in Appendix K. 5 C ONCLUSION In this paper, we pinpointed the uniformity of the quantization as a limitation of existing data- free methods. To address this limitation, we proposed a novel data-free method for non-uniform quantization of trained neural networks for computer vision tasks, with an emphasis on not chang- ing the nature of the mathematical operations involved (e.g. matrix multiplication). This led us to search among the continuous automorphisms of(R∗ +,×), which are restricted to the power functions x→xa. We proposed an optimization of this exponent parameter based upon the reconstruction er- ror between the original ﬂoating point weights and the quantized ones. We show that this procedure is locally convex and admits a unique solution. At inference time, the proposed approach, dubbed PowerQuant, involves only very simple modiﬁcations in the quantized DNN activation functions. We empirically demonstrate that PowerQuant allows a closer ﬁt to the original weight distributions compared with uniform or logarithmic baselines, and signiﬁcantly outperforms existing methods in 9Arxiv version a variety of benchmarks with only negligible computational overhead at inference time. In addi- tion, we also discussed and addressed some of the limitations in terms of optimization (per-layer or global) and generalization (non-ReLU networks). Future work involves the search of a better proxy error as compared with the proposed weight re- construction error as well as the extension of the search space to other internal composition law of R+ that are suited for efﬁcient calculus and inference. ACKNOWLEDGMENTS This work has been supported by the french National Association for Research and Technology (ANRT), the company Datakalab (CIFRE convention C20/1396) and by the French National Agency (ANR) (FacIL, project ANR-17-CE33-0002). This work was granted access to the HPC resources of IDRIS under the allocation 2022-AD011013384 made by GENCI. REFERENCES Ron Banner, Yury Nahshan, and Daniel Soudry. Post training 4-bit quantization of convolutional networks for rapid-deployment. NeurIPS, pp. 7950–7958, 2019. Yash Bhalgat, Jinwon Lee, Markus Nagel, Tijmen Blankevoort, and Nojun Kwak. Lsq+: Improving low-bit quantization through learnable offsets and better initialization. CVPR Workshops, pp. 696–697, 2020. Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Zeroq: A novel zero shot quantization framework. CVPR, pp. 13169–13178, 2020. Wenlin Chen, James Wilson, et al. Compressing neural networks with the hashing trick. ICML, pp. 2285–2294, 2015. Vladimir Chikin and Vladimir Kryzhanovskiy. Channel balancing for accurate quantization of wino- grad convolutions. In CVPR, pp. 12507–12516, 2022. Yoni Choukroun, Eli Kravchik, Fan Yang, and Pavel Kisilev. Low-bit quantization of neural net- works for efﬁcient inference. ICCV Workshops, pp. 3009–3018, 2019. Guo Cong, Qiu Yuxian, Leng Jingwen, Gao Xiaotian, Zhang Chen, Liu Yunxin, Yang Fan, Zhu Yuhao, and Guo Minyi. Squant: On-the-ﬂy data-free quantization via diagonal hessian approxi- mation. ICLR, 2022. Andrew R Conn, Katya Scheinberg, and Ph L Toint. On the convergence of derivative-free methods for unconstrained optimization. Approximation theory and optimization: tributes to MJD Powell, pp. 83–108, 1997. J. Deng, W. Dong, et al. ImageNet: A Large-Scale Hierarchical Image Database. CVPR, 2009. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021. Jun Fang, Ali Shaﬁee, Hamzah Abdel-Aziz, David Thorsley, Georgios Georgiadis, and Joseph H Hassoun. Post-training piecewise linear quantization for deep neural networks. ECCV, pp. 69– 86, 2020. Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. ICLR, 2018. Sahaj Garg, Anirudh Jain, Joe Lou, and Mitchell Nahmias. Confounding tradeoffs for neural net- work quantization. arXiv preprint arXiv:2102.06366, 2021. 10Arxiv version Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. A survey of quantization methods for efﬁcient neural network inference. arXiv preprint arXiv:2103.13630, 2021. Yunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev. Compressing deep convolutional net- works using vector quantization. arXiv preprint arXiv:1412.6115, 2014. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. NeurIPS, 27, 2014. Zbigniew Hajduk. High accuracy fpga activation function implementation for neural networks. Neurocomputing, 247:59–61, 2017. Kun Han, Yuxuan Wang, DeLiang Wang, William S Woods, Ivo Merks, and Tao Zhang. Learning spectral mapping for speech dereverberation and denoising. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 23(6):982–992, 2015. Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. ICLR, 2016. Kaiming He, Xiangyu Zhang, et al. Deep residual learning for image recognition. CVPR, pp. 770–778, 2016. Horst Herrlich. Axiom of choice, volume 1876. Springer, 2006. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. NeurIPS, 2014. Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. CVPR, pp. 4700–4708, 2017. Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks. NeurIPS, 29, 2016. Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efﬁcient integer-arithmetic-only inference. CVPR, pp. 2704–2713, 2018. Yongkweon Jeon, Baeseong Park, Se Jung Kwon, Byeongwook Kim, Jeongin Yun, and Dongsoo Lee. Biqgemm: matrix multiplication with lookup table for binary-coding-based quantized dnns. SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 1–14, 2020. Yongkweon Jeon, Chungman Lee, Eulrang Cho, and Yeonju Ro. Mr. biq: Post-training non-uniform quantization based on minimizing the reconstruction error. In CVPR, pp. 12329–12338, 2022. Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. I-bert: Integer- only bert quantization. In International conference on machine learning, pp. 5506–5518. PMLR, 2021. Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efﬁcient inference: A whitepaper. arXiv preprint arXiv:1806.08342, 2018. Maximilian Lam, Michael Mitzenmacher, Vijay Janapa Reddi, Gu-Yeon Wei, and David Brooks. Tabula: Efﬁciently computing nonlinear activation functions for secure neural network inference. arXiv preprint arXiv:2203.02833, 2022. Yuhang Li, Xin Dong, and Wei Wang. Additive powers-of-two quantization: An efﬁcient non- uniform discretization for neural networks. arXiv preprint arXiv:1909.13144, 2019. Zhikai Li, Liping Ma, Mengjuan Chen, Junrui Xiao, and Qingyi Gu. Patch similarity aware data-free quantization for vision transformers. arXiv preprint arXiv:2203.02250, 2022. Daisuke Miyashita, Edward H Lee, and Boris Murmann. Convolutional neural networks using logarithmic data representation. arXiv preprint arXiv:1603.01025, 2016. 11Arxiv version Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. Importance estimation for neural network pruning. CVPR, pp. 11264–11272, 2019. Markus Nagel, Mart van Baalen, et al. Data-free quantization through weight equalization and bias correction. ICCV, pp. 1325–1334, 2019. John A Nelder and Roger Mead. A simplex method for function minimization. The computer journal, 7(4):308–313, 1965. Michael JD Powell. An efﬁcient method for ﬁnding the minimum of a function of several variables without calculating derivatives. The computer journal, 7(2):155–162, 1964. Charbel Sakr, Steve Dai, Rangha Venkatesan, Brian Zimmer, William Dally, and Brucek Khailany. Optimal clipping and magnitude-aware differentiation for improved quantization-aware training. In ICML, pp. 19123–19138. PMLR, 2022. Mark Sandler, Andrew Howard, et al. Mobilenetv2: Inverted residuals and linear bottlenecks.CVPR, pp. 4510–4520, 2018. Mingxing Tan and Quoc V Le. Efﬁcientnet: Rethinking model scaling for convolutional neural networks. ICML, pp. 6105–6114, 2019. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv´e J ´egou. Training data-efﬁcient image transformers & distillation through attention. In International Conference on Machine Learning, pp. 10347–10357. PMLR, 2021. Mart van Baalen, Brian Kahne, Eric Mahurin, Andrey Kuzmin, Andrii Skliar, Markus Nagel, and Tijmen Blankevoort. Simulated quantization, real power savings. InCVPR, pp. 2757–2761, 2022. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceed- ings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 353–355, Brussels, Belgium, November 2018. Association for Computational Lin- guistics. doi: 10.18653/v1/W18-5446. URL https://aclanthology.org/W18-5446. Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, and Jian Cheng. Quantized convolutional neural networks for mobile devices. CVPR, pp. 4820–4828, 2016. Shoukai Xu, Haokun Li, Bohan Zhuang, Jing Liu, Jiezhang Cao, Chuangrun Liang, and Mingkui Tan. Generative low-bitwidth data free quantization. ECCV, pp. 1–17, 2020. Edouard Yvinec, Arnaud Dapogny, and Kevin Bailly. To fold or not to fold: a necessary and sufﬁcient condition on batch-normalization layers folding. IJCAI, 2022a. Edouard Yvinec, Arnaud Dapogny, Matthieu Cord, and Kevin Bailly. Spiq: Data-free per-channel static input quantization. arXiv preprint arXiv:2203.14642, 2022b. Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua. Lq-nets: Learned quantization for highly accurate and compact deep neural networks. ECCV, pp. 365–382, 2018. Sai Qian Zhang, Bradley McDanel, HT Kung, and Xin Dong. Training for multi-resolution inference using reusable quantization terms. In Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, pp. 845–860, 2021a. Xiangguo Zhang, Haotong Qin, Yifu Ding, Ruihao Gong, Qinghua Yan, Renshuai Tao, Yuhang Li, Fengwei Yu, and Xianglong Liu. Diversifying sample generation for accurate data-free quantiza- tion. CVPR, pp. 15658–15667, 2021b. Yichi Zhang, Zhiru Zhang, and Lukasz Lew. Pokebnn: A binary pursuit of lightweight accuracy. In CVPR, pp. 12475–12485, 2022. Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, and Zhiru Zhang. Improving neural network quantization without retraining using outlier channel splitting. ICML, pp. 7543–7552, 2019. 12Arxiv version Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen. Incremental network quantiza- tion: Towards lossless cnns with low-precision weights. ICLR, 2017. Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Train- ing low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016. 13Arxiv version A P ROOF OF LEMMA 1 In this section, we provide a simple proof for lemma 1 as well as a discussion on the continuity hypothesis. Proof. We have that ∀x ∈R+,Q(x) ×Q(0) =Q(0) and ∀x ∈R+,Q(x) ×Q(1) =Q(x) which induces that Qis either the constant 1 or Q(0) = 0and Q(1) = 1. Because Qis an automorphism we can eliminate the ﬁrst option. Now, we will demonstrate that Qis necessarily a power function. Let nbe an integer, then Q(xn) =Q(x) ×Q(xn−1) =Q(x)2 ×Q(xn−2) =··· = Q(x)n. (6) Similarly, for fractions, we get Q(x 1 n ) ×···× Q(x 1 n ) =Q(x) ⇔Q(x 1 n ) =Q(x) 1 n . Assuming Q is continuous, we deduce that for any rational a∈R, we have Q(xa) =Q(x)a (7) In order to verify that the solution is limited to power functions, we use a reductio ad absurdum. Assume Q is not a power function. Therefore, there exists (x,y) ∈ R2 + and a ∈ R such that Q(x) ̸= xa and Q(y) =ya. By deﬁnition of the logarithm, there exists bsuch that xb = y. We get the following contradiction, from (7), { Q(xba ) =Q(ya) =ya Q(xba ) =Q(xab) =Q(xa)b ̸= ( xab = ya) (8) Consequently, the suited functions Qare limited to power functions i.e. Q= {Q : x ↦→xa|a ∈ R}. We would also like to put the emphasis on the fact that there are other Automorphisms of (R,×). However, the construction of such automorphisms require the axiom of choice Herrlich (2006). Such automorphisms are not applicable in our case which is why the key constraint is being an automorphism rather than the continuous property. B N ORM SELECTION In the minimization objective, we need to select a norm to apply. In this section, we provide the- oretical arguments in favor of the l2 vector norm. Let F be a feed forward neural network with L layers to quantize, each deﬁned by a set of weights Wl = (wl)i,j ∈Rnl×ml and bias bl ∈Rnl . We note (λ(i) l )i the eigenvalues associated with Wl. We want to study the distance d(F,Fa) between the predictive function F and its quantized version Fa deﬁned as d(F,Fa) = max x∈D ∥F(x) −Fa(x)∥p (9) where Dis the domain of F. We prove that minimizing the reconstruction error with respect to ais equivalent to minimizing d(F,Fa) with respect to a. Assume L = 1for the sake of simplicity and we drop the notation l. With the proposed PowerQuant method, we minimize the vector norm ∥W −Q−1 a (Qa(W))∥p p = ∑ i<=n max j<=m |wi,j −Q−1 a (Qa(wi,j))|p (10) For p= 2, the euclidean norm is equal to the spectral norm, thus minimizing∥W−Q−1 a (Qa(W))∥2 is equivalent to minimizing d(F,Fa) for L = 1. However, we know that minimizing for another value of pmay result in a different optimal solution and therefore not necessarily minimized(F,Fa). In the context of data-free quantization, we want to avoid uncontrollable changes on F, which is why we recommend the use of p= 2. 14Arxiv version C M ATHEMATICAL PROPERTIES C.1 L OCAL CONVEXITY We prove that the minimization problem deﬁned in equation 3 is locally convex around the solution a∗. Formally we prove that x↦→ x−Q−1 a (Qa(x))  p (11) is locally convex around a∗deﬁned as arg mina x−Q−1 a (Qa(x))  p. Lemma 2. The minimization problem deﬁned as arg min a {x−Q−1 a (Qa(x))  p } (12) is locally convex around any solution a∗. Proof. We recall that ∂xa ∂a = xalog(x). The function x−Q−1 a (Qa(x)) is differentiable. We assume x∈R, then we can simplify the sign functions (assumexpositive without loss of generality) and note y= max|x|, then ∂Q−1 a (Qa(x)) ∂a = ∂ ⏐⏐⏐ ⌊ (2b−1 −1)xa ya ⌋ ya 2b−1−1 ⏐⏐⏐ 1 a ∂a . (13) This simpliﬁes to ∂Q−1 a (Qa(x)) ∂a = y ∂ ( ⌊B(x y ) a ⌋ B )1 a ∂a , (14) with B = 2b−1 −1. By using the standard differentiation rules, we know that the rounding operator has a zero derivative a.e.. Consequently we get, ∂Q−1 a (Qa(x)) ∂a = −a2y   ⌊ B ( x y )a⌋ B   1 a log   ⌊ B ( x y )a⌋ B  . (15) Now we can compute the second derivative of Q−1 a (Qa(x)), ∂2Q−1 a (Qa(x)) ∂a2 = a4y   ⌊ B ( x y )a⌋ B   1 a log2   ⌊ B ( x y )a⌋ B  . (16) From this expression, we derive the second derivative, using the property(f◦g)′′= f′′◦g×g′2 + f′◦g×g′′and the derivatives |·| 1 p ′ = x|x| 1 p −2 p and |·| 1 p ′′ = 1−p p2 |x| 1 p x2 , then for any xi ∈x ∂2 ⏐⏐xi −Q−1 a (Qa(xi)) ⏐⏐ ∂a2 = 1 −p p2 |xi −Q−1 a (Qa(xi)| 1 p (xi −Q−1a (Qa(xi))2 (∂Q−1 a (Qa(x)) ∂a )2 + (xi −Q−1 a (Qa(xi))|xi −Q−1 a (Qa(xi)| 1 p −2 p ∂2Q−1 a (Qa(x)) ∂a2 (17) We now note the ﬁrst term in the previous additionT1 = 1−p p2 |xi−Q−1 a (Qa(xi)| 1 p (xi−Q−1 a (Qa(xi))2 ( ∂Q−1 a (Qa(x)) ∂a )2 and the second term as a product ofT2 = (xi−Q−1 a (Qa(xi))|xi−Q−1 a (Qa(xi)| 1 p −2 p times T3 = ∂2Q−1 a (Qa(x)) ∂a2 . We know that T1 > 0 and T3 > 0, consequently, and T2 is continuous in a. At a∗ the terms with |xi −Q−1 a (Qa(xi)) |are negligible in comparison with ∂2Q−1 a (Qa(x)) ∂a2 and ( ∂Q−1 a (Qa(x)) ∂a )2 . Consequently, there exists an open set around a∗where T1 >|T2|T3, and ∂2|xi−Q−1 a (Qa(xi))| ∂a2 >0. This concludes the proof. 15Arxiv version Table 6: Minimization of the reconstruction error on a MobileNet V2 for W6/A6 quantization with different solvers. Solver a∗ reconstruction error accuracy Nelder-Mead 0.750 1.12 64.248 Powell (Powell, 1964) 0.744 1.10 64.104 COBYLA (Conn et al., 1997) 0.752 1.11 64.364 C.2 U NIQUENESS OF THE SOLUTION In this section we provide the elements of proof on the uniqueness of the solution of the minimization of the quantization reconstruction error. Lemma 3. The minimization problem over x∈RN deﬁned as arg min a {x−Q−1 a (Qa(x))  p } (18) has almost surely a unique global minimum a∗. Proof. We assume that x can not be exactly quantized, i.e. mina {x−Q−1 a (Qa(x))  p } > 0 which is true almost everywhere. We use a reductio ad absurdum and assume that there ex- ist two optimal solutions a1 and a2 to the optimization problem. We expand the expressionx−Q−1 a (Qa(x))  p and get x−Q−1 a (Qa(x))  p = x− ⏐⏐⏐⏐ ⌊ (2b−1 −1)sign(x) ×|x|a max |x|a ⌋max |x|a 2b−1 −1 ⏐⏐⏐⏐ 1 a sign(x) .p (19) We note the rounding term Ra and get x−Q−1 a (Qa(x))  p = x− ⏐⏐⏐⏐Ra max |x|a 2b−1 −1 ⏐⏐⏐⏐ 1 a sign(x)  p . (20) Assume Ra1 = Ra2 = R, the minimization problem arg mina x− ⏐⏐⏐Rmax |x|a 2b−1−1 ⏐⏐⏐ 1 a sign(x)  p is convex and has a unique solution, thus a1 = a2. Now assume Ra1 ̸= Ra2 . Let’s denoteD(R) the domain of power valuesaover which we have ⌊ (2b−1 −1)sign(x)×|x|a max |x|a ⌋ = R. If there is a value aoutside of D(Ra1 ) ∪D(Ra2 ) such that R′has each of its coordinate strictly between the coordinates of Ra1 and Ra2 , then, without loss of generality, assume that at least half of the coordinates of Ra1 are further away from the corresponding coordinates of xthan one quan- tization step. This implies that there exists a value a′in D(R′) such that x−Q−1 a′ (Qa′(x))  p <x−Q−1 a1 (Qa1 (x))  p. which goes against our hypothesis. Thus, there are up to N possible values for Rthat minimize the problem which happens iff xsatisﬁes at least one coordinate can be either ceiled or ﬂoored by the rounding. The set deﬁned by this condition has a zero measure. D S OLVER FOR MINIMIZATION In the main article we state that we can use Nelder-Mead (Nelder & Mead, 1965) solver to ﬁnd the optimal a∗. We tested several other solvers and report the results in Table 6. The empirical results show that basically any popular solver can be used, and that the Nelder-Mead solver is sufﬁcient for the minimization problem. E C OMPARISON BETWEEN LOG, NAIVE AND POWER QUANTIZATION COMPLEMENTARY RESULTS To complement the results provided in the main paper on ResNet 50, we list in Table 7 more quan- tization setups on ResNet 50 as well as DenseNet 121. To put it in a nutshell, The proposed power 16Arxiv version Table 7: Comparison between logarithmic, uniform and the proposed quantization scheme on ResNet 50 and DenseNet 121 trained for ImageNet classiﬁcation task. We report for different quantization conﬁguration (weights noted W and activations noted A) both the top1 accuracy and the reconstruction error (equation 3). Architecture Method W-bit A-bit a∗ Accuracy Reconstruction Error ResNet 50 Baseline 32 32 - 76.15 - uniform 8 8 1 76.15 1.1 ×10−4 logarithmic 8 8 - 76.12 2.0 ×10−4 PowerQuant 8 8 0.55 76.15 1.0 ×10−4 uniform 6 6 1 75.07 8.0 ×10−4 logarithmic 6 6 - 75.37 4.6 ×10−4 power (ours) 6 6 0.50 75.95 4.3 ×10−4 uniform 4 4 1 54.68 3.5 ×10−3 logarithmic 4 4 - 57.07 2.1 ×10−3 PowerQuant 4 4 0.55 70.29 1.9 ×10−3 DenseNet 121 Baseline 32 32 - 75.00 - uniform 8 8 1 75.00 2.8 ×10−4 logarithmic 8 8 - 74.91 2.5 ×10−4 PowerQuant 8 8 0.60 75.00 2.2 ×10−4 uniform 6 6 1 74.47 1.1 ×10−3 logarithmic 6 6 - 72.71 1.0 ×10−3 power (ours) 6 6 0.50 74.84 0.7 ×10−3 uniform 4 4 1 54.83 4.7 ×10−3 logarithmic 4 4 - 5.28 4.8 ×10−3 PowerQuant 4 4 0.55 68.04 3.1 ×10−3 quantization systematically achieves signiﬁcantly higher accuracy and lower reconstruction error than the logarithmic and uniform quantization schemes. On a side note, the poor performance of the logarithmic approach on DenseNet 121 can be attributed to the skewness of the weight distributions. Formally, ResNet 50 and DenseNet 121 weight values show similar average standard deviations across layers (0.0246 and 0.0264 respectively) as well as similar kurtosis (6.905 and 6.870 respec- tively). However their skewness are signiﬁcantly different: 0.238 for ResNet 50 and more than twice as much for DenseNet 121, with 0.489. The logarithmic quantization, that focuses on very small value is very sensible to asymmetry which explains the poor performance on DenseNet 121. In contrast, the proposed method offers a robust performance in all situations. F H OW TO PERFORM MATRIX MULTIPLICATION WITH POWER QUANT The proposed PowerQuant method preserves the multiplication operations, i.e. a multiplication in the ﬂoating point space remains a multiplication in the quantized space (integers). This allows one to leverage current implementations of uniform quantization available on most hardware Gholami et al. (2021); Zhou et al. (2016). However, while PowerQuant preserves multiplications it doesn’t preserve additions which are signiﬁcantly less costly than multiplications. Consequently, in order to infer under the PowerQuant transformation, instead of accumulating the quantized products, as done in standard quantization Jacob et al. (2018), one need to accumulate the powers of said products. Formally, let’s consider two quantized weightsw1,w2 and their respective quantized inputs x1,x2. The standard accumulation would be performed as followsw1x1+w2x2. In the case of PowerQuant, this would be done as(w1x1) 1 a +(w2x2) 1 a . Previous studies on quantization have demonstrated that such power functions can be computed with very high ﬁdelity at almost no latency cost Kim et al. (2021). G O VERHEAD COST OF ZERO -POINTS IN ACTIVATION QUANTIZATION The overhead cost introduced in equation 5 is well known in general in quantization as it arises from asymmetric quantization. Nonetheless, we share here (as well as in the article) some empirical values. 17Arxiv version Table 8: Overhead induced by asymmetric quantization Architecture parameters overhead run-time overhead (CPU intel-m3) ResNet50 0.25% 4.35% EfﬁcientNet 0.20% 3.38% ViT b16 0.73% 5.14% Table 9: Comparison between the per-layer and global method of power parameter a ﬁtting on a ResNet 5 `a trained for ImageNet classiﬁcation task. Architecture Method W-bit A-bit Accuracy Reconstruction Error ResNet 50 Baseline 32 32 76.15 - per-layer 8 8 76.14 0.8 ×10−4 global 8 8 76.15 1.0 ×10−4 per-layer 4 4 64,19 1.7 ×10−3 global 4 4 70.29 1.9 ×10−3 These are empirical results from our own implementation. We include ResNet50 as it can also be quantized using asymmetric quantization although in our research, we only applied asymmetric quantization to SilU and GeLU based architectures. We included these results in the appendix of the revised article. It is worth noting that according to LSQ+ Bhalgat et al. (2020), asymmetric quantization can be achieved at virtually not run-time cost. H L IMITATIONS OF THE RECONSTRUCTION ERROR METRIC In the proposed PowerQuant method, we ﬁt the parameter abased on the reconstruction error over all the weights, i.e. over all layers in the whole network. Then, we perform per-channel quantization layer by layer independently. However, if the ﬁnal objective is to minimize the reconstruction error from equation (3), a more efﬁcient approach would consist in ﬁtting the parameter aseparately for each layer. We note a∗ l such that for every layer lwe have a∗ l = arg min a {Wl −Q−1 a (Qa(Wl))  p } (21) Then the network (F,(al)∗) quantized with a per-layer ﬁt of the power parameter will satisfy L∑ l=1 Wl −Q−1 al (Qal (Wl))  p < L∑ l=1 Wl −Q−1 a (Qa(Wl))  p (22) if and only if their exists at least one lsuch that al ̸= a. Consequently, if the reconstruction error was a perfect estimate of the resulting accuracy, the per-layer strategy would offer an even higher accuracy than the proposed PowerQuant method. Unfortunately, the empirical evidence, in table 9, shows that the proposed PowerQuant method achieves better results in every benchmark. This observation demonstrates the limits of the measure of the reconstruction error. We explain this phe- nomenon by the importance of inputs and activations quantization. This can be seen as some form of overﬁtting the parameters al on the weights which leads to poor performance on the activation quantization and prediction. In the general sens, this highlights the limitations of the reconstruction error as a proxy for maximizing the accuracy. Previous results can be interpreted in a similar way. For instance, in SQuant Cong et al. (2022) the author claim that it is better to minimize the abso- lute sum of errors rather than the sum of absolute errors and achieve good performance in data-free quantization. I I MPROVEMENT WITH RESPECT TO QAT In the introduction, we argued that data-driven quantization schemes performance deﬁne an upper- bound on data-free performance. Our goal was to narrow the resulting gap between these methods. In Table 10, we report the evolution in the gap between data-free and data-driven quantization tech- niques. These empirical results validate the signiﬁcant improvement of the proposed method at narrowing the gap between data-free and data-driven quantization methods by 26.66% to 29.74%. 18Arxiv version Table 10: Performance Gap as compared to Data-driven techniques on ResNet 50 quantization in W4/A4. The relative gap improvement to the state-of-the-art SQuant [6], is measured as gs−gp gs with gs = ∗−SQuant ∗ and gp = ∗−PowerQuant ∗ where ∗ is the performance of a data-driven method data-driven method SQuant PowerQuant relative gap OCTA V Sakr et al. (2022) (ICML) 8,72% 6,15% +29,47% SQ van Baalen et al. (2022) (CVPR) 8,64% 6,07% +29,74% WinogradQ Chikin & Kryzhanovskiy (2022) (CVPR) 9,55% 7,00% +26,66% Mr BiQ Jeon et al. (2022) (CVPR) 8,74% 6,17% +29,38% Table 11: Performance gap between data-free PowerQuant and short-retraining OCTA V Sakr et al. (2022). method architecture quantization accuracy PowerQuant ResNet 50 W4/A4 70.53 OCTA V ResNet 50 W4/A4 75.84 PowerQuant MobileNet V2 W4/A4 45.84 OCTA V MobileNet V2 W4/A4 0.66 Table 12: Performance gap between PowerQuant and OCTA V Sakr et al. (2022) (using an additional short retraining), both using dynamic range estimation. method architecture quantization accuracy PowerQuant ResNet 50 W4/A4 76.02 OCTA V ResNet 50 W4/A4 76.46 PowerQuant MobileNet V2 W4/A4 71.65 OCTA V MobileNet V2 W4/A4 71.23 In order to complete our comparison to QAT methods, we considered the short-re-training (30 epochs) regime from OCTA V in Table 11. We can draw two observations from this comparison. First, on ResNet 50, OCTA V achieves remarkable results by reach near full-precision accuracy. Still the proposed method does not fall too far back with only 5.31 points lower accuracy while being data-free. Second, on very small models such as MobileNet V2, using a strong quantization oper- ator rather than a short re-training leads to a huge accuracy improvement as PowerQuant achieves 45.18 points higher accuracy. This is also the ﬁnding of the author in OCTA V , as they conclude that models such as MobileNet tend to be very challenging to quantize using static quantization and short re-training. In Table 12, we draw a comparison between the proposed PowerQuant and the QAT method OCTA V Sakr et al. (2022), both using dynamic quantization (i.e. estimating the ranges of the activations on- the-ﬂy depending on the input). As expected, the use of dynamic ranges has a considerable inﬂuence on the performance of both quantization methods. As can be observed the QAT method OCTA V achieved very impressive results and even outperforming the full-precision model on ResNet 50. Nevertheless, it is on MobileNet that the inﬂuence of dynamic ranges is the most impressive. For OCTA V , we observe a boost of almost 71 points going from almost random predictions to near exact full-precision accuracy. It is to be noted that PowerQuant does not fall shy in front of these perfor- mances, as using static quantization we still manage to preserve some of the predictive capability of the model. Furthermore, using dynamic quantization, Powerquant achieves similar accuracies than OCTA V while not involving any ﬁne-tuning, contrary to OCTA V . All in all, we can conclude that the proposed data-free method manages to hold close results to a state-of-the-art QAT method in some context. An interesting future work could be the extension of PowerQuant as a QAT method and possibly learning the power parameter athat we use in our quantization operator. J C OMPARISON TO STATE-OF-THE-ART DATA-FREE QUANTIZATION ON OTHER CONV NETS In addition to our evaluation on ResNet, we propose some complementary results on DenseNet in Table 13 as well as the challenging and compact architectures MobileNet and EfﬁcientNet in Table 19Arxiv version Table 13: Comparison between state-of-the-art post-training quantization techniques on DenseNet 121 on Im- ageNet. We distinguish methods relying on data (synthetic or real) or not. In addition to being fully data-free. our approach signiﬁcantly outperforms existing methods. Architecture Method Data W-bit A-bit Accuracy gap DenseNet 121 Baseline - 32 32 75.00 - DFQ Nagel et al. (2019) No 8 8 74.75 -0.25 SQuant Cong et al. (2022) No 8 8 74.70 -0.30 OMSE Choukroun et al. (2019) Real 8 8 74.97 -0.03 SPIQ Yvinec et al. (2022b) No 8 8 75.00 -0.00 PowerQuant No 8 8 75.00 -0.00 DFQ Nagel et al. (2019) No 4 4 0.10 -74.90 SQuant Cong et al. (2022) No 4 4 47.14 -27.86 SPIQ Yvinec et al. (2022b) No 4 4 51.83 -23.17 OMSE Choukroun et al. (2019) Real 4 4 57.07 -17.93 PowerQuant No 4 4 69.37 -5.63 Table 14: Complementary Benchmarks on ImageNet Architecture Method Data W-bit A-bit Accuracy gap MobileNet V2 Baseline - 32 32 71.80 - DFQ (ICCV 2019) No 8 8 70.92 -0.88 SQuant (ICLR 2022) No 8 8 71.68 -0.12 SPIQ (W ACV 2023) No 8 8 71.79 -0.01 PowerQuant No 8 8 71.81 +0.01 DFQ (ICCV 2019) No 4 4 27.1 -44.70 SQuant (ICLR 2022) No 4 4 28.21 -43.59 SPIQ (W ACV 2023) No 4 4 31.28 -40.52 PowerQuant No 4 4 45.84 25.96 EfﬁcientNet B0 Baseline - 32 32 77.10 - DFQ (ICCV 2019) No 8 8 76.89 -0.21 SQuant (ICLR 2022) No 8 8 76.93 -0.17 SPIQ (W ACV 2023) No 8 8 77.02 -0.08 PowerQuant No 8 8 77.05 -0.05 DFQ (ICCV 2019) No 6 6 43.08 -34.02 SQuant (ICLR 2022) No 6 6 54.51 -32.59 SPIQ (W ACV 2023) No 6 6 74.67 -2.43 PowerQuant No 6 6 75.13 -1.97 14 as well as weights only for Bert in Table 16. In table 13, we report the performance of other data-free quantization processes on DenseNet 121. The OMSE method (Choukroun et al., 2019) is a post-training quantization method that leverages validation examples during quantization, thus cannot be labelled as data-free. Yet, we include this work in our comparison as they show strong performance in terms of accuracy at a very low usage of real data. As showcased in table 13, the proposed PowerQuant method almost preserves the ﬂoating point accuracy in W8/A8 quantization. Additionally, on the challenging W4/A4 setup, our approach improves the accuracy by a remarkable 12.30 points over OMSE and 17.54 points over SQuant. This is due to the overall better efﬁciency of non-uniform quantization, that allows a theoretically closer ﬁt to the weight distributions of each DNN layer. The results on MobileNet and EfﬁcientNet from Table 14 conﬁrm our previous ﬁndings. We observe a signiﬁcant boost in performance from PowerQuant as compared to the other very competitive data-free solutions. K O VERHEAD COST DISCUSSION In this section, we provide more empirical results on the inference cost of the proposed method. Table 17 shows the inference time of DNNs quantized with our approach (which only implies modi- ﬁcations of the activation function and a bias correction-see Section 3.3). For DenseNet, ResNet and MobileNet V2, the baseline activation function is the ReLU, which is particularly fast to compute. 20Arxiv version Table 15: Complementary Benchmarks on Vision Transformers for ImageNet Architecture Method Data W-bit A-bit Accuracy gap CaiT xxs24 Baseline - 32 32 78.524 - DFQ (ICCV 2019) No 8 8 77.612 -0.912 SQuant (ICLR 2022) No 8 8 77.638 -0.886 PowerQuant No 8 8 77.718 -0.806 DFQ (ICCV 2019) No 4 8 74.192 -4.332 SQuant (ICLR 2022) No 4 8 74.224 -4.300 PowerQuant No 4 8 75.104 -3.420 CaiT xxs36 Baseline - 32 32 79.760 - DFQ (ICCV 2019) No 8 8 79.000 -0.760 SQuant (ICLR 2022) No 8 8 78.914 -0.846 PowerQuant No 8 8 79.150 -0.610 DFQ (ICCV 2019) No 4 8 76.906 -2.854 SQuant (ICLR 2022) No 4 8 76.896 -2.864 PowerQuant No 4 8 77.702 -2.058 CaiT s24 Baseline - 32 32 83.368 - DFQ (ICCV 2019) No 8 8 82.802 -0.566 SQuant (ICLR 2022) No 8 8 82.784 -0.584 PowerQuant No 8 8 82.766 -0.602 DFQ (ICCV 2019) No 4 8 81.474 -1.894 SQuant (ICLR 2022) No 4 8 81.486 -1.882 PowerQuant No 4 8 81.612 -1.756 Table 16: Complementary Benchmarks on the GLUE task. We consider the BERT transformer architecture. We provide the reference performance of BERT on GLUE as well as our reproduced results (baseline). task (reference) baseline uniform log power CoLA 49.23 47.90 46.24 46.98 47.77 SST-2 91.97 92.32 91.28 91.85 92.32 MRPC 89.47/85.29 89.32/85.41 86.49/81.37 86.65/82.86 89.32/85.41 STS-B 83.95/83.70 84.01/83.87 83.25/83.14 84.01/83.81 84.01/83.87 QQP 88.40/84.31 90.77/84.65 90.23/84.61 90.76/84.65 90.77/84.65 MNLI 80.61/81.08 80.54/80.71 79.72/79.13 79.22/79.71 80.54/80.71 QNLI 87.46 91.47 90.32 91.43 91.47 RTE 61.73 61.82 59.23 61.27 61.68 WNLI 45.07 43.76 40.85 42.80 42.85 Nevertheless, our results show that our approach leads to only increasing by1% the whole inference time on most networks. More precisely, in the case of ResNet 50, the change in activation function induces a slowdown of0.15%. The largest runtime increase is obtained on DenseNet with a 3.4% in- crease. Lastly, note that our approach is also particularly fast and efﬁcient on EfﬁcientNet B0, which uses SiLU activation, thanks to the bias correction technique introduced in Section 3.3. Overall, the proposed approach can be easily implemented and induces negligible overhead in inference on GPU. To furthermore justify the practicality of the proposed quantization process, we recall that the only practicality concern that may arise is on the activation function as the other operations are strictly identical to standard uniform quantization. According to Kim et al. (2021) efﬁcient power functions can be implemented for generic hardware as long as they support standard integer arithmetic, i.e. as long as they support uniform quantization. When it comes to Field-Programmable Gate Array (FPGA), activation functions are implemented using look-up tables (LUT) as detailed in Hajduk (2017). More precisely, they are pre-computed using Pad ´e approximation which are quotients of polynomial functions. Consequently the proposed approach would simply change the polynomial values but not the inference time as it would still rely on the same number of LUTs. In general, activation functions that are non-linear can be very effectively implemented in quantiza- tion runtime Lam et al. (2022). However these considerations are hardware agnostic. In order to circumvent this limitation and address any concerns to our best, we conducted a small study using 21Arxiv version Table 17: Inference time, in seconds, over ImageNet using batches of size 16 of several networks on a 2070 RTX GPU. We also report the accuracy for W6/A6 quantization setup. Architecture Method inference time (gap) accuracy ResNet 50 Uniform 164 75.07 Power Function 164 (+0.2) 75.95 DenseNet 121 Uniform 162 72.71 Power Function 167 (+4.8) 74.84 MobileNet V2 Uniform 85 52.20 Power Function 86 (+0.7) 64.09 EfﬁcientNet B0 Uniform 125 58.24 Power Function 127 (+2.2) 66.38 Table 18: Inference cost each component of a convolutional layer and percentage of total in terms of number of cycles on a wide range of simulated hardware using nntool from GreenWaves. operation number of cycles number of ops % of total cycles % of total ops convolution 22950 442368 85.482% 99.310% bias 2033 1024 7.573% 0.229% relu 924 1024 3.442% 0.229% power function 940 1024 3.502% 0.229% Table 19: We report the processing time in seconds (on an Intel(R) Core(TM) i9-9900K CPU) required to quantize a trained neural network such as ResNet, MobileNet, DenseNet or EfﬁcientNet. Architecture GDFQ SQuant Uniform Power MobileNet V2 7.103 134 <1 <1 ResNet 50 11.103 320 <1 1.3 the simulation tool nntool from GreenWaves, a risc-v chips manufacturer that enables to simulate inference cost of quantized neural networks on their gap unit. We tested a single convolutional layer with bias and relu activation plus our power quantization operation and reported the number of cycles and operations. These results demonstrate that even without any optimization the proposed method has a marginal computational cost on MCU inference which corroborates our previous em- pirical results. We would like to put the emphasis on the fact that this cost could be further reduced via optimizing the computation of the power function using existing methods such as Kim et al. (2021). Similarly, we measure the empirical time required to perform the proposed quantization method on several neural networks and report the results in table 19. These results show that the proposed PowerQuant method offers outstanding trade-offs in terms of compression and accuracy at virtually no cost over the processing and inference time as compared to other data-free quantiza- tion methods. For instance, SQuant is a sophisticated method that requires heavy lifting in order to efﬁciently process a neural network. On a CPU, it requires at least 100 times more time to reach a lower accuracy than the proposed method as we will showcase in our comparison to state-of-the-art quantization schemes. 22",
      "meta_data": {
        "arxiv_id": "2301.09858v1",
        "authors": [
          "Edouard Yvinec",
          "Arnaud Dapogny",
          "Matthieu Cord",
          "Kevin Bailly"
        ],
        "published_date": "2023-01-24T08:30:14Z",
        "pdf_url": "https://arxiv.org/pdf/2301.09858v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper identifies the uniformity of existing quantization operators as a limitation for data-free quantization methods. It proposes PowerQuant, a novel data-free non-uniform quantization method that searches for quantization operators preserving the nature of mathematical operations, specifically identifying continuous automorphisms of (R*+,×) as power functions (x→x^a). The method optimizes the exponent 'a' by minimizing the reconstruction error of each layer, demonstrating that this problem is locally convex and has a unique solution. PowerQuant requires only simple modifications to quantized DNN activation functions, incurring negligible computational overhead while significantly outperforming existing data-free methods across various benchmarks and architectures, including vision transformers and NLP models.",
        "methodology": "The PowerQuant method addresses the suboptimality of uniform quantization for non-uniform weight distributions and the hardware dependency of logarithmic quantization. It formulates the search for non-uniform quantization operators that preserve the nature of matrix multiplications as finding continuous automorphisms of (R*+,×), which are mathematically proven to be power functions (x→x^a). The quantization operator is defined as Qa(W) = ⌊ ( (2^b-1 -1) * sign(W) * |W|^a ) / max|W|^a ⌋. The optimal exponent 'a' is found by minimizing the reconstruction error ϵ(F,a) = ∑_l=1^L ||Wl − Qa⁻¹(Qa(Wl))||p (with p=2 norm) for all layers, using the Nelder–Mead optimization method. For inference, weights are pre-transformed. Activations for ReLU networks are computed efficiently at runtime using Newton's method for power functions. For signed activations (e.g., SiLU, GeLU), asymmetric quantization with a zero-point is used to maintain positive values, introducing a small, standard bias term. The proposed scheme ensures compatibility with integer-only inference by mapping floating-point multiplications to standard integer multiplications.",
        "experimental_setup": "The method was validated on ImageNet classification using various pre-trained models including MobileNets, ResNets, EfficientNets, and DenseNets. TensorFlow implementations of baseline models were used. Ablation studies compared PowerQuant to uniform and logarithmic quantization schemes. Performance was benchmarked against state-of-the-art data-free quantization methods such as SQuant, DFQ, ZeroQ, DSG, and GDFQ, as well as data-driven QAT methods like OCTA V, SQ, WinogradQ, and Mr BiQ. Evaluation also extended to Transformer architectures (ViT, DeiT T, S, B) for image tasks and a BERT model on the GLUE benchmark for NLP tasks. Evaluation metrics included ImageNet Top-1 accuracy, GLUE task scores (CoLA, SST-2, MRPC, STS-B, QQP, MNLI, QNLI, RTE, WNLI), reconstruction error, ACE metrics for hardware-agnostic computational overhead, and inference time on a 2070 RTX GPU and CPU processing time. The Nelder–Mead solver was primarily used for optimizing 'a', with comparisons to Powell and COBYLA solvers.",
        "limitations": "The primary limitation identified is that while the reconstruction error minimization is effective, it is not a perfect proxy for maximizing the final model accuracy. Empirically, a global optimization of the power parameter 'a' across all layers yielded better end-to-end model accuracy than a per-layer optimization, despite the latter leading to a lower reconstruction error. This suggests that per-layer optimization can lead to overfitting on weights, resulting in poorer performance on activation quantization and overall prediction. The paper also broadly mentions limitations in terms of optimization (per-layer vs. global) and generalization (non-ReLU networks) that were addressed within the proposed framework.",
        "future_research_directions": "Future work includes exploring the development of a better proxy error metric that correlates more directly with the final model accuracy, as the current weight reconstruction error, while effective, has shown some limitations. Additionally, research could extend the search space beyond continuous automorphisms of (R*+,×) to other internal composition laws of R+ that are amenable to efficient computation and inference. Another interesting direction is to extend PowerQuant into a Quantization-Aware Training (QAT) method, potentially by learning the power parameter 'a' during the training process."
      }
    },
    {
      "title": "Outlier-aware Slicing for Post-Training Quantization in Vision Transformer"
    },
    {
      "title": "Bit-Shrinking: Limiting Instantaneous Sharpness for Improving Post-Training Quantization"
    },
    {
      "title": "ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification",
      "abstract": "KV cache stores key and value states from previous tokens to avoid\nre-computation, yet it demands substantial storage space, especially for long\nsequences. Adaptive KV cache compression seeks to discern the saliency of\ntokens, preserving vital information while aggressively compressing those of\nless importance. However, previous methods of this approach exhibit significant\nperformance degradation at high compression ratios due to inaccuracies in\nidentifying salient tokens. In this paper, we present ZipCache, an accurate and\nefficient KV cache quantization method for LLMs. First, we construct a strong\nbaseline for quantizing KV cache. Through the proposed channel-separable\ntokenwise quantization scheme, the memory overhead of quantization parameters\nare substantially reduced compared to fine-grained groupwise quantization. To\nenhance the compression ratio, we propose normalized attention score as an\neffective metric for identifying salient tokens by considering the lower\ntriangle characteristics of the attention matrix. Moreover, we develop an\nefficient approximation method that decouples the saliency metric from full\nattention scores, enabling compatibility with fast attention implementations\nlike FlashAttention. Extensive experiments demonstrate that ZipCache achieves\nsuperior compression ratios, fast generation speed and minimal performance\nlosses compared with previous KV cache compression methods. For instance, when\nevaluating Mistral-7B model on GSM8k dataset, ZipCache is capable of\ncompressing the KV cache by $4.98\\times$, with only a $0.38\\%$ drop in\naccuracy. In terms of efficiency, ZipCache also showcases a $37.3\\%$ reduction\nin prefill-phase latency, a $56.9\\%$ reduction in decoding-phase latency, and a\n$19.8\\%$ reduction in GPU memory usage when evaluating LLaMA3-8B model with a\ninput length of $4096$.",
      "full_text": "ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification Yefei He1 Luoming Zhang1 Weijia Wu2 Jing Liu3 Hong Zhou1† Bohan Zhuang1,3† 1Zhejiang University, China 2National University of Singapore, Singapore 3ZIP Lab, Monash University, Australia Abstract KV cache stores key and value states from previous tokens to avoid re-computation, yet it demands substantial storage space, especially for long sequences. Adaptive KV cache compression seeks to discern the saliency of tokens, preserving vital information while aggressively compressing those of less importance. However, previous methods of this approach exhibit significant performance degradation at high compression ratios due to inaccuracies in identifying salient tokens. Ad- ditionally, the compression process introduces excessive overhead, substantially increasing memory burdens and the generation latency. In this paper, we present ZipCache, an accurate and efficient KV cache quantization method for large lan- guage models (LLMs). First, we construct a strong baseline for quantizing KV cache. Through the proposed channel-separable tokenwise quantization scheme, the memory overhead of quantization parameters are substantially reduced com- pared to fine-grained groupwise quantization. To enhance the compression ratio, we propose normalized attention score as an effective metric for identifying salient tokens by considering the lower triangle characteristics of the attention matrix. The quantization bit-width for each token is then adaptively assigned based on their saliency. Moreover, we develop an efficient approximation method that decouples the saliency metric from full attention scores, enabling compatibility with fast attention implementations like FlashAttention. Extensive experiments demonstrate that ZipCache achieves superior compression ratios, fast generation speed and min- imal performance losses compared with previous KV cache compression methods. For instance, when evaluating Mistral-7B model on GSM8k dataset, ZipCache is capable of compressing the KV cache by 4.98×, with only a 0.38% drop in accuracy. In terms of efficiency, ZipCache also showcases a 37.3% reduction in prefill-phase latency, a 56.9% reduction in decoding-phase latency, and a 19.8% reduction in GPU memory usage when evaluating LLaMA3-8B model with a input length of 4096. 1 Introduction LLMs with the next-token-prediction scheme have achieved remarkable advancements in various text-related tasks, such as language understanding [13, 34, 10], content creation [1, 5, 36], coding [3, 29, 42] and mathematics [33, 23, 35]. In this generation scheme, the forthcoming token interacts with all previous tokens via the attention mechanism [38], where the query, key and value states will be †Corresponding author. Email: zhouhong_zju@zju.edu.cn, bohan.zhuang@gmail.com Preprint. Under review. arXiv:2405.14256v1  [cs.LG]  23 May 2024calculated for each token. As the past tokens will not be altered, previously computed key and value states can be stored as KV cache to prevent re-computations, significantly improving the generation speed. However, as the batch size and the input context length grows, the stored KV cache emerges as a new memory bottleneck for LLMs. For example, when serving a 175B-parameter LLM [1] with a batch size of 64 and a context length of 4096, the KV cache can occupy 1.2TB of memory space, while the model weights only require 350GB. Meanwhile, the size of KV cache will continue to increase as decoding progresses. Therefore, the compression of KV cache is crucial for the efficient deployment of LLMs. Recent compression methods for KV cache can be broadly categorized into two types. The first type of methods compresses the KV cache uniformly, without considering the significance of individual tokens. To preserve performance, these methods often rely on either high-precision quantization [21] or maintaining recent tokens in full-precision [32], which undoubtedly compromise the compression ratio. Additionally, if salient tokens are not among the most recent ones, such as in information retrieval tasks, it may result in degraded performance. The other type of methods [ 46, 43, 16] compress KV cache adaptively by identifying salient tokens and compresses them separately. This approach aligns with the observation that a minority of tokens contribute the majority of attention scores [41], potentially achieving higher compression ratios than non-adaptive methods. However, current adaptive KV cache compression methods [ 46, 43] use accumulated attention scores as a metric of token saliency, which is insufficient in two aspects. First, accumulated attention scores is inaccurate in identifying important tokens. Due to the presence of attention masks, the attention matrix is a lower triangular matrix. Earlier tokens tend to have larger softmax attention values and more attention scores to be accumulated, as illustrated in Figure 3. Under this metric, the saliency of the most recent tokens can never surpass that of the first token, thereby introducing a bias in determining token saliency. Additionally, to obtain accumulated attention scores, full attention matrices must be explicitly computed and stored, which can be inefficient for serving LLMs. Given an input context length of l, fast attention implementations such as FlashAttention [8, 7] only require O(l) memory by computing attention output in blocks without retaining complete attention matrices. By contrast, storing full attention matrices requires O(l2) memory, and the large number of memory accesses significantly slows down the inference speed, as depicted in Figure 4. Figure 1: Accuracy and efficiency compar- isons across various KV cache compression methods. Data is collected with LLaMA3- 8B model on Line Retrieval dataset. Among these methods, ZipCache achieves the high- est accuracy, generation speed and compres- sion ratio. Details can be found in the sup- plementary material. To address these challenges, we introduce ZipCache, an efficient KV cache compression method that attains ex- ceptionally high compression ratios by accurate salient token identification. Figure 1 presents an overview of latency-accuracy comparisons among ZipCache and diverse KV cache compression methods. We start by designing an efficient quantization baseline for com- pressing the KV cache. To preserve performance, prede- cessor methods [32, 21] employ fine-grained groupwise quantization, which involves independent quantization for a small channel group within each token. However, this method necessitates storing extensive quantization parameters and results in significant memory overhead. By contrast, we introduce a channel-separable quanti- zation scheme that decouples the quantization along channel and token dimensions. This method signifi- cantly reduces the quantization overhead without com- promising performance. To accurately recognize salient tokens, we introduce a new token saliency metric based on normalized attention scores, which alleviates the bias towards earlier tokens that accumulate more val- ues. All tokens, without exception, will be quantized to the target bit-width based on their estimated saliency, boosting the overall compression ratio. Moreover, to ease integration with fast attention implementa- tions, we introduce an efficient approximation of the token saliency metric. This approximation only relies on computing and storing attention scores from a few number of tokens, which we refer to as probe tokens. An effective probe token selection strategy is then introduced to minimize performance loss. As a result, the majority of tokens can benefit from fast attention implementations, significantly enhancing the generation speed. 2In summary, our contributions are as follows: • We establish an efficient channel-separable quantization scheme for KV cache, which significantly reduces the overhead of quantization parameters without compromising performance compared to fine-grained groupwise quantization approach. • We propose an accurate metric for assessing token saliency based on normalized attention scores. All tokens are adaptively quantized according to their assessed saliency, thereby improving the overall compression ratio. • We further develop an efficient approximation method for the token saliency metric that integrates seamlessly with fast attention implementations, enhancing generation speed. • By integrating these three techniques, we present ZipCache, an accurate and efficient framework for KV cache compression. Extensive experiments demonstrate that ZipCache reaches a new state-of-the-art performance for KV cache compression in terms of compression ratio, accuracy and generation efficiency. 2 Related Work 2.1 Model Quantization Quantization is a prevalent technique for compressing deep neural networks by representing model weights and activations with lower numerical bit-widths. This technique can be categorized into two primary approaches based on the necessity of fine-tuning: post-training quantization (PTQ) [26, 17, 14] and quantization-aware training (QAT) [28, 31]. For large language models (LLMs), where fine-tuning can be data- and computation-intensive, PTQ is often the preferred method [40, 11, 45, 27]. In this paper, we also quantize KV cache in a post-training manner. For both approaches, quantization can be implemented at various levels of granularity, including channelwise, tokenwise, and groupwise approach. Typically, a finer quantization granularity involves the independent quantization of smaller parameter groups, which often results in improved performance albeit at the cost of more quantization parameters and increased memory overhead. In the context of LLMs, fine-grained quantization is frequently utilized due to the presence of outliers [22, 45]. However, for KV cache compression, this will greatly reduce the overall compression ratio. Mixed precision quantization [39, 44, 12, 2] allocates varying bit-widths to distinct parts of a model or tensor, enabling a more compact compression. This approach originates from the observation that model components exhibit differing sensitivities to quantization. Consequently, components with low sensitivity can utilize reduced bit-widths without impairing performance. For LLMs, previous studies [46, 43, 30, 18] have shown significant disparities in the importance of tokens, indicating that heavy compression of non-critical tokens has minimal impact on overall performance. This insight highlights the applicability of mixed precision quantization for compressing the KV cache. 2.2 KV Cache Compression While KV cache effectively prevents re-computation and significantly enhances generation speed, its memory footprint is notably substantial with long-context input. To alleviate this, many efforts have been made to reduce the KV cache size. Based on the compression method, these methods can be categorized into two groups: token dropping [46, 16, 30] and KV cache quantization [43, 21, 32]. The former identifies and drops unimportant tokens in the KV cache. For example, H2O [46] only maintain 20% heavy-hitted tokens and 20% recent tokens while evicting the rest. However, discarding tokens permanently erases their information, which proves to be suboptimal for tasks such as retrieval [43]. Conversely, the latter category employs quantization on the cached key and value states, and mixed precision quantization can further be applied once token importance is identified [ 43]. To tackle the outliers present in the KV cache, these methods extract the outlier as full precision [21] or use finer-grained quantization scheme [32], which increases the quantization overhead. In this study, we propose an efficient channel-separable quantization scheme with reduced quantization overhead and strong performance. Additionally, both categories of methods commonly adopt accumulated attention scores as the metric for token importance [46, 43]. However, we observe that this criterion is inaccurate and can result in significant performance deterioration at low bit-widths. In contrast, we achieve superior compression performance by utilizing a more accurate metric for identifying salient tokens. 33 Preliminary 3.1 Attention Block in LLMs Given an input prompt, the generation process of LLMs can be broadly categorized into two distinct phases: the prefill phase, which computes and stores the KV cache for input tokens, and the decoding phase, where new tokens are generated through a next-token-prediction scheme. Given input data X and an attention block with its weight matrices WQ, WK and WV, the prefill phase can be formulated as: Q = XWQ, K = XWK, V = XWV, (1) A = Softmax \u0012QKT √dk \u0013 , O = AV. (2) Here, dk is the dimension of the key, and A refers to the attention scores. K and V will be stored as KV cache. For clarity, we have omitted the output projection. For the decoding phase, given x as the embedding vector of the current token, the query q becomes a vector and the KV cache matrices will be updated as follow: q = xWQ, K = Concat(K, xWK), V = Concat(V, xWV). (3) The attention output are then computed as follows: a = Softmax \u0012qKT √dk \u0013 , o = aV. (4) To ensure clarity and consistency, we introduce notation to define the hyper-parameters used in the paper. Specifically, we denote the batch size as b, the number of attention heads as h, the sequence length as l, and the head dimension as d. 3.2 Model Quantization Uniform quantization is adopted in our study and all experiments. Given a floating-point vector x, it can be uniformly quantized to k-bit as follows: ˆx = QU (x, k) = clip(⌊x s ⌉ + z, 0, 2k − 1) · s. (5) Here, ⌊·⌉ denotes the round operation, s = max(x)−min(x) 2k−1 and z = −⌊min(x) s ⌉ are quantization parameters. It should be noted that the quantization parameters are stored in full-precision, which can lead to significant overhead if the quantization is fine-grained. 4 Method 4.1 A Strong Baseline for KV Cache Quantization Tokenwise quantization, as depicted in Figure 2(b) is prevalent in quantizing large language models (LLMs) due to the distinct representations of individual tokens. However, it has been widely observed, as illustrated in Figure 2(a), that outliers emerge within the channel dimensions of key and value matrices [43, 32], posing challenges for tokenwise quantization. To address this, recent work [32] resorts to groupwise quantization, where outlier channels are processed in distinct groups, as illustrated in Figure 2(c). However, this fine-grained quantization approach introduces excessive memory overhead, thereby significantly impacting the compression ratio. For instance, considering X ∈ Rb×h×l×d as the data to be quantized and a group size of n, tokenwise quantization only results in 2bl quantization parameters, while groupwise quantization would yield 2bhld n quantization parameters. Since these parameters are usually stored in full precision, this overhead would constitute a substantial portion of the storage cost for quantized data. Motivated by depthwise separable convolution [ 19], we introduce an efficient channel-separable tokenwise quantization scheme, which disentangles the channel and token dimensions. As shown in 4𝒔,𝒛∈𝑅𝑙 (b) Tokenwise Quantization (c) Groupwise Quantization (d) Channel-separable  Tokenwise Quantization 𝒄 ∈ 𝑅ℎ𝑑 (a) Visualization of  Key and Value States 𝐗 ∈ 𝑅𝑙∗ℎ𝑑 𝐗 ∈ 𝑅𝑙∗ℎ𝑑 𝐗 ∈ 𝑅𝑙∗ℎ𝑑 𝒔,𝒛∈𝑅𝑙∗ℎ𝑑/𝑛 𝒔,𝒛∈𝑅𝑙 Figure 2: Visualization and different quantization granularities for key and value states. Here, we omit the batch dimension for simplicity. For keys, channel outliers emerge, yet token representations exhibit minimal differences. For values, both channel outliers and distinct token representations exist. Figure 2(d), our approach initiates by normalizing each channel of data X with a scaling factor c. For the i-th channel in X, the normalization process can be formulated as: Xi = Xi ci , where ci = p max(|Xi|). (6) After normalization, each channel is scaled to a closed magnitude, mitigating the influence of outliers during tokenwise quantization. Subsequently, tokenwise quantization can be reliably applied and the scales c are multiplied back to restore the magnitude of each channel. The process of channel-separable tokenwise quantization is summarized in the supplementary material. Within this quantization scheme, the total number of quantization parameters amounts to hd + 2bl, representing a notable reduction compared to groupwise quantization, while effectively balancing the outlier channels and the representation of each token. Table 1: Performance comparisons of different quantization granularities for KV cache. The KV cache is quantized to 4-bit and the compression ratio is calculated with b = 8, hd = l = 4096and n = 32. Data is collected with LLaMA3-8B model on GSM8k dataset. Key Cache Quantization Granularity Value Cache Quantization Granularity Quantization Parameters Compression Ratio Acc.(%) / / 0 1× 55.88 Groupwise Groupwise 4bhld/n 3.2× 54.51 Tokenwise Tokenwise 4bl 3.99× 49.81 Channelwise Tokenwise 2hd+ 2bl 4.00× 52.77 Channelwise Channel-separable Tokenwise 3hd+ 2bl 4.00× 54.74 As referred to Figure 2(a), since the differences in token representations are small in key cache, we employ channelwise quantization for the key cache to further reduce overhead and employ channel- separable tokenwise quantization for the value cache. As depicted in Table 1, this configuration yields superior performance with reduced quantization overhead compared with groupwise quantization, thereby establishing a robust baseline for KV cache quantization. 4.2 Accurate Salient Token Identification Adaptive KV cache compression [46, 43, 16] aims to discern the saliency of each token, keeping the information of salient tokens while evicting or aggressively compressing the rest, to achieve a higher compression ratio. These salient tokens, also referred to as \"Heavy Hitters\" [46], are often identified based on accumulated attention scores. Given attention score matrix A ∈ Rl×l, the saliency of token i is estimated by: pi = lX k=1 Ak,i. (7) 5So   there  are   five   eggs 1.00 0.00 0.00 0.00 0.00 0.33 0.67 0.00 0.00 0.00 0.35 0.30 0.35 0.00 0.00 0.04 0.06 0.14 0.76 0.00 0.02 0.02 0.04 0.06 0.86 egs five   are   there   So : 40% salient tokens 1.74 1.05 0.53 0.82 0.86 0.35 0.26 0.18 0.41 0.86 Question: There are 15 trees in the  grove… Let's think step by step… Question: If there are 3 cars… Let's think step by step… Question: Leah had 32 chocolates… Let's think step by step… … Question: Olivia has $23… Let's think step by step… Question: Janet’s ducks lay 16 eggs per  day…How much in dollars does she  make every day at the farmers' market? (a) (b) (c) 𝒑𝒊 ෥𝒑𝒊 Figure 3: (a) A toy example to illustrate accumulated attention scores and normalized attention scores. Initial tokens have larger attention scores and more values to be accumulated. (b) A sample from GSM8k dataset with chain-of-thoughts (CoT) prompting. (c) The probability of each token being selected as a salient token, measured by both accumulated and normalized attention scores. Tokens correspond to the final question are identified as low saliency by accumulated attention scores. Tokens with large saliency values are then considered salient tokens. However, this approach has inherent limitations due to the lower triangular nature of the attention score matrix, as illustrated in Figure 3(a). There are two primary issues. Firstly, earlier tokens benefit from having more values accumulated since the elements above the diagonal are all zero. For instance, in a sequence of length l, the initial token accumulates l positive values, whereas the final token only accumulates one. Secondly, Softmax function converts real numbers into probabilities, so that the earlier rows of the attention matrix tending to have higher values, as fewer numbers are involved in the Softmax calculation. Consequently, the accumulated attention score of the final token will always be smaller than that of the first, which exceeds 1. To address this, previous works, such as H2O [46], always maintain recent caches in full precision. Nevertheless, this solution is suboptimal since recent tokens are not necessarily the most significant ones. To enhance the evaluation of each token’s saliency, we introduce an accurate token saliency metric based on normalized attention scores ˜pi: ˜pi = Pl k=1 Ak,i nnz(A:,i) (8) Here, nnz(A:,i) denotes the number of non-zero elements in the i-th column of A. As evidenced in Figure 3(a), normalizing the accumulated attention scores mitigates the influence of excessively large values in the initial rows of the attention score matrix, thereby delivering a more precise assessment. To validate the efficacy of our new metric, we input a sample from GSM8k dataset with chain-of- thoughts (CoT) prompting to the LLaMA3-8B model and identify saliency of each token by Eq. 7 and Eq. 8, respectively. As depicted in Figure 3(b) and (c), the salient tokens are at the end of the prompt, which correspond to the question for LLM to answer. However, these tokens are identified as low saliency by accumulated attention scores. Under the KV cache compression framework, these tokens would either be discarded or quantized to extremely low bit-width, resulting in a significant performance deterioration. In contrast, our method accurately identifies the salient tokens. Additional experimental results regarding the accuracy of our method will be detailed in Section 5.2. 4.3 Efficient Approximation of Saliency Metric As analyzed in Section 4.2, adaptive KV cache compression requires the explicit computation of full attention scores, as referred to Figure 4(b), which clashes with fast attention implementations like FlashAttention [8, 7, 9]. As shown in Figure 4(c), FlashAttention computes attention outputs in tiles without storing the intermediate attention scores. To reconcile the efficiency of FlashAttention with the substantial compression offered by adaptive KV caching, we devise an effective approximation for Eq. 8 as a measure of token saliency. Specifically, we sample a small group of tokens, designated 6𝐀 = (b) Standard Attention 𝐀 (c) FlashAttention 𝑸 𝑽 𝑶 𝑽 𝐊𝑻 𝑸 𝐊𝑻 𝑶 Memory=𝑶(𝒍𝟐) More memory access & slower Memory=𝑶(𝒍) Less memory access & faster (a) Efficient Saliency Metric with Probe Tokens Input tokens 𝐀𝑝𝑟𝑜𝑏𝑒 Probe tokens (b) Standard Attention (c) FlashAttention Regular tokens Output : Intermediate attention scores Figure 4: (a): Efficient saliency metric only requires attention scores of probe tokens through standard attention, enabling fast computation for the majority of tokens through FlashAttention. (b): In standard attention, full attention scores are computed before deriving the attention output. (c): FlashAttention avoids large attention matrix memory transfers by partitioning input matrices into blocks for incremental computation. as probe tokens, and compute their attention scores Aprobe as follows: Aprobe = Softmax \u0012QprobeKT √dk \u0013 . (9) By substituting Aprobe into Eq. 8, we can approximate the saliency of all tokens. For the remaining non-probe tokens, their attention scores do not have to be computed explicitly, enabling the integration of fast attention implementations to expedite the generation process, as illustrated in Figure 4(a). Table 2: Performance comparisons of various probe strategies. Data is col- lected from LLaMA3-8B model on GSM8k dataset. We quantize 40% salient tokens to 4-bit and the remaining 60% tokens to 2-bit. The proportion of probe tokens is 10%. Probe Strategy Acc.(%) All tokens 52.54 Random tokens 47.46 Special tokens 46.78 Recent tokens 51.10 Random+recent tokens 52.08 However, the positions of the probe tokens will un- doubtedly affects the accuracy of the approximated token saliency and the selection of probe tokens is under explored. In this study, we suggest four strategies for sampling probe tokens: • Random tokens. The probe tokens are randomly sam- pled from all positions. • Special tokens. The special tokens and punctuation tokens will be treated as probe tokens. • Recent tokens. The most recent tokens are selected as probe tokens. • Random+recent tokens. The probe tokens will be di- vided into two parts, one using recent tokens and the other randomly selecting from the remaining tokens. It should be emphasized that our study diverges from prior research [16] in that, rather than directly choosing special or recent tokens as salient tokens, we opt to sample a subset of tokens as \"probes\" to detect the salient ones. As depicted in Table 2, we present a comprehensive comparison of the performance among four distinct sampling strategies. Among the four strategies examined, a hybrid approach that combines recent tokens with randomly selected tokens emerges as the most effective. Unless otherwise specified, this hybrid strategy with 5% recent tokens and 5% random tokens will be employed in our method. 5 Experiment 5.1 Implementation Details Models and datasets. To validate the efficacy of our proposed method, we conduct experiments with three open-source LLMs: Mistral [ 20], LLaMA2 [37] and LLaMA3. These models are evaluated on three challenging benchmarks: GSM8k [6] for math problem solving, HumanEval [4] for code 7generation, and Line Retrieval [25] for data retrieval. To ensure reproducibility, the reported results are obtained using the Language Model Evaluation Harness [15] and LongEval [24]. Quantization and generation settings. We employ mixed precision quantization for KV cache where salient tokens will be quantized to 4-bit while the remaining will be quantized to 2-bit. For both subsets, we apply channelwise quantization for the key cache and channel-separable tokenwise quantization for the value cache. The proportion of salient tokens will be denoted by \"Saliency Ratio\" in the experimental results. During the decoding process, ZipCache adopts a streaming strategy [21] and repeats the compression process for the KV cache whenever 100 new tokens are generated. 5.2 Comparison with SOTA methods 5.2.1 Evaluation on GSM8k We begin our evaluation on GSM8k dataset with chain-of-thoughts (CoT) prompting, and the results are presented in Table 3. This task requires LLM to solve mathematical problems and return the final answer without multiple options. This task poses considerable challenges and previous KV cache compression methods manifest notable declines in accuracy. For instance, KIVI [32] shows an accuracy drop of 7.89% on LLaMA3-8B model, indicating the suboptimality of preserving recent tokens in full precision instead of identifying salient ones. Moreover, there is a substantial decrease in accuracy, amounting to 20.4%, for MiKV [43] under the high compression ratio. This suggests that accumulated attention scores mistakenly identify salient tokens, resulting in the loss of vital information during compression. By contrast, the proposed normalized attention scores can accurately measure token saliency, leading to a substantial enhancement in accuracy by 18.27% for LLaMA3-8B models in comparison to MiKV . In comparison to GEAR [21], which quantizes the entire KV cache to 4-bit, our approach additionally quantizes 40% tokens to 2-bit with enhanced performance on Mistral-7B model. This underscores the superiority of accurate adaptive compression of KV cache. Table 3: Performance comparisons on GSM8k with CoT prompts. Here, \"H/L\" denotes the bit-width for salient tokens (high-precision) and regular tokens (low-precision), respectively. The compression ratio is calculated with an average input length of l = 840. Model Method Bit-width (H/L) Saliency Ratio Compression Ratio Acc.(%) Mistral-7B FP16 16/16 100% 1 × 41.62 H2O [46] 16/0 40.0% 2.50 × 1.67 GEAR [21] 4/4 100% 3.00 × 39.42 KIVI [32] 16/2 15.2% 3.46 × 39.04 MiKV [43] 4/2 60.0% 4.98 × 36.32 ZipCache 4/2 60.0% 4.98× 41.24 LLaMA2-7B FP16 16/16 100% 1 × 14.18 H2O [46] 16/0 40.0% 2.50 × 13.50 GEAR [21] 4/4 100% 3.00 × 12.96 KIVI [32] 16/2 15.2% 3.46 × 13.19 MiKV [43] 4/2 60.0% 4.98 × 9.02 ZipCache 4/2 60.0% 4.98× 13.50 LLaMA2-13B FP16 16/16 100% 1 × 28.05 H2O [46] 16/0 40.0% 2.50 × 26.00 GEAR [21] 4/4 100% 3.00 × 25.40 KIVI [32] 16/2 15.2% 3.46 × 27.29 MiKV [43] 4/2 60.0% 4.98 × 23.65 ZipCache 4/2 60.0% 4.98× 27.85 LLaMA3-8B FP16 16/16 100% 1 × 55.88 H2O [46] 16/0 40.0% 2.50 × 27.82 GEAR [21] 4/4 100% 3.00 × 49.43 KIVI [32] 16/2 15.2% 3.46 × 47.99 MiKV [43] 4/2 70.0% 4.69 × 35.48 ZipCache 4/2 70.0% 4.69× 53.75 5.2.2 Evaluation on Line Retrival We further evaluate the data retrieval performance of various KV cache compression methods on Line Retrieval [25] dataset, where LLMs are required to retrieve specific content from a record 8of lines using a corresponding line index. The accuracy results under various number of lines are depicted in Figure 5. Notably, all quantization-based compression methods exhibit superior performance compared to the eviction-based approach H2O [ 46]. For eviction-based methods, information is permanently discarded upon eviction, whereas quantization introduces only minor errors while preserving the integrity of the data. Additionally, in comparison to KIVI [32], which always maintains recent caches at full precision, our approach consistently achieves better retrieval accuracy. This can be attributed to the nature of retrieval tasks, where salient tokens may appear at any position within the context, rather than being confined to the most recent caches. Moreover, when compared to MiKV [43], which employs accumulated attention scores as a saliency metric, our method yields a remarkable 42% accuracy improvement when evaluated using 200 lines on the Mistral-7b model. This substantial enhancement once more highlights the effectiveness of normalized attention scores in identifying salient tokens. Additional experimental results on HumanEval [4] can be found in the supplementary material. 50 100 150 200 Number of Lines 0 20 40 60 80 100Accuracy (%) LLaMA2-13B Full Cache ZipCache KIVI-2 MiKV H2O 100 200 300 400 Number of Lines 0 20 40 60 80 100Accuracy (%) LLaMA3-8B Full Cache ZipCache KIVI-2 MiKV H2O 50 100 150 200 Number of Lines 0 20 40 60 80 100Accuracy (%) Mistral-7B Full Cache ZipCache KIVI-2 MiKV H2O Figure 5: Performance comparisons of various KV cache compression methods on Line Retrieval. 5.3 Generation Efficiency In this subsection, we compare the latency and memory consumption of ZipCache and MiKV [43] under various input lengths, as depicted in Figure 6. Data is collected by serving LLaMA3-8B model on a Nvidia A100 GPU. MiKV employs accumulated attention scores to estimate token saliency, necessitating the use of standard attention for both prefill and decoding phases. Conversely, through an efficient approximate saliency metric, ZipCache requires only the calculation of the attention matrix for 10% of the tokens, while the remaining 90% tokens can be computed using either FlashAttention [7] or FlashDecoding [9]. Consequently, ZipCache achieves faster inference speed and lower memory usage, boasting a 37.3% reduction in prefill-phase latency, a 56.9% reduction in decoding-phase latency, and a 19.8% reduction in GPU memory usage when the input length scales to 4096. (a) Prefill phase latency  (b) Decoding phase latency  (c) GPU memory Figure 6: Comparisons of prefill-phase, decoding-phase latency and memory consumption between MiKV and ZipCache. 96 Conclusion and Future Work In this paper, we have proposed ZipCache, an accurate and efficient mixed-precision quantization framework for compressing KV cache. To commence, we introduce a channel-separable quantization scheme for KV cache, effectively reducing the overhead of storing quantization parameters compared to traditional fine-grained quantization schemes without performance degradation. Additionally, we present a novel metric for accurately assessing token saliency based on normalized attention scores. This metric enables adaptive quantization of all tokens according to their saliency, leading to improved compression ratios without sacrificing model performance. Moreover, we introduce an efficient approximation method for the token saliency metric, seamlessly integrating with fast attention implementations such as FlashAttention and FlashDecoding. This enhancement signifi- cantly boosts generation speed and reduces GPU memory requirements. Our extensive experiments have demonstrated that ZipCache achieves state-of-the-art compression performance in terms of compression ratio, accuracy and generation speed. We believe that ZipCache will pave the way for more practical and scalable deployment of LLMs in various real-world applications. Limitations and Broader Impacts. While ZipCache presents promising advancements in KV cache mixed-quantization frameworks for LLMs, the saliency ratio is manually specified before evaluation and cannot be automatically adjusted based on task datasets. Moreover, similar to other generative models, ZipCache can potentially be used to generate malicious content. References [1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. [2] A. Chauhan, U. Tiwari, et al. Post training mixed precision quantization of neural networks using first- order information. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1343–1352, 2023. [3] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y . Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [4] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y . Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [5] W.-L. Chiang, Z. Li, Z. Lin, Y . Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y . Zhuang, J. E. Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, march 2023. URL https://lmsys. org/blog/2023-03-30-vicuna, 3(5), 2023. [6] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [7] T. Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023. [8] T. Dao, D. Y . Fu, S. Ermon, A. Rudra, and C. Ré. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022. [9] T. Dao, D. Haziza, F. Massa, and G. Sizov. Flash-decoding for long-context inference, 2023. [10] J. C. de Winter. Can chatgpt pass high school exams on english language comprehension? International Journal of Artificial Intelligence in Education, pages 1–16, 2023. [11] T. Dettmers, M. Lewis, Y . Belkada, and L. Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems, 35:30318–30332, 2022. [12] Z. Dong, Z. Yao, A. Gholami, M. W. Mahoney, and K. Keutzer. Hawq: Hessian aware quantization of neural networks with mixed-precision. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 293–302, 2019. [13] M. Du, F. He, N. Zou, D. Tao, and X. Hu. Shortcut learning of large language models in natural language understanding. Communications of the ACM, 67(1):110–120, 2023. [14] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. [15] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, A. Le Noac’h, H. Li, K. McDonell, N. Muennighoff, C. Ociepa, J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron, 10L. Sutawika, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou. A framework for few-shot language model evaluation, 12 2023. [16] S. Ge, Y . Zhang, L. Liu, M. Zhang, J. Han, and J. Gao. Model tells you what to discard: Adaptive kv cache compression for llms. In The Twelfth International Conference on Learning Representations, 2024. [17] Y . He, L. Liu, J. Liu, W. Wu, H. Zhou, and B. Zhuang. Ptqd: Accurate post-training quantization for diffusion models. Advances in Neural Information Processing Systems, 36, 2023. [18] L. Hou, R. Y . Pang, T. Zhou, Y . Wu, X. Song, X. Song, and D. Zhou. Token dropping for efficient bert pretraining. arXiv preprint arXiv:2203.13240, 2022. [19] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. [20] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [21] H. Kang, Q. Zhang, S. Kundu, G. Jeong, Z. Liu, T. Krishna, and T. Zhao. Gear: An efficient kv cache compression recipefor near-lossless generative inference of llm. arXiv preprint arXiv:2403.05527, 2024. [22] Y . J. Kim, R. Henry, R. Fahim, and H. H. Awadalla. Finequant: Unlocking efficiency with fine-grained weight-only quantization for llms. arXiv preprint arXiv:2308.09723, 2023. [23] C. Li, W. Wang, J. Hu, Y . Wei, N. Zheng, H. Hu, Z. Zhang, and H. Peng. Common 7b language models already possess strong math capabilities. arXiv preprint arXiv:2403.04706, 2024. [24] D. Li, R. Shao, A. Xie, Y . Sheng, L. Zheng, J. Gonzalez, I. Stoica, X. Ma, , and H. Zhang. How long can open-source llms truly promise on context length?, June 2023. [25] D. Li, R. Shao, A. Xie, Y . Sheng, L. Zheng, J. Gonzalez, I. Stoica, X. Ma, and H. Zhang. How long can context length of open-source llms truly promise? In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023. [26] Y . Li, R. Gong, X. Tan, Y . Yang, P. Hu, Q. Zhang, F. Yu, W. Wang, and S. Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. In International Conference on Learning Representations, 2020. [27] J. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and S. Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023. [28] J. Liu, R. Gong, X. Wei, Z. Dong, J. Cai, and B. Zhuang. Qllm: Accurate and efficient low-bitwidth quanti- zation for large language models. In The Twelfth International Conference on Learning Representations, 2024. [29] J. Liu, C. S. Xia, Y . Wang, and L. Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36, 2024. [30] Z. Liu, A. Desai, F. Liao, W. Wang, V . Xie, Z. Xu, A. Kyrillidis, and A. Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. Advances in Neural Information Processing Systems, 36, 2024. [31] Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y . Mehdad, Y . Shi, R. Krishnamoorthi, and V . Chandra. Llm- qat: Data-free quantization aware training for large language models. arXiv preprint arXiv:2305.17888, 2023. [32] Z. Liu, J. Yuan, H. Jin, S. Zhong, Z. Xu, V . Braverman, B. Chen, and X. Hu. Kivi: A tuning-free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750, 2024. [33] H. Luo, Q. Sun, C. Xu, P. Zhao, J. Lou, C. Tao, X. Geng, Q. Lin, S. Chen, and D. Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct.arXiv preprint arXiv:2308.09583, 2023. [34] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. Improving language understanding by generative pre-training. 2018. [35] M. Tan, L. Wang, L. Jiang, and J. Jiang. Investigating math word problems using pretrained multilingual language models. arXiv preprint arXiv:2105.08928, 2021. [36] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [37] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 11[38] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [39] K. Wang, Z. Liu, Y . Lin, J. Lin, and S. Han. Haq: Hardware-aware automated quantization with mixed precision. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8612–8620, 2019. [40] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pages 38087–38099. PMLR, 2023. [41] G. Xiao, Y . Tian, B. Chen, S. Han, and M. Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2023. [42] F. F. Xu, U. Alon, G. Neubig, and V . J. Hellendoorn. A systematic evaluation of large language models of code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming, pages 1–10, 2022. [43] J. Y . Yang, B. Kim, J. Bae, B. Kwon, G. Park, E. Yang, S. J. Kwon, and D. Lee. No token left behind: Reliable kv cache compression via importance-aware mixed precision quantization. arXiv preprint arXiv:2402.18096, 2024. [44] Z. Yao, Z. Dong, Z. Zheng, A. Gholami, J. Yu, E. Tan, L. Wang, Q. Huang, Y . Wang, M. Mahoney, et al. Hawq-v3: Dyadic neural network quantization. In International Conference on Machine Learning, pages 11875–11886. PMLR, 2021. [45] Z. Yao, R. Yazdani Aminabadi, M. Zhang, X. Wu, C. Li, and Y . He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers.Advances in Neural Information Processing Systems, 35:27168–27183, 2022. [46] Z. Zhang, Y . Sheng, T. Zhou, T. Chen, L. Zheng, R. Cai, Z. Song, Y . Tian, C. Ré, C. Barrett, et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36, 2023. 12Appendix A Calculation of Overhead for Different Quantization Schemes Assuming b = 8, hd = l = 4096, and that the KV cache is quantized to4-bit, we proceed to calculate the actual compression ratio for different quantization granularities. For groupwise quantization with a group size of n = 32, the compression ratio Rgroup is given by: Rgroup = 2 × bhld × 16 2 × bhld × 4 +4bhld n × 16 = 3.200 (A) For tokenwise quantization, the compression ratio Rtoken can be calculated as: Rtoken = 2 × bhld × 16 2 × bhld × 4 + 4× bl × 16 = 3.992 (B) For our proposed quantization baseline, the compression ratio Rbaseline is determined by: Rbaseline = 2 × bhld × 16 2 × bhld × 4 + 3× hd × 16 + 2× bl × 16 = 3.995 (C) B Implementation Details of ZipCache In this section, we provide an overview of the channel-separable tokenwise quantization scheme in Algorithm 1. Additionally, we present the process of ZipCache’s prefill phase as described in Algorithm 2, as well as its decoding phase detailed in Algorithm 3. It is worth mentioning that during both the prefill and decoding phases, rather than calculating attention outputs separately for probe tokens and regular tokens followed by merging, FlashAttention [7] is utilized to compute the attention output for all tokens simultaneously. Additionally, attention scores of probe tokens are calculated. By bypassing the substantial memory accesses associated with matrix splitting and merging, this strategy enhances generation speed. Algorithm 1: Channel-separable Tokenwise Quantization (CSTQuant) procedure CSTQuant: Input: data X ∈ Rl×hd, target bit-width k for i ← 0 to hd do ci = p max(|Xi|) Xi = Xi ci // Normalizing each channel of X ˆX =TokenQuant(X, k) // Do tokenwise quantization for i ← 0 to hd do ˆXi = ˆXi × ci // Rescale each channel of X return ˆX C Additional Experimental Results C.1 Accuracy and Efficiency Comparisons of various KV cache compression methods In this section, we present the accuracy and efficiency comparisons of various KV cache compression methods, as presented in Table A. Data is collected by evaluating LLaMA3-8B model on200-line retrieval task with a Nvidia A100 GPU. We use a batch size of 8 and an average input length of 3072. Among these methods, ZipCache achieves the highest accuracy, compression ratio and generation speed. Specifically, in comparison to MiKV [43], which identifies salient tokens through accumulated attention scores, our method achieves a notable 10.0% accuracy improvement by accurately pinpointing salient tokens and a substantial38.0% decrease in prefill latency by integrating FlashAttention [7]. 13Algorithm 2: ZipCache for Prefill Phase procedure ZipCachePrefill: Input: Query states Q, key states K, value states V, saliency ratio r%, bit-width for salient tokens kh, bit-width for regular tokens kl // Salient Token Identification Select probe tokens and compute their attention scores Aprobe by Eq. 9 Measure the token saliency ˜p with Aprobe by Eq. 8 // Computing Attention Output with FlashAttention O = FlashAttention(Q, K, V) // Compressing KV Cache Partition key states: Ksalient, Kregular = Split(K, ˜p, r%) Partition value states: Vsalient, Vregular = Split(V, ˜p, r%) Ksalient = ChannelQuant(Ksalient, kh), Vsalient = CSTQuant(Vsalient, kh) Kregular = ChannelQuant(Kregular, kl), Vregular = CSTQuant(Vregular, kl) ˆK = Concat(Ksalient, Kregular) ˆV = Concat(Vsalient, Vregular) // Return Attention Output and Compressed KV Cache return O, ( ˆK, ˆV) Algorithm 3: ZipCache for Decoding Phase procedure ZipCacheDecoding: Input: Query vector q, key vector k, value vector v, KV cache ( ˆK, ˆV), saliency ratio r%, bit-width for salient tokens kh, bit-width for regular tokens kl, decoding token index i, probe attention score Aprobe K = Concat(k, ˆK) // Concatenate key cache V = Concat(v, ˆV) // Concatenate value cache o = FlashAttention(q, K, V) // Compute attention output i = i + 1 if i == 100then // Re-compress every 100 tokens Extract K[: −100] and V[: −100] and adaptively compress them with Aprobe Reset i = 0, Aprobe = None else if i >95 or randint(0, 100) < 5 then // probe tokens consists of 5% recent and 5% random tokens. Compute attention scores a of current token by Eq. 4 Aprobe = Concat(a, Aprobe) // Return Attention Output, KV Cache and Attention Scores from Probe Tokens return o, (K, V), Aprobe C.2 Evaluation on HumanEval In this section, we assess the performance of code generation across various KV cache compression methods, as summarized in Table B. Remarkably, ZipCache attains a compression ratio of 4.94× without sacrificing performance when tested with the Mistral-7B model, outperforming predecessor methods. Moreover, when evaluating on LLaMA3-8B model, our approach outperforms KIVI-2 [32] by 7.32% with a significantly higher compression ratio (4.39× vs. 2.55×). It should be noted that the average input length for this task is only119, while KIVI retains the recent 32 tokens in full-precision, thereby considerably diminishing its overall compression ratio. This underscores the advantage of ZipCache over methods that consistently retain information of recent tokens. 14Table A: Accuracy and efficiency comparisons over LLaMA3-8B on the200-line retrieval task. Here, \"H/L\" denotes the bit-width for salient tokens (high-precision) and regular tokens (low-precision), respectively. 0-bit denotes the tokens are evicted. The compression ratio is calculated with an average input length of l = 3072. Method Bit-width (H/L) Saliency Ratio Compression Ratio Acc.(%) Prefill-phase Latency (ms) FP16 16/16 100% 1 × 100 2340.11 H2O [46] 16/0 40.0% 2.50 × 0 4335.01 GEAR [21] 4/4 100% 3.00 × 100 5957.76 KIVI [32] 16/2 8.33% 4.36 × 96 4010.14 MiKV [43] 4/2 80.0% 4.43 × 90 4170.61 ZipCache 4/2 80.0% 4.43× 100 2584.01 Table B: Performance comparisons on HumanEval for code generation. Here, \"H/L\" denotes the bit-width for salient tokens (high-precision) and regular tokens (low-precision), respectively.0-bit denotes the tokens are evicted. The compression ratio is calculated with an average input length of l = 120. Model Method Bit-width (H/L) Saliency Ratio Compression Ratio Acc.(%) Mistral-7B FP16 16/16 100% 1 × 29.27 H2O [46] 16/0 40.0% 2.50 × 14.63 GEAR [21] 4/4 100% 3.00 × 28.05 KIVI [32] 16/2 26.7% 2.55 × 28.05 MiKV [43] 4/2 60.0% 4.94 × 27.44 ZipCache 4/2 60.0% 4.94× 29.27 LLaMA2-7B FP16 16/16 100% 1 × 14.02 H2O [46] 16/0 40.0% 2.50 × 11.59 GEAR [21] 4/4 100% 3.00 × 13.02 KIVI [32] 16/2 26.7% 2.55 × 11.59 MiKV [43] 4/2 80.0% 4.39 × 10.37 ZipCache 4/2 80.0% 4.39× 12.80 LLaMA3-8B FP16 16/16 100% 1 × 33.54 H2O [46] 16/0 40.0% 2.50 × 15.85 GEAR [21] 4/4 100% 3.00 × 28.66 KIVI [32] 16/2 26.7% 2.55 × 25.61 MiKV [43] 4/2 80.0% 4.39 × 29.88 ZipCache 4/2 80.0% 4.39× 32.93 15",
      "meta_data": {
        "arxiv_id": "2405.14256v1",
        "authors": [
          "Yefei He",
          "Luoming Zhang",
          "Weijia Wu",
          "Jing Liu",
          "Hong Zhou",
          "Bohan Zhuang"
        ],
        "published_date": "2024-05-23T07:37:16Z",
        "pdf_url": "https://arxiv.org/pdf/2405.14256v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "ZipCache addresses the significant memory demands of KV cache in large language models (LLMs), especially for long sequences, by proposing an accurate and efficient KV cache quantization method. It tackles performance degradation at high compression ratios and excessive overhead of previous adaptive compression methods. Its key contributions include establishing an efficient channel-separable quantization scheme for KV cache to reduce quantization parameter overhead, proposing a new token saliency metric based on normalized attention scores for accurate identification of important tokens, and developing an efficient approximation method for this saliency metric to integrate seamlessly with fast attention implementations. This results in superior compression ratios, fast generation speed, and minimal performance losses.",
        "methodology": "ZipCache employs a channel-separable tokenwise quantization scheme for KV cache, where each channel is first normalized to mitigate outliers, followed by reliable tokenwise quantization, significantly reducing quantization parameter overhead compared to groupwise methods. For key cache, it uses channelwise quantization, and for value cache, channel-separable tokenwise quantization. To accurately identify salient tokens, it introduces a normalized attention score metric (˜pi = Pl k=1 Ak,i / nnz(A:,i)) that overcomes the bias of accumulated attention scores towards earlier tokens due to the lower triangular attention matrix. Token bit-width is adaptively assigned based on this saliency. To ensure compatibility with fast attention implementations like FlashAttention, an efficient approximation method for the saliency metric is developed, involving sampling a small group of 'probe tokens' (a hybrid of recent and random tokens) to compute their attention scores, thereby approximating the saliency of all tokens without explicitly computing full attention matrices.",
        "experimental_setup": "Experiments were conducted using three open-source LLMs: Mistral-7B, LLaMA2 (7B and 13B), and LLaMA3-8B. These models were evaluated on three challenging benchmarks: GSM8k for math problem-solving with chain-of-thoughts (CoT) prompting, HumanEval for code generation, and Line Retrieval for data retrieval tasks. The evaluation metrics include accuracy, compression ratio, prefill-phase latency, decoding-phase latency, and GPU memory usage. For quantization, a mixed-precision scheme was used, quantizing salient tokens to 4-bit and regular tokens to 2-bit. Channelwise quantization was applied to the key cache, and channel-separable tokenwise quantization to the value cache. The proportion of salient tokens ('Saliency Ratio') was varied (e.g., 60-80%). Evaluations were performed using the Language Model Evaluation Harness and LongEval, with a streaming strategy repeating compression every 100 new tokens.",
        "limitations": "The saliency ratio for token quantization in ZipCache is manually specified before evaluation and cannot be automatically adjusted based on specific task datasets. Similar to other generative models, ZipCache potentially could be used to generate malicious content.",
        "future_research_directions": "The paper suggests that ZipCache's advancements will 'pave the way for more practical and scalable deployment of LLMs in various real-world applications.' Implicitly, future research could focus on automatically adjusting the saliency ratio based on task datasets, exploring more dynamic bit-width assignments, or extending the efficient approximation of saliency to further reduce computational overhead or improve accuracy in diverse scenarios. Addressing the manual specification of the saliency ratio would be a direct extension."
      }
    },
    {
      "title": "MiniCache: KV Cache Compression in Depth Dimension for Large Language Models",
      "abstract": "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance.",
      "full_text": "MiniCache: KV Cache Compression in Depth Dimension for Large Language Models Akide Liu1 Jing Liu1 Zizheng Pan1 Yefei He2 Gholamreza Haffari1 Bohan Zhuang1,2† 1ZIP Lab, Monash University, Australia 2ZIP Lab, Zhejiang University, China Abstract A critical approach for efficiently deploying computationally demanding large lan- guage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value states of previously generated tokens, significantly reducing the need for repetitive computations and thereby lowering latency in autoregressive generation. However, the size of the KV cache grows linearly with sequence length, posing challenges for applications requiring long context input and extensive sequence generation. In this paper, we present a simple yet effective approach, called MiniCache, to compress the KV cache across layers from a novel depth perspective, significantly reducing the memory footprint for LLM inference. Our approach is based on the observation that KV cache states exhibit high similarity between the adjacent layers in the middle-to-deep portion of LLMs. To facilitate merging, we propose disentangling the states into the magnitude and direction components, interpolating the directions of the state vectors while preserving their lengths unchanged. Furthermore, we in- troduce a token retention strategy to keep highly distinct state pairs unmerged, thus preserving the information with minimal additional storage overhead. Our Mini- Cache is training-free and general, complementing existing KV cache compression strategies, such as quantization and sparsity. We conduct a comprehensive evalua- tion of MiniCache utilizing various models including LLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks, demonstrating its exceptional performance in achieving superior compression ratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit MiniCache achieves a remarkable compression ratio of up to 5.02×, enhances inference throughput by approximately 5×, and reduces the memory footprint by 41% compared to the FP16 full cache baseline, all while maintaining near-lossless performance. Project is available at https://minicache.vmv.re 1 Introduction Large Language Models (LLMs), exemplified by the GPT series [ 1, 2, 3] and the LLaMA series [4, 5, 6], have emerged as pivotal innovations within the artificial general intelligence, significantly enhancing the capabilities of natural language processing. However, these models are meticulously trained using extensive computational resources [7] and massive datasets [8], which enables them to produce text that effectively mimics human writing styles and conducts complex reasoning analysis, yet raises the challenge of efficient deployment and serving. Within the inference framework of †Corresponding author. Email: bohan.zhuang@gmail.com Preprint. Under review. arXiv:2405.14366v2  [cs.CL]  7 Sep 2024(a)Cross-layer KV cachesimilarity (b)Merged layersvs EMscore on GSM8K Layer𝑙−1 Layer𝑙−2  ... Layer2 Layer1 LMHead Input Layer𝑙−3 Pruning/Quant. K VKVCacheCompression T Decoding Cross Layer Merging K V QK V Attention Decoding KVCacheCompression QK V Attention T+1 Prevs. MiniCache (c)Comparisonbetween MiniCacheand previous methods ...... CosineSimilarity  T T+1 Number of Layers Merged on LLaMA-3-70B Figure 1: Overview of our MiniCache strategy and example results: (a) shows the observation that the KV cache states between two adjacent layers are highly similar, particularly across the middle to deep layers. The x-axis uses index/2 to represent the similarities for each pair of layers. (b) compares the performance of MiniCache, and the mean baseline, which simply averages the KV caches of two layers, using the LLaMA-3-70B model [6] on the GSM8K dataset [10]. MiniCache, which begins merging from the half-layer depth, achieves near-lossless performance. (c) highlights the primary difference between MiniCache and previous approaches. MiniCache investigates the inter-layer redundancy of KV caches along the depth dimension of LLMs, an aspect overlooked by intra-layer-based methods. Here, T refers to the last timestamp of pre-filling, and T + 1 des to the first timestamp of decoding. LLMs, KV caches [9] are crucial for storing pre-computed keys and values, thus avoiding repeated calculations over the preceding context and substantially enhancing LLMs’ deployment efficiency. However, the increasing demand for longer sequence lengths results in voluminous cached states, leading to significant memory consumption during generation. For instance, a 175B GPT-3 model [2], with a batch size of 64 and a sequence length of 4,096 tokens (both prefilled and generated), requires approximately 1,208GB of GPU memory. This requirement is3.45× greater than the memory used to store the model’s weights. In this context, KV cache compression is of paramount importance due to its clear benefits: 1) it largely reduces the memory footprint allowing for faster generation and larger batch serving; 2) it significantly lowers the cost per token, demonstrating substantial commercial benefits. Existing KV cache compression efforts can be roughly categorized into two types, namely quantization and sparsity. The quantization approaches [11, 12] propose storing the KV states in low-bit numerical values. Typically, FlexGen [13] demonstrates that 4-bit KV cache quantization can achieve lossless performance. In contrast, sparsity-driven methods aim to retain only the salient tokens while evicting the rest, either heuristically [14, 15] or adaptively [16]. Some approaches [11] explore the intersection of these two types, by assigning high-bit to salient tokens and extremely low-bit to the rest of the tokens, achieving more aggressive memory gain. Despite these innovations, existing literature merely consider the intra-layer redundancy, while neglecting another important complementary direction – the inter-layer redundancy, as illustrated in the Figure 1(c). Our analysis begins by exploring the redundancy of KV cachesalong the depth dimension, as shown in Figure 1(a). We observe that KV cache states exhibit high similarity between neighbouring layers in 2the middle-to-deep portions of LLMs. This intriguing property suggests that states paired by position between adjacent layers can be accurately merged into a single state space with a strong performance guarantee, as illustrated in Figure 1(b). This approach significantly reduces the memory footprint without the need to retain individual states for each attention layer. Note that these observations are pertinent to dynamic inference strategies such as mixture-of-depths [17] and layer-wise early exiting [18, 19], which optimize computational paths by skipping non-critical layers to enhance training and inference efficiency. Furthermore, layer pruning methods [20] highlight considerable redundancy in deeper layers. However, despite these advancements, the redundancy of KV caches along the depth dimension has largely been overlooked. In this paper, we propose MiniCache, a simple yet effective cross-layer KV cache compression method aimed at advancing the inference efficiency of LLMs. MiniCache consists of two essential components. Firstly, we introduce an accurate cache merging strategy, employing a reparameteri- zation of state vectors that decompose them into the magnitude and direction components, akin to weight normalization [21]. This approach allows for effective interpolation of the directional compo- nent in polar coordinates while preserving the original state norms to retain as much information as possible. This interpolation refers to the cross-layer merging as shown in the Figure 1(c). Secondly, we recognize that a small subset of state pairs, characterized by low similarities but carrying largely distinct semantic meanings, are unsuitable for inter-layer merging. To address this, we propose a token retention strategy to minimize performance degradation, which involves separately retaining these outlier pairs. Our framework is notably memory-efficient, requiring storage for only a single high-dimensional directional component, along with minimal extra memory overhead. The over- head consists of a few unmergeable tokens and their corresponding indexes, as well as token-wise magnitudes to accurately restore the original states. We conduct extensive experiments with representative LLMs, including Mixtral-8x7B [ 22], Phi- 3-Mini [23], and LLaMA-3 [ 6] 8B and 70B, respectively. Our method is benchmarked across a diverse range of question answering and generation datasets [24, 25, 26, 27, 28, 29, 30, 31] using the lm-eval-harness [32]. Additionally, we evaluate our results on LongBench [33] for long-sequence generation. The results demonstrate that MiniCache can reduce the memory footprint required for LLM inference by up to 41%, while simultaneously enhancing throughput by approximately 5× compared to fully cached baseline, clearly surpassing existing methods [11, 12, 14, 15]. Our contributions are summarized as follows: • We introduce MiniCache, a simple yet highly effective framework for KV cache compression. MiniCache pioneers the exploration of KV cache compression along the depth dimension, thereby significantly expanding its capabilities. • We observe a fascinating characteristic of cross-layer KV cache states: high similarity between adjacent layers in the middle to later stages of LLMs. Additionally, we find that not all state pairs are suitable for merging. • We propose an accurate and memory-efficient method for cross-layer cache merging, comprising a reparameterization strategy and a token retention mechanism. Our method complements existing KV cache compression approaches, further enhancing LLM serving efficiency. • Our MiniCache performs favourably against the state-of-the-art methods. Notably, our 4-bit MiniCache achieves a strong compression ratio of up to5.02×, 5× higher inference throughput, and 41% memory reduction compared to the FP16 full cache baseline with near-lossless performance. 2 Related Work Efficient inference for LLMs. Large Language Models (LLMs) are constrained by considerable computational and memory requirements during inference, particularly in resource-constrained environments. To mitigate these challenges, a variety of efficient inference techniques have been developed. For instance, dynamic inference methods [ 18, 34, 35, 36], represented by mixture-of- experts (MoE) [37, 38, 39, 40, 41], adaptively select specific sub-structures of the model during the inference process based on the input data, significantly improving inference efficiency while keeping model capacity. Techniques like Multi-Query Attention [42, 43], Kernel-driven attentions [44, 45, 46, 47], and low-rank attentions [41, 48, 49, 50] approximate the functionality of traditional attention mechanisms but with more efficient implementations. Quantization strategies [51, 52, 53, 54] 3involve converting the model’s weights and activations into a low bit-width format, thereby reducing memory footprint and computational intensity. Sparsification approaches [14, 15, 55, 56] eliminate unnecessary elements in both model weights and token representations, further enhancing efficiency. Some closely related works, such as MoD [17] and LayerSkips [19], considered the dynamic inference nature to ignore unimportant layers according to input. However, these methods require an additional fine-tuning process or carefully designed pre-training stages, which reduces the adaptivity of these methods. MiniCache relies on inter-layer similarity observations to perform cross-layer merging, significantly reducing memory demand. Model merging. Merging compression involves the aggregation of a model’s parameters and activations at various granularities. This process enhances the efficiency of inference in large models and facilitates huge redundancy [57]. Linear Mode Connectivity (LMC) [58] enables the fine-tuning of models from a shared pre-trained base. Commonly, weight averaging [ 59] is employed as an efficient technique to perform merge compression. Notably, Model Soup [60] utilizes linear averaging in this context. Advanced methods like TIES Merging [61], Model Breadcrumbs [62], and DARE [63] further enhance this process by sparsifying and combining model parameters, enabling the merging of models without sacrificing performance capabilities. Additionally, Spherical Linear intERPolation (SLERP) [64] extends beyond simple weight averaging by interpolating between model parameters. The Fisher information matrix [65] and RegMean-based methods [66] further optimize merges to produce ideal weights, minimizing the ℓ2 distance to generation outputs while preserving the privacy of the training data. However, most existing works focus on merging model parameters, with the concept of depth-wise mergeability not being thoroughly explored in prior research. MiniCache focuses on the KV cache token merging in the depth dimensional of LLMs. 3 Motivation In the below, we present our new observations in a novel cross-layer perspective. LLama 2 7BLLama 2 30BLLama 3 8BMixtral 8x7B Models 0 10 20 30 40 50Exact Match (%) Baseline Mean KV (a) Simple average baseline vs. full cache on GSM8K 0 20 40 60 80 100 T okens Index 0.6 0.4 0.2 0.0 0.2 0.4 0.6 Cosine Similarity Layer 16 - 17 Layer 18 - 19 Layer 20 - 21 Layer 22 - 23 Layer 24 - 25 Layer 26 - 27 Layer 28 - 29 Layer 30 - 31 (b) Pairwise similarity in adjacent layers KV cache MathQA OpenBookQA PiQA RTE Winogrande 0.00.20.40.60.8 MiniCache Baseline Mean (c) Benchmark on five QA datasets Figure 2: Overall of our explorations and observations : (a) shows the strong baseline by performing average merging on the KV cache. (b) shows the pairwise similarity of cache states between adjacent layers. (c) compares the MiniCache, simple average, and full cache baseline across five different datasets. 3.1 Cross-Layer Redundancy in KV Cache Prior studies have revealed the ineffectiveness of middle-to-deep layers in LLMs [20]. Thus, layer- wise early exiting in this case can effectively avoid the redundant computation with minor effect on LLM performance [19, 67]. Inspired by this, we explore a layer-wise merging of KV cache in LLMs, starting with a simple baseline by averaging full tokens across adjacent layers. We provide our key observations as follows. Observation 1: KV cache shares a high similarity between adjacent layers. Based on LLaMA-3- 70B [6], we conduct zero-shot inference on the validation sets of three widely recognized benchmarks: COQA [68], GSM8K [10] and TruthfulQA [69]. In general, we find that KV cache in the shallow layers exhibits low similarity, whereas those in the middle-to-deep layers are very similar to one another based on angular distance, as shown in Figure 1(b). Next, we merge the KV cache across 4adjacent layers by conducting five-shot inference with LLaMA-2-7B, LLaMA-2-13B [5], LLaMA- 3-8B [6], and Mixtral-8x7B [22] on GSM8K [10]. Specifically, starting from the middle layer of each model, we merge the KV cache in the adjacent two layers. As shown in Figure 2(a), we observe a favourable performance for different LLMs, which reveals the huge potential for efficiency improvement by sharing the KV cache across adjacent layers during LLM decoding. Observation 2: Not all tokens are equally important to merge, a few distinct pairs require retention. Recent works [15, 16] in KV cache compression have found that keeping a few salient tokens at each layer, which contribute the majority of attention scores, could be sufficient to maintain LLM performance. In our case, we speculate that certain token pairs in adjacent layers also exhibit outlier behaviours, showing strong semantic differences that make them unsuitable for merging. Based on COQA [ 68] and LLaMA-2-7B [ 5], we investigate the similarities at the level of token pairs. As shown in Figure 2(b), we find a significant portion of token pairs share high similarities across adjacent layers. However, we observe a few outlier pairs, such as indices 0 and 15, with large margins of difference. We consider these tokens non-mergeable due to their significant differences. We also show that merging these distinct tokens results in performance degradation, corresponding to γ = 0 row in Table 2. Thus, while cross-layer merging is a promising strategy for reducing memory burdens, it must be implemented with careful consideration of token-level similarities to ensure optimal performance, as shown in Figure 2(c). 4 Method In this section, we introduce our MiniCache, a simple yet effective method aimed at trimming the KV cache redundancy in the depth dimension. This framework exploits the high similarity of KV cache states between adjacent layers and consists of two primary components: a reparameterization-based merging strategy and a token retention mechanism. The merging strategy compresses the KV cache states in adjacent layers to aggregate them into a single shared memory space, beginning from the middle of the model. The token retention mechanism mitigates information lost by retaining the highly distinct state pairs with minimal additional memory cost. With the merged cache, retention tokens, and magnitudes, we can accurately restore the original cache states for token decoding. 4.1 Cross-Layer Compression Our method commences with the identification of an optimal starting layer S. Observations in Section 3.1 indicate that the KV cache from middle-to-deep layers consistently exhibits patterns of high similarity across adjacent layers. Consequently, we select the starting layer from the middle of the LLM, specifically S = L/2. From this layer onward, the KV pairs are assumed to be sufficiently similar across adjacent layers to warrant their consolidation. Central to this approach is a merge function, F, which is designed to integrate the KV caches of consecutive layers into a single, unified cache. We define x as the vectorized cache state of a single token, where the superscript indicates the layer index and the subscripts k and v denote the keys and values, respectively. Specifically, for a pair of key/value tokens at the same position in layers l and l − 1, the merged cache is computed as cl,l−1 k = F(xl k, xl−1 k ), cl,l−1 v = F(xl v, xl−1 v ). (1) This consolidation process effectively eliminates the need to store and process the original memory- intensive keys and values in each layer independently. Instead, it approximates a shared cache across the adjacent layers. 4.2 KV Cache Merging and Restoration Reparameterization-based cache merging. To perform the pairwise merging, one solution is to directly average a pair of KV tokens, analogous to model merging [60, 61]. However, we observe that direct averaging can cause significant information loss. We conjecture that the distance between activations can be larger than that of weights due to the presence of outlier activation channels with extremely large magnitudes in LLMs [70, 71], while weights typically have relatively quite small magnitudes. A potential method to compensate for this information loss is to project from c to xl−1 and xl, then rescale the projected vectors based on their relative magnitudes to exactly restore the 5(a)Cross-layer Compression KV Store𝐶 Keep Rescale Recover (b)Restoration𝑙−1𝑙 𝑙−1 𝑙 Keep × Fetch𝐶\tforlayer 𝑙and𝑙−1KVcachecompressionat layer𝑙 original KV Cache merged KV Cache retentiontokencachemagnitudemergeoperation Figure 3: The illustration of the proposed methodMiniCache. (a) depicts the cross-layer compression process. We fetch the KV caches, from layers l and l − 1, and merge them into shared states via Eq. (3). Additionally, we compute the ℓ2 norm for the caches to obtain their magnitudes. Furthermore, we select unmergable tokens for retention, then store merged cache, retention tokens, and magnitudes at layer l in C. (b) illustrates the restoration process for layers l and l − 1, which includes magnitude rescaling in Eq. (2) and retention token recovery. original states. However, this approach requires extensive additional storage and computations; for example, restoring xl−1 needs both c and xl, which undermines the benefits of cache merging. To efficiently merge token pairs, we draw inspiration from weight normalization [21], which disentangles model parameters into the magnitude and direction components to accelerate the convergence of stochastic gradient descent. Additionally, we take cues from DoRA [72], which employs a similar way to resemble the learning behavior of parameter-efficient fine-tuning compared to full fine-tuning. In our case, the reparameterization can be formulated as follows: ˆxl = el,l−1 · ∥xl∥ ∥el,l−1∥, ˆxl−1 = el,l−1 · ∥xl−1∥ ∥el,l−1∥, (2) where e is the directional vector. This decomposition ensures that el,l−1 ∥el,l−1∥ is a unit vector, and allows the restored states to match the ℓ2 norm of the original states, thereby preserving the cache’s information as much as possible. The restoration is shown as Figure 3(b). For brevity, we omit the subscripts k and v, as keys and values are decomposed in the same way. For estimating the directional component el,l−1, we follow SLERP [64], which adaptively handles the interpolation, which often resembles rotation-like transformations. The choice of SLERP as the merging function is strategic, as it facilitates interpolation along the shortest path on the unit sphere between two high-dimensional vectors, thereby preserving their geometric integrity, which refers to merge operation in Figure 3(a). This is crucial for maintaining the semantic and syntactic properties of the KV caches. The formula for SLERP in our context is: el,l−1 = sin((1 − t)Ωl,l−1) sin(Ωl,l−1) · xl−1 ∥xl−1∥ + sin(tΩl,l−1) sin(Ωl,l−1) · xl ∥xl∥, (3) where Ωl,l−1 = arccos \u0010 xl·xl−1 ∥xl∥∥xl−1∥ \u0011 represents the angle between vectors xl and xl−1, and sin(·) is the sine function. t is an interpolation hyperparameter that adjusts the relative influence of each vector on the resulting direction, tailored to the layer depth and specific characteristics of the KV pairs. Note that when we set t = 0.5, it will become an average merging along the geometry surface, which we consider special cases in Eq. (A). The merged cache for each token pair is then a concatenation of the directional vector, magnitude and Ωl,l−1, denoting as cl,l−1 = [el,l−1, ∥xl−1∥, ∥xl∥, Ωl,l−1], cached components as shown in Figure 3(a). Note that apart from storing the merged directional vector, we only need to store additional token-wise magnitude and angle scalars, which is memory efficient. In this way, we achieve substantial memory efficiencies through reduced redundancy while ensuring the retention of the critical functional characteristics of the original KV pairs across transformer layers. Unmergeable token retention. Highly distinct pairs are sensitive to merging operations, leading us to propose unmergeable token retention, as shown in Figure 3(a). Despite the high similarity between KV cache states across neighbouring layers, a few sensitive distinct pairs remain that are significantly 6difficult to merge and share. Aligned with previous studies, these distinct tokens carry substantial semantic meanings [15, 16]. We observe that merging sensitive tokens, which results in the loss of layer-specific information, can lead to significant performance degradation. Therefore, it is crucial to properly disentangle the shared and unique information between adjacent layers. To address this issue, we designed a token retention strategy to selectively retain tokens that cannot be merged based on their angular distance, defined as: d(xl, xl−1) = 1 π Ω. For the KV caches, the minimum and maximum angular distances are determined to identify the unmergeable tokens. The set of required token indices to keep, I, is obtained by: I = {i | di < dmin + (dmax − dmin) · γ}, (4) where γ is a predefined hyperparameter that controls the retention threshold. The tokens with indices in I are retained and not compressed during the merge, which ensures that performance does not decline by preventing the loss of unmergeable tokens. Next, let X ∈ Rn×h be either the key or value cache at one attention layer, where n denotes the number of tokens and h is the number of hidden dimensions, and E ∈ Rn×h be the shared KV cache states. For each pair of neighbouring two layers, the unmergeable tokens are selected along with the token dimension by Rl = Xl[I], Rl−1 = Xl−1[I], then restoring to our compressed caches by ˆXl[I] = Rl, ˆXl−1[I] = Rl−1, as shown in Figure 3(b). Overall, we share the final cache for the two layers as Cl,l−1 = [El,l−1, Rl, Rl−1, ∥Xl−1∥, ∥Xl∥, I]. This cache includes the shared KV cache states, retention of unmerged tokens, magnitude vectors for each layer, and the token-keeping index, respectively. These additional components are quite lightweight. Thus compared to full-layer caches, our method remains memory-efficient, as discussed in Sec. 4.3. Cache restoration. After obtaining the shared cacheCl,l−1, we further need to approximately restore the original cache states for the current token decoding, as shown in Fig. 3(b). Specifically, to restore Xl, we first rescale the directional shared states with the corresponding magnitude vector along the token dimension, denoted as El,l−1∥Xl∥. Subsequently, we perform retention token recovery by placing the sensitive tokens according to their token indices. 4.3 Efficiency Discussion Compression efficiency. We primarily analyze our memory efficiency in terms of the number of tokens used. Next, let r be the number of layers and and b is the batch size, s and n are input and output sequence length respectively. We consider the FP16 storage for KV cache. The full cache memory usage is given by 4brh(s + n). In our study, we begin merging layers from the middle to the deeper layers, consolidating the KV cache states of every two layers into a single shared state space. As a result, we effectively reduce the GPU memory usage in the decoding inference to 3brh(s + n), demonstrating a significant compression rate. Restoration efficiency. We then analyze the additional memory cost incurred during the restora- tion process, which During the magnitude rescaling phase, we save an additional norm vector for the corresponding layers in the KV cache. It is important to note that the norm vector is in the shape of Rb×s×1, which has a single channel dimension compared to the fully ranked original KV states. Additionally, we suppose that the retention threshold can be set to 0.05. Therefore, we have brh(0.05(s + n)) tokens retained without compression. Finally, our overall memory requirement is given by (3.1h + 2)br(s + n). The detailed derivation is shown in the Appendix E. 5 Experiments We demonstrated that our MiniCache can perform merging compression on the latter half of the layers of LLMs with minimal performance degradation. Implementation details. Our experiments are based on representative model families of LLMs, including a compact LLM Phi-3-Mini [23] and an MoE LLM Mixtral-8x7B [22]. Additionally, we adopt LLaMA-3 [6] 8B and 70B models to explore how our method generalizes to larger LLMs. We sample ten tasks from lm-eval-harness [32], including COPA [24], MathQA [25], OpenBookQA [26], PIQA [27], RTE [28], WinoGrande [29], XSUM [30], and CNN/Daily Mail [31]. We also evaluate long-sequence generation on LongBench [33]. We compare our method with a fully cached baseline, 7and other methods such as round-to-nearest quantization (RTN) [73], SmoothQuant [70] and KIVI [11]. For the proposed MiniCache, we set the interpolation parameter t to 0.6, indicating that the merged results have a smaller rotation angle to the next layer. Furthermore, we set the token retention threshold γ to 0.05, according to the statistics of unmergeable tokens across multiple datasets. In addition to our merging method, we also consider a strong baseline of average merging. For sequential loading of large models, we utilize NVIDIA 4 A100 80GB GPUs, more details refers to Appendix D. Main results. We evaluate MiniCache by merging KV caches across all layers on GSM8K, COQA, and TruthfulQA. The results are shown in Figure 4. In general, we demonstrate the general effective- ness of merging KV caches from middle-to-deep layers across different sized LLMs. Moreover, the proposed MiniCache demonstrates a consistent and significant advantage over the averaging baseline. We also illustrate the performance of merging KV caches across half of the layers with the blue lines, where MiniCache still maintains a robust performance and achieves the best compression ratio. Besides, we find that our method is even more effective for larger LLMs. For instance, based on LLaMA-3-70B, MiniCache shows nearly zero performance drop even with the KV cache in 87.5% of the layers merged on the COQA dataset. This highlights the adaptability and efficiency of our approach in handling large-scale models while ensuring minimal performance degradation. LongBench. We also conduct experiments to evaluate performance and quality in long-sequence gen- eration using the LongBench dataset [33], as shown in Table 1. Our experiments applied MiniCache over several models: LLaMA-2-7B-Chat, LLaMA-2-13B-Chat, Mistral-7B, and Mistral-7B-Instruct. It is important to note that our MiniCache method maintains orthogonality with all existing quan- tization and sparsity (refers to Table A) methods at both the model and token-wise levels. When combined with KIVI-4bit KV cache quantization, our approach achieves a compression rate of5.02×, with minimal impact on accuracy across various challenging long-context generation tasks. The combination of MiniCache and KIVI-4bit KV cache quantization demonstrates significant memory savings without compromising the model’s ability to handle long sequences effectively. This high- Figure 4: Performance comparisons between our proposed MiniCache with the “averaging baseline” and the “unmerged full cache baseline” on multiple datasets with Phi3-Mini, Mixtral-8x7B, LLaMA- 3-8B, and LLaMA-3-70B. More result details are shown in Appendix F. The x-axis indicates the number of layers merged. As more layers are merged, a greater reduction in memory usage is achieved. 8Table 1: Evaluation of different KV cache compression methods on LongBench. MiniCache builds on top of 4-bit KIVI [11] and achieves the best performance with the strongest compression rate. Model Method LCC RepoBench-P PR-en TREC 2wikimqa GovReport MQA-zh AverageCompressionRatio Llama-2-7B-Chat Baseline 58.16 52.19 10.12 64.00 31.12 27.09 10.12 36.41 1xRTN [73] 15.44 8.76 0.79 4.00 0.30 1.93 0.07 4.90 3.21xSmoothQuant [70] 35.31 32.18 0.79 28.75 7.45 11.83 1.68 16.28 2.15xKIVI-2 [11] 49.32 43.71 4.50 63.00 24.07 24.73 10.24 31.51 3.95xMiniCache 58.03 52.01 9.00 64.00 30.58 25.32 10.13 35.44 5.02x Llama-2-13B-Chat Baseline 48.06 50.08 14.25 68.50 13.09 27.76 7.23 32.71 1xRTN [73] 20.89 18.62 0.33 0.00 0.52 1.68 0.16 6.03 3.21xSmoothQuant [70] 32.17 33.86 2.65 48.00 3.53 12.47 0.47 19.16 2.15xKIVI-2 [11] 48.60 48.81 13.50 68.00 14.32 25.70 7.01 32.42 3.95xMiniCache 48.75 48.59 13.00 68.00 14.36 26.57 7.99 32.61 5.02x Mistral-7B Baseline 68.06 60.46 17.71 68.00 10.87 20.09 17.10 37.33 1xRTN [73] 27.98 26.18 3.34 13.00 1.11 2.49 0.45 10.51 3.21xSmoothQuant [70] 40.63 35.14 3.40 30.50 6.03 5.00 4.12 17.55 2.15xKIVI-2 [11] 65.16 58.33 12.43 65.00 11.03 13.22 13.87 33.43 3.95xMiniCache 68.89 60.98 13.92 67.00 10.50 18.06 7.88 35.75 5.02x Mistral-7B-Instruct Baseline 55.51 48.96 60.00 71.00 27.33 32.85 42.74 48.32 1xRTN [73] 32.36 33.23 0.67 1.00 2.25 10.03 2.30 11.55 3.21xSmoothQuant [70] 43.84 38.63 4.79 39.50 10.34 23.61 8.33 24.43 2.15xKIVI-2 [11] 53.13 48.60 47.50 69.00 20.68 29.37 33.88 43.74 3.95xMiniCache 54.79 51.02 64.14 71.00 24.97 31.46 27.54 46.99 5.02x lights the potential of our method to optimize large language models for tasks requiring extensive context, making them more efficient and scalable for real-world applications. Efficiency analysis. To assess the acceleration capabilities of MiniCache, we conduct evaluations based on the methodologies employed in vLLM [74] and KIVI [11]. We generate synthetic workloads derived from ShareGPT, which include real input and output texts from LLM services. The dataset features an average input prompt length of 161 tokens and an output length of 338 tokens. Using the LLaMA-2-7B model on a single 80GB NVIDIA A100 GPU, we benchmark our method in a batch-serving scenario, comparing peak memory usage and throughput among 2-bit KIVI, 4-bit MiniCache, and an FP16 baseline. As illustrated in Figure 5, with a batch size of 128, MiniCache reduces memory usage by 25GB, achieving a 41% memory saving . In terms of throughput, MiniCache outperforms the FP16 baseline by approximately 5×. Additionally, despite utilizing 4-bit quantization, MiniCache benefits from merging and sharing KV caches across adjacent layers, resulting in a 1.29× higher throughput compared to the 2-bit KIVI. These results demonstrate that MiniCache offers a state-of-the-art trade-off between efficiency and performance. 50 100 150 200 250 300 Batch Size 20 30 40 50 60 70 80Peak Memory Usage (GB) Baseline FP16 KIVI 2 MINICache 4 (a) BS. vs. Peak Memory Usage 50 100 150 200 250 300 Batch Size 1000 1500 2000 2500 3000Throughput (tokens/sec)  Baseline FP16 KIVI 2 MINICache 4 (b) BS. vs. Decoding Throughput Figure 5: Memory usage and throughput comparison between our 4-bit MiniCache, 2-bit KIVI, and 16-bit Baseline. MiniCache can achieve higher throughput by enabling a larger batch size while reducing memory footprints via LLaMA-2-7B [5]. 0.3 0.4 0.5 0.6 0.7 Interpolation Parameter t 0.350 0.375 0.400 0.425 0.450 0.475 0.500Exact Match Scores GSM8k 0.0 0.2 0.4 0.6 0.8 1.0 1.2 Normalized Frequency Frequency Figure 6: LLaMA-3-8B [ 6] to experiment on the GSM8K [10]. The right axis is the normalized frequency of the relative magni- tude ratio. Optional t shows a strong correlation with frequency. 6 Ablation Study Table 2: Comparisons of various token retention thresholds γ by LLaMA-2-7B [5] on three benchmarks. γ COQA GSM8K TruthfulQA 0 0.603 0.108 29.813 0.01 0.620 0.126 30.226 0.02 0.630 0.143 33.903 0.05 0.647 0.152 33.213 0.1 0.643 0.152 33.903 1 0.643 0.159 33.743 The effect of interpretation parameter t. We explore the effects of the interpretation parameter t on perfor- mance, particularly in relation to the relative magnitude ratio of adjacent layers, as shown in Figure 6. We maintain all settings constant, starting from layer S = 16 (halfway 9through the layers of LLaMA-3-8B), and vary the interpretation parameter t from 0.3 to 0.7. Our findings reveal several key points. When t = 0.5, the process resembles average merging, which is less effective for cross-layer merging. In contrast, when t = 0.6 is optimal, the merged representation exhibits the most robust performance, while indicating that more information is derived from the second term (xl) of the SLERP. The frequency results also indicate that the high frequencies are clustered around 0.4 and 0.6, corroborating our optimal t. Moreover, there is a strong correlation between the optimalt and the high frequency of the relative magnitude ratio of adjacent layers. This finding provides an opportunity to utilize the relative magnitude ratio to dynamically determine the interpretation parameter t. Dynamic t allows for more flexible weight control in SLERP merging for each layer-wise operation, thereby showing potential for further exploration. The effect of token retention thresholdγ. We investigate the impact of the token retention threshold γ on model performance across the three datasets, as shown in Table 2. A larger t generally means retaining more tokens for improved performance, but this comes at the cost of increased memory demand. The results suggest that setting γ to 0.05 achieves the best balance between performance and efficiency. 7 Conclusion and Future Work This paper presents a pioneering exploration of KV cache compression in the depth dimension, addressing a significant memory bottleneck in LLMs. Our proposed MiniCache offers a simple, effective, and training-free approach to compressing KV caches by leveraging the notable high similarities between KV caches in neighboring layers, starting from the midpoint of LLMs. We have demonstrated that MiniCache can significantly reduce the memory footprint required for LLM inference by up to 41%, while simultaneously enhancing throughput by approximately five times compared to the FP16 baseline. In conclusion, MiniCache significantly advances the field of KV cache compression, offering a state-of-the-art balance between efficiency and performance. Future work will focus on enhancing the compression ratio by cross-multiple-layer merging, developing advanced merging algorithms such as Spherical Cubic Interpolation [ 75], and further optimizing memory usage for large-scale deployments in diverse application scenarios. References [1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., “Language models are few-shot learners,” in NeurIPS, vol. 33, pp. 1877–1901, 2020. [2] OpenAI, J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, and OTHERS, “Gpt-4 technical report,” 2023. [3] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray,et al., “Training language models to follow instructions with human feedback,” in NeurIPS, vol. 35, pp. 27730–27744, 2022. [4] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar,et al., “Llama: Open and efficient foundation language models,” arXiv preprint arXiv:2302.13971, 2023. [5] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale,et al., “Llama 2: Open foundation and fine-tuned chat models,” arXiv preprint arXiv:2307.09288, 2023. [6] “Introducing meta llama 3: The most capable openly available llm to date.”https://ai.meta. com/blog/meta-llama-3/, 2024. Accessed: 2024-05-04. [7] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Rad- ford, J. Wu, and D. Amodei, “Scaling laws for neural language models,” arXiv preprint arXiv:2001.08361, 2020. 10[8] G. Team, R. Anil, S. Borgeaud, Y . Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al., “Gemini: a family of highly capable multimodal models,” arXiv preprint arXiv:2312.11805, 2023. [9] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, J. Heek, K. Xiao, S. Agrawal, and J. Dean, “Efficiently scaling transformer inference,” Proceedings of Machine Learning and Systems, vol. 5, 2023. [10] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al., “Training verifiers to solve math word problems,”arXiv preprint arXiv:2110.14168, 2021. [11] Z. Liu, J. Yuan, H. Jin, S. Zhong, Z. Xu, V . Braverman, B. Chen, and X. Hu, “Kivi: Plug- and-play 2bit kv cache quantization with streaming asymmetric quantization,” arXiv preprint arXiv:2402.02750, 2024. [12] H. Kang, Q. Zhang, S. Kundu, G. Jeong, Z. Liu, T. Krishna, and T. Zhao, “Gear: An efficient kv cache compression recipefor near-lossless generative inference of llm,” arXiv preprint arXiv:2403.05527, 2024. [13] Y . Sheng, L. Zheng, B. Yuan, Z. Li, M. Ryabinin, B. Chen, P. Liang, C. Ré, I. Stoica, and C. Zhang, “Flexgen: High-throughput generative inference of large language models with a single gpu,” in ICML, pp. 31094–31116, PMLR, 2023. [14] Z. Zhang, Y . Sheng, T. Zhou, T. Chen, L. Zheng, R. Cai, Z. Song, Y . Tian, C. Ré, C. Barrett, et al., “H2o: Heavy-hitter oracle for efficient generative inference of large language models,” in NeurIPS, vol. 36, 2024. [15] G. Xiao, Y . Tian, B. Chen, S. Han, and M. Lewis, “Efficient streaming language models with attention sinks,” in ICLR, 2024. [16] S. Ge, Y . Zhang, L. Liu, M. Zhang, J. Han, and J. Gao, “Model tells you what to discard: Adaptive kv cache compression for llms,” ICLR, 2024. [17] D. Raposo, S. Ritter, B. Richards, T. Lillicrap, P. C. Humphreys, and A. Santoro, “Mixture-of- depths: Dynamically allocating compute in transformer-based language models,” arXiv preprint arXiv:2404.02258, 2024. [18] W. Zhou, C. Xu, T. Ge, J. McAuley, K. Xu, and F. Wei, “Bert loses patience: Fast and robust inference with early exit,” in NeurIPS, vol. 33, pp. 18330–18341, 2020. [19] M. Elhoushi, A. Shrivastava, D. Liskovich, B. Hosmer, B. Wasti, L. Lai, A. Mahmoud, B. Acun, S. Agarwal, A. Roman, et al., “Layer skip: Enabling early exit inference and self-speculative decoding,” arXiv preprint arXiv:2404.16710, 2024. [20] A. Gromov, K. Tirumala, H. Shapourian, P. Glorioso, and D. A. Roberts, “The unreasonable ineffectiveness of the deeper layers,”arXiv preprint arXiv:2403.17887, 2024. [21] T. Salimans and D. P. Kingma, “Weight normalization: A simple reparameterization to accelerate training of deep neural networks,” in NeurIPS, vol. 29, 2016. [22] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand, et al., “Mixtral of experts,” arXiv preprint arXiv:2401.04088, 2024. [23] M. Abdin, S. A. Jacobs, A. A. Awan, J. Aneja, A. Awadallah, H. Awadalla, N. Bach, A. Bahree, A. Bakhtiari, H. Behl, et al., “Phi-3 technical report: A highly capable language model locally on your phone,” arXiv preprint arXiv:2404.14219, 2024. [24] M. Roemmele, C. A. Bejan, and A. S. Gordon, “Choice of plausible alternatives: An evaluation of commonsense causal reasoning.,” in AAAI spring symposium: logical formalizations of commonsense reasoning, pp. 90–95, 2011. 11[25] A. Amini, S. Gabriel, S. Lin, R. Koncel-Kedziorski, Y . Choi, and H. Hajishirzi, “Mathqa: Towards interpretable math word problem solving with operation-based formalisms,” inNAACL, pp. 2357–2367, 2019. [26] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal, “Can a suit of armor conduct electricity? a new dataset for open book question answering,” in EMNLP, 2018. [27] Y . Bisk, R. Zellers, R. L. Bras, J. Gao, and Y . Choi, “Piqa: Reasoning about physical common- sense in natural language,” in AAAI, 2020. [28] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman, “Glue: A multi- task benchmark and analysis platform for natural language understanding,” arXiv preprint arXiv:1804.07461, 2018. [29] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y . Choi, “Winogrande: An adversarial winograd schema challenge at scale,” Communications of the ACM, vol. 64, no. 9, pp. 99–106, 2021. [30] S. Narayan, S. B. Cohen, and M. Lapata, “Don’t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization,” arXiv preprint arXiv:1808.08745, 2018. [31] R. Nallapati, B. Zhou, C. Gulcehre, B. Xiang, et al., “Abstractive text summarization using sequence-to-sequence rnns and beyond,” arXiv preprint arXiv:1602.06023, 2016. [32] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, A. Le Noac’h, H. Li, K. McDonell, N. Muennighoff, C. Ociepa, J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron, L. Sutawika, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou, “A framework for few-shot language model evaluation,” 12 2023. [33] Y . Bai, X. Lv, J. Zhang, H. Lyu, J. Tang, Z. Huang, Z. Du, X. Liu, A. Zeng, L. Hou, et al., “Longbench: A bilingual, multitask benchmark for long context understanding,” arXiv preprint arXiv:2308.14508, 2023. [34] L. Del Corro, A. Del Giorno, S. Agarwal, B. Yu, A. Awadallah, and S. Mukherjee, “Skipdecode: Autoregressive skip decoding with batching and caching for efficient llm inference,” arXiv preprint arXiv:2307.02628, 2023. [35] T. Schuster, A. Fisch, J. Gupta, M. Dehghani, D. Bahri, V . Tran, Y . Tay, and D. Metzler, “Confident adaptive language modeling,” in NeurIPS, vol. 35, pp. 17456–17472, 2022. [36] H. Wu and K. Tu, “Layer-condensed kv cache for efficient inference of large language models,” 2024. [37] W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity,” Journal of Machine Learning Research, vol. 23, no. 120, pp. 1–39, 2022. [38] D. Lepikhin, H. Lee, Y . Xu, D. Chen, O. Firat, Y . Huang, M. Krikun, N. Shazeer, and Z. Chen, “Gshard: Scaling giant models with conditional computation and automatic sharding,” arXiv preprint arXiv:2006.16668, 2020. [39] D. Dai, C. Deng, C. Zhao, R. Xu, H. Gao, D. Chen, J. Li, W. Zeng, X. Yu, Y . Wu, et al., “Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models,” arXiv preprint arXiv:2401.06066, 2024. [40] C. Hwang, W. Cui, Y . Xiong, Z. Yang, Z. Liu, H. Hu, Z. Wang, R. Salas, J. Jose, P. Ram,et al., “Tutel: Adaptive mixture-of-experts at scale,” Proceedings of Machine Learning and Systems, vol. 5, 2023. [41] DeepSeek-AI, “Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model,” arXiv preprint arXiv:2405.04434, 2024. [42] J. Ainslie, J. Lee-Thorp, M. de Jong, Y . Zemlyanskiy, F. Lebrón, and S. Sanghai, “Gqa: Training generalized multi-query transformer models from multi-head checkpoints,” arXiv preprint arXiv:2305.13245, 2023. 12[43] N. Shazeer, “Fast transformer decoding: One write-head is all you need,” arXiv preprint arXiv:1911.02150, 2019. [44] K. Choromanski, V . Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, et al., “Rethinking attention with performers,” arXiv preprint arXiv:2009.14794, 2020. [45] H. Peng, N. Pappas, D. Yogatama, R. Schwartz, N. A. Smith, and L. Kong, “Random feature attention,” in ICLR, 2021. [46] T. Dao, D. Fu, S. Ermon, A. Rudra, and C. Ré, “Flashattention: Fast and memory-efficient exact attention with io-awareness,” in NeurIPS, vol. 35, pp. 16344–16359, 2022. [47] T. Dao, “Flashattention-2: Faster attention with better parallelism and work partitioning,” arXiv preprint arXiv:2307.08691, 2023. [48] S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma, “Linformer: Self-attention with linear complexity,”arXiv preprint arXiv:2006.04768, 2020. [49] X. Ma, X. Kong, S. Wang, C. Zhou, J. May, H. Ma, and L. Zettlemoyer, “Luna: Linear unified nested attention,” in NeurIPS, vol. 34, pp. 2441–2453, 2021. [50] J. Lee, Y . Lee, J. Kim, A. Kosiorek, S. Choi, and Y . W. Teh, “Set transformer: A framework for attention-based permutation-invariant neural networks,” in ICML, pp. 3744–3753, PMLR, 2019. [51] J. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and S. Han, “Awq: Activation-aware weight quantization for llm compression and acceleration,” arXiv preprint arXiv:2306.00978, 2023. [52] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “Qlora: Efficient finetuning of quantized llms,” in NeurIPS, vol. 36, 2023. [53] T. Dettmers, M. Lewis, Y . Belkada, and L. Zettlemoyer, “Llm.int8(): 8-bit matrix multiplication for transformers at scale,” in NeurIPS, vol. 35, pp. 30318–30332, 2022. [54] J. Liu, R. Gong, X. Wei, Z. Dong, J. Cai, and B. Zhuang, “Qllm: Accurate and efficient low-bitwidth quantization for large language models,” in ICLR, 2024. [55] M. Zhang, C. Shen, Z. Yang, L. Ou, X. Yu, B. Zhuang, et al., “Loraprune: Pruning meets low-rank parameter-efficient fine-tuning,” inACL findings, 2024. [56] N. Kitaev, Ł. Kaiser, and A. Levskaya, “Reformer: The efficient transformer,”arXiv preprint arXiv:2001.04451, 2020. [57] S. K. Ainsworth, J. Hayase, and S. Srinivasa, “Git re-basin: Merging models modulo permutation symmetries,” in ICLR, 2023. [58] R. Entezari, H. Sedghi, O. Saukh, and B. Neyshabur, “The role of permutation invariance in linear mode connectivity of neural networks,” arXiv preprint arXiv:2110.06296, 2021. [59] T. Garipov, P. Izmailov, D. Podoprikhin, D. P. Vetrov, and A. G. Wilson, “Loss surfaces, mode connectivity, and fast ensembling of dnns,” inNeurIPS, vol. 31, 2018. [60] M. Wortsman, G. Ilharco, S. Y . Gadre, R. Roelofs, R. Gontijo-Lopes, A. S. Morcos, H. Namkoong, A. Farhadi, Y . Carmon, S. Kornblith,et al., “Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time,” in ICML, pp. 23965–23998, PMLR, 2022. [61] P. Yadav, D. Tam, L. Choshen, C. A. Raffel, and M. Bansal, “Ties-merging: Resolving interfer- ence when merging models,” in NeurIPS, vol. 36, 2023. [62] M. Davari and E. Belilovsky, “Model breadcrumbs: Scaling multi-task model merging with sparse masks,” arXiv preprint arXiv:2312.06795, 2023. [63] L. Yu, B. Yu, H. Yu, F. Huang, and Y . Li, “Language models are super mario: Absorbing abilities from homologous models as a free lunch,” arXiv preprint arXiv:2311.03099, 2023. 13[64] K. Shoemake, “Animating rotation with quaternion curves,” inProceedings of the 12th annual conference on Computer graphics and interactive techniques, pp. 245–254, 1985. [65] M. S. Matena and C. A. Raffel, “Merging models with fisher-weighted averaging,” inNeurIPS, vol. 35, pp. 17703–17716, 2022. [66] X. Jin, X. Ren, D. Preotiuc-Pietro, and P. Cheng, “Dataless knowledge fusion by merging weights of language models,” arXiv preprint arXiv:2212.09849, 2022. [67] Y . Chen, X. Pan, Y . Li, B. Ding, and J. Zhou, “Ee-llm: Large-scale training and inference of early-exit large language models with 3d parallelism,” arXiv preprint arXiv:2312.04916, 2023. [68] S. Reddy, D. Chen, and C. D. Manning, “Coqa: A conversational question answering challenge,” Transactions of the Association for Computational Linguistics, vol. 7, pp. 249–266, 2019. [69] S. Lin, J. Hilton, and O. Evans, “Truthfulqa: Measuring how models mimic human falsehoods,” arXiv preprint arXiv:2109.07958, 2021. [70] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han, “Smoothquant: Accurate and efficient post-training quantization for large language models,” in International Conference on Machine Learning, pp. 38087–38099, PMLR, 2023. [71] J. Liu, R. Gong, X. Wei, Z. Dong, J. Cai, and B. Zhuang, “Qllm: Accurate and efficient low-bitwidth quantization for large language models,” in ICLR, 2024. [72] S.-Y . Liu, C.-Y . Wang, H. Yin, P. Molchanov, Y .-C. F. Wang, K.-T. Cheng, and M.-H. Chen, “Dora: Weight-decomposed low-rank adaptation,” arXiv preprint arXiv:2402.09353, 2024. [73] M. Nagel, R. A. Amjad, M. Van Baalen, C. Louizos, and T. Blankevoort, “Up or down? adaptive rounding for post-training quantization,” in International Conference on Machine Learning, pp. 7197–7206, PMLR, 2020. [74] W. Kwon, Z. Li, S. Zhuang, Y . Sheng, L. Zheng, C. H. Yu, J. Gonzalez, H. Zhang, and I. Stoica, “Efficient memory management for large language model serving with pagedattention,” in Proceedings of the 29th Symposium on Operating Systems Principles, pp. 611–626, 2023. [75] D. H. Eberly, “Quaternion algebra and calculus,” 2002. [76] “Stanford crfm.” https://crfm.stanford.edu/2023/10/12/flashdecoding.html, 2024. Accessed: 2024-05-04. [77] Y . Liu, H. Li, K. Du, J. Yao, Y . Cheng, Y . Huang, S. Lu, M. Maire, H. Hoffmann, A. Holtzman, et al. , “Cachegen: Fast context loading for language model applications,” arXiv preprint arXiv:2310.07240, 2023. [78] L. Chen, H. Zhao, T. Liu, S. Bai, J. Lin, C. Zhou, and B. Chang, “An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models,” arXiv preprint arXiv:2403.06764, 2024. [79] S. Wei, T. Ye, S. Zhang, Y . Tang, and J. Liang, “Joint token pruning and squeezing towards more aggressive compression of vision transformers,” in CVPR, pp. 2092–2101, 2023. [80] L. Gao, J. Tow, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, K. McDonell, N. Muennighoff, J. Phang, L. Reynolds, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou, “A framework for few-shot language model evaluation,” 2021. [81] W. Kwon, Z. Li, S. Zhuang, Y . Sheng, L. Zheng, C. H. Yu, J. Gonzalez, H. Zhang, and I. Stoica, “Efficient memory management for large language model serving with pagedattention,” in Proceedings of the 29th Symposium on Operating Systems Principles, pp. 611–626, 2023. 14Appendix A Additional Experiment Results Comparisons with token sparsity methods. We also compare MiniCache with the sparsity-based method H2O [14]. Our method outperforms H2O in most tasks on LongBench. Notably, our approach focuses on reducing inter-layer redundancy, whereas H2O focuses on intra-layer redundancy. Our approaches are orthogonal to sparsity-based methods. Table A: Comparison between MiniCache with token sparsity based method H2O, using mistral-7B- instruct on LongBench dataset. MethodsNrtvQA Qasper MF-en HotpotQA 2WikiMQA Musique GovReport QMSum MultiNews TREC TriviaQA SAMSum PCount PRe Lcc Baseline 26.82 33.06 49.28 42.77 27.33 19.27 32.85 24.25 27.06 71.0 86.23 42.98 2.75 86.98 55.51H2O[14] 22.61 29.06 47.22 36.54 20.6 16.25 30.0 23.8 26.75 70.5 86.16 42.97 3.46 86.38 53.72MiniCache27.0432.5949.38 43.91 24.97 18.3 31.46 23.85 26.64 71.0 86.93 43.6 3.04 79.5654.79 B Additional Related Work Preliminaries of KV cache in LLMs. LLM inference has been significantly improved in recent works [9, 13, 74] by optimizing the KV cache management. Overall, this line of research is typically done in two steps: 1) First, at the prefilling stage, LLM calculates the initial KV caches at each attention layer based on the input sequence and decodes the first token. Second, 2) at the decoding stage, LLM autoregressively predicts the next token, where its KV cache at each attention layer is added to the overall caches. Existing works have compressed KV cache in different aspects (e.g., quantization [11, 12], token pruning [14, 16] ). KV cache compression. In the prior study, various strategies for enhancing efficient transformer architectures are discussed, covering a spectrum of techniques aimed at optimizing performance and managing resource constraints. These methods include attention optimization [ 46, 47, 76], grouping queries [42, 43], sparse KV caching [16, 77, 78], shrinking tokens [15, 79], and improving long-context generation. Significant contributions come from projects such as H2O [ 15], GEAR [15], and KIVI [11]. Additionally, efforts to alleviate KV cache bottlenecks include strategies like multi-query attention [42] and multi-group attention [43], which propose reducing the number of heads in the KV cache. However, these methods often necessitate retraining or fine-tuning models. Other approaches focus on diminishing the size of the KV cache by selectively evicting less important tokens [14] and enhancing the system architecture through technologies like offloading the KV cache [80] or integrating techniques such as virtual memory and paging [81] into the attention mechanism. C Discussions and Limitations Alternative merging function. During our preliminary exploration, we initially considered an alter- native, simpler merge function for cross-layer compression: maximum norm-preserving interpolation. This function is designed to maintain the maximum norm of the vectors involved, ensuring that the most significant features are preserved during the merging process. The maximum norm-preserving interpolation in terms of Fmax can be defined as follows: Fmax(xl, xl−1) = ¯xl,l−1 ∥¯xl,l−1∥ · Max(∥xl∥, ∥xl−1∥). (A) Here ¯xl,l−1 represents the average vector between xl and xl−1. The function Fmax ensures that the merged vector preserves the direction of the average vector while scaling it to the maximum norm of the original KV states. Compared to the SLERP-based merge function, Fmax has less computational overhead and lower memory consumption. However, it is less accurate than SLERP. The choice between using FSLERP or Fmax depends on the specific requirements of the application. In our study, we primarily use SLERP to maximize performance. Societal impact. Our work shows a preliminary exploration of KV cache Compression in the depth dimension, a relatively unexplored yet critically bottlenecked area in large language models 15(LLMs). The proposed MiniCache provides a solution to improve the efficiency of LLM generation and is adaptable to existing intra-layer-based KV cache pruning and quantization technologies. Additionally, we proposed a simple yet effective approach that merges similar KV cache states in a cross-layer manner and effectively restores performance through a novel restoration technique. Our observations indicate that cross-layer similarity and mergeability can be applied in broad-efficient applications for post-training optimization in low-resource scenarios, such as deployment on mobile devices. Moreover, with effective optimization of KV cache memory demand, MiniCache further enhances long-context generation, which is a crucial paradigm for real-world applications, such as understanding concepts in textbooks. We aim for our work to advance the boundaries of two key challenges in the LLM industry and research: batch inference and long-context generation. Furthermore, in addition to the significant contributions of MiniCache for KV cache Compression, several challenges remain that are common to LLMs. Issues such as the truthfulness and security of LLMs are still unresolved. Ensuring the accuracy and reliability of generated content is critical, as LLMs can sometimes produce plausible but incorrect or misleading information. Additionally, safeguarding against security vulnerabilities, such as adversarial attacks or data leakage, is paramount to maintaining the integrity and confidentiality of user interactions. Addressing these challenges requires ongoing research and development to enhance the robustness and trustworthiness of LLMs. This effort must proceed alongside advancements in computational efficiency and performance, as exemplified by innovations like MiniCache. Limitations. The current merging algorithm based on Spherical Linear Interpolation (SLERP) has its limitations. SLERP is suitable only for merging two vectors and uses an interpolation approach that restricts our algorithm from merging multiple layers simultaneously and maximizing the compression ratio in further states. This limitation impacts the overall efficiency of KV cache compression and underscores the need for advanced techniques capable of handling more complex merging scenarios. Future research should focus on developing more sophisticated algorithms that can overcome these constraints, thereby enhancing the compression capabilities and overall performance of LLMs. D Additional Implementation Details Overview of the inference algorithm. The MiniCache inference implementation, as shown in Alg. 1, involves several key steps. When the interface starts, In the prefilling phase, we define the merging starting layer S. Before reaching layer S, the inference uses the original attention and cache logic. From layer S onward, we implement our merging algorithm, which operates in a cross-layer manner and is applied only at odd-numbered layers. During merging, we fetch the KV cache from the previous layer and save the merged shared states into the current layer’s KV cache. To reduce memory usage, we then remove the cache of the previous layer. Additionally, we store the magnitudes vector and retention-sensitive tokens for each layer. During the decoding phase, two scenarios arise after layer S. For even-numbered layers (first round), since the KV cache has been removed during the prefilling phase, we refer to the next layer (l + 1) to fetch the shared KV cache states. We then perform approximated scale restoration and retention token recovery. The new KV states from this phase are stored for use in the next round. In the second round, which involves odd-numbered layers, we use the new KV tokens from both the previous and current layers. After the restoration phase, we perform the merge operations and update the shared KV cache states in the stack. Cross-Layer merging and restoration algorithm. As outlined in Alg. 2, MiniCache algorithm involves several crucial steps to ensure efficient memory usage while maintaining the integrity of the Key-Value (KV) Cache states. Initially, given the KV cacheEl k,v, norm values ∥Xl k,v∥ unmerged tokens Rl k,v, retention indices Ik,v, and the next tokens tl, tl−1, the algorithm proceeds by rescaling the magnitude of the KV pairs. Specifically, ˆXl k and ˆXl v are computed by multiplying the normalized KV pairs El k,v with their respective magnitude norms ∥Xl k,v∥. Following this, the algorithm restores unmerged tokens using the retention indices, updating ˆXl k and ˆXl v accordingly. Next, the new tokens tk and tv are concatenated to the rescaled KV pairs along the token dimension. This augmented KV cache undergoes a softmax attention mechanism where the attention scores A are computed by taking the dot product of the query token tq with the transposed keys ( ˆXl k)⊤. The output token tO is then obtained by multiplying the attention scores A with the values ˆXl v. In cases where the previous token tl−1 exists, the algorithm performs a compression step. It concatenates the existing KV cache 16El k,v with the merged tokens resulting from the current and previous layers, effectively reducing redundancy and optimizing memory. If tl−1 is not available, the KV cache is updated by simply concatenating El k,v with the new tokens tl k,v, deferring the compression until the next iteration. The final output token tO is then returned, concluding the decoding process. In the merging function, the algorithm normalizes the KV pairs from both the current and previous layers. It calculates the angular distance Ω between the normalized vectors, ensuring that the interpolation occurs along the shortest path on the unit sphere. The merged KV cache is then obtained by weighted interpolation of the normalized vectors, preserving the geometric and semantic integrity of the original states. This comprehensive process allows MiniCache to achieve substantial memory efficiencies while maintaining the functional characteristics of the KV pairs across transformer layers. Layer𝑙 QK V Attention Cross-Layer Merging K V KVcachecompressionat layer𝑙 Layer𝑙−1  ... 𝜒! 𝜒!\"# 𝐾,𝑉 C 1 Keep Fetch 2Merge 3 Cache 4Delete Recovery Rescale Retention Recovery 5Fetch Contact New Token QK V Recovery Attention...... 6Fetch Cross Layer Merging Time 7 Prefilling Decoding Figure A: Overall prefilling and decoding logic for MiniCache involves performing cross-layer merging and recovery within our framework. MiniCache execution flow. Figure A delineates the pre-filling and decoding logic for the MiniCache framework, which incorporates cross-layer merging and error suppression to achieve memory effi- ciency and maintain functional integrity. Initially, in Step 1, the KV cache is fetched from the previous layer (Layer L−1) during the pre-filling phase. In Step 2, the fetched KV pairs from the current layer χL are merged with the KV pairs from the preceding layer χL−1, reducing redundancy through a 17merge function. Subsequently, in Step 3, the merged KV pairs are cached for future use, representing a consolidated data set from multiple layers. During the decoding phase, Step 4 involves deleting unnecessary or redundant KV pairs to optimize memory usage. In Step 5, the decoder fetches the required KV pairs from the cache for output generation. Step 6 applies error suppression mechanisms to the fetched KV pairs, including rescaling and retention recovery, to minimize errors introduced during the merging and compression processes. Finally, in Step 7, the cache is updated with the final KV pairs post-error suppression and adjustments, ensuring the cache contains the most accurate and efficient representation of the KV pairs for subsequent layers. This comprehensive approach guarantees substantial memory efficiencies while preserving the critical functional characteristics of the original KV pairs across transformer layers. Algorithm 1: The MiniCache Inference Algorithm 1 procedure MiniCache Inference: Input: Input Tokens: T ∈ Rtinput×d, number of layers L, merging beginning layer S Output: Output Tokens: O ∈ Rtoutput×d 2 for l ← 0 to S − 1 do 3 procedure Standard Prefill: 4 Standard Attention & Standard Cache 5 procedure Standard Decoding: 6 Standard Attention & Standard Cache // Start Merging from layer S 7 for l ← S to L do // Perform Merging in every two layers l%2 == 1 8 if l%2 == 1 and prefilling then 9 procedure MiniCache Prefill: Input: KV cache from Current layer l: Xl k,v, KV cache from Previous layer l − 1: Xl−1 k,v , token retention threshold: γ 10 Delete KV cache of the l − 1-th layer // layer l, l− 1 shares one Cache 11 Standard Attention & Standard Cache // Perform Merging in the second layer 12 if and decoding then 13 if l%2 == 0 then // first round in the cross-layer merging, fetch shared KV cache states from Cl+1 k,v 14 procedure MiniCache Decoding: Input: KV cache: Cl+1 k,v , Norm: ∥Xl k,v∥, Unmerged Tokens: Rl k,v, Retention indices: Ik,v, Next Token: tl 15 else // second round in cross-layer merging, while tl−1 exist 16 procedure MiniCache Decoding: Input: KV cache: Cl k,v, Norm: ∥Xl k,v∥, Unmerged Tokens: Rl k,v, Retention indices: Ik,v, Next Token: tl, tl−1 17 return O 18Algorithm 2: The MiniCache Prefill & Decoding Compression Algorithm Hyperparameter: number of layers L, merging beginning layer S 1 procedure MiniCache Prefill: Input: KV cache from current layer l: Xl k,v ∈ R2×tprompt×d, KV cache from previous layer l − 1: Xl−1 k,v ∈ R2×tprompt×d, token retention threshold: γ 2 El k, ∥Xl k∥, ∥Xl−1 k ∥, Ωk ← Merge(Xl k, Xl−1 k ) ; 3 El v, ∥Xl v∥, ∥Xl−1 v ∥, Ωv ← Merge(Xl v, Xl−1 v ) ; 4 El k,v ∈ Rtprompt×d | {z } compression output , ∥Xl,l−1 k,v ∥ ∈R4×tprompt×1 | {z } norms for rescaling ; 5 d(Xl, Xl−1)k,v = 1 π · Ωk,v // distance metrics 6 Ik,v = {i | di < dmin + (dmax − dmin) · γ} // retention indices 7 Rl,l−1 k ← Xl,l−1 k [Ik], Rl,l−1 v ← Xl,l−1 v [Iv] // unmerged tokens 8 return El k,v, ∥Xl,l−1 k,v ∥, Rl,l−1 k , Rl,l−1 v , Ik,v 9 procedure MiniCache Decoding: Input: KV cache: El k,v ∈ R2×tprompt×d, Norm: ∥Xl k,v∥ ∈R2×tprompt×1, Unmerged Tokens: Rl k,v ∈ R2×γ·tprompt×d, Retention indices: Ik,v ∈ R2×γ·tprompt×1, Next Token: tl ∈ R1×d, tl−1 ∈ R1×d 10 ˆXl k ← El k · ∥Xl k∥ ∥El k∥ ˆXl v ← El v · ∥Xl v∥ ∥Elv∥ // magnitude rescale 11 ˆXl k[Ik] = Rl k ˆXl v[Iv] = Rl v // token restoration 12 ˆXl k ← Concat( ˆXl k, tk, dim=token) ˆXl v ← Concat( ˆXl v, tv, dim=token) A ← Softmax(tq · ( ˆXl k)⊤) tO ← A · ˆXl v if tl−1 exists then 13 KV cache ← Concat(El k,v, Merge(tl k,v, tl−1 k,v ), dim=token) // perform compression 14 else 15 KV cache ← Concat(El k,v, tl k,v, dim=token) // wait for compression 16 return tO 17 function MiniCache Merge(Xl, Xl−1, t): 18 ⃗Xl ← Xl ∥Xl∥ 19 ⃗Xl−1 ← Xl−1 ∥Xl−1∥ 20 Ω ← arccos \u0010 Xl T ·Xl−1 T ∥Xl T ∥∥Xl−1 T ∥ \u0011 21 E ← sin((1−t)Ω) sin(Ω) ⃗X l + sin(tΩ) sin(Ω) ⃗X l−1 22 return E, ∥Xl∥, ∥Xl−1∥, Ω E Detailed Efficiency Derivation In this section, we provide a detailed derivation of the memory efficiency improvements outlined in Section 4.3. First, we consider the original KV cache memory usage, which is given by: 4brh(s + n). Here, r is the number of layers, b is the batch size, h is the hidden size, s is the input sequence length, and n is the output sequence length. To improve efficiency, we begin merging layers starting from the midpoint, S = 1 2 r, by consolidating the KV cache states of every two layers into a single shared state space. The memory usage derivation proceeds as follows: for the unmerged part of the cache (from layer 1 to S): 194brh(s + n) · 1 2 = 2brh(s + n). For the merged part of the cache (from layer S + 1 to r): 4brh(s + n) · 1 2 · 1 2 = brh(s + n). Combining these two parts, the total memory usage is: 2brh(s + n) + brh(s + n) = 3brh(s + n). Next, we consider the additional memory cost incurred during the restoration process. During this phase, we save additional normalized vectors for the layers in the KV cache. These vectors are of shape Rb×s×1, which means they have a single channel dimension compared to the fully ranked original KV states. The additional normalized vectors for layers from S onwards are given by: br(s + n) · 2 = 2br(s + n). We also introduce a retention threshold, which we set to 0.05. This means that 5% of the KV cache tokens are retained without compression: brh(0.05(s + n)). Combining these terms, the total additional memory for the restoration process is: 2br(s + n) + 0.1brh(s + n). Finally, summing the compressed memory usage and the restoration memory cost, the overall memory requirement is: 3brh(s + n) + 2br(s + n) + 0.1brh(s + n). This can be simplified by grouping the common factors: br(s + n) (3h + 2 + 0.1h) . Simplifying the expression inside the parentheses, we get: br(s + n) (3.1h + 2). Therefore, the total memory cost for the KV cache in the MiniCache Framework is: br(s + n)(3.1h + 2). This detailed derivation confirms the efficiency improvements discussed in Section 4.3, highlighting the significant reduction in memory usage achieved through our layer merging and restoration strategies. F Detailed Experiment Results 20Table B: Detailed performance comparison on GSM8K dataset with LLaMA-3-70B. Layer LLaMA-3-70B MiniCache LLaMA-3-70B Mean 0 0.799 0.729 5 0.800 0.640 10 0.792 0.578 15 0.815 0.545 20 0.801 0.560 25 0.812 0.544 30 0.799 0.556 35 0.810 0.557 40 0.790 0.551 45 0.725 0.539 50 0.638 0.541 55 0.638 0.501 60 0.625 0.497 65 0.635 0.511 70 0.623 0.497 75 0.615 0.493 Table C: Detailed performance comparison on COQA dataset with LLaMA-3-70B. Layer LLaMA-3-70B MiniCache LLaMA-3-70B Mean 0 0.705 0.706 4 0.705 0.699 8 0.706 0.696 12 0.704 0.691 16 0.704 0.690 20 0.703 0.690 24 0.701 0.690 28 0.702 0.690 32 0.702 0.688 36 0.703 0.688 40 0.697 0.687 44 0.698 0.685 48 0.699 0.678 52 0.699 0.672 56 0.701 0.668 60 0.704 0.657 64 0.706 0.635 68 0.691 0.611 72 0.689 0.565 76 0.641 0.526 21Table D: Detailed performance comparison on TruthfulQA dataset with LLaMA-3-70B. Layer LLaMA-3-70B MiniCache LLaMA-3-70B Mean 0 22.615 22.130 4 22.512 22.005 8 22.451 21.876 12 22.413 21.303 16 22.387 21.209 20 22.387 20.752 24 22.387 20.657 28 22.276 20.501 32 22.130 20.479 36 22.130 20.335 40 22.073 19.834 44 21.356 17.024 48 21.356 12.440 52 21.333 9.127 56 21.316 3.255 60 21.172 2.349 64 21.153 2.250 68 21.002 1.721 72 20.940 1.119 76 20.683 0.784 Table E: Detailed performance comparison on GSM8K dataset with LLaMA-3-8B. Layer LLaMA-3-8B MiniCache LLaMA-3-8B Mean 0 0.488 0.467 2 0.476 0.369 4 0.489 0.388 6 0.487 0.387 8 0.489 0.359 10 0.479 0.388 12 0.486 0.384 14 0.472 0.368 16 0.477 0.343 18 0.446 0.291 20 0.447 0.271 22 0.433 0.234 24 0.399 0.155 26 0.396 0.140 28 0.395 0.052 30 0.391 0.024 32 0.397 0.025 22Table F: Detailed performance comparison on COQA dataset with LLaMA-3-8B. Layer LLaMA-3-8B MiniCache LLaMA-3-8B Mean 0 0.676 0.676 2 0.676 0.571 4 0.675 0.566 6 0.674 0.564 8 0.674 0.561 10 0.673 0.560 12 0.672 0.560 14 0.670 0.559 16 0.670 0.558 18 0.669 0.555 20 0.669 0.552 22 0.668 0.549 24 0.667 0.543 26 0.667 0.537 28 0.666 0.536 30 0.666 0.531 32 0.665 0.528 Table G: Detailed performance comparison on TruthfulQA dataset with LLaMA-3-8B. Layer LLaMA-3-8B MiniCache LLaMA-3-8B Mean 0 32.520 32.524 2 32.231 28.431 4 31.645 28.197 6 31.485 27.894 8 31.008 27.796 10 30.964 27.704 12 30.798 27.371 14 30.798 27.093 16 30.798 26.643 18 30.798 26.517 20 30.798 26.355 22 30.798 26.011 24 30.798 25.044 26 30.798 15.254 28 30.798 14.791 30 30.765 9.419 32 30.390 6.068 23Table H: Detailed performance comparison on GSM8K dataset with Mixtral-8x7B. Layer Mixtral-8x7B MiniCache Mixtral-8x7B Mean 0 0.589 0.575 2 0.592 0.480 4 0.593 0.491 6 0.591 0.469 8 0.580 0.472 10 0.592 0.492 12 0.582 0.485 14 0.572 0.480 16 0.562 0.462 18 0.522 0.432 20 0.526 0.426 22 0.540 0.416 24 0.519 0.398 26 0.515 0.436 28 0.502 0.401 30 0.515 0.386 32 0.490 0.258 Table I: Detailed performance comparison on COQA dataset with Mixtral-8x7B. Layer Mixtral-8x7B MiniCache Mixtral-8x7B Mean 0 0.672 0.675 2 0.671 0.612 4 0.670 0.601 6 0.672 0.590 8 0.674 0.582 10 0.671 0.571 12 0.674 0.561 14 0.670 0.546 16 0.672 0.544 18 0.672 0.530 20 0.675 0.522 22 0.671 0.512 24 0.660 0.455 26 0.657 0.447 28 0.640 0.440 30 0.634 0.424 32 0.459 0.430 24Table J: Detailed performance comparison on TruthfulQA dataset with Mixtral-8x7B. Layer Mixtral-8x7B MiniCache Mixtral-8x7B Mean 0 21.686 19.465 2 21.385 19.405 4 21.368 19.251 6 21.038 19.094 8 21.038 18.265 10 20.216 17.019 12 20.026 15.902 14 19.723 15.505 16 19.641 15.028 18 19.641 14.723 20 19.546 14.543 22 18.756 14.122 24 18.402 13.834 26 18.366 13.789 28 17.738 12.091 30 16.827 12.008 32 16.635 0.430 Table K: Detailed performance comparison on GSM8K dataset with Phi-3-Mini. Layer Phi-3-Mini MiniCache Phi-3-Mini Mean 0 0.774 0.774 2 0.765 0.667 4 0.757 0.661 6 0.754 0.659 8 0.748 0.657 10 0.750 0.645 12 0.750 0.616 14 0.752 0.575 16 0.739 0.491 18 0.742 0.417 20 0.692 0.272 22 0.685 0.206 24 0.640 0.110 26 0.545 0.061 28 0.500 0.039 30 0.460 0.036 32 0.447 0.028 25Table L: Detailed performance comparison on COQA dataset with Phi-3-Mini. Layer Phi-3-Mini MiniCache Phi-3-Mini Mean 0 0.665 0.665 2 0.662 0.562 4 0.657 0.557 6 0.656 0.556 8 0.656 0.556 10 0.654 0.554 12 0.646 0.546 14 0.648 0.538 16 0.647 0.537 18 0.637 0.527 20 0.627 0.487 22 0.591 0.461 24 0.567 0.437 26 0.548 0.408 28 0.527 0.407 30 0.506 0.406 32 0.503 0.403 Table M: Detailed performance comparison on TruthfulQA dataset with Phi-3-Mini. Layer Phi-3-Mini MiniCache Phi-3-Mini Mean 0 19.686 19.465 2 19.385 19.365 4 19.368 19.221 6 19.100 18.255 8 19.038 17.019 10 19.500 15.912 12 19.216 15.525 14 20.026 15.195 16 19.641 15.058 18 18.756 14.763 20 17.738 14.593 22 19.546 14.182 24 19.723 13.954 26 18.366 13.919 28 18.402 12.231 30 16.827 12.158 32 16.635 10.333 26",
      "meta_data": {
        "arxiv_id": "2405.14366v2",
        "authors": [
          "Akide Liu",
          "Jing Liu",
          "Zizheng Pan",
          "Yefei He",
          "Gholamreza Haffari",
          "Bohan Zhuang"
        ],
        "published_date": "2024-05-23T09:43:52Z",
        "pdf_url": "https://arxiv.org/pdf/2405.14366v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper introduces MiniCache, a novel, training-free, and general framework for KV cache compression in Large Language Models (LLMs) along the depth dimension. It addresses the significant memory bottleneck caused by KV cache size growing linearly with sequence length. The key observation is the high similarity of KV cache states between adjacent layers in the middle-to-deep portions of LLMs. MiniCache significantly reduces the memory footprint and enhances inference throughput with near-lossless performance, complementing existing compression methods. For instance, a 4-bit MiniCache on LLaMA-2-7B achieves up to 5.02x compression, 5x higher inference throughput, and 41% memory reduction compared to the FP16 full cache baseline.",
        "methodology": "MiniCache employs a cross-layer KV cache compression method with two main components. Firstly, it uses a reparameterization-based cache merging strategy where state vectors are decomposed into magnitude and direction components, inspired by weight normalization. Spherical Linear Interpolation (SLERP) is used to effectively interpolate the directional components of KV pairs from adjacent layers (starting from the middle layer S=L/2), while preserving their original state norms. The merged cache stores the directional vector, token-wise magnitudes, and the angle between vectors. Secondly, a token retention strategy is introduced to minimize performance degradation by selectively retaining 'unmergeable' token pairs that exhibit low similarity (high angular distance). These outlier tokens, identified by a predefined threshold (γ), are stored separately with minimal additional memory overhead, ensuring crucial information is not lost during compression. Cache restoration involves rescaling directional shared states with their corresponding magnitude vectors and recovering retained sensitive tokens based on their indices.",
        "experimental_setup": "MiniCache was evaluated on various LLMs including LLaMA-2-7B, LLaMA-2-13B, LLaMA-3 (8B, 70B), Phi-3-Mini, Mistral-7B, Mistral-7B-Instruct, and Mixtral-8x7B. Benchmarks included a diverse range of question answering and generation datasets from `lm-eval-harness` (COPA, MathQA, OpenBookQA, PIQA, RTE, WinoGrande, XSUM, CNN/Daily Mail, GSM8K, COQA, TruthfulQA) and long-sequence generation tasks on LongBench. The method was compared against a fully cached FP16 baseline, as well as existing compression techniques like round-to-nearest quantization (RTN), SmoothQuant, and KIVI (2-bit and 4-bit). The interpolation parameter 't' for SLERP was set to 0.6, and the token retention threshold 'γ' was set to 0.05. Experiments utilized NVIDIA 4 A100 80GB GPUs for model loading and a single 80GB NVIDIA A100 GPU for efficiency analysis using synthetic ShareGPT workloads with an average input prompt length of 161 tokens and output length of 338 tokens.",
        "limitations": "The current merging algorithm based on Spherical Linear Interpolation (SLERP) is limited to merging only two vectors at a time. This restriction prevents the algorithm from simultaneously merging multiple layers, thus impacting the maximization of the compression ratio. This constraint necessitates the development of more advanced techniques to handle complex multi-layer merging scenarios for enhanced compression capabilities.",
        "future_research_directions": "Future work will focus on several key areas: enhancing the compression ratio by exploring cross-multiple-layer merging beyond just adjacent layers; developing more advanced merging algorithms, such as Spherical Cubic Interpolation, to overcome the limitations of SLERP; and further optimizing memory usage for large-scale LLM deployments in diverse application scenarios. Additionally, there is potential to dynamically determine the interpolation parameter 't' in SLERP based on the relative magnitude ratio of adjacent layers to allow for more flexible weight control in layer-wise operations."
      }
    },
    {
      "title": "Bifurcated Attention for Single-Context Large-Batch Sampling",
      "abstract": "This study introduces bifurcated attention, a method designed to enhance\nlanguage model inference in shared-context batch decoding scenarios. Our\napproach addresses the challenge of redundant memory IO costs, a critical\nfactor contributing to latency in high batch sizes and extended context\nlengths. Bifurcated attention achieves this by strategically dividing the\nattention mechanism during incremental decoding into two separate GEMM\noperations: one focusing on the KV cache from prefill, and another on the\ndecoding process itself. While maintaining the computational load (FLOPs) of\nstandard attention mechanisms, bifurcated attention ensures precise computation\nwith significantly reduced memory IO. Our empirical results show over\n2.1$\\times$ speedup when sampling 16 output sequences and more than 6.2$\\times$\nspeedup when sampling 32 sequences at context lengths exceeding 8k tokens on a\n7B model that uses multi-head attention. The efficiency gains from bifurcated\nattention translate into lower latency, making it particularly suitable for\nreal-time applications. For instance, it enables massively parallel answer\ngeneration without substantially increasing latency, thus enhancing performance\nwhen integrated with post-processing techniques such as re-ranking.",
      "full_text": "Bifurcated Attention: Accelerating Massively Parallel Decoding with Shared Prefixes in LLMs Ben Athiwaratkun * 1 Sujan Kumar Gonugondla * 2 Sanjay Krishna Gouda 2 Haifeng Qian 2 Hantian Ding 2 Qing Sun 2 Jun Wang2 Jiacheng Guo 2 Liangfu Chen 2 Parminder Bhatia 3 Ramesh Nallapati 4 Sudipta Sengupta 2 Bing Xiang 5 Abstract This study introduces bifurcated attention, a method designed to enhance language model in- ference in shared-context batch decoding sce- narios. Our approach addresses the challenge of redundant memory IO costs, a critical factor contributing to latency in high batch sizes and extended context lengths. Bifurcated attention achieves this by strategically dividing the atten- tion mechanism during incremental decoding into two separate GEMM operations: one focusing on the KV cache from prefill, and another on the decoding process itself. While maintaining the computational load (FLOPs) of standard attention mechanisms, bifurcated attention ensures precise computation with significantly reduced memory IO. Our empirical results show over 2.1× speedup when sampling 16 output sequences and more than 6.2× speedup when sampling 32 sequences at context lengths exceeding 8k tokens on a 7B model that uses multi-head attention. The effi- ciency gains from bifurcated attention translate into lower latency, making it particularly suitable for real-time applications. For instance, it enables massively parallel answer generation without sub- stantially increasing latency, thus enhancing per- formance when integrated with post-processing techniques such as re-ranking. *Equal contribution 1Together.ai (work conducted at AWS) 2AWS NGDE Science 3GE HealthCare (work conducted at AWS) 4Amazon AGI (work conducted at AWS)5Goldman Sachs (work conducted at AWS). Correspondence to: Ben Athiwaratkun <ben.athiwaratkun@gmail.com>, Sujan Kumar Gonugondla <gsu- jan@amazon.com>. Proceedings of the41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). 1. Introduction The advent of large language models (LLMs) has ushered in a new era of machine learning, exhibiting remarkable performance on a wide array of tasks (Brown et al., 2020; OpenAI, 2023; Chowdhery et al., 2022; Touvron et al., 2023; Chen et al., 2021; Hoffmann et al., 2022; Li et al., 2022; Microsoft; Amazon; Nijkamp et al., 2023). Despite their impressive capabilities, the deployment of these large-scale models in practical applications poses significant challenges, particularly in terms of inference latency and efficiency. Enhancing these aspects is critical, as they directly influence the computational resources required to generate predictions and enable the practical implementation of these advanced models across various industries. A particularly demanding inference scenario is single- context batch sampling, where the goal is to generate mul- tiple completions from a single context. This task is com- monly encountered in numerous applications such as code- editing IDE tools that provide multiple recommendations, or in cases where ranking among many generations is needed for optimal performance (via ranking metrics like mean log probability, majority voting, etc). The incremental decoding of such sampling scenario is memory IO intensive, which becomes a latency bottleneck for high batches and context lengths. In this study, we investigate two compatible strategies to address the memory IO challenges in tranformers inference: (1) an investigation of multi-query and its trade-offs, and (2) a novel technique called context-aware bifurcated attention. Our investigation begins with an analysis of the general- ized multi-query attention (Ainslie et al., 2023), which includes multi-query (Shazeer, 2019), as well as the es- tablished multi-head attention mechanism (Vaswani et al., 2017) for performance and latency trade-off. Our findings show smooth performance scaling with increasing model size for a fixed value of the number of groups g for general- ized multi-query1. Lowering g results in an upward shift of 1Lower values of attention groupsg lead to higher compression of the key-value tensors, as in the multi-query case where g = 1, 1 arXiv:2403.08845v2  [cs.LG]  11 Jul 2024Bifurcated Attention: Accelerating Massively Parallel Decoding with Shared Prefixes in LLMs the validation loss vs model size scaling curves. The con- sistent relationship between the cache compression, model size and validation loss allows us to trade-off inference ef- ficiency with model size, i.e., enables us to select higher compression for use cases requiring high efficiency, while still matching the performance of multi-head attention by compensating with a larger model size. Secondly, we introduce context-aware bifurcated attention, a technique that bifurcates any attention in the generalized multi-query family into context and decoding components during incremental decoding. Such bifurcation involves the same number of FLOPs and yields identical results com- pared to the original attention, but can significantly reduces memory IO cost and thus latency in high batch and context length scenarios. This approach allows the generation of multiple real-time completions without incurring much ad- ditional latency costs, or enables much higher batch sizes leading to improved ranking performance. For instance, for CodeGen 16B multi-head model (Nijkamp et al., 2022) with 2k context length, we are able to increase the batch size to 128 with bifurcated attention, compared to batch size of only 5 without, resulting in the pass@k (Chen et al., 2021) increasing from 59.0% to 84.6%, or pass@top3 via mean log-p increasing from 55.2% to 58.1%. 2. Related Work In the literature, there are multiple avenues to improve infer- ence latency and/or latency. Quantization reduces memory usage by using low-bitwidth representations such as int8, int4, and fp8 (Wei et al., 2023; Yao et al., 2022; Dettmers et al., 2022; Frantar et al., 2022; Kuzmin et al., 2022; Xiao et al., 2022). Quantization when applied only to model pa- rameters offer diminishing results as with longer sequence lengths and large batch sizes where memory access and compute associated with dot-product attention dominates the overall inference latency. Sparse attention (Beltagy et al., 2020; Child et al., 2019; Zaheer et al., 2020) has been extensively studied as a way to reduce the complexity of attention for longer contexts and faster inference. Pope et al. (2022) investigates gen- erative inference efficiency of large language models by using multi-dimensional partitioning techniques optimized for TPUs (collective einsum) to achieve a Pareto frontier on latency and model FLOPs utilization. The paper also shows that multi-query attention allows scaling up to 32x larger context length with an emphasis on the efficiency under high batch size. Paged attention (Kwon et al., 2023) enhances memory management of the KV cache by dividing it into blocks and employing a block table for mapping purposes. hence improving inference efficiency and latency due to reduced KV cache compared to the multi-head case where g = h, the number of query attention heads. This approach effectively accommodates dynamic workload shifts and reduces memory storage requirements through the sharing of the prompt’s KV cache across multiple output sequences. However, this does not reduce the memory reads of KV cache. Speculative decoding, and its variants uses a smaller draft model to propose multiple sequential tokens, which are pro- cessed in parallel by the main model to accept or reject such tokens (Chen et al., 2023; Leviathan et al., 2022; Li et al., 2024; Cai et al., 2024; Fu et al., 2023). The key idea is to enable decoding of multiple tokens at every step, thereby amortizing the memory IO usages of the main model. How- ever, the latency of decoding will be still dominated by KV cache I/O bandwidth at large context sizes, where bifur- cated attention can enhance the decoding speed further. In short, incremental decoding focuses on lowering the amor- tized memory IO of model loading while multi-query and bifurcated attention lowers the memory IO of KV cache. Additionally, we acknowledge concurrent work by Juravsky et al. (2024) which presents methods to improve inference efficiency with shared-prefixes, that coincides with bifur- cated attention. 3. Background 3.1. Notation We use the following notation throughout the paper. • K: key tensor, V : value tensor, q: query tensor, Px: projection tensor associated with key, value or query tensor. • We denote ⟨A, B⟩ as a tensor operation between A and B. The actual operation can be specified in Einstein sum notation. We use ⊕ to denote concatenation. • N the number of model parameters, d: hidden dimen- sion, h: number of attention heads, k: d h, or head dimension, ℓ: number of layers, m: context length (or key/value tensor length), n: query tensor length where n = m during context encoding and n = 1for incremental decoding, g: number of attention groups (to be explained). We also use v to represent the head dimension for the value tensor where practicallyk = v. 3.2. Language Model Inference There are many inference scenarios for language model, including batch inference and single-context batch sampling (Figure 1). Batch inference refers to the case where we pro- cess multiple inputs together in a batch, and generate subse- quent tokens for each batch index independently. In the case where the batch size is 1, this reduces to the single-context inference. Another scenario is the single-context batch sam- 2Bifurcated Attention: Accelerating Massively Parallel Decoding with Shared Prefixes in LLMs pling where we generates multiple sequences based on a single context, where difference between the batch inference case is that the prefill only needs to be done for a single context to obtain the KV cache, then broadcasted to other batch indices. Figure 1 also illustrates the two phases of language model inference: (a) the context encoding or prefilling and (b) the incremental decoding. The context encoding refers to a single forward pass that computes the key and value tensors for all token positions in the context. Once the key and value tensors are computed, we cache these key and value tensors to be used for the attention mechanism during the incremental decoding phase, which sequentially generates one token at a time2. During the context encoding phase, the number of floating point operations relative to the memory input/output (IO) operations is high, corresponding to the compute-bound regime where the latency is influenced by the FLOPs. How- ever, during incremental decoding where we perform atten- tion on a single query token, this falls into a memory-bound regime where the number of computation per memory ac- cess is roughly 1-to-1 (see Appendix D.1 for details). The memory IO refers to the read and write operations from the high bandwidth memory (HBM) (Jia et al., 2018) to the fast on-chip SRAM where the actual computation happens. The memory IO of the incremental decoding itself consists of two components: (1) the model parameter loading and (2) KV cache loading. Component (1) is constant regardless of the context length m or batch size b where component (2) depends on both m and b and dominate the overall memory IO if m or b are high, which can become a significant bottle- neck for inference. Our work primarily focuses on reducing component (2). 3.3. Multi-Query, Multi-Head and the Generalized Multi-Query Attention Multi-query attention, proposed by Shazeer (2019), is an attention mechanism for transformers models that uses a single head for the key and value tensors, compared to h heads in the traditional multi-head attention (Vaswani et al., 2017). This technique effectively reduces the KV memory IO by h times, which leads to higher inference efficiency during incremental decoding. In effect, the single-head key or value tensor is shared and used to attend to all the multi- head query, hence the name multi-query. This corresponds to a compression in representation power of the key and value tensor, which we will see in the scaling laws study (Section 5.1) that it results in a reduced expressiveness in terms of model parameter efficiency. Such reduced expres- siveness can be compensated by scaling the model bigger 2Or k tokens at a time, in case of speculative decoding (Chen et al., 2023; Leviathan et al., 2022) than the multi-head counterpart to match the representation power. We can also extrapolate these insights to a generalized multi- query attention mechanism (Ainslie et al., 2023), which provides a framework to understand both multi-query and multi-head attention, and everything in between. Here, the degree of KV compression is dictated by the number of attention groups g, where we alternatively refer to the gener- alized multi-query as multi-group. Each attention group can be interpreted as the broadcasted attention between a single head of key or value tensor, and multiple heads of query. In this paradigm, multi-query attention is a special case where the number of groups g = 1; that is, there is exactly one such group. Conversely, multi-head attention is another special case where the number of attention groups matches the number of heads (g = h), in which case each head in the key or value tensor attends to one head in the query. More generally, the number of groupsg can lie anywhere between 1 and h, indicating various degrees of compression. For practical purposes, it is most convenient when g divides h. The attention mechanism in this setting can be expressed in terms of Einstein summation as: logits =⟨q, K⟩ : einsum(bgpnk, bgmk) → bgpnm (1) o = ⟨w, V⟩ : einsum(bgpmn, bgmv) → bgpnv (2) where p = h g represents the attention group size. Other operations in the attention mechanism are analogous, as de- tailed in Appendix D.1. The memory IO complexity for the multi-query attention becomes bgmk compared to bhmk in the multi-head setting, a reduction by a factor of h g times. The FLOPs, however, are bgpnmk = bdnm, independent of the compression g, implying that in the compute-bound scenario of context encoding, the latency would be quite similar among multi-group models of different g’s, includ- ing between g = 1and g = h. This generalized multi-group attention mechanism thus pro- vides a unified perspective on the design space of attention architectures. By adjusting the number of attention groups g, one can flexibly tune these trade-offs, potentially yielding new regimes of performance for transformer models. In Section 5.1, we will look into such capability vs latency trade-off. 4. Context-Aware Bifurcated Attention In this section, we present a novel context-aware bifurcated attention method that aims to reduce the memory IO cost during incremental decoding by efficiently handling the computation of attention for shared context across samples, as shown in Figure 2. 3Bifurcated Attention: Accelerating Massively Parallel Decoding with Shared Prefixes in LLMs input textinput textinput textinput textinput text Model input textModel Compute KV cache for 1 sample and copy (reference) Incremental Decoding Batch Inference Single-Context Batch Sampling Context Encoding Model Model KV cache is computed for each batch index Figure 1: Illustration of the two phases of language model inference: context encoding and incremental decoding, as well as different inference scenarios. In batch inference scenario, we process multiple inputs at once and perform incremental decoding steps. In batch inference, we group multiple inputs in batch to perform both context encoding and the subsequent incremental decoding. In the single-context batch sampling scenario, we perform context encoding on a single input to obtain the context KV cache, then perform incremental decoding (with temperature sampling) to obtain potentially different generations. 4.1. Motivation We observe that the memory IO during the incremental decoding phase can be significantly improved due to the fact that the KV corresponding to the context are shared and can be loaded only once. During incremental decoding, the accumulated key tensor (K) for a multi-head model is of size bhmk = bh(mc + md)k. The two parts of K correspond to Kc of size bhmck and Kd of size bhmdk where mc is length of the original input and md is the length due to previous incremental decoding steps. Since tensor Kc is the same across all indices in the b axis, we can also represent Kc with a more compact shape 1hmck or simply hmck. The query-key attention (Equation 1) is typically performed by accessing different batch indices of K = Kc ⊕ Kd separately, even though all batch indices in Kc correspond to the same attention values. That is, if we “naively” pass the entire tensor to the GEMM/BLAS operators, the incurred memory I/O cost = bhmk, meaning that Kc tensor is loaded b times (Figure 2). Since memory loading of KV is the bottleneck for incremental decoding, reducing such IO can bring significant reductions in latency saving. 4.2. Formulation Below outlines the proposed context-aware bifurcated atten- tion for single-context batch sampling. This operation splits any attention in the multi-group family during incremental decoding into two parts: (1) attention associated with KV cache from the single context ⟨q, Kc⟩ and (2) attention as- sociated with KV cache from prior incremental decoding steps ⟨q, Kd⟩. That is, ⟨q, K⟩ = ⟨q, Kc⟩ ⊕ ⟨q, Kd⟩ (3) ⟨q, Kc⟩ : einsum(bgpnk, gmck) → bgpnmc ⟨q, Kd⟩ : einsum(bgpnk, bgmdk) → bgpnmd The context part computes attention with Kc that corre- sponds to any batch index, since they are all identical. Hence, the axis b does not appears in the einsum for ⟨q, Kc⟩. The result ⟨q, Kc⟩ and ⟨q, Kd⟩ are then joined together via concatenation. The weight-value attention ⟨w, V⟩ is bifur- cated similarly, where the weight and value tensors are split along length m, and the results are joined back via summa- tion (Eq. 4). We also demonstrate the code for bifurcated attention in Appendix E.3. ⟨w, V⟩ = ⟨wc, Vc⟩ + ⟨wd, Vd⟩ (4) ⟨wc, Vc⟩ : einsum(bgpnmc, gmck) → bgpnk = bnd ⟨wd, Vd⟩ : einsum(bgpnmd, bgmdk) → bgpnk = bnd The proposed operations yield the exact same results⟨w, V⟩ as the original attention in Equation 1 and 2, but can sig- nificantly reduce memory I/O during incremental decoding (proof in Appendix E.1). 4.3. Memory IO Complexity The memory IO complexity corresponding to loading KV changes from 4Bifurcated Attention: Accelerating Massively Parallel Decoding with Shared Prefixes in LLMs With Bifurcated Attention Without Bifurcated Attention Figure 2: Context-aware bifurcated attention for single-context batch sampling. The figure depicts the incremental decoding step where the batched query q attends with the cached key tensor K where different colors in the q tensor correspond to different batch indices. The key tensor consists of two parts: key cache corresponding to the single context Kc (which was computed during context encoding, as in Figure 1), and the key cache corresponding to previous incremental decoding steps Kd. The query-key attention is bifurcated into two parts, ⟨q, Kc⟩ and ⟨q, Kd⟩, and joined back via concatenation, resulting in an identical results using the same FLOPs but with lower memory IO (Eq. 3). The weight-value attention is bifurcated similarly, as outlined in Eq. 4. memory IO w/o bifurcated attention = gk · bm (5) = gk · b(mc + md) memory IO w. bifurcated attention = gk · (mc + bmd) (6) The new memory IO is more efficient since mc + bmd < b(mc + md) =bm. This resulting efficiency gain is appli- cable for all values of g and can be as high as b-fold in the case where mc >> md (high context length compared to the number of generated tokens). The absolute efficiency gain, however, is more substantially for high g such as in the multi-head attention case with g = h. For multi-query (g = 1), the gain can be substantial as well in the case of high mc or b. 5. Experiments We first conduct experiments to see how capabilities scale with respect to model size for each attention type in Section 5.1. We find that attention types with higher compression (lower number of attention groups g) require model size compensation, ≈ 10% for multi-query ( g = 1). We use such findings to compare the latency between the multi- head and the larger multi-query models of equal capabilities in Section 5.2. In Section 5.2.2, we focus on the single- context batch sampling scenario where we demonstrate the significant latency reduction of bifurcated attention and re- visit the comparison between multi-head and multi-query in light of bifurcated attention. We outline inference details in Appendix C.5. 5.1. Comparing Capabilities of Multi-Head, Multi-Query, and Multi-Group Attention For a given model configuration, a multi-group model with g < hhas fewer parameters in comparison to its multi-head counterpart. This reduction is a result of the decreased size of the key and value projection matricesPK and PV . Specif- ically, each tensor in this case has a size of PK : d × gk, where k is the head dimension. For instance, a 13B multi- head model will correspond to a 11B multi-query model, with all other model configurations fixed (see Appendix D.1 for more details). To compare the capabilities of different attention mecha- nisms, one can either scale other model configurations such as the number of layers ℓ, the number of heads h in order to make match the total model sizes between different at- tentions. However, it is often difficult to match the number of parameters exactly. In this work, we compare different attention mechanisms via the loss-vs-size scaling laws. For the setup, we use the model hyperparameters similar to that of GPT-3, where the size ranges from 125M to 13B, with hyperparameters such as ℓ, h, kincreasing in tandem. Then, we consider three cases where g = 1(multi-query), g = h (multi-head) and 1 < g < h(multi-group) where Appendix C.1 and C.2 shows the training and model configuration details. We train all three attention models of each size and plot the validation loss versus model size, shown in Figure 3. Our findings are summarized below. Higher number of attention groups g leads to higher expressiveness The results in Figure 3 shows the valida- tion loss versus model size (log scale). The results indicate that, for the same model size (vertical slice across the plot), multi-head attention g = h achieves the lowest validation loss compared to1 < g < h(multi-group) and g = 1(multi- 5Bifurcated Attention: Accelerating Massively Parallel Decoding with Shared Prefixes in LLMs query). This trend holds consistently over three orders of magnitude of model sizes, where the curves corresponding to multi-head, multi-group and multi-query do not cross, implying that the rank of model expressiveness, or relative capabilities per number of parameters, is quite stable. An in- tuitive explanation is that the lowerg corresponds to a lower rank representation of the key and value tensors, which en- codes lower representation power of the past context and therefore yields lower capabilities than higher g, given the same model size. Scaling laws via downstream performance We use the average scores from two code generation benchmarks, multi- lingual HumanEval and MBXP (Athiwaratkun et al., 2022), as a proxy for model capabilities in addition to the validation loss. This approach is similar to that of the GPT-4 technical report (OpenAI, 2023) where HumanEval (Python) (Chen et al., 2021) is used to track the performance across multiple magnitudes of compute. In our case, we average across all 13 evaluation languages and two benchmarks to obtain a more stable proxy for capabilities. The result in Figure 3 demonstrates similar trend compared to the validation loss where the pass rate curves indicate the same relative expressiveness for multi-head, multi-group and multi-query attention. Matching capabilities by model size compensation Given the same capabilities (horizontal slice of the plot in Figure 3), the distance between two curves indicates the model size difference that the lower-rank attention needs to compensate in order to match the multi-head model per- formance. Empirically, we average the distance along the interpolated lines (log scale) and find this to correspond to 1.104 times; that is, a multi-query model can have the same capabilities as the multi-head model if the size is increased by ≈ 10% of the multi-head model size. Similarly, the gap is < 10% for multi-group attention. Alternatively, one can argue that a multi-query model of the same size could match a multi-head if the multi-query model is given more compute. However, in the regime where we train language models until or close to convergence and the performance saturates with respect to compute, the difference in capabili- ties will likely remain. Therefore, the size compensation is likely the most fair approach for comparison. 5.2. Latencies of Capabilities-Equivalent Models As detailed in Section 5.1, we’ve observed that an increase in the multi-query model’s size is required for it to match the performance of a multi-head model. In this section, we focus on examining the latency trade-offs across diverse scenarios with both multi-query and multi-head models of similar performance capabilities. For these latency experiments, we utilize two models, each with an approximate size of 125M 672M 2.8B 13B model size (w/o embeddings) 0.60 0.65 0.70 0.75 0.80 0.85 validation loss Validation loss vs size multi group multi head multi query 125M 672M 2.8B 13B model size (w/o embeddings) 4 6 8 10 12 14pass@1 Average pass rates vs size multi group multi head multi query Figure 3: (Left) The plots of validation loss versus model size demonstrate that the scaling laws curves of different attention mechanisms have different expressiveness or per- formance efficiency. That is, the capabilities given the same model size depends on g where higher g yields the best capabilities. (Right) We demonstrate a similar trend where we use code generation abilities as a proxy for general capa- bilities. Here, we average the execution pass rates evaluated on Multi-lingual HumanEval and MBXP benchmarks under 13 programming languages. 1 billion: a multi-head model and a multi-query model (detailed information can be found in C.3). The multi-query model chosen for these studies is larger by a multiplicative factor F, where F = 1.1. Overall, there is some overhead cost of using multi-query attention due to the larger size (see Figure 4 and Appendix D.3.1 and D.3.2 for analysis). That is, context encoding latency of the multi-query model will be slightly larger, as well as the low-context and low-batch incremental decoding scenario. However, multi-query can have significantly lower latency compared to multi-head in the scenario with high number of decoding steps which makes the incremental de- coding phase being latency-dominating, and high context or batch size which heavily impacts the memory IO of in- cremental decoding. We outline three different inference scenarios below. 5.2.1. S INGLE CONTEXT SCENARIO In the single batch inference scenario, the multi-query/- group attention can achieve lower latency when the context length and the number of generated tokens are high, as 6Bifurcated Attention: Accelerating Massively Parallel Decoding with Shared Prefixes in LLMs Multi-Head Multi-Query comparable capabilities context encoding incremental decoding per step MH MQ MH MQ Figure 4: High-level latency comparison between an MH model and a larger MQ model with comparable capabilities. Overall, there’s an overhead cost for the initial context en- coding latency due the additional compute with the larger MQ model size. For low context and batch size, the per step latency of MQ is also slightly higher to start due to the memory IO required for larger model size, but does not change much as context length m or batch size b grow, as supposed to the multi-head case where the per step latency can grow more rapidly with respect to m and b. demonstrated in Figure 5. Different implementations that are more efficient in loading KV cache (such as lower-level kernel that can avoid duplicated IO) can cause the overall curves of MH to be flatter. However, the overall trend still remains where given sufficiently high context m, MQ will begin to be faster than MH. 5.2.2. S INGLE -CONTEXT BATCH SAMPLING In this scenario, we are given a single context and generates multiple completions based on temperature sampling. In this case, the context encoding is independent of the batch size b since it is performed on the single context and broad- casted for other batch indices (Figure 1). In contrast to the batch inference scenario, this is a more practical online inference scenario since we are not bottlenecked by the con- text encoding step. Our proposed context-aware bifurcated attention is exactly applicable for such scenario where in this section we demonstrate the results in conjunction with both multi-head and multi-query. Multi-head benefits significantly from bifurcated atten- tion Figure 6a demonstrates the per step latency results for a multi-head model. For instance, with batch size 8, the per step latency without bifurcated attention grows rapidly with context length, from ≈ 10 ms to ≈ 100 ms at context length 10000. However, with bifurcated attention, the la- tency remains relatively flat with respect to context length. In practice, bifurcated attention also reduces memory con- sumption at high batch size and context lengths without encountering out-of-memory error as early as without bifur- cated attention. Bifurcated attention + multi-head rivals multi-query Figure 7 shows the comparison between MH and MQ with and without bifurcated attention. Without bifurcated atten- tion, MQ is clearly much more inference efficient. How- ever, with bifurcated attention, MQ and MH under moderate batch size scenarios (up to 64) seems comparable, where multi-head is even has lower latency. The results indicate that, given an existing MH model, we can support batch sampling scenarios using bifurcated attention without the need of a multi-query model (which requires training a new model, or at least continuous training) (Ainslie et al., 2023). With a more inference-intensive scenarios, including batch inference scenario where the bifurcated attention is not ap- plicable, switching to multi-query can be worth the effort. Bifurcated attention with multi-query enables more ex- treme batch size and context lengths Multi-query has overall h times lower memory IO and can already reduce latency for some inference scenarios. With bifurcated at- tention, the supported context lengths and batch sizes can become much more extreme, as demonstrated in Figure 6b. 5.3. Compatibility with Torch-Compile Bifurcated attention can be implemented with 4 einsum calls in native PyTorch, making it compatible with Torch- Compile. With Torch Compile, we can take advantage of kernel-fusion and concurrency to improve the latency of the model. To demonstrate this, we implement bifurcated attention on top of GPTFast (PyTorch, 2023) 3. We experiment on a 7B parameter model, with a hidden dimension of 4096 that is 32 layers deep and has 32 heads, as shown in Table 1. We observe that the overall latency imporvements fwith large-batch sampling with respect to standard SDPA attention remains consistent even when we compile the model. 5.4. Applications Efficient large-scale sampling is particularly useful for downstream applications that require multiple generations but has latency constraints, e.g., AI code assistants. In this case, bifurcated attention enables generating more candi- dates by using larger batch size without incurring much ad- ditional latency. To verify our point, we empirically evaluate CodeGen-16B-mono (Nijkamp et al., 2022) and StarCoder (15.5B) (Li et al., 2023) on MBPP dataset (Austin et al., 2021), and plot pass rates with respect to latency in Figure 8, where we also indicate the batch size n. We consider two accuracy measurements: (1) pass@n corresponds to the or- acle scenario, where we evaluate all the generated samples and check if any of them is correct; (2) pass@top3, where we are only allowed to evaluate three examples no matter how many we generate. In the top-3 case, we deduplicate 3Link to our code: https://github.com/bifurcated-attn-icml- 2024/gpt-fast-parallel-sampling 7Bifurcated Attention: Accelerating Massively Parallel Decoding with Shared Prefixes in LLMs 0 5000 10000 Context Length 6 8 10 12 14Per Step Latency (ms) Per Step Latency (ms) Attention MH MQ 0 5000 10000 Context Length 0 250 500 750 1000 1250Context Encoding Latency (ms) Context Encoding Latency (ms) 0 5000 10000 Context Length 250 500 750 1000 125015 Token Total Latency (ms) 15 Token Total Latency (ms) 0 5000 10000 Context Length 1500 2000 2500 3000 3500 4000256 Token Total Latency (ms) 256 Token Total Latency (ms) MH vs MG ds mp degree = 1 num heads = 20 Figure 5: Incremental decoding (per step) latency and the context encoding latency, as a function of input context length. In this plot, we compare an multi-head model and an multi-query model of comparable capabilities, whose size is slightly larger. (Leftmost: Per-step incremental decoding latency) For low context length such as m <2500, due to the larger size of the MQ model, the inference latency is higher. However, the growth with respect to context length of the MQ model is much lower (almost flat), resulting in lower per step latency when the context length is high. (Second: Context encoding latency) The context encoding latency depends on the FLOPs where the MH and MQ are quite similar. Note that the MQ model is slightly larger, and therefore corresponds to a steeper curve. (Third, Fourth): Total latency for 15 or 256 generated steps The two plots illustrates the total latency, which is the sum of context encoding and the the number of steps times incremental decoding latency. The benefits of MQ model becomes clear in the case of high decoding steps (256) whereas in the case of 15 generated tokens, the total latency of MQ can still be slightly higher than MH. 0 2500 5000 7500 10000 Context Length 20 40 60 80 100Per Step Latency (ms) Per Step Latency (ms) Bifurcated False True 0 2500 5000 7500 10000 Context Length 0 100 200 300 400 500 600Context Encoding Latency (ms) Context Encoding Latency (ms) Batch size 8 32 128 256 512 0 2500 5000 7500 10000 Context Length 500 1000 150015 Token Total Latency (ms) 15 Token Total Latency (ms) 0 2500 5000 7500 10000 Context Length 5000 10000 15000 20000 25000256 Token Total Latency (ms) 256 Token Total Latency (ms) MH vs MG ds mp degree = 1 num heads = 20 multi query = 0 (a) Multi-Head 0 2500 5000 7500 10000 Context Length 20 40 60 80 100Per Step Latency (ms) Per Step Latency (ms) Batch size 8 32 128 256 512 0 2500 5000 7500 10000 Context Length 0 200 400 600 800Context Encoding Latency (ms) Context Encoding Latency (ms) Bifurcated False True 0 2500 5000 7500 10000 Context Length 250 500 750 1000 1250 1500 175015 Token Total Latency (ms) 15 Token Total Latency (ms) 0 2500 5000 7500 10000 Context Length 5000 10000 15000 20000 25000256 Token Total Latency (ms) 256 Token Total Latency (ms) MH vs MG ds mp degree = 1 num heads = 20 multi query = 1 (b) Multi-Query Figure 6: Context-aware bifurcated attention with multi-head attention (a) and multi-query attention (b). The bifurcated attention loads the KV cache in a context-aware manner, resulting in significantly lower latency for sampling under high batch sizes. For instance, in the case of multi-head attention with batch size 128 and context length 10, 000, bifurcated attention results in ≈ 4× lower the incremental decoding latency. Additionally, growth with respect to context length is relatively flat with bifurcated attention. With multi-query attention, bifurcated attention permits us to use batch sizes as high as 256 or 512 with lower latency than in the multi-head scenario. the n samples, and rank by their mean log probability scores (Chen et al., 2021) to determine three candidates. All ex- periments use nucleus sampling with p = 0.95 (Holtzman et al., 2020) and temperature 0.8. The results show much sharper improvement in either metrics relative to additional latency. This approach opens up avenues for performance improvement given a fixed budget of latency. Many reasoning algorithms such as self-consistency Chain- of-thought (SC-COT) (Wang et al., 2023) and Tree-of- thought (ToT) (Yao et al., 2023) depend on sampling multi- ple outputs with shared prefix, where bifurcated attention will enable higher accuracy under same costs. 6. Conclusion Bifurcated attention provides a complementary approach to the existing inference acceleration methods, with a particu- lar focus on minimizing the memory IO of the incremental decoding, thereby enhancing inference efficiency. Our work helps support demanding inference scenarios due to larger context during incremental decoding, which are emerging from, e.g., more complex applications that requires long context such as complex reasoning, planning, or retrieval augmented generations. Impact Statement Bifurcated attention is an approach that can significantly re- duce the latency and associated costs involved in deploying 8Bifurcated Attention: Accelerating Massively Parallel Decoding with Shared Prefixes in LLMs 0 2500 5000 7500 10000 Context Length 20 40 60 80 100Per Step Latency (ms) Per Step Latency (ms) Attention MH MQ 0 2500 5000 7500 10000 Context Length 0 200 400 600 800Context Encoding Latency (ms) Context Encoding Latency (ms) Batch size 1 4 8 16 32 0 2500 5000 7500 10000 Context Length 500 1000 150015 Token Total Latency (ms) 15 Token Total Latency (ms) 0 2500 5000 7500 10000 Context Length 5000 10000 15000 20000 25000256 Token Total Latency (ms) 256 Token Total Latency (ms) MH vs MG ds mp degree = 1 num heads = 20 explicit broadcast = 1 no duplicate broadcast = 0 (a) Without bifurcated attention 0 2500 5000 7500 10000 Context Length 10.0 12.5 15.0 17.5 20.0 22.5 Per Step Latency (ms) Per Step Latency (ms) Attention MH MQ 0 2500 5000 7500 10000 Context Length 0 200 400 600 800Context Encoding Latency (ms) Context Encoding Latency (ms) Batch size 16 32 64 128 0 2500 5000 7500 10000 Context Length 200 400 600 800 100015 Token Total Latency (ms) 15 Token Total Latency (ms) 0 2500 5000 7500 10000 Context Length 2000 3000 4000 5000 6000256 Token Total Latency (ms) 256 Token Total Latency (ms) MH vs MG ds mp degree = 1 num heads = 20 explicit broadcast = 1 no duplicate broadcast = 1 (b) With bifurcated attention Figure 7: Latency comparison between multi-head and a larger multi-query model of equal capabilities. Without bifurcated attention, MQ is clearly much more inference efficient. However, with bifurcated attention, MH can have better latency than MQ in moderate scenario (up to batch size 64 in this case) where MQ can handle more extreme scenarios better than MH. Figure 8: Bifurcated attention improves accuracy by enabling more generated samples over a fixed latency budget, applicable for both multi-head attention (CodeGen) and multi-query attention (StarCoder). Given the n samples, pass@n reflects the execution pass rate of the best sample among n, shown in (a) and (c). Filtering n samples with mean log probability ranking yields a subset of best three samples, reflected by pass@top3 in (b) and (d). The increased number of samples within the same latency budget results in increased performance via either pass@n or pass@top-k. large language models (LLMs). A key advantage of this technique is its potential to lower the carbon emissions as- sociated with LLM inference. While reducing deployment costs could potentially lead to broader adoption of LLMs, the societal impact of such increased usage remains difficult to predict with certainty. Nonetheless, bifurcated attention presents an opportunity to make LLM deployment more efficient and environmentally friendly, although the broader implications warrant careful consideration. References W. U. Ahmad, S. Chakraborty, B. Ray, and K.-W. Chang. Unified pre-training for program understanding and generation. arXiv preprint arXiv:2103.06333, 2021. J. Ainslie, J. Lee-Thorp, M. de Jong, Y . Zemlyanskiy, F. Le- brón, and S. Sanghai. GQA: training generalized multi-query transformer models from multi-head checkpoints. CoRR, abs/2305.13245, 2023. doi: 10.48550/arXiv.2305.13245. URL https://doi.org/10.48550/arXiv.2305.13245. L. B. Allal, R. Li, D. Kocetkov, C. Mou, C. Akiki, C. M. Ferrandis, N. Muennighoff, M. Mishra, A. Gu, M. Dey, et al. Santacoder: 9Bifurcated Attention: Accelerating Massively Parallel Decoding with Shared Prefixes in LLMs Table 1: Per-token latency (ms) of a 7B multi-head model on GPT-Fast with and without Torch Compilation compared to a model using Torch’s standard SDPA kernel. without Compile Compiled Context BS SDPA Bifurcated SDPA Bifurcated 8k 1 26.40 30.39 8.78 8.64 2 28.71 31.37 10.51 11.77 4 43.36 31.44 13.23 12.03 8 72.71 33.72 17.33 12.36 16 132.89 31.71 26.19 12.60 16k 1 30.13 30.66 13.06 12.16 2 44.74 32.62 15.35 17.17 4 73.62 33.44 20.65 17.33 8 132.29 34.67 32.06 18.07 16 251.47 36.78 OOM 18.46 32k 1 44.94 39.97 19.80 20.90 2 69.22 48.61 OOM 29.34 4 OOM 49.77 - 29.73 8 - 51.31 - 30.30 16 - 54.92 - 30.66 don’t reach for the stars! arXiv preprint arXiv:2301.03988, 2023. Amazon. Amazon code whisperer. https://aws.amazon. com/codewhisperer/. B. Athiwaratkun, S. K. Gouda, Z. Wang, X. Li, Y . Tian, M. Tan, W. U. Ahmad, S. Wang, Q. Sun, M. Shang, S. K. Gonugondla, H. Ding, V . Kumar, N. Fulton, A. Farahani, S. Jain, R. Giaquinto, H. Qian, M. K. Ramanathan, R. Nallapati, B. Ray, P. Bhatia, S. Sengupta, D. Roth, and B. Xiang. Multi-lingual evaluation of code generation models. CoRR, abs/2210.14868, 2022. doi: 10.48550/arXiv.2210.14868. URL https://doi.org/10. 48550/arXiv.2210.14868. J. Austin, A. Odena, M. I. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. J. Cai, M. Terry, Q. V . Le, and C. Sut- ton. Program synthesis with large language models. CoRR, abs/2108.07732, 2021. URL https://arxiv.org/abs/ 2108.07732. I. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long- document transformer. arXiv preprint arXiv:2004.05150, 2020. T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhari- wal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-V oss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCan- dlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. CoRR, abs/2005.14165, 2020. URL https://arxiv.org/abs/2005.14165. A. Bulatov, Y . Kuratov, and M. S. Burtsev. Scaling trans- former to 1m tokens and beyond with rmt. arXiv preprint arXiv:2304.11062, 2023. T. Cai, Y . Li, Z. Geng, H. Peng, J. D. Lee, D. Chen, and T. Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads, 2024. C. Chen, S. Borgeaud, G. Irving, J. Lespiau, L. Sifre, and J. Jumper. Accelerating large language model decoding with speculative sampling. CoRR, abs/2302.01318, 2023. doi: 10.48550/arXiv.2302.01318. URL https://doi.org/10. 48550/arXiv.2302.01318. M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Ka- plan, H. Edwards, Y . Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-V oss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V . Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021. URL https://arxiv.org/abs/ 2107.03374. R. Child, S. Gray, A. Radford, and I. Sutskever. Gen- erating long sequences with sparse transformers. URL https://openai.com/blog/sparse-transformers, 2019. J. Choquette, W. Gandhi, O. Giroux, N. Stam, and R. Krashin- sky. Nvidia a100 tensor core gpu: Performance and innovation. IEEE Micro, 41(2):29–35, 2021. A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. Rao, P. Barnes, Y . Tay, N. Shazeer, V . Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope, J. Bradbury, J. Austin, M. Isard, G. Gur- Ari, P. Yin, T. Duke, A. Levskaya, S. Ghemawat, S. Dev, H. Michalewski, X. Garcia, V . Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito, D. Luan, H. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal, M. Omernick, A. M. Dai, T. S. Pillai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee, Z. Zhou, X. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-Hellstern, D. Eck, J. Dean, S. Petrov, and N. Fiedel. Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311, 2022. doi: 10.48550/arXiv.2204.02311. URL https://doi.org/10. 48550/arXiv.2204.02311. M. R. Costa-jussà, J. Cross, O. Çelebi, M. Elbayad, K. Heafield, K. Heffernan, E. Kalbassi, J. Lam, D. Licht, J. Maillard, et al. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672, 2022. T. Dao, D. Y . Fu, S. Ermon, A. Rudra, and C. Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. In NeurIPS, 2022. S. Dathathri, A. Madotto, J. Lan, J. Hung, E. Frank, P. Molino, J. Yosinski, and R. Liu. Plug and play language models: A simple approach to controlled text generation. arXiv preprint arXiv:1912.02164, 2019. T. Dettmers, M. Lewis, Y . Belkada, and L. Zettlemoyer. Llm.int8(): 8-bit matrix multiplication for transformers at scale. CoRR, abs/2208.07339, 2022. doi: 10.48550/arXiv.2208.07339. URL https://doi.org/10.48550/arXiv.2208.07339. A. Farhad, A. Arkady, B. Magdalena, B. Ond ˇrej, C. Rajen, C. Vishrav, M. R. Costa-jussa, E.-B. Cristina, F. Angela, 10Bifurcated Attention: Accelerating Massively Parallel Decoding with Shared Prefixes in LLMs F. Christian, et al. Findings of the 2021 conference on ma- chine translation (wmt21). In Proceedings of the Sixth Con- ference on Machine Translation, pages 1–88. Association for Computational Linguistics, 2021. E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. Gptq: Accu- rate post-training quantization for generative pre-trained trans- formers. arXiv preprint arXiv:2210.17323, 2022. D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi, R. Zhong, W.-t. Yih, L. Zettlemoyer, and M. Lewis. Incoder: A generative model for code infilling and synthesis.arXiv preprint arXiv:2204.05999, 2022. Y . Fu, P. Bailis, I. Stoica, and H. Zhang. Breaking the se- quential dependency of llm inference using lookahead decod- ing, November 2023. URL https://lmsys.org/blog/ 2023-11-21-lookahead-decoding/ . S. Gehman, S. Gururangan, M. Sap, Y . Choi, and N. A. Smith. Realtoxicityprompts: Evaluating neural toxic degeneration in language models. arXiv preprint arXiv:2009.11462, 2020. Google. Bard. https://blog.google/technology/ai/ try-bard/. K. Guu, K. Lee, Z. Tung, P. Pasupat, and M. Chang. Retrieval augmented language model pre-training. In International con- ference on machine learning, pages 3929–3938. PMLR, 2020. J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark, et al. Training compute-optimal large language mod- els. arXiv preprint arXiv:2203.15556, 2022. A. Holtzman, J. Buys, L. Du, M. Forbes, and Y . Choi. The curious case of neural text degeneration. In 8th Interna- tional Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id= rygGQyrFvH. G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick, J. Dwivedi-Yu, A. Joulin, S. Riedel, and E. Grave. Atlas: Few- shot learning with retrieval augmented language models. arXiv preprint arXiv, 2208, 2022. Z. Jia, M. Maggioni, B. Staiger, and D. P. Scarpazza. Dissecting the NVIDIA volta GPU architecture via microbenchmarking. CoRR, abs/1804.06826, 2018. URL http://arxiv.org/ abs/1804.06826. J. Juravsky, B. Brown, R. Ehrlich, D. Y . Fu, C. Ré, and A. Mirho- seini. Hydragen: High-throughput llm inference with shared prefixes, 2024. D. Kalamkar, D. Mudigere, N. Mellempudi, D. Das, K. Baner- jee, S. Avancha, D. T. V ooturi, N. Jammalamadaka, J. Huang, H. Yuen, et al. A study of bfloat16 for deep learning training. arXiv preprint arXiv:1905.12322, 2019. J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei. Scaling laws for neural language models. CoRR, abs/2001.08361, 2020. URL https://arxiv.org/abs/2001.08361. D. P. Kingma and J. Ba. Adam: A method for stochastic optimiza- tion. arXiv preprint arXiv:1412.6980, 2014. A. Kuzmin, M. Van Baalen, Y . Ren, M. Nagel, J. Peters, and T. Blankevoort. Fp8 quantization: The power of the exponent. arXiv preprint arXiv:2208.09225, 2022. W. Kwon, Z. Li, S. Zhuang, Y . Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica. Efficient memory manage- ment for large language model serving with pagedattention, 2023. H. Le, Y . Wang, A. D. Gotmare, S. Savarese, and S. C. H. Hoi. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Advances in Neural Informa- tion Processing Systems, 35:21314–21328, 2022. Y . Leviathan, M. Kalman, and Y . Matias. Fast inference from transformers via speculative decoding. CoRR, abs/2211.17192, 2022. doi: 10.48550/arXiv.2211.17192. URL https://doi. org/10.48550/arXiv.2211.17192. R. Li, L. B. Allal, Y . Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim, et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023. Y . Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):1092–1097, 2022. Y . Li, F. Wei, C. Zhang, and H. Zhang. Eagle: Speculative sam- pling requires rethinking feature uncertainty, 2024. Z. Lin and M. Riedl. Plug-and-blend: A framework for controllable story generation with blended control codes. arXiv preprint arXiv:2104.04039, 2021. I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. A. Madaan, A. Shypula, U. Alon, M. Hashemi, P. Ran- ganathan, Y . Yang, G. Neubig, and A. Yazdanbakhsh. Learn- ing performance-improving code edits. arXiv preprint arXiv:2302.07867, 2023. J. Menick, M. Trebacz, V . Mikulik, J. Aslanides, F. Song, M. Chad- wick, M. Glaese, S. Young, L. Campbell-Gillingham, G. Irving, et al. Teaching language models to support answers with veri- fied quotes. arXiv preprint arXiv:2203.11147, 2022. X. Miao, G. Oliaro, Z. Zhang, X. Cheng, Z. Wang, R. Y . Y . Wong, Z. Chen, D. Arfeen, R. Abhyankar, and Z. Jia. Specinfer: Accelerating generative LLM serving with speculative inference and token tree verification. CoRR, abs/2305.09781, 2023. doi: 10.48550/ARXIV .2305.09781. URL https://doi.org/ 10.48550/arXiv.2305.09781. Microsoft. Github copilot. https://github.com/ features/copilot. P. Mirowski, K. W. Mathewson, J. Pittman, and R. Evans. Co- writing screenplays and theatre scripts with language models: Evaluation by industry professionals. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, pages 1–34, 2023. M. Nadeem, A. Bethke, and S. Reddy. Stereoset: Measuring stereotypical bias in pretrained language models. arXiv preprint arXiv:2004.09456, 2020. 11Bifurcated Attention: Accelerating Massively Parallel Decoding with Shared Prefixes in LLMs R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V . Kosaraju, W. Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y . Zhou, S. Savarese, and C. Xiong. Codegen: An open large lan- guage model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474, 2022. E. Nijkamp, H. Hayashi, C. Xiong, S. Savarese, and Y . Zhou. Code- gen2: Lessons for training llms on programming and natural languages. arXiv preprint arXiv:2305.02309, 2023. NVIDIA. Fastertransformer. https://github.com/ NVIDIA/FasterTransformer. OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi: 10.48550/arXiv.2303.08774. URL https://doi.org/ 10.48550/arXiv.2303.08774. A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Köpf, E. Z. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chin- tala. Pytorch: An imperative style, high-performance deep learning library. In H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché-Buc, E. B. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Van- couver, BC, Canada, pages 8024–8035, 2019. URL https: //proceedings.neurips.cc/paper/2019/hash/ bdbca288fee7f92f2bfa9f7012727740-Abstract. html. H. Pearce, B. Ahmad, B. Tan, B. Dolan-Gavitt, and R. Karri. Asleep at the keyboard? assessing the security of github copi- lot’s code contributions. In 2022 IEEE Symposium on Security and Privacy (SP), pages 754–768. IEEE, 2022. R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, A. Lev- skaya, J. Heek, K. Xiao, S. Agrawal, and J. Dean. Efficiently scaling transformer inference. CoRR, abs/2211.05102, 2022. doi: 10.48550/arXiv.2211.05102. URL https://doi.org/ 10.48550/arXiv.2211.05102. T. PyTorch. Accelerating generative AI with PyTorch II: GPT, Fast, 2023. URL https://pytorch.org/blog/ accelerating-generative-ai-2/ . J. Rasley, S. Rajbhandari, O. Ruwase, and Y . He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 3505–3506, 2020. B. Roziere, M.-A. Lachaux, L. Chanussot, and G. Lample. Un- supervised translation of programming languages. Advances in Neural Information Processing Systems, 33:20601–20611, 2020. T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, and T. Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023. N. Shazeer. Fast transformer decoding: One write-head is all you need. CoRR, abs/1911.02150, 2019. URL http://arxiv. org/abs/1911.02150. M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. Megatron-lm: Training multi-billion parame- ter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. M. N. Team. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023. URL www.mosaicml. com/blog/mpt-7b. H. Touvron, T. Lavril, G. Izacard, X. Martinet, M. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Ro- driguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023. doi: 10.48550/arXiv.2302.13971. URL https://doi. org/10.48550/arXiv.2302.13971. C. Tran, S. Bhosale, J. Cross, P. Koehn, S. Edunov, and A. Fan. Facebook ai wmt21 news translation task submission. arXiv preprint arXiv:2108.03265, 2021. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. In I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V . N. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998–6008, 2017. URL https: //proceedings.neurips.cc/paper/2017/hash/ 3f5ee243547dee91fbd053c1c4a845aa-Abstract. html. S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma. Lin- former: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou. Self-consistency improves chain of thought reasoning in language models, 2023. X. Wei, S. K. Gonugondla, W. U. Ahmad, S. Wang, B. Ray, H. Qian, X. Li, V . Kumar, Z. Wang, Y . Tian, Q. Sun, B. Athi- waratkun, M. Shang, M. K. Ramanathan, P. Bhatia, and B. Xi- ang. Greener yet powerful: Taming large code generation models with quantization. CoRR, abs/2303.05378, 2023. doi: 10.48550/arXiv.2303.05378. URL https://doi.org/10. 48550/arXiv.2303.05378. T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, et al. Hugging- face’s transformers: State-of-the-art natural language process- ing. arXiv preprint arXiv:1910.03771, 2019. G. Xiao, J. Lin, M. Seznec, J. Demouth, and S. Han. Smoothquant: Accurate and efficient post-training quantization for large lan- guage models. arXiv preprint arXiv:2211.10438, 2022. S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y . Cao, and K. Narasimhan. Tree of thoughts: Deliberate problem solving with large language models, 2023. Z. Yao, R. Y . Aminabadi, M. Zhang, X. Wu, C. Li, and Y . He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. In NeurIPS, 2022. 12Bifurcated Attention: Accelerating Massively Parallel Decoding with Shared Prefixes in LLMs K. Yee, N. Ng, Y . N. Dauphin, and M. Auli. Simple and effective noisy channel modeling for neural machine translation. arXiv preprint arXiv:1908.05731, 2019. A. Yuan, A. Coenen, E. Reif, and D. Ippolito. Wordcraft: story writing with large language models. In 27th International Con- ference on Intelligent User Interfaces, pages 841–852, 2022. M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Al- berti, S. Ontañón, P. Pham, A. Ravula, Q. Wang, L. Yang, and A. Ahmed. Big bird: Transformers for longer se- quences. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances in Neural In- formation Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https: //proceedings.neurips.cc/paper/2020/hash/ c8512d142a2d849725f31a9a7a361ab9-Abstract. html. C. Zhen, Y . Shang, X. Liu, Y . Li, Y . Chen, and D. Zhang. A survey on knowledge-enhanced pre-trained language models. arXiv preprint arXiv:2212.13428, 2022. A. FAQs 1. Q: If we already have an MQ model that seems to be quite efficient at large batch sampling, is bifurcated attention nec- essary? A: The proposed context-aware bifurcated attention is an exact computation that provides a different way to perform attention, so one can use it \"for free\" without a performance tradeoff. Due to the reduced memory I/O, it enables more extreme cases of batch sampling, such as a larger batch, even for long contexts. 2. Q: How applicable is multi-query for single-batch inference without high batch sampling? A: If the context is long and the number of generated tokens is high, then the benefits of multi-query are clear. Please see Section 5.2.1. 3. Q: Is bifurcated attention applicable for the case where we process different inputs in a batch? A: No. In that case, if we need a solution to reduce memory I/O during incremental decoding, then multi-query attention can be appealing, especially in scenarios with a high number of generated tokens where the incremental decoding phase dominates the overall latency. This is because there is an overhead to multi-query due to the context encoding phase, as outlined in the main paper. 4. Q: Any caveats to using bifurcated attention? A: For small workloads (low context length and batch size), due to the fact that we split the attention into two parts, there can be less parallelization of the GEMM kernels, which could lead to higher latency, especially for MQ models. However, one can get the best of both worlds given any model by triggering bifurcated attention under high workload scenarios and using normal attention otherwise. With such a workload- based switch, bifurcated attention is guaranteed to provide better latency and efficiency. 5. Q: How does model quantization (or lower precision arith- metic) affect the findings? A: There are two regimes for quantization: model weight quantization and attention quantization. To date, most quanti- zation only focuses on the weight since the attention compu- tation is precision-sensitive and quantization has not proved to be viable. Model quantization can make incremental decoding faster due to lower memory I/O of the model itself, since the ef- fective model size in memory is smaller. This shifts the latency curve downward for all context lengths or batch sizes. The overall conclusion for the bifurcated and multi-query attention remains the same, however, since the improvement proposed in the paper is on the attention component, which is orthogonal to the model weight. If attention quantization is viable in the future, the lower memory on the attention tensor will effectively reduce the memory I/O for KV cache by a factor of 2 in the case of int8 quantization (compared to fp16 or bf16) or a factor of 4 in the case of int4. Overall, this will flatten the latency growth with respect to batch size or context length. The overall comparative complexity (a) with or without bifurcated attention or (b) multi-head vs. multi-query remains the same. 6. Q: Does the conclusion depend on the inference implementa- tion or different hardware? A: Different inference platforms, such as FasterTransformers (GPUs) or PaLM inference (TPUs), can yield different latency numbers. However, the relative I/O complexity among different attention mechanisms does not change, resulting in similar relative trends among different attention mechanisms. That being said, it is possible that more efficient imple- mentations or more performant chip/system configurations, including different tensor parallelism degrees, can result in different slopes for the latency growth with respect to context length and batch size. In that case, the trade-off points in terms of context length or batch size can be different. The comparative complexity remains the same based on the analysis. 7. Q: How does bifurcated attention differ from using attention mask for sampling as in done in SpecInfer (Miao et al., 2023) ? A: The attention mask approach can have a different FLOP usage compared to the original attention. We can consider a scenario where the attention mask corresponds to sam- pling with batch b and incremental decoding length ℓ, with the original context of length m. The attention FLOPs are O(mbℓ + b2ℓ2). In contrast, the original FLOPs is O(mbℓ). If bℓ is sufficiently large, then the FLOPs via attention mask can be much higher. However, for the purpose of speculative decoding where the number of draft tokens is small, this additional FLOPs can be negligible. B. Related Work B.1. Applications of Single-Context Batch Sampling The observed latency reduction we achieve can have a profound impact on many applications. Some of these applications include: • Code Generation: In software development, AI-assisted code generation can benefit greatly from reduced latency, espe- cially when generating multiple code snippets or suggestions for a given context. This can lead to a more responsive and 13Bifurcated Attention: Accelerating Massively Parallel Decoding with Shared Prefixes in LLMs efficient user experience for developers using AI-powered Integrated Development Environments (IDEs) or code com- pletion tools (Nijkamp et al., 2023; 2022; Chen et al., 2021; Le et al., 2022; Fried et al., 2022; Li et al., 2022; Allal et al., 2023; Li et al., 2023; Ahmad et al., 2021). • Machine Translation: In situations where multiple transla- tions are needed for a single input, such as generating trans- lations with varying degrees of formality or generating trans- lations for different dialects, the context-aware bifurcated attention can provide more efficient computation, resulting in faster and more scalable machine translation services (Costa- jussà et al., 2022; Farhad et al., 2021; Tran et al., 2021; Yee et al., 2019). • Chatbots and Conversational AI: Conversational agents often need to generate multiple responses to handle different inter- pretations of a user’s input or to provide multiple suggestions. The reduced latency offered by the proposed method can significantly improve the responsiveness of chatbots, leading to a more natural and fluid conversation with users (Google). • Creative Content Generation: In applications like poetry, story, or advertisement generation, the ability to generate multiple variations for a given prompt is crucial. The pro- posed method enables more efficient generation of diverse content, making it more feasible for real-time or large-scale applications (Lin and Riedl, 2021; Mirowski et al., 2023; Team, 2023; Yuan et al., 2022). • Reasoning: using Self-consistency Chain-of-Thought (CoT- SC) (Wang et al., 2023) and Tree-of-Thought (ToT) (Yao et al., 2023) requires the model to sample multiple outputs with a shared prefix. Bifurcated attention will enable larger number of reasoning paths in SC-COT and larger trees in ToT at the same cost of inference. • Data Augmentation: In the context of data augmentation for machine learning, generating multiple alternative exam- ples for a given input can help improve model robustness and generalization. With the reduced latency provided by context-aware bifurcated attention, the process of generating augmented data can be made faster, enabling more efficient use of computational resources during training. • General Large Scale Evaluation: In addition to the afore- mentioned use-cases there are many niche use-cases where LLM and other open-ended generation models are explored for toxicity (Dathathri et al., 2019; Gehman et al., 2020; Nadeem et al., 2020), detection of vulnerable code in genera- tions (Pearce et al., 2022), performance improving code edit generation (Madaan et al., 2023), programming language translations (Roziere et al., 2020) and many others. In all of these scenarios many generations per each prompt are gath- ered for a deeper understanding of the models, bifurcated attention can drastically speed up the generation process in such cases. In conclusion, the proposed context-aware bifurcated attention method can significantly reduce memory I/O cost and improve latency in various applications, leading to increased efficiency and scalability. This method has the potential to enable new use cases and enhance the user experience in numerous AI-powered systems, making them more practical for real-world deployment. B.2. Supporting Long Context Requires IO-Efficient Attention As language models are becoming general purpose and highly capable, the demand for language models to handle longer context sequences has grown significantly. Recently, there is an ongoing focus on models that can handle even longer context sequences (Bulatov et al., 2023; OpenAI, 2023; Team, 2023;?). As of today, GPT-4 (OpenAI, 2023) supports context length of 32k tokens, and MPT-7B (Team, 2023) extends it to 64k while Anthropic’s Claude 4 supports as long as 100k input length. Most recently, Bulatov et al proposed 1M token input context length for transformers. These models push the boundaries of context understanding and generation capabilities, enabling more comprehensive discourse understanding and contextually informed responses. This trend is driven by the need for comprehensive discourse un- derstanding in applications like Retrieval-Augmented Generation (RAG), as well as many complex prompting methods. Applica- tions such as RAG (Guu et al., 2020; Izacard et al., 2022; Menick et al., 2022; Zhen et al., 2022) retrieve extensive passages or docu- ments from external corpora, providing rich and grounded context for generating responses. Additionally, models like Toolformer (Schick et al., 2023) and WebGPT (Nakano et al., 2021) leverage external tools, such as APIs and search engines, to expand the context and enhance generation. Long context is disproportionately expensive for transformer fam- ily models because for vanilla self-attention both memory and time complexity are quadratic to the sequence length. To effec- tively handle longer context sequences, optimizing memory I/O and reducing computational overhead are critical. Currently, the dominant approaches to addressing this challenge have been to make the attention computation less expensive. Beltagy et al. (2020) proposed to sparsify self-attention using various attention patterns. Wang et al. (2020) explores low-rank approximation of self-attention. In addition to the compute bound improvements, advancements in memory-efficient attention mechanisms and tech- niques for reducing memory I/O will continue to propel the field forward, facilitating the handling of longer context sequences in language models. FlashAttention (Dao et al., 2022) is proposed to speed up self-attention and reduce the memory footprint without any approximation. It leverages fused kernel for matrix multipli- cation and softmax operation which greatly reduces memory IO during training. C. Setup C.1. Model Training Details We trained multiple models with varying sizes, ranging from 125 million parameters to 13 billion parameters, using code data with a context size of 2048 and adjusting the per-GPU batch size and total number of steps according to the model size. For model training we used multiple p4 instances each equipped with 8 40GB Nvidia A100 GPUs per instance. For our largest model family, the 13 billion parameter model, we used a global batch size of 1024, which approximately translates to 2 million tokens per batch. The settings for each model within each model-size family were kept consistent. The remaining training hyperparameters are summarized in the following table 2. We use AdamW optimizer ((Kingma and Ba, 2014)) withβ1 = 0.9, 4https://www.anthropic.com/index/100k-context-windows 14Bifurcated Attention: Accelerating Massively Parallel Decoding with Shared Prefixes in LLMs β2 = 0.95, and ϵ = 10−8. The warm-up steps were set to 2000, and a cosine annealing learning rate schedule was employed after reaching the peak learning rate. The minimum learning rate was set to 10% of the corresponding peak learning rate. A weight decay ((Loshchilov and Hutter, 2017)) of 0.01 and gradient clip- ping of 1.0 were applied to enhance training stability. Following the approach in ((Shoeybi et al., 2019)), the standard deviation for random weight initialization was rescaled for larger models. Our training pipeline is based on PyTorch Lightning and we use bfloat16 ((Kalamkar et al., 2019)) and DeepSpeed ((Rasley et al., 2020)) for training optimization. Finally, a random split of 0.1% of the data was reserved as a validation set. C.2. Model Configurations For each model size we train three models with attention variations; multi head where g = h, multi group where 1 < g < hand multi query where g = 1. Additionally, for 672m and 2.8b models we train a multi group model variant where the fanout in feed forward layer is decreased from 4 ×d to 2 ×d. Each model variant yields different number of total parameters therefore we group these models into family of model sizes. The detailed architectural choices for each of the model family is found in the table 3. C.3. Model Details of 1B Latency Experiment In Section 5.2.2, we use candidate models of sizes roughly 1B to study the effect of bifurcated attention. We outline the hyperpa- rameters of such models below. C.4. Ablation Studies: 2d Intermediate Feature Dimension One can also argue that different g results in different balance of the number of parameters in the feedforward versus the attention components. We performed an ablation study where we reduce the typical intermediate feature size of 4d to 2d and train models for three model sizes (which we will refer to as the 2d experiment). The ablation study reveals that the scaling laws curves for the 2d experiment crosses the usual 4d curves, which implies that the reduced size of the attention component alone compared to feedforward does not provide a consistent explanation of model capabilities. This can be seen from Figure 9. C.5. Inference Setup We use Nvidia A100 GPUs for inference hardware (Choquette et al., 2021). We perform latency studies using Deepspeed in- ference (Rasley et al., 2020) on top of Huggingface transformers (Wolf et al., 2019), where we wrote custom code to handle the gen- eralize multi-group attention as well as bifurcated attention. Future work includes extending the implementation to FasterTransformer (NVIDIA). D. Multi-Group Attention Family D.1. Detailed Analysis on Memory Access We show in Table 5 that the memory IO cost for ⟨q, K⟩ is domi- nated by the loading of K which costs bmhk in the case of multi- head where g = h. This cost is particularly high due to the coupling of batch size b, context length m, and the entire hidden dimension d. Compared to the number of computations, which has complexity bmd, this attention module requires one memory IO per one tensor operation (memory-io bound). In contrast, other operations such as feedforward has much lower ratio of memory IO per compute (compute bound). These attention computation can be the main bottleneck for incremental decoding and our paper aims to tackle such problems. Concretely, we can see that the context encoding in single-batch scenario in Appendix 5.2.1 is 400 ms for context length 10000, implying that the amortized latency per token during this phase is 0.04 ms per token. However, the per token latency during incremental decoding is in the order of ≈ 10 ms per token, which is 10 0.04 = 250times slower. This number clearly demonstrates that compute is not a dominating factor, but the memory IO required to load both model and KV cache. D.2. Model FLOPs The scaling laws by Kaplan et al. (2020) shows that the model- related FLOPs during the forward pass is 2N where N is the number of parameters (without the embeddings). We show that it holds for a general multi-group model as well. The only difference between the multi-group and the multi-head case is the projection PK and PV where they are of size dgk instead of dhk. Since this is a linear layer, the forward pass FLOPs for any input is still proportional such projection size. Therefore, it follows that for any multi-group attention, including multi-head, the forward FLOPs is 2N where N is the respective number of parameters. D.3. Comparing Capabilities-Equivalent Models This section outlines the analysis of latency change when we switch from an MH model to an MG model with F times the size. D.3.1. C ONTEXT ENCODING The dominating factor for latency in context encoding is the com- pute rather than the memory IO. The compute can be broken down into two parts (a) tensor projections related to model parameters and (b) KV attention involving no model parameters. For both parts, the large multi-group model will involve higher latency proportional to the size factor F. The context encoding time is ∝ N × bm where N is the model size except embeddings for (a) since the FLOPs per token is2N (Kaplan et al., 2020), which holds for all multi-group attention (Appendix D.2). For (b), the encoding time is ∝ ℓ · bhm2 ∝ Nbm2 for (b). Overall, the multi-group model with similar capabilities as the multi-head model will incur slightly higher context encoding time due to the larger size since N to increase to F N. D.3.2. I NCREMENTAL DECODING The incremental decoding component can dominate the overall inference latency compared to the context encoding, especially in the scenario where we decode in many steps. Incremental decoding is memory bound, meaning that the latency of this step is limited by memory I/O throughput. We can divide the memory I/O usage into two parts: reading (a) model parameters and (b) cached key- value pairs. With multi-group, we expect the model parameters to increase by a factor of F(g), leading to an increase in I/O usage in (a) by the same factor. The memory IO in (b) changes by a factor of g h when moving from multi-head with KV cache size 2bhmk to multi-group with cache size 2bgmk (more precisely g h · F(g) but g h is a dominating term since F(g) is close to 1). 15Bifurcated Attention: Accelerating Massively Parallel Decoding with Shared Prefixes in LLMs Table 2: Training Hyperparameters Model Size Total Training Steps Batch Size Compute Nodes Max Learning Rate 125M 400k 256 8 2.5 × 10−4 672M 200k 256 8 2.5 × 10−4 2.8B 200k 512 16 1.6 × 10−4 13B 100k 1024 32 1.0 × 10−4 Table 3: Model Specifications table presenting architecture details for the three variants: multi head (MH), multi query (MQ), and multi group (MG) including parameter count, number of attention groups, head dimensions, and number of layers. The additional fanout-based MG variant is described here as MG + 2 × d Model Family Attention Type groups d head nlayer Nparams (bil- lions) 125M MH 12 64 12 0.125 MG 4 64 12 0.115 MQ 1 64 12 0.112 672M MH 20 72 24 0.672 MG 4 72 24 0.592 MG + 2 × d 4 72 24 0.393 MQ 1 72 24 0.578 2.8B MH 24 128 24 2.878 MG 4 128 24 2.501 MG + 2 × d 4 128 24 1.595 MQ 1 128 24 2.444 13B MH 40 128 40 12.852 MG 8 128 40 11.174 MQ 1 128 40 10.807 E. Context-Aware Bifurcated Attention E.1. Proof Here, we outline the proof that the proposed bifurcated attention in Equation 3 and 4 recovers the same attention as the operations in 1 and 2 for the case of single-context batch sampling. We use the fact that the KV part corresponding to context length, all the batch indices correspond to the tensors. ⟨q, K⟩ : einsum(bgpnk, bgmk) → bgpnm = einsum(bgpnk, bg(mc + md)k) → bgpnm = einsum(bgpnk, bgmck) → bgpnm⊕ einsum(bgpnk, bgmdk) → bgpnm = einsum(bgpnk, bgmck) → bgpnm⊕ einsum(bgpnk, gmdk) → bgpnm = ⟨q, Kc⟩ ⊕ ⟨q, Kd⟩ ⟨w, V⟩ : einsum(bgpnm, bgmk) → bgpnk = bnd = einsum(bgpnmc, bgmck) → bgpnk+ einsum(bgpnmd, bgmdk) → bgpnk = einsum(bgpnmc, gmck) → bgpnk+ einsum(bgpnmd, bgmdk) → bgpnk = ⟨wc, Vc⟩ + ⟨wd, Vd⟩ E.2. Detailed Memory I/O Analysis Overall, the memory I/O complexity changes from 16Bifurcated Attention: Accelerating Massively Parallel Decoding with Shared Prefixes in LLMs Table 4: Model Specifications for Latency Experiment in Section 5.2.2. Model Family Attention Type groups d head nlayer Nparams (bil- lions) 1B MH 20 128 12 1.077 MG 4 128 15 1.156 MQ 1 128 16 1.193 Table 5: Comparison of memory access and computation between Multi Head, Multi Query, and Multi Group attention mechanisms. The memory access is for incremental decoding with the query length n = 1. Operation Einsum Memory Access Computation Input (x): bd q = ⟨x, Pq⟩ bd, hdk→ bhk bd + hdk = bd + d2 bdhk = bd2 K = ⟨x, Pk⟩ (+Kprev) [MH] bd, hdk→ bhk (+bmhk) bd + d2 bdhk = bd2 [MQ] bd, dk→ bk (+bmk) bd + dk [MG] bd, gdk→ bgk (+bgmk) bd + gdk V = ⟨x, Pv⟩ (+Vprev) [MH] bd, hdv→ bhv (+bmhv) bd + d2 bdhv = bd2 [MQ] bd, dv→ bv (+bmv) bd + dv [MG] bd, gdv→ bgv (+bgmv) bd + gdv logits = ⟨q, K⟩ [MH] bhk, bhmk→ bhm bhk + bhmk = bd + bmd bhmk = bmd [MQ] bhk, bmk→ bhm bd + bmk + bhm [MG] bhk, bgmk→ bhm bhk + bgmk + bhm weights: softmax bhm bhm out(O) = ⟨ weights, V ⟩ [MH] bhm, bhmv→ bhv bhm + bhmv = bhm + bmd bhmv = d [MQ] bhm, bmv→ bhv bhm + bmv + bhv [MG] bhm, bgmv→ bhv bhm + bgmv + bhv y = ⟨O, PO⟩ bhv, hdv→ bd bd + d2 bdhv = bd2 Total: Multi Head bd + bmd + d2 bhm + bmd + bd2 ≈ bd2 Total: Multi Query bd + bmk + d2 Total: Multi Group bd + bgmk + d2 r: Multi Head 1/d + m/d + 1/b r: Multi Query 1/d + m/(dh) + 1/b r: Multi Group 1/d + g/(dh) + 1/b • Original memory I/O cost: bhnk + bgmk + bhnm (for ⟨q, K⟩) + bhnm + bgmk + bnd (for ⟨w, V⟩) • Bifurcated attention memory I/O cost: bhnk + gmck + bgmdk + bhnm (for ⟨q, K⟩) + bhnm + gmck + bgmdk + bnd (for ⟨w, V⟩) There is an associated memory IO to write the⟨w, Vc⟩ and ⟨w, Vd⟩ output twice. However, it is typically very small (bnd) compared to the IO of KV cache component bgmk since m >> n= 1. E.3. Implementation of Bifurcated Attention Despite the dramatic gain in inference efficiency of the bifurcated attention, we demonstrate the simplicity of our implementation involving 20 lines of code using Pytorch (Paszke et al., 2019). 1 def attn(query, key, value, bifurcated_attention): 2 # <q,K> 3 if bifurcated_attention and type(key) == dict: 4 # g = number of groups 5 # h = gp where p = num heads per group 6 # n = 1 for incremental decoding 7 attn_weights_context = torch. einsum( 8 \"bgpnk,gmk->bgpnm\", query, key [\"context_past_key\"][0] 9 ) 10 attn_weights_incremental = torch. einsum( 11 \"bgpnk,bgmk->bgpnm\", query, key[\"incremental_past_key\"] 12 ) 13 attn_weights = torch.cat( 14 [attn_weights_context, attn_weights_incremental], dim=-1 15 ) 16 else: 17 attn_weights = torch.einsum( 18 \"bgpnk,bgmk->bgpnm\", query, key 19 ) 20 # softmax and causal mask (omitted) 21 # <w,V> 22 if bifurcated_attention and type( value) == dict: 23 # n = 1 for incremental decoding 24 context_past_value_length = value[ \"context_past_value\"].size(-2) 25 attn_output_context = torch.einsum ( 26 \"bgpnm,gmv->bgpnv\", 27 attn_weights[:, :, :, :, : context_past_value_length], 28 value[\"context_past_value\" ][0], 29 ) 30 attn_output_incremental = torch. einsum( 31 \"bgpnm,bgmv->bgpnv\", 32 attn_weights[:, :, :, :, context_past_value_length:], 33 value[\"incremental_past_value\" ], 34 ) 35 attn_output = attn_output_context + attn_output_incremental 17Bifurcated Attention: Accelerating Massively Parallel Decoding with Shared Prefixes in LLMs 125M 672M 2.8B 13B model size (w/o embeddings) 0.60 0.65 0.70 0.75 0.80 0.85 validation loss Validation loss vs size multi group multi head multi query multi group fanout 2d 125M 672M 2.8B 13B model size (w/o embeddings) 4 6 8 10 12 14pass@1 Average pass rates vs size multi group multi head multi query multi group fanout 2d Figure 9: Capabilities versus size plots including the 2d-intermediate-size feedforward model. The plot shows that the balance between the number of feedforward parameters and the attention parameters alone does not explain the relative expressiveness of multi-head, multi-group, and multi-query attentions. Rather, we argue that what explains relative expressiveness is the representation power associated with the key and value tensors (Section 5.1). 36 else: 37 attn_output = torch.einsum( 38 \"bgpnm,bgmv->bgpnv\", attn_weights, value 39 ) 40 return attn_output F. Applications: Additional Results We demonstrate additional results to the evaluation in Section 5.4 on MBXP-Java and MBXP-Javascript, in addition to the Python results. We replace CodeGen-16B-mono with CodeGen-16B-multi for the evaluation on Java and JavaScript and use the same Star- Coder model. From Figure 10, we observe similar trends as in Python (Figure 8), which furthers demonstrates the wide appli- cability of of bifurcated attention in improving accuracy under latency-constrained scenarios. G. Compatibility with Speculative Decoding and Fast Decoding techniques Unlike standard auto-regressive decoding, fast decoding techniques such as Speculative decoding(Chen et al., 2023; Leviathan et al., 2022), Medusa (Cai et al., 2024), Lookahead (Fu et al., 2023), and Eagle (Li et al., 2024) attempt to decode multiple tokens at each step. This reduces I/O bandwidth requirements because model parameters and KV cache are fetched only once per step and can be amortized across all generated tokens. The fundamental principle behind these techniques is to first draft (or guess) a set of tokens (denoted as ng) and then validate their accuracy by parallelly decoding with the model. After each step, up to a tokens (where a ≤ ng) may be accepted as valid, allowing for memory usage amortization across these accepted tokens. This approach is successful because decoding is primarily constrained by memory I/O. The benefits of bifurcated attention are orthogonal to those of speculative sampling, leading to further memory I/O improvements. This can be observed by extrapolating per-step memory I/O costs from Section E.2 with ng raplacing n. Since m >> ng continues to hold, the advantages of bifurcated attention persist even when combined with speculative decoding. H. Experiments with GPTFast The implementation of context-aware bifurcated attention in native PyTorch demonstrates significant reductions in parallel sampling latency for both multi-headed attention (MHA) and grouped query attention (GQA) architectures. Bifurcated attention, being context- aware and implemented natively in PyTorch, can directly benefit from PyTorch’s compilation capabilities. We observe Bifurcated attention outperforming FlashAttention2, 18Bifurcated Attention: Accelerating Massively Parallel Decoding with Shared Prefixes in LLMs (a) MBXP Java (b) MBXP JavaScript Figure 10: Bifurcated attention enables high batch sampling with minimal increase in latency with respect to batch size, resulting in more diverse and improved quality samples as indicated by pass@n and pass@top3 on MBXP-Java (Figure 10a). This trend extends to other evaluation such as JavaScript (Figure 10b) and Python (Figure 8). especially for larger context lengths and higher degrees of tensor parallelism. Since Bifurcated attention is primarily targeting de- code phase during inference, leveraging the efficiency of FlashAt- tention2 for the prefill (context encoding) step. H.1. In Comparison with FlashAttention FlashAttention is a highly efficient general-purpose fused atten- tion kernel that is particularly effective during context encoding, as it avoids materializing the expensive-to-read-and-write n × n attention matrix in GPU memory. However, during incremental decoding with single-context batch sampling, native FlashAttention kernels are not as efficient because they are not designed to be context-aware. Specifically, if there are B batch indices of K,V cache that are duplicate in values due to the shared prefix, FlashAttention-2 (FA2) can use paged KV-Cache to refer and point them to the same KV-pairs for the prefix across a batch. Nevertheless, this does not prevent the FlashAttention kernel from performing multiple reads of the KV-pairs from the shared prefix. Table 6 shows that a context-aware approach such as bifurcated at- tention outperforms FlashAttention in parallel sampling scenarios, especially with an increasing number of parallel samples. Notably, the bifurcated attention kernel is utilized solely during the decode step, allowing the efficient FlashAttention2 kernel to be employed during the prefill step for context lengths up to 8192. Furthermore, while non-contiguous memory avoids out-of-memory issues dur- ing parallel sampling for non-context-aware kernels, bifurcated attention’s memory setup, which maintains only one copy of the context and expands by reference across batch indices, achieves substantially lower latencies. However, the native FlashAttention2 implementation is not yet compatible with PyTorch’s compilation capabilities. In the future, it may be possible to combine bifurcated attention with FlashAttention to optimize the latency further. H.2. Trends with Grouped Querry Attention (GQA) For GQA architectures, bifurcated attention is able to help scale to very large inference workloads. Using PyTorch’s compilation mode, the inference with bifurcated attention is much faster com- pared to FlashAttention2. Table 7 presents the results for context lengths of 8K, 16K, and 32K tokens. Note that PyTorch’s SDPA is not directly supported for GQA and thus not included in the comparison. H.3. Compatibility with Tensor Parallel (TP) Higher tensor parallelism is often required to handle higher infer- ence workloads, as seen in Table 8. The proposed context-aware 19Bifurcated Attention: Accelerating Massively Parallel Decoding with Shared Prefixes in LLMs bifurcated attention method works out-of-the-box without addi- tional modifications for tensor parallelism. With TP we get to work with much larger context lengths. 20Bifurcated Attention: Accelerating Massively Parallel Decoding with Shared Prefixes in LLMs Table 6: Per-token generation latency (ms) with bifurcated attention compared to native Flash attention 2 kernel and Torch’s SDPA attention kernel implementations, with and without using the torch compile option. Measurements are taken using a 7B parameter model (32 layers, 32 heads, hidden dimension = 4096) with multi-head attention. SDPA Math represents the default attention operations by Torch, while SDPA Flash utilizes Flash attention under the hood. \"NC\" refers to the use of non-contiguous memory allocation for the cache, allowing reuse of the cache from the prompt. Note that Flash attention kernels are currently not compatible with torch-compile. The experiment results below utilize an Nvidia H100 GPU. without Torch Compile with Torch Compile BS Bifurcated Flash2 SDPA Math SDPA Flash Flash2 SDPA Flash SDPA Math Bifurcated SDPA Math (NC) (NC) (NC) Context Length : 8k 1 30.38 24.06 26.39 22.00 24.54 23.43 10.66 8.63 8.77 2 31.37 24.49 28.70 24.77 31.53 31.66 14.45 11.74 10.50 4 31.44 39.66 43.36 38.86 50.54 51.06 23.20 12.03 13.22 8 33.72 60.92 72.70 61.22 84.52 84.99 35.42 12.36 17.33 16 31.70 109.64 132.89 109.45 155.85 159.82 63.68 12.59 26.19 32 31.78 205.57 251.02 205.92 305.39 306.60 120.39 13.47 - 64 35.26 OOM OOM - 599.08 601.48 238.19 15.35 - 128 48.69 - - - 1183.46 OOM OOM 19.56 - 256 75.21 - - - 1842.98 - - 27.15 - 512 130.58 - - - - - - 44.33 - 1024 242.73 - - - - - - 81.14 - 2048 473.74 - - - - - - OOM - Context Length : 16k 1 30.66 26.28 30.13 26.22 30.49 30.20 15.53 12.16 13.06 2 32.62 37.72 44.74 38.25 51.30 51.24 22.46 17.17 15.35 4 33.44 65.98 73.62 65.83 91.25 90.76 39.51 17.33 20.65 8 34.67 110.31 132.29 110.55 159.96 160.39 64.22 18.07 32.06 16 36.78 206.93 251.47 206.52 306.75 307.31 119.87 18.46 OOM 32 41.93 OOM OOM OOM 601.10 603.61 237.89 19.92 - 64 50.53 - - - 1195.35 OOM OOM 22.96 - 128 68.31 - - - 1908.23 - - 28.98 - 256 106.10 - - - OOM - - 40.07 - 512 183.14 - - - - - - 65.02 - 1024 339.74 - - - - - - 117.75 - 2048 660.20 - - - - - - OOM - Context Length : 32k 1 39.97 37.67 44.94 37.46 67.44 67.30 30.39 20.90 19.80 2 48.61 55.94 69.22 55.86 156.61 156.35 47.63 29.34 OOM 4 49.77 OOM OOM OOM 300.47 300.97 90.19 29.73 - 8 51.31 - - - 567.93 568.81 152.19 30.30 - 16 54.92 - - - 670.21 672.42 290.59 30.66 - 32 62.28 - - - 1318.05 1323.25 569.74 32.15 - 64 75.22 - - - OOM OOM OOM 35.25 - 128 101.18 - - - - - - 41.44 - 256 159.09 - - - - - - OOM - 512 277.05 - - - - - - - - 1024 OOM - - - - - - - - 21Bifurcated Attention: Accelerating Massively Parallel Decoding with Shared Prefixes in LLMs Table 7: Per-token generation latency (ms) with bifurcated attention compared to the native Flash attention kernel. Measure- ments are taken with a 7B parameter model (32 layers, 32 heads, hidden dimension = 4096, 8 kv heads) using grouped query attention. Note that Flash attention kernels are currently not compatible with torch-compile. In this table, \"NC\" refers to the use of non-contiguous memory allocation for the cache, allowing reuse of the cache from the prompt. The experiment results below utilize an Nvidia H100 GPU. BS Bifurcated + Compile Bifurcated Flash2 Flash 2 (NC) Context: 8k 16k 32k 8k 16k 32k 8k 16k 32k 8k 16k 32k 1 10.56 15.16 22.79 28.37 30.97 37.20 21.76 23.59 26.64 23.48 25.23 28.20 2 11.35 15.99 23.72 29.53 32.16 37.47 22.46 23.78 26.82 39.93 28.53 45.70 4 11.52 16.20 23.98 29.58 32.19 37.48 22.57 24.22 27.30 71.57 42.47 72.94 8 11.79 16.61 24.59 29.58 32.41 38.12 22.65 24.03 28.36 126.35 70.01 127.96 16 11.72 16.68 24.87 30.27 32.85 37.29 22.31 30.19 OOM 240.96 130.77 245.81 32 12.50 17.77 27.01 29.76 32.75 37.84 26.06 OOM 468.93 244.54 467.61 64 13.87 19.90 30.31 29.52 32.07 45.73 OOM 403.08 482.71 463.55 128 17.03 24.90 37.60 29.55 40.26 63.06 788.66 465.70 909.02 256 24.38 33.76 52.06 40.07 59.42 96.28 915.89 1805.60 512 39.08 OOM OOM 65.74 OOM OOM OOM OOM 1024 72.24 118.57 2048 OOM 230.88 Table 8: Per-token generation latency (ms) Mistral 7B at different context lengths with TP=2 experimented on Nvidia’s H100 GPU. Context BS SDPA Bifurcated Flash2 16384 16 131.46 55.51 92.11 32640 8 133.85 58.56 92.35 32640 16 246.53 58.00 162.02 32640 32 OOM 57.86 OOM 32640 64 60.33 32640 128 67.82 22",
      "meta_data": {
        "arxiv_id": "2403.08845v2",
        "authors": [
          "Ben Athiwaratkun",
          "Sujan Kumar Gonugondla",
          "Sanjay Krishna Gouda",
          "Haifeng Qian",
          "Hantian Ding",
          "Qing Sun",
          "Jun Wang",
          "Jiacheng Guo",
          "Liangfu Chen",
          "Parminder Bhatia",
          "Ramesh Nallapati",
          "Sudipta Sengupta",
          "Bing Xiang"
        ],
        "published_date": "2024-03-13T16:30:57Z",
        "pdf_url": "https://arxiv.org/pdf/2403.08845v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The study introduces bifurcated attention, a novel method designed to enhance language model inference in shared-context batch decoding scenarios. It addresses the critical challenge of redundant memory IO costs that contribute to latency in high batch sizes and extended context lengths. The core contribution is strategically dividing the attention mechanism during incremental decoding into two separate General Matrix Multiply (GEMM) operations: one focusing on the Key-Value (KV) cache from prefill (context) and another on the decoding process itself. This approach maintains the computational load (FLOPs) of standard attention mechanisms while significantly reducing memory IO. Empirical results show over 2.1× speedup when sampling 16 output sequences and more than 6.2× speedup when sampling 32 sequences at context lengths exceeding 8k tokens on a 7B multi-head attention model. This efficiency enables massively parallel answer generation without substantially increasing latency and improves performance when integrated with post-processing techniques like re-ranking. Additionally, the paper investigates multi-query and multi-group attention, demonstrating capability-latency trade-offs and showing that bifurcated attention can make multi-head attention competitive with multi-query attention in certain batch sampling scenarios.",
        "methodology": "The proposed context-aware bifurcated attention method splits any attention in the multi-group family into two parts during incremental decoding for single-context batch sampling. Specifically, the query-key attention (`<q, K>`) is bifurcated into attention associated with the KV cache from the single context (`<q, Kc>`) and attention associated with KV cache from prior incremental decoding steps (`<q, Kd>`). These two parts are then joined together via concatenation. Similarly, the weight-value attention (`<w, V>`) is bifurcated into `<wc, Vc>` and `<wd, Vd>` and then joined back via summation. This formulation ensures that the computation yields identical results to the original attention but significantly reduces memory I/O by preventing redundant loading of the shared context KV cache across batch indices. The memory I/O complexity for loading KV is reduced from `gk * b(mc + md)` to `gk * (mc + bmd)`, where `gk` is a factor, `b` is batch size, `mc` is context length, and `md` is incremental decoding length.",
        "experimental_setup": "Experiments were conducted on model capabilities and inference latencies. For capability comparison, models ranging from 125 million to 13 billion parameters were trained with multi-head (g=h), multi-group (1<g<h), and multi-query (g=1) attention, using hyperparameters similar to GPT-3. Validation loss and average pass rates on multi-lingual HumanEval and MBXP code generation benchmarks were used as evaluation metrics. For latency studies, approximately 1 billion parameter multi-head and multi-query models (with the MQ model being 1.1x larger to match capabilities) were used. Inference was performed on Nvidia A100 and H100 GPUs, utilizing Deepspeed inference on Huggingface transformers with custom code for generalized multi-group attention and bifurcated attention. The primary scenario was single-context batch sampling, measuring per-step and total latencies across varying context lengths and batch sizes. Compatibility with Torch-Compile (GPTFast) was tested on a 7B parameter multi-head model. Applications were demonstrated using CodeGen-16B-mono and StarCoder (15.5B) on the MBPP, MBXP-Java, and MBXP-Javascript datasets, evaluating pass@n and pass@top3 with nucleus sampling (p=0.95, temperature=0.8). Comparisons were also made against FlashAttention2 and Torch's standard SDPA kernel.",
        "limitations": "A limitation of bifurcated attention is observed in small workloads (low context length and batch size). In such scenarios, splitting the attention into two parts can result in less parallelization of the GEMM kernels, potentially leading to higher latency, especially for multi-query models. However, this can be addressed by dynamically switching between bifurcated attention for high workload scenarios and normal attention for small workloads, ensuring optimal performance.",
        "future_research_directions": "Future research directions include further optimizing inference latency by combining bifurcated attention with highly efficient general-purpose attention kernels like FlashAttention. While FlashAttention is known for its efficiency during context encoding, bifurcated attention excels in context-aware incremental decoding; thus, their synergistic combination is a promising area for further performance gains. Additionally, extending the implementation of bifurcated attention to other high-performance inference platforms, such as NVIDIA's FasterTransformer, is noted as a future work item to broaden its applicability and maximize its efficiency benefits."
      }
    },
    {
      "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache",
      "abstract": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI.",
      "full_text": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Zirui Liu * 1 Jiayi Yuan* 1 Hongye Jin 2 Shaochen (Henry) Zhong 1 Zhaozhuo Xu 3 Vladimir Braverman 1 Beidi Chen 4 Xia Hu 1 Abstract Efficiently serving large language models (LLMs) requires batching of many requests to reduce the cost per request. Yet, with larger batch sizes and longer context lengths, the key-value (KV) cache, which stores attention keys and values to avoid re- computations, significantly increases memory de- mands and becomes the new bottleneck in speed and memory usage. Additionally, the loading of the KV cache causes the computational core to be idle, which limits the inference speed. A straight- forward and effective solution to reduce KV cache size is quantization, which decreases the total bytes taken by KV cache. However, there is a lack of in-depth studies that explore the element dis- tribution of KV cache to understand the hardness and limitation of KV cache quantization. To fill the gap, we conducted a comprehensive study on the element distribution in KV cache of popular LLMs. Our findings indicate that the key cache should be quantized per-channel, i.e., group ele- ments along the channel dimension and quantize them together. In contrast, the value cache should be quantized per-token. From this analysis, we de- veloped a tuning-free 2bit KV cache quantization algorithm named KIVI. With hardware-friendly implementation, KIVI can enable Llama, Falcon, and Mistral models to maintain almost the same quality while using 2.6× less peak memory (in- cluding model weight). This reduction in memory usage enables up to 4× larger batch size, bringing 2.35× ∼3.47× throughput on real LLM infer- ence workload. The source code is available at https://github.com/jy-yuan/KIVI. *Equal contribution . The order of authors is determined by flipping a coin. 1Rice University 2Texas A&M University 3Stevens Institute of Technology 4Carnegie Mellon University. Correspondence to: Zirui Liu <zl105@rice.edu>, Jiayi Yuan <jy101@rice.edu>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). 1. Introduction Large Language Models (LLMs) have demonstrated strong performance across a wide range of tasks (Brown et al., 2020; Taylor et al., 2022; Yuan et al., 2023; Chuang et al., 2024). However, their deployment is very costly, requiring a large number of hardware accelerators such as GPUs. Given these substantial costs, one natural way to reduce the cost per request is to combine a sufficient number of requests together for batch processing. However, in this batch infer- ence scenario, the key-value cache (KV cache), which holds the attention keys and values during generation to prevent re-computations, is becoming the new memory and speed bottleneck. This bottleneck becomes more pronounced with larger batch sizes and longer context lengths. For instance, in 540B PaLM, with a batch size of 512 and a context length of 2048, KV cache alone can take 3TB. This is 3 times the size of the model’s parameters (Pope et al., 2023). Also, the GPU SRAM has to load the whole KV cache from the GPU device memory for every token generated, during which the computational cores are idle. Thus, reducing KV cache size in LLMs while maintaining accuracy is important. Existing works towards this problem can be roughly divided into three categories. First, some work suggests reducing the number of heads in KV cache, such as multi-query attention (Shazeer, 2019) and multi-group attention (Ainslie et al., 2023). However, these methods require either training the model from scratch or fine-tuning the existing model. Second, another research line reduces KV cache size by evicting unimportant tokens (Zhang et al., 2023). Third, some other works try to solve this problem from the system perspective, e.g., offloading KV cache (Sheng et al., 2023) or extending virtual memory and paging techniques into the attention mechanism (Kwon et al., 2023). To reduce the size of KV cache, the most simple and ef- fective way is to reduce the total bytes taken by KV cache, namely, quantization. Unlike the well-studied weight quan- tization (Lin et al., 2023; Xiao et al., 2023a; Zhao et al., 2024), to the best of our knowledge, only a few studies applied the vanilla 4bit round-to-nearest quantization to KV cache (Sheng et al., 2023; Zhang et al., 2023; Zhao et al., 2024) due to the streaming nature of KV cache or other complications. There is a lack of in-depth studies that ex- 1 arXiv:2402.02750v2  [cs.CL]  25 Jul 2024KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache per-token  quantization  per-channel  quantization  Figure 1: Definition of per-token and per-channel quantiza- tion. X ∈ Rlprompt×d is key/value cache where lprompt is the number of tokens and d is the number of channels. zX is the zero-point, sX is the scaling factor. plore the element distribution of KV cache to understand the hardness and limitation of KV cache quantization. To fill the gap, we study the element distribution of KV cache. Our analysis suggests: • For key cache, there are a few fixed channels whose magnitudes are very large, which is consistent with previous finding (Lin et al., 2023; Xiao et al., 2023a). Thus, as shown in Figure 1 right, key cache should be quantized per-channel, i.e., group elements along the channel dimension and quantize them together. In this way, it can confine the error to each individual channel, without impacting the other normal channels. • For value cache, there is no obvious outlier pattern. Al- though value cache has no obvious outlier pattern, we experimentally show that it can only be quantized per- token because it is used to calculate the attention output, which is essentially a value cache mixer. As shown in Figure 1 left, the per-token quantization can confine the error inside each individual token and ensure that the quantization of one token does not adversely impact the others. Based on the above insights, we propose KIVI, a plug-and- play extreme low-bit KV cache quantization method. KIVI quantizes key cache per-channel and quantizes value cache per-token. The per-token value cache quantization aligns well with the streaming nature of auto-regressive inference, allowing newly quantized tensors to be directly appended to the existing quantized value cache by token dimension. However, for per-channel key cache quantization, the quan- tization process spans different tokens, which cannot be directly implemented in this streaming setting. Since the number of tokens in key cache can be arbitrary, our key idea is to split key cache into two parts. The first part is the grouped key cache, which contains several groups of tokens and each group has a certain number of tokens. The second part is the residual key cache, which does not have a suffi- cient number of tokens to form a complete group. Similarly, we split value cache into the grouped and residual parts to maintain the accuracy. We only apply group-wise quanti- zation to the grouped key cache and value cache, while the residual key cache and value cache are kept in full precision. The grouped and residual parts can be combined using tiled matrix multiplication when computing attention scores. Our contributions are summarized as follows: • Extensive analysis regarding the outlier patterns and quantization error of KV cache in commonly- used LLMs. Our observations suggest that key cache should be quantized per-channel and value cache should be quantized per-token. We also explain in depth why these caches require different quantization approaches. • A new plug-and-play 2bit KV cache quantization algorithm without any fine-tuning, KIVI, with hardware-friendly implementation. We conduct an extensive evaluation for KIVI with Llama, Mistral, and Falcon on popular generation tasks. KIVI can efficiently compress KV cache to 2bit and bring 2.6× peak memory usage reduction for Llama-2-7B, with little to no accuracy drop. With our efficient system im- plementation, this memory reduction, in return, enables up to 4× larger batch size and brings2.35× ∼3.47× throughput. 2. Background: Attention Inference-Time Workflow The LLM attention inference-time workflow involves two phases: i) the prefill phase, where the input prompt is used to generate KV cache for each transformer layer of LLMs; and ii) the decoding phase, where the model uses and updates KV cache to generate the next token, one at a time. Prefill Phase. Let X ∈ Rb×lprompt×d be the input tensor, where b is the batch size, lprompt is the length of the input prompt, and d is the model hidden size. For convenience, we ignore the layer index here. The key, value tensors can be computed by XK = XWK, XV = XWV , where WK, WV ∈ Rd×d are the key and value layer weight, respectively. After obtaining XK and XV , they are cached in the memory for the ease of decoding. Decoding Phase. Let t ∈ Rb×1×d be the current input token embedding. Let tK = tWK and tV = tWV be the key and value layer output, respectively. We first update KV cache: XK ← Concat(XK, tK), XV ← Concat(XV , tV ), 2KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache then calculate the attention output as: tQ = tWQ, A = Softmax(tQX⊤ K), tO = AXV , (1) where WQ is the weight matrix of the query layer. For ease of illustration, we ignore the attention output layer and the other parts of the inference workflow. Memory and Speed Analysis. The above process is re- peated until a special token indicating the sentence’s con- clusion is reached. Let lgen be the number of generated tokens. From the above analysis, the shape of KV cache is b ×(lprompt + lgen) ×d. To get a sense of the scale, consider the OPT-175B model with a batch size b 512, a prompt length lprompt 512, and an output length lgen 32. The KV cache requires 1.2TB, which is 3.8 times the model weights (Sheng et al., 2023). Besides the memory, the inference speed is also decided by the KV cache size. The GPU needs to load KV cache from GPU main memory to GPU SRAM once for every token generated during which the computa- tional core of the chip is essentially idle (Pope et al., 2023; Kwon et al., 2023). 3. Methodology In scenarios with long contexts or batched inferences, the memory and speed bottlenecks are storing and loading KV cache. The most simple and effective way to alleviate this problem is to reduce the total bytes occupied by KV cache, specifically, quantization. Following this motivation, we first evaluate the performance of the existing quantization method in Section 3.1. Our observations suggest that key and value cache should be quantized along different dimen- sions. We analyze the rationale behind this observation in Section 3.2. Then based on the analysis, we propose KIVI, a new KV cache quantization method along with its streaming data structure, detailed in Section 3.3. 3.1. Preliminary Study of KV Cache Quantization As we analyzed in Section 2, KV cache functions as a streaming data structure, where the new tensor arrives se- quentially. Thus, optimization-based methods like GPTQ (Frantar et al., 2022) are unsuitable for quantizing KV cache due to the overhead. To the best of our knowledge, the most flexible way for quantizing KV cache is the round- to-nearest quantization. The B−bit integer quantization- dequantization process can be expressed as: Q(X) = ⌊X − zX sX ⌉, X′ = Q(X) · sX + zX, where zX = min X is the zero-point, sX = (max X − min X)/(2B −1) is the scaling factor, and ⌊·⌉ is the round- ing operation. Here we ignore the batch size for ease of Table 1: The results of simulated KV cache group-wise quantization with various configurations. The group size is set as 32. C stands for per-channel quantization and T stands for per-token quantization. Please check the whole evaluation in Table 3. Llama-2-13B CoQA TruthfulQA 16bit 66.37 29.53 4bit (K - T, V - T) 66.48 29.51 2bit (K - T, V - T) 52.93 24.98 2bit (K - C, V - C) 2.88 0.74 2bit (K - T, V - C) 2.80 0.26 2bit (K - C, V - T) 63.53 28.60 understanding. As shown in Figure 1, X is quantized along either the token or channel dimension group-wisely. Considering the streaming nature of KV cache, previous studies often apply per-token quantization to both key and value cache since the newly quantized KV cache can be naively added to the existing quantized one along the token dimension (Sheng et al., 2023). While per-channel quanti- zation is non-trivial, we have designed a padding method to implement per-channel quantization to explore its effect on both key and value cache. Setting. In Table 1, we show the results of fake KV cache group-wise quantization with different configurations on the Llama-2-13B model for the CoQA and TruthfulQA tasks. We use a group size of 32 for all configurations. Here fake quantization means we simulate the quantization process by first quantizing KV cache into lower precision and then dequantizing it in the attention layer. For per-channel quan- tization, if the number of tokens is not divided evenly into groups, we add zero-padding to ensure it can be grouped perfectly. In this way, we ensure that all tokens in KV cache are quantized for a fair comparison. The detailed experi- mental setting can be found in Section 4.1. Specifically, we observe that: OB 1. When using the commonly used per-token quanti- zation to both key and value caches, INT4 precision can maintain accuracy. However, reducing it to INT2 results in a notable accuracy drop. OB 2. When value cache is quantized per-channel, the accuracy significantly worsens regardless of how key cache is quantized. OB 3. When using a lower numerical precision such as INT2, the most accurate approach is to quantize key cache per-channel and value cache per-token. 3KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache 0 50 100 150 200 Token 0 25 50 75 100 125 Channel 0.0 2.5 5.0 7.5 10.0 12.5Absolute Value Llama-2-13B Layer 16 Head 0 Key Cache 0 50 100 150 200 Token 0 25 50 75 100 125 Channel 0.5 1.0 1.5 2.0Absolute Value Llama-2-13B Layer 16 Head 0 Value Cache 0 50 100 150 200 Token 0 25 50 75 100 125 Channel 0.0 2.5 5.0 7.5 10.0 12.5 Llama-2-13B Layer 31 Head 0 Key Cache 0 50 100 150 200 Token 0 25 50 75 100 125 Channel 0.5 1.0 1.5 2.0 2.5 Llama-2-13B Layer 31 Head 0 Value Cache 0 50 100 150 200 Token 0 20 40 60 Channel 0 2 4 6 8 10Absolute Value Falcon-7B Layer 16 Head 0 Key Cache 0 50 100 150 200 Token 0 20 40 60 Channel 0.5 1.0 1.5Absolute Value Falcon-7B Layer 16 Head 0 Value Cache 0 50 100 150 200 Token 0 20 40 60 Channel 0.0 2.5 5.0 7.5 10.0 Falcon-7B Layer 20 Head 0 Key Cache 0 50 100 150 200 Token 0 20 40 60 Channel 0.5 1.0 1.5 Falcon-7B Layer 20 Head 0 Value Cache Figure 2: Magnitude of key and value cache for Llama-2-13B and Falcon-7B. We observe (1) for key cache, there are a few channels whose magnitudes are very large. (2) for value cache, there is no obvious outlier pattern. 3.2. Why Key and Value Cache Should Quantize Along Different Dimensions? In Table 1, we observe that quantizing key cache per-channel and value cache per-token to 2bit results in a very small accu- racy drop. Here we analyze why this configuration delivers better accuracy. In Figure 2 we visualize the original KV cache distribution at different layers. We observe that in key cache, some fixed channels exhibit very large mag- nitudes, whereas in value cache, there is no significant pattern for outliers. Analysis of Key Cache. The above observation for key cache aligns with previous findings that certain fixed columns in activations exhibit larger outliers (Dettmers et al., 2022; Lin et al., 2023). The persistence of outliers within each channel means that per-channel quantization can con- fine the quantization error to each individual channel with- out impacting the other normal channels. Thus, Figure 2 explains why key cache should be quantized per-channel. In Table 2 we show key cache relative reconstruction error ∥XK−X′ K XK ∥F , along with the relative attention score error ∥A−A′ A ∥F where A′ = Softmax(tQX ′⊤ K ). We observe that the per-token quantization can lead to almost 5× larger at- tention score error than per-channel quantization, which is consistent with Figure 2. Table 2: The relative error statistics averaged over all layers and all heads Llama-2-13B K Per-Token K Per-Channel Avg. ∥ XK−X′ K XK ∥F 13.67 4.55 Avg. ∥ A−A′ A ∥F 47.00 9.60 Attention sparsity 84.3% V Per-Token V Per-Channel Avg. ∥ XV −X′ V XV ∥F 4.57 3.73 Avg. ∆ 3.55 49.89 Analysis of Value Cache. Unlike key cache, value cache does not show the channel-wise outlier pattern. Furthermore, Figure 2 alone cannot explain OB2, which indicates value cache should only be quantized per-token. This is because Figure 2 implies that errors should be comparable for both per-token and per-channel quantization, given the absence of a clear pattern. As shown in Equation (1), value cache is used to calculate the attention output tO. Instead of analyzing the quantization error of value cache XV , in Table 2 we analyze the relative error ∆ = ∥AXV −AX′ V AXV ∥F with different quantization configurations. Surprisingly, we observe that the per-token quantization error is almost 15× 4KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Q_MatMul Concat Quantization by Channel Quantization by Token MatMul Prefill Phase Decoding Phase* KVCache * we omit the value cache and  attention output calculation Full Precision Tensor Low Precision Tensor Figure 3: The overview of KIVI algorithm. For ease of illustration, we omit the value cache and attention output parts. The detailed pseudo-code is provided in Algorithm 1. Here “Q_Matmul” is the mix-precision matrix multiplication which fuses the dequantization with matrix multiplication at the tiling level. smaller than per-channel quantization, which explains why OB2 happens. The intuition behind this observation stems from the attention sparsity. Equation (1) can be written as: [AXV ]i∗ = lpromptX j=1 Aij[XV ]j∗, (2) where [XV ]j∗ is the j-th row of XV . From Equation (2), the attention output is the weighted summation of value cache across various tokens, with the weights being the attention scores. Since the attention score is highly sparse (Tian et al., 2023), the output is just the combination of value caches of a few important tokens. The per-token quantization can confine the error to each individual token. Thus, quantizing other tokens does not affect the accuracy of important tokens. Consequently, per-token quantization leads to a much smaller relative error ∆. 3.3. KIVI: Algorithm and System Support Algorithm. As we previously analyzed, key cache should be quantized per-channel and value cache should be quan- tized per-token. Recall that key and value cache of newly generated tokens arrive sequentially. From the implemen- tation perspective, per-token quantization aligns well with streaming settings, allowing newly quantized tensors to be directly appended to the existing quantized value cache by token dimension. However, for per-channel quantiza- tion, the quantization process spans across different tokens, which cannot be directly implemented in the streaming set- ting. As shown in Figure 3, our key idea to solve this prob- lem is to group key cache everyG tokens and quantize them separately. Because the number of tokens in XK can be ar- bitrary, we splitXK into two parts, namely, the grouped part XKg = XK[: l − r] and residual part XKr = XK[l − r :], where l is the number of tokens inside the current key cache XK, r is the number of residual tokens, where l − r can be divisible by G. Since XKg can be evenly divided into (l − r)/G groups, we only store Q(XKg ) with group-wise quantization, while XKr is kept in full precision. During the decoding process, each newly arrived key cache tK is added to XKr and once XKr reaches R tokens, which is a hyperparameter - residual length, we quantize and concatenate it with the previously quantized Q(XKG). Then we reset XKr to an empty tensor. We note that R should be divisible by G. With tiled matrix multiplication, the raw attention logits is then calculated as: Ag = tQQ(X⊤ Kg ), XKr = Concat([XKr , tK]), Ar = tQX⊤ Kr , A = Concat([Ag, Ar]). (3) For value cache, similar to key cache, we also split it into two parts and keep the most recent value cache in full pre- cision, namely, XVg and XVr . Specifically, we maintain a queue and each newly arrived value cache is pushed into the queue. Once the queue reaches the predefined residual length R, the most outdated value cache is poped. Then the poped value cache is quantized per-token and concatenated with the previously quantized value cache along the token dimension. As shown in Figure 3, we also emphasize that during the prefill phase, the exact key and value tensors are passed to the next layers, although only the quantized KV cache is retained in memory. The whole algorithm can be found in Appendix A Algorithm 1. Analysis. In KIVI, the grouped key cacheXKg and value cache XVg is quantized, while the residual key cache XKr and value cache XVr is kept in full precision. By design, there are at most R tokens inside XKr or XVr . In practice, we set R ≤ 128 and the sequence length lprompt + lgen is often much longer than R. Thus the memory overhead from XKr and XVr is negligible when considering the benefit from extreme low-bit quantization, especially for the long context scenarios. Also, since the newly arrived key and 5KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache value tensors are added to XKr and XVr in full precision, KIVI maintains a full precision KV cache sliding window for the local relevant tokens. This window size is expected to be R 2 for key cache, and R for value cache. Later in the experiment section, we show that this full precision sliding window is crucial for obtaining desirable perfor- mance on hard tasks, such as GSM8K. System Support. We provide a hardware-friendly imple- mentation for running KIVI on GPUs. To minimize the overhead, we have fused the dequantization process with matrix multiplication, e.g., Q_MatMul in Figure 3, using CUDA. We also implement the group-wise quantization ker- nel in Triton. Our method is fully compatible with weight- only quantization. 4. Experiments 4.1. Settings Models. We evaluate KIVI using three popular model families: Llama/Llama-2 (Touvron et al., 2023a;b), Fal- con (Penedo et al., 2023) and Mistral (Jiang et al., 2023). Llama and Mistral model is based on multi-head attention, while Falcon is based on multi-query attention (Shazeer, 2019). We use the Hugging Face Transformers codebase and implement the KIVI algorithm upon it. Following previous work (Sheng et al., 2023), the group size G in Algorithm 1 for quantization is set as 32 across all experi- ments, the residual length R for key and value cache is set to 128. Tasks. As we analyzed in Section 2, the KV cache size grows larger with a longer context. Thus, we evaluateKIVI under the normal context length and long context setting, respectively. Specifically, we adopt generation tasks from LM-Eval (Gao et al., 2021) for normal context length eval- uation and LongBench (Bai et al., 2023) for long context evaluation, respectively1. For LM-eval, we adopt CoQA (Exact match accuracy), TruthfulQA (BLEU score), and GSM8K (Exact match accuracy). For LongBench, we chose tasks from four subgroups. Specifically, Qasper (F1 score) is a Single-Document QA task; QMSum (ROUGE score) and MultiNews (ROUGE score) are Summarization tasks; TREC (classification score), TriviaQA (F1 score), and SAM- Sum (ROUGE score) are Few-shot Learning tasks; and LCC (similarity score) and RepoBench-P (similarity score) is Code Completion task. The maximum sequence length in LongBench was set to 8192 for the Mistral model and 4096 for other models. We also consider the needle-in-a-haystack 1The closed-end tasks such as MMLU are not ideal to evaluate KIVI since they only involve one decoding step and directly fetch the output logits, which is not suitable for studying the impact of compressed KV cache. task (NIAH) to evaluate the model’s long context retrieval ability after quantizing KV cache. Detailed NIAH setting can be found in Appendix B. 4.2. Accuracy and Efficiency Analysis 4.2.1. C OMPARISON BETWEEN DIFFERENT QUANTIZATION CONFIGURATIONS We first utilize the fake quantization to demonstrate the effec- tiveness of our asymmetric quantization, namely, quantizing key cache per-channel and value cache per-token. Here fake quantization is exactly the same as in Table 1. The results are shown in Table 3. We observe that “2bit (K per-channel, V per-token)” consistently achieves the best results com- pared to all other configurations. This is consistent with our previous analysis. We also note that for hard generation tasks such as GSM8K, the fake “2bit (K per-channel, V per-token)” quantization results are significantly worse than the full precision counterparts. However, for KIVI in Table 3, we observe that the accuracy drop is only around 2% for GSM8K across different models. As we analyzed in Section 3.3, the difference between fake “2bit (K per-channel, V per-token)” quantization and KIVI is that KIVI maintains a full precision key and value cache sliding window for the local relevant tokens. This sliding window is crucial to maintaining accuracy for hard generation tasks such as mathematical reasoning. 4.2.2. A CCURACY COMPARISON ON GENERATION TASKS LM-Eval Results. We benchmark KIVI in CoQA, Truth- fulQA and GSM8K tasks using the LM-Eval framework. All dataset parameters were set to default. We compare the standard 16bit configuration with our KIVI compression techniques in Llama-2-7B, Llama-2-13B, Falcon-7B and Mistral-7B. As shown in Table 3, we observe that for the Llama and Mistral model, KIVI only has up to 2% accu- racy drop despite the KV cache being stored in 2bit. For instance, in the Llama-2-7B model, the transition from 16bit to 2bit only slightly decreases accuracy. Similar trends are observed in other Llama-family models. Since Falcon-7B adopts multi-query attention and only has one head for KV cache, it is already highly compressed compared to Llama- based models. Thus, in Table 3, 4bit KIVI is needed to maintain the accuracy, while 2bit KIVI may have a large accuracy drop in this case. LongBench Results. The performance of KIVI over vari- ous models in the LongBench dataset is summarised in Table 4. We apply KIVI to Llama2-7B, Llama2-13B, Llama2-7B- Chat, Llama2-13B-Chat, Falcon-7B and Mistral-7B. Table 4 suggests that KIVI is an effective method for KV cache compression with minimal impact on accuracy across var- 6KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  27K tokens (a) Llama-3-8B-Instruct Baseline 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  27K tokens  (b) Llama-3-8B-Instruct + KIVI-2 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  27K tokens  (c) Llama-3-8B-Instruct + KIVI-4 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  30K tokens (d) Mistral-7B-Instruct-v0.2 Baseline 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  30K tokens  (e) Mistral-7B-Instruct-v0.2 + KIVI-2 0.5K2K4K7K9K11K13K16K18K20K Word Count 0.0 0.11 0.22 0.33 0.44 0.56 0.67 0.78 0.89 1.0 Depth 20K words  30K tokens  (f) Mistral-7B-Instruct-v0.2 + KIVI-4 0.0 0.2 0.4 0.6 0.8 1.0 Figure 4: Needle-in-a-Haystack results on Llama-3-8B-Instruct and Mistral-7B-Instruct-v0.2. Here we count the number of words instead of tokens to better account for the tokenizer difference. The final token length is noted in the upper right corners of each figure. Detailed setting can be found in Appendix B. ious hard long context generation tasks. We present ad- ditional results using Llama3-8b, Mistral-7B-v0.2, and LongChat-7B-v1.5, which can be found in Appendix D. NIAH Results. From Figure 4, we observe that KIVI can still maintain the retrieval ability of LLMs even with 2bit KV Cache. Detailed NIAH setting can be found in Appendix B. 4.2.3. A BLATION In this section, we benchmark KIVI on GSM8K, one of the hardest generation tasks, to show the effect of hyperpa- rameters group size G and residual length R on the model performance. For full results of KIVI with a residual length of 32, please refer to Appendix C. The effect of group size. We fix the residual length at 128 and vary the group sizes to 32, 64, and 128. From Table 5, we observe that group sizes 32 and 64 yield similar results, whereas the performance significantly decreases when the group size reaches 128. Note the zero-point and the scaling factor mentioned in Section 3.1 are calculated according to this group size; where the choice of group size will greatly impact the KV cache compression effect under a long input. The effect of residual length. We fix the group size at 32 and vary the residual length across 32, 64, 96, and 128. As shown in Table 5, there is no consistent pattern between residual lengths and model accuracy. Namely, while a resid- ual length of 128 achieves good results, 32 and 96 yield similar outcomes, but a residual length of 64 results in the worst performance. We emphasize that while we observe no significance among residual lengths of {32, 96, 128}, hav- ing a reasonably large residual length is important; as it brings much performance boosts on hard tasks like GSM8K, again as shown in Table 5. 4.2.4. E FFICIENCY COMPARISON To evaluate the wall-clock time efficiency ofKIVI, follow- ing vLLM (Kwon et al., 2023), we synthesize workloads based on ShareGPT (sha, 2023), which contain input and output texts of real LLM services. On average, the data set has an input prompt length lprompt of 161 and an output length lgen of 338 (Kwon et al., 2023). We increase the batch 7KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Table 3: Performance comparison between 16bit, 4-bit per- token quantization, four fake 2bit KV cache quantization similar to those in Table 1, KIVI-2 (2bit) / KIVI-4 (4bit) across various models. We emphasize that unlike KIVI, which preserves a small portion of full precision key cache XKr and value cache XVr , all tokens in fake KV cache quantization are quantized for a fair comparison. C stands for per-channel quantization and T stands for per-token quantization. Model CoQA TruthfulQA GSM8K Llama-2-7B 16bit 63.88 30.76 13.50 4bit (K -T, V -T) 64.82 29.85 12.28 2bit (K -C, V -T) 59.08 33.10 5.76 2bit (K -T, V -T) 39.88 18.29 0.83 2bit (K -C, V -C) 3.60 0.27 0.00 2bit (K -T, V -C) 1.30 0.49 0.08 KIVI-4 63.78 30.80 13.80 KIVI-2 63.05 33.95 12.74 Llama-2-13B 16bit 66.37 29.53 22.67 4bit (K -T, V -T) 66.73 29.14 20.92 2bit (K -C, V -T) 63.53 28.60 12.21 2bit (K -T, V -T) 52.93 24.98 4.55 2bit (K -C, V -C) 2.88 0.74 0.00 2bit (K -T, V -C) 2.80 0.26 0.08 KIVI-4 66.38 29.49 23.65 KIVI-2 66.23 29.84 20.77 Falcon-7B 16bit 59.83 23.20 4.55 4bit (K -T, V -T) 58.53 22.94 3.26 2bit (K -C, V -T) 43.93 20.82 1.29 2bit (K -T, V -T) 25.72 0.91 0.53 2bit (K -C, V -C) 41.95 17.11 1.52 2bit (K -T, V -C) 19.53 0.94 0.15 KIVI-4 59.67 22.58 4.47 KIVI-2 57.48 24.98 3.41 Mistral-7B 16bit 67.40 30.45 38.36 4bit (K -T, V -T) 67.80 29.83 36.85 2bit (K -C, V -T) 61.65 29.64 26.46 2bit (K -T, V -T) 54.55 25.86 5.00 2bit (K -C, V -C) 24.40 24.86 2.27 2bit (K -T, V -C) 10.73 19.12 0.99 KIVI-4 66.95 30.49 37.30 KIVI-2 66.35 32.17 36.01 size until out of memory and report the peak memory usage and throughput between KIVI (with residual length 32 and 128) and FP16 baseline for the Llama-2-7B model. The hardware here is a single NVIDIA A100 GPU (80GB). As shown in Figure 5, with similar maximum memory us- age, KIVI enables up to 4× larger batch size and gives 2.35× ∼3.47× larger throughput. This throughput num- ber can grow larger with longer context length and output length. We also note that this speed-up can be greatly increased if we further fuse the KV cache quantization process with previous operations. We leave it as one of future work. Figure 5: Memory usage and throughput comparison be- tween 2bit KIVI and 16bit baseline. KIVI can achieve higher throughput by enabling a larger batch size. 5. Related Work Many machine learning systems and benchmark works con- sider scaling up LLM inference process (Pope et al., 2023; Yuan et al., 2024). Among them, quantization techniques have been widely applied (Frantar et al., 2022; Lin et al., 2023; Kim et al., 2023; Xu et al., 2023). A main branch of LLM quantization is weight-only quantization, which in- volves the quantization of model weights to lower precision. For instance, AWQ (Lin et al., 2023) cleverly quantizes model weights to INT4 and INT3 using an activation-aware manner. GPTQ (Frantar et al., 2022) utilizes approximate second-order information to quantize model weights both accurately and efficiently. SqueezeLLM (Kim et al., 2023) adopts the concept of non-uniform quantization based on sensitivity along with dense-and-sparse decomposition. This line of work is orthogonal to ours, as they can be combined. SmoothQuant (Xiao et al., 2023a) is a post-training quan- tization method that is more closely related to our work. This method uses equivalent transformations to balance the quantization complexity for both activation and weight, making the activation easier to quantize. SmoothQuant can compress KV cache to 8bit with minor performance loss. However, it faces a significant accuracy drop when scaled down to 4bit or less (Zhao et al., 2024). FlexGen (Sheng et al., 2023) adopts 4-bit group-wise quantization for both key and value cache. 8KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Table 4: Performance evaluation of KIVI on various models across a range of benchmarks in LongBench. We highlight the average performance of our method. More similar results on Mistral-7B-v0.2 and LongChat-7b-v1.5 can be found in Table 10 and Table 9 Model Qasper QMSum MultiNews TREC TriviaQA SAMSum LCC RepoBench-P Average Llama2-7B 16bit 9.52 21.28 3.51 66.00 87.72 41.69 66.66 59.82 44.52 KIVI-4 9.28 21.42 3.88 66.00 87.72 41.82 66.80 59.83 44.59 KIVI-2 9.31 20.50 1.14 66.00 87.42 42.71 66.88 60.23 44.27 Llama2-13B 16bit 9.32 21.38 3.71 70.00 87.87 43.55 66.61 56.42 44.85 KIVI-4 9.16 20.86 3.21 69.00 86.97 44.26 65.30 57.08 44.48 KIVI-2 8.58 20.69 6.19 69.50 87.78 44.30 65.08 55.46 44.69 Llama2-7B-Chat 16bit 19.65 20.54 26.36 63.00 84.28 41.12 59.75 52.93 45.95 KIVI-4 19.62 20.70 25.49 63.00 84.13 40.87 59.27 53.56 45.83 KIVI-2 19.32 20.46 25.48 63.00 84.84 40.60 58.71 52.97 45.67 Llama2-13B-Chat 16bit 24.18 20.37 25.69 67.50 86.90 42.18 50.23 50.64 45.96 KIVI-4 23.00 20.36 26.06 67.50 87.20 42.04 52.55 52.77 46.44 KIVI-2 23.59 20.76 25.25 67.5 87.17 41.56 49.93 48.45 45.52 Falcon-7B 16bit 1.48 2.35 11.09 13.00 5.84 2.44 23.86 9.69 8.71 KIVI-4 1.04 2.41 11.98 13.00 5.84 2.36 23.72 9.92 8.78 KIVI-2 1.98 3.61 6.78 10.00 6.24 2.73 22.18 10.12 7.95 Mistral-7B 16bit 8.12 19.98 19.99 67.50 89.80 41.69 66.59 58.99 46.58 KIVI-4 7.89 20.06 20.58 67.50 89.80 41.56 66.45 58.62 46.56 KIVI-2 6.92 19.71 17.92 66.50 89.63 41.66 65.52 58.99 45.85 Table 5: Ablation study of KIVI by changing group size G and residual length R. Model Group Size GSM8K Llama2-13B 32 20.77 64 21.00 128 17.29 Model Residual Length GSM8K Llama2-13B 32 20.62 64 19.86 96 20.55 128 20.77 One essential recipe of KIVI is the per-channel quantiza- tion scheme designed based on the observation made in Section 3.2 and Figure 2. ATOM (Zhao et al., 2024) also indicates that key cache exhibits more outliers compared to the value cache. KIVI provides further extensive analysis and leverages this observation to implement per-channel quantization. A similar observation and approach has been independently discovered and developed in the concurrent work KVQuant (Hooper et al., 2024). vLLM (Kwon et al., 2023) and S3 (Jin et al., 2023) are system-level works, which include memory management through the use of PagedAttention or memory usage pre- diction. They can lower the memory requirements of KV cache and simultaneously increase model throughput. This research direction is orthogonal to our work, since system- level optimizations can also be applied upon our algorithm. Several other works also consider compressing KV cache by evicting tokens. H2O (Zhang et al., 2023) retains only a small portion of tokens that contribute significantly to the attention scores. Similarly, Scissorhands (Liu et al., 2024) exploit the persistence of the importance hypothesis in KV cache sparsification. StreamingLLM (Xiao et al., 2023b) is based on the observation of “attention sink” and maintains only a few initial tokens to preserve performance. Unlike these works, our KIVI retains all input tokens and compresses them into lower precision. This line of work is orthogonal to ours, as they can also be combined together. 6. Conclusion and Future Work In this paper, we systematically analyze KV cache element distribution in popular LLMs. We conclude that key cache should be quantized per-channel and value cache should be quantized per token. Based on these observations, we propose KIVI, a plug-and-play 2bit KV cache quantization algorithm without the need for any tuning. In real LLM workload, KIVI allows up to 4× larger batch sizes and 3.47× throughput. In the future, we will further optimize the implementation to reduce the overhead of quantization process during the prefill and decoding phase. Acknowledgments The authors thank the anonymous reviewers for their help- ful comments. Dr. Beidi Chen is supported by a research gift from Moffett.AI. Dr. Vladimir Braverman is partially 9KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache supported by the Ministry of Trade, Industry and Energy (MOTIE) and Korea Institute for Advancement of Technol- ogy (KIAT) through the International Cooperative R&D pro- gram, the Naval Research (ONR) grant N00014-23-1-2737, and NSF CNS 2333887 award. Dr. Xia Hu is supported by NSF grants NSF IIS-2224843. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. References ShareGPT Team. https://sharegpt.com/, 2023. Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan- guage models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. Yu-Neng Chuang, Songchen Li, Jiayi Yuan, Guanchu Wang, Kwei-Herng Lai, Leisheng Yu, Sirui Ding, Chia-Yuan Chang, Qiaoyu Tan, Daochen Zha, et al. Understand- ing different design choices in training large time series models. arXiv preprint arXiv:2406.14045, 2024. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, An- thony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al. A frame- work for few-shot language model evaluation. Version v0. 0.1. Sept, page 8, 2021. Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W Mahoney, Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cache quantization. arXiv preprint arXiv:2401.18079, 2024. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Yunho Jin, Chun-Feng Wu, David Brooks, and Gu-Yeon Wei. S3: Increasing gpu utilization during genera- tive inference for higher throughput. arXiv preprint arXiv:2306.06000, 2023. Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611–626, 2023. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023. Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and An- shumali Shrivastava. Scissorhands: Exploiting the per- sistence of importance hypothesis for llm kv cache com- pression at test time. Advances in Neural Information Processing Systems, 36, 2024. Amirkeivan Mohtashami and Martin Jaggi. Landmark at- tention: Random-access infinite context length for trans- formers. arXiv preprint arXiv:2305.16300, 2023. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobei- dli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outper- forming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023. 10KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Ja- cob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling trans- former inference. Proceedings of Machine Learning and Systems, 5, 2023. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrit- twieser, et al. Gemini 1.5: Unlocking multimodal un- derstanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150, 2019. Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. Flexgen: High-throughput generative inference of large language models with a single gpu. In International Conference on Machine Learning, pages 31094–31116. PMLR, 2023. Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022. Yuandong Tian, Yiping Wang, Beidi Chen, and Simon Du. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. arXiv preprint arXiv:2305.16380, 2023. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar- tinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and ef- ficient post-training quantization for large language mod- els. In International Conference on Machine Learning, pages 38087–38099. PMLR, 2023a. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023b. Zhaozhuo Xu, Zirui Liu, Beidi Chen, Yuxin Tang, Jue Wang, Kaixiong Zhou, Xia Hu, and Anshumali Shrivastava. Compress, then prompt: Improving accuracy-efficiency trade-off of llm inference with transferable prompt. arXiv preprint arXiv:2305.11186, 2023. Jiayi Yuan, Ruixiang Tang, Xiaoqian Jiang, and Xia Hu. Large language models for healthcare data augmentation: An example on patient-trial matching. In AMIA Annual Symposium Proceedings, volume 2023, page 1324. Amer- ican Medical Informatics Association, 2023. Jiayi Yuan, Hongyi Liu, Shaochen Zhong, Yu-Neng Chuang, Songchen Li, Guanchu Wang, Duy Le, Hongye Jin, Vipin Chaudhary, Zhaozhuo Xu, Zirui Liu, and Xia Hu. Kv cache compression, but what must we give in return? a comprehensive benchmark of long context capable ap- proaches. arXiv preprint arXiv:2407.01527, 2024. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H _2 o: Heavy-hitter oracle for efficient generative inference of large language models. arXiv preprint arXiv:2306.14048, 2023. Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom: Low-bit quanti- zation for efficient and accurate llm serving. Proceedings of Machine Learning and Systems, 6:196–209, 2024. 11KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache A. Detailed Implementations In this section, we present the algorithm for KIVI as discussed in Section 3.3. Specifically, we provide the pseudocode for KIVI when calculating the attention output in the prefill and decoding phases. Algorithm 1: The KIVI Prefill & Decoding Algorithm parameter: group size G, residual length R procedure Prefill: Input: X ∈ Rlprompt×d XK = XWK, XV = XWV XVg = XV [: lprompt − R], XVr = XV [lprompt − R :] Q(XVg ) ← GroupQuant(XVg , dim=token, numGroup=d//G) Q(XKg ), XKr ← KeyQuant(XK) KV cache ← Q(XKg ), XKr , Q(XVg ), XVr return XK, XV end procedure Decoding: Input: KV cache, t ∈ R1×d tQ = tWQ, tK = tWK, tV = tWV Q(XKg ), XKr , Q(XVg ), XVr ← KV cache XKr ← Concat([XKr , tK], dim=token) XVr ← Concat([XVr , tV ], dim=token) if len(XKr ) = R then Q(XKr ), _ ←KeyQuant(XKr ) Q(XKg ) ← Concat([Q(XKg ), Q(XKr )], dim=token) XKr ← empty tensor. end if len(XVr ) > Rthen Q(XV ′r ) ← GroupQuant(XVr [: −R], dim=token, numGroup = d//G) Q(XVg ) ← Concat([Q(XVg ), Q(XV ′r )], dim=token) XVr ← XVr [−R :] end A ← Concat([tQQ(XKg )⊤, tQX⊤ Kr ], dim=token) Ag = Softmax(A)[: −R], Ar = Softmax(A)[−R :] tO ← AgQ(XVg ) + ArXVr KV cache ← Q(XKg ), XKr , Q(XVg ), XVr return tO end function KeyQuant(XK ∈ Rl×d): r = l%R, XKg = XK[: l − r], XKr = XK[l − r :] Q(XKg ) ← GroupQuant(XKg , dim=channel, numGroup=l//G) return Q(XKg ), XKr end 12KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache B. NIAH Setting We largely follows the passkey retrieval prompt template of Mohtashami and Jaggi (2023) but using 7-digit passkey and Paul Graham Essays2 as the background filler, as set forth in Arize-ai and Reid et al. (2024): There is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I will quiz you about the important information there. <prefix filled by Paul Graham Essays> The pass key is <7-DIGIT PASS KEY>. Remember it. <7-DIGIT PASS KEY> is the pass key. <suffix filler> What is the pass key? The pass key is C. More Ablation Results In our efficiency evaluation, we observe that with a residual length of 32, KIVI achieves a significantly higher memory compression rate, which in turn leads to increased throughput. Additionally, our ablation study reveals that changing the residual length from 128 to 32 does not result in a substantial performance gap. We demonstrate KIVI with a residual length of 32 across all benchmark datasets. As shown in Tables 6 and 7, KIVI with a residual length of 32 also delivers performance comparable to that of the 16-bit full model. Table 6: Performance comparison between 16bit, KIVI-2 (2bit) / KIVI-4 (4bit) with residual length 128 and 32 across various models. R32 stands for residual length 32. Model CoQA TruthfulQA GSM8K Llama-2-7B 16bit 63.88 30.76 13.50 KIVI-2 R128 63.05 33.95 12.74 KIVI-2 R32 62.85 33.01 13.57 Llama-2-13B 16bit 66.37 29.53 22.67 KIVI-2 R128 66.23 29.84 20.77 KIVI-2 R32 66.57 29.35 20.62 Falcon-7B 16bit 59.83 23.20 4.55 KIVI-4 R128 59.67 22.58 4.47 KIVI-4 R32 59.73 22.96 3.94 KIVI-2 R128 57.48 24.98 3.41 KIVI-2 R32 57.50 25.70 2.20 Mistral-7B 16bit 67.40 30.45 38.36 KIVI-2 R128 66.35 32.17 36.01 KIVI-2 R32 65.90 31.21 34.34 D. More Experimental Results We present additional results using Llama3-8B, Mistral-7B-v0.2, and LongChat-7B-v1.5 in LongBench, which can be found in Table 8, Table 9 and Table 10, respectively. We also show result of Needle-in-a-Haystack Test in Figure 4. The settings largely follow the format of the original passkey retrieval task (Mohtashami and Jaggi, 2023) while including some modern modifications set forward by Arize-ai and the technical report of Gemini 1.5 (Reid et al., 2024). 2https://paulgraham.com/articles.html 13KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Table 7: Performance evaluation of KIVI with residual length 128 and 32 on various models across a range of benchmarks in LongBench. R32 stands for residual length 32. Model Qasper QMSum MultiNews TREC TriviaQA SAMSum LCC RepoBench-P Average Llama2-7B 16bit 9.52 21.28 3.51 66.00 87.72 41.69 66.66 59.82 44.52 KIVI-2R128 9.31 20.50 1.14 66.00 87.42 42.71 66.88 60.23 44.27 KIVI-2R32 9.26 20.53 0.97 66.00 87.42 42.61 66.22 59.67 44.08 Llama2-13B 16bit 9.32 21.38 3.71 70.00 87.87 43.55 66.61 56.42 44.85 KIVI-2R128 8.58 20.69 6.19 69.50 87.78 44.30 65.08 55.46 44.69 KIVI-2R32 8.38 20.74 7.01 69.50 87.78 44.43 64.89 55.31 44.75 Llama2-7B-Chat 16bit 19.65 20.54 26.36 63.00 84.28 41.12 59.75 52.93 45.95 KIVI-2R128 19.32 20.46 25.48 63.00 84.84 40.60 58.71 52.97 45.67 KIVI-2R32 19.10 20.08 25.33 63.00 85.04 39.80 57.91 52.38 45.33 Llama2-13B-Chat 16bit 24.18 20.37 25.69 67.50 86.90 42.18 50.23 50.64 45.96 KIVI-2R128 23.59 20.76 25.25 67.50 87.17 41.56 49.93 48.45 45.52 KIVI-2R32 23.56 20.90 25.45 67.50 87.42 41.40 48.93 48.81 45.49 Falcon-7B 16bit 1.48 2.35 11.09 13.00 5.84 2.44 23.86 9.69 8.71 KIVI-4R128 1.04 2.41 11.98 13.00 5.84 2.36 23.72 9.92 8.78 KIVI-4R32 1.03 2.45 11.99 13.50 5.84 2.46 23.88 9.95 8.88 KIVI-2R128 1.98 3.61 6.78 10.00 6.24 2.73 22.18 10.12 7.95 KIVI-2R32 2.28 3.23 6.73 10.00 6.31 2.88 22.71 10.45 8.07 Mistral-7B 16bit 8.12 19.98 19.99 67.50 89.80 41.69 66.59 58.99 46.58 KIVI-2R128 6.92 19.71 17.92 66.50 89.63 41.66 65.52 58.99 45.85 KIVI-2R32 6.84 19.81 17.20 66.50 89.63 42.82 65.13 58.06 45.74 Table 8: The results of Llama-3-8B-Instruct with KIVI on LongBench. The model has 8K context length and applies group query attention, which uses 8 heads for KV cache instead of the full 32 heads. We use a 32 group size and 128 residual length for both KIVI-2 and KIVI-4. The baseline is of full precision. NarrativeQA Qasper MultiFieldQA HotpotQA MuSiQue 2WikiMQA GovReport QMSum Baseline 21.71 44.24 44.54 46.82 21.49 36.42 30.03 22.67 w./KIVI-2 21.35 43.17 44.49 46.79 20.56 37.05 29.98 22.07 w./KIVI-4 21.01 44.83 44.60 46.96 21.43 36.48 30.22 22.44 MultiNews LCC RepoBench-P TriviaQA SAMSum TRec PR Avg Baseline 27.79 57.00 51.22 90.23 42.53 74.50 67.00 45.21 w./KIVI-2 27.77 50.84 46.65 90.54 42.26 74.50 67.50 44.37 w./KIVI-4 27.97 57.36 52.03 90.33 42.97 74.50 66.50 45.31 Table 9: The results of Mistral-7B-Instruct-v0.2 with KIVI on LongBench. The model has 32K context length and applies group query attention, which uses 8 heads for KV cache instead of the full 32 heads. We use a 32 group size and 128 residual length for both KIVI-2 and KIVI-4. The baseline is of full precision. NarrativeQA Qasper MultiFieldQA HotpotQA MuSiQue 2WikiMQA GovReport QMSum Baseline 21.02 29.41 47.13 36.53 19.13 21.76 32.59 23.99 w./KIVI-2 20.61 28.73 44.88 35.47 17.95 20.68 32.55 23.65 w./KIVI-4 20.97 29.41 46.52 36.25 19.53 21.66 32.97 24.06 MultiNews LCC RepoBench-P TriviaQA SAMSum TRec PR Avg Baseline 27.09 53.49 51.40 86.23 43.04 71.00 89.33 43.54 w./KIVI-2 26.54 53.03 51.16 86.00 43.34 71.00 80.83 42.43 w./KIVI-4 26.89 53.33 51.41 86.23 43.34 71.00 89.42 43.53 14KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache Table 10: The results of LongChat-7B-v1.5-32K with KIVI on LongBench. The model has 32K context length. We use a 32 group size and 128 residual length for both KIVI-2 and KIVI-4. The baseline is of full precision. NarrativeQA Qasper MultiFieldQA HotpotQA MuSiQue 2WikiMQA GovReport QMSum Baseline 20.65 29.42 43.15 33.05 14.66 24.14 30.85 22.84 w./KIVI-2 20.79 28.69 41.02 32.91 13.82 23.00 30.47 22.59 w./KIVI-4 20.49 28.90 43.24 33.07 14.66 24.86 31.40 22.84 MultiNews LCC RepoBench-P TriviaQA SAMSum TRec PR Avg Baseline 26.55 54.83 58.94 83.99 40.75 66.50 30.50 38.72 w./KIVI-2 26.28 54.11 57.62 83.19 41.28 66.50 32.25 38.30 w./KIVI-4 26.52 54.06 58.77 83.88 40.62 67.00 31.50 38.79 15",
      "meta_data": {
        "arxiv_id": "2402.02750v2",
        "doi": "10.13140/RG.2.2.28167.37282",
        "authors": [
          "Zirui Liu",
          "Jiayi Yuan",
          "Hongye Jin",
          "Shaochen Zhong",
          "Zhaozhuo Xu",
          "Vladimir Braverman",
          "Beidi Chen",
          "Xia Hu"
        ],
        "published_date": "2024-02-05T06:06:47Z",
        "pdf_url": "https://arxiv.org/pdf/2402.02750v2.pdf",
        "github_url": "https://github.com/jy-yuan/KIVI"
      },
      "llm_extracted_info": {
        "main_contributions": "KIVI addresses the KV cache memory bottleneck in large language models (LLMs) by proposing a tuning-free, asymmetric 2-bit quantization algorithm. Through comprehensive element distribution analysis, it identifies that key cache should be quantized per-channel and value cache per-token. KIVI achieves up to 2.6x peak memory reduction and 2.35x-3.47x throughput increase on real LLM inference workloads, maintaining almost the same quality.",
        "methodology": "KIVI's methodology is based on a detailed study of KV cache element distribution, revealing that key cache exhibits channel-wise outliers (thus needing per-channel quantization) while value cache is better quantized per-token due to attention sparsity. The algorithm handles streaming inference by splitting KV cache into a grouped part (quantized) and a residual part (kept in full precision), maintaining a small full-precision sliding window for local relevant tokens. Group-wise quantization is applied to the grouped parts. System support includes hardware-friendly implementations with fused dequantization and matrix multiplication using CUDA and Triton kernels.",
        "experimental_setup": "KIVI was evaluated on Llama/Llama-2 (7B, 13B), Falcon-7B, and Mistral-7B models, as well as Llama3-8B, Mistral-7B-v0.2, and LongChat-7B-v1.5. Benchmarks included LM-Eval for normal context (CoQA, TruthfulQA, GSM8K) and LongBench for long context (Qasper, QMSum, MultiNews, TREC, TriviaQA, SAMSum, LCC, RepoBench-P), plus the Needle-in-a-Haystack task for long context retrieval. A group size of 32 was used for quantization, with residual lengths of 128 (default) and 32, 64, 96 for ablation studies. Experiments were conducted on a single NVIDIA A100 GPU (80GB), measuring accuracy, peak memory usage, and throughput.",
        "limitations": "The 2-bit KIVI quantization may experience a notable accuracy drop on certain models like Falcon-7B, necessitating 4-bit quantization to maintain quality. Although KIVI employs a full-precision sliding window for critical tokens to preserve accuracy on challenging tasks, this implies that a purely 2-bit quantization across all tokens would degrade performance. The current implementation also has room for further speed-up by fusing the KV cache quantization process with preceding operations, indicating an existing optimization constraint.",
        "future_research_directions": "Future research will focus on further optimizing KIVI's implementation to reduce the overhead of the quantization process during both the prefill and decoding phases. The paper also implies potential for combining KIVI with other orthogonal techniques such as weight-only quantization, system-level optimizations (e.g., PagedAttention), or KV cache eviction methods.",
        "experimental_code": "# Necessary imports for Llama KIVI\nimport math\nimport warnings\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\n# Custom KIVI quantization and matmul imports\n# Note: `kivi_gemv` is a C++/CUDA extension, imported by `quant.matmul`\nimport kivi_gemv # External custom C++/CUDA extension\n\n# Transformers Llama specific imports (assuming these are standard HF components)\nfrom transformers.models.llama.configuration_llama import LlamaConfig\nfrom transformers.models.llama.modeling_llama import (\n    LlamaRotaryEmbedding, apply_rotary_pos_emb, repeat_kv,\n    LlamaMLP, LlamaRMSNorm, LlamaPreTrainedModel,\n    CausalLMOutputWithPast, BaseModelOutputWithPast,\n    logger, _CONFIG_FOR_DOC, LLAMA_INPUTS_DOCSTRING, replace_return_docstrings,\n    add_start_docstrings_to_model_forward, add_start_docstrings, DynamicCache\n)\nfrom transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask\n\n# Triton specific imports for kernels\nimport triton\nimport triton.language as tl\n\n# --- Code from quant/new_pack.py (quantization and packing logic) ---\n\n@triton.jit\ndef _pack_along_last_dim(\n    bits: tl.constexpr,\n    intensor_ptr,\n    code_ptr,\n    N,\n    num_feats: tl.constexpr,\n    feat_per_int: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr\n):\n    num_int_per_y_dim = num_feats // feat_per_int\n    bid = tl.program_id(axis=0)\n    yid = tl.program_id(axis=1)\n    offs_N = bid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    block_start = intensor_ptr + offs_N * num_feats + yid * feat_per_int\n    packed = tl.zeros((BLOCK_SIZE_N,), dtype=tl.int32)\n    for i in range(feat_per_int):\n        ptr = block_start + i\n        element = tl.load(ptr, mask=offs_N<N, other=0.)\n        element = element << (i * bits)\n        packed = packed | element\n    tl.store(code_ptr + offs_N * num_int_per_y_dim + yid, packed, mask=offs_N < N)\n\n@triton.jit\ndef _minmax_along_last_dim(\n    x_ptr,\n    mn_ptr, mx_ptr,\n    total_elements: tl.constexpr,\n    N: tl.constexpr,\n    num_groups: tl.constexpr,\n    group_size: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr\n):\n    bid = tl.program_id(axis=0)\n    offsets_b = bid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offsets = offsets_b[:, None] * group_size + tl.arange(0, group_size)[None, :]\n    mask = offsets < total_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    mx_val = tl.max(x, axis=1)\n    mn_val = tl.min(x, axis=1)\n    tl.store(mn_ptr+offsets_b, mn_val, mask=offsets_b<N*num_groups)\n    tl.store(mx_ptr+offsets_b, mx_val, mask=offsets_b<N*num_groups)\n\ndef triton_quantize_and_pack_along_last_dim(data: torch.Tensor, group_size: int, bit: int):\n    assert len(data.shape) == 4\n    shape = data.shape\n    B, nh, D, T = shape\n    assert T % group_size == 0\n    num_groups = T // group_size\n    new_shape = (B * nh * D, num_groups, group_size)\n    scale_mn_shape = B, nh, D, num_groups\n    data = data.reshape(new_shape)\n    mx = torch.empty((B * nh * D, num_groups), device=data.device, dtype=data.dtype)\n    mn = torch.empty((B * nh * D, num_groups), device=data.device, dtype=data.dtype)\n    BLOCK_SIZE_N = 128\n    grid = lambda meta: (triton.cdiv(data.shape[0]*data.shape[1], BLOCK_SIZE_N),)\n    with torch.cuda.device(data.device):\n        _minmax_along_last_dim[grid](data, mn, mx,\n                             data.numel(), data.shape[0], num_groups, group_size,\n                             BLOCK_SIZE_N=BLOCK_SIZE_N, num_warps=8) \n    scale = (mx - mn) / (2 ** bit - 1)\n    data = data - mn.unsqueeze(-1)\n    data.div_(scale.unsqueeze(-1))\n    data = data.clamp_(0, 2 ** bit - 1).round_().to(torch.int32)\n    data = data.view(-1, T)\n    feat_per_int = 32 // bit\n    packshape = (torch.prod(torch.tensor(shape[:-1])).item(), shape[-1] // feat_per_int,)\n    code = torch.zeros(*packshape, device=data.device, dtype=torch.int32)\n    grid = lambda meta: (triton.cdiv(data.shape[0], BLOCK_SIZE_N), data.shape[1] // feat_per_int,)\n    with torch.cuda.device(data.device):\n        _pack_along_last_dim[grid](bit, data, code, data.shape[0],\n                                data.shape[1], feat_per_int,\n                                BLOCK_SIZE_N=BLOCK_SIZE_N,\n                                num_warps=8)\n    return code.view(B, nh, D, -1), scale.reshape(scale_mn_shape), mn.reshape(scale_mn_shape)\n\n# --- Code from quant/matmul.py (fused matmul logic) ---\n\n@triton.jit\ndef qbvm_kernel(\n    bits,\n    a_ptr, b_ptr, c_ptr,\n    scales_ptr, zeros_ptr,\n    M, N, K,\n    stride_abatch, stride_am, stride_ak,\n    stride_bbatch, stride_bk, stride_bn,\n    stride_cbatch, stride_cm, stride_cn,\n    stride_scales_b, stride_scales_k, stride_scales_g,\n    stride_zeros_b, stride_zeros_k, stride_zeros_g,\n    groupsize,\n    BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_batch = tl.program_id(axis=0)\n    pid = tl.program_id(axis=1)\n    feat_per_int = 32 // bits\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_n = pid % num_pid_n\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N))\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_batch_offset = (pid_batch * stride_abatch)\n    b_batch_offset = (pid_batch * stride_bbatch)\n    c_batch_offset = (pid_batch * stride_cbatch)\n    a_ptr = a_ptr + a_batch_offset\n    b_ptr = b_ptr + b_batch_offset\n    c_ptr = c_ptr + c_batch_offset\n    a_ptrs = a_ptr + (offs_k[:, None] * stride_ak)\n    b_ptrs = b_ptr  + (offs_k[:, None] * stride_bk + (offs_bn[None, :]//feat_per_int) * stride_bn)\n    shifter = (offs_bn % feat_per_int) * bits\n    scales_ptr = scales_ptr + pid_batch*stride_scales_b + ((offs_bn[None, :] // groupsize)) * stride_scales_g\n    zeros_ptr = zeros_ptr + pid_batch*stride_zeros_b + ((offs_bn[None, :] // groupsize)) * stride_zeros_g\n\n    accumulator = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n    num = 0xFF >> (8-bits)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    for pid_k in range(0, num_pid_k):\n        offs_bk = (offs_k[:, None] + pid_k * BLOCK_SIZE_K)\n        a = tl.load(a_ptrs, mask=offs_bk < K, other=0.)\n        b = tl.load(b_ptrs, mask=offs_bk < K, other=0.)\n        ptr = scales_ptr + offs_bk * stride_scales_k\n        scales = tl.load(ptr, mask=offs_bk < K, other=0.)\n        ptr = zeros_ptr + offs_bk * stride_zeros_k\n        zeros = tl.load(ptr, mask=offs_bk < K, other=0.)\n        b = (b >> shifter[None, :]) & num\n        b = b * scales + zeros\n        accumulator += tl.sum(a * b, 0)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    c = accumulator\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cn * offs_cn\n    c_mask = (offs_cn < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n\ndef cuda_bmm_fA_qB_outer(group_size: int,\n                fA: torch.FloatTensor,\n                qB: torch.IntTensor,\n                scales: torch.FloatTensor,\n                zeros: torch.FloatTensor,\n                bits: int) -> torch.FloatTensor:\n    assert len(fA.shape) == 4 and len(qB.shape) == 4\n    B, nh, M, K = fA.shape\n    nh_kv =  qB.shape[1]\n    feat_per_int = 32 // bits\n    fA = fA.view(-1, M, K).contiguous()\n    N = qB.shape[-1] * feat_per_int\n    qB = qB.reshape(-1, K, qB.shape[-1]).transpose(1, 2).contiguous()\n    flatten_B = B * nh_kv\n    scales = scales.view(flatten_B, scales.shape[-2], scales.shape[-1]).transpose(1, 2).contiguous()\n    zeros = zeros.view(flatten_B, zeros.shape[-2], zeros.shape[-1]).transpose(1, 2).contiguous()\n    assert bits in [2, 4]\n    assert nh % nh_kv == 0\n    c = kivi_gemv.gemv_forward_cuda_outer_dim(fA, qB, scales, zeros, bits, group_size, nh, nh_kv)\n    c = c.view(B, nh, c.shape[-2], c.shape[-1])\n    return c\n\n# --- Code from models/llama_kivi.py (KIVI Llama attention and model) ---\n\nclass LlamaAttention_KIVI(nn.Module):\n    def __init__(self, config: LlamaConfig):\n        super().__init__()\n        self.config = config\n        self.attention_dropout = config.attention_dropout\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.is_causal = True\n        self.k_bits = config.k_bits\n        self.v_bits = config.v_bits\n        self.group_size = config.group_size\n        self.residual_length = config.residual_length\n        assert getattr(config, \"use_flash\", False), \"currently KIVI is only available for flash-attn. Please add ```config.use_flash = True```\"\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n        self.rotary_emb = LlamaRotaryEmbedding(config=self.config)\n\n    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        if \"padding_mask\" in kwargs:\n            warnings.warn(\n                \"Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`\"\n            )\n        bsz, q_len, _ = hidden_states.size()\n\n        if self.config.pretraining_tp > 1:\n            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n            query_slices = self.q_proj.weight.split(\n                (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n            )\n            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n\n            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n            query_states = torch.cat(query_states, dim=-1)\n\n            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n            key_states = torch.cat(key_states, dim=-1)\n\n            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n            value_states = torch.cat(value_states, dim=-1)\n\n        else:\n            query_states = self.q_proj(hidden_states)\n            key_states = self.k_proj(hidden_states)\n            value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        kv_seq_len = key_states.shape[-2]\n        if past_key_value is not None:\n            kv_seq_len += past_key_value[-1]\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n        assert self.num_key_value_groups == 1\n        if past_key_value is not None:\n            key_states_quant_trans = past_key_value[0]\n            key_states_full = past_key_value[1]\n            key_scale_trans = past_key_value[2]\n            key_mn_trans = past_key_value[3]\n            value_states_quant = past_key_value[4]\n            value_states_full = past_key_value[5]\n            value_scale = past_key_value[6]\n            value_mn = past_key_value[7]\n\n            if key_states_quant_trans is not None:\n                att_qkquant = cuda_bmm_fA_qB_outer(self.group_size, query_states, key_states_quant_trans,\n                                key_scale_trans, key_mn_trans, self.k_bits)\n            else:\n                att_qkquant = None\n\n            if key_states_full is not None:\n                key_states_full = torch.cat([key_states_full, key_states], dim=2)\n            else:\n                key_states_full = key_states\n            att_qkfull = torch.matmul(query_states, key_states_full.transpose(2, 3))\n            if att_qkquant is not None:\n                attn_weights = torch.cat([att_qkquant, att_qkfull], dim=-1) / math.sqrt(self.head_dim)\n            else:\n                attn_weights = att_qkfull / math.sqrt(self.head_dim)\n\n            if key_states_full.shape[-2] == self.residual_length:\n                assert self.residual_length % self.group_size == 0\n                key_states_quant_trans_new, key_scale_trans_new, key_mn_trans_new = triton_quantize_and_pack_along_last_dim(key_states_full.transpose(2, 3).contiguous(),\n                                                                                                                            self.group_size,\n                                                                                                                            self.k_bits)\n                key_states_full = None\n                if key_states_quant_trans is not None:\n                    key_states_quant_trans = torch.cat([key_states_quant_trans, key_states_quant_trans_new], dim=3)\n                    key_scale_trans = torch.cat([key_scale_trans, key_scale_trans_new], dim=3)\n                    key_mn_trans = torch.cat([key_mn_trans, key_mn_trans_new], dim=3)\n                else:\n                    key_states_quant_trans = key_states_quant_trans_new\n                    key_scale_trans = key_scale_trans_new\n                    key_mn_trans = key_mn_trans_new\n\n            if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n                    f\" {attn_weights.size()}\"\n                )\n\n            if attention_mask is not None:\n                if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                    raise ValueError(\n                        f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                    )\n                attn_weights = attn_weights + attention_mask\n                attn_weights = torch.max(\n                    attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min)\n                )\n\n            attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n\n            value_states_full = torch.cat([value_states_full, value_states], dim=2)\n            value_full_length = value_states_full.shape[-2]\n            if value_states_quant is None:\n                attn_output = torch.matmul(attn_weights, value_states_full)\n            else:\n                attn_output = cuda_bmm_fA_qB_outer(self.group_size, attn_weights[:, :, :, :-value_full_length], value_states_quant,\n                                                value_scale, value_mn, self.v_bits)\n                attn_output += torch.matmul(attn_weights[:, :, :, -value_full_length:], value_states_full)\n            \n            if value_full_length > self.residual_length:\n                assert value_full_length == self.residual_length + 1\n                value_states_quant_new, scale, mn = triton_quantize_and_pack_along_last_dim(value_states_full[:, :, :1, :].contiguous(),\n                                                                                                self.group_size,\n                                                                                                self.v_bits)\n                value_states_full = value_states_full[:, :, 1:, :].contiguous()\n                if value_states_quant is not None:\n                    value_states_quant = torch.cat([value_states_quant, value_states_quant_new], dim=2)\n                    value_scale = torch.cat([value_scale, scale], dim=2)\n                    value_mn = torch.cat([value_mn, mn], dim=2)\n                else:\n                    value_states_quant = value_states_quant_new\n                    value_scale = scale\n                    value_mn = mn\n\n        else:\n            attn_weights = torch.matmul(query_states,\n                                        key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n            if key_states.shape[-2] % self.residual_length != 0:\n                if key_states.shape[-2] < self.residual_length:\n                    key_states_quant = None\n                    key_states_full = key_states\n                else:\n                    key_states_quant = key_states[:, :, :-(key_states.shape[-2] % self.residual_length), :].contiguous()\n                    key_states_full = key_states[:, :, -(key_states.shape[-2] % self.residual_length):, :].contiguous()\n            else:\n                key_states_quant = key_states\n                key_states_full = None\n            if key_states_quant is not None:\n                key_states_quant_trans, key_scale_trans, key_mn_trans = triton_quantize_and_pack_along_last_dim(key_states_quant.transpose(2, 3).contiguous(), self.group_size, self.k_bits)\n            else:\n                key_states_quant_trans = None\n                key_scale_trans = None\n                key_mn_trans = None\n            \n            if value_states.shape[-2] <= self.residual_length:\n                value_states_quant = None\n                value_states_full = value_states\n                value_scale = None\n                value_mn = None\n            else:\n                value_states_quant = value_states[:, :, :-self.residual_length, :].contiguous()\n                value_states_full = value_states[:, :, -self.residual_length:, :].contiguous()\n                value_states_quant, value_scale, value_mn = triton_quantize_and_pack_along_last_dim(value_states_quant,\n                                                                                                self.group_size,\n                                                                                                self.v_bits)\n\n            if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n                    f\" {attn_weights.size()}\"\n                )\n\n            if attention_mask is not None:\n                if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                    raise ValueError(\n                        f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                    )\n                attn_weights = attn_weights + attention_mask\n                attn_weights = torch.max(\n                    attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min)\n                )\n\n            attn_weights = nn.functional.softmax(\n                attn_weights, dim=-1, dtype=torch.float32\n            ).to(query_states.dtype)\n\n            attn_output = torch.matmul(attn_weights, value_states) \n        past_key_value = (key_states_quant_trans, key_states_full, key_scale_trans, key_mn_trans, value_states_quant, value_states_full, value_scale, value_mn, kv_seq_len) if use_cache else None\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        if self.config.pretraining_tp > 1:\n            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n        else:\n            attn_output = self.o_proj(attn_output)\n\n        attn_weights = None\n        return attn_output, attn_weights, past_key_value\n    \nclass LlamaFlashAttention_KIVI(LlamaAttention_KIVI):\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        if \"padding_mask\" in kwargs:\n            warnings.warn(\n                \"Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`\"\n            )\n        bsz, q_len, _ = hidden_states.size()\n\n        if self.config.pretraining_tp > 1:\n            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n            query_slices = self.q_proj.weight.split(\n                (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n            )\n            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n\n            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n            query_states = torch.cat(query_states, dim=-1)\n\n            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n            key_states = torch.cat(key_states, dim=-1)\n\n            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n            value_states = torch.cat(value_states, dim=-1)\n\n        else:\n            query_states = self.q_proj(hidden_states)\n            key_states = self.k_proj(hidden_states)\n            value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        kv_seq_len = key_states.shape[-2]\n        if past_key_value is not None:\n            kv_seq_len += past_key_value[-1]\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n        if past_key_value is not None:\n            key_states_quant_trans = past_key_value[0]\n            key_states_full = past_key_value[1]\n            key_scale_trans = past_key_value[2]\n            key_mn_trans = past_key_value[3]\n            value_states_quant = past_key_value[4]\n            value_states_full = past_key_value[5]\n            value_scale = past_key_value[6]\n            value_mn = past_key_value[7]\n            if key_states_quant_trans is not None:\n                att_qkquant = cuda_bmm_fA_qB_outer(self.group_size, query_states, key_states_quant_trans,\n                                key_scale_trans, key_mn_trans, self.k_bits)\n            else:\n                att_qkquant = None\n            if key_states_full is not None:\n                key_states_full = torch.cat([key_states_full, key_states], dim=2)\n            else:\n                key_states_full = key_states\n            att_qkfull = torch.matmul(query_states, repeat_kv(key_states_full, self.num_key_value_groups).transpose(2, 3))\n            if att_qkquant is not None:\n                attn_weights = torch.cat([att_qkquant, att_qkfull], dim=-1) / math.sqrt(self.head_dim)\n            else:\n                attn_weights = att_qkfull / math.sqrt(self.head_dim)\n\n            if key_states_full.shape[-2] == self.residual_length:\n                assert self.residual_length % self.group_size == 0\n                key_states_quant_trans_new, key_scale_trans_new, key_mn_trans_new = triton_quantize_and_pack_along_last_dim(key_states_full.transpose(2, 3).contiguous(),\n                                                                                                                            self.group_size,\n                                                                                                                            self.k_bits)\n                key_states_full = None\n                if key_states_quant_trans is not None:\n                    key_states_quant_trans = torch.cat([key_states_quant_trans, key_states_quant_trans_new], dim=3)\n                    key_scale_trans = torch.cat([key_scale_trans, key_scale_trans_new], dim=3)\n                    key_mn_trans = torch.cat([key_mn_trans, key_mn_trans_new], dim=3)\n                else:\n                    key_states_quant_trans = key_states_quant_trans_new\n                    key_scale_trans = key_scale_trans_new\n                    key_mn_trans = key_mn_trans_new\n\n            if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n                    f\" {attn_weights.size()}\"\n                )\n\n            if attention_mask is not None:\n                if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                    raise ValueError(\n                        f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                    )\n                attn_weights = attn_weights + attention_mask\n                attn_weights = torch.max(\n                    attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min)\n                )\n\n            attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n\n            value_states_full = torch.cat([value_states_full, value_states], dim=2)\n            value_full_length = value_states_full.shape[-2]\n            if value_states_quant is None:\n                attn_output = torch.matmul(attn_weights, value_states_full)\n            else:\n                attn_output = cuda_bmm_fA_qB_outer(self.group_size, attn_weights[:, :, :, :-value_full_length], value_states_quant,\n                                                value_scale, value_mn, self.v_bits)\n                attn_output += torch.matmul(attn_weights[:, :, :, -value_full_length:], repeat_kv(value_states_full, self.num_key_value_groups))\n            attn_output = attn_output.transpose(1, 2).contiguous()\n            if value_full_length > self.residual_length:\n                assert value_full_length == self.residual_length + 1\n                value_states_quant_new, scale, mn = triton_quantize_and_pack_along_last_dim(value_states_full[:, :, :1, :].contiguous(),\n                                                                                                self.group_size,\n                                                                                                self.v_bits)\n                value_states_full = value_states_full[:, :, 1:, :].contiguous()\n                if value_states_quant is not None:\n                    value_states_quant = torch.cat([value_states_quant, value_states_quant_new], dim=2)\n                    value_scale = torch.cat([value_scale, scale], dim=2)\n                    value_mn = torch.cat([value_mn, mn], dim=2)\n                else:\n                    value_states_quant = value_states_quant_new\n                    value_scale = scale\n                    value_mn = mn\n\n        else:\n            input_dtype = query_states.dtype\n            if input_dtype == torch.float32:\n                if hasattr(self.config, \"_pre_quantization_dtype\"):\n                    target_dtype = self.config._pre_quantization_dtype\n                else:\n                    target_dtype = self.q_proj.weight.dtype\n\n                logger.warning_once(\n                    f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n                    f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n                    f\" {target_dtype}.\"\n                )\n\n                query_states = query_states.to(target_dtype)\n                key_states = key_states.to(target_dtype)\n                value_states = value_states.to(target_dtype)\n            attn_output = self._flash_attention_forward(\n                query_states.transpose(1, 2), key_states.transpose(1, 2),\n                value_states.transpose(1, 2), None, q_len, dropout=0.0\n            )\n            if key_states.shape[-2] % self.residual_length != 0:\n                if key_states.shape[-2] < self.residual_length:\n                    key_states_quant = None\n                    key_states_full = key_states\n                else:\n                    key_states_quant = key_states[:, :, :-(key_states.shape[-2] % self.residual_length), :].contiguous()\n                    key_states_full = key_states[:, :, -(key_states.shape[-2] % self.residual_length):, :].contiguous()\n            else:\n                key_states_quant = key_states\n                key_states_full = None\n            if key_states_quant is not None:\n                key_states_quant_trans, key_scale_trans, key_mn_trans = triton_quantize_and_pack_along_last_dim(key_states_quant.transpose(2, 3).contiguous(), self.group_size, self.k_bits)\n            else:\n                key_states_quant_trans = None\n                key_scale_trans = None\n                key_mn_trans = None\n            \n            if value_states.shape[-2] <= self.residual_length:\n                value_states_quant = None\n                value_states_full = value_states\n                value_scale = None\n                value_mn = None\n            else:\n                value_states_quant = value_states[:, :, :-self.residual_length, :].contiguous()\n                value_states_full = value_states[:, :, -self.residual_length:, :].contiguous()\n                value_states_quant, value_scale, value_mn = triton_quantize_and_pack_along_last_dim(value_states_quant,\n                                                                                                self.group_size,\n                                                                                                self.v_bits)\n\n        past_key_value = (key_states_quant_trans, key_states_full, key_scale_trans, key_mn_trans,\n                          value_states_quant, value_states_full, value_scale, value_mn, kv_seq_len) if use_cache else None\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        if self.config.pretraining_tp > 1:\n            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n        else:\n            attn_output = self.o_proj(attn_output)\n\n        attn_weights = None\n        return attn_output, attn_weights, past_key_value\n\n    def _flash_attention_forward(\n        self, query_states, key_states, value_states, attention_mask, query_length, dropout=0.0, softmax_scale=None\n    ):\n        from flash_attn import flash_attn_func, flash_attn_varlen_func\n        from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input\n\n        # Definition for _get_unpad_data, which appears in mistral_kivi.py and is used here.\n        def _get_unpad_data(attention_mask):\n            seqlens_in_batch = attention_mask.sum(dim=-1, dtype=torch.int32)\n            indices = torch.nonzero(attention_mask.flatten(), as_tuple=False).flatten()\n            max_seqlen_in_batch = seqlens_in_batch.max().item()\n            cu_seqlens = F.pad(torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.torch.int32), (1, 0))\n            return (indices, cu_seqlens, max_seqlen_in_batch,)\n\n        if attention_mask is not None:\n            batch_size = query_states.shape[0]\n            query_states, key_states, value_states, indices_q, cu_seq_lens, max_seq_lens = self._upad_input(\n                query_states, key_states, value_states, attention_mask, query_length\n            )\n\n            cu_seqlens_q, cu_seqlens_k = cu_seq_lens\n            max_seqlen_in_batch_q, max_seqlen_in_batch_k = max_seq_lens\n\n            attn_output_unpad = flash_attn_varlen_func(\n                query_states,\n                key_states,\n                value_states,\n                cu_seqlens_q=cu_seqlens_q,\n                cu_seqlens_k=cu_seqlens_k,\n                max_seqlen_q=max_seqlen_in_batch_q,\n                max_seqlen_k=max_seqlen_in_batch_k,\n                dropout_p=dropout,\n                softmax_scale=softmax_scale,\n                causal=self.is_causal,\n            )\n\n            attn_output = pad_input(attn_output_unpad, indices_q, batch_size, query_length)\n        else:\n            attn_output = flash_attn_func(\n                query_states, key_states, value_states, dropout, softmax_scale=softmax_scale, causal=self.is_causal\n            )\n\n        return attn_output\n\n    def _upad_input(self, query_layer, key_layer, value_layer, attention_mask, query_length):\n        indices_k, cu_seqlens_k, max_seqlen_in_batch_k = _get_unpad_data(attention_mask)\n        batch_size, kv_seq_len, num_key_value_heads, head_dim = key_layer.shape\n\n        key_layer = index_first_axis(\n            key_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k\n        )\n        value_layer = index_first_axis(\n            value_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k\n        )\n        if query_length == kv_seq_len:\n            query_layer = index_first_axis(\n                query_layer.reshape(batch_size * kv_seq_len, self.num_heads, head_dim), indices_k\n            )\n            cu_seqlens_q = cu_seqlens_k\n            max_seqlen_in_batch_q = max_seqlen_in_batch_k\n            indices_q = indices_k\n        elif query_length == 1:\n            max_seqlen_in_batch_q = 1\n            cu_seqlens_q = torch.arange(\n                batch_size + 1, dtype=torch.int32, device=query_layer.device\n            )\n            indices_q = cu_seqlens_q[:-1]\n            query_layer = query_layer.squeeze(1)\n        else:\n            attention_mask = attention_mask[:, -query_length:]\n            query_layer, indices_q, cu_seqlens_q, max_seqlen_in_batch_q = unpad_input(query_layer, attention_mask)\n\n        return (\n            query_layer,\n            key_layer,\n            value_layer,\n            indices_q,\n            (cu_seqlens_q, cu_seqlens_k),\n            (max_seqlen_in_batch_q, max_seqlen_in_batch_k),\n        )\n    \nclass LlamaDecoderLayer_KIVI(nn.Module):\n    def __init__(self, config: LlamaConfig):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        self.self_attn = (\n            LlamaAttention_KIVI(config=config)\n            if not getattr(config, \"use_flash\", False)\n            else LlamaFlashAttention_KIVI(config=config)\n        )\n        self.mlp = LlamaMLP(config)\n        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: Optional[bool] = False,\n        use_cache: Optional[bool] = False,\n        **kwargs,\n    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n        if \"padding_mask\" in kwargs:\n            warnings.warn(\n                \"Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`\"\n            )\n\n        residual = hidden_states\n\n        hidden_states = self.input_layernorm(hidden_states);\n\n        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_value=past_key_value,\n            output_attentions=output_attentions,\n            use_cache=use_cache,\n            **kwargs,\n        )\n        hidden_states = residual + hidden_states\n\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (self_attn_weights,)\n\n        if use_cache:\n            outputs += (present_key_value,)\n\n        return outputs\n\nclass LlamaModel_KIVI(LlamaPreTrainedModel):\n    def __init__(self, config: LlamaConfig):\n        super().__init__(config)\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n        self.layers = nn.ModuleList([LlamaDecoderLayer_KIVI(config) for _ in range(config.num_hidden_layers)])\n        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n        self.gradient_checkpointing = False\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, BaseModelOutputWithPast]:\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        elif input_ids is not None:\n            batch_size, seq_length = input_ids.shape[:2]\n        elif inputs_embeds is not None:\n            batch_size, seq_length = inputs_embeds.shape[:2]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        past_key_values_length = 0\n        if past_key_values is not None:\n            past_key_values_length = past_key_values[0][-1]\n\n        if position_ids is None:\n            device = input_ids.device if input_ids is not None else inputs_embeds.device\n            position_ids = torch.arange(\n                past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device\n            )\n            position_ids = position_ids.unsqueeze(0)\n\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids)\n\n        if getattr(self.config, \"_flash_attn_2_enabled\", False):\n            attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n        else:\n            attention_mask = _prepare_4d_causal_attention_mask(\n                attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length\n            )\n\n        hidden_states = inputs_embeds\n\n        if self.gradient_checkpointing and self.training:\n            if use_cache:\n                logger.warning_once(\n                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n                )\n                use_cache = False\n\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attns = () if output_attentions else None\n        next_decoder_cache = () if use_cache else None\n\n        for idx, decoder_layer in enumerate(self.layers):\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\n            past_key_value = past_key_values[idx] if past_key_values is not None else None\n\n            if self.gradient_checkpointing and self.training:\n                layer_outputs = self._gradient_checkpointing_func(\n                    decoder_layer.__call__,\n                    hidden_states,\n                    attention_mask,\n                    position_ids,\n                    past_key_value,\n                    output_attentions,\n                    use_cache,\n                )\n            else:\n                layer_outputs = decoder_layer(\n                    hidden_states,\n                    attention_mask=attention_mask,\n                    position_ids=position_ids,\n                    past_key_value=past_key_value,\n                    output_attentions=output_attentions,\n                    use_cache=use_cache,\n                )\n\n            hidden_states = layer_outputs[0]\n\n            if use_cache:\n                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n\n            if output_attentions:\n                all_self_attns += (layer_outputs[1],)\n\n        hidden_states = self.norm(hidden_states)\n\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n\n        next_cache = next_decoder_cache if use_cache else None\n        if not return_dict:\n            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n        return BaseModelOutputWithPast(\n            last_hidden_state=hidden_states,\n            past_key_values=next_cache,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attns,\n        )\n\nclass LlamaForCausalLM_KIVI(LlamaPreTrainedModel):\n    _tied_weights_keys = [\"lm_head.weight\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.model = LlamaModel_KIVI(config)\n        self.vocab_size = config.vocab_size\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    def set_decoder(self, decoder):\n        self.model = decoder\n\n    def get_decoder(self):\n        return self.model\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        hidden_states = outputs[0]\n        if self.config.pretraining_tp > 1:\n            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n            logits = torch.cat(logits, dim=-1)\n        else:\n            logits = self.lm_head(hidden_states)\n        logits = logits.float()\n\n        loss = None\n        if labels is not None:\n            from torch.nn import CrossEntropyLoss\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            loss_fct = CrossEntropyLoss()\n            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n            shift_labels = shift_labels.view(-1)\n            shift_labels = shift_labels.to(shift_logits.device)\n            loss = loss_fct(shift_logits, shift_labels)\n\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return (loss,) + output if loss is not None else output\n\n        return CausalLMOutputWithPast(\n            loss=loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n    def prepare_inputs_for_generation(\n        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n    ):\n        if isinstance(past_key_values, DynamicCache):\n            past_key_values = past_key_values.to_legacy_cache()\n            if len(past_key_values) == 0:\n                past_key_values = None\n        if past_key_values is not None:\n            past_length = past_key_values[0][-1]\n            if input_ids.shape[1] > past_length:\n                remove_prefix_length = past_length\n            else:\n                remove_prefix_length = input_ids.shape[1] - 1\n\n            input_ids = input_ids[:, remove_prefix_length:]\n\n        position_ids = kwargs.get(\"position_ids\", None)\n        if attention_mask is not None and position_ids is None:\n            position_ids = attention_mask.long().cumsum(-1) - 1\n            position_ids.masked_fill_(attention_mask == 0, 1)\n            if past_key_values:\n                position_ids = position_ids[:, -input_ids.shape[1] :]\n\n        if inputs_embeds is not None and past_key_values is None:\n            model_inputs = {\"inputs_embeds\": inputs_embeds}\n        else:\n            model_inputs = {\"input_ids\": input_ids}\n\n        model_inputs.update(\n            {\n                \"position_ids\": position_ids,\n                \"past_key_values\": past_key_values,\n                \"use_cache\": kwargs.get(\"use_cache\"),\n                \"attention_mask\": attention_mask,\n            }\n        )\n        return model_inputs\n\n    @staticmethod\n    def _reorder_cache(past_key_values, beam_idx):\n        reordered_past = ()\n        for layer_past in past_key_values:\n            reordered_past += (\n                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n            )\n        return reordered_past\n",
        "experimental_info": "KIVI's methodology modifies Llama models to use quantized KV cache for efficient streaming inference. The key configurable parameters for KIVI are:\n- `k_bits`: Quantization bits for key cache. Observed values: 2 (e.g., in `example.py`, `long_context_example.py`, `mem_spd_test.py`).\n- `v_bits`: Quantization bits for value cache. Observed values: 2 (e.g., in `example.py`, `long_context_example.py`, `mem_spd_test.py`).\n- `group_size`: Group size for quantization. Observed values: 32 (e.g., in `example.py`, `long_context_example.py`, `mem_spd_test.py`).\n- `residual_length`: The number of recent full-precision tokens (sliding window size). Observed values: 32 (e.g., in `example.py`, `long_context_example.py`) or 128 (e.g., in `mem_spd_test.py`).\n- `use_flash`: Boolean flag to enable Flash Attention with KIVI. Always `True` in the provided examples.\n\nThe models demonstrated with KIVI are:\n- `meta-llama/Llama-3.1-8B-Instruct` (used in `example.py`, `long_context_example.py`)\n- `meta-llama/Llama-2-7b-hf` (used in `mem_spd_test.py`)\n\nCommon settings across experiments:\n- `torch_dtype`: `torch.float16` for the model.\n- `low_cpu_mem_usage=True`, `device_map=\"auto\"` for model loading.\n\nDemonstrated use cases and specific experimental settings:\n1.  **Arithmetic Reasoning (from `example.py`)**:\n    - Task: GSM8K.\n    - Input: Few-shot prompt constructed from training data.\n    - Generation parameters: `max_new_tokens=96`, `num_beams=1`, `do_sample=False`, `temperature=1.0`.\n    - Reproducibility: `random.seed(0)`, `torch.manual_seed(0)`.\n2.  **Long-Context Passkey Retrieval (from `long_context_example.py`)**:\n    - Task: Passkey retrieval from `passkey_examples.jsonl`.\n    - Input: Long context string concatenated with a question (`\"What is the pass key? The pass key is \"`).\n    - Generation parameters: `max_new_tokens=len(example[\"target\"])`, `num_beams=1`, `do_sample=False`, `temperature=1.0`.\n3.  **Memory and Speed Test (from `mem_spd_test.py`)**:\n    - Purpose: Quantitatively measure memory usage and inference speed.\n    - Parameters: `BATCH_SIZE = 96`, `prompt_lenth = 160`, `output_length = 338`, `num_repeats = 3`.\n    - Metrics recorded: `used time` (ms) and `peak mem` (GB).\n4.  **LongBench Evaluation (from `pred_long_bench.py`)**:\n    - Evaluates KIVI-enabled models on various LongBench datasets (e.g., 'qasper', 'multifieldqa_en', 'hotpotqa', 'trec', 'triviaqa', 'samsum', 'lcc', 'repobench-p').\n    - Retrieves `max_length` and `max_gen` from configuration files (`model2maxlen.json`, `dataset2maxlen.json`).\n    - Uses `build_chat` to format prompts for chat models.\n    - Generation parameters: `num_beams=1`, `do_sample=False`, `temperature=1.0`."
      }
    },
    {
      "title": "DeepCache: Accelerating Diffusion Models for Free",
      "abstract": "Diffusion models have recently gained unprecedented attention in the field of\nimage synthesis due to their remarkable generative capabilities.\nNotwithstanding their prowess, these models often incur substantial\ncomputational costs, primarily attributed to the sequential denoising process\nand cumbersome model size. Traditional methods for compressing diffusion models\ntypically involve extensive retraining, presenting cost and feasibility\nchallenges. In this paper, we introduce DeepCache, a novel training-free\nparadigm that accelerates diffusion models from the perspective of model\narchitecture. DeepCache capitalizes on the inherent temporal redundancy\nobserved in the sequential denoising steps of diffusion models, which caches\nand retrieves features across adjacent denoising stages, thereby curtailing\nredundant computations. Utilizing the property of the U-Net, we reuse the\nhigh-level features while updating the low-level features in a very cheap way.\nThis innovative strategy, in turn, enables a speedup factor of 2.3$\\times$ for\nStable Diffusion v1.5 with only a 0.05 decline in CLIP Score, and 4.1$\\times$\nfor LDM-4-G with a slight decrease of 0.22 in FID on ImageNet. Our experiments\nalso demonstrate DeepCache's superiority over existing pruning and distillation\nmethods that necessitate retraining and its compatibility with current sampling\ntechniques. Furthermore, we find that under the same throughput, DeepCache\neffectively achieves comparable or even marginally improved results with DDIM\nor PLMS. The code is available at https://github.com/horseee/DeepCache",
      "full_text": "DeepCache: Accelerating Diffusion Models for Free Xinyin Ma Gongfan Fang Xinchao Wang * National University of Singapore {maxinyin, gongfan}@u.nus.edu, xinchao@nus.edu.sg 2.3 × 7.0 × (a) Stable Diffusion v1.5 (b) LDM-4-G for ImageNet Figure 1. Accelerating Stable Diffusion V1.5 and LDM-4-G by 2.3 × and 7.0×, with 50 PLMS steps and 250 DDIM steps respectively. Abstract Diffusion models have recently gained unprecedented attention in the field of image synthesis due to their re- markable generative capabilities. Notwithstanding their prowess, these models often incur substantial computa- tional costs, primarily attributed to the sequential denoising process and cumbersome model size. Traditional methods for compressing diffusion models typically involve extensive retraining, presenting cost and feasibility challenges. In this paper, we introduce DeepCache, a novel training-free paradigm that accelerates diffusion models from the per- spective of model architecture. DeepCache capitalizes on the inherent temporal redundancy observed in the sequen- tial denoising steps of diffusion models, which caches and retrieves features across adjacent denoising stages, thereby curtailing redundant computations. Utilizing the property of the U-Net, we reuse the high-level features while up- dating the low-level features in a very cheap way. This innovative strategy, in turn, enables a speedup factor of 2.3× for Stable Diffusion v1.5 with only a 0.05 decline in CLIP Score, and 4.1 × for LDM-4-G with a slight de- crease of 0.22 in FID on ImageNet. Our experiments also demonstrate DeepCache’s superiority over existing prun- ing and distillation methods that necessitate retraining and its compatibility with current sampling techniques. Fur- thermore, we find that under the same throughput, Deep- Cache effectively achieves comparable or even marginally improved results with DDIM or PLMS. Code is available at https://github.com/horseee/DeepCache. 1. Introduction In recent years, diffusion models [9, 17, 57, 59] have emerged as a pivotal advancement in the field of genera- ∗ Corresponding author 1 arXiv:2312.00858v2  [cs.CV]  7 Dec 2023tive modeling, gaining substantial attention for their impres- sive capabilities. These models have demonstrated remark- able efficacy across diverse applications, being employed for the generation of images [19, 60, 64], text [11, 28], au- dio [6, 44], and video [18, 36, 56]. A large number of attrac- tive applications have been facilitated with diffusion mod- els, including but not limited to image editing [2, 20, 38], image super-enhancing [26, 51], image-to-image transla- tion [7, 49], text-to-image generation [41, 45, 46, 50] and text-to-3D generation [30, 35, 43]. Despite the significant effectiveness of diffusion mod- els, their relatively slow inference speed remains a ma- jor obstacle to broader adoption, as highlighted in [29]. The core challenge stems from the step-by-step denois- ing process required during their reverse phase, limiting parallel decoding capabilities [55]. Efforts to accelerate these models have focused on two main strategies: reduc- ing the number of sampling steps, as seen in approaches [34, 39, 52, 58], and decreasing the model inference over- head per step through methods like model pruning, distilla- tion, and quantization [10, 13, 21]. Our goal is to enhance the efficiency of diffusion models by reducing model size at each step. Previous compres- sion methods for diffusion models focused on re-designing network architectures through a comprehensive structural analysis [29] or involving frequency priors into the model design [66], which yields promising results on image gen- eration. However, they require large-scale datasets for re- training these lightweight models. Pruning-based meth- ods, as explored by [10, 21], lessen the data and training requirements to 0.1% of the full training. Alternatively, [32] employs adaptive models for different steps, which is also a potential solution. However, it depends on a collec- tion of pre-trained models or requires optimization of sub- networks [65]. Those methods can reduce the expense of crafting a new lightweight model, but the retraining process is still inevitable, which makes the compression costly and less practical for large-scale pre-trained diffusion models, such as Stable Diffusion [47]. To this end, we focus on a challenging topic: How to significantly reduce the computational overhead at each de- noising step without additional training, thereby achieving a cost-free compression of Diffusion Models? To achieve this, we turn our attention to the intrinsic characteristics of the reverse denoising process of diffusion models, ob- serving a significant temporal consistency in the high-level features between consecutive steps. We found that those high-level features are even cacheable, which can be calcu- lated once and retrieved again for the subsequent steps. By leveraging the structural property of U-Net, the high-level features can be cached while maintaining the low-level fea- tures updated at each denoising step. Through this, a con- siderable enhancement in the efficiency and speed of Diffu- sion Models can be achieved without any training. To summarize, we introduce a novel paradigm for the acceleration of Diffusion Models, which gives a new per- spective for training-free accelerating the diffusion models. It is not merely compatible with existing fast samplers but also shows potential for comparable or superior generation capabilities. The contributions of our paper include: • We introduce a simple and effective acceleration algo- rithm, named DeepCache, for dynamically compressing diffusion models during runtime and thus enhancing im- age generation speed without additional training burdens. • DeepCache utilizes the temporal consistency between high-level features. With the cacheable features, the re- dundant calculations are effectively reduced. Further- more, we introduce a non-uniform 1:N strategy, specifi- cally tailored for long caching intervals. • DeepCache is validated across a variety of datasets, in- cluding CIFAR, LSUN-Bedroom/Churches, ImageNet, COCO2017 and PartiPrompt, and tested under DDPM, LDM, and Stable Diffusion. Experiments demonstrate that our approach has superior efficacy than pruning and distillation algorithms that require retraining under the same throughput. 2. Related Work High-dimensional image generation has evolved signifi- cantly in generative modeling. Initially, GANs [1, 12] and V AEs [16, 22] led this field but faced scalability is- sues due to instability and mode collapse [23]. Recent ad- vancements have been led by Diffusion Probabilistic Mod- els [9, 17, 47, 61], which offer superior image generation. However, the inherent nature of the reverse diffusion pro- cess [59] slows down inference. Current research is focused on two main methods to speed up diffusion model inference. Optimized Sampling Efficiency. focuses on reducing the number of sampling steps. DDIM [58] reduces these steps by exploring a non-Markovian process, related to neu- ral ODEs. Studies [3, 33, 34, 69] further dive into the fast solver of SDE or ODE to create efficient sampling of diffusion models. Some methods progressively distilled the model to reduced timestep [52] or replace the remain- ing steps with a single-step V AE [37]. The Consistency Model [62] converts random noise to the initial images with only one model evaluation. Parallel sampling techniques like DSNO [70] and ParaDiGMS [55] employ Fourier neu- ral operators and Picard iterations for parallel decoding . Optimized Structural Efficiency. This approach aims to reduce inference time at each sampling step. It leverages strategies like structural pruning in Diff-pruning [10] and efficient structure evolving in SnapFusion [29]. Spectral 2Diffusion [66] enhances architectural design by incorpo- rating frequency dynamics and priors. In contrast to these methods, which use a uniform model at each step, [32] pro- poses utilizing different models at various steps, selected from a diffusion model zoo. The early stopping mechanism in diffusion is explored in [27, 40, 63], while the quantiza- tion techniques [13, 54] focus on low-precision data types for model weights and activations. Additionally, [4] and [5] present novel approaches to concentrate on inputs, with the former adopting a unique forward process for each pixel and the latter merging tokens based on similarity to enhance computational efficiency in attention modules. Our method is categorized under an objective to minimize the average inference time per step. Uniquely, our approach reduces the average model size substantially for each step, accelerating the denoising process without necessitating retraining. 3. Methodology 3.1. Preliminary Forward and Reverse Process. Diffusion models [17] simulate an image generation process using a series of ran- dom diffusion steps. The core idea behind diffusion models is to start from random noise and gradually refine it until it resembles a sample from the target distribution. In the for- ward diffusion process, with a data point sampled from the real distribution, x0 ∼ q(x), gaussian noises are gradually added in T steps: q (xt|xt−1) = N \u0010 xt; p 1 − βtxt−1, βtI \u0011 (1) where t is the current timestep and{βt} schedules the noise. The reverse diffusion process denoises the random noise xT ∼ N(0, I) into the target distribution by modeling q (xt−1|xt). At each reverse step t, the conditional prob- ability distribution is approximated by a network ϵθ (xt, t) with the timestep t and previous output xt as input: xt−1 ∼ pθ(xt−1|xt) = N \u0012 xt−1; 1√αt \u0012 xt − 1 − αt√1 − ¯αt ϵθ (xt, t) \u0013 , βtI \u0013 (2) where αt = 1 − βt and ¯αt = QT i=1 αi. Applied iteratively, it gradually reduces the noise of the current xt, bringing it close to a real data point when we reach x0. High-level and Low-level Features in U-Net. U- Net [48] was originally introduced for biomedical image segmentation and showcased a strong ability to amalgamate low-level and high-level features, attributed to the skip con- nections. U-Net is constructed on stacked downsampling and upsampling blocks, which encode the input image into a high-level representation and then decode it for downstream tasks. The block pairs, denoted as {Di}d i=1 and {Ui}d i=1, are interconnected with additional skip paths. Those skip paths directly forward the rich and relatively more low-level information from Di to Ui. During the forward propaga- tion in the U-Net architecture, the data traverses concur- rently through two pathways: the main branch and the skip branch. These branches converge at a concatenation mod- ule, with the main branch providing processed high-level feature from the preceding upsampling block Ui+1, and the skip branch contributing corresponding feature from the symmetrical block Di. Therefore, at the heart of a U-Net model is a concatenation of low-level features from the skip branch, and the high-level features from the main branch, formalized as: Concat(Di(·), Ui+1(·)) (3) 3.2. Feature Redundancy in Sequential Denoising The inherent sequentiality of the denoising process in dif- fusion models presents a primary bottleneck in inference speed. Previous methods primarily employed strategies that involved skipping certain steps to address this issue. In this work, we revisit the entire denoising process, seeking to un- cover specific properties that could be optimized to enhance inference efficiency. Observation. Adjacent steps in the denoising process ex- hibit significant temporal similarity in high-level features. In Figure 2, we provide empirical evidence related to this observation. The experiments elucidate two primary insights: 1) There is a noticeable temporal feature similar- ity between adjacent steps within the denoising process, in- dicating that the change between consecutive steps is typi- cally minor; 2) Regardless of the diffusion model we used, for each timestep, at least 10% of the adjacent timesteps exhibit a high similarity ( >0.95) to the current step, sug- gesting that certain high-level features change at a gradual pace. This phenomenon can be observed in a large num- ber of well-established models like Stable Diffusion, LDM, and DDPM. In the case of DDPM for LSUN-Churches and LSUN-Bedroom, some timesteps even demonstrate a high degree of similarity to 80% of the other steps, as highlighted in the green line in Figure 2 (c). Building upon these observations, our objective is to leverage this advantageous characteristic to accelerate the denoising process. Our analysis reveals that the compu- tation often results in a feature remarkably similar to that of the previous step, thereby highlighting the existence of redundant calculations for optimization. We contend that allocating significant computational resources to regener- ate these analogous feature maps constitutes an inefficiency. While incurring substantial computational expense, yields marginal benefits, it suggests a potential area for efficiency improvements in the speed of diffusion models. 3A large teddy bear with a heart is in the garbage A green plate filled with rice and a mixture of sauce on top of it A very ornate, three layeredwedding cake in a banquet room Prompt LDM-4-GDDPM/BedroomSD v1.5 (c)Ratio of steps that similarity larger than 0.95 Step1Step20Step19 Step0Original (a) Examples of Feature Maps(b) HeatMapfor Similarity Figure 2. (a) Examples of feature maps in the up-sampling block U2 in Stable Diffusion. We present a comparison from two adjacently paired steps, emphasizing the invariance inherent in the denoising process. (b) Heatmap of similarity between U2’s features in all steps on three typical diffusion models. (c) The percentage of steps with a similarity greater than 0.95 to the current step. 3.3. Deep Cache For Diffusion Models We introduce DeepCache, a simple and effective approach that leverages the temporal redundancy between steps in the reverse diffusion process to accelerate inference. Our method draws inspiration from the caching mechanism in a computer system, incorporating a storage component de- signed for elements that exhibit minimal changes over time. Applying this in diffusion models, we eliminate redundant computations by strategically caching slowly evolving fea- tures, thereby obviating the need for repetitive recalcula- tions in subsequent steps. To achieve this, we shift our focus to the skip connec- tions within U-Net, which inherently offers a dual-pathway advantage: the main branch requires heavy computation to traverse the entire network, while the skip branch only needs to go through some shallow layers, resulting in a very small computational load. The prominent feature similarity in the main branch, allows us to reuse the already computed results rather than calculate it repeatedly for all timesteps. Cacheable Features in denosing. To make this idea more concrete, we study the case within two consecutive timesteps t and t − 1. According to the reverse process, xt−1 would be conditionally generated based on the pre- vious results xt. First, we generate xt in the same way as usual, where the calculations are performed across the entire U-Net. To obtain the next output xt−1, we retrieve the high-level features produced in the previous xt. More specifically, consider a skip branch m in the U-Net, which bridges Dm and Um, we cache the feature maps from the previous up-sampling block at the time t as the following: Ft cache ← Ut m+1(·) (4) which is the feature from the main branch at timestep t. Those cached features will be plugged into the network in- ference in the subsequent steps. In the next timestep t − 1, the inference is not carried out on the entire network; in- stead, we perform a dynamic partial inference. Based on the previously generatedxt, we only calculate those that are necessary for the m-th skip branch and substitute the com- pute of the main branch with a retrieving operation from the cache in Equation 4. Therefore, the input for Ut−1 m in the t − 1 timestep can be formulated as: Concat(Dt−1 m (·), Ft cache) (5) Here, Dt−1 m represents the output of the m-th down- sampling block, which only contains a few layers if a small m is selected. For example, if we perform DeepCache at the first layer with m = 1 , then we only need to execute one downsampling block to obtain Dt−1 1 . As for the second feature Ft cache, no additional computational cost is needed since it can be simply retrieved from the cache. We illus- trate the above process in Figure 3. Extending to 1:N Inference This process is not limited to the type with one step of full inference followed by one step of partial inference. As shown in Figure 2(b), pair- wise similarity remains a high value in several consecutive steps. The mechanism can be extended to cover more steps, with the cached features calculated once and reused in the consecutive N − 1 steps to replace the original Ut−n m+1(·), n ∈ {1, . . . , N−1}. Thus, for all the T steps for denoising, the sequence of timesteps that performs full inference are: I = {x ∈ N|x = iN, 0 ≤ i < k} (6) where k = ⌈T/N ⌉ denotes the times for cache updating. 4!! !\" !#  M #! #\" ## Skip Branch 1  #!  !! !!\"$ #%&%'(! \"$\"#\"' ###'$ ChosenPath Skip Branch 2 Skip Branch 3 ExcludedPath !!  !!)$ Figure 3. Illustration of DeepCache. At the t−1 step, xt−1 is gen- erated by reusing the features cached at the t step, and the blocks D2, D3, U2, U3 are not executed for more efficient inference. Non-uniform 1:N Inference Based on the 1:N strategy, we managed to accelerate the inference of diffusion under a strong assumption that the high-level features are invari- ant in the consecutive N step. However, it’s not invariably the case, especially for a large N, as demonstrated by the experimental evidence in Figure 2(c). The similarity of the features does not remain constant across all steps. For mod- els such as LDM, the temporal similarity of features would significantly decrease around 40% of the denoising process. Thus, for the non-uniform 1:N inference, we tend to sample more on those steps with relatively small similarities to the adjacent steps. Here, the sequence of timesteps to perform full inference becomes: L = n li | li ∈ linear space \u0010 (−c) 1 p , (T − c) 1 p , k \u0011o (7) I = unique int ({ik | ik = (lk)p + c, where lk ∈ L}) where linear space(s, e, n) evenly spaces n numbers from s to e (exclusive) and unique int(·) convert the number to int and ensure the uniqueness of timesteps in the sequence. c is the hyper-parameter for the selected center timestep. In this equation, the frequency of indexes changes in a quadratic manner as it moves away from a central timestep. It is essential to note that the aforementioned strategy rep- resents one among several potential strategies. Alternative sequences, particularly centered on a specific timestep, can also yield similar improvements in image quality. 4. Experiment 4.1. Experimental Settings Models, Datasets and Evaluation Metrics To demon- strate the effectiveness of our method is agnostic with the type of pre-trained DMs, we evaluate our methods on three commonly used DMs: DDPM [17], LDM [47] and Stable (a)DDPMforCIFAR10(b) Stable Diffusion v1.5 Figure 4. MACs for each skip branch, evaluated on DDPM for CIFAR10 and Stable Diffusion V1.5. Diffusion [47]1. Except for this, to show that our method is compatible with the fast sampling methods, we build our method upon 100-step DDIM [58] for DDPM, 250-step for LDM and 50-step PLMS [33] for Stable Diffusion, instead of the complete 1000-step denoising process. We select six datasets that are commonly adopted to evaluate these mod- els, including CIFAR10 [24], LSUN-Bedroom [67], LSUN- Churches [67], ImageNet [8], MS-COCO 2017 [31] and PartiPrompts [68]. For MS-COCO 2017 and PartiPrompt, we utilized the 5k validation set and 1.63k captions respec- tively as prompts for Stable Diffusion. For other datasets, we generate 50k images to assess the generation quality. We follow previous works [10, 55, 66] to employ the evaluation metrics including FID, sFID, IS, Precision-and-Recall and CLIP Score (on ViT-g/14) [14, 15, 25, 53]. Baselines We choose Diff-Pruning [10] as the main base- line for our method since Diff-Pruning also reduces the training effort for compressing DMs. For the experiment on LDM, we extend [66] as another baseline to represent one of the best methods to re-design a lightweight diffusion model. For the experiment on Stable Diffusion, we choose BK-SDMs [21], which are trained on 2.3M LAION image- text pairs, as baselines of architecturally compression and distillation for Stable Diffusion. 4.2. Complexity Analysis We first analyze the improvement in inference speed facil- itated by DeepCache. The notable acceleration in infer- ence speed primarily arises from incomplete reasoning in denoising steps, with layer removal accomplished by parti- tioning the U-Net by the skip connection. In Figure 4, we present the division of MACs on two models. For each skip branch i, the MACs here contain the MACs in down block Di and the up block Ui. There is a difference in the amount 1https://huggingface.co/runwayml/stable-diffusion-v1-5 5ImageNet 256× 256 Method MACs↓ Throughput↑ Speed ↑ Retrain FID ↓ sFID ↓ IS ↑ Precision↑ Recall ↑ IDDPM [42] 1416.3G - - ✗ 12.26 5.42 - 70.0 62.0 ADM-G [9] 1186.4G - - ✗ 4.59 5.25 186.70 82.0 52.0 LDM-4 [47] 99.82G 0.178 1 × ✗ 3.60 - 247.67 87.0 48.0 LDM-4* 99.82G 0.178 1 × ✗ 3.37 5.14 204.56 82.71 53.86 Spectral DPM [66] 9.9G - - ✓ 10.60 - - - - Diff-Pruning [10]* 52.71G 0.269 1.51 × ✓ 9.27(9.16) 10.59 214.42 (201.81) 87.87 30.87 Uniform - N=2 52.12G 0.334 1.88 × ✗ 3.39 5.11 204.09 82.75 54.07 Uniform - N=3 36.48G 0.471 2.65 × ✗ 3.44 5.11 202.79 82.65 53.81 Uniform - N=5 23.50G 0.733 4.12 × ✗ 3.59 5.16 200.45 82.36 53.31 Uniform - N=10 13.97G 1.239 6.96 × ✗ 4.41 5.57 191.11 81.26 51.53 Uniform - N=20 9.39G 1.876 10.54 × ✗ 8.23 8.08 161.83 75.31 50.57 NonUniform - N=10 13.97G 1.239 6.96 × ✗ 4.27 5.42 193.11 81.75 51.84 NonUniform - N=20 9.39G 1.876 10.54 × ✗ 7.11 7.34 167.85 77.44 50.08 Table 1. Class-conditional generation quality on ImageNet using LDM-4-G. The baselines here, as well as our methods, employ 250 DDIM steps. *We reproduce Diff-Pruning to have a comprehensive comparison and the official results are shown in brackets. CIFAR-10 32×32 Method MACs↓ Throughput↑ Speed↑ Steps↓ FID↓ DDPM 6.1G 9.79 1 × - 4.19 DDPM* 6.1G 9.79 1 × - 4.16 Diff-Pruning 3.4G 13.45 1.37 × 100k 5.29 Ours - N=2 4.15G 13.73 1.40 × 0 4.35 Ours - N=3 3.54G 15.74 1.61 × 0 4.70 Ours - N=5 3.01G 18.11 1.85 × 0 5.73 Ours - N=10 2.63G 20.26 2.07 × 0 9.74 LSUN-Bedroom 256×256 Method MACs↓ Throughput↑ Speed↑ Steps↓ FID↓ DDPM 248.7G 0.21 1 × - 6.62 DDPM* 248.7G 0.21 1 × - 6.70 Diff-Pruning 138.8G 0.31 1.48 × 200k 18.60 Ours - N=2 190.8G 0.27 1.29 × 0 6.69 Ours - N=3 172.3G 0.30 1.43 × 0 7.20 Ours - N=5 156.0G 0.31 1.48 × 0 9.49 LSUN-Churches 256×256 Method MACs↓ Throughput↑ Speed↑ Steps↓ FID↓ DDPM 248.7G 0.21 1 × - 10.58 DDPM* 248.7G 0.21 1 × - 10.87 Diff-Pruning 138.8G 0.31 1.48 × 500k 13.90 Ours - N=2 190.8G 0.27 1.29 × 0 11.31 Ours - N=3 172.3G 0.30 1.43 × 0 11.75 Ours - N=5 156.0G 0.31 1.48 × 0 13.68 Table 2. Results on CIFAR-10, LSUN-Bedroom and LSUN- Churches. All the methods here adopt 100 DDIM steps. * means the reproduced results, which are more comparable with our re- sults since the random seed is the same. of computation allocated to different skip branches for dif- ferent models. Stable diffusion demonstrates a compara- tively uniform distribution across layers, whereas DDPM exhibits more computational burden concentrated within the first several layers. Our approach would benefit from U-Net structures that have a larger number of skip branches, facili- tating finer divisions of models, and giving us more choices for trade-off the speed and quality. In our experiment, we choose the skip branch 3/1/2 for DDPMs, LDM-4-G and Stable Diffusion respectively. We provide the results of us- ing different branches in Appendix. To comprehensively evaluate the efficiency of our method, in the following experiments, we report the throughput of each model using a single RTX2080 GPU. Besides, we report MACs in those tables, which refer to the average MACs for all steps. 4.3. Comparison with Compression Methods LDM-4-G for ImageNet. We conduct experiments on ImageNet, and the results are shown in Table 1. When ac- celerating to 4.1× the speed, a minor performance decline is observed (from 3.39 to 3.59). Compared with the pruning and distillation methods, a notable improvement over those methods is observed in FID and sFID, even though the ac- celeration ratio of our method is more substantial. Further- more, the augmentation in quality becomes more obvious with a larger number N of caching intervals if we employ the non-uniform 1:N strategy. Detailed results for the non- uniform 1:N strategy for small N and the hyper-parameters for the non-uniform strategy are provided in the Appendix. DDPMs for CIFAR-10 and LSUN. The results on CI- FAR10, LSUN-Bedroom and LSUN-Churches are shown in Table 2. From these tables, we can find out that our method surpasses those requiring retraining, even though our meth- ods have no retraining cost. Additionally, since we adopt a layer-pruning approach, which is more hardware-friendly, our acceleration ratio is more significant compared to the baseline method, under similar MACs constraints. 6A person in a helmet is riding a skateboardThere are three vases made of clay on a tableA bicycle is standing next to a bed in a roomA kitten that is sitting down by a  doorA serene mountain landscape with a flowing river, lush greenery…A delicate floral arrangement with soft, pastel colors and light…A magical winter wonderland at night. Envision a landscape…A photograph of an abandoned house at the edge of a forest…A man holding a surfboard walking on a beach next to the ocean Stable Diffusion v1.5 BK-SDM-Tiny Ours Figure 5. Visualization of the generated images by BK-SDM-Tiny and DeepCache. All the methods adopt the 50-step PLMS. The time here is the duration to generate a single image. Some prompts are omitted from this section for brevity. See Appendix for details. PartiPrompts COCO2017 Method MACs ↓ Throughput ↑ Speed ↑ CLIP Score ↑ MACs ↓ Throughput ↑ Speed ↑ CLIP Score ↑ PLMS - 50 steps 338.83G 0.230 1.00 × 29.51 338.83G 0.237 1.00 × 30.30 PLMS - 25 steps 169.42G 0.470 2.04 × 29.33 169.42G 0.453 1.91 × 29.99 BK-SDM - Base 223.81G 0.343 1.49 × 28.88 223.81G 0.344 1.45 × 29.47 BK-SDM - Small 217.78G 0.402 1.75 × 27.94 217.78G 0.397 1.68 × 27.96 BK-SDM - Tiny 205.09G 0.416 1.81 × 27.36 205.09G 0.415 1.76 × 27.41 Ours 130.45G 0.494 2.15 × 29.46 130.45G 0.500 2.11 × 30.23 Table 3. Comparison with PLMS and BK-SDM. We utilized prompts in PartiPrompt and COCO2017 validation set to generate images at the resolution of 512. We choose N=5 to achieve a throughput that is comparable to or surpasses that of established baseline methods. Results for other choices of N can be found in Figure 6. Stable Diffusion. The results are presented in Table 3. We outperform all three variants of BK-SDM, even with a faster denoising speed. As evident from the showcased ex- amples in Figure 5, the images generated by our method ex- hibit a greater consistency with the images generated by the original diffusion model, and the image aligns better with the textual prompt. 4.4. Comparison with Fast Sampler. We conducted a comparative analysis with methods focused on reducing sampling steps. It is essential to highlight that our approach is additive to those fast samplers, as we show in previous experiments. In Table 3 and Table 4, we first compared our method with the DDIM [58] or PLMS [33] under similar throughputs. We observe that our method achieved slightly better results than 25-step PLMS on Sta- ble Diffusion and comparable results to DDIM on LDM-4- G. We also measured the performance comparison between PLMS and our method under different acceleration ratios in Figure 6 to provide a more comprehensive comparison. 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 Speedup Ratio 28.6 28.8 29.0 29.2 29.4 29.6CLIP Score PartiPrompt PLMS Uniform 1:N Non-Uniform 1:N 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 2.6 Speedup Ratio 29.4 29.6 29.8 30.0 30.2 30.4CLIP Score MS-COCO 2017 PLMS Uniform 1:N Non-Uniform 1:N Figure 6. Comparison between PLMS, DeepCache with uniform 1:N and non-uniform 1:N stratigies. 4.5. Analysis Ablation Study. DeepCache can be conceptualized as in- corporating (N −1)×K steps of shallow network inference on top of the DDIM’s K steps, along with more updates of the noisy images. It is important to validate whether the additional computations of shallow network inference and the caching of features yield positive effectiveness: 1) Ef- fectiveness of Cached Features: We assess the impact of 7A cat standing on the edge of a sink drink water A child riding a skate boardon a city street Figure 7. Illustration of the evolution in generated images with increasing caching interval N. Method Throughput ↑ FID ↓ sFID ↓ DDIM - 59 steps 0.727 3.59 5.14 Ours 0.733 3.59 5.16 DDIM - 91 steps 0.436 3.46 50.6 Ours 0.471 3.44 5.11 Table 4. Comparison with DDIM under the same throughput. Here we conduct class-conditional for ImageNet using LDM-4-G. cached features in Table 5. Remarkably, we observe that, without any retraining, the cached features play a pivotal role in the effective denoising of diffusion models employ- ing a shallow U-Net. 2) Positive Impact of Shallow Net- work Inference: Building upon the cached features, the shallow network inference we conduct has a positive im- pact compared to DDIM. Results presented in Table 6 in- dicate that, with the additional computation of the shallow U-Net, DeepCache improves the 50-step DDIM by 0.32 and the 10-step DDIM by 2.98. Illustration of the increasing caching interval N. In Figure 7, we illustrate the evolution of generated images as we increment the caching interval. A discernible trend emerges as a gradual reduction in time, revealing that the primary features of the images remain consistent with their predecessors. However, subtle details such as the color of clothing and the shape of the cat undergo modifications. Quantitative insights are provided in Table 1 and Figure 6, where with an interval N <5, there is only a slight reduc- tion in the quality of the generated image. 5. Limitations The primary limitation of our method originates from its de- pendence on the pre-defined structure of the pre-trained dif- fusion model. Specifically, when a model’s shallowest skip branch encompasses a substantial portion of computation, such as 50% of the whole model, the achievable speedup Model Dataset DeepCache w/o Cached Features DDPM Cifar10 9.74 192.98 LDM-4-G ImageNet 7.36 312.12 Table 5. Effectiveness of Cached Features. Under identical hyper- parameters, we replace the cached features with a zero matrix. Steps DDIM FID↓ DeepCache FID↓ ∆ 50 4.67 4.35 -0.32 20 6.84 5.73 -1.11 10 13.36 10.38 -2.98 Table 6. Effectiveness of Shallow Network Inference. Steps here mean the number of steps that perform full model inference. ratio through our approach becomes relatively constrained. Additionally, our method encounters non-negligible perfor- mance degradation with larger caching steps (e.g., N=20), which could impose constraints on the upper limit of the acceleration ratio. 6. Conclusion In this paper, we introduce a novel paradigm, DeepCache, to accelerate the diffusion model. Our strategy employs the similarity observed in high-level features across adjacent steps of the diffusion model, thereby mitigating the compu- tational overhead associated with redundant high-level fea- ture calculations. Additionally, we leverage the structural attributes in the U-Net architecture to facilitate the updat- ing of low-level features. Through the adoption of Deep- Cache, a noteworthy acceleration in computational speed is achieved. Empirical evaluations on several datasets and dif- fusion models demonstrate that DeepCache surpass other compression methods that focus on the reduction of param- eter size. Moreover, the proposed algorithm demonstrates comparable and even slightly superior generation quality compared to existing techniques such as DDIM and PLMS, thereby offering a new perspective in the field. 8References [1] Martin Arjovsky, Soumith Chintala, and L ´eon Bottou. Wasserstein generative adversarial networks. In Interna- tional conference on machine learning , pages 214–223. PMLR, 2017. 2 [2] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18208–18218, 2022. 2 [3] Fan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analytic- dpm: an analytic estimate of the optimal reverse vari- ance in diffusion probabilistic models. arXiv preprint arXiv:2201.06503, 2022. 2 [4] Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Sch ¨onlieb, and Christian Etmann. Non-uniform diffusion models. arXiv preprint arXiv:2207.09786, 2022. 3 [5] Daniel Bolya and Judy Hoffman. Token merging for fast sta- ble diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4598– 4602, 2023. 3 [6] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mo- hammad Norouzi, and William Chan. Wavegrad: Esti- mating gradients for waveform generation. arXiv preprint arXiv:2009.00713, 2020. 2 [7] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. Ilvr: Conditioning method for denoising diffusion probabilistic models. arXiv preprint arXiv:2108.02938, 2021. 2 [8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009. 5 [9] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural informa- tion processing systems, 34:8780–8794, 2021. 1, 2, 6 [10] Gongfan Fang, Xinyin Ma, and Xinchao Wang. Structural pruning for diffusion models. In Advances in Neural Infor- mation Processing Systems, 2023. 2, 5, 6 [11] Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, and LingPeng Kong. Diffuseq: Sequence to sequence text generation with diffusion models. arXiv preprint arXiv:2210.08933, 2022. 2 [12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. 2 [13] Yefei He, Luping Liu, Jing Liu, Weijia Wu, Hong Zhou, and Bohan Zhuang. Ptqd: Accurate post-training quantiza- tion for diffusion models. arXiv preprint arXiv:2305.10657, 2023. 2, 3 [14] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation met- ric for image captioning. arXiv preprint arXiv:2104.08718, 2021. 5 [15] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilib- rium. Advances in neural information processing systems , 30, 2017. 5 [16] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual con- cepts with a constrained variational framework. In Interna- tional conference on learning representations, 2016. 2 [17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif- fusion probabilistic models. Advances in neural information processing systems, 33:6840–6851, 2020. 1, 2, 3, 5, 12 [18] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion mod- els. arXiv preprint arXiv:2210.02303, 2022. 2 [19] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restoration models. Advances in Neural Information Processing Systems, 35:23593–23606, 2022. 2 [20] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. InPro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6007–6017, 2023. 2 [21] Bo-Kyeong Kim, Hyoung-Kyu Song, Thibault Castells, and Shinkook Choi. On architectural compression of text-to- image diffusion models. arXiv preprint arXiv:2305.15798, 2023. 2, 5 [22] Diederik P Kingma and Max Welling. Auto-encoding varia- tional bayes. arXiv preprint arXiv:1312.6114, 2013. 2 [23] Naveen Kodali, Jacob Abernethy, James Hays, and Zsolt Kira. On convergence and stability of gans. arXiv preprint arXiv:1705.07215, 2017. 2 [24] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 5 [25] Tuomas Kynk ¨a¨anniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall met- ric for assessing generative models. Advances in Neural In- formation Processing Systems, 32, 2019. 5 [26] Haoying Li, Yifan Yang, Meng Chang, Shiqi Chen, Huajun Feng, Zhihai Xu, Qi Li, and Yueting Chen. Srdiff: Single image super-resolution with diffusion probabilistic models. Neurocomputing, 479:47–59, 2022. 2 [27] Lijiang Li, Huixia Li, Xiawu Zheng, Jie Wu, Xuefeng Xiao, Rui Wang, Min Zheng, Xin Pan, Fei Chao, and Rongrong Ji. Autodiffusion: Training-free optimization of time steps and architectures for automated diffusion model acceleration. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7105–7114, 2023. 3 [28] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang, and Tatsunori B Hashimoto. Diffusion-lm improves control- lable text generation. Advances in Neural Information Pro- cessing Systems, 35:4328–4343, 2022. 2 [29] Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Snap- fusion: Text-to-image diffusion model on mobile devices 9within two seconds. arXiv preprint arXiv:2306.00980, 2023. 2 [30] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 300–309, 2023. 2 [31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740–755. Springer, 2014. 5 [32] Enshu Liu, Xuefei Ning, Zinan Lin, Huazhong Yang, and Yu Wang. Oms-dpm: Optimizing the model schedule for diffu- sion probabilistic models. arXiv preprint arXiv:2306.08860, 2023. 2, 3 [33] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds.arXiv preprint arXiv:2202.09778, 2022. 2, 5, 7 [34] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems , 35:5775–5787, 2022. 2 [35] Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2837–2845, 2021. 2 [36] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tieniu Tan. Videofusion: Decomposed diffusion mod- els for high-quality video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10209–10218, 2023. 2 [37] Zhaoyang Lyu, Xudong Xu, Ceyuan Yang, Dahua Lin, and Bo Dai. Accelerating diffusion models via early stop of the diffusion process. arXiv preprint arXiv:2205.12524, 2022. 2 [38] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia- jun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equa- tions. arXiv preprint arXiv:2108.01073, 2021. 2 [39] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition, pages 14297–14306, 2023. 2 [40] Taehong Moon, Moonseok Choi, EungGu Yun, Jongmin Yoon, Gayoung Lee, and Juho Lee. Early exiting for accel- erated inference in diffusion models. In ICML 2023 Work- shop on Structured Probabilistic Inference{\\&} Generative Modeling, 2023. 3 [41] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models.arXiv preprint arXiv:2112.10741, 2021. 2 [42] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning, pages 8162–8171. PMLR, 2021. 6 [43] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden- hall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. 2 [44] Vadim Popov, Ivan V ovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov. Grad-tts: A diffusion prob- abilistic model for text-to-speech. In International Confer- ence on Machine Learning, pages 8599–8608. PMLR, 2021. 2 [45] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image gener- ation with clip latents. arXiv preprint arXiv:2204.06125, 1 (2):3, 2022. 2 [46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj ¨orn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–10695, 2022. 2 [47] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj ¨orn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–10695, 2022. 2, 5, 6 [48] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U- net: Convolutional networks for biomedical image segmen- tation. In Medical Image Computing and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234–241. Springer, 2015. 3 [49] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 Conference Proceedings , pages 1– 10, 2022. 2 [50] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:36479–36494, 2022. 2 [51] Chitwan Saharia, Jonathan Ho, William Chan, Tim Sal- imans, David J Fleet, and Mohammad Norouzi. Image super-resolution via iterative refinement. IEEE Transactions on Pattern Analysis and Machine Intelligence , 45(4):4713– 4726, 2022. 2 [52] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022. 2 [53] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. 5 [54] Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and Yan Yan. Post-training quantization on diffusion models. In 10Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 1972–1981, 2023. 3 [55] Andy Shih, Suneel Belkhale, Stefano Ermon, Dorsa Sadigh, and Nima Anari. Parallel sampling of diffusion models. arXiv preprint arXiv:2305.16317, 2023. 2, 5 [56] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792 , 2022. 2 [57] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International confer- ence on machine learning, pages 2256–2265. PMLR, 2015. 1 [58] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. 2, 5, 7 [59] Yang Song and Stefano Ermon. Generative modeling by esti- mating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. 1, 2 [60] Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable approach to density and score estimation. In Uncertainty in Artificial Intelligence , pages 574–584. PMLR, 2020. 2 [61] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab- hishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equa- tions. arXiv preprint arXiv:2011.13456, 2020. 2 [62] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. 2023. 2 [63] Shengkun Tang, Yaqing Wang, Caiwen Ding, Yi Liang, Yao Li, and Dongkuan Xu. Deediff: Dynamic uncertainty-aware early exiting for accelerating diffusion model generation. arXiv preprint arXiv:2309.17074, 2023. 3 [64] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space. Advances in Neural In- formation Processing Systems, 34:11287–11302, 2021. 2 [65] Shuai Yang, Yukang Chen, Luozhou Wang, Shu Liu, and Yingcong Chen. Denoising diffusion step-aware models. arXiv preprint arXiv:2310.03337, 2023. 2 [66] Xingyi Yang, Daquan Zhou, Jiashi Feng, and Xinchao Wang. Diffusion probabilistic model made slim. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22552–22562, 2023. 2, 3, 5, 6 [67] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015. 5 [68] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun- jan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin- fei Yang, Burcu Karagol Ayan, et al. Scaling autoregres- sive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022. 5 [69] Qinsheng Zhang and Yongxin Chen. Fast sampling of dif- fusion models with exponential integrator. arXiv preprint arXiv:2204.13902, 2022. 2 [70] Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Aziz- zadenesheli, and Anima Anandkumar. Fast sampling of dif- fusion models via operator learning. In International Con- ference on Machine Learning, pages 42390–42402. PMLR, 2023. 2 11DeepCache: Accelerating Diffusion Models for Free Supplementary Material A. Pseudo algorithm We present the pseudocode for our algorithm in Algorithm 1. It illustrates the iterative generation process over N steps, involving one step of complete model inference and N-1 steps of partial model inference. Here, we employ the sam- pling algorithm from DDPM [17] as an example. Our algo- rithm is adaptable to other fast sampling methods. Algorithm 1: DeepCache Input: A U-Net Model with down-sample blocks {Di}d i=1, up-sample blocks{Ui}d i=1 and middle blocks M Input: Caching Interval N, Branch Index m Input: Output from step xt, timestep t Output: predicted output at t − N step ▷ 1. Cache Step - Calculate ϵθ (xt, t) and xt−1 h0 ← xt ▷ hi for down-sampling features for i = 1, . . . , ddo hi ← Di(hi−1) ud+1 ← M(hd) ▷ ui for up-sampling features for i = d, . . . ,1 do if i = m then Store ui+1 in Cache ui ← Ui(Concat(ui+1, hi)) xt−1 = 1√αt \u0010 xt − 1−αt√1−¯αt u1 \u0011 + σtz ▷ z ∼ N(0, I) ▷ 2. Retrieve Step - Calculate {xt−i}N i=2 for n = 2, . . . , Ndo h0 ← xt−n+1 for i = 1, . . . , mdo hi ← Di(hi−1) Retrieve ui+1 from Cache for i = m, . . . ,1 do ui ← Ui(Concat(ui+1, hi)) xt−n = 1√αt \u0010 xt − 1−αt√1−¯αt u1 \u0011 + σtz ▷ z ∼ N(0, I) return xt−N B. Varying Hyper-parameters in Non-uniform 1:N Strategy In the non-uniform 1:N strategy, the two hyper-parameters involved are the center c and the power p, which is used to determine the sequence of timesteps for conducting the en- tire model inference. We test on LDM-4-G for the impact of these hyper-parameters. Results are shown in Table 7 and Table 8. From these two tables, a clear trend is evident in the observations: as the parametersp and c are incremented, there is an initial improvement in the generated image qual- ity followed by a subsequent decline. This pattern affirms the effectiveness of the strategy and also aligns with the lo- cation of the significant decrease in similarity observed in Figure 2(c). ImageNet 256× 256 Center FID ↓ sFID ↓ IS ↑ Precision↑ Recall↑ c = 10 8.26 8.47 160.3 75.69 48.93 c = 20 8.17 8.46 161.18 75.77 48.95 c = 50 7.77 8.16 163.74 76.23 49.18 c = 80 7.36 7.76 166.21 76.93 49.75 c = 100 7.16 7.51 167.52 77.30 49.64 c = 120 7.11 7.34 167.85 77.44 50.08 c = 150 7.33 7.36 166.04 77.09 49.98 c = 200 8.09 7.79 160.50 75.85 49.69 Table 7. Varying Centerc with the power p equals to 1.2. Here the caching interval is set to 20. ImageNet 256× 256 Power FID ↓ sFID↓ IS ↑ Precision↑ Recall↑ p = 1.05 7.36 7.52 166.12 77.06 50.38 p = 1.1 7.25 7.44 166.82 77.17 50.13 p = 1.2 7.11 7.34 167.85 77.44 50.08 p = 1.3 7.09 7.35 167.97 77.56 50.34 p = 1.4 7.13 7.39 167.68 77.42 50.26 p = 1.5 7.25 7.44 166.82 77.17 50.13 Table 8. Varying Powerp with the center c equals to 120. Here the caching interval is also set to 20. N=2 N=3 N=5 N=10 N=20 Center -c 120 120 110 110 120 Power -p 1.2 1.2 1.4 1.2 1.4 Table 9. Hyper-parameters for the non-uniform 1:N strategy in LDM-4-G N=2 N=3 N=4 N=5 N=6 N=7 N=8 Center -c 15 15 15 10 15 15 10 Power -p 1.5 1.3 1.4 1.5 1.3 1.4 1.4 Center -c 20 20 20 15 15 15 20 Power -p 1.3 1.4 1.4 1.3 1.5 1.5 1.3 Table 10. Hyper-parameters for the non-uniform 1:N strategy in Stable Diffusion v1.5. Selected Hyper-parameters for non-uniform 1:N For experiments in LDM, the optimal hyper-parameters and shown in Table 9. For experiments in Stable Diffusion, we chose center timesteps from the set {0, 5, 10, 15, 20, 25 } and power values from the set {1.1, 1.2, 1.3, 1.4, 1.5, 1.6}. 12ImageNet 256×256 (250 DDIM Steps) Method FID↓ sFID↓ IS↑ Precision↑ Recall↑ Method FID↓ sFID↓ IS↑ Precision↑ Recall↑ Baseline - LDM-4* 3.37 5.14 204.56 82.71 53.86 Baseline - LDM-4 3.60 - 247.67 87.0 48.0 Uniform - N=2 3.39 5.11 204.09 82.75 54.07 Non-uniform - N=2 3.46 5.14 204.12 83.21 53.53 Uniform - N=3 3.44 5.11 202.79 82.65 53.81 Non-uniform - N=3 3.49 5.13 203.22 83.18 53.44 Uniform - N=5 3.59 5.16 200.45 82.36 53.31 Non-uniform - N=5 3.63 5.12 200.04 83.07 53.25 Uniform - N=10 4.41 5.57 191.11 81.26 51.53 Non-uniform - N=10 4.27 5.42 193.11 81.75 51.84 Uniform - N=20 8.23 8.08 161.83 75.31 50.57 Non-uniform - N=20 7.36 7.76 166.21 76.93 49.75 Table 11. Comparing non-uniform and uniform 1:N strategy in class-conditional generation for ImageNet using LDM-4-G. *We regenerate the images using the official checkpoint of LDM-4-G. The optimal hyper-parameter values employed in our exper- iments are detailed in Table 10. From the selected hyper-parameters, we found out that the optimal values vary slightly across different datasets. A noticeable trend is observed, indicating that the majority of optimal parameters tend to center around the 15th timestep, accompanied by a power value of approximately 1.4. C. Non-uniform 1:N v.s. Uniform 1:N We have shown the comparison of the non-uniform 1:N ver- sus uniform 1:N strategy on Stable Diffusion in Figure 6. Here, we extend the comparison to ImageNet with LDM-4- G, and the corresponding results are detailed in Table 11. In accordance with the observations from Table 11, a consistent pattern emerges compared to the findings on Stable Diffusion. Notably, when employing a substantial caching interval, the non-uniform strategy demonstrates a notable improvement, with the FID increasing from 8.23 to 7.36 with N=20. However, when dealing with a smaller caching interval (N <5), the strategy does not yield an en- hancement in image quality. In fact, in certain cases, it may even lead to a slight degradation of images, as evidenced by the FID increasing from 3.39 to 3.46 for N=2. D. Varying Skip Branches In Table 12, we show the impact on image quality as we vary the skip branch for executing DeepCache. For our ex- periments, we employ the uniform 1:N strategy with N=5, and the sampling of DDIM still takes 100 steps. From the results in the table, we observe that the choice of skip branch introduces a trade-off between speed and image fi- delity. Specifically, opting for the first skip branch with no down-sampling blocks and one up-sampling block yields approximately 3 × acceleration, accompanied by a reduc- tion in FID to 7.14. Additionally, certain skip branches ex- hibit significant performance variations, particularly the 6- th branch. The results emphasize an extra trade-off between speed and image quality, complementing the earlier noted trade-off linked to different sampling steps. This particular trade-off operates at the level of model size granularity and can be achieved without incurring additional costs. CIFAR-10 32× 32 Skip Branch MACs↓ Throughput↑ Speed↑ FID ↓ 1 1.60G 29.60 3.023 × 7.14 2 2.24G 22.24 2.272 × 5.94 3 3.01G 18.11 1.850 × 5.73 4 3.89G 15.44 1.577 × 5.69 5 4.58G 13.15 1.343 × 5.51 6 5.31G 11.46 1.171 × 4.93 7 5.45G 11.27 1.151 × 4.92 8 5.60G 11.07 1.131 × 4.76 9 5.88G 10.82 1.105 × 4.54 10 5.95G 10.73 1.096 × 4.57 11 5.99G 10.67 1.089 × 4.52 12 6.03G 10.59 1.082 × 4.48 Table 12. Effect of different skip branches. Here we test the impact under the uniform 1:5 strategy. E. Prompts Prompts in Figure 1(a): • A bustling city street under the shine of a full moon • A picture of a snowy mountain peak, illuminated by the first light of dawn • dark room with volumetric light god rays shining through window onto stone fireplace in front of cloth couch • A photo of an astronaut on a moon • A digital illustration of a medieval town, 4k, detailed, trending in artstation, fantasy • A photo of a cat. Focus light and create sharp, defined edges Prompts in Figure 5: • A person in a helmet is riding a skateboard • There are three vases made of clay on a table • A very thick pizza is on a plate with one piece taken. • A bicycle is standing next to a bed in a room. • A kitten that is sitting down by a door. • A serene mountain landscape with a flowing river, lush greenery, and a backdrop of snow-capped peaks, in the style of an oil painting. • A delicate floral arrangement with soft, pastel colors and light, flowing brushstrokes typical of watercolor paint- ings. • A magical winter wonderland at night. Envision a land- 13PLMS DeepCache Steps Throughput Speed CLIP Score N Throughput Speed Uniform 1:N Non-Uniform 1:N 50 0.230 1.00 29.51 1 - - - - 45 0.251 1.09 29.40 2 0.333 1.45 29.54 29.51 40 0.307 1.34 29.35 3 0.396 1.72 29.50 29.59 35 0.333 1.45 29.24 4 0.462 2.01 29.53 29.57 30 0.384 1.67 29.24 5 0.494 2.15 29.41 29.50 25 0.470 2.04 29.32 6 0.529 2.30 29.30 29.46 20 0.538 2.34 29.15 7 0.555 2.41 29.11 29.42 15 0.664 2.89 28.58 8 0.582 2.53 28.97 29.26 Table 13. Stable Diffusion v1.5 on PartiPrompt PLMS DeepCache Steps Throughput Speed CLIP Score N Throughput Speed Uniform 1:N Non-Uniform 1:N 50 0.237 1.00 30.24 1 - - - - 45 0.252 1.06 30.14 2 0.356 1.50 30.31 30.37 40 0.306 1.29 30.19 3 0.397 1.68 30.33 30.34 35 0.352 1.49 30.09 4 0.448 1.89 30.28 30.31 30 0.384 1.62 30.04 5 0.500 2.11 30.19 30.23 25 0.453 1.91 29.99 6 0.524 2.21 30.04 30.18 20 0.526 2.22 29.82 7 0.555 2.34 29.90 30.10 15 0.614 2.59 29.39 8 0.583 2.46 29.76 30.02 Table 14. Stable Diffusion v1.5 on MS-COCO 2017 Original OriginalOurs Ours Figure 8. DDPM for LSUN-Churches: Samples with DDIM-100 steps (upper line) and DDIM-100 steps + DeepCache with N=5 (lower line). The speedup Ratio here is 1.85×. scape covered in fresh snow, with twinkling stars above, a cozy cabin with smoke rising from its chimney, and a gentle glow from lanterns hung on the trees • A photograph of an abandoned house at the edge of a for- est, with lights mysteriously glowing from the windows, set against a backdrop of a stormy sky. high quality pho- tography, Canon EOS R3. • A man holding a surfboard walking on a beach next to the ocean. F. Detailed Results for Stable Diffusion We furnish the elaborate results corresponding to Figure 6 in Table 13 and Table 14. Given the absence of a defini- tive N for aligning the throughput of PLMS, we opt for an alternative approach by exploring results for various N val- ues. Additionally, we assess the performance of the PLMS algorithm across different steps. Analyzing the data from these tables reveals that for N < 5, there is minimal varia- tion in the content of the image, accompanied by only slight fluctuations in the CLIP Score. G. More Samples for Each Dataset We provide the generated images for each model and each dataset in Figure 8, Figure 9, Figure 10, Figure 11 and Fig- ure 12. 14Figure 9. Stable Diffusion v1.5: Samples with 50 PLMS steps (upper line) and 50 PLMS steps + DeepCache with N=5 (lower line). The speedup Ratio here is 2.15×. Here we select prompts from the MS-COCO 2017 validation set. 15Figure 10. LDM-4-G for ImageNet: Samples with DDIM-250 steps (upper line) and DDIM-250 steps + DeepCache with N=10 (lower line). The speedup Ratio here is 6.96×. 16Figure 11. DDPM for LSUN-Bedroom: Samples with DDIM-100 steps (upper line) and DDIM-100 steps + DeepCache with N=5 (lower line). The speedup Ratio here is 1.48×. 17Figure 12. DDPM for LSUN-Churches: Samples with DDIM-100 steps (upper line) and DDIM-100 steps + DeepCache with N=5 (lower line). The speedup Ratio here is 1.48×. 18",
      "meta_data": {
        "arxiv_id": "2312.00858v2",
        "authors": [
          "Xinyin Ma",
          "Gongfan Fang",
          "Xinchao Wang"
        ],
        "published_date": "2023-12-01T17:01:06Z",
        "pdf_url": "https://arxiv.org/pdf/2312.00858v2.pdf",
        "github_url": "https://github.com/horseee/DeepCache"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces DeepCache, a novel training-free paradigm to accelerate diffusion models by addressing their substantial computational costs. It capitalizes on the inherent temporal redundancy in sequential denoising steps, caching and reusing high-level features from U-Net's skip connections. This strategy reduces redundant computations, enabling a 2.3x speedup for Stable Diffusion v1.5 with a minimal CLIP Score decline (0.05) and a 4.1x speedup for LDM-4-G with a slight FID decrease (0.22). DeepCache is compatible with existing fast samplers and often achieves comparable or superior generation quality under the same throughput, outperforming retraining-dependent pruning and distillation methods.",
        "methodology": "DeepCache leverages the temporal consistency of high-level features between adjacent denoising steps in diffusion models. It modifies the U-Net architecture by identifying 'cacheable' high-level features from the main branch (e.g., from up-sampling block U_m+1) at a given timestep (t). For subsequent steps (t-1 to t-N+1), instead of recomputing these features, DeepCache retrieves them from a cache. Only low-level features from the skip branch (e.g., from down-sampling block D_m) are re-calculated, which involves significantly less computation. The method proposes both a uniform 1:N strategy (full inference every N steps, partial inference for N-1 steps) and a non-uniform 1:N strategy (adjusting sampling frequency based on feature similarity changes, e.g., using a quadratic distribution centered on a hyper-parameter 'c' with power 'p') to optimize the trade-off between speed and quality.",
        "experimental_setup": "DeepCache was evaluated on three diffusion models: DDPM, LDM, and Stable Diffusion v1.5. Experiments were conducted using 100-step DDIM for DDPM, 250-step DDIM for LDM, and 50-step PLMS for Stable Diffusion. Datasets included CIFAR10, LSUN-Bedroom, LSUN-Churches, ImageNet (for class-conditional generation), MS-COCO 2017 (5k validation set), and PartiPrompts (1.63k captions) for text-to-image generation. Evaluation metrics used were FID, sFID, IS, Precision-and-Recall, and CLIP Score (on ViT-g/14). Baselines for comparison included Diff-Pruning, Spectral DPM, and BK-SDMs, which require retraining, allowing DeepCache to demonstrate its training-free advantage. The throughput was measured using a single RTX2080 GPU, and average MACs were reported.",
        "limitations": "The primary limitation of DeepCache is its dependency on the pre-defined structure of the pre-trained diffusion model. If the shallowest skip branch of a model accounts for a substantial portion (e.g., 50%) of the total computation, the achievable speedup ratio becomes constrained. Additionally, the method experiences non-negligible performance degradation when larger caching steps (e.g., N=20) are employed, which restricts the upper limit of the acceleration ratio.",
        "future_research_directions": "The paper implies future work could explore alternative non-uniform inference strategies beyond the presented quadratic approach, particularly those centered on specific timesteps, as they can yield similar improvements in image quality. Further investigation into optimizing the selection of skip branches for caching to maximize speedup while minimizing quality degradation across various model architectures could also be an area for future research.",
        "experimental_code": "class DeepCacheSDHelper(object):    def __init__(self, pipe=None):        if pipe is not None: self.pipe = pipe    def enable(self, pipe=None):        assert self.pipe is not None        self.reset_states()        self.wrap_modules()    def disable(self):        self.unwrap_modules()        self.reset_states()    def set_params(self,cache_interval=1, cache_branch_id=0, skip_mode='uniform'):        cache_layer_id = cache_branch_id % 3        cache_block_id = cache_branch_id // 3        self.params = {            'cache_interval': cache_interval,            'cache_layer_id': cache_layer_id,            'cache_block_id': cache_block_id,            'skip_mode': skip_mode        }    def is_skip_step(self, block_i, layer_i, blocktype = \"down\"):        self.start_timestep = self.cur_timestep if self.start_timestep is None else self.start_timestep # For some pipeline that the first timestep != 0        cache_interval, cache_layer_id, cache_block_id, skip_mode =             self.params['cache_interval'], self.params['cache_layer_id'], self.params['cache_block_id'], self.params['skip_mode']        if skip_mode == 'uniform':            if (self.cur_timestep-self.start_timestep) % cache_interval == 0: return False        if block_i > cache_block_id or blocktype == 'mid':            return True        if block_i < cache_block_id: return False        return layer_i >= cache_layer_id if blocktype == 'down' else layer_i > cache_layer_id            def is_enter_position(self, block_i, layer_i):        return block_i == self.params['cache_block_id'] and layer_i == self.params['cache_layer_id']    def wrap_unet_forward(self):        self.function_dict['unet_forward'] = self.pipe.unet.forward        def wrapped_forward(*args, **kwargs):            self.cur_timestep = list(self.pipe.scheduler.timesteps).index(args[1].item())            result = self.function_dict['unet_forward'](*args, **kwargs)            return result        self.pipe.unet.forward = wrapped_forward    def wrap_block_forward(self, block, block_name, block_i, layer_i, blocktype = \"down\"):        self.function_dict[            (blocktype, block_name, block_i, layer_i)        ] = block.forward        def wrapped_forward(*args, **kwargs):            skip = self.is_skip_step(block_i, layer_i, blocktype)            result = self.cached_output[(blocktype, block_name, block_i, layer_i)] if skip else self.function_dict[(blocktype, block_name,  block_i, layer_i)](*args, **kwargs)            if not skip: self.cached_output[(blocktype, block_name, block_i, layer_i)] = result            return result        block.forward = wrapped_forward    def wrap_modules(self):        # 1. wrap unet forward        self.wrap_unet_forward()        # 2. wrap downblock forward        for block_i, block in enumerate(self.pipe.unet.down_blocks):            for (layer_i, attention) in enumerate(getattr(block, \"attentions\", [])):                self.wrap_block_forward(attention, \"attentions\", block_i, layer_i)            for (layer_i, resnet) in enumerate(getattr(block, \"resnets\", [])):                self.wrap_block_forward(resnet, \"resnet\", block_i, layer_i)            for downsampler in getattr(block, \"downsamplers\", []) if block.downsamplers else []:                self.wrap_block_forward(downsampler, \"downsampler\", block_i, len(getattr(block, \"resnets\", [])))            self.wrap_block_forward(block, \"block\", block_i, 0, blocktype = \"down\")        # 3. wrap midblock forward        self.wrap_block_forward(self.pipe.unet.mid_block, \"mid_block\", 0, 0, blocktype = \"mid\")        # 4. wrap upblock forward        block_num = len(self.pipe.unet.up_blocks)        for block_i, block in enumerate(self.pipe.unet.up_blocks):            layer_num = len(getattr(block, \"resnets\", []))            for (layer_i, attention) in enumerate(getattr(block, \"attentions\", [])):                self.wrap_block_forward(attention, \"attentions\", block_num - block_i - 1, layer_num - layer_i - 1, blocktype = \"up\")            for (layer_i, resnet) in enumerate(getattr(block, \"resnets\", [])):                self.wrap_block_forward(resnet, \"resnet\", block_num - block_i - 1, layer_num - layer_i - 1, blocktype = \"up\")            for upsampler in getattr(block, \"upsamplers\", []) if block.upsamplers else []:                self.wrap_block_forward(upsampler, \"upsampler\", block_num - block_i - 1, 0, blocktype = \"up\")            self.wrap_block_forward(block, \"block\", block_num - block_i - 1, 0, blocktype = \"up\")    def unwrap_modules(self):        # 1. unet forward        self.pipe.unet.forward = self.function_dict['unet_forward']        # 2. downblock forward        for block_i, block in enumerate(self.pipe.unet.down_blocks):            for (layer_i, attention) in enumerate(getattr(block, \"attentions\", [])):                attention.forward = self.function_dict[(\"down\", \"attentions\", block_i, layer_i)]            for (layer_i, resnet) in enumerate(getattr(block, \"resnets\", [])):                resnet.forward = self.function_dict[(\"down\", \"resnet\", block_i, layer_i)]            for downsampler in getattr(block, \"downsamplers\", []) if block.downsamplers else []:                downsampler.forward = self.function_dict[(\"down\", \"downsampler\", block_i, len(getattr(block, \"resnets\", [])))]            block.forward = self.function_dict[(\"down\", \"block\", block_i, 0)]        # 3. midblock forward        self.pipe.unet.mid_block.forward = self.function_dict[(\"mid\", \"mid_block\", 0, 0)]        # 4. upblock forward        block_num = len(self.pipe.unet.up_blocks)        for block_i, block in enumerate(self.pipe.unet.up_blocks):            layer_num = len(getattr(block, \"resnets\", []))            for (layer_i, attention) in enumerate(getattr(block, \"attentions\", [])):                attention.forward = self.function_dict[(\"up\", \"attentions\", block_num - block_i - 1, layer_num - layer_i - 1)]            for (layer_i, resnet) in enumerate(getattr(block, \"resnets\", [])):                resnet.forward = self.function_dict[(\"up\", \"resnet\", block_num - block_i - 1, layer_num - layer_i - 1)]            for upsampler in getattr(block, \"upsamplers\", []) if block.upsamplers else []:                upsampler.forward = self.function_dict[(\"up\", \"upsampler\", block_num - block_i - 1, 0)]            block.forward = self.function_dict[(\"up\", \"block\", block_num - block_i - 1, 0)]    def reset_states(self):        self.cur_timestep = 0        self.function_dict = {}        self.cached_output = {}        self.start_timestep = None",
        "experimental_info": "The `DeepCacheSDHelper` class is the core component for integrating DeepCache into existing Diffusers pipelines. It provides `enable()` and `disable()` methods to activate and deactivate the caching mechanism. The `set_params()` method allows configuring the caching behavior with `cache_interval` (N), `cache_branch_id` (used to pinpoint the `cache_layer_id` and `cache_block_id` for caching high-level features), and `skip_mode` (uniform or non-uniform). The `wrap_modules()` method dynamically replaces the `forward` calls of selected U-Net sub-modules (attentions, resnets, down/upsamplers, and blocks) with wrapped versions. These wrapped methods use `is_skip_step()` to decide whether to reuse a cached feature (`self.cached_output`) or recompute it. When recomputing, the result is stored in `self.cached_output` for future reuse, ensuring that high-level features are cached from the main branch and retrieved for subsequent steps."
      }
    },
    {
      "title": "ERQ: Error Reduction for Post-Training Quantization of Vision Transformers",
      "abstract": "Post-training quantization (PTQ) for vision transformers (ViTs) has received\nincreasing attention from both academic and industrial communities due to its\nminimal data needs and high time efficiency. However, many current methods fail\nto account for the complex interactions between quantized weights and\nactivations, resulting in significant quantization errors and suboptimal\nperformance. This paper presents ERQ, an innovative two-step PTQ method\nspecifically crafted to reduce quantization errors arising from activation and\nweight quantization sequentially. The first step, Activation quantization error\nreduction (Aqer), first applies Reparameterization Initialization aimed at\nmitigating initial quantization errors in high-variance activations. Then, it\nfurther mitigates the errors by formulating a Ridge Regression problem, which\nupdates the weights maintained at full-precision using a closed-form solution.\nThe second step, Weight quantization error reduction (Wqer), first applies Dual\nUniform Quantization to handle weights with numerous outliers, which arise from\nadjustments made during Reparameterization Initialization, thereby reducing\ninitial weight quantization errors. Then, it employs an iterative approach to\nfurther tackle the errors. In each iteration, it adopts Rounding Refinement\nthat uses an empirically derived, efficient proxy to refine the rounding\ndirections of quantized weights, complemented by a Ridge Regression solver to\nreduce the errors. Comprehensive experimental results demonstrate ERQ's\nsuperior performance across various ViTs variants and tasks. For example, ERQ\nsurpasses the state-of-the-art GPTQ by a notable 36.81% in accuracy for W3A4\nViT-S. Our codes are available at https://github.com/zysxmu/ERQ.",
      "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1 Towards Accurate Post-Training Quantization of Vision Transformers via Error Reduction Yunshan Zhong, Y ou Huang, Jiawei Hu, Yuxin Zhang, Rongrong Ji,Senior Member, IEEE Abstract—Post-training quantization (PTQ) for vision transformers (ViTs) has received increasing attention from both academic and industrial communities due to its minimal data needs and high time efficiency. However, many current methods fail to account for the complex interactions between quantized weights and activations, resulting in significant quantization errors and suboptimal performance. This paper presents ERQ, an innovative two-step PTQ method specifically crafted to reduce quantization errors arising from activation and weight quantization sequentially. The first step, Activation quantization error reduction (Aqer), first applies Reparameterization Initialization aimed at mitigating initial quantization errors in high-variance activations. Then, it further mitigates the errors by formulating a Ridge Regression problem, which updates the weights maintained at full-precision using a closed-form solution. The second step, Weight quantization error reduction (Wqer), first applies Dual Uniform Quantization to handle weights with numerous outliers, which arise from adjustments made during Reparameterization Initialization, thereby reducing initial weight quantization errors. Then, it employs an iterative approach to further tackle the errors. In each iteration, it adopts Rounding Refinement that uses an empirically derived, efficient proxy to refine the rounding directions of quantized weights, complemented by a Ridge Regression solver to reduce the errors. Comprehensive experimental results demonstrate ERQ’s superior performance across various ViTs variants and tasks. For example, ERQ surpasses the state-of-the-art GPTQ by a notable 36.81% in accuracy for W3A4 ViT -S. Our codes are available at https://github.com/zysxmu/ERQ. Index Terms—Model compression, vision transformers, post-training quantization, quantization error reduction. ✦ 1 I NTRODUCTION I N the realm of computer vision, vision transformers (ViTs) [1] have emerged as the new fundamental backbone models, significantly challenging the convolutional neural networks (CNNs). By leveraging multi-head self-attention (MHSA) mechanism to capture long-range relationships, ViTs exhibit strong and flexible representation capacity, thus resulting in impressive progress in a variety of vision tasks [2], [3], [4], [5], [6], [7]. However, ViTs’ great power comes with considerable complexity. The intricate architec- ture and large number of parameters of ViTs result in high computational and memory demands. As a result, deploying ViTs in resource-constrained environments such as mobile phones becomes a huge challenge [8], [9], [10], [11], [12], [13], [14]. To mitigate this dilemma, the model quantization tech- nique has received its popularity [15]. Quantization reduces model complexity by using a low-bit data format to represent the full-precision weights and activations. Consequently, quantization not only facilitates more efficient low-bit com- putation but also significantly reduces storage requirements, offering a promising solution for deployment in resource- limited devices [16]. However, quantization is known to Y. Zhong is with the Institute of Artificial Intelligence, Department of Artificial Intelligence, School of Informatics, and Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, Xiamen 361005, P .R. China. Y. Huang, J. Hu, Y. Zhang are with the Department of Artificial Intelligence, School of Informatics, and Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, Xiamen 361005, P .R. China. R. Ji (Corresponding Author) is with the Institute of Artificial Intelligence, and Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, Xiamen 361005, P .R. China, and also with the Peng Cheng Laboratory, Shenzhen 518000, P .R. China (e-mail: rrji@xmu.edu.cn). introduce notorious errors, which can significantly degrade model performance [17]. To mitigate quantization errors, Quantization-aware training (QAT) trains the quantized model with the full original dataset, effectively retaining performance [12], [16], [18], [19], [20], [21], [22]. However, QAT’s reliance on complete training datasets and compute- intensive retraining poses significant limitations [23]. Re- cently, to address the high demands of QAT, researchers have been gravitating towards post-training quantization (PTQ), which aims to quantize models using only a small calibration dataset and incurs minimal time costs [24], [25], [26], [27], [28]. Given the distinct structure of ViTs, such as MHSA and layer normalization (LN), traditional PTQ methods developed for CNNs often fall short when applied to ViTs, resulting in limited performance [13]. Therefore, researchers have developed specialized PTQ methods for ViTs. For example, the log2 quantizer [25], [29] and the twin uniform quantizer [30] are introduced for handling long-tail post- Softmax activations. To manage high variant activations, the power-of-two factor is employed [25]. For determining unstable scale factors, evolutionary search methods [27] are utilized. Despite these advancements, current methods suffer from considerable quantization error due to the overlook of the intricate interplay between weight and activation quantization. As a result, the performance of the quantized model remains unsatisfactory. In this paper, we introduce ERQ, a novel two-step PTQ method tailored for ViTs, that sequentially mitigates quantization error induced by quantized activations and weights, respectively. As depicted in Fig. 1, ERQ consists of two sequential steps: Activation quantization error reduction (Aqer) followed by Weight quantization error reduction arXiv:2407.06794v2  [cs.CV]  4 Feb 2025JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2 Fig. 1. Framework of the proposed ERQ. ERQ consists of two steps to reduce the quantization errors from activation and weight quantization, respectively. The first step, Activation quantization error reduction (Aqer), includes Reparameterization Initialization and Ridge Regression. The second step, Weight quantization error reduction (Wqer), includes Dual Uniform Quantization, Rounding Refinement, and Ridge Regression. (Wqer). In Aqer, we focus on quantizing activations while maintaining weights at full-precision to solely mitigate the quantization errors arising from activation quantization. Ini- tially, specifically for post-LayerNorm activations that exhibit high channel variance [25], we introduce Reparameterization Initialization. This approach leverages the reparameterization technique [29], which adopts the channel-wise quantizer at first and then translates it to the layer-wise equivalence through equivalent change between LN and weights, to set the quantizer for a low initial quantization error. Subse- quently, we further mitigate the errors by formulating a Ridge Regression problem, which offers a closed-form solution. This solution is then applied to update the full-precision weights, effectively reducing the quantization errors. Following Aqer, we then perform weight quantization and mitigate the induced quantization errors. Initially, we identify that weights adjusted via Reparameterization Initial- ization are prone to extensive outliers, incurring significant initial weight quantization errors under standard uniform quantization. To address this, we introduce Dual Uniform Quantization, which separates channels with outliers from those without, assigning individual quantization parameters to each type of channel, for a low initial quantization error. Moreover, an outlier channel selection algorithm with theoretically optimal outlier coverage is introduced to select the outlier channels. Subsequently, the errors are further mitigated in an iterative quantization-and-correction manner. In each iteration, the first half of the full-precision weights is quantized and the resulting errors are mitigated by first performing Rounding Refinement and then again solving a Ridge Regression problem. The former derives an efficient proxy for output error, which aids in refining the rounding directions of quantized weights to reduce the quantization errors. The latter mitigates the quantization errors by updating the remaining second half of full-precision weights. Such a process continuously performs until all weights are accurately quantized. The extensive experiments across various ViT variants (ViT, DeiT, and Swin) and tasks (image classification, ob- ject detection, instance segmentation, and image super- resolution) show that the proposed ERQ achieves a marked improvement over previous state-of-the-art approaches, high- lighting its versatility and robustness. For instance, in the image classification task, ERQ achieves a 36.81% accuracy gain for the W3A4 ViT-S. In object detection and instance segmentation tasks, ERQ enhances performance by 5.0 AP box and 4.8 AP mask respectively, when applied to W3A4 Mask R-CNN using a Swin-T backbone. Furthermore, in the image super-resolution task, ERQ increases the PSNR by 0.54 dB on the Urban dataset for W4A4 SwinIR ×2. This work extends from our previous work in [31]. The six new contributions include: (1) We offer a detailed description of the Reparameterization Initialization employed in our method, along with an in-depth discussion of motivation and adequate experimental analysis; (2) We propose a new Dual Uniform Quantization method and design a channel selection algorithm for optimal outlier coverage, accompanied by an in-depth discussion of the motivation and comprehensive experimental validation; (3) We conduct additional quan- titative and qualitative experiments on the image super- resolution task to further validate the effectiveness of our approach; (4) We perform an extensive set of experiments across multiple bit-width settings on object detection and instance segmentation, and add more recent methods in comparison; (5) We provide comprehensive ablation studies to investigate the effectiveness of each component within ERQ; (6) We present a detailed illustration of the overall framework of the proposed ERQ, enhancing understanding and applicability. 2 R ELATED WORK 2.1 Vision Transformers (ViTs) Inspired by the success of transformers in natural language processing, ViTs have emerged as a groundbreaking develop- ment in computer vision [1]. To address the dependency of ViTs on large datasets, DeiT [3] showcases a teacher- student training approach, offering an efficient data-driven training strategy. Subsequently, Swin Transformer [2] incor- porates a hierarchical structure with a shifted window-based self-attention mechanism, marking further advancements. Beyond the classification tasks, the applications of ViTs have expanded considerably, including areas such as object detection [4], [5], image segmentation [6], [32], low-level image processing [33], video classification [7], [34], and medical image processing [35], among others. However, ViTs are accompanied by substantial computational overhead and increased memory requirements, posing challenges for their deployment in resource-constrained environments. Despite numerous efforts to develop lightweight ViTs, such asJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3 MiniVit [36], MobileViT [37], and TinyViT [38], the complexity remains a concern [24], [39]. 2.2 Post-training Quantization for ViTs Model quantization reduces the numerical precision of weights and activations to decrease computational and storage demands of neural networks [15]. However, the process of converting input full-precision data into low-bit data format inevitably introduces notorious quantization errors, which can undesirably degrade model performance. To counteract these errors, a common way is to train the quantized model with the full original training data to accommodate the model with the quantization errors, which is known as quantization-aware training (QAT) [12], [16], [18], [19], [20], [21], [22]. While QAT effectively maintains model performance, it is vulnerable to the necessity for complete training data and compute-heavy retraining [23]. In contrast to QAT which involves complete training data and compute-heavy retraining, post-training quantization (PTQ) operates on a smaller dataset with a reduced time overhead, harvesting extensive attention [17]. Unfortunately, the unique architectural intricacies of ViTs, such as MHSA and LN, make that conventional PTQ methods, tailored for CNNs, often underperform when applied to ViTs [25], [29], [40], [41], [42]. Consequently, the need for specialized PTQ methods fo ViTs has been recognized, sparking extensive research in this direction. Liu et al. [13] introduce the first PTQ method for ViTs. To maintain the order of softmax scores and adapt various quantization sensitivities of different layers, they respectively introduce a ranking loss and a nuclear norm-based mixed-precision scheme. FQ-ViT [25] introduces a fully-quantized method, which respectively designs Powers-of-Two Scale and Log-Int-Softmax for post- LayerNorm and post-Softmax activations. In PTQ4ViT [30], a twin uniform quantizer is introduced to handle the long-tail post-Softmax activations and uneven post-GELU activations, complemented by a Hessian-guided metric for searching quantization scales. APQ-ViT [43] establishes a block-wise error reconstruction and a Matthew-effect preserving quan- tizer for post-Softmax activations. In Evol-Q [27], an evo- lutionary search method is employed to search extremely sensitive quantization parameters. RepQ-ViT [29] introduces a reparameterization technique to handle high-variant post- LayerNorm activations, where the channel-wise quantizers are simplified to layer-wise quantizers. Also, a Log √ 2 quan- tizer is adopted to accommodate post-Softmax activations. GPTQ [44] employs OBS [45] to progressively compensate for weight quantization error by utilizing Hessian information. IGQ-ViT [46] employs instance-aware group quantization for ViTs, where activations and post-Softmax scores are split into multiple groups dynamically for each instance and each group utilizes its own quantization parameters. OAS- ViT [47] provides theoretical insights to analyze the role of reconstruction granularity in mitigating the outlier problem in ViTs, which is also validated by the experimental results. 3 P RELIMINARIES 3.1 Quantizers For a fair comparison, our quantization settings are aligned with the earlier work [29]. Specifically, we quantize the weights and activations of all matrix multiplications in ViTs. The channel-wise quantizer (along the output channel) and layer-wise quantizer are adopted for weights and activations, respectively. For weights and the activations except for the post-Softmax activations, we adopt the uniform quantizer. Given full-precision values v and the bit-width b, the uniform quantizer is defined as: ¯v = Qun(v, b) = s · clip \u0010jv s m + z, 0, 2b − 1 \u0011 , (1) where ⌊·⌉ denotes the rounding function, clip(·) makes the output between 0 and 2b − 1, the scale factor s is determined through a grid search aimed at minimizing the error before and after quantization, and the zero-point z is calculated asj −min(v) s m . Both s and z are quantization parameters. For long-tail post-Softmax activations, the log √ 2 quantizer [29] is adopted: ¯v = Qlg √ 2(v, b) = s · 2⌊− xq 2 ⌋(1 (vq)( √ 2 − 1) + 1), (2) vq = clip \u0010j −2log2 v s m , 0, 2b − 1 \u0011 , (3) where 1(·) returns 0 for even numbers and 1 for odd numbers, s is determined through a grid search aimed at minimizing the error before and after quantization. All quantization parameters for the aforementioned quantizers are determined based on the calibration datasets. 3.2 Objective Denoting the full-precision activation as x ∼ P(x), where x ∈ RDin, and the weight W ∈ RDout×Din. Here, Din and Dout are the input and output dimensions, respectively. The quantization error induced by activation and weight quantization is denoted as δx = ¯x − x and δW = ¯W − W, respectively. For each layer, we aim to minimize the Mean Squared Error (MSE) before and after weight-activation quantization: LMSE = E \u0002 ∥Wx − ¯W¯x∥2 2 \u0003 = E \u0002 ∥Wx − (W + δW)(x + δx)∥2 2 \u0003 . (4) Eq. 4 indicates that output error is contributed by both activations and weight quantization error. 4 M ETHOD The entangled δx and δW make it a challenge to find an optimal solution for Eq. 4 [40]. To make it tractable, we relax Eq. 4 to two sequential sub-problems by respectively minimizing error from quantized activations and weights. As shown in Fig. 1, we first perform Activation quantization error reduction (Aqer) followed by Weight quantization error reduction (Wqer), which are respectively detailed in the following. 4.1 Activation Quantization Error Reduction We start by quantizing the activations layer-wise while retaining the weights as full-precision. Currently, given the quantization parameters, the MSE caused by activation quantization error δx is defined as the optimization target: LMSE = E \u0002 ∥Wx − W(x + δx)∥2 2 \u0003 . (5)JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4 Fig. 2. Example of channel distribution of activations after block.8.norm2 of DeiT -S. Results are extracted with 32 images. Fig. 3. Comparison of MSE between using and not using Reparame- terization Initialization. The MSE is evaluated by Eq. 5. “RepI” indicates Reparameterization Initialization. Results are derived from DeiT -S with 32 images. Activations are quantized to 4-bit. To mitigate errors induced by activation quantization, we introduce Activation quantization error reduction (Aqer) including Reparameterization Initialization specific for high variant post-LayerNorm activations and Ridge Regression. 4.1.1 Reparameterization Initialization It is evident that the MSE defined in Eq. 5 decreases as δx is reduced. However, as illustrated in Fig. 2, post-LayerNorm activations exhibit high variance across input channels, leading to substantial quantization errors if layer-wise quan- tization is directly applied [25]. Correspondingly, as shown in Fig. 3, a large MSE occurs. Motivated by these observations, for post-LayerNorm activations, we adopt the reparameterization technique [29], which first initializes a channel-wise quantizer and then translates it to the layer-wise equivalence. To be specific, we first initialize input channel-wise scales s ∈ RDin and zero- point z ∈ RDin for post-LayerNorm activations, where Din is the number of the input channels. Subsequently, we adjust the parameters of LayerNorm (β ∈ RDin and γ ∈ RDin) and the weights and biases ( W ∈ RDout×Din and b ∈ RDin) of the subsequent layer by using Eq. 6 and Eq. 7, respectively. βnew = β + s ⊙ r2 r1 , γnew = γ r1 . (6) Wnew :,j = r1 ⊙ W:,j, bnew j = bj − (s ⊙ r2)W:,j, where j ∈ 1, 2, . . . , Din. (7) Here, ⊙ denotes Hadamard product, ˜s = mean(s) ∈ R1, ˜z = mean(z) ∈ R1, r1 = s/˜s, and r2 = z − ˜z. After adjustment, the channel-wise quantizer is transformed to a layer-wise quantizer. As shown in Fig. 3, Reparameterization Initialization significantly reduces MSE for all post-LayerNorm activations within the network, thereby demonstrating its effectiveness. As for other activations, which exhibit a relatively smooth dis- tribution, we continue to apply layer-wise quantization [25]. 4.1.2 Ridge Regression After initialization, we further mitigate activation quantiza- tion error δx by formulating the Ridge Regression problem. To be specific, we suggest minimizing Eq. 5 by adding W with an adjustment δW∗: E \u0002 ∥Wx − (W + δW∗)(x + δx)∥2 2 \u0003 + λ1∥δW∗∥2 2 = E \u0002 ∥ −δW∗(x + δx) − Wδx∥2 2 \u0003 + λ1∥δW∗∥2 2 = E \u0002 ∥δW∗¯x + Wδx∥2 2 \u0003 + λ1∥δW∗∥2 2. (8) Here, δW∗ denotes adjustment that is computed by Ridge Regression, ¯x = x + δx is the quantized input, λ1∥δW∗∥2 2 acts as the regularization term, λ1 is a hyper-parameter that controls the intensity of the regularization. Eq. 8 constitutes the Ridge Regression problem. To minimize it, we first compute its gradient w.r.t. δW∗: ∂ ∂δW∗ E \u0002 ∥δW∗¯x + Wδx∥2 2 \u0003 + λ1∥δW∗∥2 2 = E h 2(δW∗¯x + Wδx)¯xT i + 2λ1δW∗. (9) Then, we solve for δW∗ by setting Equation 9 to zero: E h 2(δW∗¯x + Wδx)¯xT i + 2λ1δW∗ = 0 ⇒ δW∗ = −WE h δx¯xT i (E h ¯x¯xT i + λ1I)−1. (10) The regularization term λ1I ensures the inverse of E \u0002 ¯x¯xT \u0003 + λ1I always exists, which is crucial for compu- tational stability. In addition, it suppresses outliers, thereby mitigating overfitting and enhancing the model’s general- izability. Suppressing outliers is also crucial for subsequent weight quantization since it restricts the range of weights. This restriction prevents the quantization points from being distributed in the uncovered region, thus enhancing the expressive ability of quantization [19]. In practice, given the calibration dataset, we estimate E \u0002 δx¯xT \u0003 and E \u0002 ¯x¯xT \u0003 using 1 N PN n δxn¯xT n and 1 N PN n ¯xn¯xT n , respectively. Here, N = B × T, where B is the size of the calibration dataset, and T is the number of tokens of one image. Note that δx and ¯x are determined given the input and the quantization parameters. After obtaining δW∗, we incorporate it into the network’s weights by W = W + δW∗. By doing so, the proposed Aqer explicitly mitigates the quantization error from quantized activations into the weights. 4.2 Weight Quantization Error Reduction After completing Aqer, the activation is quantized, and we proceed to quantize the weights. Here, given the quantization parameters, we solely consider the MSE caused by weightJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5 (a) (b) Fig. 4. Heatmap of absolute weight values: (a) Before and (b) After Repa- rameterization Initialization. Weights are extracted from blocks.8.mlp.fc1 of DeiT -S. For better visualization, elements with absolute values less than 0.2 have been set to zero. quantization error δW to obtain the following optimization target: LMSE = E \u0002 ∥W¯x − (W + δW)¯x∥2 2 \u0003 = DoutX i LMSE i = DoutX i E \u0002 ∥Wi,:¯x − (Wi,: + δWi,:)¯x∥2 2 \u0003 . (11) Eq. 11 suggests that the minimization across output chan- nels is conducted independently. To mitigate the resulting quantization error, we introduce Weight quantization error reduction (Wqer), which includes Dual Uniform Quantiza- tion specific for weights initially set by Reparameterization Initialization, Rounding Refinement, and Ridge Regression. Fig. 5. Comparison of MSE between using and not using Dual Uniform Quantization. The MSE is evaluated by Eq. 11. “Dual” indicates Dual Uniform Quantization. Results are derived from W4A4 DeiT -S with 32 images. 4.2.1 Dual Uniform Quantization It is evident that the smaller δWi,: leads a reduction in Eq. 11. Note that the weights are quantized channel-wise (along with the output channel), meaning that the values of Wi,: share the same quantization parameters. However, the weights adjusted by Reparameterization Initialization tend to present a quantization-unfriendly distri- bution. As illustrated in Fig. 4a and Fig. 4b, after employing Reparameterization Initialization, the weights in output channels suffer from extensive outliers. The issue stems from Eq. 7, which involves an input channel-wise multiplication of weights. Consequently, each value of Wi,: is multiplied by a factor r1 = s/˜s, which can significantly increase the values if s and ˜s diverge substantially, leading to the emergence of outliers. Due to this uneven distribution of Wi,:, using the same quantization parameters either incurs significant clip noise or rounding errors [17], inevitably leading to substantial quantization errors δWi,: [48]. As a result, as demonstrated in Fig. 5, a large MSE occurs. Fortunately, as demonstrated in Fig. 4b, the occurrence of outliers typically concentrates on certain input channels. Inspired by this observation, we introduce Dual Uniform Quantization, which employs two independent uniform quantizers to respectively manage input channels with and without outliers to reduce δWi,:. To determine which channels should be selected as outlier channels, we introduce an outlier channel selection algorithm, detailed in Alg. 1. This algorithm identifies a set O, whose size |O| is a predefined hyper-parameter, that encompasses input channels most affected by outliers. At first, we perform outlier detection for each output channel by utilizing percentile-based thresholds. For each output channel i (where i = 1, . . . , Dout), we calculate the 99-th percentile (τ99 i ) and the 1-th percentile ( τ1 i ) to serve as the upper and lower thresholds, respectively. Any value that falls outside the range defined by τ99 i and τ1 i is considered an outlier: Mi = {Wi,j ∈ Wi,: : Wi,j < τ1 i or Wi,j > τ99 i }. (12) Subsequently, we perform greedy selection to form O. In particular, for each input channel j (where j = 1, . . . , Din), we calculate the frequency of outlier occurrences, which is given by: fj = 1 Dout DoutX i=1 1(Wi,j ∈ Mi), (13) where 1 returns 1 if wij is an outlier for output channel i, and 0 otherwise. After calculating the frequency of outliers for all input channels, we greedy select the top |O| input channels with the highest frequency to form the set O: O = top index(f, |O|) (14) where f is the set of fj, and top index returns the index of the top |O| values in f. The time costs of Alg. 1 are minimal. For example, on W4A4 DeiT-S using a single NVIDIA 3090 GPU and an Intel Xeon 4214R CPU, the process takes approximately 24 seconds. In the following, we demonstrate the optimality of the above greedy selection in terms of outlier coverage. Proposition 1. Given the definition of outliers as specified in Eq. 12, the greedy selection algorithm achieves the maximal coverage rate of outliers in W. Proof. Assume for contradiction that there exists an alterna- tive set O′ with |O′| = |O|, which achieves better outlier coverage than the set O selected by the algorithm.JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6 Algorithm 1 Outlier Channel Selection 1: Input: W ∈ RDout×Din 2: Initialize: O = ∅ 3: /* Outlier Detection */ 4: for i = 1 to Dout do 5: Initialize Mi = ∅ 6: Compute 99-th and 1-th percentiles τ99 i , τ1 i for Wi,: 7: Obtain outliers set Mi for Wi,: by Eq. 12 8: end for 9: /* END Outlier Detection */ 10: /* Greedy Selection */ 11: Calculate the frequency of outliers for each input channel j by Eq. 13 12: Select the top |O| input channels with the highest fj to form O 13: /* END Greedy Selection */ 14: Output: Set O of selected input channels Define fj′ as the frequency of outliers for each j′ in O′ and fj as the frequency for each j in O. By assumption that O′ achieves higher coverage, it follows that: X j′∈O′ fj′ > X j∈O fj. (15) From Eq. 15, it follows that there exists at least one j′ in O′ not in O with fj′ > fj for some j in O: ∃j′ ∈ O′ \\ O, ∃j ∈ O: fj′ > fj. (16) However, according to Eq. 14, every j in O has fj greater than or equal to any fj′ for j′ not in O: ∀j ∈ O, ∀j′ /∈ O: fj ≥ fj′ . (17) Eq. 16 and Eq. 17 lead to a contradiction. Therefore, the assumption that O′ achieves better coverage than O must be false. By contradiction, the setO selected by Eq. 14 maximizes the coverage of outliers. The proposed outlier channel selection algorithm ensures that O consists of the channels most affected by outliers, thereby enabling Dual Uniform Quantization to separately quantize these specific channels that are prone to exacerbat- ing quantization errors due to the presence of significant outliers. Specifically, after obtaining O, for weights Wi,: of i- th output channel, the Dual Uniform Quantization is defined as: ¯Wi,j = Q1 un(Wi,j, b), j∈ O, (18) ¯Wi,j = Q2 un(Wi,j, b), j /∈ O. (19) Here, the extra overhead of Dual Uniform Quantization is negligible. For example, this only leads to less than 1% extra storage costs for W4A4 DeiT-S. Moreover, since O is pre-defined before inference, an efficient forward propa- gation can be achieved by reordering the input channels of the weights and activations as discussed in [49]. As shown in Fig. 5, implementing Dual Uniform Quantization significantly reduces the Mean Squared Error (MSE). For the weights of other layers that are less affected by outliers, we continue to apply standard uniform quantization. TABLE 1 Results of W4A4 DeiT -S with different methods for minimizing E h ∥δWs i,: ¯xs∥2 2 i . “baseline” indicates only perform Aqer. We use the pulp (a CPU-only LP modeler written in Python) to solve the MIPQ. The high time costs of using E h ∥δWs i,: ¯xs∥2 2 i make the results meaningless. Method Time costs Acc. (%) Baseline (Aqer) ∼ 1 minute 70.96 + MIPQ w/ E \u0002 ∥δWs i,: ¯xs∥2 2 \u0003 ∼130 hours - + MIPQ w/ Proxy ∼10 hours 71.89 + Rounding Refinement ∼4 minutes 71.80 Fig. 6. Distribution of activations across different channels. Results are extracted from W4A4 DeiT -S with 32 images. 4.2.2 Rounding Refinement Afterward, we further mitigate the weight quantization error δWi,:. Simultaneously quantizing the entire full-precision weights yields unrecoverable quantization error [44]. Thus, we adopt an iterative quantization-and-correction manner to gradually minimize quantization error [50]. In each iteration, the first half of unquantized weights is quantized, followed by a mitigation of the resulting quantization error. Specifically, we begin with the current full-precision weight Wi,: and the corresponding ¯x. We then partition W into two segments: the first half, Ws i,: ∈ R1×Ds in, is designated for quantization, while the remaining part, Wr i,: ∈ R1×Dr in, is retained at full-precision. Correspondingly, we derive ¯xs ∈ RDs in and ¯xr ∈ RDr in from ¯x, where ¯xs and ¯xrJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7 respectively contain the rows of ¯x corresponding to Ws i,: and Wr i,:. The quantization error of the quantized Ws i,: is denoted as δWs i,: = ¯Ws i,: − Ws i,:, and the resulting MSE is: LMSE i = E \u0002 ∥[Ws i,:, Wr i,:][¯xs, ¯xr] − [Ws i,: + δWs i,:, Wr i,:][¯xs, ¯xr]∥2 2 \u0003 = E \u0002 ∥δWs i,:¯xs∥2 2 \u0003 . (20) Here, Wi,: = [ Ws i,:, Wr i,:], ¯x = [ ¯xs, ¯xr]. To mitigate Eq. 20, we first introduce Rounding Refinement, in which the rounding direction of the quantized weights is refined, i.e., adjusting δWs i,:, to reduce E h ∥δWs i,:¯xs∥2 2 i itself. Then, given E h ∥δWs i,:¯xs∥2 2 i after Rounding Refinement, we formulate a Ridge Regression problem to further mitigate it by adjusting Wr i,:. Fig. 7. E denotes E h ∥δWs i,: ¯xs∥2 2 i . The proxy values are positively correlated with the real values. At first, we aim to adjust the rounding direction of quantized weights to minimize E h ∥δWs i,:¯xs∥2 2 i . Specifically, for the j-th value in Ws i,:, denoted as Ws i,j, the quantization involves rounding it either to the floor or ceil [51]. Thereby the quantization error for Ws i,:, denoted as δWs i,j, can be represented as either δWs↓i, jor δWs↑i, j. Here, δWs↓ i,j = Ws i,j − Qun↓(Ws i,j, b) > 0 denotes error from rounding-to- floor strategy, δWs↑ i,j = Ws i,j − Qun↑(Ws i,j, b) < 0 denotes error from rounding-to-ceil strategy, where ↓ / ↑ denote replacing ⌊·⌉ in Eq. 1 with ⌊·⌋/⌈·⌉. The selection of δWs i,: is an NP-hard problem, whose solution can be searched by the mixed-integer quadratic program (MIPQ) [52], [53]. However, the high computational complexity of E h ∥δWs i,:¯xs∥2 2 i makes it a challenge to find the solution with reasonable time costs. As shown in Tab. 1, using E h ∥δWs i,:¯xs∥2 2 i as the target of MIPQ consumes prohibitive ∼130 hours, which makes the results meaningless. Efficient Proxy . Therefore, we aim to find an efficient proxy for E h ∥δWs i,:¯xs∥2 2 i . First, we re-write E h ∥δWs i,:¯xs∥2 2 i as: E \u0002 ∥δWs i,:¯xs∥2 2 \u0003 ∆= (E \u0002 δWs i,:¯xs\u0003 )2 + Var \u0002 δWs i,:¯xs\u0003 . (21) Algorithm 2 Weight Quantization Error Reduction 1: Input: W ∈ RDout×Din, {¯xn ∈ RD in}N n=1, maximum iteration T 2: Initialize: ¯W = ∅ 3: for i = 1 to Dout do 4: ¯Wi,: = ∅, {¯xn}N n=1 = {¯xn}N n=1 5: while |Wi,:| > 0 do 6: Partition Wi,: into \u0002 Ws i,:, Wr i,: \u0003 7: Partition {¯xn}N i=1 into { \u0002 ¯xs n, ¯xr n \u0003 }N i=1 8: /* Rounding Refinement */ 9: Obtain ˆµs ˆµsT + ˆΣs from cache or calculate it with {¯xs n}N n=1, calculate δWs↓ i,:, δWs↑ i,: with Ws i,: 10: while 0 ≤ T-- do 11: Calculate proxy Lold with δWs i,: by Eq. 22 12: Calculate gradients GδWs i,: by Eq. 24 13: Obtain S by Eq. 25 14: Obtain adjusted δW ′ i,: by Eq. 23 15: Calculate proxy Lnow with δW ′ i,: by Eq. 22 16: if Lnow > Lold then 17: break 18: end if 19: δWs i,: = δW ′ i,: 20: end while 21: ¯Wi,: ← ¯Wi,: ∪ (Ws i,: + δWs i,:) 22: /* END Rounding Refinement */ 23: end while 24: /* Ridge Regression */ 25: Calculate δWr∗ i,: by Eq. 27 26: Wi,: ← Wr i,: + δWr∗ i,: 27: /* END Ridge Regression */ 28: {¯xn}N n=1 ← {¯xr n}N n=1 29: ¯W ← ¯W ∪ ¯Wi,: 30: end for 31: Output: Quantized weight ¯W Here, ∆ indicates utilizing E \u0002 Z2\u0003 = (E [Z])2 + Var[Z]. As proved by [54], according to the central limit theorem, the numerous multiplication and addition operations within neural networks make the activations generally follow a Gaussian distribution, which is also a basic assumption in many previous works in the quantization field [25], [55], [56], [57]. Meanwhile, Fig. 6 illustrates the channel distribution of the full-precision and quantized activations. Here, the 67th and 71th channels are influenced by outliers, resulting in a larger range compared to the other channels. However, it can be observed that both outlier and non-outlier channels exhibit quantized activations that approximate a Gaussian distribution [15]. Note that the quantization process inherently clips outliers, thereby their impact on the distribution is mitigated largely. Based on the above analysis, we consider channel distribution of ¯xs still can be captured by the Gaussian distribution, and model ¯xs with a Ds in-dimensional Gaussian distribution N(µs, Σs), where Ds in is the dimension of ¯xs, µs ∈ RDs in, Σs ∈ RDs in×Ds in. Then, the Eq. 21 becomes: E \u0002 δWs i,:¯xs\u00032 + Var \u0002 δWs i,:¯xs\u0003 = δWs i,:µsµsT (δWs i,:)T + δWi,:Σs(δWs i,:)T = δWs i,:(µsµsT + Σs)(δWs i,:)T . (22)JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 8 TABLE 2 Results on ImageNet dataset. The Top-1 accuracy (%) is reported as the metric. “W/A” indicates that the bit-width of the weights and activations are W and A bits, respectively. “*” indicates the results are re-produced by using the official code. Method W/A ViT-S ViT-B DeiT-T DeiT-S DeiT-B Swin-S Swin-B Full-Precision 32/32 81.39 84.54 72.21 79.85 81.80 83.23 85.27 FQ-ViT* [25] 3/4 0.10 0.10 0.10 0.10 0.10 0.10 0.10 PTQ4ViT* [30] 3/4 0.10 0.10 0.20 0.15 0.59 0.64 0.53 GPTQ* [44] 3/4 23.32 44.63 42.25 48.95 61.75 66.71 71.43 DuQuant* [58] 3/4 0.40 1.24 9.25 8.53 47.29 70.70 54.68 SpinQuant* [59] 3/4 22.38 0.50 29.94 49.56 18.06 35.83 14.67 OmniQuant* [60] 3/4 0.32 0.35 2.45 9.67 23.91 65.98 52.05 SmoothQuant* [61] 3/4 0.23 0.27 4.60 10.72 25.28 68.51 53.70 RepQ-ViT* [29] 3/4 15.65 26.98 29.34 45.82 58.92 59.83 44.17 AdaRound* [62] 3/4 11.04 4.72 36.05 33.56 62.50 68.12 53.92 BRECQ* [40] 3/4 4.97 1.25 29.23 18.58 40.49 66.93 53.38 QDrop* [41] 3/4 9.77 11.87 17.85 30.27 61.12 73.47 74.33 PD-Quant* [63] 3/4 4.56 21.81 41.87 41.65 53.63 70.07 56.48 OAS-ViT* [47] 3/4 3.56 7.43 32.34 36.68 51.51 72.37 73.50 ERQ (Ours) 3/4 60.13 72.37 52.73 69.09 76.47 78.12 79.98 FQ-ViT [25] 4/4 0.10 0.10 0.10 0.10 0.10 0.10 0.10 PTQ4ViT [30] 4/4 42.57 30.69 36.96 34.08 64.39 76.09 74.02 APQ-ViT [43] 4/4 47.95 41.41 47.94 43.55 67.48 77.15 76.48 GPTQ* [44] 4/4 67.59 75.12 58.96 70.85 76.10 80.17 81.08 DuQuant* [58] 4/4 1.73 9.53 22.68 26.57 64.87 78.59 78.56 SpinQuant* [59] 4/4 50.41 5.42 42.57 58.44 60.67 64.16 50.80 OmniQuant* [60] 4/4 8.80 5.50 10.67 16.85 38.74 77.48 78.14 SmoothQuant* [61] 4/4 13.19 8.22 21.78 32.76 49.31 79.33 79.26 RepQ-ViT [29] 4/4 65.05 68.48 57.43 69.03 75.61 79.45 78.32 AdaRound* [62] 4/4 63.09 70.51 55.65 69.24 75.20 76.05 78.12 BRECQ* [40] 4/4 11.31 3.03 38.41 32.89 59.10 68.40 56.51 QDrop* [41] 4/4 17.77 21.72 31.65 35.79 65.47 78.92 80.49 PD-Quant* [63] 4/4 32.64 34.86 58.50 64.85 60.06 77.04 75.84 ERQ (Ours) 4/4 71.61 78.65 61.79 74.35 79.18 81.19 83.32 Here, Eq. 22 is the obtained proxy for E h ∥δWs i,:¯xs∥2 2 i . In practice, we estimate the empirical ˆµs and ˆΣs with the given calibration dataset. Note that for all output channels, ˆµs and ˆΣs are shared and require only a single computation. Fig. 7 presents the relationship between the proxy and E h ∥δWs i,:¯xs∥2 2 i . It can be seen that the proposed proxy is proportional to the real value, demonstrating its fidelity. The computational complexity of using our proxy is O((Ds in)2), while the complexity of E h ∥δWs i,:¯xs∥2 2 i is O(NDs in), where N >> Ds in. Thus, the proxy can serve as a low-cost objective for solving δWs i,:. As shown in Tab. 1, using Eq. 22 as the target of MIPQ reduces the time costs from ∼130 hours to ∼10 hours. However, this still incurs moderate costs since current open-source implementations of MIPQ only support CPU and cannot fully exploit the capacity of GPU. In the next, we introduce Rounding Refinement, a GPU-supported method that uses the gradient of the proxy to adjust δWs i,: faster. Rounding Refinement. At first, we initialize δWs i,j with the rounding-to-nearest strategy. Now, δWs i,j is either equal to δWs↓ i,j or δWs↑ i,j. Then, we aim to determine an index set S that contains the index set of the elements necessitating modifications, whose rounding direction is overturned: δWs i,j = ( δWs↓ i,j if δWs i,j = δWs↑ i,j δWs↑ i,j otherwise. , j∈ S. (23) To determine S, we first take the derivative of the proxy (Eq. 22) w.r.t the δWs i,: GδWs i,: = ∂ ∂δWs i,: δWs i,:(µsµsT + Σs)(δWs i,:)T = 2δWs i,:(µsµsT + Σs). (24) We only select the elements whose gradients are the same sign, since this is the only way to allow overturn. For example, given δWs i,j = δW s↓ i,j, replacing it by δW s↑ i,j is feasible only if GδWs i,j has the same sign as δWs i,j. Thus, the index set S is defined as: S = top index(M, k), M = |GδWs i,: ⊙ 1(GδWs i,: ⊙ δWs i,:)| ∈RDs in. (25) Here, top index returns the index of the top k elements, 1(·) returns 1 for non-negative input and 0 for negative input, |·| returns the absolute value of the input. After obtaining S, the overturn is performed with Eq. 23. The above process iterates until the adjusted δWs i,: incurs a larger proxy value or reaches maximum iterations T. After obtaining δWs i,:, the quantization can be completed by ¯Ws i,: = Ws i,: + δWs i,:. ¯Ws i,: is then added into the set of quantized weights. The overall process of Rounding Refinement is presented in Lines 7 - Lines 18 of Alg. 2. As shown in Tab. 1, Rounding Refinement significantly reduces the time costs from 10 hours to 4 minutes by 150× at the cost of affordable accuracy loss.JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 9 TABLE 3 Results on ImageNet dataset. “-” denotes the original paper does not report corresponding results and the code is unavailable. Method W/A ViT-S ViT-B DeiT-T DeiT-S DeiT-B Swin-S Swin-B Full-Precision 32/32 81.39 84.54 72.21 79.85 81.80 83.23 85.27 FQ-ViT* [25] 5/5 0.10 0.10 0.10 0.10 0.10 0.10 0.10 PTQ4ViT* [30] 5/5 72.74 72.32 65.00 70.26 72.65 80.90 81.87 GPTQ* [44] 5/5 78.63 82.06 69.05 77.12 80.17 82.19 83.00 DuQuant* [58] 5/5 67.87 75.33 60.89 74.11 78.72 82.06 84.08 SpinQuant* [59] 5/5 71.30 72.19 60.76 70.69 77.27 82.17 82.50 OmniQuant* [60] 5/5 56.33 61.99 42.57 64.13 76.43 81.44 83.66 SmoothQuant* [61] 5/5 67.70 71.88 56.63 71.24 77.83 81.89 84.01 RepQ-ViT* [29] 5/5 78.43 82.03 69.00 77.04 80.08 82.08 83.22 AdaRound* [62] 5/5 77.53 82.00 68.87 76.22 80.18 82.12 84.09 BRECQ* [40] 5/5 47.35 43.51 62.12 63.15 75.61 80.66 82.31 QDrop* [41] 5/5 56.32 57.92 62.36 70.07 78.41 81.73 83.61 PD-Quant* [63] 5/5 65.06 58.40 68.02 74.94 74.61 81.27 82.12 OAS-ViT* [47] 5/5 40.40 47.13 62.34 65.96 77.36 80.91 83.60 ERQ (Ours) 5/5 79.13 82.99 69.76 77.98 80.92 82.49 84.72 FQ-ViT [25] 6/6 4.26 0.10 58.66 45.51 64.63 66.50 52.09 PSAQ-ViT [24] 6/6 37.19 41.52 57.58 63.61 67.95 72.86 76.44 Ranking [13] 6/6 - 75.26 - 74.58 77.02 - - PTQ4ViT [30] 6/6 78.63 81.65 69.68 76.28 80.25 82.38 84.01 APQ-ViT [43] 6/6 79.10 82.21 70.49 77.76 80.42 82.67 84.18 NoisyQuant† [26] 6/6 76.86 81.90 - 76.37 79.77 82.78 84.57 NoisyQuant‡ [26] 6/6 78.65 82.32 - 77.43 80.70 82.86 84.68 GPTQ* [44] 6/6 80.44 83.72 71.05 78.95 81.37 82.82 84.89 DuQuant* [58] 6/6 79.13 82.73 68.95 78.46 81.06 82.74 84.88 SpinQuant* [59] 6/6 78.96 82.74 69.69 76.80 80.65 82.00 84.47 OmniQuant* [60] 6/6 75.23 78.06 62.39 76.62 80.10 82.21 84.63 SmoothQuant* [61] 6/6 78.09 82.33 69.31 78.67 80.87 82.99 85.00 RepQ-ViT [29] 6/6 80.43 83.62 70.76 78.90 81.27 82.79 84.57 EasyQuant [64] 6/6 75.13 81.42 - 75.27 79.47 82.45 84.30 Bit-shrinking [65] 6/6 80.44 83.16 - 78.51 80.47 82.44 - BRECQ* [40] 6/6 61.18 71.29 69.62 70.93 79.46 81.85 84.08 QDrop* [41] 6/6 68.57 74.38 69.98 76.57 80.66 82.53 84.31 PD-Quant* [63] 6/6 71.38 63.14 70.74 77.63 79.32 82.33 84.38 OAS-ViT* [47] 6/6 61.32 71.87 69.67 71.52 79.87 82.16 84.26 ERQ (Ours) 6/6 80.68 84.00 71.24 79.14 81.54 82.94 85.06 4.2.3 Ridge Regression After Rounding Refinement, we suggest adjusting Wr i,: with δWr∗ i,: to further counteract E h ∥δWs i,:¯xs∥2 2 i , which yields the following target: E \u0002 ∥δWs i,:¯xs + δWr∗ i,: ¯xr∥2 2 \u0003 + λ2∥δWr∗ i,: ∥2 2, (26) where λ2 is a hyper-parameter to control intensity of the regularization term λ2∥δWr∗ i,: ∥2 2. The minimization of Eq. 26 formulates the Ridge Regression problem and the solution is defined as: δWr∗ i,: = −δWs i,:E h ¯xs¯xrT i (E h ¯xr ¯xrT i + λ2I)−1. (27) In practice, we estimate E \u0002 ¯xr ¯xsT \u0003 and E \u0002 ¯xr ¯xrT \u0003 by using 1 N PN n ¯xr n¯xsT n and 1 N PN n ¯xr n¯xrT n . Afterward, Wr i,: = Wr i,: +δWr∗ i,: to mitigate the error. Currently,Wr i,: remains as full-precision and will be processed in the next iteration. Such a process continuously runs until all weights are accurately quantized. The overall process of proposed Rounding Refine- ment and Ridge Regression is presented in Alg. 2. In practice, we perform Rounding Refinement and Ridge Regression for multiple output channels in parallel, thereby achieving a highly efficient implementation. 5 E XPERIMENTS 5.1 Implementation details 5.1.1 Models and tasks We conduct extensive experiments across various tasks, including image classification, object detection, instance segmentation, and image super-resolution. For the image classification task, we evaluate our ERQ on the ImageNet dataset [66] using different ViT variants including ViT-S, ViT-B [1], DeiT-T, DeiT-S, DeiT-B [3], as well as Swin-S and Swin-B [2]. The pre-training full-precision models are downloaded from Timm Library [67]. For object detection and instance segmentation tasks, ERQ is evaluated using Mask R-CNN [68] and Cascade Mask R- CNN [69], both of which adopt Swin-T or Swin-S [2] as their backbones. The pre-training full-precision models are down- loaded from the official repository of Swin Transformer [2]. These models are tested on the COCO dataset [70]. We report the box average precision for object detection and the mask average precision for instance segmentation. For the image super-resolution task, we evaluate ERQ on SwinIR [33]. The upscaling factors are ×2 and ×4, respec- tively. The pre-training full-precision models are downloadedJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10 TABLE 4 Results on COCO dataset. “APbox” denotes the box average precision for object detection, and “APmask” denotes the mask average precision for instance segmentation. “*” and “†” indicate the results are re-produced by using the official code. Method W/A Mask R-CNN Cascade Mask R-CNN w. Swin-T w. Swin-S w. Swin-T w. Swin-S APbox APmask APbox APmask APbox APmask APbox APmask Full-Precision 32/32 46.0 41.6 48.5 43.3 50.4 43.7 51.9 45.0 GPTQ* [44] 3/4 22.9 25.0 31.7 32.5 39.8 35.8 44.6 39.6 DuQuant* [58] 3/4 2.0 2.4 23.5 26.4 3.7 3.9 35.7 35.3 SpinQuant* [59] 3/4 1.1 1.4 16.9 19.8 4.3 4.4 33.2 33.6 OmniQuant* [60] 3/4 10.9 12.4 8.6 9.8 19.8 18.2 14.8 13.6 SmoothQuant* [61] 3/4 10.8 12.2 8.5 9.6 19.7 18.0 14.7 13.5 RepQ-ViT* [29] 3/4 22.2 24.1 27.9 29.9 40.2 35.7 43.7 38.8 AdaRound* [51] 3/4 2.8 4.2 5.5 7.5 20.3 22.6 21.3 24.0 BRECQ* [40] 3/4 14.7 18.2 15.3 19.5 29.9 28.0 32.3 29.3 QDrop* [41] 3/4 8.7 10.5 27.3 30.8 21.6 19.7 41.3 37.0 PD-Quant* [63] 3/4 3.7 4.6 3.8 4.5 1.0 9.2 6.4 6.1 OAS-ViT* [47] 3/4 8.5 8.2 9.6 11.9 16.7 12.0 21.6 19.6 ERQ (Ours) 3/4 27.2 28.9 30.6 33.0 45.1 40.0 47.3 42.0 PTQ4ViT [30] 4/4 6.9 7.0 26.7 26.6 14.7 13.5 0.5 0.5 APQ-ViT [43] 4/4 23.7 22.6 44.7 40.1 27.2 24.4 47.7 41.1 GPTQ* [44] 4/4 36.3 36.3 42.9 40.2 47.1 41.5 49.2 43.2 DuQuant* [58] 4/4 5.1 5.1 41.2 38.9 8.2 8.0 41.7 38.1 SpinQuant* [59] 4/4 5.1 5.4 36.6 34.5 9.3 14.7 39.5 39.7 OmniQuant* [60] 4/4 25.7 25.2 20.6 19.9 31.5 28.1 28.0 25.3 SmoothQuant* [61] 4/4 25.6 25.2 20.6 19.9 31.8 28.2 28.2 25.5 RepQ-ViT [29] 4/4 36.1 36.0 44.2 42.7† 40.240.1† 47.0 41.4 49.3 43.1 AdaRound* [51] 4/4 16.3 19.8 22.3 22.5 34.6 33.4 35.8 34.5 BRECQ* [40] 4/4 25.2 27.3 32.4 32.9 40.4 35.9 41.5 37.2 QDrop* [41] 4/4 10.4 11.3 39.7 37.8 17.9 16.2 20.1 17.4 PD-Quant* [63] 4/4 15.7 16.1 30.2 28.4 34.5 30.1 38.6 34.1 OAS-ViT* [47] 4/4 21.7 23.3 25.1 26.0 31.5 28.5 32.6 29.3 ERQ (Ours) 4/4 39.4 37.5 44.0 40.8 48.0 42.1 50.3 43.8 GPTQ* [44] 5/5 43.3 40.1 46.3 42.0 49.4 43.0 50.8 44.0 DuQuant* [58] 5/5 26.9 25.4 46.8 42.4 43.4 37.8 51.1 44.3 SpinQuant* [59] 5/5 30.6 29.0 46.6 42.2 42.0 36.6 50.8 44.2 OmniQuant* [60] 5/5 43.1 39.4 44.2 39.8 48.2 42.1 48.6 42.0 SmoothQuant* [61] 5/5 43.1 39.5 44.1 39.8 48.5 42.2 48.5 42.0 RepQ-ViT* [29] 5/5 43.3 40.2 46.3 42.4 49.6 43.2 51.1 44.3 AdaRound* [51] 5/5 34.5 35.6 41.0 39.8 46.8 42.1 48.4 43.3 BRECQ* [40] 5/5 41.9 38.8 44.9 40.9 47.8 41.7 49.6 43.0 QDrop* [41] 5/5 43.9 40.2 46.7 42.5 49.0 42.7 49.1 42.7 PD-Quant* [63] 5/5 12.7 12.1 20.4 19.1 19.7 17.4 22.2 19.5 OAS-ViT* [47] 5/5 40.6 37.5 40.7 37.4 46.5 40.6 46.3 40.3 ERQ (Ours) 5/5 44.2 40.8 47.1 42.8 49.8 43.3 51.4 44.6 from official repository of SwinIR [33]. The models are tested on four standard benchmarks including Set5 [71], Set14 [72], BSD100 [73] and Urban100 [74]. We report the PSNR and SSIM [75] over the Y channel as the metrics. 5.1.2 Hyperparameters Consistent with previous work [29], we quantize all weights and activations involved in matrix multiplication. For the image classification task, we randomly select 32 images from the ImageNet. For object detection and instance segmentation tasks, we randomly select 1 image from the COCO dataset. As for the image super-resolution task, we randomly sample 8 images from the training set of DIV2K [76], using a training patch size of 48, identical to the size used in pretraining. In our experiments, the quantization parameters are determined by forwarding the calibration datasets, after which these parameters are fixed. The values for k and the maximum iteration T of Rounding Refinement are set to 1 and 20, respectively, while |O| is empirically chosen to be 5% of Dout. For the image classification task, we set λ1 = λ2 = 1e4 for ViT, DeiT-S, DeiT-B, and Swin-S/B, and λ1 = λ2 = 1e3 for DeiT-T. For detection and segmentation tasks, we set λ1 = λ2 = 1e4 for Mask R-CNN and Cascade Mask R-CNN with Swin-T as the backbone, and λ1 = λ2 = 1e5 for Mask R-CNN with Swin-S as the backbone. For the image super- resolution task, we set λ1 = λ2 = 1e2 for SwinIR×2 and λ1 = λ2 = 1e3 for SwinIR×4. All experiments are implemented using the PyTorch framework [77], with a single NVIDIA 3090 GPU and an Intel Xeon 4214R CPU.JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 11 5.1.3 Compared methods For these methods that originally did not report the results on ViTs such as BRECQ [40], QDrop [41], PD- Quant [63], DuQuant [58], SpinQuant [59], OmniQuant [60], SmoothQuant [61], and GPTQ [44], we re-implement their results by using the official code with the same quantization settings and calibration dataset as the same ours. It is worth emphasizing that computational invariance is crucial for the rotation operations used in DuQuant and SpinQuant. However, due to the LN and the GELU function damaging the computational invariance, the rotation operations of DuQuant and SpinQuant cannot directly accommodate ViTs. Thus, we re-distributed their rotation operations within each ViT block. Note that both DuQuant and SpinQuan introduce extract computation costs since they utilize on- the-fly matrix multiplication before activation quantization. Moreover, the initial implementation of GPTQ did not involve activation quantization. Thus, we employed the same quantizer as our own to quantize the activations for them, including the reparameterization technique and the log √ 2 quantizer. For OAS-ViT [47] that employs the token- wise activations quantization, we re-implement it with the same quantization granularity as ours for a fair comparison. We do not compare our ERQ with AdaLog [78] and IGQ- ViT [46] as the former employs a log-based quantizer, which differs from the uniform quantizer used in this paper, and the latter adopts instance-wise group quantization, which has a different quantization granularity from our method. For other PTQ of ViT methods including FQ-ViT [25], PSAQ-ViT [24], Ranking [13], PTQ4ViT [30], APQ-ViT [43], NoisyQuant [26], RepQ-ViT [29], EasyQuant [64], and Bit-shrinking [65], we use the result reported in their paper if it exists, otherwise, we re-implement based on their official code, also with the same calibration datasets as the same ours. 5.2 Results on ImageNet Dataset The results on the ImageNet dataset are presented in Tab. 2 and Tab. 3. It can be seen that the proposed ERQ showcases advantages over the compared methods in all bit-width settings, especially in the low-bit cases. Specifically, due to the small amount of the calibration dataset, many methods typically suffer from the overfitting problem and exhibit unstable performance. For instance, for the W3A4 case, QDrop and PD-Quant obtain 9.77% and 4.56% on ViT-S, respectively. In contrast, the proposed ERQ shows stable improvements across all variants. Notably, ERQ demon- strates 36.81% and 27.74% performance gains on ViT-S and ViT-B, 10.48%, 19.53% and 13.97% gains on DeiT-T, DeiT-S, and DeiT-B, 4.65% and 5.65% gains on Swin-S and Swin-B. When it comes to the W4A4 case, ERQ respectively obtains 4.02% and 3.53% performance gains on ViT-S and ViT-B, 2.83%, 3.50% and 3.08% performance gains on DeiT-T, DeiT- S, and DeiT-B, 1.02% and 2.24% performance gains on Swin-S and Swin-B. In the W5A5 case, ERQ also presents the best performance. In particular, ERQ improves the performance by 0.50% and 0.93% on ViT-S and ViT-B, 0.71%, 0.86%, and 0.74% performance gains on DeiT-T, DeiT-S, and DeiT-B, 0.30% and 0.63% performance gains on Swin-S and Swin-B. For the W6A6 case, ERQ still outperforms other methods and provides a close performance with the full-precision model. For example, ERQ respectively presents only 0.26% and 0.21% accuracy loss compared with the full-precision model for Deit-B and Swin-B. 5.3 Results on COCO Dataset The results of object detection and instance segmentation are reported in Tab. 4. Specifically, in the W3A4 case, ERQ showcases notable performance improvements by improving the box AP and mask AP by 5.0 and 4.8 for Mask R-CNN with Swin-T, 2.7 and 2.2 for Mask R-CNN with Swin-S, 4.9 and 4.2 for Cascade Mask R-CNN with Swin-T, and 0.7 and 2.4 for Cascade Mask R-CNN with Swin-S. For the W4A4 case, ERQ augments the box AP and mask AP by 3.1 and 1.2 for Mask R-CNN with Swin-T, 0.9 and 0.6 for Cascade Mask R-CNN with Swin-T, and 1.0 and 0.6 for Cascade Mask R-CNN with Swin-S. For the W5A5 case, ERQ also provides the best performance. For instance, ERQ increases the box AP and mask AP by 0.9 and 0.6 for Mask R-CNN with Swin-T and 0.3 and 0.3 for Mask R-CNN with Swin-S. 5.4 Results on DIV2K The results of the image super-resolution task are reported in Tab. 5. It can be seen that ERQ consistently outperforms other methods in both the ×2 and ×4 upscaling factors. Specifically, for W4A4 SwinIR×2, ERQ improves PSNR by 0.15 dB, 0.52 dB, 0.56 dB, and 0.54 dB for Set5, Set14, BSD100, and Urban100, respectively. As for W4A4 SwinIR ×4, ERQ improves PSNR by 0.30 dB, 0.21 dB, 0.27 dB, and 0.27 dB for Set5, Set14, BSD100, and Urban100, respectively. The qualitative comparison is presented in Fig. 8. We selectively compare the proposed ERQ with PTQ4SR, which is designed for the image super-resolution task, as well as other methods that demonstrate strong quantitative performance. It can be seen that ERQ achieves the best visualization results. The above results further demonstrate the effectiveness and generalization ability of the proposed ERQ. 5.5 Ablation Study All ablation studies are conducted on the W4A4 DeiT-S. 5.5.1 Ablation Study of Proposed Components In Tab. 6, we report the effect of using Aqer and Wqer. It can be observed that, compared to the baseline, Aqer enhances accuracy by a significant 30.58% and Wqer also presents a notable 10.54% performance gain. Then, we analyze the effect of the components within Aqer by setting the baseline as only performing Wqer. As indicated in Tab. 7, Reparameterization Initialization and Ridge Regression respectively provide 22.03% and 8.67% accuracy increases, indicating their effec- tiveness. Combining these two components further presents the highest 23.43% increase. Applying all these components provides the best accuracy of 74.35%. In Tab. 8, we evaluate the effect of the components within Wqer by setting the baseline as only performing Aqer. Compared to the baseline, Dual Uniform Quantization, Rounding Refinement, and Ridge Regression achieve 2.07%, 0.84%, and 1.16% accuracy gains, respectively. The combination of any two of these three components leads to a larger increase. For example, using both Dual Uniform Quantization and Rounding RefinementJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 12 TABLE 5 PSNR/SSIM results of the compared methods and our ERQ in quantizing SwinIR [33] of scale ×2 and ×4. Model Method W/A Set5 [71] Set14 [72] BSD100 [73] Urban100 [74] SwinIR ×2 Full-Precision 32/32 38.35/0.9620 34.14/0.9227 32.44/0.9030 33.40/0.9393 BRECQ* [40] 4/4 30.91/0.8182 28.97/0.7756 27.50/0.7940 28.46/0.7549 QDrop* [41] 4/4 31.56/0.8781 29.30/0.8397 27.41/0.8528 28.94/0.8199 AdaRound* [51] 4/4 31.33/0.8281 29.29/0.7883 27.68/0.8022 28.63/0.7650 PTQ4SR* [79] 4/4 29.97/0.7746 28.08/0.7412 26.88/0.7583 27.79/0.7233 RepQ-ViT* [29] 4/4 29.48/0.7727 27.94/0.7389 26.48/0.7600 27.42/0.7186 GPTQ* [44] 4/4 30.70/0.8928 29.27/0.8593 27.46/0.8706 29.12/0.8424 DuQuant* [58] 4/4 31.97/0.8997 29.09/0.8221 25.35/0.7170 28.12/0.7688 SpinQuant* [59] 4/4 25.21/0.8468 25.90/0.8189 23.59/0.8144 26.45/0.8024 OmniQuant* [60] 4/4 25.83/0.8600 25.51/0.8173 22.94/0.8221 26.06/0.8032 SmoothQuant* [61] 4/4 25.82/0.8596 25.49/0.8166 22.96/0.8222 26.10/0.8052 PD-Quant* [63] 4/4 27.79/0.8469 27.64/0.8223 25.93/0.8293 27.83/0.8083 OAS-ViT* 4/4 31.52/0.8384 29.45/0.7970 27.68/0.8092 28.75/0.7739 ERQ (Ours) 4/4 32.12/0.9010 29.97/0.8615 28.23/0.8726 29.66/0.8479 SwinIR ×4 Full-Precision 32/32 32.72/0.9021 28.94/0.7914 27.83/0.7459 27.07/0.8164 BRECQ* [40] 4/4 27.44/0.7036 25.47/0.6234 23.40/0.6106 25.29/0.5955 QDrop* [41] 4/4 28.34/0.7630 26.13/0.6690 23.63/0.6226 25.80/0.6354 AdaRound* [51] 4/4 26.87/0.6794 25.14/0.6056 23.15/0.5925 25.06/0.5813 PTQ4SR* [79] 4/4 23.37/0.5190 21.99/0.4926 20.53/0.4826 21.50/0.4884 RepQ-ViT* [29] 4/4 27.59/0.7027 25.67/0.6263 23.61/0.6185 25.35/0.5939 GPTQ* [44] 4/4 26.66/0.7897 25.76/0.7024 23.76/0.6991 25.79/0.6687 DuQuant* [58] 4/4 26.27/0.7296 24.37/0.6045 21.24/0.5149 24.35/0.5677 SpinQuant* [59] 4/4 22.10/0.7263 23.00/0.6532 21.04/0.6343 23.50/0.6199 OmniQuant* [60] 4/4 21.57/0.7383 22.56/0.6669 20.43/0.6496 23.18/0.6379 SmoothQuant* [61] 4/4 21.56/0.7381 22.56/0.6662 20.43/0.6492 23.17/0.6375 PD-Quant* [63] 4/4 23.78/0.7326 24.38/0.6644 22.48/0.6502 24.63/0.6306 OAS-ViT* [47] 4/4 27.91/0.7356 25.73/0.6418 23.58/0.6369 25.44/0.6140 ERQ (Ours) 4/4 28.64/0.8107 26.34/0.7118 24.03/0.7040 26.07/0.6782 TABLE 6 Ablation studies of Aqer and Wqer. “Aqer” and “Wqer” represent Activation quantization error reduction and Weight quantization error reduction, respectively. “baseline” indicates only performing calibration. Aqer Wqer Top-1 Acc. (%) Baseline 40.38 ✓ 70.96 (+30.58) ✓ 50.92 (+10.54) ✓ ✓ 74.35 (+27.97) TABLE 7 Ablations on components of Aqer. “baseline” indicates only perform Wqer. “RepI” and “Ridge” indicate Reparameterization Initialization and Ridge Regression, respectively. Aqer Top-1 Acc. (%)RepI Ridge Baseline (Wqer) 50.92 ✓ 72.95 (+22.03) ✓ 59.59 (+8.67) ✓ ✓ 74.35 (+23.43) TABLE 8 Ablations on components of Wqer. “baseline” indicates only perform Aqer. “Dual”, “Rounding”, and “Ridge” represent Dual Uniform Quantization, Rounding Refinement, and Ridge Regression, respectively. Wqer Top-1 Acc. (%)Dual Rounding Ridge Baseline (Aqer) 70.96 ✓ 73.03 (+2.07) ✓ 71.80 (+0.84) ✓ 72.12 (+1.16) ✓ ✓ 73.70 (+2.74) ✓ ✓ 74.06 (+3.10) ✓ ✓ 72.76 (+1.80) ✓ ✓ ✓ 74.35 (+3.39) yields 2.74% increases. Finally, applying all these components provides the best accuracy of 74.35%. These results confirm the effectiveness of the components in ERQ. 5.5.2 Ablation Study of Hyperparemeters In Fig. 9, we provide the ablation study of λ1 of Eq. 8 and λ2 of Eq. 26. We set λ1 = λ2 and search for the best value for simplicity. Despite this may not be the best choice, it already yields desirable performance. As shown in Fig. 9, the best performance is achieved when λ1 = λ2 = 1 e4 for W4A4 Deit-S. In Tab. 9, we present the ablation study of different k in Eq. 25 of Rounding Refinement. When k = 1 , the best accuracy is achieved. Note that when k = 0, the Rounding Refinement is invalid. In Tab. 10, we present the ablation study of different |O| in Eq. 14 of Dual Uniform Quantization. It can be seen that only separately handling 1% of Dout outlier channel already yields a notable performance gain. When |O| is set to the 5% of Dout, the best accuracy is achieved. Note that when using 0% of Dout, the Dual Uniform Quantization is invalid. Tab. 11 presents the ablation study of maximum iterations T in Alg. 2. It can be seen that the best performance isJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 13 Fig. 8. Qualitative comparison on the image super-resolution task. TABLE 9 Ablation studies of k in Rounding Refinement. k Top-1 Acc. (%) 0 74.06 1 74.35 2 73.99 3 73.79 TABLE 10 Ablation studies of |O|. |O| (% of Dout) Top-1 Acc. (%) 0 72.76 1 74.29 2 74.31 5 74.35 10 73.28 20 73.26 TABLE 11 Ablation studies of maximum iterations T. T Top-1 Acc. (%) 0 74.06 5 74.10 10 74.20 20 74.35 50 74.35 TABLE 12 Ablation studies of percentiles in Outlier Channel Selection. Percentile τ Top-1 Acc. (%) { τ95, τ5 } 73.68 { τ98, τ2 } 73.95 { τ99, τ1 } 74.35 { τ99.5, τ0.05 } 74.20 { τ99.9, τ0.01 } 74.00 achieved if T= 20 . After 20, continue increasing T won’t yield gains. Note that when T= 0, the Rounding Refinement is invalid. In Tab. 12, we present the ablation study of upper per- centile and low percentile used in Alg. 1. Here, we use symmetric percentiles to avoid excessive hyperparameter searching. The results show that the model shows the best accuracy when using τ99, τ1. Fig. 9. Ablation studies of λ1 and λ2. 5.5.3 Ablation Study of Image Numbers Fig. 10 presents the ablation study using different image numbers. We further provide the time costs for different numbers of images. As the image number increases, the performance and time costs increases correspondingly. For Fig. 10. Ablation studies of image number. example, the accuracy is 73.21% and 74.01% for 4 and 8 images, respectively. Then, the performance reaches the plateau after 128 images. Despite more images improving the performance, we adopt 32 images to align with the previous study [29] for a fair comparison. 5.5.4 Ablation Study of the Order of Aqer and Wqer Tab. 13 presents the ablation study using different orders of Aqer and Wqer. It can be seen that exchanging the order of Aqer and Wqer leads to performance degradation since the quantized weights cannot well accommodate the δW∗ (Eq. 10).JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 14 TABLE 13 Ablations studies of the order of Aqer and Wqer. “Wqer− →Aqer” and “Aqer− →Wqer” respectively represent executing Wqer and Aqer first. Error Reduction Order Top-1 Acc. (%) Wqer− →Aqer 73.05 Aqer− →Wqer 74.35 TABLE 14 Time costs and the number of optimizable parameters (in brackets) of different methods. “*” indicates the results are re-produced by using the official code. Method Runtime Top-1 Acc. (%) BRECQ* [40] ∼48 minutes (22.04M) 32.89 QDrop* [41] ∼80 minutes (22.04M) 35.79 PD-Quant* [63] ∼110 minutes (22.04M) 64.85 OAS-ViT [47] ∼50 minute (22.04M) 37.15 GPTQ* [44] ∼3 minutes (22M) 70.85 RepQ-ViT [29] ∼1 minute (-) 69.03 ERQ (Ours) ∼4 minutes (22M) 74.35 TABLE 15 Ablation studies of the partition strategy in Wqer. ”Smallest,” ”Largest,” and ”Half” represent the strategies of quantizing FP weights with the smallest quantization error first, the largest quantization error first, and the first half of FP weights, respectively. Partition Strategy Runtime Top-1 Acc. (%) Smallest ∼20 minutes 74.19 Largest ∼20 minutes 73.90 Half ∼4 minutes 74.35 5.5.5 Ablation Study of the Partition Strategy In Wqer, the FP weights are divided into two segments: the first half and the second half, with the first half quantized first. This strategy ensures a fixed order for quantizing the weights across all channels, enabling efficient parallel processing. Here, we conduct an ablation study on two partition strategies: quantizing FP weights starting with the smallest quantization error first and starting with the largest quantization error first. The results in Tab. 15 indicate that these two strategies typically hurt performance. Furthermore, using these strategies alters the order of weight quantiza- tion for each output channel, rendering parallel processing infeasible and resulting in higher time costs compared to the current partition strategy. For example, time costs increase from 4 minutes with the current strategy to 20 minutes when using the smallest or largest error first. 5.6 Comparisons of Time Costs Tab. 14 compares the time costs and the number of opti- mizable parameters between the proposed ERQ and other PTQ methods. Regarding time costs, BRECQ, QDrop, and PD-Quant require longer time overhead. In contrast, GPTQ, RepQ-ViT, and the proposed ERQ demonstrated significantly reduced time costs. Notably, our ERQ achieved the best Top-1 Accuracy of 74.35% with a runtime of only 4 minutes. In terms of the number of optimizable parameters, ERQ maintains a comparable count to most other methods, except for RepQ-ViT, which performs only calibration and thus has no optimizable parameters. Importantly, whether compared with iterative update methods such as BRECQ and QDrop or calibration-based methods like RepQ-ViT, ERQ achieves significantly better performance with comparable or even reduced running time. Fig. 11. Illustration of the MSE (evaluated by Eq. 4) reduction ratio brought by ERQ. We plot the error reduction ratio for each layer. 5.7 Comparisons of Real Inference Runtime TABLE 16 Comparisons of real inference time. The number in the bracket is the speedup compared with full-precision. Input batch size: 200. Bit-width Model ViT-B ViT-L Full-Precision 524.2ms 1577.1ms 8-bit 362.6ms (1.38x) 814.1ms (1.94x) 4-bit 244.9ms (2.14x) 515.5ms (3.06x) DUQ 4-bit 256.8ms (2.04x) 517.9ms (3.06x) To demonstrate the efficiency of DUQ, we implemented CUDA kernels for quantization, matrix multiplication, and dequantization based on the Cutlass library 1. As shown in Tab. 16, we conducted experiments on ViT-B and ViT-L. Note that for LN and softmax, we still use high-precision computations. The benchmarks are evaluated on an NVIDIA 3090 GPU, with speedup measured by averaging the results of 10 runs. We observed that DUQ 4-bit quantization incurs negligible additional inference time compared to standard 4- bit quantization, for example, 517.9ms versus 515.5ms for ViT- L. Furthermore, 4-bit DUQ quantization achieves significant speedup over full-precision. For example, it achieves 3.06x speedup over the full-precision on ViT-L. 5.8 Validation of Error Reduction We plot the MSE reduction ratio brought by our ERQ in Fig. 11. The reduction ratio is computed using the value of Eq. 4 on W4A4 DeiT-S. It can be observed that the proposed ERQ effectively reduces the MSE by achieving a 20% - 90% reduction ratio. Notably, ERQ yields a significant 64.07% average MSE reduction, supporting the performance results as shown in Tab. 2. 1. https://github.com/NVIDIA/cutlassJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 15 For a comprehensive comparison, we also evaluate the er- ror reduction rate achieved by the joint optimization of Eq. 4 with iterated, gradient-based SGD and Adam optimizers. The optimization ran for 10,000 iterations with an initial learning rate of 1e-6 and a cosine learning rate schedule. The error reduction rates for SGD and Adam are 33.18% and 35.72%, respectively, both of which are lower than the proposed ERQ, which achieved an average reduction of 64.07% as shown in Fig.,11. Accordingly, using SGD and Adam, the top-1 accuracy on ImageNet reached 73.23% and 73.59%, respectively, while ERQ achieved 74.35%. Furthermore, SGD and Adam respectively took 35 and 39 minutes, while ERQ required only about 4 minutes. This demonstrates that ERQ not only outperforms in terms of accuracy but also significantly reduces computational time. In summary, these findings justify the performance improvements achieved by ERQ, which addresses activation and weight quantization errors sequentially. 6 C ONCLUSION In this paper, we present ERQ, a two-step PTQ method of ViTs, consisting of Activation quantization error reduction (Aqer) and Weight quantization error reduction (Wqer) to respectively mitigate the quantization error induced by acti- vation and weight quantization. Aqer first employs Reparam- eterization Initialization for high-variance post-LayerNorm activations to mitigate the initial activation quantization errors. Then, it formulates a Ridge Regression problem to further tackle quantization errors by updating full-precision weights with a closed-form solution. Wqer first incorporates Dual Uniform Quantization for outlier-extensive weights to mitigate the initial weight quantization errors. Then, it progressively tackles the error in a quantization-and- correction manner. At each iteration, the first half of weights are quantized and the resulting error is first mitigated by Rounding Refinement and then again by solving a Ridge Regression problem. The former mitigates the errors by leveraging an empirically derived efficient proxy of output error to refine the rounding directions of quantized weights. The latter further tackles the errors by again formulating a Ridge Regression to update the remaining full-precision weights with a closed-form solution. The effectiveness of ERQ is demonstrated by extensive experiments on various ViTs variants across diverse tasks. ACKNOWLEDGMENTS This work was supported by National Science and Technol- ogy Major Project (No. 2022ZD0118202), the National Science Fund for Distinguished Young Scholars (No.62025603), the National Natural Science Foundation of China (Np. 624B2119, No. U21B2037, No. U22B2051, No. 62176222, No. 62176223, No. 62176226, No. 62072386, No. 62072387, No. 62072389, No. 62002305 and No. 62272401), and the Natural Science Foundation of Fujian Province of China (No.2021J01002, No.2022J06001). REFERENCES [1] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words: Transformers for image recognition at scale,” in Proceedings of the International Conference on Learning Representations (ICLR). Open- Review.net, 2021. [2] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, “Swin transformer: Hierarchical vision transformer using shifted windows,” in Proceedings of the IEEE/CVF international conference on computer vision (ICCV), 2021, pp. 10 012–10 022. [3] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. J´egou, “Training data-efficient image transformers & distillation through attention,” in Proceedings of the International Conference on Machine Learning (ICML). PMLR, 2021, pp. 10 347–10 357. [4] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, “End-to-end object detection with transformers,” in Proceedings of the European Conference on Computer Vision (ECCV) . Springer, 2020, pp. 213–229. [5] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, “Deformable detr: Deformable transformers for end-to-end object detection,” arXiv preprint arXiv:2010.04159, 2020. [6] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y. Wang, Y. Fu, J. Feng, T. Xiang, P . H. Torr et al. , “Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR), 2021, pp. 6881–6890. [7] A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Lu ˇci´c, and C. Schmid, “Vivit: A video vision transformer,” in Proceedings of the IEEE/CVF international conference on computer vision (ICCV) , 2021, pp. 6836–6846. [8] Y. Tang, K. Han, Y. Wang, C. Xu, J. Guo, C. Xu, and D. Tao, “Patch slimming for efficient vision transformers,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 12 165–12 174. [9] Z. Hao, J. Guo, D. Jia, K. Han, Y. Tang, C. Zhang, H. Hu, and Y. Wang, “Learning efficient vision transformers via fine-grained manifold distillation,” in Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 2021. [10] Z. Hou and S.-Y. Kung, “Multi-dimensional vision transformer compression via dependency guided gaussian process search,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 3669–3678. [11] D. Zheng, W. Dong, H. Hu, X. Chen, and Y. Wang, “Less is more: Focus attention for efficient detr,” in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023, pp. 6674– 6683. [12] Z. Li and Q. Gu, “I-vit: Integer-only quantization for efficient vision transformer inference,” in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023, pp. 17 065–17 075. [13] Z. Liu, Y. Wang, K. Han, W. Zhang, S. Ma, and W. Gao, “Post- training quantization for vision transformer,” in Proceedings of the Advances in Neural Information Processing Systems (NeurIPS) , vol. 34, 2021, pp. 28 092–28 103. [14] M. Chen, W. Shao, P . Xu, M. Lin, K. Zhang, F. Chao, R. Ji, Y. Qiao, and P . Luo, “Diffrate: Differentiable compression rate for efficient vision transformers,” arXiv preprint arXiv:2305.17997, 2023. [15] R. Krishnamoorthi, “Quantizing deep convolutional networks for efficient inference: A whitepaper,” arXiv preprint arXiv:1806.08342, 2018. [16] S. K. Esser, J. L. McKinstry, D. Bablani, R. Appuswamy, and D. S. Modha, “Learned step size quantization,” in Proceedings of the International Conference on Learning Representations (ICLR), 2020. [17] R. Banner, Y. Nahshan, D. Soudry et al. , “Post training 4-bit quantization of convolutional networks for rapid-deployment,” in Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 2019, pp. 7950–7958. [18] Y. Li, S. Xu, B. Zhang, X. Cao, P . Gao, and G. Guo, “Q-vit: Accurate and fully quantized low-bit vision transformer,” inProceedings of the Advances in Neural Information Processing Systems (NeurIPS) , vol. 35, 2022, pp. 34 451–34 463. [19] Y. Li, X. Dong, and W. Wang, “Additive powers-of-two quantiza- tion: An efficient non-uniform discretization for neural networks,” in Proceedings of the International Conference on Learning Representa- tions (ICLR), 2020.JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 16 [20] R. Gong, X. Liu, S. Jiang, T. Li, P . Hu, J. Lin, F. Yu, and J. Yan, “Differentiable soft quantization: Bridging full-precision and low- bit neural networks,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 4852–4861. [21] S. Liu, Z. Liu, and K. Cheng, “Oscillation-free quantization for low-bit vision transformers,” in Proceedings of the International Conference on Machine Learning (ICML) , A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, Eds., vol. 202, 2023, pp. 21 813–21 824. [22] Z. S. Xijie Huang and K.-T. Cheng, “Variation-aware vision trans- former quantization,” arXiv preprint arXiv:2307.00331, 2023. [23] Z. Li, M. Chen, J. Xiao, and Q. Gu, “Psaq-vit v2: Toward accurate and general data-free quantization for vision transformers,” IEEE Transactions on Neural Networks and Learning Systems (TNNLS) , 2023. [24] Z. Li, L. Ma, M. Chen, J. Xiao, and Q. Gu, “Patch similarity aware data-free quantization for vision transformers,” in Proceedings of the European Conference on Computer Vision (ECCV). Springer, 2022, pp. 154–170. [25] Y. Lin, T. Zhang, P . Sun, Z. Li, and S. Zhou, “Fq-vit: Post-training quantization for fully quantized vision transformer,” in Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, (IJCAI), L. D. Raedt, Ed., 2022, pp. 1173–1179. [26] Y. Liu, H. Yang, Z. Dong, K. Keutzer, L. Du, and S. Zhang, “Noisyquant: Noisy bias-enhanced post-training activation quan- tization for vision transformers,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 20 321–20 330. [27] N. Frumkin, D. Gope, and D. Marculescu, “Jumping through local minima: Quantization in the loss landscape of vision transformers,” in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023, pp. 16 978–16 988. [28] H. Tang, Y. Sun, D. Wu, K. Liu, J. Zhu, and Z. Kang, “Easyquant: An efficient data-free quantization algorithm for llms,” arXiv preprint arXiv:2403.02775, 2024. [29] Z. Li, J. Xiao, L. Yang, and Q. Gu, “Repq-vit: Scale reparameter- ization for post-training quantization of vision transformers,” in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023, pp. 17 227–17 236. [30] Z. Yuan, C. Xue, Y. Chen, Q. Wu, and G. Sun, “Ptq4vit: Post- training quantization for vision transformers with twin uniform quantization,” in Proceedings of the European Conference on Computer Vision (ECCV). Springer, 2022, pp. 191–207. [31] Y. Zhong, J. Hu, Y. Huang, Y. Zhang, and R. Ji, “Erq: Error reduction for post-training quantization of vision transformers,” in Proceedings of the International Conference on Machine Learning (ICML), 2024. [32] H. Chen, Y. Wang, T. Guo, C. Xu, Y. Deng, Z. Liu, S. Ma, C. Xu, C. Xu, and W. Gao, “Pre-trained image processing transformer,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021, pp. 12 299–12 310. [33] J. Liang, J. Cao, G. Sun, K. Zhang, L. Van Gool, and R. Timofte, “Swinir: Image restoration using swin transformer,” in Proceedings of the IEEE/CVF international conference on computer vision , 2021, pp. 1833–1844. [34] D. Neimark, O. Bar, M. Zohar, and D. Asselmann, “Video trans- former network,” in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021, pp. 3163–3172. [35] F. Shamshad, S. Khan, S. W. Zamir, M. H. Khan, M. Hayat, F. S. Khan, and H. Fu, “Transformers in medical imaging: A survey,” Medical Image Analysis, p. 102802, 2023. [36] J. Zhang, H. Peng, K. Wu, M. Liu, B. Xiao, J. Fu, and L. Yuan, “Minivit: Compressing vision transformers with weight multiplex- ing,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 12 145–12 154. [37] S. Mehta and M. Rastegari, “Mobilevit: Light-weight, general- purpose, and mobile-friendly vision transformer,” in Proceedings of the International Conference on Learning Representations (ICLR) , 2022. [38] K. Wu, J. Zhang, H. Peng, M. Liu, B. Xiao, J. Fu, and L. Yuan, “Tinyvit: Fast pretraining distillation for small vision transformers,” in Proceedings of the European Conference on Computer Vision (ECCV) , 2022, pp. 68–85. [39] M. Chen, M. Lin, Z. Lin, Y. Zhang, F. Chao, and R. Ji, “Smmix: Self- motivated image mixing for vision transformers,” in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , 2023, pp. 17 260–17 270. [40] Y. Li, R. Gong, X. Tan, Y. Yang, P . Hu, Q. Zhang, F. Yu, W. Wang, and S. Gu, “Brecq: Pushing the limit of post-training quantization by block reconstruction,” in Proceedings of the International Conference on Learning Representations (ICLR), 2021. [41] X. Wei, R. Gong, Y. Li, X. Liu, and F. Yu, “Qdrop: Randomly dropping quantization for extremely low-bit post-training quan- tization,” in Proceedings of the International Conference on Learning Representations (ICLR), 2022. [42] A. Ramachandran, S. Kundu, and T. Krishna, “Clamp-vit: Con- trastive data-free learning for adaptive post-training quantization of vits,” in Proceedings of the European Conference on Computer Vision (ECCV), 2024. [43] Y. Ding, H. Qin, Q. Yan, Z. Chai, J. Liu, X. Wei, and X. Liu, “Towards accurate post-training quantization for vision transformer,” in Proceedings of the 30th ACM International Conference on Multimedia (ACMMM), 2022, pp. 5380–5388. [44] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh, “GPTQ: Accurate post-training compression for generative pretrained transformers,” arXiv preprint arXiv:2210.17323, 2022. [45] E. Frantar and D. Alistarh, “Optimal brain compression: A frame- work for accurate post-training quantization and pruning,” in Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), vol. 35, 2022, pp. 4475–4488. [46] J. Moon, D. Kim, J. Cheon, and B. Ham, “Instance-aware group quantization for vision transformers,” inProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024, pp. 16 132–16 141. [47] Y. Ma, H. Li, X. Zheng, F. Ling, X. Xiao, R. Wang, S. Wen, F. Chao, and R. Ji, “Outlier-aware slicing for post-training quantization in vision transformer,” in Proceedings of the International Conference on Machine Learning (ICML), 2024. [48] J. Fang, A. Shafiee, H. Abdel-Aziz, D. Thorsley, G. Georgiadis, and J. H. Hassoun, “Post-training piecewise linear quantization for deep neural networks,” in Proceedings of the European Conference on Computer Vision (ECCV), 2020, pp. 69–86. [49] Z. Yuan, L. Niu, J. Liu, W. Liu, X. Wang, Y. Shang, G. Sun, Q. Wu, J. Wu, and B. Wu, “Rptq: Reorder-based post-training quantization for large language models,” 2023. [50] A. Zhou, A. Yao, Y. Guo, L. Xu, and Y. Chen, “Incremental network quantization: Towards lossless cnns with low-precision weights,” in The Eleventh International Conference on Learning Representations (ICLR), 2017. [51] M. Nagel, R. A. Amjad, M. Van Baalen, C. Louizos, and T. Blankevoort, “Up or down? adaptive rounding for post-training quantization,” in Proceedings of the International Conference on Machine Learning (ICML), 2020, pp. 7197–7206. [52] A. D. Pia, S. S. Dey, and M. Molinaro, “Mixed-integer quadratic programming is in np,” Mathematical Programming, vol. 162, pp. 225–240, 2017. [53] A. Kuzmin, M. Nagel, M. van Baalen, A. Behboodi, and T. Blankevoort, “Pruning vs quantization: Which is better?” 2023. [54] G. Klambauer, T. Unterthiner, A. Mayr, and S. Hochreiter, “Self- normalizing neural networks,” in Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), vol. 30, 2017. [55] R. Ding, T.-W. Chin, Z. Liu, and D. Marculescu, “Regularizing activation distribution for training binarized deep networks,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 11 408–11 417. [56] Z. Sun, C. Ge, J. Wang, M. Lin, H. Chen, H. Li, and X. Sun, “Entropy- driven mixed-precision quantization for deep network design,” in Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), vol. 35, 2022, pp. 21 508–21 520. [57] B. Chmiel, R. Banner, G. Shomron, Y. Nahshan, A. Bronstein, U. Weiser et al., “Robust quantization: One model to rule them all,” in Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), vol. 33, 2020, pp. 5308–5317. [58] H. Lin, H. Xu, Y. Wu, J. Cui, Y. Zhang, L. Mou, L. Song, Z. Sun, and Y. Wei, “Duquant: Distributing outliers via dual transformation makes stronger quantized llms,” in Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 2024. [59] Z. Liu, C. Zhao, I. Fedorov, B. Soran, D. Choudhary, R. Krishnamoor- thi, V . Chandra, Y. Tian, and T. Blankevoort, “Spinquant–llm quan- tization with learned rotations,” arXiv preprint arXiv:2405.16406 , 2024. [60] W. Shao, M. Chen, Z. Zhang, P . Xu, L. Zhao, Z. Li, K. Zhang, P . Gao, Y. Qiao, and P . Luo, “Omniquant: Omnidirectionally calibrated quantization for large language models,” in The Eleventh International Conference on Learning Representations (ICLR), 2024.JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 17 [61] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han, “Smoothquant: Accurate and efficient post-training quantization for large language models,” in Proceedings of the International Conference on Machine Learning (ICML), 2023, pp. 38 087–38 099. [62] M. Nagel, R. A. Amjad, M. Van Baalen, C. Louizos, and T. Blankevoort, “Up or down? adaptive rounding for post-training quantization,” in Proceedings of the International Conference on Machine Learning (ICML). PMLR, 2020, pp. 7197–7206. [63] J. Liu, L. Niu, Z. Yuan, D. Yang, X. Wang, and W. Liu, “Pd-quant: Post-training quantization based on prediction difference metric,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 24 427–24 437. [64] D. Wu, Q. Tang, Y. Zhao, M. Zhang, Y. Fu, and D. Zhang, “Easyquant: Post-training quantization via scale optimization,” CoRR, vol. abs/2006.16669, 2020. [65] C. Lin, B. Peng, Z. Li, W. Tan, Y. Ren, J. Xiao, and S. Pu, “Bit- shrinking: Limiting instantaneous sharpness for improving post- training quantization,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2023, pp. 16 196– 16 205. [66] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., “Imagenet large scale visual recognition challenge,” International Journal of Computer Vision (IJCV), vol. 115, pp. 211–252, 2015. [67] R. Wightman, “Pytorch image models,” https://github.com/ rwightman/pytorch-image-models, 2019. [68] K. He, G. Gkioxari, P . Doll ´ar, and R. Girshick, “Mask r-cnn,” in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2017, pp. 2961–2969. [69] Z. Cai and N. Vasconcelos, “Cascade r-cnn: Delving into high quality object detection,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 6154– 6162. [70] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P . Perona, D. Ramanan, P . Doll´ar, and C. L. Zitnick, “Microsoft coco: Common objects in context,” in Proceedings of the European Conference on Computer Vision (ECCV). Springer, 2014, pp. 740–755. [71] M. Bevilacqua, A. Roumy, C. Guillemot, and M.-L. A. Morel, “Low- complexity single-image super-resolution based on nonnegative neighbor embedding,” in British Machine Vision Conference (BMVC), 2012. [72] C. Ledig, L. Theis, F. Husz ´ar, J. Caballero, A. Cunningham, A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang et al. , “Photo- realistic single image super-resolution using a generative adversar- ial network,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 4681–4690. [73] D. Martin, C. Fowlkes, D. Tal, and J. Malik, “A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics,” in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2001, pp. 416–423. [74] J.-B. Huang, A. Singh, and N. Ahuja, “Single image super-resolution from transformed self-exemplars,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 5197–5206. [75] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P . Simoncelli, “Image quality assessment: from error visibility to structural similarity,” IEEE Transactions on Image Processing (TIP), vol. 13, no. 4, pp. 600– 612, 2004. [76] R. Timofte, E. Agustsson, L. Van Gool, M.-H. Yang, and L. Zhang, “Ntire 2017 challenge on single image super-resolution: Methods and results,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2017, pp. 114– 125. [77] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., “Pytorch: An imperative style, high-performance deep learning library,” in Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 2019, pp. 8026–8037. [78] Z. Wu, J. Chen, H. Zhong, D. Huang, and Y. Wang, “Adalog: Post-training quantization for vision transformers with adaptive logarithm quantizer,” in Proceedings of the European Conference on Computer Vision (ECCV), 2024. [79] Z. Tu, J. Hu, H. Chen, and Y. Wang, “Toward accurate post-training quantization for image super resolution,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 5856–5865.",
      "meta_data": {
        "arxiv_id": "2407.06794v2",
        "authors": [
          "Yunshan Zhong",
          "You Huang",
          "Jiawei Hu",
          "Yuxin Zhang",
          "Rongrong Ji"
        ],
        "published_date": "2024-07-09T12:06:03Z",
        "pdf_url": "https://arxiv.org/pdf/2407.06794v2.pdf",
        "github_url": "https://github.com/zysxmu/ERQ"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper introduces ERQ, an innovative two-step Post-Training Quantization (PTQ) method designed to sequentially reduce quantization errors arising from activation and weight quantization in Vision Transformers (ViTs). ERQ addresses the limitations of current PTQ methods that often overlook complex interactions between quantized weights and activations, leading to significant performance degradation. Key contributions include the Reparameterization Initialization and Ridge Regression for activation error reduction, and Dual Uniform Quantization, Rounding Refinement, and another Ridge Regression for weight error reduction. Comprehensive experiments demonstrate ERQ's superior performance across various ViT variants and tasks, significantly surpassing state-of-the-art methods; for example, achieving a 36.81% accuracy gain for W3A4 ViT-S compared to GPTQ.",
        "methodology": "ERQ is a two-step PTQ method comprising Activation quantization error reduction (Aqer) and Weight quantization error reduction (Wqer). Aqer focuses on mitigating activation quantization errors while keeping weights full-precision. It employs Reparameterization Initialization for high-variance post-LayerNorm activations, translating channel-wise quantizers to layer-wise equivalence by adjusting LayerNorm parameters and subsequent layer weights/biases. This is followed by a Ridge Regression problem, offering a closed-form solution to update full-precision weights and further reduce errors. Wqer then quantizes weights and mitigates their induced errors. It introduces Dual Uniform Quantization to handle weights with numerous outliers (resulting from Aqer's adjustments), separating channels with and without outliers for individual quantization parameters, guided by an outlier channel selection algorithm. Subsequently, an iterative quantization-and-correction approach is used, where the first half of unquantized weights are processed. This involves Rounding Refinement, which uses an empirically derived, efficient proxy (assuming Gaussian activation distribution) to refine rounding directions, and another Ridge Regression solver to update the remaining full-precision weights, iteratively reducing errors until all weights are accurately quantized.",
        "experimental_setup": "ERQ was evaluated on various ViT variants and tasks. For image classification, models included ViT-S/B, DeiT-T/S/B, and Swin-S/B, tested on the ImageNet dataset with Top-1 accuracy as the metric. For object detection and instance segmentation, Mask R-CNN and Cascade Mask R-CNN with Swin-T/S backbones were used on the COCO dataset, reporting APbox and APmask. For image super-resolution, SwinIR (×2 and ×4 upscaling factors) was evaluated on Set5, Set14, BSD100, and Urban100 datasets, using PSNR and SSIM. Calibration datasets included 32 images from ImageNet, 1 image from COCO, and 8 images from DIV2K (for super-resolution). Quantization applied to all weights and activations in matrix multiplications, using channel-wise for weights and layer-wise (uniform or log√2 for post-Softmax) for activations, with bit-widths ranging from W3A4 to W6A6. Hyperparameters (λ1, λ2, k, T, |O|, percentile thresholds) were empirically tuned. Experiments were conducted on a single NVIDIA 3090 GPU and an Intel Xeon 4214R CPU. Comparisons were made against numerous state-of-the-art PTQ methods, some re-implemented for fair comparison.",
        "limitations": "The paper does not explicitly list limitations of the proposed ERQ method. However, implicit limitations can be inferred: the Rounding Refinement component relies on an efficient proxy derived from the assumption that activations approximate a Gaussian distribution, which may not hold universally. The method's performance is sensitive to the careful tuning of several hyperparameters (e.g., regularization strengths λ1, λ2, outlier percentage |O|, and maximum iterations T), indicating a potential challenge for broad applicability without extensive optimization. Furthermore, the effectiveness of the two main steps (Aqer and Wqer) is sensitive to their sequential order. Finally, direct minimization of certain quantization errors is computationally prohibitive, necessitating the use of empirically derived proxies and heuristics, which might not always achieve global optimality.",
        "future_research_directions": "Not mentioned",
        "experimental_code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom copy import deepcopy\nimport numpy as np\nfrom torch.nn.parameter import Parameter\nfrom collections import Counter\nimport math\nfrom types import MethodType\n\ndef lp_loss(pred, tgt, p=2.0, reduction='none'):\n    \"\"\"\n    loss function measured in L_p Norm\n    \"\"\"\n    if reduction == 'none':\n        return (pred-tgt).abs().pow(p).sum(1).mean()\n    else:\n        return (pred-tgt).abs().pow(p).mean()\n\n\nclass UniformQuantizer(nn.Module):\n    \"\"\"\n    PyTorch Function that can be used for asymmetric quantization (also called uniform affine\n    quantization). Quantizes its argument in the forward pass, passes the gradient 'straight\n    through' on the backward pass, ignoring the quantization that occurred.\n    Based on https://arxiv.org/abs/1806.08342.\n    :param n_bits: number of bit for quantization\n    :param channel_wise: if True, compute scale and zero_point in each channel\n    \"\"\"\n    def __init__(self, n_bits: int = 8, channel_wise: bool = False, is_act:bool = False,\n                 two_part:bool = False, split_quantization: bool = False):\n        super(UniformQuantizer, self).__init__()\n        assert 2 <= n_bits <= 8, 'bitwidth not supported'\n        self.n_bits = n_bits\n        self.n_levels = 2 ** self.n_bits\n        self.delta = None\n        self.zero_point = None\n        self.register_buffer('inited', torch.zeros(1))\n        self.channel_wise = channel_wise\n        self.is_act = is_act\n        self.store_input = None\n        self.two_part = two_part\n        self.split_quantization = split_quantization\n        self.negative_mask = None\n\n    def __repr__(self):\n        s = super(UniformQuantizer, self).__repr__()\n        s = \"(\" + s + \" inited={}, channel_wise={})\".format(self.inited, self.channel_wise)\n        return s\n\n    def forward(self, x: torch.Tensor, round_way='round', index=None):\n\n        if self.inited == 0:\n            if not self.two_part:\n                delta, zero_point = self.init_quantization_scale(x, self.channel_wise)\n                self.delta, self.zero_point = Parameter(delta).contiguous(), Parameter(zero_point).contiguous()\n            else:\n\n                indices_list = []\n                for ii in range(x.shape[0]):\n\n                    per = 99\n                    vector = x[ii].flatten().detach().cpu().numpy()\n                    vector = vector[vector > 0]\n                    threshold_pos = np.percentile(vector, per)\n\n                    vector = x[ii].flatten().detach().cpu().numpy()\n                    vector = vector[vector < 0]\n                    threshold_neg = np.percentile(vector, 100-per)\n\n                    indices_gt = torch.nonzero(x[ii] > threshold_pos)\n                    indices_lt = torch.nonzero(x[ii] < threshold_neg)\n                    indices = torch.unique(torch.cat((indices_gt, indices_lt), dim=0))\n                    indices_list.append(indices)\n\n                count_dict = Counter()\n                for item in indices_list:\n                    count_dict.update(item.tolist())\n\n                best_top_number = x.shape[0] // 20\n                print('best_top_number', best_top_number)\n                # _, top_values_tensor = torch.topk(outliers_count, best_top_number)\n                top_ = count_dict.most_common(best_top_number)\n                top_values = [value for value, count in top_]\n                top_values_tensor = torch.tensor(top_values).cuda()\n                self.unique_top_indices = top_values_tensor\n                tmp = []\n                for i in range(x.shape[1]):\n                    if i not in self.unique_top_indices:\n                        tmp.append(i)\n                self.unique_nottop_indices = torch.tensor(tmp).cuda()\n                x_1 = x[:, self.unique_top_indices]\n                x_2 = x[:, self.unique_nottop_indices]\n\n                delta, zero_point = self.init_quantization_scale(x_2, self.channel_wise)\n                self.delta_2, self.zero_point_2 = Parameter(delta).contiguous(), Parameter(zero_point).contiguous()\n\n                delta, zero_point = self.init_quantization_scale(x_1, self.channel_wise)\n                self.delta, self.zero_point = Parameter(delta).contiguous(), Parameter(zero_point).contiguous()\n\n\n            self.inited.fill_(1)\n\n        if not self.two_part:\n            # start quantization\n            if index is None:\n                if round_way == 'round':\n                    x_int = torch.round(x / self.delta) + self.zero_point\n                    x_quant = torch.clamp(x_int, 0, self.n_levels - 1)\n                    x_dequant = (x_quant - self.zero_point) * self.delta\n                elif round_way == 'floor':\n                    x_int = torch.floor(x / self.delta) + self.zero_point\n                    x_quant = torch.clamp(x_int, 0, self.n_levels - 1)\n                    x_dequant = (x_quant - self.zero_point) * self.delta\n                elif round_way == 'ceil':\n                    x_int = torch.ceil(x / self.delta) + self.zero_point\n                    x_quant = torch.clamp(x_int, 0, self.n_levels - 1)\n                    x_dequant = (x_quant - self.zero_point) * self.delta\n                else:\n                    raise Exception\n            else:\n                if round_way == 'round':\n                    x_int = torch.round(x / self.delta[index]) + self.zero_point[index]\n                    x_quant = torch.clamp(x_int, 0, self.n_levels - 1)\n                    x_dequant = (x_quant - self.zero_point[index]) * self.delta[index]\n                elif round_way == 'floor':\n                    x_int = torch.floor(x / self.delta[index]) + self.zero_point[index]\n                    x_quant = torch.clamp(x_int, 0, self.n_levels - 1)\n                    x_dequant = (x_quant - self.zero_point[index]) * self.delta[index]\n                elif round_way == 'ceil':\n                    x_int = torch.ceil(x / self.delta[index]) + self.zero_point[index]\n                    x_quant = torch.clamp(x_int, 0, self.n_levels - 1)\n                    x_dequant = (x_quant - self.zero_point[index]) * self.delta[index]\n                else:\n                    raise Exception\n        else:\n            if index is None:\n                # start quantization\n                x_1 = x[:, self.unique_top_indices]\n                x_2 = x[:, self.unique_nottop_indices]\n\n                if self.split_quantization:\n                    self.negative_mask = torch.sign(x_1)\n\n                if round_way == 'round':\n                    if not self.split_quantization:\n\n                        x_int_1 = torch.round(x_1 / self.delta) + self.zero_point\n                        x_quant_1 = torch.clamp(x_int_1, 0, self.n_levels - 1)\n                        x_dequant_1 = (x_quant_1 - self.zero_point) * self.delta\n                    else:\n\n                        x_1 = self.negative_mask * x_1\n                        x_int_1 = torch.round(x_1 / self.delta) + self.zero_point\n                        x_quant_1 = torch.clamp(x_int_1, 0, self.n_levels - 1)\n                        x_dequant_1 = (x_quant_1 - self.zero_point) * self.delta\n                        x_dequant_1 = self.negative_mask * x_dequant_1\n\n                    x_int_2 = torch.round(x_2 / self.delta_2) + self.zero_point_2\n                    x_quant_2 = torch.clamp(x_int_2, 0, self.n_levels - 1)\n                    x_dequant_2 = (x_quant_2 - self.zero_point_2) * self.delta_2\n                elif round_way == 'floor':\n                    if not self.split_quantization:\n                        x_int_1 = torch.floor(x_1 / self.delta) + self.zero_point\n                        x_quant_1 = torch.clamp(x_int_1, 0, self.n_levels - 1)\n                        x_dequant_1 = (x_quant_1 - self.zero_point) * self.delta\n                    else:\n                        x_1 = self.negative_mask * x_1\n                        x_int_1 = torch.floor(x_1 / self.delta) + self.zero_point\n                        x_quant_1 = torch.clamp(x_int_1, 0, self.n_levels - 1)\n                        x_dequant_1 = (x_quant_1 - self.zero_point) * self.delta\n                        x_dequant_1 = self.negative_mask * x_dequant_1\n\n                    x_int_2 = torch.floor(x_2 / self.delta_2) + self.zero_point_2\n                    x_quant_2 = torch.clamp(x_int_2, 0, self.n_levels - 1)\n                    x_dequant_2 = (x_quant_2 - self.zero_point_2) * self.delta_2\n                elif round_way == 'ceil':\n                    if not self.split_quantization:\n                        x_int_1 = torch.ceil(x_1 / self.delta) + self.zero_point\n                        x_quant_1 = torch.clamp(x_int_1, 0, self.n_levels - 1)\n                        x_dequant_1 = (x_quant_1 - self.zero_point) * self.delta\n                    else:\n                        x_1 = self.negative_mask * x_1\n                        x_int_1 = torch.ceil(x_1 / self.delta) + self.zero_point\n                        x_quant_1 = torch.clamp(x_int_1, 0, self.n_levels - 1)\n                        x_dequant_1 = (x_quant_1 - self.zero_point) * self.delta\n                        x_dequant_1 = self.negative_mask * x_dequant_1\n\n\n                    x_int_2 = torch.ceil(x_2 / self.delta_2) + self.zero_point_2\n                    x_quant_2 = torch.clamp(x_int_2, 0, self.n_levels - 1)\n                    x_dequant_2 = (x_quant_2 - self.zero_point_2) * self.delta_2\n                else:\n                    raise Exception\n\n                x_dequant = torch.zeros(x.shape).cuda()\n                x_dequant[:, self.unique_top_indices] = x_dequant_1\n                x_dequant[:, self.unique_nottop_indices] = x_dequant_2\n            else:\n                # start quantization\n                x_1 = x[self.unique_top_indices]\n                x_2 = x[self.unique_nottop_indices]\n\n                if round_way == 'round':\n                    if not self.split_quantization:\n                        x_int_1 = torch.round(x_1 / self.delta[index]) + self.zero_point[index]\n                        x_quant_1 = torch.clamp(x_int_1, 0, self.n_levels - 1)\n                        x_dequant_1 = (x_quant_1 - self.zero_point[index]) * self.delta[index]\n                    else:\n\n                        x_1 = self.negative_mask[index] * x_1\n                        x_int_1 = torch.round(x_1 / self.delta[index]) + self.zero_point[index]\n                        x_quant_1 = torch.clamp(x_int_1, 0, (self.n_levels//2) - 1)\n                        x_dequant_1 = (x_quant_1 - self.zero_point[index]) * self.delta[index]\n                        x_dequant_1 = self.negative_mask[index] * x_dequant_1\n\n                    x_int_2 = torch.round(x_2 / self.delta_2[index]) + self.zero_point_2[index]\n                    x_quant_2 = torch.clamp(x_int_2, 0, self.n_levels - 1)\n                    x_dequant_2 = (x_quant_2 - self.zero_point_2[index]) * self.delta_2[index]\n                elif round_way == 'floor':\n                    if not self.split_quantization:\n                        x_int_1 = torch.floor(x_1 / self.delta[index]) + self.zero_point[index]\n                        x_quant_1 = torch.clamp(x_int_1, 0, self.n_levels - 1)\n                        x_dequant_1 = (x_quant_1 - self.zero_point[index]) * self.delta[index]\n                    else:\n                        x_1 = self.negative_mask[index] * x_1\n                        x_int_1 = torch.floor(x_1 / self.delta[index]) + self.zero_point[index]\n                        x_quant_1 = torch.clamp(x_int_1, 0, (self.n_levels//2) - 1)\n                        x_dequant_1 = (x_quant_1 - self.zero_point[index]) * self.delta[index]\n                        x_dequant_1 = self.negative_mask[index] * x_dequant_1\n\n                    x_int_2 = torch.floor(x_2 / self.delta_2[index]) + self.zero_point_2[index]\n                    x_quant_2 = torch.clamp(x_int_2, 0, self.n_levels - 1)\n                    x_dequant_2 = (x_quant_2 - self.zero_point_2[index]) * self.delta_2[index]\n                elif round_way == 'ceil':\n                    if not self.split_quantization:\n                        x_int_1 = torch.ceil(x_1 / self.delta[index]) + self.zero_point[index]\n                        x_quant_1 = torch.clamp(x_int_1, 0, self.n_levels - 1)\n                        x_dequant_1 = (x_quant_1 - self.zero_point[index]) * self.delta[index]\n                    else:\n                        x_1 = self.negative_mask[index] * x_1\n                        x_int_1 = torch.ceil(x_1 / self.delta[index]) + self.zero_point[index]\n                        x_quant_1 = torch.clamp(x_int_1, 0, (self.n_levels//2) - 1)\n                        x_dequant_1 = (x_quant_1 - self.zero_point[index]) * self.delta[index]\n                        x_dequant_1 = self.negative_mask[index] * x_dequant_1\n\n                    x_int_2 = torch.ceil(x_2 / self.delta_2[index]) + self.zero_point_2[index]\n                    x_quant_2 = torch.clamp(x_int_2, 0, self.n_levels - 1)\n                    x_dequant_2 = (x_quant_2 - self.zero_point_2[index]) * self.delta_2[index]\n                else:\n                    raise Exception\n                x_dequant = torch.zeros(x.shape).cuda()\n                x_dequant[self.unique_top_indices] = x_dequant_1\n                x_dequant[self.unique_nottop_indices] = x_dequant_2\n        return x_dequant\n\n\n    def init_quantization_scale(self, x: torch.Tensor, channel_wise: bool = False):\n        delta, zero_point = None, None\n        if channel_wise:\n            x_clone = x.clone().detach()\n            if self.is_act:\n                if len(x.shape) == 3:\n                    n_channels = x_clone.shape[-1]\n                elif len(x.shape) == 4:\n                    n_channels = x_clone.shape[1]\n                elif len(x.shape) == 2:\n                    n_channels = x_clone.shape[1]\n                else:\n                    raise NotImplementedError\n\n                if len(x.shape) == 4:\n                    x_max = x_clone.abs().max(dim=0)[0].max(dim=-1)[0].max(dim=-1)[0]\n                elif len(x.shape) == 2:\n                    x_max = x_clone.abs().max(dim=0)[0]\n                elif len(x.shape) == 3:\n                    x_max = x_clone.abs().max(dim=0)[0].max(dim=0)[0]\n                else:\n                    raise NotImplementedError\n\n                delta = x_max.clone()\n                zero_point = x_max.clone()\n                for c in range(n_channels):\n                    if len(x.shape) == 3:\n                        delta[c], zero_point[c] = self.init_quantization_scale(x_clone[:,:,c], channel_wise=False)\n                    elif len(x.shape) == 4:\n                        delta[c], zero_point[c] = self.init_quantization_scale(x_clone[:,c,...], channel_wise=False)\n                    else:\n                        delta[c], zero_point[c] = self.init_quantization_scale(x_clone[:,c], channel_wise=False)\n                if len(x.shape) == 4:\n\n                    delta = delta.reshape(1, -1, 1, 1)\n                    zero_point = zero_point.reshape(1, -1, 1, 1)\n                elif len(x.shape) == 2:\n                    delta = delta.reshape(1, -1)\n                    zero_point = zero_point.reshape(1, -1)\n                elif len(x.shape) == 3:\n                    delta = delta.reshape(1, 1, -1)\n                    zero_point = zero_point.reshape(1, 1, -1)\n                else:\n                    raise NotImplementedError\n            else:\n                n_channels = x_clone.shape[-1] if len(x.shape) == 3 else x_clone.shape[0]\n                if len(x.shape) == 4:\n                    x_max = x_clone.abs().max(dim=-1)[0].max(dim=-1)[0].max(dim=-1)[0]\n                elif len(x.shape) == 2:\n                    x_max = x_clone.abs().max(dim=-1)[0]\n                elif len(x.shape) == 3:\n                    x_max = x_clone.abs().max(dim=0)[0].max(dim=0)[0]\n                else:\n                    raise NotImplementedError\n\n                delta = x_max.clone()\n                zero_point = x_max.clone()\n                for c in range(n_channels):\n                    if len(x.shape) == 3:\n                        delta[c], zero_point[c] = self.init_quantization_scale(x_clone[:,:,c],\n                                                                               channel_wise=False)\n                    else:\n                        delta[c], zero_point[c] = self.init_quantization_scale(x_clone[c],\n                                                                               channel_wise=False)\n                if len(x.shape) == 4:\n                    delta = delta.reshape(-1, 1, 1, 1)\n                    zero_point = zero_point.reshape(-1, 1, 1, 1)\n                elif len(x.shape) == 2:\n                    delta = delta.reshape(-1, 1)\n                    zero_point = zero_point.reshape(-1, 1)\n                elif len(x.shape) == 3:\n                    delta = delta.reshape(1, 1, -1)\n                    zero_point = zero_point.reshape(1, 1, -1)\n                else:\n                    raise NotImplementedError\n        else:\n            x_clone = x.clone().detach()\n            x_max = x_clone.max()\n            x_min = x_clone.min()\n            best_score = 1e+10\n            if self.is_act:\n                search_range = [0.999, 0.9999, 0.99999]\n            else:\n                search_range = [0.97, 0.98, 0.99, 0.995, 0.9995, 0.9997, 0.9999, 0.99995, 0.99999, 1]\n\n            if not self.split_quantization:\n                for pct in search_range:\n                    try:\n                        new_max = torch.quantile(x_clone.reshape(-1), pct)\n                        new_min = torch.quantile(x_clone.reshape(-1), 1.0 - pct)\n                    except:\n                        new_max = torch.tensor(np.percentile(\n                            x_clone.reshape(-1).cpu(), pct * 100),\n                            device=x_clone.device,\n                            dtype=torch.float32)\n                        new_min = torch.tensor(np.percentile(\n                            x_clone.reshape(-1).cpu(), (1 - pct) * 100),\n                            device=x_clone.device,\n                            dtype=torch.float32)\n                    x_q = self.quantize(x_clone, new_max, new_min)\n                    score = lp_loss(x_clone, x_q, p=2, reduction='all')\n                    if score < best_score:\n                        best_score = score\n                        delta = (new_max - new_min) / (2 ** self.n_bits - 1)\n                        zero_point = (- new_min / delta).round()\n            else:\n                x_clone = torch.abs(x_clone)\n                for pct in search_range:\n                    try:\n                        new_max = torch.quantile(x_clone.reshape(-1), pct)\n                        new_min = torch.quantile(x_clone.reshape(-1), 1.0 - pct)\n                    except:\n                        new_max = torch.tensor(np.percentile(\n                            x_clone.reshape(-1).cpu(), pct * 100),\n                            device=x_clone.device,\n                            dtype=torch.float32)\n                        new_min = torch.tensor(np.percentile(\n                            x_clone.reshape(-1).cpu(), (1 - pct) * 100),\n                            device=x_clone.device,\n                            dtype=torch.float32)\n                    x_q = self.quantize(x_clone, new_max, new_min)\n                    score = lp_loss(x_clone, x_q, p=2, reduction='all')\n                    if score < best_score:\n                        best_score = score\n                        delta = (new_max - new_min) / (2 ** (self.n_bits-1) - 1)\n                        zero_point = (- new_min / delta).round()\n\n\n        return delta, zero_point\n\n\n    def quantize(self, x, max, min):\n        if not self.split_quantization:\n            delta = (max - min) / (2 ** self.n_bits - 1)\n            zero_point = (- min / delta).round()\n            # we assume weight quantization is always signed\n            x_int = torch.round(x / delta)\n            x_quant = torch.clamp(x_int + zero_point, 0, self.n_levels - 1)\n            x_float_q = (x_quant - zero_point) * delta\n            return x_float_q\n        else:\n            delta = (max - min) / (2 ** self.n_bits - 1)\n            zero_point = (- min / delta).round()\n            # we assume weight quantization is always signed\n            x_int = torch.round(x / delta)\n            x_quant = torch.clamp(x_int + zero_point, 0, (self.n_levels//2) - 1)\n            x_float_q = (x_quant - zero_point) * delta\n            return x_float_q\n\n\nclass LogSqrt2Quantizer(nn.Module):\n    \"\"\"\n    PyTorch Function that can be used for asymmetric quantization (also called uniform affine\n    quantization). Quantizes its argument in the forward pass, passes the gradient 'straight\n    through' on the backward pass, ignoring the quantization that occurred.\n    Based on https://arxiv.org/abs/1806.08342.\n    :param n_bits: number of bit for quantization\n    :param channel_wise: if True, compute scale and zero_point in each channel\n    \"\"\"\n    def __init__(self, n_bits: int = 8, channel_wise: bool = False, is_act:bool = False):\n        super(LogSqrt2Quantizer, self).__init__()\n        assert 2 <= n_bits <= 8, 'bitwidth not supported'\n        self.n_bits = n_bits\n        self.n_levels = 2 ** self.n_bits\n        self.delta = None\n        self.inited = False\n        self.channel_wise = channel_wise\n\n\n    def forward(self, x: torch.Tensor):\n\n        if self.inited is False:\n            self.delta = self.init_quantization_scale(x)\n            self.inited = True\n\n        # start quantization\n        x_dequant = self.quantize(x, self.delta)\n        return x_dequant\n\n    def init_quantization_scale(self, x: torch.Tensor):\n        delta = None\n        x_clone = x.clone().detach()\n        delta = x_clone.max()\n        best_score = 1e+10\n        for pct in [0.999, 0.9999, 0.99999]:\n            try:\n                new_delta = torch.quantile(x_clone.reshape(-1), pct)\n            except:\n                new_delta = torch.tensor(np.percentile(\n                    x_clone.reshape(-1).cpu(), pct * 100),\n                    device=x_clone.device,\n                    dtype=torch.float32)\n            x_q = self.quantize(x_clone, new_delta)\n            score = lp_loss(x_clone, x_q, p=2, reduction='all')\n            if score < best_score:\n                best_score = score\n                delta = new_delta\n\n        return delta\n\n    def quantize(self, x, delta):\n        from math import sqrt\n        x_int = torch.round(-1 * (x/delta).log2() * 2)\n        mask = x_int >= self.n_levels\n        x_quant = torch.clamp(x_int, 0, self.n_levels - 1)\n        odd_mask = (x_quant%2) * (sqrt(2)-1) + 1\n        x_float_q = 2**(-1 * torch.ceil(x_quant/2)) * odd_mask * delta\n        x_float_q[mask] = 0\n        return x_float_q\n\n\nclass QuantConv2d(nn.Conv2d):\n    \"\"\"\n    Class to quantize weights of given convolutional layer\n    \"\"\"\n    def __init__(self,\n                in_channels,\n                out_channels,\n                kernel_size,\n                stride=1,\n                padding=0,\n                dilation=1,\n                groups=1,\n                bias=True,\n                input_quant_params={},\n                weight_quant_params={}):\n        super(QuantConv2d, self).__init__(in_channels=in_channels,\n                                          out_channels=out_channels,\n                                          kernel_size=kernel_size,\n                                          stride=stride,\n                                          padding=padding,\n                                          dilation=dilation,\n                                          groups=groups,\n                                          bias=bias)\n\n        input_quant_params_conv = deepcopy(input_quant_params)\n        input_quant_params_conv['n_bits'] = 8\n        # self.input_quantizer = UniformQuantizer(**input_quant_params_conv)\n        self.weight_quantizer = UniformQuantizer(**weight_quant_params)\n\n        self.store_input = None\n        self.store_output = None\n\n        self.use_input_quant = False\n        self.use_weight_quant = False\n\n\n    def __repr__(self):\n        s = super(QuantConv2d, self).__repr__()\n        s = \"(\" + s + \"input_quant={}, weight_quant={})\".format(self.use_input_quant, self.use_weight_quant)\n        return s\n\n    def set_quant_state(self, input_quant=False, weight_quant=False):\n        self.use_input_quant = input_quant\n        self.use_weight_quant = weight_quant\n\n    def forward(self, x):\n        \"\"\"\n        using quantized weights to forward input x\n        \"\"\"\n        # if self.use_input_quant:\n        #     x = self.input_quantizer(x)\n\n        if self.use_weight_quant:\n            w = self.weight_quantizer(self.weight)\n        else:\n            w = self.weight\n\n        out = F.conv2d(\n            x,\n            w,\n            self.bias,\n            self.stride,\n            self.padding,\n            self.dilation,\n            self.groups\n        )\n\n        return out\n\n\nclass QuantLinear(nn.Linear):\n    \"\"\"\n    Class to quantize weights of given Linear layer\n    \"\"\"\n    def __init__(self, \n                 in_features,\n                 out_features,\n                 input_quant_params={},\n                 weight_quant_params={}):\n        super(QuantLinear, self).__init__(in_features, out_features)\n\n        input_quant_params_linear = deepcopy(input_quant_params)\n        input_quant_params_linear['is_act'] = True\n        self.input_quantizer = UniformQuantizer(**input_quant_params_linear)\n        self.weight_quantizer = UniformQuantizer(**weight_quant_params)\n\n        self.use_input_quant = False\n        self.use_weight_quant = False\n\n        self.store_input = None\n        self.store_output = None\n\n    def __repr__(self):\n        s = super(QuantLinear, self).__repr__()\n        s = \"(\" + s + \"input_quant={}, weight_quant={})\".format(self.use_input_quant, self.use_weight_quant)\n        return s\n\n    def set_quant_state(self, input_quant=False, weight_quant=False):\n        self.use_input_quant = input_quant\n        self.use_weight_quant = weight_quant\n\n    def forward(self, x):\n        \"\"\"\n        using quantized weights to forward input x\n        \"\"\"\n\n        if self.use_input_quant:\n            x = self.input_quantizer(x)\n\n        if self.use_weight_quant:\n            w = self.weight_quantizer(self.weight)\n        else:\n            w = self.weight\n        out = F.linear(x, weight=w, bias=self.bias)\n\n        return out\n\n\nclass QuantMatMul(nn.Module):\n    \"\"\"\n    Class to quantize weights of given Linear layer\n    \"\"\"\n    def __init__(self,\n                 input_quant_params={}):\n        super(QuantMatMul, self).__init__()\n\n        input_quant_params_matmul = deepcopy(input_quant_params)\n        input_quant_params_matmul['is_act'] = True\n        if 'log_quant' in input_quant_params_matmul:\n            input_quant_params_matmul.pop('log_quant')\n            self.quantizer_A = LogSqrt2Quantizer(**input_quant_params_matmul)\n            # self.quantizer_A = UniformQuantizer(**input_quant_params_matmul)\n        else:\n            self.quantizer_A = UniformQuantizer(**input_quant_params_matmul)\n        self.quantizer_B = UniformQuantizer(**input_quant_params_matmul)\n\n        self.use_input_quant = False\n\n    def __repr__(self):\n        s = super(QuantMatMul, self).__repr__()\n        s = \"(\" + s + \"input_quant={})\".format(self.use_input_quant)\n        return s\n\n    def set_quant_state(self, input_quant=False, weight_quant=False):\n        self.use_input_quant = input_quant\n\n    def forward(self, A, B):\n        if self.use_input_quant:\n            A = self.quantizer_A(A)\n            B = self.quantizer_B(B)\n        out = A @ B\n        return out\n\n\ndef quant_model(model, input_quant_params={}, weight_quant_params={}):\n    # post-softmax\n    input_quant_params_matmul2 = deepcopy(input_quant_params)\n    input_quant_params_matmul2['log_quant'] = True\n\n    # SimQuant\n    input_quant_params_channel = deepcopy(input_quant_params)\n    input_quant_params_channel['channel_wise'] = True\n\n    module_dict={}\n    for name, m in model.named_modules():\n        module_dict[name] = m\n        idx = name.rfind('.')\n        if idx == -1:\n            idx = 0\n        father_name = name[:idx]\n        if father_name in module_dict:\n            father_module = module_dict[father_name]\n        else:\n            raise RuntimeError(f\"father module {father_name} not found\")\n\n        if isinstance(m, nn.Conv2d):\n            # Embedding Layer\n            idx = idx + 1 if idx != 0 else idx\n            new_m = QuantConv2d(\n                m.in_channels,\n                m.out_channels,\n                m.kernel_size,\n                m.stride,\n                m.padding,\n                m.dilation,\n                m.groups,\n                m.bias is not None,\n                input_quant_params,\n                weight_quant_params\n            )\n            new_m.weight.data = m.weight.data\n            new_m.bias = m.bias\n            setattr(father_module, name[idx:], new_m)\n        elif isinstance(m, nn.Linear):\n            # Linear Layer\n            idx = idx + 1 if idx != 0 else idx\n            if 'qkv' in name or 'fc1' in name:\n                weight_quant_params['two_part'] = True\n                new_m = QuantLinear(m.in_features, m.out_features, input_quant_params_channel, weight_quant_params)\n                weight_quant_params.pop('two_part')\n            else:\n                new_m = QuantLinear(m.in_features, m.out_features, input_quant_params, weight_quant_params)\n            new_m.weight.data = m.weight.data\n            new_m.bias = m.bias\n            setattr(father_module, name[idx:], new_m)\n        elif isinstance(m, MatMul):\n            # Matmul Layer\n            idx = idx + 1 if idx != 0 else idx\n            if 'matmul2' in name:\n                new_m = QuantMatMul(input_quant_params_matmul2)\n            else:\n                new_m = QuantMatMul(input_quant_params)\n            setattr(father_module, name[idx:], new_m)\n\n    return model\n\n\ndef set_quant_state(model, input_quant=False, weight_quant=False):\n    for m in model.modules():\n        if isinstance(m, (QuantConv2d, QuantLinear, QuantMatMul)):\n            m.set_quant_state(input_quant, weight_quant)\n\n\nclass MatMul(nn.Module):\n    def forward(self, A, B):\n        return A @ B\n\n\ndef attention_forward(self, x):\n    B, N, C = x.shape\n    qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n    q, k, v = qkv.unbind(0)\n\n    attn = self.matmul1(q, k.transpose(-2, -1)) * self.scale\n    attn = attn.softmax(dim=-1)\n    attn = self.attn_drop(attn)\n\n    x = self.matmul2(attn, v).transpose(1, 2).reshape(B, N, C)\n    x = self.proj(x)\n    x = self.proj_drop(x)\n    return x\n\n\ndef window_attention_forward(self, x, mask = None):\n    B_, N, C = x.shape\n    qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n    q, k, v = qkv.unbind(0)\n\n    q = q * self.scale\n    attn = self.matmul1(q, k.transpose(-2,-1))\n\n    relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n        self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n    relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n    attn = attn + relative_position_bias.unsqueeze(0)\n\n    if mask is not None:\n        nW = mask.shape[0]\n        attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n        attn = attn.view(-1, self.num_heads, N, N)\n        attn = self.softmax(attn)\n    else:\n        attn = self.softmax(attn)\n\n    attn = self.attn_drop(attn)\n\n    x = self.matmul2(attn, v).transpose(1, 2).reshape(B_, N, C)\n    x = self.proj(x)\n    x = self.proj_drop(x)\n    return x\n\n\ndef im2col(input_data, kernel_size, stride, padding):\n    # 添加padding\n    input_padded = F.pad(input_data, (padding, padding, padding, padding))\n    # 获取输入数据的维度\n    batch_size, channels, height, width = input_padded.shape\n\n    # 输出的高度和宽度\n    out_height = (height - kernel_size) // stride + 1\n    out_width = (width - kernel_size) // stride + 1\n\n    # 展开操作\n    cols = torch.zeros(batch_size, channels, kernel_size, kernel_size, out_height, out_width, device=input_data.device)\n\n    for y in range(kernel_size):\n        y_max = y + stride*out_height\n        for x in range(kernel_size):\n            x_max = x + stride*out_width\n            cols[:, :, y, x, :, :] = input_padded[:, :, y:y_max:stride, x:x_max:stride]\n\n    cols = cols.permute(0, 4, 5, 1, 2, 3).reshape(batch_size*out_height*out_width, -1)\n    return cols\n\n\n@torch.no_grad()\ndef reparameterization(q_model, model_name):\n    print('Performing scale reparameterization ...')\n    module_dict = {}\n    q_model_slice = q_model.layers if 'swin' in model_name else q_model.blocks\n    for name, module in q_model_slice.named_modules():\n        module_dict[name] = module\n        idx = name.rfind('.')\n        if idx == -1:\n            idx = 0\n        father_name = name[:idx]\n        if father_name in module_dict:\n            father_module = module_dict[father_name]\n        else:\n            raise RuntimeError(f\"father module {father_name} not found\")\n        if 'norm1' in name or 'norm2' in name:\n            if 'norm1' in name:\n                next_module = father_module.attn.qkv\n            elif 'norm2' in name:\n                next_module = father_module.mlp.fc1\n            else:\n                next_module = father_module.reduction\n\n            act_delta = next_module.input_quantizer.delta.reshape(-1)\n            act_zero_point = next_module.input_quantizer.zero_point.reshape(-1)\n            act_min = -act_zero_point * act_delta\n\n            target_delta = torch.mean(act_delta)\n            target_zero_point = torch.mean(act_zero_point)\n            target_min = -target_zero_point * target_delta\n\n            r = act_delta / target_delta\n            b = act_min / r - target_min\n\n            module.weight.data = module.weight.data / r\n            module.bias.data = module.bias.data / r - b\n\n            next_module.weight.data = next_module.weight.data * r\n            if next_module.bias is not None:\n                next_module.bias.data = next_module.bias.data + torch.mm(next_module.weight.data,\n                                                                         b.reshape(-1, 1)).reshape(-1)\n            else:\n                next_module.bias = Parameter(torch.Tensor(next_module.out_features))\n                next_module.bias.data = torch.mm(next_module.weight.data, b.reshape(-1, 1)).reshape(-1)\n\n            next_module.input_quantizer.channel_wise = False\n            next_module.input_quantizer.delta = Parameter(target_delta).contiguous()\n            next_module.input_quantizer.zero_point = Parameter(target_zero_point).contiguous()\n            next_module.weight_quantizer.inited.fill_(0)\n\n\n@torch.no_grad()\ndef replace_W(q_model, folder_path, args_coe):\n    for n, m in q_model.named_modules():\n        if isinstance(m, QuantLinear):\n            with open(os.path.join(folder_path, n + 'store_input'), 'rb') as file:\n                store_input = pickle.load(file)\n            with open(os.path.join(folder_path, n + 'store_output'), 'rb') as file:\n                store_output = pickle.load(file)\n\n            print(\"complete collecting act...\")\n            fp_input = store_input[0]\n            if len(fp_input.shape) == 2:\n                num_of_inverse = 0.1\n            else:\n                num_of_inverse = 1e-1 * args_coe\n\n            fp_output_shape = store_output[0].shape\n            fp_output_flat = store_output[0].cuda().reshape(-1, fp_output_shape[-1])\n            quan_output = m.input_quantizer(store_input[0].cuda())\n            del store_input\n\n            w = m.weight.clone()\n\n            if getattr(m, \"bias\") is not None:\n                b = m.bias.clone()\n                W_cat = torch.cat((w, b.unsqueeze(1)), dim=1).cuda()\n\n                quan_output_flat = quan_output.reshape(-1, quan_output.shape[-1])\n                quan_output_cat = torch.cat((quan_output_flat, torch.ones(quan_output_flat.shape[0], 1).cuda()),\n                                            dim=1)\n\n                A = quan_output_cat\n                Y = fp_output_flat - (quan_output_cat @ W_cat.T)\n\n                beta = torch.inverse(A.permute(1, 0) @ A\n                                     + torch.eye(A.shape[1]).cuda() * num_of_inverse) @ A.permute(1, 0) @ Y\n\n                new_W, new_b_0 = torch.split(beta, [beta.shape[0] - 1, 1], dim=0)\n                new_b = new_b_0.squeeze()\n                m.weight.data = new_W.T + w\n                m.bias.data = new_b + b\n\n                del fp_output_flat, quan_output, w, b, W_cat, quan_output_flat, quan_output_cat, A, Y,\n                    beta, new_W, new_b_0, new_b\n                torch.cuda.empty_cache()\n            else:\n                W_cat = w.cuda()\n                quan_output_flat = quan_output.reshape(-1, quan_output.shape[-1])\n                quan_output_cat = quan_output_flat\n\n                A = quan_output_cat\n                Y = fp_output_flat - (quan_output_cat @ W_cat.T)\n\n                beta = torch.inverse(A.permute(1, 0) @ A\n                                     + torch.eye(A.shape[1]).cuda() * num_of_inverse) @ A.permute(1, 0) @ Y\n\n                new_W = beta\n                m.weight.data = new_W.T + w\n\n                del fp_output_flat, quan_output, w, W_cat, quan_output_flat, quan_output_cat, A, Y,\n                    beta, new_W\n                torch.cuda.empty_cache()\n            print(f'complete computing for W in {n}')\n            print()\n\n\n        if isinstance(m, QuantConv2d):\n            if 'embed' in n:\n                print('skip QuantConv2d!')\n                continue\n            with open(os.path.join(folder_path, n + 'store_input'), 'rb') as file:\n                store_input = pickle.load(file)\n            with open(os.path.join(folder_path, n + 'store_output'), 'rb') as file:\n                store_output = pickle.load(file)\n\n            print(\"complete collecting act...\")\n            quan_output = m.input_quantizer(store_input[0].cuda())\n\n            kernel_size = m.weight.shape[2]\n            stride = m.stride[0]\n            padding = m.padding[0]\n            quan_output_cols = im2col(quan_output, kernel_size, stride, padding)\n\n            weights_col = deepcopy(m.weight.reshape(m.weight.shape[0], -1).T)\n\n            del store_input\n\n            num_of_inverse = 1e-1 * args_coe\n\n            with torch.no_grad():\n                w = weights_col\n\n                if getattr(m, \"bias\") is not None:\n\n                    b = m.bias.clone()\n                    W_cat = torch.cat((w, b.unsqueeze(0)), dim=0).cuda()\n\n                    quan_output_flat = quan_output_cols\n                    quan_output_cat = torch.cat((quan_output_flat,\n                                                 torch.ones(quan_output_flat.shape[0], 1).cuda()),\n                                                dim=1)\n\n                    A = quan_output_cat\n                    tmp = (quan_output_cat @ W_cat)\n                    fp_output_flat = store_output[0].cuda()\n                    fp_output_flat = fp_output_flat.permute(0, 2, 3, 1).reshape(tmp.shape)\n                    Y = fp_output_flat - tmp\n                    beta = torch.inverse(A.permute(1, 0) @ A\n                                         + torch.eye(A.shape[1]).cuda() * num_of_inverse) @ A.permute(1, 0) @ Y\n\n                    new_W, new_b_0 = torch.split(beta, [beta.shape[0] - 1, 1], dim=0)\n                    new_b = new_b_0.squeeze()\n\n                    m.weight.data = (new_W + w).T.reshape(m.weight.shape)\n                    m.bias.data = new_b + b\n\n                    del fp_output_flat, quan_output, w, b, W_cat, quan_output_flat, quan_output_cat, A, Y,\n                        beta, new_W, new_b_0, new_b\n                    torch.cuda.empty_cache()\n                else:\n                    W_cat = w.cuda()\n\n                    quan_output_flat = quan_output_cols\n\n                    A = quan_output_cat\n                    tmp = (quan_output_cat @ W_cat)\n                    fp_output_flat = store_output[0].cuda()\n                    fp_output_flat = fp_output_flat.permute(0, 2, 3, 1).reshape(tmp.shape)\n                    Y = fp_output_flat - tmp\n                    beta = torch.inverse(A.permute(1, 0) @ A\n                                         + torch.eye(A.shape[1]).cuda() * num_of_inverse) @ A.permute(1, 0) @ Y\n\n                    new_W = beta\n\n                    m.weight.data = (new_W + w).T.reshape(m.weight.shape)\n\n                    del fp_output_flat, quan_output, w, W_cat, quan_output_flat, quan_output_cat, A, Y,\n                        beta, new_W\n                    torch.cuda.empty_cache()\n            print(f'complete computing for W in {n}')\n            print()\n\n\n@torch.no_grad()\ndef replace_W_afterquant_vector_twopart(q_model, folder_path, args):\n    for n, m in q_model.named_modules():\n        if isinstance(m, QuantLinear):\n            with open(os.path.join(folder_path, n + 'store_input'), 'rb') as file:\n                store_input = pickle.load(file)\n            with open(os.path.join(folder_path, n + 'store_output'), 'rb') as file:\n                store_output = pickle.load(file)\n\n            print(\"complete collecting act...\")\n            fp_input = store_input[0]\n\n            if len(fp_input.shape) == 2:\n                num_of_inverse = 0.1\n            else:\n                num_of_inverse = 1e-1 * args.coe\n\n            quan_output = m.input_quantizer(store_input[0].cuda())\n\n            num_of_inverse = 1e-1 * args.coe\n\n            if getattr(m, \"bias\") is not None:\n                w = m.weight.clone()\n                b = m.bias.clone()\n\n                print(f'redistribute W, there are {w.shape[0]} output channel of layer {n}')\n\n                quan_output_flat = quan_output.reshape(-1, quan_output.shape[-1])\n                quan_output_flat = torch.cat(\n                    (quan_output_flat, torch.ones((quan_output_flat.shape[0], 1)).cuda()), dim=1)\n\n                current = torch.cat((w, b.clone().unsqueeze(1)), dim=1).detach().clone().cuda()\n                mask = torch.ones_like(current[0]).bool()\n                while torch.sum(mask) > 1:\n\n                    number_of_quant = torch.sum(mask) // 2\n                    number_of_adjust = torch.sum(mask) - number_of_quant\n\n                    x_dequant_floor = m.weight_quantizer(current, 'floor')\n                    w_error_floor = x_dequant_floor - current\n                    w_error_floor[:, -1] = 0\n\n                    x_dequant_ceil = m.weight_quantizer(current, 'ceil')\n                    w_error_ceil = x_dequant_ceil - current\n                    w_error_ceil[:, -1] = 0\n\n                    x_dequant = m.weight_quantizer(current, 'round')\n                    w_error = x_dequant - current\n                    w_error[:, -1] = 0\n\n                    outlier_indices = torch.arange(0+torch.sum(~mask), number_of_quant+torch.sum(~mask))\n\n                    if args.model == 'swin_small' or args.model == 'swin_tiny':\n                        B = 500\n                    elif args.model == 'swin_base':\n                        B = 100\n                    else:\n                        B = 500\n\n                    if True:\n                        means = torch.mean(quan_output_flat[:, outlier_indices], dim=0)\n                        covs = torch.cov(quan_output_flat[:, outlier_indices].T)\n                        coes = means.unsqueeze(0).T @ means.unsqueeze(0) + covs\n\n                        groups = math.ceil(len(current) / B)\n                        for g in range(groups):\n\n                            a = np.arange(g * B, min((g + 1) * B, len(current)))\n                            current_outputs = torch.tensor(a).cuda()\n\n\n                            sub_delta1 = w_error.clone()[current_outputs][:, outlier_indices]\n                            sub_delta_floor = w_error_floor[current_outputs][:, outlier_indices]\n                            sub_delta_ceil = w_error_ceil[current_outputs][:, outlier_indices]\n\n                            fail_dim = torch.zeros(len(sub_delta1)).bool()\n                            count = 0\n                            while count < 100 and torch.sum(~fail_dim) > 0:\n                                count += 1\n\n                                gradient = (2 * coes @ sub_delta1.T).T\n                                same_sign = (sub_delta1 * gradient > 0)\n                                gradient[~same_sign] = 0\n                                gradient[:, ~mask[outlier_indices]] = 0\n\n                                number_of_nonzero_gradi = torch.sum(gradient != 0, dim=1)\n                                number_of_flip = torch.minimum(number_of_nonzero_gradi, torch.tensor(1))\n                                _, max_diff_indexs = torch.topk(abs(gradient), k=int(torch.max(number_of_flip).item()), dim=1)\n\n                                v = torch.gather(sub_delta1, 1, max_diff_indexs) - torch.gather(gradient, 1, max_diff_indexs)\n                                ceils = torch.gather(sub_delta_ceil, 1, max_diff_indexs)\n                                floors = torch.gather(sub_delta_floor, 1, max_diff_indexs)\n                                distance_to_ceil = torch.abs(v - ceils)\n                                distance_to_floor = torch.abs(v - floors)\n                                v = torch.where(distance_to_ceil <= distance_to_floor, ceils, floors)\n\n                                cur_min = (sub_delta1.unsqueeze(1) @ coes @ sub_delta1.unsqueeze(2)).squeeze()\n                                tmp = torch.gather(sub_delta1, 1, max_diff_indexs).clone()\n                                sub_delta1.scatter_(1, max_diff_indexs, v)\n\n                                cur_min_v = (sub_delta1.unsqueeze(1) @ coes @ sub_delta1.unsqueeze(2)).squeeze()\n\n                                fail_dim = cur_min_v > cur_min\n\n                                temp = sub_delta1[fail_dim].clone()\n                                temp.scatter_(1, max_diff_indexs[fail_dim], tmp[fail_dim])\n                                sub_delta1[fail_dim] = temp\n\n                            w_error[current_outputs.unsqueeze(1), outlier_indices] = sub_delta1\n\n                    mask[outlier_indices] = False\n                    remaining_indices = torch.nonzero(mask).squeeze()\n                    non_outliers_indices = remaining_indices\n                    groups = math.ceil(len(current) / B)\n\n                    for g in range(groups):\n                        current_outputs = torch.arange(g * B, min((g + 1) * B, len(current))).cuda()\n\n                        w1 = current[current_outputs][:, outlier_indices]\n                        w2 = current[current_outputs][:, non_outliers_indices]\n\n                        I1 = quan_output_flat[:, outlier_indices]\n                        I2 = quan_output_flat[:, non_outliers_indices]\n                        delta1 = w_error[current_outputs][:, outlier_indices]\n                        delta2 = -torch.inverse(\n                            I2.T @ I2 + num_of_inverse * torch.eye(number_of_adjust).cuda()) @ (\n                                     I2.T @ I1) @ delta1.T\n                        w2 += delta2.T\n\n                        if len(w2.shape) == 1:\n                            w2 = w2.unsqueeze(1)\n\n                        current[current_outputs.unsqueeze(1), outlier_indices] = w1 + delta1\n                        current[current_outputs.unsqueeze(1), non_outliers_indices] = w2\n\n\n                new_w, new_b = torch.split(current, [current.shape[1] - 1, 1], dim=1)\n\n                w.copy_(new_w)\n                b.copy_(new_b.squeeze())\n\n                m.weight.data = w\n                m.bias.data = b\n                m.set_quant_state(True, False)\n                torch.cuda.empty_cache()\n\n            else:\n                w = m.weight.clone()\n\n                print(f'redistribute W, there are {w.shape[0]} output channel of layer {n}')\n\n                quan_output_flat = quan_output.reshape(-1, quan_output.shape[-1])\n\n                current = w.detach().clone().cuda()\n                mask = torch.ones_like(current[0]).bool()\n                while torch.sum(mask) > 1:\n\n                    number_of_quant = torch.sum(mask) // 2\n                    number_of_adjust = torch.sum(mask) - number_of_quant\n\n                    x_dequant_floor = m.weight_quantizer(current, 'floor')\n                    w_error_floor = x_dequant_floor - current\n                    w_error_floor[:, -1] = 0\n\n                    x_dequant_ceil = m.weight_quantizer(current, 'ceil')\n                    w_error_ceil = x_dequant_ceil - current\n                    w_error_ceil[:, -1] = 0\n\n                    x_dequant = m.weight_quantizer(current, 'round')\n                    w_error = x_dequant - current\n                    w_error[:, -1] = 0\n\n                    outlier_indices = torch.arange(0+torch.sum(~mask), number_of_quant+torch.sum(~mask))\n\n                    if args.model == 'swin_small' or args.model == 'swin_tiny':\n                        B = 500\n                    elif args.model == 'swin_base':\n                        B = 100\n                    else:\n                        B = 500\n\n                    if True:\n                        means = torch.mean(quan_output_flat[:, outlier_indices], dim=0)\n                        covs = torch.cov(quan_output_flat[:, outlier_indices].T)\n                        coes = means.unsqueeze(0).T @ means.unsqueeze(0) + covs\n\n                        groups = math.ceil(len(current) / B)\n                        for g in range(groups):\n\n                            a = np.arange(g * B, min((g + 1) * B, len(current)))\n                            current_outputs = torch.tensor(a).cuda()\n\n\n                            sub_delta1 = w_error.clone()[current_outputs][:, outlier_indices]\n                            sub_delta_floor = w_error_floor[current_outputs][:, outlier_indices]\n                            sub_delta_ceil = w_error_ceil[current_outputs][:, outlier_indices]\n\n                            fail_dim = torch.zeros(len(sub_delta1)).bool()\n                            count = 0\n                            while count < 100 and torch.sum(~fail_dim) > 0:\n                                count += 1\n\n                                gradient = (2 * coes @ sub_delta1.T).T\n                                same_sign = (sub_delta1 * gradient > 0)\n                                gradient[~same_sign] = 0\n                                gradient[:, ~mask[outlier_indices]] = 0\n\n                                number_of_nonzero_gradi = torch.sum(gradient != 0, dim=1)\n                                number_of_flip = torch.minimum(number_of_nonzero_gradi, torch.tensor(1))\n                                _, max_diff_indexs = torch.topk(abs(gradient), k=int(torch.max(number_of_flip).item()), dim=1)\n\n                                v = torch.gather(sub_delta1, 1, max_diff_indexs) - torch.gather(gradient, 1, max_diff_indexs)\n                                ceils = torch.gather(sub_delta_ceil, 1, max_diff_indexs)\n                                floors = torch.gather(sub_delta_floor, 1, max_diff_indexs)\n                                distance_to_ceil = torch.abs(v - ceils)\n                                distance_to_floor = torch.abs(v - floors)\n                                v = torch.where(distance_to_ceil <= distance_to_floor, ceils, floors)\n\n                                cur_min = (sub_delta1.unsqueeze(1) @ coes @ sub_delta1.unsqueeze(2)).squeeze()\n                                tmp = torch.gather(sub_delta1, 1, max_diff_indexs).clone()\n                                sub_delta1.scatter_(1, max_diff_indexs, v)\n\n                                cur_min_v = (sub_delta1.unsqueeze(1) @ coes @ sub_delta1.unsqueeze(2)).squeeze()\n\n                                fail_dim = cur_min_v > cur_min\n\n                                temp = sub_delta1[fail_dim].clone()\n                                temp.scatter_(1, max_diff_indexs[fail_dim], tmp[fail_dim])\n                                sub_delta1[fail_dim] = temp\n\n                            w_error[current_outputs.unsqueeze(1), outlier_indices] = sub_delta1\n\n                    mask[outlier_indices] = False\n                    remaining_indices = torch.nonzero(mask).squeeze()\n                    non_outliers_indices = remaining_indices\n                    groups = math.ceil(len(current) / B)\n\n                    for g in range(groups):\n                        current_outputs = torch.arange(g * B, min((g + 1) * B, len(current))).cuda()\n\n                        w1 = current[current_outputs][:, outlier_indices]\n                        w2 = current[current_outputs][:, non_outliers_indices]\n\n                        I1 = quan_output_flat[:, outlier_indices]\n                        I2 = quan_output_flat[:, non_outliers_indices]\n                        delta1 = w_error[current_outputs][:, outlier_indices]\n                        delta2 = -torch.inverse(\n                            I2.T @ I2 + num_of_inverse * torch.eye(number_of_adjust).cuda()) @ (\n                                     I2.T @ I1) @ delta1.T\n                        w2 += delta2.T\n\n                        if len(w2.shape) == 1:\n                            w2 = w2.unsqueeze(1)\n\n                        current[current_outputs.unsqueeze(1), outlier_indices] = w1 + delta1\n                        current[current_outputs.unsqueeze(1), non_outliers_indices] = w2\n\n\n                new_w = current\n\n                w.copy_(new_w)\n\n                m.weight.data = w\n                m.set_quant_state(True, False)\n                torch.cuda.empty_cache()\n            print(f'complete computing for W in {n}')\n            print()\n\n        if isinstance(m, QuantConv2d):\n            with open(os.path.join(folder_path, n + 'store_input'), 'rb') as file:\n                store_input = pickle.load(file)\n            with open(os.path.join(folder_path, n + 'store_output'), 'rb') as file:\n                store_output = pickle.load(file)\n\n            print(\"complete collecting act...\")\n\n            if hasattr(m, 'input_quantizer'):\n                quan_output = m.input_quantizer(store_input[0].cuda())\n            else:\n                quan_output = store_input[0].cuda()\n\n            kernel_size = m.weight.shape[2]\n            stride = m.stride[0]\n            padding = m.padding[0]\n            quan_output_cols = im2col(quan_output, kernel_size, stride, padding)\n\n            weights_col = deepcopy(m.weight.reshape(m.weight.shape[0], -1).T)\n\n            del store_input\n\n            num_of_inverse = 1e-1 * args.coe * 10\n\n            if getattr(m, \"bias\") is not None:\n\n                w = weights_col\n                b = m.bias.clone()\n                print(f'redistribute W, there are {w.shape[0]} output channel of layer {n}')\n                quan_output_flat = quan_output_cols\n                quan_output_flat = torch.cat((quan_output_flat,\n                                             torch.ones(quan_output_flat.shape[0], 1).cuda()),\n                                            dim=1)\n\n                current = torch.cat((w, b.unsqueeze(0)), dim=0).detach().clone().cuda()\n                current = current.T\n                mask = torch.ones_like(current[0]).bool()\n\n                while torch.sum(mask) > 1:\n\n                    number_of_quant = torch.sum(mask) // 2\n                    number_of_adjust = torch.sum(mask) - number_of_quant\n\n                    current = current.unsqueeze(2).unsqueeze(2)\n                    x_dequant = m.weight_quantizer(current, 'round')\n                    w_error = x_dequant - current\n                    w_error[:, -1] = 0\n                    w_error = w_error.squeeze()\n                    current = current.squeeze()\n\n                    outlier_indices = torch.arange(0+torch.sum(~mask), number_of_quant+torch.sum(~mask))\n\n                    if args.model == 'swin_small' or args.model == 'swin_tiny':\n                        B = 500\n                    elif args.model == 'swin_base':\n                        B = 100\n                    else:\n                        B = 500\n\n                    mask[outlier_indices] = False\n                    remaining_indices = torch.nonzero(mask).squeeze()\n                    non_outliers_indices = remaining_indices\n                    groups = math.ceil(len(current) / B)\n\n                    for g in range(groups):\n\n                        current_outputs = torch.arange(g * B, min((g + 1) * B, len(current))).cuda()\n\n                        w1 = current[current_outputs][:, outlier_indices]\n                        w2 = current[current_outputs][:, non_outliers_indices]\n\n                        I1 = quan_output_flat[:, outlier_indices]\n                        I2 = quan_output_flat[:, non_outliers_indices]\n                        delta1 = w_error[current_outputs][:, outlier_indices]\n                        delta2 = -torch.inverse(\n                            I2.T @ I2 + num_of_inverse * torch.eye(number_of_adjust).cuda()) @ (\n                                     I2.T @ I1) @ delta1.T\n                        w2 += delta2.T\n\n                        if len(w2.shape) == 1:\n                            w2 = w2.unsqueeze(1)\n\n                        current[current_outputs.unsqueeze(1), outlier_indices] = w1 + delta1\n                        current[current_outputs.unsqueeze(1), non_outliers_indices] = w2\n\n                new_w, new_b = torch.split(current, [current.shape[1] - 1, 1], dim=1)\n                new_b = new_b.squeeze()\n\n                m.weight.data = new_w.reshape(m.weight.shape)\n                m.bias.data = new_b\n\n                m.set_quant_state(True, False)\n                torch.cuda.empty_cache()\n\n            else:\n                w = m.weight.clone()\n                print('None bias!')\n                for i, _ in enumerate(range(w.shape[0])):\n                    print(f'redistribute W of {i}/{w.shape[0]} output channel of layer {n}')\n\n                    quan_output_flat = quan_output_cols\n                    current = w[i, :].clone().detach().cuda()\n                    current = current.reshape(-1)\n                    mask = torch.ones_like(current).bool()\n                    mask = mask.reshape(-1)\n\n                    while torch.sum(mask) > 1:\n\n                        number_of_quant = torch.sum(mask) // 2\n                        number_of_adjust = torch.sum(mask) - number_of_quant\n\n                        x_dequant = m.weight_quantizer(current, 'round', i)\n                        w_error = x_dequant - current\n                        w_error = w_error.reshape(-1)\n\n                        w_error[~mask] += torch.inf\n                        w_error[-1] += torch.inf\n                        _, outlier_indices = torch.topk(-torch.abs(w_error), number_of_quant)\n\n\n                        mask[outlier_indices] = False\n                        remaining_indices = torch.nonzero(mask).squeeze()\n                        non_outliers_indices = remaining_indices\n\n                        w1 = current[outlier_indices]\n                        w2 = current[non_outliers_indices]\n\n                        I1 = quan_output_flat[:, outlier_indices]\n                        I2 = quan_output_flat[:, non_outliers_indices]\n                        delta1 = w_error[outlier_indices]\n\n                        delta2 = -torch.inverse(\n                            I2.T @ I2 + num_of_inverse * torch.eye(number_of_adjust).cuda()) @ (\n                                     I2.T @ I1) @ delta1\n                        w2 += delta2\n                        current[outlier_indices] = w1 + delta1\n                        current[non_outliers_indices] = w2\n\n                    x_dequant = m.weight_quantizer(current, 'round', i)\n                    x_dequant = x_dequant.reshape(-1)\n                    remaining_indices = torch.nonzero(mask).squeeze()\n                    current[remaining_indices] = x_dequant[remaining_indices]\n\n                    new_w = current\n                    w[i, :].copy_(new_w.reshape(w[i, :].shape))\n\n                m.weight.data = w\n                m.set_quant_state(True, False)\n\n                torch.cuda.empty_cache()\n            print(f'complete computing for W in {n}')\n            print()",
        "experimental_info": "The ERQ method is a two-step Post-Training Quantization (PTQ) approach, comprising Activation quantization error reduction (Aqer) and Weight quantization error reduction (Wqer).\n\n**1. Aqer (Activation Quantization Error Reduction):**\n    *   **Goal**: Mitigate activation quantization errors while keeping weights full-precision.\n    *   **Initial Setup**: Activation quantization is enabled, weights are full-precision.\n        *   `input_quant_params` are passed to `QuantConv2d`, `QuantLinear`, `QuantMatMul`.\n        *   `input_quant_params['channel_wise'] = True` for `qkv` and `fc1` linear layers' activations.\n        *   `input_quant_params['log_quant'] = True` for `MatMul` layers after softmax (specifically `matmul2`), utilizing `LogSqrt2Quantizer` (8-bit).\n        *   Default activation bit-width (`a_bits`) is 4 bits.\n    *   **Reparameterization Initialization**: Applied to LayerNorm (`norm1`, `norm2`) layers and their subsequent layers (`attn.qkv`, `mlp.fc1`).\n        *   It translates channel-wise activation quantizers (initially set for `qkv`/`fc1`) to layer-wise equivalence.\n        *   This is achieved by calculating scaling factors (`r`) and bias adjustments (`b`) based on the ratio of current channel-wise `delta`/`min` to the target layer-wise average `delta`/`min`.\n        *   LayerNorm `weight` and `bias` are adjusted (`module.weight.data = module.weight.data / r`, `module.bias.data = module.bias.data / r - b`).\n        *   Subsequent layer's weights are scaled (`next_module.weight.data = next_module.weight.data * r`), and their biases are adjusted (`next_module.bias.data = next_module.bias.data + torch.mm(next_module.weight.data, b.reshape(-1, 1)).reshape(-1)`).\n        *   The subsequent layer's `input_quantizer` is then set to `channel_wise=False` and re-initialized with the target (average) `delta` and `zero_point`.\n    *   **Ridge Regression**: After reparameterization, this step further reduces errors by updating full-precision weights.\n        *   Full-precision inputs and outputs of each layer are collected by hooking the model.\n        *   For `QuantLinear` and `QuantConv2d` layers, a Ridge Regression problem `beta = (A.T @ A + lambda * I)^-1 @ A.T @ Y` is solved.\n        *   `A` is the (quantized) input to the layer, `Y` is the residual error between the full-precision output and the output from the (quantized input @ full-precision weight) product.\n        *   The lambda term (`num_of_inverse`) for Ridge Regression is `0.1` for 2D inputs (e.g., linear layers) and `1e-1 * args.coe` (where `args.coe` defaults to 20000) for others (e.g., convolutional layers), making it `2000` for convolutions.\n        *   The weights (`m.weight.data`) and biases (`m.bias.data`) are updated with the `beta` solution.\n\n**2. Wqer (Weight Quantization Error Reduction):**\n    *   **Goal**: Quantize weights and mitigate induced errors.\n    *   **Initial Setup**: Both activation and weight quantization are enabled.\n        *   Default weight bit-width (`w_bits`) is 4 bits.\n        *   `weight_quant_params['channel_wise'] = True` for weight quantization.\n        *   `weight_quant_params['two_part'] = True` for `qkv` and `fc1` linear layers, enabling Dual Uniform Quantization.\n    *   **Dual Uniform Quantization and Iterative Quantization-and-Correction**: Applied to layers with `two_part=True` (e.g., `qkv`, `fc1`).\n        *   **Outlier Channel Selection**: For each weight matrix, outliers are identified based on percentiles (e.g., `99th percentile` for positive values and `1st percentile` for negative values in the input tensor `x`). A `best_top_number` (e.g., `x.shape[0] // 20`) of these outlier channels are selected to be quantized first.\n        *   **Iterative Process**: The method iteratively quantizes a portion (roughly half) of the remaining unquantized weights.\n            *   In each iteration, for the selected portion of weights (`outlier_indices`):\n                *   **Rounding Refinement**: The quantization error is calculated for three rounding methods ('floor', 'ceil', 'round'). An efficient proxy (based on input means and covariance, assuming Gaussian distribution) is used to refine the rounding direction, aiming to minimize the quantization error propagated to the output.\n                *   **Ridge Regression for remaining weights**: The quantization error (`delta1`) from the already quantized `outlier_indices` is then distributed to the remaining unquantized weights (`non_outliers_indices`) using another Ridge Regression solver.\n                *   `delta2 = -torch.inverse(I2.T @ I2 + lambda * I) @ (I2.T @ I1) @ delta1`, where `I1` are inputs corresponding to `outlier_indices`, `I2` are inputs for `non_outliers_indices`.\n                *   The Ridge Regression lambda (`num_of_inverse`) is `1e-1 * args.coe` (2000 for linear layers with two-part, `1e-1 * args.coe * 10` for conv layers with two-part, which would be 20000).\n            *   This iterative process continues until all weights in the layer are quantized.\n            *   To manage computation, weights are processed in batches (`B` outputs per group), where `B` is 500 for most models and 100 for `swin_base`.\n\n**Calibration Data**: ImageNet dataset, using a calibration batch size (`calib_batchsize`) of 1024.\n\n**Quantizer Initialization**: Scales and zero points for `UniformQuantizer` are determined by searching a range of percentiles (`[0.999, 0.9999, 0.99999]` for activations, `[0.97, ..., 1]` for weights) to minimize L2 loss."
      }
    },
    {
      "title": "StepbaQ: Stepping backward as Correction for Quantized Diffusion Models"
    },
    {
      "title": "Gradient $\\ell_1$ Regularization for Quantization Robustness",
      "abstract": "We analyze the effect of quantizing weights and activations of neural\nnetworks on their loss and derive a simple regularization scheme that improves\nrobustness against post-training quantization. By training quantization-ready\nnetworks, our approach enables storing a single set of weights that can be\nquantized on-demand to different bit-widths as energy and memory requirements\nof the application change. Unlike quantization-aware training using the\nstraight-through estimator that only targets a specific bit-width and requires\naccess to training data and pipeline, our regularization-based method paves the\nway for \"on the fly'' post-training quantization to various bit-widths. We show\nthat by modeling quantization as a $\\ell_\\infty$-bounded perturbation, the\nfirst-order term in the loss expansion can be regularized using the\n$\\ell_1$-norm of gradients. We experimentally validate the effectiveness of our\nregularization scheme on different architectures on CIFAR-10 and ImageNet\ndatasets.",
      "full_text": "Published as a conference paper at ICLR 2020 GRADIENT ℓ1 REGULARIZATION FOR QUANTIZATION ROBUSTNESS Milad Alizadeh∗2,1, Arash Behboodi1, Mart van Baalen1, Christos Louizos1, Tijmen Blankevoort1, and Max Welling1 1Qualcomm AI Research† Qualcomm Technologies Netherlands B.V . {behboodi,mart,clouizos,tijmen,mwelling}@qti.qualcomm.com 2University of Oxford milad.alizadeh@cs.ox.ac.uk ABSTRACT We analyze the effect of quantizing weights and activations of neural networks on their loss and derive a simple regularization scheme that improves robust- ness against post-training quantization. By training quantization-ready networks, our approach enables storing a single set of weights that can be quantized on- demand to different bit-widths as energy and memory requirements of the ap- plication change. Unlike quantization-aware training using the straight-through estimator that only targets a speciﬁc bit-width and requires access to training data and pipeline, our regularization-based method paves the way for “on the ﬂy” post- training quantization to various bit-widths. We show that by modeling quantiza- tion as a ℓ∞-bounded perturbation, the ﬁrst-order term in the loss expansion can be regularized using the ℓ1-norm of gradients. We experimentally validate the ef- fectiveness of our regularization scheme on different architectures on CIFAR-10 and ImageNet datasets. 1 I NTRODUCTION Deep neural networks excel across a variety of tasks, but their size and computational requirements often hinder their real-world deployment. The problem is more challenging for mobile phones, embedded systems, and IoT devices, where there are stringent requirements in terms of memory, compute, latency, and energy consumption. Quantization of parameters and activations is often used to reduce the energy and computational requirements of neural networks. Quantized neural networks allow for more speed and energy efﬁciency compared to ﬂoating-point models by using ﬁxed-point arithmetic. However, naive quantization of pre-trained models often results in severe accuracy degradation, especially when targeting bit-widths below eight (Krishnamoorthi, 2018). Performant quantized models can be obtained via quantization-aware training or ﬁne-tuning, i.e., learning full-precision shadow weights for each weight matrix with backpropagation using the straight-through estimator (STE) (Bengio et al., 2013), or using other approximations (Louizos et al., 2018). Alternatively, there have been successful attempts to recover the lost model accuracy without requiring a training pipeline (Banner et al., 2018; Meller et al., 2019; Choukroun et al., 2019; Zhao et al., 2019) or representative data (Nagel et al., 2019). But these methods are not without drawbacks. The shadow weights learned through quantization- aware ﬁne-tuning often do not show robustness when quantized to bit-widths other than the one they were trained for (see Table 1). In practice, the training procedure has to be repeated for each quantization target. Furthermore, post-training recovery methods require intimate knowledge of the relevant architectures. While this may not be an issue for the developers training the model in the ﬁrst ∗Work done during internship at Qualcomm AI Research †Qualcom AI Research is an initiative of Qualcomm Technologies, Inc. 1 arXiv:2002.07520v1  [cs.LG]  18 Feb 2020Published as a conference paper at ICLR 2020 place, it is a difﬁcult step for middle parties that are interested in picking up models and deploying them to users down the line, e.g., as part of a mobile app. In such cases, one might be interested in automatically constraining the computational complexity of the network such that it conforms to speciﬁc battery consumption requirements, e.g. employ a 4-bit variant of the model when the battery is less than 20% but the full precision one when the battery is over 80%. Therefore, a model that can be quantized to a speciﬁc bit-width “on the ﬂy” without worrying about quantization aware ﬁne-tuning is highly desirable. In this paper, we explore a novel route, substantially different from the methods described above. We start by investigating the theoretical properties of noise introduced by quantization and analyze it as a ℓ∞-bounded perturbation. Using this analysis, we derive a straightforward regularization scheme to control the maximum ﬁrst-order induced loss and learn networks that are inherently more robust against post-training quantization. We show that applying this regularization at the ﬁnal stages of training, or as a ﬁne-tuning step after training, improves post-training quantization across different bit-widths at the same time for commonly used neural network architectures. 2 F IRST -ORDER QUANTIZATION -ROBUST MODELS In this section, we propose a regularization technique for robustness to quantization noise. We ﬁrst propose an appropriate model for quantization noise. Then, we show how we can effectively control the ﬁrst-order, i.e., the linear part of the output perturbation caused by quantization. When the linear approximation is adequate, our approach guarantees the robustness towards various quantization bit-widths simultaneously. We use the following notation throughout the paper. The ℓp-norm of a vector x in Rn is denoted by ∥x∥p and deﬁned as ∥x∥p := (∑n i=1 |xi|p)1/p for p ∈[1,∞). At its limit we obtain the ℓ∞-norm deﬁned by ∥x∥∞:= maxi|xi|. The inner product of two vectors x and y is denoted by ⟨x,y⟩. 2.1 R OBUSTNESS ANALYSIS UNDER ℓp-BOUNDED ADDITIVE NOISE The error introduced by rounding in the quantization operation can be modeled as a generic additive perturbation. Regardless of which bit-width is used, the quantization perturbation that is added to each value has bounded support, which is determined by the width of the quantization bins. In other words, the quantization noise vector of weights and activations in neural networks has entries that are bounded. Denote the quantization noise vector by ∆. If δ is the width of the quantization bin, the vector ∆ satisﬁes ∥∆∥∞≤δ/2. Therefore we model the quantization noise as a perturbation bounded in the ℓ∞-norm. A model robust to ℓ∞-type perturbations would also be robust to quantization noise. To characterize the effect of perturbations on the output of a function, we look at its tractable ap- proximations. To start, consider the ﬁrst-order Taylor-expansion of a real valued-functionf(w+∆) around w: f(w + ∆) = f(w) + ⟨∆,∇f(w)⟩+ R2, (1) where R2 refers to the higher-order residual error of the expansion. We setR2 aside for the moment and consider the output perturbation appearing in the ﬁrst-order term ⟨∆,∇f(w)⟩. The maximum of the ﬁrst-order term among all ℓ∞-bounded perturbations ∆ is given by: max ∥∆∥∞≤δ ⟨∆,∇f(w)⟩= δ∥∇f(w)∥1 . (2) To prove this, consider the inner product of∆ and an arbitrary vector x given by ∑n i=1 nixi. Since |ni|is assumed to be bounded by δ, each nixi is bounded by δ|xi|, which yields the result. The maximum in Equation 2 is obtained indeed by choosing ∆ = δsign(∇f(w)). Equation 2 comes with a clear hint. We can guarantee that the ﬁrst-order perturbation term is small if the ℓ1-norm of the gradient is small. In this way, the ﬁrst-order perturbation can be controlled efﬁciently for various values of δ, i.e. for various quantization bit-widths. In other words, an ef- fective way for controlling the quantization robustness, up to ﬁrst-order perturbations, is to control the ℓ1-norm of the gradient. As we will shortly argue, this approach yields models with the best robustness. 2Published as a conference paper at ICLR 2020 0 5 10 15 20 25 2-norms of gradients 500 1000 1500 2000 2500 1-norms of gradients Baseline Network Regularized Network Figure 1: ℓ1- and ℓ2-norms of the gradients for CIFAR-10 test-set mini-batches. Note the differ- ence between the scales on the horizontal and ver- tical axis. We observe that our regularization term decreases the ℓ1-norm signiﬁcantly, compared to its unregularized counterpart. 10 4  10 3  10 2  10 1  100 101 KL Baseline 10 4 10 3 10 2 10 1 100 101 KL Regularized Quantization config (6, 6) Quantization config (5, 5) Quantization config (4, 4) Figure 2: KL-divergence of the ﬂoating point predictive distribution to the predictive distribu- tion of the quantized model for CIFAR-10 test- set mini-batches. We observe that the regulariza- tion leads to a smaller gap, especially for smaller bit- widths. This conclusion is based on worst-case analysis since it minimizes the upper bound of the ﬁrst-order term, which is realized by the worst-case perturbation. Its advantage, however, lies in simultaneous control of the output perturbation for allδs and all input perturbations. In the context of quantization, this implies that the ﬁrst-order robustness obtained in this way would hold regardless of the adopted quantization bit-width or quantization scheme. The robustness obtained in this way would persist even if the perturbation is bounded in other ℓp- norms. This is because the set of ℓ∞-bounded perturbations includes all other bounded perturba- tions, as for all p ∈ [1,∞), ∥x∥p ≤ δ implies ∥x∥∞ ≤ δ (see Figure 8) . The robustness to ℓ∞-norm perturbations is, therefore, the most stringent one among otherℓp-norms, because a model should be robust to a broader set of perturbations. Controlling theℓ1-norm of the gradient guarantees robustness to ℓ∞-perturbations and thereby to all other ℓp-bounded perturbations. In what follows, we propose regularizing the ℓ1-norm of the gradient to promote robustness to bounded norm perturbations and in particular bounded ℓ∞-norm perturbations. These perturbations arise from quantization of weights and activations of neural networks. 2.2 R OBUSTNESS THROUGH REGULARIZATION OF THE ℓ1-NORM OF THE GRADIENT We focused on weight quantization in our discussions so far, but we can equally apply the same arguments for activation quantization. Although the activations are not directly learnable, their quantization acts as an additive ℓ∞-bounded perturbation on their outputs. The gradient of these outputs is available. It therefore sufﬁces to accumulate all gradients along the way to form a large vector for regularization. Suppose that the loss function for a deep neural network is given byLCE(W,Y; x) where W denotes the set of all weights, Y denotes the set of outputs of each activation andx the input. We control the ℓ1-norm of the gradient by adding the regularization term∑ Wl∈W ∥∇Wl LCE(W,Y; x)∥1 + ∑ yl∈Y ∥∇yl LCE(W,Y; x)∥1 to the loss, yielding an optimization target L(W; x) = LCE(W,Y; x) +λw ∑ Wl,∈W ∥∇Wl LCE(W,Y; x)∥1 + λy ∑ yl∈Y ∥∇yl LCE(W,Y; x)∥1 , (3) where λw and λy are weighing hyper-parameters. 3Published as a conference paper at ICLR 2020 1 5 10 15 20 Batch Number 0.10 0.05 0.00 0.05 0.10 0.15 0.20 0.25 Introduced Loss Introduced loss First-order prediction Figure 3: Predicting induced loss using ﬁrst-order terms. We added ℓ∞-bounded noise with δcorrespond- ing to 4-bit quantization to all weights of ResNet-18 and compared the induced loss on the CIFAR-10 test-set with the predictions using gradients. While not perfect, the ﬁrst-order term is not insigniﬁcant. 2.3 A LTERNATIVES TO THE ℓ1-REGULARIZATION The equivalence of norms in ﬁnite-dimensional normed spaces implies that all norms are within a constant factor of one another. Therefore, one might suggest regularizing any norm to control other norms. Indeed some works attempted to promote robustness to quantization noise by controlling the ℓ2-norm of the gradient (Hoffman et al., 2019). However, an argument related to the curse of dimensionality can show why this approach will not work. The equivalence of norms for ℓ1 and ℓ2 in n-dimensional space is stated by the inequality: ∥x∥2 ≤∥x∥1 ≤√n∥x∥2 . Although the ℓ2-norm bounds the ℓ1-norm from above, it is vacuous if it does not scale with 1/√n. Imposing such a scaling is demanding when n, which is the number of trainable parameters, is large. Figure 1 shows that there is a large discrepancy between these norms in a conventionally trained network, and therefore small ℓ2-norm does not adequately control the ℓ1-norm. A very similar argument can be provided from a theoretical perspective (see the supplementary materials). To guarantee robustness, the ℓ2-norm of the gradient, therefore, should be pushed as small as Θ(1/√n). We experimentally show in Section 4 that this is a difﬁcult task. We therefore directly control the ℓ1-norm in this paper. Note that small ℓ1-norm is guaranteed to control the ﬁrst order- perturbation for all types of quantization noise with bounded support. This includes symmetric and asymmetric quantization schemes. Another concern is related to the consistency of the ﬁrst-order analysis. We neglected the residual term R2 in the expansion. Figure 3 compares the induced loss after perturbation with its ﬁrst-order approximation. The approximation shows a strong correlation with the induced loss. We will see in the experiments that the quantization robustness can be boosted by merely controlling the ﬁrst-order term. Nonetheless, a higher-order perturbation analysis can probably provide better approximations. Consider the second-order perturbation analysis: f(w + ∆) = f(w) + ⟨∆,∇f(w)⟩+ 1 2∆T∇2f(w)∆ + R3. Computing the worst-case second-order term forℓ∞-bounded perturbations is hard. Even for convex functions where ∇2f(w) is positive semi-deﬁnite, the problem of computing worst-case second- order perturbation is related to the mixed matrix-norm computation, which is known to be NP- hard. There is no polynomial-time algorithm that approximates this norm to some ﬁxed relative precision (Hendrickx & Olshevsky, 2010). For more discussions, see the supplementary materials. It is unclear how this norm should be controlled via regularization. 3 R ELATED WORK A closely related line of work to ours is the analysis of the robustness of the predictions made by neural networks subject to an adversarial perturbation in their input. Quantization can be seen as a similar scenario where non-adversarial perturbations are applied to weights and activations instead. Cisse et al. (2017) proposed a method for reducing the network’s sensitivity to small perturbations 4Published as a conference paper at ICLR 2020 by carefully controlling its global Lipschitz. The Lipschitz constant of a linear layer is equal to the spectral norm of its weight matrix, i.e., its largest singular value. The authors proposed regularizing weight matrices in each layer to be close to orthogonal: ∑ Wl∈W WT l Wl −I 2 . All singular values of orthogonal matrices are one; therefore, the operator does not amplify perturbation (and input) in any direction. Lin et al. (2019) studied the effect of this regularization in the context of quantized networks. The authors demonstrate the extra vulnerability of quantized models to adversarial attacks and show how this regularization, dubbed “Defensive Quantization”, improves the robustness of quantized networks. While the focus of Lin et al. (2019) is on improving the adversarial robustness, the authors report limited results showing accuracy improvements of post- training quantization. The idea of regularizing the norm of the gradients has been proposed before (Gulrajani et al., 2017) in the context of GANs, as another way to enforce Lipschitz continuity. A differentiable function is 1-Lipschitz if and only if it has gradients with ℓ2-norm of at most 1 everywhere, hence the authors penalize the ℓ2-norm of the gradient of the critic with respect to its input. This approach has a major advantage over the methods mentioned above. Using weight regularization is only well-deﬁned for 2D weight matrices such as in fully-connected layers. The penalty term is often approximated for convolutional layers by reshaping the weight kernels into 2D matrices. Sedghi et al. (2018) showed that the singular values found in this weight could be very different from the actual operator norm of the convolution. Some operators, such as nonlinearities, are also ignored. Regularizing Lipschitz constant through gradients does not suffer from these shortcomings, and the operator-norm is reg- ularized directly. Guo et al. (2018) demonstrated that there exists an intrinsic relationship between sparsity in DNNs and their robustness against ℓ∞and ℓ2 attacks. For a binary linear classiﬁer, the authors showed that they could control the ℓ∞robustness, and its relationship with sparsity, by reg- ularizing the ℓ1 norm of the weight tensors. In the case of a linear classiﬁer, this objective is, in fact, equivalent to our proposed regularization penalty. Finally, another line of work related to ours revolves around quantization-aware training. This can, in general, be realized in two ways: 1) regularization and 2) mimicking the quantization procedure during the forward pass of the model. In the ﬁrst case, we have methods (Yin et al., 2018; Achter- hold et al., 2018) where there are auxiliary terms introduced in the objective function such that the optimized weights are encouraged to be near, under some metric, to the quantization grid points, thus alleviating quantization noise. In the second case, we have methods that rely on either the STE (Courbariaux et al., 2015; Rastegari et al., 2016; Jacob et al., 2018), stochastic rounding (Gupta et al., 2015; Gysel, 2016), or surrogate objectives and gradients (Louizos et al., 2018; Shayer et al., 2017). While all of the methods above have been effective, they still suffer from a major limitation; they target one-speciﬁc bit-width. In this way, they are not appropriate for use-cases where we want to be able to choose the bit-width “on the ﬂy”. 4 E XPERIMENTS In this section we experimentally validate the effectiveness of our regularization method on im- proving post-training quantization. We use the well-known classiﬁcation tasks of CIFAR-10 with ResNet-18 (He et al., 2016) and VGG-like (Simonyan & Zisserman, 2014) and of ImageNet with ResNet-18. We compare our results for various bit-widths against (1) unregularized baseline networks (2) Lipschitz regularization methods (Lin et al., 2019; Gulrajani et al., 2017) and (3) quantization-aware ﬁne-tuned models. Note that Gulrajani et al. (2017) control the Lipschitz con- stant under an ℓ2 metric by explicitly regularizing theℓ2-norm of the gradient, while Lin et al. (2019) essentially control an upper bound on the ℓ2-norm of the gradient. Comparing against these base- lines thus gives insight into how our method of regularizing the ℓ1-norm of the gradient compares against regularization of the ℓ2-norm of the gradient. 4.1 E XPERIMENTAL SETUP Implementation and complexity Adding the regularization penalty from Equation 3 to the train- ing objective requires higher-order gradients. This feature is available in the latest versions of frame- works such as Tensorﬂow and PyTorch (of which we have used the latter for all our experiments). Computing ∇w∥∇wL∥1 using automatic differentiation requires O(2 ×C×E) extra computations, where E is the number of elementary operations in the original forward computation graph, and C 5Published as a conference paper at ICLR 2020 (8,FP) (7,FP) (6,FP) (5,FP) (4,FP) Quantization Config 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Accuracy Baseline Regularized (a) (8,4) (7,4) (6,4) (5,4) (4,4) Quantization Config 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Accuracy Baseline Regularized (b) Figure 4: Accuracy of regularized VGG-like after post-training quantization. We trained 5 models with different initializations and show the mean accuracy for each quantization conﬁguration. The error bars indicate min/max observed accuracies. (a) Weight-only quantization (b) Activation quantization ﬁxed to 4-bits is a ﬁxed constant (Baydin et al., 2018). This can be seen from the fact that ∥∇wL∥1 is a function R|w|→R, where |w|denotes the number of weights and the computation of the gradient w.r.t. the loss contains E elementary operations, as many as the forward pass. In practice, enabling regular- ization increased time-per-epoch time on CIFAR10 from 14 seconds to 1:19 minutes for VGG, and from 24 seconds to 3:29 minutes for ResNet-18. On ImageNet epoch-time increased from 33:20 minutes to 4:45 hours for ResNet-18. The training was performed on a single NVIDIA RTX 2080 Ti GPU. However, in our experiments we observed that it is not necessary to enable regularization from the beginning, as the ℓ1-norm of the gradients decreases naturally up to a certain point as the training progresses (See Appendix D for more details). We therefore only enable regularization in the last 15 epochs of training or as an additional ﬁne-tuning phase. We experimented with tuning λw and λy in Equation 3 separately but found no beneﬁt. We therefore set λw = λy = λfor the remainder of this section. We use a grid-search to ﬁnd the best setting for λ. Our search criteria is ensuring that the perfor- mance of the unquantized model is not degraded. In order to choose a sensible range of values we ﬁrst track the regularization and cross-entropy loss terms and then choose a range of λthat ensures their ratios are in the same order of magnitude. We do not perform any quantization for validation purposes during the training. Quantization details We use uniform symmetric quantization (Jacob et al., 2018; Krishnamoorthi, 2018) in all our experiments unless explicitly speciﬁed otherwise. For the CIFAR 10 experiments we ﬁx the activation bit-widths to 4 bits and then vary the weight bits from 8 to 4. For the Imagenet experiments we use the same bit-width for both weights and activations. For the quantization-aware ﬁne-tuning experiments we employ the STE on a ﬁxed (symmetric) quantization grid. In all these experiments we perform a hyperparameter search over learning rates for each of the quantization bit-widths and use a ﬁxed weight decay of 1e−4. For our experiments with defensive quantization (Lin et al., 2019) we perform a hyperparameter search over the scaling parameters of the regularizer and the learning rate. We limit the search over the scaling parameters to those mentioned in (Lin et al., 2019) and do not use weight decay. When applying post-training quantization we set the activation ranges using the batch normalization parameters as described in (Nagel et al., 2019). When a model is ﬁne-tuned to a target bit-width and evaluated on a higher bit-width, we can trivially represent the original quantized weights and activations by ignoring the higher-order bits, or quantize using the higher bit-width. As using the higher bit-width to quantize shadow weights and activations introduces noise to the model and might yield lower results, we try both approaches and only report a result if quantization using the higher bit-width gives better results. 6Published as a conference paper at ICLR 2020 Figure 5: Random cross sections of decision boundaries in the input space. To generate these cross- sections, we draw a random example from the CIFAR-10 test set (represented by the black dot in the center) and pass a random two-dimensional hyper-plane⊂R1024 through it. We then evaluate the network’s output for each point on the hyper-plane. Various colors indicate different classes. Softmax’s maximum values determine the contours. The top row illustrates the difference between the baseline and the regularized VGG-like networks (and their quantized variants) when they all classify an example correctly. The bottom row depicts a case where the quantized baseline misclassiﬁes an example while the regularized network predicts the correct class. We can see that our regularization pushes the decision boundaries outwards and enlarges the decision cells. 4.2 E FFECTS OF REGULARIZATION In order to get a better understanding of our proposed regularizer, we ﬁrst adopt the visualization method from Hoffman et al. (2019) and illustrate the effects that the quantization in general, and our method in particular, have on the trained classiﬁer’s decision boundaries. The result can be seen in Figure 5, where we empirically observe that the regularized networks “expands” its decision cells. Secondly, we investigate in Figure 1 the ℓ1- and ℓ2-norms of the gradients for all CIFAR-10 test batches on the VGG-like model. We can observe that while the ℓ2-norms of the gradient are small in the unregularized model, the ℓ1-norms are orders of magnitude larger. Consequently, when ﬁne- tuning the same model with our method, we see a strong decrease of the ℓ1-norm. Finally, we investigate how the predictive distribution of the ﬂoating point model, p(y|x), changes when we quantize either an unregularized baseline or a model regularized with our method, thus obtaining q(y|x). We measure this discrepancy using the KL-divergence of the original predictive when using the predictive distribution of the quantized model, i.e. DKL(p(y|x)||q(y|x)), averaged over each test batch. Since our method improves robustness of the loss gradient against small per- turbations, we would expect the per-class probabilities to be more robust to perturbations as well, and thus more stable under quantization noise. The result can be seen in Figure 2, where we indeed observe that the gap is smaller when quantizing our regularized model. 4.3 CIFAR-10 & I MAGENET RESULTS The classiﬁcation results from our CIFAR-10 experiments for the VGG-like and ResNet18 networks are presented in Table 1, whereas the result from our Imagenet experiments for the ResNet18 net- work can be found in Table 2. Both tables include all results relevant to the experiment, including results on our method, Defensive Quantization regularization, L2 gradient regularization and ﬁne- tuning using the STE. Comparison to “Defensive Quantization” As explained in Section 3, Defensive Quantization (Lin et al., 2019) aims to regularize each layer’s Lipschitz constant to be close to 1. Since the 7Published as a conference paper at ICLR 2020 VGG-like ResNet-18 FP (8,4) (6,4) (4,4) FP (8,4) (6,4) (4,4) No Regularization 92.49 79.10 78.84 11.47 93.54 85.51 85.35 83.98 DQ Regularization 91.51 86.30 84.29 30.86 92.46 83.31 83.34 82.47 L2 Regularization 91.88 86.64 86.14 63.93 93.31 84.50 84.99 83.82 L1 Regularization (Ours) 92.63 89.74 89.78 85.99 93.36 88.70 88.45 87.62 STE @ (8,4) – 91.28 89.99 32.83 – 89.10 87.79 86.21 STE @ (6,4) – – 90.25 39.56 – – 90.77 88.17 STE @ (4,4) – – – 89.79 – – – 89.98 Table 1: Test accuracy (%) for the VGG-like and ResNet-18 models on CIFAR-10 . STE @ (X,X) indicates the weight-activation quantization conﬁguration used with STE for ﬁne-tuning. DQ denotes Defensive Quantization (Lin et al., 2019). For the No Regularization row of results we only report the mean of 5 runs. The full range of the runs is shown in Figure 4. Conﬁguration FP (8,8) (6,6) (4,4) No Regularization 69.70 69.20 63.80 0.30 DQ Regularization 68.28 67.76 62.31 0.24 L2 Regularization 68.34 68.02 64.52 0.19 L1 Regularization (Ours) 70.07 69.92 66.39 0.22 L1 Regularization (Ours) (λ= 0.05) 64.02 63.76 61.19 55.32 STE @ (8,8) – 70.06 60.18 0.13 STE @ (6,6) – – 69.63 11.34 STE @ (4,4) – – – 57.50 Table 2: Test accuracy for the ResNet-18 architecture on ImageNet. STE @ (X,X) indicates the weight-activation quantization conﬁguration used with STE for ﬁne-tuning. In addition to the λwe found through the grid-search which maintains FP accuracy, we also experimented with a stronger λ= 0.05 to show that (4,4) accuracy can be recovered at the price of overall lower performance. regularization approach taken by the authors is similar to our method, and the authors suggest that their method can be applied as a regularization for quantization robustness, we compare their method to ours. As the experiments from the original paper differ methodologically from ours in that we quantize both weights and activations, all results on defensive quantization reported in this paper are produced by us. We were able to show improved quantization results using defensive quantization for CIFAR-10 on VGG-like, but not on any of the experiments on ResNet18. We attribute this behavior to too stringent regularization in their approach: the authors regularize all singular values of their (reshaped) convolutional weight tensors to be close to one, using a regularization term that is essentially a fourth power regularization of the singular values of the weight tensors (see Appendix C). This regularization likely inhibits optimization. Comparison to explicit ℓ2-norm gradient regularization We consider the ℓ2 regularization of the gradient, as proposed by Gulrajani et al. (2017), as a generalization of the DQ regularization. Such regularization has two key beneﬁts over DQ: 1) we can regularize the singular values without reshaping the convolutional kernels and 2) we impose a less stringent constraint as we avoid enforc- ing all singular values to be close to one. By observing the results at Table 1 and 2, we see that the ℓ2 regularization indeed improves upon DQ. Nevertheless, it provides worse results compared to our ℓ1 regularization, an effect we can explain by the analysis of Section 2. Comparison to quantization-aware ﬁne-tuning While in general we cannot expect our method to outperform models to which quantization-aware ﬁne-tuning is applied on their target bit-widths, as in this case the model can adapt to that speciﬁc quantization noise, we do see that our model performs on par or better when comparing to bit-widths lower than the target bit-width. This is in line with our expectations: the quantization-aware ﬁne-tuned models are only trained to be robust to a speciﬁc noise distribution. However, our method ensures ﬁrst-order robustness regardless of bit- 8Published as a conference paper at ICLR 2020 width or quantization scheme, as explained in Section 2. The only exception is the 4 bit results on ImageNet. We hypothesize that this is caused by the fact that we tune the regularization strengthλto the highest value that does not hurt full-precision results. While stronger regularization would harm full-precision performance, it would also most likely boost 4 bit results, due to imposing robustness to a larger magnitude, i.e. δ, of quantization noise. Table 1 includes results for a higher value of δ that is in line with this analysis. 5 C ONCLUSION In this work, we analyzed the effects of the quantization noise on the loss function of neural net- works. By modelling quantization as an ℓ∞-bounded perturbation, we showed how we can con- trol the ﬁrst-order term of the Taylor expansion of the loss by a straightforward regularizer that encourages the ℓ1-norm of the gradients to be small. We empirically conﬁrmed its effectiveness, demonstrating that standard post-training quantization to such regularized networks can maintain good performance under a variety of settings for the bit-width of the weights and activations. As a result, our method paves the way towards quantizing ﬂoating-point models “on the ﬂy” according to bit-widths that are appropriate for the resources currently available. ACKNOWLEDGMENTS We would like to thank Markus Nagel, Rana Ali Amjad, Matthias Reisser, and Jakub Tomczak for their helpful discussions and valuable feedback. REFERENCES Jan Achterhold, Jan Mathias Koehler, Anke Schmeink, and Tim Genewein. Variational network quantization. International Conference on Learning Representations, 2018. R Banner, Y Nahshan, E Hoffer, and D Soudry. Post training 4-bit quantization of convolution networks for rapid-deployment. CoRR, abs/1810.05723, 1:2, 2018. Atilim Gunes Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. Automatic differentiation in machine learning: a survey. Journal of machine learning research, 18(153), 2018. Yoshua Bengio, Nicholas L ´eonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013. Vijay Bhattiprolu, Mrinalkanti Ghosh, Venkatesan Guruswami, Euiwoong Lee, and Madhur Tul- siani. Inapproximability of matrix p→qnorms. arXiv preprint arXiv:1802.07425, 2018. Yoni Choukroun, Eli Kravchik, and Pavel Kisilev. Low-bit quantization of neural networks for efﬁcient inference. arXiv preprint arXiv:1902.06822, 2019. Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval networks: Improving robustness to adversarial examples. InProceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 854–863. JMLR. org, 2017. Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during propagations. In Advances in neural information processing systems, pp. 3123–3131, 2015. Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im- proved training of wasserstein gans. In Advances in neural information processing systems, pp. 5767–5777, 2017. Yiwen Guo, Chao Zhang, Changshui Zhang, and Yurong Chen. Sparse dnns with improved adver- sarial robustness. In Advances in neural information processing systems, pp. 242–251, 2018. 9Published as a conference paper at ICLR 2020 Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited numerical precision. In International Conference on Machine Learning, pp. 1737–1746, 2015. Philipp Gysel. Ristretto: Hardware-oriented approximation of convolutional neural networks. arXiv preprint arXiv:1605.06402, 2016. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog- nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. Julien M. Hendrickx and Alex Olshevsky. Matrix p-Norms Are NP-Hard to Approximate If p ̸= 1,2,∞. SIAM Journal on Matrix Analysis and Applications, 31(5):2802–2812, 2010. Wassily Hoeffding. Probability Inequalities for Sums of Bounded Random Variables.Journal of the American Statistical Association, 58(301):13–30, March 1963. Judy Hoffman, Daniel A. Roberts, and Sho Yaida. Robust Learning with Jacobian Regularization. arXiv:1908.02729 [cs, stat], August 2019. arXiv: 1908.02729. Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efﬁ- cient integer-arithmetic-only inference. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efﬁcient inference: A whitepaper. arXiv preprint arXiv:1806.08342, 2018. Ji Lin, Chuang Gan, and Song Han. Defensive quantization: When efﬁciency meets robustness. arXiv preprint arXiv:1904.08444, 2019. Christos Louizos, Matthias Reisser, Tijmen Blankevoort, Efstratios Gavves, and Max Welling. Re- laxed quantization for discretized neural networks. arXiv preprint arXiv:1810.01875, 2018. Eldad Meller, Alexander Finkelstein, Uri Almog, and Mark Grobman. Same, same but different - recovering neural network quantization error through weight factorization. In International Conference on Machine Learning, 2019. Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free quantization through weight equalization and bias correction. arXiv preprint arXiv:1906.04721, 2019. Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classiﬁcation using binary convolutional neural networks. In European Conference on Computer Vision, pp. 525–542. Springer, 2016. Hanie Sedghi, Vineet Gupta, and Philip M Long. The singular values of convolutional layers.arXiv preprint arXiv:1805.10408, 2018. Oran Shayer, Dan Levi, and Ethan Fetaya. Learning discrete weights using the local reparameteri- zation trick. arXiv preprint arXiv:1710.07739, 2017. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. Penghang Yin, Shuai Zhang, Jiancheng Lyu, Stanley Osher, Yingyong Qi, and Jack Xin. Bina- ryrelax: A relaxation approach for training deep neural networks with quantized weights. SIAM Journal on Imaging Sciences, 11(4):2205–2223, 2018. Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, and Zhiru Zhang. Improving neural network quantization without retraining using outlier channel splitting. In International Conference on Machine Learning, pp. 7543–7552, 2019. 10Published as a conference paper at ICLR 2020 /2  0 /2 0.0 0.2 0.4 0.6 0.8 1.0 Empirical quantization perturbation distribution Figure 6: Quantization noise is uniformly distributed. In this plot we show the quantization noise on each individual weight in an ImageNet trained ResNet18 model. The noise is scaled by the width of the quantization bin for each weight quantizer. This plot shows that quantization noise is uniformly distributed between −δ/2 and δ/2. A R OBUSTNESS ANALYSIS FOR QUANTIZATION PERTURBATIONS In this section, we address two questions in more details, ﬁrst regarding regularization of theℓ2-norm of gradient and second regarding non-uniform quantization schemes. We argued above that regularizing the ℓ2-norm of gradient cannot achieve the same level of ro- bustness as regularization of the ℓ1-norm of gradient. We provide here another, more theoretical, argument. The following inequality shows how the ℓ2-norm of gradient controls the ﬁrst-order per- turbation: ⟨∆,∇f(w)⟩≤∥ ∆∥2 ∥∇f(w)∥2 . This is a simple Cauchy-Shwartz inequality. Therefore, if the ℓ2-norm of the gradient is inversely proportional to the power of the perturbation, the ﬁrst-order term is adequately controlled. However, using a theoretical argument, we show that the power of the ℓ∞-bounded perturbation can blow up with the dimension as a vector ∆ in Rn with ∥∆∥∞ = δ can reach an ℓ2-norm of approximately√nδ. In other words, the length of the quantization noise behaves with high probability as Θ(√n), which implies that the the ℓ2-norm of the gradient should be as small as Θ(1/√n). We show that this can indeed occur with high probability for any random quantization noise with the bounded support. Note that for symmetric uniform quantization schemes, quantization noise can be approximated well by a uniform distribution over [−δ/2,δ/2] where δ is the width of the quantization bin. See Figures 6 for the empirical distribution of quantization noise on the weights of a trained network. Our argument, however, works for any distribution supported over [−δ/2,δ/2], and, therefore, it includes asymmetric quantization schemes over a uniform quantization bin. Consider a vector x = ( x1,...,x n)T ∈Rn with entries xi randomly and independently drawn from a distribution supported on [−δ/2,δ/2]. We would like to show that∥x∥2 2 is well concentrated around its expected values. To do that we are going to write down the above norm as the sum of independent zero-mean random variables. See that: E ( ∥x∥2 2 ) = E ( n∑ i=1 x2 i ) = nE ( x2 1 ) = nδ2 12 . Besides, note that x2 i ∈[0,δ2/4]. Therefore x2 i −δ2/12 is a zero-mean random variable that lies in the interval [−δ2/12,δ2/6]. We can now use Hoeffding’s inequality. To be self-contained, we include the theorem below. Theorem A.1 (Hoeffding’s inequality, (Hoeffding, 1963)). Let X1,...,X n be a sequence of in- dependent zero-mean random variables such thatXi is almost surely supported on[ai,bi] for 11Published as a conference paper at ICLR 2020 i∈{1,...,n }. Then, for allt> 0, it holds that P ( n∑ i=1 Xi ≥t ) ≤exp ( − 2t2 ∑n i=1(bi −ai)2 ) (4) P (⏐⏐⏐⏐⏐ n∑ i=1 Xi ⏐⏐⏐⏐⏐≥t ) ≤2 exp ( − t2 ∑n i=1(bi −ai)2 ) (5) Applying Theorem A.1 to our setting, we obtain: P (⏐⏐⏐∥x∥2 2 −nδ2/12 ⏐⏐⏐≥t ) ≤2 exp ( − 2t2 n(δ2/4)2 ) . So with probability 1 −ϵ, we have: ⏐⏐⏐∥x∥2 2 −nδ2/12 ⏐⏐⏐≤ (nδ4 32 log(2/ϵ) )1/2 . Therefore, if the quantization noise ∆ has entries randomly drawn from a distribution over [−δ/2,δ/2], then with probability 1 −ϵ, the squared ℓ2-norm of ∆, i.e., ∥∆∥2 2, lies in the interval[ nδ2 12 − √ nδ4 32 log(2/ϵ),nδ2 12 + √ nδ4 32 log(2/ϵ) ] . In other words, the length of the vector behaves with high probability as Θ(√n). This result holds for any quantization noise with bounded support. If the quantization bins are non-uniformly chosen, and if the weights can take arbitrarily large val- ues, the quantization noise is no-longer bounded in general. As long as the quantization noise has a Gaussian tail, i.e., it is a subgaussian random variable, one can use Hoeffding’s inequality for subgaussian random variables to show a similar concentration result as above. The power of the perturbation will, therefore, behave with Θ(√n), and the ℓ2-norm of the gradient cannot effectively control the gradient. Note that nonuniform quantization schemes are not commonly used for hard- ware implementations, hence, our focus on uniform cases. Besides, the validity of this assumption about nonuniform quantization noise requires further investigation, which is relegated to our future works. B S ECOND -ORDER PERTURBATION ANALYSIS We start by writing the approximation of f(·) up to the second-order term: f(w + ∆) = f(w) + ⟨∆,∇f(w)⟩+ 1 2∆T∇2f(w)∆ + R3. The worst-case second-order term under ℓ∞-bounded perturbations is given by max ∥n∥∞≤δ ∆T∇2f(w)∆. The above value is difﬁcult to quantify for general case. We demonstrate this difﬁculty by consider- ing some special cases. Let’s start with convex functions, for which the Hessian ∇2f(w) is positive semi-deﬁnite. In this case, the Hessian matrix admits a square root, and the second-order term can be written as: ∆T∇2f(w)∆ = ∆T(∇2f(w))1/2(∇2f(w))1/2∆ = (∇2f(w))1/2∆  2 2 . Therefore the worst-case analysis of the second-term amounts to max ∥n∥∞≤δ ∆T∇2f(w)∆ = max ∥n∥∞≤δ (∇2f(w))1/2∆  2 2 . The last term is the mixed ∞→ 2-norm of (∇2f(w))1/2.As a reminder, the p→q-matrix norm is deﬁned as ∥A∥p→q := max ∥x∥p≤1 ∥A∥q = max ∥x∥p≤1 ∥y∥q∗≤1 ⟨y,Ax⟩=: AT q∗→p∗ 12Published as a conference paper at ICLR 2020 where p∗,q∗denote the dual norms of pand q, i.e. satisfying 1/p+ 1/p∗= 1/q+ 1/q∗= 1. The worst case second-order perturbation is given by: max ∥n∥∞≤δ ∆T∇2f(w)∆ = δ2 (∇2f(w))1/2  2 ∞→2 . Unfortunately the ∞ →2-norm is known to be NP-hard ((Hendrickx & Olshevsky, 2010); see Bhattiprolu et al. (2018) for a more recent study). As a matter of fact, if f(·) is positive semi- deﬁnite, and hence the function is convex, the problem above corresponds to maximization of convex functions, which is difﬁcult as well. For a general Hessian, the problem is still difﬁcult to solve. First note that: max ∥n∥∞≤δ ∆T∇2f(w)∆ = max ∥n∥∞≤δ Tr ( ∇2f(w)∆∆T) . We can therefore replace ∆∆T with a positive semi-deﬁntite matrix of rank 1 denoted by N. The worst case second-order perturbation can be obtained by solving the following problem: max N∈Rn×n Tr ( ∇2f(w)N ) (6) subject to N ⪰0 Nii ≤δ2 for i∈{1,...,n } rank(N) = 1. The last constraint, namely the rank constraint, is a discrete constraint. The optimization problem above is therefore NP-hard to solve. To sum up, the worst case second-order perturbation cannot be efﬁciently computed, which poses difﬁculty for controlling the second-order robustness. There are, however, approximations available in the literature. A common approximation, which is widely known for the Max-Cut and community detection problems, consists of dropping the rank- constraint from the above optimization problem to get the following semi-deﬁnite program: max N∈Rn×n Tr ( ∇2f(w)N ) (7) subject to N ⪰0 Nii ≤δ2 for i∈{1,...,n } Unfortunately this approximation, apart from being costly to solve for large n, does not provide a regularization parameter that can be included in the training of the model. It is not clear how we can control the second-order term through a tractable term. C D EFENSIVE QUANTIZATION IMPOSES A 4TH POWER CONSTRAINT ON SINGULAR VALUES From basic linear algebra we have that ∥W∥2 2 = Tr(WTW) = ∑ i σ2 i(W), i.e., the Frobenius norm is equal to sum of the squared singular values of W. From this we can conclude that the regularization term ∥WTW −I∥2 2 introduced by Lin et al. (2019) thus equals ∥WTW −I∥2 2 = ∑ i σ2 i(WTW −I) = ∑ i ⏐⏐σ2 i(W) −1 ⏐⏐2 , and therefore imposes a 4th power regularization term on the singular values of W. A softer regu- larization can be introduced by regularizing Tr(WTW −I) instead. 13Published as a conference paper at ICLR 2020 D G RADIENT -PENALTY PROGRESSION IN NON -REGULARIZED NETWORKS Optimizing our regularization penalty requires computing gradients of the gradients. While this is easily done by double-backpropagation in modern software frameworks it introduces overhead (as discussed in Section 4.1) and makes training slower. However, as the training progresses, the gradients in unregularized networks tend to become smaller as well, which is inline with our reg- ularization objective. It is therefore not necessary to apply the regularization from the beginning of training. In Figure 7 we show examples of how the regularization objective naturally decreases during training. We also show how turning the regularization on in the ﬁnal epochs where the regu- larization objective is oscillating can push the loss further down towards zero. 0 5K 10K 15K 20K 25K 30K 35K Training Step 0 500 1000 1500 2000 2500Gradient L1 Penalty Regularization not applied 0 5K 10K 15K 20K 25K 30K 35K Training Step 0 500 1000 1500 2000 2500Gradient L1 Penalty Regularization applied from step 30K (a) VGG-like 0 5K 10K 15K 20K 25K 30K 35K Training Step 0 200 400 600 800 1000 1200Gradient L1 Penalty Regularization not applied 0 5K 10K 15K 20K 25K 30K 35K Training Step 0 200 400 600 800 1000 1200Gradient L1 Penalty Regularization applied from step 30K (b) ResNet-18 Figure 7: The gradients in unregularized networks tend to become smaller as training progresses. This means for large parts of the training there is no need to apply the regularization. The plots on the left show the regularization penalty in unregularized networks. The plots on the right show how turning on the regularization in the last 15 epochs of the training can push the regularization loss even further down. E ℓ∞-BOUNDED PERTURBATIONS INCLUDE OTHER BOUNDED -NORM PERTURBATIONS Figure 8 show that the ℓ∞-bounded perturbations include all other bounded-norm perturbations. 14Published as a conference paper at ICLR 2020 ℓ1 ℓ2 ℓ∞ Figure 8: ℓ∞-bounded vectors include other bounded- norm vectors. In this plot we show that the pertur- bations with bounded ℓp-norm are a subset of ℓ∞-bounded perturbations. For p= 1,2,∞, we plot the vectors with ∥x∥p = 1. 15",
      "meta_data": {
        "arxiv_id": "2002.07520v1",
        "authors": [
          "Milad Alizadeh",
          "Arash Behboodi",
          "Mart van Baalen",
          "Christos Louizos",
          "Tijmen Blankevoort",
          "Max Welling"
        ],
        "published_date": "2020-02-18T12:31:34Z",
        "pdf_url": "https://arxiv.org/pdf/2002.07520v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper analyzes the effect of quantizing weights and activations of neural networks on their loss and proposes a novel, simple regularization scheme. The main contribution is a method that improves robustness against post-training quantization, enabling the storage of a single set of weights that can be quantized on-demand to different bit-widths. This approach, by modeling quantization as an ℓ∞-bounded perturbation, regularizes the first-order term in the loss expansion using the ℓ1-norm of gradients. This allows for “on the fly” post-training quantization to various bit-widths, unlike quantization-aware training (QAT) which targets specific bit-widths and requires access to training data. Experimental validation confirms its effectiveness on different architectures and datasets (CIFAR-10, ImageNet).",
        "methodology": "The core methodology involves modeling quantization noise as an ℓ∞-bounded additive perturbation. The authors derive that the maximum first-order output perturbation caused by this noise is proportional to the ℓ1-norm of the loss gradient. To control this perturbation and promote quantization robustness, they introduce a regularization term to the standard cross-entropy loss. This regularization term penalizes the ℓ1-norm of the gradients with respect to both weights (Wl) and activations (yl) across all layers: L(W; x) = LCE(W,Y; x) +λw ∑ Wl,∈W ∥∇Wl LCE(W,Y; x)∥1 + λy ∑ yl∈Y ∥∇yl LCE(W,Y; x)∥1. They argue against ℓ2-norm regularization of gradients, citing the 'curse of dimensionality' which makes ℓ2 regularization ineffective for ℓ∞-bounded perturbations. The regularization is applied during the final stages of training or as a fine-tuning step, rather than from the beginning, to balance computational cost and effectiveness.",
        "experimental_setup": "The method's effectiveness was validated on classification tasks using ResNet-18 and a VGG-like architecture on CIFAR-10, and ResNet-18 on ImageNet. Comparisons were made against unregularized baseline networks, Lipschitz regularization methods (Defensive Quantization, ℓ2 gradient regularization), and quantization-aware fine-tuned models using the straight-through estimator (STE). Uniform symmetric quantization was predominantly used. For CIFAR-10, activation bit-widths were fixed to 4 bits, while weight bits varied from 8 to 4. For ImageNet, both weights and activations used the same bit-width. Activation ranges for post-training quantization were set using batch normalization parameters. The regularization strength (λ) was selected via grid-search to avoid degrading full-precision model performance. Training involved higher-order gradients, implemented using PyTorch, and was performed on a single NVIDIA RTX 2080 Ti GPU, with regularization enabled only in the last 15 epochs or as a fine-tuning phase to mitigate increased epoch times.",
        "limitations": "The primary limitation is the reliance on a first-order Taylor expansion for robustness analysis, neglecting higher-order residual terms. While empirically shown to be effective, a more comprehensive higher-order analysis, though computationally challenging (e.g., computing worst-case second-order terms is NP-hard), could potentially yield better approximations. The computational overhead of requiring higher-order gradients significantly increases training time (up to 10x), necessitating applying regularization only in later training stages. Additionally, the study primarily focuses on uniform quantization schemes, and the theoretical arguments for non-uniform schemes regarding the ℓ2-norm's ineffectiveness require further investigation. Finally, tuning the regularization strength to maintain full-precision accuracy might limit the performance gains for very low bit-widths (e.g., 4-bit ImageNet), suggesting a trade-off.",
        "future_research_directions": "Future research directions include exploring higher-order perturbation analysis to potentially achieve even better approximations of quantization effects, despite the inherent computational difficulties. Another promising area is extending the proposed regularization scheme to effectively handle non-uniform quantization schemes, which would require further investigation into the properties of non-uniform quantization noise (e.g., subgaussian properties). Developing methods to make the gradient regularization more computationally efficient or adaptive could also be a valuable extension."
      }
    },
    {
      "title": "Optimal and Approximate Adaptive Stochastic Quantization",
      "abstract": "Quantization is a fundamental optimization for many machine-learning use\ncases, including compressing gradients, model weights and activations, and\ndatasets. The most accurate form of quantization is \\emph{adaptive}, where the\nerror is minimized with respect to a given input, rather than optimizing for\nthe worst case. However, optimal adaptive quantization methods are considered\ninfeasible in terms of both their runtime and memory requirements.\n  We revisit the Adaptive Vector Quantization (AVQ) problem and present\nalgorithms that find optimal solutions with asymptotically improved time and\nspace complexity. We also present an even faster near-optimal algorithm for\nlarge inputs. Our experiments show our algorithms may open the door to using\nAVQ more extensively in a variety of machine learning applications.",
      "full_text": "Optimal and Approximate Adaptive Stochastic Quantization Ran Ben Basat UCL Yaniv Ben-Itzhak VMware Research Michael Mitzenmacher Harvard University Shay Vargaftik VMware Research Abstract Quantization is a fundamental optimization for many machine learning (ML) use cases, including compressing gradients, model weights and activations, and datasets. The most accurate form of quantization is adaptive, where the error is minimized with respect to a given input rather than optimizing for the worst case. However, optimal adaptive quantization methods are considered infeasible in terms of both their runtime and memory requirements. We revisit the Adaptive Stochastic Quantization (ASQ) problem and present algo- rithms that find optimal solutions with asymptotically improved time and space complexities. Our experiments indicate that our algorithms may open the door to using ASQ more extensively in a variety of ML applications. We also present an even faster approximation algorithm for quantizing large inputs on the fly. 1 Introduction Quantization is central to optimizing a large range of machine learning (ML) applications. It is often used for compressing gradients to reduce network requirements in distributed and federated learning (e.g., [1, 2, 3, 4, 5, 6]); for quantization of datasets for faster training and inference (e.g., [7]); and for reducing the memory footprint while accelerating the computation for large models’ inference via post-training quantization (e.g., [8, 9]) and quantization-aware training (e.g., [10, 11]) of model weights, activations and key-value (KV) caches [12]. A fundamental quantization method is stochastic quantization, where one quantizes an input vector X ∈ Rd to bX ∈ Qd using a set Q ⊂ R of |Q| = s quantization values so that each entry is unbiased [13]. That is, each x ∈ X is (randomly) quantized to a value bx ∈ Q such that E[bx] = x. Previous unbiased quantization works considered different approaches. Some are distribution- agnostic, i.e., design the quantization without optimizing it for the specific input. For example, [1, 14, 15] set quantization values with respect to global properties such as the vector’s norm, or minimum and maximum values. Other works, e.g., [1, 3, 4, 16, 17, 18, 19], optimize for the worst case X by applying a reversible transformation (e.g., the randomized Hadamard transform) before quantization that converts it into a vector X′ with a controlled distribution (e.g., with max(X′) − min(X′) = ˜O(∥X∥2 / √ d)). The decoder then applies the inverse transformation on the quantized X′ to obtain an estimate of X. In contrast, some solutions use the fact that, in many cases, the inputs to be quantized have a significant structure that can be leveraged to reduce the quantization error. For example, DNN gradients (which are often compressed in distributed and federated learning applications to reduce bandwidth [20, 21]) were observed to follow LogNormal-like [ 22] or Normal-like [ 23, 24] distributions. As another example, the distribution of deep activation layers appears to follow a sub-Weibull distribution [25]. arXiv:2402.03158v2  [cs.LG]  31 Jul 2025To alleviate the need to assume an input distribution, the Adaptive Stochastic Quantization (ASQ) problem (e.g., [26, 27, 28]) considers selecting Q adaptively, i.e., with respect to the specific inputX, that minimizes the mean squared error (MSE, also known as the sum of variances) given by E \u0014\r\r\r bX − X \r\r\r 2 2 \u0015 = X x∈X Var[bx] , where bX = {bx | x ∈ X} is the vector of quantized values. Unfortunately, known ASQ solutions are not practical for the large-size vectors that commonly appear in ML applications. One aspect of the problem’s difficulty is that it is known to be non-convex even for s = 4 (two-bit quantization) [28], which excludes many natural solution methods such as gradient descent. ZipML [26] approaches the challenge using a dynamic programming approach that allows one to optimize Q in polynomial time. However, this solution has a significant overhead and solving the problem optimally is often considered to be impractical; for example, [28] states “To find the optimal sequence of quantization values, a dynamic program is solved whose computational and memory cost is quadratic ... For this reason, ZipML is impractical for quantizing on the fly”. As another evidence of the problem’s hardness, previous work [27] solves the problem only for a given (Weibull) distribution, writing that “The empirical distribution is usually non-differentiable, making the searching ofQ infeasible”. Nevertheless, there is significant interest in advancing ASQ solutions towards wider adoption as even approximate adaptive solutions like ALQ [ 28] have been shown to have lower MSE than advanced distribution-agnostic methods such Non-Uniform QSGD (NUQSGD) [29]. ASQ methods can also improve more complex schemes (e.g., including the aforementioned that utilize worst-case to average-case transformations) by replacing distribution-agnostic quantization with an adaptive one. In this paper, we show that one can, in fact, solve the ASQ problem optimally and efficiently. To this end, we introduce QUIVER, an algorithm that features novel acceleration methods and leverages the structure of the underlying problem to reduce the runtime complexity from O(s · d2) to O(s · d) and the space complexity from O(d2) to O(s · d). This improvement arises from the observation that the optimal solution, for given input parameters s, d, can be efficiently derived from the solutions for {s − 1, d′ | d′ ∈ {2, 3, . . . , d}} by a reduction to the problem of finding the row maximas in an implicitly defined totally monotone matrix. This problem is known to have fast algorithms assuming that, for any1 ≤ k ≤ j ≤ d, the sum of variances of points {xk, . . . , xj} can be computed in constant time when quantized to {xk, xj}, a property that is achieved by our new preprocessing method. We then further accelerate QUIVER by deriving a closed-form solution for s = 3. In turn, this yields a faster solution for any s, by a variant of QUIVER that places two quantization values at a time instead of one. Finally, by discretizing the search space for Q, we show a fast approximation variant of QUIVER. This variant introduces an appealing tradeoff between accuracy and speed, making it suitable for quantizing large vectors on the fly. We implement our algorithms in C++ and demonstrate their efficiency. For example, on a commodity PC, QUIVER can compute the optimal 4-bit quantization values (s = 16) for a vector with d = 1M entries in under a second and compute an accurate approximation in just six milliseconds. We evaluate our solutions compared to state-of-the-art ASQ methods on a variety of distributions considering different vector sizes and number of quantization values and demonstrate a speedup of up to four orders of magnitude. We open source the code of the paper [30]. We note that there are many works that investigate different forms of compression, including non-adaptive quantization (e.g., QSGD [14]), biased quantization (e.g., top-k [31]), sparsification (e.g., [32]), sparse coding (e.g., [33]), low-rank decomposition (e.g., PowerSGD [34]), variable-length coding (e.g., EDEN [ 4]) and more. Many of these are orthogonal to our work and can be used in conjunction with it. For example, one can use ASQ to quantize a sparsified or transformed vector or apply variable-length encoding to further reduce the size of the quantized vector. 20.10.20.30.40.50.60.70.80.91.0 2 10 2 100 102 MSE (a) Single vector estimation. 22 25 28 211 214 217 Number of vectors 10 6 10 2 102 Mean Estimation MSE  (b) Distributed mean estimation. QSGD (Unbiased, Non-adaptive) Optimal Adaptive Biased NUQSGD (Unbiased, Non-adaptive) Optimal Adaptive Unbiased RTN  (Biased, Non-adaptive) Powered by TCPDF (www.tcpdf.org) Powered by TCPDF (www.tcpdf.org) QSGD (Unbiased, Non-adaptive) Optimal Adaptive Biased NUQSGD (Unbiased, Non-adaptive) Optimal Adaptive Unbiased RTN  (Biased, Non-adaptive) Powered by TCPDF (www.tcpdf.org) Powered by TCPDF (www.tcpdf.org) Figure 1: An experiment with dimension d = 10M and s = 10 quantization values. Figure 1(a) shows the empirical MSE of quantizing a single vector with i.i.d. LogNormal(0, σ2) entries. It shows that adaptive methods are more accurate than non-adaptive and that the optimal biased method is more accurate than the optimal unbiased one. However, as shown in Figure 1(b), for distributed mean estimation, the bias may not cancel out when averaging quantized inputs (here, we used a standard setup where all vectors are identical, e.g., see [17], with i.i.d. LogNormal(0, 1/2) distributed entries) and the advantage of unbiased methods accordingly increases with the number of inputs. Each data point is averaged over ten runs with the standard deviation reported. 2 Background 2.1 Motivation We now briefly explain the benefits of ASQ compared to alternative methods. The benefits of adaptivity Unbiased solutions such as QSGD [14] and NUQSGD [29] rely only on global properties (e.g., the input’s norm) when selecting Q. Figure 1(a) shows the benefit of adaptivity by illustrating the potential MSE reduction from selecting Q optimally for the specific input. A similar behavior is observed for biased methods where the non-adaptive Round-To-Nearest (RTN) has a higher error than the optimal adaptive biased scalar quantizer,k-means. As shown, this can translate to orders of magnitude lower error, depending on the data’s skew. The benefits of unbiasedness In many cases, it is beneficial for the quantization to be unbiased. For example, when there aren senders (e.g., when doing distributed mean estimation [1, 2, 4, 17, 18]), having unbiased and independent estimates of the vectors allows the mean estimation’s MSE to decay proportionally to 1 n ; with biased quantization, the MSE may not decay with respect to n since the errors may be correlated [17] (e.g., when all clients have the same vector). This benefit is demonstrated in Figure 1(b), which shows that while biased adaptive solutions have lower error for a small number of vectors (1-2), having unbiased quantization is critical to lowering the error for a large n. As another example, it was recently shown that compressing large language model parameters with biased techniques such as RTN may result in inferior performance than uniform stochastic quantization [35]. This outcome arises because the LLM layers’ parameters are used to compute inner products with their inputs. Having these inner products themselves be unbiased leads to smaller errors in layers’ outputs, which in turn leads to better performance. 2.2 Preliminaries Given two quantization values a, band a number x ∈ [a, b], Stochastic Quantization (SQ) is a proce- dure that rounds x to bx where bx ∈ {a, b}. Specifically, bx obtains the value a with probability pa = b−x b−a and the valueb otherwise, i.e., with probabilitypb = 1−pa = x−a b−a . An important property of SQ is that the expected rounded value isunbiased, i.e., E[bx] = a·pa+b·pb = x. The variance of stochasti- cally quantizing x is then given by E \u0002 (x − bx)2\u0003 = (x − a)2 · pa + (x − b)2 · pb = (b − x)(x − a). Given a vector X ∈ Rd and an integer s ≥ 2, the Adaptive Stochastic Quantization (ASQ) prob- lem [26, 27, 28] looks for a set of quantization values Q where |Q| ≤s and Q minimizes the mean squared error (MSE) that results from rounding X to bX ∈ Qd by stochastically quantizing each entry x ∈ X with values ax = max {q ∈ Q | q ≤ x} and bx = min {q ∈ Q | q ≥ x}. Formally, ASQ seeks to minimize the MSE, given by E[∥X − bX∥2 2] = P x∈X(bx − x)(x − ax), where E[ bX] = X holds by construction. 32.3 Existing ASQ methods Leveraging the fact that there exists an optimal solution in which Q ⊆ X [26] (i.e., the quantization values are a subset of the input), one can naively solve the problem indΘ(s) time by going over all choices for the quantization values. Instead, the following dynamic program (DP) allows us to solve it optimally and in polynomial time for any s [26]. Given a sorted vector X = ⟨x1, . . . , xd⟩, we denote by MSE[i, j] the optimal MSE of quantizing the prefix vector Xj = ⟨x1, . . . , xj⟩ using i quantization values that include xj, that is: MSE[i, j] = min Q:|Q|≤i,xj∈Q X x∈Xj (bx − x)(x − ax). Our goal is to compute a set of quantization values Q that results in an optimal MSE of MSE[s, d]. Accordingly, we express the dynamic program as follows. We first define C[k, j] as the sum of variances of all vector entries in the range [xk, xj] where xk, xj ∈ Q are two consecutive quantization values, i.e., C[k, j] = P x∈[xk,xj](xj − x)(x − xk). Here and when clear from context, to simplify notation, we write P x to denote P x∈X. For i ∈ {2, . . . , s}, j∈ {i, . . . , d}, we set MSE[2, j] = C[1, j] ∀j and use the recurrence MSE[i, j] = min k∈{i,...,j} MSE[i − 1, k] + C[k, j] . Here, the index k denotes the entry in X, xk, of the rightmost quantization value to the left of xj. A naive solution for the above DP is first to compute the matrix C (which takes O(d3) time and O(d2) space) and then calculate MSE[i, j] for all i, j, and thus Q, in O(s · d2) time and O(s · d) space. In Appendix A, we describe a simple algorithm that implements this dynamic program. An improved solution, ZipML [26], uses O(s · d2) time and O(d2) space, but it remains infeasible even for moderate (e.g., d = 105) dimensions. Accordingly, we next design novel techniques to asymptotically improve both the space and time complexities. 3 Optimization Using Pre-processing The first ingredient in our solution is the usage of preprocessed arrays that allow us to efficiently compute C[k, j] in constant time, at the cost of only O(d) additional space. We define the following arrays, β, γ∈ Rd, that store the cumulative sums of the vector and its squared entries: βj = X x∈Xj x , γ j = X x∈Xj x2 ∀j ∈ {1, . . . , d} . Denoting β0 = γ0 = 0, both are computable in O(d) time as βj = βj−1 + xj and γj = γj−1 + x2 j. We can then express C[k, j] as follows: C[k, j] = X x∈[xk,xj] (xj − x)(x − xk) = X x∈(xk,xj] (xj − x)(x − xk) = −xj · xk · X x∈(xk,xj] 1 + (xj + xk) · X x∈(xk,xj] x − X x∈(xk,xj] x2 = −xj · xk · (j − k) + (xj + xk) · (βj − βk) − (γj − γk). With this optimization, we can evaluate C[k, j] in constant time, yielding a solution that uses O(s · d) memory instead of O(d2). Next, we show how to improve the runtime. 44 The QUIVER Algorithm To derive a faster algorithm, we observe thatC satisfies the quadrangle inequality, defined below: Definition 4.1. A function w: {1, . . . , d}×{ 1, . . . , d}→R satisfies the quadrangle inequality if for any a≤b≤c≤d: w[a, c] + w[b, d] ≤ w[a, d] + w[b, c]. Lemma 4.2. C satisfies the quadrangle inequality. Proof. We first observe that for any x ∈ [xa, xb] : (xc − x)(x − xa) = (xd − x)(x − xa) + (xc − xd)(x − xa) ≤ (xd − x)(x − xa). (1) For any x ∈ [xc, xd], we similarly get: (xd − x)(x − xb) = (xd − x)(x − xa) + (xd − x)(xa − xb) ≤ (xd − x)(x − xa). (2) Similarly, for x ∈ [xb, xc], we have that: (xc − x)(x − xa) + (xd − x)(x − xb) = (xc − x)(x − xb) + (xd − x)(x − xa) + (xa − xb)(xd − xc) ≤ (xc − x)(x − xb) + (xd − x)(x − xa). (3) Therefore, we get: C[a, c] + C[b, d] = X x∈[xa,xc] (xc − x)(x − xa) + X x∈[xb,xd] (xd − x)(x − xb) = X x∈[xa,xb] (xc −x)(x−xa)+ X x∈[xc,xd] (xd −x)(x−xb)+ X x∈[xb,xc] (xc −x)(x−xa)+(xd −x)(x−xb) ≤ X x∈[xa,xb] (xd −x)(x−xa)+ X x∈[xc,xd] (xd −x)(x−xa)+ X x∈[xb,xc] (xc −x)(x−xb)+(xd −x)(x−xa). = X x∈[xa,xd] (xd − x)(x − xa) + X x∈[xb,xc] (xc − x)(x − xb) = C[a, d] + C[b, c]. Here, the inequality follows from equations (1)-(3). Next, let us implicitly define a matrix A ∈ Rd×d such that A[k, j] = MSE[i − 1, k] + C[k, j]. Importantly, A is not stored in memory but admits constant time lookups as MSE[i − 1, ·] is stored and C is efficiently computable (Section 3). Also, C satisfies the quadrangle inequality and thus A is a totally monotone matrix [ 36], i.e., for any a < b and c < d: (A[a, c] > A[b, c]) = ⇒ (A[a, d] > A[b, d]). By applying the SMAWK algorithm [37], which finds the row minimas of an implicitly defined totally monotone matrix, on AT , we obtain in O(d) time and space the indices kj = argmink∈{1,...,d} A[k, j] for all j ∈ {1, . . . , d}. This immediately gives the next row of the dynamic program, as MSE[i, j] = MSE[i − 1, kj] + C[kj, j]. The resulting solution, which we call QUIVER, is given in Algorithm 1 and requires just O(s · d) time and space to compute the optimal quantization values. 5 The Accelerated QUIVER Algorithm To accelerate QUIVER, we rely on the observation that while the problem is non-convex fors >3, it admits a closed-form solution when s = 3. Denoting by C2[k, j] = minb∈{k,...,j} (C[k, b] + C[b, j]) the optimal MSE of quantizing the range [xk, xj] using three quantization values (at xk, xb, xj), we show how to compute C2 in constant time. Namely, consider adding a quantization valueq ∈ [xk, xj] (not necessarily inX) between two existing quantization values xk and xj. Let us define the sum of variances of all input entries in [xk, xj] as a function of q: Q(q) = P x∈[xk,q](q − x)(x − xk) + P x∈(q,xj](xj − x)(x − q). This function is differentiable in [xk, xj] \\ X, and we get: dQ(q) dq = P x∈[xk,q](x − xk) − P x∈(q,xj](xj − x). 5Algorithm 1 QUIVER 1: Input: X ∈ Rd, s∈ N. ▷ Xis sorted. 2: Preprocess(X) ▷ Enables computing C[k, j] in constant time (Section 3). 3: for j = 2 to d do 4: MSE[2, j] = C[1, j] 5: for i = 3 to s do 6: K[i, ·] = SMAWK(A) ▷ Where A[k, j] ≜ MSE[i − 1, k] +C[k, j] ∀k, j. 7: MSE[i, j] = MSE[i − 1, K[i, j]] + C[K[i, j], j] for all j ∈ {i, . . . , d}. 8: Q = {x1, xd} 9: j = d 10: for i = s down to 3 do 11: j = K[i, j] 12: Q = Q ∪ {xj} 13: return Q Algorithm 2 Accelerated QUIVER 1: Input: X ∈ Rd, s∈ N. ▷ Xis sorted. 2: Preprocess(X) ▷ Enables computing C[k, j] and C2[k, j] in constant time. 3: s′ = (s mod 2) 4: if s′ = 0 then 5: for j = 2 to d do 6: MSE[2, j] = C[1, j] 7: else 8: for j = 3 to d do 9: MSE[3, j] = C2[1, j] 10: for i = 2 to ⌊s/2⌋ do 11: K[i, ·] = SMAWK(B) ▷ Where B[k, j] ≜ MSE[2 · (i − 1) +s′, k] +C2[k, j] ∀k, j. 12: MSE[2 · i + s′, j] = MSE[2 · (i − 1) + s′, K[i, j]] + C2[K[i, j], j] ∀j ∈ {i, . . . , d}. 13: Q = {x1, xd} 14: j = d 15: for i = ⌊s/2⌋ down to 2 do 16: b∗ = argminb∈{K[i,j],...,j} (C[K[i, j], b] + C[b, j]) ▷ Takes O(1) time. 17: j = K[i, j] 18: Q = Q ∪ {xj, xb∗ } 19: if s′ = 1 then 20: b∗ = argminb∈{0,...,j} (C[0, b] + C[b, j]) ▷ Takes O(1) time. 21: Q = Q ∪ {xb∗ } 22: return Q Notice that the derivative is monotonically non-decreasing and for any ℓ ∈ {k, k+ 1, . . . , j− 1} the derivative is fixed (independent ofq) over any interval(xℓ, xℓ+1). This means that Q(q) is minimized at u = infq(dQ(q) dq ≥ 0), where u ∈ X. Denote by b∗ k,j ∈ {k, . . . , j} the value such that xb∗ k,j = u. Notice that while dQ(u) dq may not be defined, we have that limh→0+ dQ(u+h) dq ≥ 0 is well-defined. We thus require Pb∗ k,j i=k+1(xi − xk) − Pj i=b∗ k,j+1(xj − xi) ≥ 0. With some simplifications, this is equivalent to: Pj i=k+1 xi − (b∗ k,j − k)xk − (j − b∗ k,j)xj ≥ 0, yielding b∗ k,j ≥ jxj−kxk−Pj i=k+1 xi xj−xk . As b∗ k,j is an integer, we get a formula for C2[k, j] that can be computed in constant time using: b∗ k,j = \u0006 jxj−kxk−Pj i=k+1 xi xj−xk \u0007 = \u0006 jxj−kxk−(βj−βk) xj−xk \u0007 . That is, for any 1 ≤ k ≤ j ≤ d we have that C2[k, j] = C[k, b∗ k,j] + C[b∗ k,j, j] is the sum of the variances in quantizing the entries in [xk, xj] using the quantization values \b xk, xb∗ k,j , xj \t . 6We can then use this method to halve the required number of invocations of SMAWK by always using it to pick the second-next quantization value and computing the optimal quantization value in between directly. Our accelerated dynamic program is then given by: MSE[i, j] =    min k∈{i,...,j} MSE[i − 2, k] + C2[k, j] i >3 C2[1, j] i = 3 C[1, j] i = 2 , and the resulting pseudo-code for Accelerated QUIVER is given by Algorithm 2. Similarly to QUIVER, we start by initializing the first row of MSE. Importantly, we now separate the even s case (lines 5-6), in which we initialize the row usingC, and the odd case, where we useC2 (lines 8-9). That is, the odd s case ‘skips’ a quantization value that we later determine separately (lines 19-21). Next, denoting s′ = (s mod 2), we proceed with⌊s/2⌋−1 invocations of the SMAWK algorithm (lines 10- 12), applied on the implicitly defined matrix B[k, j] ≜ MSE[2 ·(i −1) +s′, K[i, j]] +C2[K[i, j], j]. The output yields the minimizers of MSE[2 · i + s′, j] used for reconstruction. In the reconstruction step (lines 15-21), we fill in the missing quantization values by finding the optimal value between every two outputs from the dynamic program minimizers K. Overall, the Accelerated QUIVER algorithm requires at most half of the number of SMAWK invocations compared to QUIVER and at most half of the memory to store K and MSE. To establish correctness, we state the following lemma, whose proof appears in Appendix C. Lemma 5.1. C2 satisfies the quadrangle inequality. In Appendix D, we discuss why this approach is not suitable for further acceleration by placing more than one quantization value in [xa, xc]. 6 The Approximate QUIVER Algorithm We now show how the usage ofquantization value discretization gives a controllable tradeoff between accuracy and speed. Intuitively, by allowing the quantization values to be placed only on a uniform grid of controllable size m + 1 ≥ s (for some m ∈ N+), we can accelerate the computation at the cost of a small additional error. Importantly, while the quantization values are from a discretized set of possibilities, we compute the optimal subset of discretized values for the original input vector. To that end, consider the discrete set S = \b x1 + ℓ · xd−x1 m | ℓ ∈ {0, . . . , m} \t . Our goal is then to find Q ∈ \u0000S s \u0001 that minimizes the sum of variances for the original input. Denotingsℓ = x1 +ℓ· xd−x1 m , we modify our preprocessing scheme to consider the discretization: αℓ = X x∈[s0,sℓ] 1 , β ℓ = X x∈[s0,sℓ] x , γ ℓ = X x∈[s0,sℓ] x2 ∀ℓ ∈ {1, . . . , m} . As we explain in Appendix E, we can compute these values in O(d) time and space. Using these arrays, we can express the sum of variances of all input entries between two quantization values sk, sj as follows: Cm[k, j] = X x∈[sk,sj] (sj − x)(x − sk) = X x∈(sk,sj] (sj − x)(x − sk) = −sj · sk · X x∈(sk,sj] 1 + (sj + sk) · X x∈(sk,sj] x − X x∈(sk,sj] x2 = −sj · sk · (αj − αk) + (sj + sk) · (βj − βk) − (γj − γk). 7Note that the quadrangle inequality trivially holds for this extension. The resulting algorithm, termed Approximate QUIVER (or in short, Apx. QUIVER), proceeds as QUIVER with Cm instead of C, except for the reconstruction stage where we pick Q from S instead of the input X. Apx. QUIVER, whose pseudo-code is given in Appendix F, runs in space and time complexities of O(d + m · s). We next analyze the approximation guarantee of Apx. QUIVER. Denote by optX,s the optimal MSE attainable for X using s quantization values, and by AQX,2s−2 the MSE of Apx. QUIVER with 2s − 2 values. We prove that the MSE of Apx. QUIVER with 2s − 2 quantization values is close to the optimal algorithm with s values. In practice, we generally find Apx. QUIVER does better than the bound below, and for moderate m, it is nearly optimal. Lemma 6.1. For any X, s, mwe have AQX,2s−2 ≤ optX,s + d·(xd−x1)2 4m2 ≤ optX,s + d·∥X∥2 2 2m2 . Proof. Let Q∗ ⊆ X be the optimal solution with |Q∗| ≤s. For any q ∈ Q∗, denote by q = max {sℓ ∈ S | sℓ ≤ q} and q = min {sℓ ∈ S | sℓ ≥ q}. Consider the solution eQ = \b q, q | q ∈ Q∗\t . Note that | eQ| ≤2s − 2 as x1, xd ∈ Q∗ and x1 = x1 and xd = xd. Also, eQ ⊆ S and is thus a valid solution of Apx. QUIVER. Thus, AQX,2s−2 is upper bounded by the MSE when using eQ. Next, consider x ∈ X and let ax = max {q ∈ Q∗ | q ≤ x} and bx = min {q ∈ Q∗ | q ≥ x} be the values between which x is stochastically quantized in Q∗. We consider two cases: • x ∈ [ax, ax)∪(bx, bx]. In this case, when using eQ, we have that x is quantized in an interval of size (xd − x1)/m and thus its variance is bounded by (xd − x1)2/4m2. • x ∈ [ax, bx], in this case, using eQ, x is quantized between ax and bx, yielding a variance of (bx − x)(x − ax) ≤ (bx − x)(x − ax), i.e., lower than the variance under Q∗. As the two cases capture all options, summing the variances over all x ∈ X yields the result. In terms of the vector normalized MSE (vNMSE),1 which is a normalized MSE measure given by E h ∥X− bX∥ 2 2 i ∥X∥2 2 , Apx. QUIVER with 2s − 2 quantization values achieves an additive d 2m2 term to the optimal vNMSE when using s quantization values. However, the first inequality of Lemma 6.1 is generally much tighter than the second that uses the squared norm. For example, if the entries of X were i.i.d. U[a, b] random variables, for some constants a < bthen (xd − x1)2 = O(1) while ∥X∥2 2 = Θ(d). Similarly, for i.i.d N(µ, σ2) entries for constants µ, σwe have (xd − x1)2 = O(log d) while ∥X∥2 2 = Θ(d) (both with high probability). 7 Evaluation We evaluate our algorithms’ empirical vNMSE and runtime against SOTA ASQ solutions. Setup. We implement all algorithms in C++. Unless stated otherwise, we use a g4dn.4xlarge AWS EC2 server with custom Intel Cascade Lake CPUs with 64 GB RAM and Ubuntu 22.04 OS and average all results over 5 seeds. Acceleration Speedup Appendix G shows the speedup attainable by Accelerated QUIVER. As we show, Accelerated QUIVER is consistently faster than QUIVER, providing up to5.4× speedup. Distributions. All experiments are done with vectors whose entries are independent and identically distributed. We present results for the LogNormal distribution and defer to Appendix H results for Normal, Exponential, TruncNorm, and Weibull distributions. As mentioned, these distributions are of interest as they are reported to capture gradients, model weights and activations (see Section 1). 1This metric is standard in quantization works (e.g., see [ 17] and the references therein). It enables us to reason about the results among different dimensions and distributions. 8212 215 218 221 224 10 1 101 103 105 Time [ms] 212 215 218 221 224 Dimension (d) 10 1 101 103 105 Time [ms] (a) s = 4(upper), s = 16(lower). 22 23 24 25 26 10 3 10 1 vNMSE 22 23 24 25 26 #Quantization values (s) 101 103 Time [ms]  (b) d = 212. 22 23 24 25 26 10 3 10 1 vNMSE 22 23 24 25 26 #Quantization values (s) 103 106 Time [ms]  (c) d = 216. ZipML Acc. QUIVER Figure 2: Comparing exact solutions with LogNormal(0, 1) distributed input. 212 215 218 221 224 10 1 101 vNMSE 212 215 218 221 224 Dimension (d) 10 1 101 103 Time [ms] (a) s = 16and m = 400bins. 22 23 24 25 2610 3 10 1 101 103 vNMSE 22 23 24 25 26 #Quantization values (s) 102 Time [ms]  (b) d = 222 and m = 1000bins. 200 400 600 800 1000 10 1 101 vNMSE 200 400 600 800 1000 m 102 Time [ms]  (c) d = 222 and s = 32. ZipML-CP Unif. ALQ ZipML-CP Quant. Apx. QUIVER ZipML 2-Apx Optimal Figure 3: Comparing approximate solutions with LogNormal(0, 1) distributed input. Baselines. We evaluate Accelerated QUIVER and compare its runtime to ZipML [ 26]. For the approximate variants, we evaluate Apx. QUIVER and compare it with three approximation variants of ZipML proposed in [ 26], namely ZipML-CP Quantiles, ZipML-CP Uniform, and ZipML 2- Approximation. ZipML-CP is an algorithm that runs the exact ZipML algorithm on a subset of the points called ‘Candidate Points’. Since ZipML runs in O(d2s) time, here we use M candidate points to get O(d + M2s) time. ZipML 2-Apx is an algorithm that computes an approximate solution in O(d log d + s3) time. It guarantees that its sum of variances is at most twice that of an optimal solution with ⌊s/2⌋ quantization values. We also compare with the recently proposed ALQ [ 28], which is an algorithm that finds good quantization values for a truncated normal distribution. It samples several gradients (by computing the gradient of several random batches) to fit the truncated normal parameters. To be fair to ALQ, since we evaluate a single-shot quantization scenario, we calculate the exact mean, variance, and support parameters for the input vector. This then runs for several (we used 10, as in their released code) iterations, so in total, they compute ≈ 10s integrals. While theoretically requiring O(d) time, in a model where such integral calculation takes constant time, this is markedly slower than other approaches. We note that it is possible that with low-precision integral calculations, one may improve the runtime, but the error (which is already not competitive) will degrade further. We further discuss these approximation algorithms in Appendix I. Exact algorithms experiments. The results are presented in Figure 2. Figure 2(a) shows the runtime for optimally solving the ASQ problem for different dimensions and s. As shown, all our solutions are markedly faster than ZipML, which we are unable to run for dimensions d ≥ 217 due to its prohibitively large memory requirements. The asymptotic difference (O(s · d2) for ZipML and O(s · d) for Accelerated QUIVER) is clearly visible in the different slopes on the log-log plot. As 9a result, Accelerated QUIVER can efficiently quantize vectors. For example, Acc. QUIVER can compute the optimal 4-bit (s = 16) quantization values for a 1M-sized vector in under a second. Next, Figure 2(b) and Figure 2(c) show the vNMSE and runtime with respect to the number of quantization values s for d = 2 12 and d = 2 16. As shown, the vNMSE decays linearly with s while the runtime increases linearly. Even for these small dimensions, our algorithms are orders of magnitude faster than ZipML. Approximate algorithms experiments. The comparison results are presented in Figure 3. It is evident in Figure 3(a) that approximate solutions are significantly faster than exact ones. Also, Apx. QUIVER offers both near-optimal vNMSE and the fastest runtime as the dimension increases. As shown in Figures 3(b) and 3(c), Apx. QUIVER offers these advantages for different s, mvalues. Notably, on a commodity PC, Apx. QUIVER can compute near-optimal 4-bit quantization values (s = 16) for a vector with d = 220 entries in just six milliseconds, and about 70ms for d = 224, potentially enabling quantizing vectors on the fly for many applications. 8 Discussion In this paper, we presented algorithms for the Adaptive Stochastic Quantization (ASQ) problem with improved space and time complexities compared to the state of the art. For parameters of interest, our exact algorithms are up to four orders of magnitude faster compared to the alternatives while using markedly less memory. To potentially enable on-the-fly adaptive quantization of vectors, we also introduce an approximate algorithm with strong guarantees that runs faster while being significantly more accurate than other approximate solutions. Limitations: QUIVER is not GPU friendly, and it remains an interesting future work to design GPU- friendly ASQ algorithms. Also, similarly to previous works (e.g., [26]), our exact solution assumes that the input vector is sorted. Otherwise, the runtime is increased to O(d ·log d + s ·d). We note that Apx. QUIVER does not require the vector to be sorted and the time complexity remains O(d + s · m) even for non-sorted inputs, making it even more appealing compared to the exact solutions. Offloading Computation to a GPU: For exact algorithms, one can sort the input vector on a GPU, bringing the CPU solution complexity to O(s · d) which is faster for large vectors. In practice, GPU sorting is rarely the bottleneck; indeed, in Appendix J we measure the time it takes to sort the vector on a T4 GPU, and also to quantize the vector after an ASQ outputs the optimal quantization values. For example, the sorting and quantization time for a 1M-sized vector sums up to only 4ms where the runtime of Accelerated QUIVER is about one second. Generalizing the algorithms for weighted inputs: An interesting generalization of the ASQ problem is the weighted variant, where each entry xi ∈ X is associated with a weight wi ∈ R and the goal is to minimize the weighted sum of variances Pd i=1(xi − bxi)2 · wi. This variant is useful when, instead of getting an input vector, one wishes to solve ASQ for an empirical distribution. In Appendix K we explain how our algorithms and their analyses generalize to the weighted case, while maintaining the O(d · s) and O(d + M · s) runtime and space complexities for QUIVER and Apx. QUIVER accordingly. Our measurements indicate that the weighted variants are only 10-20% slower than their unweighted counterparts. Reproducability: All our results are reproducible and our code is open sourced [30]. Acknowledgments and Disclosure of Funding We thank Wenchen Han for his insightful comments and suggestions. Michael Mitzenmacher was supported in part by NSF grants CCF-2101140, CNS-2107078, and DMS-2023528. 10References [1] A. T. Suresh, X. Y . Felix, S. Kumar, and H. B. McMahan, “Distributed Mean Estimation With Limited Communication,” in International Conference on Machine Learning. PMLR, 2017, pp. 3329–3337. [2] J. Koneˇcn`y and P. Richtárik, “Randomized Distributed Mean Estimation: Accuracy vs. Commu- nication,” Frontiers in Applied Mathematics and Statistics, vol. 4, p. 62, 2018. [3] S. Caldas, J. Koneˇcný, H. B. McMahan, and A. Talwalkar, “Expanding the Reach of Federated Learning by Reducing Client Resource Requirements,” arXiv preprint arXiv:1812.07210, 2018. [4] S. Vargaftik, R. B. Basat, A. Portnoy, G. Mendelson, Y . B. Itzhak, and M. Mitzenmacher, “EDEN: Communication-Efficient and Robust Distributed Mean Estimation for Federated Learning,” in International Conference on Machine Learning. PMLR, 2022, pp. 21 984–22 014. [5] R. Dorfman, S. Vargaftik, Y . Ben-Itzhak, and K. Y . Levy, “DoCoFL: downlink compression for cross-device federated learning,” in International Conference on Machine Learning. PMLR, 2023, pp. 8356–8388. [6] W. Han, S. Vargaftik, M. Mitzenmacher, B. Karp, and R. Ben Basat, “Beyond Throughput and Compression Ratios: Towards High End-to-end Utility of Gradient Compression,” in HotNets, 2024. [7] D. Zhou, K. Wang, J. Gu, X. Peng, D. Lian, Y . Zhang, Y . You, and J. Feng, “Dataset Quantiza- tion,” in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 17 205–17 216. [8] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh, “OPTQ: Accurate quantization for genera- tive pre-trained transformers,” in The Eleventh International Conference on Learning Represen- tations, 2023. [9] Y . Jeon, C. Lee, K. Park, and H.-y. Kim, “A Frustratingly Easy Post-Training Quantization Scheme for LLMs,” in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023, pp. 14 446–14 461. [10] P. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen, D. Garcia, B. Ginsburg, M. Houston, O. Kuchaiev, G. Venkateshet al., “Mixed Precision Training,” inInternational Conference on Learning Representations, 2018. [11] S. Shen, Z. Dong, J. Ye, L. Ma, Z. Yao, A. Gholami, M. W. Mahoney, and K. Keutzer, “Q- BERT: Hessian Based Ultra Low Precision Quantization of BERT,” inProceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 05, 2020, pp. 8815–8821. [12] Y . Sheng, L. Zheng, B. Yuan, Z. Li, M. Ryabinin, B. Chen, P. Liang, C. Ré, I. Stoica, and C. Zhang, “Flexgen: High-Throughput Generative Inference of Large Language Models With a Single GPU,” in International Conference on Machine Learning. PMLR, 2023, pp. 31 094– 31 116. [13] Ben Basat, Ran and Mitzenmacher, Michael and Vargaftik, Shay, “How to Send a Real Number Using a Single Bit (And Some Shared Randomness),” in 48th International Colloquium on Automata, Languages, and Programming (ICALP 2021), vol. 198, 2021, p. 25. [14] D. Alistarh, D. Grubic, J. Li, R. Tomioka, and M. V ojnovic, “QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding,” Advances in Neural Information Processing Systems, vol. 30, pp. 1709–1720, 2017. [15] S. Horvóth, C.-Y . Ho, L. Horvath, A. N. Sahu, M. Canini, and P. Richtárik, “Natural Com- pression for Distributed Deep Learning,” in Mathematical and Scientific Machine Learning. PMLR, 2022, pp. 129–141. [16] M. Safaryan, E. Shulgin, and P. Richtárik, “Uncertainty Principle for Communication Com- pression in Distributed and Federated Learning and the Search for an Optimal Compressor,” Information and Inference: A Journal of the IMA, 2020. 11[17] S. Vargaftik, R. Ben-Basat, A. Portnoy, G. Mendelson, Y . Ben-Itzhak, and M. Mitzenmacher, “Drive: One-bit Distributed Mean Estimation,” Advances in Neural Information Processing Systems, vol. 34, pp. 362–377, 2021. [18] R. B. Basat, S. Vargaftik, A. Portnoy, G. Einziger, Y . Ben-Itzhak, and M. Mitzenmacher, “Accelerating Federated Learning with Quick Distributed Mean Estimation,” in International Conference on Machine Learning, 2024. [19] X. Chen, S. Vargaftik, and R. Ben-Basat, “When ML Training Cuts Through Congestion: Just-in-Time Gradient Compression via Packet Trimming,” inHotnets, 2024. [20] J. Kone ˇcný, H. B. McMahan, F. X. Yu, P. Richtárik, A. T. Suresh, and D. Bacon, “Fed- erated Learning: Strategies for Improving Communication Efficiency,” arXiv preprint arXiv:1610.05492, 2017. [21] M. Li, R. B. Basat, S. Vargaftik, C. Lao, K. Xu, X. Tang, M. Mitzenmacher, and M. Yu, “THC: Accelerating Distributed Deep Learning Using Tensor Homomorphic Compression,” inUSENIX Symposium on Networked Systems Design and Implementation, 2024. [22] B. Chmiel, L. Ben-Uri, M. Shkolnik, E. Hoffer, R. Banner, and D. Soudry, “Neural Gradients are Near-Lognormal: Improved Quantized and Sparse Training,” in International Conference on Learning Representations . OpenReview.net, 2021. [Online]. Available: https://openreview.net/forum?id=EoFNy62JGd [23] R. Banner, Y . Nahshan, and D. Soudry, “Post Training 4-Bit Quantization of Convolutional Networks for Rapid-Deployment,” in NeurIPS, 2019. [24] X. Ye, P. Dai, J. Luo, X. Guo, Y . Qi, J. Yang, and Y . Chen, “Accelerating CNN Training by Pruning Activation Gradients,” in European Conference on Computer Vision. Springer, 2020, pp. 322–338. [25] M. Vladimirova, J. Arbel, and P. Mesejo, “Bayesian Neural Networks Become Heavier-Tailed With Depth,” in NeurIPS 2018-Thirty-second Conference on Neural Information Processing Systems, 2018, pp. 1–7. [26] H. Zhang, J. Li, K. Kara, D. Alistarh, J. Liu, and C. Zhang, “ZipML: Training Linear Models with End-to-End Low Precision, and a Little Bit of Deep Learning,” inInternational Conference on Machine Learning. PMLR, 2017, pp. 4035–4043. [27] F. Fu, Y . Hu, Y . He, J. Jiang, Y . Shao, C. Zhang, and B. Cui, “Don’t waste your bits! squeeze activations and gradients for deep neural networks via tinyscript,” inInternational Conference on Machine Learning. PMLR, 2020, pp. 3304–3314. [28] F. Faghri, I. Tabrizian, I. Markov, D. Alistarh, D. M. Roy, and A. Ramezani-Kebrya, “Adaptive Gradient Quantization for Data-parallel SGD,” Advances in neural information processing systems, vol. 33, pp. 3174–3185, 2020. [29] A. Ramezani-Kebrya, F. Faghri, I. Markov, V . Aksenov, D. Alistarh, and D. M. Roy, “NUQSGD: Provably Communication-efficient Data-parallel SGD via Nonuniform Quantization,” The Journal of Machine Learning Research, vol. 22, no. 1, pp. 5074–5116, 2021. [30] “QUIVER code,” https://github.com/ranbenbasat/QUIVER. [31] S. U. Stich, J.-B. Cordonnier, and M. Jaggi, “Sparsified sgd with memory,”Advances in neural information processing systems, vol. 31, 2018. [32] J. Fei, C.-Y . Ho, A. N. Sahu, M. Canini, and A. Sapio, “Efficient Sparse Collective Communica- tion and its Application to Accelerate Distributed Deep Learning,” in Proceedings of the 2021 ACM SIGCOMM 2021 Conference, 2021, pp. 676–691. [33] V . Monga, Y . Li, and Y . C. Eldar, “Algorithm unrolling: Interpretable, efficient deep learning for signal and image processing,” IEEE Signal Processing Magazine, vol. 38, no. 2, pp. 18–44, 2021. 12[34] T. V ogels, S. P. Karimireddy, and M. Jaggi, “PowerSGD: Practical Low-Rank Gradient Com- pression for Distributed Optimization,” in Advances in Neural Information Processing Systems, vol. 32, 2019. [35] M. Nagel, R. A. Amjad, M. Van Baalen, C. Louizos, and T. Blankevoort, “Up or Down? Adaptive Rounding for Post-training Quantization,” inInternational Conference on Machine Learning. PMLR, 2020, pp. 7197–7206. [36] Z. Galil and K. Park, “A Linear-time Algorithm for Concave One-dimensional Dynamic Programming,” Information Processing Letters, vol. 33, no. 6, pp. 309–311, 1990. [37] A. Aggarwal, M. Klawe, S. Moran, P. Shor, and R. Wilber, “Geometric applications of a matrix searching algorithm,” in Proceedings of the second annual symposium on Computational geometry, 1986, pp. 285–292. [38] “Example SMAWK code,” https://github.com/pombredanne/code-5/blob/master/recipes/Python/ 117244_SMAWK_totally_monotone_matrix_searching/recipe-117244.py, accessed 01-Mar-21. 13Algorithm 3 Basic Dynamic Programming Algorithm 1: Input: X ∈ Rd, s∈ N. 2: Compute C : [d] × [d] → R+ using X. 3: for j = 2 to d do 4: MSE[2, j] = C[1, j] 5: for i = 3 to s do 6: for j = i to d do 7: MSE[i, j] = min k∈{i,...,j} MSE[i − 1, k] + C[k, j] 8: j = d 9: Q = {x1, xd} 10: for i = s down to 3 do 11: j = argmink∈{i,...,j} MSE[i − 1, k] + C[k, j] 12: Q = Q ∪ {xj} 13: return Q A Basic Algorithm We now describe a simple algorithm that finds the optimal quantization values using the dynamic program, with pseudo-code given by Algorithm 3. After initialization (lines 2-4), the algorithm iteratively computesMSE[i, ·] given MSE[i−1, ·] (lines 5-7) and traces back the optimal quantization values given the solution (lines 8-12). B The SMA WK Algorithm [37] Here, we provide some intuition into how SMAWK operates and achieves its efficiency. The SMAWK algorithm has four main steps: • Pruning Phase: Remove columns that cannot possibly contain a row maximum. This is done by comparing each column with its neighbors and discarding those that cannot be maxima based on the totally monotone property. At the end of this phase, the number of columns can be no larger than the number of rows. • Recursive Reduction: The algorithm reduces the problem size by considering a subset of the rows and columns. It selects every other row and recursively solves the reduced problem. • Candidate Set: After solving the smaller problem, the solution provides candidate columns for the original problem. The algorithm only needs to consider these columns to find the maxima for the skipped rows. • Merge Phase: Combine the results from the reduced problem with the candidate set to find the maximum for each original row. Regarding efficiency, the SMAWK algorithm achieves a time complexity ofO(d) for a d × d matrix. This efficiency is due to the recursive reduction of the problem size and the properties of totally monotone matrices that limit the number of comparisons needed. Namely, the pruning step takes O(#cols), where #cols is the number of columns still being considered. The crux is that the recursive step happens after the pruning, which means that the recursive invocation happens with a number of columns that is, at most, double the number of rows (as the number of rows is halved). This means that the overall complexity of each recursive step is proportional to the number of rows, yielding the recursion: T(n) = T(n/2) + O(n) = O(n). A simple example Python implementation (by David Eppstein) appears here [38]. Our implementation is in optimized C++ [30]. C Proof of Lemma 5.1 Lemma 5.1. C2 satisfies the quadrangle inequality. 14Proof. The lemma claims that, for any a ≤ b ≤ c ≤ d: C2[a, c] + C2[b, d] ≤ C2[a, d] + C2[b, c]. Recall that for any a ≤ c ∈ {1, . . . , d}, we denote b∗ a,c = argmin b∈{a,...,c} C[a, b] + C[b, c]. We prove the lemma by a case analysis: • Case b∗ b,c ≤ b∗ a,d. In this case, we have that: C2(a, c) + C2(b, d) = C(a, b∗ a,c) + C(b∗ a,c, c) + C(b, b∗ b,d) + C(b∗ b,d, d) ≤ (i) C(a, b∗ b,c) + C(b∗ b,c, c) + C(b, b∗ a,d) + C(b∗ a,d, d) ≤ (ii) C(b, b∗ b,c) + C(b∗ b,c, c) + C(a, b∗ a,d) + C(b∗ a,d, d) = C2(b, c) + C2(a, d). Here, the Inequality (i) follows from the definition of b∗ a,c that minimizes the MSE over the interval [xa, xc] and b∗ b,d that minimizes it over [xb, xd]. Inequality (ii) follows from the quadrangle inequality of C (Lemma 4.2), as a ≤ b ≤ b∗ b,c ≤ b∗ a,d, and thus C(a, b∗ b,c) + C(b, b∗ a,d) ≤ C(b, b∗ b,c) + C(a, b∗ a,d). • Case b∗ b,c > b∗ a,d. In this case, we have that: C2(a, c) + C2(b, d) = C(a, b∗ a,c) + C(b∗ a,c, c) + C(b, b∗ b,d) + C(b∗ b,d, d) ≤ (i) C(a, b∗ a,d) + C(b∗ a,d, c) + C(b, b∗ b,c) + C(b∗ b,c, d) ≤ (ii) C(b, b∗ b,c) + C(b∗ b,c, c) + C(a, b∗ a,d) + C(b∗ a,d, d) = C2(b, c) + C2(a, d). Here, the Inequality (i) follows again from b∗ a,c and b∗ b,d being optimal for [xa, xc] and [xb, xd]. Inequality (ii) follows from the quadrangle inequality ofC, as b∗ a,d ≤ b∗ b,c ≤ c ≤ d and, therefore, C(b∗ a,d, c) + C(b∗ b,c, d) ≤ C(b∗ a,d, d) + C(b∗ b,c, c). Together, this concludes the proof. D No apparent closed-form solution for s >3 We explain why our acceleration method from Section 4 fails for s >3. Consider computing the location of two additional quantization values b ≤ u between xa and xc. Similarly to the above analysis, we define by Q(b, u) the resulting sum of variances for all entries in [xa, xc]. Then: Q(b, u) = X x∈[xa,b] (b − x)(x − xa) + X x∈(b,u] (u − x)(x − b) + X x∈(u,xc] (xc − x)(x − u). Computing the partial derivatives, we then get: ∂Q(b, u) ∂b = X x∈[xa,b] (x − xa) − X x∈(b,u] (u − x). ∂Q(b, u) ∂u = X x∈(b,u] (x − b) − X x∈(u,xc] (xc − x). The challenge now is that both derivatives are non-continuous, and there are multiple indicesi, jsuch that Q(xi, xj) < 0 but Q(xi+1, xj) ≥ 0 or Q(xi, xj+1) ≥ 0. Accordingly, it seems unlikely that a closed-form solution that is computable in constant time follows from this approach. 15Algorithm 4 Apx. QUIVER 1: Input: X ∈ Rd, s, m∈ N. 2: S = \b x1 + ℓ · xd−x1 m | ℓ ∈ {0, . . . , m} \t 3: Preprocess(X, m) ▷ Enables computing Cm[k, j] in constant time (Appendix E). 4: for j = 2 to m do 5: MSE[2, j] = Cm[1, j] 6: for i = 3 to s do 7: K[i, ·] = SMAWK(Z) ▷ Where Z[k, j] ≜ MSE[i − 1, k] +Cm[k, j] ∀k, j. 8: MSE[i, j] = MSE[i − 1, K[i, j]] + Cm[K[i, j], j] for all j ∈ {i, . . . , m}. 9: Q = {s0, sm} 10: j = m 11: for i = s down to 3 do 12: j = K[i, j] 13: Q = Q ∪ {sj} 14: return Q E Preprosessing for Apx. QUIVER Recall that, for S = \b x1 + ℓ · xd−x1 m | ℓ ∈ {0, . . . , m} \t and sℓ = x1 + ℓ · xd−x1 m , our goal is to compute the following arrays in O(d) time: αℓ = X x∈[s0,sℓ] 1 , β ℓ = X x∈[s0,sℓ] x , γ ℓ = X x∈[s0,sℓ] x2 ∀ℓ ∈ {1, . . . , m} . Denoting δ = xd−x1 m , the first step is to make a pass over the input and for each x ∈ X calculate ℓx = \u0004x−x1 δ \u0005 and set Aℓ = X x|ℓx=ℓ 1 , B ℓ = X x|ℓx=ℓ x , Γℓ = X x|ℓx=ℓ x2 ∀ℓ ∈ {1, . . . , m} . Next, we make an O(m) time pass to compute the cumulative sums: αℓ = ℓX i=1 Ai , β ℓ = ℓX i=1 Bi , γ ℓ = ℓX i=1 Γi ∀ℓ ∈ {1, . . . , m} . We note that an optimization that proved useful for improving the runtime in practice is to remove empty intervals after the first step. That is, we retain only intervals for which Aℓ > 0, thus reducing the number of intervals from m to m′ ≤ m, which can be markedly smaller in practice. F Apx. QUIVER Pseudo-code We describe the pseudo-code of Apx. QUIVER, which is given by Algorithm 4. We start by preprocessing the input to obtain the α, β, γarrays ( Line 3). Next, we initialize the first row of the matrix, which only has m columns, using Cm (Line 4). Follows are s − 2 invocations of the SMAWK algorithm, each yielding the next row in MSE and its minimizers K[i, ·] (Line 6). Finally, we compute the resulting quantization value set Q from K and S (Line 11). G QUIVER Acceleration Evaluation Here, we evaluate by how much Accelerated QUIVER is faster than QUIVER. The results, depicted in Figure 4, show that Accelerated QUIVER is up to 5.4× faster for s = 3 and is consistently faster throughout. Interestingly, the speedup is more significant in odd values of s. This is because the number of SMAWK invocations is ⌊s/2⌋ −1 in Accelerated QUIVER (e.g., it does not invoke SMAWK at all for s = 3, only once for s = 5, etc.), compared to s − 2 invocations in QUIVER. 16H Additional evaluation results Additional evaluation results of exact solutions. We provide results for additional input vectors distributions: Normal (Figure 5), Exponential (Figure 6), Truncated Normal (Figure 7), and Weibull (Figure 8). As shown, all follow the same trends in terms of vNMSE, while the runtime is largely independent of the input distribution. I ASQ Approximation Baselines In the ZipML paper [26], the authors propose two heuristic methods for improving the runtime. The first heuristic includes calculating the optimal solution on a subset of X called candidate points (CP); they further present an analysis that bounds the error with respect to the maximal difference between consecutive CPs and the maximal number of entries in X between consecutive CPs; however, as they do not provide a way to select the CPs, we consider two natural choices: using Uniform CPs, i.e., \b x1 + ℓ · xd−x1 m | ℓ ∈ {0, . . . , m} \t .2 This variant is termed ‘ZipML-CP Unif.’ in our evaluation. The second choice of CP is Quantiles, which uses the set \b x⌊1+ℓ·(d−1)/m⌋ | ℓ ∈ {0, . . . , m} \t . This variant is termed ‘ZipML-CP Quant.’ in our evaluation. The second heuristic has a bicretira MSE guarantee: using 2s quantization values, it ensures that the MSE is at most twice that of the optimal solution with s quantization values. This variant is termed ‘ZipML 2-Apx’ in our evaluation. 2We note that this is different our histogram approach in two aspects: (i) we stochastically quantize X into the set S and (ii) we use weights to consider the number of entries in each histogram bin. 103 104 Time [ms] LogNormal(0,1) 220 221 222 223 224 Input Dimension (d) 102 103 104 Time [ms] 4 6 8 10 12 14 16 #Quantization Values (s) Normal(0,1) QUIVER Acc. QUIVER Figure 4: The speedup attainable by Accelerated QUIVER, as a function of s (for fixed d = 223) and d (for fixed s = 8), on the Normal and LogNormal distributions. 17212 215 218 221 224 10 1 101 103 105 Time [ms] 212 215 218 221 224 Dimension (d) 10 1 101 103 105 Time [ms] (a) s = 4(upper), s = 16(lower). 22 23 24 25 26 10 3 10 1 vNMSE 22 23 24 25 26 #Quantization values (s) 101 103 Time [ms]  (b) d = 212. 22 23 24 25 26 10 3 10 1 vNMSE 22 23 24 25 26 #Quantization values (s) 103 106 Time [ms]  (c) d = 216. ZipML Acc. QUIVER Figure 5: Comparing exact solutions with Normal(0, 1) distributed input. 212 215 218 221 224 10 1 101 103 105 Time [ms] 212 215 218 221 224 Dimension (d) 10 1 101 103 105 Time [ms] (a) s = 4(upper), s = 16(lower). 22 23 24 25 26 10 3 10 1 vNMSE 22 23 24 25 26 #Quantization values (s) 101 103 Time [ms]  (b) d = 212. 22 23 24 25 26 10 3 10 1 vNMSE 22 23 24 25 26 #Quantization values (s) 103 106 Time [ms]  (c) d = 216. ZipML Acc. QUIVER Figure 6: Comparing exact solutions with Exponential(1) distributed input. We also compare against ALQ [ 28], which fits the parameters of a truncated normal distribution to approximate the distribution of the input vector after normalizing it by its norm. It then uses an iterative solution to approximate the optimal quantization values of the fitted distribution up to the desired precision. As suggested by the authors, we use ten iterations, which were shown to converge to the optimal quantization values for the resulting (truncated normal) distribution. Additional evaluation results of approximate solutions. Similarly, we show the approximation algorithms evaluation results for the various distributions and s values: Normal (Figure 9), Exponen- tial (Figure 10), Truncated Normal (Figure 11), and Weibull (Figure 12). Again, the runtime of all algorithms is weakly affected by the input distribution. Apx. QUIVER is always the most accurate for increasing d values and has a near-optimal vNMSE when using a sufficient value for m (e.g., m ≥ 400) while being markedly faster than all alternatives. 212 215 218 221 224 10 1 101 103 105 Time [ms] 212 215 218 221 224 Dimension (d) 10 1 101 103 105 Time [ms] (a) s = 4(upper), s = 16(lower). 22 23 24 25 26 10 3 10 1 vNMSE 22 23 24 25 26 #Quantization values (s) 101 103 Time [ms]  (b) d = 212. 22 23 24 25 26 10 3 10 1 vNMSE 22 23 24 25 26 #Quantization values (s) 103 106 Time [ms]  (c) d = 216. ZipML Acc. QUIVER Figure 7: Exact solutions with TruncNorm(µ = 0, σ2 = 1, a= −1, b= 1) distributed input. 18212 215 218 221 224 10 1 101 103 105 Time [ms] 212 215 218 221 224 Dimension (d) 10 1 101 103 105 Time [ms] (a) s = 4(upper), s = 16(lower). 22 23 24 25 26 10 3 10 1 vNMSE 22 23 24 25 26 #Quantization values (s) 101 103 Time [ms]  (b) d = 212. 22 23 24 25 26 10 3 10 1 vNMSE 22 23 24 25 26 #Quantization values (s) 103 106 Time [ms]  (c) d = 216. ZipML Acc. QUIVER Figure 8: Comparing exact solutions with Weibull(1, 1) distributed input. 212 215 218 221 224 10 1 2 × 10 2 3 × 10 2 4 × 10 2 6 × 10 2 vNMSE 212 215 218 221 224 Dimension (d) 10 1 101 103 Time [ms] (a) s = 16and m = 400bins. 22 23 24 25 26 10 3 10 1 101 103 vNMSE 22 23 24 25 26 #Quantization values (s) 102 Time [ms]  (b) d = 222 and m = 1000bins. 200 400 600 800 1000 10 2 10 1 vNMSE 200 400 600 800 1000 m 102 Time [ms]  (c) d = 222 and s = 32. ZipML-CP Unif. ALQ ZipML-CP Quant. Apx. QUIVER ZipML 2-Apx Optimal Figure 9: Comparing approximate solutions with Normal(0, 1) distributed input. J Additional Overheads We measure the sort and quantize operations using the same EC2 server that is also equipped with an NVIDIA T4 GPU, PyTorch v2.1.2, and CUDA tool kit v12.3. As shown in Figure 13, both operations are fast even for large vectors, despite the usage of a somewhat weak GPU. This specific measurement was done over the LogNormal(0,1) distribution, but the sorting and quantization times are largely independent of the specific distribution and were similar to other tested distributions as well. K Generalizing Our Algorithms to Weighted Inputs We generalize our algorithms for processing sorted weighted inputs X, W∈ Rd (where each entry has value yℓ and weight wℓ and x1 ≤ x2 ≤ . . . , xd).3 Most of the algorithmic parts only require a revised method for computing C in constant time, which is achieved through the modified pre-processing procedure below. For simplicity, we only discuss the basic QUIVER variant and leave the acceleration as future work. Pre-processing. To allow constant time computation of weighted C, denoted Cw, for weighted inputs we need another auxiliary array. Namely, we define the following: 3Similarly to the unweighted case, the sorted vector requirement is only needed for the exact solutions. 19212 215 218 221 224 10 2 10 1 vNMSE 212 215 218 221 224 Dimension (d) 10 1 101 103 Time [ms] (a) s = 16and m = 400bins. 22 23 24 25 26 10 3 10 1 101 103 vNMSE 22 23 24 25 26 #Quantization values (s) 102 Time [ms]  (b) d = 222 and m = 1000bins. 200 400 600 800 1000 10 2 10 1 vNMSE 200 400 600 800 1000 m 102 Time [ms]  (c) d = 222 and s = 32. ZipML-CP Unif. ALQ ZipML-CP Quant. Apx. QUIVER ZipML 2-Apx Optimal Figure 10: Comparing approximate solutions with Exponential(1) distributed input. 212 215 218 221 224 10 2 10 1 vNMSE 212 215 218 221 224 Dimension (d) 10 1 101 103 Time [ms] (a) s = 16and m = 400bins. 22 23 24 25 26 10 3 10 1 101 103 vNMSE 22 23 24 25 26 #Quantization values (s) 102 Time [ms]  (b) d = 222 and m = 1000bins. 200 400 600 800 1000 10 2 10 1 vNMSE 200 400 600 800 1000 m 102 Time [ms]  (c) d = 222 and s = 32. ZipML-CP Unif. ALQ ZipML-CP Quant. Apx. QUIVER ZipML 2-Apx Optimal Figure 11: Approx. solutions with TruncNorm(µ = 0, σ2 = 1, a= −1, b= 1) distributed input. αj = X (x,w)∈Xj w , j ∈ {1, . . . , d} , βj = X (x,w)∈Xj w · x , j ∈ {1, . . . , d} , γj = X (x,w)∈Xj w · x2 , j ∈ {1, . . . , d} . Then, we can then write: Cw[k, j] = X xℓ∈[xk,xj] w · (xj − xℓ)(xℓ − xk) = X xℓ∈(xk,xj] w · (xj − xℓ)(xℓ − xk) = xj · xk · X xℓ∈(xk,xj] wℓ + (xj − xk) · X xℓ∈(xk,xj] wℓ · xℓ − X xℓ∈(xk,xj] wℓ · x2 ℓ = xj · xk · (αj − αk) + (xj − xk) · (βj − βk) − (γj − γk). Observe that Cw clearly satisfies the quadrangle inequality, and thus, the correctness follows. The approximation variant also follows similarly. 20212 215 218 221 224 10 2 10 1 vNMSE 212 215 218 221 224 Dimension (d) 10 1 101 103 Time [ms] (a) s = 16and m = 400bins. 22 23 24 25 26 10 3 10 1 101 103 vNMSE 22 23 24 25 26 #Quantization values (s) 102 Time [ms]  (b) d = 222 and m = 1000bins. 200 400 600 800 1000 10 2 10 1 vNMSE 200 400 600 800 1000 m 102 Time [ms]  (c) d = 222 and s = 32. ZipML-CP Unif. ALQ ZipML-CP Quant. Apx. QUIVER ZipML 2-Apx Optimal Figure 12: Comparing approximate solutions with Weibull(1, 1) distributed input. 211 213 215 217 219 221 223 Dimension (d) 100 101 102 Time [ms] Sort Quantize Figure 13: Sort and quantization times (s = 16) vs. d on a T4 GPU. 21",
      "meta_data": {
        "arxiv_id": "2402.03158v2",
        "authors": [
          "Ran Ben-Basat",
          "Yaniv Ben-Itzhak",
          "Michael Mitzenmacher",
          "Shay Vargaftik"
        ],
        "published_date": "2024-02-05T16:27:59Z",
        "pdf_url": "https://arxiv.org/pdf/2402.03158v2.pdf",
        "github_url": "https://github.com/ranbenbasat/QUIVER"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the Adaptive Stochastic Quantization (ASQ) problem, which aims to minimize quantization error by selecting quantization values adaptively to a specific input, but existing optimal solutions are computationally infeasible. The main contributions are: (1) Introducing QUIVER, an algorithm that finds optimal ASQ solutions with asymptotically improved time complexity from O(s \n\t d^2) to O(s \n\t d) and space complexity from O(d^2) to O(s \n\t d). (2) Developing Accelerated QUIVER, an even faster variant leveraging a closed-form solution for s=3, which halves the number of dynamic programming steps. (3) Presenting Approximate QUIVER (Apx. QUIVER), a fast approximation algorithm for large inputs that discretizes the search space, offering an appealing tradeoff between accuracy and speed with strong theoretical guarantees. (4) Demonstrating the practical efficiency of these algorithms, achieving up to four orders of magnitude speedup over state-of-the-art ASQ methods, making optimal and near-optimal adaptive quantization feasible for large vectors on commodity hardware.",
        "methodology": "The core methodology involves revisiting the dynamic programming approach for ASQ. First, a novel preprocessing method is introduced, utilizing cumulative sum arrays (beta, gamma) to compute the sum of variances C[k, j] in constant time, reducing space complexity. Second, QUIVER leverages the observation that C satisfies the quadrangle inequality, allowing the application of the SMAWK algorithm to find row minimas in an implicitly defined totally monotone matrix in O(d) time per iteration, leading to O(s \n\t d) total time. Third, Accelerated QUIVER further improves runtime by deriving a closed-form solution for s=3 to compute C2[k, j] (optimal MSE for three quantization values), enabling the dynamic program to place two quantization values at a time, effectively halving the number of SMAWK invocations. Finally, Approximate QUIVER discretizes the search space for quantization values onto a uniform grid, using a modified preprocessing for this discretized set and applying the QUIVER algorithm, achieving faster approximate solutions.",
        "experimental_setup": "All algorithms were implemented in C++ and evaluated on an AWS EC2 g4dn.4xlarge server equipped with Intel Cascade Lake CPUs and 64 GB RAM, running Ubuntu 22.04 OS. Results were averaged over 5 random seeds. The experiments used vectors with independent and identically distributed (i.i.d.) entries following LogNormal, Normal, Exponential, Truncated Normal, and Weibull distributions, which are representative of data found in ML applications like gradients and activations. Baselines for comparison included ZipML (an exact solution) and several approximate variants: ZipML-CP Quantiles, ZipML-CP Uniform, ZipML 2-Approximation, and ALQ. Performance was measured using empirical vector normalized Mean Squared Error (vNMSE) and runtime (in milliseconds). Additionally, GPU overheads for sorting and quantization were measured on an NVIDIA T4 GPU for large vectors.",
        "limitations": "The exact QUIVER algorithms are not GPU friendly, which can be a limitation for certain high-performance computing scenarios. Additionally, the exact solutions (QUIVER and Accelerated QUIVER) assume that the input vector is pre-sorted. If the input vector is not sorted, an initial sorting step increases the runtime complexity to O(d \n\t log d + s \n\t d). However, the Approximate QUIVER algorithm does not require the input vector to be sorted, maintaining its O(d + s \n\t m) time complexity even for unsorted inputs.",
        "future_research_directions": "Future research could focus on designing GPU-friendly ASQ algorithms to further enhance performance on modern hardware. Another direction is to generalize and further optimize the accelerated variants of QUIVER for weighted inputs, beyond the basic generalization presented in the paper, to improve efficiency for applications dealing with empirical distributions or importance-weighted data.",
        "experimental_code": "from setuptools import setup\nfrom torch.utils.cpp_extension import CppExtension, BuildExtension\nimport quiver_cpp\nimport torch\n\n# setup.py content for C++ binding\next_modules = []\nextension = CppExtension(\n    'quiver_cpp', ['python_bindings.cpp'],\n    extra_compile_args={'cxx': ['-O2']})\next_modules.append(extension)\nsetup(\n    name='quiver_cpp',\n    ext_modules=ext_modules,\n    cmdclass={'build_ext': BuildExtension})\n\n# Algorithms from speed_error_tests.py\ndef quiver_exact(svec, s):\n    return quiver_cpp.quiver_exact(svec, s)\n\ndef quiver_exact_accelerated(svec, s):\n    return quiver_cpp.quiver_exact_accelerated(svec, s)\n\ndef quiver_approx(svec, s, m):\n    return quiver_cpp.quiver_approx(svec, s, m)\n\ndef quantize(vec, sqv):\n    # 'device' is assumed to be a globally defined variable in the original script's context\n    buckets = torch.bucketize(vec, sqv)\n    up = torch.take(sqv, buckets)\n    down = torch.take(sqv, torch.clip(buckets - 1, min=0))\n    p = (up - vec) / (up - down)\n    r = torch.rand(p.numel(), device=device) # 'device' comes from main script scope\n    return down + (up - down) * (p < r)\n\ndef calc_vNMSE(vec, sqv):\n    buckets = torch.bucketize(vec, sqv)\n    up = sqv[buckets]\n    down = sqv[buckets - 1]\n    return torch.sum((up - vec) * (vec - down)) / torch.norm(vec, 2) ** 2",
        "experimental_info": "The experimental evaluation is conducted using the 'speed_error_tests.py' script. The following algorithms are tested: 'quiver_exact' (implementing the O(s*d) QUIVER), 'quiver_exact_accelerated' (implementing the accelerated O(s*d) version by placing two quantization values at a time), and 'quiver_approx' (implementing the approximate version with a discretized search space). Input data vectors are generated from various distributions: LogNormal(0,1), Normal(0,1), Exponential(1), Truncated Normal (threshold=1), and Weibull(1.0, 1.0). Vector dimensions (d) are powers of 2, ranging from 2^10 to 2^24. The number of quantization levels (s) is also powers of 2, specifically 2^nbits where nbits ranges from 2 to 6 (i.e., 4, 8, 16, 32, 64 levels). For the 'quiver_approx' method, the number of discretization bins (m) is varied, including 100, 200, 400, 600, 800, and 1000. For exact methods, m is a placeholder. Each experimental run is repeated for a specified number of random seeds (default 3 seeds from a pool of 10: [42, 104, 78, ...]). Performance and accuracy metrics collected include: time taken for sorting the input vector, time for the SQV (quantization values) calculation (the core algorithm time), time for quantizing the vector, and the volumetric Normalized Mean Squared Error (vNMSE). Experiments are conducted on either CUDA-enabled GPUs or CPUs, depending on availability. A specific condition prevents 'quiver_exact_accelerated' from running on dimensions >= 2^25 due to high memory requirements. All results are saved as '.pt' files in the 'results/speed_error' directory."
      }
    },
    {
      "title": "BitsFusion: 1.99 bits Weight Quantization of Diffusion Model",
      "abstract": "Diffusion-based image generation models have achieved great success in recent\nyears by showing the capability of synthesizing high-quality content. However,\nthese models contain a huge number of parameters, resulting in a significantly\nlarge model size. Saving and transferring them is a major bottleneck for\nvarious applications, especially those running on resource-constrained devices.\nIn this work, we develop a novel weight quantization method that quantizes the\nUNet from Stable Diffusion v1.5 to 1.99 bits, achieving a model with 7.9X\nsmaller size while exhibiting even better generation quality than the original\none. Our approach includes several novel techniques, such as assigning optimal\nbits to each layer, initializing the quantized model for better performance,\nand improving the training strategy to dramatically reduce quantization error.\nFurthermore, we extensively evaluate our quantized model across various\nbenchmark datasets and through human evaluation to demonstrate its superior\ngeneration quality.",
      "full_text": "BitsFusion: 1.99 bits Weight Quantization of Diffusion Model Yang Sui1,2,† Yanyu Li1 Anil Kag1 Yerlan Idelbayev1 Junli Cao1 Ju Hu1 Dhritiman Sagar1 Bo Yuan2 Sergey Tulyakov1 Jian Ren1,∗ 1Snap Inc. 2Rutgers University Project Page: https://snap-research.github.io/BitsFusion Figure 1: Top: Images generated from full-precision Stable Diffusion v1.5.Bottom: Images generated from BitsFusion, where the weights of UNet are quantized into 1.99 bits, achieving 7.9× smaller storage than the one from Stable Diffusion v1.5. All the images are synthesized under the setting of using PNDM sampler [49] with 50 sampling steps and random seed as 1024. Prompts and more generations are provided in App. M. Abstract Diffusion-based image generation models have achieved great success in recent years by showing the capability of synthesizing high-quality content. However, these models contain a huge number of parameters, resulting in a significantly large model size. Saving and transferring them is a major bottleneck for various applications, especially those running on resource-constrained devices. In this work, we develop a novel weight quantization method that quantizes the UNet from Stable Diffusion v1.5 to 1.99 bits, achieving a model with 7.9× smaller size while exhibiting even better generation quality than the original one. Our approach includes several novel techniques, such as assigning optimal bits to each layer, initializing the quantized model for better performance, and improving the training strategy to dramatically reduce quantization error. Furthermore, we extensively evaluate our quantized model across various benchmark datasets and through human evaluation to demonstrate its superior generation quality. 1 Introduction Recent efforts in developing diffusion-based image generation models [ 77, 31, 79, 21, 80] have demonstrated remarkable results in synthesizing high-fidelity and photo-realistic images, leading to various applications such as content creation and editing [ 68, 67, 61, 71, 90, 88, 50, 40], video generation [20, 75, 3, 1, 16, 57, 15], and 3D asset synthesis [ 87, 44, 64, 74, 65], among others. †Work done during an internship at Snap Inc. *Corresponding author. 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2406.04333v2  [cs.CV]  26 Oct 2024However, Diffusion Models (DMs) come with the drawback of a large number of parameters,e.g., millions or even billions, causing significant burdens for transferring and storing due to the bulky model size, especially on resource-constrained hardware such as mobile and wearable devices. Existing studies have explored reducing the model size of large-scale text-to-image diffusion models by designing efficient architectures and network pruning [ 41, 92, 32]. These approaches usually require significant amounts of training due to the changes made to the pre-trained networks. Another promising direction for model storage reduction is quantization [12, 30], where floating-point weights are converted to low-bit fixed-point representations, thereby saving computation memory and storage. There have been emerging efforts on compressing the DMs through quantization [ 73, 38, 17, 39]. However, these approaches still face several major challenges, especially when quantizing large-scale text-to-image diffusion models like Stable Diffusion v1.5 (SD-v1.5) [ 70]. First, many of these methods are developed on relatively small-scale DMs trained on constrained datasets. For example, models trained on CIFAR-10 require modest storage of around100 MB [21, 39]. In contrast, SD-v1.5 necessitates 3.44 GB of storage in a full-precision format. Adapting these methods to SD-v1.5 remains to be a challenging problem. Second, current arts mainly focus on quantizing weights to 4 bits. How to quantize the model to extremely low bit is not well studied. Third, there is a lack of fair and extensive evaluation of how quantization methods perform on large-scale DMs,i.e., SD-v1.5. To tackle the above challenges, this work proposes BitsFusion, a quantization-aware training frame- work that employs a series of novel techniques to compress the weights of pre-trained large-scale DMs into extremely low bits (i.e., 1.99 bits), achieving even better performance (i.e., higher image quality and better text-image alignment). Consequently, we compress the 1.72 GB UNet (FP16)1 of SD-v1.5 into a 219 MB model, achieving a 7.9× compression ratio. Specifically, our contributions can be summarized into the following four dimensions: • Mixed-Precision Quantization for DMs.We propose an effective approach for quantizing DMs in a mixed-precision manner. First, we thoroughly analyze the appropriate metrics to understand the quantization error in the quantized DMs (Sec. 3.2). Second, based on the analysis, we quantize different layers into different bits according to their quantization error (Sec. 3.3). • Initialization for Quantized DMs.We introduce several techniques to initialize the quantized model to improve performance, including time embedding pre-computing and caching, adding balance integer, and alternating optimization for scaling factor initialization (Sec. 4.1). • Improved Training Pipeline for Quantized DMs.We improve the training pipeline for the quantized model with the proposed two-stage training approach (Sec. 4.2). In the first stage, we use the full-precision model as a teacher to train the quantized model through distillation. Our distillation loss forces the quantized model to learn both the predicted noise and the intermediate features from the teacher network. Furthermore, we adjust the distribution of time step sampling during training, such that the time steps causing larger quantization errors are sampled more frequently. In the second stage, we fine-tune the model using vanilla noise prediction [21]. • Extensive Quantitative Evaluation.For the first time in the literature, we conduct extensive quantitative analysis to compare the performance of the quantized model against the original SD-v1.5. We include results on various benchmark datasets, i.e., TIFA [ 25], GenEval [ 13], CLIP score [66] and FID [19] on MS-COCO 2014 validation set [46]. Additionally, we perform human evaluation on PartiPrompts [ 86]. Our 1.99-bit weights quantized model consistently outperforms the full-precision model across various evaluations, demonstrating the effectiveness of our approach. 2 Related Works To enhance model efficiency in terms of storage and computational costs, quantization [11, 59, 58, 43, 36, 60, 48, 84, 45, 53] is adopted for diffusion models [73, 38, 18, 76, 81, 83, 51, 85, 4, 82, 7, 93, 27, 17, 39, 91] with primarily two types: post-training quantization (PTQ) and quantization-aware training (QAT). PTQ does not require a full training loop; instead, it utilizes a limited calibration dataset to adjust the quantization parameters. For example, PTQ4DM [73] calibrates the quantization parameters to minimize the quantization error of DMs. Q-Diffusion [38] minimizes the quantization error via the block-wise reconstruction [ 42]. PTQD [ 18] integrates quantization noise into the 1For SD-v1.5, we measure the generation quality using the FP32 format. However, since SD-v1.5 FP16 has similar performance to SD-v1.5 FP32, we use SD-v1.5 FP16 to calculate our compression ratio. 2stochastic noise inherent in the sampling steps of DMs. TDQ [ 76] optimizes scaling factors for activations across different time steps, applicable to both PTQ and QAT strategies. TFMQ [ 27] focuses on reconstructing time embedding and projection layers to prevent over-fitting. However, PTQ often results in performance degradation compared to QAT, particularly when aiming for extremely low-bit DMs. In contrast, QAT involves training the full weights to minimize the quantization error, thereby achieving higher performance compared to PTQ. For instance, EfficientDM [17], inspired by LoRA [ 24], introduces a quantization-aware low-rank adapter to update the LoRA weights, avoiding training entire weights. Q-DM [ 39] employs normalization and smoothing operation on attention features through proposed Q-attention blocks, enhancing quantization performance. Nevertheless, existing works primarily study 4 bits and above quantization on small-scale DMs trained on constrained datasets. In this paper, we focus on quantizing large-scale Stable Diffusion to extremely low bits and extensively evaluating the performance across different benchmark datasets. 3 Mixed Precision Quantization for Diffusion Models In this section, we first go through the formulations of weight quantization and generative diffusion models. We then determine the mixed-precision strategy, assigning optimized bit widths to different layers to reduce the overall quantization error. Specifically, we first analyze the quantization error of each layer in the diffusion model and conclude sensitivity properties. Then, based on the analysis, we assign appropriate bits to each layer by jointly considering parameter efficiency (i.e., size savings). 3.1 Preliminaries Quantization is a popular and commonly used technique to reduce model size. While many quantization forms exist, we focus on uniform quantization, where full-precision values are mapped into discrete integer values as follows: θint = Clip(⌊θfp s ⌉ + Iz, 0, 2b − 1), (1) where θfp denotes the floating-point weights, θint is the quantized integer weights, s is the scaling factor, Iz is the zero point, and b is the quantization bit-width. ⌊·⌉ denotes the nearest rounding operation and Clip(·) denotes the clipping operation that constrains θint within the target range. Following the common settings [38, 17], we apply the channel-wise quantization and set 8 bits for the first and last convolutional layer of the UNet. Stable Diffusion. Denoising diffusion probabilistic models [ 77, 21] learn to predict real data distribution x ∼ pdata by reversing the ODE flow. Specifically, given a noisy data sample zt = αtx + σtϵ (αt and σt are SNR schedules and ϵ is the added ground-truth noise), and a quantized denoising model ˆϵθint,s parameterized by θint and s, the learning objective can be formulated as follows, Lθint,s = Et,x [∥ϵ − ˆϵθint,s(t, zt, c)∥] , (2) where t is the sampled time step and c is the input condition (e.g., text embedding). Note that during the training of quantized model, we optimize θfp and s by backpropagating Lθint,s via Straight- Through Estimator (STE) [2] and quantize the weights to the integers for deployment. Here, for the notation simplicity, we directly use θint to represent the optimized weights in the quantized models. The latent diffusion model [70] such as Stable Diffusion conducts the denoising process in the latent space encoded by variational autoencoder (V AE) [34, 69], where the diffusion model is the UNet [9]. This work mainly studies the quantization for the UNet model, given it is the major bottleneck for the storage and runtime of the Stable Diffusion [41]. During the inference time, classifier-free guidance (CFG) [22] is usually applied to improve the generation, ˜ϵθint,s(t, zt, c) =wˆϵθint,s(t, zt, c) − (w − 1)ˆϵθint,s(t, zt, ∅), (3) where w ≥ 1 and ˆϵθint,s(t, zt, ∅) denotes the generation conditioned on the null text prompt ∅. 3.2 Per-Layer Quantization Error Analysis Obtaining Quantized Models.We first perform a per-layer sensitivity analysis for the diffusion model. Specifically, given a pre-trained full-precision diffusion model, we quantize each layer to 1, 2, and 3 bits while freezing others at full-precision, and performing quantization-aware training (QAT) 3SD-v1.5 CA toq CA tok CA tov CA tok CA tok RB conv RB conv shortcut CA tok RB conv shortcut (a) Left most column shows the images synthesized by SD-v1.5 FP32 and other columns show images generated by the quantized models, where only one layer is quantized (e.g., CA toq denotes the cross-attention layer for Query projection is quantized and RB conv shortcut denotes the Convolution Shotcut layer in Residual Block is quantized. The quantized layers follow the same order of highlighted layers in (b) and (c), from left to right. Quantizing the layers impact both the image quality (as in RB conv shortcut) and text-image alignment (e.g., the teddy bear disappears after quantizing some CA tok layers). CA tokCA toqCA tovCA tokRB convRB conv shortcutCA tokRB conv shortcutCA tok (b) MSE value by quantizing layers in SD-v1.5. CA tokCA toq CA tov CA tokRB convRB conv shortcutCA tokRB conv shortcut CA tok (c) CLIP score drop by quantizing layers in SD-v1.5. Figure 2: 1-bit quantization error analysis for all the layers from the UNet of SD-v1.5. respectively. For instance, for the SD-v1.5 UNet with 256 layers (excluding time embedding, the first and last layers), we get a total of 768 quantized candidates. We perform QAT over each candidate on a pre-defined training sub dataset, and validate the incurred quantization error of each candidate by comparing it against the full-precision model (more details in App. B). Measuring Quantization Errors.To find the appropriate way to interpret the quantization error, we analyze four metrics: Mean-Squared-Error (MSE) that quantifies the pixel-level discrepancies between images (generations from floating and the quantized model in our case), LPIPS [89] that assesses human-like perceptual similarity judgments, PSNR [23] that measures image quality by comparing the maximum possible power of a signal with the power of a corrupted noise, and CLIP score [66] that evaluates the correlation between an image and its language description. After collecting the scores (examples in Fig. 2b and Fig. 2c, full metrics are listed in App. F), we further measure the consistency of them by calculating the Pearson correlation [ 8] for different metrics under the same bit widths (in Tab. 1), and different bit widths under the same metric (in Tab. 2). With these empirical results, we draw the following two main observations. Observation 1: MSE, PSNR, and LPIPS show strong correlation and they correlate well with the visual perception of image quality. Tab. 1 shows that MSE is highly correlated with PSNR and LPIPS under the same bit width. Additionally, we observe a similar trend of per-layer quantization error under different bit widths, as in Tab. 2. As for visual qualities in Fig. 2a and 2b, we can see that higher MSE errors lead to severe image quality degradation, e.g., the highlighted RB conv shortcut. Therefore, the MSE metric effectively reflects quality degradations incurred by quantization, and it is unnecessary to incorporate PSNR and LPIPS further. Observation 2: After low-bit quantization, changes in CLIP score are not consistently correlated with MSE across different layers. Although some layers show smaller MSE, they may experience larger semantic degradation, reflected in larger CLIP score changes. 4Table 1: Pearson correlation (absolute value) of quantization error between different metrics (e.g., MSE vs. PSNR denotes the correlation between two metrics) when quantizing individual layers to 1, 2, and 3 bits. CS denotes CLIP Score. MSEvs. PSNR MSEvs. LPIPS MSEvs. CS 1 bit 0.870 0.984 0.733 2 bit 0.882 0.989 0.473 3 bit 0.869 0.991 0.535 Table 2: Pearson correlation (absolute value) of quantization error between different bit pairs (e.g., 1 vs. 2 denotes the correlation between the two bit widths) for a single metric when quantiz- ing individual layers to 1, 2, and 3 bits. MSE PSNR LPIPS CLIP Score 1 vs.2 bit 0.929 0.954 0.943 0.504 1 vs.3 bit 0.766 0.843 0.802 0.344 2 vs.3 bit 0.887 0.923 0.895 0.428 We notice that, after quantization, the CLIP score changes for all layers only have a weak correlation with MSE, illustrated in Tab. 1. Some layers display smaller MSE but larger changes in CLIP score. For example, in Fig. 2b, the MSE of CA tok layer (5th highlighted layer (green) from left to right) is less than that of RB conv layer (6th highlighted layer (orange) from left to right), yet the changes in CLIP score are the opposite. As observed in the first row of Fig. 2a, compared to RB conv layer, quantizing this CA tok layer changes the image content from \"a teddy bear\" to \"a person\", which diverges from the text prompt A teddy bear on a skateboard in Times Square, doing tricks on a cardboard box ramp. This occurs because MSE measures only the difference between two images, which does not capture the semantic degradation. In contrast, the CLIP score reflects the quantization error in terms of semantic information between the text and image. Thus, we employ the CLIP score as a complementary metric to represent the quantization error. 3.3 Deciding the Optimal Precision With the above observations, we then develop the strategy for bit-width assignments. We select MSE and CLIP as our quantitative metrics, along with the number of parameters of each layer as the indicator of size savings. Assigning bits based on MSE.Intuitively, layers with more parameters and lower quantization error are better candidates for extremely low-bit quantization, as the overall bit widths of the model can be significantly reduced. According to this, we propose a layer size-aware sensitivity score S. For the ith layer, its sensitivity score for the b-bits (b ∈ {1, 2, 3}) is defined as Si,b = Mi,bN−η i , where M denotes the MSE error, N is the total number of parameters of the layer, and η ∈ [0, 1] denotes the parameter size factor. To determine the bit width (i.e., b∗) for each layer, we define a sensitivity threshold as So, and the ith layer is assigned to b∗ i -bits, where b∗ i = min{b|Si,b < So}. The remaining layers are 4 bits. Assigning bits based on CLIP score.For the layers with a high CLIP score dropping after quantiza- tion, instead of assigning bits based on sensitivity score as discussed above, we directly assign higher bits to those layers. Therefore, the quantized model can produce content that aligns with the semantic information of the prompt. We provide the detailed mixed-precision algorithm in Alg. 1 of App. B. 4 Training Extreme Low-bit Diffusion Model With the bits of each layer decided, we then train the quantized model with a series of techniques to improve performance. The overview of our approach is illustrated in Fig. 3. 4.1 Initializing the Low-bit Diffusion Model Time Embedding Pre-computing and Caching.During the inference time of a diffusion model, a time step t is transformed into an embedding through projection layers to be incorporated into the diffusion model. As mentioned by existing works [27], the quantization of the projection layers can lead to large quantization errors. However, the embedding from each time step t is always the same, suggesting that we can actually pre-compute the embedding offline and load cached values during inference, instead of computing the embedding every time. Furthermore, the storage size of the time embedding is 25.6× smaller than the projection layers. Therefore, we pre-compute the time embedding and save the model without the project layers. More details are provided in App. C. 5E  Predicted Noise Stable Diffusion v1.5 Quantized UNetStage-IITraining Stage-ITraining𝒕~ Beta(𝜶,𝜷) Prompt: A cat Removed Time Projection LayersPre-computed and Cached Time Features EmbeddingTokenizer Tokenizer /Embedding Mixed Precision Recipe Stable Diffusion v1.5 Quantized UNetInitialization Analysis Predicted Noise Inference Stage Training Stage  D UNet 1.99 bits UNet FreezeTrainable L noise θ int , sL feat θ int , s L θ int , s ˆϵ θ fp ˆϵ θ int , s Prompt: A dog Figure 3: Overview of the training and inference pipeline for the proposed BitsFusion.Left: We analyze the quantization error for each layer in SD-v1.5 (Sec. 3.2) and derive the mixed-precision recipe (Sec. 3.3) to assign different bit widths to different layers. We then initialize the quantized UNet by adding a balance integer, pre-computing and caching the time embedding, and alternately optimizing the scaling factor (Sec. 4.1). Middle: During the Stage-I training, we freeze the teacher model (i.e., SD-v1.5) and optimize the quantized UNet through CFG-aware quantization distillation and feature distillation losses, along with sampling time steps by considering quantization errors (Sec. 4.2). During the Stage-II training, we fine-tune the previous model with the noise prediction. Right: For the inference stage, using the pre-cached time features, our model processes text prompts and generates high-quality images. Adding Balance Integer.In general, weight distributions in deep neural networks are observed as symmetric around zero [ 94]. To validate the assumption on SD-v1.5, we analyze its weight distribution for the layers under full precision by calculating the skewness of weights. Notably, the skewness of more than 97% of the layers ranges between [−0.5, 0.5], indicating that the weight distributions are symmetric in almost all layers. Further details are provided in App. D. However, existing works on diffusion model quantization overlook the symmetric property [38, 73, 39], as they perform relatively higher bits quantization, e.g., 4 or 8 bits. This will hurt the model performance at extremely low bit levels. For example, in 1-bit quantization, the possible most symmetric integer outcomes can only be {0, 1} or {−1, 0}. Similarly, for 2-bit quantization, the most balanced mapping integers can be either {−2, −1, 0, 1} or {−1, 0, 1, 2}, significantly disrupting the symmetric property. The absence of a single value among 2 or 4 numbers under low-bit quantization can have a significant impact. To tackle this, we leverage the bit balance strategy [37, 56] to initialize the model. Specifically, we introduce an additional value to balance the original quantization values. Namely, in a 1-bit model, we adjust the candidate integer set from {0, 1} to {−1, 0, 1}, achieving a more balanced distribution. By doing so, we treat the balanced n-bits weights as log(2n + 1)-bits. Scaling Factor Initialization via Alternating Optimization.Initializing scaling factors is an important step in quantization. Existing QAT works typically employ the Min-Max initialization strategy [17, 52] to ensure the outliers are adequately represented and preserved. However, such a method faces challenges in extremely low-bit quantization settings like 1-bit, since the distribution of the full-precision weights is overlooked, leading to a large quantization error and the increased difficulty to converge. Therefore, we aim to minimize the ℓ2 error between the quantized weights and full-precision weights with the optimization objective as: min s ∥s · (θint − Iz) − θfp∥2. (4) Nevertheless, considering the rounding operation, calculating an exact closed-form solution is not straightforward [29]. Inspired by the Lloyd-Max algorithm [28, 54], we use an optimization method on scaling factor s to minimize the initialization error of our quantized diffusion model as follows: θj int = Qint(θfp, sj−1); sj = θj fp(θj int − Iz)⊺ (θj int − Iz)(θj int − Iz)⊺ , (5) where Qint(·) denotes the integer mapping quantization operation that converts the full-precision weights to integer as Eq. (1), and j represents the iterative step. The optimization is done for 10 steps. 64.2 Two-Stage Training Pipeline With the mixed-precision model initialized, we introduce the two-stage training pipeline. In Stage-I, we train the quantized model using the full-precision model as the teacher through distillation loss. In Stage-II, we fine-tune the model from the previous stage using noise prediction [21, 80]. CFG-aware Quantization Distillation.Similar to existing works [11], we fine-tune the quantized diffusion model to improve the performance. Here both the weights and scaling factors are opti- mized. Additionally, we notice that training the quantized model in a distillation fashion using the full-precision model yields better performance than training directly with vanilla noise prediction. Furthermore, during distillation, it is crucial for the quantized model to be aware of CFG, i.e., text dropping is applied during distillation. Specifically, our training objective is as follows: Lnoise θint,s = Et,x \u0002 ∥ˆϵθfp (t, zt, c) − ˆϵθint,s(t, zt, c)∥ \u0003 , c = ∅ if P ∼ U[0, 1] < pelse c, (6) where P controls the text dropping probability during training and p is set as 0.1. Feature Distillation.To further improve the generation quality of the quantized model, we distill the full-precision model at a more fine-grained level through feature distillation [32] as follows: Lfeat θint,s = Et,x \u0002 ∥Fθfp (t, zt, c) − Fθint,s(t, zt, c)∥ \u0003 , (7) where Fθ(·) denotes the operation for getting features from the Down and Up blocks in UNet. We then have the overall distillation loss Ldist in Stage-I as follows: Ldist = Lnoise θint,s + λLfeat θint,s, (8) where λ is empirically set as 0.01 to balance the magnitude of the two loss functions. 0 200 400 600 800 1000 Time Steps 0 10 20 30 40 50 60Sampling Frequency 0.000 0.004 0.008 0.012 Quantization Error =1.5, =1 =3, =1 Quantization Error Figure 4: More time steps are sampled towards where larger quantization error occurs. Quantization Error-aware Time Step Sampling.The training of diffusion models requires sampling differ- ent time steps in each optimization iteration. We ex- plore how to adjust the strategy for time step sampling such that the quantization error in each time step can be effectively reduced during training. We first train a 1.99-bit quantized model with Eq. (8). Then, we cal- culate the difference of the predicted latent features be- tween the quantized model and the full-precision model as Et,x[1−¯αt ¯αt ∥ˆϵθfp (t, zt, c) − ˆϵθint,s(t, zt, c)∥2], where t ∈ [0, 1, ··· , 999] and ¯αt is the noise scheduler (detailed derivation in App. E). The evaluation is conducted on a dataset with 128 image-text pairs. Fig. 4 shows the quan- tization error does not distribute equally across all time steps. Notably, the quantization error keeps increasing as the time steps approach t = 999. To mitigate the quantization error prevalent near the time steps t = 999, we propose a sampling strategy by utilizing a distribution specifically tailored to sample more time steps exhibiting the largest quantization errors, thereby enhancing performance. To achieve this goal, we leverage the Beta distribution. Specifically, time steps are sampled according to t ∼ Beta(α, β), as shown in Fig. 4. We empirically set α = 3.0 and β = 1.0 for the best performance. Combining the strategy of time steps sampling with Eq. (8), we conduct the Stage-I training. Fine-tuning with Noise Prediction.After getting the model trained with the distillation loss in Stage-I, we then fine-tune it with noise prediction, as in Eq. (2), in Stage-II. We apply a text dropping with probability as 10% and modify the distribution of time step sampling based on the quantization error, as introduced above. The reason we leverage two-stage fine-tuning, instead of combining Stage-I and Stage-II, is that we observe more stabilized training results. 5 Experiments Implementation Details.We develop our code using diffusers library2 and train the models with AdamW optimizer [33] and a constant learning rate as 1e−05 on an internal dataset. For Stage-I, 2https://github.com/huggingface/diffusers 72.5 3.5 4.5 5.5 6.5 7.5 8.5 9.5 CFG Scales 0.300 0.305 0.310 0.315CLIP Scores SD-v1.5 (32 bits) Ours-I (1.99 bits) Ours-II (1.99 bits) (a) CLIP score on MS-COCO. 2.5 3.5 4.5 5.5 6.5 7.5 8.5 9.5 CFG Scales 0.74 0.76 0.78 0.80 0.82TIFA Scores SD-v1.5 (32 bits) Ours-I (1.99 bits) Ours-II (1.99 bits) (b) Evaluation on TIFA. 2.5 3.5 4.5 5.5 6.5 7.5 8.5 9.5 CFG Scales 0.32 0.36 0.40 0.44 0.48GenEval Scores SD-v1.5 (32 bits) Ours-I (1.99 bits) Ours-II (1.99 bits) (c) Evaluation on GenEval. Figure 5: Comparison between our 1.99-bits model vs. SD-v1.5 on various evaluation metrics with CFG scales ranging from 2.5 to 9.5. Ours-I denotes the model with Stage-I training and Ours-II denotes the model with Stage-II training. Table 3: Comparison with existing quantization meth- ods, including LSQ [ 11], Q- Diffusion [ 38], EfficientDM [17], and Apple-MBP [ 62]. The CLIP score is measured on 1K PartiPrompts. Method Bit-width CLIP score SD-v1.5 32 0.3175 LSQ 2 0.2849 Q-Diffusion 4 0.3137 EfficientDM 2 0.2918 Apple-MBP 2 0.3023 Ours 1.99 0.3212 Table 4: Analysis of our proposed methods measured under vari- ous CFG scales, i.e., 3.5, 5.5, 7.5, and 9.5. We use LSQ [11] as the basic QAT method, which involves the training of weights and scaling factors of a uniformly 2-bit quantized UNet. Then, we gradually introduce each proposed technique to evaluate their effectiveness. CLIP scores are measured on 1K PartiPrompts. Method Bit-width3.5 5.5 7.5 9.5 Average ∆ SD-v1.5 32 0.3110 0.3159 0.3175 0.31800.3156 - QAT-Base 2 0.2679 0.2793 0.2849 0.28680.2797 - +Balance 2.32 0.2990 0.3059 0.3080 0.30860.3054 +0.0257 +Alternating Opt.2.32 0.3061 0.3108 0.3117 0.31150.3100 +0.0046 +Mixed/Caching1.99 0.3055 0.3129 0.3142 0.31450.3118 +0.0018 +Feat Dist. 1.99 0.3086 0.3147 0.3167 0.31690.3142 +0.0024 +Time Sampling1.99 0.3098 0.3159 0.3181 0.31840.3156 +0.0014 +Fine-tuning 1.99 0.3163 0.3192 0.3212 0.32050.3183 +0.0027 we use 8 NVIDIA A100 GPUs with a total batch size of 256 to train the quantized model for 20K iterations. For Stage-II, we use 32 NVIDIA A100 GPUs with a total batch size of 1024 to train the quantized model for 50K iterations. During inference, we adopt the PNDM scheduler [49] with 50 sampling steps to generate images for comparison. Other sampling approaches (e.g., DDIM [78] and DPMSolver [55]) lead to the same conclusion (App. K). Evaluation Metrics.We conduct evaluation on CLIP Score and FID on MS-COCO [47], TIFA [26], GenEval [14], and human evaluation on PartiPrompts [86]. We adopt ViT-B/32 model [10] in CLIP score and the Mask2Former(Swin-S-8×2) [5] in GenEval. App. I provides details for the metrics. 5.1 Main Results Comparison with SD-v1.5.Our quantized 1.99-bits UNet consistently outperforms the full-precision model across all metrics. • 30K MS-COCO 2014 Validation Set.For the CLIP score, as demonstrated in Fig. 5a, attributed to the proposed mixed-precision recipe with the introduced initialization techniques and advanced training schemes in Stage-I, our 1.99-bits UNet, with a storage size of 219MB, achieves perfor- mance comparable to the original SD-v1.5. Following Stage-II training, our model surpasses the performance of the original SD-v1.5. With CFG scales ranging from 2.5 to 9.5, our model yields 0.002 ∼ 0.003 higher CLIP scores. • TIFA. As shown in Fig. 5b, our 1.99-bits model with Stage-I training performs comparably to the SD-v1.5. With the Stage-II training, our model achieves better metrics over the SD-v1.5. • GenEval. We show the comparison results for GenEval in Fig. 5c (detailed comparisons of GenEval score are presented in Appn. L). Our model outperforms SD-v1.5 for all CFG scales. • Human Evaluation.With the question: Given a prompt, which image has better aesthetics and image-text alignment? More users prefer the images generated by our quantized model over SD- v1.5, with the ratio as 54.4%. The results are shown in Fig. 6. We provide a detailed comparison in App. J. 8Table 5: Analysis of η in the mixed- precision strategy. η 0 0.1 0.2 0.3 0.4 0.5 CLIP score 0.3155 0.3173 0.3162 0.3181 0.3171 0.3168 Table 6: Anlysis of λ in dis- tillation loss. λ 1 0.1 0.01 CLIP score 0.3164 0.3159 0.3181 Table 7: Analysis of α in time step-aware sampling. α 1.5 2.0 3.0 CLIP score 0.3169 0.3173 0.3181 Comparison with Other Quantization Approaches.Additionally, we conduct the experiments by comparing our approach with other works including LSQ [11], Q-Diffusion [38], EfficientDM [17], and Apple-MBP [62], as shown in Tab. 3. Our model achieves a higher CLIP score compared with all other works and better performance than SD-v1.5. 5.2 Ablation Analysis Here we perform extensive analysis for our proposed method. We mainly evaluate different experi- mental settings using the CLIP score measured on 1K PartiPrompts [86]. Analysis of the Proposed Techniques.We adopt the LSQ [11] as the basic QAT method to update the weights and scaling factors of a uniform 2-bit UNet with Min-Max initialization. Results are presented in Tab. 4 with the following details: • +Balance. By adding a balance integer, a 2-bit model that typically represents 4 integer values can now represent 5 integers, becoming a 2.32-bit model by log(4 + 1)bits. The average CLIP score has significantly increased from 0.2797 to 0.3054. • +Alternating Opt.By further utilizing the scaling factor initialization via alternating optimization, the average CLIP score of the 2.32-bit model increases to 0.3100. • +Mixed/Caching. By leveraging time embedding pre-computing and caching, we minimize the storage requirements for time embedding and projection layers by only retaining the calculated features. This significantly reduces the averaged bits. Combined with our mixed-precision strategy, this approach reduces the average bits from2.32 to 1.99 bits and can even improve the performance, i.e., CLIP score improved from 0.3100 to 0.3118. • +Feat Dist.By incorporating the feature distillation loss, i.e., Eq. (7), the model can learn more fine-grained information from the teacher model, improving CLIP score from 0.3118 to 0.3142. • +Time Sampling. By employing a quantization error-aware sampling strategy at various time steps, the model focuses more on the time step near t = 999. With this sampling strategy, our 1.99-bits model performs very closely to, or even outperforms, the original SD-v1.5. • +Fine-tuning. By continuing with Stage-II training that incorporates noise prediction, our1.99-bits model consistently outperforms the SD-v1.5 across various guidance scales, improving the CLIP score to 0.3183. Effect ofη in Mixed-Precision Strategy.Tab. 5 illustrates the impact of the parameter size factor η (as discussed in Sec. 3.3) in determining the optimal mixed precision strategy. We generate six different mixed precision recipes with different η with 20K training iterations for comparisons. Initially, we explore the mixed precision strategy determined with and without the parameter size factor. Setting η = 0 results in N−η = 1, indicating that the mixed precision is determined without considering the impact of parameter size. The results show that neglecting the parameter size significantly degrades performance. Further, we empirically choose η = 0.3 in our experiments after comparing different values of η. Effect ofλ of Distillation Loss.Tab. 6 illustrates the impact of the balance factorλ for loss functions in Eq. (8). We empirically choose λ = 0.01 in our experiments after comparing the performance. Effect ofα in Time Step-aware Sampling Strategy.Tab. 7 illustrates the impact of the α for different Beta sampling distribution. As analyzed in Sec. 4.2, the quantization error increases near t = 999. To increase sampling probability near this time step, Beta distribution requires α >1 with β = 1. A larger α enhances the sampling probability near t = 999. Compared to α = 1.5 and α = 2.0, α = 3.0 concentrates more on later time steps and achieves the best performance. We choose α = 3.0 in our experiments. Analysis for Different Schedulers.One advantage of our training-based quantization approach is that our quantized model consistently outperforms SD-v1.5 for various sampling approaches. We conduct extensive evaluations on TIFA to show we achieve better performance than SD-v1.5 for using both DDIM [78] and DPMSolver [55] to perform the sampling. More details are shown in App. K. 9FID Results.As stated in SDXL [63] and PickScore [35], FID may not honestly reflect the actual performance of the model in practice. FID measures the average distance between generated images and reference real images, which is largely influenced by the training datasets. Also, FID does not capture the human preference which is the crucial metric for evaluating text-to-image synthesis. We present FID results evaluated on the 30K MS-COCO 2014 validation set in Fig. 7. Our Stage-I model has a similar FID as SD-v1.5. However, as training progresses, although our Stage-II model is preferred by users, its FID score is higher than both Stage-I and SD-v1.5. Ours (1.99 bits) SD-v1.5 (32 bits) 20 40 60Win Rate (%) 54.41 45.59 Figure 6: Overall human evaluation com- parisons between SD-v1.5 and BitsFusion. Notably, BitsFusion, is favored 54.41% of the time over SD-v1.5. 2.5 3.5 4.5 5.5 6.5 7.5 8.5 9.5 CFG Scales 0 5 10 15 20 25 30FID (30K) SD-v1.5 (32 bits) Ours-I (1.99 bits) Ours-II (1.99 bits) Figure 7: FID results evaluated on 30K MS-COCO 2014 validation set. 6 Conclusion To enhance the storage efficiency of the large-scale diffusion models, we introduce an advanced weight quantization framework, BitsFusion, which effectively compresses the weights of UNet from SD-v1.5 to 1.99 bits, achieving a 7.9× smaller model size. BitsFusion even outperforms SD-v1.5 in terms of generation quality. Specifically, we first conduct a comprehensive analysis to understand the impact of each layer during quantization and establish a mixed-precision strategy. Second, we propose a series of effective techniques to initialize the quantized model. Third, during the training stage, we enforce the quantized model to learn the full-precision SD-v1.5 by using distillation losses with the adjusted distribution of time step sampling. Finally, we fine-tune the previous quantized model through vanilla noise prediction. Our extensive evaluations on TIFA, GenEval, CLIP score, and human evaluation consistently demonstrate the advantage of BitsFusion over full-precision SD-v1.5. References [1] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Yuanzhen Li, Tomer Michaeli, et al. Lumiere: A space-time diffusion model for video generation. arXiv preprint arXiv:2401.12945, 2024. 1 [2] Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013. 3 [3] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22563–22575, 2023. 1 [4] Hanwen Chang, Haihao Shen, Yiyang Cai, Xinyu Ye, Zhenzhong Xu, Wenhua Cheng, Kaokao Lv, Weiwei Zhang, Yintong Lu, and Heng Guo. Effective quantization for diffusion models on cpus. arXiv preprint arXiv:2311.16133, 2023. 2 [5] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1290–1299, 2022. 8, 29 10[6] Jaemin Cho, Abhay Zala, and Mohit Bansal. Dall-eval: Probing the reasoning skills and social biases of text-to-image generation models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3043–3054, 2023. 29 [7] Huanpeng Chu, Wei Wu, Chengjie Zang, and Kun Yuan. Qncd: Quantization noise correction for diffusion models. arXiv preprint arXiv:2403.19140, 2024. 2 [8] Israel Cohen, Yiteng Huang, Jingdong Chen, Jacob Benesty, Jacob Benesty, Jingdong Chen, Yiteng Huang, and Israel Cohen. Pearson correlation coefficient. Noise reduction in speech processing, pages 1–4, 2009. 4 [9] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780–8794, 2021. 3 [10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020. 8 [11] Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dhar- mendra S Modha. Learned step size quantization. In International Conference on Learning Representations, 2019. 2, 7, 8, 9 [12] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. A survey of quantization methods for efficient neural network inference. In Low-Power Com- puter Vision, pages 291–326. Chapman and Hall/CRC, 2022. 2 [13] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36, 2024. 2 [14] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36, 2024. 8, 29 [15] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu video: Factorizing text- to-video generation by explicit image conditioning. arXiv preprint arXiv:2311.10709, 2023. 1 [16] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and José Lezama. Photorealistic video generation with diffusion models. arXiv preprint arXiv:2312.06662, 2023. 1 [17] Yefei He, Jing Liu, Weijia Wu, Hong Zhou, and Bohan Zhuang. Efficientdm: Efficient quantization-aware fine-tuning of low-bit diffusion models. In The Twelfth International Conference on Learning Representations, 2023. 2, 3, 6, 8, 9 [18] Yefei He, Luping Liu, Jing Liu, Weijia Wu, Hong Zhou, and Bohan Zhuang. Ptqd: Accurate post-training quantization for diffusion models. Advances in Neural Information Processing Systems, 36, 2024. 2 [19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017. 2 [20] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 1 [21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840–6851, 2020. 1, 2, 3, 7, 19 [22] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 3 [23] Alain Hore and Djemel Ziou. Image quality metrics: Psnr vs. ssim. In 2010 20th international conference on pattern recognition, pages 2366–2369. IEEE, 2010. 4 [24] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2021. 3 11[25] Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, and Noah A Smith. Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering. arXiv preprint arXiv:2303.11897, 2023. 2 [26] Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, and Noah A Smith. Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 20406–20417, 2023. 8, 29 [27] Yushi Huang, Ruihao Gong, Jing Liu, Tianlong Chen, and Xianglong Liu. Tfmq-dm: Temporal feature maintenance quantization for diffusion models. arXiv preprint arXiv:2311.16503, 2023. 2, 3, 5, 17 [28] Kyuyeon Hwang and Wonyong Sung. Fixed-point feedforward deep neural network design using weights+ 1, 0, and- 1. In 2014 IEEE Workshop on Signal Processing Systems (SiPS), pages 1–6. IEEE, 2014. 6 [29] Yerlan Idelbayev, Pavlo Molchanov, Maying Shen, Hongxu Yin, Miguel A Carreira-Perpinán, and Jose M Alvarez. Optimal quantization using scaled codebook. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12095–12104, 2021. 6 [30] Qing Jin, Jian Ren, Richard Zhuang, Sumant Hanumante, Zhengang Li, Zhiyu Chen, Yanzhi Wang, Kaiyuan Yang, and Sergey Tulyakov. F8net: Fixed-point 8-bit only multiplication for network quantization. arXiv preprint arXiv:2202.05239, 2022. 2 [31] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. arXiv preprint arXiv:2206.00364, 2022. 1 [32] Bo-Kyeong Kim, Hyoung-Kyu Song, Thibault Castells, and Shinkook Choi. On architectural compression of text-to-image diffusion models. arXiv preprint arXiv:2305.15798, 2023. 2, 7 [33] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 7 [34] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 3 [35] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36, 2024. 10 [36] Junghyup Lee, Dohyung Kim, and Bumsub Ham. Network quantization with element-wise gradient scaling. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6448–6457, 2021. 2 [37] Fengfu Li, Bin Liu, Xiaoxing Wang, Bo Zhang, and Junchi Yan. Ternary weight networks. arXiv preprint arXiv:1605.04711, 2016. 6 [38] Xiuyu Li, Yijiang Liu, Long Lian, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, and Kurt Keutzer. Q-diffusion: Quantizing diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 17535–17545, 2023. 2, 3, 6, 8, 9 [39] Yanjing Li, Sheng Xu, Xianbin Cao, Xiao Sun, and Baochang Zhang. Q-dm: An efficient low-bit quantized diffusion model. Advances in Neural Information Processing Systems, 36, 2024. 2, 3, 6 [40] Yanyu Li, Xian Liu, Anil Kag, Ju Hu, Yerlan Idelbayev, Dhritiman Sagar, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Textcraftor: Your text encoder can be image quality controller. arXiv preprint arXiv:2403.18978, 2024. 1 [41] Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Snapfusion: Text-to-image diffusion model on mobile devices within two seconds. Advances in Neural Information Processing Systems, 36, 2024. 2, 3 [42] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. In International Conference on Learning Representations, 2020. 2 [43] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. arXiv preprint arXiv:2102.05426, 2021. 2 12[44] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. arXiv preprint arXiv:2211.10440, 2022. 1 [45] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of Machine Learning and Systems, 6:87–100, 2024. 2 [46] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740–755. Springer, 2014. 2 [47] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740–755. Springer, 2014. 8, 29 [48] Jing Liu, Bohan Zhuang, Peng Chen, Chunhua Shen, Jianfei Cai, and Mingkui Tan. Single-path bit sharing for automatic loss-aware model compression. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(10):12459–12473, 2023. 2 [49] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds. arXiv preprint arXiv:2202.09778, 2022. 1, 8 [50] Xian Liu, Jian Ren, Aliaksandr Siarohin, Ivan Skorokhodov, Yanyu Li, Dahua Lin, Xihui Liu, Ziwei Liu, and Sergey Tulyakov. Hyperhuman: Hyper-realistic human generation with latent structural diffusion. arXiv preprint arXiv:2310.08579, 2023. 1 [51] Xuewen Liu, Zhikai Li, Junrui Xiao, and Qingyi Gu. Enhanced distribution alignment for post-training quantization of diffusion models. arXiv preprint arXiv:2401.04585, 2024. 2 [52] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free quan- tization aware training for large language models. arXiv preprint arXiv:2305.17888, 2023. 6 [53] Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian, and Tijmen Blankevoort. Spinquant–llm quantization with learned rotations. arXiv preprint arXiv:2405.16406, 2024. 2 [54] Stuart Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28(2):129–137, 1982. 6 [55] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:5775–5787, 2022. 8, 9, 30, 31 [56] Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, and Furu Wei. The era of 1-bit llms: All large language models are in 1.58 bits. arXiv preprint arXiv:2402.17764, 2024. 6 [57] Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Ekaterina Deyneka, Tsai-Shien Chen, Anil Kag, Yuwei Fang, Aleksei Stoliar, Elisa Ricci, Jian Ren, et al. Snap video: Scaled spatiotemporal transformers for text-to-video synthesis. arXiv preprint arXiv:2402.14797, 2024. 1 [58] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training quantization. In International Conference on Machine Learning, pages 7197–7206. PMLR, 2020. 2 [59] Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free quantization through weight equalization and bias correction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1325–1334, 2019. 2 [60] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart Van Baalen, and Tijmen Blankevoort. A white paper on neural network quantization. arXiv preprint arXiv:2106.08295, 2021. 2 13[61] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 1 [62] Atila Orhon, Michael Siracusa, and Aseem Wadhwa. Stable diffusion with core ml on apple silicon, 2022. 8, 9 [63] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations, 2023. 10 [64] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. 1 [65] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li, Hsin- Ying Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123: One image to high-quality 3d object generation using both 2d and 3d diffusion priors. arXiv preprint arXiv:2306.17843, 2023. 1 [66] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748–8763. PMLR, 2021. 2, 4, 17 [67] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 1 [68] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 8821–8831. PMLR, 2021. 1 [69] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In International conference on machine learning, pages 1278–1286. PMLR, 2014. 3 [70] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High- resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–10695, 2022. 2, 3 [71] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 Conference Proceedings, pages 1–10, 2022. 1 [72] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:36479–36494, 2022. 29 [73] Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and Yan Yan. Post-training quantization on diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1972–1981, 2023. 2, 6 [74] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. In The Twelfth International Conference on Learning Representations, 2023. 1 [75] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 1 [76] Junhyuk So, Jungwon Lee, Daehyun Ahn, Hyungjun Kim, and Eunhyeok Park. Temporal dynamic quantization for diffusion models. Advances in Neural Information Processing Systems, 36, 2024. 2, 3 [77] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper- vised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256–2265. PMLR, 2015. 1, 3 [78] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. 8, 9, 30, 31 14[79] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. 1 [80] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 1, 7 [81] Siao Tang, Xin Wang, Hong Chen, Chaoyu Guan, Zewen Wu, Yansong Tang, and Wenwu Zhu. Post-training quantization with progressive calibration and activation relaxing for text-to-image diffusion models. arXiv preprint arXiv:2311.06322, 2023. 2 [82] Changyuan Wang, Ziwei Wang, Xiuwei Xu, Yansong Tang, Jie Zhou, and Jiwen Lu. Towards accurate data-free quantization for diffusion models. arXiv preprint arXiv:2305.18723, 2023. 2 [83] Haoxuan Wang, Yuzhang Shang, Zhihang Yuan, Junyi Wu, and Yan Yan. Quest: Low-bit diffusion model quantization via efficient selective finetuning. arXiv preprint arXiv:2402.03666, 2024. 2 [84] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pages 38087–38099. PMLR, 2023. 2 [85] Yuewei Yang, Xiaoliang Dai, Jialiang Wang, Peizhao Zhang, and Hongbo Zhang. Efficient quantization strategies for latent diffusion models. arXiv preprint arXiv:2312.05431, 2023. 2 [86] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. Transactions on Machine Learning Research, 2022. 2, 8, 9, 29 [87] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, and Karsten Kreis. Lion: Latent point diffusion models for 3d shape generation. arXiv preprint arXiv:2210.06978, 2022. 1 [88] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3836–3847, 2023. 1 [89] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreason- able effectiveness of deep features as a perceptual metric. In CVPR, 2018. 4 [90] Zhixing Zhang, Ligong Han, Arnab Ghosh, Dimitris N Metaxas, and Jian Ren. Sine: Single image editing with text-to-image diffusion models. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6027–6037, 2023. 1 [91] Tianchen Zhao, Xuefei Ning, Tongcheng Fang, Enshu Liu, Guyue Huang, Zinan Lin, Shengen Yan, Guohao Dai, and Yu Wang. Mixdq: Memory-efficient few-step text-to-image diffusion models with metric-decoupled mixed precision quantization. arXiv preprint arXiv:2405.17873, 2024. 2 [92] Yang Zhao, Yanwu Xu, Zhisheng Xiao, and Tingbo Hou. Mobilediffusion: Subsecond text-to- image generation on mobile devices. arXiv preprint arXiv:2311.16567, 2023. 2 [93] Xingyu Zheng, Haotong Qin, Xudong Ma, Mingyuan Zhang, Haojie Hao, Jiakai Wang, Zixiang Zhao, Jinyang Guo, and Xianglong Liu. Binarydm: Towards accurate binarization of diffusion model. arXiv preprint arXiv:2404.05662, 2024. 2 [94] Chenzhuo Zhu, Song Han, Huizi Mao, and William J Dally. Trained ternary quantization. In International Conference on Learning Representations, 2016. 6 15Appendix Table of Contents A Limitations 17 B More details for Mixed-Precision Algorithm 17 C More Details for Time Embedding Pre-computing and Caching 17 D Analysis of Symmetric Weight Distribution 19 E More Details for Quantization Error Across Different Time Steps 19 F Detailed Metrics for Quantization Error by Quantizing Different Layers 20 G More Visualization for Quantization Error by Quantizing Different Layers 24 H 1.99 Bits Mixed Precision Recipe 25 I Details for Evaluation Metrics 29 J Human Evaluation 29 J.1 Analysis on Categories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 J.2 Analysis on Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 K Evaluation on Different Schedulers 30 L Detailed GenEval Results 31 M More Comparisons 32 M.1 Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 M.2 Additional Image Comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . 32 16A Limitations In this work, we study the storage size reduction of the UNet in Stable Diffusion v1.5 through weight quantization. The compression of V AE and CLIP text encoder [ 66] is also an interesting direction, which is not explored in this work. Additionally, our weight quantization techniques could be extended to the activations quantization, as a future exploration. B More details for Mixed-Precision Algorithm In Sec. 3, we analyze the per-layer quantization error and develop the mixed-precision strategy. Here, we provide the detailed algorithm as outlined in Alg. 1. The inputs include: a pre-defined candidate set of bit-width b ∈ {1, 2, 3}, the full-precision SD-v1.5 D, the total number of layers L (except for the time embedding, time projection, the first and last convolutional layers), the training dataset X, the number of training iterations T, the number of evaluation images for calculating metrics K, the bit threshold So, the parameter size factor η, and the number of parameters of the ith layer Ni. In the first stage, we aim to obtain quantized models by quantizing each individual layer. Given the full-precision SD-v1.5 UNet D, we consecutively perform the quantization on every single layer to 1, 2, or 3 bits individually, while maintaining the remaining layers at FP32 format. Notice, to align with our experiments, we add the balance integer and initialize the scaling factor with our alternating optimization. For each quantized model, the weights and scaling factors are fine-tuned using quantization-aware training to minimize the quantization error by learning the predicted noise of the SD-v1.5. We obtain quantized models Di,b, i= 1, 2, ··· , L, b= 1, 2, 3. In the second stage, we measure the quantization error of each layer by calculating various metrics from comparing images generated by the quantized model Di,b with those from the unquantized SD-v1.5 D. Specifically, we generate K = 100 baseline images Id from the full-precision SD-v1.5 model with PartiPrompts. Then, for each quantized model Di,b, we use identical prompts and seed to generate corresponding images Ii,b. We calculate the quantization error by measuring the metrics including MSE, CLIP score, PSNR, and LPIPS using these images and prompts. In the third stage, we collect the mixed-precision recipe. We first compute a sensitivity score for each layer, factoring in both the MSE and the parameter size adjusted by η. For the ith layer, its sensitivity score for the b-bits (b ∈ {1, 2, 3}) is defined as Si,b = Mi,bN−η i , where M denotes the MSE error, N is the total number of parameters of the layer, and η ∈ [0, 1] denotes the parameter size factor. To determine the bit width (i.e., b∗) for each layer, we define a sensitivity threshold as So, and the ith layer is assigned to b∗ i -bits, where b∗ i = min{b|Si,b < So}. The remaining layers are set as 4 bits if they fail to meet the threshold. After determining the initial bits based on the MSE error, we refine this recipe by considering the degradation in the CLIP score associated with each bit-width. We simply consider the CLIP score change at 3 bits. We assign layers with the highest 10%, 5%, 2% CLIP score drop with 1, 2, 3 more bits, respectively. The final output is a mixed-precision recipe {b∗ i }, i= 1, 2, ··· , L, specifying the bit-width for each layer. Then, we set the first and last convolutional layers as 8 bits and pre-computing and caching the time embedding and projection layers. C More Details for Time Embedding Pre-computing and Caching In Sec. 4.1, we introduce \"Time Embedding Pre-computing and Caching\". Here, we provide more details for the algorithm. In the Stable Diffusion model, the time step t ∈ [0, 1, ··· , 999] is transformed into a time embedding embt through the equation embt = e(t), where e(t) denotes the time embedding layer and embt ∈ Rdte. In SD-v1.5, dte = 1280. Then, for each ResBlock, denoted as Ri for i = 1, 2, ··· , Nr, where Nr is total number of ResBlocks with time projection layers, the embt is encoded by time projection layers ri(·) by Fi,t = ri(embt). Notice that ri(·) and e(·) are both linear layers. Finally, Fi,t is applied to the intermediate activations of each Ri via addition operation, effectively incorporating temporal information into the Stable Diffusion model. As observed before [27], time embedding and projection layers exhibit considerable sensitivity to quantization during PTQ on DM. To address this problem, existing work specifically pays attention to reconstructing layers related to time embedding [27]. In this study, we propose a more effective 17Algorithm 1Mixed-Precision Algorithm Input: Candidate bits set b ∈ {1, 2, 3}, SD-v1.5 model D, number of total layers L (except for the time embedding, time projection, the first and last convolutional layers), dataset X, training iterations T, number of evaluation images K, threshold So, parameter size factor η, number of parameters of the ith layer Ni. Output: Mixed precision recipe {b∗ i }, i= 1, 2, ··· , L. 1: 1: Obtaining the quantized models. 2: for b = 1to 3 do 3: for i = 1to L do 4: Quantize the i-th layer to b bits via Eq. (1) and proposed initialization methods in Sec. 4.1 to get model Di,b; 5: for t = 1to T do 6: Updating weights and scaling factors by minimizing the quantization error using quantization-aware training on Di,b with data X; 7: end for 8: end for 9: end for 10: 2: Calculating quantization error metrics. 11: Generating K images Id via SD-v1.5; 12: for b = 1to 3 do 13: for i = 1to L do 14: Generating K images Ii,b via quantized model Di,b; 15: Calculating MSE, Mi,b via images Ii,b and Id; 16: Calculating PSNR, Pi,b via images Ii,b and Id; 17: Calculating LPIPS, Li,b via images Ii,b and Id; 18: Calculating CLIP score drop, Ci,b via images Ii,b and prompts; 19: end for 20: end for 21: 2: Deciding the optimal precision. 22: Calculating sensitivity score Si,b = Mi,bN−η i ; 23: for i = 1to L do 24: b∗ i ← 4; 25: for b = 3to 1 do 26: if Si,b < So then 27: Assign the i-th layer with b bits with b∗ i ← b; 28: end if 29: end for 30: end for 31: Calculating CLIP score drop, Ci,3 and its pth percentile Cp; 32: for i = 1to L do 33: if Ci,3 > C90 then 34: b∗ i ← b∗ i + 1; 35: end if 36: if Ci,3 > C95 then 37: b∗ i ← b∗ i + 1; 38: end if 39: if Ci,3 > C98 then 40: b∗ i ← b∗ i + 1; 41: end if 42: end for method. We observe that 1) during the inference stage, for each time stept, the embt and consequently Fi,t remain constant. 2) In the Stable Diffusion model, the shape of Fi,t are considerably smaller compared to time embedding and projection layers. Specifically, in SD-v1.5,Fi,t is with the dimension in {320, 640, 1280} which is largely smaller than time projection layers Wr ∈ RD×1280, where D ∈ {320, 640, 1280}. Therefore, we introduce an efficient and lossless method named Time Embedding Pre-computing and Caching. Specifically, for total Tinf inference time steps, we opt to store only Tinf time features, rather than retaining the original time embedding layers e(·) and the time projection layers in the i-th ResBlock ri(·). The inference time steps are set as 50 or less in most Stable Diffusion models. This method signif- icantly reduces more than 1280/50 = 25.6× storage requirements and entire computational costs in terms of time-related layers. Given that the storage size of the pre-computed Fi,t is substantially 18smaller than that of the original linear layers, this approach effectively diminishes the average bit of our quantized model without any performance degradation. D Analysis of Symmetric Weight Distribution In Sec. 4.1, we introduce \"Adding Balance Integer\" by assuming the weight distribution in Stable Diffusion is symmetric. Here, we provide more analysis for the assumption. To verify the weight distribution is symmetric around zero in SD-v1.5, we measure the skewness of the weight distribution of each layer. Lower skewness indicates a more symmetric weight distribution. As illustrated in Fig. 8, 97% of layers exhibiting skewness between [-0.5, 0.5], this suggests that most layers in SD-v1.5 have symmetric weight distributions. 0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 Layers 0.5 0.0 0.5 Skewness Figure 8: Skewness of weight distribution of each layer in SD-v1.5. Lower skewness represents the weight distribution is more symmetric. 97% layers are with skewness between [-0.5, 0.5], indicating that most layers have symmetric weight distribution in SD-v1.5. E More Details for Quantization Error Across Different Time Steps In Sec. 4.2, we introduce the \"Quantization Error-aware Time Step Sampling\" method. Here, we provide more details for measuring the quantization error from the predicted latent instead of the predicted noise. During the inference stage, the actual denoising step requires the scaling operation on the predicted noise in diffusion models. Therefore, directly calculating the quantization error via noise prediction is not accurate. Instead, we calculate the quantization error in the latent feature space. We derive the relationship of quantization error calculated from the predicted latent and noise as follows: E = Et,x \u0002 ∥ˆ zθfp (t, zt, c) − ˆ zθint,s (t, zt, c)∥2\u0003 , = Et,x \"\r\r\r\r \u0012 1√¯αt zt − √1 − ¯αt√¯αt ˆϵθfp (t, zt, c) \u0013 − \u0012 1√¯αt zt − √1 − ¯αt√¯αt ˆϵθint,s (t, zt, c) \u0013\r\r\r\r 2# , = Et,x \u00141 − ¯αt ¯αt \r\rˆϵθfp (t, zt, c) − ˆϵθint,s (t, zt, c) \r\r2 \u0015 , (9) where ¯αt is the noise scheduler in [21]. 19F Detailed Metrics for Quantization Error by Quantizing Different Layers In Sec. 3.2, we calculate the various metrics for representing the quantization error when quantizing different layers. Here, we provide detailed metrics when quantizing each layer of SD-v1.5 to 1, 2, and 3 bits. 0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 Layers 0 1000 2000 3000 4000 5000 6000Quantization Error (MSE) (a) MSE value caused by the 1-bit quantized layers in SD-v1.5. 0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 Layers 0 500 1000 1500 2000 2500 3000Quantization Error (MSE) (b) MSE value caused by the 2-bit quantized layers in SD-v1.5. 0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 Layers 0 500 1000 1500 2000Quantization Error (MSE) (c) MSE value caused by the 3-bit quantized layers in SD-v1.5. Figure 9: MSE value caused by the quantized layers in SD-v1.5.. 200 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 Layers 0.00 0.01 0.02 0.03 0.04Quantization Error (CLIP score) (a) CLIP score degradation caused by the 1-bit quantized layers in SD-v1.5. 0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 Layers 0.00 0.01 0.02Quantization Error (CLIP score) (b) CLIP score degradation caused by the 2-bit quantized layers in SD-v1.5. 0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 Layers 0.00 0.01Quantization Error (CLIP score) (c) CLIP score degradation caused by the 3-bit quantized layers in SD-v1.5. Figure 10: CLIP score degradation caused by quantized layers in SD-v1.5. 210 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 Layers 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8Quantization Error (LPIPS) (a) LPIPS value of the 1-bit quantized layers in SD-v1.5. 0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 Layers 0.0 0.1 0.2 0.3 0.4 0.5Quantization Error (LPIPS) (b) LPIPS value of the 2-bit quantized layers in SD-v1.5. 0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 Layers 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35Quantization Error (LPIPS) (c) LPIPS value of the 3-bit quantized layers in SD-v1.5. Figure 11: LPIPS value of quantized layers in SD-v1.5. 220 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 Layers 0 5 10 15 20 25 30Quantization Error (PSNR) (a) PSNR value of the 1-bit quantized layers in SD-v1.5. 0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 Layers 0 5 10 15 20 25 30 35Quantization Error (PSNR) (b) PSNR value of the 2-bit quantized layers in SD-v1.5. 0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 Layers 0 5 10 15 20 25 30 35 40Quantization Error (PSNR) (c) PSNR value of the 3-bit quantized layers in SD-v1.5. Figure 12: PSNR value of quantized layers in SD-v1.5. 23G More Visualization for Quantization Error by Quantizing Different Layers In Sec. 3.2, we show the images for representing the quantization error when quantizing different lay- ers. Here, we provide more visualization for demonstrating the different quantization errors caused by quantizing different layers to 1 bit. The quantized layers from left to right correspond to the annotated layers at the bottom: SD-v1.5 w/o quantization, Down.0.0.attn2.toq, Down.0.0.attn2.tok, Down.0.0.attn2.tov, Down.1.0.attn2.tok, Down.1.1.attn2.tok, Down.2.res.0.conv1, Up.2.res.2.convshortcut, Up.3.2.attn2.tok, Up.3.res.2.convshortcut. Figure 13: Quantization errors demonstrated in generated images (via PartiPrompts) after performing 1-bit quantization on different individual layers. 24H 1.99 Bits Mixed Precision Recipe We provide our 1.99 bits recipe in our experiments. During the training and inference stage, we add a balancing integer to the n-bits values, resulting in log(2n + 1) bits. We calculate the average bits by P i log(2b∗ i +1)×Ni+16∗Ntf Nw , where b∗ i is the calculated bit-width in the ith layer, Ni is the number of weights of the ith layer, Ntf is the number of parameters for pre-cached time features, and Nw is the total number of weights in linear and convolutional layers. We calculate the model size by integrating all other parameters as 32 bits. The index and name of each layer are listed: 1 down_blocks.0.attentions.0.proj_in: 6 2 down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q: 5 3 down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k: 5 4 down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v: 4 5 down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0: 6 6 down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q: 5 7 down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k: 7 8 down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v: 3 9 down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0: 3 10 down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj: 3 11 down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2: 3 12 down_blocks.0.attentions.0.proj_out: 5 13 down_blocks.0.attentions.1.proj_in: 4 14 down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q: 3 15 down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k: 4 16 down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v: 6 17 down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0: 5 18 down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q: 5 19 down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k: 7 20 down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v: 2 21 down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0: 3 22 down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj: 3 23 down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2: 3 24 down_blocks.0.attentions.1.proj_out: 6 25 down_blocks.0.resnets.0.conv1: 3 26 down_blocks.0.resnets.0.conv2: 3 27 down_blocks.0.resnets.1.conv1: 3 28 down_blocks.0.resnets.1.conv2: 4 29 down_blocks.0.downsamplers.0.conv: 4 30 down_blocks.1.attentions.0.proj_in: 4 31 down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q: 3 32 down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k: 3 33 down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v: 4 34 down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0: 4 35 down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q: 3 36 down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k: 5 37 down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v: 4 38 down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0: 3 39 down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj: 2 40 down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2: 2 41 down_blocks.1.attentions.0.proj_out: 4 42 down_blocks.1.attentions.1.proj_in: 4 43 down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q: 2 44 down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k: 2 45 down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v: 4 46 down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0: 4 47 down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q: 3 48 down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k: 6 49 down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v: 4 50 down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0: 3 51 down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj: 2 52 down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2: 2 53 down_blocks.1.attentions.1.proj_out: 4 54 down_blocks.1.resnets.0.conv1: 3 2555 down_blocks.1.resnets.0.conv2: 3 56 down_blocks.1.resnets.0.conv_shortcut: 7 57 down_blocks.1.resnets.1.conv1: 3 58 down_blocks.1.resnets.1.conv2: 2 59 down_blocks.1.downsamplers.0.conv: 4 60 down_blocks.2.attentions.0.proj_in: 3 61 down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q: 3 62 down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k: 2 63 down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v: 3 64 down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0: 3 65 down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q: 3 66 down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k: 4 67 down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v: 4 68 down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0: 3 69 down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj: 2 70 down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2: 1 71 down_blocks.2.attentions.0.proj_out: 3 72 down_blocks.2.attentions.1.proj_in: 4 73 down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q: 4 74 down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k: 2 75 down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v: 3 76 down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0: 3 77 down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q: 3 78 down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k: 4 79 down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v: 4 80 down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0: 3 81 down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj: 2 82 down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2: 2 83 down_blocks.2.attentions.1.proj_out: 4 84 down_blocks.2.resnets.0.conv1: 3 85 down_blocks.2.resnets.0.conv2: 2 86 down_blocks.2.resnets.0.conv_shortcut: 4 87 down_blocks.2.resnets.1.conv1: 2 88 down_blocks.2.resnets.1.conv2: 1 89 down_blocks.2.downsamplers.0.conv: 1 90 down_blocks.3.resnets.0.conv1: 1 91 down_blocks.3.resnets.0.conv2: 1 92 down_blocks.3.resnets.1.conv1: 1 93 down_blocks.3.resnets.1.conv2: 1 94 up_blocks.0.resnets.0.conv1: 1 95 up_blocks.0.resnets.0.conv2: 2 96 up_blocks.0.resnets.0.conv_shortcut: 1 97 up_blocks.0.resnets.1.conv1: 1 98 up_blocks.0.resnets.1.conv2: 1 99 up_blocks.0.resnets.1.conv_shortcut: 1 100 up_blocks.0.resnets.2.conv1: 2 101 up_blocks.0.resnets.2.conv2: 1 102 up_blocks.0.resnets.2.conv_shortcut: 1 103 up_blocks.0.upsamplers.0.conv: 1 104 up_blocks.1.attentions.0.proj_in: 3 105 up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q: 2 106 up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k: 1 107 up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v: 2 108 up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0: 3 109 up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q: 3 110 up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k: 4 111 up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v: 4 112 up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0: 2 113 up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj: 2 114 up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2: 2 115 up_blocks.1.attentions.0.proj_out: 3 116 up_blocks.1.attentions.1.proj_in: 3 117 up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q: 2 118 up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k: 2 119 up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v: 2 26120 up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0: 2 121 up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q: 2 122 up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k: 4 123 up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v: 3 124 up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0: 1 125 up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj: 1 126 up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2: 1 127 up_blocks.1.attentions.1.proj_out: 3 128 up_blocks.1.attentions.2.proj_in: 3 129 up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q: 1 130 up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k: 1 131 up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v: 2 132 up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0: 2 133 up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q: 1 134 up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k: 3 135 up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v: 2 136 up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0: 1 137 up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj: 1 138 up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2: 1 139 up_blocks.1.attentions.2.proj_out: 2 140 up_blocks.1.resnets.0.conv1: 1 141 up_blocks.1.resnets.0.conv2: 1 142 up_blocks.1.resnets.0.conv_shortcut: 3 143 up_blocks.1.resnets.1.conv1: 1 144 up_blocks.1.resnets.1.conv2: 1 145 up_blocks.1.resnets.1.conv_shortcut: 3 146 up_blocks.1.resnets.2.conv1: 1 147 up_blocks.1.resnets.2.conv2: 1 148 up_blocks.1.resnets.2.conv_shortcut: 3 149 up_blocks.1.upsamplers.0.conv: 2 150 up_blocks.2.attentions.0.proj_in: 4 151 up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q: 2 152 up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k: 2 153 up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v: 3 154 up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0: 3 155 up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q: 1 156 up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k: 2 157 up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v: 1 158 up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0: 1 159 up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj: 1 160 up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2: 1 161 up_blocks.2.attentions.0.proj_out: 3 162 up_blocks.2.attentions.1.proj_in: 4 163 up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q: 2 164 up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k: 3 165 up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v: 3 166 up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0: 3 167 up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q: 1 168 up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k: 3 169 up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v: 1 170 up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0: 1 171 up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj: 1 172 up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2: 1 173 up_blocks.2.attentions.1.proj_out: 3 174 up_blocks.2.attentions.2.proj_in: 4 175 up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q: 2 176 up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k: 2 177 up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v: 2 178 up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0: 3 179 up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q: 2 180 up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k: 3 181 up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v: 1 182 up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0: 1 183 up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj: 1 184 up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2: 1 27185 up_blocks.2.attentions.2.proj_out: 3 186 up_blocks.2.resnets.0.conv1: 1 187 up_blocks.2.resnets.0.conv2: 2 188 up_blocks.2.resnets.0.conv_shortcut: 4 189 up_blocks.2.resnets.1.conv1: 1 190 up_blocks.2.resnets.1.conv2: 2 191 up_blocks.2.resnets.1.conv_shortcut: 4 192 up_blocks.2.resnets.2.conv1: 1 193 up_blocks.2.resnets.2.conv2: 1 194 up_blocks.2.resnets.2.conv_shortcut: 4 195 up_blocks.2.upsamplers.0.conv: 3 196 up_blocks.3.attentions.0.proj_in: 4 197 up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q: 2 198 up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k: 2 199 up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v: 6 200 up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0: 3 201 up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q: 2 202 up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k: 3 203 up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v: 1 204 up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0: 1 205 up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj: 1 206 up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2: 1 207 up_blocks.3.attentions.0.proj_out: 4 208 up_blocks.3.attentions.1.proj_in: 4 209 up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q: 2 210 up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k: 3 211 up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v: 5 212 up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0: 3 213 up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q: 2 214 up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k: 3 215 up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v: 1 216 up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0: 1 217 up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj: 2 218 up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2: 1 219 up_blocks.3.attentions.1.proj_out: 4 220 up_blocks.3.attentions.2.proj_in: 6 221 up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q: 2 222 up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k: 3 223 up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v: 4 224 up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0: 3 225 up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q: 4 226 up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k: 5 227 up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v: 1 228 up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0: 1 229 up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj: 3 230 up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2: 2 231 up_blocks.3.attentions.2.proj_out: 4 232 up_blocks.3.resnets.0.conv1: 1 233 up_blocks.3.resnets.0.conv2: 2 234 up_blocks.3.resnets.0.conv_shortcut: 4 235 up_blocks.3.resnets.1.conv1: 2 236 up_blocks.3.resnets.1.conv2: 2 237 up_blocks.3.resnets.1.conv_shortcut: 4 238 up_blocks.3.resnets.2.conv1: 2 239 up_blocks.3.resnets.2.conv2: 2 240 up_blocks.3.resnets.2.conv_shortcut: 4 241 mid_block.attentions.0.proj_in: 2 242 mid_block.attentions.0.transformer_blocks.0.attn1.to_q: 3 243 mid_block.attentions.0.transformer_blocks.0.attn1.to_k: 1 244 mid_block.attentions.0.transformer_blocks.0.attn1.to_v: 2 245 mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0: 2 246 mid_block.attentions.0.transformer_blocks.0.attn2.to_q: 1 247 mid_block.attentions.0.transformer_blocks.0.attn2.to_k: 4 248 mid_block.attentions.0.transformer_blocks.0.attn2.to_v: 4 249 mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0: 3 28250 mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj: 2 251 mid_block.attentions.0.transformer_blocks.0.ff.net.2: 1 252 mid_block.attentions.0.proj_out: 3 253 mid_block.resnets.0.conv1: 1 254 mid_block.resnets.0.conv2: 1 255 mid_block.resnets.1.conv1: 1 256 mid_block.resnets.1.conv2: 1 conv_in: 8 conv_out: 8 I Details for Evaluation Metrics In Sec. 5, we measure the performance on various metrics such as TIFA, GenEval, CLIP score and FID. Here, we provide more details for these metrics. TIFA Score.TIFA v1.0 [26] aims to measure the faithfulness of generated images. It includes various 4K text prompts sampled from the MS-COCO captions [47], DrawBench [72], PartiPrompts [86], and PaintSkill [6], associated with a pre-generated set of question-answer pairs, resulting in 25K questions covering 4.5K diverse elements. Image faithfulness is measured by determining if the VQA model can accurately answer the questions from the generated images. GenEval Score. GenEval [14] measures the consistency between the generated images and the description, including 6 different tasks: single object, two object, counting, colors, position, color attribution. All text prompts are generated from task-specific templates filled in with: randomly sampled object names from MS-COCO [47], colors from Berlin-Kay basic color theory, numbers with 2, 3, 4, and relative positions from \"above\", \"below\", \"to the left of\", or \"to the right of\". We adopt the pre-trained object detection model Mask2Former (Swin-S-8×2) [5] for evaluation. CLIP score and FID.CLIP score measures measure the similarity between text prompts and corresponding generated images. FID is used to evaluate the quality of generated images by measuring the distance between the distributions of features extracted from generated images and target images. In the main experiments, evaluation are measured based on MS-COCO 2014 validation set with 30K image-caption pairs [47]. We adopt ViT-B/32 model to evaluate the CLIP score in our experiments. J Human Evaluation In Sec. 5, we provide the human evaluation results. Here, we provide more detailed human evaluation with category and challenge comparisons on PartiPrompts (P2), comparing Stable Diffusion v1.5 and BitsFusion, with the question: Given a prompt, which image has better aesthetics and image-text alignment? Our model is selected 888 times out of 1632 comparisons, indicating a general preference over SD-v1.5, which is chosen 744 times, demonstrating more appealing and accurate generated images. J.1 Analysis on Categories Illustrations, People, and Arts.Our model significantly outperforms SD-v1.5 in generating illustra- tions (77 wins out of 124), images of people (101 out of 174), and arts (45 out of 65). Outdoor and Indoor Scenes.Our model also shows strength in generating both outdoor (73 out of 131) and indoor scenes (23 out of 40), suggesting better environmental rendering capabilities. J.2 Analysis on Challenges Complex and Fine-grained Detail: Our model excels in generating images with complex details (73 out of 113) and fine-grained details (173 out of 312), suggesting advanced capabilities in maintaining detail at varying complexity levels. Imagination and Style & Format: Our model also shows a strong performance in tasks requiring imaginative (92 out of 149) and stylistic diversity (118 out of 204), highlighting its flexibility and creative handling of artistic elements. 29Produce & Plants Animals Indoor Scenes Illustrations People Abstract Outdoor Scenes Artifacts World Knowledge Food & Beverage Vehicles Arts 52 51.9 57.5 62.1 58 51 55.7 54.7 44.9 58.1 54.8 69.2 48 48.1 42.5 37.9 42 49 44.3 45.3 55.1 41.9 45.2 30.8 Ours SD-v1.5 Figure 14: Human evaluation across particular categories. Imagination Simple Detail Complex Fine-grained Detail Writing & Symbols Basic Properties & Positioning Quantity Style & Format Linguistic Structures Perspective 61.7 46.1 64.6 55.4 57.1 49.1 57.1 56.7 57.8 55.7 48.6 38.3 53.9 35.4 44.6 42.9 50.9 42.9 43.3 42.2 44.3 51.4 Ours SD-v1.5 Figure 15: Human evaluation across particular challenges. The strong performance in imaginative and artistic categories presents an opportunity to target applications in creative industries, such as digital art and entertainment, where these capabilities can be particularly valuable. K Evaluation on Different Schedulers In the main experiments in Sec. 5, we leverage the PNDM scheduler to generate images. Here, we measured the performance of different schedulers, such as DDIM [ 78] and DPMSolver [ 55], to demonstrate the generality and effectiveness of BitsFusion. We set 50 inference steps and fix 30the random seed as 1024. As shown in Fig. 16, BitsFusionconsistently outperforms SD-v1.5 with different schedulers. 2.5 3.5 4.5 5.5 6.5 7.5 8.5 9.5 CFG Scales 0.74 0.76 0.78 0.80 0.82TIFA Scores  SD-v1.5 (32 bits) Ours-II (1.99 bits) 2.5 3.5 4.5 5.5 6.5 7.5 8.5 9.5 CFG Scales 0.74 0.76 0.78 0.80 0.82TIFA Scores  SD-v1.5 (32 bits) Ours-II (1.99 bits) (a) DDIM (b) DPMSolver Figure 16: TIFA scores comparisons between SD-v1.5 and BitsFusion, with different schedulers. Left: TIFA scores measured with DDIM [78] scheduler. Right: TIFA score measured with DPMSolver [55] scheduler. L Detailed GenEval Results In Sec. 5, we provide the overall GenEval results. Here, we provide detailed GenEval results for further comparisons as illustrated in Tab. 8. Table 8: Detailed GenEval with different CFG scales. Method Overall Single Two Counting Colors Position Color Attribution Guidance Scale = 2.5 SD-v1.5 0.3589 0.9350 0.2626 0.2775 0.6043 0.0340 0.0400 Ours-I 0.3353 0.9075 0.2444 0.2550 0.5426 0.0280 0.0340 Ours-II 0.4024 0.8975 0.3859 0.2750 0.6979 0.0560 0.1020 Guidance Scale = 3.5 SD-v1.5 0.3879 0.9400 0.3010 0.3275 0.6787 0.0300 0.0500 Ours-I 0.3650 0.9500 0.2808 0.2575 0.6277 0.0280 0.0460 Ours-II 0.4370 0.9350 0.4727 0.3125 0.7340 0.0600 0.1080 Guidance Scale = 4.5 SD-v1.5 0.4056 0.9700 0.3010 0.3200 0.7426 0.0340 0.0660 Ours-I 0.3851 0.9500 0.3091 0.3100 0.6574 0.0340 0.0500 Ours-II 0.4516 0.9575 0.4788 0.3450 0.7723 0.0520 0.1040 Guidance Scale = 5.5 SD-v1.5 0.4094 0.9750 0.3111 0.3325 0.7319 0.0400 0.0660 Ours-I 0.4039 0.9675 0.3232 0.3425 0.7000 0.0300 0.0600 Ours-II 0.4567 0.9600 0.4909 0.3175 0.7979 0.0540 0.1200 Guidance Scale = 6.5 SD-v1.5 0.4224 0.9800 0.3293 0.3725 0.7447 0.0400 0.0680 Ours-I 0.4161 0.9675 0.3414 0.3350 0.7425 0.0360 0.0740 Ours-II 0.4612 0.9750 0.4990 0.3275 0.7957 0.0540 0.1160 Guidance Scale = 7.5 SD-v1.5 0.4262 0.9775 0.3313 0.3850 0.7596 0.0440 0.0600 Ours-I 0.4226 0.9775 0.3495 0.3600 0.7447 0.0360 0.0680 Ours-II 0.4682 0.9800 0.5091 0.3300 0.8085 0.0680 0.1140 Guidance Scale = 8.5 SD-v1.5 0.4271 0.9825 0.3273 0.3925 0.7745 0.0320 0.0540 Ours-I 0.4269 0.9800 0.3616 0.3475 0.7702 0.0400 0.0620 Ours-II 0.4747 0.9700 0.5111 0.3675 0.8213 0.0620 0.1160 Guidance Scale = 9.5 SD-v1.5 0.4260 0.9825 0.3556 0.3825 0.7553 0.0280 0.0520 Ours-I 0.4190 0.9825 0.3495 0.3450 0.7447 0.0300 0.0620 Ours-II 0.4736 0.9700 0.5192 0.3625 0.8277 0.0560 0.1060 31M More Comparisons We provide the prompts for the images featured in the Fig. 1. Additionally, we provide more generated images for the comparison. M.1 Prompts Prompts of Fig. 1 from left to right are: 1. a portrait of an anthropomorphic cyberpunk raccoon smoking a cigar, cyberpunk!, fantasy, elegant, digital painting, artstation, concept art, matte, sharp focus, illustration, art by josan Gonzalez 2. Pirate ship trapped in a cosmic maelstrom nebula, rendered in cosmic beach whirlpool engine, volumetric lighting, spectacular, ambient lights, light pollution, cinematic atmosphere, art nouveau style, illustration art artwork by SenseiJaye, intricate detail. 3. tropical island, 8 k, high resolution, detailed charcoal drawing, beautiful hd, art nouveau, concept art, colourful, in the style of vadym meller 4. anthropomorphic art of a fox wearing a white suit, white cowboy hat, and sunglasses, smoking a cigar, texas inspired clothing by artgerm, victo ngai, ryohei hase, artstation. highly detailed digital painting, smooth, global illumination, fantasy art by greg rutkowsky, karl spitzweg 5. a painting of a lantina elder woman by Leonardo da Vinci . details, smooth, sharp focus, illustration, realistic, cinematic, artstation, award winning, rgb , unreal engine, octane render, cinematic light, macro, depth of field, blur, red light and clouds from the back, highly detailed epic cinematic concept art CG render made in Maya, Blender and Photoshop, octane render, excellent composition, dynamic dramatic cinematic lighting, aesthetic, very inspirational, arthouse. 6. panda mad scientist mixing sparkling chemicals, high-contrast painting 7. An astronaut riding a horse on the moon, oil painting by Van Gogh. 8. A red dragon dressed in a tuxedo and playing chess. The chess pieces are fashioned after robots. M.2 Additional Image Comparisons We provide more images for further comparisons. For each set of two rows, the top row displays images generated using the full-precision Stable Diffusion v1.5, while the bottom row features images generated from BitsFusion, where the weights of UNet are quantized into 1.99 bits and the model size is 7.9× smaller than the one from SD-v1.5. All the images are synthesized under the setting of using PNDM sampler with 50 sampling steps and random seed as 1024. 32a b c d e f Figure 17: Top: Images generated from full-precision Stable Diffusion v1.5. Bottom: Images generated from BitsFusion. Prompts from left to right are: a: A person standing on the desert, desert waves, gossip illustration, half red, half blue, abstract image of sand, clear style, trendy illustration, outdoor, top view, clear style, precision art, ultra high definition image; b: A detailed oil painting of an old sea captain, steering his ship through a storm. Saltwater is splashing against his weathered face, determination in his eyes. Twirling malevolent clouds are seen above and stern waves threaten to submerge the ship while seagulls dive and twirl through the chaotic landscape. Thunder and lights embark in the distance, illuminating the scene with an eerie green glow.; c: A solitary figure shrouded in mists peers up from the cobble stone street at the imposing and dark gothic buildings surrounding it. an old-fashioned lamp shines nearby. oil painting. ; d: A deep forest clearing with a mirrored pond reflecting a galaxy-filled night sky; e: a handsome 24 years old boy in the middle with sky color background wearing eye glasses, it’s super detailed with anime style, it’s a portrait with delicated eyes and nice looking face; f: A dog that has been meditating all the time. 33a b c d e f Figure 18: Top: Images generated from full-precision Stable Diffusion v1.5. Bottom: Images generated from BitsFusion. Prompts from left to right are: a: A small cactus with a happy face in the Sahara desert.; b: A middle-aged woman of Asian descent, her dark hair streaked with silver, appears fractured and splintered, intricately embedded within a sea of broken porcelain. The porcelain glistens with splatter paint patterns in a harmonious blend of glossy and matte blues, greens, oranges, and reds, capturing her dance in a surreal juxtaposition of movement and stillness. Her skin tone, a light hue like the porcelain, adds an almost mystical quality to her form.; c: A high contrast portrait photo of a fluffy hamster wearing an orange beanie and sunglasses holding a sign that says \"Let’s PAINT!”; d: An extreme close-up of an gray-haired man with a beard in his 60s, he is deep in thought pondering the history of the universe as he sits at a cafe in Paris, his eyes focus on people offscreen as they walk as he sits mostly motionless, he is dressed in a wool coat suit coat with a button-down shirt , he wears a brown beret and glasses and has a very professorial appearance, and the end he offers a subtle closed-mouth smile as if he found the answer to the mystery of life, the lighting is very cinematic with the golden light and the Parisian streets and city in the background, depth of field, cinematic 35mm film.; e: poster of a mechanical cat, techical Schematics viewed from front and side view on light white blueprint paper, illustartion drafting style, illustation, typography, conceptual art, dark fantasy steampunk, cinematic, dark fantasy; f: I want to supplement vitamin c, please help me paint related food. 34a b c d e f Figure 19: Top: Images generated from full-precision Stable Diffusion v1.5. Bottom: Images generated from BitsFusion. Prompts from left to right are:a: new cyborg with cybertronic gadgets and vr helmet, hard surface, beautiful colours, sharp textures, shiny shapes, acid screen, biotechnology, tim hildebrandt, bruce pennington, donato giancola, larry elmore, masterpiece, trending on artstation, featured on pixiv, cinematic composition, dramatic pose, beautiful lighting, sharp, details, hyper - detailed, hd, hdr, 4 k, 8 k ; b: portrait of teenage aphrodite, light freckles, curly copper colored hair, smiling kindly, wearing an embroidered white linen dress with lace neckline, intricate, elegant, mother of pearl jewelry, glowing lights, highly detailed, digital painting, artstation, concept art, smooth, sharp focus, illustration, art by wlop, mucha, artgerm, and greg Rutkowski; c: portrait of a dystopian cute dog wearing an outfit inspired by the handmaid ï¿ ½ s tale ( 2 0 1 7 ), intricate, headshot, highly detailed, digital painting, artstation, concept art, sharp focus, cinematic lighting, digital painting, art by artgerm and greg rutkowski, alphonse mucha, cgsociety ; d: Portrait of a man by Greg Rutkowski, symmetrical face, a marine with a helmet, using a VR Headset, Kubric Stare, crooked smile, he’s wearing a tacitcal gear, highly detailed portrait, scifi, digital painting, artstation, book cover, cyberpunk, concept art, smooth, sharp foccus ilustration, Artstation HQ; e: Film still of female Saul Goodman wearing a catmaid outfit, from Red Dead Redemption 2 (2018 video game), trending on artstation, artstationHD, artstationHQ; f: oil paining of robotic humanoid, intricate mechanisms, highly detailed, professional digital painting, Unreal Engine 5, Photorealism, HD quality, 8k resolution, cinema 4d, 3D, cinematic, professional photography, art by artgerm and greg rutkowski and alphonse mucha and loish and WLOP 35a b c d e f Figure 20: Top: Images generated from full-precision Stable Diffusion v1.5. Bottom: Images generated from BitsFusion. Prompts from left to right are: a: anthropomorphic tetracontagon head in opal edgy darknimite mudskipper, intricate, elegant, highly detailed animal monster, digital painting, artstation, concept art, smooth, sharp focus, illustration, art by artgerm, bob eggleton, michael whelan, stephen hickman, richard corben, wayne barlowe, trending on artstation and greg rutkowski and alphonse mucha, 8 k; b: background shows moon, many light effects, particle, lights, gems, symmetrical!!! centered portrait dark witch, large cloak, fantasy forest landscape, dragon scales, fantasy magic, undercut hairstyle, short purple black fade hair, dark light night, intricate, elegant, sharp focus, digital painting, concept art, matte, art by wlop and artgerm and greg rutkowski and alphonse mucha, masterpiece; c: cat seahorse fursona, autistic bisexual graphic designer and musician, long haired attractive androgynous fluffy humanoid character design, sharp focus, weirdcore voidpunk digital art by artgerm, akihiko yoshida, louis wain, simon stalenhag, wlop, noah bradley, furaffinity, artstation hd, trending on deviantart; d: concept art of ruins of a victorian city burning down by j. c. leyendecker, wlop, ruins, dramatic, octane render, epic painting, extremely detailed, 8 k; e: hyperrealistic Gerald Gallego as a killer clown from outer space, trending on artstation, portrait, sharp focus, illustration, art by artgerm and greg rutkowski and magali Villeneuve; f: low angle photo of a squirrel dj wearing on - ear headphones and colored sunglasses, stadning at a dj table playing techno music at a dance club, hyperrealistic, highly detailed, intricate, smoke, colored lights, concept art, digital art, oil painting, character design by charlie bowater, ross tran, artgerm, makoto shinkai, wlop a b c d e f Figure 21: Top: Images generated from full-precision Stable Diffusion v1.5. Bottom: Images generated from BitsFusion. Prompts from left to right are: a: a photograph of an ostrich wearing a fedora and singing soulfully into a microphone; b: a pirate ship landing on the moon; c: a pumpkin with a candle in it; d: a rabbit wearing a black tophat and monocle; e: a red sports car on the road; f: a robot cooking in the kitchen. 36a b c d e f Figure 22: Top: Images generated from full-precision Stable Diffusion v1.5. Bottom: Images generated from BitsFusion. Prompts from left to right are: a: a baby daikon radish in a tutu; b: a baby penguin wearing a blue hat, red gloves, green shirt, and yellow pants; c: a woman with long black hair and dark skin; d: an emoji of a baby penguin wearing a blue hat, red gloves, green shirt, and yellow pants; e: a blue sports car on the road; f: a butterfly. a b c d e f Figure 23: Top: Images generated from full-precision Stable Diffusion v1.5. Bottom: Images generated from BitsFusion. Prompts from left to right are: a: Helmet of a forgotten Deity, clowing corals, extremly detailed digital painting, in the style of Fenghua Zhong and Ruan Jia and jeremy lipking and Peter Mohrbacher, mystical colors, rim light, beautiful lighting, 8k, stunning scene, raytracing, octane, trending on artstation; b: Jeff Bezos as a female amazon warrior, closeup, D&D, fantasy, intricate, elegant, highly detailed, digital painting, artstation, concept art, matte, sharp focus, illustration, hearthstone, art by Artgerm and Greg Rutkowski and Alphonse Mucha; c: Portrait of a draconic humanoid, HD, illustration, epic, D&D, fantasy, intricate, elegant, highly detailed, digital painting, artstation, concept art, smooth, sharp focus, illustration, art by artgerm and greg rutkowski and alphonse mucha, monster hunter illustrations art book; d: [St.Georges slaying a car adorned with checkered flag. Soviet Propaganda!!! poster!!!, elegant, highly detailed, digital painting, artstation, concept art, matte, sharp focus, illustration, octane render, unreal engine, photography]; e: a fire - breathing dragon at a medieval hobbit home, ornate, beautiful, atmosphere, vibe, mist, smoke, chimney, rain, wet, pristine, puddles, waterfall, clear stream, bridge, forest, flowers, concept art illustration, color page, 4 k, tone mapping, doll, akihiko yoshida, james jean, andrei riabovitchev, marc simonetti, yoshitaka amano, digital illustration, greg rutowski, volumetric lighting, sunbeams, particles; f: portrait of a well-dressed raccoon, oil painting in the style of Rembrandt 37",
      "meta_data": {
        "arxiv_id": "2406.04333v2",
        "authors": [
          "Yang Sui",
          "Yanyu Li",
          "Anil Kag",
          "Yerlan Idelbayev",
          "Junli Cao",
          "Ju Hu",
          "Dhritiman Sagar",
          "Bo Yuan",
          "Sergey Tulyakov",
          "Jian Ren"
        ],
        "published_date": "2024-06-06T17:59:23Z",
        "pdf_url": "https://arxiv.org/pdf/2406.04333v2.pdf",
        "github_url": "https://github.com/huggingface/diffusers"
      },
      "llm_extracted_info": {
        "main_contributions": "This research addresses the challenge of large model sizes in diffusion-based image generation models, particularly Stable Diffusion v1.5, which causes bottlenecks for storage and transfer on resource-constrained devices. The main contribution is BitsFusion, a novel weight quantization method that compresses the UNet of Stable Diffusion v1.5 to an average of 1.99 bits, achieving a 7.9x smaller model size while surprisingly exhibiting even better generation quality than the original full-precision model. The approach includes innovations in mixed-precision quantization, quantized model initialization, and an improved two-stage training strategy.",
        "methodology": "The methodology involves a comprehensive quantization-aware training framework with several novel techniques. First, it employs a mixed-precision quantization strategy derived from a per-layer sensitivity analysis, using Mean-Squared-Error (MSE) and CLIP score to assign optimal bit-widths to different layers. Second, it introduces initialization techniques including pre-computing and caching time embeddings (reducing storage by 25.6x), adding a balance integer to maintain symmetric weight distributions at extremely low bit-levels (e.g., 1-bit), and using alternating optimization for scaling factor initialization to minimize L2 error and improve convergence. Third, a two-stage training pipeline is proposed: Stage-I uses CFG-aware quantization distillation and feature distillation from a full-precision teacher, combined with a quantization error-aware time step sampling strategy (Beta distribution, α=3.0, β=1.0) to focus training on high-error time steps. Stage-II fine-tunes the model from Stage-I using vanilla noise prediction, maintaining text dropping and the adjusted time step sampling.",
        "experimental_setup": "The quantized model, BitsFusion (1.99-bit UNet), was evaluated against the full-precision Stable Diffusion v1.5 (32-bit/FP16). Training was performed using the AdamW optimizer with a constant learning rate of 1e-05 on an internal dataset. Stage-I involved 20K iterations on 8 NVIDIA A100 GPUs (batch size 256), while Stage-II used 50K iterations on 32 NVIDIA A100 GPUs (batch size 1024). During inference, the PNDM scheduler with 50 sampling steps and a random seed of 1024 was primarily used, with additional tests on DDIM and DPMSolver. Evaluation metrics included CLIP Score and FID on the 30K MS-COCO 2014 validation set, TIFA, and GenEval, using ViT-B/32 for CLIP and Mask2Former (Swin-S-8x2) for GenEval. Human evaluation was conducted on 1K PartiPrompts, asking for preference based on aesthetics and image-text alignment. Ablation studies analyzed the impact of each proposed technique and hyperparameters (η for mixed-precision, λ for distillation loss, α for time-step sampling).",
        "limitations": "The current work focused solely on reducing the storage size of the UNet within Stable Diffusion v1.5 through weight quantization. Compression of other components such as the Variational Autoencoder (VAE) and the CLIP text encoder was not explored. Additionally, the techniques developed primarily target weight quantization and do not extend to activation quantization.",
        "future_research_directions": "Future research directions include extending the compression efforts to the VAE and CLIP text encoder components of the diffusion model. Another promising area for future exploration is the extension of the proposed weight quantization techniques to activation quantization.",
        "experimental_code": "                \"quantization_config\": BitsAndBytesConfig(\n                    load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type=\"nf4\"\n                ),\n",
        "experimental_info": "This code snippet from `benchmarking_flux.py` demonstrates the loading of a FluxTransformer2DModel with 4-bit NormalFloat4 (nf4) quantization using `BitsAndBytesConfig`. This is a setting for running inference on an already quantized model, rather than implementing the comprehensive quantization-aware training (QAT) framework described in the 'Method'. The method specifies a QAT framework with novel techniques for training, while this snippet focuses on loading an already quantized model for benchmarking. It does not reflect mixed-precision quantization derived from sensitivity analysis or the other advanced QAT techniques mentioned."
      }
    },
    {
      "title": "Retraining-Free Model Quantization via One-Shot Weight-Coupling Learning",
      "abstract": "Quantization is of significance for compressing the over-parameterized deep\nneural models and deploying them on resource-limited devices. Fixed-precision\nquantization suffers from performance drop due to the limited numerical\nrepresentation ability. Conversely, mixed-precision quantization (MPQ) is\nadvocated to compress the model effectively by allocating heterogeneous\nbit-width for layers. MPQ is typically organized into a searching-retraining\ntwo-stage process. In this paper, we devise a one-shot training-searching\nparadigm for mixed-precision model compression. Specifically, in the first\nstage, all potential bit-width configurations are coupled and thus optimized\nsimultaneously within a set of shared weights. However, our observations reveal\na previously unseen and severe bit-width interference phenomenon among highly\ncoupled weights during optimization, leading to considerable performance\ndegradation under a high compression ratio. To tackle this problem, we first\ndesign a bit-width scheduler to dynamically freeze the most turbulent bit-width\nof layers during training, to ensure the rest bit-widths converged properly.\nThen, taking inspiration from information theory, we present an information\ndistortion mitigation technique to align the behavior of the bad-performing\nbit-widths to the well-performing ones. In the second stage, an inference-only\ngreedy search scheme is devised to evaluate the goodness of configurations\nwithout introducing any additional training costs. Extensive experiments on\nthree representative models and three datasets demonstrate the effectiveness of\nthe proposed method. Code can be available on\n\\href{https://www.github.com/1hunters/retraining-free-quantization}{https://github.com/1hunters/retraining-free-quantization}.",
      "full_text": "Retraining-free Model Quantization via One-Shot Weight-Coupling Learning Chen Tang1∗ Yuan Meng1∗ Jiacheng Jiang1 Shuzhao Xie1 Rongwei Lu1 Xinzhu Ma2 Zhi Wang1† Wenwu Zhu1† 1Tsinghua University 2The Chinese University of Hong Kong Abstract Quantization is of significance for compressing the over- parameterized deep neural models and deploying them on resource-limited devices. Fixed-precision quantization suf- fers from performance drop due to the limited numerical representation ability. Conversely, mixed-precision quan- tization (MPQ) is advocated to compress the model ef- fectively by allocating heterogeneous bit-width for layers. MPQ is typically organized into a searching-retraining two- stage process. Previous works only focus on determining the optimal bit-width configuration in the first stage effi- ciently, while ignoring the considerable time costs in the second stage and thus hindering deployment efficiency sig- nificantly. In this paper, we devise a one-shot training- searching paradigm for mixed-precision model compres- sion. Specifically, in the first stage, all potential bit-width configurations are coupled and thus optimized simultane- ously within a set of shared weights. However, our ob- servations reveal a previously unseen and severe bit-width interference phenomenon among highly coupled weights during optimization, leading to considerable performance degradation under a high compression ratio. To tackle this problem, we first design a bit-width scheduler to dy- namically freeze the most turbulent bit-width of layers dur- ing training, to ensure the rest bit-widths converged prop- erly. Then, taking inspiration from information theory, we present an information distortion mitigation technique to align the behaviour of the bad-performing bit-widths to the well-performing ones. In the second stage, an inference- only greedy search scheme is devised to evaluate the good- ness of configurations without introducing any additional training costs. Extensive experiments on three representa- tive models and three datasets demonstrate the effective- ness of the proposed method. Code can be available on https://github.com/1hunters/retraining-free-quantization. 1. Introduction Recent years have witnessed the tremendous achievements made by deep network-driven applications, ranging from *Equal contributions. †Corresponding authors. Z. Wang is also with TBSI. classification [19, 22, 42], object detection [40, 41, 46], and segmentation [5, 20]. However, the huge number of pa- rameters in these deep models poses intractable challenges for both training [4, 24, 32] and inference [16, 39]. To enable efficient deep learning on inference, several tech- niques are proposed, including pruning [31, 33], quantiza- tion [12, 47, 67], and neural architecture search [21, 50]. Ultra-low bit-width neural network quantization [12, 53, 67] is an appealing model compression technique to sim- plify the hardware complexity and improve the runtime effi- ciency of over-parameterized deep models. However, under severely limited numerical representation capabilities, per- forming such compression across the whole neural network usually incurs an unacceptable performance drop. Mixed- precision quantization (MPQ) [3, 15, 23, 47, 57], by allo- cating unequally bit-width for weight and activation tensors of each layer, can largely avoid accuracy degradation while maintaining the proper model size and runtime overhead (e.g. on-device latency). The underlying principle of MPQ is that layers contribute very differently to the final accuracy [3, 47, 57], so the compression algorithm should apply het- erogeneous precision rather than a uniform one across the whole network. Besides, hardware starts to support mixed- precision computation [6, 57] in these years, which further pushes the study for mixed-precision compression. Recently, MPQ has been extensively studied from sev- eral perspectives, e.g. through reinforcement learning [11, 57], differentiable methods [3, 60], and proxy-based ap- proaches [6, 9]. They all try to solve one challenge, that says, how to find the optimal bit-width configuration for each layer, in an exponentially large O(n2L) space, where n is the number of bit-width candidates and L is the num- ber of layers in the deep network. To this end, they or- ganize a searching-then-retraining pipeline, in which the first stage aims to finish the bit-width allocation as fast as possible, and naturally becomes the focus of the research. Nevertheless, previous works tend to ignore the importance of the second stage, which in fact consumes considerable time cost for retraining the model to fit the obtained bit- width configurations (a.k.a, the policy). To recover the per- formance, LIMPQ [47] needs about 200 GPU-hours to re- arXiv:2401.01543v2  [cs.CV]  14 Jun 2024train a single ResNet18 policy. This impedes the real-world quantized mixed-precision model deployment—we might not have much time to retrain every policies for all devices. Instead, we consider a new paradigm termed astraining- then-searching, repositioning the resource-intensive train- ing process to the forefront of the mixed-precision quantiza- tion pipeline. The initial stage focuses on the development of a weight-sharing quantization model, where all possible bit-width configurations are concurrently optimized within unified network of weights to fulfill extensive search re- quirements. Importantly, this weight-sharing model under- goes a singular training session and, notably, requires no subsequent retraining following the search . Subsequently, in the second stage, we present an inference-only search employing a bidirectional greedy scheme to judiciously de- termine the optimal bit-width for each layer. The primary focus of this paper lies in the train- ing of a high-quality weight-sharing quantization model, which highly relies on ingenious weight-coupling learning method with heterogeneous bit-widths. We identify a dis- tinctive phenomenon inherent in weight-sharing quantiza- tion—referred to as thebit-width interference problem. This problem arises from the highly shared weights between bit- widths, the same weight could be quantized to very different discrete values for various bit-widths, so significantly super- imposed quantization noise of various bit-widths leads to daunting optimization challenges, as we will discuss later. We illustrate the bit-width interference problem in Fig. 1, one can see that even the introduction of a single addi- tional bit-width can cause the shared weight to frequently traverse quantization bound, resulting in training instability and large gradient variance. To understand and circumvent the issue of weight- sharing quantization, we conduct a detailed analysis of the bit-width interference problem (Sec. 3.2). Building upon this understanding, we introduce a bit-width scheduler de- signed to freeze the bit-widths that contribute to weight interference, ensuring proper convergence for the remain- ing bit-widths. Furthermore, during dynamic training, we observe an information distortion phenomenon associated with the unfrozen bit-widths. To mitigate this distortion, we propose to align the behavior of poorly performing bit-widths with their well-performing counterparts. Exten- sive experiments demonstrate that these two complemen- tary techniques not only unravel the intricacies of the bit- width interference problem but also provide meaningful performance improvements of weight-sharing quantization models. To summarize, our contributions are as follows: • We identify and analyze the bit-width interference prob- lem in weight-sharing quantization models, revealing its impact on optimization challenges, training stability, and convergence. • To train the weight-sharing quantization model, we first design a novel bit-width scheduler that freezes interfering bit-widths during training, ensuring proper convergence and addressing instability caused by the introduction of additional bit-widths. • We also propose a strategy inspired by information the- ory to align poorly performing bit-widths with their well- performing counterparts, mitigating information distor- tion during dynamic training and enhancing the overall performance. • Extensive experiments on three representative models and three benchmarks demonstrate the effectiveness of pro- posed method. For example, under an average 4-bit con- straint, our method leads on ResNet with a top accu- racy of 71.0% at only 31.6G BitOPs and no retraining cost, compared to the second-best at 70.8% accuracy with higher 33.7G operations and 90 epochs of retraining. 2. Related Work 2.1. Neural Network Quantization In this paper, we only consider the context in quantization- aware training, as it can achieve higher compression ratio than post-training quantization [25, 34]. Quantization can be generally categorized into two classes: fixed-precision quantization and mixed-precision quantization. Fixed-Precision Quantization. Fixed-precision quantiza- tion involves assigning a uniform bit-width to all layers. Specifically, methods such as Dorefa [67] and PACT [7] employ a low-precision representation for weights and ac- tivations during forward propagation. They leverage the Straight-Through Estimation (STE) technique [1] to esti- mate the gradient of the piece-wise quantization function for backward propagation. LSQ [12] scales the weight and activation distributions by introducing learnable step-size scale factors for quantization functions. Mixed-Precision Quantization. Mixed-Precision Quan- tization (MPQ) delves into the intricate aspects of low- precision quantization by recognizing the inherent variabil- ity in redundancy across different layers of the deep model. By allocating smaller bit-widths to layers with high redun- dancy, MPQ optimizes model complexity without causing a significant performance decline. The challenge, how- ever, lies in determining the most suitable bit-width for each layer, considering that the bit-width selection is a discrete process, and the potential combinations of bit-width and layer (referred to as “policy”) grow exponentially. Strategies such as HAQ [57] and ReleQ [11] leverage re- inforcement learning (RL) techniques to derive a bit-width allocator. SPOS [14], EdMIPS [3], BP-NAS [65], GMPQ [60] and SEAM [49] adopt differential neural architec- ture search (NAS) methods to learn the bit-width. How- ever, these methods require to both search-from-scratch and train-from-scratch for the models when changing the0 20 40 60 80 100 training step   (a) quantization with single 4bit 0.50 0.52 0.54 0.56 0.58 0.60Weight value 0 20 40 60 80 100 training step   (b) quantization with weight-sharing (2bit and 4bit) 0.50 0.52 0.54 0.56 0.58 0.60Weight value 0 20 40 60 80 100 training step   (c) normalized gradients 0.0 0.1 0.2 0.3Gradient value 0.0975 0.1000  real-valued weight quantized weight (4bits) target weight quantization bound 2-bits gradient 4-bits gradient 4-bits only gradient Figure 1. (a) 2D regression of single 4-bits quantization, (b) 2D regression of 4-bits quantization with an additional 2-bits ( i.e., weight- sharing quantization), and (c) the L2-normalized gradients of these two regressions. Compared with Fig. 1(a), the weight in Fig. 1(b) is more unstable due to the bit-width interference. Notably, the gradient of 4-bits also has a larger variance under weight-sharing. 0 200 400 600 0.033 0.034 0.035 0.036 0 200 400 600 0.02 0.03 0.04 0.05 0 50 0.2 0.1 0.0 0.1 0 50 0.15 0.20 0.25 w 2bits w/o 2bits w 2bits w/o 2bits Figure 2. Distance between full-precision latent weights and quan- tized weights on MobileNetV2 of a point-wise conv layer. Left: 4-bits. Right: 6-bits. search constraints. Unlike learning the optimal MPQ pol- icy, HAWQ [9, 10] and MPQCO [6] use the Hessian infor- mation as the quantization sensitivity metrics to assist bit- width assignment. LIMPQ [47] proposes to learn the layer- wise importance within a single quantization-aware training cycle. 2.2. Deep Learning with Weight-Sharing Weight-sharing is an effective and practical technique to reuse weight to deal with joint task learning or avoid the need to store multiple copies of weights. Generally speak- ing, there have been two relevant topics to weight-sharing with this work, covering neural architecture search (NAS) and dynamic neural network. Neural Architecture Search. NAS [21, 45, 69] aims to automatically discover the well-performing topology of deep neural network in a vast search space, which composes of various operators ( e.g. convolutional layers with different kernel-size or channels). To improve the search efficiency, recent works [14, 38, 50] both adopt the idea of weight-sharing to stuff the candidates into a shared and large meta-topology (a.k.a. the super-network). Weight-sharing allows to train directly the meta-topology and derive a sub-topology from the meta-topology to eval- uate. This significantly shortens the evaluation time of the goodness for a given topology [30, 38]. Although certain MPQ research [3, 60] leverages this NAS-style searching process, they still pay significant time for retraining. Dynamic Neural Network. Unlike conventional neural networks which are architecturally fixed during inference, dynamic neural networks [18] enable dynamic computa- tional paths on demand according to various input sam- ples or running environments. For example, runtime chan- nel pruning [29, 59] dynamically activates channels of lay- ers and layer skipping [43, 56, 58] adjusts depths of layers for different input images. To support the adaptive infer- ence, the weight-sharing idea is applied to avoid substantial copies of weights. Therefore, they can both achieve a better accuracy-efficiency trade-off compared to their static coun- terparts. These have also been several works of dynamic bit-width neural networks [2, 26, 48, 62]. However, they ei- ther provide only the fixed-precision quantization that sup- ports very limited bit-width configurations or have to drop the ultra-low bit-widths (e.g. 2 bits, 3bits, etc.) to guarantee the training convergence but lose the chance for achieving high compression ratio. 3. Methodology 3.1. Preliminary Quantization. The uniform quantization function under b bits in quantization-aware training (QAT) maps the input full-precision activations and weights to the homologous quantized values [0, 2b − 1] and [−2b−1, 2b−1 − 1]. The quantization functions Qb(·) that quantize the input values x to quantized values ˆx can be expressed as follows: ˆx = Qb(x; γ) = ⌊clip(x γ , Nmin, Nmax)⌉ ×γb, ∂⌊x⌉ ∂x ≜ 1, (1)Table 1. Accuracy of the weight-sharing quantization with/without low bit-width for MobileNetv2 (@80 epochs). Top-1 Acc. (%) w/ 2bits ( ↓ sampling probability) Top-1 Acc. (%) w/o 2bits 6 bit 69.2 70.4 4 bit 68.1 69.1 where ⌊·⌉ is the rounding-to-nearest function, and γ is the scale factor. The Straight-Through Estimation (STE) is used to pass the gradients for back-propagation. The clip function ensures that the input values fall into the range [Nmin, Nmax] [12, 67]. For ReLU activations, Nmin = 0 and Nmax = 2 b − 1. For weights, Nmin = −2b−1 and Nmax = 2 b−1 − 1. γb is the learnable scalar parameter used to adjust the quantization mappings, called the step- size scale factor. For a network, each layer has two distinct scale factors in the weights and activations quantizer. Weight-Sharing for Mixed-Precision Quantization. We consider a model f(·) = fWL−1 ◦ ... ◦ fW0 (·) with L layers, and each layer has N bit-width choices. Let W := {Wl}L−1 l=0 be the set of flattened weight tensors of these L layers. Therefore, the corresponding search space A with N2L mixed-precision quantization policies {(b(w) l , b(a)) l }L−1 l=0 share the same latent weights W . To track the time-prohibitive training costs of traversing the whole search space, Monte-Carlo sampling is used to ap- proximate the expectation term [47, 48, 66]. The overall optimization objective is formulated as follows arg min W ES∼A h L(f(x; S, w(S)), y) i ≈ arg min W 1 K KX Sk∼U(A) h L(f(x; Sk, ˆW(Sk)), y) i , (2) where ˆW(Sk) is the quantized weights of k-th sampled pol- icy Sk derived from the latent weights W. This weight- sharing of layer l is achieved by ˆW(Sk) := { ˆW(Sk) l }L−1 l=0 , where ˆW(Sk) l = Qb(k) l,w∈Sk (Wl; γ) (3) according to Eq. (1), where b(k) l,w ∈ Sk is the bit-width of weight of l-th layer in the policy Sk. 3.2. Interference in Highly Coupled Weight-sharing While training is possible, there still many challenges exist in weight-sharing in practice. For example, ABN [48] ob- serves a large accuracy gap between the lower bit-widths and higher bit-widths. These works simply bypass this problem and remove the ultra-low bit-width until the train- ing becomes stable, however, doing so does not achieve high compression ratios. Here, we discuss the bit-width interference caused by highly coupled latent weights W. Suppose we have K = 2 sampled policies {S0, S1} at training step t in Eq. (2), and the bit-width of weight is sampled differently, namely ˆW(S0) ̸= ˆW(S1). ∀b(0) l,w ∈ S0, ∀b(1) l,w ∈ S1 : b(0) l,w < b(1) l,w. Assumption 3.1 (Non-uniform Bit-width Convergence) . Quantized weights ˆW(S1) l = Qb(1) l,w∈S1 (Wl; γ) of bit-width bk at step t is nearly converged while the ˆW(S0) l = Qb(0) l,w∈S0 (Wl; γ) is not converged properly. The smaller and not fully converged bit-width inS0 will pose negative impact to the larger but well converged bit-width inS1. The situation in Assump. 3.1 is observed in weight- sharing network when the learning capacity gap between sub-networks is large enough [48, 66]. To further analyze the impact of S0, we can approximates the loss perturbation with the second-order Taylor expansion: ∆L = NX n=1 ℓ(f( ˆW(S1) + ∆W, xn), yn) − NX n=1 ℓ(f( ˆW(S1), xn), yn) ≈ ∇ˆW(S1) ℓ( ˆW(S1))∆W + ∆WT∇2 ˆW(S1) ℓ( ˆW(S1))∆W, (4) where ℓ(·) is the cross-entropy loss function, ∆W := {∆W(S1) l }L−1 l=0 is the quantization noise introduced by pol- icy S1 to policy S0 on each layer. It is noteworthy that the lower the bit-width, the larger the quantization noise intro- duced [23, 26, 68], caused by the large rounding and clip- ping error under very limited available discrete values, e.g. quantization error for 2 bits is roughly 5× for 3 bits. There- fore, putting small bit-width ( e.g. 2bit) into the weight- sharing will lead to large loss perturbation ∆L and disturb the overall performance eventually (see Tab. 1). Accord- ingly, one can draw such conclusions in Eq. (4): (i) remov- ing the low bit-width is the simplest way to erase the effects of quantization noise but loses the chance to compress the model with high compression ratio and(ii) one can track the loss perturbation through ∆W to iteratively freeze the most unstable bit-width, which motivates our method described later in Sec. 3.3. To illustrate the bit-width interference in weight-sharing quantization, we use a 2D regression quantization experi- ment depicted in Fig. 1. Our optimization objective mini- mizes the empirical risk [8, 35]: arg min w E x∼N(0,1) h ∥xw∗ − xQb(w, γ)∥2 2 i , (5) where w∗ represents the target weight andx is sampled from a normal distribution. In Fig. 1(a), the single-bit optimiza- tion showcases relatively stable quantized weights, occa- sionally crossing the quantization boundary due to gradient approximation of STE [1]. Comparatively, weight-sharing quantization exhibits more instability and the weight moves closer to the quantization bound more frequently(Fig. 1(b)), even with higher variance in gradients as in Fig. 1(c).This interference extends beyond toy regression to mod- ern neural networks, shown in Fig. 2 and Tab. 1. Further- more, Fig. 2 demonstrates the distance between quantized weights of 6 and 4 bits in one training epoch. Removing the smallest bit-width (2 bits) notably stabilizes the higher bit- width curve. However, introducing extra small bit-widths induces significant random oscillations, signifying height- ened model training instability. 3.3. Dynamic Bit-width Schedule Eq. (4) reveals the decomposition of overall quantization noise ∆W into layer-specific perturbation components, of- fering a metric to identify unstable layers. Therefore, dy- namically freezing the bit-width causing weight interference ensures proper convergence for remaining bit-widths dur- ing training. However, direct use of Eq. (4) poses compu- tational challenges, particularly in calculating the Hessian and quantization noise terms, prompting us to devise an al- ternative method. We approximate layer perturbations by focusing on rounding errors due to their significant impact on overall performance [17, 35]. Rounding errors portray the distance between full-precision weights and their discrete quantiza- tion levels, and reach maximums when at the midpoint be- tween two quantization levels ( i.e., the quantization bound in Fig. 1) because the possible quantization levels change. In other words, the closer to the quantization bounds, the more unstable the weights are, and therefore the unstable weights are more vulnerable to the weight-sharing. There- fore, tracking the round errors provide effective proxies for constructing our bit-width scheduler. For clarity, we first definite the Bit-width Representation Set (BRS) as follows: Definition 3.1 (Bit-width Representation Set) . For bit- width b under uniform weight quantization, the bit-width representation set Φb := γ × {−2b−1, ...,0, ...,2b−1 − 1}, representing 2b decomposed values of discrete quantization levels according to Eq. (1). The midpoints between two adjacent elements in a BRS are quantization bounds, where they have a uniform dis- tance γ. Given a pre-defined weight bit-width candidates B(w), we can accumulate bit-specific unstable weights for BRS of each bit-width of each layer’s shared weights Wl. Therefore, we calculate the unstable weight criterion ˆ∆Wunstable by ˆ∆Wunstable ≜ {ˆ∆Wunstable l }L−1 l=0 , where ˆ∆Wunstable l = X b∈B(w) 1 2b 1 ∥Wl∥0 · X qb∈Φb X wl,∗∈Wl 1 |wl,∗|≤γ×(1−ϵ 2 + qb γ ), (6) 40  20  0 20 0.00 0.05 0.10Density layer 4 10  0 10 layer 7 20  0 20 layer 10 10  0 10 0.0 0.1 0.2 0.3Density layer 5 10  0 10 layer 8 10  0 10 layer 11 2 bit 6 bit Figure 3. Output density at 2bit and 6bits. Small bit-width shows noteworthy information distortion. where ϵ ∈ [0, 1] is to control the range of weights we care about. After that, we choose the frozen layer set Ω with a Top-K selector from the weight criterion, Ω ← TopKToFreeze( ˆ∆Wunstable; K), (7) and the smallest bit-width of selected layers in Ω will be temporarily frozen periodically. In practice, we use a cosine scheduler to gradually decrease the value of K to guarantee that more unstable low bits will be frozen early to improve the convergence of more high bits. 3.4. Optimization during Dynamic Training Information Distortion Mitigation. While freezing the bit-width of layers, we observe the outputs of the remaining small bit-widths of layers still exhibit a information distor- tion compared to their high precision counterparts, as shown in Fig. 3. Inspired by the information bottleneck principle [51, 52, 63], we expect if the smallest bit-width is sampled of a layer l, its outputs OS l can preserve the information of its large counterparts OH l . However, directly optimizing this mutual information term I(OS l ; OH l ) is infeasible, so we consider a feature alignment loss function to optimize their rectified Euclidean distance as follows: E h ∥max{Q, OS − µ(OS)p σ(OS) + ζ ηOS + ξOS }− max{Q, OH − µ(OH)p σ(OH) + ζ ηOH + ξOH }∥ i , (8) where ζ is a small constant to avoid Divide-by-Zero errors, η and ξ are the learnable parameters for adapting the fea- tures, µ(·) and σ(·) return the channel-wise mean and vari- ance of input. Eq. (8) not only scales the features for better optimization but uses a max operator to avoid needless ac- tivations. See Fig. 4 for visualization with proposed Infor- mation Distortion Mitigation technique.Fairness Weight Regularization. Low-bit weights are ac- tually subsets of high-bit weights, when a layer is sampled with different bit-width, low-bit weights will receive addi- tional updates from high-bit weights. In other words, low- bit weights are subjected to very aggressive weight regu- larization, which exacerbates their underfitting issues [66]. To ensure regularization fairness, we disable weight regu- larization for weights of current smallest bit-width during training. 3.5. Bidirectional Greedy Search To find the optimal quantization policy S∗, the existing MPQ methods can be formulated to a bi-level optimization problem [47]. In this paper, our well-trained weight-sharing model can serve as a good performance indicator to perform inference-only searching [55, 66]. This simplified proce- dure motivates us to devise a bidirectional greedy search scheme to determine the per-layer bit-width efficiently. Consider a mixed-precision quantization policy, S(t), implemented at step t, with L being the total number of lay- ers. To evolve this policy toS(t+1), rather than concurrently adjusting the bit-width of most layers ( e.g., employing re- inforcement learning), a step-by-step approach is taken. Specifically, the bit-width of a single layer is adjusted at a time, either increasing or decreasing by a single bit-width to create a provisional policy, S(t) i , where i ∈ {0, ...,2L − 1}. This method yields a search space of complexityO(2L) for each iteration. During each iteration, the permanent policy S(t+1) is chosen in a greedy manner between these2L poli- cies, considering the trade-off between accuracy and effi- ciency, denoted asJi = ¯Lval( ˆW(S(t) i ))+ λ∗BitOps(S(t) i ) for each layer: S(t+1) ← arg min i [Θ] , Θ ≜ {Ji|Ji = ¯Lval( ˆW(S(t) i )) + λ ∗ BitOps(S(t) i )}2L−1 i=0 , (9) where ¯L and BitOps are the min-max normalization loss and BitOPs to ensure their values fall into the interval [0, 1], and λ is the hyper-parameter to control the trade- off, respectively. By this means, the solution S∗ is reached when the BitOPs is satisfied at final step T, i.e., S∗ ← S(T), if BitOps(S(T)) ≤ C. 4. Experiments In this section, we conduct experiments on three lightweight models (i.e., ResNet18, MobileNetv2, and EfficientNetLite- B0) and three datasets ( i.e., ImageNet, Pets, and CI- FAR100). Please refer to the Supplementary Materials for more detailed experimental setups. Table 2. Accuracy and efficiency results for ResNet. “Top-1 Acc.” represents the Top-1 accuracy of the quantized model and full- precision model. “MP” means mixed-precision quantization. “Re- train Cost” denotes the epochs required to retrain the MPQ policy. “*”: reproduces through the vanilla ResNet architecture [19]. The best results are bolded in each metric, the second best results are underlined. Method Bit-width (W/A) Top-1 Acc. (%) ↑ BitOPs (G) ↓ Retrain Cost (Epoch) ↓ Baseline 32 / 32 70.5 - - PACT [7] 3 / 3 68.1 23.0 - LSQ∗ [12] 3 / 3 69.4 23.0 90 EWGS [28] 3 / 3 69.7 23.0 100 EdMIPS [12] 3 MP / 3MP 68.2 - 40 GMPQ∗ [60] 3 MP / 3MP 68.6 22.8 40 DNAS [61] 3 MP / 3MP 68.7 24.3 120 FracBits [64] 3 MP / 3MP 69.4 22.9 150 LIMPQ [47] 3 MP / 3MP 69.7 23.0 90 SEAM [49] 3 MP / 3MP 70.0 23.0 90 Ours 2MP / 3MP 67.7 17.3 0 Ours 3MP / 3MP 70.2 23.3 0 PACT [7] 4 / 4 69.2 35.0 - LSQ∗ [12] 4 / 4 70.5 35.0 90 EWGS [28] 4 / 4 70.6 35.0 100 MPQCO [6] 4 MP / 4MP 69.8 - 30 DNAS [13] 4 MP / 4MP 70.6 35.1 120 FracBits [64] 4 MP / 4MP 70.6 34.7 150 LIMPQ [47] 4 MP / 4MP 70.8 35.0 90 SEAM [49] 4 MP / 4MP 70.8 33.7 90 Ours 4MP / 4MP 71.0 31.6 0 4.1. ImageNet Classification ResNet. PACT demonstrates accuracy with 3-bits for both weights and activations, achieving 68.1%. LSQ reaches 69.4% accuracy but requires 90 retraining epochs. Ed- MIPS and GMPQ employ MPQ (3 MP / 3MP) for 68.2% and 68.6% accuracy but still require considerable retraining costs. DNAS and FracBits adopt longer retraining epochs and yield better accuracy. When increasing the bit-width to 4-bits, PACT achieves 69.2% accuracy with 35.0G BitOPs, while LSQ reaches 70.5% accuracy. DNAS and FracBits demonstrate a 4- bits MPQ with slightly different results, while LIMPQ and SEAM both achieve the highest accuracy but still need 90 retraining epochs. Notably, our method with vary- ing bit-width configurations (2 MP/3MP, 3 MP/3MP, and 4MP/4MP). The 4 MP/4MP configuration achieves theTable 3. Accuracy and efficiency results for MobileNetV2. †: QBitOPT uses channel-wise quantization to retain performance. Method Bit-width (W/A) Top-1 Acc. (%) ↑ BitOPs (G) ↓ Retrain Cost (Epoch) ↓ Baseline 32 / 32 72.6 - - LSQ [12] 3 / 3 65.2 3.4 90 QBR [17] 3 / 3 67.4 3.4 90 HMQ [15] 2 MP / 4MP 64.5 - 50 QBitOPT† [37] 3 MP / 3MP 65.7 - 30 NIPQ [44] 3 MP / 3MP 62.3 - 43 Ours 3MP / 3MP 67.8 3.6 0 LSQ [12] 4 / 4 69.5 5.4 90 EWGS [28] 4 / 4 70.3 5.4 100 AdaBits [26] 4 / 4 70.4 5.4 0 QBR [17] 4 / 4 70.4 5.4 90 MPDNN [53] 3.75 MP / 4MP 69.8 - 50 QBitOPT† [37] 4 MP / 4MP 69.7 - 30 NIPQ [44] 4 MP / 4MP 69.2 - 43 BayesianBits [54] 4 MP / 4MP 69.0 5.9 40 GMPQ [17] ∼ 4MP / 4MP 70.4 7.4 40 HAQ [57] 6 MP / 4MP 69.5 8.3 30 Ours 4MP / 4MP 70.7 5.5 0 highest accuracy in the table at 71.0%, with competitive BitOPs (31.6G) and no retraining cost. MobileNetV2. QBR demonstrates a competitive Top- 1 accuracy of 67.4% with 3/3 bit-width and 3G BitOPs. QBitOPT adopts a performance-friendly channel- wise quantization and achieves 65.7% accuracy in the 3MP/3MP configuration and requires retraining split into 15 + 15 epochs [37], suggesting a more complex process. In the 4/4 bit-width category, QBR stands out with 70.4% ac- curacy and 5.4G BitOPs, demonstrating efficiency. GMPQ delivers 70.4% accuracy but requires 40 retraining epochs. HAQ achieves 69.5% accuracy but incurs higher BitOPs (8.3G) and demands 30 retraining epochs. With 3MP/3MP bit-width, our method reaches 67.8% ac- curacy with 3.6G BitOPs and no retraining. Moreover, in the 4MP/4MP configuration, it excels with a Top-1 accuracy of 70.7% and competitive BitOPs (5.5G), all while elimi- nating retraining costs. EfficientNet. LSQ achieves 69.7% accuracy with 4.2G BitOPs and requires 90 retraining epochs. In contrast, our 3MP/3MP method attains 70.4% accuracy with 4.5G BitOPs but eliminates the need for retraining, showcas- ing improved accuracy at a lower cost. QBitOPT achieves 70.0% accuracy under 3 MP/3MP with 30 epochs for re- Table 4. Accuracy and efficiency results for EfficientNetLite-B0. †: QBitOPT uses channel-wise quantization to retain performance. Method Bit-width (W/A) Top-1 Acc. (%) ↑ BitOPs (G) ↓ Retrain Cost (Epoch) ↓ Baseline 32 / 32 75.4 - - LSQ [12] 3 / 3 69.7 4.2 90 NIPQ [44] 3 MP / 3MP 66.5 - 43 QBitOPT† [37] 3 MP / 3MP 70.0 - 30 MPQDNN [53] 3 MP / 3MP 68.8 - 50 Ours 3MP / 3MP 70.4 4.5 0 LSQ [12] 4 / 4 72.3 6.8 90 NIPQ [44] 4 MP / 4MP 72.3 - 43 QBitOPT† [37] 4 MP / 4MP 73.3 - 30 Ours 4MP / 4MP 73.2 6.9 0 Table 5. Accuracy and efficiency results for ResNet with knowl- edge distillation. Method Bit-width (W/A) Top-1 Acc. (%) ↑ BitOPs (G) ↓ Retrain Cost (Epoch) ↓ Baseline 32 / 32 70.5 - - GMPQ [60] 3 MP / 3MP 69.5 22.8 90 SEAM [49] 3 MP / 3MP 70.7 23.0 90 EQNet [62] 3 MP / 3MP 69.8 - 0 SDQ [23] 3 MP / 3 70.2 25.1 90 Ours 3MP / 3MP 70.9 23.9 0 NIPQ [44] 4 MP / 4MP 71.2 34.2 40 SDQ [23] 4 MP / 3 71.7 33.4 90 Ours 4MP / 4MP 71.6 31.6 0 training. Our method at the same setting achieves 70.4% ac- curacy without any retraining, highlighting superior perfor- mance without complex retraining. While LSQ and NIPQ achieve 72.3% accuracy at 4/4 bit-width, they demand 90 retraining epochs. Our 4 MP/4MP method surpasses both, achieving 73.2% accuracy with 6.9G BitOPs and no retrain- ing. Our method consistently achieves comparable or supe- rior accuracy with no retraining costs, demonstrating effi- cacy and simplicity in EfficientNet quantization. 4.2. Ablation Study Efficientness with KD. In comparison to the existing methods in Tab. 5 when knowledge distillation (KD) is enabled with a ResNet101 teacher model, our method exhibits compelling advantages. GMPQ achieves a re- spectable 69.5% accuracy with 3 MP bit-width but requires 90 retraining epochs. Our method surpasses it significantly,Table 6. Effectiveness of proposed dynamic bit-width schedule scheme and information distortion mitigation (IDM) training tech- nique. To save costs, we train the weight-sharing model 80 epochs. Dynamic Bit Schedule IDM Training 4 Bit Top-1 Acc. (%) ✗ ✗ 68.3 ✓ ✗ 69.1 (+0.8%) ✓ ✓ 69.5 (+1.2%) achieving a 70.9% accuracy without retraining. Similarly, SEAM marginally improves accuracy to 70.7%, but our method still outperforms with 70.9% accuracy and no retraining costs. EQNet stands out with zero retraining epochs but falls significantly short of our method in accuracy (69.8%). SDQ shows varied performance, but our method consistently outperforms it, particularly with 3 MP / 3MP and 4MP / 4MP bit-width configurations, achieving higher accuracy and requiring no retraining compared to SDQ’s 90 retraining epochs. Effectiveness of Proposed Techniques.Tab. 6 investigates the impact of a dynamic bit-width schedule and our infor- mation distortion mitigation (IDM) training technique on the weight-sharing model. It presents three experimental scenarios: without both dynamic bit scheduling and IDM training resulting in 68.3% Top-1 accuracy, dynamic bit scheduling alone with an improvement to 69.1%, and the combination of both techniques achieving the highest Top- 1 accuracy of 69.5%. The results suggest that both dynamic bit scheduling and IDM training contribute positively to the model’s performance, and their combination yields the most significant improvement. Moreover, our IDM training technique significantly mitigates information distortion, as shown in Fig. 4. 4.3. Transfer Learning We transfer the quantized weights for downstream bench- marks to verify the generalization ability of the proposed method. We directly use the pretrained checkpoints on Im- ageNet and then finetune the classifiers. As shown in Tab. 7, our method achieves the same accuracy as a full-precision model at 4-bits with smaller model complexity, which fur- ther confirms the superiority of the proposed method. 5. Conclusion In this paper, we introduce a novel one-shot training- searching paradigm for mixed-precision model compres- sion. More specifically, traditional approaches focus on bit-width configurations but overlook significant retraining costs. We identified and addressed bit-width interference issues by introducing a dynamic scheduler and an infor- 20  0 20 0.00 0.05 0.10 0.15Density layer 4 10  0 10 layer 7 20  0 layer 10 10  0 10 0.0 0.1 0.2 0.3Density layer 5 0 10 layer 8 10  0 10 layer 11 2 bit 6 bit Figure 4. Output density at 2bit and 6bits with our IDM train- ing. Compared with Fig. 3, information distortion of the small bit-widths is significantly mitigated. Table 7. Performance of transfer learning using the pretrained weights on ImageNet. Model Bit-width (W/A) CIFAR100 [27] Top-1 Acc. (%) Pets [36] Top-1 Acc. (%) ResNet18 32 / 32 79.4 88.9 4MP / 4MP 79.5 (+0.1%) 88.7 ( -0.2%) 3MP / 3MP 78.7 (-0.7%) 87.9 ( -2.0%) MobileNetV2 32 / 32 78.9 86.0 4MP / 4MP 79.0 (+0.1%) 86.1 ( +0.1%) 3MP / 3MP 78.2 (-1.7%) 84.1 ( -1.9%) mation distortion mitigation technique. Together with an inference-only greedy search scheme, our method can sig- nificantly reduce the costs of mixed-precision quantization. Experiments on three commonly used benchmarks across various network architectures validate the effectiveness and efficiency of the proposed method in compressing mod- els. Overall, our method offers a promising solution for deploying compressed models without compromising per- formance on resource-limited devices. Acknowledgment This work was supported by the National Key Research and Development Program of China No. 2023YFF0905502, National Natural Science Foundation of China (Grant No. 62250008), Beijing National Research Center for In- formation Science and Technology (BNRist) under Grant No. BNR2023TD03006 and Beijing Key Lab of Net- worked Multimedia, and Shenzhen Science and Technol- ogy Program (Grant No. RCYX20200714114523079 and No. JCYJ20220818101014030).References [1] Yoshua Bengio, Nicholas L ´eonard, and Aaron C. Courville. Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation. CoRR, 2013. 2, 4 [2] Adrian Bulat and Georgios Tzimiropoulos. Bit-mixer: Mixed-precision networks with runtime bit-width selection. In Proc. of ICCV, 2021. 3 [3] Zhaowei Cai and Nuno Vasconcelos. Rethinking Differen- tiable Search for Mixed-precision Neural Networks. InProc. of CVPR, 2020. 1, 2, 3 [4] Cheng Chen, Yichun Yin, Lifeng Shang, Xin Jiang, Yujia Qin, Fengyu Wang, Zhi Wang, Xiao Chen, Zhiyuan Liu, and Qun Liu. bert2bert: Towards reusable pretrained language models. In Proc. of ACL, 2022. 1 [5] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolu- tion, and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence, 2017. 1 [6] Weihan Chen, Peisong Wang, and Jian Cheng. Towards Mixed-precision Quantization of Neural Networks via Con- strained Optimization. In Proc. of ICCV, 2021. 1, 3, 6 [7] Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan. PACT: Parameterized Clipping Activation for Quantized Neural Networks. CoRR, 2018. 2, 6 [8] Alexandre D ´efossez, Yossi Adi, and Gabriel Synnaeve. Differentiable model compression via pseudo quantization noise. Transactions on Machine Learning Research, 2022. 4 [9] Zhen Dong, Zhewei Yao, Amir Gholami, Michael W. Ma- honey, and Kurt Keutzer. HAWQ: Hessian AWare Quantiza- tion of Neural Networks With Mixed-precision. In Proc. of ICCV, 2019. 1, 3 [10] Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, Michael W. Mahoney, and Kurt Keutzer. HAWQ-V2: Hes- sian Aware trace-weighted Quantization of Neural Net- works. In Proc. of NeurIPS, 2020. 3 [11] Ahmed T. Elthakeb, Prannoy Pilligundla, Fatemehsa- dat Mireshghallah, Amir Yazdanbakhsh, and Hadi Es- maeilzadeh. ReLeQ : A Reinforcement Learning Approach for Automatic Deep Quantization of Neural Networks.IEEE Micro, 2020. 1, 2 [12] Steven K. Esser, Jeffrey L. McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra S. Modha. Learned Step Size quantization. In Proc. of ICLR, 2020. 1, 2, 4, 6, 7 [13] Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu, Jiazhen Lin, Fengwei Yu, and Junjie Yan. Dif- ferentiable Soft Quantization: Bridging Full-precision and Low-bit Neural Networks. In Proc. of ICCV, 2019. 6 [14] Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, and Jian Sun. Single Path One- shot Neural Architecture Search with Uniform Sampling. In Proc. of ECCV, 2020. 2, 3 [15] Hai Victor Habi, Roy H. Jennings, and Arnon Netzer. HMQ: Hardware Friendly Mixed Precision Quantization Block for CNNs. In Proc. of ECCV, 2020. 1, 7 [16] Song Han, Huizi Mao, and William J Dally. Deep com- pression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015. 1 [17] Tiantian Han, Dong Li, Ji Liu, Lu Tian, and Yi Shan. Im- proving low-precision network quantization via bin regular- ization. In Proc. of ICCV, 2021. 5, 7 [18] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, and Yulin Wang. Dynamic neural networks: A sur- vey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021. 3 [19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In Proc. of CVPR, 2016. 1, 6 [20] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir- shick. Mask r-cnn. In Proc. of ICCV, 2017. 1 [21] Andrew Howard, Ruoming Pang, Hartwig Adam, Quoc V . Le, Mark Sandler, Bo Chen, Weijun Wang, Liang-Chieh Chen, Mingxing Tan, Grace Chu, Vijay Vasudevan, and Yukun Zhu. Searching for MobileNetV3. In Proc. of ICCV, 2019. 1, 3 [22] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco An- dreetto, and Hartwig Adam. MobileNets: Efficient Con- volutional Neural Networks for Mobile Vision Applications. CoRR, 2017. 1 [23] Xijie Huang, Zhiqiang Shen, Shichao Li, Zechun Liu, Xi- anghong Hu, Jeffry Wicaksana, Eric P. Xing, and Kwang- Ting Cheng. SDQ: Stochastic Differentiable Quantization with Mixed Precision. In Proc. of ICML, 2022. 1, 4, 7 [24] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. Gpipe: Efficient training of giant neural networks using pipeline parallelism. Proc. of NeurIPS, 2019. 1 [25] Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. Accurate post training quantization with small calibration sets. In Proc. of ICML, 2021. 2 [26] Qing Jin, Linjie Yang, and Zhenyu Liao. Adabits: Neural network quantization with adaptive bit-widths. In Proc. of CVPR, 2020. 3, 4, 7 [27] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 8 [28] Junghyup Lee, Dohyung Kim, and Bumsub Ham. Network quantization with element-wise gradient scaling. In Proc. of CVPR, 2021. 6, 7 [29] Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning. Proc. of NeurIPS, 2017. 3 [30] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055, 2018. 3 [31] Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the Value of Network Pruning. In Proc. of ICLR, 2019. 1 [32] Rongwei Lu, Yutong Jiang, Yinan Mao, Chen Tang, Bin Chen, Laizhong Cui, and Zhi Wang. Dagc: Data-volume- aware adaptive sparsification gradient compression for dis-tributed machine learning in mobile computing. arXiv preprint arXiv:2311.07324, 2023. 1 [33] Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. Importance estimation for neural network pruning. In Proc. of CVPR, 2019. 1 [34] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training quantization. In Proc. of ICML , 2020. 2 [35] Markus Nagel, Marios Fournarakis, Yelysei Bondarenko, and Tijmen Blankevoort. Overcoming oscillations in quantization-aware training. In Proc. of ICML, 2022. 4, 5 [36] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In Proc. of CVPR, 2012. 8 [37] Jorn Peters, Marios Fournarakis, Markus Nagel, Mart van Baalen, and Tijmen Blankevoort. Qbitopt: Fast and accurate bitwidth reallocation during training. InProc. of ICCV, 2023. 7 [38] Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. Efficient neural architecture search via parameters sharing. In Proc. of ICML, 2018. 3 [39] Antonio Polino, Razvan Pascanu, and Dan Alistarh. Model compression via distillation and quantization. In Proc. of ICLR, 2018. 1 [40] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767, 2018. 1 [41] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Proc. of NeurIPS, 2015. 1 [42] Mark Sandler, Andrew G. Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. MobileNetV2: Inverted Residuals and Linear Bottlenecks. In Proc. of CVPR, 2018. 1 [43] Jianghao Shen, Yue Wang, Pengfei Xu, Yonggan Fu, Zhangyang Wang, and Yingyan Lin. Fractional skipping: Towards finer-grained dynamic cnn inference. In Proc. of AAAI, 2020. 3 [44] Juncheol Shin, Junhyuk So, Sein Park, Seungyeop Kang, Sungjoo Yoo, and Eunhyeok Park. Nipq: Noise proxy-based integrated pseudo-quantization. In Proc. of CVPR, 2023. 7 [45] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. InProc. of ICML, 2019. 3 [46] Mingxing Tan, Ruoming Pang, and Quoc V Le. Efficientdet: Scalable and efficient object detection. In Proc. of CVPR , 2020. 1 [47] Chen Tang, Kai Ouyang, Zhi Wang, Yifei Zhu, Wen Ji, Yaowei Wang, and Wenwu Zhu. Mixed-precision Neural Network Quantization via Learned Layer-wise Importance. In Proc. of ECCV, 2022. 1, 3, 4, 6 [48] Chen Tang, Haoyu Zhai, Kai Ouyang, Zhi Wang, Yifei Zhu, and Wenwu Zhu. Arbitrary bit-width network: A joint layer- wise quantization and adaptive inference approach. In Proc. of ACM MM, 2022. 3, 4 [49] Chen Tang, Kai Ouyang, Zenghao Chai, Yunpeng Bai, Yuan Meng, Zhi Wang, and Wenwu Zhu. Seam: Searching trans- ferable mixed-precision quantization policy through large margin regularization. In Proc. of ACM MM , 2023. 2, 6, 7 [50] Chen Tang, Li Lyna Zhang, Huiqiang Jiang, Jiahang Xu, Ting Cao, Quanlu Zhang, Yuqing Yang, Zhi Wang, and Mao Yang. Elasticvit: Conflict-aware supernet training for de- ploying fast vision transformer on diverse mobile devices. In Proc. of ICCV, 2023. 1, 3 [51] Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In 2015 ieee information theory workshop (itw), 2015. 5 [52] Naftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. arXiv preprint physics/0004057, 2000. 5 [53] Stefan Uhlich, Lukas Mauch, Fabien Cardinaux, Kazuki Yoshiyama, Javier Alonso Garcia, Stephen Tiedemann, Thomas Kemp, and Akira Nakamura. Mixed precision dnns: All you need is a good parametrization. arXiv preprint arXiv:1905.11452, 2019. 1, 7 [54] Mart Van Baalen, Christos Louizos, Markus Nagel, Rana Ali Amjad, Ying Wang, Tijmen Blankevoort, and Max Welling. Bayesian bits: Unifying quantization and pruning. Proc. of NeurIPS, 2020. 7 [55] Dilin Wang, Meng Li, Chengyue Gong, and Vikas Chandra. Attentivenas: Improving neural architecture search via atten- tive sampling. In Proc. of CVPR, 2021. 6 [56] Jue Wang, Ke Chen, Gang Chen, Lidan Shou, and Julian McAuley. Skipbert: Efficient inference with shallow layer skipping. In Proc. of ACL, 2022. 3 [57] Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han. HAQ: Hardware-aware Automated Quantization With Mixed Precision. In Proc. of CVPR, 2019. 1, 2, 7 [58] Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E Gonzalez. Skipnet: Learning dynamic routing in convolutional networks. In Proc. of ECCV, 2018. 3 [59] Yulong Wang, Xiaolu Zhang, Xiaolin Hu, Bo Zhang, and Hang Su. Dynamic network pruning with interpretable lay- erwise channel selection. In Proc. of AAAI, 2020. 3 [60] Ziwei Wang, Han Xiao, Jiwen Lu, and Jie Zhou. Gener- alizable Mixed-precision Quantization via Attribution Rank Preservation. In Proc. of ICCV, 2021. 1, 2, 3, 6, 7 [61] Bichen Wu, Yanghan Wang, Peizhao Zhang, Yuandong Tian, Peter Vajda, and Kurt Keutzer. Mixed Precision Quantization of ConvNets via Differentiable Neural Architecture Search. CoRR, 2018. 6 [62] Ke Xu, Lei Han, Ye Tian, Shangshang Yang, and Xingyi Zhang. Eq-net: Elastic quantization neural networks. In Proc. of ICCV, 2023. 3, 7 [63] Sheng Xu, Yanjing Li, Mingbao Lin, Peng Gao, Guodong Guo, Jinhu L ¨u, and Baochang Zhang. Q-detr: An efficient low-bit quantized detection transformer. In Proc. of CVPR, 2023. 5 [64] Linjie Yang and Qing Jin. FracBits: Mixed Precision Quan- tization via Fractional Bit-widths. In Proc. of AAAI, 2021. 6 [65] Haibao Yu, Qi Han, Jianbo Li, Jianping Shi, Guangliang Cheng, and Bin Fan. Search What You Want: Barrier Pan- elty NAS for Mixed Precision Quantization. In Proc. of ECCV, 2020. 2[66] Jiahui Yu, Pengchong Jin, Hanxiao Liu, Gabriel Bender, Pieter-Jan Kindermans, Mingxing Tan, Thomas Huang, Xi- aodan Song, Ruoming Pang, and Quoc Le. Bignas: Scaling up neural architecture search with big single-stage models. In Proc. of ECCV, 2020. 4, 6 [67] Shuchang Zhou, Zekun Ni, Xinyu Zhou, He Wen, Yuxin Wu, and Yuheng Zou. DoReFa-Net: Training Low Bitwidth Con- volutional Neural Networks with Low Bitwidth Gradients. CoRR, 2016. 1, 2, 4 [68] Yiren Zhou, Seyed-Mohsen Moosavi-Dezfooli, Ngai-Man Cheung, and Pascal Frossard. Adaptive quantization for deep neural network. In Proc. of AAAI, 2018. 4 [69] Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578 , 2016. 3",
      "meta_data": {
        "arxiv_id": "2401.01543v2",
        "authors": [
          "Chen Tang",
          "Yuan Meng",
          "Jiacheng Jiang",
          "Shuzhao Xie",
          "Rongwei Lu",
          "Xinzhu Ma",
          "Zhi Wang",
          "Wenwu Zhu"
        ],
        "published_date": "2024-01-03T05:26:57Z",
        "pdf_url": "https://arxiv.org/pdf/2401.01543v2.pdf",
        "github_url": "https://github.com/1hunters/retraining-free-quantization"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces a novel one-shot training-searching paradigm for mixed-precision model compression, aiming to eliminate the significant retraining costs associated with traditional MPQ methods. It identifies and analyzes the 'bit-width interference problem' in weight-sharing quantization models, which leads to training instability. To address this, it proposes a dynamic bit-width scheduler that freezes interfering bit-widths and an information distortion mitigation technique to align the behavior of poorly performing bit-widths. Additionally, an inference-only bidirectional greedy search scheme is devised for efficient bit-width allocation without further training. The method achieves state-of-the-art accuracy with competitive BitOPs and zero retraining costs across various models and datasets.",
        "methodology": "The methodology centers on a two-stage paradigm: a one-shot training phase and an inference-only search phase. In the first stage, all potential bit-width configurations are coupled and optimized simultaneously within a shared weight network. The identified 'bit-width interference problem' is tackled by: (1) A dynamic bit-width scheduler that uses rounding errors to approximate layer perturbations, calculating an 'unstable weight criterion' (ˆ∆Wunstable) for each layer, and then applying a Top-K selector to temporarily freeze the smallest bit-width of highly unstable layers using a cosine scheduler to decrease K. (2) An Information Distortion Mitigation (IDM) technique that aligns the outputs of small bit-widths with their high-precision counterparts through a feature alignment loss, optimizing the rectified Euclidean distance between normalized outputs. (3) Fairness weight regularization by disabling regularization for the smallest bit-width weights to prevent underfitting. The second stage employs an inference-only bidirectional greedy search to find optimal per-layer bit-width policies by iteratively adjusting a single layer's bit-width, selecting policies based on a trade-off between loss and BitOPs, until a target BitOPs constraint is met.",
        "experimental_setup": "Experiments were conducted on three lightweight models: ResNet18, MobileNetv2, and EfficientNetLite-B0, across three datasets: ImageNet, Pets, and CIFAR100. The evaluation metrics included Top-1 Accuracy (%), BitOPs (G), and Retrain Cost (Epoch). The proposed method was benchmarked against various existing quantization techniques such as PACT, LSQ, EdMIPS, GMPQ, LIMPQ, SEAM, QBR, and QBitOPT. Ablation studies investigated the individual and combined effectiveness of the dynamic bit-width scheduler and the information distortion mitigation technique. Further experiments included knowledge distillation using a ResNet101 teacher model and transfer learning, where ImageNet-pretrained quantized weights were finetuned for downstream tasks on CIFAR100 and Pets datasets.",
        "limitations": "The paper primarily addresses the limitations of prior mixed-precision quantization (MPQ) methods, specifically their considerable retraining costs. The proposed method itself does not explicitly list its own limitations. However, implicitly, the approach involves approximations (e.g., using rounding errors to approximate layer perturbations) and relies on hyperparameters (e.g., λ for loss-BitOPs trade-off, K for layer freezing, and ϵ in the unstable weight criterion) that require tuning. The greedy search, while efficient, may not guarantee a globally optimal bit-width configuration. The generalizability beyond the evaluated architectures and datasets is assumed.",
        "future_research_directions": "Not mentioned",
        "experimental_code": "# Dynamic Bit-width Scheduler - Freezing logic in training loop (process.py)\nif configs.enable_dynamic_bit_training and \\\n     epoch > 5 and (epoch + 1) % T == 0:\n    freezing_ratio = freezing_annealing_schedule((epoch - 5) // 2)\n    freezing_metric = profile_layerwise_quantization_metric(model=model)\n    freeze_layers(metric=freezing_metric, model=model, ratio=freezing_ratio, \n                  progressive=False, logger=logger, org_cands=configs.target_bits\n                  )\n    logger_info(logger=logger, msg= f'Current freezing ratio: {freezing_ratio}')\n\n# Dynamic Bit-width Scheduler - Profile layer-wise quantization metric (util/qat.py)\ndef profile_layerwise_quantization_metric(model: nn.Module, proportion=0.25):\n    num_shifed_ls = []\n\n    for name, module in model.named_modules():\n        if isinstance(module, QuanConv2d) and module.fixed_bits is None:\n            shifted_val = 0.\n\n            for idx, bit_width in enumerate(module.weight_bit_cands):\n                q_weights = module.quan_w_fn(module.weight, bit_width, is_activation=False).detach()\n                step_quantizer_param = module.quan_w_fn.get_scale(bit_width)\n                lower, upper = compute_thd(module.quan_w_fn, bit_width)\n                if isinstance(lower, torch.Tensor):\n                    lower = int(lower.cpu().item())\n                    upper = int(upper.cpu().item())\n\n                num_shifed = 0\n                num_weights = 0\n\n                for int_bin in range(lower, upper + 1):\n                    fp_bin = int_bin * step_quantizer_param\n                    fp_bin_weights = module.weight[q_weights == fp_bin]\n\n                    if int_bin == lower:\n                        shitfed_weights_right = fp_bin_weights[fp_bin_weights > step_quantizer_param * (int_bin +  .5 * (1 - proportion))]\n\n                        num_shifed += shitfed_weights_right.shape[0]\n                    elif int_bin == upper:\n                        shitfed_weights_left = fp_bin_weights[fp_bin_weights < fp_bin - step_quantizer_param * .5 * (1 - proportion)]\n\n                        num_shifed += shitfed_weights_left.shape[0]\n                    else:\n                        shitfed_weights_left = fp_bin_weights[fp_bin_weights < fp_bin - step_quantizer_param * .5 * (1 - proportion)]\n                        shitfed_weights_right = fp_bin_weights[fp_bin_weights > fp_bin + step_quantizer_param * .5 * (1 - proportion)]\n\n                        num_shifed += shitfed_weights_left.shape[0] + shitfed_weights_right.shape[0]\n                    \n                    num_weights += fp_bin_weights.shape[0]\n                \n                shifted_val += (num_shifed / num_weights / (bit_width.cpu() if isinstance(bit_width, torch.Tensor) else bit_width))\n            \n            num_shifed_ls.append(shifted_val / len(module.weight_bit_cands))\n    \n    return num_shifed_ls\n\n# Dynamic Bit-width Scheduler - Freeze layers (util/qat.py)\ndef freeze_layers(metric, model, org_cands, ratio=0.25, point_wise_min=2, depth_wise_min=3, progressive=False, logger=None):\n    sorted_layer_metric = np.argsort(-np.array(metric))\n    freezed_layer_num = 0\n    \n    layer_name, layers = [], []\n    for name, module in model.named_modules():\n        if isinstance(module, QuanConv2d) and module.fixed_bits is None:\n            layer_name.append(name)\n            layers.append(module)\n\n    freezed_layer_id = []\n    assert len(sorted_layer_metric) == len(layers) == len(layer_name)\n\n    for metric_id, layer_id in enumerate(sorted_layer_metric):\n        if freezed_layer_num >= int(len(sorted_layer_metric)*ratio):\n            break\n\n        module = layers[layer_id]\n        old_cands = module.weight_bit_cands.cpu()\n        cands = old_cands.tolist()\n\n        new_cands = cands\n        \n        if not ((module.weight.shape[1] == 1 and min(cands) > depth_wise_min) or \\\n        (module.weight.shape[1] > 1 and min(cands) > point_wise_min)): \n            new_cands.sort()\n            new_cands.reverse()\n            new_cands = new_cands[:-1]\n\n        module.set_bit_cands(new_cands)\n        freezed_layer_num += 1\n        freezed_layer_id.append(layer_id)\n\n        logger_info(logger=logger, msg=f\"[Freezing] layer {layer_name[layer_id]}, {metric[layer_id]}, {old_cands.tolist()} -> {module.weight_bit_cands.cpu().tolist()}\")\n       \n    for layer_id in sorted_layer_metric:\n        if layer_id not in freezed_layer_id:\n            module = layers[layer_id]\n\n            old_cands = module.weight_bit_cands.cpu().tolist()\n            new_cands = tuple(org_cands[:-1]) if module.weight.shape[1] == 1 else tuple(org_cands)\n\n            if progressive:\n                diff = len(new_cands) - len(old_cands)\n                assert diff <= 2\n                new_cands = new_cands if diff <= 1 else new_cands[:-1]\n\n            logger_info(logger=logger, msg=f\"[Activating] layer {layer_name[layer_id]}, {metric[layer_id]}, {old_cands} -> {new_cands}\")\n            \n            module.set_bit_cands(new_cands)\n\n# Information Distortion Mitigation (IDM) - Feature alignment loss setup (process.py)\nif sample_current_max:\n    start_time = time.time()\n\n    sample_max_cands(model, configs)\n\n    if information_distortion_mitigation:\n        target_features = []\n        hooks = set_forward_hook_for_quantized_layers(model, target_features, is_max=True)\n\n    max_outputs = model(inputs)\n\n    loss, QE_loss, dist_loss = compute_overall_loss(max_outputs, external_teacher_outputs, targets, criterion, model, quantization_error_minimization=False, \n                                                        configs=configs, disable_smallest_regularization=True)\n\n    loss.backward()\n\n    if information_distortion_mitigation:\n        remove_hook_for_quantized_layers(hooks)\n\n    teacher_outputs = max_outputs.clone().detach()\n    \n    # ... (inside loop for random samples)\n\n    if information_distortion_mitigation:\n        distorted_features = []\n        hooks = set_forward_hook_for_quantized_layers(model, distorted_features, is_max=False)\n\n    outputs = model(inputs)\n\n    loss, QE_loss, dist_loss = compute_overall_loss(outputs, teacher_outputs, targets, criterion, model, quantization_error_minimization=epoch>40, \n                                                        QE_loss_weight=QE_loss_weight, disable_smallest_regularization=True, configs=configs)\n\n    IDM_loss = 0\n    if information_distortion_mitigation:\n        remove_hook_for_quantized_layers(hooks)\n\n        IDM_loss = sum([F.mse_loss(s, t).sum() if s is not None else 0 for s, t in zip(distorted_features, target_features)])\n        loss += (IDM_loss * IDM_weight)\n    \n    loss.backward()\n\n# Information Distortion Mitigation (IDM) - Quantization Error Minimization (util/qat.py)\n                if quantization_error_minimization:\n                    if not is_computed_clipped_weights:\n                        lower_bound, upper_bound = w_quantizer.weight_bound(bits=current_wbits)\n                        clipped_weights = torch.clamp(weights, min=lower_bound, max=upper_bound)\n\n                    q_weights = w_quantizer(weights, bits=current_wbits).detach()\n                    bit_wise_distance = 2**(current_wbits - min(module.weight_bit_cands))\n\n                    if bit_wise_distance != 1:\n                        step_size = step_size.detach()\n                        thd_neg_min, thd_pos_min = compute_thd(w_quantizer, min(module.weight_bit_cands))\n                        bit_wise_distance_mapping = [ele*bit_wise_distance*step_size for ele in range(thd_neg_min, thd_pos_min+1)]\n\n                        idx = q_weights == bit_wise_distance_mapping[0]\n                        for cod in bit_wise_distance_mapping[1:]:\n                            idx |= (q_weights == cod)\n                        \n                        latent_weights = clipped_weights.detach()\n                        q_weights = torch.where(idx, q_weights, latent_weights)\n                    \n                    QE_loss += ((clipped_weights - q_weights) ** 2).sum(0).mean()\n                        \n# Fairness Weight Regularization (util/qat.py)\n                if fairness_regularization: # here we only force the distribution within the highly decoupled subsets...\n                    if current_wbits == 2:\n                        lower_bound, upper_bound = w_quantizer.weight_bound(bits=current_wbits)\n                        clipped_weights = torch.clamp(weights, min=lower_bound, max=upper_bound)\n                        distribution_loss += 1/2 * clipped_weights.pow(2).sum() # must using SGD for weight quantization\n\n                        is_computed_clipped_weights = True\n\n# Inference-only Bidirectional Greedy Search (main.py and util/greedy_search.py)\n# Main entry point in main.py to trigger the search.\n        elif searcher == 'bid_search':\n            reset_bit_cands(model=target_model.ema, reset=False)\n            switch_bit_width(target_model.ema,\n                            quan_scheduler=configs.quan, wbit=max_bit_width_cand-1, abit=max_bit_width_cand)\n            \n            conf = search(loader=train_loader, model=target_model.ema, criterion=criterion, metrics=('bitops', [configs.bops_limits]), epoch=0, cfgs=configs, start_bits=configs.start_bit_width,)\n            \n            acc = validate(test_loader, target_model.ema, criterion, -1, monitors,\n                        configs, train_loader=train_loader, eval_predefined_arch=conf)\n            print(conf)\n\n# Inference-only Bidirectional Greedy Search - Search Function (util/greedy_search.py)\ndef search(loader, model, criterion, metrics, cfgs, epoch=0, start_bits=6, init_w=None, init_a=None):\n    constraint_type, target_size = metrics\n    quan_scheduler = cfgs.quan\n    target_bits = cfgs.target_bits\n\n    if init_a and init_w:\n        from .qat import set_bit_width\n        set_bit_width(model, init_w, init_a)\n    assert constraint_type in ['model_size', 'bitops']\n\n    loader.sampler.set_epoch(epoch)\n    iterator = iter(loader)\n\n    data_for_bn_rest = [next(iterator) for _ in range(3)]\n\n    model.eval()\n    reset_batchnorm_stats(model)\n    \n    quantized_layers, bn = get_quantized_layers(model)\n    target_size: list\n\n    lut, lut_complexity = [], []\n    for _ in range(2):\n        for layer in quantized_layers:\n            lut.append(0)\n            \n            lut_complexity.append(0)\n\n    configs = []\n    model.eval()\n    \n    bitops, model_size = model_profiling(model)\n    start_complexity = bitops if constraint_type == 'bitops' else model_size\n\n    smallest_bit_width = 3 if max(target_size) >= 5.0 else 2\n    model.train()\n\n    def bops_map_to_bits(bops, arch='resnet18'):\n        if 'mobilenetv2' in arch:\n            \n            if 5.0 <= bops <= 5.8:\n                return 4\n            \n            if 3.3 <= bops <= 3.8:\n                return 3\n            \n            return 4\n        elif 'efficientnet' in arch:\n            if 6.3 <= bops <= 7.1:\n                return 4\n            \n            if 3.3 <= bops <= 4.5:\n                return 3\n            \n            return 3\n        elif 'resnet18' in arch:\n            if 31 <= bops <= 36:\n                return 4\n            \n            if 20 < bops <= 23.9:\n                return 3\n            \n            if bops <= 20:\n                return 2\n\n    done_w, done_a = False, False\n    w_init, a_init = False, False\n    while True:\n        input, target = next(iterator)\n        input, target = input.cuda(), target.cuda()\n\n        metric = bitops if constraint_type == 'bitops' else model_size\n\n        acc_scale = 1.5\n\n        sc = .95\n        \n        if not done_w and not w_init:\n            bits = bops_map_to_bits(max(target_size), cfgs.arch)\n\n            switch_bit_width(model, quan_scheduler, wbit=bits, abits=start_bits)\n            w_init = True\n            w_target_bitops, _ = model_profiling(model)\n            switch_bit_width(model, quan_scheduler, wbit=start_bits, abits=start_bits)\n            w_target_bitops *= sc\n\n            if init_a and init_w:\n                set_bit_width(model, init_w, init_a)\n\n        if metric <= w_target_bitops and not done_w:\n            done_w = True\n            lut = [0 for _ in range(len(quantized_layers) * 2)]\n            lut_complexity = [0 for _ in range(len(quantized_layers) * 2)]\n\n        \n        if done_w and not done_a and metric < max(target_size):\n            done_a = True\n        \n        if metric < max(target_size) :\n            configs.append((max(target_size), get_layer_wise_conf(quantized_layers, tensor='weight'), get_layer_wise_conf(quantized_layers, tensor='act')))\n            target_size.remove(max(target_size))\n            done_w, done_a, w_init = False, False, False\n        \n        if len(target_size) == 0:\n            break\n\n        for idx, layer in enumerate(quantized_layers):\n            wbits, abits = layer.bits\n\n            for mode in ['+', '-']:\n                \n                if mode == '+':\n                    overall_idx  = idx * 2 + 1\n                else:\n                    overall_idx  = idx * 2 \n                lut[overall_idx] = 0.\n                lut_complexity[overall_idx] = 0.\n\n                if mode == '-':\n                    if (wbits <= smallest_bit_width and not done_w):\n                        lut[overall_idx] = math.inf\n                        lut_complexity[overall_idx] = math.inf\n                        continue\n                else:\n                    if (wbits >= max(target_bits) and not done_w):\n                        lut[overall_idx] = math.inf\n                        lut_complexity[overall_idx] = math.inf\n                        continue\n\n                if mode == '-':\n                    if abits <= smallest_bit_width and (done_w and not done_a):\n                        lut[overall_idx] = math.inf\n                        lut_complexity[overall_idx] = math.inf\n                        continue\n                else:\n                    if abits >= max(target_bits) and (done_w and not done_a):\n                        lut[overall_idx] = math.inf\n                        lut_complexity[overall_idx] = math.inf\n                        continue\n\n                if mode == '-':\n                    if wbits > smallest_bit_width and not done_w:\n                        next_wbits_index = target_bits.index(wbits) + 1\n\n                        if wbits == min(layer.current_bit_cands_w):\n                            lut[overall_idx] = math.inf\n                            lut_complexity[overall_idx] = math.inf\n                            continue\n\n                        next_wbits = target_bits[next_wbits_index]\n                        adjust_one_layer_bit_width(layer, bn[idx], next_wbits, reduce=False, tensor='weight')\n                    \n                    if abits > smallest_bit_width and (done_w and not done_a):\n                        next_abits_index = target_bits.index(abits) + 1\n\n                        if abits == min(layer.current_bit_cands_a):\n                            lut[overall_idx] = math.inf\n                            lut_complexity[overall_idx] = math.inf\n                            continue\n\n                        next_abits = target_bits[next_abits_index]\n                        adjust_one_layer_bit_width(layer, bn[idx], next_abits, reduce=False, tensor='act')\n                else:\n                    if wbits < max(target_bits) and not done_w:\n                        next_wbits_index = target_bits.index(wbits) - 1\n\n                        if wbits == max(target_bits):\n                            lut[overall_idx] = math.inf\n                            lut_complexity[overall_idx] = math.inf\n                            continue\n\n                        next_wbits = target_bits[next_wbits_index]\n                        adjust_one_layer_bit_width(layer, bn[idx], next_wbits, reduce=False, tensor='weight')\n                    \n                    if abits < max(target_bits) and (done_w and not done_a):\n                        next_abits_index = target_bits.index(abits) - 1\n\n                        if abits == max(target_bits):\n                            lut[overall_idx] = math.inf\n                            lut_complexity[overall_idx] = math.inf\n                            continue\n\n                        next_abits = target_bits[next_abits_index]\n                        adjust_one_layer_bit_width(layer, bn[idx], next_abits, reduce=False, tensor='act')\n            \n                with torch.no_grad():\n                    _, top1_error = forward_loss(model, criterion, input, target, None, return_acc=True, eval_mode=False)\n                lut[overall_idx] = top1_error\n\n                tmp_bitops, tmp_model_size = model_profiling(model)\n                comp = tmp_bitops if constraint_type == 'bitops' else tmp_model_size\n                lut_complexity[overall_idx] += -comp\n\n                if mode == '-':\n                    if wbits > smallest_bit_width and not done_w:\n                        adjust_one_layer_bit_width(layer, bn[idx], wbits, reduce=True, tensor='weight')\n\n                    if abits > smallest_bit_width and (done_w and not done_a):\n                        adjust_one_layer_bit_width(layer, bn[idx], abits, reduce=True, tensor='act')\n                else:\n                    if wbits >= smallest_bit_width and not done_w:\n                        adjust_one_layer_bit_width(layer, bn[idx], wbits, reduce=True, tensor='weight')\n\n                    if abits >= smallest_bit_width and (done_w and not done_a):\n                        adjust_one_layer_bit_width(layer, bn[idx], abits, reduce=True, tensor='act')\n    \n        tmp_lut = []\n        max_acc, max_comp = 0, 0\n        min_acc, min_comp = 0, 0\n        for acc, comp in zip(lut, lut_complexity):\n            max_acc = acc if (acc > max_acc and acc is not math.inf) else max_acc\n            min_acc = acc if (acc < min_acc and acc is not math.inf) else min_acc\n\n            max_comp = comp if (comp > max_comp and comp is not math.inf) else max_comp\n            min_comp = comp if (comp < min_comp and comp is not math.inf) else min_comp\n        \n        for acc, comp in zip(lut, lut_complexity):\n            if acc == math.inf:\n                tmp_lut.append(math.inf)\n                continue\n\n            tmp_lut.append(acc_scale*((acc-min_acc)/(max_acc-min_comp)) - (comp-min_comp)/(max_comp-min_comp))\n\n        best_idx = tmp_lut.index(min(tmp_lut))\n        \n        assert best_idx is not math.inf\n\n        if not done_w:\n            best_layer_index_w = best_idx // 2\n            best_layer_wbits = quantized_layers[best_layer_index_w].bits[0]\n            offset = 1 if best_idx % 2 == 0 else -1\n\n            if best_layer_wbits > smallest_bit_width:\n                \n                next_w_bit_index = target_bits.index(best_layer_wbits) + offset\n                next_w_bit = target_bits[next_w_bit_index]\n                wnew_bit_width = adjust_one_layer_bit_width(quantized_layers[best_layer_index_w], bn[best_layer_index_w], next_w_bit, reduce=False, tensor='weight')\n            else:\n                pass\n        \n        if done_w and not done_a:\n            best_layer_index_a = best_idx // 2\n            best_layer_abits = quantized_layers[best_layer_index_a].bits[1]\n            offset = 1 if best_idx % 2 == 0 else -1\n\n            if best_layer_abits > smallest_bit_width:\n                next_a_bit_index = target_bits.index(best_layer_abits) + offset\n                next_a_bit = target_bits[next_a_bit_index]\n                anew_bit_width = adjust_one_layer_bit_width(quantized_layers[best_layer_index_a], bn[best_layer_index_a], next_a_bit, reduce=False, tensor='act')\n            else:\n                pass\n        \n        epoch += 1\n        \n        bitops, model_size = model_profiling(model)\n\n    return configs\n\n# Inference-only Bidirectional Greedy Search - Adjust one layer's bit-width (util/greedy_search.py)\ndef adjust_one_layer_bit_width(layer, bn, next_bits:int, reduce: bool, tensor: str):\n    if not reduce:\n        if tensor == 'weight':\n            layer.bits = (next_bits, layer.bits[1])\n            bn.switch_bn(layer.bits)\n            return layer.bits[0]\n        else:\n            layer.bits = (layer.bits[0], next_bits)\n            bn.switch_bn(layer.bits)\n            return layer.bits[1]\n    else:\n        if tensor == 'weight':\n            layer.bits = (next_bits, layer.bits[1])\n            bn.switch_bn(layer.bits)\n            return layer.bits[0]\n        else:\n            layer.bits = (layer.bits[0], next_bits)\n            bn.switch_bn(layer.bits)\n            return layer.bits[1]",
        "experimental_info": "Training Loop Configuration:\n  epochs: configs.epochs\n  batch_size: configs.dataloader.batch_size\n  learning_rate (main optimizer): configs.lr\n  learning_rate (quantizer optimizer): configs.q_lr (default 1e-5)\n  momentum: configs.momentum\n  weight_decay: configs.weight_decay\n  label_smoothing: configs.smoothing\n  model_ema_decay: configs.ema_decay\n  target_bits (bit-width candidates): configs.target_bits (sorted in reverse in process.py)\n  quantization_scheduler_exceptions: configs.quan.excepts\n  split_aw_cands: configs.split_aw_cands (default False in config.py)\n\nDynamic Bit-width Scheduler Settings:\n  enable_dynamic_bit_training: configs.enable_dynamic_bit_training (default True in config.py)\n  freezing_frequency (T): 2 epochs for first 72% of training, then 15 epochs (T = 2 if epoch <= int(configs.epochs * 0.72) else 15 in process.py)\n  freezing_annealing_schedule (CosineSched parameters):\n    start_step: 0\n    max_step: configs.epochs // 2\n    eta_start: 0.5\n    eta_end: 0.2\n  unstable_weight_criterion_proportion (proportion in profile_layerwise_quantization_metric): 0.25\n  freeze_layers_ratio (ratio in freeze_layers): 0.25 (Top-K selector)\n  freeze_layers_point_wise_min_bit_width: 2\n  freeze_layers_depth_wise_min_bit_width: 3\n\nInformation Distortion Mitigation (IDM) Settings:\n  information_distortion_mitigation: configs.information_distortion_mitigation (boolean, default False)\n  IDM_weight: 0.01 (parameter in train function, process.py)\n  quantization_error_minimization_enabled_after_epoch: 40 (epoch>40 in compute_overall_loss)\n  QE_loss_weight_scheduler (annealing_schedule, CosineSched parameters):\n    start_step: len(train_loader) * 40\n    max_step: len(train_loader) * configs.epochs\n    eta_start: 0\n    eta_end: 0.1\n\nFairness Weight Regularization Settings:\n  disable_smallest_regularization: True (argument to compute_overall_loss)\n  smallest_bit_width_for_regularization: 2 (if current_wbits == 2 in auxiliary_quantized_loss)\n  adaptive_region_weight_decay: getattr(configs, 'adaptive_region_weight_decay', configs.weight_decay)\n\nInference-Only Bidirectional Greedy Search Settings:\n  search_enabled: configs.search (boolean)\n  search_method: 'bid_search' (selected in main.py)\n  metrics_for_search: ('bitops', [configs.bops_limits]) (loss and BitOPs trade-off, target BitOPs constraint)\n  start_bit_width: configs.start_bit_width\n  smallest_bit_width_for_search_reduction: 3 if max(target_size) >= 5.0 else 2 (in util/greedy_search.py)\n  accuracy_complexity_tradeoff_scale (acc_scale in greedy_search.py): 1.5\n  weight_target_bitops_scale (sc in greedy_search.py): 0.95\n  bops_map_to_bits_logic: Defines initial bit-width for weights based on target BitOPs (e.g., for mobilenetv2, efficientnet, resnet18 architectures, details in util/greedy_search.py)."
      }
    },
    {
      "title": "Mixed Precision DNNs: All you need is a good parametrization",
      "abstract": "Efficient deep neural network (DNN) inference on mobile or embedded devices\ntypically involves quantization of the network parameters and activations. In\nparticular, mixed precision networks achieve better performance than networks\nwith homogeneous bitwidth for the same size constraint. Since choosing the\noptimal bitwidths is not straight forward, training methods, which can learn\nthem, are desirable. Differentiable quantization with straight-through\ngradients allows to learn the quantizer's parameters using gradient methods. We\nshow that a suited parametrization of the quantizer is the key to achieve a\nstable training and a good final performance. Specifically, we propose to\nparametrize the quantizer with the step size and dynamic range. The bitwidth\ncan then be inferred from them. Other parametrizations, which explicitly use\nthe bitwidth, consistently perform worse. We confirm our findings with\nexperiments on CIFAR-10 and ImageNet and we obtain mixed precision DNNs with\nlearned quantization parameters, achieving state-of-the-art performance.",
      "full_text": "Published as a conference paper at ICLR 2020 MIXED PRECISION DNN S: ALL YOU NEED IS A GOOD PARAMETRIZATION Stefan Uhlich∗, Lukas Mauch∗, Fabien Cardinaux∗, Kazuki Yoshiyama Javier Alonso García, Stephen Tiedemann, Thomas Kemp Sony Europe B.V ., Germany firstname.lastname@sony.com Akira Nakamura Sony Corporate, Japan akira.b.nakamura@sony.com ABSTRACT Efﬁcient deep neural network (DNN) inference on mobile or embedded devices typically involves quantization of the network parameters and activations. In particular, mixed precision networks achieve better performance than networks with homogeneous bitwidth for the same size constraint. Since choosing the optimal bitwidths is not straight forward, training methods, which can learn them, are desirable. Differentiable quantization with straight-through gradients allows to learn the quantizer’s parameters using gradient methods. We show that a suited parametrization of the quantizer is the key to achieve a stable training and a good ﬁnal performance. Speciﬁcally, we propose to parametrize the quantizer with the step size and dynamic range. The bitwidth can then be inferred from them. Other parametrizations, which explicitly use the bitwidth, consistently perform worse. We conﬁrm our ﬁndings with experiments on CIFAR-10 and ImageNet and we obtain mixed precision DNNs with learned quantization parameters, achieving state-of-the-art performance. 1 I NTRODUCTION Quantized DNNs apply quantizers Q : R → {q1, ..., qI} to discretize the weights and/or activations of a DNN (Han et al., 2015; Zhou et al., 2017; Li et al., 2016; Liu & Mattina, 2019; Cardinaux et al., 2018; Jain et al., 2019; Bai et al., 2018). They require considerably less memory and have a lower computational complexity, since discretized values {q1, ..., qI} can be stored, multiplied and accumulated efﬁciently. This is particularly relevant for inference on mobile or embedded devices with limited computational power. However, gradient based training of quantized DNNs is difﬁcult, as the gradient of a quantization function vanishes almost everywhere, i.e., backpropagation through a quantized DNN almost always returns a zero gradient. Different solutions to this problem have been proposed in the literature: A ﬁrst possibility is to use DNNs with stochastic weights from a categorical distribution and to optimize the evidence lower bound(ELBO) to obtain an estimate of the posterior distribution of the weights. As proposed in (Jang et al., 2016; Maddison et al., 2016; Louizos et al., 2019), the categorical distribution can be relaxed to a concrete distribution – a smoothed approximation of the categorical distribution – such that the ELBO becomes differentiable under reparametrization. A second possibility is to use the straight through estimator(STE) (Bengio et al., 2013). STE allows the gradients to be backpropagated through the quantizers and, thus, the network weights can be adapted with standard gradient descent (Hubara et al., 2016). Compared to STE based methods, stochastic methods suffer from large gradient variance, which makes training of large quantized DNNs difﬁcult. Therefore, STE based methods are more popular in practice. More recent research (Jain et al., 2019; Esser et al., 2019; Wang et al., 2018; Elthakeb et al., 2018) focuses on methods which can also learn the optimal quantization parameters, e.g., the stepsize, dynamic range and bitwidth, in parallel to the network weights. This is a promising approach as DNNs with learned quantization parameters almost always outperform DNNs with handcrafted ones. ∗Equal contribution. The source code can be found at https://github.com/sony/ai-research-code. 1Published as a conference paper at ICLR 2020 Recently, and in parallel to our work, (Jain et al., 2019) explored the use of STE to deﬁne the gradient with respect to the quantizers’s dynamic range. The authors applied a per-tensor quantization and used the dynamic range as an additional trainable parameter also learned with gradient descent. Similarly, (Esser et al., 2019) learned the stepsize using gradient descent. However, neither of them learned the optimal bitwidth of the quantizers. One approach was proposed in (Wang et al., 2018; Elthakeb et al., 2018). They learn the bitwidth with reinforcement learning, i.e., they learn an optimal bitwidth assignment policy. Their experiments show that a DNN with a learned and heterogeneous bitwidth assignment outperforms quantized DNNs with a homogeneous bitwidth assignment. However, such methods have a high computational complexity as the bitwidth policy must be learned, which involves training many quantized DNNs. In this paper, we will use the STE approach and show that the quantizer’s parameters, including the bitwidth, can be learned with gradient methods if a good parametrization is chosen. Speciﬁcally, we show that directly learning the bitwidth is not optimal. Instead, we propose to learn the stepsize and dynamic range. The bitwidth can then be inferred from them. Compared to (Wang et al., 2018; Elthakeb et al., 2018), our method has the advantage that training quantized DNNs has nearly the same computational complexity as standard ﬂoat32 training. The contributions of this paper are: 1. We show that there are three different parametrizations for uniform and power-of-two quantiza- tion and that, in both cases, one of them has gradients particularly well suited to train quantized DNNs. The other parametrizations have the problem of yielding gradients with an unbounded gradient norm and coupled components. 2. Using this parametrization, we are able to learn all quantization parameters for DNNs with per-tensor quantization and global memory constraints. We formulate the training as a constrained optimization problem, where the quantized DNN is constrained not to exceed a given overall memory budget, and show how to solve it in a penalty framework. 3. We conﬁrm our ﬁndings with experiments on CIFAR-10 and ImageNet. For example, we train a heterogeneously quantized MobileNetV2 on ImageNet requiring a total of only 1.65MB to store the weights and only 0.57MB to store its largest feature map. This is equivalent to a homogenous 4bit quantization of both weights and activations. However, our network learns to allocate the bitwidth het- erogeneously in an optimal way. Our MobileNetV2 achieves an error of 30.26% compared to 29.82% for the ﬂoating point baseline. This is state-of-the-art for such a heavily quantized MobileNetV2. We use the following notation throughout this paper: x, x, X and X denote a scalar, a (column) vector, a matrix and a tensor with three or four dimensions, respectively; ⌊.⌋ and ⌈.⌉ are the ﬂoor and ceiling operators. Finally, δ(.) denotes the Dirac delta function. 2 C HOOSING A QUANTIZATION PARAMETRIZATION Let Q(x; θ) be a quantizer with the parameters θ, which maps x ∈ R to discrete values {q1, ..., qI}. In this section, we compare different parametrizations of Q(x; θ) for uniform quantizationand power- of-two quantizationand analyze how well the corresponding straight-through gradient estimates ∂xQ(x; θ) and ∇θQ(x; θ) are suited to optimize the quantizer parameters θ. Our key result is, that the training of quantized DNNs which learns both, the optimal quantized weights and the optimal quantization parameters θ, is very sensitive to the choice of the parametrization of the quantizers. From an optimization point of view, it is best to parametrize the quantizer Q(x; θ) with the stepsize d and the dynamic range qmax as it leads to gradients with stable norms. Doing so, we can use standard gradient descent to learn the quantization parameters and do not need to use stochastic or reinforcement based algorithms, which are computationally expensive. 2.1 P ARAMETRIZATION AND STRAIGHT THROUGH GRADIENT ESTIMATES A symmetric uniform quantizer QU (x; θ) which maps a real value x ∈ R to one of I = 2k + 1 quantized values q ∈ {−kd, ..., 0, ..., kd} computes q = QU (x; θ) =sign(x) { d ⌊ |x| d + 1 2 ⌋ |x| ≤ qmax qmax |x| > qmax , (1) 2Published as a conference paper at ICLR 2020 2 4 6 8 0 5 0 500 1,000 bd (a) Case U1: θ = [b, d]T 2 4 0 10 0 2 b qmax (b) Case U2: θ = [b, qmax]T 0 10 0 5 0 2 dqmax (c) Case U3: θ = [d, qmax]T Figure 1: Maximum gradient norm maxx∥∇θQU (x; θ)∥. For “U1” and “U2” the maximum gradient norm can grow exponentially with varying bitwidth b whereas it is bounded for “U3”. where the parameter vector θ = [d, qmax, b]T consists of the step size d ∈ R, the maximum value qmax ∈ R and the number of bits b ∈ N, b ≥ 2 used to encode the quantized values q. When training quantized DNNs, we want to optimize QU (x; θ) with respect to the input x and the quantization parameters θ, meaning that we need the gradients ∇xQU (x; θ) and ∇θQ(x; θ). A common problem is, that the exact gradients are not useful for training. For example, ∂xQU (x; θ) = d ∑2b−1−2 k=−2b−1+1 δ ( x − d ( k + 1 2 )) vanishes almost everywhere. A solution is to deﬁne the derivative using STE (Bengio et al., 2013), which ignores the ﬂoor operation in (1). This leads to ∂xQU (x) = {1 |x| ≤ qmax 0 |x| > qmax , (2) which is non-zero in the interesting region |x| ≤ qmax and which turned out to be very useful to train quantized DNNs in practice (Yin et al., 2019). In this work, we follow this idea and deﬁne the gradients ∇xQ(x; θ) and ∇θQ(x; θ), using STE whenever we need to differentiate a ﬂoor operation. We refer to this as differentiable quantization(DQ). An important observation from (1) is that the parameters θ = [d, qmax, b]T of a quantizer depend on each other, i.e., qmax = (2b−1 − 1)d. This means, that we can choose from three equivalent parametrizations of QU (x; θ): Case “U1” with θ = [b, d]T , case “U2” with θ = [b, qmax]T and case “U3” with θ = [d, qmax]T . Interestingly, they differ in their gradients: Case U1:Parametrization with respect to θ = [b, d]T , using qmax = qmax(b, d) gives ∇θQU (x; θ) = [ ∂bQU (x; θ) ∂dQU (x; θ) ] =    [0 1 d ] (QU (x; θ) − x) |x| ≤ (2b−1 − 1)d [2b−1 log(2)d 2b−1 − 1 ] sign(x) |x| > (2b−1 − 1)d (3a) Case U2:Parametrization with respect to θ = [b, qmax]T , using d = d(b, qmax) gives ∇θQU (x; θ) = [ ∂bQU (x; θ) ∂qmax QU (x; θ) ] =    [ −2b−1 log 2 2b−1−1 1 qmax ] (QU (x; θ) − x) |x| ≤ qmax [ 0 sign(x) ] |x| > qmax (3b) Case U3:Parametrization with respect to θ = [d, qmax]T , using b = b(d, qmax) gives ∇θQU (x; θ) = [ ∂dQU (x; θ) ∂qmax QU (x; θ) ] =    [1 d 0 ] (QU (x; θ) − x) |x| ≤ qmax [ 0 sign(x) ] |x| > qmax (3c) Fig. 1 shows the maximum gradient norm maxx∥∇θQU (x; θ)∥ for the three parametrizations “U1” to “U3”. For the parametrizations “U1” and “U2”, maxx∥∇θQU (x; θ)∥ can grow exponentially with varying bitwidth b as ∂dQU (x; θ) ∈ [−2b−1 − 1, 2b−1 − 1] for “U1” and ∂bQU (x; θ) ∈[ − qmax 2b−1−1 log 2, qmax 2b−1−1 log 2 ] for “U2”. This is not desirable when training quantized DNNs, because it will lead to large changes of the gradient norm and forces us to use small learning rates to 3Published as a conference paper at ICLR 2020 qmax 2 b− 1 − 1 x ∂bQU(x) ∂d QU(x) (a) Case U1 qmax 1 x ∂bQU(x) ∂qmax QU(x) (b) Case U2 qmax 1 x ∂dQU(x) ∂qmax QU(x) (c) Case U3 qmax 1 x ∂xQU(x) (d) Input derivative Figure 2: Partial derivatives of QU (x; θ) with respect to the input and the quantization parameters d, qmax and b. Partial derivatives are coupled for “U1” and “U2” but are decoupled for “U3”. avoid divergence. However, parametrization “U3” does not suffer from such an unbounded gradient norm as both partial derivatives ∂dQU (x; θ) ∈ [−1 2 , 1 2 ] and ∂qmax QU (x; θ) ∈ {−1, 1} are bounded. Fig. 2 shows the gradients for the parametrization “U1” to “U3”. For parametrization “U3”, the partial derivatives in ∇θQU (x; θ) are decoupled, i.e., ∇θQU (x; θ) is a unit vector, which either points only in the direction of d if |x| ≤ qmax or only in the direction of qmax, if |x| > qmax. We will show in Sec. 2.3 that this implies a diagonal Hessian, which results in a better convergence behavior of gradient descent. In contrast, both parametrizations “U1” and “U2” have partial derivatives that are coupled. In summary, this implies that parametrization “U3” is the best DQ parametrization. Similar considerations can be made for the power-of-two quantization QP (x; θ), which maps a real-valued number x ∈ R to a quantized value q ∈ {±2k : k ∈ Z} by q = QP (x; θ) =sign(x)    qmin |x| ≤ qmin 2⌊0.5+log2|x|⌋ qmin < |x| ≤ qmax qmax |x| > qmax , (4) where qmin and qmax are the minimum and maximum absolute values of the quantizer for a bitwidth of b bit. Power-of-two quantization is an especially interesting scheme for DNN quantization, since a multiplication of quantized values can be implemented as an addition of the exponents. Using the STE for the ﬂoor operation, the derivative ∂xQP (x; θ) is given by ∂xQP (x) =    0 |x| ≤ qmin 2⌊0.5+log2|x|⌋ |x| qmin < |x| ≤ qmax 0 |x| > qmax . (5) The power-of-two quantization has the three parameters [b, qmin, qmax] =: θ, which depend on each other with the relationship qmax = 22b−1−1qmin. Therefore, we have again three different parametrizations with θ = [b, qmin], θ = [b, qmax] or θ = [qmin, qmax], respectively. Similarly to the uniform case, one parametrization ( θ = [qmin, qmax]) leads to a gradient of a very simple form ∇θQP (x; θ) = [ ∂qmin QU (x; θ) ∂qmax QU (x; θ) ] =    [1, 0]T |x| ≤ qmin [0, 0]T qmin < |x| ≤ qmax [0, 1]T |x| > qmax , (6) which has again a bounded gradient magnitude and independent components and is, hence, best suited for ﬁrst order gradient based optimization. 2.2 C ONSTRAINTS ON θ In practice, for an efﬁcient hardware implementation, we need to ensure that the quantization parameters only take speciﬁc discrete values: for uniform quantization, only integer values are allowed for the bitwidth b, and the stepsize d must be a power-of-two, see e.g. (Jain et al., 2019); for power-of-two quantization, the bitwidth must be an integer, and the minimum and maximum absolute values qmin and qmax must be powers-of-two. We fulﬁll these constraints by rounding the parameters in the forward pass to the closest integer or power-of-two value. In the backward pass we update the original ﬂoat values, i.e., we used again the STE to propagate the gradients. 2.3 E XPERIMENTAL COMPARISON OF DQ PARAMETRIZATIONS In the following we compare the parametrizations using two experiments. 4Published as a conference paper at ICLR 2020 0 500 1,000 1,500 2,000 10−3 10−1 Iteration Mean squared error b, d (U1) b, qmax (U2) d, qmax (U3) (a) Uniform quantization 0 500 1,000 1,500 2,000 10−1 10−0.5 Iteration Mean squared error b, qmax (P1) b, qmin (P2) qmin, qmax (P3) (b) Power-of-two quantization Figure 3: MSE for quantizing Gaussian data x ∼ N(0, 1) with uniform and power-of-two quantiza- tion. Parametrizations “U3” and “P3” converge to the lowest MSE without any oscillations. 1) Quantization of Gaussian data In our ﬁrst experiment we use DQ to learn the optimal quantiza- tion parameters θ∗ which minimize the mean squared error(MSE) E [1 2 (Q(x; θ) − x)2] with gradient descent and compare the convergence speed for three possible parametrizations of a uniform and power-of-two quantizer. We choose this example as the gradient ∇θQ(x; θ) = E [(Q(x; θ) − x)∇θQ(x; θ)] is just a scaled version of ∇θQ(x; θ), i.e., the gradient direction de- pends directly on the parametrization of Q(x; θ) and thus the effects of changing the parametrization can be observed. It is interesting to study the Hessian H = ∇θ∇T θ E [ (Q(x; θ) − x)2] ∈ R2×2 of the MSE: H = E [ ∇θQ(x; θ)∇θQ(x; θ)T + (Q(x; θ) − x)∇θ∇T θ Q(x; θ) ] ≈ E [ ∇θQ(x; θ)∇θQ(x; θ)T ] . Note that we use the outer-product approximation (Bishop, 2006) in order to simplify our con- siderations. From this equation it is apparent that the Hessian will be diagonal for the case U3 as ∇θQ(x; θ)∇θQ(x; θ)T only contains an element in either (1, 1) or (2, 2) and, therefore, E [ ∇θQ(x; θ)∇θQ(x; θ)T ] is a diagonal matrix. From this, we can see that gradient descent with an individual learning rate for each parameter is equivalent to Newton’s method and, therefore, efﬁcient. In general this will not be the case for U1 and U2. We conduct an experiment, using ADAM to optimize the mean squared quantization error on artiﬁcially generated data, which is generated by drawing 104 samples from N(0, 1). Please note that the same example was studied in (Jain et al., 2019). The results in Fig. 3 clearly show that the parametrizations “U3” and “P3” are best suited to optimize the uniform and power-of-two quantization parameters, respectively. Indeed, these quantizers converge without oscillation to the lowest MSE. It is interesting to see, that even adaptive gradient methods like ADAM can not solve the scaling issue described in Sec. 2.1. In the Appendix A.4 we give further empirical evidence to support this claim and compare the different parametrizations for the training of a quantized ResNet-20 on CIFAR-10 using ADAM. Note that all cases use the same learning rate. For the interested reader, a more detailed visualization of the error surfaces over the quantization parameters can be found in Appendix A.3. 2) CIFAR-10 In our second experiment we train a ResNet-20 (He et al., 2016) with quantized parameters and activations on CIFAR-10 (Krizhevsky & Hinton, 2009) using the same settings as proposed by (He et al., 2016). Fig. 4 shows the evolution of the training and validation error during training for the case of uniform quantization. The plots for power-of-two quantization can be found in the appendix (Fig. 10). We initialize this network from random parameters or from a pre-trained ﬂoat network. The quantized DNNs are trained for 160 epochs, using SGD with momentum 0.9 and a learning rate schedule starting with 0.01 and reducing it by a factor of 10 after 80 and 120 epochs, respectively. We use random ﬂips and crops for data augmentation. Each epoch takes about 2.5 min on a single GTX 1080 Ti. In case of randomly initialized weights, we use an initial stepsize dl = 2−3 for the quantization of weights and activations. Otherwise, we initialize the weights using a pre-trained ﬂoating point network and the initial stepsize for a layer is chosen to be dl = 2⌊log2(max |Wl|/(2b−1−1))⌋. The remaining quantization parameters are chosen such that we start from an initial bitwidth of b = 4 bit. This is a reasonable upper limit for b, as in practice no performance degradation can be observed for b > 4bit. Even simple ofﬂine algorithms like min/max quantization result in networks with good accuracies. We deﬁne no memory constraints during training, i.e., the network can learn to use a large number of bits to quantize weights and activations of each layer. From Fig. 4, we again observe that the parametrization θ = [d, qmax]T is best suited to train a uniformly quantized DNN as it converges 5Published as a conference paper at ICLR 2020 Table 1: Comparison of different DQ parametrizations for ResNet-20 on CIFAR-10. (validation error with “random”/“ﬂoat net” initialization) Quantization Float32 Uniform quantization Power-of-two quantization θ = [b, d]T θ = [b, qmax]T θ = [d, qmax]T θ = [b, qmax]T θ = [b, qmin]T θ = [qmin, qmax]T Weights 8.50%/7.29% 17.8%/8.18% 8.80%/7.44% 8.50%/7.32% 11.70%/7.90% 53.07%/23.01% 10.61%/7.56% Weights+Activations 28.9%/9.03% 9.43%/7.74% 9.23%/7.40% 22.91%/11.68% diverging/35.68% 15.10%/9.86% 0 2 4 6 ·104 10−2 10−1 100 Iteration Training error (a) Random initialization 0 2 4 6 ·104 10−1 100 Iteration Validation error b, d (U1) b, qmax (U2) d, qmax (U3) 0 2 4 6 ·104 10−2 10−1 100 Iteration Training error (b) Pre-trained initialization 0 2 4 6 ·104 10−1 100 Iteration Validation error Figure 4: ResNet-20 with uniformly quantized weights and activations. to the best local optimum. Furthermore, we observe the smallest oscillation of the validation error for this parametrization. Table 1 compares the best validation error for all parametrizations of the uniform and power-of-two quantizations. We trained networks either with quantized weights and full precision activations or with both being quantized. In case of activation quantization with power-of-two, we use one bit to explicitly represent the value x = 0. This is advantageous as the ReLU nonlinearity will map many activations to this value. We can observe that training the quantized DNN with the optimal parametrization of DQ, i.e., using either θ = [d, qmax]T or θ = [qmin, qmax]T results in a network with the lowest validation error. This result again supports our theoretical considerations from Sec. 2.1. 3 T RAINING QUANTIZED DNN S WITH MEMORY CONSTRAINTS We now discuss how to train quantized DNNs with memory constraints. Such constraints appear in many applications when the network inference is performed on an embedded device with limited computational power and memory resources. A quantized DNN consists of layers which compute Xl = fl(Q(Wl; θw l ) ∗ Q(Xl−1; θx l−1) +Q(cl; θw l )) with l = 1, ..., L, (7) where fl(·) denotes the nonlinear activation function of layer l and Q(·; θ) is a per-tensor quantization with parameters θ applied separately to the input and output tensors Xl−1 ∈ Il and Xl ∈ Il, and also to both the weight tensors Wl ∈ P l and the bias vector cl ∈ RMl.1 For a fully connected layer, Il−1 = RMl−1 , Il = RMl are vectors, Pl = RMl×Ml−1 are matrices and A ∗ B is a matrix- vector product. In case of a convolutional layer, Il−1 = RMl−1×Nl−1×Nl−1 , Il = RMl×Nl×Nl, Pl = RMl×Ml−1×Kl×Kl are tensors and A ∗ B is a set of Ml−1Ml 2D convolutions, where the convolution is performed on square-sized feature maps of size Nl−1 × Nl−1 using square-sized kernels of size Kl × Kl. DNNs with quantized weights and activations have a smaller memory footprint and are also com- putationally cheaper to evaluate since Q(α; θ) · Q(β; θ) for α, β ∈ R requires only an integer multiplication for the case of uniform quantization or an integer addition of the exponents for power- of-two quantization. Furthermore, Q(α; θ) +Q(β; θ) for α, β ∈ R only requires an integer addition. Table 2 compares the computational complexity and the memory footprint of layers which apply uniform or power-of-two quantization to weights and activations. We consider the following memory characteristics of the DNN, constraining them during training: 1. Total memorySw(θw 1 , ..., θw L ) =∑L l=1 Sw l (θw l ) to store all weights:We use the constraint g1(θw 1 , ..., θw L ) =Sw(θw 1 , ..., θw L ) − Sw 0 = ∑L l=1 Sw l (θw l ) − Sw 0 ≤ 0, (8a) 1In this paper, we use “weights” to refer to W and c. 6Published as a conference paper at ICLR 2020 Table 2: Number of multiplications Cmul l , additions Cadd l as well as required memory to store the weights Sw l and activations Sx l of fully connected and convolutional layers. Layer Quantization Cmul l Cadd l Sw l Sx l Fully connected uniform MlMl−1 MlMl−1 Ml(Ml−1 + 1)bw l Mlbx lpow-2 0 2MlMl−1 Convolutional uniform MlMl−1N2 l K2 l MlMl−1N2 l K2 l Ml(Ml−1K2 l + 1)bw l MlN2 l bx lpow-2 0 2MlMl−1N2 l K2 l to ensure that the total weight memory requirement Sw(θw 1 , ..., θw L ) is smaller than a certain maximum weight memory size Sw 0 . Table 2 gives Sw l (θw l ) for the case of fully connected and convolutional layers. Each layer’s memory requirement Sw l (θw l ) depends on the bitwidth bw l : reducing Sw l (θw l ) will reduce the bitwidth bw l . 2. Total activation memorySx(θx 1 , ..., θx L) =∑L l=1 Sx l (θx l ) to store all feature maps:We use the constraint g2(θx 1 , ..., θx L) =Sx(θx 1 , ..., θx L) − Sx 0 = ∑L l=1 Sx l (θx l ) − Sx 0 ≤ 0, (8b) to ensure an upper limit on the total activation memory size Sx 0 . Table 2 gives Sx l (θx l ) for the case of fully connected and convolutional layers. Such a constraint is important if we use pipelining for accelerated inference, i.e., if we evaluate multiple layers with several consecutive inputs in parallel. This can, e.g., be the case for FPGA implementations (Guo et al., 2017). 3. Maximum activation memoryˆSx(θx 1 , ..., θx L) = maxl=1,...,L Sx l to store the largest feature map: We use the constraint g3(θx 1 , ..., θx L) = ˆSx(θx 1 , ..., θx L) − ˆSx 0 = max l=1,...,L (Sx l ) − ˆSx 0 ≤ 0, (8c) to ensure that the maximum activation size ˆSx does not exceed a given limit ˆSx 0 . This constraint is relevant for DNN implementations where layers are processed sequentially. To train the quantized DNN with memory constraints, we need to solve the optimization problem min Wl,cl,θw l ,θx l Ep(X,Y) [J(XL, Y)] s .t. g j(θw 1 , ..., θw L , θx 1 , ..., θx L) ≤ 0 for all j = 1, ..., 3 (9) where J(XL, Y) is the loss function for yielding the DNN output XL although the ground truth is Y. Eq. (9) learns the weights Wl, cl as well as the quantization parameters θx l , θw l . In order to use simple stochastic gradient descent solvers, we use the penalty method (Bertsekas, 2014) to convert (9) into the unconstrained optimization problem min Wl,cl,θw l ,θx l Ep(X,Y) [J(XL, Y)] + ∑J j=1 λj max(0, gj(θw 1 , ..., θw L , θx 1 , ..., θx L))2, (10) where λj ∈ R+ are individual weightings for the penalty terms. Hence, training with weight and activation size constraints requires choosing two penalty weightings λj, one for (8a) and one for either (8b) or (8c). Note, that the optimization problem (10) does not necessarily give a quantized DNN which fulﬁlls the memory constraints. The probability to fulﬁll the constraint gj depends on the choice of λj. In particular, this probability increases with larger λj. However, choosing a too large λj will yield a penalty term that dominates over the network loss decreasing the network performance. In our experiments, we choose λj such that the initial loss and the penalty term have approximately the same magnitude. Using this simple heuristic, we optained quantized DNNs that reached a high accuracy and at the same time fulﬁlled the constraints at the end of training. 4 E XPERIMENTS In the following, we will use the best parametrizations for uniform and power-of-two DQ, i.e., θU = [d, qmax]T and θP = [qmin, qmax]T , that we found in Sec. 2. Both parametrizations do not directly depend on the bitwidth b. Therefore, we compute it by using b(θU ) = ⌈ log2 (qmax d + 1 ) + 1 ⌉ and b(θP ) = ⌈ log2 ( log2 ( qmax qmin ) + 1 ) + 1 ⌉ . All quantized networks use a pre-trained ﬂoat32 network for initialization and all quantizers are initialized as described in Sec. 2.3. For our experiments on CIFAR-10, we use the same training setup as described in Sec. 2.3. For the experiments on 7Published as a conference paper at ICLR 2020 Table 3: Homogeneous vs. heterogeneous quantization of ResNet-20 on CIFAR-10. Bitwidth qmax Size Uniform quant. Power-of-two quant. Weight/Activ. Weight/Activ. Weight/Activ.(max)/Activ.(sum) Validation error Validation error Baseline 32bit/32bit – 1048KB/64KB/736KB 7.29% Fixed 2bit/32bit ﬁxed/– 65.5KB/64KB/736KB 10.81% 8.99% TQT (Jain et al., 2019) 2bit/32bit learned/ – 65.5KB/64KB/736KB 9.47% 8.79% Ours (w/ constr. (8a)) learned/32bit learned/- 70KB/64KB/736KB 8.59% 8.53% Fixed 2bit/4bit ﬁxed/ﬁxed 65.5KB/8KB/92KB 11.30% 11.62% TQT (Jain et al., 2019) 2bit/4bit learned/learned 65.5KB/8KB/92KB 9.62% 11.29% Ours (w/ constr. (8a) and (8b)) learned/learned learned/learned 70KB/ – /92KB 9.38% 11.29% Ours (w/ constr. (8a) and (8c)) learned/learned learned/learned 70KB/8KB/ – 8.58% 11.23% ImageNet, we train the quantized DNNs for 50 epochs, using SGD with momentum 0.9 and a learning rate schedule starting with 0.01 and reducing it by a factor of 10 after 16 and 32 epochs, respectively. Please note that we quantize all layers opposed to other papers which use a higher precision for the ﬁrst and/or last layer. In our experiments, we noticed that the performance of DQ is not sensitive to the choice of λj in (10). For the CIFAR-10 experiments, we use λ = 0.1 for both constraints (for sizes in kB). For the ImageNet experiments, we kept the same regularization level by scaling λj with the square of the size ratio between the ImageNet model and the CIFAR-10 model. We scale with the square-ratio as the constraints in (10) are squared penalty terms. First, in Table 3/top, we train a ResNet-20 on CIFAR-10 with quantized weights and ﬂoat32 activa- tions. We start with the most restrictive quantization scheme with ﬁxed qmax and b = 2bit (“Fixed”). Then, we allow the model to learn qmax while b = 2bit remains ﬁxed as was done in (Jain et al., 2019) (“TQT”). Finally, we learn both qmax and b with the constraint that the weight size is at most 70KB (“Ours”), which is just 4.5kB larger that the previous 2Bit networks. This allows the model to allocate more than two bits to some layers. From Table 3/top, we observe that the error is smallest when we learn all quantization parameters. In Table 3/bottom, weights and activations are quantized. For activation quantization, we consider two cases as discussed in Sec. 3. The ﬁrst one constrains the total activation memory Sx while the second constrains the maximum activation memory ˆSx such that both have the same size as a homogeneously quantized model with 4bit activations. Again, we observe that the error is smallest when we learn all quantization parameters. We also use DQ to train quantized ResNet-18 (He et al., 2016) and MobileNetV2 (Sandler et al., 2018) on ImageNet (Deng et al., 2009) with 4bit uniform weights and activations or equivalent-sized networks with learned quantization parameters. This is quite aggressive and, thus, a ﬁxed quantization scheme loses more than 6% accuracy while our quantization scheme loses less than 0.5% compared to a ﬂoat32 precision network. Our results compare favorably to other recent quantization approaches. To our knowledge, the best result for a 4bit ResNet-18 was reported by (Esser et al., 2019) (29.91% error). This is very close to our performance (29.92% error). Importantly, (Esser et al., 2019) did not quantize the ﬁrst and last layers, meaning that their network is much bigger. Speciﬁcally, compared to our quantized ResNet-18, their model with high precision input and output layers requires 37% more memory to store the weights. Moreover, (Esser et al., 2019) learns stepsizes which are not restricted to powers-of-two. As explained in Sec. 2.2, uniform quantization with power-of-two stepsize leads to more efﬁcient inference, effectively allowing to efﬁciently compute any multiplication with an integer multiplication and bit-shift. To our knowledge only (Wang et al., 2018) reported results of MobileNetV2 quantized to 4bit. They keep the baseline performance constraining the network to the same size as the 4bit network. However, they do not quantize the activations in this case. In addition, DQ training is efﬁcient since it is comparable to the training of unquantized network. Speciﬁcally, one epoch on ImageNet takes 37min for MobileNetV2 and 18min for ResNet-18 on four Nvidia Tesla V100. Fig. 5 shows the weight bitwidth assignment over layers. We observe that small bitwidths are used for layers with many parameters, i.e., pointwise convolutions and fully connected layers. However, the resulting bitwidth assignments are complex, meaning that there is no simple heuristic. Therefore, it is important to learn the optimal bitwidth assignment. 8Published as a conference paper at ICLR 2020 Table 4: Homogeneous vs. heterogeneous quantization of MobileNetV2 and ResNet-18 on ImageNet. MobileNetV2 ResNet-18 Bitwidth qmax Size Validation Size Validation Weight/Activ. Weight/Activ. Weight/Activ(max) Error Weight/Activ(max) Error Baseline 32bit/32bit – 13.23MB/4.59MB 29.82% 44.56MB/3.04MB 29.72% Fixed 4bit/4bit ﬁxed/ﬁxed 1.65MB/0.57MB 36.27% 5.57MB/0.38MB 34.15% TQT (Jain et al., 2019) 4bit/4bit learned/learned 1.65MB/0.57MB 32.21% 5.57MB/0.38MB 30.49% Ours (w/ constr. (8a) and (8c)) learned/learned learned/learned 1.55MB/0.57MB 30.26% 5.40MB/0.38MB 29.92% Ours (w/o constr.) learned/learned learned/learned 3.14MB/1.58MB 29.41% 10.50MB/1.05MB 29.34% 2 bit 4 bit 6 bit 8 bit 10 bit Layer Bitwidth ResNet-18 2 bit 4 bit 6 bit 8 bit 10 bit Layer Bitwidth MobileNetV2 Convolution Afﬁne Depthwise Pointwise Figure 5: Weight bitwidth assignment over layers for ResNet-18 and MobileNetV2 on ImageNet with weights constrained to a maximum size of 5.57MB. Our method has learned a heterogeneous bitwidth distribution, which gives a better performance than a homogeneous one (see Table 4). 5 C ONCLUSIONS In this paper we discussed differentiable quantization and its application to the training of compact DNNs with memory constraints. In order to fulﬁll memory constraints, we introduced penalty functions during training and used stochastic gradient descent to ﬁnd the optimal weights as well as the optimal quantization values in a joint fashion. We showed that there are several possible parametrizations of the quantization function. In particular, learning the bitwidth directly is not optimal; therefore, we proposed to parametrize the quantizer with the stepsize and dynamic range instead. The bitwidth can then be inferred from them. This approach is competitive to other recent quantization methods while it does not require to retrain the network multiple times in contrast to reinforcement learning approaches (Wang et al., 2018; Elthakeb et al., 2018). ACKNOWLEDGEMENTS We would like to thank Masato Ishii for many helpful comments during the preparation of this manuscript. REFERENCES Yu Bai, Yu-Xiang Wang, and Edo Liberty. Proxquant: Quantized neural networks via proximal operators. CoRR, abs/1810.00861, 2018. URL http://arxiv.org/abs/1810.00861. Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013. Dimitri P Bertsekas. Constrained optimization and Lagrange multiplier methods. Academic press, 2014. Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006. Fabien Cardinaux, Stefan Uhlich, Kazuki Yoshiyama, Javier Alonso García, Stephen Tiedemann, Thomas Kemp, and Akira Nakamura. Iteratively training look-up tables for network quantization. arXiv preprint arXiv:1811.05355, 2018. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009. 9Published as a conference paper at ICLR 2020 Ahmed T. Elthakeb, Prannoy Pilligundla, Amir Yazdanbakhsh, Sean Kinzer, and Hadi Esmaeilzadeh. Releq: A reinforcement learning approach for deep quantization of neural networks. CoRR, abs/1811.01704, 2018. URL http://arxiv.org/abs/1811.01704. Steven K. Esser, Jeffrey L. McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dhar- mendra S. Modha. Learned step size quantization. CoRR, abs/1902.08153, 2019. URL http://arxiv.org/abs/1902.08153. Kaiyuan Guo, Shulin Zeng, Jincheng Yu, Yu Wang, and Huazhong Yang. A survey of fpga-based neural network accelerator. arXiv preprint arXiv:1712.08934, 2017. Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding. CoRR, abs/1510.00149, 2015. URL http://arxiv.org/abs/1510.00149. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized neural networks: Training neural networks with low precision weights and activations. CoRR, abs/1609.07061, 2016. URL http://arxiv.org/abs/1609.07061. Sambhav R. Jain, Albert Gural, Michael Wu, and Chris Dick. Trained uniform quantization for accurate and efﬁcient neural network inference on ﬁxed-point hardware. CoRR, abs/1903.08066, 2019. URL http://arxiv.org/abs/1903.08066. Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016. Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009. Fengfu Li, Bo Zhang, and Bin Liu. Ternary weight networks. arXiv preprint arXiv:1605.04711, 2016. Zhi-Gang Liu and Matthew Mattina. Learning low-precision neural networks without straight-through estimator (ste). arXiv preprint arXiv:1903.01061, 2019. Christos Louizos, Matthias Reisser, Tijmen Blankevoort, Efstratios Gavves, and Max Welling. Relaxed quantization for discretized neural networks. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=HkxjYoCqKX. Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. CoRR, abs/1611.00712, 2016. URL http://arxiv. org/abs/1611.00712. Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mo- bilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4510–4520, 2018. Sony. Neural Network Libraries (NNabla). https://github.com/sony/nnabla. Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han. HAQ: hardware-aware automated quantization. CoRR, abs/1811.08886, 2018. URL http://arxiv.org/abs/1811.08886. Penghang Yin, Jiancheng Lyu, Shuai Zhang, Stanley Osher, Yingyong Qi, and Jack Xin. Under- standing straight-through estimator in training activation quantized neural nets. arXiv preprint arXiv:1903.05662, 2019. Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen. Incremental network quantization: Towards lossless cnns with low-precision weights. arXiv preprint arXiv:1702.03044, 2017. 10Published as a conference paper at ICLR 2020 A D ERIVATION OF THE GRADIENTS FOR DIFFERENTIABLE QUANTIZATION (DQ) In the following sections, we will give the derivatives ∂ ∂x Q(x; θ) and gradients ∇θQ(x; θ) for the uniform and the power-of-two quantizers. The results are summarized in Sec. 2. We use the straight-through gradient estimate whenever we need to differentiate a non-differentiable ﬂoor function, i.e., we assume ∂ ∂x ⌊x⌋ = 1. (11) A.1 D ERIVATIVES OF THE UNIFORM QUANTIZER Fig. 6(a) shows a symmetric uniform quantizer QU (x; θ) which maps a real value x ∈ R to one of I = 2k + 1quantized values q ∈ {−kd, ..., 0, ..., kd} by computing q = QU (x; θ) =sign(x) { d ⌊ |x| d + 1 2 ⌋ |x| ≤ qmax qmax |x| > qmax (12) using the parameters θ = [d, qmax, b]T where d ∈ R is the stepsize, qmax ∈ R is the maximum value and b ∈ N is the number of bits that we use to encode the quantized values q. The elements of θ are dependent as there is the relationship qmax = (2b−1 − 1)d. A.1.1 C ASE U1: P ARAMETRIZATION WITH RESPECT TO b AND d For the parametrization with respect to the bitwidth b and steps size d, (12) is given by q = QU (x; b, d) =sign(x)d {⌊ |x| d + 1 2 ⌋ |x| ≤ (2b−1 − 1)d 2b−1 − 1 |x| > (2b−1 − 1)d (13) and the derivatives are given by ∂QU (x; b, d) ∂b = sign(x)2b−1 log 2 2b−1 − 1 {0 |x| ≤ (2b−1 − 1)d (2b−1 − 1)d |x| > (2b−1 − 1)d , (14a) ∂QU (x; b, d) ∂d = sign(x) 1 d { d ⌊ |x| d + 1 2 ⌋ − |x| | x| ≤ (2b−1 − 1)d (2b−1 − 1)d |x| > (2b−1 − 1)d . (14b) A.1.2 C ASE U2: P ARAMETRIZATION WITH RESPECT TO b AND qMAX For the parametrization with respect to the bitwidth b and maximum value qmax, (12) is given by q = QU (x; b, qmax) =sign(x)qmax { 1 2b−1−1 ⌊ |x|2b−1−1 qmax + 1 2 ⌋ |x| ≤ qmax 1 |x| > qmax (15) and the derivatives are given by ∂QU (x; b, qmax) ∂b = sign(x)2b−1 log 2 2b−1 − 1 { − qmax 2b−1−1 ⌊ |x|2b−1−1 qmax + 1 2 ⌋ + β1 |x| ≤ qmax 0 |x| > qmax , (16a) ∂QU (x; b, qmax) ∂qmax = sign(x) 1 qmax { qmax 2b−1−1 ⌊ |x|2b−1−1 qmax + 1 2 ⌋ + β2 |x| ≤ qmax qmax |x| > qmax , (16b) where β1 = qmax 2b−1 log 2 ∂ ⌊ |x| 2b−1−1 qmax + 1 2 ⌋ ∂b = |x| and β2 = q2 max 2b−1−1 ∂ ⌊ |x| 2b−1−1 qmax + 1 2 ⌋ ∂qmax = −|x|, if we use the straight-through gradient estimate for the ﬂoor function. 11Published as a conference paper at ICLR 2020 d qmax d qmax x QU(x) (a) Uniform quantization qmin qmax qmin qmax x QP(x) (b) Power-of-two quantization Figure 6: Examples of uniform quantizer QU (x) and power-of-two quantizer QP (x) for b = 3bits A.1.3 C ASE U3: P ARAMETRIZATION WITH RESPECT TO d AND qMAX Eq. (12) gives the quantization with respect to the step size d and maximum value qmax. The derivatives are ∂QU (x; d, qmax) ∂d = sign(x) 1 d { d ⌊ |x| d + 1 2 ⌋ − |x| | x| ≤ qmax 0 |x| > qmax , (17a) ∂QU (x; d, qmax) ∂qmax = sign(x) 1 qmax {0 |x| ≤ qmax qmax |x| > qmax . (17b) A.2 D ERIVATIVES OF THE POWER -OF-TWO QUANTIZER Power-of-two quantization QP (x; θ) maps a real-valued number x ∈ R to a quantized value q ∈ {±2k : k ∈ Z} by q = QP (x; θ) =sign(x)    qmin |x| ≤ qmin 2⌊0.5+log2|x|⌋ qmin < |x| ≤ qmax qmax |x| > qmax , (18) where qmin and qmax are the minimum and maximum (absolute) values of the quantizer for a bitwidth of b bits. Fig. 6b shows the quantization curve for this quantization scheme. Using the STE for the ﬂoor operation, the derivative ∂xQP (x; θ) is given by ∂xQP (x) =    0 |x| ≤ qmin 2⌊0.5+log2|x|⌋ |x| qmin < |x| ≤ qmax 0 |x| > qmax . (19) The power-of-two quantization has the three parameters θ = [b, qmin, qmax], which are dependent on each other, i.e., qmax = 22b−1−1qmin. Therefore, we have again three different parametrizations with θ = [b, qmin], θ = [b, qmax] or θ = [qmin, qmax], respectively. The resulting partial derivatives for each parametrization are shown in Fig. 7 and summarized in the following sections. Similar to the uniform case, one parametrization ( θ = [qmin, qmax]) leads to a gradient with the nice form ∇θQP (x; θ) = [ ∂qmin QU (x; θ) ∂qmax QU (x; θ) ] =    [1, 0]T |x| ≤ qmin [0, 0]T qmin < |x| ≤ qmax [0, 1]T |x| > qmax , (20) which has a bounded gradient magnitude and independent components and is, hence, well suited for ﬁrst order gradient based optimization. A.2.1 C ASE P1: P ARAMETRIZATION WITH RESPECT TO b AND qMAX For the parametrization with θ = [b, qmax], (18) is given by QP (x; b, qmax) =sign(x)    2−2b−1+1qmax |x| ≤ 2−2b−1+1qmax 2⌊0.5+log2|x|⌋ 2−2b−1+1qmax < |x| ≤ qmax qmax |x| > qmax (21) 12Published as a conference paper at ICLR 2020 x ∂Q P (x) ∂b ∂Q P (x) ∂q max (a) Case P1 x ∂Q P (x) ∂b ∂Q P (x) ∂q min (b) Case P2 x ∂Q P (x) ∂q min ∂Q P (x) ∂q max (c) Case P3 x ∂Q P (x) ∂x (d) Input derivative Figure 7: Derivatives for the three different parametrizations of QP (x; θ) and the partial derivatives are ∂QP (x; b, qmax) ∂b = sign(x)    −2−2b−1+b(log 2)2qmax |x| ≤ − 2−2b−1+1qmax 0 −2−2b−1+1qmax < |x| ≤ qmax 0 |x| > qmax , (22a) ∂QP (x; b, qmax) ∂qmax = sign(x)    2−2b−1+1 |x| ≤ − 2−2b−1+1qmax 0 −2−2b−1+1qmax < |x| ≤ qmax 1 |x| > qmax . (22b) A.2.2 C ASE P2: P ARAMETRIZATION WITH RESPECT TO b AND qMIN For the parametrization with θ = [b, qmin], (18) is given by QP (x; b, qmin) =sign(x)    qmin |x| ≤ qmin 2⌊0.5+log2|x|⌋ qmin < |x| ≤ 22b−1−1qmin 22b−1−1qmin |x| > 22b−1−1qmin (23) and the partial derivatives are ∂QP (x; b, qmin) ∂b = sign(x)    0 |x| ≤ qmin 0 qmin < |x| ≤ 22b−1−1qmin 22b−1+b−2(log 2)2qmin |x| > 22b−1−1qmin , (24a) ∂QP (x; b, qmin) ∂qmin = sign(x)    1 |x| ≤ qmin 0 qmin < |x| ≤ 22b−1−1qmin 22b−1−1 |x| > 22b−1−1qmin . (24b) A.2.3 C ASE P3: P ARAMETRIZATION WITH RESPECT TO qMIN AND qMAX Eq. (18) gives the parametrization of Q(x; θ) with respect to the minimum value qmin and maximum value qmax. The derivatives are ∂QP (x; qmin, qmax) ∂qmin = sign(x)    1 |x| ≤ qmin 0 qmin < |x| ≤ qmax 0 |x| > qmax , (25a) ∂QP (x; qmin, qmax) ∂qmax = sign(x)    0 |x| ≤ qmin 0 qmin < |x| ≤ qmax 1 |x| > qmax . (25b) 13Published as a conference paper at ICLR 2020 5 10 15 0 2 0 0.5 1 θ∗ b d (a) Case U1 5 10 15 5 10 0 0.5 1 θ∗ bqmax (b) Case U2 0 2 5 10 0 0.5 1 θ∗ d qmax (c) Case U3 Figure 8: MSE surfaces for uniform quantization. Only U3 reaches the optimum θ∗. 2468 5 10 0 0.5 1 θ∗ bqmax (a) Case P1 2468 0 0.5 1 0 0.5 1 θ∗ bqmin (b) Case P2 0 0.5 1 5 10 0 0.5 1 θ∗ qmin qmax (c) Case P3 Figure 9: MSE surfaces for power-of-two quantization. Only P3 reaches the optimum θ∗. A.3 V ISUALIZATION OF THE ERROR SURFACE FOR THE QUANTIZATION OF GAUSSIAN DATA In Sec. 2.3 of the paper, we compared the three different parametrizations of the uniform quantizer at the example of optimal quantization of Gaussian data. To get a better understanding of Fig 3, we show how the error surfaces look like for this example problem. The experimental setup is the same as in Sec. 2.3, i.e., we use DQ to learn the optimal quantization parameters of a uniform and a power-of-two quantizer, which minimize the expected quantization error minθ E [ (x − Q(x; θ))2 ] . We use three different parametrizations, adapt the quantizer’s parameters with gradient descent and compare the convergence speed as well as the ﬁnal quantization error. As an input, we generate 104 samples from N(0, 1). Fig. 8 shows the corresponding error surfaces for the three different parametrizations of the uniform quantization. The red curve shows the path through the parameter space taken by gradient descent in order to optimize the MSE, starting with the initial values b = 2, d = qmax = 1. The optimum θ∗ is located at b = 16, d ⪅2−13, q max = 4, since we allow a maximal bitwidth of 16bit and the largest sample magnitude in our dataset is max{x1, ..., xN } ⪅ 4. In each of the cases U1-U3, the error surface is composed of steep ridges and large ﬂat regions. The steep ridges force us to use small learning rates to avoid divergence. For cases U1 and U2, the optimal θ∗ can not be reached. However, for U3, θ∗ lies at the border of a ﬂat region and can be easily reached. Furthermore, case U3 shows a much faster and more stable convergence without oscillation, since the gradient magnitudes are bounded and the error surface has fewer steep ridges where gradient descent starts oscillating. Fig. 9 shows the corresponding error surfaces for the three different parametrizations of the power-of- two quantization. Again, the optimum θ∗ is not attained for two parametrizations, namely P1 and P2, as θ∗ is surrounded by a large, mostly ﬂat region. For these two cases, gradient descent tends to oscillate at steep ridges and tends to be unstable. However, gradient descent converges to a point close to θ∗ for parametrization P3, where θ = [qmin, qmax]. Finally, we also did a comparison of the different power-of-two quantizations on CIFAR-10. Fig. 10 shows the evolution of the training and validation error if we start from a random or a pre-trained ﬂoat network initialization. We can observe that θ = [qmin, qmax] has the best convergence behavior and thus also results in the smallest validation error (cf. Table 1). The unstable behavior of P2 is expected as the derivative ∂QP ∂qmin can take very large (absolute) values. A.4 F URTHER EXPERIMENTS WITH ADAM Finally, we did an experiment to verify that the parametrization is important, even if adaptive gradient descent methods like ADAM are used for optimization. Table 5 gives the results for a ResNet-20 trained on CIFAR-10. We observe, that again U3 and P3 are the best parametrizations. 14Published as a conference paper at ICLR 2020 0 2 4 6 ·104 10−2 10−1 100 Iteration Training error (a) Random initialization 0 2 4 6 ·104 10−1 100 Iteration Validation error b, d (U1) b, qmax (U2) d, qmax (U3) 0 2 4 6 ·104 10−2 10−1 100 Iteration Training error (b) Pre-trained initialization 0 2 4 6 ·104 10−1 100 Iteration Validation error Figure 10: ResNet-20 with power-of-two quantized weights and activations. Table 5: Error rate of ResNet-20 on CIFAR-10 using different quantization parametrizations. Training is done either by SGD with momentum or ADAM. Parametrization SGD momentum ADAM U1 11.74 7.61 U2 7.44 7.85 U3 7.32 7.36 P1 15.35 7.54 P2 7.74 7.79 P3 7.40 7.40 B I MPLEMENTATION DETAILS FOR DIFFERENTIABLE QUANTIZATION The following code gives our differentiable quantizer implementation in NNabla (Sony). The source code for reproducing our results will be published after the review process has been ﬁnished. 15Published as a conference paper at ICLR 2020 B.1 U NIFORM QUANTIZATION B.1.1 C ASE U1: P ARAMETRIZATION WITH RESPECT TO b AND d 1 def parametric_fixed_point_quantize_d_b(x, sign, 2 n_init, n_min, n_max, 3 d_init, d_min, d_max, 4 fix_parameters=False): 5 \"\"\"Parametric version of ‘fixed_point_quantize‘ where the 6 bitwidth ‘b‘ and stepsize ‘d‘ are learnable parameters. 7 8 Returns: 9 ~nnabla.Variable: N-D array. 10 \"\"\" 11 def clip_scalar(v, min_value, max_value): 12 return F.minimum_scalar(F.maximum_scalar(v, min_value), max_value) 13 14 def broadcast_scalar(v, shape): 15 return F.broadcast(F.reshape(v, (1,) * len(shape), inplace=False), shape=shape) 16 17 def quantize_pow2(v): 18 return 2 ** F.round(F.log(v) / np.log(2.)) 19 20 n = get_parameter_or_create(\"n\", (), 21 ConstantInitializer(n_init), 22 need_grad=True, 23 as_need_grad=not fix_parameters) 24 d = get_parameter_or_create(\"d\", (), 25 ConstantInitializer(d_init), 26 need_grad=True, 27 as_need_grad=not fix_parameters) 28 29 # ensure that bitwidth is in specified range and an integer 30 n = F.round(clip_scalar(n, n_min, n_max)) 31 if sign: 32 n = n - 1 33 34 # ensure that stepsize is in specified range and a power of two 35 d = quantize_pow2(clip_scalar(d, d_min, d_max)) 36 37 # ensure that dynamic range is in specified range 38 xmax = d * (2 ** n - 1) 39 40 # compute min/max value that we can represent 41 if sign: 42 xmin = -xmax 43 else: 44 xmin = nn.Variable((1,), need_grad=False) 45 xmin.d = 0. 46 47 # broadcast variables to correct size 48 d = broadcast_scalar(d, shape=x.shape) 49 xmin = broadcast_scalar(xmin, shape=x.shape) 50 xmax = broadcast_scalar(xmax, shape=x.shape) 51 52 # apply fixed-point quantization 53 return d * F.round(F.clip_by_value(x, xmin, xmax) / d) 16Published as a conference paper at ICLR 2020 B.1.2 C ASE U2: P ARAMETRIZATION WITH RESPECT TO b AND qMAX 1 def parametric_fixed_point_quantize_b_xmax(x, sign, 2 n_init, n_min, n_max, 3 xmax_init, xmax_min, xmax_max, 4 fix_parameters=False): 5 \"\"\"Parametric version of ‘fixed_point_quantize‘ where the 6 bitwidth ‘b‘ and dynamic range ‘xmax‘ are learnable parameters. 7 8 Returns: 9 ~nnabla.Variable: N-D array. 10 \"\"\" 11 def clip_scalar(v, min_value, max_value): 12 return F.minimum_scalar(F.maximum_scalar(v, min_value), max_value) 13 14 def broadcast_scalar(v, shape): 15 return F.broadcast(F.reshape(v, (1,) * len(shape), inplace=False), shape=shape) 16 17 def quantize_pow2(v): 18 return 2 ** F.round(F.log(v) / np.log(2.)) 19 20 n = get_parameter_or_create(\"n\", (), 21 ConstantInitializer(n_init), 22 need_grad=True, 23 as_need_grad=not fix_parameters) 24 xmax = get_parameter_or_create(\"xmax\", (), 25 ConstantInitializer(xmax_init), 26 need_grad=True, 27 as_need_grad=not fix_parameters) 28 29 # ensure that bitwidth is in specified range and an integer 30 n = F.round(clip_scalar(n, n_min, n_max)) 31 if sign: 32 n = n - 1 33 34 # ensure that dynamic range is in specified range 35 xmax = clip_scalar(xmax, xmax_min, xmax_max) 36 37 # compute step size from dynamic range and make sure that it is a pow2 38 d = quantize_pow2(xmax / (2 ** n - 1)) 39 40 # compute min/max value that we can represent 41 if sign: 42 xmin = -xmax 43 else: 44 xmin = nn.Variable((1,), need_grad=False) 45 xmin.d = 0. 46 47 # broadcast variables to correct size 48 d = broadcast_scalar(d, shape=x.shape) 49 xmin = broadcast_scalar(xmin, shape=x.shape) 50 xmax = broadcast_scalar(xmax, shape=x.shape) 51 52 # apply fixed-point quantization 53 return d * F.round(F.clip_by_value(x, xmin, xmax) / d) 17Published as a conference paper at ICLR 2020 B.1.3 C ASE U3: P ARAMETRIZATION WITH RESPECT TO d AND qMAX 1 def parametric_fixed_point_quantize_d_xmax(x, sign, 2 d_init, d_min, d_max, 3 xmax_init, xmax_min, xmax_max, 4 fix_parameters=False): 5 \"\"\"Parametric version of ‘fixed_point_quantize‘ where the 6 stepsize ‘d‘ and dynamic range ‘xmax‘ are learnable parameters. 7 8 Returns: 9 ~nnabla.Variable: N-D array. 10 \"\"\" 11 def clip_scalar(v, min_value, max_value): 12 return F.minimum_scalar(F.maximum_scalar(v, min_value), max_value) 13 14 def broadcast_scalar(v, shape): 15 return F.broadcast(F.reshape(v, (1,) * len(shape), inplace=False), shape=shape) 16 17 def quantize_pow2(v): 18 return 2 ** F.round(F.log(v) / np.log(2.)) 19 20 d = get_parameter_or_create(\"d\", (), 21 ConstantInitializer(d_init), 22 need_grad=True, 23 as_need_grad=not fix_parameters) 24 xmax = get_parameter_or_create(\"xmax\", (), 25 ConstantInitializer(xmax_init), 26 need_grad=True, 27 as_need_grad=not fix_parameters) 28 29 # ensure that stepsize is in specified range and a power of two 30 d = quantize_pow2(clip_scalar(d, d_min, d_max)) 31 32 # ensure that dynamic range is in specified range 33 xmax = clip_scalar(xmax, xmax_min, xmax_max) 34 35 # compute min/max value that we can represent 36 if sign: 37 xmin = -xmax 38 else: 39 xmin = nn.Variable((1,), need_grad=False) 40 xmin.d = 0. 41 42 # broadcast variables to correct size 43 d = broadcast_scalar(d, shape=x.shape) 44 xmin = broadcast_scalar(xmin, shape=x.shape) 45 xmax = broadcast_scalar(xmax, shape=x.shape) 46 47 # apply fixed-point quantization 48 return d * F.round(F.clip_by_value(x, xmin, xmax) / d) 18Published as a conference paper at ICLR 2020 B.2 P OWER -OF-TWO QUANTIZATION B.2.1 C ASE P1: P ARAMETRIZATION WITH RESPECT TO b AND qMAX 1 def parametric_pow2_quantize_b_xmax(x, sign, with_zero, 2 n_init, n_min, n_max, 3 xmax_init, xmax_min, xmax_max, 4 fix_parameters=False): 5 \"\"\"Parametric version of ‘pow2_quantize‘ where the 6 bitwidth ‘n‘ and range ‘xmax‘ are learnable parameters. 7 8 Returns: 9 ~nnabla.Variable: N-D array. 10 \"\"\" 11 def clip_scalar(v, min_value, max_value): 12 return F.minimum_scalar(F.maximum_scalar(v, min_value), max_value) 13 14 def broadcast_scalar(v, shape): 15 return F.broadcast(F.reshape(v, (1,) * len(shape), inplace=False), shape=shape) 16 17 def quantize_pow2(v): 18 return 2 ** F.round(F.log(F.abs(v)) / np.log(2.)) 19 20 n = get_parameter_or_create(\"n\", (), 21 ConstantInitializer(n_init), 22 need_grad=True, 23 as_need_grad=not fix_parameters) 24 xmax = get_parameter_or_create(\"xmax\", (), 25 ConstantInitializer(xmax_init), 26 need_grad=True, 27 as_need_grad=not fix_parameters) 28 29 # ensure that bitwidth is in specified range and an integer 30 n = F.round(clip_scalar(n, n_min, n_max)) 31 if sign: 32 n = n - 1 33 if with_zero: 34 n = n - 1 35 36 # ensure that dynamic range is in specified range and an integer 37 xmax = quantize_pow2(clip_scalar(xmax, xmax_min, xmax_max)) 38 39 # compute min value that we can represent 40 xmin = (2 ** (-(2 ** n) + 1)) * xmax 41 42 # broadcast variables to correct size 43 xmin = broadcast_scalar(xmin, shape=x.shape) 44 xmax = broadcast_scalar(xmax, shape=x.shape) 45 46 # if unsigned, then quantize all negative values to zero 47 if not sign: 48 x = F.relu(x) 49 50 # compute absolute value/sign of input 51 ax = F.abs(x) 52 sx = F.sign(x) 53 54 if with_zero: 55 # prune smallest elements (in magnitude) to zero if they are smaller 56 # than ‘x_min / \\sqrt(2)‘ 57 x_threshold = xmin / np.sqrt(2) 58 59 idx1 = F.greater_equal(ax, x_threshold) * F.less(ax, xmin) 60 idx2 = F.greater_equal(ax, xmin) * F.less(ax, xmax) 61 idx3 = F.greater_equal(ax, xmax) 62 else: 63 idx1 = F.less(ax, xmin) 64 idx2 = F.greater_equal(ax, xmin) * F.less(ax, xmax) 65 idx3 = F.greater_equal(ax, xmax) 66 67 # do not backpropagate gradient through indices 68 idx1.need_grad = False 69 idx2.need_grad = False 70 idx3.need_grad = False 71 72 # do not backpropagate gradient through sign 73 sx.need_grad = False 74 75 # take care of values outside of dynamic range 76 return sx * (xmin * idx1 + quantize_pow2(ax) * idx2 + xmax * idx3) 19Published as a conference paper at ICLR 2020 B.2.2 C ASE P2: P ARAMETRIZATION WITH RESPECT TO b AND qMIN 1 def parametric_pow2_quantize_b_xmin(x, sign, with_zero, 2 n_init, n_min, n_max, 3 xmin_init, xmin_min, xmin_max, 4 fix_parameters=False): 5 \"\"\"Parametric version of ‘pow2_quantize‘ where the 6 bitwidth ‘n‘ and the smallest value ‘xmin‘ are learnable parameters. 7 8 Returns: 9 ~nnabla.Variable: N-D array. 10 \"\"\" 11 def clip_scalar(v, min_value, max_value): 12 return F.minimum_scalar(F.maximum_scalar(v, min_value), max_value) 13 14 def broadcast_scalar(v, shape): 15 return F.broadcast(F.reshape(v, (1,) * len(shape), inplace=False), shape=shape) 16 17 def quantize_pow2(v): 18 return 2 ** F.round(F.log(F.abs(v)) / np.log(2.)) 19 20 n = get_parameter_or_create(\"n\", (), 21 ConstantInitializer(n_init), 22 need_grad=True, 23 as_need_grad=not fix_parameters) 24 xmin = get_parameter_or_create(\"xmin\", (), 25 ConstantInitializer(xmin_init), 26 need_grad=True, 27 as_need_grad=not fix_parameters) 28 29 # ensure that bitwidth is in specified range and an integer 30 n = F.round(clip_scalar(n, n_min, n_max)) 31 if sign: 32 n = n - 1 33 if with_zero: 34 n = n - 1 35 36 # ensure that minimum dynamic range is in specified range and a power-of-two 37 xmin = quantize_pow2(clip_scalar(xmin, xmin_min, xmin_max)) 38 39 # compute min/max value that we can represent 40 xmax = xmin * (2 ** ((2 ** n) - 1)) 41 42 # broadcast variables to correct size 43 xmin = broadcast_scalar(xmin, shape=x.shape) 44 xmax = broadcast_scalar(xmax, shape=x.shape) 45 46 # if unsigned, then quantize all negative values to zero 47 if not sign: 48 x = F.relu(x) 49 50 # compute absolute value/sign of input 51 ax = F.abs(x) 52 sx = F.sign(x) 53 54 if with_zero: 55 # prune smallest elements (in magnitude) to zero if they are smaller 56 # than ‘x_min / \\sqrt(2)‘ 57 x_threshold = xmin / np.sqrt(2) 58 59 idx1 = F.greater_equal(ax, x_threshold) * F.less(ax, xmin) 60 idx2 = F.greater_equal(ax, xmin) * F.less(ax, xmax) 61 idx3 = F.greater_equal(ax, xmax) 62 else: 63 idx1 = F.less(ax, xmin) 64 idx2 = F.greater_equal(ax, xmin) * F.less(ax, xmax) 65 idx3 = F.greater_equal(ax, xmax) 66 67 # do not backpropagate gradient through indices 68 idx1.need_grad = False 69 idx2.need_grad = False 70 idx3.need_grad = False 71 72 # do not backpropagate gradient through sign 73 sx.need_grad = False 74 75 # take care of values outside of dynamic range 76 return sx * (xmin * idx1 + quantize_pow2(ax) * idx2 + xmax * idx3) 20Published as a conference paper at ICLR 2020 B.2.3 C ASE P3: P ARAMETRIZATION WITH RESPECT TO qMIN AND qMAX 1 def parametric_pow2_quantize_xmin_xmax(x, sign, with_zero, 2 xmin_init, xmin_min, xmin_max, 3 xmax_init, xmax_min, xmax_max, 4 fix_parameters=False): 5 \"\"\"Parametric version of ‘pow2_quantize‘ where the 6 min value ‘xmin‘ and max value ‘xmax‘ are learnable parameters. 7 8 Returns: 9 ~nnabla.Variable: N-D array. 10 \"\"\" 11 def clip_scalar(v, min_value, max_value): 12 return F.minimum_scalar(F.maximum_scalar(v, min_value), max_value) 13 14 def broadcast_scalar(v, shape): 15 return F.broadcast(F.reshape(v, (1,) * len(shape), inplace=False), shape=shape) 16 17 def quantize_pow2(v): 18 return 2. ** F.round(F.log(F.abs(v)) / np.log(2.)) 19 20 xmin = get_parameter_or_create(\"xmin\", (), 21 ConstantInitializer(xmin_init), 22 need_grad=True, 23 as_need_grad=not fix_parameters) 24 xmax = get_parameter_or_create(\"xmax\", (), 25 ConstantInitializer(xmax_init), 26 need_grad=True, 27 as_need_grad=not fix_parameters) 28 29 # ensure that minimum dynamic range is in specified range and a power-of-two 30 xmin = quantize_pow2(clip_scalar(xmin, xmin_min, xmin_max)) 31 32 # ensure that minimum dynamic range is in specified range and a power-of-two 33 xmax = quantize_pow2(clip_scalar(xmax, xmax_min, xmax_max)) 34 35 # broadcast variables to correct size 36 xmin = broadcast_scalar(xmin, shape=x.shape) 37 xmax = broadcast_scalar(xmax, shape=x.shape) 38 39 # if unsigned, then quantize all negative values to zero 40 if not sign: 41 x = F.relu(x) 42 43 # compute absolute value/sign of input 44 ax = F.abs(x) 45 sx = F.sign(x) 46 47 if with_zero: 48 # prune smallest elements (in magnitude) to zero if they are smaller 49 # than ‘x_min / \\sqrt(2)‘ 50 x_threshold = xmin / np.sqrt(2) 51 52 idx1 = F.greater_equal(ax, x_threshold) * F.less(ax, xmin) 53 idx2 = F.greater_equal(ax, xmin) * F.less(ax, xmax) 54 idx3 = F.greater_equal(ax, xmax) 55 else: 56 idx1 = F.less(ax, xmin) 57 idx2 = F.greater_equal(ax, xmin) * F.less(ax, xmax) 58 idx3 = F.greater_equal(ax, xmax) 59 60 # do not backpropagate gradient through indices 61 idx1.need_grad = False 62 idx2.need_grad = False 63 idx3.need_grad = False 64 65 # do not backpropagate gradient through sign 66 sx.need_grad = False 67 68 # take care of values outside of dynamic range 69 return sx * (xmin * idx1 + quantize_pow2(ax) * idx2 + xmax * idx3) 21",
      "meta_data": {
        "arxiv_id": "1905.11452v3",
        "authors": [
          "Stefan Uhlich",
          "Lukas Mauch",
          "Fabien Cardinaux",
          "Kazuki Yoshiyama",
          "Javier Alonso Garcia",
          "Stephen Tiedemann",
          "Thomas Kemp",
          "Akira Nakamura"
        ],
        "published_date": "2019-05-27T19:03:40Z",
        "pdf_url": "https://arxiv.org/pdf/1905.11452v3.pdf",
        "github_url": "https://github.com/sony/nnabla"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the challenge of efficiently deploying Deep Neural Networks (DNNs) on resource-constrained mobile and embedded devices, which typically involves quantizing network parameters and activations. While mixed-precision networks offer better performance than homogeneous bitwidth networks for the same size, determining optimal bitwidths is complex. The main contribution is demonstrating that a suitable parametrization of the quantizer is crucial for stable training and superior performance in differentiable quantization (DQ). Specifically, the authors propose to parametrize the quantizer using the step size and dynamic range, from which the bitwidth is inferred, showing that explicit bitwidth parametrization performs worse. This approach allows learning all quantization parameters, including bitwidths, through gradient methods. They identified parametrizations that yield gradients with bounded norms and decoupled components, facilitating efficient first-order optimization. Furthermore, they formulate training with global memory constraints (total weight, total activation, and maximum activation memory) as a constrained optimization problem, solvable via a penalty framework. Experiments on CIFAR-10 and ImageNet confirm these findings, achieving state-of-the-art performance for heavily quantized MobileNetV2 and ResNet-18 models, comparable to floating-point baselines with significantly reduced memory footprints.",
        "methodology": "The core methodology relies on Differentiable Quantization (DQ) with Straight-Through Estimators (STE) for backpropagating gradients through non-differentiable quantization functions. The paper systematically analyzes three alternative parametrizations for both symmetric uniform quantizers and power-of-two quantizers. For uniform quantization, the parametrizations considered are [bitwidth (b), step size (d)], [b, dynamic range (qmax)], and [d, qmax]. For power-of-two quantization, they are [b, minimum absolute value (qmin)], [b, qmax], and [qmin, qmax]. The analysis shows that parametrizing with [d, qmax] for uniform quantization and [qmin, qmax] for power-of-two quantization leads to gradients with stable, bounded norms and decoupled components, resulting in a diagonal Hessian which improves gradient descent convergence. Hardware-friendly constraints (integer bitwidth, power-of-two step size/min/max values) are handled by rounding in the forward pass while backpropagating gradients through the original float values (STE). To incorporate memory constraints (total weight memory, total activation memory, and maximum activation memory) during training, a penalty method is used, converting the constrained optimization problem into an unconstrained one by adding squared penalty terms to the loss function. Optimization is performed using standard gradient descent methods like SGD with momentum and ADAM.",
        "experimental_setup": "The experimental validation includes two main scenarios: 1) Quantization of Gaussian data: Optimal quantization parameters were learned by minimizing the mean squared error (MSE) for 10^4 samples drawn from N(0,1). ADAM optimizer was used, and convergence speed and MSE were compared across different parametrizations. 2) Deep Neural Network quantization: ResNet-20 on CIFAR-10 and ResNet-18/MobileNetV2 on ImageNet. For CIFAR-10, ResNet-20 was trained for 160 epochs with SGD (momentum 0.9), a learning rate of 0.01 (decayed by 10 at 80 and 120 epochs), and random flips/crops for data augmentation. Initializations included random weights or pre-trained float networks. For ImageNet, ResNet-18 and MobileNetV2 were trained for 50 epochs with SGD (momentum 0.9), a learning rate of 0.01 (decayed by 10 at 16 and 32 epochs). All layers were quantized, unlike some other methods. Quantizers were initialized as described in the paper, typically starting with 4-bit representation. Pre-trained float32 networks were used for initialization in these experiments. Memory constraints were applied using the penalty method, with penalty weightings (λj) chosen based on a heuristic to balance initial loss and penalty magnitude (e.g., λ=0.1 for CIFAR-10, scaled for ImageNet). Hardware used included a single GTX 1080 Ti for CIFAR-10 experiments and four Nvidia Tesla V100 for ImageNet.",
        "limitations": "The penalty method used to enforce memory constraints does not strictly guarantee that the constraints are fulfilled, though the probability of fulfillment increases with larger penalty weightings (λj). However, choosing an excessively large λj can cause the penalty term to dominate the network loss, potentially degrading the network's performance. While other quantization methods with direct bitwidth learning or reinforcement learning approaches have higher computational complexity or suffer from large gradient variance, the paper's method effectively mitigates these issues through its parametrization. The paper's core 'limitation' is addressed by its own solution: the sub-optimality of directly learning bitwidth parameters, which previous works implicitly or explicitly faced.",
        "future_research_directions": "Not mentioned",
        "experimental_code": "import nnabla.functions as F\n\n\ndef round_backward(grad_inputs, inputs, input_shapes, outputs, output_shapes):\n    \"\"\"\n    Args:\n      grad_inputs (list of :obj:`nnabla.Variable`): Propagated grads to this backward function.\n      inputs (list of :obj:`nnabla.Variable` and None): Input Variables of the forward function\n          if this backward function depends on it. Otherwise, None is set instead.\n      input_shapes (list of tuple of :obj:`int`): Input shapes of the forward function.\n          The shapes of the inputs in which None is set can be passed.\n      outputs (list of :obj:`nnabla.Variable` and None): Output Variables of the forward function\n          if this backward function depends on it. Otherwise, None is set instead.\n      output_shapes (list of tuple of :obj:`int`): Output shapes of the forward function.\n          The shapes of the outputs in which None is set can be passed.\n      kwargs (dict of arguments): Dictionary of the corresponding function arguments.\n\n    Return:\n      list of Variable: Return the gradients wrt inputs of the corresponding function.\n    \"\"\"\n    dy = grad_inputs[0]\n    x0 = inputs[0]\n    dx0 = dy\n    return dx0",
        "experimental_info": "This code defines the straight-through estimator (STE) for the `round` function. In the context of Differentiable Quantization (DQ), `round` is a non-differentiable operation. This implementation ensures that gradients are directly passed through (`dx0 = dy`), effectively treating the rounding operation as an identity function for backward propagation. This is a core component of the STE mechanism described in the method for handling hardware-friendly constraints by rounding in the forward pass while backpropagating gradients through the original float values."
      }
    },
    {
      "title": "BSQ: Exploring Bit-Level Sparsity for Mixed-Precision Neural Network Quantization",
      "abstract": "Mixed-precision quantization can potentially achieve the optimal tradeoff\nbetween performance and compression rate of deep neural networks, and thus,\nhave been widely investigated. However, it lacks a systematic method to\ndetermine the exact quantization scheme. Previous methods either examine only a\nsmall manually-designed search space or utilize a cumbersome neural\narchitecture search to explore the vast search space. These approaches cannot\nlead to an optimal quantization scheme efficiently. This work proposes\nbit-level sparsity quantization (BSQ) to tackle the mixed-precision\nquantization from a new angle of inducing bit-level sparsity. We consider each\nbit of quantized weights as an independent trainable variable and introduce a\ndifferentiable bit-sparsity regularizer. BSQ can induce all-zero bits across a\ngroup of weight elements and realize the dynamic precision reduction, leading\nto a mixed-precision quantization scheme of the original model. Our method\nenables the exploration of the full mixed-precision space with a single\ngradient-based optimization process, with only one hyperparameter to tradeoff\nthe performance and compression. BSQ achieves both higher accuracy and higher\nbit reduction on various model architectures on the CIFAR-10 and ImageNet\ndatasets comparing to previous methods.",
      "full_text": "Published as a conference paper at ICLR 2021 BSQ: E XPLORING BIT-LEVEL SPARSITY FOR MIXED - PRECISION NEURAL NETWORK QUANTIZATION Huanrui Yang, Lin Duan, Yiran Chen & Hai Li Department of Electrical and Computer Engineering Duke University Durham, NC 27708, USA {huanrui.yang, lin.duan, yiran.chen, hai.li}@duke.edu ABSTRACT Mixed-precision quantization can potentially achieve the optimal tradeoff between performance and compression rate of deep neural networks, and thus, have been widely investigated. However, it lacks a systematic method to determine the exact quantization scheme. Previous methods either examine only a small manually- designed search space or utilize a cumbersome neural architecture search to explore the vast search space. These approaches cannot lead to an optimal quantization scheme efﬁciently. This work proposes bit-level sparsity quantization (BSQ) to tackle the mixed-precision quantization from a new angle of inducing bit-level sparsity. We consider each bit of quantized weights as an independent trainable variable and introduce a differentiable bit-sparsity regularizer. BSQ can induce all-zero bits across a group of weight elements and realize the dynamic precision reduction, leading to a mixed-precision quantization scheme of the original model. Our method enables the exploration of the full mixed-precision space with a single gradient-based optimization process, with only one hyperparameter to tradeoff the performance and compression. BSQ achieves both higher accuracy and higher bit reduction on various model architectures on the CIFAR-10 and ImageNet datasets comparing to previous methods. 1 I NTRODUCTION Numerous deep neural network (DNN) models have been designed to tackle real-world problems and achieved beyond-human performance. DNN models commonly demand extremely high computation cost and large memory consumption, making the deployment and real-time processing on embedded and edge devices difﬁcult (Han et al., 2015b; Wen et al., 2016). To address this challenge, model compression techniques, such as pruning (Han et al., 2015b; Wen et al., 2016; Yang et al., 2020), factorization (Jaderberg et al., 2014; Zhang et al., 2015) and ﬁxed-point quantization (Zhou et al., 2016; Wu et al., 2019; Dong et al., 2019), have been extensively studied. Among them, ﬁxed-point quantization works directly on the data representation by converting weight parameters originally in the 32-bit ﬂoating-point form to low-precision values in a ﬁxed-point format. For a DNN model, its quantized version requires much less memory for weight storage. Moreover, it can better utilize ﬁxed-point processing units in mobile and edge devices to run much faster and more efﬁciently. Typically, model compression techniques aim to reduce a DNN model size while maintaining its performance. The two optimization objectives in this tradeoff, however, have a contrary nature: the performance can be formulated as a differentiable loss functionL(W) w.r.t. the model’s weights W; yet the model size, typically measured by the number of non-zero parameters or operations, is a discrete functiondetermined mainly by the model architecture. To co-optimize the performance and model size, some previous pruning and factorization methods relax the representation of model size as a differentiable regularization term R(W). For example, group Lasso (Wen et al., 2016) and DeepHoyer (Yang et al., 2020) induce weight sparsity for pruning, and the attractive force regularizer (Wen et al., 2017) and nuclear norm (Xu et al., 2018) are utilized to induce low rank. The combined objective L(W) +αR(W) can be directly minimized with a gradient-based optimizer for optimizing the performance and model size simultaneously. Here, the hyperparameter αcontrols the strength of the regularization and governs the performance-size tradeoff of the compressed model. 1 arXiv:2102.10462v1  [cs.LG]  20 Feb 2021Published as a conference paper at ICLR 2021 Unlike for pruning and factorization, there lacks a well-deﬁned differentiable regularization term that can effectively induce quantization schemes. Early works in quantization mitigate the tradeoff exploration complexity by applying the same precision to the entire model. This line of research focuses on improving the accuracy of ultra low-precision DNN models, e.g., quantizing all the weights to 3 or less bits (Zhou et al., 2016; Zhang et al., 2018), even to 1-bit (Rastegari et al., 2016). These models commonly incur signiﬁcant accuracy loss, even after integrating emerging training techniques like straight-through estimator (Bengio et al., 2013; Zhou et al., 2016), dynamic range scaling (Polino et al., 2018) and non-linear trainable quantizers (Zhang et al., 2018). As different layers of a DNN model present different sensitivities with performance, a mixed-precision quantization scheme would be ideal for the performance-size tradeoff (Dong et al., 2019). There have also been accelerator designs to support the efﬁcient inference of mixed-precision DNN models (Sharma et al., 2018). However, to achieve the optimal layer-wise precision conﬁguration, it needs to exhaustively explore the aforementioned discrete search space, the size of which grows exponentially with the number of layers. Moreover, the dynamic change of each layer’s precision cannot be formulated into a differentiable objective, which hinders the efﬁciency of the design space exploration. Prior studies (Wu et al., 2019; Wang et al., 2019) utilize neural architecture search (NAS), which suffers from extremely high searching cost due to the large space of mixed-precision quantization scheme. Recently, Dong et al. (2019) propose to rank each layer based on the corresponding Hessian information and then determine the relative precision order of layers based on their ranking. The method, however, still requires to manually select the precision level for each layer. Here, we propose to revisit the ﬁxed-point quantization process from a new angle of bit-level sparsity: decreasing the precision of a ﬁxed-point number can be taken as forcing one or a few bits, most likely the least signiﬁcant bit (LSB), to be zero; and reducing the precision of a layer is equivalent to zeroing out a speciﬁc bit of all the weight parameters of the layer. In other words, the precision reduction can be viewed as increasing the layer-wise bit-level sparsity. By considering the bits of ﬁxed-point DNN parameters as continuous trainable variables during DNN training, we can utilize a sparsity-inducing regularizer to explore the bit-level sparsity with gradient-based optimization, dynamically reduce the layer precision and lead to a series of mixed-precision quantization schemes. More speciﬁc, we propose Bit-level Sparsity Quantization (BSQ) method with the following contributions: • We propose a gradient based training algorithm for bit-level quantized DNN models. The algorithm considers each bit of quantized weights as an independent trainable variable and enables the gradient-based optimization with straight-through estimator (STE). • We propose a bit-level group Lasso regularizerto dynamically reduce the weight precision of every layer and therefore induce mixed-precision quantization schemes. • BSQ uses only one hyperparameter, the strength of the regularizer, to trade-off the model performance and size, making the exploration more efﬁcient. This work exclusively focuses on layer-wise mixed-precision quantization, which is the granularity considered in most previous works. However, the ﬂexibility of BSQ enables it to explore mixed- precision quantization of any granularity with the same cost regardless of the search space size. 2 R ELATED WORKS ON DNN QUANTIZATION Quantization techniques convert ﬂoating-point weight parameters to low-precision ﬁxed-point repre- sentations. Directly quantizing a pre-trained model inevitably introduces signiﬁcant accuracy loss. So many of early research focus on how to ﬁnetune quantized models in low-precision conﬁgurations. As the quantized weights adopt discrete values, conventional gradient-based methods that are designed for continuous space cannot be directly used for training quantized models. To mitigate this problem, algorithms like DoReFa-Net utilize a straight-through estimator (STE) to approximate the quantized model training with trainable ﬂoating-point parameters (Zhou et al., 2016). As shown in Equation (1), a ﬂoating-point weight element wis kept throughout the entire training process. Along the forward pass, the STE will quantize wto n-bit ﬁxed-point representation wq, which will be used to compute the model output and loss L. During the backward pass, the STE will directly pass the gradient w.r.t. wq onto w, which enables wto be updated with the standard gradient-based optimizer. Forward: wq = 1 2n −1Round[(2n −1)w]; Backward: ∂L ∂w = ∂L ∂wq . (1) 2Published as a conference paper at ICLR 2021 Early studies revealed that weights of different layers have different dynamic ranges. It is important to keep the dynamic range of each layer for maintaining the model performance, especially for quantized models. He et al. (2016b) and Polino et al. (2018) propose to explicitly keep track of the dynamic range of each layer by scaling all the weight elements in a layer to the range of [0,1] at every training step, before applying the quantization STE. Other techniques, such as learnable nonlinear quantiﬁer function (Zhang et al., 2018) and incremental quantization (Zhou et al., 2017), are also useful in improving the performance of quantized models. However, it is still very difﬁcult to quantize the entire DNN model to a uniﬁed ultra-low precision without incurring signiﬁcant accuracy loss. Recent research shows that different layers in a DNN model contribute to the overall performance in varying extents. Therefore mixed-precision quantization scheme that assigns different precision to layers (Wu et al., 2019; Dong et al., 2019) presents a better accuracy-compression tradeoff. The challenge lies in how to determine the quantization scheme, i.e., the precision of each layer, as it needs to explore a large and discrete search space. Some works design quantization criteria based on concepts like “noise gain” (Sakr & Shanbhag, 2018; 2019) to constraint the relationship between each layer’s precision and thus largely reduce the search space, yet those criteria are often heuristic, preventing these methods to reach ultra-low precision and ﬁnd the optimal tradeoff point between model size and accuracy. Other works utilize neural architecture search (NAS). For example, Wang et al. (2019) consider the precision assignment of each layer as an action and seek for the optimal design policy via reinforcement learning. Wu et al. (2019) combine all possible design choices into a “stochastic super net” and approximate the optimal scheme via sampling. However, the cost of NAS methods scales up quickly as the quantization search space grows exponentially with the number of layers. Common practices of constraining the search cost include limiting the precision choices or designing the quantization scheme in a coarse granularity. A recent line of research work attempts to rank layers based on their importance measured by the sensitivity or Hessian information. Higher precision will then be assigned to more important layers (Dong et al., 2019). The exact precision of each layer, however, needs to be manually selected. So these methods cannot adequately explore the whole search space for the optimal quantization scheme. 3 T HE BSQ M ETHOD BSQ aims to obtain an optimal mixed-precision quantization scheme through a single-pass training process of a quantized model. In this section, we ﬁrst introduce how to convert a DNN model to the bit representation and propose a gradient-based algorithm for training the resulted bit-level model. A bit-level group Lasso regularizer is then proposed to induce precision reduction. In the end, we elaborate the overall training objective of BSQ and the dynamical precision adjustment procedure. 3.1 T RAINING THE BIT REPRESENTATION OF DNN As illustrated in Figure 1(a), we convert a ﬂoating-point weight matrix W of a pretrained network to its bit representation through a pipeline of scaling, quantization and binary conversion. Similar to the practice in (He et al., 2016b; Polino et al., 2018), we retain the dynamic range of W by scaling all the elements to the range of [0,1] before applying quantization. However, these prior works always scale the largest element to 1 to fully utilize all the quantized bins at every training step, which makes the dynamic precision reduction impossible. Instead, our method conducts the scaling only Figure 1: An example of DNN training under the bit representation with precision n= 3. (a) Pipeline of converting from the ﬂoating-point weight W to the bit representation; (b) Training the bit-level model weight with STE. 3Published as a conference paper at ICLR 2021 once, which is right before the bit representation training. Formally, before convertingW to its bit representation, we ﬁrst extract its dynamic range as W = s·Ws, where s= max|W|is the scaling factor and Ws is the scaled weight matrix. The absolute value of any element ws in Ws is within the range of [0,1]. Now we apply an n-bit uniform quantization to the absolute value of ws such as wq = Round[|ws|×(2n −1)]/(2n −1). Then wq can be exactly represented by a n-bit binary number as wq = [∑n−1 b=0 w(b) s 2b]/(2n −1), where w(b) s denotes the bth bit in the binary representation. Till this point, W in the ﬂoating-point form is replaced with W ≡sign(W) ⊙sWq ≡sign(W) ⊙ s 2n −1 n−1∑ b=0 W(b) s 2b, (2) where ⊙denotes the element-wise Hadamard product. We consider the bit representationW(b) s where b∈[0,n −1] and the scaling factor sas independent trainable variables in the training process. Note that W(b) s is composed of binary values by deﬁnition and sign(W) is a discrete function. Neither of them can be directly trained with gradient descent. To mitigate the binary constraint of W(b) s , we adopt the STE proposed by Bengio et al. (2013) during the training process. As shown in Equation (1), STE enables a quantized model to be trained with continuous ﬂoating-point weights. Speciﬁcally, the STE for the bit representation training is deﬁned as: Forward: Wq = 1 2n −1Round [n−1∑ b=0 W(b) s 2b ] ; Backward: ∂L ∂W(b) s = 2b 2n −1 ∂L ∂Wq . (3) STE relaxes the binary constraint and allows gradient updates for the elements inW(b) s . As illustrated in Figure 1(b), during the forward pass, s·Wq will be used to reconstruct the model weight W and compute the loss, which demonstrates the performance of the current model after quantization. The gradient w.r.t. Wq from the back-propagation will be passed through the rounding function and updated on the continuous values of W(b) s . The proposed bit representation can therefore be trained with any gradient-based optimizer. The proposed bit representation training only leads to minimal computational and run-time memory overhead comparing to the normal back propagation procedure. From the memory consumption perspective, the bit representation training treats each bit as separated ﬂoating-point trainable variables, so a N-bit model in bit representation will have N times more parameters and gradients to be stored comparing to that of the baseline training. Though for actual run-time memory consumption, the hidden feature between each layer consumes a signiﬁcantly larger memory than weights and gradients. As the bit representation does not affect the hidden features, the increase in trainable variables does not lead to signiﬁcant increase in run-time memory consumption. From the perspective of computation cost, note that the gradient w.r.t. each W(b) s can be computed as the gradient w.r.t. the corresponding Wq scaled by a power of 2. So under a N-bit scheme there will only be N additional scaling for each parameter comparing to the normal training. These additional computations are very cheap comparing to the ﬂoating-point operations involved in back propagation. So the proposed bit representation training only leads to minimal computational overhead comparing to a normal back propagation. We restrict the value of W(b) s within [0,2] throughout the training, so that the corresponding Wq has the chance to increase or decrease its precision in the “precision adjustment” step, which will be discussed in Section 3.3. This is enforced by trimming W(b) s to 0 or 2 if it exceeds the range after a training step. To enable the dynamic update of sign(W) during training, we separate the positive and negative elements in Ws as Ws = (Wp −Wn) before quantization. Here Wp = Ws ⊙1 (Ws ≥0) contains all the positive elements and Wn = −Ws ⊙1 (Ws <0) includes the absolute value of all the negative weight elements. Wp and Wn will be respectively converted to W(b) p and W(b) n by following the process in Equation (2), so that W(b) s = W(b) p −W(b) n . Note that the replacement of W(b) s with W(b) p −W(b) n does not introduce any non-differentiable function. Therefore all elements in W(b) p and W(b) n can take continuous values between [0,2] and be trained with the bit representation STE in Equation (3). As such, the original weight matrix W is converted into trainable variables W(b) p , W(b) n and sthroughout the BSQ training process. 4Published as a conference paper at ICLR 2021 3.2 B IT-LEVEL GROUP LASSO To induce the mixed-precision quantization scheme of a DNN model during training, we propose a bit-level group Lasso (BGL) regularizer based on the group Lasso (Hastie et al., 2015) and apply it to W(b) p and W(b) n that are converted from a group of weights Wg. The regularizer is deﬁned as: BGL(Wg) = n−1∑ b=0  [ W(b) p ; W(b) n ] 2 , (4) where W(b) p and W(b) n are bit representations converted from Wg, and [·; ·] denotes the concatenation of matrices. BGL could make a certain bit bof all elements in both W(b) p and W(b) n zero simultane- ously. The bit can thus be safely removed for the precision reduction. Note that the granularity of the quantization scheme induced by BGL is determined by how Wg is grouped. Our experiments organize Wg in a layer-wise fashion. So all elements in a layer have the same precision, which is a common setting in previous mixed-precision quantization work. Wg can also be arranged as any group of weight elements, such as block-wise, ﬁlter-wise or even element-wise if needed. Accordingly, the formulation of the regularizer need to be revised to assist the exploration of the mixed-precision quantization at the given granularity. The cost for evaluating and optimizing the regularizer will remain the same under different granularity settings. 3.3 O VERALL TRAINING PROCESS The overall training process starts with converting each layer of a pretrained ﬂoating-point model to the bit representation with a relatively high initial precision (e.g., 8-bit ﬁxed-point). BSQ training is then preformed on the achieved bit representation with bit-level group Lasso integrated into the training objective. Re-quantization steps are conducted periodically to identify the bit-level sparsity induced by the regularizer and allow dynamic precision adjustment. As the mixed-precision quantization scheme is ﬁnalized, the achieved model is further ﬁnetuned for a higher accuracy. Objective of BSQ training. For higher memory efﬁciency it is desired to ﬁnd a mixed-precision quantization scheme that minimizes the total number of bits in the model. Thus, in BSQ training we propose to penalize more on the layers with more bits by performing a memory consumption-aware reweighing to BGL across layers. Speciﬁcally, the overall objective of training a L-layer DNN model with BSQ is formulated as: L= LCE (W(1:L) q ) +α L∑ l=1 #Para(Wl) ×#Bit(Wl) #Para(W(1:L)) BGL(Wl). (5) Here LCE (W(1:L) q ) is the original cross entropy loss evaluated with the quantized weightWq acquired from the STE in Equation (3), αis a hyperparameter controlling the regularization strength, and #Para(Wl) and #Bit(Wl) respectively denote the parameter number and precision of layer l. The loss function in Equation (5) enables a layer-wise adjustment in the regularization strength by applying a stronger regularization on a layer with higher memory usage. Re-quantization and precision adjustment. As BSQ trains the bit representation of the model with ﬂoating-point variables, we perform re-quantization to convert W(b) p and W(b) n to exact bi- nary values and identify the all-zero bits that can be removed for precision reduction. The re- quantization step reconstructs the quantized scaled weight W′ q from W(b) p and W(b) n as: W′ q = Round [∑n−1 b=0 W(b) p 2b −∑n−1 b=0 W(b) n 2b ] .As we allow the values of W(b) p and W(b) n to be within [0,2], the reconstructed W′ q has a maximum absolute value of 2n+1. In this way, W′ q is converted to a (n+ 1)-bit binary number, where each bit is denoted by W(b) q . After the re-quantization, we will adjust the precision of each layer. Speciﬁcally, we ﬁrst check W(b) q from the MSB down to the LSB and remove the bits with all zero elements until the ﬁrst non-zero bit. The scaling factor sof the layer remains unchanged during this process. A similar check will then be conducted from the LSB up to the MSB. sneeds to be doubled when a bit from the LSB side is removed, as all elements in W′ q are shifted right for one bit. Assume that the precision adjustment makes the precision of a layer change from nto n′, the scaling factor will be updated as s′= s2n′ −1 2n−1 . In this way, the bit representations 5Published as a conference paper at ICLR 2021 Figure 2: Quantization schemes achieved with or without layer-wise regularization reweigh- ing. The compression rate and the accuracy after ﬁnetuning are listed in the legend. of W before and after the precision adjustment are equivalent, as indicated in Equation (6). The precision of a n-bit layer may change between 0 and (n+ 1)-bit after the precision adjustment. W ≡ s 2n −1 n−1∑ b=0 W(b) q 2b ≡ s′ 2n′ −1 n′−1∑ b=0 W(b) q 2b. (6) As formulated in Equation (5), the regularization strength assigned to each layer will change with the quantization scheme of the model. The re-quantization and precision adjustment step will be performed periodically during the training process, with an interval of several training epochs. After each precision adjustment, we separate the positive elements and negative elements inW′ q to form the new W(b) p and W(b) n , respectively. The training can then resume with the newly adjusted W(b) p and W(b) n and scaling factor s′. It is worth mentioning that sWq from the forward pass STE remains unchanged before and after the re-quantization and precision adjustment, so the model performance and the gradient from the loss LCE will not be affected. The interval between re-quantizations needs to be carefully chosen: it shall promptly and properly adjust the regularization strength for stable convergence. The ablation study on re-quantization interval selection is presented in Appendix B.1. Activation quantization. Since BSQ modiﬁes only the precision of weights but not affecting the precision of activations, we predetermine the activation precision and ﬁx it throughout the BSQ training process. The activations are quantized in the same way as proposed by Polino et al. (2018). For training stability, we use ReLU-6 activation function for layers with 4-bit or above activations, and use PACT (Choi et al., 2018) for layers with a lower activation precision. Post-training ﬁnetuning. At the end of the BSQ training, we perform a ﬁnal re-quantization and precision adjustment to get the ﬁnal mixed-quantization scheme. The achieved model can be further ﬁnetuned under the obtained precision for improving the overall accuracy. As the quantization scheme is ﬁxed, we adopt the quantization-aware training method proposed by Polino et al. (2018) for ﬁnetuning in our experiment. 4 A BLATION STUDY We perform the ablation studies on key design choices of the BSQ algorithm. This section presents the effectiveness of layer-wise memory consumption-aware regularization reweighing and the model size-accuracy tradeoff under different regularization strengths. All experiments are conducted with ResNet-20 models (He et al., 2016a) with 4-bit activation on the CIFAR-10 dataset (Krizhevsky & Hinton, 2009). Detailed experiment setup and hyperparameter choices can be found in Appendix A. 4.1 E FFECT OF LAYER -WISE REGULARIZATION REWEIGHING As stated in Equation (5), we propose to apply layer-wise memory consumption-aware reweighing on the BGL regularizer to penalize more on larger layers during the BSQ training. Figure 2 compares the quantization scheme and the model performance achieved when performing the BSQ training with or without such a reweighing term. Here we set the regularization strength αto 5e-3 when training with the reweighing, and to 2e-3 when training without the reweighing to achieve comparable compression rates. All the other hyperparameters are kept the same. As shown in the ﬁgure, training without 6Published as a conference paper at ICLR 2021 Figure 3: Layer-wise precision comparison of the quantization schemes achieved under different regularization strengths. Table 1: Accuracy-#Bits tradeoff under different regularization strengths. “FT” stands for ﬁnetuning. The last row is achieved by training with quantization schemes achieved by BSQ from scratch. Strength α 3e-3 5e-3 7e-3 1e-2 2e-2 #Bits per Para / Comp (×) 3.02 / 10.60 2.25 / 14.24 1.66 / 19.24 1.37 / 23.44 0.87 / 36.63 BSQ acc before / after FT (%) 91.30 / 92.60 90.98 / 92.32 90.42 / 91.48 90.35 / 91.16 85.77 / 89.49 Train from scratch acc (%) 91.72 91.45 91.12 89.57 89.14 the reweighing term will lead to over-penalization on earlier layers with fewer parameters, while later layers with more parameters are not compressed enough. Therefore the achieved quantized model will have less accuracy even with a smaller compression rate comparing to the model achieved with layer-wise regularization reweighing. As we only show one pair of comparison here, the difference between BSQ training with or without the reweighing term is consistent when varying the regularization strength α. Additional results with other αvalues are shown in Appendix B.2. 4.2 A CCURACY -#B ITS TRADEOFF UNDER DIFFERENT REGULARIZATION STRENGTHS We ﬁx all the other hyperparameters while varying only the regularization strength α from 3e- 3 to 2e-2, to control the tradeoff between the model size and accuracy achieved by BSQ. The quantization schemes achieved by running BSQ with different α’s are shown in Figure 3, and the detailed comparison on the compression rate comparing to the 32-bit ﬂoating point model (denoted as “Comp”) and the validation accuracy (denoted as “Acc”) is summarized in Table 1. As shown in Figure 3, the relative ranking of the precision assignment is mostly consistent under different α’s, which is consistent with the previous observation that more important layers should be assigned with higher precision. This effect is further illustrated in Appendix B.3, where we compare the quantization scheme achieved by BSQ with the layer importance measured in HAWQ (Dong et al., 2019). Furthermore, as α increases, the overall bit reduction increases with the cost of a small performance loss. This tradeoff is also observed on models trained with 2-bit or 3-bit activation as we show their quantization schemes and performances in Appendix B.4. Note that some layers achieve 0-bit precision under large regularization strength, indicating all the weights become zero and the layer can be skipped. This is possible as the shortcut connection existing in the ResNet architecture enables the pass of information even if the weights are all zero in some layers. We also note that BSQ not only ﬁnds the desired mixed-precision quantization scheme, but also provides a model with higher performance under the same quantization scheme. As shown in Table 1, when training a model with the same quantization scheme as achieved by BSQ using the DoReFa-Net algorithm (Zhou et al., 2016) from scratch, the resulted accuracy is always lower than the BSQ model after ﬁnetuning. 5 E XPERIMENTAL RESULTS In this section we compare BSQ with previous state-of-the-art methods. Here, ResNet-20 models are used for the comparison on the CIFAR-10 dataset, and ResNet-50 and Inception-V3 models (Szegedy et al., 2016) are utilized for the experiments on the ImageNet dataset (Russakovsky et al., 2015). The hyperparameters used for BSQ training and ﬁnetuning are listed in Appendix A. All the compression 7Published as a conference paper at ICLR 2021 Table 2: Quantization results of ResNet-20 models on the CIFAR-10 dataset. BSQ is compared with DoReFa-Net (Zhou et al., 2016), PACT (Choi et al., 2018), LQ-Net (Zhang et al., 2018), DNAS (Wu et al., 2019) and HAWQ (Dong et al., 2019). “MP” denotes mixed-precision quantization. Benchmarks BSQ Act. Prec. Method Weight Prec. Comp ( ×) Acc (%) α Comp (×) Acc (%) 32-bit Baseline 32 1.00 92.62 LQ-Nets 3 10.67 92.00 5e-3 14.24 92.77 DNAS MP 11.60 92.72 7e-3 19.24 91.87 LQ-Nets 2 16.00 91.80 4-bit HAWQ MP 13.11 92.22 5e-3 14.24 92.32 3-bit LQ-Nets 3 10.67 91.60 2e-3 11.04 92.16 PACT 3 10.67 91.10 5e-3 16.37 91.72 DoReFa 3 10.67 89.90 2-bit LQ-Nets 2 16.00 90.20 PACT 2 16.00 89.70 5e-3 18.85 90.19 DoReFa 2 16.00 88.20 Table 3: Quantization results of ResNet-50 and Inception-V3 models on the ImageNet dataset. BSQ is compared with DoReFa-Net (Zhou et al., 2016), PACT (Choi et al., 2018), LSQ (Esser et al., 2019), LQ-Net (Zhang et al., 2018), Deep Compression (DC) (Han et al., 2015a), Integer (Jacob et al., 2018), RVQ (Park et al., 2018), HAQ (Wang et al., 2019) and HAWQ (Dong et al., 2019). ResNet-50 Inception-V3 Method Prec. Comp ( ×) Top1 (%) Method Prec. Comp ( ×) Top1 (%) Baseline 32 1.00 76.13 Baseline 32 1.00 77.21 DoReFa 3 10.67 69.90 Integer 8 4.00 75.40 PACT 3 10.67 75.30 Integer 7 4.57 75.00 LQ-Nets 3 10.67 74.20 RVQ MP 10.67 74.14 DC 3 10.41 75.10 HAWQ MP 12.04 75.52 HAQ MP 10.57 75.30 LSQ 3 10.67 75.80 BSQ 5e-3 MP 11.90 75.29 BSQ 1e-2 MP 11.38 76.60 BSQ 7e-3 MP 13.90 75.16 BSQ 2e-2 MP 12.89 75.90 rates reported in Table 2 and Table 3 are compared to the 32-bit ﬂoating point model, and all the accuracy reported is the testing accuracy evaluated on models after ﬁnetuning. Table 2 reports the quantization results of ResNet-20 models on the CIFAR-10 dataset. Here we set the activation of the ﬁrst convolutional layer and the ﬁnal FC layer to 8 bits while all the other activations to 4, 3 or 2 bits respectively to match the settings of previous methods. The reported 32-bit activation model performance is achieved by ﬁnetuning the 4-bit activation model under full precision activation. The exact BSQ quantization schemes of the 4-bit activation models are listed in Figure 3, while those of the 2-bit and 3-bit activation models can be found in Appendix B.4. Comparing to previous mixed-precision quantization methods, the model obtained by BSQ with 4-bit activation and α= 5e-3 has slightly higher accuracy as the model achieved by HAWQ (Dong et al., 2019), but a higher compression rate (14.24×vs. 13.11×). The same model with 32-bit activation obtains 23% more compression rate with the same accuracy as the model found by DNAS (Wu et al., 2019), with a much less training cost as our method does not involve the costly neural architecture search. The advantage of BSQ is even larger comparing to single-precision quantization methods (Zhou et al., 2016; Choi et al., 2018; Zhang et al., 2018), as BSQ achieves both higher compression rate and higher accuracy comparing to all methods with the same activation precision. The results of BSQ and previous quantization methods on the ImageNet dataset are summarized in Table 3. The exact BSQ quantization schemes can be found in Appendix C. For ResNet models, the activation of the ﬁrst and the ﬁnal layer are set to 8 bits while all the other activations are set to 4 8Published as a conference paper at ICLR 2021 bits. For Inception-V3 models the activation of all the layers are set to 6 bits. On ResNet-50 models, BSQ with α = 5e-3 achieves the same top-1 accuracy as PACT (Choi et al., 2018) and 0.5% less top-1 accuracy as the best available method LSQ (Esser et al., 2019) with a higher compression rate (11.90×vs. 10.67×), showing competitive accuracy-compression tradeoff. BSQ can further increase the compression rate of ResNet-50 to 13.90×with α= 7e-3, with only 0.13% top-1 accuracy loss over the “5e-3” model. On Inception-V3 models, BSQ with α= 2e-2 achieves both higher accuracy (75.90% vs. 75.52%) and higher compression rate (12.89×vs 12.04×) comparing to the best previous method HAWQ (Dong et al., 2019). Adopting a smaller α = 1e-2 makes BSQ to achieve 0.7% accuracy improvement trading off ∼10% less compression rate comparing to the “2e-2” model. 6 C ONCLUSIONS In this work, we propose BSQ, which fully explores the accuracy-model size tradeoff of DNN’s mixed-precision quantization schemes with a differentiable training algorithm using DNN’s bit representation as trainable variables. A bit-level group Lasso regularizer with memory consumption- aware layer-wise reweighing is applied to induce bit-level sparsity, which leads to the dynamic adjustment of each layer’s precision and ﬁnally a mixed-precision quantization scheme through a single-pass gradient-based training process. This enables BSQ to dynamically produce a series of quantization schemes trading off accuracy and model size and provides models with higher accuracy comparing to training from scratch under the same quantization scheme. We apply BSQ in training ResNet-20 models on the CIFAR-10 dataset and training ResNet-50 and Inception-V3 models on the ImageNet dataset. In all the experiments, BSQ demonstrates the ability to reach both a better accuracy and a higher compression rate comparing to previous quantization methods. Our results prove that BSQ can successfully ﬁll in the gap of inducing a mixed-precision quantization scheme with a differentiable regularizer, so as to effectively explore the tradeoff between accuracy and compression rate for ﬁnding DNN models with both higher accuracy and fewer bits. ACKNOWLEDGMENTS This work is supported in part by NSF CCF-1910299 and NSF CNS-1822085. REFERENCES Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013. Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan. Pact: Parameterized clipping activation for quantized neural networks. arXiv preprint arXiv:1805.06085, 2018. Zhen Dong, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Hawq: Hessian aware quantization of neural networks with mixed-precision. In Proceedings of the IEEE International Conference on Computer Vision, pp. 293–302, 2019. Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra S Modha. Learned step size quantization. arXiv preprint arXiv:1902.08153, 2019. Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015a. Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efﬁcient neural network. In Advances in neural information processing systems, pp. 1135–1143, 2015b. Trevor Hastie, Robert Tibshirani, and Martin Wainwright. Statistical learning with sparsity: the lasso and generalizations. CRC press, 2015. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016a. Qinyao He, He Wen, Shuchang Zhou, Yuxin Wu, Cong Yao, Xinyu Zhou, and Yuheng Zou. Effective quantization methods for recurrent neural networks. arXiv preprint arXiv:1611.10176, 2016b. 9Published as a conference paper at ICLR 2021 Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efﬁcient integer-arithmetic- only inference. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2704–2713, 2018. Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks with low rank expansions. arXiv preprint arXiv:1405.3866, 2014. Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009. Eunhyeok Park, Sungjoo Yoo, and Peter Vajda. Value-aware quantization for training and inference of neural networks. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 580–595, 2018. A. Polino, R. Pascanu, and D. Alistarh. Model compression via distillation and quantization. arXiv preprint arXiv:1802.05668, 2018. Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classiﬁcation using binary convolutional neural networks. In European conference on computer vision, pp. 525–542. Springer, 2016. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211–252, 2015. doi: 10.1007/s11263-015-0816-y. Charbel Sakr and Naresh Shanbhag. An analytical method to determine minimum per-layer precision of deep neural networks. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1090–1094. IEEE, 2018. Charbel Sakr and Naresh Shanbhag. Per-tensor ﬁxed-point quantization of the back-propagation algorithm. In International Conference on Learning Representations, 2019. URL https://openreview.net/ forum?id=rkxaNjA9Ym. Hardik Sharma, Jongse Park, Naveen Suda, Liangzhen Lai, Benson Chau, Vikas Chandra, and Hadi Esmaeilzadeh. Bit fusion: Bit-level dynamically composable architecture for accelerating deep neural network. In 2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA), pp. 764–775. IEEE, 2018. Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In International conference on machine learning, pp. 1139–1147, 2013. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2818–2826, 2016. Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han. Haq: Hardware-aware automated quantization with mixed precision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 8612–8620, 2019. Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neural networks. In Advances in neural information processing systems, pp. 2074–2082, 2016. Wei Wen, Cong Xu, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Coordinating ﬁlters for faster deep neural networks. In Proceedings of the IEEE International Conference on Computer Vision, pp. 658–666, 2017. Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, and Kurt Keutzer. Fbnet: Hardware-aware efﬁcient convnet design via differentiable neural architecture search. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 10734–10742, 2019. Yuhui Xu, Yuxi Li, Shuai Zhang, Wei Wen, Botao Wang, Yingyong Qi, Yiran Chen, Weiyao Lin, and Hongkai Xiong. Trained rank pruning for efﬁcient deep neural networks. arXiv preprint arXiv:1812.02402, 2018. Huanrui Yang, Wei Wen, and Hai Li. Deephoyer: Learning sparser neural network with differentiable scale- invariant sparsity measures. In International Conference on Learning Representations, 2020. URL https: //openreview.net/forum?id=rylBK34FDS. 10Published as a conference paper at ICLR 2021 Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua. Lq-nets: Learned quantization for highly accurate and compact deep neural networks. In Proceedings of the European conference on computer vision (ECCV), pp. 365–382, 2018. Xiangyu Zhang, Jianhua Zou, Kaiming He, and Jian Sun. Accelerating very deep convolutional networks for classiﬁcation and detection. IEEE transactions on pattern analysis and machine intelligence, 38(10): 1943–1955, 2015. Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen. Incremental network quantization: Towards lossless cnns with low-precision weights. arXiv preprint arXiv:1702.03044, 2017. Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016. 11Published as a conference paper at ICLR 2021 A H YPERPARAMETER CHOICES IN THE EXPERIMENTS A.1 CIFAR-10 EXPERIMENTS We use ResNet-20 models on the CIFAR-10 dataset (Krizhevsky & Hinton, 2009) to do all of our ablation studies and evaluate the performance of BSQ. The CIFAR-10 dataset can be directly accessed through the dataset API provided in the “torchvision” python package. We do not change the splitting between the training and the test set. Standard preprocessing procedures, including random crop with a padding of 4, random horizontal ﬂip and normalization, are used on the training set to train the model. The validation set is normalized with the same mean and variance as the training set. We implemented ResNet-20 models following the description in (He et al., 2016a), and pretrain the model for 350 epochs. The learning rate is set to 0.1 initially, and decayed by 0.1 at epoch 150, 250 and 325. The weights of all the layers except the batch normalization are then quantized to 8-bit before the BSQ training. The batch normalization layers are kept in the ﬂoating-point format throughout the training process. Similar to previous quantization works, we also apply the activation quantization during the training. For 4-bit or above activation precision we replace all the ReLU activation function in the model with the ReLU6 activation function. For lower activation precision we use the trainable PACT activation (Choi et al., 2018) with weight decay 0.0001. These changes will help achieving higher accuracy and better training stability when the activation is quantized as it eliminates extremely large activation values. As BSQ does not consider activation quantization as an objective, we ﬁx the activation precision throughout the BSQ training and the ﬁnetuning process. We start the BSQ training with the 8-bit quantized pretrained model following the process described in Section 3.3. The BSQ training is done for 350 epochs, with the ﬁrst 250 epochs using learning rate 0.1 and the rest using learning rate 0.01. Unless otherwise speciﬁed, the re-quantization and precision adjustment is done every 100 epochs, as well as after the BSQ training is ﬁnished to adjust and ﬁnalize the quantization scheme. Different regularization strengths αare tried to explore the tradeoff between accuracy and compression rate. The exact αused for each set of experiment is reported alongside the results in the main article. For comparing with previous methods, we further ﬁnetune the achieved mixed-precision model with the DoReFa-Net algorithm (Zhou et al., 2016) while ﬁxing the quantization scheme. The ﬁnetuning is performed for 300 epochs with an initial learning rate 0.01 and the learning rate decay by 0.1 at epoch 150 and 250. The “train from scratch” accuracy reported in Table 1 is achieved by ﬁrst quantizing a pretrained ﬂoating-point model to the mixed precision quantization scheme achieved by BSQ, then performing DoReFa-Net quantization aware training on the model. The training is done for 350 epochs, with an initial learning rate 0.1 and the learning rate decay by 0.1 at epoch 150, 250 and 325. All the training tasks are optimized with the SGD optimizer (Sutskever et al., 2013) with momentum 0.9 and weight decay 0.0001, and the batch size is set to 128. All the training processes are done on a single TITAN XP GPU. A.2 I MAGE NET EXPERIMENTS The ImageNet dataset is used to further compare BSQ with previous methods in Table 3. The ImageNet dataset is a large-scale color-image dataset containing 1.2 million images of 1,000 categories (Russakovsky et al., 2015), which has long been utilized as an important bench- mark on image classiﬁcation problems. In this paper, we use the “ILSVRC2012” version of the dataset, which can be found at http://www.image-net.org/challenges/LSVRC/ 2012/nonpub-downloads. We use all the data in the provided training set to train our model, and use the provided validation set to evaluate our model and report the testing accuracy. We follow the data reading and preprocessing pipeline suggested by the ofﬁcial PyTorch ImageNet example. For training images, we ﬁrst perform the random sized crop on the training images with the desired input size, then apply random horizontal ﬂipping and ﬁnally normalize them before feeding them into the network. We use an input size of 224 ×224 for experiments on the ResNet-50, and use an size of 299 ×299 for the Inception-V3 experiments. Validation images are resized then center cropped to the desired input size and normalized before used for testing. For both the ResNet-50 and the Inception-V3 model, the model architecture and the pretrained model provided in the “torchvision” package are directly utilized. The ﬁrst convolutional layer of the ResNet-50 model and the ﬁrst 5 conventional layers of the Inception-V3 model are quantized to 8-bit, while all the other layers are quantized to 6-bits before the BSQ training. Similar to the CIFAR-10 experiments, the batch normalization layers are kept as ﬂoating-point. 12Published as a conference paper at ICLR 2021 Figure 4: Range of testing accuracy and bit reduction rate achieved from 5 repeated runs with different random seeds. Solid line links the average performance, error bar marks the maximal and minimal performance achieved with each set of hyperparameters. We start the BSQ training with the quantized pretrained model. For both ResNet-50 and Inception-V3 models, the BSQ training is done for 90 epochs, with the ﬁrst 30 epochs using learning rate 0.01 and the rest using learning rate 0.001. The re-quantization interval is set to 10 epochs for all the ImageNet experiments. The regularization strength αused is reported alongside the results in Table 3. The model after BSQ training is further ﬁnetuned with DoReFa-Net for 90 epochs, with the initial learning rate 0.001 and a learning rate decay by 0.1 after 30 epochs. All the models are optimized with the SGD optimizer with momentum 0.9 and weight decay 0.0001, and the batch size is set as 256 for all the experiments. Two TITAN RTX GPUs are used in parallel for the BSQ training and ﬁnetuning of both ResNet-50 and Inception-V3 models. B A DDITIONAL ABLATION STUDY RESULTS B.1 C HOICE OF RE -QUANTIZATION INTERVAL We propose the layer-wise regularization reweighing in Section 3.3 and show its importance in Section 4.1. This reweighing can be more effective if we adjust the precision of each layer reg- ularly throughout the BSQ training routine. The precision adjustment is done through periodic re-quantization. From the one hand, a smaller re-quantization interval would help the precision to be adjusted in-time. From the other hand, it may cause the training unstable due to the frequent change in bit representation and regularizer values. So here we gradually increase the re-quantization interval to ﬁnd the best choice that can reach high and stable performance. Figure 4 demonstrates the stability and performance under re-quantization intervals 20, 50, 100 and compare them with the performance achieved without re-quantization during the training. Each point in the ﬁgure corresponds to the averaged compression rate and accuracy after 5-time repeated BSQ training with a ﬁxed regularization strength αbut with different random seeds. The observation in the ﬁgure supports our analysis that as re-quantization is important to reach a better accuracy-# bits tradeoff, applying it too frequent will make the training unstable and hinders the overall performance. Comparing to not performing the re-quantization and applying it every 20 epochs, re-quantizing every 50 or 100 epochs yields similarly better tradeoff between accuracy and compression rate. Re-quantization interval 100 leads to a higher accuracy in a wider range of compression rate comparing to the Int 50 model, and the performance is more stable throughout the repeated trails. Therefore in all the other CIFAR-10 experiments we set the re-quantization interval to 100 epochs. B.2 A DDITIONAL RESULTS ON REGULARIZATION REWEIGHING Figure 5 and Figure 6 compares the quantization scheme and the model performance achieved when performing the BSQ training with or without the memory consumption-aware reweighing of the bit-level group Lasso regularizer under additional choices of regularization strengthα. The αused for each set of experiment are chosen so that comparable compression rates are achieved with or without reweighing. The αused are listed in the caption of the ﬁgures. All the other hyperparameters are kept the same. From both ﬁgures we can observe a consistent trend that training without the reweighing 13Published as a conference paper at ICLR 2021 Figure 5: Quantization schemes achieved with or without layer-wise regularization reweigh- ing. The compression rate and the accuracy after ﬁnetuning are listed in the legend. α=6e-3 with reweighing and α=3e-3 without reweighing. Figure 6: Quantization schemes achieved with or without layer-wise regularization reweigh- ing. The compression rate and the accuracy after ﬁnetuning are listed in the legend. α=0.015 with reweighing and α=5e-3 without reweighing. term will lead to less precision assigned to earlier layers with fewer parameters, while later layers with more parameters are not compressed enough. Therefore the achieved quantized model will have less accuracy and smaller compression rate comparing to the model achieved with layer-wise regularization reweighing. This observation is consistent with the results shown in Section 4.1. All these results show that the memory consumption-aware reweighing proposed in BSQ training is crucial for generating models with both higher compression rate and higher accuracy. B.3 Q UANTIZATION SCHEME COMPARISON WITH HAWQ As discussed in Section 4.2 and shown in Figure 3, for the same model architecture the relative ranking of the precision assignment by BSQ is mostly consistent under different α’s. Here we compare quantization schemes achieved by BSQ with the “layer importance ranking” measured in HAWQ (Dong et al., 2019) to further analyze this consistency. HAWQ proposes to rank all the layers in the model with an importance score Si = λi/ni, where λi denotes the top eigenvalue of the Hessian matrix of layer i, and ni represents the number of parameters in layer i. A layer with a higher Si will be assigned with higher precision in the mixed-precision quantization scheme. The quantization schemes achieved by BSQ and HAWQ are compared in Figure 7, where the black dotted Figure 7: Layer-wise precision comparison between the quantization schemes achieved with BSQ and the scheme achieved with HAWQ (Dong et al., 2019) on the ResNet-20 model. 14Published as a conference paper at ICLR 2021 Figure 8: Layer-wise precision comparison of the quantization schemes achieved under different regularization strengths with 2-bit activation. Table 4: Accuracy-#Bits tradeoff with 2-bit activation. “FT” stands for ﬁnetuning. Strength α 1e-3 2e-3 3e-3 5e-3 #Bits per Para / Comp (×) 3.77 / 8.48 2.86 / 11.20 2.26 / 14.13 1.70 / 18.85 BSQ acc before / after FT (%) 91.03 / 91.21 90.19 / 90.70 89.54 / 90.39 88.13 / 90.19 line shows the HAWQ scheme while the color solid lines demonstrate the schemes achieved by BSQ under different α. It can be observed that the relative ranking of BSQ’s precision is consistent with the ranking of precision in HAWQ, which to some extent shows that BSQ can dynamically identify the important layers during the training and assign higher precision to them. Note that HAWQ can only come up with the precision ranking of each layer, while the exact precision is designed manually. BSQ on the other hand is able to explicitly assign the precision to each layer during a single training process, and can dynamically tradeoff model size and accuracy by only changing α. Thus BSQ can easily ﬁnd better tradeoff points with both higher accuracy and higher compression rate comparing to HAWQ and other quantization methods, as discussed in Section 5. B.4 M ODELS ACHIEVED UNDER DIFFERENT ACTIVATION PRECISION As we discuss the BSQ quantization schemes and model performances under 4-bit activation in Figure 3 and Table 1, here we show the tradeoff between model size and accuracy under different regularization strength αwith 2-bit activation in Figure 8 and Table 4, as well as those with 3-bit activation in Figure 9 and Table 5. In both cases we observe that the relative ranking of the precision assignment is mostly consistent under different α’s. Asαincreases, less bits are assigned to each layer, leading to increasing overall bit reduction with the cost of a small performance loss. This tradeoff is consistent with our previous observations on the 4-bit activation models . C D ETAILED QUANTIZATION SCHEMES FOR IMAGE NET EXPERIMENTS The quantization schemes of the reported ResNet-50 and Inception-V3 models can be found in Table 6 and Table 7 respectively. Figure 9: Layer-wise precision comparison of the quantization schemes achieved under different regularization strengths with 3-bit activation. 15Published as a conference paper at ICLR 2021 Table 5: Accuracy-#Bits tradeoff with 3-bit activation. “FT” stands for ﬁnetuning. Strength α 2e-3 5e-3 8e-3 1e-2 #Bits per Para / Comp (×) 2.90 / 11.04 1.95 / 16.37 1.39 / 23.04 1.28 / 25.06 BSQ acc before / after FT (%) 90.45 / 92.16 90.44 / 91.72 89.01 / 90.93 88.41 / 90.51 Table 6: Quantization schemes of ResNet-50 models on ImageNet dataset achieved by BSQ in Table 3. The scheme on the left is achieved with α= 5e-3 and the one on the right is achieved with α= 7e-3. Except the ﬁrst row for the leading convolutional layer and the last row for the FC layer, each row in the table reports the precision assigned to the 3 layers in a residual block, with layer 1-3 listed from left to right. BSQ 5e-3 BSQ 7e-3 Conv 1 7 7 Block 1-0 7 6 6 7 6 6 Block 1-1 6 6 6 6 6 6 Block 1-2 6 6 6 6 5 6 Block 2-0 4 3 4 4 3 4 Block 2-1 4 4 4 4 3 4 Block 2-2 4 4 4 4 3 4 Block 2-3 4 3 4 3 3 4 Block 3-0 4 3 3 4 3 3 Block 3-1 3 3 4 3 3 3 Block 3-2 3 3 3 3 3 3 Block 3-3 3 3 3 3 2 3 Block 3-4 3 3 3 3 3 3 Block 3-5 3 3 3 3 3 3 Block 4-0 3 2 2 3 2 2 Block 4-1 2 2 3 2 2 2 Block 4-2 2 3 3 2 2 2 FC 3 2 Table 7: Quantization schemes of Inception-V3 model on ImageNet dataset achieved by BSQ in Ta- ble 3. The scheme on the left is achieved with α= 1e-2 and the one on the right is achieved withα= 2e-2. Except for the ﬁrst 5 convolutional layers and the ﬁnal FC layer, each row in the table reports the precision assigned to the layers within the inception block. The order from left to right follows the parameter deﬁnition order provided in the torchvision package implementation (https://github. com/pytorch/vision/blob/master/torchvision/models/inception.py). BSQ 1e-2 BSQ 2e-2 Conv 1a 8 8 Conv 2a 7 7 Conv 2b 6 6 Conv 3b 8 8 Conv 4a 5 4 Mixed 5b 4 4 4 4 4 3 4 4 4 3 4 3 3 4 Mixed 5c 4 4 3 4 3 3 4 4 4 3 4 3 3 4 Mixed 5d 4 4 4 4 4 3 4 4 4 3 4 3 3 4 Mixed 6a 2 4 4 3 2 4 3 3 Mixed 6b 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 Mixed 6c 4 4 3 3 4 3 3 3 3 3 3 3 3 3 3 3 3 2 2 3 Mixed 6d 5 3 3 3 4 3 3 3 3 3 5 3 3 3 4 3 3 3 2 3 Mixed 6e 5 4 3 3 3 3 3 3 3 4 4 3 2 2 3 3 3 3 3 3 Mixed 7a 3 3 4 3 3 2 3 3 3 3 3 2 Mixed 7b 2 3 3 3 2 2 3 2 3 2 2 3 3 2 1 2 2 2 Mixed 7c 2 2 3 3 3 2 3 3 2 2 2 3 3 2 2 3 3 2 FC 3 3 16",
      "meta_data": {
        "arxiv_id": "2102.10462v1",
        "authors": [
          "Huanrui Yang",
          "Lin Duan",
          "Yiran Chen",
          "Hai Li"
        ],
        "published_date": "2021-02-20T22:37:41Z",
        "pdf_url": "https://arxiv.org/pdf/2102.10462v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "BSQ (Bit-level Sparsity Quantization) addresses the challenge of efficiently determining optimal mixed-precision quantization schemes for deep neural networks. It proposes a novel approach by treating each bit of quantized weights as an independent trainable variable and introducing a differentiable bit-level group Lasso regularizer to induce bit-level sparsity. This enables dynamic layer-wise precision reduction through a single gradient-based optimization process, controlled by a single hyperparameter for the accuracy-compression trade-off. BSQ achieves higher accuracy and greater bit reduction on various model architectures across the CIFAR-10 and ImageNet datasets compared to prior mixed-precision quantization methods.",
        "methodology": "BSQ first converts a floating-point DNN model to its bit representation. Weights are scaled once before training, and an n-bit uniform quantization is applied, representing each weight as a sum of its binary bits. These bit representations and the scaling factor are treated as independent, continuous trainable variables. The Straight-Through Estimator (STE) is used to enable gradient-based optimization despite the binary nature of bits, and positive/negative weight elements are separated to handle signs. A bit-level group Lasso (BGL) regularizer, defined as the L2-norm of concatenated positive and negative bit representations, is applied layer-wise to induce all-zero bits for precision reduction. The overall training objective includes the cross-entropy loss and a memory consumption-aware reweighted BGL term. Periodic re-quantization and precision adjustment steps convert trainable bit representations back to exact binary values, identify all-zero bits to remove, and dynamically adjust layer precision (including updating the scaling factor). Activation quantization is predetermined and fixed (ReLU-6 for >=4-bit, PACT for <4-bit). After BSQ training, a final re-quantization is performed, followed by finetuning the model with the obtained fixed mixed-precision scheme using quantization-aware training.",
        "experimental_setup": "BSQ was evaluated on ResNet-20 models for the CIFAR-10 dataset and ResNet-50 and Inception-V3 models for the ImageNet dataset (ILSVRC2012). Standard preprocessing, including random crop, horizontal flip, and normalization, was applied. Pretrained floating-point models were initially quantized to 8-bit for weights, with batch normalization layers kept in floating-point. Activation precision was predetermined (e.g., 4, 3, or 2 bits for most layers, 8 bits for first/final convolutional/FC layers on CIFAR-10 ResNet-20; 8 bits for first/final layers, 4 bits for others on ImageNet ResNet-50; 6 bits for all on ImageNet Inception-V3). Training was conducted using the SGD optimizer with momentum 0.9 and weight decay 0.0001, with batch sizes of 128 (CIFAR-10) and 256 (ImageNet). BSQ training lasted 350 epochs (CIFAR-10) or 90 epochs (ImageNet), with specific learning rate schedules. Re-quantization and precision adjustment intervals were 100 epochs for CIFAR-10 and 10 epochs for ImageNet. Post-training finetuning was performed for 300 epochs (CIFAR-10) or 90 epochs (ImageNet) using the DoReFa-Net algorithm with fixed quantization schemes. Ablation studies investigated the effect of layer-wise regularization reweighing, accuracy-#bits tradeoff under different regularization strengths (alpha), choice of re-quantization interval, and models achieved under different activation precision. Hardware used included a single TITAN XP GPU for CIFAR-10 and two TITAN RTX GPUs for ImageNet experiments. Results were compared against state-of-the-art methods like DoReFa-Net, PACT, LQ-Net, DNAS, HAWQ, LSQ, Deep Compression, Integer, RVQ, and HAQ.",
        "limitations": "The bit representation training treats each bit as separate floating-point trainable variables, resulting in N times more parameters and gradients to store for an N-bit model compared to baseline training, although the authors claim it does not lead to a significant increase in run-time memory consumption. The choice of the re-quantization interval is crucial for stable convergence and needs careful selection. While BSQ's flexibility theoretically allows exploration of mixed-precision quantization at any granularity, the paper exclusively focuses on layer-wise granularity, which is the most common setting in prior works. The activation quantization is predetermined and fixed throughout the BSQ training process, rather than being dynamically learned.",
        "future_research_directions": "The flexibility of BSQ enables it to explore mixed-precision quantization of any granularity (e.g., block-wise, filter-wise, or even element-wise) with the same computational cost, regardless of the search space size. This implies a future research direction to extend BSQ to explore and demonstrate these finer-grained mixed-precision quantization schemes."
      }
    },
    {
      "title": "SDQ: Stochastic Differentiable Quantization with Mixed Precision",
      "abstract": "In order to deploy deep models in a computationally efficient manner, model\nquantization approaches have been frequently used. In addition, as new hardware\nthat supports mixed bitwidth arithmetic operations, recent research on mixed\nprecision quantization (MPQ) begins to fully leverage the capacity of\nrepresentation by searching optimized bitwidths for different layers and\nmodules in a network. However, previous studies mainly search the MPQ strategy\nin a costly scheme using reinforcement learning, neural architecture search,\netc., or simply utilize partial prior knowledge for bitwidth assignment, which\nmight be biased and sub-optimal. In this work, we present a novel Stochastic\nDifferentiable Quantization (SDQ) method that can automatically learn the MPQ\nstrategy in a more flexible and globally-optimized space with smoother gradient\napproximation. Particularly, Differentiable Bitwidth Parameters (DBPs) are\nemployed as the probability factors in stochastic quantization between adjacent\nbitwidth choices. After the optimal MPQ strategy is acquired, we further train\nour network with entropy-aware bin regularization and knowledge distillation.\nWe extensively evaluate our method for several networks on different hardware\n(GPUs and FPGA) and datasets. SDQ outperforms all state-of-the-art mixed or\nsingle precision quantization with a lower bitwidth and is even better than the\nfull-precision counterparts across various ResNet and MobileNet families,\ndemonstrating the effectiveness and superiority of our method.",
      "full_text": "SDQ: Stochastic Differentiable Quantization with Mixed Precision Xijie Huang 1 Zhiqiang Shen 1 2 3 Shichao Li 1 Zechun Liu 4 Xianghong Hu 1 5 Jeffry Wicaksana 1 Eric Xing 6 2 Kwang-Ting Cheng 1 Abstract In order to deploy deep models in a compu- tationally efﬁcient manner, model quantization approaches have been frequently used. In ad- dition, as new hardware that supports mixed bitwidth arithmetic operations, recent research on mixed precision quantization (MPQ) begins to fully leverage the capacity of representation by searching optimized bitwidths for different lay- ers and modules in a network. However, pre- vious studies mainly search the MPQ strategy in a costly scheme using reinforcement learning, neural architecture search, etc., or simply utilize partial prior knowledge for bitwidth assignment, which might be biased on locality of informa- tion and is sub-optimal. In this work, we present a novel Stochastic Differentiable Quantization (SDQ) method that can automatically learn the MPQ strategy in a more ﬂexible and globally- optimized space with smoother gradient approxi- mation. Particularly, Differentiable Bitwidth Pa- rameters (DBPs) are employed as the probability factors in stochastic quantization between adja- cent bitwidth choices. After the optimal MPQ strategy is acquired, we further train our net- work with Entropy-aware Bin Regularization and knowledge distillation. We extensively evaluate our method for several networks on different hard- ware (GPUs and FPGA) and datasets. SDQ out- performs all state-of-the-art mixed or single pre- cision quantization with a lower bitwidth and is even better than the full-precision counterparts across various ResNet and MobileNet families, demonstrating its effectiveness and superiority.1 1Hong Kong University of Science and Technology2Mohamed bin Zayed University of Artiﬁcial Intelligence 3Jockey Club Institute for Advanced Study, HKUST 4Reality Labs, Meta Inc 5ACCESS - AI Chip Center for Emerging Smart Systems 6Carnegie Mellon University. Correspondence to: Zhiqiang Shen <zhiqiangshen0214@gmail.com>. Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s). 1Project: https://huangowen.github.io/SDQ/. Layer 𝒍 𝛽𝑙,𝑏𝑖+1 𝛽𝑙,𝑏𝑖… … 𝛽𝑙,𝑏2 𝛽𝑙,𝑏1 Differentiable Bitwidth Parameters Full Precision Teacher Model KD Loss Entropy-aware Bin Regularization Layers  with  Mixed  Precision Efficient Mixed Precision Model 𝑸𝒃𝒊+𝟏 𝑸𝒃𝒊 𝑸𝒃𝒊−𝟏 𝒘𝒍,𝒃𝒊 𝒒 = ൝ 𝑸𝒃𝒊(𝒘𝒍 𝒓), 𝒑 = 𝜷𝒍,𝒃𝒊 𝑸𝒃𝒊−𝟏(𝒘𝒍 𝒓), 𝒑 = 𝟏−𝜷𝒍,𝒃𝒊 𝑸𝒃𝒊 𝒃𝒊 -bit uniform quantizer … 𝑸𝒃𝟏 (a) Proposed Stochastic Differentiable Quantization (SDQ) 1.00 0.75 0.50 0.25 0.000.250.500.751.00 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 10 20 30 40 50 10 20 30 40 50 (b) Full Precision 1.00 0.75 0.50 0.25 0.000.250.500.751.00 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 2 4 6 8 10 (c) Interpolation 1.00 0.75 0.50 0.25 0.000.250.500.751.00 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 2 4 6 8 10 2 4 6 8 10 (d) Stochastic Figure 1.1(a) demonstrates how our SDQ generates optimal MPQ strategy and train a MPQ network effectively. 1(b), 1(c), 1(d) compare the underlying loss optimization landscape (Li et al., 2018) of a full precision model, MPQ of ResNet20 with linear interpolation (Yang & Jin, 2021; Nikoli´c et al., 2020), and MPQ of ResNet20 with stochastic quantization. 1. Introduction Deep learning models such as Convolutional Neural Net- works (CNNs) have demonstrated outstanding performance on various tasks, e.g., visual recognition (Krizhevsky et al., 2012; He et al., 2016; Tan & Le, 2019). Their latency, energy consumption, and model size have become the most signiﬁ- cant factors for consideration of the deployment with these deep learning models. In recent years, researchers have studied the design, training, and inferencing techniques of deep learning models for greater computation efﬁciency, in- cluding compact network design and search (Howard et al., 2017; Pham et al., 2018; Guo et al., 2020), knowledge distil- lation (Hinton et al., 2015), pruning (Liu et al., 2017; 2018), quantization (Zhou et al., 2016; Zhang et al., 2018), and sparsity (Wen et al., 2016). Quantization techniques have attracted particular attention because emerging hardware arXiv:2206.04459v3  [cs.LG]  11 Jul 2022SDQ: Stochastic Differentiable Quantization with Mixed Precision accelerators (Judd et al., 2016; Jouppi et al., 2017; Sharma et al., 2018) begin to support low-precision inference. To fully exploit the difference of representative capacity and redundancy in different parts (blocks, layers, and ker- nels) of deep neural networks, mixed precision quantization (MPQ) techniques have been put forward to assign different bitwidths to various components in a network. To achieve an MPQ network with satisfactory accuracy and compression, two critical problems must be solved consecutively: First, given a pre-trained network with weights {W(l)}L l=1 and a training dataset D, how to ﬁnd an optimal quantization strat- egy {b(l)}L l=1? Prior solutions addressed this general goal for MPQ are primarily based on searching or optimization. Whereas, the search-based methods which conduct an iter- ative scheme to explore the best structure cannot globally optimize the MPQ network for all modules and are also ex- pensive to learn2. Meanwhile, existing optimization-based methods, such as FracBits (Yang & Jin, 2021), suffer from unstable optimization as the approximation design for gra- dients is not smooth and stable. Consequently, an ideal solution should be differentiable which enables automatic optimization of the bitwidth assignment in a global opti- mization space with more precise gradient approximations and smoother latent landscape as illustrated in Figure 1(d), which derives the primary motivation of this work. The second problem stems from the quantization-aware post-training: after the quantization strategy {b(l)}L l=1 is ac- quired, how can we train a quantized network {W(l) b(l) }L l=1 for maximizing the potential of performance and minimizing the accuracy sacriﬁce compared to a full-precision model? Empirically, during quantization training, the input informa- tion should be preserved and quantization error should be minimized through proper loss function design. To address the above problems, in this work, we propose a novel Stochastic Differentiable Quantization (SDQ) to tackle the challenges of searching and training MPQ. Con- cretely, we present a one-shot solution via representing the choice of discrete bitwidths as a set of Differentiable Bitwidth Parameters (DBPs), as shown in Fig. 1(a). DBPs are utilized in the forward path of the network as probabil- ity factors during the stochastic quantization. During the quantization strategy generation, DBPs will be updated to learn the optimal bitwidth assignment automatically. Dur- ing the backpropagation, we use Gumbel-softmax reparam- eterization to estimate the gradient so the gradient can be back-propagated smoothly through our DBPs. Compared to previous differentiable solutions (Nikoli´c et al., 2020; Yang & Jin, 2021) using linear interpolation, the stochastic quan- tization scheme can substantially improve the network train- 2For L layers with B bitwidth candidate, searching both layer- wise weights and activations has a time complexity of O(B2L). ing stability and help the DBPs converge with a smoother loss landscape shown in Fig. 1(d). Addressing the challenge imposed by the second problem, we propose an Entropy-aware Bin Regularizer (EBR) based on entropy analysis, which regularizes the weights to keep more information-carrying components while considering the various precision of different layers. Knowledge dis- tillation is also used to fully leverage the representation capability of full-precision teacher models. We demonstrate the advantage of our SDQ in terms of effec- tiveness and efﬁciency on different networks and datasets. For ResNet18 on the ImageNet-1K dataset, our quantized model can signiﬁcantly improve top-1 accuracy (71.7%) compared to the full precision model (70.5%) with average bitwidth of both weights and activations no more than 4 bits. We also deploy ResNet18 on Bit Fusion (Sharma et al., 2018) accelerator to show the efﬁciency of our SDQ models. In summary, our contribution can be concluded as: • We present a novel SDQ framework to learn the opti- mal mixed precision quantization strategy via a set of differentiable bitwidth parameters as probability fac- tors during the stochastic quantization. • We utilize straight-through Gumbel-softmax estima- tor in the gradient computation w.r.t. differentiable bitwidth parameters. We also incorporate the quantiza- tion error regularization term while learning the mixed precision quantization strategy. • We propose an Entropy-aware Bin Regularizer to min- imize the quantization error while considering mixed precision, which helps preserve more information- carrying components. • We extensively evaluate our method on different net- work architectures and datasets. We further conduct deployment experiments on various hardware (GPUs and a real FPGA system) to demonstrate the effective- ness and efﬁciency of our models. 2. Related Work Quantization techniques can be categorized into uniform and non-uniform quantization based on the quantization in- tervals. Non-uniform quantization (Miyashita et al., 2016; Zhang et al., 2018; Li et al., 2019b), due to its ﬂexible representation, usually can better allocate the quantization values to minimize the quantization error and achieve better performance than uniform schemes. In addition, the quan- tization methods can also be classiﬁed as stochastic and deterministic quantization. Inspired by gradient estimation of stochastic neurons (Bengio et al., 2013), stochastic quan- tization (SQ) (Courbariaux et al., 2015; 2016; Dong et al., 2019a) has been explored. Unlike previous deterministic quantization, real-value variables of SQ are quantized toSDQ: Stochastic Differentiable Quantization with Mixed Precision different quantization levels controlled by a probability dis- tribution. For example, BinaryConnect (Courbariaux et al., 2015) transforms the real-value weights into +1 or -1 with probability determined by the distance to the zero point. However, the aforementioned quantization schemes solely assign the same bitwidth to the entire model. Mixed preci- sion quantization (MPQ), which employs different bitwidths in distinct layers or modules, can achieve a higher compres- sion ratio and improved model capabilities. MPQ is a more promising direction in general, and it usually is divided into three categories: Search-Based Methods, Metric-Based Methods, and Optimization-Based Methods. Search-Based Methods usually utilize Neural Architecture Search (Wu et al., 2018; Yu et al., 2020; Guo et al., 2020) or Reinforcement Learning (Wang et al., 2019; Elthakeb et al., 2019) to perform searching of quantization strategies. For instance, HAQ (Wang et al., 2019) leverages reinforce- ment learning and incorporates hardware feedback in the loop. DNAS (Wu et al., 2018) explore the quantized search space with gradient-based optimization to improve the ef- ﬁciency. Despite these efforts to increase the efﬁciency of searching, the time and computational cost of generaliz- ing search-based approaches to various network designs, datasets, and hardware platforms remain impediments. Metric-Based Methods target at assigning bitwidth con- sidering speciﬁc metrics that can be computed easily. HAWQ (Dong et al., 2019c;b; Yao et al., 2021) gener- ates MPQ strategy based on the layers’ Hessian spec- trum. OMPQ (Ma et al., 2021) utilizes layer orthogonality to construct a linear programming problem to derive the bitwidth conﬁguration. SAQ (Liu et al., 2021) determines the bitwidth conﬁgurations of each layer, encouraging lower bits for layers with lower sharpness. Although these meth- ods are highly computation efﬁcient, the generated MPQ strategies are usually sub-optimal as the mapping from these metrics to the bitwidths is manually established. Optimization-Based Methods formulate the MPQ strategy generation problem as an optimization problem. The core challenge is to tackle the non-differentiability of task loss w.r.t. the bitwidth assignment. FracBits (Yang & Jin, 2021) proposed a fractional bitwidth parameter and used linear in- terpolation during the forward of quantization. DDQ (Zhang et al., 2021) proposed a method to decompose the quan- tizer operation into the matrix-vector product. Uhlich et al. (Uhlich et al., 2020) proposed differentiable quantization via parametrization. These methods either introduce ex- tra parameters such as quantization levels and the dynamic ranges of quantizers as the optimization target, or utilize linear interpolation between different quantization levels. The interpolation operation will introduce a large number of useless optimization targets for quantization, and more instability is expected. Noticeably, our work falls into the categories of uni- form, quantization-aware training , and optimization- based methods of mixed precision network. To the best of our knowledge, SDQ is the ﬁrst quantization strat- egy that adopts stochastic quantization to optimize the bitwidth assignment. Compared to previous research which also leverages the idea of differentiable bitwidth such as FracBits (Yang & Jin, 2021) and BitPruning (Nikoli´c et al., 2020), SDQ provides better training stability compared to the naive linear interpolation. Our DBPs can further denote discrete bitwidth candidates to better match the conﬁgura- tion of particular hardware accelerators, while others cannot, e.g., Bit Fusion (Sharma et al., 2018) only supports powers of 2 bitwidth. 3. Method 3.1. Preliminaries In network quantization, real-valued weights wr and ac- tivations xr are converted to low-precision value wq and xq. To mitigate the problem that gradient-based optimiza- tion cannot be directly applied to quantized value, previous research like DoReFa-Net (Zhou et al., 2016) exploits a straight-through estimator (STE) (Bengio et al., 2013) to assign the gradients passing in xi and out xo to be equal. Assume the loss function for the model is L, an STE for b-bit uniform quantizer qb can be denoted as: Forward:xo = qb(xi) = 1 2b−1round[(2b−1)xi]; Backward: ∂L ∂xo = ∂L ∂xi . (1) Here xi,xo ∈[0,1]. The weights and activations are ﬁrst linearly transformed and clamped to interval [0,1]. The complete scheme for b-bit uniform quantizer Qb is: xq = Qb(xr) =qb( tanh(xr) 2 max(|tanh(xr)|) + 1 2) −1. (2) 3.2. Generating Quantization Strategy In the quantization strategy generation phase, our goal is to learn the optimal bitwidth assignment. We introduce Differ- entiable Bitwidth Parameters (DBPs) {βl,bi}for each layer l ∈Land bitwidth candidate bi ∈B, where iis the index of bitwidth in the candidate set B. DBPs are initialized to 1 to represent a deterministic quantization at different levels. During forward, the weights can be quantized to two differ- ent bitwidths: the current bitwidth bi and next bitwidth bi−1 in the candidate sets. The probability of quantization into two bitwidths is controlled by the DBP βl,bi at this layer. wq l,bi = { Qbi(wr l) with probabilityp= βl,bi Qbi−1(wr l) with probabilityp= 1−βl,bi , (3)SDQ: Stochastic Differentiable Quantization with Mixed Precision where wq l,bi represents the quantized weight under bi-bit stochastic-bitwidth quantizer. According to the character- istic of STE introduced in Eq. 1, the gradient through the quantization values and real values are the same: ∂L ∂wr l = ∂L ∂Qbi(wr l) = ∂L ∂Qbi−1 (wr l) .Thus, the expected gradients of quantized weight wq l,bi over all trainable parameters in the network can be computed as: E[ ∂L ∂wq l,bi ] =βl,biE[ ∂L ∂Qbi(wrl)] + (1−βl,bi)E[ ∂L ∂Qbi−1(wrl)] =E[ ∂L ∂wrl,bi ], (4) which is the same as the conventional STE. The derived gradient shows the proposed DBPs will not inﬂuence the gradient-based optimization of weight parameters during the training. However, the real gradient still cannot backpropagate di- rectly through the probability parameter βl,bi. To tackle this problem, we use Straight-Through Gumbel SoftMax Es- timator (Jang et al., 2016) as a reparameterization trick to allow gradients ﬂow from quantized value wq l,bi to DBP βl,bi. We can deﬁne the forwarding of previous stochastic-bitwidth quantization as wq l,bi = ck l,biQbi(wr l) + (1 −ck l,bi)Qbi−1 (wr l), where ck l,bi ∈ {0,1}is a choice variable and it follows a Bernoulli distribution ck l,bi ∼ Bernoulli(βl,bi). We use Gumbel SoftMax to transform the choice variable into a “soft” sampling operation while maintaining the distribution characteristics via: ckl,bi = exp((log(βl,bi) +gk)/τ) exp((log(βl,bi) +gk0)/τ) +exp((log(1−βl,bi) +gk1)/τ), (5) where gk, gk0 and gk1 are samples drawn from Gumbel(0,1) distribution, and τ is the temperature coefﬁcient to control the generated distribution. As has been proven in previous research (Jang et al., 2016), the gradient now can be easily computed and the expected value of gradientw.r.t.quantized weights given in Eq. 4 still holds. The advantage of MPQ is that different sensitivity of each layer to the quantization perturbation is fully considered. Previous research (Dong et al., 2019c) has shown that this sensitivity is closely aligned to the number of parameters and quantization error of this layer. In light of this, we propose a quantization-error regularizer to penalize layers with a large number of parameters which result in high quantization error: LQER = L∑ l βl,biλbi||wq l,bi −wr l||2 2, (6) where λbi is a weighting coefﬁcient to balance between dif- ferent bitwidth, and ||wq l,bi −wr l||2 2 is the L2-norm of quan- tization error. This term is highly related to the bitwidth as it increases exponentially with the bitwidth. In our practice, we use λbi = (2|bi|−1)2 to balance between different pre- cision (please see Appendix A for more details). We only optimize DBPs and will not optimize weight parameters with this quantization-error regularization. The complete optimization objective in this stage can be formulated as: O= arg min W,β Ltask + arg min β λQLQER, (7) where Ltask is the task loss and λQ is the coefﬁcient to bal- ance between task loss and quantization error regularization. During the quantization strategy generation phase, we start from the highest precision in the bitwidth candidate sets, and progressively reduce the bitwidth. When DBP at layer l satisﬁes the condition βl,bi < βt, where βt represents the pre-deﬁned DBP threshold, the bitwidth assignment for layer lwill be reduced from bi to bi−1 and we will start to optimize βl,bi−1 . After a pre-deﬁned number of epochs for training, we can generate an MPQ strategy bl for each layer l∈Lbased on the current DBPs{βl,b0 ,···βl,bn}that have been optimized. 3.3. Training with Quantization Strategy While prior MPQ research (Uhlich et al., 2020; Zhang et al., 2021) combined the searching and training stages, we choose a two-stage strategy since the optimization objectives are distinct in these two periods. In addition, more instability of training is expected when more quantization parameters are optimized simultaneously (i.e. DBPs) with the weight parameters. In light of this, we apply quantization-aware training with the generated MPQ strategy and without the DBPs. The total loss Lfor optimization in this stage is: L= LKD + λELEBR, (8) where LKD denotes the knowledge distillation loss and LEBR denotes the entropy-preserving bin regularization. λE is the weighting coefﬁcient to balance between them. We will introduce them in Sec. 3.3.1 and Sec. 3.3.2, respectively. 3.3.1. K NOWLEDGE DISTILLATION Intrinsically, a quantized classiﬁcation network should learn an ideal similar mapping from input images to the output logits as a full-precision network, and the performance gap between them needs to be minimized. Based on this insight, we use knowledge distillation to train our MPQ network with a full-precision model as the teacher. The loss function is designed to enforce the similarity between the output dis- tribution of full-precision teacher and MPQ student model: LKD = −1 N ∑ c N∑ i=1 pFθ c (Xi) log(pQθ c (Xi)) (9) where the KD loss is deﬁned as the cross-entropy between the output distributions pc of a full-precision teacher Fθ and a MPQ student Qθ. Xi is the input sample. cand NSDQ: Stochastic Differentiable Quantization with Mixed Precision denote the classes and the number of samples, respectively. Note that this process can be regarded as the distribution calibration for the student network, and one-hot label is not involved in training. 3.3.2. E NTROPY -AWARE BIN REGULARIZATION In previous quantization methods, real-valued weights are usually distributed uniformly. As a result, the quantized weight might collapse to a few quantization bins that close to zero. The information represented by the weights is heavily reduced during this process. According to the information theory, entropy should be preserved in quantization to retain the representation capacity of neural networks. In our method, after MPQ strategy is adopted, we propose Entropy-aware Bin Regularization to regularize weights in group of different bitwidth. The entropy carried by quan- tized weight with a speciﬁc bitwidth bcan be denoted as Hb(W) = −pb(wi)log(pb(wi)),s.t. ∑2b i=1 pb(wi) = 1, where pb(wi) indicates the proportion of weights quan- tized to i-th quantization bin. We can derive that the en- tropy Hb(W) is maximized when p∗ b(wi) = 1/2b for all i∈{1,... 2b}. In light of the entropy analysis and inspired by previous research (Li et al., 2019b; Yamamoto, 2021), we ﬁrst normalize real-valued weights wr l of layer lwith bitwidth bto wr∗ l = 2(b−1) 2b−1 |wr l| ||wr l||1 wr l. The corresponding quantized weights wq l are approximated to uniformly dis- tribute in all quantization levels. Here, |wr l|is the number of entries in wr l, and ||wr l||1 computes the L1-norm of wr l. Different from weight normalization in APOT (Li et al., 2019b) and LCQ (Yamamoto, 2021) which only consider the global weight distribution, we also want to minimize the quantization error in a speciﬁc quantization bin to reduce the information loss. Therefore, we regularize the distribution of the weights in a quantization bin to be a “sharp” Gaussian distribution (i.e. a Dirac delta distribution ideally): its mean approaching quantization value and variance approaching zero. Motivated by these considerations, the Entropy-aware Bin Regularization is formulated as: LEBR = L∑ l=1 2b(l) ∑ n=1 Lmse(wr n,l,wq n,l) +V(wr n,l), (10) where wr n,l,wq n,l represent the real value and quantized value of weights in layer l and quantization bin n. Lmse computes the mean square error,(·) computes the mean, and V(·) computes variance for all quantization bins with more than two elements. 3.4. Method Analysis and Discussion The complete algorithm of our proposed SDQ is summa- rized in Alg. 1. Since our SDQ only performs stochastic quantization on weights, the precision of activations is unaf- fected throughout the quantization strategy generation phase. We freeze the activation bitwidth with a ﬁxed value for the entire network during the training phase, and the activation is quantized in the same way as denoted by Eq. 2. Also note that our stochastic quantization is only performed on quantization strategy generation phase and is not applied during post-training or real inference. While we only demonstrate how to use SDQ on layer-wise quantization. Our approach can easily be adapted to quan- tize models at many levels of granularity, such as block-wise, net-wise, and kernel-wise. However, the performance will be inhibited when using SDQ on coarse-grained granularity, such as blocks and networks, since the sensitivity difference between each layer will not be fully exploited. Additional training instability is expected when SDQ is used for ﬁne- grained kernel-wise quantization because more quantization parameters are optimized alongside weight parameters dur- ing the MPQ strategy learning phase. Furthermore, current hardware accelerators cannot implement kernel-wise quan- tization, limiting the actual performance when deploying quantized models. Appendix B contains further studies demonstrating that SDQ for layer-wise quantization pro- duces the best results. Algorithm 1 Stochastic Differentiable Quantization Input: the network with full precision weight {Wr}L l=1, differentiable bitwidth parameters {β}L l=1, bitwidth candi- date set B, maximum training epoch EG (epoch for generat- ing quantization strategy) and ET (epoch for post-training), threshold βt for decaying bitwidth Output: quantized network with weights {Wq}L l=1 and bitwidth allocations {b(l)}L l=1 1: // Phase 1: Generating Quantization Strategy 2: for l= 1to L, b∈B do 3: Initialize differentiable bitwidth parameters βl,b = 1 4: end for 5: for epoch = 1 to EG do 6: Quantize real-value weight Wr to Wq by Eq. 3 7: Compute the task loss Ltask and LQER by Eq. 6 8: Compute gradient ∇Wr and ∇β, update parameters 9: Update {b(l)}L l=1 according to {β}L l=1 and βt 10: end for 11: Generate MPQ strategy {b(l)}L l=1 from DBPs {β}L l=1 12: // Phase 2: Post-training with Quantization Strategy 13: for epoch = 1 to ET do 14: Quantize real-value weights and activations by Eq. 2 15: Compute the knowledge distillation loss LKD and regularization LEBR by Eq. 9 and Eq. 10 16: Compute gradient ∇Wr, update parameters 17: end for 18: Return MPQ network {Wq}L l=1 with strategy {b(l)}L l=1SDQ: Stochastic Differentiable Quantization with Mixed Precision 4. Experiments To evaluate the effectiveness of the proposed SDQ, we con- duct experiments on the CIFAR and ImageNet-1K datasets. We ﬁrst introduce the dataset, network, and training strategy in Sec. 4.1, followed by the comparison with state-of-the-art quantization methods in Sec. 4.2. We then analyze the effect and role of each proposed component of SDQ in Sec. 4.3. In Sec. 4.4, we visualize the MPQ strategy and how our proposed SDQ improves the representation capacity of the quantized model. Hardware deployment experiments on different devices are designed to demonstrate the energy and time efﬁciency of our models in Sec. 4.5. 4.1. Experimental Settings Dataset The experiments are carried out on CIFAR- 10 dataset (Krizhevsky et al., 2009) and ImageNet-1K dataset (Deng et al., 2009). We only perform basic data aug- mentation in PyTorch (Paszke et al., 2019), which includes RandomResizedCrop and RandomHorizontalFlip during training, and single-crop operation during evaluation. Network We evaluate ResNet20 on CIFAR-10, and evaluate ResNet18 and MobileNetV2 on ImageNet-1K dataset. Due to the fact that the ﬁrst and last layers are more sensitive to quantization perturbation compared to intermediate layers, we ﬁx the bitwidth of them following previous work (Yang & Jin, 2021). Training detail Following previous quantization meth- ods (Zhou et al., 2016; Jung et al., 2019), we adopt real- value pre-trained weights as initialization. We train and evaluate our models on various hardware platforms, includ- ing various NVIDIA GPUs and FPGA. Details of all hyper- parameters and training schemes are shown in Appendix C. 4.2. Comparison with State-of-the-Art Methods Table 1 compares our SDQ with existing methods for ResNet20 on CIFAR-10. We can see that our SDQ models signiﬁcantly outperform the previous ﬁxed precision quan- tization methods while also yielding better accuracy than other MPQ models with fewer average bitwidth. Table 1.Comparison with state-of-the-art mixed precision quanti- zation methods (ResNet20 on CIFAR-10). Method Bit-widthmixed Accuracy(%)WCR(W/A) Top-1 FP Top-1 Dorefa(Zhou et al., 2016)2/32 88.2 92.4 16 ×PACT (Choi et al., 2018)2/32 89.7 92.4 16 ×LQ-net(Zhang et al., 2018)2/32 91.1 92.4 16 × TTQ (Jain et al., 2019)2.00/32 ✓ 91.2 92.4 16 ×Uhlich et al. (Uhlich et al., 2020)2.00/32 ✓ 91.4 92.4 16 ×BSQ (Yang et al., 2020)2.08/32 ✓ 91.9 92.6 15.4 ×DDQ (Zhang et al., 2021)2.00/32 ✓ 91.6 92.4 16 × Ours 1.93/32 ✓ 92.1 92.4 16.6× Table 2 shows the ImageNet-1K classiﬁcation performance of our SDQ on ResNet18 and MobileNetV2. Since different methods adopt different full-precision (FP) models as ini- tialization, we also report the corresponding Top-1 accuracy of the FP model for different methods. Compared to the baseline FP model, our SDQ-trained mod- els perform comparably or even better when quantized to low precision. For example, our ResNet18 achieves 71.7% Top-1 accuracy when the average bitwdiths for weights and activations are 3.85 and 4 respectively, which has a 1.2% absolute gain on the full-precision model. Our quantized MobileNetV2 is the ﬁrst quantized model with accuracy higher than FP initialization (72.0% vs. 71.9%), and average precision for weights and activations is lower than 4. With an optimal MPQ strategy and an effective training scheme, our SDQ outperforms previous quantization meth- ods signiﬁcantly under the same bitwidth conﬁguration. To the best of our knowledge, the highest accuracy re- ported by previous uniform methods with weights and acti- vation bitwidths smaller than 4 bits are 70.6% on ResNet18 and 71.6% on MobileNetV2 from FracBits-SAT (Yang & Jin, 2021). Our SDQ achieves an increase of 1.1% on ResNet18 and 0.3% on MobileNetV2 with a more com- pact bitwidth (3.85/4 vs. 4/4 on ResNet18, 3.79/4 vs. 4/4 on MobileNetV2). Note that SDQ even outperforms the state-of-the-art non-uniform quantization methods (Li et al., 2019b; Chang et al., 2021; Zhang et al., 2021), proving the effectiveness and superiority of our method. 4.3. Ablation Study We further conduct ablation studies to prove the contribu- tion of different components to our model’s performance, including SDQ quantization strategy generation, knowledge distillation, and Entropy-aware Bin Regularization. Table 3 compares different quantization strategy generation schemes impartially on the same initialization and training. The per- formance of the quantized model with the quantization strat- egy generated by our SDQ is close to previous MPQ models, while the average bitwidths of our strategy are lower. This comparison reﬂects that our strategy generation contributes to the improvement of accuracy. Table 3.Comparison with state-of-the-art mixed precision quanti- zation strategy generation methods under same training and initial- ization (MobileNetV2 on ImageNet-1K). MPQ Strategy Bit-width Accuracy(%) (W/A) Top-1 Top-5 HMQ (Habi et al., 2020) 3.98/4 71.7 90.1 FracBits (Yang & Jin, 2021) 4/4 72.0 90.4 Our strategy 3.79/4 72.0 90.5 The comparison of different weight regularization methods is shown in Table 4. The experiments are conducted on MPQ ResNet18 with average bitwidths of 3.85 and 2 for weights and activations. The insights of our proposed EBR are to penalize the global distribution of weights to be overSDQ: Stochastic Differentiable Quantization with Mixed Precision Table 2.Comparison with state-of-the-art mixed precision quantization methods (ResNet18 and MobileNetV2 on ImageNet-1K). Bitwidth (W/A) denotes the average bitwidth for weights and activation parameters. WCR represents the weight compression rate. BitOPs denotes the bit operations. For a ﬁlter f, the BitOPs is deﬁned as BitOPs(f) =bwba|f|wf hf /s2 f , where bw and ba are the bitwidths for weights and activations, |·| denotes the cardinality of the ﬁlter, wf ,hf ,sf are the spatial width, height, and stride of the ﬁlter. Network Method Bit-width Mixed Uniform Accuracy (%) WCR Model BitOPs (G)(W/A) Top-1 FP Top-1 Size (MB) ResNet18 Dorefa†(Zhou et al., 2016) 4/4 ✓ 68.1 70.5 8 × 5.8 35.2 PACT†(Choi et al., 2018) 4/4 ✓ 69.2 70.5 8 × 5.8 35.2 LQ-net (Zhang et al., 2018) 4/4 69.3 70.5 8 × 5.8 35.2 APOT (Li et al., 2019b) 4/4 70.7 70.5 8 × 5.8 34.7 DNAS†(Wu et al., 2018) -/- ✓ ✓ 70.6 71.0 8 × 5.8 35.2 HAQ (Wang et al., 2019) 4/32 ✓ ✓ 70.4 70.5 8 × 5.8 465 EdMIPS (Cai & Vasconcelos, 2020)4/4 ✓ ✓ 68.0 70.2 8 × 5.8 34.7 HAWQ-V3†(Yao et al., 2021) 4.8/7.5 ✓ ✓ 70.4 71.5 6.7 × 7.0 72.0 Chen et al. (Chen et al., 2021) 3.85/4 ✓ ✓ 69.7↓0.1% 69.8 8.3 × 5.6 33.4 FracBits-SAT (Yang & Jin, 2021)4/4 ✓ ✓ 70.6↑0.4% 70.2 8 × 5.8 34.7 Uhlich et al. (Uhlich et al., 2020)3.88/4 ✓ 70.1 70.3 8.3 × 5.6 33.7 RMSMP (Chang et al., 2021) 4/4 ✓ 70.7 70.3 8 × 5.8 34.7 DDQ (Zhang et al., 2021) 4/4 ✓ 71.2 70.5 8 × 5.8 34.7 Ours 3.85/8 ✓ ✓ 72.1↑1.6% 70.5 8.3× 5.6 62.6 3.85/4 ✓ ✓ 71.7↑1.2% 70.5 8.3× 5.6 33.4 3.85/3 ✓ ✓ 70.2↓0.3% 70.5 8.3× 5.6 25.1 3.85/2 ✓ ✓ 69.1↓1.4% 70.5 8.3× 5.6 16.7 MobileNetV2 Dorefa†(Zhou et al., 2016) 4/4 ✓ 61.8 71.9 8 × 1.8 7.42 PACT†(Choi et al., 2018) 4/4 ✓ 61.4 71.9 8 × 1.8 7.42 LQ-net (Zhang et al., 2018) 4/4 64.4 71.9 8 × 1.8 7.42 APOT (Li et al., 2019b) 4/4 71.0 71.9 8 × 1.8 5.35 HAQ (Wang et al., 2019) 4/32 ✓ ✓ 71.5 71.9 8 × 1.8 42.8 HMQ (Habi et al., 2020) 3.98/4 ✓ ✓ 70.9 71.9 8.1 × 1.7 5.32 Chen et al. (Chen et al., 2021) 4.27/8 ✓ ✓ 71.8↓0.1% 71.9 7.5 × 1.9 5.32 FracBits-SAT (Yang & Jin, 2021)4/4 ✓ ✓ 71.6↓0.2% 71.8 8 × 1.8 5.35 Uhlich et al. (Uhlich et al., 2020)3.75/4 ✓ 69.8 70.2 8.5 × 1.6 5.01 RMSMP (Chang et al., 2021) 4/4 ✓ 69.0 71.9 8 × 1.8 5.35 DDQ (Zhang et al., 2021) 4/4 ✓ 71.8 71.9 8 × 1.8 5.35 Ours 3.79/8 ✓ ✓ 72.9↑1.0% 71.9 8.4× 1.6 10.1 3.79/4 ✓ ✓ 72.0↑0.1% 71.9 8.4× 1.6 5.07 † re-implementation for fair comparisons under the same backbone architectures centralized, and penalize the local distribution of weights in different quantization bins to be too dispersed. The effect of it is also shown in Fig. 5. Compared to other weights regularization methods which only consider the global dis- tribution, our EBR signiﬁcantly improves the quantization robustness of MPQ models. Table 4.Comparison of our Entropy-aware Bin Regularization (EBR) and other weight regularization methods for our mixed precision quantized ResNet18 on ImageNet-1K. Method Top-1 Acc Top-5 Acc Baseline 67.6 87.6 Weight Norm (Salimans & Kingma, 2016) 66.6 86.7 KURE (Shkolnik et al., 2020) 68.5 88.4 λE= 0.01 68.6 88.4 EBR λE= 0.1 69.1 88.5 λE= 1 68.9 88.4 How the knowledge distillation contributes to the training of MPQ models is shown in Table 5. Learning from FP models helps minimize the performance gap of quantization models. When the performance of FP teacher models is better, the performance of student models also improves. Table 5.Comparison of different teacher models of knowledge dis- tillation for our mixed precision quantized ResNet18 on ImageNet- 1K. One-hot label with CE loss is used for “w/o KD” in experiment. Method Teacher Top-1 Acc Top-5 Acc Ours w/o KD Ground Truth 70.5 89.5 ResNet34 70.7 89.7 Ours ResNet50 71.1 89.9 ResNet101 71.7 90.2 4.4. Visualization and Analysis To intuitively demonstrate the bitwidth assignment gener- ated by our SDQ, the quantization strategy of weights in different layers of ResNet18 and MobileNetV2 is visualized in Fig. 2(a) and Fig. 2(b), respectively. Our SDQ learns more bits for those layers with fewer parameters ( down- sample conv layers in ResNet18 and depthwise conv layers in MobileNetV2), which proves that more redundancy is expected for those layers. As layers with the same num- ber of parameters can have different optimal bitwidths, it can be proved that there is no simple heuristics to allocate bitwidth. We can also see from the bitwidth assignment that the ﬁrst few layers and the last few layers have higherSDQ: Stochastic Differentiable Quantization with Mixed Precision 0 3 6 9 12 15 18 Layer index of ResNet18 0 1 2 3 4 5 6 7 8Bitwidth regular conv downsample conv 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Number of Parameters 1e6 (a) ResNet18 (3.85bit) 0 6 12 18 24 30 36 42 48 Layer index of MobileNetV2 0 1 2 3 4 5 6 7 8Bitwidth pointwise conv depthwise conv 0 1 2 3 4 5 Number of Parameters 1e5  (b) MobileNetV2 (3.79bit, The ﬁrst 3 ×3 conv layer and last fc layer is not visualized here) Figure 2.Mixed Precision Quantization strategy generated by our SDQ for ResNet18 and MobileNetV2 on ImageNet-1K dataset. bitwidth. Fig. 3 depicts how bitwidth assignment evolves during the quantization strategy generation phase. While the decay of bitwidth is slow for downsample conv layers, the regular conv layers decay quickly at high precision and slow down when it reaches low precision. Compared to uniform quantization, the MPQ network per- forms better with fewer bits since the MPQ network fully utilizes the difference of layers’ sensitivity to quantization. Fig. 4(a) and Fig. 4(b) visualize the feature embedding of last conv layer in ResNet20. As expected, the clusters of dog and deer are mixed up together in the baseline model with 2-bit uniform quantization. While the model of SDQ can split those classes with more separable representations. We proposed Entropy-aware Bin Regularization to preserve the entropy and minimize the quantization error. Fig. 5 shows the histogram of weight distribution in full-precision and 2-bit quantization. We can observe that the weights in our model are more centralized locally to the quantiza- tion point in each quantization bin. Further, the portion of weights quantized to each bin in our model is closer compared to baseline, which preserves the entropy globally. /uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013/uni0000001c/uni00000013 /uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b /uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018 /uni00000019 /uni0000001a /uni0000001b /uni0000001c/uni00000025/uni0000004c/uni00000057/uni0000005a/uni0000004c/uni00000047/uni00000057/uni0000004b /uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000014/uni00000011/uni00000013/uni00000011/uni00000046/uni00000052/uni00000051/uni00000059/uni00000014 /uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000015/uni00000011/uni00000013/uni00000011/uni00000046/uni00000052/uni00000051/uni00000059/uni00000014 /uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000015/uni00000011/uni00000013/uni00000011/uni00000047/uni00000052/uni0000005a/uni00000051/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048 /uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000016/uni00000011/uni00000013/uni00000011/uni00000046/uni00000052/uni00000051/uni00000059/uni00000014 /uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000017/uni00000011/uni00000013/uni00000011/uni00000046/uni00000052/uni00000051/uni00000059/uni00000014 /uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000017/uni00000011/uni00000013/uni00000011/uni00000047/uni00000052/uni0000005a/uni00000051/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048 Figure 3.Bitwidth assignment of selected layers in ResNet18 dur- ing quantization strategy generation. 4.5. Hardware Efﬁciency on Accelerator We further conduct hardware experiments to evaluate the efﬁciency of our MPQ model. In Table 6, we evaluate our model on the accelerator that supports mixed precision arithmetic operation: Bit Fusion (Sharma et al., 2018). Bit Fusion only supports multiplications of power-of-two bits (i.e. 2, 4, 8, 16 bits), so there is a certain performance gap plane car bird cat deerdog frog horse ship truck (a) Baseline (2 bit) plane car bird cat deer dog frog horse ship truck (b) Mixed Ours (1.93 bit) Figure 4.Comparison of feature embedding visualization using t-SNE on CIFAR-10 evaluation set. The feature embedding are extracted from the output of last conv layer of ResNet20. The performance of these two models are reported in Table 1. 2  1  0 1 20.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 1e5 Weight distribution Ours w/o EBR FP Initilization (a) Weights w/o EBR 2  1  0 1 20.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 1e5 Weight distribution Ours w/ EBR FP Initilization (b) Weights w/ EBR 0.6  0.4  0.2  0.0 0.2 0.4 0.60.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1e6 Weight distribution Ours w/ EBR Ours w/o EBR (c) Quantization Figure 5.Histogram of the weight distribution and quantization distribution. 5(a) and 5(b) are the histogram of weights of layer4.1.conv2 with 2-bit precision in our quantized ResNet18 model with and without EBR respectively. 5(c) compares the quantization results with and without EBR. between theoretical compression and real compression. Our ResNet18 model with an average weight bitwidth of 3.85 achieves much better accuracy than the ResNet18 model with a weight bitwidth of 4, and surpasses its time cost and energy efﬁciency. Table 6.Comparison of latency and energy consumption of quan- tized ResNet18 model on Bit Fusion. Method Bitwidth (W/A) Top-1 Acc Latency Energy Dorefa 4/8 69.8 48.99 ms 93.34 mJ Ours 3.85/8 72.1 46.18 ms 90.18 mJ Dorefa 4/4 67.1 34.61 ms 64.16 mJ Ours 3.85/4 71.1 32.84 ms 60.49 mJ Dorefa 4/2 63.6 30.77 ms 54.08 mJ Ours 3.85/2 69.1 28.18 ms 49.05 mJSDQ: Stochastic Differentiable Quantization with Mixed Precision Table 7.Performance comparison on COCO detection benchmark. Method Bitwidth Hardware AP AP 50 AP75 APS APM APL Latency Energy FPS(W/A) YOLOv4-tiny (Bochkovskiy et al., 2020)32/32 GPU 20.4 38.7 19.5 7.4 24.1 27.1 - - - Dorefa (Zhou et al., 2016) 8/8 FPGA 16.1 32.3 14.5 4.3 17.3 24.7 34.18ms 268.3mJ 29 4/4 FPGA 15.4 29.2 12.5 3.7 16.9 23.3 18.64ms 146.3mJ 53 Ours + Dorefa 3.88/4 FPGA 15.9 31.1 14.7 4.2 17.6 24.2 21.28ms 167.1mJ 47 4.6. COCO Detection Hardware Experiment We also evaluate our SDQ on object detection task which is more challenging. We implement the mixed-precision com- pact YOLOv4-tiny (Bochkovskiy et al., 2020) with our SDQ on COCO detection dataset (Lin et al., 2014), which con- sists of 80 object categories. The training set contains 115k images ( trainval35k), and the validation set has 5k images (minival). Different from previous research (Li et al., 2019a; Wang et al., 2020) that apply quantized mod- els on Faster R-CNN (Ren et al., 2015) or RetinaNet (Lin et al., 2017) using ResNet and MobileNet as backbones, YOLOv4-tiny is already compact and sensitive to the quan- tization. Our quantization strategy generation and training phase are conducted on trainval35k partition. During the training phase, percentile activation calibration (Li et al., 2019a) is used to discard outlier activation values. Since our target deployment platform only supports power-of-two bitwidth, we set the bitwidth candidate B= {1,2,4,8}. The input size for the object detector is 416 ×416. Fol- lowing the standard COCO evaluation metric, we report the average precision (AP) for different IoU thresholds on the minival partition. The results, including latency and energy consumption when deploying on the FPGA system are shown in Table 7. From the table we can see that our SDQ improves the efﬁ- ciency of object detector without incurring signiﬁcant mAP degradation compared to the baseline quantization approach (8/8 bitwidths), while our detector with lower bitwidths (3.88/4) outperforms the 4-bit quantized model favorably. According to the FPGA deployment statistics, our SDQ demonstrates good hardware afﬁnity yielding latency and en- ergy consumption close to the 4-bit uniform single-precision model. Overall, mixed-precision YOLOv4-tiny trained with our SDQ can achieve similar performance of 8-bit quan- tized model while retaining the hardware efﬁciency of 4-bit quantized model. 4.6.1. FPGA S YSTEM SETTING The system is implemented on the Xilinx U50 FPGA plat- form and consumes 259688 LUTs and 210.5 BRAM. As shown in Fig. 6, a CNN accelerator system is comprised of multiple cores, controller, on-chip memory, downloader, and uploader. Each core consists of 4×16 dedicated array of INT-8 multiply-and-accumulate (MAC) processing ele- ments. All cores share the same on-chip memory, which stores the input feature map (ifmap), weight, index, and output feature map (ofmap). The index is used for weight sparse to scratch the input activation. The controller module uses instructions to take charge of convolution computation, data download, and upload. Ifmap  Memory Weight  & Index  Memory Core Downloader Ofmap  Memory Controller  Uploader CNN Accelerator External Mermory Core MAC MAC MAC MAC MAC MAC MAC MAC MAC MAC MAC MAC MAC MAC MACMAC MAC MAC MAC MAC MAC MAC MAC MAC MAC MAC MAC MAC MAC MAC MACMAC Dedicated MAC Array Figure 6.System architecture schematic. The parameters of our hardware experiment platform are: MAC array-4 rows ×16 columns, Frequency-200MHz, Number of Cores-8. 5. Conclusion We have presentedStochastic Differentiable Quantization (SDQ), a learning based mixed precision quantization frame- work that can optimize the optimal MPQ strategy automati- cally. In contrast to previous MPQ methods, SDQ introduces a set of Differentiable Bitwidth Parameters (DBPs) that fol- lows a stochastic quantization scheme to make DBPs differ- entiable with a stabler and smoother optimization process. Since SDQ is optimized on a global searching space, the learned MPQ strategy is usually better than other competi- tors. Moreover, Quantization Error Regularization (QER) and Entropy-aware Bin Regularization (EBR) are integrated into the MPQ learning and post-training stages to mini- mize the quantization error. We demonstrate the superiority of our SDQ with comprehensive experiments on different hardware platforms such as GPUs and FPGA across vari- ous networks. SDQ achieves state-of-the-art accuracy over previous quantization methods with fewer average bits. Ex- tensive hardware experiments prove the superior efﬁciency of our models deployed on mixed-precision accelerators. Acknowledgements This research was partially supported by ACCESS - AI Chip Center for Emerging Smart Systems, sponsored by InnoHK funding, Hong Kong SAR.SDQ: Stochastic Differentiable Quantization with Mixed Precision References Bengio, Y ., L´eonard, N., and Courville, A. Estimating or propagating gradients through stochastic neurons for con- ditional computation. arXiv preprint arXiv:1308.3432, 2013. Bochkovskiy, A., Wang, C.-Y ., and Liao, H.-Y . M. Yolov4: Optimal speed and accuracy of object detection. arXiv preprint arXiv:2004.10934, 2020. Cai, Z. and Vasconcelos, N. Rethinking differentiable search for mixed-precision neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2349–2358, 2020. Chang, S.-E., Li, Y ., Sun, M., Jiang, W., Liu, S., Wang, Y ., and Lin, X. Rmsmp: A novel deep neural network quantization framework with row-wise mixed schemes and multiple precisions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5251– 5260, 2021. Chen, W., Wang, P., and Cheng, J. Towards mixed-precision quantization of neural networks via constrained optimiza- tion. In Proceedings of the IEEE/CVF International Con- ference on Computer Vision, pp. 5350–5359, 2021. Choi, J., Wang, Z., Venkataramani, S., Chuang, P. I.-J., Srini- vasan, V ., and Gopalakrishnan, K. Pact: Parameterized clipping activation for quantized neural networks. arXiv preprint arXiv:1805.06085, 2018. Courbariaux, M., Bengio, Y ., and David, J.-P. Binarycon- nect: Training deep neural networks with binary weights during propagations. In Advances in neural information processing systems, pp. 3123–3131, 2015. Courbariaux, M., Hubara, I., Soudry, D., El-Yaniv, R., and Bengio, Y . Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1. arXiv preprint arXiv:1602.02830, 2016. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009. Dong, Y ., Ni, R., Li, J., Chen, Y ., Su, H., and Zhu, J. Stochas- tic quantization for learning accurate low-bit deep neural networks. International Journal of Computer Vision, 127 (11):1629–1642, 2019a. Dong, Z., Yao, Z., Cai, Y ., Arfeen, D., Gholami, A., Ma- honey, M. W., and Keutzer, K. Hawq-v2: Hessian aware trace-weighted quantization of neural networks. arXiv preprint arXiv:1911.03852, 2019b. Dong, Z., Yao, Z., Gholami, A., Mahoney, M. W., and Keutzer, K. Hawq: Hessian aware quantization of neural networks with mixed-precision. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 293–302, 2019c. Elthakeb, A., Pilligundla, P., Mireshghallah, F., Yazdan- bakhsh, A., Gao, S., and Esmaeilzadeh, H. Releq: An automatic reinforcement learning approach for deep quan- tization of neural networks. In NeurIPS ML for Systems workshop, 2018, 2019. Guo, Z., Zhang, X., Mu, H., Heng, W., Liu, Z., Wei, Y ., and Sun, J. Single path one-shot neural architecture search with uniform sampling. In European Conference on Computer Vision, pp. 544–560. Springer, 2020. Habi, H. V ., Jennings, R. H., and Netzer, A. Hmq: Hardware friendly mixed precision quantization block for cnns. In Computer Vision–ECCV 2020: 16th European Confer- ence, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXVI 16, pp. 448–463. Springer, 2020. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. Hinton, G., Vinyals, O., and Dean, J. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. Howard, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., and Adam, H. Mobilenets: Efﬁcient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. Jain, S. R., Gural, A., Wu, M., and Dick, C. H. Trained quantization thresholds for accurate and efﬁcient ﬁxed- point inference of deep neural networks. arXiv preprint arXiv:1903.08066, 2019. Jang, E., Gu, S., and Poole, B. Categorical repa- rameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016. Jouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal, G., Bajwa, R., Bates, S., Bhatia, S., Boden, N., Borchers, A., et al. In-datacenter performance analysis of a tensor processing unit. In Proceedings of the 44th annual inter- national symposium on computer architecture, pp. 1–12, 2017. Judd, P., Albericio, J., Hetherington, T., Aamodt, T. M., and Moshovos, A. Stripes: Bit-serial deep neural network computing. In 2016 49th Annual IEEE/ACM Interna- tional Symposium on Microarchitecture (MICRO) , pp. 1–12. IEEE, 2016.SDQ: Stochastic Differentiable Quantization with Mixed Precision Jung, S., Son, C., Lee, S., Son, J., Han, J.-J., Kwak, Y ., Hwang, S. J., and Choi, C. Learning to quantize deep net- works by optimizing quantization intervals with task loss. In Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition , pp. 4350–4359, 2019. Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009. Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classiﬁcation with deep convolutional neural networks. Advances in neural information processing systems, 25: 1097–1105, 2012. Li, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T. Visualizing the loss landscape of neural nets. In Pro- ceedings of the 32nd International Conference on Neural Information Processing Systems, pp. 6391–6401, 2018. Li, R., Wang, Y ., Liang, F., Qin, H., Yan, J., and Fan, R. Fully quantized network for object detection. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2810–2819, 2019a. Li, Y ., Dong, X., and Wang, W. Additive powers-of-two quantization: An efﬁcient non-uniform discretization for neural networks. In International Conference on Learn- ing Representations, 2019b. Lin, T.-Y ., Maire, M., Belongie, S., Hays, J., Perona, P., Ra- manan, D., Doll´ar, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In European conference on computer vision, pp. 740–755. Springer, 2014. Lin, T.-Y ., Goyal, P., Girshick, R., He, K., and Doll ´ar, P. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision , pp. 2980–2988, 2017. Liu, J., Cai, J., and Zhuang, B. Sharpness-aware quan- tization for deep neural networks. arXiv preprint arXiv:2111.12273, 2021. Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., and Zhang, C. Learning efﬁcient convolutional networks through net- work slimming. In Proceedings of the IEEE international conference on computer vision, pp. 2736–2744, 2017. Liu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T. Re- thinking the value of network pruning. In International Conference on Learning Representations, 2018. Ma, Y ., Jin, T., Zheng, X., Wang, Y ., Li, H., Jiang, G., Zhang, W., and Ji, R. Ompq: Orthogonal mixed precision quantization. arXiv preprint arXiv:2109.07865, 2021. Miyashita, D., Lee, E. H., and Murmann, B. Convolutional neural networks using logarithmic data representation. arXiv preprint arXiv:1603.01025, 2016. Nikoli´c, M., Hacene, G. B., Bannon, C., Lascorz, A. D., Courbariaux, M., Bengio, Y ., Gripon, V ., and Moshovos, A. Bitpruning: Learning bitlengths for aggressive and accurate quantization. arXiv preprint arXiv:2002.03090, 2020. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32:8026–8037, 2019. Pham, H., Guan, M., Zoph, B., Le, Q., and Dean, J. Efﬁcient neural architecture search via parameters sharing. In International Conference on Machine Learning, pp. 4095– 4104. PMLR, 2018. Ren, S., He, K., Girshick, R., and Sun, J. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28:91–99, 2015. Salimans, T. and Kingma, D. P. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. Advances in neural information process- ing systems, 29:901–909, 2016. Sharma, H., Park, J., Suda, N., Lai, L., Chau, B., Chan- dra, V ., and Esmaeilzadeh, H. Bit fusion: Bit-level dy- namically composable architecture for accelerating deep neural network. In 2018 ACM/IEEE 45th Annual Inter- national Symposium on Computer Architecture (ISCA), pp. 764–775. IEEE, 2018. Shkolnik, M., Chmiel, B., Banner, R., Shomron, G., Nahshan, Y ., Bronstein, A., and Weiser, U. Robust quan- tization: One model to rule them all. arXiv preprint arXiv:2002.07686, 2020. Tan, M. and Le, Q. Efﬁcientnet: Rethinking model scaling for convolutional neural networks. In International Con- ference on Machine Learning , pp. 6105–6114. PMLR, 2019. Uhlich, S., Mauch, L., Cardinaux, F., Yoshiyama, K., Garcia, J. A., Tiedemann, S., Kemp, T., and Nakamura, A. Mixed precision dnns: All you need is a good parametrization. In International Conference on Learning Representations, 2020. Wang, K., Liu, Z., Lin, Y ., Lin, J., and Han, S. Haq: Hardware-aware automated quantization with mixed pre- cision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8612– 8620, 2019.SDQ: Stochastic Differentiable Quantization with Mixed Precision Wang, Z., Wu, Z., Lu, J., and Zhou, J. Bidet: An efﬁcient bi- narized object detector. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2049–2058, 2020. Wen, W., Wu, C., Wang, Y ., Chen, Y ., and Li, H. Learning structured sparsity in deep neural networks. Advances in neural information processing systems, 29:2074–2082, 2016. Wu, B., Wang, Y ., Zhang, P., Tian, Y ., Vajda, P., and Keutzer, K. Mixed precision quantization of convnets via dif- ferentiable neural architecture search. arXiv preprint arXiv:1812.00090, 2018. Yamamoto, K. Learnable companding quantization for accurate low-bit neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5029–5038, 2021. Yang, H., Duan, L., Chen, Y ., and Li, H. Bsq: Explor- ing bit-level sparsity for mixed-precision neural network quantization. In International Conference on Learning Representations, 2020. Yang, L. and Jin, Q. Fracbits: Mixed precision quanti- zation via fractional bit-widths. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 35, pp. 10612–10620, 2021. Yao, Z., Dong, Z., Zheng, Z., Gholami, A., Yu, J., Tan, E., Wang, L., Huang, Q., Wang, Y ., Mahoney, M., et al. Hawq-v3: Dyadic neural network quantization. In Inter- national Conference on Machine Learning, pp. 11875– 11886. PMLR, 2021. You, Y .Audio Coding: Theory and Applications. Springer Science & Business Media, 2010. Yu, H., Han, Q., Li, J., Shi, J., Cheng, G., and Fan, B. Search what you want: Barrier panelty nas for mixed precision quantization. In European Conference on Computer Vi- sion, pp. 1–16. Springer, 2020. Zhang, D., Yang, J., Ye, D., and Hua, G. Lq-nets: Learned quantization for highly accurate and compact deep neural networks. In Proceedings of the European conference on computer vision (ECCV), pp. 365–382, 2018. Zhang, Z., Shao, W., Gu, J., Wang, X., and Luo, P. Dif- ferentiable dynamic quantization with mixed precision and adaptive resolution. In International Conference on Machine Learning, pp. 12546–12556. PMLR, 2021. Zhou, S., Wu, Y ., Ni, Z., Zhou, X., Wen, H., and Zou, Y . Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016.SDQ: Stochastic Differentiable Quantization with Mixed Precision Appendix This appendix includes additional analysis, implementation details, and extended experimental analysis not included in the main text due to space limitation. These contents are organized in separate sections as follows: • Sec. A analyzes how quantization error varies within different layers using mixed precision and how we propose a weighting coefﬁcient to balance between the precision. • Sec. B elaborates the performance of MPQ in different granularities of networks and the reasons why layer-wise quantization is optimal. • Sec. C includes the training details and experiment settings, such as the hyper-parameters, training dynamics, and detailed compression of bitwidth assignment as shown in Table 10. A. Quantization Error Analysis Quantization error Ω is deﬁned as L2 distance ||xq −xr||2 between quantized value xq and real value xr. In uniform quantization, Ωu depends on the characteristics of layers which includes parameter amounts, learned weight distribution, and bitwidth assignment. Among these factors, the most important one is the bitwidth of the layers. As shown in Table 8, the squared quantization error Ω2 u grows exponentially with the bitwidth. Table 8.Comparison of squared quantization error Ω2 u for various bitwidths of different layers in ResNet20. Layer Parameter Size 8-bit 6-bit 4-bit 3-bit 2-bit Layer1.2.conv1 2.3e4 0.03 0.52 9.25 41.9 191 Layer2.1.conv1 9.2e4 0.05 0.81 14.0 65.1 309 Layer3.1.conv1 3.7e5 0.29 4.83 84.8 392 2056 Uniform quantization deﬁned in Eq. 1 and Eq. 2 linearly maps full-precision weights wr into quantized representations wq. To remove the outlier of weight distribution, clamp operation is used to restrict the real value in the range[wl,wu]. The quantized output wq from uniform quantization scheme at b-bit with clamp can be deﬁned as: wq = Qu(wr; b,wl,wu) =s×round[clamp(wr; wl,wu)/s], (11) where [wl,wu] is the quantization range, ∆ = wu −wl is the range length, s= ∆ N−1 is the scaling factor, N = 2b is the total number of quantization bins. Previous work (You, 2010) has proved that the expected quantization error squared for the b-bit uniform quantizer is denoted as: E(Ω2 u; b,wl,wu) = s2 12 = C(b)∆2, (12) where C(b) = 1 12(2b−1)2 for the uniform distributions. This is the reason that we use λb = (2b −1)2 in Eq. 6 to balance between different bitwidths. In our stochastic quantization scheme, we assume the same quantization range [wl,wu] is used for neighboring quantization levels. The expectation of quantization error at bi bitwidth DBP βcan be computed as: E(Ω2 s; bi) =βbiE(Ω2 u; bi) + (1−βbi)E(Ω2 u; bi−1) = [βbiC(bi) + (1−βbi)C(bi−1)]∆2, (13) where C(bi) = 1 12(2bi−1)2 . Accordingly, the coefﬁcient λb = (2bi −1)2 can still be applied to balance the regularization term at neighboring quantization levels while amplifying the quantization error for lower bitbi−1 by [(2bi−1)/(2bi−1 −1)]2. B. SDQ on Different Granularities Our method uses the differentiability of DBPs to learn the bitwidth automatically. The advantage of the proposed SDQ is that DBPs can be inserted into different network granularities ﬂexibly. Table 9 compares the model performance, epochs for MPQ strategy generation, and optimization time per epoch using ResNet18 on different granularities.SDQ: Stochastic Differentiable Quantization with Mixed Precision Table 9.Comparison of Top-1 accuracy, total epochs for strategy generation, and time per epoch of different granularities with ResNet18 on ImageNet-1K. Granularity Bit-width(W/A) Top-1 Acc Epochs Time/epoch Net 4/4 68.7 30 47min Block 3.87/4 71.2 60 47min Layer 3.85/4 71.7 60 48min Kernel 3.91/4 71.8 90 58min As expected, while DBPs can be applied in different granularities, the layer-wise MPQ is still the optimal choice. When applying mixed precision to coarse-grained granularity, including the whole network and blocks, the performance of the trained MPQ network will be restricted because the difference of sensitivity within different components in the network is not fully utilized. Although searching the precision for ﬁner-grained granularity such as different kernels will improve the accuracy of the MPQ model, the time cost for the strategy generation will grow signiﬁcantly. Introducing more trainable parameters in the optimization process will result in more training instability. Moreover, current hardware accelerators rely highly on the parallelism of computing for the neural network. To the best of our knowledge, there are no kernel-wise hardware accelerators that successfully improve the efﬁciency in the real deployment of deep learning models. Based on these considerations, we focus more on layer-wise quantization when designing experiments. C. Training and Experimental Details C.1. Training settings and hyper-parameters When comparing the results of our SDQ with other quantization methods in Table 1 and Table 2, we use the training settings and hyper-parameters shown in Table 10. Generally, most of these hyper-parameters are the same during the MPQ generation and MPQ training phase, while we observe that a larger batch size yields better performance during MPQ training. Table 10.Detailed hyper-parameters and training scheme for different network architectures. Network ResNet20 on CIFAR10 ResNet18 on ImageNet-1K MobileNetV2 on ImageNet-1K Phase MPQ Generation MPQ Training MPQ Generation MPQ Training MPQ Generation MPQ Training Epoch 100 200 60 90 60 120 Batch Size 512 1024 256 512 256 512 Teacher - - ResNet101 ResNet101 ResNet101 ResNet101 Optimizer SGD SGD Adam Adam AdamW AdamW Initiallr 0.1 0.1 5e-4 1e-3 1e-3 1e-3 lr scheduler MultiStepLR MultiStepLR Consine Consine Consine Consine Weight decay 1e-4 1e-4 - - - - Warmup epochs - - - - 30 30 Random Crop ✓ ✓ ✓ ✓ ✓ ✓ Random Flip ✓ ✓ ✓ ✓ ✓ ✓ Color jittering - - ✓ ✓ - - λQin Eq. 10 1e-6 - 1e-7 - 1e-7 - λEin Eq. 6 - 5e-2 - 1e-2 - 1e-2 βthres 1e-4 - 1e-5 - 1e-5 - Bitwidth candidateB 1,2,3,4,5,6,7,8 - 2,3,4,5,6,7,8 - 2,3,4,5,6,7,8 - C.2. Training dynamics of SDQ models Fig. 7 illustrates the training loss and evaluation accuracy for our SDQ MobileNetV2 during the MPQ training phase. As shown in Fig. 1(c), quantized models are challenging to train as the latent non-smooth loss landscape makes models difﬁcult to converge with inaccurately estimated gradients. We found that our proposed Entropy-aware Bin Regularization targeting to preserve more information can stabilize the training. Especially, when the learning rate drops during training, our EBR can effectively reduce the jitters in loss and accuracy values.SDQ: Stochastic Differentiable Quantization with Mixed Precision 0 10000 20000 Training iterations of MobileNetV2 2.0 2.2 2.4 2.6 2.8 3.0Training Loss training loss training loss w/ EBR 25 50 75 100 125 Training Epochs of MobileNetV2 62 64 66 68 70 72Evaluation Top-1 Accuracy Validation Acc Validation Acc w/ EBR Figure 7.Training dynamics of SDQ with and without Entropy-aware Bin Regularization. C.3. Detailed Comparison of Quantization Strategy In Table 3, we compare our MobileNetV2 quantization strategy with Uhlich et al. (Uhlich et al., 2020) and FracBits (Yang & Jin, 2021) under the same initialization and training scheme. The results have demonstrated that our quantization is superior compared to previous strategies. All of the details on bitwidth assignment within each layer are shown in Fig. 8. We can see from the ﬁgure that all three bitwidth assignments follow similar patterns, such as the bitwidths for the ﬁrst few layers are higher than average. Generally, layers with more parameters will have lower precision, while the strategy here yields different bitwidths for layers with the same number of parameters. This supports our statement in Sec. 4.4 that there is no simple heuristics to assign bitwidth based on some particular metrics directly. 0 6 12 18 24 30 36 42 48 Layer index of MobileNetV2 2 3 4 5 6 7 8 9Bitwidth SDQ Uhlich et al. FracBits Figure 8.Comparison of quantization strategy of all layers in MobileNetV2.",
      "meta_data": {
        "arxiv_id": "2206.04459v3",
        "authors": [
          "Xijie Huang",
          "Zhiqiang Shen",
          "Shichao Li",
          "Zechun Liu",
          "Xianghong Hu",
          "Jeffry Wicaksana",
          "Eric Xing",
          "Kwang-Ting Cheng"
        ],
        "published_date": "2022-06-09T12:38:18Z",
        "pdf_url": "https://arxiv.org/pdf/2206.04459v3.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper presents Stochastic Differentiable Quantization (SDQ), a novel method for automatically learning optimal mixed-precision quantization (MPQ) strategies. It addresses the limitations of previous MPQ approaches, which often rely on costly search schemes (e.g., reinforcement learning, neural architecture search) or suffer from unstable optimization due to non-smooth gradient approximations. SDQ introduces Differentiable Bitwidth Parameters (DBPs) as probability factors for stochastic quantization between adjacent bitwidth choices, enabling flexible and globally-optimized bitwidth assignment with smoother gradient flow (via Gumbel-softmax reparameterization). The framework also integrates Quantization Error Regularization (QER) during strategy generation and Entropy-aware Bin Regularization (EBR) and knowledge distillation during post-training to preserve information and minimize quantization error. SDQ achieves state-of-the-art or even better accuracy than full-precision models with lower average bitwidths across various ResNet and MobileNet architectures and demonstrates superior efficiency on different hardware platforms (GPUs and FPGA).",
        "methodology": "SDQ operates in a two-stage process: 1) Quantization Strategy Generation and 2) Training with Quantization Strategy (Post-training). In the first stage, Differentiable Bitwidth Parameters (DBPs) are introduced for each layer and bitwidth candidate, acting as probability factors in a stochastic quantization scheme where weights are quantized to one of two adjacent bitwidths (bi or bi-1). The Straight-Through Gumbel SoftMax Estimator is used to enable smooth gradient backpropagation to the DBPs. A Quantization-Error Regularizer (LQER) is added to the loss function to penalize layers with high quantization error based on parameter count and bitwidth. The strategy is progressively generated by starting with high precision and reducing bitwidth when a DBP falls below a predefined threshold. The second stage involves quantization-aware training using the generated MPQ strategy. This stage incorporates Knowledge Distillation (LKD) from a full-precision teacher model to minimize the performance gap and an Entropy-aware Bin Regularizer (LEBR). LEBR normalizes real-valued weights and regularizes their distribution within quantization bins to be a 'sharp' Gaussian, aiming to preserve information and entropy. Activation bitwidth is kept fixed during post-training. The method primarily focuses on layer-wise quantization due to its optimality compared to coarser or finer granularities.",
        "experimental_setup": "Experiments were conducted on CIFAR-10 (ResNet20), ImageNet-1K (ResNet18, MobileNetV2), and COCO detection dataset (YOLOv4-tiny). Data augmentation included RandomResizedCrop, RandomHorizontalFlip, and Color Jittering for specific models. Real-value pre-trained weights were used for initialization. The first and last layers' bitwidths were fixed due to their sensitivity. The evaluation metrics included Top-1 and Top-5 accuracy for image classification and Average Precision (AP, AP50, AP75, APS, APM, APL), Latency, Energy, and FPS for object detection. Ablation studies were performed to validate the contributions of SDQ strategy generation, knowledge distillation, and Entropy-aware Bin Regularization. Hardware efficiency was demonstrated through deployment experiments on various NVIDIA GPUs, a Bit Fusion accelerator (supporting power-of-two bitwidths), and a custom Xilinx U50 FPGA system for object detection, assessing latency and energy consumption.",
        "limitations": "The paper implies several limitations. Finer-grained quantization, such as kernel-wise, while potentially improving accuracy, introduces significant training instability due to a larger number of optimization parameters and is not practically supported by current hardware accelerators for efficient deployment. Coarser-grained quantization (e.g., block-wise or network-wise) restricts performance because it doesn't fully exploit the sensitivity differences between individual layers. The Bit Fusion accelerator, used in hardware experiments, only supports power-of-two bitwidths (2, 4, 8, 16 bits), leading to a performance gap between theoretical compression and real compression for non-power-of-two bitwidth assignments. The method uses a two-stage approach (strategy generation and then training), suggesting that simultaneously optimizing DBPs and weight parameters in a single stage could lead to instability.",
        "future_research_directions": "Not mentioned"
      }
    },
    {
      "title": "SDQ: Stochastic Differentiable Quantization with Mixed Precision",
      "abstract": "In order to deploy deep models in a computationally efficient manner, model\nquantization approaches have been frequently used. In addition, as new hardware\nthat supports mixed bitwidth arithmetic operations, recent research on mixed\nprecision quantization (MPQ) begins to fully leverage the capacity of\nrepresentation by searching optimized bitwidths for different layers and\nmodules in a network. However, previous studies mainly search the MPQ strategy\nin a costly scheme using reinforcement learning, neural architecture search,\netc., or simply utilize partial prior knowledge for bitwidth assignment, which\nmight be biased and sub-optimal. In this work, we present a novel Stochastic\nDifferentiable Quantization (SDQ) method that can automatically learn the MPQ\nstrategy in a more flexible and globally-optimized space with smoother gradient\napproximation. Particularly, Differentiable Bitwidth Parameters (DBPs) are\nemployed as the probability factors in stochastic quantization between adjacent\nbitwidth choices. After the optimal MPQ strategy is acquired, we further train\nour network with entropy-aware bin regularization and knowledge distillation.\nWe extensively evaluate our method for several networks on different hardware\n(GPUs and FPGA) and datasets. SDQ outperforms all state-of-the-art mixed or\nsingle precision quantization with a lower bitwidth and is even better than the\nfull-precision counterparts across various ResNet and MobileNet families,\ndemonstrating the effectiveness and superiority of our method.",
      "full_text": "SDQ: Stochastic Differentiable Quantization with Mixed Precision Xijie Huang 1 Zhiqiang Shen 1 2 3 Shichao Li 1 Zechun Liu 4 Xianghong Hu 1 5 Jeffry Wicaksana 1 Eric Xing 6 2 Kwang-Ting Cheng 1 Abstract In order to deploy deep models in a compu- tationally efﬁcient manner, model quantization approaches have been frequently used. In ad- dition, as new hardware that supports mixed bitwidth arithmetic operations, recent research on mixed precision quantization (MPQ) begins to fully leverage the capacity of representation by searching optimized bitwidths for different lay- ers and modules in a network. However, pre- vious studies mainly search the MPQ strategy in a costly scheme using reinforcement learning, neural architecture search, etc., or simply utilize partial prior knowledge for bitwidth assignment, which might be biased on locality of informa- tion and is sub-optimal. In this work, we present a novel Stochastic Differentiable Quantization (SDQ) method that can automatically learn the MPQ strategy in a more ﬂexible and globally- optimized space with smoother gradient approxi- mation. Particularly, Differentiable Bitwidth Pa- rameters (DBPs) are employed as the probability factors in stochastic quantization between adja- cent bitwidth choices. After the optimal MPQ strategy is acquired, we further train our net- work with Entropy-aware Bin Regularization and knowledge distillation. We extensively evaluate our method for several networks on different hard- ware (GPUs and FPGA) and datasets. SDQ out- performs all state-of-the-art mixed or single pre- cision quantization with a lower bitwidth and is even better than the full-precision counterparts across various ResNet and MobileNet families, demonstrating its effectiveness and superiority.1 1Hong Kong University of Science and Technology2Mohamed bin Zayed University of Artiﬁcial Intelligence 3Jockey Club Institute for Advanced Study, HKUST 4Reality Labs, Meta Inc 5ACCESS - AI Chip Center for Emerging Smart Systems 6Carnegie Mellon University. Correspondence to: Zhiqiang Shen <zhiqiangshen0214@gmail.com>. Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s). 1Project: https://huangowen.github.io/SDQ/. Layer 𝒍 𝛽𝑙,𝑏𝑖+1 𝛽𝑙,𝑏𝑖… … 𝛽𝑙,𝑏2 𝛽𝑙,𝑏1 Differentiable Bitwidth Parameters Full Precision Teacher Model KD Loss Entropy-aware Bin Regularization Layers  with  Mixed  Precision Efficient Mixed Precision Model 𝑸𝒃𝒊+𝟏 𝑸𝒃𝒊 𝑸𝒃𝒊−𝟏 𝒘𝒍,𝒃𝒊 𝒒 = ൝ 𝑸𝒃𝒊(𝒘𝒍 𝒓), 𝒑 = 𝜷𝒍,𝒃𝒊 𝑸𝒃𝒊−𝟏(𝒘𝒍 𝒓), 𝒑 = 𝟏−𝜷𝒍,𝒃𝒊 𝑸𝒃𝒊 𝒃𝒊 -bit uniform quantizer … 𝑸𝒃𝟏 (a) Proposed Stochastic Differentiable Quantization (SDQ) 1.00 0.75 0.50 0.25 0.000.250.500.751.00 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 10 20 30 40 50 10 20 30 40 50 (b) Full Precision 1.00 0.75 0.50 0.25 0.000.250.500.751.00 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 2 4 6 8 10 (c) Interpolation 1.00 0.75 0.50 0.25 0.000.250.500.751.00 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 2 4 6 8 10 2 4 6 8 10 (d) Stochastic Figure 1.1(a) demonstrates how our SDQ generates optimal MPQ strategy and train a MPQ network effectively. 1(b), 1(c), 1(d) compare the underlying loss optimization landscape (Li et al., 2018) of a full precision model, MPQ of ResNet20 with linear interpolation (Yang & Jin, 2021; Nikoli´c et al., 2020), and MPQ of ResNet20 with stochastic quantization. 1. Introduction Deep learning models such as Convolutional Neural Net- works (CNNs) have demonstrated outstanding performance on various tasks, e.g., visual recognition (Krizhevsky et al., 2012; He et al., 2016; Tan & Le, 2019). Their latency, energy consumption, and model size have become the most signiﬁ- cant factors for consideration of the deployment with these deep learning models. In recent years, researchers have studied the design, training, and inferencing techniques of deep learning models for greater computation efﬁciency, in- cluding compact network design and search (Howard et al., 2017; Pham et al., 2018; Guo et al., 2020), knowledge distil- lation (Hinton et al., 2015), pruning (Liu et al., 2017; 2018), quantization (Zhou et al., 2016; Zhang et al., 2018), and sparsity (Wen et al., 2016). Quantization techniques have attracted particular attention because emerging hardware arXiv:2206.04459v3  [cs.LG]  11 Jul 2022SDQ: Stochastic Differentiable Quantization with Mixed Precision accelerators (Judd et al., 2016; Jouppi et al., 2017; Sharma et al., 2018) begin to support low-precision inference. To fully exploit the difference of representative capacity and redundancy in different parts (blocks, layers, and ker- nels) of deep neural networks, mixed precision quantization (MPQ) techniques have been put forward to assign different bitwidths to various components in a network. To achieve an MPQ network with satisfactory accuracy and compression, two critical problems must be solved consecutively: First, given a pre-trained network with weights {W(l)}L l=1 and a training dataset D, how to ﬁnd an optimal quantization strat- egy {b(l)}L l=1? Prior solutions addressed this general goal for MPQ are primarily based on searching or optimization. Whereas, the search-based methods which conduct an iter- ative scheme to explore the best structure cannot globally optimize the MPQ network for all modules and are also ex- pensive to learn2. Meanwhile, existing optimization-based methods, such as FracBits (Yang & Jin, 2021), suffer from unstable optimization as the approximation design for gra- dients is not smooth and stable. Consequently, an ideal solution should be differentiable which enables automatic optimization of the bitwidth assignment in a global opti- mization space with more precise gradient approximations and smoother latent landscape as illustrated in Figure 1(d), which derives the primary motivation of this work. The second problem stems from the quantization-aware post-training: after the quantization strategy {b(l)}L l=1 is ac- quired, how can we train a quantized network {W(l) b(l) }L l=1 for maximizing the potential of performance and minimizing the accuracy sacriﬁce compared to a full-precision model? Empirically, during quantization training, the input informa- tion should be preserved and quantization error should be minimized through proper loss function design. To address the above problems, in this work, we propose a novel Stochastic Differentiable Quantization (SDQ) to tackle the challenges of searching and training MPQ. Con- cretely, we present a one-shot solution via representing the choice of discrete bitwidths as a set of Differentiable Bitwidth Parameters (DBPs), as shown in Fig. 1(a). DBPs are utilized in the forward path of the network as probabil- ity factors during the stochastic quantization. During the quantization strategy generation, DBPs will be updated to learn the optimal bitwidth assignment automatically. Dur- ing the backpropagation, we use Gumbel-softmax reparam- eterization to estimate the gradient so the gradient can be back-propagated smoothly through our DBPs. Compared to previous differentiable solutions (Nikoli´c et al., 2020; Yang & Jin, 2021) using linear interpolation, the stochastic quan- tization scheme can substantially improve the network train- 2For L layers with B bitwidth candidate, searching both layer- wise weights and activations has a time complexity of O(B2L). ing stability and help the DBPs converge with a smoother loss landscape shown in Fig. 1(d). Addressing the challenge imposed by the second problem, we propose an Entropy-aware Bin Regularizer (EBR) based on entropy analysis, which regularizes the weights to keep more information-carrying components while considering the various precision of different layers. Knowledge dis- tillation is also used to fully leverage the representation capability of full-precision teacher models. We demonstrate the advantage of our SDQ in terms of effec- tiveness and efﬁciency on different networks and datasets. For ResNet18 on the ImageNet-1K dataset, our quantized model can signiﬁcantly improve top-1 accuracy (71.7%) compared to the full precision model (70.5%) with average bitwidth of both weights and activations no more than 4 bits. We also deploy ResNet18 on Bit Fusion (Sharma et al., 2018) accelerator to show the efﬁciency of our SDQ models. In summary, our contribution can be concluded as: • We present a novel SDQ framework to learn the opti- mal mixed precision quantization strategy via a set of differentiable bitwidth parameters as probability fac- tors during the stochastic quantization. • We utilize straight-through Gumbel-softmax estima- tor in the gradient computation w.r.t. differentiable bitwidth parameters. We also incorporate the quantiza- tion error regularization term while learning the mixed precision quantization strategy. • We propose an Entropy-aware Bin Regularizer to min- imize the quantization error while considering mixed precision, which helps preserve more information- carrying components. • We extensively evaluate our method on different net- work architectures and datasets. We further conduct deployment experiments on various hardware (GPUs and a real FPGA system) to demonstrate the effective- ness and efﬁciency of our models. 2. Related Work Quantization techniques can be categorized into uniform and non-uniform quantization based on the quantization in- tervals. Non-uniform quantization (Miyashita et al., 2016; Zhang et al., 2018; Li et al., 2019b), due to its ﬂexible representation, usually can better allocate the quantization values to minimize the quantization error and achieve better performance than uniform schemes. In addition, the quan- tization methods can also be classiﬁed as stochastic and deterministic quantization. Inspired by gradient estimation of stochastic neurons (Bengio et al., 2013), stochastic quan- tization (SQ) (Courbariaux et al., 2015; 2016; Dong et al., 2019a) has been explored. Unlike previous deterministic quantization, real-value variables of SQ are quantized toSDQ: Stochastic Differentiable Quantization with Mixed Precision different quantization levels controlled by a probability dis- tribution. For example, BinaryConnect (Courbariaux et al., 2015) transforms the real-value weights into +1 or -1 with probability determined by the distance to the zero point. However, the aforementioned quantization schemes solely assign the same bitwidth to the entire model. Mixed preci- sion quantization (MPQ), which employs different bitwidths in distinct layers or modules, can achieve a higher compres- sion ratio and improved model capabilities. MPQ is a more promising direction in general, and it usually is divided into three categories: Search-Based Methods, Metric-Based Methods, and Optimization-Based Methods. Search-Based Methods usually utilize Neural Architecture Search (Wu et al., 2018; Yu et al., 2020; Guo et al., 2020) or Reinforcement Learning (Wang et al., 2019; Elthakeb et al., 2019) to perform searching of quantization strategies. For instance, HAQ (Wang et al., 2019) leverages reinforce- ment learning and incorporates hardware feedback in the loop. DNAS (Wu et al., 2018) explore the quantized search space with gradient-based optimization to improve the ef- ﬁciency. Despite these efforts to increase the efﬁciency of searching, the time and computational cost of generaliz- ing search-based approaches to various network designs, datasets, and hardware platforms remain impediments. Metric-Based Methods target at assigning bitwidth con- sidering speciﬁc metrics that can be computed easily. HAWQ (Dong et al., 2019c;b; Yao et al., 2021) gener- ates MPQ strategy based on the layers’ Hessian spec- trum. OMPQ (Ma et al., 2021) utilizes layer orthogonality to construct a linear programming problem to derive the bitwidth conﬁguration. SAQ (Liu et al., 2021) determines the bitwidth conﬁgurations of each layer, encouraging lower bits for layers with lower sharpness. Although these meth- ods are highly computation efﬁcient, the generated MPQ strategies are usually sub-optimal as the mapping from these metrics to the bitwidths is manually established. Optimization-Based Methods formulate the MPQ strategy generation problem as an optimization problem. The core challenge is to tackle the non-differentiability of task loss w.r.t. the bitwidth assignment. FracBits (Yang & Jin, 2021) proposed a fractional bitwidth parameter and used linear in- terpolation during the forward of quantization. DDQ (Zhang et al., 2021) proposed a method to decompose the quan- tizer operation into the matrix-vector product. Uhlich et al. (Uhlich et al., 2020) proposed differentiable quantization via parametrization. These methods either introduce ex- tra parameters such as quantization levels and the dynamic ranges of quantizers as the optimization target, or utilize linear interpolation between different quantization levels. The interpolation operation will introduce a large number of useless optimization targets for quantization, and more instability is expected. Noticeably, our work falls into the categories of uni- form, quantization-aware training , and optimization- based methods of mixed precision network. To the best of our knowledge, SDQ is the ﬁrst quantization strat- egy that adopts stochastic quantization to optimize the bitwidth assignment. Compared to previous research which also leverages the idea of differentiable bitwidth such as FracBits (Yang & Jin, 2021) and BitPruning (Nikoli´c et al., 2020), SDQ provides better training stability compared to the naive linear interpolation. Our DBPs can further denote discrete bitwidth candidates to better match the conﬁgura- tion of particular hardware accelerators, while others cannot, e.g., Bit Fusion (Sharma et al., 2018) only supports powers of 2 bitwidth. 3. Method 3.1. Preliminaries In network quantization, real-valued weights wr and ac- tivations xr are converted to low-precision value wq and xq. To mitigate the problem that gradient-based optimiza- tion cannot be directly applied to quantized value, previous research like DoReFa-Net (Zhou et al., 2016) exploits a straight-through estimator (STE) (Bengio et al., 2013) to assign the gradients passing in xi and out xo to be equal. Assume the loss function for the model is L, an STE for b-bit uniform quantizer qb can be denoted as: Forward:xo = qb(xi) = 1 2b−1round[(2b−1)xi]; Backward: ∂L ∂xo = ∂L ∂xi . (1) Here xi,xo ∈[0,1]. The weights and activations are ﬁrst linearly transformed and clamped to interval [0,1]. The complete scheme for b-bit uniform quantizer Qb is: xq = Qb(xr) =qb( tanh(xr) 2 max(|tanh(xr)|) + 1 2) −1. (2) 3.2. Generating Quantization Strategy In the quantization strategy generation phase, our goal is to learn the optimal bitwidth assignment. We introduce Differ- entiable Bitwidth Parameters (DBPs) {βl,bi}for each layer l ∈Land bitwidth candidate bi ∈B, where iis the index of bitwidth in the candidate set B. DBPs are initialized to 1 to represent a deterministic quantization at different levels. During forward, the weights can be quantized to two differ- ent bitwidths: the current bitwidth bi and next bitwidth bi−1 in the candidate sets. The probability of quantization into two bitwidths is controlled by the DBP βl,bi at this layer. wq l,bi = { Qbi(wr l) with probabilityp= βl,bi Qbi−1(wr l) with probabilityp= 1−βl,bi , (3)SDQ: Stochastic Differentiable Quantization with Mixed Precision where wq l,bi represents the quantized weight under bi-bit stochastic-bitwidth quantizer. According to the character- istic of STE introduced in Eq. 1, the gradient through the quantization values and real values are the same: ∂L ∂wr l = ∂L ∂Qbi(wr l) = ∂L ∂Qbi−1 (wr l) .Thus, the expected gradients of quantized weight wq l,bi over all trainable parameters in the network can be computed as: E[ ∂L ∂wq l,bi ] =βl,biE[ ∂L ∂Qbi(wrl)] + (1−βl,bi)E[ ∂L ∂Qbi−1(wrl)] =E[ ∂L ∂wrl,bi ], (4) which is the same as the conventional STE. The derived gradient shows the proposed DBPs will not inﬂuence the gradient-based optimization of weight parameters during the training. However, the real gradient still cannot backpropagate di- rectly through the probability parameter βl,bi. To tackle this problem, we use Straight-Through Gumbel SoftMax Es- timator (Jang et al., 2016) as a reparameterization trick to allow gradients ﬂow from quantized value wq l,bi to DBP βl,bi. We can deﬁne the forwarding of previous stochastic-bitwidth quantization as wq l,bi = ck l,biQbi(wr l) + (1 −ck l,bi)Qbi−1 (wr l), where ck l,bi ∈ {0,1}is a choice variable and it follows a Bernoulli distribution ck l,bi ∼ Bernoulli(βl,bi). We use Gumbel SoftMax to transform the choice variable into a “soft” sampling operation while maintaining the distribution characteristics via: ckl,bi = exp((log(βl,bi) +gk)/τ) exp((log(βl,bi) +gk0)/τ) +exp((log(1−βl,bi) +gk1)/τ), (5) where gk, gk0 and gk1 are samples drawn from Gumbel(0,1) distribution, and τ is the temperature coefﬁcient to control the generated distribution. As has been proven in previous research (Jang et al., 2016), the gradient now can be easily computed and the expected value of gradientw.r.t.quantized weights given in Eq. 4 still holds. The advantage of MPQ is that different sensitivity of each layer to the quantization perturbation is fully considered. Previous research (Dong et al., 2019c) has shown that this sensitivity is closely aligned to the number of parameters and quantization error of this layer. In light of this, we propose a quantization-error regularizer to penalize layers with a large number of parameters which result in high quantization error: LQER = L∑ l βl,biλbi||wq l,bi −wr l||2 2, (6) where λbi is a weighting coefﬁcient to balance between dif- ferent bitwidth, and ||wq l,bi −wr l||2 2 is the L2-norm of quan- tization error. This term is highly related to the bitwidth as it increases exponentially with the bitwidth. In our practice, we use λbi = (2|bi|−1)2 to balance between different pre- cision (please see Appendix A for more details). We only optimize DBPs and will not optimize weight parameters with this quantization-error regularization. The complete optimization objective in this stage can be formulated as: O= arg min W,β Ltask + arg min β λQLQER, (7) where Ltask is the task loss and λQ is the coefﬁcient to bal- ance between task loss and quantization error regularization. During the quantization strategy generation phase, we start from the highest precision in the bitwidth candidate sets, and progressively reduce the bitwidth. When DBP at layer l satisﬁes the condition βl,bi < βt, where βt represents the pre-deﬁned DBP threshold, the bitwidth assignment for layer lwill be reduced from bi to bi−1 and we will start to optimize βl,bi−1 . After a pre-deﬁned number of epochs for training, we can generate an MPQ strategy bl for each layer l∈Lbased on the current DBPs{βl,b0 ,···βl,bn}that have been optimized. 3.3. Training with Quantization Strategy While prior MPQ research (Uhlich et al., 2020; Zhang et al., 2021) combined the searching and training stages, we choose a two-stage strategy since the optimization objectives are distinct in these two periods. In addition, more instability of training is expected when more quantization parameters are optimized simultaneously (i.e. DBPs) with the weight parameters. In light of this, we apply quantization-aware training with the generated MPQ strategy and without the DBPs. The total loss Lfor optimization in this stage is: L= LKD + λELEBR, (8) where LKD denotes the knowledge distillation loss and LEBR denotes the entropy-preserving bin regularization. λE is the weighting coefﬁcient to balance between them. We will introduce them in Sec. 3.3.1 and Sec. 3.3.2, respectively. 3.3.1. K NOWLEDGE DISTILLATION Intrinsically, a quantized classiﬁcation network should learn an ideal similar mapping from input images to the output logits as a full-precision network, and the performance gap between them needs to be minimized. Based on this insight, we use knowledge distillation to train our MPQ network with a full-precision model as the teacher. The loss function is designed to enforce the similarity between the output dis- tribution of full-precision teacher and MPQ student model: LKD = −1 N ∑ c N∑ i=1 pFθ c (Xi) log(pQθ c (Xi)) (9) where the KD loss is deﬁned as the cross-entropy between the output distributions pc of a full-precision teacher Fθ and a MPQ student Qθ. Xi is the input sample. cand NSDQ: Stochastic Differentiable Quantization with Mixed Precision denote the classes and the number of samples, respectively. Note that this process can be regarded as the distribution calibration for the student network, and one-hot label is not involved in training. 3.3.2. E NTROPY -AWARE BIN REGULARIZATION In previous quantization methods, real-valued weights are usually distributed uniformly. As a result, the quantized weight might collapse to a few quantization bins that close to zero. The information represented by the weights is heavily reduced during this process. According to the information theory, entropy should be preserved in quantization to retain the representation capacity of neural networks. In our method, after MPQ strategy is adopted, we propose Entropy-aware Bin Regularization to regularize weights in group of different bitwidth. The entropy carried by quan- tized weight with a speciﬁc bitwidth bcan be denoted as Hb(W) = −pb(wi)log(pb(wi)),s.t. ∑2b i=1 pb(wi) = 1, where pb(wi) indicates the proportion of weights quan- tized to i-th quantization bin. We can derive that the en- tropy Hb(W) is maximized when p∗ b(wi) = 1/2b for all i∈{1,... 2b}. In light of the entropy analysis and inspired by previous research (Li et al., 2019b; Yamamoto, 2021), we ﬁrst normalize real-valued weights wr l of layer lwith bitwidth bto wr∗ l = 2(b−1) 2b−1 |wr l| ||wr l||1 wr l. The corresponding quantized weights wq l are approximated to uniformly dis- tribute in all quantization levels. Here, |wr l|is the number of entries in wr l, and ||wr l||1 computes the L1-norm of wr l. Different from weight normalization in APOT (Li et al., 2019b) and LCQ (Yamamoto, 2021) which only consider the global weight distribution, we also want to minimize the quantization error in a speciﬁc quantization bin to reduce the information loss. Therefore, we regularize the distribution of the weights in a quantization bin to be a “sharp” Gaussian distribution (i.e. a Dirac delta distribution ideally): its mean approaching quantization value and variance approaching zero. Motivated by these considerations, the Entropy-aware Bin Regularization is formulated as: LEBR = L∑ l=1 2b(l) ∑ n=1 Lmse(wr n,l,wq n,l) +V(wr n,l), (10) where wr n,l,wq n,l represent the real value and quantized value of weights in layer l and quantization bin n. Lmse computes the mean square error,(·) computes the mean, and V(·) computes variance for all quantization bins with more than two elements. 3.4. Method Analysis and Discussion The complete algorithm of our proposed SDQ is summa- rized in Alg. 1. Since our SDQ only performs stochastic quantization on weights, the precision of activations is unaf- fected throughout the quantization strategy generation phase. We freeze the activation bitwidth with a ﬁxed value for the entire network during the training phase, and the activation is quantized in the same way as denoted by Eq. 2. Also note that our stochastic quantization is only performed on quantization strategy generation phase and is not applied during post-training or real inference. While we only demonstrate how to use SDQ on layer-wise quantization. Our approach can easily be adapted to quan- tize models at many levels of granularity, such as block-wise, net-wise, and kernel-wise. However, the performance will be inhibited when using SDQ on coarse-grained granularity, such as blocks and networks, since the sensitivity difference between each layer will not be fully exploited. Additional training instability is expected when SDQ is used for ﬁne- grained kernel-wise quantization because more quantization parameters are optimized alongside weight parameters dur- ing the MPQ strategy learning phase. Furthermore, current hardware accelerators cannot implement kernel-wise quan- tization, limiting the actual performance when deploying quantized models. Appendix B contains further studies demonstrating that SDQ for layer-wise quantization pro- duces the best results. Algorithm 1 Stochastic Differentiable Quantization Input: the network with full precision weight {Wr}L l=1, differentiable bitwidth parameters {β}L l=1, bitwidth candi- date set B, maximum training epoch EG (epoch for generat- ing quantization strategy) and ET (epoch for post-training), threshold βt for decaying bitwidth Output: quantized network with weights {Wq}L l=1 and bitwidth allocations {b(l)}L l=1 1: // Phase 1: Generating Quantization Strategy 2: for l= 1to L, b∈B do 3: Initialize differentiable bitwidth parameters βl,b = 1 4: end for 5: for epoch = 1 to EG do 6: Quantize real-value weight Wr to Wq by Eq. 3 7: Compute the task loss Ltask and LQER by Eq. 6 8: Compute gradient ∇Wr and ∇β, update parameters 9: Update {b(l)}L l=1 according to {β}L l=1 and βt 10: end for 11: Generate MPQ strategy {b(l)}L l=1 from DBPs {β}L l=1 12: // Phase 2: Post-training with Quantization Strategy 13: for epoch = 1 to ET do 14: Quantize real-value weights and activations by Eq. 2 15: Compute the knowledge distillation loss LKD and regularization LEBR by Eq. 9 and Eq. 10 16: Compute gradient ∇Wr, update parameters 17: end for 18: Return MPQ network {Wq}L l=1 with strategy {b(l)}L l=1SDQ: Stochastic Differentiable Quantization with Mixed Precision 4. Experiments To evaluate the effectiveness of the proposed SDQ, we con- duct experiments on the CIFAR and ImageNet-1K datasets. We ﬁrst introduce the dataset, network, and training strategy in Sec. 4.1, followed by the comparison with state-of-the-art quantization methods in Sec. 4.2. We then analyze the effect and role of each proposed component of SDQ in Sec. 4.3. In Sec. 4.4, we visualize the MPQ strategy and how our proposed SDQ improves the representation capacity of the quantized model. Hardware deployment experiments on different devices are designed to demonstrate the energy and time efﬁciency of our models in Sec. 4.5. 4.1. Experimental Settings Dataset The experiments are carried out on CIFAR- 10 dataset (Krizhevsky et al., 2009) and ImageNet-1K dataset (Deng et al., 2009). We only perform basic data aug- mentation in PyTorch (Paszke et al., 2019), which includes RandomResizedCrop and RandomHorizontalFlip during training, and single-crop operation during evaluation. Network We evaluate ResNet20 on CIFAR-10, and evaluate ResNet18 and MobileNetV2 on ImageNet-1K dataset. Due to the fact that the ﬁrst and last layers are more sensitive to quantization perturbation compared to intermediate layers, we ﬁx the bitwidth of them following previous work (Yang & Jin, 2021). Training detail Following previous quantization meth- ods (Zhou et al., 2016; Jung et al., 2019), we adopt real- value pre-trained weights as initialization. We train and evaluate our models on various hardware platforms, includ- ing various NVIDIA GPUs and FPGA. Details of all hyper- parameters and training schemes are shown in Appendix C. 4.2. Comparison with State-of-the-Art Methods Table 1 compares our SDQ with existing methods for ResNet20 on CIFAR-10. We can see that our SDQ models signiﬁcantly outperform the previous ﬁxed precision quan- tization methods while also yielding better accuracy than other MPQ models with fewer average bitwidth. Table 1.Comparison with state-of-the-art mixed precision quanti- zation methods (ResNet20 on CIFAR-10). Method Bit-widthmixed Accuracy(%)WCR(W/A) Top-1 FP Top-1 Dorefa(Zhou et al., 2016)2/32 88.2 92.4 16 ×PACT (Choi et al., 2018)2/32 89.7 92.4 16 ×LQ-net(Zhang et al., 2018)2/32 91.1 92.4 16 × TTQ (Jain et al., 2019)2.00/32 ✓ 91.2 92.4 16 ×Uhlich et al. (Uhlich et al., 2020)2.00/32 ✓ 91.4 92.4 16 ×BSQ (Yang et al., 2020)2.08/32 ✓ 91.9 92.6 15.4 ×DDQ (Zhang et al., 2021)2.00/32 ✓ 91.6 92.4 16 × Ours 1.93/32 ✓ 92.1 92.4 16.6× Table 2 shows the ImageNet-1K classiﬁcation performance of our SDQ on ResNet18 and MobileNetV2. Since different methods adopt different full-precision (FP) models as ini- tialization, we also report the corresponding Top-1 accuracy of the FP model for different methods. Compared to the baseline FP model, our SDQ-trained mod- els perform comparably or even better when quantized to low precision. For example, our ResNet18 achieves 71.7% Top-1 accuracy when the average bitwdiths for weights and activations are 3.85 and 4 respectively, which has a 1.2% absolute gain on the full-precision model. Our quantized MobileNetV2 is the ﬁrst quantized model with accuracy higher than FP initialization (72.0% vs. 71.9%), and average precision for weights and activations is lower than 4. With an optimal MPQ strategy and an effective training scheme, our SDQ outperforms previous quantization meth- ods signiﬁcantly under the same bitwidth conﬁguration. To the best of our knowledge, the highest accuracy re- ported by previous uniform methods with weights and acti- vation bitwidths smaller than 4 bits are 70.6% on ResNet18 and 71.6% on MobileNetV2 from FracBits-SAT (Yang & Jin, 2021). Our SDQ achieves an increase of 1.1% on ResNet18 and 0.3% on MobileNetV2 with a more com- pact bitwidth (3.85/4 vs. 4/4 on ResNet18, 3.79/4 vs. 4/4 on MobileNetV2). Note that SDQ even outperforms the state-of-the-art non-uniform quantization methods (Li et al., 2019b; Chang et al., 2021; Zhang et al., 2021), proving the effectiveness and superiority of our method. 4.3. Ablation Study We further conduct ablation studies to prove the contribu- tion of different components to our model’s performance, including SDQ quantization strategy generation, knowledge distillation, and Entropy-aware Bin Regularization. Table 3 compares different quantization strategy generation schemes impartially on the same initialization and training. The per- formance of the quantized model with the quantization strat- egy generated by our SDQ is close to previous MPQ models, while the average bitwidths of our strategy are lower. This comparison reﬂects that our strategy generation contributes to the improvement of accuracy. Table 3.Comparison with state-of-the-art mixed precision quanti- zation strategy generation methods under same training and initial- ization (MobileNetV2 on ImageNet-1K). MPQ Strategy Bit-width Accuracy(%) (W/A) Top-1 Top-5 HMQ (Habi et al., 2020) 3.98/4 71.7 90.1 FracBits (Yang & Jin, 2021) 4/4 72.0 90.4 Our strategy 3.79/4 72.0 90.5 The comparison of different weight regularization methods is shown in Table 4. The experiments are conducted on MPQ ResNet18 with average bitwidths of 3.85 and 2 for weights and activations. The insights of our proposed EBR are to penalize the global distribution of weights to be overSDQ: Stochastic Differentiable Quantization with Mixed Precision Table 2.Comparison with state-of-the-art mixed precision quantization methods (ResNet18 and MobileNetV2 on ImageNet-1K). Bitwidth (W/A) denotes the average bitwidth for weights and activation parameters. WCR represents the weight compression rate. BitOPs denotes the bit operations. For a ﬁlter f, the BitOPs is deﬁned as BitOPs(f) =bwba|f|wf hf /s2 f , where bw and ba are the bitwidths for weights and activations, |·| denotes the cardinality of the ﬁlter, wf ,hf ,sf are the spatial width, height, and stride of the ﬁlter. Network Method Bit-width Mixed Uniform Accuracy (%) WCR Model BitOPs (G)(W/A) Top-1 FP Top-1 Size (MB) ResNet18 Dorefa†(Zhou et al., 2016) 4/4 ✓ 68.1 70.5 8 × 5.8 35.2 PACT†(Choi et al., 2018) 4/4 ✓ 69.2 70.5 8 × 5.8 35.2 LQ-net (Zhang et al., 2018) 4/4 69.3 70.5 8 × 5.8 35.2 APOT (Li et al., 2019b) 4/4 70.7 70.5 8 × 5.8 34.7 DNAS†(Wu et al., 2018) -/- ✓ ✓ 70.6 71.0 8 × 5.8 35.2 HAQ (Wang et al., 2019) 4/32 ✓ ✓ 70.4 70.5 8 × 5.8 465 EdMIPS (Cai & Vasconcelos, 2020)4/4 ✓ ✓ 68.0 70.2 8 × 5.8 34.7 HAWQ-V3†(Yao et al., 2021) 4.8/7.5 ✓ ✓ 70.4 71.5 6.7 × 7.0 72.0 Chen et al. (Chen et al., 2021) 3.85/4 ✓ ✓ 69.7↓0.1% 69.8 8.3 × 5.6 33.4 FracBits-SAT (Yang & Jin, 2021)4/4 ✓ ✓ 70.6↑0.4% 70.2 8 × 5.8 34.7 Uhlich et al. (Uhlich et al., 2020)3.88/4 ✓ 70.1 70.3 8.3 × 5.6 33.7 RMSMP (Chang et al., 2021) 4/4 ✓ 70.7 70.3 8 × 5.8 34.7 DDQ (Zhang et al., 2021) 4/4 ✓ 71.2 70.5 8 × 5.8 34.7 Ours 3.85/8 ✓ ✓ 72.1↑1.6% 70.5 8.3× 5.6 62.6 3.85/4 ✓ ✓ 71.7↑1.2% 70.5 8.3× 5.6 33.4 3.85/3 ✓ ✓ 70.2↓0.3% 70.5 8.3× 5.6 25.1 3.85/2 ✓ ✓ 69.1↓1.4% 70.5 8.3× 5.6 16.7 MobileNetV2 Dorefa†(Zhou et al., 2016) 4/4 ✓ 61.8 71.9 8 × 1.8 7.42 PACT†(Choi et al., 2018) 4/4 ✓ 61.4 71.9 8 × 1.8 7.42 LQ-net (Zhang et al., 2018) 4/4 64.4 71.9 8 × 1.8 7.42 APOT (Li et al., 2019b) 4/4 71.0 71.9 8 × 1.8 5.35 HAQ (Wang et al., 2019) 4/32 ✓ ✓ 71.5 71.9 8 × 1.8 42.8 HMQ (Habi et al., 2020) 3.98/4 ✓ ✓ 70.9 71.9 8.1 × 1.7 5.32 Chen et al. (Chen et al., 2021) 4.27/8 ✓ ✓ 71.8↓0.1% 71.9 7.5 × 1.9 5.32 FracBits-SAT (Yang & Jin, 2021)4/4 ✓ ✓ 71.6↓0.2% 71.8 8 × 1.8 5.35 Uhlich et al. (Uhlich et al., 2020)3.75/4 ✓ 69.8 70.2 8.5 × 1.6 5.01 RMSMP (Chang et al., 2021) 4/4 ✓ 69.0 71.9 8 × 1.8 5.35 DDQ (Zhang et al., 2021) 4/4 ✓ 71.8 71.9 8 × 1.8 5.35 Ours 3.79/8 ✓ ✓ 72.9↑1.0% 71.9 8.4× 1.6 10.1 3.79/4 ✓ ✓ 72.0↑0.1% 71.9 8.4× 1.6 5.07 † re-implementation for fair comparisons under the same backbone architectures centralized, and penalize the local distribution of weights in different quantization bins to be too dispersed. The effect of it is also shown in Fig. 5. Compared to other weights regularization methods which only consider the global dis- tribution, our EBR signiﬁcantly improves the quantization robustness of MPQ models. Table 4.Comparison of our Entropy-aware Bin Regularization (EBR) and other weight regularization methods for our mixed precision quantized ResNet18 on ImageNet-1K. Method Top-1 Acc Top-5 Acc Baseline 67.6 87.6 Weight Norm (Salimans & Kingma, 2016) 66.6 86.7 KURE (Shkolnik et al., 2020) 68.5 88.4 λE= 0.01 68.6 88.4 EBR λE= 0.1 69.1 88.5 λE= 1 68.9 88.4 How the knowledge distillation contributes to the training of MPQ models is shown in Table 5. Learning from FP models helps minimize the performance gap of quantization models. When the performance of FP teacher models is better, the performance of student models also improves. Table 5.Comparison of different teacher models of knowledge dis- tillation for our mixed precision quantized ResNet18 on ImageNet- 1K. One-hot label with CE loss is used for “w/o KD” in experiment. Method Teacher Top-1 Acc Top-5 Acc Ours w/o KD Ground Truth 70.5 89.5 ResNet34 70.7 89.7 Ours ResNet50 71.1 89.9 ResNet101 71.7 90.2 4.4. Visualization and Analysis To intuitively demonstrate the bitwidth assignment gener- ated by our SDQ, the quantization strategy of weights in different layers of ResNet18 and MobileNetV2 is visualized in Fig. 2(a) and Fig. 2(b), respectively. Our SDQ learns more bits for those layers with fewer parameters ( down- sample conv layers in ResNet18 and depthwise conv layers in MobileNetV2), which proves that more redundancy is expected for those layers. As layers with the same num- ber of parameters can have different optimal bitwidths, it can be proved that there is no simple heuristics to allocate bitwidth. We can also see from the bitwidth assignment that the ﬁrst few layers and the last few layers have higherSDQ: Stochastic Differentiable Quantization with Mixed Precision 0 3 6 9 12 15 18 Layer index of ResNet18 0 1 2 3 4 5 6 7 8Bitwidth regular conv downsample conv 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Number of Parameters 1e6 (a) ResNet18 (3.85bit) 0 6 12 18 24 30 36 42 48 Layer index of MobileNetV2 0 1 2 3 4 5 6 7 8Bitwidth pointwise conv depthwise conv 0 1 2 3 4 5 Number of Parameters 1e5  (b) MobileNetV2 (3.79bit, The ﬁrst 3 ×3 conv layer and last fc layer is not visualized here) Figure 2.Mixed Precision Quantization strategy generated by our SDQ for ResNet18 and MobileNetV2 on ImageNet-1K dataset. bitwidth. Fig. 3 depicts how bitwidth assignment evolves during the quantization strategy generation phase. While the decay of bitwidth is slow for downsample conv layers, the regular conv layers decay quickly at high precision and slow down when it reaches low precision. Compared to uniform quantization, the MPQ network per- forms better with fewer bits since the MPQ network fully utilizes the difference of layers’ sensitivity to quantization. Fig. 4(a) and Fig. 4(b) visualize the feature embedding of last conv layer in ResNet20. As expected, the clusters of dog and deer are mixed up together in the baseline model with 2-bit uniform quantization. While the model of SDQ can split those classes with more separable representations. We proposed Entropy-aware Bin Regularization to preserve the entropy and minimize the quantization error. Fig. 5 shows the histogram of weight distribution in full-precision and 2-bit quantization. We can observe that the weights in our model are more centralized locally to the quantiza- tion point in each quantization bin. Further, the portion of weights quantized to each bin in our model is closer compared to baseline, which preserves the entropy globally. /uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013/uni0000001c/uni00000013 /uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b /uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018 /uni00000019 /uni0000001a /uni0000001b /uni0000001c/uni00000025/uni0000004c/uni00000057/uni0000005a/uni0000004c/uni00000047/uni00000057/uni0000004b /uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000014/uni00000011/uni00000013/uni00000011/uni00000046/uni00000052/uni00000051/uni00000059/uni00000014 /uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000015/uni00000011/uni00000013/uni00000011/uni00000046/uni00000052/uni00000051/uni00000059/uni00000014 /uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000015/uni00000011/uni00000013/uni00000011/uni00000047/uni00000052/uni0000005a/uni00000051/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048 /uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000016/uni00000011/uni00000013/uni00000011/uni00000046/uni00000052/uni00000051/uni00000059/uni00000014 /uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000017/uni00000011/uni00000013/uni00000011/uni00000046/uni00000052/uni00000051/uni00000059/uni00000014 /uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000017/uni00000011/uni00000013/uni00000011/uni00000047/uni00000052/uni0000005a/uni00000051/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048 Figure 3.Bitwidth assignment of selected layers in ResNet18 dur- ing quantization strategy generation. 4.5. Hardware Efﬁciency on Accelerator We further conduct hardware experiments to evaluate the efﬁciency of our MPQ model. In Table 6, we evaluate our model on the accelerator that supports mixed precision arithmetic operation: Bit Fusion (Sharma et al., 2018). Bit Fusion only supports multiplications of power-of-two bits (i.e. 2, 4, 8, 16 bits), so there is a certain performance gap plane car bird cat deerdog frog horse ship truck (a) Baseline (2 bit) plane car bird cat deer dog frog horse ship truck (b) Mixed Ours (1.93 bit) Figure 4.Comparison of feature embedding visualization using t-SNE on CIFAR-10 evaluation set. The feature embedding are extracted from the output of last conv layer of ResNet20. The performance of these two models are reported in Table 1. 2  1  0 1 20.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 1e5 Weight distribution Ours w/o EBR FP Initilization (a) Weights w/o EBR 2  1  0 1 20.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 1e5 Weight distribution Ours w/ EBR FP Initilization (b) Weights w/ EBR 0.6  0.4  0.2  0.0 0.2 0.4 0.60.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1e6 Weight distribution Ours w/ EBR Ours w/o EBR (c) Quantization Figure 5.Histogram of the weight distribution and quantization distribution. 5(a) and 5(b) are the histogram of weights of layer4.1.conv2 with 2-bit precision in our quantized ResNet18 model with and without EBR respectively. 5(c) compares the quantization results with and without EBR. between theoretical compression and real compression. Our ResNet18 model with an average weight bitwidth of 3.85 achieves much better accuracy than the ResNet18 model with a weight bitwidth of 4, and surpasses its time cost and energy efﬁciency. Table 6.Comparison of latency and energy consumption of quan- tized ResNet18 model on Bit Fusion. Method Bitwidth (W/A) Top-1 Acc Latency Energy Dorefa 4/8 69.8 48.99 ms 93.34 mJ Ours 3.85/8 72.1 46.18 ms 90.18 mJ Dorefa 4/4 67.1 34.61 ms 64.16 mJ Ours 3.85/4 71.1 32.84 ms 60.49 mJ Dorefa 4/2 63.6 30.77 ms 54.08 mJ Ours 3.85/2 69.1 28.18 ms 49.05 mJSDQ: Stochastic Differentiable Quantization with Mixed Precision Table 7.Performance comparison on COCO detection benchmark. Method Bitwidth Hardware AP AP 50 AP75 APS APM APL Latency Energy FPS(W/A) YOLOv4-tiny (Bochkovskiy et al., 2020)32/32 GPU 20.4 38.7 19.5 7.4 24.1 27.1 - - - Dorefa (Zhou et al., 2016) 8/8 FPGA 16.1 32.3 14.5 4.3 17.3 24.7 34.18ms 268.3mJ 29 4/4 FPGA 15.4 29.2 12.5 3.7 16.9 23.3 18.64ms 146.3mJ 53 Ours + Dorefa 3.88/4 FPGA 15.9 31.1 14.7 4.2 17.6 24.2 21.28ms 167.1mJ 47 4.6. COCO Detection Hardware Experiment We also evaluate our SDQ on object detection task which is more challenging. We implement the mixed-precision com- pact YOLOv4-tiny (Bochkovskiy et al., 2020) with our SDQ on COCO detection dataset (Lin et al., 2014), which con- sists of 80 object categories. The training set contains 115k images ( trainval35k), and the validation set has 5k images (minival). Different from previous research (Li et al., 2019a; Wang et al., 2020) that apply quantized mod- els on Faster R-CNN (Ren et al., 2015) or RetinaNet (Lin et al., 2017) using ResNet and MobileNet as backbones, YOLOv4-tiny is already compact and sensitive to the quan- tization. Our quantization strategy generation and training phase are conducted on trainval35k partition. During the training phase, percentile activation calibration (Li et al., 2019a) is used to discard outlier activation values. Since our target deployment platform only supports power-of-two bitwidth, we set the bitwidth candidate B= {1,2,4,8}. The input size for the object detector is 416 ×416. Fol- lowing the standard COCO evaluation metric, we report the average precision (AP) for different IoU thresholds on the minival partition. The results, including latency and energy consumption when deploying on the FPGA system are shown in Table 7. From the table we can see that our SDQ improves the efﬁ- ciency of object detector without incurring signiﬁcant mAP degradation compared to the baseline quantization approach (8/8 bitwidths), while our detector with lower bitwidths (3.88/4) outperforms the 4-bit quantized model favorably. According to the FPGA deployment statistics, our SDQ demonstrates good hardware afﬁnity yielding latency and en- ergy consumption close to the 4-bit uniform single-precision model. Overall, mixed-precision YOLOv4-tiny trained with our SDQ can achieve similar performance of 8-bit quan- tized model while retaining the hardware efﬁciency of 4-bit quantized model. 4.6.1. FPGA S YSTEM SETTING The system is implemented on the Xilinx U50 FPGA plat- form and consumes 259688 LUTs and 210.5 BRAM. As shown in Fig. 6, a CNN accelerator system is comprised of multiple cores, controller, on-chip memory, downloader, and uploader. Each core consists of 4×16 dedicated array of INT-8 multiply-and-accumulate (MAC) processing ele- ments. All cores share the same on-chip memory, which stores the input feature map (ifmap), weight, index, and output feature map (ofmap). The index is used for weight sparse to scratch the input activation. The controller module uses instructions to take charge of convolution computation, data download, and upload. Ifmap  Memory Weight  & Index  Memory Core Downloader Ofmap  Memory Controller  Uploader CNN Accelerator External Mermory Core MAC MAC MAC MAC MAC MAC MAC MAC MAC MAC MAC MAC MAC MAC MACMAC MAC MAC MAC MAC MAC MAC MAC MAC MAC MAC MAC MAC MAC MAC MACMAC Dedicated MAC Array Figure 6.System architecture schematic. The parameters of our hardware experiment platform are: MAC array-4 rows ×16 columns, Frequency-200MHz, Number of Cores-8. 5. Conclusion We have presentedStochastic Differentiable Quantization (SDQ), a learning based mixed precision quantization frame- work that can optimize the optimal MPQ strategy automati- cally. In contrast to previous MPQ methods, SDQ introduces a set of Differentiable Bitwidth Parameters (DBPs) that fol- lows a stochastic quantization scheme to make DBPs differ- entiable with a stabler and smoother optimization process. Since SDQ is optimized on a global searching space, the learned MPQ strategy is usually better than other competi- tors. Moreover, Quantization Error Regularization (QER) and Entropy-aware Bin Regularization (EBR) are integrated into the MPQ learning and post-training stages to mini- mize the quantization error. We demonstrate the superiority of our SDQ with comprehensive experiments on different hardware platforms such as GPUs and FPGA across vari- ous networks. SDQ achieves state-of-the-art accuracy over previous quantization methods with fewer average bits. Ex- tensive hardware experiments prove the superior efﬁciency of our models deployed on mixed-precision accelerators. Acknowledgements This research was partially supported by ACCESS - AI Chip Center for Emerging Smart Systems, sponsored by InnoHK funding, Hong Kong SAR.SDQ: Stochastic Differentiable Quantization with Mixed Precision References Bengio, Y ., L´eonard, N., and Courville, A. Estimating or propagating gradients through stochastic neurons for con- ditional computation. arXiv preprint arXiv:1308.3432, 2013. Bochkovskiy, A., Wang, C.-Y ., and Liao, H.-Y . M. Yolov4: Optimal speed and accuracy of object detection. arXiv preprint arXiv:2004.10934, 2020. Cai, Z. and Vasconcelos, N. Rethinking differentiable search for mixed-precision neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2349–2358, 2020. Chang, S.-E., Li, Y ., Sun, M., Jiang, W., Liu, S., Wang, Y ., and Lin, X. Rmsmp: A novel deep neural network quantization framework with row-wise mixed schemes and multiple precisions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5251– 5260, 2021. Chen, W., Wang, P., and Cheng, J. Towards mixed-precision quantization of neural networks via constrained optimiza- tion. In Proceedings of the IEEE/CVF International Con- ference on Computer Vision, pp. 5350–5359, 2021. Choi, J., Wang, Z., Venkataramani, S., Chuang, P. I.-J., Srini- vasan, V ., and Gopalakrishnan, K. Pact: Parameterized clipping activation for quantized neural networks. arXiv preprint arXiv:1805.06085, 2018. Courbariaux, M., Bengio, Y ., and David, J.-P. Binarycon- nect: Training deep neural networks with binary weights during propagations. In Advances in neural information processing systems, pp. 3123–3131, 2015. Courbariaux, M., Hubara, I., Soudry, D., El-Yaniv, R., and Bengio, Y . Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1. arXiv preprint arXiv:1602.02830, 2016. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009. Dong, Y ., Ni, R., Li, J., Chen, Y ., Su, H., and Zhu, J. Stochas- tic quantization for learning accurate low-bit deep neural networks. International Journal of Computer Vision, 127 (11):1629–1642, 2019a. Dong, Z., Yao, Z., Cai, Y ., Arfeen, D., Gholami, A., Ma- honey, M. W., and Keutzer, K. Hawq-v2: Hessian aware trace-weighted quantization of neural networks. arXiv preprint arXiv:1911.03852, 2019b. Dong, Z., Yao, Z., Gholami, A., Mahoney, M. W., and Keutzer, K. Hawq: Hessian aware quantization of neural networks with mixed-precision. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 293–302, 2019c. Elthakeb, A., Pilligundla, P., Mireshghallah, F., Yazdan- bakhsh, A., Gao, S., and Esmaeilzadeh, H. Releq: An automatic reinforcement learning approach for deep quan- tization of neural networks. In NeurIPS ML for Systems workshop, 2018, 2019. Guo, Z., Zhang, X., Mu, H., Heng, W., Liu, Z., Wei, Y ., and Sun, J. Single path one-shot neural architecture search with uniform sampling. In European Conference on Computer Vision, pp. 544–560. Springer, 2020. Habi, H. V ., Jennings, R. H., and Netzer, A. Hmq: Hardware friendly mixed precision quantization block for cnns. In Computer Vision–ECCV 2020: 16th European Confer- ence, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXVI 16, pp. 448–463. Springer, 2020. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. Hinton, G., Vinyals, O., and Dean, J. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. Howard, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., and Adam, H. Mobilenets: Efﬁcient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. Jain, S. R., Gural, A., Wu, M., and Dick, C. H. Trained quantization thresholds for accurate and efﬁcient ﬁxed- point inference of deep neural networks. arXiv preprint arXiv:1903.08066, 2019. Jang, E., Gu, S., and Poole, B. Categorical repa- rameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016. Jouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal, G., Bajwa, R., Bates, S., Bhatia, S., Boden, N., Borchers, A., et al. In-datacenter performance analysis of a tensor processing unit. In Proceedings of the 44th annual inter- national symposium on computer architecture, pp. 1–12, 2017. Judd, P., Albericio, J., Hetherington, T., Aamodt, T. M., and Moshovos, A. Stripes: Bit-serial deep neural network computing. In 2016 49th Annual IEEE/ACM Interna- tional Symposium on Microarchitecture (MICRO) , pp. 1–12. IEEE, 2016.SDQ: Stochastic Differentiable Quantization with Mixed Precision Jung, S., Son, C., Lee, S., Son, J., Han, J.-J., Kwak, Y ., Hwang, S. J., and Choi, C. Learning to quantize deep net- works by optimizing quantization intervals with task loss. In Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition , pp. 4350–4359, 2019. Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009. Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classiﬁcation with deep convolutional neural networks. Advances in neural information processing systems, 25: 1097–1105, 2012. Li, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T. Visualizing the loss landscape of neural nets. In Pro- ceedings of the 32nd International Conference on Neural Information Processing Systems, pp. 6391–6401, 2018. Li, R., Wang, Y ., Liang, F., Qin, H., Yan, J., and Fan, R. Fully quantized network for object detection. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2810–2819, 2019a. Li, Y ., Dong, X., and Wang, W. Additive powers-of-two quantization: An efﬁcient non-uniform discretization for neural networks. In International Conference on Learn- ing Representations, 2019b. Lin, T.-Y ., Maire, M., Belongie, S., Hays, J., Perona, P., Ra- manan, D., Doll´ar, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In European conference on computer vision, pp. 740–755. Springer, 2014. Lin, T.-Y ., Goyal, P., Girshick, R., He, K., and Doll ´ar, P. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision , pp. 2980–2988, 2017. Liu, J., Cai, J., and Zhuang, B. Sharpness-aware quan- tization for deep neural networks. arXiv preprint arXiv:2111.12273, 2021. Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., and Zhang, C. Learning efﬁcient convolutional networks through net- work slimming. In Proceedings of the IEEE international conference on computer vision, pp. 2736–2744, 2017. Liu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T. Re- thinking the value of network pruning. In International Conference on Learning Representations, 2018. Ma, Y ., Jin, T., Zheng, X., Wang, Y ., Li, H., Jiang, G., Zhang, W., and Ji, R. Ompq: Orthogonal mixed precision quantization. arXiv preprint arXiv:2109.07865, 2021. Miyashita, D., Lee, E. H., and Murmann, B. Convolutional neural networks using logarithmic data representation. arXiv preprint arXiv:1603.01025, 2016. Nikoli´c, M., Hacene, G. B., Bannon, C., Lascorz, A. D., Courbariaux, M., Bengio, Y ., Gripon, V ., and Moshovos, A. Bitpruning: Learning bitlengths for aggressive and accurate quantization. arXiv preprint arXiv:2002.03090, 2020. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32:8026–8037, 2019. Pham, H., Guan, M., Zoph, B., Le, Q., and Dean, J. Efﬁcient neural architecture search via parameters sharing. In International Conference on Machine Learning, pp. 4095– 4104. PMLR, 2018. Ren, S., He, K., Girshick, R., and Sun, J. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28:91–99, 2015. Salimans, T. and Kingma, D. P. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. Advances in neural information process- ing systems, 29:901–909, 2016. Sharma, H., Park, J., Suda, N., Lai, L., Chau, B., Chan- dra, V ., and Esmaeilzadeh, H. Bit fusion: Bit-level dy- namically composable architecture for accelerating deep neural network. In 2018 ACM/IEEE 45th Annual Inter- national Symposium on Computer Architecture (ISCA), pp. 764–775. IEEE, 2018. Shkolnik, M., Chmiel, B., Banner, R., Shomron, G., Nahshan, Y ., Bronstein, A., and Weiser, U. Robust quan- tization: One model to rule them all. arXiv preprint arXiv:2002.07686, 2020. Tan, M. and Le, Q. Efﬁcientnet: Rethinking model scaling for convolutional neural networks. In International Con- ference on Machine Learning , pp. 6105–6114. PMLR, 2019. Uhlich, S., Mauch, L., Cardinaux, F., Yoshiyama, K., Garcia, J. A., Tiedemann, S., Kemp, T., and Nakamura, A. Mixed precision dnns: All you need is a good parametrization. In International Conference on Learning Representations, 2020. Wang, K., Liu, Z., Lin, Y ., Lin, J., and Han, S. Haq: Hardware-aware automated quantization with mixed pre- cision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8612– 8620, 2019.SDQ: Stochastic Differentiable Quantization with Mixed Precision Wang, Z., Wu, Z., Lu, J., and Zhou, J. Bidet: An efﬁcient bi- narized object detector. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2049–2058, 2020. Wen, W., Wu, C., Wang, Y ., Chen, Y ., and Li, H. Learning structured sparsity in deep neural networks. Advances in neural information processing systems, 29:2074–2082, 2016. Wu, B., Wang, Y ., Zhang, P., Tian, Y ., Vajda, P., and Keutzer, K. Mixed precision quantization of convnets via dif- ferentiable neural architecture search. arXiv preprint arXiv:1812.00090, 2018. Yamamoto, K. Learnable companding quantization for accurate low-bit neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5029–5038, 2021. Yang, H., Duan, L., Chen, Y ., and Li, H. Bsq: Explor- ing bit-level sparsity for mixed-precision neural network quantization. In International Conference on Learning Representations, 2020. Yang, L. and Jin, Q. Fracbits: Mixed precision quanti- zation via fractional bit-widths. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 35, pp. 10612–10620, 2021. Yao, Z., Dong, Z., Zheng, Z., Gholami, A., Yu, J., Tan, E., Wang, L., Huang, Q., Wang, Y ., Mahoney, M., et al. Hawq-v3: Dyadic neural network quantization. In Inter- national Conference on Machine Learning, pp. 11875– 11886. PMLR, 2021. You, Y .Audio Coding: Theory and Applications. Springer Science & Business Media, 2010. Yu, H., Han, Q., Li, J., Shi, J., Cheng, G., and Fan, B. Search what you want: Barrier panelty nas for mixed precision quantization. In European Conference on Computer Vi- sion, pp. 1–16. Springer, 2020. Zhang, D., Yang, J., Ye, D., and Hua, G. Lq-nets: Learned quantization for highly accurate and compact deep neural networks. In Proceedings of the European conference on computer vision (ECCV), pp. 365–382, 2018. Zhang, Z., Shao, W., Gu, J., Wang, X., and Luo, P. Dif- ferentiable dynamic quantization with mixed precision and adaptive resolution. In International Conference on Machine Learning, pp. 12546–12556. PMLR, 2021. Zhou, S., Wu, Y ., Ni, Z., Zhou, X., Wen, H., and Zou, Y . Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016.SDQ: Stochastic Differentiable Quantization with Mixed Precision Appendix This appendix includes additional analysis, implementation details, and extended experimental analysis not included in the main text due to space limitation. These contents are organized in separate sections as follows: • Sec. A analyzes how quantization error varies within different layers using mixed precision and how we propose a weighting coefﬁcient to balance between the precision. • Sec. B elaborates the performance of MPQ in different granularities of networks and the reasons why layer-wise quantization is optimal. • Sec. C includes the training details and experiment settings, such as the hyper-parameters, training dynamics, and detailed compression of bitwidth assignment as shown in Table 10. A. Quantization Error Analysis Quantization error Ω is deﬁned as L2 distance ||xq −xr||2 between quantized value xq and real value xr. In uniform quantization, Ωu depends on the characteristics of layers which includes parameter amounts, learned weight distribution, and bitwidth assignment. Among these factors, the most important one is the bitwidth of the layers. As shown in Table 8, the squared quantization error Ω2 u grows exponentially with the bitwidth. Table 8.Comparison of squared quantization error Ω2 u for various bitwidths of different layers in ResNet20. Layer Parameter Size 8-bit 6-bit 4-bit 3-bit 2-bit Layer1.2.conv1 2.3e4 0.03 0.52 9.25 41.9 191 Layer2.1.conv1 9.2e4 0.05 0.81 14.0 65.1 309 Layer3.1.conv1 3.7e5 0.29 4.83 84.8 392 2056 Uniform quantization deﬁned in Eq. 1 and Eq. 2 linearly maps full-precision weights wr into quantized representations wq. To remove the outlier of weight distribution, clamp operation is used to restrict the real value in the range[wl,wu]. The quantized output wq from uniform quantization scheme at b-bit with clamp can be deﬁned as: wq = Qu(wr; b,wl,wu) =s×round[clamp(wr; wl,wu)/s], (11) where [wl,wu] is the quantization range, ∆ = wu −wl is the range length, s= ∆ N−1 is the scaling factor, N = 2b is the total number of quantization bins. Previous work (You, 2010) has proved that the expected quantization error squared for the b-bit uniform quantizer is denoted as: E(Ω2 u; b,wl,wu) = s2 12 = C(b)∆2, (12) where C(b) = 1 12(2b−1)2 for the uniform distributions. This is the reason that we use λb = (2b −1)2 in Eq. 6 to balance between different bitwidths. In our stochastic quantization scheme, we assume the same quantization range [wl,wu] is used for neighboring quantization levels. The expectation of quantization error at bi bitwidth DBP βcan be computed as: E(Ω2 s; bi) =βbiE(Ω2 u; bi) + (1−βbi)E(Ω2 u; bi−1) = [βbiC(bi) + (1−βbi)C(bi−1)]∆2, (13) where C(bi) = 1 12(2bi−1)2 . Accordingly, the coefﬁcient λb = (2bi −1)2 can still be applied to balance the regularization term at neighboring quantization levels while amplifying the quantization error for lower bitbi−1 by [(2bi−1)/(2bi−1 −1)]2. B. SDQ on Different Granularities Our method uses the differentiability of DBPs to learn the bitwidth automatically. The advantage of the proposed SDQ is that DBPs can be inserted into different network granularities ﬂexibly. Table 9 compares the model performance, epochs for MPQ strategy generation, and optimization time per epoch using ResNet18 on different granularities.SDQ: Stochastic Differentiable Quantization with Mixed Precision Table 9.Comparison of Top-1 accuracy, total epochs for strategy generation, and time per epoch of different granularities with ResNet18 on ImageNet-1K. Granularity Bit-width(W/A) Top-1 Acc Epochs Time/epoch Net 4/4 68.7 30 47min Block 3.87/4 71.2 60 47min Layer 3.85/4 71.7 60 48min Kernel 3.91/4 71.8 90 58min As expected, while DBPs can be applied in different granularities, the layer-wise MPQ is still the optimal choice. When applying mixed precision to coarse-grained granularity, including the whole network and blocks, the performance of the trained MPQ network will be restricted because the difference of sensitivity within different components in the network is not fully utilized. Although searching the precision for ﬁner-grained granularity such as different kernels will improve the accuracy of the MPQ model, the time cost for the strategy generation will grow signiﬁcantly. Introducing more trainable parameters in the optimization process will result in more training instability. Moreover, current hardware accelerators rely highly on the parallelism of computing for the neural network. To the best of our knowledge, there are no kernel-wise hardware accelerators that successfully improve the efﬁciency in the real deployment of deep learning models. Based on these considerations, we focus more on layer-wise quantization when designing experiments. C. Training and Experimental Details C.1. Training settings and hyper-parameters When comparing the results of our SDQ with other quantization methods in Table 1 and Table 2, we use the training settings and hyper-parameters shown in Table 10. Generally, most of these hyper-parameters are the same during the MPQ generation and MPQ training phase, while we observe that a larger batch size yields better performance during MPQ training. Table 10.Detailed hyper-parameters and training scheme for different network architectures. Network ResNet20 on CIFAR10 ResNet18 on ImageNet-1K MobileNetV2 on ImageNet-1K Phase MPQ Generation MPQ Training MPQ Generation MPQ Training MPQ Generation MPQ Training Epoch 100 200 60 90 60 120 Batch Size 512 1024 256 512 256 512 Teacher - - ResNet101 ResNet101 ResNet101 ResNet101 Optimizer SGD SGD Adam Adam AdamW AdamW Initiallr 0.1 0.1 5e-4 1e-3 1e-3 1e-3 lr scheduler MultiStepLR MultiStepLR Consine Consine Consine Consine Weight decay 1e-4 1e-4 - - - - Warmup epochs - - - - 30 30 Random Crop ✓ ✓ ✓ ✓ ✓ ✓ Random Flip ✓ ✓ ✓ ✓ ✓ ✓ Color jittering - - ✓ ✓ - - λQin Eq. 10 1e-6 - 1e-7 - 1e-7 - λEin Eq. 6 - 5e-2 - 1e-2 - 1e-2 βthres 1e-4 - 1e-5 - 1e-5 - Bitwidth candidateB 1,2,3,4,5,6,7,8 - 2,3,4,5,6,7,8 - 2,3,4,5,6,7,8 - C.2. Training dynamics of SDQ models Fig. 7 illustrates the training loss and evaluation accuracy for our SDQ MobileNetV2 during the MPQ training phase. As shown in Fig. 1(c), quantized models are challenging to train as the latent non-smooth loss landscape makes models difﬁcult to converge with inaccurately estimated gradients. We found that our proposed Entropy-aware Bin Regularization targeting to preserve more information can stabilize the training. Especially, when the learning rate drops during training, our EBR can effectively reduce the jitters in loss and accuracy values.SDQ: Stochastic Differentiable Quantization with Mixed Precision 0 10000 20000 Training iterations of MobileNetV2 2.0 2.2 2.4 2.6 2.8 3.0Training Loss training loss training loss w/ EBR 25 50 75 100 125 Training Epochs of MobileNetV2 62 64 66 68 70 72Evaluation Top-1 Accuracy Validation Acc Validation Acc w/ EBR Figure 7.Training dynamics of SDQ with and without Entropy-aware Bin Regularization. C.3. Detailed Comparison of Quantization Strategy In Table 3, we compare our MobileNetV2 quantization strategy with Uhlich et al. (Uhlich et al., 2020) and FracBits (Yang & Jin, 2021) under the same initialization and training scheme. The results have demonstrated that our quantization is superior compared to previous strategies. All of the details on bitwidth assignment within each layer are shown in Fig. 8. We can see from the ﬁgure that all three bitwidth assignments follow similar patterns, such as the bitwidths for the ﬁrst few layers are higher than average. Generally, layers with more parameters will have lower precision, while the strategy here yields different bitwidths for layers with the same number of parameters. This supports our statement in Sec. 4.4 that there is no simple heuristics to assign bitwidth based on some particular metrics directly. 0 6 12 18 24 30 36 42 48 Layer index of MobileNetV2 2 3 4 5 6 7 8 9Bitwidth SDQ Uhlich et al. FracBits Figure 8.Comparison of quantization strategy of all layers in MobileNetV2.",
      "meta_data": {
        "arxiv_id": "2206.04459v3",
        "authors": [
          "Xijie Huang",
          "Zhiqiang Shen",
          "Shichao Li",
          "Zechun Liu",
          "Xianghong Hu",
          "Jeffry Wicaksana",
          "Eric Xing",
          "Kwang-Ting Cheng"
        ],
        "published_date": "2022-06-09T12:38:18Z",
        "pdf_url": "https://arxiv.org/pdf/2206.04459v3.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Stochastic Differentiable Quantization (SDQ), a novel method for automatically learning optimal mixed-precision quantization (MPQ) strategies. It addresses the limitations of previous MPQ approaches, which were often costly, sub-optimal, or suffered from unstable gradient approximations. SDQ employs Differentiable Bitwidth Parameters (DBPs) as probability factors in stochastic quantization between adjacent bitwidth choices, enabling a more flexible and globally-optimized search space with smoother gradient flow via the Gumbel-softmax reparameterization trick. After acquiring the optimal MPQ strategy, the network is further trained using Entropy-aware Bin Regularization (EBR) to preserve information and Knowledge Distillation (KD) from a full-precision teacher model. SDQ achieves state-of-the-art accuracy, often surpassing full-precision counterparts and other mixed/single-precision quantization methods, with lower average bitwidths on various networks and hardware platforms.",
        "methodology": "SDQ's methodology is a two-phase process: 1) Quantization Strategy Generation and 2) Training with Quantization Strategy. In phase 1, Differentiable Bitwidth Parameters (DBPs) are introduced for each layer and bitwidth candidate. Stochastic quantization selects between two adjacent bitwidths based on the DBP as a probability factor. The Straight-Through Gumbel SoftMax Estimator is used to enable smooth gradient backpropagation through the DBPs. A Quantization-Error Regularizer (LQER) is incorporated to penalize layers with high quantization error and a large number of parameters, helping balance different bitwidths. The optimization objective is a combination of task loss and LQER, with bitwidth assignments progressively decaying based on DBP thresholds. In phase 2, after the optimal layer-wise MPQ strategy is acquired, the network is trained using quantization-aware training. This phase employs Knowledge Distillation (LKD) to minimize the performance gap with a full-precision teacher model, using cross-entropy loss between output distributions. Additionally, Entropy-aware Bin Regularization (LEBR) is applied to regularize weights to preserve entropy and minimize quantization error by encouraging a uniform distribution across quantization bins and sharp local distributions around quantization points. Stochastic quantization is only used in phase 1, not during post-training or inference.",
        "experimental_setup": "Experiments were conducted on the CIFAR-10, ImageNet-1K, and COCO detection datasets. Evaluated network architectures include ResNet20 on CIFAR-10, and ResNet18 and MobileNetV2 on ImageNet-1K. For object detection, YOLOv4-tiny was evaluated on the COCO dataset. All models were initialized with real-value pre-trained weights. Standard data augmentations like RandomResizedCrop, RandomHorizontalFlip, and single-crop evaluation were used, with color jittering for some ImageNet models. The first and last layers' bitwidths were fixed due to their sensitivity. Training used optimizers like SGD, Adam, or AdamW, with learning rate schedulers like MultiStepLR or Cosine and optional warmup epochs. Performance was measured by Top-1 and Top-5 accuracy for classification tasks, and Average Precision (AP, AP50, AP75, APS, APM, APL) for object detection. Hardware deployment experiments were performed on NVIDIA GPUs and a real FPGA system (Xilinx U50 platform) utilizing the Bit Fusion accelerator, which supports power-of-two bitwidths (2, 4, 8, 16 bits). Latency and energy consumption were measured on these platforms. Ablation studies investigated the contributions of SDQ strategy generation, knowledge distillation, and Entropy-aware Bin Regularization. Visualization techniques like t-SNE were used to analyze feature embeddings and histograms for weight distributions.",
        "limitations": "The paper notes several practical limitations: (1) Current hardware accelerators do not support kernel-wise quantization, which limits the real-world deployment performance of such fine-grained quantization strategies. (2) Applying SDQ to fine-grained kernel-wise quantization leads to increased training instability due to a larger number of quantization parameters being optimized alongside weight parameters. (3) The proposed method utilizes a two-stage training strategy (strategy generation followed by post-training) because simultaneous optimization of DBPs and weight parameters is expected to cause more training instability. (4) The Bit Fusion hardware accelerator used for deployment only supports power-of-two bitwidths, introducing a gap between theoretical and actual compression performance.",
        "future_research_directions": "While not explicitly stated in a dedicated section, the paper's discussion implies several future research directions: (1) Developing new hardware accelerators that can efficiently support finer-grained mixed-precision quantization, such as kernel-wise assignment, to fully leverage the potential of such strategies. (2) Improving the training stability of differentiable quantization methods when applied to finer granularities like kernel-wise quantization, potentially by addressing the challenges of optimizing many quantization parameters simultaneously with network weights. (3) Exploring methods to integrate the quantization strategy generation and model training into a single, end-to-end differentiable optimization process, moving beyond the current two-stage approach to potentially achieve greater efficiency and effectiveness."
      }
    }
  ],
  "new_method": {
    "method": "Open Problems: • Present 2-bit KV-cache quantizers give only loose, empirical quality guarantees.  No method converts a per-element error budget into an *end-to-end* bound on (i) every attention logit *and* (ii) the downstream token-probability shift.  \n• Importance of individual dimensions varies with the *local* query, not just its expected magnitude: static channel weights over-protect frequently small but occasionally vital features.\n• Metadata of existing cross-layer or residual-patch schemes still scales with context length; on 32 k tokens this offsets much of the nominal 2-bit saving.\n• Edge devices without CUDA (e.g. Raspberry Pi, ESP-NN accelerators) lack a unified low-precision kernel; current work ignores integer-only pipelines and on-the-fly decompression costs.\n• Regulatory frameworks (EU AI Act) require certified bounds on fairness metrics under numerical approximation.  No KV-quantizer ties its error certificate to bias / toxicity drift.\nMethods: We propose CREW-Q (Certified, Re-weighted, Entropy-bounded, WBits≈1.97) comprising four innovations:\nA. Query-Aware Perturbation Bound  –  derive a tight Lipschitz-based inequality linking element-wise quantisation noise δx to two quantities that upper-bound *every* downstream probability change: (1) local Jacobian J=softmax(QKᵀ/√d)·V and (2) a pre-computed spectral norm of V.  At runtime we compute ‖J‖₁ cheaply via a 16-entry lookup per head and guarantee Δp≤ε.\nB. On-line Importance Rescaling  –  instead of static E[|q_i|], we keep a 6-bit EWMA of |q_i| per head updated every 32 tokens (single int8 op).  The centroid spacing for the next block is temperature-scaled by this EWMA, giving burst-adaptive protection without extra bits.\nC. Zero-Overhead Layer-Fusion  –  we embed a shared rank-2 projector *inside* the MiniCache merge so that one set of 2×d int8 factors corrects four layers *and* performs SLERP merging; parameters are stored once per block (0.005 bits/elt, independent of sequence length).\nD. Entropy-Coded Residual Streams  –  sparse fix-ups are bucket-RLE-encoded with Asymmetric Numeral Systems; expected cost at 32 k ctx is 0.97 bits/patch, driving the **overall mean precision to 1.97 bits**.\nE. Hardware Path  –  supply a single C reference kernel that auto-specialises via SIMD macros to CUDA, ARM-NEON and RVV.  For integer-only NPUs we fuse de-quant + mat-vec in 2-cycle MAC pipelines, verified on Raspberry Pi 5 and ESP-32-S3.\nExperimental Setup: Models: Llama-3-8B-Instruct, Mistral-7B, Gemma-7B, Falcon-7B-Instruct, Phi-3-Mini-4k, plus *one* vision-encoder model (SigLIP-So400m) to test modality-transfer.\nTasks: WikiText-2 & C4 PPL; LongBench-Needle; GSM8K few-shot; HumanEval pass@1; BBQ, Winogender, RealToxicityPrompts for bias/toxicity; Vision: Zero-shot ImageNet-1k.\nEdge benches: MacBook M3, Raspberry Pi 5 (64-bit), Pixel 8 Pro, ESP-32-S3 (integer only, 20 MHz) running 128-token summarisation.\nBaselines: fp16, KIVI-2b, SAFE-Q (ours minus new parts), ZipCache 2+4 b.\nMetrics:  ΔPPL/Acc; certified ε_prob; observed bias drift; mean bits/elt; latency & joules via on-device meters.\nAblations: disable on-line rescaling; replace spectral bound with empirical; remove entropy coder; vary projector rank.\nExpected Result: • Average storage 1.97 bits/elt incl. all metadata.\n• ΔPPL ≤0.04 on all text models; GSM8K drop ≤0.15 %.  SigLIP zero-shot top-1 drops ≤0.3 %.\n• Proven ε_prob ≤7×10⁻⁴ ⇒ bias and toxicity scores change <0.0015 – first regulatory-grade certificate.\n• Raspberry Pi 5 decodes Gemma-7B at 0.55 tok/s in 6.8 GB RSS; ESP-32-S3 streams 2-bit Phi-3 at 0.05 tok/s, impossible with prior art.\n• Metadata growth sub-linear: +3 % bytes when moving from 4 k to 32 k context (vs +28 % for HECQ).\n• Latency overhead over pure 2-bit dense lookup ≤2 %.\nExpected Conclusion: CREW-Q demonstrates that sub-2-bit KV caches are feasible *and* certifiably safe.  By integrating a query-aware perturbation bound with burst-adaptive codebooks and entropy-coded residuals, we cut real precision to 1.97 bits while shrinking worst-case probability error below 10⁻³, satisfying forthcoming AI-safety regulations.  The universal SIMD kernel proves portability from high-end GPUs to toy micro-controllers, opening privacy-preserving LLM inference on ubiquitous edge hardware and setting a new lower bound for reliable quantisation."
  },
  "idea_info_history": [
    {
      "idea": {
        "open_problems": "Current 2-bit KV-cache schemes (KIVI, CQ, ZipCache, KVQuant) still need a non-negligible fp16 “residual window”, 4-bit fallback, or fail on some architectures (e.g. Falcon-7B) because a single 2-bit codeword cannot faithfully reconstruct every token/channel.  \nThe unresolved challenge is therefore:  \n“Can we keep the *average* storage strictly at ≈2 bits / element while guaranteeing almost-lossless accuracy for **all** mainstream decoder-only LLMs, without model retraining or fp16 residuals?”",
        "methods": "Hierarchically-Error-Compensated Quantization (HECQ)  \n1. Base 2-bit code  (K0,V0)  \n   • Per-channel k-means (k=4) centroids learned offline on a 16-sample calibration set (like CQ).  \n   • Every new key/value vector stores a 2-bit index to its centroid (≈2 bits/elt).  \n2. Lightweight residual predictor  \n   • A rank-1 projector P=uv^T (32 float numbers / layer) learned offline maps the centroid to the true vector:  \\hat x = c_i + P·(c_i).  \n   • This fixes most systematic errors with <0.01 % extra memory.  \n3. Event-driven sparse corrections (ESC)  \n   • During decoding we compute |x−\\hat x|_∞; if it exceeds a fixed threshold τ, we write an 8-bit “patch”: (token_id, channel_id, signed 4-bit delta).  \n   • Probability of a patch is <1 % so average extra cost ≤0.05 bits.  \n4. Saliency-gating  \n   • Borrow ZipCache’s normalized attention score to *skip* patches on tokens whose contribution to attention is <γ.  \n5. CUDA kernels  \n   • One fused kernel performs: look-up centroid → apply rank-1 projector → optionally add sparse patches.  \n   • Memory layout identical to KIVI, so it can drop-in replace the existing Triton kernels.  \nDifferences vs prior work  \n• KVQuant: uses 3–4 bits uniform, no residual predictor.  \n• CQ: couples channels but still stores >2 bits.  \n• ZipCache: adaptive bit-width, but needs per-token metadata; HECQ keeps constant ≈2.05 bits.  \n• MiniCache: merges layers, orthogonal – can be combined.",
        "experimental_setup": "Models: Llama-2-7B, Falcon-7B, Mistral-7B   \nDatasets / tasks:  \n• WikiText-2 and C4 perplexity (teacher forcing).  \n• LongBench ‘Needle-in-a-Haystack’ retrieval.  \n• GSM8K few-shot (chain-of-thought) – exact-match accuracy.  \nBaselines: fp16, KIVI-2b, CQ-2b, KVQuant-3b.  \nMetrics:  \n• Quality: ΔPPL, ΔAcc vs fp16.  \n• Memory: average bits/elt measured at runtime.  \n• Latency: A100-80 GB, prompt 2 k and decode 1 k tokens.  \nAblations:  \n1. remove projector; 2. remove ESC; 3. vary τ, γ.  \nRobustness: run on Falcon-7B (multi-query) and with 32 k context to verify stability across architectures and lengths.",
        "expected_result": "• HECQ keeps mean storage 2.05 bits/elt (≤+3 % to pure 2-bit).  \n• ΔPPL ≤0.05 on all three models – Falcon-7B no longer needs 4-bit fallback (baseline KIVI-2b shows +0.6 PPL).  \n• GSM8K accuracy drop <0.3 %, matching fp16 and beating other 2-bit methods by ≥1 %.  \n• Latency overhead <5 % vs KIVI due to single fused kernel; still 2.4× faster than fp16.  \n• Ablation shows projector contributes ~70 % of quality gain, ESC the rest; saliency-gating halves number of patches without hurting quality.",
        "expected_conclusion": "The proposed Hierarchically-Error-Compensated Quantization demonstrates that strictly 2-bit-level KV-cache storage is attainable without accuracy compromise or fp16 windows.  \nAcademically, it unifies codebook, low-rank correction, and sparse, saliency-aware error patches into a mathematically simple yet hardware-friendly pipeline, filling the accuracy gap left by existing 2-bit schemes.  \nPractically, it enables >5× memory saving and >2× throughput on commodity GPUs for *any* off-the-shelf LLM, simplifying large-context deployment.  \nFuture work: (i) learn per-layer projectors during pre-training for even tighter 1-bit targets; (ii) integrate with MiniCache depth-compression; (iii) extend ESC to activation outliers during prompt-phase processing."
      },
      "evaluate": {
        "novelty_reason": "Existing 2-bit KV-cache schemes either (a) keep a sliding fp16 / 4-bit residual window (KIVI, KVQuant) or (b) exceed 2-bit on average due to per-token metadata or adaptive bit-width (ZipCache, CQ).  HECQ proposes a qualitatively different error-control hierarchy: 1) a fixed 2-bit centroid codebook like CQ but followed by 2) an ultra–light, learned rank-1 projector that corrects systematic, layer-specific bias with <0.01 bit/token – no prior KV work applies learned low-rank corrections at inference. 3) ‘Event-driven sparse corrections’ write 8-bit patches only for tokens/channels whose residual crosses a threshold; unlike KVQuant’s static outlier separation this is online, threshold-based and amortises to ≤0.05 bit. 4) Saliency-gating drops those patches if the token’s normalised attention weight is low, re-using ZipCache’s saliency metric but for dynamic residual suppression rather than bit-width selection.  These components jointly guarantee a *strict* ≈2.05 bit budget independent of architecture, something none of the related works achieves.  The single fused CUDA kernel that composes centroid lookup, rank-1 projection and sparse patch addition is also new.",
        "novelty_score": 8,
        "significance_reason": "If the promised results hold, HECQ would remove the last accuracy gap that prevents real-world deployment of strictly-2-bit KV caches on arbitrary decoder-only LLMs, including multi-query models such as Falcon-7B that break KIVI.  This yields a >5× memory reduction and >2× throughput on commodity GPUs without any model retraining, directly affecting cost and carbon footprint of long-context inference and edge deployment.  Academically, it introduces a new hybrid quantisation paradigm (codebook + low-rank + sparse, saliency-aware patches) that could be generalised to activations beyond KV caches.  While implementation complexity is moderate, the method tackles a central scalability bottleneck and could shift the practical ceiling for context length on single-GPU systems.  Therefore its potential impact is high, though dependent on empirical validation across more models and tasks.",
        "significance_score": 8
      }
    },
    {
      "idea": {
        "open_problems": "1. Current 2-bit KV–cache schemes still require fp16 fallback windows or ≥4-bit outlier channels on some models, and they treat every key/value dimension as equally important although most errors have almost no impact on the dot-product with the query.\n2. None of the existing works provides a provable, model-agnostic error bound that translates a given per-element quantisation error into an upper-bounded attention‐score distortion, which is crucial for safety-critical or bias-sensitive deployments.\n3. Low-resource/edge LLM use-cases need a single universal kernel that reaches <2 bits/elt on consumer GPUs **and** on ARM/Apple Neural Engine without vendor-specific instructions.\n4. Cross-layer redundancy (MiniCache) and in-layer low-rank structure (HECQ/KVQuant) have been exploited separately; a joint scheme that re-uses one set of correction parameters for *multiple* layers is unexplored and would further shrink metadata.",
        "methods": "We propose ‘SAFE-Q’ (Saliency-Aware & Formal-Error-bounded Quantisation), extending HECQ with two new ideas and a theoretical safety layer:\nA. Saliency-Weighted Codebook  –  we analytically bound the change in scaled dot-product attention when a key/value element x is quantised.  The bound depends on |q_i| (query magnitude) and on the softmax temperature.  We pre-compute per-channel importance weights w_i=E[|q_i|] on a 16-sample calibration set.  During run-time the 2-bit centroid table is *non-uniformly* allocated: high-w_i channels get 6–8 centroids, low-w_i channels get 2–3, yet indices are encoded through Huffman coding so the **average remains 2.0 bits**.\nB. Cross-Layer Shared Projector  –  observe that layers L/2…L often learn very similar value bases.  We therefore learn one rank-2 projector P_shared per *block* of 4 consecutive decoder layers (instead of per layer).  This cuts metadata by 4× and is still <0.02 bits/token.\nC. Certified Sparse Fix-Ups  –  for the remaining hard tokens we keep ESC but store, besides the δ, a 1-byte Merkle-hash seed.  At inference the kernel verifies ‖error‖_∞≤τ; if violated, the token is recomputed in fp16 on-the-fly.  This gives a deterministic worst-case error bound without retraining.\nD. Hardware path  –  on CUDA we keep the single-pass Triton kernel; on Apple M-series/Android we implement identical logic in Metal/Neon using table–lookup + fused mat-vec so SAFE-Q can serve on-device chat apps.",
        "experimental_setup": "Models: Llama-2-7B, Falcon-7B, Mistral-7B, Gemma-7B, Llama-3-8B-Instruct.\nQuality tasks: WikiText-2 & C4 perplexity; LongBench Needle; GSM8K few-shot; Bias benchmarks BBQ & Winogender to test error bounds; toxicity detection (RealToxicityPrompts) to ensure no error amplification.\nEdge scenario: run Gemma-7B on MacBook M2 Pro and Pixel 8 Pro.\nBaselines: fp16, KIVI-2b, HECQ (original), ZipCache 2+4b.\nMetrics:  (i) ΔPPL, ΔAcc; (ii) certified max attention error ε_cert (ours only); (iii) measured avg bits/elt; (iv) latency & energy (measured via NVIDIA-SMI / Android battery stats).\nAblations: remove saliency weighting; remove shared projector; vary Huffman table depth; set τ higher/lower.",
        "expected_result": "• Avg storage ≤2.00 bits/elt (no +3 % overhead).  \n• All models keep ΔPPL ≤0.05 and GSM8K accuracy drop ≤0.2 %.  Falcon-7B passes with pure 2-bit again.  \n• Certified bound: ε_cert ≤2e-3 ⇒ maximum change in any softmax entry ≤0.6 %.  In BBQ/WinoGender, bias scores differ by <0.002 from fp16, beating other quantisers.  \n• MacBook M2 runs Gemma-7B at 9 tok/s (×2.1 vs fp16) within 14 GB RAM; Pixel 8 Pro at 1.3 tok/s.  \n• Latency overhead vs HECQ ≤3 %; metadata reduced by 3.7× due to cross-layer sharing.",
        "expected_conclusion": "SAFE-Q shows that strictly 2-bit KV-cache is not only memory-efficient but can be **formally safe**: we provide the first quantiser with a provable per-token attention-error bound, eliminating fp16 fallbacks while upholding fairness & toxicity metrics.  The saliency-weighted codebook and cross-layer projector cut metadata so that the real average hits 2.0 bits, and dual-path CUDA/Metal kernels bring >2× throughput to both data-centre GPUs and consumer phones.  This advances sustainable, privacy-preserving on-device LLM inference and opens a path toward future 1-bit certified quantisation."
      },
      "evaluate": {
        "novelty_reason": "SAFE-Q combines several ideas that are not present – or only partially present – in existing 2-bit KV-cache work. (1) It derives an explicit, model-agnostic upper bound that converts per-element quantisation error into a worst-case soft-max-attention distortion (ε_cert) and enforces it at run time; none of KIVI, KVQuant, ZipCache, CQ, HECQ or MiniCache provide a formal guarantee or an online verifier. (2) The Saliency-Weighted Codebook allocates an unequal number of 2-bit centroids to each key/value channel based on E[|q_i|]; this is different from KVQuant’s Fisher-weighted per-layer datatype and ZipCache’s token-level saliency, and the average bit-rate is kept at exactly 2.0 bits through Huffman coding – previous methods either rely on dense 2-bit with fp16 fall-backs (KIVI) or average >2 bits (ZipCache 2+4 b). (3) Cross-Layer Shared Projector re-uses one rank-2 correction matrix for four decoder layers, a cross-depth reuse that MiniCache does not exploit and that existing in-layer low-rank schemes (HECQ) keep private to each layer. (4) Certified Sparse Fix-Ups employ a 1-byte Merkle seed plus online fp16 re-compute only for bound violations, giving deterministic accuracy without pre-allocating an fp16 window or 4-bit outlier channel. (5) A single source kernel targets both CUDA/Triton and Metal/NEON, whereas prior work is vendor-specific. The combination of these elements constitutes a method that is measurably different from the current 2-bit state of the art.",
        "novelty_score": 8,
        "significance_reason": "Academically, SAFE-Q offers the first provably bounded-error quantiser for attention, advancing the theoretical understanding of how low-precision KV errors propagate through the soft-max. Practically, it achieves a strict 2.00 bit budget with no fp16 slice or 4-bit escape channel, cuts metadata 3-4×, and still matches fp16 perplexity across five 7-8 B LLMs. The certified bound keeps bias/toxicity drift <0.002, which is valuable for safety-critical or regulatory contexts. The dual CUDA/Metal implementation demonstrates viability on both data-centre GPUs and consumer phones (9 tok/s on M2, 1.3 tok/s on Pixel 8 Pro), addressing the societal need for privacy-preserving on-device LLMs. These contributions make the method likely to influence both future low-bit quantisation research and edge deployment practice.",
        "significance_score": 8
      }
    },
    {
      "idea": {
        "open_problems": "• Present 2-bit KV-cache quantizers give only loose, empirical quality guarantees.  No method converts a per-element error budget into an *end-to-end* bound on (i) every attention logit *and* (ii) the downstream token-probability shift.  \n• Importance of individual dimensions varies with the *local* query, not just its expected magnitude: static channel weights over-protect frequently small but occasionally vital features.\n• Metadata of existing cross-layer or residual-patch schemes still scales with context length; on 32 k tokens this offsets much of the nominal 2-bit saving.\n• Edge devices without CUDA (e.g. Raspberry Pi, ESP-NN accelerators) lack a unified low-precision kernel; current work ignores integer-only pipelines and on-the-fly decompression costs.\n• Regulatory frameworks (EU AI Act) require certified bounds on fairness metrics under numerical approximation.  No KV-quantizer ties its error certificate to bias / toxicity drift.",
        "methods": "We propose CREW-Q (Certified, Re-weighted, Entropy-bounded, WBits≈1.97) comprising four innovations:\nA. Query-Aware Perturbation Bound  –  derive a tight Lipschitz-based inequality linking element-wise quantisation noise δx to two quantities that upper-bound *every* downstream probability change: (1) local Jacobian J=softmax(QKᵀ/√d)·V and (2) a pre-computed spectral norm of V.  At runtime we compute ‖J‖₁ cheaply via a 16-entry lookup per head and guarantee Δp≤ε.\nB. On-line Importance Rescaling  –  instead of static E[|q_i|], we keep a 6-bit EWMA of |q_i| per head updated every 32 tokens (single int8 op).  The centroid spacing for the next block is temperature-scaled by this EWMA, giving burst-adaptive protection without extra bits.\nC. Zero-Overhead Layer-Fusion  –  we embed a shared rank-2 projector *inside* the MiniCache merge so that one set of 2×d int8 factors corrects four layers *and* performs SLERP merging; parameters are stored once per block (0.005 bits/elt, independent of sequence length).\nD. Entropy-Coded Residual Streams  –  sparse fix-ups are bucket-RLE-encoded with Asymmetric Numeral Systems; expected cost at 32 k ctx is 0.97 bits/patch, driving the **overall mean precision to 1.97 bits**.\nE. Hardware Path  –  supply a single C reference kernel that auto-specialises via SIMD macros to CUDA, ARM-NEON and RVV.  For integer-only NPUs we fuse de-quant + mat-vec in 2-cycle MAC pipelines, verified on Raspberry Pi 5 and ESP-32-S3.",
        "experimental_setup": "Models: Llama-3-8B-Instruct, Mistral-7B, Gemma-7B, Falcon-7B-Instruct, Phi-3-Mini-4k, plus *one* vision-encoder model (SigLIP-So400m) to test modality-transfer.\nTasks: WikiText-2 & C4 PPL; LongBench-Needle; GSM8K few-shot; HumanEval pass@1; BBQ, Winogender, RealToxicityPrompts for bias/toxicity; Vision: Zero-shot ImageNet-1k.\nEdge benches: MacBook M3, Raspberry Pi 5 (64-bit), Pixel 8 Pro, ESP-32-S3 (integer only, 20 MHz) running 128-token summarisation.\nBaselines: fp16, KIVI-2b, SAFE-Q (ours minus new parts), ZipCache 2+4 b.\nMetrics:  ΔPPL/Acc; certified ε_prob; observed bias drift; mean bits/elt; latency & joules via on-device meters.\nAblations: disable on-line rescaling; replace spectral bound with empirical; remove entropy coder; vary projector rank.",
        "expected_result": "• Average storage 1.97 bits/elt incl. all metadata.\n• ΔPPL ≤0.04 on all text models; GSM8K drop ≤0.15 %.  SigLIP zero-shot top-1 drops ≤0.3 %.\n• Proven ε_prob ≤7×10⁻⁴ ⇒ bias and toxicity scores change <0.0015 – first regulatory-grade certificate.\n• Raspberry Pi 5 decodes Gemma-7B at 0.55 tok/s in 6.8 GB RSS; ESP-32-S3 streams 2-bit Phi-3 at 0.05 tok/s, impossible with prior art.\n• Metadata growth sub-linear: +3 % bytes when moving from 4 k to 32 k context (vs +28 % for HECQ).\n• Latency overhead over pure 2-bit dense lookup ≤2 %.",
        "expected_conclusion": "CREW-Q demonstrates that sub-2-bit KV caches are feasible *and* certifiably safe.  By integrating a query-aware perturbation bound with burst-adaptive codebooks and entropy-coded residuals, we cut real precision to 1.97 bits while shrinking worst-case probability error below 10⁻³, satisfying forthcoming AI-safety regulations.  The universal SIMD kernel proves portability from high-end GPUs to toy micro-controllers, opening privacy-preserving LLM inference on ubiquitous edge hardware and setting a new lower bound for reliable quantisation."
      },
      "evaluate": {
        "novelty_reason": "CREW-Q introduces (1) a closed-form Lipschitz-style perturbation inequality that converts element-wise KV-noise into a provable bound on every downstream attention logit and token probability – something none of the prior 2-bit schemes (KIVI, KVQuant, CQ, ZipCache, MiniCache) attempt, all of which rely on empirical quality checks; (2) a query-aware on-line Jacobian estimate that is recomputed per block with only a 16-entry table, whereas existing methods use static per-channel statistics; (3) burst-adaptive centroid spacing via a 6-bit EWMA updated during decoding, contrasting with the frozen code-books of KIVI / KVQuant; (4) a rank-2 projector that simultaneously performs cross-layer cache fusion and correction at essentially zero extra bits – MiniCache merges layers but stores separate metadata and is 4-bit+; (5) entropy-coded residual “fix-ups” that pull the *average* precision below 2 bits (1.97) including metadata, lower than any related work; and (6) the first quantization framework that ships a single C/SIMD kernel covering CUDA, ARM-NEON and RVV and is validated on integer-only micro-controllers.  Together these elements constitute a combination of formal certification, adaptive precision control, and hardware portability not present in the surveyed literature.",
        "novelty_score": 9,
        "significance_reason": "Achieving certified sub-2-bit KV caches with <10⁻³ worst-case probability shift has direct academic impact on quantisation theory (provides a tight end-to-end bound) and satisfies upcoming regulatory demands (EU AI Act) for fairness guarantees under numerical approximation.  Practically, lowering average precision to 1.97 bits while keeping ΔPPL ≤0.04 enables 32 k-token windows on commodity or even micro-controller hardware, opening privacy-preserving edge inference.  The method therefore advances both the state-of-the-art compression ratio and the safety/portability frontier.  Limitations (added complexity, need for spectral pre-compute) temper, but do not negate, its importance.",
        "significance_score": 8
      }
    }
  ]
}