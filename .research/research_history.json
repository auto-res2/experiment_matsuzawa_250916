{
  "research_topic": "Graph Attention Networkの学習の高速化",
  "queries": [
    "fast GAT training",
    "GAT training optimization",
    "graph attention acceleration",
    "efficient graph attention",
    "scalable GAT parallelization"
  ],
  "research_study_list": [
    {
      "title": "Are GATs Out of Balance?"
    },
    {
      "title": "Bandit Samplers for Training Graph Neural Networks"
    },
    {
      "title": "Dynamic Rescaling for Training GNNs"
    },
    {
      "title": "Transformer Quality in Linear Time"
    },
    {
      "title": "Transformer Quality in Linear Time"
    },
    {
      "title": "Are GATs Out of Balance?"
    },
    {
      "title": "Dynamic Rescaling for Training GNNs"
    },
    {
      "title": "Bandit Samplers for Training Graph Neural Networks"
    },
    {
      "title": "GATE: How to Keep Out Intrusive Neighbors"
    },
    {
      "title": "Adaptive Structural Fingerprints for Graph Attention Networks"
    },
    {
      "title": "Bandit Samplers for Training Graph Neural Networks"
    },
    {
      "title": "SAC: Accelerating and Structuring Self-Attention via Sparse Adaptive Connection"
    },
    {
      "title": "FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision"
    },
    {
      "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"
    },
    {
      "title": "KDEformer: Accelerating Transformers via Kernel Density Estimation"
    },
    {
      "title": "SAC: Accelerating and Structuring Self-Attention via Sparse Adaptive Connection"
    },
    {
      "title": "Adaptive Structural Fingerprints for Graph Attention Networks"
    },
    {
      "title": "Transformers meet Stochastic Block Models: Attention with Data-Adaptive Sparsity and Cost"
    },
    {
      "title": "A Flexible, Equivariant Framework for Subgraph GNNs via Graph Products and Graph Coarsening"
    },
    {
      "title": "Efficient Attention via Control Variates"
    },
    {
      "title": "GOAT: A Global Transformer on Large-scale Graphs"
    },
    {
      "title": "GNNAutoScale: Scalable and Expressive Graph Neural Networks via Historical Embeddings"
    },
    {
      "title": "Graph Ladling: Shockingly Simple Parallel GNN Training without Intermediate Communication"
    },
    {
      "title": "Dynamic Rescaling for Training GNNs"
    },
    {
      "title": "A Comprehensive Study on Large-Scale Graph Training: Benchmarking and Rethinking"
    }
  ]
}